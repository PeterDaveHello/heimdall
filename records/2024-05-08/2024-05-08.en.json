[
  {
    "id": 40286029,
    "title": "Apple Unveils Powerful M4 Chip for New iPad Pro",
    "originLink": "https://www.apple.com/newsroom/2024/05/apple-introduces-m4-chip/",
    "originBody": "Apple Newsroom needs your permission to enable desktop notifications when new articles are published PRESS RELEASE May 7, 2024 Apple introduces M4 chip M4 enables the breakthrough design and stunning display of the new iPad Pro, while delivering a giant leap in performance M4 is a system on a chip (SoC) that advances the industry-leading power-efficient performance of Apple silicon. CUPERTINO, CALIFORNIA Apple today announced M4, the latest chip delivering phenomenal performance to the all-new iPad Pro. Built using second-generation 3-nanometer technology, M4 is a system on a chip (SoC) that advances the industry-leading power efficiency of Apple silicon and enables the incredibly thin design of iPad Pro. It also features an entirely new display engine to drive the stunning precision, color, and brightness of the breakthrough Ultra Retina XDR display on iPad Pro. A new CPU has up to 10 cores, while the new 10-core GPU builds on the next-generation GPU architecture introduced in M3, and brings Dynamic Caching, hardware-accelerated ray tracing, and hardware-accelerated mesh shading to iPad for the first time. M4 has Apple’s fastest Neural Engine ever, capable of up to 38 trillion operations per second, which is faster than the neural processing unit of any AI PC today. Combined with faster memory bandwidth, along with next-generation machine learning (ML) accelerators in the CPU, and a high-performance GPU, M4 makes the new iPad Pro an outrageously powerful device for artificial intelligence. “The new iPad Pro with M4 is a great example of how building best-in-class custom silicon enables breakthrough products,” said Johny Srouji, Apple’s senior vice president of Hardware Technologies. “The power-efficient performance of M4, along with its new display engine, makes the thin design and game-changing display of iPad Pro possible, while fundamental improvements to the CPU, GPU, Neural Engine, and memory system make M4 extremely well suited for the latest applications leveraging AI. Altogether, this new chip makes iPad Pro the most powerful device of its kind.” 1) tandem OLED support, 2) brightness and color compensation, and 3) 10Hz-120Hz ProMotion support. A graphic representing the new M4 chip, highlighting its four performance cores and detailing their 1) improved branch prediction, 2) wider decode and execution engines, and 3) next-generation ML accelerators. The graphic also highlights M4’s six efficiency cores, and details their 1) improved branch prediction, 2) deeper execution engine, and 3) next-generation ML accelerators. A graphic representing the new M4 chip, highlighting its 10-core GPU and detailing its 1) next-generation architecture, 2) Dynamic Caching, 3) mesh shading, and 4) ray tracing. A graphic representing the new M4 chip, highlighting its Neural Engine and detailing its 1) 16-core design and 2) that it’s faster and more efficient. M4 features a new display engine enabling the stunning precision, color accuracy, and brightness uniformity of the Ultra Retina XDR display. The new CPU of M4 delivers up to 1.5x faster CPU performance over the powerful M2 in the previous iPad Pro. Pro rendering with the new up-to-10-core GPU of M4 is now up to four times faster than on M2. M4 includes Apple’s most powerful Neural Engine ever, capable of 38 trillion operations per second — 60x faster than the first Neural Engine in A11 Bionic. previous next New Technologies Enabling the New iPad Pro Delivering a giant leap in performance over the previous iPad Pro with M2, M4 consists of 28 billion transistors built using a second-generation 3-nanometer technology that further advances the power efficiency of Apple silicon. M4 also features an entirely new display engine designed with pioneering technologies, enabling the stunning precision, color accuracy, and brightness uniformity of the Ultra Retina XDR display, a state-of-the-art display created by combining the light of two OLED panels. M4 is designed with pioneering technologies that help enable the incredibly thin design of iPad Pro. New 10-core CPU M4 has a new up-to-10-core CPU consisting of up to four performance cores and now six efficiency cores. The next-generation cores feature improved branch prediction, with wider decode and execution engines for the performance cores, and a deeper execution engine for the efficiency cores. And both types of cores also feature enhanced, next-generation ML accelerators. M4 delivers up to 1.5x faster CPU performance over the powerful M2 in the previous iPad Pro.1 Whether working with complex orchestral music files in Logic Pro or adding highly demanding effects to 4K video in LumaFusion, M4 boosts performance across pro workflows. M4 delivers up to 1.5x faster CPU performance over the powerful M2 in the previous iPad Pro, boosting performance across pro workflows in apps like Logic Pro. GPU Brings New Capabilities to iPad Pro The new 10-core GPU of M4 builds upon the next-generation graphics architecture of the M3 family of chips. It features Dynamic Caching, an Apple innovation that allocates local memory dynamically in hardware and in real time to dramatically increase the average utilization of the GPU. This significantly increases performance for the most demanding pro apps and games. The new GPU of M4 will bring hardware-accelerated ray tracing to iPad for the first time, enabling even more realistic shadows and reflections in games like Diablo Immortal, and other graphically rich experiences. Hardware-accelerated ray tracing comes to iPad for the first time, and enables even more realistic shadows and reflections in games and other graphically rich experiences. Hardware-accelerated mesh shading is also built into the GPU, and delivers greater capability and efficiency in geometry processing, enabling more visually complex scenes in games and graphics-intensive apps. Pro rendering performance in apps like Octane gets a huge boost with M4, and is now up to four times faster than on M2.1 With these improvements to the CPU and GPU, M4 maintains Apple silicon’s industry-leading performance per watt. M4 can deliver the same performance as M2 using just half the power. And compared with the latest PC chip in a thin and light laptop, M4 can deliver the same performance using just a fourth of the power.2 Pro rendering performance in apps like Octane gets a huge boost with M4, and is now up to four times faster than on iPad Pro with M2. The Most Powerful Neural Engine Ever M4 has a blazing-fast Neural Engine — an IP block in the chip dedicated to the acceleration of AI workloads. This is Apple’s most powerful Neural Engine ever, capable of an astounding 38 trillion operations per second — a breathtaking 60x faster than the first Neural Engine in A11 Bionic. Together with next-generation ML accelerators in the CPU, the high-performance GPU, and higher-bandwidth unified memory, the Neural Engine makes M4 an outrageously powerful chip for AI. And with AI features in iPadOS like Live Captions for real-time audio captions, and Visual Look Up, which identifies objects in video and photos, the new iPad Pro allows users to accomplish amazing AI tasks quickly and on device. iPad Pro with M4 can easily isolate a subject from its background throughout a 4K video in Final Cut Pro with just a tap, and can automatically create musical notation in real time in StaffPad by simply listening to someone play the piano. And inference workloads can be done efficiently and privately while minimizing the impact on app memory, app responsiveness, and battery life. The Neural Engine in M4 is Apple’s most capable yet, and is more powerful than any neural processing unit in any AI PC today. M4 has a blazing-fast Neural Engine that can easily isolate a subject from its background throughout a 4K video in Final Cut Pro with just a tap. Advanced Media Engine for Smooth, Efficient Streaming The Media Engine of M4 is the most advanced to come to iPad. In addition to supporting the most popular video codecs, like H.264, HEVC, and ProRes, it brings hardware acceleration for AV1 to iPad for the first time. This provides more power-efficient playback of high-resolution video experiences from streaming services. Better for the Environment The power-efficient performance of M4 helps the all-new iPad Pro meet Apple’s high standards for energy efficiency and deliver all-day battery life. This results in less time needing to be plugged in and less energy consumed over its lifetime. Today, Apple is carbon neutral for global corporate operations, and by 2030, plans to be carbon neutral across the entire manufacturing supply chain and life cycle of every product. Share article Media Text of this article May 7, 2024 PRESS RELEASE Apple introduces M4 chip M4 enables the breakthrough design and stunning display of the new iPad Pro, while delivering a giant leap in performance CUPERTINO, CALIFORNIA Apple today announced M4, the latest chip delivering phenomenal performance to the all-new iPad Pro. Built using second-generation 3-nanometer technology, M4 is a system on a chip (SoC) that advances the industry-leading power efficiency of Apple silicon and enables the incredibly thin design of iPad Pro. It also features an entirely new display engine to drive the stunning precision, color, and brightness of the breakthrough Ultra Retina XDR display on iPad Pro. A new CPU has up to 10 cores, while the new 10-core GPU builds on the next-generation GPU architecture introduced in M3, and brings Dynamic Caching, hardware-accelerated ray tracing, and hardware-accelerated mesh shading to iPad for the first time. M4 has Apple’s fastest Neural Engine ever, capable of up to 38 trillion operations per second, which is faster than the neural processing unit of any AI PC today. Combined with faster memory bandwidth, along with next-generation machine learning (ML) accelerators in the CPU, and a high-performance GPU, M4 makes the new iPad Pro an outrageously powerful device for artificial intelligence. “The new iPad Pro with M4 is a great example of how building best-in-class custom silicon enables breakthrough products,” said Johny Srouji, Apple’s senior vice president of Hardware Technologies. “The power-efficient performance of M4, along with its new display engine, makes the thin design and game-changing display of iPad Pro possible, while fundamental improvements to the CPU, GPU, Neural Engine, and memory system make M4 extremely well suited for the latest applications leveraging AI. Altogether, this new chip makes iPad Pro the most powerful device of its kind.” New Technologies Enabling the New iPad Pro Delivering a giant leap in performance over the previous iPad Pro with M2, M4 consists of 28 billion transistors built using a second-generation 3-nanometer technology that further advances the power efficiency of Apple silicon. M4 also features an entirely new display engine designed with pioneering technologies, enabling the stunning precision, color accuracy, and brightness uniformity of the Ultra Retina XDR display, a state-of-the-art display created by combining the light of two OLED panels. New 10-core CPU M4 has a new up-to-10-core CPU consisting of up to four performance cores and now six efficiency cores. The next-generation cores feature improved branch prediction, with wider decode and execution engines for the performance cores, and a deeper execution engine for the efficiency cores. And both types of cores also feature enhanced, next-generation ML accelerators. M4 delivers up to 1.5x faster CPU performance over the powerful M2 in the previous iPad Pro.1 Whether working with complex orchestral music files in Logic Pro or adding highly demanding effects to 4K video in LumaFusion, M4 boosts performance across pro workflows. GPU Brings New Capabilities to iPad Pro The new 10-core GPU of M4 builds upon the next-generation graphics architecture of the M3 family of chips. It features Dynamic Caching, an Apple innovation that allocates local memory dynamically in hardware and in real time to dramatically increase the average utilization of the GPU. This significantly increases performance for the most demanding pro apps and games. Hardware-accelerated ray tracing comes to iPad for the first time, and enables even more realistic shadows and reflections in games and other graphically rich experiences. Hardware-accelerated mesh shading is also built into the GPU, and delivers greater capability and efficiency in geometry processing, enabling more visually complex scenes in games and graphics-intensive apps. Pro rendering performance in apps like Octane gets a huge boost with M4, and is now up to four times faster than on M2.1 With these improvements to the CPU and GPU, M4 maintains Apple silicon’s industry-leading performance per watt. M4 can deliver the same performance as M2 using just half the power. And compared with the latest PC chip in a thin and light laptop, M4 can deliver the same performance using just a fourth of the power.2 The Most Powerful Neural Engine Ever M4 has a blazing-fast Neural Engine — an IP block in the chip dedicated to the acceleration of AI workloads. This is Apple’s most powerful Neural Engine ever, capable of an astounding 38 trillion operations per second — a breathtaking 60x faster than the first Neural Engine in A11 Bionic. Together with next-generation ML accelerators in the CPU, the high-performance GPU, and higher-bandwidth unified memory, the Neural Engine makes M4 an outrageously powerful chip for AI. And with AI features in iPadOS like Live Captions for real-time audio captions, and Visual Look Up, which identifies objects in video and photos, the new iPad Pro allows users to accomplish amazing AI tasks quickly and on device. iPad Pro with M4 can easily isolate a subject from its background throughout a 4K video in Final Cut Pro with just a tap, and can automatically create musical notation in real time in StaffPad by simply listening to someone play the piano. And inference workloads can be done efficiently and privately while minimizing the impact on app memory, app responsiveness, and battery life. The Neural Engine in M4 is Apple’s most capable yet, and is more powerful than any neural processing unit in any AI PC today. Advanced Media Engine for Smooth, Efficient Streaming The Media Engine of M4 is the most advanced to come to iPad. In addition to supporting the most popular video codecs, like H.264, HEVC, and ProRes, it brings hardware acceleration for AV1 to iPad for the first time. This provides more power-efficient playback of high-resolution video experiences from streaming services. Better for the Environment The power-efficient performance of M4 helps the all-new iPad Pro meet Apple’s high standards for energy efficiency and deliver all-day battery life. This results in less time needing to be plugged in and less energy consumed over its lifetime. Today, Apple is carbon neutral for global corporate operations, and by 2030, plans to be carbon neutral across the entire manufacturing supply chain and life cycle of every product. About Apple Apple revolutionized personal technology with the introduction of the Macintosh in 1984. Today, Apple leads the world in innovation with iPhone, iPad, Mac, AirPods, Apple Watch, and Apple Vision Pro. Apple’s six software platforms — iOS, iPadOS, macOS, watchOS, visionOS, and tvOS — provide seamless experiences across all Apple devices and empower people with breakthrough services including the App Store, Apple Music, Apple Pay, iCloud, and Apple TV+. Apple’s more than 150,000 employees are dedicated to making the best products on earth and to leaving the world better than we found it. Testing was conducted by Apple in March and April 2024. See apple.com/ipad-pro for more information. Testing was conducted by Apple in March and April 2024 using preproduction 13-inch iPad Pro (M4) units with a 10-core CPU and 16GB of RAM. Performance was measured using select industry‑standard benchmarks. PC laptop chip performance data is from testing ASUS Zenbook 14 OLED (UX3405MA) with Core Ultra 7 155H and 32GB of RAM. Performance tests are conducted using specific computer systems and reflect the approximate performance of iPad Pro. Press Contacts Todd Wilder Apple wilder@apple.com Apple Media Helpline media.help@apple.com Copy text Images in this article Download all images About Apple Apple revolutionized personal technology with the introduction of the Macintosh in 1984. Today, Apple leads the world in innovation with iPhone, iPad, Mac, AirPods, Apple Watch, and Apple Vision Pro. Apple’s six software platforms — iOS, iPadOS, macOS, watchOS, visionOS, and tvOS — provide seamless experiences across all Apple devices and empower people with breakthrough services including the App Store, Apple Music, Apple Pay, iCloud, and Apple TV+. Apple’s more than 150,000 employees are dedicated to making the best products on earth and to leaving the world better than we found it. Testing was conducted by Apple in March and April 2024. See apple.com/ipad-pro for more information. Testing was conducted by Apple in March and April 2024 using preproduction 13-inch iPad Pro (M4) units with a 10-core CPU and 16GB of RAM. Performance was measured using select industry‑standard benchmarks. PC laptop chip performance data is from testing ASUS Zenbook 14 OLED (UX3405MA) with Core Ultra 7 155H and 32GB of RAM. Performance tests are conducted using specific computer systems and reflect the approximate performance of iPad Pro. Press Contacts Todd Wilder Apple wilder@apple.com Apple Media Helpline media.help@apple.com Latest News PRESS RELEASE Apple unveils the redesigned 11‑inch and all‑new 13‑inch iPad Air with M2 May 7, 2024 PRESS RELEASE Final Cut Pro transforms video creation with Live Multicam on iPad and new AI features on Mac May 7, 2024 PRESS RELEASE Logic Pro takes music-making to the next level with new AI features May 7, 2024",
    "commentLink": "https://news.ycombinator.com/item?id=40286029",
    "commentBody": "Apple introduces M4 chip (apple.com)1331 points by excsn 19 hours agohidepastfavorite1626 comments rsp1984 16 hours agoTogether with next-generation ML accelerators in the CPU, the high-performance GPU, and higher-bandwidth unified memory, the Neural Engine makes M4 an outrageously powerful chip for AI. In case it is not abundantly clear by now: Apple's AI strategy is to put inference (and longer term even learning) on edge devices. This is completely coherent with their privacy-first strategy (which would be at odds with sending data up to the cloud for processing). Processing data at the edge also makes for the best possible user experience because of the complete independence of network connectivity and hence minimal latency. If (and that's a big if) they keep their APIs open to run any kind of AI workload on their chips it's a strategy that I personally really really welcome as I don't want the AI future to be centralised in the hands of a few powerful cloud providers. reply andsoitis 15 hours agoparent> In case it is not abundantly clear by now: Apple's AI strategy is to put inference (and longer term even learning) on edge devices. This is completely coherent with their privacy-first strategy (which would be at odds with sending data up to the cloud for processing). Their primary business goal is to sell hardware. Yes, they’ve diversified into services and being a shopping mall for all, but it is about selling luxury hardware. The promise of privacy is one way in which they position themselves, but I would not bet the bank on that being true forever. reply klabb3 13 hours agorootparent> but it is about selling luxury hardware. Somewhat true but things are changing. While there are plenty of “luxury” Apple devices like Vision Pro or fully decked out MacBooks for web browsing we no longer live in a world where tech are just lifestyle gadgets. People spend hours a day on their phones, and often run their life and businesses through it. Even with the $1000+/2-3y price tag, it’s simply not that much given how central role it serves in your life. This is especially true for younger generations who often don't have laptops or desktops at home, and also increasingly in poorer-but-not-poor countries (say eg Eastern Europe). So the iPhone (their best selling product) is far, far, far more a commodity utility than typical luxury consumption like watches, purses, sports cars etc. Even in the higher end products like the MacBooks you see a lot of professionals (engineers included) who choose it because of its price-performance-value, and who don’t give a shit about luxury. Especially since the M1 launched, where performance and battery life took a giant leap. reply coffeebeqn 10 hours agorootparentEngineers use MacBook pros because it’s the best built laptop, the best screen, arguably the best OS and most importantly - they’re not the ones paying for them. reply pjmlp 40 minutes agorootparentUS Engineers, and in countries of similar income, the rest of the world is pretty much settled in a mix of Windows and GNU/Linux desktop/laptops. reply chipdart 4 hours agorootparentprev> Engineers use MacBook pros because it’s the best built laptop, the best screen, arguably the best OS and most importantly - they’re not the ones paying for them. I know engineers from a FANG that picked MacBook pros in spite of the specs and only because of the bling/price tag. Them they spent their whole time using it as a remote terminal for Linux servers, and they still complained about the thing being extremely short on RAM and HD. One of them even tried to convince their managers to give the vision pro a try, even though there was zero use cases for it. Granted, they drive multiple monitors well with a single USB-C plug, at least with specific combinations of monitors and hubs. It's high time that the \"Apple sells high end gear\" shtick is put to rest. Even their macOS treadmill is becoming tiring. reply theshrike79 2 hours agorootparentThe build quality of Apple laptops is still pretty unmatched in every price category. Yes, there are 2k+ laptops from Dell/Lenovo that match and exceed a similarly priced MacBook in pure power, but usually lack battery life and/or build quality. reply al_borland 9 hours agorootparentprevAnd they can typically setup their dev environment without a VM, while also getting commercial app support if they need it. Windows requires a VM, like WSL, for a lot of people, and Linux lacks commercial support. macOS strikes a good balance in the middle that makes it a pretty compelling choice. reply forty 1 hour agorootparentWhat do you mean without a VM? I guess you don't count docker/podman as VMs then? reply BytesAndGears 22 minutes agorootparentLikely that most devs want to use Unix tools — terminal, etc. reply ensignavenger 6 hours agorootparentprevThere are a plethora of companies offering commercial support for various Linux distributions. reply al_borland 5 hours agorootparentI was thinking more about software like the Adobe suite, Microsoft Office, or other closed source software that hasn’t released on Linux. Electron has made things a bit better, but there are still a lot of bigs gaps for the enterprise, unless the company is specifically choosing software to maintain Linux support for end users. Sure, Wine exists, but it’s not something I’d want to rely on for a business when there are alternatives like macOS which will offer native support. reply rob74 3 hours agorootparentMost people don't need the Adobe Suite, and the web version of M$-Office is more than Ok for occasional use. Most other enterprise software are web apps too nowadays, so it's much less relevant what OS your machine is running than it was ten years ago... reply seec 1 hour agorootparentYep, that's pretty much it. Apple fanboys like to talk about how cool and long lasting a MacBook Air is but a 500 bucks Chromebook will do just as well while allowing pretty much 90% of the use cases. Sure, the top end power is much lower but at the same time considering the base RAM/storage combo Apple gives it is not that relevant. If you starting loading it up, that puts the pricing in an entirely different category and in my opinion the MacBook Air becomes seriously irrelevant when compared to serious computing devices in the same price range... reply close04 39 minutes agorootparentThere's still a huge market for people who want higher end hardware and to run workloads locally. Same for people who put a higher price on privacy. For people who want to keep their data close to their chest, and particularly now with the AI bloom, being able to perform all tasks on device is more valuable than ever. A Chromebook \"does the job\" but it's closer to a thin client than a workstation. A lot of the job is done remotely and you may not want that. reply qalmakka 48 minutes agorootparentprevYou usually don't need either for software development though, and if you do the free or online alternatives are often good enough for the rare occasions you need them. If you are a software developer and you have to spend significant time using Office it means you either are developing extensions for Office or your company management is somewhat lacking and you are forced to handle things you should not (like bureaucracy for instance). reply overgard 5 hours agorootparentprevIf you mean WSL for containers, macOS needs a VM too. If youre doing C++ macOS dev tools are .. bleak. Great for webdev though reply repelsteeltje 1 hour agorootparent↑ This! I would love to buy Apple hardware, but not from Apple. I mean: M2 13 inch notebook with access to swap/extend memory and storage, regular US keyboard layout and proper desktop Linux (Debian, Alpine, Mint, PopOS!, Fedora Cinamon) or windows. MacOS and the Apple eco system just gets in your way when you're just trying to maintain a multi-platform C++/Java/Rust code base. reply al_borland 5 hours agorootparentprevWSL for normal stuff. My co-worker is on Windows and had to setup WSL to get a linter working with VS Code. It took him a week to get it working the first time, and it breaks periodically, so he needs to do it all over again every few months. reply int_19h 4 hours agorootparentUnless he is doing Linux development in the first place, that sounds very weird. You most certainly don't need to set up WSL to lint Python or say JS in VSCode on Windows. reply pjmlp 38 minutes agorootparentprevAs Windows/UNIX developer, I only use WSL for Linux containers. reply trimethylpurine 4 hours agorootparentprevI'm developing on Windows for Windows, Linux, Android, and web, including C, Go, Java, TSQL and MSSQL management. I do not necessarily need WSL except for C. SSH is built directly into the Windows terminal and is fully scriptable in PS. WSL is also nice for Bash scripting, but it's not necessary. It is a check box in the \"Add Features\" panel. There is nothing to install or setup. Certainly not for linting, unless, again, you're using a Linux tool chain. But if you are, just check the box. No setup beyond VS Code, bashrc, vimrc, and your tool chain. Same as you would do on Mac. If anything, all the Mac specific quirks make setting up the Linux tool chains much harder. At least on WSL the entire directory structure matches Linux out of the box. The tool chains just work. While some of the documentation is in its infancy, the workflow and versatility of cross platform development on Windows, I think, is unmatched. reply pineaux 3 hours agorootparentprevWSL is not a VM. Edit: TIL WSL2 is a VM. I develop on mac and linux computers so should have kept my mouth shut anyways reply worthless-trash 2 hours agorootparentHey, you learned and corrected yourself, dont be so hard on yourself mate. reply tomcam 1 hour agorootparentUsername highly inaccurate ;) reply pompino 6 hours agorootparentprev\"Engineers\" - ironically the term used in the software industry for people who never standardize anything, solve the same problem solved by other \"engineers\" over and over again (how many libraries do you need for arrays and vectors and guis and buttons and text boxes and binary trees and sorting, yada yada?) while making the same mistakes and learning the hard way each time, also vehemently argue about software being \"art\" might like OSX, but even that is debatable. Meanwhile actual Engineers (the ones with the license) the people who need CAD and design tools for building bridges and running manufacturing plants stay far away from OSX. reply bee_rider 4 hours agorootparentI did EE in college but we mostly just used Windows because the shitty semi-proprietary SPICE simulator we had to use, and stuff like that, only supported Windows. The company that makes your embedded processor might only support Windows (and begrudgingly at that). I think engineers using software should not be seen as an endorsement. They seem to have an incredible tolerance for bad UI. reply pompino 3 hours agorootparentYou seem to be suggesting that a chunk of the hundreds of millions of people who use a UI that you don't like, secretly hate it or are forced to tolerate it. Not a position I'd personally want to argue or defend, so I'll leave it at that. reply paulmd 3 hours agorootparentWhat an oddly aggressive and hostile response to such a banal observation. Yes, millions of people use software they hate, all the time, that’s wildly uncontroversial. reply pompino 2 hours agorootparentIts not an \"observation\" its someone making it up. Why are you so upset if I disagree? reply PaulHoule 6 hours agorootparentprevIf you look at creative pros such as photographers and Hollywood ‘film’ editors, VFX artists, etc. you will see a lot of Windows and Linux as people are more concerned about getting absolute power at a fair price and don’t care if it is big, ugly. etc. reply pompino 6 hours agorootparentOh, I'm sure there are lots of creatives who use OSX, so I don't mean to suggest nobody uses OSX, so I'll admit it was a bit in jest to poke fun at the stereotype. I'm definitely oldschool - but to me It's a bit cringe to hear \"Oh, I'm an engineer..\" or \"As an engineer..\" from people sit at a coffee shop writing emails or doing the most basic s/w dev work. I truly think silicon valley people would benefit from talking to technical people who are building bridges and manufacturing plants and cars and hardware and chips and all this stuff on r/engineeringporn that everyone takes for granted. I transitioned from s/w to hardcore manufacturing 15 years ago, and it was eye opening, and very humbling. reply 2muchcoffeeman 3 hours agorootparentI’d assume a lot of this is because you can’t get the software on MacOS. Not a choice. Who is choosing to use Windows 10/11 where you get tabloid news in the OS by default? Or choosing to hide the button to create local user accounts? reply steve1977 23 minutes agorootparentWho is choosing to use macOS, where non-Apple monitors and other 3rd party hardware just stops working after minor updates and then starts working again after another update, without any official statement from Apple that there was a problem and a fix? reply pompino 3 hours agorootparentprevPeople overwhelmingly choose windows world-wide to get shit done. That answers the who. reply 2muchcoffeeman 2 hours agorootparentSo the same software exists on multiple platforms, there are no legacy or hardware compatibility considerations, interoperability considerations, no budget considerations, and the users have a choice in what they use? I.e the same functionality exists with no draw backs and money was no object. And they chose Windows? Seriously why? reply pompino 2 hours agorootparentWe use the sales metrics and signals available to us. I don't know what to say except resign to the fact that the world is fundamentally unfair, and you won't ever get to run the A/B experiment that you want. So yes, Windows it is ! reply shykes 6 hours agorootparentprev\"silicon valley people would benefit from talking to people who build chips\", that's a good one! reply cafed00d 6 hours agorootparentprevI always likened \"engineers\"[1] to \"people who are proficient in calculus\"; and \"computers\"[1] to \"people who are proficient at calculations\". There was brief sidestep from late 1980s to early 2010s (~2012) where the term \"software engineer\" came into vogue and completely ran orthogonal to \"proficiency in calculus\". I mean, literally 99% of software engineers never learned calculus! But it's nice to see that ever since ~2015 or so (and perhaps even going forward) proficiency in calculus is rising to the fore. We call those \"software engineers\" \"ML Engineers\" nowadays, ehh fine by me. And all those \"computers\" are not people anymore -- looks like carefully arranged sand (silicon) in metal took over. I wonder if it's just a matter of time before the carefully-arranged-sand-in-metal form factor will take over the \"engineer\" role too. One of those Tesla/Figure robots becomes \"proficient at calculus\" and \"proficient at calculations\" better than \"people\". Reference: [1]: I took the terms \"engineer\" and \"computer\" literally out of the movie \"Hidden Figures\" https://en.wikipedia.org/wiki/Hidden_Figures#Plot It looks like ever since humankind learned calculus there was an enormous benefit to applying it in the engineering of rockets, aeroplanes, bridges, houses, and eventually \"the careful arrangement of sand (silicon)\". Literally every one of those jobs required learning calculus at school and applying calculus at work. reply pompino 3 hours agorootparentHmm, that is an interesting take. Calculus does seems like the uniting factor. I've come to appreciate the fact that domain knowledge has a more dominant role in solving a problem than technical/programming knowledge. I often wonder how s/w could align with other engineering practices in terms of approach design in a standardized way so we can just churn out code w/o an excessive reliance on quality assurance. I'm really hoping visual programming is going to be the savior here. It might allow SMEs and Domain experts to utilize a visual interface to implement their ideas. Its interesting how python dominated C/C++ in the case of the NumPy community. One would have assumed C/C++ to be a more a natural fit for performance oriented code. But the domain knowledge overpowered technical knowledge and eventually people started asking funny questions like https://stackoverflow.com/questions/41365723/why-is-my-pytho... reply techcode 3 hours agorootparentprevWhy pointing out Calculus as opposed to just Math? Might be just my Eastern Europe background where it was all just \"Math\" and both equations (that's Algebra I guess) and simpler functions/analysis (Calculus?) are taught in elementary school around age 14 or 15. Maybe I'm missing/forgetting something - I think I used Calculus more during electrical engineering than for computer/software engineering. reply KptMarchewa 50 minutes agorootparentIn my central european university we've learned \"Real Analysis\" that was way more concerned about theorems and proofs rather than \"calculating\" something - if anything, actually calculating derivatives or integrals was a warmup problem to the meat of the subject. reply pompino 2 hours agorootparentprevTrue, we learnt calculus before college in my home country - but it was just basic stuff. But I learnt a lot more of it including partial derivatives in first year of engineering college. >I think I used Calculus more during electrical engineering than for computer/software engineering. I think that was OPs point - most engineering disciplines teach it. reply Shorel 1 hour agorootparentprevFor this engineering, I think calculus is not the main proficiency enhancer you claim it to be. Linear Algebra, combinatorics, probability and number theory are more relevant. Calculus was important during the world wars because it means we could throw shells to the enemy army better, and that was an important issue during that period. Nowadays, calculus is just a stepping stone to more relevant mathematics. reply GeneralMaximus 1 hour agorootparentprevThis checks out. I'm a software developer who took math all through high school and my first three years of college. I barely scraped through my calculus exams, but I excelled at combinatorics, probability, matrix math, etc. (as long as it didn't veer into calculus for some reason). I guess I just enjoy things more when I can count them. reply ido 2 hours agorootparentprevThat 99% guess seems high considering calculus is generally a required subject when studying computer science (or software engineering) at most universities I know of. reply hyperadvanced 4 hours agorootparentprevMost software engineering just doesn’t require calculus, though it does benefit from having the understanding of functions and limit behaviors that higher math does. But if you look at a lot of meme dev jobs they’ve transitioned heavily away from the crypto craze of the past 5 years towards “prompt engineering” or the like to exploit LLMs in the same way that the “Uber for X” meme of 2012-2017 exploited surface level knowledge of JS or API integration work. Fundamentally, the tech ecosystem desires low skill employees, LLMs are a new frontier in doing a lot with a little in terms of deep technical knowledge. reply fecal_henge 2 hours agorootparentprevI'd say a lot of engineers (bridges, circuit boards, injection mouldings) are kept far away from OSX (and linux). Honestly, I'd just love a operating system that doesn't decide its going to restart itself periodically! reply mogiddy55 5 hours agorootparentprevFrom what I've heard (not an OSX user) Windows is the best operating system for multiple screens; OSX and Linux glitch way more. Most anyone doing 3D sculpture or graphics/art on a professional level will eventually move to working with 2-3 screens, and since there are no exclusively Mac design programs, OSX will be suboptimal. There's little things too, like some people using gaming peripherals (multi-button MMO mice and left hand controllers, etc.) for editing, which might not be compatible with OSX. And also, if you're mucking around with two 32 inch 4k monitors and a 16 inch Wacom it might start to feel a little ridiculous trying to save space with a Mac Pro. reply Shorel 53 minutes agorootparentBesides Windows having more drivers for USB adapters than Linux*, which is a reflection of the market, I find Linux having much fewer glitches using multiple screens. Once it works, Linux is more reliable than Windows. And virtual desktops have always worked better on Linux than on Windows. So I disagree with you on that front. * In my case, this means I had to get an Anker HDMI adapter, instead of any random brand. reply KptMarchewa 49 minutes agorootparent>I find Linux having much fewer glitches using multiple screens. Maybe as long as you don't need working fractional scaling with different DPI monitors, which is nothing fancy now. reply Toutouxc 40 minutes agorootparentprevNitpick: it hasn’t been called “OS X” for almost eight years now, starting with macOS Sierra. reply egypturnash 4 hours agorootparentprevI’ve been doing art on a pro level for twenty five years and I dislike multiple monitors. reply mogiddy55 4 hours agorootparentI am just commenting about what I've seen at concept artist desks / animation studios / etc. reply tlrobinson 4 hours agorootparentprevWho do you think writes those CAD and design tools that help “actual engineers” solve the same problems over and over? reply pompino 3 hours agorootparentWould you like me to explain how it works to you? I'm not sure why you added a question mark. reply nineteen999 5 hours agorootparentprevMaybe we need a new moniker \"webgineer\". The average HN/FAANG web programmer does appear to vastly overestimate the value of their contributions to the world. reply techcode 3 hours agorootparentHave we done full circle? When I started doing this \"Internet stuff\" we were called \"webmasters\", and job would actually include what today we call: - DevOps - Server/Linux sysadmin - DB admin - Full stack (backend and frontend) engineer And I might have forgot some things. reply KptMarchewa 47 minutes agorootparentprevPeople who cobble together new printers or kettles overestimate the value of their contributions to the world too. The delineation isn't between JS devs and JPL or ASML engineers. reply peterleiser 4 hours agorootparentprev1999 indeed! I haven't heard that term since around 1999 when I was hired as a \"web engineer\" and derisively referred to myself as a \"webgineer\". I almost asked if I could change my title to \"sciencematician\". reply tlrobinson 4 hours agorootparentprevI don’t think it’s at all unreasonable for an engineer using a device for 8+ hours every day to pay an additional, say, 0.5% of their income (assuming very conservatively $100,000 income after tax, $1,000 extra for a MacBook, 2 year product lifespan) for the best built laptop, best screen, and best OS. reply rfoo 1 hour agorootparent> and best OS I do networking stuff and macOS is on par with Windows - I can't live on it without running into bugs or very questionable behavior for longer than a week. Same as Windows. reply yunobcool 4 hours agorootparentprev$100,000 after tax does not seem conservative to me (at least outside the US). reply tlrobinson 4 hours agorootparent$50,000 income, 4 year product lifespan? Obviously doesn’t apply to all engineers. reply anArbitraryOne 9 hours agorootparentprevAnd the M1 chip on mine really alters productivity. Every time we want to update a library, we need some kind of workaround. It's great having a chip that is so much different than what our production infrastructure uses. reply gibolt 6 hours agorootparentThis should be a temporary problem solved with time. The battery and performance gains are completely worth most workarounds required. reply anArbitraryOne 5 hours agorootparentNot worth it at all. I rarely use battery power, so I'd rather have an intel or AMD chip with more cores and a higher clock speed at the expense of the battery. Oh, and an OS that can actually manage its windows, and customize keyboard settings, and not require an account to use the app store reply davedx 2 hours agorootparentprevI’m freelance so I’ve absolutely paid for my last 3 Macbooks. They’re best in class tools and assets for my business. reply OtomotO 5 hours agorootparentprevIf it weren't for the OS I would've bought a MacBook instead of a Lenovo laptop. I've set up my OS exactly as I want it. (I use arch btw ;-)) reply ZiiS 2 hours agorootparentArch works fairly well on Apple silicon now, though Fedora is easier/recomended. Limited emulation due to the 16KB pages and no thunderbolt display out. reply nirse 3 hours agorootparentprevSame, but on gentoo :-p reply BobbyTables2 10 hours agorootparentprevI have one and hate it with a passion. A MacBook Air bought new in the past 3 years should be able to use Teams (alone) without keeling over. Takes over a minute to launch Outlook. My 15 year old Sony laptop can do better. Even if Microsoft on Mac is an unmitigated dumpster fire, this is ridiculous. I avoid using it whenever possible. If people email me, it’d better not be urgent. reply riddlemethat 9 hours agorootparentI avoid using Outlook on any device, but I wouldn't complain about my Surface tablet's performance based on how poorly iTunes performs... reply safety1st 7 hours agorootparentprevMeanwhile here I am, running linux distros and XFCE on everything. My hardware could be a decade old and I probably wouldn't notice. (In fact I DO have a spare 13 year old laptop hanging around that still gets used for web browsing, mail and stuff. It is not slow.) reply freedomben 6 hours agorootparentIndeed, I have a 15-year-old desktop computer that is still running great on Linux. I upgraded the RAM to the maximum supported by the motherboard, which is 8 GB, and it has gone through three hard drives in its life, but otherwise it is pretty much the same. As a basic web browsing computer, and for light games, it is fantastic. reply safety1st 4 hours agorootparentIt also performs pretty well for the particular brand of web development I do, which basically boils down to running VS Code, a browser, and a lot of ssh. It's fascinating to me how people are still attached to the hardware upgrade cycle as an idea that matters, and yet for a huge chunk of people and scenarios, basically an SSD, 8gb of RAM and an Intel i5 from a decade ago could have been the end of computing history with no real loss to productivity. I honestly look at people who use Apple or Windows with a bit of pity, because those ecosystems would just give me more stuff to worry about. reply josephg 9 hours agorootparentprevIs it an Apple silicon or Intel machine? Intel macs are crazy slow - especially since the most recent few versions of macOS. And especially since developers everywhere have upgraded to an M1 or better. reply saagarjha 4 hours agorootparentNo MacBook Air from the last 3 years is Intel-based reply rootusrootus 9 hours agorootparentprevSounds a bit like my Intel MBP, in particular after they (the company I work for) installed all the lovely bloatware/tracking crap IT thinks we need to be subjected to. Most of the day the machine runs with the fans blasting away. Still doesn't take a minute to launch Outlook, but I understand your pain. I keep hoping it will die, because it would be replaced with an M-series MBP and they are way, way, WAY faster than even the best Intel MBP. reply kolinko 7 hours agorootparentprevThat’s not an issue with Macboom but with MS. MS has an incentive to deliver such a terrible experience on macs. reply int_19h 4 hours agorootparentMS has literally thousands of managers running Outlook and Teams on their company-provided ARM MacBooks daily. reply WWLink 8 hours agorootparentprev> Even if Microsoft on Mac is an unmitigated dumpster fire, this is ridiculous. It is Microsoft. I could rant all day about the dumpster fire that is the \"NEW Microsoft Teams (Work or School)\" It's like the perfect shining example of how MS doesn't give a flaming fuck about their end users. reply 486sx33 7 hours agorootparentprevOutlook (old) is okay on Mac Teams is a dumpster fire on every platform reply bigboy12 7 hours agorootparentprevI suppose you like bloatware and ads in your taskbar and 49 years of patch Tuesday. Have fun with that. I’ll take Mac over any windows. reply synergy20 7 hours agorootparentprevno, no, NO and yes. I actually rejected a job offer when heard I will be given a macbook pro. Apple, been the most closed company these days, should be avoided as much as you can, not to mention its macos is useless for linux developers like me, anything else is better. its keyboard is dumb to me(that stupid command/ctrl key difference), can not even mouse-select and paste is enough for me to avoid Macos at all costs. reply insaneirish 7 hours agorootparent> I actually rejected a job offer when heard I will be given a macbook pro. Probably best for you both. reply ekimekim 6 hours agorootparentprev> I actually rejected a job offer when heard I will be given a macbook pro. For what it's worth, I've had a good success rate at politely asking to be given an equivalent laptop I can put linux on, or provide my own device. I've never had to outright reject an offer due to being required to use a Mac. At worst I get \"you'll be responsible for making our dev environment work on your setup\". reply 486sx33 7 hours agorootparentprevI think I had similar feelings but took an open mind and love my m2 pro Sometimes an open mind reaps rewards friend reply gibolt 6 hours agorootparentI selected Mac + iOS devices when a job offered a choice, specifically to try out the option, while personally sticking with Windows and Android. Now the performance of Mx Macs convinced me to switch, and I'll die on the hill of Android for life reply fooblaster 6 hours agorootparentprevwhat amazing laptop must an employer give you to not be summarily rejected? reply synergy20 5 hours agorootparentany thing runs Linux,even wsl2 is fine,no macos is the key. and yes it costs the employer about half of the expensive Apple devices that can not even be upgraded, its hardware is as closed as its software. reply saagarjha 4 hours agorootparentEmployers typically also care about costs like “how hard is it to provision the devices” and “how long is the useful life of this” or “can I repurpose an old machine for someone else”. reply p_l 2 hours agorootparentProvisioning is a place where Windows laptops win hands down, though. Pretty much everything going wrong with provisioning involves going extra weird on hw (usually for cheap supplier) and/or pushing weird third party \"security\" crapware. reply fastball 7 hours agorootparentprevmacOS is clearly better for linux devs than Windows, given it is unix under-the-hood. I don't even know what you mean by mouse-select and paste. reply int_19h 4 hours agorootparentOn Windows these days, you get WSL, which is actual Linux, kernel and all. There are still some differences with a standalone Linux system, but they are far smaller than macOS, in which not only the kernel is completely different, but the userspace also has many rather prominent differences that you will very quickly run afoul of (like different command line switches for the same commands). Then there's Docker. Running amd64 containers on Apple silicon is slow for obvious reasons. Running arm64 containers is fast, but the actual environment you will be deploying to is almost certainly amd64, so if you're using that locally for dev & test purposes, you can get some surprises in prod. Windows, of course, will happily run amd64 natively. reply paulmd 3 hours agorootparent> the actual environment you will be deploying to is almost certainly amd64 that’s up to your team of course, but graviton is generally cheaper than x86 instances nowadays and afaik the same is true on google and the other clouds. reply aforwardslash 2 hours agorootparentArm is an ISA, not a family of processors. You may expect Apple chips and Graviton to be wildly different, and perform completely different in the same scenario. In fact, most Arm cpus also have specific extensions that are not found in other manufacturers. So yes, while both recognize a base set of instructions, thats about it - expect that everything else is different. I know, amd64 is also technically an ISA, but you have 2 major manufacturers, with very similar and predictable performance characteristics. And even then, sometimes something on AMD behaves quite differently from Intel. For most devs, doing crud stuff or writing high-level scripting languages, this isn't really a problem. For some devs, working on time-sensitive problems or with strict baseline performance requirements, this is important. For devs developing device drivers, emulation can only get you so far. reply Reason077 5 hours agorootparentprev> \"I don't even know what you mean by mouse-select and paste.\" Presumably they mean linux-style text select & paste, which is done by selecting text and then clicking the middle mouse button to paste it (no explicit \"copy\" command). macOS doesn't have built-in support for this, but there are some third-party scripts/apps to enable it. For example: https://github.com/lodestone/macpaste reply BenjiWiebe 6 hours agorootparentprevOn most Linux environments: text you highlight with the mouse (or highlight by double/triple clicking) can be \"pasted\" by middle-clicking. reply Cyphase 5 hours agorootparentAnd it's a separate clipboard from Ctrl+C/right-click-and-copy. The number of times I miss that on non-Linux... reply Lio 1 hour agorootparentPersonally, I use tmux on both Linux and macOS to get multiple clipboards and the mouse behaviour I’m used to. reply cpill 5 hours agorootparentprevnot machines learning Devs reply nxicvyvy 8 hours agorootparentprevThis hasn't been true for a long time. reply wubrr 8 hours agorootparentprevNo, no, no, yes. reply resonious 8 hours agorootparentprevM* has caused nothing but trouble for most mac user engineers I know (read: most engineers I know) who upgraded. Now not only are they building software for a different OS, they're building for a different architecture! They do all of their important compute in Docker, wasting CPU cycles and memory on the VM. All for what: a nice case? nice UI (that pesters you to try Safari)? It looks like Apple's silicon and software is really good for those doing audio/video. Why people like it for dev is mostly a mystery to me. Though I know a few people who don't really like it but are just intimidated by Linux or just can't handle the small UX differences. reply jetpks 7 hours agorootparentI'm an engineer that has both an apple silicon laptop (mbp, m2) and a linux laptop (arch, thinkpad x1 yoga.) I choose the mac every day of the week and it's not even close. I'm sure it's not great for specific engineering disciplines, but for me (web, rails, sre) it really can't be beat. The UX differences are absolutely massive. Even after daily-driving that thinkpad for months, Gnome always felt kinda not quite finished. Maybe KDE is better, but it didn't have Wayland support when I was setting that machine up, which made it a non-starter. The real killer though is battery life. I can work literally all day unplugged on the mbp and finish up with 40-50% remaining. When i'm traveling these days, i don't even bring a power cable with me during the day. The thinkpad, despite my best efforts with powertop, the most aggressive frequency scaling i could get, and a bunch of other little tricks, lasts 2 hours. There are niceties about Linux too. Package management is better and the docker experience is _way_ better. Overall though, i'd take the apple silicon macbook 10 times out of 10. reply jwells89 5 hours agorootparentBattery life followed by heat and fan noise have been my sticking points with non-mac laptops. My first gen ThinkPad Nano X1 would be an excellent laptop, if it weren’t for the terrible battery life even in power save mode (which as an aside, slows it down a lot) and its need to spin up a fan to do something as trivial as driving a rather pedestrian 2560x1440 60hz display. It feels almost like priorities are totally upside down for x86 laptop manufacturers. I totally understand and appreciate that there are performance oriented laptops that aren’t supposed to be good with battery life, but there’s no good reason for there being so few ultraportable and midrange x86 laptops that have good battery life and won’t fry your lap or sound like a jet taking off when pushed a little. It’s an endless sea of mediocrity. reply klabb3 7 hours agorootparentprev> The thinkpad, […], lasts 2 hours. This echoes my experiences for anything that needs power management. Not just that the battery life is worse, but that it degrades quickly. In two years it’s barely usable. I’ve seen this with non-Apple phones and laptops. iPhone otoh is so good these days you don’t need to upgrade until EOL of ~6 years (and even if you need it battery is not more expensive than any other proprietary battery). My last MacBook from 2011 failed a couple of years ago only because of a Radeon GPU inside with a known hw error. > There are niceties about Linux too. Yes! If you haven’t tried in years, the Linux desktop experience is awesome (at least close enough) for me – a dev who CAN configure stuff if I need to but find it excruciatingly menial if it isn't related to my core work. It’s really an improvement from a decade ago. reply xarope 7 hours agorootparentprevI'd like to offer a counterpoint, I have an old'ish T480s which runs linuxmint, several lxd containers for traefik, golang, python, postgres and sqlserver (so not even dockerized, but full VMs running these services), and I can go the whole morning (~4-5 hours). I think the culprit is more likely the power hungry intel CPU in your yoga? Going on a slight tangent; I've tried but do not like the mac keyboards, they feel very shallow to me, hence why I'm still using my old T480s. The newer thinkpad laptop keyboards all seem to be going that way though (going thinner), much to my dismay. Perhaps a P14s is my next purchase, despite it's bulk. Anybody with a framework 13 want to comment on their keyboard? reply freedomben 6 hours agorootparentI really like the keyboards on my frameworks. I have both the 13 and the new 16, and they are pretty good. Not as good as the old T4*0s I'm afraid, but certainly usable. reply resonious 7 hours agorootparentprevInteresting. I do similar (lots of Rails) but have pretty much the opposite experience (other than battery life - Mac definitely wins there). Though I use i3/Sway more than Gnome. The performance of running our huge monolith locally is much better for Linux users than Mac users where I work. I used a Mac for awhile back in 2015 but it never really stood out to me UX-wise, even compared to Gnome. All I really need to do is open a few windows and then switch between them. In i3 or Sway, opening and switching between windows is very fast and I never have to drag stuff around. reply pompino 6 hours agorootparentprev>The UX differences are absolutely massive. Examples? reply freedomben 6 hours agorootparentprevInterestingly enough, the trend I am seeing is all the MacBook engineers moving back to native development environments. Basically, no longer using docker. And just as expected, developers are getting bad with docker and are finding it harder to use. They are getting more and more reliant on devops help or to lean on the team member who is on Linux to handle all of that stuff. We were on a really great path for a while there in development where we were getting closer to the ideal of having development more closely resemble production, and to have developers understand the operations tools. Now we're cruising firmly in the opposite direction because of this Apple switch to arm. Mainly it wouldn't bother me so much if people would recognize that they are rationalizing because they like the computers, but they don't. They just try to defend logically a decision they made emotionally. I do it too, every human does, but a little recognition would be nice. reply int_19h 4 hours agorootparentIt's not even a problem with MacBooks as such. They are still excellent consumer devices (non-casual gaming aside). It's this weird positioning of them as the ultimate dev laptop that causes so many problems, IMO. reply daviddever23box 4 hours agorootparentprevRemember, though, that the binaries deployed in production environments are not being built locally on individual developer machines, but rather in the cloud, as reproducible builds securely deployed from the cloud to the cloud. Modern language tooling (Go, Rust et al) allows one to build and test on any architecture, and the native macOS virtualization (https://developer.apple.com/documentation/virtualization) provides remarkably better performance compared to Docker (which is a better explanation for its fading from daily use). Your \"trend\" may, in fact, not actually reflect the reality of how cloud development works at scale. And I don't know a single macOS developer that \"lean(s) on the team member who is on Linux\" to leverage tools that are already present on their local machine. My own development environments are IDENTICAL across all three major platforms. reply tsimionescu 3 hours agorootparentVirtualization and Docket are orthogonal technologies. The reason you use docker, especially in dev, is to have the exact same system libraries, dependencies, and settings on each build. The reason you use virtualization is to access hardware and kernel features that are not present on your hardware or native OS. If you deploy on docker (or Kubernetes) on Linux in production, then ideally you should be using docker on your local system as well. Which, for Windows or MacOS users, requires a Linux VM as well. reply niij 8 hours agorootparentprevIn my experience as a backend services Go developer (and a bit of Scala) the switch to arm has been mostly seamless. There was a little config at the beginning to pull dual-image docker images (x64 and arm) but that was a one time configuration. Otherwise I'm still targeting Linux/x64 with Go builds and Scala runs on the JVM so it's supported everywhere anyway; they both worked out of the box. My builds are faster, laptop stays cooler, and battery lasts longer. I love it. If I was building desktop apps I assume it would be a less pleasant experience like you mention. reply phlakaton 5 hours agorootparentThe pain for me has been in the VM scene, as VirtualBox disappeared from the ecosystem with the switch to ARM. reply herval 7 hours agorootparentprevI don’t know a single engineer who had issues with M chips, and most engineers I know (me included) benefited considerably from the performance gains, so perhaps your niche isn’t that universal? reply resonious 7 hours agorootparentMy niche is Ruby on Rails web dev, which is definitely not universal, but not all that narrow either! reply Lio 1 hour agorootparentI’m doing Ruby on Rails dev too. I don’t notice a hige difference between macOS and Linux for how I work. There’s quirks to either OS. Eg when on Gnome it drives me mad that it won’t focus a recently launched apps. On macOS it annoys me that I have install a 3rd party util to move windows around. Meh, you just adapt after a while. reply fiddlerwoaroof 6 hours agorootparentprevYou must have an unusual setup because, between Rosetta and rosetta in Virtualization.framework VMs (configurable in Docker Desktop or Rancher Desktop), I’ve never had issues running intel binaries on my Mac reply eloisant 6 hours agorootparentprevWe have to cross-compile anyway because now we're deploying to arm64 Linux (AWS Graviton) in addition to x86 Linux. So even if all developers of your team are using Linux, unless you want to waste money by ignoring arm64 instances on cloud computing, you'll have to setup cross compilation. reply spullara 8 hours agorootparentprev1) macs are by far the best hardware and also performance running intel code is faster than running intel code on the previous intel macs: https://discourse.slicer.org/t/hardware-is-apple-m1-much-fas... 2) they should use safari to keep power usage low and browser diversity high reply khaki54 8 hours agorootparentprevIt's basically required for iOS development. Working around it is extremely convoluted any annoying reply resonious 7 hours agorootparentI forgot to mention that as an obvious exception. Of course developing for Apple is best on Apple hardware. reply AtomicOrbital 8 hours agorootparentprevI strongly suggest putting in the time to learn how to install and maintain a linux laptop ... Ubuntu 24.04 is a great engineer platform reply ActorNightly 3 hours agorootparentprev>Even in the higher end products like the MacBooks you see a lot of professionals (engineers included) who choose it because of its price-performance-value, and who don’t give a shit about luxury. Most CS professionals who write code have no idea what it takes to build a desktop, so the hardware that they chose is pretty much irrelevant because they aren't specifically choosing for hardware. The reason Apple gets bought is mostly by anyone, including tech people, is because of ecosystem. The truth is, nobody really care that much about actual specs as long as its good enough to do basic stuff, and when you are indifferent to the actual difference but all your friends are in the ecosystem, the choice is obvious. You can easily see this yourself: ask these \"professionals\" about the details of the Apple Neural engine, and its a very high chance that they will repeat some marketing material, while failing to mention that Apple does not publish any real docs for ANE, you have to sign your code to run on ANE, and you have to basically use Core ML to utilize the ANE. I.e if they really cared about inference, all of them would be buying laptops with discrete 4090s for almost the same price. Meanwhile, if you look at people who came from EE/ECE (who btw on the average are far better coders than people with CS background, based on my 500+ interviews in the industry across several sectors), you see a way larger skew towards Android/custom built desktops/windows laptops running Linux. If you lived and breathed Linux and low level OS, you tend appreciate all the power and customization that it gives you because you don't have to go learn how to do things. reply _the_inflator 12 hours agorootparentprevI disagree. Apple is selling hardware and scaling AI by utilizing it is simply a smart move. Instead of building huge GPU clusters, having to deal with NVIDIA for GOUs (Apple kicked NVIDIA out years ago because of disagreements), Apple is building mainly on existing hardware. This is in other terms utilizing CPU power. On the other hand this helps their marketing keeping high price points when Apple now is going to differentiate their COU power and therefore hardware prices over AI functionality correlating with CPU power. This is also consistent with Apple stopping the MHz comparisons years ago. reply klabb3 10 hours agorootparentDid you reply to the right comment? Feels like we’re talking about different things altogether. reply bingbingbing777 10 hours agorootparentprevWhat AI is Apple scaling? reply bionhoward 9 hours agorootparentSeen MLX folks post on X about nice results running local LLMs. https://github.com/ml-explore/mlx Also, Siri, and consider: you’re scaling AI on apple’s hardware, too, you can develop your own local custom AI on it, there’s more memory available for linear algebra in a maxed out MBP than the biggest GPUs you can buy. They scale the VRAM capacity with unified memory and that plus a ton of software is enough to make the Apple stuff plenty competitive with the corresponding NVIDIA stuff for the specific task of running big AI models locally. reply arvinsim 3 hours agorootparentprevMacbooks are not bang-for-buck. Most engineers I know buy it because it's like Windows but with Unix tools built-in. reply aplummer 3 hours agorootparentI would be interested if there exists a single better value machine in $ per hour than my partners 2012 MacBook Air, which still goes reply nox101 4 hours agorootparentprevprice-performance is not a thing for a vast majority of users. Sure I'd like a $40k car but I can only afford a $10k car. It's not nice but it gets me from a to b on my min-wage salary. Similarly, I know plenty of friends and family. They can either get 4 macs for $1000 each (mom, dad, sister, brother) so $4k. Or they can get 4 windows PCs for $250 so $1k total. The cheap Windows PCs suck just like a cheap car sucks (ok, they suck more), but they still get the job done. You can still browse the web, read your email, watch a youtube video, post a youtube video, write a blog, etc.. My dad got some HP celeron. It took 4 minutes to boot. It still ran though and he paid probably $300 for it vs $999 for a mac. He didn't have $999. reply chipdart 4 hours agorootparentprev> Somewhat true but things are changing. While there are plenty of “luxury” Apple devices like Vision Pro or fully decked out MacBooks for web browsing we no longer live in a world where tech are just lifestyle gadgets. I notice your use of the weasel word \"just\". We undoubtedly live in a world where Apple products are sold as lifestyle gadgets. Arguably it's more true today than it ever was. It's also a world where Apple's range of Veblen goods managed to gain footing in social circles to an extent that we have kids being bullied for owning Android phones. Apple's lifestyle angle is becoming specially relevant because they can no longer claim they sell high-end hardware, as the difference in specs between Apple's hardware and product ranges from other OEMs is no longer noticeable. Apple's laughable insistence on shipping laptops with 8GB of RAM is a good example. > Even in the higher end products like the MacBooks you see a lot of professionals (engineers included) who choose it because of its price-performance-value, and who don’t give a shit about luxury. I don't think so, and that contrasts with my personal experience. All my previous roles offered a mix of MacBooks and windows laptops, and MacBooks were opted by new arrivals because they were seen as perks and the particular choice of windows ones in comparison were not as impressive, even though they out-specced Apple's offering (mid-range HP and Dell). In fact in a recent employee's review their main feedback was that the MacBook pro line was under-specced because at best it shipped with only 16GB of RAM while the less impressive HP ones already came with 32GB. In previous years, they called for the replacement of the MacBook line due to the rate of keyboard malfunctions. Meaning, engineers were purposely picking the underperforming option for non-technical reasons. reply lynx23 1 hour agorootparentI bought my first Apple product roughly 11 years ago explicitly because it had the best accessibility support at the time (and that is still true). While I realize you only see your slice of the world, I really cringe when I see the weasel-word \"lifestyle\". This \"Apple is for the rich kids\"-fairytale is getting really really old. reply hinkley 2 hours agorootparentprevMost of Apple’s money comes from iPhones. reply jojobas 6 hours agorootparentprevSpending your life on a phone is still a lifestyle \"choice\". reply KptMarchewa 12 hours agorootparentprevnext [5 more] [flagged] vr46 11 hours agorootparentCountering a lazy reference with some weird racist stereotype was the best you could do? reply klabb3 10 hours agorootparentprevClumsily phrased. What I meant is that iPhones or similar priced smartphones are affordable and common for say middle class in countries with similar purchase power to Eastern European countries. You’d have to go to poorer countries like Vietnam or Indonesia for iPhones to be “out of reach”, given the immense value it provides. Heck now I see even Vietnam iPhone is #1 vendor with a 28% market penetration according to statcounter. That’s more than I thought, even though I was just there… Speaking of India, they’re at 4% there. That’s closer to being luxury. reply immibis 8 hours agorootparentI think US is their main market, though. The rest of the world prefers cheaper better phones and doesn't mind using WhatsApp for messaging, instead of iMessage. reply klabb3 6 hours agorootparentAs a single market, US is probably biggest. I’m seeing numbers that say that the “Americas” is a bit less than half of global revenue, and that would include Canada and all of South and Latin America. So the rest of the world is of course very important to Apple, at least financially. > doesn't mind using WhatsApp for messaging Well WhatsApp was super early and way ahead of any competition, and the countries where it penetrated had no reason to leave, so it’s not exactly like they settle for less. It has been a consistently great service (in the category of proprietary messaging apps), even after Zuck took over. reply moritzwarhier 12 hours agorootparentprev> > In case it is not abundantly clear by now: Apple's AI strategy is to put inference (and longer term even learning) on edge devices. This is completely coherent with their privacy-first strategy (which would be at odds with sending data up to the cloud for processing). > Their primary business goal is to sell hardware. There is no contradiction here. No need for luxury. Efficient hardware scales, Moore's law has just been rewritten, not defeated. Power efficiency combined with shared and extremely fast RAM, it is still a formula for success as long as they are able to deliver. By the way, M-series MacBooks have crossed bargain territory by now compared to WinTel in some specific (but large) niches, e.g. the M2 Air. They are still technically superior in power efficiency and still competitive in performance in many common uses, be it traditional media decoding and processing, GPU-heavy tasks (including AI), single-core performance... By the way, this includes web technologies / JS. reply ewhanley 9 hours agorootparentThis is it. An M series air is an incredible machine for most people - people who likely won’t ever write a line of js or use a GPU. Email, banking, YouTube, etc ona device with incredible battery and hardware that will likely be useful for a decade is perfect. The average user hasn’t even heard of HN. reply xvector 8 hours agorootparentIt's great for power users too. Most developers really enjoy the experience of writing code on Macs. You get a Unix based OS that's just far more usable and polished than a Linux laptop. If you're into AI, there's objectively literally no other laptop on the planet that is competitive with the GPU memory available on an MBP. reply 8fingerlouie 1 hour agorootparentprev> but it is about selling luxury hardware. While Apple is first and foremost a hardware company, it has more or less always been about the \"Apple experience\". They've never \"just\" been a hardware company. For as long as Apple has existed, they've done things \"their way\" both with hardware and software, though they tend to want to abstract the software away. If it was merely a question of selling hardware, why does iCloud exist ? or AppleTV+, or Handoff ? or iMessage, or the countless other seemingly small life improvements that somehow the remainder of the industry cannot seem to figure out how to do well. Just a \"simple\" thing as switching headphones seamlessly between devices is something i no longer think about, it just happens, and it takes a trip with a Windows computer and a regular bluetooth headset to remind me how things used to be. As part of their \"privacy first\" strategy, iMessage also fits in nicely. Apple doesn't have to operate a huge instant messaging network, which undoubtedly is not making a profit, but they do, because having one entry to secure, encrypted communication fits well with the Apple Experience. iMessage did so well at abstracting the ugly details of encryption that few people even think about that that's what the blue bubble is actually about, it more or less only means your message is end to end encrypted. As a side effect you can also send full resolution images (and more), but that's in no way unique to iMessage. reply VelesDude 10 hours agorootparentprevI think it was Paul Thurrott on Windows Weekly podcast who said that all these companies don't really care about privacy. Apple takes billions of dollar a year to direct data towards Google via the search defaults. Clearly privacy has a price. And I suspect it will only get worse with time as they keep chasing the next quarter. Tim Cook unfortunately is so captured in that quarterly mindset of 'please the share holders' that it is only a matter of time. reply Cthulhu_ 1 hour agorootparentThe Google payments are an interesting one; I don't think it's a simple \"Google pays them to prefer them\", but a \"Google pays them to stop them from building a competitor\". Apple is in the position to build a competing search product, but the amount Google pays is the amount of money they would have to earn from it, and that is improbable even if it means they can set their own search engine as default. reply spacebanana7 54 minutes agorootparentprevApple isn't ethically Mullvad, but they're much better than some of their android competitors who allow adverts on the lock screen. reply cvwright 10 hours agorootparentprevIt doesn’t matter to me if they “really care” about privacy or not. Megacorps don’t “really care” about anything except money. What matters to me is that they continue to see privacy as something they can sell in order to make money. reply VelesDude 9 hours agorootparentYeah, some poor phrasing on my behalf. I do hope that those working in these companies actually building the tools do care. But unfortunately, it seems that corruption is an emergent property of complexity. reply serial_dev 14 hours agorootparentprevIt doesn't need to stay true forever. The alternative is Google / Android devices and OpenAI wrapper apps, both of which usually offer a half baked UI, poor privacy practices, and a completely broken UX when the internet connection isn't perfect. Pair this with the completely subpar Android apps, Google dropping support for an app about once a month, and suddenly I'm okay with the lesser of two evils. I know they aren't running a charity, I even hypothesized that Apple just can't build good services so they pivoted to focusing on this fake \"privacy\" angle. In the end, iPhones are likely going to be better for edge AI than whatever is out there, so I'm looking forward to this. reply rfoo 14 hours agorootparent> The alternative is Google / Android devices No, the alternative is Android devices with everything except firmware built from source and signed by myself. And at the same time, being secure, too. You just can't have this on Apple devices. On Android side choices are limited too, I don't like Google and especially their disastrous hardware design, but their Pixel line is the most approachable one able to do all these. Heck, you can't even build your own app for your own iPhone without buying another hardware (a Mac, this is not a software issue, this is a legal issue, iOS SDK is licensed to you on the condition of using on Apple hardware only) and a yearly subscription. How is this acceptable at all? reply mbreese 12 hours agorootparent> No, the alternative is Android devices with everything except firmware built from source and signed by myself Normal users will not do this. Just because many of the people here can build and sign a custom Android build doesn't mean that is a viable commercial alternative. It is great that is an option for those of us who can do it, but don't present it as a viable alternative to the iOS/Google ecosystems. The fraction of people who can and will be willing to do this is really small. And even if you can do it, how many people will want to maintain their custom built OSes? reply rodgerd 10 hours agorootparent> Normal users will not do this. J Unfortunately a lot of the \"freedom\" crowd think that unless you want to be an 80s sysadmin you don't deserve security or privacy. Or computers. reply muyuu 9 hours agorootparentthe main reason the masses don't have privacy and security-centred systems is that they don't demand them and they will trade it away for a twopence or for the slightest increment in convenience a maxim that seems to hold true at every level of computing is that users will not care about security unless forced into caring with privacy they may care more, but they are easily conditioned to assume it's there or that nothing can be realistically be done about losing it reply notpushkin 7 hours agorootparentprevI, an engineer, am not doing this myself, too. There is a middle ground though: just use a privacy-oriented Android build, like DivestOS. [1] There are a couple caveats: 1. It is still a bit tricky for a non-technical person to install. Should not be a problem if they know somebody who can help, though. There's been some progress making the process more user friendly recently (e.g. WebUSB-based GrapheneOS installer). 2. There are some papercuts if you don't install Google services on your phone. microG [2] helps with most but some still remain. My main concern with this setup is that I can't use Google Pay this way, but having to bring my card with me every time seems like an acceptable trade off to me. [1]: https://divestos.org/ [2]: https://microg.org/ reply int_19h 4 hours agorootparentThe biggest problem with these kinds of setups is usually the banking apps which refuse to run if it's not \"safe\". reply simfree 14 hours agorootparentprevWebGPU and many other features on iOS are unimplemented or implemented in half-assed or downright broken ways. These features work on all the modern desktop browsers and on Android tho! reply Aloisius 13 hours agorootparent> WebGPU and many other features WebGPU isn't standardized yet. Hell, most of the features people complain about aren't part of any standard, but for some reason there's this sense that if it's in Chrome, it's standard - as if Google dictates standards. reply moooo99 12 hours agorootparent> but for some reason there's this sense that if it's in Chrome, it's standard - as if Google dictates standards. Realistically, given the market share of Chrome and Chromium based browsers, they kind of do. reply rootusrootus 9 hours agorootparentI didn't like it when Microsoft dominated browsers, and I'm no happier now. I've stopped using Chrome. reply notpushkin 7 hours agorootparentJust curious – what are you using now? reply feisuzhu 6 hours agorootparentI’ve been using Firefox since the Quantum version is out. It feels slightly slower to Chrome but it's negligible to me. Otherwise I can't tell a difference (except some heavy web based Office like solutions screaming 'Your browser is not supported!' but actually works fine). reply yencabulator 12 hours agorootparentprevMeanwhile, Apple has historically dictated that Google can't publish Chrome for iOS, only a reskinned Safari. People in glass-walled gardens shouldn't throw stones. reply simfree 3 hours agorootparentprevFirefox has an implementation of WebGPU, why is Safari missing in action? reply nrb 13 hours agorootparentprev> How is this acceptable at all? Because as you described, the only alternatives that exist are terrible experiences for basically everyone, so people are happy to pay to license a solution that solves their problems with minimal fuss. Any number of people could respond to “use Android devices with everything except firmware built from source and signed by myself” with the same question. reply adamomada 14 hours agorootparentprevThe yearly subscription is for publishing your app on Apple’s store and definitely helps keep some garbage out. Running your own app on your own device is basically solved with free third party solutions now (see AltStore and since a newer method I can’t recall atm) reply dns_snek 1 hour agorootparentNotice that parent never talked about publishing apps, just building and running apps on their own device. \"Publishing on AltStore\" (or permanently running the app on your own device in any other way) still requires a $100/year subscription as far as I'm aware. reply tsimionescu 3 hours agorootparentprevThose are only available in the EU, and Apple has been huffing and puffing even here. reply fsflover 2 hours agorootparentprev> No, the alternative is Android devices with everything except firmware built from source and signed by myself I wouldn't bet on this long term, since it fully relies on Google hardware, and Google's long-term strategy is to remove your freedom piece by piece, cash on it, not to support it. The real alternative is GNU/Linux phones, Librem 5 and Pinephone, without any ties to greedy, anti-freedom corporations. reply ToucanLoucan 13 hours agorootparentprev> Heck, you can't even build your own app for your own iPhone without buying another hardware (a Mac, this is not a software issue, this is a legal issue, iOS SDK is licensed to you on the condition of using on Apple hardware only) and a yearly subscription. How is this acceptable at all? Because they set the terms of use of the SDK? You're not required to use it. You aren't required to develop for iOS. Just because Google gives it all away for free doesn't mean Apple has to. reply rfoo 53 minutes agorootparent> You aren't required to develop for iOS. Sure, as a SWE I'm not going to buy a computer unable to run my own code. A smartphone is an ergonomic portable computer, so I say no to iPhone and would like to remind others who didn't have a deep think into this about it. reply ClumsyPilot 12 hours agorootparentprev> You aren't required to develop for iOS Do you have a legal right to write software or run your own software for hardware you bought? Because it’s very easy to take away a right by erecting aritificial barriers, just like how you could discriminate by race at work, but pretend you are doing something else, reply paulmd 2 hours agorootparent> Do you have a legal right to write software or run your own software for hardware you bought? No, obviously not. Do you have a right to run a custom OS on your PS5? Do you have a right to run a custom application on your cable set-top box? Etc. Such a right obviously doesn’t exist and most people generally are somewhere between “don’t care” and actively rejecting it for various reasons (hacking in games, content DRM, etc). It’s fine if you think there should be, but it continues this weird trend of using apple as a foil for complaining about random other issues that other vendors tend to be just as bad or oftentimes even worse about, simply because they’re a large company with a large group of anti-fans/haters who will readily nod along. Remember when the complaint was that the pelican case of factory OEM tools you could rent (or buy) to install your factory replacement screen was too big and bulky, meaning it was really just a plot to sabotage right to repair? https://www.theverge.com/2022/5/21/23079058/apple-self-servi... reply dns_snek 58 minutes agorootparent> Remember when the complaint was that the pelican case of factory OEM tools you could rent (or buy) to install your factory replacement screen was too big and bulky, meaning it was really just a plot to sabotage right to repair? Yes, I do. That was and continues to be a valid complaint, among all other anti-repair schemes Apple have come up with over the years. DRM for parts, complete unavailability of some commonly repaired parts, deliberate kneecapping of \"Apple authorized service providers\", leveraging the US customs to seize shipments of legitimate and/or unlabeled replacement parts as \"counterfeits\", gaslighting by official representatives on Apple's own forums about data recovery, sabotaging right to repair laws, and even denial of design issues[1] to weasel out of warranty repair just to name a few. All with the simple anti-competitive goal of making third party repair (both authorized and independent) a less attractive option due to artificially increased prices, timelines to repair, or scaremongering about privacy. https://arstechnica.com/gadgets/2022/12/weakened-right-to-re... https://www.pcgamer.com/ifixit-says-apples-iphone-14-is-lite... [1] Butterfly keyboards, display cables that were too short and failed over time reply ToucanLoucan 12 hours agorootparentprev> Do you have a legal right to write software or run your own software for hardware you bought? I've never heard of such a thing. Ideally I'd like that, but I don't have such freedoms with the computers in my cars, for example, or the one that operates my furnace, or even for certain parts of my PC. reply ClumsyPilot 12 hours agorootparentSo you bought \"a thing' but you can't control what it does, how it does it, you don't get to decide what data it collects or who can see that data. You aren't allowed to repair the \"thing' because the software can detect you changed something and will refuse to boot. And whenever it suits the manufacturer, they will decide when the 'thing' is declared out of support and stops functioning. I would say you are not an owner then, you (and me) and just suckers that are paying for the party. Maybe it's a lease. But then we also pay when it breaks, so it more of a digital feudalism. reply jocaal 14 hours agorootparentprev> better for edge AI than whatever is out there, so I'm looking forward to this What exactly are you expecting? The current hype for AI is large language models. The word 'large' has a certain meaning in that context. Much larger that can fit on your phone. Everyone is going crazy about edge AI, what am I missing? reply gopher_space 13 hours agorootparent> Everyone is going crazy about edge AI, what am I missing? If you clone a model and then bake in a more expensive model's correct/appropriate responses to your queries, you now have the functionality of the expensive model in your clone. For your specific use case. The size of the resulting case-specific models are small enough to run on all kinds of hardware, so everyone's seeing how much work can be done on their laptop right now. One incentive for doing so is that your approaches to problems are constrained by the cost and security of the Q&A roundtrip. reply jitl 14 hours agorootparentprevQuantized LLMs can run on a phone, like Gemini Nano or OpenLLAMA 3B. If a small local model can handle simple stuff and delegate to a model in the data center for harder tasks and with better connectivity you could get an even better experience. reply mr_toad 8 hours agorootparentUsing RAG a smaller local LLM combined with local data (e.g. your emails, iMessages etc) can be useful than a large external LLM that doesn’t have your data. No point asking GPT4 “what time does John’s party start?”, but a local LLM can do better. reply jwells89 4 hours agorootparentThis is why I think Apple’s implementation of LLMs is going to be a big deal, even if it’s not technically as capable. Just making Siri better able to converse (e.g. ask clarifying questions) and giving it the context offered by user data will make it dramatically more useful than silo’d off remote LLMs. reply SmellTheGlove 14 hours agorootparentprev> If a small local model can handle simple stuff and delegate to a model in the data center for harder tasks and with better connectivity you could get an even better experience. Distributed mixture of experts sounds like an idea. Is anyone doing that? reply cheschire 13 hours agorootparentSounds like an attack vector waiting to happen if you deploy enough competing expert devices into a crowd. I’m imagining a lot of these LLM products on phones will be used for live translation. Imagine a large crowd event of folks utilizing live AI translation services being told completely false translations because an actor deployed a 51% attack. reply jagger27 12 hours agorootparentI’m not particularly scared of a 51% attack between the devices attached to my Apple ID. If my iPhone splits inference work with my idle MacBook, Apple TV, and iPad, what’s the problem there? reply moneywoes 12 hours agorootparentprevwhat about in situations with no bandwidth? reply callalex 14 hours agorootparentprevIn the hardware world, last year’s large has a way of becoming next year’s small. For a particularly funny example of this, check out the various letter soup names that people keep applying to screen resolutions. https://en.m.wikipedia.org/wiki/Display_resolution_standards... reply jchanimal 14 hours agorootparentprevIt fits on your phone, and your phone can offload battery burning tasks to nearby edge servers. Seems like the path consumer-facing AI will take. reply cbsmith 14 hours agorootparentprevGoogle has also been working on (and provides kits for) local machine learning on mobile devices... and they run on both iOS and Android. The Gemini App does send data in to Google for learning, but even that you can opt out of. Apple's definitely pulling a \"Heinz\" move with privacy, and it is true that they're doing a better job of it overall, but Google's not completely horrible either. reply kernal 14 hours agorootparentprev>subpar Android apps Care to cite these subpar Android apps? The app store is filled to the brim with subpar and garbage apps. >Google dropping support for an app about once a month I mean if you're going to lie why not go bigger >I'm okay with the lesser of two evils. So the more evil company is the one that pulled out of China because they refused to hand over their users data to the Chinese government on a fiber optic silver plate? reply martimarkov 14 hours agorootparentGoogle operates in China albeit via their HK domain. They also had project DragonFly if you remember. The lesser of two evils is that one company doesn’t try to actively profile me (in order for their ads business to be better) with every piece of data it can find and forces me to share all possible data with them. Google is famously known to kill apps that are good and used by customers: https://killedbygoogle.com/ As for the subpar apps: there is a massive difference between the network traffic when on the Home Screen between iOS and Android. reply kernal 12 hours agorootparent>Google operates in China albeit via their HK domain. The Chinese government has access to the iCloud account of every Chinese Apple user. >They also had project DragonFly if you remember. Which never materialized. >The lesser of two evils is that one company doesn’t try to actively profile me (in order for their ads business to be better) with every piece of data it can find and forces me to share all possible data with them. Apple does targeted and non targeted advertising as well. Additionally, your carrier has likely sold all of the data they have on you. Apple was also sued for selling user data to ad networks. Odd for a Privacy First company to engage in things like that. >Google is famously known to kill apps that are good and used by customers: https://killedbygoogle.com/ Google has been around for 26 years I believe. According to that link 60 apps were killed in that timeframe. According to your statement that Google kills an app a month that would leave you 252 apps short. Furthermore, the numbers would indicate that Google has killed 2.3 apps per year or .192 apps per month. >As for the subpar apps: there is a massive difference between the network traffic when on the Home Screen between iOS and Android. Not sure how that has anything to do with app quality, but if network traffic is your concern there's probably a lot more an Android user can do than an iOS user to control or eliminate the traffic. reply FireBeyond 9 hours agorootparent> Google has been around for 26 years I believe. According to that link 60 apps were killed in that timeframe. According to your statement that Google kills an app a month that would leave you 252 apps short. Furthermore, the numbers would indicate that Google has killed 2.3 apps per year or .192 apps per month. Most of the \"Services\" on that list are effectively apps, too: VPN by Google One, Album Archive, Hangouts, all the way back to Answers, Writely, and Deskbar. I didn't touch hardware, because I think that should be considered separately. The first of 211 services on that site was killed in 2006. The first of the 60 apps on that site was killed in 2012. So even apps alone, 4.28 a year. But more inclusively, 271 apps or services in 17 years is ~16/year, over one a month. You need to remind yourself of the site guidelines about assuming the worst. Your comments just come across condescendingly. reply kernal 3 hours agorootparent>Most of the \"Services\" on that list are effectively apps, too: Even with the additional apps you've selected it still doesn't come close to the one app per month claim. >I didn't touch hardware, because I think that should be considered separately. So why even mention it? Is Apple impervious to discontinuing hardware? >The first of 211 services on that site was killed in 2006. So we're talking about services now? Or apps? Or apps and services? The goal posts keep moving. >You need to remind yourself of the site guidelines about assuming the worst. Your comments just come across condescendingly. I suggest you also consult the guidelines in regards to calling people names. My comments were never intended to be inferred that way. reply Refusing23 3 hours agorootparentprevTheir 2nd largest revenue source (at ... 20-25%, below only the iphone) is software services. iCloud, App store revenue, apple tv, and so on reply kortilla 13 hours agorootparentprevThe MacBook Air is not a luxury device. That meme is out of date reply lolinder 11 hours agorootparentI can't buy a MacBook Air for less than $999, and that's for a model with 8GB RAM, an 8-core CPU and 256GB SSD. The equivalent (based on raw specs) in the PC world runs for $300 to $500. How is something that is twice as expensive as the competition not a luxury device? EDIT: Because there's repeated confusion in the replies: I am not saying that a MacBook Air is not objectively a better device. I'm saying it is better by metrics that fall strictly into the \"luxury\" category. Better build quality, system-on-a-chip, better OS, better battery life, aluminum case—all of these are luxury characteristics that someone who is looking for a functional device that meets their needs at a decent price won't have as dealbreakers. reply insaneirish 7 hours agorootparent> How is something that is twice as expensive as the competition not a luxury device? You can buy a version offrom Walmart at 1/2 price of a \"normal\" retailer. Does that mean every \"normal\" retailer is actually a luxury goods dealer? Is my diner a luxury restaurant because a burger costs twice as much as McDonald's? Stop the silliness. reply lolinder 6 hours agorootparentAll I'm learning from comments like this is that there are a lot of people who are very resistant to the idea that they buy luxury goods. reply Dylan16807 3 hours agorootparentprev> You can buy a version offrom Walmart at 1/2 price of a \"normal\" retailer. Does that mean every \"normal\" retailer is actually a luxury goods dealer? What percent of that retailer's products does that comparison apply to? If it's more than half then yeah that's probably a luxury goods dealer. reply zombiwoof 9 hours agorootparentprevI bought a used thinkpad x13 for 350 bucks. It won me over from my m3 MacBook Pro that costs 4 times as much reply skibbityboop 8 hours agorootparentI got a Latitude 9430 on eBay for $520. This thing is an amazing laptop and I'd put it right there with the Macs I have to work with at dayjob, as far as build quality/feel. reply kotaKat 10 hours agorootparentprevI can walk into many Walmarts in the US right now with $699 and walk out with a MBA with the M1. That's a damn good deal. reply lolinder 10 hours agorootparentThat's still ~twice as expensive as the items I linked to below, and that's at clearance prices. A good deal on a luxury item still gets you a luxury item. And if we want to compare Walmart to Walmart, this thing currently runs for $359 and has 16GB RAM, 512GB SSD, and a CPU that benchmarks slightly faster than the M2: https://www.walmart.com/ip/Acer-Aspire-3-15-6-inch-Laptop-AM... reply eigen 5 hours agorootparent> this thing currently runs for $359 and has 16GB RAM, 512GB SSD, and a CPU that benchmarks slightly faster than the M2: I'm seeing significant differences in the performance between Acer Aspire A315 to M2 Macbook Air; the Acer is ~33% of the M2 for 50% the price. https://browser.geekbench.com/v6/cpu/compare/5942766?baselin... reply darkwater 1 hour agorootparentNowadays (well actually this has been true for the last 10 years) a normal user won't care about that extra perf ratio. The Aspire is \"fast enough\". reply AdamN 2 hours agorootparentprevThat's more like cheap vs middle of the road. There is no luxury space in laptops - displays, iPads, and workstations maybe but that's it (and those are more pro than luxury). $999 amortized over 3 years is $30/mo which is less than what even middle class people spend on coffee. reply epcoa 8 hours agorootparentprevOh dear. 16:10 screen with superior resolution, brightness and gamut - and it still gets superior battery life driving all those pixels.. that’s a headline feature that even a non-propellerhead can observe (I was honestly surprised when I looked up that Acer screen what a dim, narrow piece of shit it is) - notably there are ballpark priced systems with better screens. I think you unjustifiably downplay how much of a selling point a screen that looks great (or at least decent) on the floor is. And I know tons of devs that put up with the 45% NTSC abominations on Thinkpads that aren’t even suitable for casual photo editing or web media, just because you make do with that doesn’t automatically make a halfway decent display on a laptop a “luxury”. Sorry, but don’t buy the “everything that isn’t a $300 econo shit laptop is luxury” thesis repeated ad nauseum. reply int_19h 4 hours agorootparentWhat defines \"luxury\" exactly if not the combination of price and \"premium experience\"? reply tsimionescu 2 hours agorootparent\"Luxury\" often includes some amount of pure status symbols added to the package, and often on what is actually a sub-par experience. The quintessential luxury tech device were the Vertu phones from just before and even early in the smartphone era - mid-range phones tech and build quality-wise, with encrusted gems and gold inserts and other such bling, sold at several thousand dollars (Edit: they actually ranged between a few thousand dollars all the way to 50,000+). But the definition of luxury varies a lot by product category. Still, high-end and luxury are separate concepts, which ven when they do overlap. reply darkwater 1 hour agorootparentYou just made up the \"sub-par experience\" as a defining point of a luxury product. A luxury product is defined by being a status symbol (check for all Apple devices) and especially by its price. A luxury car like a Bentley will still you bring from point A to point B like the cheapest Toyota. reply positus 8 hours agorootparentprevI doubt I am alone in saying that I would gladly pay twice the price to avoid having to use Windows. It's the most user-hostile, hand-holdy, second-guess-and-confirm-my-explicit-command-ey os I've used to date. And bloatware baked in? No thanks. reply Dylan16807 3 hours agorootparentGood news, not using windows is free. reply bigstrat2003 4 hours agorootparentprevWindows is pretty shit these days, but it's not the only other option. Linux is far more sane than MacOS or Windows. reply neoromantique 8 hours agorootparentprev...but that is a luxury. reply positus 8 hours agorootparentYou're probably right. I am in the middle-class, maybe lower middle-class, and I live in the US. I have advantages and opportunities that many in other circumstances do not and I am sincerely grateful for them. reply fl0ki 8 hours agorootparentprevDamn right it is. reply FireBeyond 9 hours agorootparentprevThe Walmart variant was introduced 6 weeks ago to offload excess stocks of a four year old discontinued model. I'm not sure your argument of \"at only 70% of the price of a model two generations newer\" is the sales pitch you think it is. reply vinkelhake 10 hours agorootparentprevWalmart is currently selling Apple's old stock of M1 Airs. You can get the 8GB 256GB version for $699. reply lolinder 10 hours agorootparentSee my reply to the person that beat you to it: > https://news.ycombinator.com/item?id=40292804 tl;dr is that Walmart is also selling an Acer for $359 that beats that device on every headline metric. It's nice to know that I could get the old-gen model for slightly cheaper, but that's still an outrageous price if the MacBook Air isn't to be considered a luxury item. reply zer0zzz 9 hours agorootparentIt’s half the price because by the time the MacBook Air dies you’re on your second or third acer. reply lolinder 9 hours agorootparentMy last Acer lasted me six years until I decided to replace it for more power (which, notably, I would have done with a MacBook by then too). They're not as well built as a MacBook, but they're well built enough for the average laptop turnover rate. reply zer0zzz 4 hours agorootparentThat’s fair reply Der_Einzige 8 hours agorootparentprevThe apple defense force would rather die than admit that Apple hardware is overpriced and a bad value. 8gb of ram was pathetic in 2018, and is SUPER pathetic in 2024. reply fl0ki 8 hours agorootparentIf it was actually bad value they wouldn't sell as high as they do and review with as much consumer satisfaction as they do. These products may not offer you much value and you don't have to buy them. Clearly plenty of people and institutions bought them because they believed they offered the best value to them. reply lolinder 7 hours agorootparentAgreed. I'd definitely make the same arguments here as I would for an Audi. There's clearly a market, and that means they're not a bad value for a certain type of person. reply bigstrat2003 4 hours agorootparentprevIf people were actually rational that might be true, but they aren't. Apple survives entirely on the fact that they have convinced people they are cool, not because they actually provide good value. reply vampiresdoexist 10 hours agorootparentprevBuild quality, battery life, clean os install, and the value held over time has no Windows equivalent even at some much higher price points. reply lolinder 10 hours agorootparentThe same thing can be (and is!) said about luxury car brands. That's what makes the MacBook Air a luxury item. Most people, when given the pitch you just gave me for a 2x increase in price, will choose the cheaper item, just like they choose the cheaper car. reply vundercind 9 hours agorootparentThey’re tools. This attempt to treat them as luxury goods doesn’t hold with those. It’s entirely common for even people who want to do some home repair—let alone professionals—but aren’t clueless about DIY to spend 2x the cheapest option, because they know the cheapest one is actually worth $0. More will advocate spending way more than 2x, as long as you’re 100% sure you’re going to use it a lot (like, say, a phone or laptop, even for a lot of non-computer-geeks). This is true even if they’re just buying a simple lowish-power impact driver, nothing fancy, not the most powerful one, not the one with the most features. Still, they’ll often not go for the cheapest one, because those are generally not even fit for their intended purpose. [edit] I mean sure there are people who just want the Apple logo, I’m not saying there are zero of those, but they’re also excellent, reliable tools (by the standards of computers—so, still bad) and a good chunk of their buyers are there for that. Even the ones who only have a phone. reply lolinder 9 hours agorootparentI didn't go for the cheapest option: I'm typing this on a laptop that I bought a few months ago for $1200. It has an aluminum case, 32GB RAM, an AMD Ryzen CPU that benchmarks similar to the M3, and 1TB SSD. I can open it up and replace parts with ease. The equivalent from Apple would currently run me $3200. If I'm willing to compromise to 24GB of RAM I can get one for $2200. What makes an Apple device a luxury item isn't that it's more expensive, it's that no matter what specs you pick it will always be much more expensive than equivalent specs from a non-luxury provider. The things that Apple provides are not the headline stats that matter for a tool-user, they're luxury properties that don't actually matter to most people. Note that there's nothing wrong with buying a luxury item! It's entirely unsurprising that most people on HN looking at the latest M4 chip prefer luxury computers, and that's fine! reply musicale 3 hours agorootparent> no matter what specs you pick it will always be much more expensive than equivalent specs from a non-luxury provider On the phone side, I guess you would call Samsung and Google luxury providers? On the laptop side there are a number of differentiating features that are of general interest. > The things that Apple provides are not the headline stats that matter for a tool-user, they're luxury properties that don't actually matter to most people Things that might matter to regular people (and tool users): - design and build for something you use all day - mic and speakers that don't sound like garbage (very noticeable and relevant in the zoom/hybrid work era) - excellent display - excellent battery life - seamless integration with iPhone, iPad, AirPods - whole widget: fewer headaches vs. Windows (ymmv); better app consistency vs. Linux - in-person service/support at Apple stores It's hard to argue that Apple didn't reset expectations for laptop battery life (and fanless performance) with the M1 MacBook Air. If Ryzen has caught up, then competition is a good thing for all of us (maybe not intel though...) In general Apple isn't bleeding edge, but they innovate with high quality, very usable implementations (wi-fi (1999), gigabit ethernet (2001), modern MacBook Pro design (2001), \"air\"/ultrabook form factors (2008), thunderbolt (2011), \"retina\" display and standard ssd (2012), usb-c (2016), M1: SoC/SiP/unified memory/ARM/asymmetric cores/neural engine/power efficiency/battery life (2020) ...and occasionally with dubious features like the touchbar and butterfly keyboard (2016).) reply vundercind 8 hours agorootparentprevHuh. Most of the folks I know on Apple stuff started out PC (and sometimes Android—I did) and maybe even made fun of Apple devices for a while, but switched after exposure to them because they turned out to be far, far better tools. And not even much more expensive, if at all, for TCO, given the longevity and resale value. reply lolinder 7 hours agorootparentEh, I have to use a MacBook Pro for work because of IT rules and I'm still not sold. Might be because I'm a Linux person who absolutely must have a fully customizable environment, but MacOS always feels so limited. The devices are great and feel great. Definitely high quality (arguably, luxury!). The OS leaves a lot to be desired for me. reply vundercind 7 hours agorootparentI spent about a decade before switching using Linux as my main :-) Mostly Gentoo and Ubuntu (man, it was good in the first few releases) Got a job in dual-platform mobile dev and was issued a MacBook. Exposure to dozens of phones and tablets from both ecosystem. I was converted within a year. (I barely customize anything these days, fwiw—hit the toggle for “caps as an extra ctrl”, brew install spectacle, done. Used to have opinions about my graphical login manager, use custom icon sets, all that stuff) reply bigstrat2003 5 hours agorootparentprevAlso, those things aren't even true about Apple devices. Apple fanboys have been convinced that their hardware really is way better than everything else for decades. It has never been true and still isn't. reply pompino 6 hours agorootparentprevWhat metrics are you using for build quality? Admittedly I don't know a ton of mac people (I'm an engineer working in manufacturing) but the mac people I know, stuff always breaks, but they're bragging about how apple took care of it for free. reply nxicvyvy 8 hours agorootparentprevClean os install? You haven't used windows in a while have you? Im a Linux guy but am forced to use Mac's and windows every now and then. Windows has outpaced macos for a decade straight. Macos looks like it hasn't been updated in years. It's constantly bugging me for passwords for random things. It is objectively the worst OS. I'd rather work on a Chromebook. reply theshackleford 7 hours agorootparentI’m not a single operating system guy like you. I use all three professionally. I’ve never had the bizzare struggle you describe. reply worthless-trash 6 hours agorootparentI think he has different critera on what bothers him, thats okay though isn't it. I get a little annoyed at anything where I have to use a touchpad, not enough to rant about it, but it definitely increases friction (haha) in my thought process. reply datadrivenangel 11 hours agorootparentprevHow much does it cost to get a device with comparable specs, performance, and 18 hour battery life? Closer to $999 then $500. reply lolinder 11 hours agorootparentThis CPU benchmarks in the same ballpark as the M2 and it runs for $329: https://www.amazon.com/Lenovo-IdeaPad-Ryzen5-5500U-1920x1080... An 18 hour battery life is a luxury characteristic, not something penny pinchers will typically be selecting on. reply outworlder 11 hours agorootparentWhat about the rest of the system? The SSD, for example? Apple likes to overcharge for storage, but the drives are _really_ good. reply lolinder 11 hours agorootparentWhen you're breaking out SSD speeds you're definitely getting into the \"luxury\" territory. As I said in another comment: The point isn't that the MacBook Air isn't better by some metrics than PC laptops. A Rolls-Royce is \"better\" by certain metrics than a Toyota, too. What makes a device luxury is if it costs substantially more than competing products that the average person would consider a valid replacement. reply goosedragons 8 hours agorootparentprevThey're average. A 512GB M3 MBA gets like 3000MBps for read/write. A 1TB Samsung 990 Pro, which costs less than the upgrade from 256GB to 512GB on the Air is over twice as fast. And on base models Apple skimps and speeds are slower. reply hatsix 10 hours agorootparentprevThere is no user buying a lowest-tier Macbook Air who would be able to tell the difference between the Lenovo SSD and the Macbook SSD. reply FireBeyond 9 hours agorootparentprevWhen I bought my cheesegrater Mac Pro, I wanted 8TB of SSD. Except Apple wanted $3,000 for 7TB of SSD (considering the base price already included 1TB). Instead, I bought a 4xM.2 PCI card, and 4 2TB Samsung Pro SSDs. I paid $1,300 for it, got to keep the 1TB \"system\" SSD. And I get faster speeds from it, 6.8GBps versus 5.5GBps off the system drive. For $2,000 I could have got the PCI 4.0 version and SSDs, and get 26GBps. reply ukuina 8 hours agorootparentExpandability is no longer an option with Apple Silicon. reply FireBeyond 5 hours agorootparentNot technically true. The Mac Pro 2023 has 6 PCI slots... ... for an eye watering $3,000 over the exact same spec Mac Studio. I liked my cheesegrater, though I didn't like the heat output. And I cannot justify that. I sacrificed half the throughput (2800MBps) for $379 and got an external 4 x M.2 TB3 enclosure. Oh, and a USB 3 hub to replace one I had installed in the cheesegrater to augment the built in ports. $400 give or take. reply hatsix 10 hours agorootparentprevYes, but having all three of those things (well, specs/performance is probably just one thing, but treating them as separate as you did means that I don't have to do the heavy lifting of figuring out what a third thing would actually be) IS, in fact, a luxury. Nobody is away from a power source for longer than 18 hours. MOST people don't need the performance that a macbook air has, their NEEDS would be met by a raspberry pi... that is, basic finances, logging into various services, online banking, things that first world citizens \"rely\" on. The definition of luxury is \"great comfort and extravagance\", and every current Apple product fits that definition. Past Apple definitely had non-luxury products, as recently as the iPhone C (discontinued 10 years ago)... but Apple has eliminated all low-value options from their lineup. reply sfmike 3 hours agorootparentprevGood question, I think the answer is even at thousands a window device battery can't hit 18 hour specs. Can someone name a windows device even at 2k+ that acts like an M chip? In fact the pricier windows usually mean GPU and those have worse battery then cheap windows(my 4090 is an hour or so off charge) reply p_l 1 hour agorootparentThinkpad X250, admittedly at max specs, did 21 hours in 2018. My T470 from 2020 did over 27 hours at max charge. M-series Macs is when MacBooks stopped sucking at battery life without sleeping and wrecking state every moment they could. reply pquki4 11 hours agorootparentprevWhat the point of comparison? Isn't 18 hour battery and Genius Bar part of the \"luxury\"? Like I say Audi is a luxury car because a Toyota costs less than half as much, and you ask \"what about a Toyota with leather seats\"? reply FireBeyond 9 hours agorootparentI am all in on Apple, to be clear. Mac Pros, multiple MBPs, Studio, Pro Display XDR, multiple Watches, phones, iPad Pro. My experiences (multiple) with Genius Bar have been decidedly more \"meh\" to outright frustrating, versus \"luxury\", oftentimes where I know more than the Genius. Logic Board issues where on a brand new macOS install I could reproducibly cause a kernel panic around graphics hardware. There was an open recall (finally, after waiting MONTHS) on this. It covered my Mac. But because it passed their diagnostic tool, they would only offer to replace the board on a time and materials basis. I had a screen delamination issue. \"It's not that bad - you can't see it when the screen is on, and you have to look for it\". Huh. Great \"luxury\" experience. And then the multiple \"we are going to price this so outrageously, and use that as an excuse to try to upsell\". Like the MBA that wouldn't charge due to a circuit issue. Battery fine, healthy. Laptop, fine, healthy, on AC. Just couldn't deliver current to the battery. Me, thinking sure, $300ish maybe with a little effort. \"That's going to be $899 to repair. That's only $100 less than a new MBA, maybe we should take a look at some of the new models?\" Uh, no. I'm not paying $900 for a laptop that spends 99% (well, 100% now) of its life on AC power. reply spurgu 11 hours agorootparentprevReally? You can find a laptop with the equivalent of Apple Silicon for $3-500? And while I haven't used Windows in ages I doubt it runs as well with 8 GB as MacOS does. reply lolinder 11 hours agorootparentSure, then try this one from HP with 16GB RAM and a CPU that benchmarks in the same ballpark as the M2, for $387: https://www.amazon.com/HP-Pavilion-i7-11370H-Micro-Edge-Anti... The point isn't that the MacBook Air isn't better by some metrics than PC laptops. A Rolls-Royce is \"better\" by certain metrics than a Toyota, too. What makes a device luxury is if it costs substantially more than competing products that the average person would consider a valid replacement. reply zlsa 6 hours agorootparentI'm not sure a machine that benchmarks half as fast as an M2 can be said to be in the same ballpark. MacBook Air (",
    "originSummary": [
      "Apple has unveiled the M4 chip in the latest iPad Pro, boasting a 10-core CPU, 10-core GPU, and advanced Neural Engine for better performance.",
      "The chip incorporates second-generation 3-nanometer tech for improved graphics and rendering, a new display engine, faster CPU and GPU, and advanced AI features.",
      "This initiative aligns with Apple's 2030 carbon neutrality target, highlighting the company's commitment to innovation, environmental sustainability, and robust AI capabilities in the tech industry."
    ],
    "commentSummary": [
      "Apple has launched the advanced M4 chip focusing on AI, emphasizing edge devices for privacy and low latency.",
      "Engineers favor MacBooks for their quality, OS, and performance, despite a debate on operating systems for development and a trend towards web apps.",
      "The transition to ARM processors on Macs benefits developers but presents challenges in virtual machines and specific development needs, including debates on luxury status, privacy, and customization limitations by Apple."
    ],
    "points": 1331,
    "commentCount": 1627,
    "retryCount": 0,
    "time": 1715092652
  },
  {
    "id": 40284823,
    "title": "Revolutionizing Cold Brew Coffee: Ultrasonic Waves Brew in 3 Minutes",
    "originLink": "https://www.unsw.edu.au/newsroom/news/2024/05/Ultrasonic_cold_brew_coffee_ready_under_three_minutes",
    "originBody": "Study Study menu Study options Explore degrees Short courses Online Our faculties Discover UNSW Undergraduate study Postgraduate study International students Higher Degree Research Our campus Accommodation Support for students How to apply Domestic undergraduate Domestic postgraduate International Higher Degree Research Fees Scholarships Help centre Ask a question Speak to a student Received an offer? Information for parents Information for educators Agent Hub Apply now Connect with us Research Research menu Partnerships Partner with us Small businesses Alliances and networks Precincts Higher degree research Find a Supervisor or Project Scholarships Application process Fees and costs Industry engagement International engagement opportunities Research capabilities Areas of focus Find a researcher Our impact Innovations Newsroom Infrastructure Facilities Faculties Faculties menu Our faculties Arts, Design & Architecture Business School Engineering Law & Justice Medicine & Health Science UNSW Canberra Engage with us Engage with us menu Engage with UNSW Social media Community outreach Complaints Giving Overview See the impact Why give to UNSW Areas to support Give now Alumni Overview News & events Benefits & services Ways to get involved Update your details Find an expert Find an expert Industry partnerships Partner with us Services and solutions Funding opportunities Small businesses Impact and case studies About us About us menu Our story Leadership & governance Our strategy Our culture Our people Our impact Social impact Innovation Enterprise Centres & institutes Collaboration Community Industry Government Our campus Faculties & Schools Respect & diversity Library Excellence Education Research Human resources News Events myUNSW Alumni & Giving Contact Us News Events myUNSW Alumni & Giving Contact Us Type in a search term Search Advanced search Search news Find an expert Subscribe Annual reports Contact Follow Follow Follow UNSW on LinkedIn Follow UNSW on Instagram Follow UNSW on Facebook Follow UNSW on WeChat Follow UNSW on TikTok Hear that? That’s the sound of an ultrasonic cold brew coffee ready in under three minutes Yesterday Photo: Getty Images Neil Martin, UNSW Sydney engineers have utilised sound waves to cut the time it takes to make a cold brew coffee from many hours down to mere minutes. Fans of cold brew coffee often rave about the smoother, less acidic and less bitter taste compared to a regular hot brew. There’s just one major problem – it takes anywhere from 12 to 24 hours to fully steep the grounds and allow the flavours to slowly be extracted using only cold water. That’s far from ideal if you want or need your caffeine fix in a hurry in the morning. However, engineers from UNSW have now developed a new way to make cold brew coffee in under three minutes – just like a regular hot brew – without sacrificing on the taste experience. The process involves using an ultrasonic reactor to speed up the brew of the grounded coffee beans, with the research published in Ultrasonics Sonochemistry. The ultrasonic brew Addressing the challenge of speeding up the brewing process of a cold brew, the UNSW team led by Dr Francisco Trujillo, used an existing model of a Breville espresso machine and superimposed their own patented sound transmission system. The system connects a bolt-clamped transducer with the brewing basket via a metallic horn – transforming a standard espresso filter basket into a powerful ultrasonic reactor. The reactor works by injecting sound waves at a frequency of 38.8 kHz at multiple points through its walls – generating multiple regions for acoustic cavitation within the reactor. The new faster cold brew system subjects coffee grounds in the filter basket to ultrasonic sound waves from a transducer, via a specially adapted horn. UNSW/Francisco Trujillo Media enquiries For enquiries about this story and interview requests please contact Neil Martin. Email: n.martin@unsw.edu.au Featured experts Francisco Trujillo, The study also found that this setup can double the extraction yield and caffeine concentration compared to unsonicated samples. Dr Trujillo, corresponding author of the paper and UNSW food processing engineer in the School of Chemical Engineering, says the ultrasound process speeds up the extraction of the oils, flavours and aroma of the ground coffee. “Ultrasounds accelerate the extraction process due to acoustic cavitation,” he says. “When acoustic bubbles collapse near the grounded coffee, they generate micro-jets with enough force to pit and fracture the coffee grounds – intensifying the extraction of the aroma and flavours of the brew. “And the acceleration is enormous – we are reducing what would typically take 12 to 24 hours to less than three minutes.” Dr. Trujillo says the aim was to make the process as easy as making a regular espresso coffee. “We’re able to demonstrate that this can be adapted to existing espresso machine,” he says. “We are very excited about developing this technology that can be used by companies that already manufacture coffee machines, so consumers can enjoy an ultrasonic cold brew at home in less than three minutes. “Very importantly, this breakthrough opens the door for coffee shops and restaurants to produce on-demand brews comparable to 24-hour cold brews, supplying the rising demand while eliminating the need for large semi-industrial brewing units and the associated requirement for extensive refrigeration space.” The UNSW team who developed the three-minute cold brew - Shih-Hao Chiu, Dr Francisco Trujillo and Nikunj Naliyadhara. UNSW/Cecilia Duong Same taste in less time Collaborating with researchers from University of Queensland, samples of coffee were sent to the Queensland Alliance for Agriculture and Food (QAAFI) where they underwent a thorough sensory analysis. The samples included cold brew sonicated at room temperature for one minute, the second was sonicated for 3 minutes, and the third was a regular 24 hours stepped brew at four degrees Celsius without sonication. They were evaluated on their aroma, texture, flavour, and aftertaste attributes. “Compared to the 24-hour brew, the sonicated one-minute brew sample received similar ratings, especially in flavour, aftertaste attributes including bitterness, sourness, fullness texture and aroma,” says Dr. Trujillo. “However, it scored lower in aroma intensity and dark chocolate aroma. This suggests that the sonicated 1-minute sample is slightly under-extracted compared to the 24-hour cold brew. “Meanwhile, the sonicated three-minute sample provided a similar dark chocolate aroma and aroma intensity to the 24-hour cold brew, but slightly more bitter. “The results of the sensory study showed that a sonication time between one and three minutes is ideal for creating a coffee comparable with 24-hour cold brew coffee, depending on the interest of customers.” University of Queensland Sensory Scientist and Flavour Chemist Professor Heather Smyth, a co-author of the paper, said the new method created a coffee as good as cold brew steeped overnight. “Our trained sensory panel tastings proved that we can get a very similar taste profile to either a traditional cold brew or an espresso in the time it takes to brew a hot espresso,” Prof. Smyth said. “Further work could explore different types of beans, different regions of beans and for companies or cafes that specialise in single origin or high-quality beans, this would be another string to the bow to show yet another dimension of coffee. “Once again, Australia has new technology at our fingertips that moves on from traditional methods to modern methods of coffee making to give consumers a new premium experience.” Shaking up the beverage industry Ultrasounds can be applied to several areas across the food industry including drying, extraction, emulsification and crystallisation – making the process faster and more efficient. Dr. Trujillo says this technology can be also used to speed up the brewing process of teas as well. “With this technology, we’re offering coffee shops, restaurants and even home coffee-brewers the chance to produce on-demand brews, eliminating the need for large brewing and refrigeration units.” Share this story Share this page on Email Share this page on Facebook Share this page on Twitter Share this page on LinkedIn Share this page on WhatsApp Share this page on FacebookMessenger Share this page on WeChat Share this page on Copy Related stories Why plant-based 'milks' are rising to the top 27 July 2022 Safe enough to eat - dishing up the answers in the food labelling debate 6 June 2021 Breakfast impacts student success, but not in the way you might think 15 April 2024 Salt, calcium and vitamin D – do you get enough, or too much? 19 January 2023 Subscribe to our daily news digest Subscribe Top UNSW.edu.au Engage with us Contact us Find an expert Careers at UNSW Education at UNSW Study UNSW International Degree Finder Student Portal Academic Calendar News, Media & Events Newsroom UNSW events Research news About us Our rankings & reputation Faculties and schools Campus locations Centres and institutes UNSW Library UNSW Sydney NSW 2052 Australia Telephone: +61 2 93851000 UNSW CRICOS Provider Code: 00098G TEQSA Provider ID: PRV12055 ABN: 57 195 873 179 Acknowledgement of Country UNSW is located on the unceded territory of the Bidjigal (Kensington campus), Gadigal (City and Paddington Campuses) and Ngunnawal peoples (UNSW Canberra) who are the Traditional Owners of the lands where each campus of UNSW is situated. The Uluru Statement Follow Us Follow Us Follow UNSW on LinkedIn Follow UNSW on Twitter (X) Follow UNSW on Facebook Follow UNSW on Instagram Follow UNSW on TikTok Follow UNSW on YouTube Follow UNSW on WeChat Follow UNSW on Weibo Privacy statement Copyright & Disclaimer Accessibility Site feedback Complaints",
    "commentLink": "https://news.ycombinator.com/item?id=40284823",
    "commentBody": "Cold brew coffee in 3 minutes using acoustic cavitation (unsw.edu.au)454 points by ople 21 hours agohidepastfavorite286 comments dahart 19 hours agoRecently I discovered that many coffee shops, maybe half in my sampling of a couple dozen in different cities, are selling cold coffee (brewed hot, then refrigerated) under the name cold brew, and even the ones that actually cold-brew them seem to be under the impression that it needs to be served cold. I was laughed at in one hipster joint for asking for a steamed or warmed cold-brew, and another one initially refused my request to warm it up saying that would make the coffee extremely sour. (It didn’t) Oh, and at least one other, maybe two, said they couldn’t warm cold brew (in view of both a steamer and microwave) or would have to charge extra (while someone’s cheaper latte was being steamed). Reading the paper, it’s not clear whether their cold brew has lower acidity (higher pH) than the same coffee hot brewed. It does say that the sonic-brew has the same pH as the normal long-steep cold brew. I’m also curious if this cavitation/sonication brewing process is basically agitating the coffee, or doing something different, and how different it is from manually agitating a cold brew compared to letting it sit still for hours. reply solardev 9 hours agoparentThis is why I love my fully automated luxury mediocre espresso maker (https://www.seattlecoffeegear.com/blogs/scg-blog/jura-a1-rev...). I push a button, it grinds the coffee and does stuff inside that I can't see, then moments later I have a perfectly average cup of something resembling burnt bean soup. I don't know whether it's megasonically brewed or absolute-zero infused or just wet caffeine pills. Sometimes it's OK, other times it's mediocre, but it's never been excellent or terrible. That's the same kind of consistency my code has, so I'm fine with it. It comes out lukewarm, hovering somewhere between room temperature and minutes-old vomit. If I want it hot, I microwave it. If I want it cold, I add ice. If I want a cold latte, I add milk. If I want a hot latte, I'm in the wrong house. It costs less than $1 for a quad shot. It provides caffeine or at least a close-enough placebo effect. What more could an old, washed-out dev ask for? reply WilTimSon 32 minutes agorootparentWell, you're lucky to not have spoiled your palate, in a way. I learned to love good coffee through my friends and I now only drink regular espressos from a machine if there's no other choice. And I'm not being snobby or anything, I fully understand why people drink it, I used to do it all the time. But my friends had to go and teach me about brewing methods and beans and yadda yadda. Friend of mine had a similar issue where his parents were wine people and, ever since he could drink, taught him how to pick good wine. Except he realised he does not want to pay that much for alcohol so he now just sticks to beer. reply Xerox9213 9 hours agorootparentprevBut how do you waste time in the morning if you’re not concocting some over complicated drink? I feel like my morning coffee is the one part of my day that I can control. You bet I’m gonna take my time and make it enjoyable. reply jorvi 58 minutes agorootparentI'd say in life you generally want to stick to 20/80 80/20. Spend 20% in effort and/or currency to get 80% of the optimal result. Spending the other 80% to get another 20% gain is rarely worth it. reply kimixa 5 hours agorootparentprevI also like the \"ritual\" of brewing a coffee to help me wake up in the morning. And even then, it's less than 10 minutes total preparing a pour over, it probably takes less time than cleaning out a filter machine would. And for people thinking you need some $$$ machine, even the most pretentious experts agree that pour over is about as good as that style of brewing can get (IE not something like espresso). A hand grinder and plastic v60 will get you to the point you're not limited by the equipment. The biggest expense is the beans themselves, and that's as much a taste thing as anything else. But I guess like any hobby you there will be people selling you all kinds of things at any price people are willing to pay - maybe it makes it a bit easier, maybe a bit quicker, maybe it looks better on your counter. Maybe it gives you something a bit different and unusual. But none of it gives you better coffee. reply lostlogin 1 hour agorootparent> there will be people selling you all kinds of things at any price people are willing to pay - maybe it makes it a bit easier, maybe a bit quicker, maybe it looks better on your counter. Maybe it gives you something a bit different and unusual. But none of it gives you better coffee. The Faema e61 Legend isn’t quicker or easier, but damn it is pretty. And that makes the coffee better. It’s also an effective space heater. https://www.faema.com/uk-en/product/E61-MONOFASE-ANNIVERSARI... reply lostlogin 1 hour agorootparentprev> But how do you waste time in the morning if you’re not concocting some over complicated drink? Many relaxing things are part ritual. I like your thought process. reply dahart 6 hours agorootparentprevI love that! The context I didn’t put in my top comment but added elsewhere is that this old washed-out dev requires low acidity because regular coffee was causing inflammation in my throat. That’s really all I want, less acid in my coffee, not something fancy. Except that I don’t want it cold, I want it hot like I’ve always had it. Other than that, I’m like you, I’m also perfectly fine with a perfectly average mediocre cup of joe. reply IIsi50MHz 6 hours agorootparentSmall amounts of potassium bicarbonate can be used to neutralise the remaining acidity. This helps prevent bitterness, and should not harm other aspects of the taste, unlike sodium bicarbonate. Dissolve into cold water, mix throughly, then add coffee grounds and stir. If you intend to store it, complete absence of light and absence of O2 wild also help prevent bitterness. reply ChrisMarshallNY 8 hours agorootparentprevFriend of mine, with money, has a coffee machine, by a company I’d never heard of, called “Jura”[0]. I’d not heard of it, because it’s not for plebes, like me. Grinds and brews a perfect cuppa in about a minute. I used to have a toddy maker, where you dumped a whole pound of ground coffee into a bucket of water, let it sit overnight, in the fridge, then you drained through a filter. The resulting thick liquid was like really good instant coffee. You threw a bit into a cup, added hot water, and it tasted great. [0] https://us.jura.com/en reply Cthulhu_ 9 minutes agorootparentJura seems to be more aimed at companies (who also have money), we had a few at HQ until we \"upgraded\" to a full on coffee/espresso machine, the type you see at proper coffee shops. reply beAbU 19 minutes agorootparentprevWe have 2x Juras in my office. They cost a significant portion of my annual salary last time I checked. Reliable, repeatable, the coffee is not super duper excellent, but it's also not terrible. It's perfectly passable and it's free. My type of coffee. reply CydeWeys 6 minutes agorootparentI'm not sure I'd even want a Jura in a home environment. There's so much cleaning and maintenance required! My style is more like good hand grinder (simple, very low maintenance, doesn't take up counter-top space) and an Aeropress and V60 (both simple pieces of plastic, super easy to clean). It's kind of unavoidable that it's going to be expensive and higher-maintenance to be able to make espresso at home, so I simply don't. Not worth it for me when I already have access to high-end espresso machines at work 5 days a week. reply croo 3 hours agorootparentprevWe have a Jura machine at my workplace since the beginning of time. It brews coffee as good as good the bean you use. After a month it asks for a cleaning cycle which is arcane magic, but other than that it just works and creates its daily 50-100 coffee as usual. I can recommend it, it just works. reply dc96 8 hours agorootparentprevLooks like the comment you replied to links to a Jura as well reply ChrisMarshallNY 8 hours agorootparentYeah. My friend has one that cost around 3 grand. The main issue that I would have, is maintenance. I would guess that it would need a fair bit of cleaning. I just use a fairly basic Braun Melitta filter, and it does great. reply solardev 7 hours agorootparentIt's got a decent self-cleaning cycle. Every month or two, it'll ask you to put in a cleaning tablet (about $2 for the OEM version, or there are cheaper generic ones). It goes through an internal cleanse and poops out a slop of muckety muck. I put on soft yoga music for mine, but that's optional. Ten minutes later it's ready to use again. About half as frequently, it'll also ask you to put in a descaling tablet. Similar process. Beyond that, day-to-day, it can make about 8 shots of espresso before the grounds hopper is full. You just dump it and rinse it in the sink (no need for a thorough wash) and it's ready to use again. Less cleanup than a regular drip coffee maker (no filters to deal with, no grind dust to rinse/brush, no glassware, nothing to dry). It's super convenient. The main downside is really just taste. I tried to do a blind taste test with my coffee snob friend (he's the kinda guy who measures everything down to the milligram and gives his grounds acupuncture before sending them to the spa). We used the same bag of beans, same water, same cups, etc. His came out with a layer of fine oils and sparkling foam. Mine looked like someone opened a dishwasher prematurely. We couldn't even get to the taste test part because you could smell the difference with your eyes closed. And I had a clogged nose that day. Maybe the $3k Jura is different, but my janky little unit is definitely a poor man's machine – the hand-me-down Civic of superautomatic coffee makers. I'd buy it again in a heartbeat though. reply AdamJacobMuller 7 hours agorootparentKind of interesting. I have the Jura Giga 5 and I would say it produces coffee which is better than I can get in any chain store, and, better than the average specialty shop as well. Obviously there are some specialty shops which produce excellent coffee which is better than the Jura, but, not by a massive margin. Interesting that you find the A1 to be much poorer in quality, or my pallet just sucks. Either way I fully agree with you that having \"push button, make coffee\" is fantastic, I don't want to fiddle with scales and worrying about blooming my coffee grounds for 14 seconds at 92c before brewing with water at 90c. Push button. Make good coffee. The other \"maintenance\" item I've noted after having this machine for >10 years is that every 5 years or so it breaks and I have to send it back to Jura. Their warranty service is a flat-rate $500 and they either repair yours or send you a refurb unit, for something which I spent almost 5k on, I'm very happy that they seem to have the option to basically keep the Jura working forever if I want. The Giga 5 prefers to listen to the Spotify \"upbeat pop music\" playlist when I run the cleaning cycle FYI. reply rfrey 6 hours agorootparentprevI would buy a book of you talking about coffee. reply solardev 8 hours agorootparentprevThat's actually the brand I have, and I'm as plebe-y as they come! I even have a mining pick in my garage that I use for manual labor. I'm nothing like the guy in the photo, more like the guy who makes the coffee for the guy who makes HIS coffee. You can get Juras for pretty cheap if you go bargain hunting. There are also other similar but cheaper brands. As a category, they're called \"superautomatic espresso machines\". I don't know why they're not more popular, but it's been a total game-changer for me. I got my Jura A1 (their discontinued base model) used, third-hand re-refurbished, a decade ago for like $700. A chunk of change upfront to be sure, but since then it's consistently made like 4-8 shots of espresso a day, every day. If reliable mediocrity were a virtue, this thing would be the patron saint of saints. And if a cup of store coffee were $5 (which is cheap nowadays), the machine pays for itself in 3-5 months. Best purchase I ever made. reply King-Aaron 7 hours agorootparentprevI like the cut of your jib. reply taylorportman 9 hours agorootparentprevYour ambivalence contributes little to the conversation. reply jdlshore 8 hours agorootparentWhereas I appreciated the unexpected spice, only to quickly be reminded by your comment that, yes, this is HackerNews. reply ClumsyPilot 9 hours agorootparentprevIt represents the same position of mediocre acceptance that many espresso affectionadors will arrrive at after spending many months/years mucking about with expensive kit. It’s the natural progression of most hobbies reply solardev 8 hours agorootparentprevWait till you see my cookbook, \"1-minute Meal Replacement Powders\", the perfect complement to my self-help/self-sabotage title, \"Better Living Through Lower Standards\". YOLO, and no one gets it right on the first try anyway... reply enjeyw 8 hours agorootparentAre you offering shares in your venture? I’d be mildly interested in investing on the expectation that I’d see returns that are slightly worse than inflation. Not that interested though, so whatever. reply toast0 7 hours agorootparentprevYou may be interested in my philosophy on uptime: nine nines is hard, how about eight eights. reply lamename 18 hours agoparentprevThank you for saying this. When cold brew first came out, it was promoted as a brewing process that resulted in smoother (I'm guessing lower acidity) tasting coffee. Heating it up seemed natural, and its use in iced coffee seemed simply opportunistic. (In my experience at least). Then it quickly caught on as a novelty, with nitro et al, and when I tell people I drink cold brew warmed I get looks of confusion or turned up noses. But brew temp and serving temp are orthogonal. reply SamBam 16 hours agorootparentBarismo in Cambridge does (or did) a \"hot draft\" coffee, that is always on tap, is delicious, and is remarkably like hot cold brew. Their method is apparently a secret (although I'm sure more digging could find it), but I wouldn't be surprised if it wasn't basically on-demand heated cold brew. [1] 1. https://www.baristamagazine.com/the-function-and-future-of-b... reply Morizero 15 hours agorootparentThanks for the tip, I'll hit them up tomorrow! reply Terr_ 15 hours agorootparentprev> But brew temp and serving temp are orthogonal. And if anyone doesn't believe this, challenge them to find a truly \"iced\" coffee. :p reply dahart 18 hours agorootparentprev> brew temp and serving temp are orthogonal Yes!!! I’m biting my tongue a little on how infuriating the process has been to ask cafes for warmed cold brews, but you’re spot on and exactly right. I’m baffled that so many people who sell coffee for a living, think they know a lot about it, and act like coffee snobs, don’t seem to understand what cold brew even is. (Or, in a few cases in my sampling I’m certain it was willful ignorance, laziness, because it takes a little more work and more space to cold brew.) I will say that one of my local cafes understood completely and they’re happy to make hot cold-brewed coffee, and made me feel welcome for asking for it. One or two others were very good about it, but hands down the majority of cafes were a bad experience when asking for a warmed cold brew. Good luck to them, they’ve lost my business. reply macNchz 14 hours agorootparentIn my experience having worked in coffee shops, restaurants and bars, there is a considerable overlap between the people asking for something off-menu that nobody has ever asked for, and the people who will never be satisfied with their order. It might seem like a very simple ask, but I think many people working in those jobs have learned it can be expedient to just say “we can’t do that” and short circuit the interaction, rather than to attempt whatever it is, have the customer send it back, attempt it again and have the customer start insulting them for not being able to “get it right”. This is particularly the case if there’s any sort of line, where one person sending something back will make every other customer angry. I’m not at all saying you are doing this yourself, just offering context on why you might encounter this reaction. As in many fields, a fraction of people are kind of awful and unfortunately their behavior winds up shaping how many things operate. reply dahart 7 hours agorootparentI can understand that. I do feel like the reaction I’m sometimes getting is reflexive and not given any thought. My daughter worked at Starbucks for a while, and told me about crappy customers doing this all the time - ordering drinks with questionable modifications and then sending them back when it wasn’t as good as they hoped. Her initial reaction to my steamed cold brew story was that I’m asking for something unusual. For the record, I have never sent back a warmed cold-brew for any reason. That said, part of what I’m blabbering on about is that I think cold-brew served hot should not be considered off-menu, I don’t think that’s entirely fair. Since cold brew served cold is an assumption in the first place, it seems like hot cold-brew is (or should be considered) just as on-menu as cold-served cold-brew. It’s fine that the assumption exists, I just don’t understand the pushback when I specify warm. I feel like calling a steamed cold-brew off-menu is exaggerating, considering that a) iced coffee exists; b) steaming espresso drinks is extremely common(!); c) many cafes that make espresso drinks essentially offer all combinations of brewing process, coffee, milks of various kinds, and flavorings. It’s so crazy to me to get shit for asking for a steamed cold-brew when something like a Caramel Ribbon Crunch Frappuccino with an Affogato shot and extra espresso exists and isn’t even considered weird or extreme. Maybe some cafes are pushing back against customers with Starbucks expectations, but they still offer a selection like espresso, cortado, mocha, latte, americano, flat white, blah blah blah. It’s like Mexican food, there is a name for every possible permutation of grounds, water, milk, sugar, and heat. Given that they have cold-brew, that they have a steamer, and that serving hot coffee and steaming things are both standard every-day every-order kinds of things, I simply can’t understand why I’d get pushback even if I am asking for something weird. I’m asking for something weird that is completely and trivially doable. Anyway, you’re right. I know I’m peeing into the wind just a little. It is what it is, which is why it’s a waste of energy to fight it or complain about it. :P reply NoGravitas 16 hours agorootparentprev> I’m baffled that so many people who sell coffee for a living, think they know a lot about it, and act like coffee snobs, don’t seem to understand what cold brew even is. Light roasts came (back?) into style among coffee snobs a few years ago because it highlights the difference between different sources/regions/whatever. Ever since then, the former best coffee shop in my town has been exclusively producing sour, vegetal, under-extracted brews. The justified reaction to Charbucks among coffee snobs has produced an objectively worse cup of coffee. reply kibibyte 7 hours agorootparentLight roasts are great! But there are just a lot of straight-up bad renditions of them, as a result of lack of adequate training on either roasting or brewing, to the consternation of many of those coffee snobs. Unfortunately, this just happens when shops follow trends. Roasting well in general is already quite challenging and is a lot more than just arriving at a certain bean color or temperature. Vegetal flavors are very much a roasting mistake that's being passed off as an inherent characteristic of a light roast. Combine that with techniques better suited to brewing (or pulling shots of) darker roasted, and you have a recipe for a dull, astringent, sour cup. That being said, a sour espresso shot is always possible regardless of dark the coffee is, so I'd argue it has a lot to do with a cafe owner's willingness to train themselves and their staff to work with lighter roasted coffee. reply cafebee 5 hours agorootparentWhat are the variables that a roaster can tweak? Temperature, time, and lots of others I assume? Which variable can produce vegetal flavors? I’d never thought much about roasting but now I’m curious! reply lostlogin 1 hour agorootparentI roast my own with a heat gun. 5 mins or under and it’s scorched, 10 mins or over and it’s baked. I want in the middle. Too dark and oily and it’s not great. Too light and I miss the bitterness and it tastes weak. I need to stir it a lot or the roast is uneven. I spray water on it at the end to arrest the roasting. reply an_aparallel 12 hours agorootparentprevi live in Sydney - not far from UNSW :P Campos coffee are phenomenal roaster imo. There's no hype around these light roasts - i would never cold brew them though - i've tried - and it's an incredibly inefficient form of brewing. But light roast itself is a phenomenal thing :) reply panopticon 15 hours agorootparentprevThis is mainly what got me into roasting my own coffee. It was becoming a pain to find high quality dark roasts as all of the boutique roasters turned their efforts to light roasts. reply sq_ 11 hours agorootparentI hadn't thought of that as part of why finding good dark roasts has been hard for me. I've been annoyed at light roasts for a long time because I tend to find them acidic to the point that they're not enjoyable. I appreciate the bitterness and toastiness of a good dark roast, but finding good ones has been few and far between in my experience. reply thrixton 12 hours agorootparentprevHow did you go about roasting your own if you don’t mind me asking. reply panopticon 10 hours agorootparentIt was a journey! My wife got me a roaster for Christmas several years ago, and I had absolutely no idea how to use it. After tons of reading, YouTube, and trial-and-error I eventually got the hang of it. I still use the same roaster she bought me, but I upgraded to a double-walled chamber to make winter roasting more consistent and temperature probes [1] to record the roast process with Artisan [2]. Since collecting data is fun and makes it easier to get consistent results. I don't roast beans for cold brew anymore since I drink way too much and it was becoming a chore, but I still roast ~8oz every two weeks for pour overs. If you're interested there are a lot of great resources online. Sweet Maria's [3] has been a constant go-to for knowledge, equipment, and green coffee beans. And of course, YouTube. 1: https://imgur.com/T0WW90R 2: https://artisan-scope.org/ 3: https://www.sweetmarias.com/ reply mikestew 9 hours agorootparentprevNot OP, but if you just want to experiment and you have a cast iron frying pan, there are instructions out there on how to roast beans with a frying pan. I'll leave the instruction search as an exercise for the reader ('cuz I don't know which ones I used), but basically just keep those beans stirred until they start to pop like popcorn, and you're done. CAUTION: this will make a ton of smoke, as in, if you have a way to do this outside then do it. It's what keeps me from making a habit of it. That said, much like home-brew beer: best beans I've ever had (granted, I'm no snob). Just writing this makes me want to order a bag of unroasted beans off Amazon and give it another whirl. Or go the easy route and just order what sibling comment recommends. :-) reply waldothedog 8 hours agorootparentprevI’d recommend getting a stove top popcorn popper and a range hood vent :) Or if you’re like me and live in an apartment, get a window fan blowing out, and be prepared for your apartment to smell amazing/terrible (depending on your perspective) for a few days reply senderista 8 hours agorootparentprevAgreed, many supposedly good coffee shops in the Seattle area (e.g. Trabant) produce espresso that is way too sour for my taste. (If you want a point of reference, IMO Espresso Vivace does it perfectly.) reply WirelessGigabit 15 hours agorootparentprev> I’m baffled that so many people who sell coffee for a living, think they know a lot about it, and act like coffee snobs, don’t seem to understand what cold brew even is. Anecdotally, this is something I've experienced in the USA more than in Europe. When I ask a question in store a lot of times I get the feeling that the person answering considers themselves an expert and quickly make claims that I know for a fact are false. It's like in the USA saying \"I don't know, but let me get someone who does\" isn't allowed. reply lb1lf 15 hours agorootparent> It's like in the USA saying \"I don't know, but let me get someone who does\" isn't allowed. -Anecdotally, I believe this sentiment is inversely related to worker protections - it appears that the easier it is to fire you, the less likely you are to volunteer that you are not at the top of your game at all times. reply dmix 15 hours agorootparentprevYou're saying Europeans are less smug than Americans? reply Spivak 13 hours agorootparentIt's less about smugness and more that customer-service people are expected to behave like AI and always have an answer whether it's correct or not. I hope at some point we can drop the facade and \"I don't know man, I just run the till. Do you want coffee or not?\" becomes an acceptable response. reply ClumsyPilot 9 hours agorootparentprevIt’s more about behaving like a chatbot or pre-programmed robot. If dealing with situation B is not in the manual, they will tell u situation B can’t happen, even if it happening right now. reply kurthr 18 hours agorootparentprevIt's funny cuz Charbux has no problem pouring hot espresso into cold ice milk with caramel and cream on top. I think it's pretty clearly just \"baristas\" justifying not changing their process, which is fine if a bit lazy and argumentative. Many coffee people are totally interested in finding new ways to do stuff, but they have to have the mental space to do it. reply look_lookatme 14 hours agorootparentStarbucks cold brew is actually not bad as far as cold brews go. It’s definitely not iced coffee. reply toast0 7 hours agorootparentThey do serve both, and they're different. Close enough for me, so I get the iced coffee cause it costs less and I respond to incentives. reply hinkley 16 hours agorootparentprevPouring hot coffee onto plastic… reply LargoLasskhyfv 16 hours agorootparentprevFunny. Exactly that stuff is the only reason I've ever entered a Starbucks. Venti! But TBH one can do that with better coffee, caramel sirup, some crushed ice from the fridge, and even spray cream out of a can for much less money, just not 'on-demand' and anywhere/anytime. Also, too much (spray) cream and caramel in Venti amounts can't be that good for your body. reply waldothedog 8 hours agorootparentWhy not trenta? reply eddd-ddde 5 hours agorootparentprevI had zero idea that's what it was supposed to be. Cold brew reads to me as brew that is now cold. Wouldn't it be a better term cold-brew or something like that? reply cratermoon 17 hours agorootparentprevAs far as I can recall, I've always considered \"cold\" to apply to the brewing, not the drinking. I learned the technique as \"brew extra strong, then add hot water to taste\", and that's how I've been doing it since I bought my first cold-brew maker. Where I live we don't get a lot of hot weather, so drinking cold brew cold is strictly a high summer activity for me. reply taude 15 hours agorootparentme, too. the Toddy brewer 30 years ago, even spoke of this. It was a way to premake your coffee concentrate, and then mix with water and microwave to heat up.... reply lamename 15 hours agorootparentThis was the first cold-brewing device I ever encountered....as a guest at someone else's house. Didn't know it had a name. Thanks! reply cratermoon 14 hours agorootparentprevThat's exactly the one I started with, and used for years until the plastic got brittle and cracked. reply ada1981 18 hours agorootparentprevThe more complex, expensive and counter intuitive your morning coffee, the better! 90% of the experience is looking cool! reply lytfyre 15 hours agorootparentLook, my coffee routine is _perfectly reasonable_. Fractional gram dosing, multiple pours at different temperatures, timed switch from immersion to percolation, and benchmarking different filter papers has a _measurable impact_ on my coffee. And I have the data and refractometry measurement data to show it. ... Admittedly the refractometer was expensive, and incorporating it into the routine is complex and not very intuitive. I can also assure you that I don't look cool while doing it. reply ok123456 17 hours agorootparentprevYou can buy cold brew coffee at Costco in bulk in cans. Just throw some in the fridge. reply klyrs 16 hours agorootparentI was excited by this in theory, but it's some of the worst cold brew I've had. For shame, Costco is usually pretty good for selecting quality. Then again, my favorite preparation of cold brew is a shot of concentrate straight from the toddy. reply look_lookatme 14 hours agorootparentI agree. Bought a flat of it a couple of months ago and haven’t drank more than half of them. reply kibibyte 7 hours agorootparentprevYou poke fun at it, but great irony is that a lot of knowledgeable coffee people started poking at all the techniques recently and we've learned that almost everything towards the showy complicated side of it are completely not worth thinking about or will have worse results. Osmotic pouring looks neat but will underextract anything that isn't dark. 4:6 is still questionable over most simpler 1-2 pour techniques. That showy thing of raising the kettle up and down a lot also will underextract. reply Y_Y 17 hours agorootparentprevPretension is nine tenths of the law reply opwieurposiu 18 hours agoparentprevAt my friends coffee shop they make cold brew with an elaborate laboratory glassware setup that drips ice-water through a filter. Looks pretty neat and sciencey. They got 4 of these devices running all the time in the back room. I was telling him all the other shops in town just fake it, he should put one of the devices out front so people can see his cold-brew is for real. reply this_steve_j 15 hours agorootparentPerhaps the Yama cold brew tower CDM25 is the device you saw. It is used at many fine establishments including my kitchen, and I also have the smaller CDM8. The numbers 8 and 25 correspond to the number of cups of diluted 1:1 cold brew that it produces in a single cycle (6-12 hours). https://yama-glass.com/collections/cold-brew-towers reply lytfyre 10 hours agorootparentIf any HNers are looking for a cheap entry point into the style without spending $250+ on a beautiful bunch of laboratory glass ware, there's the puckpuck[1], which sits on top of an aeropress and turns it into functionally the same device as the Yama towers - slow controlled valve dripping cold water at a controlled rate onto a bed of coffee and a paper filter over the course of hours. I've found it a nice way to play with the style without investing the money and space. Still want one of those towers if I ever see them cheap though. [1] https://puckpuck.me/ - no connection, just a customer. reply AdamJacobMuller 7 hours agorootparentI have a toddy, produces excellent quality cold brew. https://www.amazon.com/Toddy-THM-Cold-Brew-System/dp/B0006H0... reply giancarlostoro 18 hours agorootparentprevI would literally go to a shop doing this, like as often as possible. As a coffee lover, this is peak coffee shop. He could even double down and make Breaking Bad references around the shop, since thats what this makes me think of. I'm guessing it's not in Florida, or I would ask you for the address. He should at least get a window into that backroom installed or something to that effect. reply samcheng 18 hours agorootparentprevI think this is called \"Kyoto-style slow drip\" coffee. I agree with you that the contraption should be in plain view of the customers! I remember being really interested in a cup of Kyoto-style one day, only to be told to make a reservation and come back tomorrow... it was worth the wait. reply Spivak 13 hours agorootparentIt seems to be a niche taste among my friends, yes it's sour and bitter that's why I like it! reply bobthepanda 18 hours agorootparentprevTo be fair all you need to actually make cold brew is a pitcher and a cheesecloth or some other filter. The absence of an apparatus does not make it fake. reply TrueSlacker0 1 hour agorootparent100% agree. This is exactly what we do at our brewery and it comes out great. A simple fermenter with a pour spigot works wonderfully (we use this one because we already had it [1]). Soak the ground beans for 12-24 hours in a mesh bag in the cooler, remove the bag and drink on it all week.. or till it's gone. [1] https://www.ssbrewtech.com/products/brewbucket reply bananskalhalk 18 hours agorootparentprevChilling warm brewed coffee is definitely faking it. And I presume all places selling \"cold brew\" is faking it so this guy is losing money by not showing off doing real cold brew with his apparatus. reply bobthepanda 18 hours agorootparentI’m not talking about chilling warm brewed coffee. All you need to make cold brew coffee is soak coffee grounds in cold water. Whether that is in a mason jar or through a thousand dollar complex laboratory setup is entirely an aesthetic choice. https://www.loveandlemons.com/cold-brew-coffee/ reply klyrs 15 hours agorootparentMy favorite route to camp coffee is running this GSI filter backwards: https://gsioutdoors.com/products/h2jo-filter Put a week's worth of grounds in the bottle, screw on the filter, pour in some cold water, steep for 24h, and transfer to another bottle. If somebody wants \"drip\" strength they can cut it with water, hot or cold. reply kjkjadksj 12 hours agorootparentprevThe way I've seen it done is with one of those massive plastic commercial kitchen lidded containers, and a softball sized teabag of coffee sold by the restaurant supply company specifically for cold brewing like this. Then they put it in the walk in for a while to steep and sell it after a certain number of hours. reply janalsncm 17 hours agorootparentprevIt’s a market for lemons at this point. Unless you can see an expensive apparatus or observe them soaking the coffee, there’s no way to know if it’s correct, and as a customer it means it’s risky to buy if you care about the difference between refrigerated hot coffee and cold brew. reply deadlydose 16 hours agorootparentRisky to buy? It's not real estate it's a cup of coffee. And if you're worried about people faking it, just buy a $2 mason jar and make some in the fridge while you sleep. I don't get the fascination with paying exorbitant prices and constantly complaining when it's next to zero effort to make it at home, cold or hot. And the best part is you get to choose where your beans come from, you don't have to worry about the political slant du jour of the coffee shop, and you can do it all for a fraction of the price even when using the most expensive beans. reply metabagel 12 hours agorootparent> you don't have to worry about the political slant du jour of the coffee shop I can't say this has ever been an issue for me. Generally, they just want to sell me some coffee. reply janalsncm 12 hours agorootparentprevYes, risky to buy. In the same way a slot machine has an expected value of 80-99% payout it’s still a bad use of money even if you only put in $5. If you object to the word “risky” I used it in the sense of “uncertain you will get the value you expected”. Perhaps there’s a better word. reply paulcole 16 minutes agorootparent> In the same way a slot machine has an expected value of 80-99% payout it’s still a bad use of money even if you only put in $5. Why is this a bad use of money? When I go to a restaurant and give them $20 for dinner, it’s not like I’m getting $20 worth of ingredients. It sounds like you just don’t like gambling? reply rhaps0dy 15 hours agorootparentprev> It’s a market for lemons at this point. Unless you can see an expensive apparatus or observe them soaking the coffee, there’s no way to know if it’s correct Presumably the taste should tell you whether it's correct. Otherwise why care if they fake it? reply kimixa 5 hours agorootparentIf they \"fake it\" and it tastes better... then why are you going out of your way chasing some marketing term that tastes worse to you? reply klyrs 15 hours agorootparentprevI prefer to know what I'm buying before paying for it. reply majormajor 15 hours agorootparentprevUnless I'm a visiting tourist I'm likely to go back to a good coffee shop many times. Being surprised my cold brew isn't cold brew - both the caffeine content and taste are tells IMO - for one visit isn't life or death here. I just don't get it again. reply enobrev 2 hours agorootparentprevFrench press with cool water over 24 hours is simple and works well. reply shagie 6 hours agorootparentprevOne that I'm aware of - Black Blood of the Earth - https://www.funraniumlabs.com/the-black-blood-of-the-earth/ > Fast forward 14 years. An acquaintance working and living in Japan went on holiday and discovered a bar with this exceptionally beautiful rig for the preparation of Viennese Triple Cold Extraction Coffee. Upon sampling this, he felt that, and I quote, “I could see colors that weren’t in the visible spectrum, and could vibrate through walls.” I looked at this I said to myself, “Hey, you’ve got enough virgin laboratory glassware lying around the house that you could probably build something like that.” Probably several somethings, actually, but that’s beside the point. It has a picture of the original glassware in Japan and his first iteration in the kitchen. A sibling comment links to Yama glass ... which is out of Taiwan. The similarities might not be coincidence. reply tedmiston 15 hours agorootparentprev> At my friends coffee shop they make cold brew with an elaborate laboratory glassware setup that drips ice-water through a filter. Looks pretty neat and sciencey. Likely the Yama cold brew tower [1]. [1]: https://prima-coffee.com/equipment/yama/yamcdm25sbk-yama-pp reply dahart 18 hours agorootparentprevThat is awesome! I would absolutely love it if more cafes did this. Let us know where it is so we can go patronize. reply elevatedastalt 15 hours agoparentprevThe emperor really has no clothes when it comes to food fads. And pricing is a completely orthogonal and obtuse concept too. Cold brew is putatively low effort and low cost. Just let coffee grounds soak in water overnight and you have cold brew. But it's often charged more than regular coffee or espresso-based drinks, which a) use more expensive equipment b) need more skilled operation c) more material [milk etc] reply askvictor 12 hours agorootparentCafes often charge more for niche items, mainly because they can. There's also the workflow of the barista to consider - leaving the espresso machine to get ice from the freezer disrupts the flow of constantly pumping out lattes. reply fwip 14 hours agorootparentprevSpeculation: Cold brew takes more refrigerator space, which is relatively inflexible. Since it brews overnight, you have to put aside enough fridge space for all the cold brew you expect to sell that day. Contrast with regular coffee, which you make largely on-demand, with only the coffee beans to store overnight, on a shelf. So raising the price might be the sensible thing to do, to discourage purchases and/or pay for the extra refrigerator space. reply elevatedastalt 14 hours agorootparentThat's a good point. From what I understand, cold brew can be made at room temperature too. So it will need storage but not cold storage. But it's possible I don't have the full picture. reply Bluecobra 13 hours agorootparentI have found no difference in just using cold water and having it sit on my counter overnight vs. keeping in the fridge. It is nice to still refrigerate it to use less ice or no ice though. In a retail setting, I would think customers would demand a cold product (I would). The Starbucks near me serves Nitro Cold Brew and for some reason it's always warm/room temp, and they purposely do not put any ice in it. Other locations seem to not have this issue so maybe it's broken equipment or training. reply stephenhuey 13 hours agorootparentCold brew coffee is by definition not brewed with hot water the way most coffee is (in other words, room temperature is within the range of \"cold\" when talking about brewing with \"cold\" water). So, it's not necessary for it to be served cold to enjoy the benefits of the cold brew method--sometimes I like it chilled, but I also enjoy the taste at room temperature or warmer. reply elevatedastalt 12 hours agorootparentprevCold brew is so much more caffeinated that I've seen most shops add tons of ice to create enough drinkable volume without giving you enough caffeine to knock a horse down. reply tracker1 14 hours agorootparentprevAs to C, that applies to cold brew just as well as hot brewed coffee. reply elevatedastalt 13 hours agorootparentPossibly, but I've seen cold brew served mostly just by itself or just some syrup / cream, as opposed to cappuccinos / lattes which need much more milk. reply cout 18 hours agoparentprevAs I understand, the serving temperature of coffee does have an effect on perceived acidity (which is NOT the same as pH), though I don't understand the science behind it. Here is one paper that claims it is due to release of volatiles at higher temperatures: https://www.sciencedirect.com/science/article/abs/pii/S03088... If that's correct, then warming the coffee again to that temperature would again speed up the release of volatile compounds, though what effect that might have on flavor is anyone's guess. reply dahart 18 hours agorootparentI could buy it, that it might have some perceived effect that’s measurable. I have tasted many a warmed up cold brew, and it tastes like coffee to me. The ones that are actually cold brewed are milder, and the hot brewed fakes are noticeably sharper, and I feel it later in my throat… The thing for me is that the actual acidity of coffee has started causing some inflammation. The reason I’m seeking cold brew is my doctor recommended it. I’m less worried about the perception of acidity and more worried about issues caused by too low pH. reply hinkley 15 hours agorootparentprevCold brew isn’t about pH, it’s about bitters. The oils aren’t (just) acidic, they’re bitter, and that’s what you don’t get when you make real cold brew instead of failing it. Acetic acid is sour. Alkaloids are bitter. reply rrrrrrrrrrrryan 15 hours agorootparentprevAs a complete layman, when I tried tossing cold brew into the microwave it ended up tasting pretty gross. Like, jarringly so. I won't pretend to know the science behind it, or perhaps I warmed it differently than what the parent poster does, but I definitely sympathize with the barista's hesitation in his story. reply ch4s3 13 hours agorootparentYeah, there are still a lot of dissolved coffee solids in cold brew that get further extracted if heated. There are also a lot of volatile compounds that break down when heated. The idea that you can just reheat coffee, even cold brew and have it taste the same is just ridiculous. reply nkozyra 17 hours agorootparentprevAh, just posted that I detect a change in flavor after I heat cold brew (but didn't know a mechanism behind it)! reply YetAnotherNick 17 hours agorootparentprevAlso, in my experience heating the coffee in microwave increase perceived acidity even more. Which kind of make sense with this explaination as there could be pockets of superheated water when it is microwaved. reply eternauta3k 14 hours agorootparentCould you try giving it a good stir right before putting it in the microwave? reply klausa 17 hours agorootparentprevIsn’t that a pretty generic and well known (and not coffee-specific) effect, that flavor is pretty temperature dependent? Warm Coke is disgusting; freezing cold Diet Coke is the nectar of gods. reply parpfish 16 hours agorootparentroom temp soda is my favorite. there's so much more flavor when it's not cold. however HOT cola (which can happen if you leave it in a car) is physically painful because something weird happens to the bubbles reply D13Fd 16 hours agoparentprevI drink only cold brew normally, and I've noticed this as well. It's 50/50 whether you will get actually cold brew at any given coffee shop or just iced hot-brew coffee, which tastes different and has much less caffeine. Cold brew needs a new name or it will likely fade away over time. reply fuzzzerd 14 hours agorootparentThere's nothing wrong with the name cold brew. It describes the difference from \"standard\" coffee. It is the charlatans selling warm brew over ice that devalue it. As someone that greatly prefers coffee brewed cold and served on ice, I hope it doesn't fade away, because without it I have a lot less reason to get coffee out as opposed to at home. reply dahart 12 hours agorootparentThere shouldn’t be a problem with the name, but my experience tells me that people don’t comprehend it the majority of the time, and assume incorrectly that cold brewed must be served cold. This mentality is even here in this thread in a few places. It does seem like some other word than “cold” might help avoid leading people into an erroneous conclusion. reply D13Fd 9 hours agorootparentprevYeah I greatly enjoy cold brew too. But it's too easy for people working these stands to just take any old coffee that's cold (or even just \"not hot) and call it \"cold brew.\" Far too often people just don't know the difference and serve plain iced coffee. If it were called something else, maybe there wouldn't be this level of confusion, or people couldn't get away with the cheap/lazy way of just serving yesterday's leftover hot coffee as \"cold brew.\" reply mmanfrin 12 hours agoparentprev> I was laughed at in one hipster joint for asking for a steamed or warmed cold-brew Not to discount the rest of your comment but it's a mild irony here for you to add the 'hipster' qualifier to a coffee place when you ask for steamed cold brew reply dahart 12 hours agorootparentWhy’s that? What’s ironic? Do you feel like that’s a fair gotcha? Do you know why I asked for a heated cold brew? I’m asking for cold brew because my esophagus gets inflamed with higher acidity coffee. I used hipster not as pejorative, but to indicate this is a cafe that claims to, and should, know the difference between cold brewed and hot brewed coffee. In fact, I’m certain the owner does know the difference and the snarky barista who refused to help me that day does not. First she said, “Uh, we call that a drip.” When I offered the acidity reason and that it’s my doctor’s recommendation, she replied with “it’s not on the menu”. Cold brew was on the menu. reply omnimus 1 hour agorootparentSounds like you are the snarky hipster. The barista didnt know this process and didnt have it on menu. In their eyes its just strange request asking for trouble. I think of myself as very knowledgable barista and this is first time i am hearing this. I checked usual resources like James Hoffman and nobody is even mentioning heated cold brew. So its very niche and sounds more like something from camping. I believe you that health wise its for some people better. But cold brew overall is not that popular with coffee fans because the coffee is underexttacted - it tastes ok but you will get more flavours using hot water. Its kinda waste to use high quality coffee on cold brew. But thats where i personally like cold brew - with lower quality coffee it can taste better than hot extraction. reply vallode 19 hours agoparentprevThe paper[1] seems to imply agitation is exactly what this method is promoting: \"Furthermore, acoustic streaming induced greater mixing and enhanced mass transfer during brewing.\". I assume the 100W of ultrasonic energy would be pretty hard to reproduce by just shaking your cold brew container though! [1]: https://www.sciencedirect.com/science/article/pii/S135041772... reply dahart 18 hours agorootparentVery interesting, and I missed the 100W bit, thanks! Yeah that would be really hard to do by hand for 2 minutes. Maybe Guinness records needs to see who can shake their cold brew hardest/longest. So this begs further questions for me, like can I shake with 10W for 30 minutes, or 10 minutes, or…? Does the frequency matter? Can we use one of those chem lab agitator machines to cold brew? reply kurthr 18 hours agorootparentI find it interesting that 100W for ~120 seconds is ~0.3kcal which for a 100ml cup is ~3C. They are right at the limit of power to flow rate. Much faster flow and presumably the cavitation wouldn't \"brew\" enough, while much slower and it would warm up the coffee noticeably. I'm doubtful the frequency matters much if the cavitation is what is causing the mixing since those are just bubbles emerging and popping, but the efficiency of coupling from ultrasonic wand to liquid could change a lot. Since you could presumably put 2 of these in parallel and have 2x100ml cups in 2min with 200W without changing the recipe (or 1 cup in half the time), this seems pretty scalable with increased cost and area. Unagitated cold brew is in the 10hour region, but with agitation/pump through it seems like you can do 8 cups in 20min which is almost as fast as the cavitation method. I suspect the grind size starts having really big effects here. https://instantpot.com/products/instant-cold-brewer reply aaarrm 17 hours agorootparentprevI think the size of the vibrations is important here. The paper mentions acoustic cavitation, which I believe would only really occur at small frequencies like the ones stated in the paper, not large shakes that you or I would do. reply petre 17 hours agorootparentprevYes, it's the frequency and also the amplitude that makes it faster. One could use a lab agitator but it would still be too slow. I think if you pour the water into an ultrasonic cleaner along with the coffee and filter the mix you might get the same result. reply m463 15 hours agoparentprevI've been doing cold brew for a while now. There seems to be two ways to cold brew coffee. The more traditional method (like in the toddy system) uses paper filters, and the newer method uses reusable metal filters. They are slightly different. the paper filters remove the oil, while the metal filters let it through. I suspect this might have flavor/aroma effects. I also read because of the oils, the metal filter method is higher on cholesterol (if that makes any difference to you) I've also seem drip cold brewers at some coffee shops that probably let the oils through. There seems to be a container of ice at the top, it melts and drips on a glass container of coffee and that drains through a circular glass thing (looks like a slinky) into an output carafe. reply throwaway2037 13 minutes agorootparentI did a double take here: > higher on cholesterol As I understand, cholesterol only occurs in animal products. That is why vegan diets are cholesterol-free. Zero trolling here: I assume that coffee beans are animal-free. Google tells me: > Though brewed coffee does not contain actual cholesterol, it does have two natural oils that contain chemical compounds -- cafestol and kahweol -- which can raise cholesterol levels. And studies have shown that older coffee drinkers have higher levels of cholesterol. reply tracker1 14 hours agoparentprevI like, and use the term \"iced cold brew\" for serving cold... not sure about serving cold without ice... In that my first introduction to it, the person would be taking it from a container in the fridge, adding some water and microwaving it to heat it up. After trying it, I liked it a lot... I always drink coffee over ice (usually with a lot of cream and sweetener), as I'm not so much a coffee fan as a caffeine consumer a few times a month. I like the more mellow taste of cold brew. reply dev-tacular 17 hours agoparentprevDoes cold brew served hot taste good? I have never really considered asking for it hot before simply because I thought it would just be like regular coffee. But, I guess if regular coffee tastes different cold and hot, cold brew should too. reply HankB99 16 hours agorootparentGood is subjective. I've trialed cold brew at home and have been serving it warmed. I prefer my coffee hot. I thought it tasted fine, different but neither better nor worse than hot brew (drip in my case.) Since it's DIY I have no one to argue with about how to serve. I have stopped making cold brew for the most part because it seems to require more coffee beans than hot brew. Maybe I'm doing it wrong. I don't have a \"cold brewer\" and just add water to grounds in a glass jar with lid and shake when I walk by before filtering it the next day. Neither have I compared the cost of electricity for drip vs. the extra beans for cold so I don't really know which is more cost effective. reply rpdillon 17 hours agorootparentprevCold brew is much less acidic, and I find warmed cold brew to be exceptionally smooth compared with hot brew. I got my recipe from a NYT interview with the CEO (I think?) of Blue Bottle, though I've since lost the link. This is my copy: https://rpdillon.net/recipes/new-orleans-cold-brew-coffee.ht... reply dahart 17 hours agorootparentprevTastes good to me. It is like regular coffee, just lower acidity, which is what I need. I want it to taste the same as regular coffee! ;) To be fair, it usually tastes milder than how brewed coffee, and this is one of the things people like about cold brew. reply bunderbunder 17 hours agorootparentprevYes. The major reason for cold brewing coffee (or tea!) over simply cooling and icing hot brew is that you extract a different mix of compounds from the bean (or leaf) due to different chemicals having different levels of solubility at different temperatures. Serving temperature affects flavor, too, of course. Darn near everything does. See, for example: https://www.mdpi.com/2304-8158/9/7/902 reply marcoalopez 17 hours agorootparentThis is only partly true. Because low-temperature extraction is much less efficient, it requires a much longer immersion/exposure time than hot extraction at ambient or slightly higher pressure. One of the effects of this type of cold extraction is the oxidation of the coffee, which is much greater with this method, giving the coffee an oxidised taste which, although not bad in itself (nothing is set in stone about personal taste), is not to everyone's liking. For my taste, I prefer to make it hot (with a higher coffee/water ratio than usual) and cool it down by diluting it with water or ice. reply bunderbunder 14 hours agorootparentDefinitely. Though the oxidation seems different? I don't find cold brew to taste \"stale\" the way hot brewed coffee that's been stored overnight in the refrigerator does. I have a bunch of kegging equipment that I don't use anymore because I've lost my taste for beer, and I keep wanting to see what happens if I use it to make cold brewed coffee under a bed of CO2. Also, y'know, coffee on tap at home. reply eternauta3k 14 hours agorootparentWon't the CO2 make the water acidic? reply bethekind 17 hours agoparentprevUsing low frequency sound will agitate the solute and solvent, speeding up the natural cold brew process. Using ultrasonic will do it even faster, but since ultrasonic underwater induces cavitation bubbles, it's much more violent. Ultrasonic has been used for dermal infusion quite successfully, but it is....painful, as bubbles are exploding against your skin. I would presume the same to be for the sonic coffee. Agitation speeds up the process, until cavitation occurs, where it becomes more violent reply csmpltn 18 hours agoparentprevThis is an extreme form of coffee hipsterism. “Cold brew, but steamed please”. Brother, this is starting to get ridiculous. reply windexh8er 16 hours agorootparentFor people who are accustomed to cold brew (we always have cold brew in the fridge and grounds steeping) there's a very large difference in flavor and profile using the same beans. This also translates when it's heated, although I will say I don't do that often because I enjoy it cold more than hot. I'd disagree though that it's \"extreme\". There are local chains in my area (Midwest - US) that offer a variety of hot cold brew drinks that are quite popular offerings. I was pleasantly surprised when I ran across this more than a year ago. But I still do run into a number of coffee shops where baristas fail to understand the difference between cold brew and an iced coffee. There's really no comparison when you're explicitly looking for cold brew. It's also often hard to find available in the winter months in my region. Not sure why, but to me that's akin to pausing ice cream sales because there's snow on the ground. Just because it's a cold drink doesn't mean I don't drink it during cold weather. reply dahart 17 hours agorootparentprevMy doctor recommended cold brew for lower acidity, because hot brew is bothering me. Cold brew is just a brewing process and has nothing to do with serving temperature. Cold brew has never meant served cold. So what, exactly, is ridiculous about warming up cold brewed coffee? I like my coffee hot, like a lot of people, and I’d like it if I can keep drinking coffee and don’t have to stop due to the acidity. reply sushid 13 hours agorootparentI'm a coffee snob and I can respect your preferences but have you considered non-coffee alternatives? Japanese and Chinese tea culture can be fun to delve into and tea would probably be easier on your stomach. reply dahart 13 hours agorootparentI definitely have considered alternatives, and do drink tea, more now than before, in part because of my negative experience experiences trying to get hot coffee that was cold brewed. Finding the fake cold brew many places really scares me off cold brew more than baristas who don’t want to heat the coffee. Single shot americano is another alternative with a bit lower acidity, I believe, and even with that I often get mild push-back from baristas… “you sure?? We pull two shots anyway…” The main problem is that I love the flavor of coffee, and I feel especially jealous on a Saturday morning when I can smell it but can’t drink it. Secondary problem is adjusting to a consistent caffeine level without getting headaches. reply ackfoobar 17 hours agorootparentprevIt might be obvious but I have to ask, is that a stomach problem? reply aaarrm 17 hours agorootparentNot the person you are asking, but I do have GI issues due to the acidity with coffee. It can cause reflux, ulcers, and more for people if they are sensitive to the added acidity. reply dahart 17 hours agorootparentprevNot for me, but it is for some people. My current issue is more esophageal, my throat gets inflamed with too much acid then it feels like I have food stuck and can’t swallow it. Other people have stomach issues, ulcers, etc. but still want to enjoy coffee, so I think there’s plenty of reasons for cold brew served hot to exist. reply everettp 3 hours agorootparentprevWould you say that a discussion about GPU densities was an \"extreme form of processing hipsterism\"? Unless your contention is that there is no observable difference between the types of drinks described by OP, this comment is an embarrassing ad hominem and has no place here reply lytfyre 10 hours agorootparentprevWouldn't knock it till you try it! The drink kinda had it's moment a couple years ago - \"Aerocanos\" or \"Steamed Iced Americanos\" are the names I've heard for the drink. Often made with the kind of post-brew chilled cold brew that OP was railing about. Cold coffee, steamed to frothy, then either pour over ice to re-chill or serve warm. Wouldn't try to randomly talk a barista into making one, but if you see them on the menu at a shop or have an espresso machine, they're pretty neat. Video on them: https://www.youtube.com/watch?v=tD_4hOg_SWU reply surement 15 hours agorootparentprev> extreme form of coffee hipsterism I know a lot of coffee nerds and cold brew is disdained for not extracting enough flavors. This is just someone who never adapted to the world hoping the world will adapt to them. reply nkrisc 18 hours agorootparentprevIt’s no different than drinking an iced coffee drink that was made from coffee brewed traditionally. reply dahart 17 hours agorootparentThat’d be true if putting ice in coffee reduced its acidity. Heating cold brew brings a lower acidity liquid up to a nice warm good morning temperature. reply kjkjadksj 12 hours agoparentprevI wonder if you can deal with the acidity issue in a quick and dirty manner by just throwing in some baking soda into your grinds and brewing hot? reply dahart 7 hours agorootparentOkay this is a totally wild and hilarious coincidence, but I came home tonight and told my wife about this thread. Turns out that, today of all days, without knowing I was ranting about cold brew on HN, she did a careful home experiment with her coffee using baking soda, so I can report on what she found. She has suffered me complaining about fake cold brew and rude baristas before, but it’s been months since we discussed it. We do have some pH test strips sitting around from my investigations into different brewing methods, and she used 4 of them doing this experiment twice. For each experiment, she made 2 cups of coffee, using 1 scoop of grounds (0.25 oz) to make a pour-over with 8 oz water heated to 200 degrees Fahrenheit for each cup. In one of them she added 1/16th teaspoon (which is a tiny pinch) of baking soda to the grounds before pouring the water over them. In the other cup, it was only grounds with no baking soda. She said she could see the water foaming in the cup with baking soda. She reported that there was no noticeable negative flavors at all, no hint of baking soda taste, the coffee made with baking soda was as good as the control, perhaps slightly better because it was less acidic. For the pH measurement, she measured a pH of 6 for the normal coffee and a pH of 8 for the baking soda added cup — it actually made the coffee slightly alkaline! PH of 6 sounds like a pretty weak coffee, I was usually getting a pH of 5 IIRC. PH strips are a pretty blunt measure and don’t give you fractional pH values, but she showed me the strips and I can confirm her conclusion. The 2nd experiment was the same setup, and the result was identical. When I read your comment earlier, I thought it was a good question, but assumed baking soda would change the flavor negatively. I’m surprised to hear that it totally works, so I think you have a good idea. I might even consider making this my routine if it works that well. Now I’m curious if you can sprinkle in a tiny pinch of baking soda into an already brewed coffee and reduce acidity without damaging flavor… reply bobthepanda 4 hours agorootparentI suppose this makes sense? Baking soda’s main flavor profile is bitter, but coffee is already bitter so I suppose it isn’t going to make things worse. (Compare this to its use in baking, where you have to be careful not to use too much so the bitterness doesn’t ruin the baked good.) reply atoav 3 hours agoparentprevAs someone who regularly makes cold brew during hot summers, the tale is that the acidity is lower — or at least of a different kind. Hot brewed coffee starts to taste bad after a few hours if you let it go cold, cold brewed coffee tastes differently from the start, but won't develop that bad flavour even after a week in the fridge. The key for a good cold brew is however that the bean/roast is of very good quality. And it is quite simple to make. The way I do it: 1. Grind coffee coarsly and put it in a glas jar that can be closed. The amount of coffee can be adjusted quite freely, but I'd go with one fourth/fifth of the volume of the jar. More bean = more concentrated coffee. 2. Add cold water and stir 3. Put in the fridge and stir at least once in the morning, once in the evening. 4. After ca. 24 hours you can run the whole thing through a coffee filter to extract the coffee. It is also possible to reuse the coffee-sludge once if you add some fresh beans. That is not too complicated and worth a try. Please avoid pre-grind cheap coffee for this, It will taste like bullshit. reply sharpshadow 19 hours agoparentprevI think they mentioned that cold brew has lower acidity than hot brew. Yes it’s agitation with the mentioned frequency. Technically it should move the grounded coffee particles back and worth and so extracting the components. reply dahart 19 hours agorootparentYes, they mentioned that cold brew has a reputation for being lower acidity than hot brew, but they didn’t measure the pH of the coffee they were testing using a hot brew process. They reported the pH of the normal cold brew and sonicated cold brew as both about 5.1. For frequency, does it matter if it’s high or low frequency? I’m wondering if I can shake my cold brew for 3 minutes and get close to the same effect. reply sharpshadow 14 hours agorootparentYes hot brew coffee has more acidity tan cold brew. I like hand filter coffee and I use water around 90 degree Celsius. The higher I go with to boiling water the more it washes out bitter and acidic components. reply jmilloy 17 hours agoparentprevEverything you say makes sense, with the exception of your expectation not to pay extra for an unusual order (for that cafe). Consider if I asked for my salad to be roasted, and balked at a surcharge on the grounds that they also have roasted brussel sprouts! I don't think it's up to you to decide what orders fit into their flow and which cost extra. I'm glad you found the places that will make the coffee the you like. reply dahart 17 hours agorootparentIt’s not an unusual order, hot coffee is sold millions of times per day, and steaming comes with many coffee drinks, it’s par for the course with espresso drinks. Roasting a salad is weird, hot coffee is not, so your example is straw man. The “cold” in cold brew is not referring to serving temperature, that’s your own misunderstanding, so I find the suggestion that hot cold brew is weird to be pretty funny. As has been said many times in this thread, nobody balks at the idea of cold hot brew, nor do they charge extra. reply artimaeis 15 hours agorootparentFormer barista, it _is_ an unusual order from the perspective behind the bar. Unusual in that I never have heard of someone ordering a warmed cold brew. I'm not certain how I'd warm it up. I suppose it could be poured into a clean frothing pitcher and steamed directly, I'd somewhat worry that might dilute the flavor of the coffee. I don't reckon someone's going to want me to microwave their cold brew, but it certainly seems like it'd be the quickest way to do it. Most cafe workers get into flows of orders. Lattes means you always pump syrups into the cup, start the pour, then steam milk. Cold coffees usually means you ready the cup (syrups, milk) and pour the cold brewed coffee onto it. Warming the cold brew totally breaks that flow, and is why it would be unexpected. Hope that perspective helps. I do want to try it now though! I could imagine it being pretty good. reply lytfyre 10 hours agorootparentCoffee nerd, I like to play with this stuff. Steaming cold brew (flash or regular) will give a very smooth frothy texture - almost nitro like foam. You can serve them over ice to get them back to cold, or serve warm. Tasty if you use good coffee, and pretty unique honestly - \"steamed iced americano\" or \"aerocano\" are the two names I've heard if you want other people's reports on them. You're right that it's very different from the experience of microwaved cold brew, and a customers response can be all over the place depending on what they're expecting. reply dahart 15 hours agorootparentprevI appreciate that, it does help, thank you. Indeed I found a few cafes that were used to it, and quite a few that weren’t. I think you’re totally right that this trips up some people’s flow especially when they’re not used to it. I’m okay with accepting it actually is unusual for some and just instead being the person complaining that it should be usual or expected even if it isn’t always. FWIW I have tried it microwaved, and it’s fine, but never in a cafe - baristas have always steamed it until warm or hot. That also works for me. reply shawabawa3 16 hours agorootparentprevBut hot cold brew is weird, as you've just been complaining about people not doing it or finding it weird If hot cold brew was common you wouldn't pay extra for it reply dahart 15 hours agorootparentThe biggest sticking point is not the heating of the coffee at all, it’s the widespread misunderstanding of the what the word cold means in the term “cold brew”. It is incorrectly assumed that brewed relatively cold (room temperature) means served ice cold. Somehow a lot of people can’t understand the verb brew has nothing to do with serving temperature. Does that make sense to you? reply papertokyo 10 hours agorootparentAlthough you are entirely correct in a technical sense, and it's common to serve hot brewed coffee cold, the opposite is so rarely desired that it will be considered 'weird' (in the sense of 'unusual') 9 out of 10 times. Every extra process involved in making a coffee is going to add complexity and time to the workflow, which many cafes will elect to charge extra for. There is some consternation in Australia about paying more for iced coffees compared to hot ones too: https://www.broadsheet.com.au/national/food-and-drink/articl... reply dahart 7 hours agorootparentI know, I know. You’re right. But… even if it is weird, it is so easy! It’s not really the default assumption that’s frustrating, it’s when I get push-back for a request that is normal and default for other espresso drinks, something trivially doable, something every barista does dozens and dozens of times a day. reply nkozyra 17 hours agoparentprevI find that heating cold brew reintroduces some of the flavor of hot brewed coffee, though I've never been able to pinpoint a mechanism for it. reply bunderbunder 17 hours agorootparentSpeculation: A lot of it is that chemicals that contribute to flavor are more volatile at higher temperatures. The same things happens with beer. Many traditional and craft beer styles are intended to be served at a higher temperature than what it will be right out of the refrigerator, and you really do get more (and, to my palate, better) flavor out of them if you let the bottle warm up on the counter for a while before you open it. reply romafirst3 12 hours agoparentprevAll coffees tend to be in the same ph range regardless of brewing technique. reply petre 17 hours agoparentprevThat reminds me of the Soul Kitchen movie when a client of a fancy restaurant asked for a hot gazpacio. The waiter escalated to the chef who calmly explained to the client what a gazpacio is. After the client insisted, shouting at him, the chef refused driving his knife into the client's table. https://youtu.be/rQ61MfRBSQg reply SamBam 16 hours agorootparent> We had gazpacho soup for starters... I didn't know that gazpacho soup was meant to be served cold. I called over the chef and told him to take it away and bring it back hot! So he did... the looks on their faces still haunt me today! I thought they were laughing at the chef, when all the time they were laughing at me as I ate my piping hot gazpacho soup! I never ate at the Captain's table again. That was the end of my career. – Rimmer, Red Dwarf https://www.youtube.com/watch?v=3ZGJHDegPcU reply aspenmayer 13 hours agorootparentRed Dwarf is one of the most unhinged shows I’ve seen, in the best way. Nice reference, as I was also thinking of hot gazpacho soup, but had forgotten this scene until you reminded me of it. reply the_optimist 18 hours agoparentprev“Old Brew” can be more valuable. You must check the vintage and terrioir. reply draw_down 18 hours agoparentprevI agree people can be really annoying about heating up cold brew. Iced espresso no problem, heated cold brew = mind blown. reply 0xbadcafebee 14 hours agoparentprevFlavor compound release is affected by method, temperature, and wat. The method (which includes temperature, but is not restricted to it) extracts certain compounds. The release is affected by temperature and the amount and type of compounds in solution. It's entirely possible that warming up a cold-brewed coffee could create off-flavors. If you let coffee and water sit for long enough, all the compounds that can be extracted, will be extracted. But those compounds will also start to break down over time. Heating and oxidation accelerates the breakdown. When you go to taste coffee, the compounds in the coffee either expand or contract depending on the temperature, and solubility. So the temperature you drink it at, along with water concentration, determines the flavors. (Flavor is actually aroma, taste only has 5 basic senses) reply peteforde 19 hours agoprevFunny thing is that I'm currently drinking cold brew that I prepared on my acoustic cavitation brewer. It's called Osma Pro. The company that made it sadly did not survive, and they took a lot of heat (no pun initially intended) for the price point and various complaints about how it worked. https://www.engadget.com/osma-pro-cold-brew-coffee-machine-r... Luckily, mine works great and I like it a lot. I use it every morning. Takeaway point: maybe Google your idea to see if other people have also had it before describing it as new. reply triceratops 19 hours agoparent> The system connects a bolt-clamped transducer with the brewing basket via a metallic horn – transforming a standard espresso filter basket into a powerful ultrasonic reactor. Now it can be done without shelling out $695 for a dedicated machine. That's progress. I wouldn't be so dismissive. reply karaterobot 17 hours agorootparent> However, engineers from UNSW have now developed a new way to make cold brew coffee in under three minutes – just like a regular hot brew – without sacrificing on the taste experience. I think the commenter you're responding to was calling out the use of the word 'new' (which is used five times in the article) to describe the process, when it's evidently not new at all. The commenter didn't say anything about the price point, and neither does the article. reply iancmceachern 19 hours agorootparentprevBut you have to shell out $10k for a commercial ultrasonic welder from Branson, etc. Just the horn they show in the paper probably cost $6k to make. I wouldn't be too dismissive. reply daniel_reetz 19 hours agorootparentAlso it will be punishing for pets and MEMs microphones. reply bongodongobob 16 hours agorootparentprevThey make ultrasonic jewelry cleaners for $75 that should work just fine. You're referencing something that is intended to be used with hundreds of gallons of water. reply iancmceachern 14 hours agorootparentI'm just talking about the paper. If they had referenced what you mention I would have referenced that reply triceratops 19 hours agorootparentprevIt's a prototype. reply fallat 18 hours agorootparentprevWhat? I went on Aliexpress and found a 40KHz transducer for $40. You can 3D print the attachment, can't you? reply iancmceachern 18 hours agorootparentA woofer from AliExpress is not a speaker with amp you can use for University level audio work. I expect they used something like these: https://www.emerson.com/en-us/automation/branson You cannot 3d print anything in the ultrasonic chain, they need to be machined from specific metal, that's why the horns cost so much. reply fallat 18 hours agorootparentI looked up \"transducer\" and it seems to be what you'd need. But! You seem to have found where I was wrong: the horn. The horn seems to be the actual expensive part here. They could start a business I guess selling these en-masse. If anyone has any ideas how a maker could create this within reason please reply. reply iancmceachern 18 hours agorootparentYou can't really sell them in big quantities because they're typically custom to the task. They need to be custom shaped for each application. reply srmatto 15 hours agorootparentPortafilters are made in mass so you could make horns for common portafilters. reply parineum 18 hours agorootparentprev> A woofer from AliExpress is not a speaker with amp you can use for University level audio work. We're talking about coffee here... reply iancmceachern 18 hours agorootparentExactly! Wars have been fought over coffee, people regularly spend thousands for coffee machines. There are entire companies that exist just to cater to this market. Me, I'm a tea guy. reply JoBrad 15 hours agorootparentA wholly different set of wars reply hobolord 7 hours agoparentprevThere's also this ultrasound company that sells lab equipment, but has a cold brewing via sonification page as well https://www.hielscher.com/ultrasonic-extraction-of-caffeine-... reply kjkjadksj 12 hours agoparentprev$695 isn't even bad, have people seen what even an entry level espresso machine can cost? reply Alex3917 18 hours agoparentprevI hope the creator does another run of the original Sora Pot, I would buy that in a heart beat. reply seanhunter 19 hours agoprevThis is awesome in an overengineered way, but if you want to make cold brew the normal way it's very easy. Basically you make it using a cafetière/French press/Bodum[1] using cold tap water in the fridge overnight (probably put some cling film over it), then press it and run it through a drip filter. The secret (if there is one) is to use fine ground coffee like you would use for an espresso rather than coarse ground (like you would use for a normal drip coffee or French press). It's very easy and very lovely. Just don't skimp on coffee. More detailed recipe here https://www.uncarved.com/articles/cold-brew/ [1] UK/US/French name but you know the thing with the plunger reply arijun 19 hours agoparentIf you use a French press for cold brew, be aware that the harder you press, the more bitter it will come out. I use a $5 nut milk bag instead, it lets me brew way more at once; I do 1/4 kilo grounds with 2 liters of water. It also has the benefit of reducing cafestol, which makes it healthier, according to some. reply senkora 18 hours agorootparent> It also has the benefit of reducing cafestol, which makes it healthier, according to some. I often filter my french press cold brew after-the-fact with a paper filter to achieve this. reply base698 7 hours agorootparentIsn't the paper filter supposed to strip flavorful oils? Which negates using a French press? reply senkora 5 hours agorootparentProbably. To be honest I just find it convenient to use a French press as a cold brew maker and first-pass filter, because it is IMO easier to keep clean than a nut milk bag or coffee sock. reply hammock 17 hours agorootparentprev>the harder you press, the more bitter it will come out Huh? I don't plunge until after it brews, and the purpose of the plunge is just to keep the water out of the grounds from that point forward, to (essentially) stop the brewing process. Or so I thought reply i_am_jl 13 hours agorootparentPlunging pushes your coffee through the metal filter. Pushing harder pushes the coffee through the filter with a higher pressure. Higher pressure pushes particles past the filter more easily than lower pressure. Coffee particulate has a tendency to make coffee more bitter. Anything that gets more small particles past your filter will add to your coffee's bitterness. This can happen when using a ground coffee that has a lot of fine particles, or if you agitate when you pour/plunge. reply hammock 12 hours agorootparentI don’t know anyone who’s jumping up and down on the plunger or doing anything more than pressing it slowly. Have to imagine the coarseness/evenness of your grind is a 10x bigger factor in that regard reply NegativeLatency 12 hours agorootparent> or doing anything more than pressing it slowly I have a french press for camping, it's fine but when I want that first cup of coffee and it's cold and rainy outside you better believe I'm not pressing it slowly. reply hammock 9 hours agorootparentThree seconds faster? You must really like coffee reply NegativeLatency 9 hours agorootparentHm maybe not that fast, whats considered fast? reply bschmidt1 10 hours agorootparentprevI used to plunge down hard and fast because I liked to see the bubbles. But I realized less disturbing is better, and I started barely using the plunge - just to catch grounds when pouring. After a while I went back to pour-over. The flavor is so much better - French press is now used for loose leaf tea and I have a lot more fun plunging. reply vincentrolfs 17 hours agorootparentprevI agree with your confusion, are we all talking about the same thing? reply nubinetwork 5 hours agoparentprevI just pour coffee grinds into a recycled pop bottle... no need to get fancy with a French press. reply dhritzkiv 13 hours agoparentprevOne thing I'll mention is that cold brew doesn't need to be put in the fridge to \"brew\". It can be left out on the counter at room temperature. As a result, this takes the brew time from 24 hours to about 12 hours. reply spython 14 hours agoprevOr you could use a (quite affordable) ultrasonic machine designed for gentle cleaning of jewelry, dentures, glasses.. I've used one to extract fragrance from biological material for an artistic project[0], and it worked really well. Instead of having to wait for a few weeks for a tincture to finish, you put the same tincture (alcohol and material you want to extract fragrance from) into a plastic bag for just 15 minutes. Sure, it smells not quite the same, but the speed is often worth it. I've even heard about some guy trying to turn vodka into whiskey with an ultrasonic machine and wood chips. There are quite a few ultrasonic machines on the market. I've tried EMAG and multiple Chinese no-name machines that are just as powerful but cheaper. Sadly the no-name machines are quite a bit louder - you can't stay in the same room while it's running basically. Still, they all work well for this kind of fast and dirty extraction. [0] https://rybakov.com/blog/smelling_cz/ reply surfingdino 18 hours agoprevJames Hoffman's video on the subject incoming in 3... 2... 1... can't wait :-) reply eichin 15 hours agoprevOoh an excuse to upgrade some homelab equipment :-) (My preferred \"weird\" coffee is sous-vide: 125g/liter, 2h @ 150f - so you don't leave as much flavor \"on the table\", but by not getting near boiling you leave more of the bitter compounds behind. Refrigerated but served iced or warmed with boiling water, to taste.) reply beefman 18 hours agoprevA few years ago there was a crowdfunded instant cold brew machine based on acoustic cavitation called Osma.[1] I don't think it's still in production. I met the founder during testing at Chromatic Coffee in San Jose, and took delivery of one of the first units. Cool concept, but it didn't work very well [1] https://www.engadget.com/osma-pro-cold-brew-coffee-machine-r... reply conchy 9 hours agoprevAs someone who makes cold brew every day, this is one of the two approaches I've considered to speed things up, the other being one of those magnetic stirrers they have in chemistry lab. However, after careful consideration, the real low-hanging-fruit here is the time it takes to grind the coffee, load it, fill the water, clean the filter, and rinse the jar. If a cold brew machine could automate these steps (like some hot coffee machines do) you wouldn't care about making a cup in 3 minutes because you'd always have an automatic jar ready for you from 24 hours ago. reply mianos 12 hours agoprev\"Ultrasounds can be applied to several areas across the food industry including drying, extraction, emulsification and crystallisation – making the process faster and more efficient.\", .. I think they have the tense wrong. It should read \"Ultrasounds are now commonly applied ..\" People have been doing this sort of thing for years. I have an ultrasonic tub for extracting flavours into ethanol. I didn't invent it, people were talking about it years ago on the forums on this stuff. Maybe they don't have internet over there at UNSW. They can come over and borrow mine for a bit of a search. I am just a few ks away. Or go to that internet cafe in Kensington in the next block. reply papertokyo 10 hours agoparentThe innovation here is integrating the tech into a home espresso machine package. I do wonder if the sound is any less awful though. Does the ultrasonic tub enable better extraction at lower proof or are you using it purely to speed up the maceration process? reply mianos 7 hours agorootparentThe ultrasonic tub, (Vevor LOL, like everyone else gets for their bike parts), speeds up the process, specially oak shards. For comparison, I also have a Soxhlet apparatus. As the extraction is boiled in the bottom with the recycling solvent it does not make the best flavours but it's the fastest way to get stuff like cinnamon. Between a longer time, soxhlet and ultrasonic, the ultrasonic gives flavours as smooth as a longer time without the wait. (The ultrasonic bath heating up is still a problem so I have to cycle it). Time beats both. I think a short path soxhlet maybe be as good. My setup is a bit fragile for a good vacuum though. reply nottorp 3 hours agoprevWhy 3 minutes? Fancy ways of brewing coffee bring more value via their ritualistic part than via the final product. Same for smoking a pipe, it's the filling up and cleaning that matters. If you want speed, get a fully automatic machine. Ideally with a timer so your coffee is ready when you wake up. reply qwerty456127 2 hours agoprevYou can also make reasonable (not excellent, not terrible) brandy this way by putting vodka and oak wood shavings of a suitable kind into an ultrasound machine. reply xkcd-sucks 19 hours agoprevIt's really just a matter of mindset -- Many things go faster if you chuck them into a microwave or a sonicator :) I'm definitely going to try this out with cold brew in a cheapie bath sonicator. The thing here is like a big ass probe sonicator butted up against an espresso portafilter which is probably a bit louder than a loud steamer reply kazinator 19 hours agoparentI have two sonicators, aged 3.5 and 5.5. The older one is going into grade 1 in September. reply bethekind 17 hours agorootparentThis had me cackle :D reply 01100011 9 hours agoprevHmm.. I didn't think to try my ultrasonic machine. Last time I made cold brew and got impatient, I just warmed it up to around 100F and put the whole thing in my vacuum chamber for a bit. Seemed to speed things up but I don't know. I don't brew enough to have a recent comparison. The result is pretty great though. reply showerst 19 hours agoprevThis is incredibly cool and I want to go build one, though it feels like that cutaway model of injecting the ultrasound from the side would lead to very uneven extraction. I also chuckled at graf about doubling the caffeine content, as if that's necessarily a good thing =). Those cheap HC-SR04 ultrasonic modules output at 40kHz, so maybe this is home-brewable. reply htrp 19 hours agoparentI think the idea is that the vibration transmits through the entire filter. The question is whether it's geometry/frequency specific reply floatrock 18 hours agorootparentThe embedded youtube video at 0:58 shows a laptop screen with some pretty fancy-looking simulation jiggles and heatmaps on what looks like an espresso filter. It also looks like a PDF of a paper, so presumably they have a paper that talks about the geometry/frequency interactions. reply eichin 13 hours agorootparentThe article includes a link to the paper, which has simulation models of the acoustics, showing pressure levels and areas where cavitation can occur. (I didn't see anything about heating from dissipating the acoustic energy, though?) reply sharpshadow 19 hours agorootparentprevIt’s likly not specific one could reduce the cold brewing time by shaking the jar. reply skrunch 19 hours agoprevQuick, someone let James Hoffman know! reply blkhawk 19 hours agoparentI was just about to suggest he same thing reply cabirum 13 hours agoprevSearch for \"portable ultrasonic washing machine\" on Amazon/Aliexpress, looks like a hockey puck you drop into a bucket with water and clothes. Looks like the principle of operation is the same, just add a coffee filter. reply senderista 8 hours agoprevOur espresso machine has been on the fritz for a while, but I barely miss it after discovering that a few oz. of TJ's cold brew with a splash of milk in the microwave for 30s is practically as good. I think warmed-up cold brew is one of my favorite life hacks. reply vletal 8 hours agoprevCan I just take an ultrasonic jewelry cleaner and fill it with water and ground coffee? reply eezurr 19 hours agoprevI discovered you can mix Nestle instant coffee with cold (charcoal filtered) water and it dissolves fine. Somehow the added sugar easily dissolves too. I also add heavy cream It actually tastes pretty good. Most of the acrid taste from (hot) instant isnt there reply triceratops 19 hours agoparentBut then you miss out on the self-flagellation aspect of drinking instant. reply all2 18 hours agorootparentYou can no longer mutter to your coworkers I just need the caffeine as you add the 5th scoop of instant to your Styrofoam cup of hot water. reply brotchie 19 hours agoprevCoffee fracking IRL reply doodlebugging 19 hours agoparentGet yourself fracffiened. reply simondw 19 hours agorootparentAh yes, with a delicious frackuccino. reply papertokyo 10 hours agorootparentI'll have two frack whites to go thanks. reply ugh123 16 hours agoprevHere's my iced coffee prep for those who like it strong with a bite: Prepare a \"lungo\" via espresso by brewing through twice (or more) as much water through a single espresso puck. Don't do this over ice. Put the cup in the freezer. Depends on the cup (I use ceramic) but should be close to room temperature or slightly cold after about 30 minutes. Now pour over ice. Can also do this over night for larger brews in the fridge (non freezer). This is as close as I could get to a Starbucks iced coffee that isn't watered down and still has bite. reply thsksbd 17 hours agoprevI'm guessing few on this board have heard an uktrasonicator in action. I just showed to article to my mate, and he enthusiastically said we'll brew some... as soon as we leave the lab to break for lunch reply askvictor 12 hours agoparent> heard an uktrasonicator Is this a joke? reply papertokyo 10 hours agorootparentNo. These machines sound like robot hornets operating dental drills. reply askvictor 8 hours agorootparentI thought ultra sonic was defined as being above the sonic range of humans. Genuine question: why would an ultrasonic noise generator create sound audible frequencies? Can it not provide enough power without doing so? reply thsksbd 5 hours agorootparentUltrasonicators for the purpose of cavitation research are insanely powerful and non linear material response cause all sorts of vibrational modes to boom. Its unpleasant. reply blincoln 19 hours agoprevI'm disappointed that their test didn't include a comparison to traditional drip coffee made using the same beans, then chilled to match the temperature of the cold brew. I haven't looked into ultrasonic cavitation in years, but since it can produce enough heat and light to make some people wonder if it was a form of nuclear fusion back in the early 2000s, I feel like maybe it's affecting the flavour of the coffee at least as much as using hot water would. reply bschmidt1 3 hours agoparentInteresting, microwaving with sound reply 36 more comments... GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The University of New South Wales offers diverse study programs, including support for international students and Higher Degree Research, along with accommodation information, fee details, scholarships, and application procedures.",
      "The university emphasizes research, industry partnerships, and various research areas, focusing on social impact, innovation, and collaboration within its community.",
      "Researchers at the university have innovated an ultrasonic sound wave method to expedite cold brew coffee production from hours to minutes, maintaining taste quality, potentially transforming the coffee industry with a quicker and more effective brewing process."
    ],
    "commentSummary": [
      "The discussion dives into various aspects of cold brew coffee, such as brewing methods, equipment, personal preferences, challenges of roasting, and debates over heating cold brew.",
      "Users exchange tips, experiences, and recommendations on coffee-making techniques, including acidity, flavor, authenticity, and health effects of cold brew.",
      "Topics like ultrasonic technology, custom orders, and storage methods are also explored, highlighting the significance of personal taste and balance between simplicity and complexity in coffee brewing."
    ],
    "points": 454,
    "commentCount": 286,
    "retryCount": 0,
    "time": 1715085974
  },
  {
    "id": 40286734,
    "title": "LPCAMM2: The Future of Laptop Memory Upgrades",
    "originLink": "https://www.ifixit.com/News/95078/lpcamm2-memory-is-finally-here",
    "originBody": "Exclusive LPCAMM2 Is Finally Here, and It’s a Big Deal Article by: Carsten Frauenheim @carsten May 7, 2024 Filed under: Tech News, Exclusive 15 Comments Share If you’ve ever tried to “future-proof” a purchase by paying for everything you might eventually need up front, you know it can be a sucker’s game. The problem? We can’t actually see the future. But today we got our hands on LPCAMM2 for the first time, and this looks like the future to us. LPCAMM2 is a totally modular, repairable, upgradeable memory standard for laptops, using the latest LPDDR chips for maximum speed and efficiency. So instead of overpaying (or under-speccing) based on guesswork about your future memory needs, you’ll hopefully be able to buy your next laptop and then install more RAM as needed. Imagine that! We say “hopefully” because the laptop on our teardown table today is the first of its kind, thanks to a collaboration between Micron and Lenovo—and it remains to be seen how many other big laptop makers will adopt LPCAMM2 technology. But judging by the results of our initial hands-on, the writing is on the wall for laptops with soldered-down, non-serviceable memory. What is LPDDR, and why do manufacturers solder it down? Repairable, upgradeable RAM isn’t exactly a new idea. As anyone who has ever built a PC knows, we’ve had swappable DDR RAM sticks (also known as DIMMs, or Dual In-Line Memory Modules) since basically forever. From old Gateway towers to today’s gaming powerhouses to zillion-dollar industrial servers, upgradeable and replaceable RAM is still very much a thing. And for many years, the same was true of laptops, which used a slightly more compact (“Small-Outline” DIMM, or SO-DIMM) version of those same RAM sticks. More recently though, we’ve seen increasing adoption of LPDDR—a low-power flavor of RAM (hence the “LP”) developed for mobile devices like phones and tablets. Whereas conventional DDR RAM excels at performance applications where power consumption isn’t a primary concern, like video editing or gaming, LPDDR wins the day when it comes to efficiency—a.k.a. battery life. And so for laptops in particular, the benefits of LPDDR are hard to beat. The drawback of LPDDR, though, is that it has to be soldered to the main board in close proximity to the processor—making repairs and upgrades completely impractical. But why? LPDDR operates at lower voltages compared to DDR, giving it the edge in power efficiency. But, the lower voltage makes signal integrity between the memory and processor challenging, requiring tighter tolerances and shorter trace distances—that is, the farther the signal has to travel, the more voltage you need for a reliable signal. This is why LPDDR is soldered down as close to the processor as possible. Historically, LPDDR chips (left) had to be soldered very close to the main processor (right). In short, laptop makers and consumers alike have faced an unfortunate dilemma: conventional SO-DIMM RAM for serviceability and upgradeability, or soldered LPDDR chips for longer battery life. Today, that changes. Enter LPCAMM2 Standing for “Low-Power Compression-Attached Memory Module,” the new tech is as the name suggests: LPDDR chips on a compact board that screws in place very close to a laptop’s CPU. Combining the efficiency and speed of LPDDR with a thin, lightweight, upgradeable design and a trick interface that gets everything up close and personal with the CPU, LPCAMM2 seemingly does it all. And with dual-channel performance already baked in, a single LPCAMM2 module can do the job of a pair of the old socketed SO-DIMM sticks with a much smaller footprint and better thermals to boot. Finally: modular, performant, power-efficient laptop memory for the masses. Lenovo’s ThinkPad P1 (Gen 7) is the first laptop you can actually buy that uses LPCAMM2 tech—check out our repair guide. Even though LPCAMM2 is arriving initially from Micron, in a Lenovo product, the technology owes its existence to an alliance of tech companies working together over the course of several years. The first iteration, known as CAMM, was an in-house project at Dell, with the first DDR5-equipped CAMM modules installed in Dell Precision 7000 series laptops. And thankfully, after doing the initial R&D to make the tech a reality, Dell didn’t gatekeep. Their engineers believed that the project had such a good chance at becoming the next widespread memory standard that instead of keeping it proprietary, they went the other way and opened it up for standardization. They were right. Only a few short years later, with the blessing of the JEDEC standards body, LPCAMM2 is here and ready to take the torch. Dell is one hero in this story, creating something the tech world sorely needed and then sharing instead of keeping it for themselves. Thankfully, this story is full of heroes: Micron and Lenovo are first to bring LPCAMM2 to market, with Samsung, ADATA, and others backing it as well. Instead of everyone going their own way, they’ve rallied around a new industry standard—meaning an off-the-shelf LPCAMM2 module should theoretically work in any laptop that adopts the technology, regardless of who manufactures it. With the industry as a whole on the same page, backing standards like this, the world becomes a more repairable place. Designing for a repairable future The advent of LPCAMM2 is especially gratifying for repair advocates, who for years have been told that repairability simply can’t coexist with cutting-edge tech in thin-and-light devices. We’re not ready to accept that, and we’ve long argued that OEMs who are willing to innovate with repairability in mind can do better. Maybe we can’t see the future, but we can envision one that’s more repairable than what we’ve been sold in recent years—and we’re grateful when companies like Micron and Lenovo take the leap to make that future a reality. LPCAMM2 exemplifies our ability to advance technologically while designing with sustainability in mind. It represents a significant step forward in the fight against planned obsolescence. By fighting for a modular, upgradeable memory solution for chips previously stuck in a soldered hellscape, manufacturers are demonstrating their commitment to creating devices that stand the test of time. There’s so much to gain here: from increasing device lifespan at schools and businesses, to reducing anxiety for consumers at the point of purchase, to enabling hassle-free repairs for devices that would otherwise be scrapped. As more companies rally behind this standard, we can look forward to a future where more laptops are built to last, and where repairs and upgrades aren’t only possible, but encouraged. There’s no question that the potential for this technology to make a tangible difference is real, and it’s right in front of us. Full Disclosure: iFixit has prior business relationships with both Micron and Lenovo, and we are hopelessly biased in favor of repairable products. Related Stories iFixit iFixit and Lenovo Work Together to Make Laptop Repairability the Standard Opinion How Valve is Changing Game Console Design Opinion More PC Gaming Mice Should Follow ROG’s Repairable Switches",
    "commentLink": "https://news.ycombinator.com/item?id=40286734",
    "commentBody": "LPCAMM2 is a modular, repairable, upgradeable memory standard for laptops (ifixit.com)308 points by leduyquang753 18 hours agohidepastfavorite131 comments orev 17 hours agoI’m glad they explained why RAM has become soldered to the board recently. It’s easy to be cynical and assume they were doing it for profit motive purposes (which might be a nice side effect), but it’s good to know that there’s also a technical reason to solder it. Even better to know that it’s been recognized and a solution is being worked on. reply OJFord 14 hours agoparentI didn't find that a particularly complete explanation - and the slot can't be closer to the CPU because? - I think it must be more about parasitic properties of the card edge connector on DIMMs being problematic at lower voltage (and higher frequencies) or something. Note the solution is a ball grid connection and the whole thing's shielded. I suppose in fairness and to the explanation it does give, the other thing that footprint allows is a shorter path for the pins that would otherwise be near the ends of the daughter board (e.g. on a DIMM), since they can all go roughly straight across (on multiple layers) instead of a longer diagonal according to how far off centre they are. But even if that's it, that's what I mean by it seeming incomplete. :) reply Tuna-Fish 11 hours agorootparent> and the slot can't be closer to the CPU because? All the traces going into the slot need to be length-matched to obscene precision, and the physical width of the slot and the room required by the \"wiggles\" made in the middle traces to length-match them restrict how close you can put the slot. Most modern boards are designed to place it as close as possible. LPCAMM2 fixes this by having a lot of the length-matching done in the connector. reply throwaway48476 11 hours agorootparentprevCompetes with space for VRM's. reply smolder 12 hours agorootparentprevYeah, you can only make the furthest RAM chip in DIMM be so close to the CPU based on the form factor, and the other traces need to match that length. Distance is critical and edge connectors sure don't help. reply klysm 10 hours agoparentprevI didn’t really appreciate the insanity of the electrical engineering involved in high frequency stuff till I tried to design some PCBs. A simplistic mental model of wires and interconnects rapidly falls apart as frequencies increase reply yread 2 hours agoparentprevIf they soldered a decent amount that gou can be sure you don't ever need to upgrade it would be fine (seriously, 64GB ram costs like 100eur, non issue in a 1000eur laptop). 8 is not enough already and 16 will soon be limiting too. reply tombert 13 hours agoparentprevYeah, I was actually surprised to learn there was a reason other than \"Apple wants you to buy a new Macbook or overspec your current one\". It's annoying, but at least there's a plausible reason to why they do it. reply seanp2k2 10 hours agorootparent\"...and they charge 4x what the retail of premium RAM would otherwise be per GB\" do storage next. reply klausa 7 hours agorootparentprevApple's RAM is not soldered to the _motherboard_, it's part of the SoC package. reply Vogtinator 2 hours agorootparentOnly recently. It started out as soldered to the main board. reply drivingmenuts 15 hours agoparentprevThe problem is getting manufacturers to implement the new RAM standard. While the justifications given are great for the consumer, I didn't see any reason for a manufacturer to sign on. They are going to lose money when people buy new RAM, rather than a whole new laptop. While processor speeds and size haven't plateaued yet, it's going to take a while to develop significant new speed upgrades and in the meantime, the only other upgrade is disk size/long-term storage, which, aside from Apple, they don't totally control. So, why should they relenquish that to the user? reply AnthonyMouse 15 minutes agorootparent> They are going to lose money when people buy new RAM, rather than a whole new laptop. You're thinking about this the wrong way around. Suppose the user has $800 to buy a new laptop. That's enough to get one with a faster processor than they have right now or more memory, but not both. If they buy one and it's not upgradable, that's not worth it. Wait another year, save up another $200, then buy the one that has both. Whereas if it can be upgraded, you buy the new one with the faster CPU right away and upgrade the memory in a year. Manufacturer gets your money now instead of later, meanwhile the manufacturer who didn't offer this not only doesn't sell to you in a year, they just lost your business to the competition. reply cesarb 13 hours agorootparentprev> While the justifications given are great for the consumer, I didn't see any reason for a manufacturer to sign on. [...] So, why should they relenquish that to the user? It makes sense that the first ones to use this new standard would be Dell and Lenovo. They both have \"business\" lines of computers, which usually offer on-site repairs (they send the parts and a technician to your office) for a somewhat long time (often 3 or 5 years). To them, it's a cost advantage to make these computers easier to repair. Having the memory (which is a part which not rarely fails) in a separate module means they don't have to replace and refurbish the whole logic board, and having it easy to remove and replace means less time used by the on-site technician (replacing the main logic board or the chassis often means dismantling nearly everything until it can be removed). reply masklinn 11 hours agorootparent> To them, it's a cost advantage to make these computers easier to repair. Alternatively, it allows them to use more efficient RAM in computer lines they can't make non-repairable so they can boast of higher battery life. reply babypuncher 7 hours agorootparentprevThey also charge a lot more for these \"business-class\" machines. That higher margin captures the revenue lost to DIY repairs and upgrades. reply makeitdouble 10 hours agorootparentprevI'd see two angles: - the manufacturer themselves benefit from easier to repair machines. If DELL can replace the RAM and send back the laptop in a matter of minutes instead of replacing the whole motherboard to then have it salvaged somewhere else, it's a clear win. - prosumers will be willing to invest more in a laptop that has better chance to survive a few years. Right now we're all expecting to have parts fail within 2 to 3 years on the higher end, and budget accordingly. You need a serious reason to buy a 3000$/€ laptop that might be dead in 2 years. Knowing it could weather RAM failure without manufacturer repair is a plus. reply bugfix 14 hours agorootparentprevEven if it's just Lenovo using these new modules, I still think it's a win for the consumer (if the modules aren't crazy expensive). reply 7speter 12 hours agorootparentprevThese companies did plenty well 12+ years ago when users could upgrade their systems memory. reply kjkjadksj 12 hours agoparentprevThey can have their technical fig leaf to hide behind but in practice, how many watts are we really saving between lpddr5 and ddr5? is it worth the ewaste tradeoff to have a laptop we can't modularly upgrade to meet our needs? I would guess not. reply masklinn 11 hours agorootparent> how many watts are we really saving between lpddr5 and ddr5? From what I gathered, it's around a watt per when idling (which is when it's most critical): the sources I found seem to indicate that ddr5 always runs at 1.1V (or more but probably not in laptops), while lpddr5 can be downvolted. That's an extra 10% idle power consumption per. reply mmastrac 17 hours agoprevUgh, finally. And it's not just a repurposed desktop memory standard either! The overall space requirements look to be similar to the BGA that you'd normally solder on (perhaps 2-3x as thick?). I'm sure they can reduce that overhead going forward. I love the disclosure at the bottom: Full Disclosure: iFixit has prior business relationships with both Micron and Lenovo, and we are hopelessly biased in favor of repairable products. reply Aurornis 15 hours agoparent> Ugh, finally. FYI, the '2' at the end is because this isn't the first time this has been done. :) LPCAMM spec has been out for a while. LPCAMM2 is the spec for next-generation parts. Don't expect either to become mainstream. It's relatively more expensive and space-consuming to build an LPCAMM motherboard versus dropping the RAM chips directly on to the motherboard. reply audunw 29 minutes agorootparentNot to mention putting the RAM directly on a System-in-Package chip like Apple does now. That's going to be unbeatable in terms of space and possibly have an edge when it comes to power consumption too. I wouldn't be surprised if future standards will require on-package RAM. I kind of wish we could establish a new level in the memory hierarchy. Like, just make a slot where you can add slower more power hungry DDR RAM that acts as a big cache for the NVM storage, or that the OS can offload some of the stuff in main memory if it's not used much. It could be unpopulated in base models, and then you can buy an upgrade to stick in there to get some extra performance later if needed. reply nrp 14 hours agorootparentprevMy recollection of this is that LPCAMM was a proposal from Dell that they put into the JEDEC standardization process, and LPCAMM2 is the resulting standard, named that way to avoid confusion with the non-standard LPCAMM that Dell trialed on a small number of commercial systems. reply cjk2 17 hours agoparentprevYeah they even gloss over Lenovo's crappy soldered on the motherboard USB-C connectors which is always the weak point on modern thinkpads. Well that and Digital River (Lenovo's distributor) carries absolutely no spare parts at all for any Lenovos in Europe, and if they do they only rarely turn up, so you can't replace any replaceable bits because you can't get any. reply chpatrick 16 hours agorootparentHave you tried https://www.lenovopartsales.com/LenovoEsales ? reply sspiff 12 hours agorootparentprevDigital River is shit at everything. From spare parts, to delivery and tracking, to customer communications, to warranty claims. Every single interaction with them is a nightmare. It is the single reason I prefer to buy Lenovo from resellers rather than directly. reply zxcvgm 17 hours agoprevI remember when Dell was the first to introduce [1] these Compression Attached Memory Modules in their laptops in an attempt to move away from soldered-on RAM. Glad this is now being more widely adopted and standardized. [1] https://www.pcworld.com/article/693366/dell-defends-its-cont... reply AlexDragusin 15 hours agoparent> The first iteration, known as CAMM, was an in-house project at Dell, with the first DDR5-equipped CAMM modules installed in Dell Precision 7000 series laptops. And thankfully, after doing the initial R&D to make the tech a reality, Dell didn’t gatekeep. Their engineers believed that the project had such a good chance at becoming the next widespread memory standard that instead of keeping it proprietary, they went the other way and opened it up for standardization. reply baby_souffle 17 hours agoprevThis is fantastic news. Hopefully the cost to manufacturers is only marginal and they find a suitable replacement for their current \"each tier in RAM comes with a 5-20% price bump\" pricing scheme. Too bad apple is almost guaranteed to not adopt the standard. I miss being able to upgrade the ram in macbooks. reply Aurornis 15 hours agoparent> Too bad apple is almost guaranteed to not adopt the standard. Apple would require multiple LPCAMM2 modules to provide the bus width necessary for their chips. Up to 4 x LPCAMM2 modules depending on the processor. The size of each LPCAMM2 module is almost as big as the entire size of an Apple CPU combined with the unified RAM chips, so putting 2-4 LPCAMM2 modules on the board is completely infeasible without significantly increasing the size of the laptop. Remember, the Apple architecture is a combined CPU/GPU architecture and has memory bandwidth to match. It's closer to your GPU than the CPU in your non-Mac machine. Asking to have upgradeable RAM on Apple laptops is akin to almost like asking for upgradeable RAM on your GPU (which would not be cheap or easy) For every 1 person who thinks they'd want a bigger MacBook Pro if it enabled memory upgrades, there are many, many more people who would gladly take the smaller size of the integrated solution we have today. reply coolspot 15 hours agorootparent> like asking for upgradeable RAM on your GPU Can I please have upgradeable RAM on GPU? Pwetty pwease? reply thfuran 15 hours agorootparentSure, as long as you're willing to pay in cost, size, and performance. reply kokada 14 hours agorootparentprev> Up to 4 x LPCAMM2 modules depending on the processor. The non-Pro/Max versions (e.g. M3) uses 128-bits, and arguably is the kind of notebook that mostly needs to be upgraded later since they commonly come with only 8GB of RAM. Even the Pro versions (e.g. M3 Pro) use up-to 256-bits, that would be 2 x LPCAMM2 modules, that seem plausible. For the M3 Max in the Macbook Pro, yes, 4 x LPCAMM2 would be impossible (probably). But I think you could have something like the Mac Studio have them, that is arguably also the kind of device that you probably want to increase memory in the future. reply throwaway48476 10 hours agorootparentIt would only need to be 2x per board side. reply sliken 17 hours agoparentprevApple ships 128 bit, 256 bit, and 512 bit wide memory interfaces on laptops (up to 1024 bit wide on desktops). Is it feasible to fit memory bandwidth like the M3 Max (512 bits wide LPDDR5-6400) with LPCAMM2 in a thin/light laptop? reply AnthonyMouse 2 minutes agorootparentApple does this because their CPU and GPU use the same memory, and it's generally the GPU that benefits from more memory bandwidth. Whereas in a PC optimized for GPU work you'd have a discrete GPU that has its own memory which is even faster than that. reply pja 16 hours agorootparentprevThis PDF[1] suggests that an LPCAMM2 module has a 128 bit wide memory interface, so the epic memory bandwidth of the M3 max won’t be achievable with one of these memory modules. High end devices could potentially have two or more of them arranged around the CPU though? [1] https://investors.micron.com/node/47186/pdf reply 7speter 12 hours agorootparentApple could just make lower tier macbooks but mac fanboys wouldnt be able to ask “but what about apples quarterly profits?” Most macbooks dont need high memory bandwidth, most users are using their macs for word processing, excel and vscode. reply pmontra 3 hours agorootparentAs a non Mac reference, I work on a HP laptop from 2014. It was a high end laptop by then. It's between 300 and 600 Euro refurbished now. I expanded it to 32 GB RAM, 3 TB SSD but it's still a i7 4xxx with 1666 MHz RAM. And yet it's OK for Ruby, Python, Node, PostgreSQL, docker. I don't feel the need to upgrade. I will when I'll get a major failure and no spare parts to fix it. So yes, low end Macs are probably good for nearly everything. reply sliken 4 hours agorootparentprevEven low end gaming, simulations, and even fun webGL toys can require a fair amount of memory bandwidth with an iGPU, like apple's M series. It also helps quite a bit for inference. I MBP with a M3 max can run models requiring multiple GPUs on a desktop and still get decent perf for single users. reply consp 3 hours agorootparent> I MBP with a M3 max can run models requiring multiple GPUs on a desktop and still get decent perf for single users. Good for your niche case, the other 99.8% still only does web and low performance desktop applications (which includes IDEs) reply teaearlgraycold 10 hours agorootparentprevYes but Apple’s trying to build an ecosystem where users get highly quality, offline, low latency AI computed on their device. Today there’s not much of that. And I don’t think they even really know what’s going to justify all of that silicon in the neural engine and the memory bandwidth. Imagine 5 years from now people have built whole stacks on that foundation. And then competing laptops need to ship that compute to the cloud, with all of the unsolvable problems that come with that. Privacy, service costs (ads?), latency, reliability. reply jwells89 7 hours agorootparentApple is also deliberately avoiding having “celeron” type products in their lineup because those ultimately mar the brand’s image due to being kinda crap, even if they’re technically adequate for the tasks they’re used for. They instead position midrange products from 1-2 gens ago as their entry level which isn’t quite as cheap but is usually also much more pleasant to use than the usual bargain basement stuff. reply wmf 16 hours agorootparentprevFor 512 bits you would need four LPCAMM2s. I could imagine putting two on opposite sides of the SoC but four might require a huge motherboard. reply kristianp 10 hours agorootparentPerhaps future LPCAMM generations will require more bits? I still can't imagine apple using them unless required by right to repair laws. But those laws probably don't extend to making RAM upgradeable. reply jauntywundrkind 16 hours agorootparentprevHoping we see AMD Strix Halo with it's 256-bit interface crammed into an aggressively cooled fairly-thin fairly-light. But it's going to require heavy cooling to make full use of. Heck, make it only run full tilt when on an active cooling dock. Let it run half power when unassisted. reply seanp2k2 10 hours agorootparentKinda hilarious to see gamers buying laptops that can't actually leave the house in any practical meaningful way. I feel like some of them would be better off with SFF PCs and the external monitors they already use. I guess the biggest appeal I've seen is the ability to fold up the gaming laptop and put the dock away to get it off the desk, but then moving to an SFF on the ground plus a wireless gaming keyboard and wireless mouse that they already use with the normal laptop + one of those compact \"portable\" monitors seems like it'd solve the same problem. reply kristianp 10 hours agorootparentMy wife can get an hour of gaming out of her gaming laptop. They're good for being able to game in an area of the house where the rest of the family is, even if that means being plugged in at the dining table. Our home office isn't close enough. Also a gaming laptop is handy if you want to travel and game at your hotel. reply jwells89 7 hours agorootparentprevI’ve been wondering for a while now why ASUS or some other gaming laptop manufacturer doesn’t take one of their flagship gaming laptop motherboards, put some beefy but quiet cooling on it, put it in a pizza-box/console enclosure, and sell it as a silent compact gaming desktop. A machine like that could still be relatively small but still be dramatically better cooled than even the thickest laptop due to not having to make space for a battery, keyboard, etc. reply antonkochubey 4 hours agorootparentZOTAC does these - there are ZBOX Magnus with laptop-grade RTX 4000 series GPUs in 2-3 liter chassis. However their performance and acoustics are rather.. compromised, compared to a proper SFF desktop (which can be built in ~3x the volume) reply jwells89 4 hours agorootparentYeah, those look like they’re too small to be reasonably cooled. What I had in mind is shaped like the main body of a laptop but maybe 2-3x as thick (to be able to fit plenty of heatsink and proper 120/140mm fans), stood up on its side. reply j16sdiz 7 hours agoparentprevUnified memory is basically L3 cache speed with zero copy between CPU and GPU. They have engineering difference. Depends on who you ask, it may or may not worth it reply enragedcacti 7 hours agorootparentAssuming you mean latency, Apple's unified memory isn't lower latency than other soldered or socketed solutions e.g. M1 Max with 111ns latency on cache miss vs 13900k with 93ns latency. Certainly not L3 level latency. Zero copy between CPU/GPU is great but not unique to unified memory or soldered ram. As far as bandwidth goes, you would only need one or two LPCAMM2 modules to match or exceed the bandwidth of non-Max M series chips. Accommodating Max chips in a macbook with LPCAMM2 would definitely be a difficult packaging problem. https://www.anandtech.com/show/17024/apple-m1-max-performanc... https://www.anandtech.com/show/17047/the-intel-12th-gen-core... reply cjk2 17 hours agoparentprevGiven enough pressure ... reply armarr 17 hours agorootparentYou mean pressure from regulators, surely. Because 99% of consumers will not notice or know the difference in a spec sheet. reply colinng 15 hours agorootparentprevThey will maliciously comply. They might even have 4 sockets for the 512-bit wide systems. But then they’ll keep the SSD devices soldered - just like they’ve done for a long time. Or cover them with epoxy, or rig it with explosives. That’ll show you for trying to upgrade! How dare you ruin the beautiful fat profit margin that our MBAs worked so hard to design in?!? reply cjk2 3 hours agorootparentThis is hyperbole. They are replaceable. It's just more difficult. reply 7speter 12 hours agorootparentprevApple lines perimeter of the nand chips on modern mac minis with an array of tiny capacitors, so even the crazy people with heater boards can’t unsolder the nand and replace them with higher density NAND. reply wtallis 10 hours agorootparentHave you not looked at the NAND packages on any regular SSDs? Tiny decoupling caps alongside the NAND is pretty standard practice. reply cjk2 3 hours agorootparentprevThis is normal. They are called decoupling capacitors and are there to provide energy if the SSD requires short bursts of it. If you put them any further away the bit of wire between them and the gate turns into an inductor and has some somewhat undesirable characteristics. Also replacing them is not rocket science. I reckon I could do one fine (used to do rework). The software side is the bugbear. reply redeeman 11 hours agoparentprevand they wont so long as people buy regardless reply quailfarmer 2 hours agoprevI'm sure this will find use in Business-Class \"Mobile workstations\", but having integrated DDR4 in my own hardware, I have a hard time seeing this as the mainstream path forward for mobile computing. There's lots of value in tight integration. Improved signal integrity (ie, faster), improved reliability, better thermal flow, smaller packaging, and lower cost. Do I really want to compromise all of those things just to make RAM upgrades easier? And how many times do I need to upgrade the RAM in a laptop, really? Twice? Why make all those sacrifices to use a connector, instead of just reworking the DRAM parts? A robotic reflow machine is not so complex that a small repair shop couldn't afford one, which is what you see if you to to parts of the world where repair is taken seriously. Why do I need to be able to do it at home? I can't re-machine my engine at home. It's the most advanced nanotechnology humanity can produce, why is a $5k repair setup unreasonable? This is not to mention the direction things are really going, DRAM on Package/Die. The signaling speed and bus widths possible with co-packaged memory and HBM are impossible to avoid, and I'm not going to complain about the fact that I can't upgrade the RAM separately from the CPU, any more than I complain about not being able to upgrade my L2 cache today. The memory is part of the compute, in the same way the GPU memory is part of the GPU. I hope players like iFixit and Framework aren't too stubborn in opposing the tight integration of modern platforms. \"Repairable\" doesn't need to mean the same thing it did 10 years ago, and there are so many repairability battles that are actually worth fighting, that being stubborn about the SOTA isn't productive. reply Timshel 1 hour agoparent>I'm sure this will find use in Business-Class \"Mobile workstations\", but having integrated DDR4 in my own hardware, I have a hard time seeing this as the mainstream path forward for mobile computing. Don't know would say the reverse, workstation might need the performance of DRAM on Package/Die, but I don't believe it's the case for mainstream user. > A robotic reflow machine Same maybe to service enterprise customer but probably way too expensive for mainstream. I certainly hope that players continue to oppose tight integration and I'll try to support them. I value the ability that anyone can swap ram and disks to easily upgrade or repair their device more than an increase of performance or even battery life. I recently cobbled up a computer for a friend's child with component from three different computers; any additional cost would have made the exercise worthless. reply doublextremevil 17 hours agoprevCant wait to see this in a framework laptop reply OJFord 14 hours agoparentFor the presumed improvement to battery life? Because Fw already uses SO-DIMMs. reply universa1 13 hours agorootparentThat's also nice, but the memory speed is also higher, Ddr5-7266 vs 5600 iirc. The resulting higher bandwidth translates more or less directly into more performance for the iGPU. reply wmf 14 hours agorootparentprevIt's also faster (7500 vs. 5600). reply userbinator 7 hours agoprevA bit of a disingenious argument intended to sell this as being more revolutionary than it really is --- BGA sockets already exist for LPDDR as well as other things like CPUs/SoCs, but they're very expensive due to low volumes. If the volume went up, they'd go down in price significantly just like LGA sockets for CPUs have. https://www.ironwoodelectronics.com/products/lpddr/ reply kristianp 10 hours agoprevSo this is going into the ThinkPad P1 (Gen 7), which is too expensive and power hungry for my use cases. How long until it filters down into less expensive SKUs? Are we talking next years generation? Ifixit also links to a repair guide: https://www.ifixit.com/Device/Lenovo_ThinkPad_P1_Gen_7 reply PTOB 8 hours agoprevThe current Dell version of this: upgrade to 64GB is $1200. Found this the hard way when trying to get my engineering team what I thought would be a $200 upgrade per machine from their stock 32GB Precision laptop workstations. reply zokier 15 hours agoprevI wonder if this will bring a new widely available high-performance connector to the wider market. SO-DIMM connectors have been occasionally repurposed to other uses, most notably by Raspberry Pi Compute Models 1-3 among other similar SOM/COM boards. RPi CM4 switched to 2x 100pin mezzanine connectors; maybe some future module could use CAMM connectors, I'd imagine they are capable enough reply wmf 14 hours agoparentThe compression connector looks flimsier than a mezzanine so it should probably be a last resort for multi-gigahertz single-ended signaling. reply Dwedit 12 hours agoprevCan it become loose then suddenly not have all pins attached properly? This is something that's unlikely to happen with SODIMM slots, but I've seen so many times when screw receptacles fail. reply farmdve 17 hours agoprevRemember that Haswell laptops were the last to feature socketed CPUs. RAM is nice to upgrade, for sure. As well as an SSD, but CPUs are still a must. I would even suggest upgradeable GPUs but I don't think the money is there for the manufacturers. Why allow you to upgrade when you can buy a whole new laptop? reply zamadatix 17 hours agoparentI'm not sure I really get much value out of a socketed CPU, particularly in a laptop, vs something like a swappable MB+CPU combo where the CPU is not socketed. RAM/Storage are great upgrades because 5 years from now you can pop in 4x the capacity at a bargain since it's the \"old slow type\". CPUs don't really get the same growth in a socket's lifespan. reply farmdve 16 hours agorootparentAs I said to the comment above, it makes perfect sense. In 2014 we purchased a dual core Haswell. Almost a decade later I revive the laptop by installing more ram, an SSD and the best possible quad core CPU for that laptop. The gain in processing power were massive and made the laptop useable again. reply zamadatix 16 hours agorootparentI'm sure it's all subjective (e.g. I'm sure someone here even considers the original dual core Haswell more than fine without upgrade in 2024) but going from a dual core Haswell to a quad core Haswell (or even a generation or two beyond, had it been supported) as an upgrade a decade after the fact just doesn't seem worth it to me. The RAM/SSD sure - a 2 TB consumer SSD wasn't even a possible thing to buy until a year after that laptop would have come out and you can get that for3950x -> 5950x but Intel is better for the particular game I play 90% of the time. - Every time you upgrade, sell the stuff you've upgraded ASAP. If you do this right and never pay above MSRP for parts, you can usually keep running very high-end hardware for minimal TCO. - Buy a great case, ToTL >1000w PSU (Seasonic or be quiet!), and ToTL cooling system (currently on half a dozen 140mm Noctua fans and a Corsair 420mm AIO). This should last at least 3 generations of upgrading the other stuff. - Storage moves more slowly than the rest, and I've had cycles where I've re-used RAM as well, so again here go for the good stuff to maximize perf, but older SSDs work great for home servers or whatever else. - Monitor and other peripherals are outside of the scope of this but should hopefully last at least 3 upgrade generations. I bit when OLED TVs supported 4K 120hz G-Sync, so I've got a 55\" LG G1 that I'm still quite happy with and not wanting to immediately upgrade, though I do wish they made it in a 42\" size, and 16:10 would be just perfect. reply immibis 16 hours agorootparentprevSocket AM4 had a really good run. Maybe we just have to pressure manufacturers to make old-socket variations of modern processors. The technical differences between sockets aren't usually huge. Upgrade the memory standard here, add or remove PCIe lanes there. Using new cores with an older memory controller may or may not be doable, but it's quite simple to not connect all the PCIe lanes the die supports. reply seanp2k2 9 hours agorootparentbut then what excuse would you have to throw another $500 at Asus for their latest board that while being the best chance the platform has, still feels like it runs a beta BIOS for the first 9 months of ownership? reply Night_Thastus 16 hours agoparentprevOn a laptop it's not very practical. Because you can't swap the motherboard, your options for CPUs are going to be quite limited. Generally, only higher-tier CPUs of that same generation - which draw more power and require more cooling. Generally a laptop is built designed to provide a specific budget of power to the CPU and has a limited amount of cooling. Even if you could swap out the CPU, it wouldn't work properly if the laptop couldn't provide the necessary power or cooling. reply farmdve 16 hours agorootparentI can't say I agree. Back in 2014 a laptop was purchased with a dual-core haswell CPU. 8 years later I revive the laptop by upgrading the CPU to almost the best possible CPU, which is a 4-core 8 thread CPU or 4-core 4 threads, I am unsure which of these it was, but the speed boost was massive. This is how you keep old tech alive. And the good thing about mobile CPUs is that they have almost the same TDP across the various dual-quad versions(or whatever is the norm today). reply Rohansi 11 hours agorootparentHow old was the new CPU though? Probably the same or similar generation to what it originally came with since the socket needs to be the same. IMO the switch to an SSD would have been the biggest boost. reply yencabulator 12 hours agorootparentprev> On a laptop it's not very practical. > Because you can't swap the motherboard, https://frame.work/ has entered the chat. reply leduyquang753 17 hours agoparentprevThe Framework laptop 16 features replaceable GPU. reply freedomben 11 hours agorootparentI'm writing this from my Framework 16 with GPU and it is the best laptop I've ever known. It's heavy and big and not the most portable, but I knew that would be the case going into it and I have no regrets reply FloatArtifact 14 hours agorootparentprev> The Framework laptop 16 features replaceable GPU. In a way I don't mind having non-replaceable ram in the framework ecosystem as an option. Put simply because the motherboard itself is modular and needs to be upgraded for the CPU. At that point though I would prefer on integrated ram CPU/GPU. reply farmdve 17 hours agorootparentprevThese are very obscure, or perhaps I mean to say niche laptop manufacturers. We need this standard for all of them, HP, Lenovo, Acer etc. reply nwah1 16 hours agorootparentFramework open sources most of their schematics, if I understand correctly. So it should be possible for others to use the same standard, if they wanted to. (they don't want to) reply nrp 14 hours agorootparentPublished here: https://github.com/FrameworkComputer/ExpansionBay reply Dylan16807 5 hours agorootparentprevThe form factor isn't great for being a vendor-neutral thing. If we can convince the companies to actually try for compatibility, then a revival of MXM is probably a significantly better option. reply seanp2k2 10 hours agoparentprevThey've done upgradeable laptop GPUs before with MXM: https://en.wikipedia.org/wiki/Mobile_PCI_Express_Module Looks like the best card they have out with MXM right now is a Quadro RTX 5000 Mobile which seem to be going for ~$1000 on eBay. reply sojuz151 17 hours agoparentprevI would say it would make the most sense to have a replaceable entire ram+cpu+gpu assemble. Just have some standard form factors and connectors for external connectors. This way, you could keep power consumption low and be able to upgrade cpu to a new generation reply immibis 16 hours agoparentprevLaptops have always been trading size for upgradeability and other factors, and soldering everything is the way to make them tiny. If you ask me they've gotten too extreme in size. The first laptops were way too bulky, but they hit a sweet spot around 2005-2010, being just thick enough to hold all those D-Sub connectors (VGA, serial, etc). And soldering stuff to the board is the default way to make something when upgradeability isn't a feature. reply sharpshadow 13 hours agoprevIs it possible to have both LPDDR and LPCAMM2 in use at the same time? reply wtallis 13 hours agoparentLPCAMM2 is a connector and form factor standard for modules carrying LPDDR type memory chips. reply masklinn 11 hours agorootparentI assume they mean having some memory soldered and an expansion slot. I've seen laptops like that, with e.g. 8GB soldered and a sodimm slot. reply snvzz 7 hours agoprevI see no mention of ECC. It worries me. reply dvh 17 hours agoprevWhat's wrong with DIMM? reply magicalhippo 15 hours agoparentThe physical size of the socket and having the connections on the edge means you're forced to have much longer traces. Longer traces means slower signalling and more power loss due to higher resistance and parasitics. This[1] Anandtech article from last year has a better look at how the LPCAMM module works. Especially note how the connectors are now densely packed directly under the memory chips, significantly reducing the trace length needed. Not just on the memory module itself but also on the motherboard due to the more compact memory module. It also allows for more pins to be connected, thus higher bandwidth (more bits per cycle). [1]: https://www.anandtech.com/show/21069/modular-lpddr-becomes-a... reply kjkjadksj 12 hours agorootparentI'd wager for most consumers capacity is more important than bandwidth and the power losses are going to be small compared to the rest of the stack. reply magicalhippo 5 hours agorootparent> power losses are going to be small compared to the rest of the stack While certainly not the largest losses, they do not appear insignificant. In LPDDDR4 they introduced[1] a new low-voltage signalling, which I doubt they could have gotten working with SODIMMs due to the extra parasitics. If you look at this[2] presentation you can see that at 3200MHz a DDR4 SODIMM would consume around 2 x 16 x 4 x 6.5mW x 3.2GHz = 2.6W for signalling going full tilt. Thanks to the new signalling LPDDR4 reduces this by 40% to around 1.6W. Compare that to a low-power CPU having a TDP of 10W or less a full 1W reduction per SODIMM just due to signalling isn't insignificant. To further put it into perspective, the recent Lenovo ThinkPad X1[3] uses around 4.15W average during normal usage, and that includes the screen. Obviously the memory isn't going full tilt at normal load, but say average 0.25W x 2 sticks would reduce the X1's battery lifetime by 10%. edit: yes I'm aware the presentation is about LPDDR4 yet the X1 uses LPDDR5, just trying add context using available sources. [1]: https://www.jedec.org/news/pressreleases/jedec-releases-lpdd... [2]: https://www.jedec.org/sites/default/files/JY_Choi_Mobile_For... [3]: https://www.tomshardware.com/reviews/lenovo-thinkpad-x1-carb... reply bmicraft 11 hours agorootparentprevBandwidth translates directly into better (igpu) performance reply linsomniac 17 hours agoparentprevIt requires too much power, according to the article. This allows using \"LP\" (Low Power) parts to be removable, they normally have to be soldered on board close to the CPU because of the low voltage tolerances. reply adgjlsfhk1 16 hours agoparentprevOne of the biggest problems is that edge connections don't give you enough density. Edge connections are great for serves where you stack 16 channels next to each other, but in a laptop form factor, your capacity is already limited, so you can get more wires coming out of the ram by connecting to the face rather than the edge. reply 0x457 15 hours agoparentprevThere is literally an entire section explaining why LPDDR needs to be soldered down as close as possible to the memory controller. reply armarr 17 hours agoparentprevLarger footprint, taller, longer traces and signal degradation in the connectors. reply rangerelf 17 hours agoparentprevThere's nothing _wrong_ with it, it performs according to spec, but it has limitations: trace length, power requirements, signal limitations, heat, etc. reply mmastrac 17 hours agoparentprevThe size, the sockets, the heat distribution, etc, etc, etc. reply cryptonector 12 hours agoprevYes please. Also, can we haz ECC? reply seanp2k2 9 hours agoparentWhy are you trying to bankrupt Intel??? Without being able to charge 5x as much for Xeons for ECC support, why would anyone ever pony up for one? reply Tran84jfj 14 hours agoprevI would welcome something like Raspberry Pi compute module, that contains CPU+RAM and communicates with other parts via PCIE. This standard can last decades! Yet another standard for memory will just fail. reply ThinkBeat 16 hours agoprevMeanwhile Apple bakes the RAM,CPU,GPU all into the same \"chip\". Good luck with that. reply 0x457 15 hours agoparentMeanwhile, Apple ships machines with a 1024bit wide memory bus, while this solution offers just 128 bits per \"stick\". reply Dylan16807 5 hours agorootparentCompared to how big the CPU package is on those machines, 4 of these sticks on each side of the motherboard should fit acceptably. And you'd be able to have a lot more than 192GB. reply colinng 15 hours agoparentprevDon’t forget - they solder in the flash too even though there is no technical reason to do so. Unless “impossibly far profit margin” is a technical requirement. reply mschuster91 15 hours agorootparent> Don’t forget - they solder in the flash too even though there is no technical reason to do so. There is, Apple uses flash memory as swap to get away with low RAM specs, and the latency and speed required for that purpose all but necessitates putting the flash memory directly next to the SoC. reply wmf 15 hours agorootparentThis is not really true; Apple's SSDs are no faster than off-the-shelf premium NVMe SSDs. reply wtallis 11 hours agorootparentAnd the latency of flash memory is several orders of magnitude higher than even the slowest interconnect used for internal SSDs. reply Rohansi 11 hours agorootparentprevYeah but some people need to justify their $1,800 USD purchase of laptop that comes with only 8 GB of RAM. Even though most laptops manufactured today would also come with NVMe (PCIe directly connected to the CPU, usually) flash storage, which is used by all operating systems as swap. reply mschuster91 1 minute agorootparentNVMe by no means is directly connected to the CPU directly, usually it's connected through at least one PCIe switch. p0w3n3d 15 hours agoprevApple hates it reply oneplane 16 hours agoprev [–] On the other hand, with a reflow station everything becomes modular and repairable. I do hope that a more widespread usage of compressed attachment gives us some development in that area where projects that were promising modular devices failed (remember those 'modular' phone concepts? available physical interconnects were one of the failures...). Sockets for BGAs have existed for a while, but were not really end-user friendly (not that LGA or PGA are that amazing), so maybe my hope is misplaced and many-contact connections will always be worse than direct attachment (be it PCB or SiP/SoC/CPU shared substrate). reply RetroTechie 14 hours agoparent> maybe my hope is misplaced and many-contact connections will always be worse than direct attachment As much as I like socketed / user-replaceable parts, fact is that soldering down a BGA is a very reliable way to make those many connections. On devices like smartphones & tablets RAM would hardly ever be upgraded even if possible. On laptops most users don't bother. On Raspberry Pi style SBCs it's not doable. Desktops, workstations & servers are the exception here. Basically the high-speed parts of a system need to be as close together as physically possible. Especially if low power consumption is important. Want easy upgrades? Then compute module + carrier board setups might be the way to go. Keep your I/O connectors / display / SSD etc, swap out the CPU/GPU/RAM part. reply jcotton42 16 hours agoparentprev> On the other hand, with a reflow station everything becomes modular and repairable. Not for the average person. reply redeeman 11 hours agorootparenttrue, but can the average person replace the innertube on a bicycle wheel? :) reply pezezin 9 hours agorootparentYes? I did it many, many times as a kid, it is not that difficult. reply zokier 15 hours agoparentprev [–] > On the other hand, with a reflow station everything becomes modular and repairable. until you hit custom undocumented unobtainium proprietary chips. good luck repairing anything with those. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "LPCAMM2 is a new memory standard designed for laptops, featuring modularity, repairability, and upgradeability, using LPDDR chips to allow seamless upgrades and repairs.",
      "It aims to resolve the conflict between serviceability and battery life typically related to soldered LPDDR chips, representing a collaborative effort from tech giants like Dell, Micron, and Lenovo.",
      "The release of LPCAMM2 signifies a progressive move towards sustainable and repairable devices in the tech industry."
    ],
    "commentSummary": [
      "The discussion highlights the introduction of LPCAMM2, a modular and repairable laptop memory standard, addressing soldered RAM issues.",
      "Users are frustrated with overspecified models and high upgrade costs, while manufacturers like Dell and Lenovo see benefits in upgradable RAM for easier repairs and cost-effectiveness.",
      "Debates include the financial impact, practical advantages, future tech developments, and comparisons between unified and traditional memory systems, emphasizing the benefits of socketed components and the value of upgrading laptops with RAM, SSDs, and CPUs."
    ],
    "points": 308,
    "commentCount": 131,
    "retryCount": 0,
    "time": 1715095068
  },
  {
    "id": 40284219,
    "title": "Pyspread: Python-Powered Spreadsheet Solution",
    "originLink": "https://pyspread.gitlab.io/",
    "originBody": "Home Install Docs Contribute API Docs Issues Code Welcome to pyspread pyspread is a non-traditional spreadsheet application that is based on and written in the programming language Python. The goal of pyspread is to be the most pythonic spreadsheet. pyspread expects Python expressions in its grid cells, which makes a spreadsheet specific language obsolete. Each cell returns a Python object that can be accessed from other cells. These objects can represent anything including lists or matrices. pyspread is free software. It is released under the GPL v3 licence. The latest release is pyspread v2.2.3 (source code). It requires Python 3.6+. The last (outdated) release that is compatible with Python 2.x is pyspread v1.1.3. Features Cells accept Python code and return Python objects Access to Python modules from within cells including e.g. NumPy Imports CSV, SVG Exports CSV, SVG, PDF* Cells may display text, markup, images, dates** or charts** Matplotlib and R charts. Plotnine and R packages graphics, lattice, ggplot2 supported via dialog.** Spell checker** git-able pysu save file format blake2b based save file signatures that prevent foreign code execution Notes: * PDF export via print to file, ** requires optional dependencies Target User Group Directly using Python code in a grid is a core feature of pyspread. The target user group has experience with or wants to learn the programming language Python: Clara is a research engineer. She systematically compares results of different parameter sets. She is proficient with Python and has used it for her scientific analyses. Clara wants a quick initial overview of how changing specific parameters affects her results. With pyspread, Clara is displaying each result in one cell. This allows her to quickly figure out, which parameters she is going to focus on next. Peter is using spreadsheets to prepare business decisions. He separates data from code by keeping his business data in CSV files and accessing these files from pyspread. Furthermore, Peter is worried about algorithmic errors and calculation inaccuracies is his complicated tables. He is importing the Money datatype from py-moneyed in the Macro panel. The dedicated Money class helps him avoiding floating point errors. Peter has implemented his analysis tools together with unit tests in a separate Python module, which he is importing as well. Therefore, he is worrying less about wrong results that may lead to bad business decisions. Not part of the target user group are Donna and Jack: Donna is looking for a free replacement for Ms. Excel. She does not know any programming language, and she has no time to learn Python. Jack does computation intensive data analysis. He is looking for a spreadsheet that can parallelize calculations - ideally out of the box as a cluster or a cloud solution. This does not mean that Donna or Jack cannot work with pyspread. However, Donna might find the learning curve for using Python code in cells too steep. Jack on the other hand might be disappointed because he has to ensure manually that his long running tasks are not locking up pyspread. Contact For user questions or user feedback please use the pyspread community board on gitter. For contributions, patches, development discussions and ideas please create an issue using the pyspread issue tracker © Martin Manns and the pyspread team",
    "commentLink": "https://news.ycombinator.com/item?id=40284219",
    "commentBody": "Pyspread – Pythonic Spreadsheet (pyspread.gitlab.io)300 points by Qem 22 hours agohidepastfavorite116 comments justin_oaks 15 hours agoI'd like to give praise for the \"Target User Group\" section on the homepage. Not only does it say what users the app is for, but also who it is NOT for. I think this kind of information is invaluable in deciding whether or not to use or suggest an app. I can understand if app developers want EVERYBODY to user their app (whether or not its the best for the job) or if the app developer just doesn't want to take the time to write out who the app is NOT for. But I will praise those who do include that information. reply okigan 10 hours agoparentAfter reading the parent comment - I thought to myself “so what”. But after reading it on the product page - I fully agree: seeing clearly the targeted personas and out of scope usage significantly elevates my trust in the product and the team behind it. Have not used the software, but now I want to try reply TheCleric 13 hours agoparentprevI like it and I'll take it a step further. I think this is important information for a developer to admit to THEMSELVES at least. Being all things to all people is a route straight to burnout for an open source project. reply worldwidelies 6 hours agoparentprevI found a typo in the \"Target User Group\" section. Original Sentence: > Furthermore, Peter is worried about algorithmic errors and calculation inaccuracies is his complicated tables. Correction: > Furthermore, Peter is worried about algorithmic errors and calculation inaccuracies in his complicated tables. reply behnamoh 12 hours agoparentprevAnd yet, it is not available on macOS, presumably THE platform where that target user group lives. reply jhbadger 5 hours agorootparentpip install pyspread works just fine on macOS (assuming you have pip and python installed, which anyone who would be interested in having a python-enabled spreadsheet would have). reply hanche 2 hours agorootparentprevNot quite true, as others have pointed out. However, the installation instructions say this: > While there have been reports that pyspread can be used on OS X, OS X is currently unsupported (can you help?). I think that would make many non-technical users uncomfortable. reply yazzku 8 hours agorootparentprevAnd what makes you think that? reply tazu 7 hours agorootparentI wish Stackoverflow included \"daily operating system\" in their surveys, but I'd wager that 50%+ of professional developers use MacOS. For many reasons probably, but for me it's simply because it's a nice BSD system that translates well to Linux servers (prod). reply throwaway2037 7 hours agorootparent> but I'd wager that 50%+ of professional developers use MacOS Ok, I will take that bet against you. If you think it true, then you are living in a bubble. Probably 90% use MS Windows with no choice, because they work in a big enterprise that uses MS Office and Exchange/Outlook. For enterprise email, is there any competition at this point? I don't know any. reply tazu 4 hours agorootparent> If you think it true, then you are living in a bubble. This is probably true. I only really talk to US startup devs. reply throwaway2037 53 minutes agorootparentIn that case, I would take your side of the bet! US startup devs appears (from HN) to have pretty liberal hardware policies. If choosing a laptop, many will choose the best hardware, which is currently a MacBook. reply WillAdams 20 hours agoprevI would really like to see a distribution which puts all the best alternative software together: - pyspread for a spreadsheet - LyX for a word-processor - OpenSCAD for a 3D modeler - TkzEdt (or ipe) for 2D drawing &c. (and I'd be interested in suggestions for similar software for other tasks, esp. presentations and database work) reply LeifCarrotson 19 hours agoparentKiCad for electronics development. Blender for 3D graphics. Gimp for photo editing. Inkscape for illustration. Though I'd recommend FreeCAD over OpenSCAD as a 3D modeling tool for most users as an alternative to traditional CAD tools...and therein lies the distro problem. reply johnmaguire 16 hours agorootparentGimp is really not great for photo editing IMO - it really shines at photo manipulation. (i.e. it is Photoshop, not Lightroom.) For RAW development (a la Lightroom), DarkTable and RawTherapee exist. I've only tried the former, and found it incredibly difficult to use. I also prefer Krita for digital illustration (though maybe that's unfair - it's best at digital painting specifically.) Of course, if you need vector support, Inkscape is the obvious answer. reply WillAdams 19 hours agorootparentprevGood points. I was trying to think of unusual programs with non-standard approaches. Perhaps rather than FreeCAD either BRL-CAD, or maybe the recently announced Dune 3D: https://news.ycombinator.com/item?id=40228068 Also, Blender w/ CADsketcher (the Solvespace solver) is quite good: https://news.ycombinator.com/item?id=34856383 Though maybe Solvespace would be a better match? https://news.ycombinator.com/item?id=33571555 reply synergy20 18 hours agorootparentprevdrawio for diagrams reply diath 17 hours agoparentprevBut why do you even need a distro to begin with just to ship certain software set? Install Arch/Gentoo and install whatever is your preferred software of choice, or hell, you can even do that on any other distro. reply comte7092 16 hours agorootparentThe value a dedicated distro provides here is that you don’t have to do the legwork to research and find the best tools. As power users we may want to do that ourselves, but a lot of people place value in having that curation done for them. reply WillAdams 17 hours agorootparentprevI think it would be an interesting thing for a distro to market/focus on, and it might help to find/identify/encourage additional such software. reply everforward 14 hours agorootparentIsn’t that basically package groups (or whatever the district-specific terms are)? Eg Im pretty sure Ubuntu Desktop has some kind of “Productivity” package group that includes a word processor and spreadsheets and an email client and what not. I’m pretty sure it’s selected by default when you do a full desktop install. I don’t recall what the actual software is, but I would imagine LibreOffice. I would agree with OP that it doesn’t really make sense for a distro, though. People really want to “make a distro” for some reason so we end up with silly shit like Kubuntu (Ubuntu… with KDE pre-installed). My general rule of thumb is if I can point the distro’s OS package manager to the distro’s upstream (ie Ubuntu for Kubuntu, or Debian for Ubuntu) and everything works or mostly works, it should be a script or apt repo and not a distro. There are way too many “Ubuntu but with a different default DE” distros that could really just be a modified install ISO or post-install script. reply cryptonector 10 hours agorootparentprevA pkg group is enough. reply TheCleric 13 hours agoparentprevI tend to use AlternativeTo for that. For example: https://alternativeto.net/software/pyspread/ reply throwaway2037 7 hours agoparentprevDebian should do. reply heggy 19 hours agoparentprevWhen would you take OpenSCAD over Blender? reply gibspaulding 19 hours agorootparentOpenSCAD is really good for parametric designs. One of the first things I designed in OpenSCAD was a bicycle sprocket where you could input how many teeth you wanted, link length, how thick, etc. and OpenSCAD would generate the sprocket according to those parameters. I recall seeing a while back that blender was adding support for this kind of thing so I'm sure it's possible, but it was very intuitive in OpenSCAD. For any large project I'm sure Blender (or FreeCAD) would be a better choice, but as someone with some programming background just starting out, OpenSCAD feels way more accessible. reply WillAdams 18 hours agorootparentI've tried a lot of 3D software over the years, and OpenSCAD has been the one I've been most successful with (along w/ Carbide Create, but I work for that company....) Currently working on a library which makes the two work together: https://github.com/WillAdams/gcodepreview (which is currently quite primitive/basic, and even when fully developed is not likely to be used by anyone else) reply bitdivision 19 hours agorootparentprevBlender wasn't originally designed to be used for CAD type applications. It's possible of course, but you're likely better off with something specifically meant for precision CAD models. Specifically for 3d printing, I don't think blender is ideal. I think most programmers like OpenSCAD - what better way to make parametric CAD models than by writing code? reply giancarlostoro 19 hours agorootparentprevMy understanding is that with CAD software you can go backwards and have (near?) infinite edit history, in a way you cannot with Blender. Which makes it very, very useful. reply spott 19 hours agorootparentA lot of cad software actually has an editable history: you can go back 100 steps and modify a dimension or a step and have the rest of the model updated to take that new dimension into account. I wouldn’t work with any cad software that didn’t have that ability. reply constantcrying 15 hours agorootparentprevThis is called parametric design. Since CAD programs do not operate on meshes, this allows you to e.g. change the dimension on some part and have the rest adjust accordingly. How well this works depends on the situation, if changing a parameter causes new faces to appear on the object this is usauayvwry difficult to handle, even for commercial CAD programs. reply constantcrying 16 hours agorootparentprevBlender can't do CAD. At least it can't do what you would want to do in a CAD program. The most important difference is that Blender operates on meshes, CAD programs don't. reply stainablesteel 8 hours agoparentprevanything that replaces powerpoint? reply dang 17 hours agoprevRelated. Others? Pyspread – Spreadsheet with deep Python integration - https://news.ycombinator.com/item?id=30426053 - Feb 2022 (1 comment) Pyspread – Spreadsheet implementation in Python, cells return Python objects - https://news.ycombinator.com/item?id=7593603 - April 2014 (1 comment) Spreadsheets using Python - Have you seen this? - https://news.ycombinator.com/item?id=1884896 - Nov 2010 (47 comments) PySpread: A spreadsheet that accepts a pure python expressions - https://news.ycombinator.com/item?id=265132 - Aug 2008 (3 comments) reply ssl232 21 hours agoprevI see it uses numerical designations for both the columns and rows allowing indexing like a 2D matrix. Nice. One of the many annoyances of Excel is the alphabetic columns that make even less sense beyond 26. reply tichiian 21 hours agoparentThat A1 mode is Excel \"Baby-Mode\", you can switch to R1C1 mode in settings. Things are far easier then, and more sane. His Excel-lence also recommends this: https://www.youtube.com/watch?v=JxBg4sMusIg reply falcor84 19 hours agorootparentBaby mode?! As a full grown adult, I find the cognitive load of the base 26 arithmetic of \"what column is an offset of 8 to the right of AW\" to be exactly what I needed to fully forget about what I came there to do in the first place reply epcoa 19 hours agorootparentBaby’s lack object persistence and a formed theory of mind, seems to check out. reply mhh__ 15 hours agorootparentprevNever seen anyone actually use this. The \"real\" answer is to use names and tables. That and generally not having data flying up down left and right. reply steine65 15 hours agorootparentAgreed! Hard cell references should rarely be used. Names for metadata variables (eg. Current_Month) and tables for datasets. reply tichiian 11 hours agorootparentYes. But many parts of excel don't support those. E.g. conditional formatting just doesn't deal with names. Also, when you need names, it is a sign to reach for a proper programming enviroment like Delphi or Lazarus. reply cyanydeez 9 hours agoparentprevsome of us have some logical dyslexia, so when I see a tuple of numerics, even if you say (this is the row, this is the column, this is the table), when I'm manipulating references, or finding a tuple in the dark, I easily mix up the order of the bits, because lets face it, it's arbitrary. So, while your logic makes sense, it voids humanity in a rather stark way which I suppose is fine, but it's not like I'm incapable of programming. I just have to be careful about how I name variables and use keywords in python more often than args. reply dfox 20 hours agoprevIt seems to not be a true spreadsheet with dependency tracking and instead it recomputes everything after each change. Whis probably kind-of obvious as how to track the dependencies for arbitrary Python code is not that obvious. But without resorting to some static analysis magic one could just track accesses to the magic S[] during the excecution of the cell, its value cannot depend on anything else than what was accessed during the evaluation. reply movpasd 19 hours agoparentOne issue with this is that arbitrary Python code can have arbitrary side-effects. Your suggestion reminds me a lot of fine-grained reactivity like in SolidJS, which makes sense, since spreadsheets basically operate on reactive programming. Some great articles by Ryan Carnatio on the topic. The side-effects thing comes in if a user puts in some side-effect in a dependent cell, which is equivalent to adding side-effects in a memo in reactive-speak. reply dgacmu 19 hours agoparentprevThat gets broken by constructs like sum(all of column 5) when new rows or columns are added. (It's very similar to the problem of locking in databases) reply cmcconomy 19 hours agoparentprevwhat if you wrap all your cell contents with a memoize reply mewpmewp2 19 hours agorootparentOr maybe worst case allow user to add deps like in React useEffect. reply pasc1878 3 hours agoprevThere used to be a python spreadsheet that worked as a spreadsheet - Resolver One. It was .Net based but did not make enough money. I wonder if done now there would be enough support to open source something like this. For a review see https://blog.jonudell.net/2007/09/27/first-look-at-resolver-... reply JosephRedfern 13 hours agoprevInterestingly, Giles Thomas (the guy behind Python Anywhere) originally developed a Pythonic Spreadsheet application, called Resolver One, and then iterated to a a web-based version, called Project Dirigible. Python Anywhere eventually span out from Project Dirigible when they realised that it was mostly being used for \"generic\" Python development rather than being used as a spreadsheet. There's an interesting write-up on this evolution here: https://blog.pythonanywhere.com/197/, and a demo of Dirigible here: https://www.youtube.com/watch?v=2ZoIp-5NaiQ. Project dirigible ended up being open sourced here: https://github.com/pythonanywhere/dirigible-spreadsheet. reply brudgers 19 hours agoprevRelated? Python in Excel: https://support.microsoft.com/en-us/office/get-started-with-... reply ptx 16 hours agoparentNot actually \"in Excel\", though. The Python code runs on Microsoft's servers (they say in the introduction) and Excel is just a client. There's no reason they couldn't embed CPython in Excel, but maybe the intention was for the online version of Excel to have feature parity without having to compile Python to JavaScript? reply cyanydeez 9 hours agorootparentthe intention is to lock in orgs to their cloud services. This is a value-add. They really know that Excel, Word are \"feature complete\" and the only way they're going to make money on it is by harvesting and locking in the users. reply mhh__ 15 hours agoparentprevAwful. They don't seem to understand excel anymore. If you want to see this done properly (on some limit) look at pyxll. reply oulipo 21 hours agoprevI guess it would be \"easy\" to compile to WASM using pyodide or such, and have a full in-browser version? might be cool reply codingglass 20 hours agoparentLooks like the interface/windowing is built on Qt/PyQt. QT does have a WebAssembly build, but I don't think it's all that simple of a transition. reply breckognize 13 hours agoprevShameless plug: If you have bigger data sets, check out https://rowzero.io We scale up to hundreds of millions of rows and have native Python support. You can define functions in Python and call them as formulas from any spreadsheet cell. We seamlessly marshal Pandas dataframes from Python land to spreadsheet land and back. [1] We're also hosted and support real time collaboration like Google Sheets. We reimplemented the Excel formula language. We connect directly to Postgres, S3, Snowflake, Redshift, and Databricks. And the first workbook is free. [1] https://rowzero.io/docs/code-window reply blagie 9 hours agoparentShameless suggestion: Instead of one free workbook, make unlimited free workbooks, but unusable for corporate settings. I'd recommend binary sharing: - Only I have access (private / personal use) - The universe has access (open-source use) But one free workbook still beats the typical 30-day trial. reply regularfry 20 hours agoprevShame it's GPL3. That counts it out of being included in FreeCAD, which I can't help but feel would be an improvement on the current spreadsheet workbench. reply cognomano 6 hours agoprev> Execution order between cells is not guaranteed to be stable and may differ for different versions of Python. reply golem14 13 hours agoprevI'm curious how it compares to emacs' calc / spreadsheet mode in org. Looks like it is a) a stand-alone X11 app (not easily runnable on macos) b) using python instead of elisp. Is that roughly accurate ? I can't easily understand from the post. reply zero-sharp 21 hours agoprevI wonder what the performance is like? reply frognumber 21 hours agoprevThis looks a very good idea. This would be an ideal place for a semi-technical user to contribute documentation. Good starter open-source contribution. Without that, it's hard to figure out if it would work for me, let alone give feedback. reply sevagh 21 hours agoparentEvery dev here loves throwing unglamorous volunteer work to non-developers, acting as if it's a favor. reply JulianChastain 20 hours agorootparentSome of the motivation for this comes from how often devs want to contribute to open source but are intimidated by how difficult the barrier of entry is, particularly for large projects. It's surprisingly hard to find a good list of projects that a beginner or even intermediate programmer can substantially contribute to. The ones that do exist tend to have the low hanging fruit plucked pretty quickly. reply frognumber 12 hours agorootparentA few points: - Most of my career was made by being the author of one popular open source platform which happened to do well. - I've recruited people based on open-source contributions. If I want an expert in [X], finding someone who contributed to [X] is a good bet. - The flip side is I've made (minor, helpful) contributions to many projects in part for exposure. My name is in the commit list of many systems in domains where I have wanted to work. - Many mid-sized contributions look good on a resume, especially for a junior developer. Indeed, I've made one case to promote someone based, in part, on contributing to a library we were using (even if only tangentially). If you want a job in e.g. network security, find something in a firewall, anonymzing proxy, packet sniffer, or whatnot, and make a PR. It's often quick, easy, and helpful. A corollary is you do actually learn a lot about a system by contributing. I have no axe to grind here, but I think the cynicism is unwarranted. reply sevagh 11 hours agorootparentI love open source. My cynicism isn't about open-source, but about the OP's first post being \"these docs suck, snaps fingers maybe one of you non-devs can work on it.\" reply frognumber 11 hours agorootparentGood life lesson: You don't know until you ask. That's more a salesperson mantra than SWE, where for every 10-100 people you ask, someone buys something. However, I've raised money many times simply by shamelessly asking. Second good life lesson: Don't assume things about others. People who make $1/day, $10/day, $100/day, $1000/day, $10,000/day, etc. have fundamentally different priorities and motivations. Ditto on many other axes. Good synergies are leveraged working across such differences. reply blitzar 21 hours agorootparentprev\"You will be paid in exposure\" - this is where the venn diagram for devs & social media influencers intersect. reply jjmarr 20 hours agorootparentExcept when you contribute to open-source, you typically have a legal document (the licence) that explains how the exposure will be paid. reply psychoslave 21 hours agorootparentprevSo what, who want raw wild naked large exposure? If you don’t get an army of free PR specialists, lawyers and body guards to protect you and your beloved ones h24 every single day for the rest your lives, it’s an obviously net negative situation. reply resource_waste 19 hours agorootparentprev>Every dev here loves doing volunteer work as a favor. Devs are expected to contribute to FOSS, write free educational blog posts about technology, and fix your friends computer! Ask a doctor why something hurts and they tell you to come into their office and insurance will bill you. reply sevagh 16 hours agorootparentThis sounds made up tbh. I mean, I agree it's common to feel this \"pressure\" but the day you discover the pressure isn't real is the day you level up. And I charge plenty for fixing peoples' software pains. I do open-source and write blog posts to satisfy my own desire to publicize my work, and for exposure, and to put my skills on display. Not as a race to the bottom. I could have had a career without those things. reply photochemsyn 20 hours agoparentprevpandas has a good documentation model for installation and usage: https://pandas.pydata.org/docs/getting_started/install.html https://pandas.pydata.org/docs/user_guide/10min.html reply tichiian 21 hours agoprevPython is a bad language for this, because one-liners are awkward. Multiline code is possible but ugly due to indentation-based syntax. Brace-based languages would be far more suitable here. reply toss1 18 hours agoparentAre you suggesting a flavor of Lisp? Because THAT could be very nice in a cell-format. reply hodapp 15 hours agorootparentThere is SIAG (Scheme in a Grid) that has been around for ages: https://siag.nu/siag/ reply behnamoh 12 hours agorootparentI like Lisp but why are almost ALL Lisp-related websites so ugly? They still have the 90s look and feel. reply Kim_Bruning 19 hours agoprevnix run nixpkgs#pyspread Not saying Nix(os) Is The Way, but sometimes it does ok. reply ingenieroariel 19 hours agoparentI did not believe you and just typed it on OSX, half a minute later the app was ready for me to use. nix run nixpkgs#pyspread [0/1 built, 3/113/132 copied (1311.8/1721.6 MiB), 280.4/300.7 MiB DL] fetching llvm-16.0.6 from https://cache.nixos.org https://pasteboard.co/P1eh7B7W8C9R.png reply just_testing 10 hours agoparentprevI just downloaded nixos for WSL2. Way easier than I thought. Thank you! reply DrNosferatu 21 hours agoprevI couldn't understand what's the datatype of each cell. - How to use NumPy and others? reply kwhitefoot 21 hours agoprevI tried to install, and discovered yet again why I fell out of love with Python. Executing: pip3 install -r requirements.txt gave this error: \" Traceback (most recent call last): File \"/tmp/tmprhazpypn\", line 126, in prepare_metadata_for_build_wheel hook = backend.prepare_metadata_for_build_wheel AttributeError: module 'sipbuild.api' has no attribute 'prepare_metadata_for_build_wheel' ... ERROR: Command errored out with exit status 1: /usr/bin/python3 /tmp/tmprhazpypn prepare_metadata_for_build_wheel /tmp/tmpl72s0sfd Check the logs for full command output.\" I'm on Linux Mint 19. reply sorenjan 20 hours agoparentUse pipx to install applications instead. It even works on Windows. > pipx install pyspread installed package pyspread 2.2.3, installed using Python 3.12.3 These apps are now globally available - pyspread.exe done! reply kreddor 21 hours agoparentprevI tried to install it on ubuntu 22.04 and got a different error using pip. I'm not experienced enough with Python to quickly figure out how to proceed. It's a shame it isn't easier to install. reply ceving 19 hours agoparentprevDebian has it out of the box: $ LANG=C apt-cache policy pyspread pyspread: Installed: 2.2.3-1 Candidate: 2.2.3-1 Version table: *** 2.2.3-1 500 500 http://ftp.de.debian.org/debian trixie/main amd64 Packages 500 http://ftp.de.debian.org/debian trixie/main i386 Packages 100 /var/lib/dpkg/status reply kwhitefoot 17 hours agorootparentThank you! reply pasc1878 13 hours agoparentprevCompare this with the comments above re nix where it just ran - although to get a python app to build on nix is a pain but at least only one person has to do it. reply globular-toast 16 hours agoparentprevYou're not really supposed to pip install applications. pip is a developer tool. There is pipx which is great, but I still think it's a developer tool. It's annoying when projects say to pip install stuff. It's never the right thing to do. reply Qem 21 hours agoparentprevUnder Fedora 39 I was able to pip install and run it withhout problems, inside a venv environment: (pyspread) [xxxx@fedora ~]$ pip install pyspread Collecting pyspread Obtaining dependency information for pyspread from https://files.pythonhosted.org/packages/a5/e2/19ddb20b46ae46f11102f9095bf5bf00cb28cc79b35f38257f84a98ecd1d/pyspread-2.2.3-py3-none-any.whl.metadata Downloading pyspread-2.2.3-py3-none-any.whl.metadata (4.8 kB) Collecting PyQt5>=5.10 (from pyspread) Obtaining dependency information for PyQt5>=5.10 from https://files.pythonhosted.org/packages/2f/e6/a1f9853e4933c312c6de9c79d126c7d92ef69ae0e53895fb1ceb0ecc77a6/PyQt5-5.15.10-cp37-abi3-manylinux_2_17_x86_64.whl.metadata Downloading PyQt5-5.15.10-cp37-abi3-manylinux_2_17_x86_64.whl.metadata (2.1 kB) Collecting markdown2>=2.3 (from pyspread) Obtaining dependency information for markdown2>=2.3 from https://files.pythonhosted.org/packages/5a/09/a9ef8d5fe4b08bfd0dd133084deefcffc4b2a37a9ca35a22b48622d59262/markdown2-2.4.13-py2.py3-none-any.whl.metadata Downloading markdown2-2.4.13-py2.py3-none-any.whl.metadata (2.0 kB) Collecting numpy>=1.1 (from pyspread) Obtaining dependency information for numpy>=1.1 from https://files.pythonhosted.org/packages/0f/50/de23fde84e45f5c4fda2488c759b69990fd4512387a8632860f3ac9cd225/numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB) ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 61.0/61.0 kB 311.8 kB/s eta 0:00:00 Collecting setuptools>=40.0 (from pyspread) Obtaining dependency information for setuptools>=40.0 from https://files.pythonhosted.org/packages/f7/29/13965af254e3373bceae8fb9a0e6ea0d0e571171b80d6646932131d6439b/setuptools-69.5.1-py3-none-any.whl.metadata Downloading setuptools-69.5.1-py3-none-any.whl.metadata (6.2 kB) Collecting PyQt5-sip=12.13 (from PyQt5>=5.10->pyspread) Obtaining dependency information for PyQt5-sip=12.13 from https://files.pythonhosted.org/packages/3c/ab/f8f1e970768fcb4ab118d4aabbfcb9b7f781088b71e1f26d813fd51c4701/PyQt5_sip-12.13.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata Downloading PyQt5_sip-12.13.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata (504 bytes) Collecting PyQt5-Qt5>=5.15.2 (from PyQt5>=5.10->pyspread) Obtaining dependency information for PyQt5-Qt5>=5.15.2 from https://files.pythonhosted.org/packages/83/d4/241a6a518d0bcf0a9fcdcbad5edfed18d43e884317eab8d5230a2b27e206/PyQt5_Qt5-5.15.2-py3-none-manylinux2014_x86_64.whl.metadata Downloading PyQt5_Qt5-5.15.2-py3-none-manylinux2014_x86_64.whl.metadata (535 bytes) Downloading pyspread-2.2.3-py3-none-any.whl (1.7 MB) ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.7/1.7 MB 2.7 MB/s eta 0:00:00 Downloading markdown2-2.4.13-py2.py3-none-any.whl (41 kB) ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 41.3/41.3 kB 169.4 kB/s eta 0:00:00 Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB) ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 18.0/18.0 MB 4.1 MB/s eta 0:00:00 Downloading PyQt5-5.15.10-cp37-abi3-manylinux_2_17_x86_64.whl (8.2 MB) ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 8.2/8.2 MB 4.8 MB/s eta 0:00:00 Downloading setuptools-69.5.1-py3-none-any.whl (894 kB) ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 894.6/894.6 kB 3.1 MB/s eta 0:00:00 Downloading PyQt5_Qt5-5.15.2-py3-none-manylinux2014_x86_64.whl (59.9 MB) ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 59.9/59.9 MB 1.8 MB/s eta 0:00:00 Downloading PyQt5_sip-12.13.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.whl (360 kB) ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 361.0/361.0 kB 2.0 MB/s eta 0:00:00 Installing collected packages: PyQt5-Qt5, setuptools, PyQt5-sip, numpy, markdown2, PyQt5, pyspread Successfully installed PyQt5-5.15.10 PyQt5-Qt5-5.15.2 PyQt5-sip-12.13.0 markdown2-2.4.13 numpy-1.26.4 pyspread-2.2.3 setuptools-69.5.1 [notice] A new release of pip is available: 23.2.1 -> 24.0 [notice] To update, run: pip install --upgrade pip (pyspread) [xxxx@fedora ~]$ python3 -m pyspread reply resource_waste 19 hours agoparentprevStop using debian-family. Dealing with outdated/bugs is just a regular day under the guise of 'stable'. The only reason you even are using Mint is because conical did a marketing trick sending free CDs in 2000s. reply amias 21 hours agoparentprevstop trying to use the system python for application development. You will break your OS and you will make your code machine specific. Go read up on Virtual Envs , you were the problem here. reply evilduck 20 hours agorootparentPython has like a dozen competing solutions in their package management space. Python is clearly the problem, not the user. Their docs don’t say that, they advertise exactly what the GP ran. Python needs to unfuck its ecosystem. reply smaudet 19 hours agorootparentYeah they kinda screwed the pooch on that. That being said, venv is the official solution, and it works fairly well. pipenv/pipx are some alternatives...but yeah. What exactly is your system installation for if you aren't supposed to install anything to it? The package system should be able to attempt installing via venv or be able to block uncompatible packages with sane error messages. reply sensen7 16 hours agorootparentprevIf venvs were a reasonable solution here, presumably the pyspread authors would have included them in the installation instructions. Instead they just say to \"pip install\", which predictably does not work for most people. It sounds a bit unfair to tell someone that \"they're the problem\" when they followed the official installation instructions to the letter. I would agree in classifying this as another case of python developers being unable to provide a working way to install their software (if code that the average person can't even get to execute can even be called \"software\".) reply tupolef 20 hours agorootparentprevAnd for the lazy ones, pipx uses Virtual Envs and can install most things from a path, an archive or a package name. Install pipx and check pipx install --help. reply kwhitefoot 18 hours agorootparentprevWhy is this directed at me? I'm not the one doing the software development, I was just trying to install an application. reply adolph 19 hours agorootparentprevvenv not always a panacea for system python $ docker run --rm -it python bash [...] root@211646f0fa99:/# which python /usr/local/bin/python root@211646f0fa99:/# python -m venv .venv root@211646f0fa99:/# source .venv/bin/activate (.venv) root@211646f0fa99:/# which python /.venv/bin/python (.venv) root@211646f0fa99:/# ls -l \"$(which python)\" lrwxrwxrwx 1 root root 21 May 7 14:47 /.venv/bin/python -> /usr/local/bin/python reply metadat 17 hours agorootparentCorrect, virtualenv isolates package installation but doesn't handle installing or managing arbitrary Python versions. Node.js is similar with npm. Is there something like nvm but for Python? reply djd20 16 hours agorootparentTry pyenv - very handy, includes funkier editions like pypy reply UncleEntity 17 hours agorootparentprev> stop trying to use the system python for application development. Umm... who exactly makes sure the app runs correctly with the version of python installed by the system? I, for one, aren't going to bump up a python version to run a single app in a virtual environment because some dev decided to use 'python nightly' to code against. reply amias 21 hours agoprevthis seems like a huge reinvention of jupyiter , i really hope they aren't going to recreate all the bugs. reply cdrini 20 hours agoparentI think one major difference between spreadsheets and Jupyter notebooks is state/dependency graph. In a notebook, cells don't depend on each other. They can be run many times, and the order they're run in matters. This is one of the major gotchas with notebooks. With spreadsheets, they're generally deterministic. There is no notion of \"cell order\", cells are recomputed automatically depending on the dependency graph between the cells. This results in a pretty big user experience difference. reply ziml77 19 hours agorootparentBut according to other comments here, that's unfortunately not how this spreadsheet program works. reply cdrini 16 hours agorootparentYes in that it doesn't appear to use a dependency graph to track recomputes; it recomputes everything on a cell change (according to another commenter anyways). But the UX effect is still the same in that the user never has to think about cell order, and every cell is always consistent with each other. reply Aliyekta 13 hours agoprevthe api page doesn't seem to work. reply rbanffy 21 hours agoprevThe Gods have listened to me! reply adolfopd 20 hours agoprevEneble number line to 2^25 or minimal 2^22 by default. Kill many user from Excel limitation 2^20!! reply voxelghost 21 hours agoprevI am Jacks endless disappointment. j/k - it looks nice - how would you compare/position it against something like jupyter lab? reply fab13n 21 hours agoparentthe magic power of spreadsheets is that they encourage improvisation, and it probably applies to that one. you have only one data structure (the 2D table), data types are super-weak, there are no variable names... all of this guarantee a maintenance nightmare, and rightfully scares developers. But it's also a very low barrier to entry. You've got data, you paste them into the grid, and you start toying with them, before having figured anything about them. That's an amazing superpower, when targeting non-developers, and that's why Excel is the most used programming language over the world, by far: it's probably got an order of magnitude more users than there are trained developers in the world. reply WillAdams 20 hours agorootparentand _that_ is why I'm still very sad that Lotus Improv didn't make it in the marketplace --- gathering all the formulae into one pane was _incredible_ for organization and providing a single top-level view of what a spreadsheet was doing. I really wish Flexisheet would get to a usable point, or that Quantrix wasn't so expensive. reply metadat 17 hours agoparentprevWhat is the meme behind this reference: \"I am Jack's endless disappointment\"? I searched the web but still unclear. reply fishyjoe 16 hours agorootparenthttps://www.quora.com/What-does-Im-Jacks-complete-lack-of-su... reply voxelghost 5 hours agorootparentThis is of course the correct answer to the question. But additionally, I made the joke because I somewhat Identified with the 'Jack' use case mentioned in one of the pages on the PySpreads site reply eigenket 18 hours agoprev [–] I'm also interested in the least pythonic spreadsheet, has anyone worked on that? reply voxelghost 5 hours agoparent [–] I heard microsoft made an attempt reply eigenket 32 minutes agorootparent [–] That was the joke, I guess it wasn't appreciated ;) reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Pyspread is a Python-based spreadsheet application enabling users to input Python expressions in cells, eliminating the need for a specific spreadsheet language.",
      "It is open source, offering features like Python module access, data export, and versatile content display, targeting Python-savvy individuals like researchers and business analysts.",
      "Users like Clara and Peter benefit from integrating Python code for data analysis, while those without programming experience, like Donna and Jack, or requiring cluster computing may find pyspread less suitable."
    ],
    "commentSummary": [
      "The importance of defining target users and out-of-scope usage for software like Pyspread is discussed.",
      "Challenges and solutions for installing and running Pyspread in different operating systems are addressed.",
      "The conversation covers the value of curated software in Linux distributions, potential impact of open-source contributions, user experience, and limitations of spreadsheets like Excel."
    ],
    "points": 301,
    "commentCount": 116,
    "retryCount": 0,
    "time": 1715080136
  },
  {
    "id": 40285211,
    "title": "Revitalizing Downtowns: The Trend of Converting Office Towers into Apartments",
    "originLink": "https://www.newyorker.com/magazine/2024/05/06/can-turning-office-towers-into-apartments-save-downtowns",
    "originBody": "Our Local Correspondents Can Turning Office Towers Into Apartments Save Downtowns? Nathan Berman has helped rescue Manhattan’s financial district from a “doom loop” by carving attractive living spaces from hulking buildings that once housed fields of cubicles. By D. T. Max April 29, 2024 Berman, through his firm, Metro Loft Management, has added some five thousand units to New York City’s housing stock. His company has just announced its largest conversion yet: Pfizer’s former headquarters, on East Forty-second Street, will be refashioned to house thirteen hundred apartments.Photo illustration by Josh Cochran; Source photograph by Klaus Vedfelt / Getty There are about a thousand real-estate developers in New York City. Nathan Berman is one of them, and he’s become rich doing it. But, he told me recently, “I never built a building from scratch, and never wanted to.” Instead, Berman, who is sixty-four, specializes in taking existing structures and converting them into apartments, a useful trick in a city that’s always starved for housing—and newly wary of the five-day-a-week office routine. In 2017, he converted 443 Greenwich Street, a former warehouse and book bindery in Tribeca, built in 1883, into a luxury condo; among the celebrities who now own apartments there are Harry Styles and Jake Gyllenhaal. (The building was designed to be “paparazzi-proof,” so it features an underground parking area with a valet.) It’s not much of a feat, though, to redo an industrial space that has a rudimentary interior. Berman is more excited by the transformation of huge, obsolete office towers into warrens of one- and two-bedroom apartments. He compares the effort to extract as much residential rental space as possible out of such buildings to solving a Rubik’s Cube. Since 1997, Berman, through his firm, Metro Loft Management, has turned eight Manhattan office towers into rental-apartment complexes, adding some five thousand units to the city’s housing stock. His company has just signed a contract for the largest conversion yet in the United States: Pfizer’s former headquarters, on East Forty-second Street, will be refashioned to house about fifteen hundred apartments. Berman has no patience for nostalgia. “You’re tearing down something that simply doesn’t work anymore,” he explained. Although Metro Loft has offices at 40 Wall Street, Berman often works at home himself, on the Upper East Side. He happily spends hours poring over blueprints, dividing former fields of cubicles into small but clever residences and reconceiving onetime copy-machine nooks as mini laundry rooms or skinny kitchens. All his apartments are market-rate properties, so what he creates is élite but ordinary, luxurious but cramped, permanent but marginal. Avinash Malhotra, an architect who has done several conversions with Berman, noted that a single office tower can be carved up into hundreds of little units, as in a hotel. “He is not making housing for the homeless,” Malhotra said. “But I often joke among my employees that what we do is slums for the rich.” One day in December, I went to the financial district and joined Berman in the stark white lobby of 55 Broad Street, a thirty-story former office tower that was built in 1967 by Emery Roth & Sons. Berman has started converting it into five hundred and seventy-one apartments, many of them studios aimed at professionals just out of college. Scaffolding surrounded the bottom of the tower, imprisoning a Starbucks by the entrance. Berman dresses to understated effect. He wore a quiet-luxury ensemble—unzipped Brunello Cucinelli vest, Loro Piana sweater, John Lobb shoes—and carried nothing in his hands but his phone. He was overhauling 55 Broad Street under complicated conditions: it still had office tenants inside. The day we visited, five of the floors were still occupied by companies that had not yet left. (One, a property-management outfit called Solstice Residential Group, even sued to stay, but ultimately settled and moved nearby.) Every so often, an office worker rushed through the lobby, looking as lonely as a ghost. The entrance was renovated twenty years ago by the building’s original owners—the Rudin family, a New York real-estate powerhouse—and featured a revolving door, white marble walls, harsh Kubrickian lighting, and a long security credenza. Berman said that he would put in a hinged door, lower the lighting, cover the walls with wood panelling, add a fireplace and an inviting couch or two, and install wide stairs that flowed down to amenity rooms on the floor below. “Walking into the building will seem like walking into a lounge that people are hanging out in,” he told me. “And you just happen to be one of the people that lives here.” Among white-collar workers, the covid-19 pandemic led to a profound shift: even when it became safe to return to the office, many employees preferred to work remotely. Nationwide, offices are only about fifty per cent full. Since 2019, according to a recent academic study, downtown street foot traffic has fallen by an average of twenty-six per cent in America’s fifty-two biggest cities. Urban theorists describe a phenomenon called the “doom loop”: once workers stop filling up downtown offices, the stores and restaurants that serve them close, which in turn makes the area even emptier. And who wants to work somewhere with no services? In St. Louis, whole swaths of the downtown business district are vacant. Not long ago, the A.T. & T. Tower, one of the city’s marquee properties, which was sold for two hundred and five million dollars in 2006, was off-loaded for $3.6 million. In New York, the rebound has been stronger. On Wall Street, where numerous executives have expressed sharp impatience with remote work—David Solomon, the C.E.O. of Goldman Sachs, has called it an “aberration” that undercuts the company’s “collaborative apprenticeship culture”—foot traffic has returned to eighty per cent of its pre-pandemic level. But on Mondays and Fridays many Manhattan towers become as sparsely populated as an Edward Hopper painting. Some company accountants have started to see the rental of large office spaces—which in New York can cost more than three hundred dollars per square foot—as a colossal waste. In lower Manhattan, major renters such as Spotify and Meta have begun shrinking their footprints, vacating entire floors that once bustled with employees. For the past three years, about twenty-two per cent of office space in New York has gone unrented—that’s a hundred million vacant square feet, the equivalent of nearly thirty-five Empire State Buildings. For the owners of half-empty towers, it’s become increasingly apparent that a new financial strategy is needed. Berman has helped show desperate office-tower owners a way out. Although fewer people may want to work in Manhattan, more than enough still want to live there. The over-all vacancy rate for apartments in the city is now 1.4 per cent—the tightest market in fifty years. The reasons that the city’s work and residential fortunes have not moved in step are various. “There is only one New York,” Berman told me. “Culture, diversity, business, technology, medicine, education—all in one small island.” New York remains a place where many ambitious young people go to start their careers, if not to stay, and this demographic is ideal for the hotel-style conversions for which office towers are most suitable. Moreover, Berman said, “young people are social—they don’t want to sit in the middle of a forest on a Zoom call.” Converting offices into apartments won’t be a panacea for New York’s real-estate titans: there is simply too much square footage that is going unused, and this will be a problem as long as companies continue switching to smaller premises. Berman told me, “If we ultimately absorb twenty per cent of the office space, that would be optimistic.” But, he added, conversions will energize neighborhoods that otherwise would be among the worst hit, like the financial district. There, Berman foresees apartments replacing half the empty offices. The tower at 55 Broad Street has spent most of its existence as an unlovable building in an unlivable neighborhood. In the Art Deco era, the architectural firm founded by Emery Roth was an innovator—it designed the San Remo and the Beresford apartment buildings, on Central Park West—but by the late nineteen-sixties it was known for maximizing rentable office space above all else. At 55 Broad, which is right around the corner from the Stock Exchange, two adjoining ten-story structures came down to make way for a much taller new building. It was a time of rapid growth on Wall Street—between 1958 and 1973, the amount of office space downtown doubled. The design ethos was “do your own thing.” “This is not the Renaissance, or an age of uniform standards of beautiful buildings,” a member of the City Planning Commission explained to the Times in 1973. “No one agrees on anything.” The result at 55 Broad was a dark curtain-wall tower with windows and brown panels spaced between thick steel pinstripes. Deep rectangular floors were set back every ten stories, creating a three-tiered wedding cake. Two renovations followed over the decades, but the building remained what it had always been: a dull stack of boxes. Shortly after the Rudins built the tower, they attracted as its anchor tenant Goldman Sachs, which was then in a period of wild ascent. Four years after the building opened, a Times reporter dropped by Goldman and excitedly described an “assemblage of young men with longish haircuts and bright colored shirts” on a trading floor that “rips with action.” Goldman was so successful that it eventually built its own building, two blocks south, leaving 55 Broad half empty. In 1985, Drexel Burnham Lambert, the firm that pioneered the junk bond, moved in. Within five years, it had fallen under indictment and gone bankrupt, forcing the Rudins to scramble again. The family spent millions to make 55 Broad into a state-of-the-art tech hub, borrowing strategies from “Being Digital,” by the nineties tech guru Nicholas Negroponte. Broadband was installed on every floor, and for a time the mid-century structure was “one of the most wired in the world,” according to Forbes. This incarnation lasted until the dot-com bust of 2000, when many of 55 Broad’s tenants went under or moved out. In the next decade, terabytes replaced gigabytes, and the number of servers that a cutting-edge tech firm needed could have taken up an entire warehouse. In 2014, plans were leaked for a proposed fifty-three-story replacement at 55 Broad, but it was never built. A lot of time and money is required to safely dismantle a thirty-story tower on a narrow, busy street. Six years later, the pandemic hollowed out the city, particularly the business districts. By July, 2023, the Rudins had concluded that 55 Broad—then only sixty per cent rented—had no future as an office tower. They sold most of their interest in the building to Berman, keeping a small part so they could observe how he handled conversions. (Silverstein Properties, which rebuilt the World Trade Center, also became a partner in the project.) The decision to convert to residential was a hard one for the Rudins. “We don’t like selling our buildings,” Bill Rudin, one of the chairs of the family’s company, told me. “That’s kind of a mantra for us.” The opportunity to learn from Berman was a big factor: “We wanted to see the maestro, like a front-row seat to see Leonard Bernstein.” “Let me read to you from a recently fictionalized version of the procedure . . . ” Cartoon by P. C. Vey Copy link to cartoon Link copied Shop The sale price for 55 Broad was $172.5 million. The construction loan was set at two hundred and twenty million dollars. The total cost of the project—nearly four hundred million dollars—was considerable, but replacing the office tower with a new building, Berman told me, would have cost “well over six hundred million.” (Upgrading it in the hope of attracting new office tenants, according to Berman, would have cost roughly eighty million dollars.) And, because of zoning reforms, no new building would be allowed to overwhelm a Manhattan street the way the hulking towers of the postwar period did. A developer who constructed a tower the same height as 55 Broad would likely have to sacrifice twenty per cent of the rentable space. Early in the conversion process, Berman’s construction team removed the fluorescent-tube lighting and the dropped PVC ceilings. Then workers knocked down the drywall that had once delineated corner offices, windowless offices, rest rooms, mop closets. “We do a very thorough gut renovation,” Berman told me. “We literally take everything out.” At 55 Broad, the result was nearly four hundred thousand square feet of raw space, with a potential to generate more than thirty million dollars in rental income annually. But Berman still had a major puzzle to solve: If no one wanted to work in a glum, out-of-date building, why would anyone want to live there? In the lobby at 55 Broad, Berman pressed the Up button. “This building is way over-elevatored,” he said. Soon, five elevators would be torn out. Apartment buildings, he explained, generally need fewer than half the elevators that office buildings do. “Residents don’t mind waiting twenty seconds more for the elevator,” he said. A visit to the sixth floor offered a bleak sight—it was an empty, dark space half the size of a football field, interrupted only by steel support beams and rusted copper waste pipes. The floor was unsealed concrete, and transverse beams along the ceiling were coated with intumescent paint, a fire-resistant covering that looks like bubbling-hot marshmallow. When I stood at the center of the building, the windows were so far away that they looked almost like portholes. Berman gave me a detailed tour of the thirteenth floor. In his business, a crucial metric for turning a profit is the time lag between borrowing construction money and renting out units. So he works fast. Just four months had passed since Berman, Silverstein, and Rudin had closed their deal, but the thirteenth floor already felt like part of a new apartment complex. Workers were measuring, drilling, staple-gunning. Metal track had been laid down where new walls would go, and a few drywall panels had already been installed—they were covered in a playful-looking purple glaze, to make them resistant to mold. “It’s a little bit more expensive,” Berman said. “But we don’t want any issues down the road.” On one piece of drywall, “Apt. 10” was scratched in pen. There was even a handsome tub in a bathroom without walls, like a guest who’d arrived too early for a party. Renters are now used to the layouts of chain hotels, where there’s one window by the bed, so Berman’s bathrooms and kitchens didn’t need to be sunny, and the kitchens could have a minimal footprint. “Our demographic doesn’t cook,” he said. He referred to the other rooms without windows as “home offices.” Now that working from home was common, I observed, such spaces were likely to get a lot of use. He smiled, then said that many would wind up as bedrooms. This is technically forbidden, because in New York City every bedroom must have a window that can be opened, but it’s a widespread practice nonetheless. Berman laid out a rental scenario: “Imagine two or three Goldman Sachs associates who came to New York just after college and want a little bit more spending money.” (In real-estate ads, a one-bedroom with a windowless office is often called a “convertible two-bedroom.”) Berman told me that he could repurpose any office building to residential if the sale price was right. But he acknowledged that 55 Broad posed special challenges. Until the mid-twenty-tens, office-tower conversions in Manhattan mostly involved prewar buildings. These had narrow, smaller floors that divided easily into apartments, and because they were built before air-conditioning they often had courtyards or ventilation shafts. You therefore didn’t have to create odd layouts to give bedrooms some sun. (Natural light tends to peter out about thirty feet into a building’s interior.) Prewar buildings were also full of setbacks, which could become private terraces, and they had oak-panelled elevators that felt homey. I had recently visited the first such building to undergo a major office-to-residence conversion in the financial district, 55 Liberty Street, which long served as the headquarters of Sinclair Oil Corporation. An architect named Joseph Pell Lombardi had converted the building in 1980. I checked out the apartment of one of the first purchasers, on the twenty-third floor. The view was magnificent in three directions, the vista broken only by the gargoyles that the original architect, Henry Ives Cobb, had mounted on the Gothic Revival façade. Looking down from one window, I saw the august Federal Reserve Bank, with its vaults full of gold bars. The view matched the fantasy we all have of living in New York. As the architect Robert A. M. Stern told the Times in 1996, “Who doesn’t want to live in a skyscraper? Everybody in movies lives in apartments on the top of Manhattan.” But few towers like 55 Liberty remain available for conversion in the financial district. What are left are postwar structures—many with deep, dark interiors, low ceilings, and scant visual appeal. Berman did what he could to add comfort to such buildings while holding on to his wallet. He could repurpose extra elevator shafts as garbage chutes, for example. In one building, he turned elevator-shaft spaces into foyers for a line of apartments. The double-height mechanical floor of 55 Broad, which once contained giant heating and cooling systems, would be turned into two floors of apartments. Residents would be provided with compact hvac units under certain windows, as in a motel. These units required much less space than the old systems, and were far more energy-efficient. Berman noted that 55 Broad would be the first all-electric, emission-free apartment building in Manhattan. This was not only environmentally beneficial; it also saved him the cost of inserting thousands of feet of piping into concrete floors. It was but one example of how Berman’s monetary interest and the common good conveniently aligned. We looked out a window at an adjacent nondescript office building, and he saw prey. “That’s going to be that way for maybe three to five more years,” he predicted. “That building will be converted, too.” Adaptive reuse is a form of recycling, a point that Berman often makes. According to a recent paper by the National Bureau of Economic Research, converting an out-of-date office building into an apartment complex can increase its energy efficiency by as much as eighty per cent. (In a residential building, not everyone blasts the air-conditioning 24/7.) According to a report by the Arup Group, an engineering firm, converting a Manhattan office tower releases, on average, less than half the carbon that building one from scratch does. As expensive as these projects may seem, the cheaper cost of repurposing an old building can allow rental prices to be set lower than they would be in a new one. Berman estimated the minimum monthly rent for a studio apartment in a new lower-Manhattan building at well over four thousand dollars, whereas a comparable apartment in 55 Broad will go for about thirty-five hundred. Although this is a considerable sum for one person, it’s not especially expensive by Manhattan standards, and, as Berman acknowledged, many of his units will end up being shared. He stressed to me that he is not particularly interested in what goes on inside the apartments, or in what the tenant experience is like. “A renter is not a condominium owner,” he told me several times. He isn’t trying to re-create 443 Greenwich Street, his celebrity-friendly condo development, with its wine cellar and tiled hammam. “Our profile is a young person,” he said. “Maybe twenty-four, twenty-five, who stays one or two years, maybe three. They’re not committing.” His clients are in the city-hopping phase of life: “ ‘O.K., next year, the year is up and I’m going because I need to be in Boston, or I need to be in Chicago, or I’m going to San Francisco.’ ” Berman had considered improving 55 Broad’s dated façade, but decided that it was money poorly spent. “Renters pay less attention to these things,” he said. New York renters don’t have much choice, anyway. “We’ve never had this kind of imbalance between demand and supply before,” Berman said, with the pleasure of a person who likes his odds. The vacancy rate in the five or so buildings that he currently owns is about one and a half per cent. He estimated that all the units at 55 Broad would be rented within six months of going on the market. A few of Berman’s redevelopment schemes have been more architecturally adventurous. In 2017, he worked with Avinash Malhotra to convert 180 Water Street, also in the financial district. The building, like 55 Broad, was a thick rectangular slab designed by Emery Roth & Sons, and had interior spaces more than seventy feet long. Berman could have rented out these extra-long apartments as they were, but instead he decided to remove the core of the building, where mechanical equipment was taking up space, thereby creating a courtyard and cutting the apartment layouts down to normal length. Though such a restructuring had never been tried before, he took the risk, at a cost of several million dollars. The result gave tenants more light, he said, but that was incidental. New York City law permitted him to add the removed square footage to the top of the building—he gained four floors and a roof with a pool. “If I couldn’t have done that, I wouldn’t have had cost-efficient units,” he said. The architects for 55 Broad are John Cetra and his spouse and professional partner, Nancy J. Ruddy. They are well respected in the industry, but they are not starchitects, a type that Berman has no time for. “A young-professional renter isn’t going to pay me more money because my building was designed by Norman Foster,” he told me. One day, Cetra and Ruddy met me at 55 Broad. Cetra described the back-and-forth that he and Berman have on their projects. (55 Broad is their sixth.) Berman sketches out a plan first, then passes it to Cetra. “He wants to make it more efficient,” Cetra said. “I want to make it a little better. ‘Nathan, let’s give this foyer a bit more room.’ ” Whereas Berman focusses on the architect Cass Gilbert’s definition of the skyscraper as a “machine that makes the land pay,” Cetra and Ruddy emphasize pleasure. Cetra showed me his floor plan for 55 Broad: apartments curled around apartments like frolicsome seals. He explained that he and Ruddy always sought the “wow factor,” adding, “Ideally, in as many apartments as you can, when you open the door you see light and you walk toward light.” Shiny wood floors would have heightened this effect, but, Cetra noted a bit sheepishly, the floors at 55 Broad would be covered in something called “vinyl plank flooring.” Wood scuffs too easily in a building where people are constantly moving in and out, and, Cetra said, vinyl flooring was getting better. “They’re able to create patterns that don’t repeat,” he said. Ruddy said that it was fun to fit apartment layouts into the constraints set by an office tower’s shape—each unit had “the intricacy of a watch.” She recounted a notable success for which they’d won an award. In 2014, while converting the former Flatotel, on Fifty-second Street, into condos, they had reconfigured an old loading dock—a concrete area where trucks parked and dumpsters were stored—into a new mid-block entrance. “We created this sort of magical lobby out of it,” Ruddy said. “I don’t think anyone had ever converted a loading dock before.” Cetra jabbed at his floor plan for 55 Broad to amplify the point: “If this were a new building, every one-bedroom would be exactly the same. But look here. This is a one-bedroom, that’s a studio, that’s a one-bedroom studio, and every one has different proportions.” (A resident of 20 Broad Street, an earlier project that Cetra and Ruddy developed with Berman, complained to Bloomberg News last year about her studio: “It was a very awkward space. It wasn’t square, it wasn’t a rectangle, it had all kinds of bizarre edges and weird corners.”) One feature would be standard at 55 Broad: a washer and dryer. “People do their laundry in their pajamas or their underwear while they’re watching television,” Ruddy explained. In the basement, public space that might otherwise be devoted to a large communal laundry room would be aimed at helping tenants meet one another. Small apartments make people want amenities, and amenities make people accept small apartments. The new generation expects post-college life to resemble college. “We’re in an amenities war,” Ruddy said. All the buildings converted in the financial district are full of co-working spaces, gyms, and plush couches. One of Cetra and Ruddy’s signature moves, they told me, is to adorn a public space with a modular shelving unit that contains small sculptures and ceramics that “feel like they could have been picked up on a trip overseas.” The architects also include a pile of art books—“Jazzlife,” “Helmut Newton: Work,” a book of Ai Weiwei’s installations. I objected that these seemed like the sorts of books people never actually read, but they disagreed. Tenants did pull them down. In fact, Cetra and Ruddy told me, the books at AVA DoBro, a new apartment building in downtown Brooklyn that they had designed, once disappeared entirely. “It turned out it was a construction worker who had grown up without books,” Ruddy said. “So I replaced them.” “They’re good books,” Cetra added. “For the last time—because it’s relaxing and romantic!” Cartoon by Sofia Warren Copy link to cartoon Link copied Shop We went to 55 Broad’s roof, where we stood in front of a long, empty concrete pit. Ruddy pulled out an iPad to show me a rendering of a future pool: eleven by forty-five feet, set off by a dozen deck chairs facing east and a tasteful border of shrubs to increase, as Ruddy said, “connectivity with nature.” There was what looked to me like a pool house but turned out to be “an indoor-outdoor working space.” The 55 Broad tower is four hundred feet tall, but in the financial district that makes it midsize. I pointed out that remnant workers in the neighboring towers could easily peek out their office windows and observe whatever action was ripping on 55 Broad’s rooftop. Cetra said, “That’s part of the fun!” In the late seventies, my father and mother, an Upper West Side couple, separated. My father, a corporate lawyer, had long worked at 77 Water Street, a steel-and-glass-curtain edifice, designed by Emery Roth, that still functions as an office building. A few years later, he moved into a nearby one-bedroom apartment, in one of the first converted office towers. It was a prewar building, and the impressive lobby made you feel as though you were heading for an appointment with Mr. Morgan. For my father, the short walk to work, after a professional lifetime of taking the 2 or 3 train up and down Manhattan’s spine, was a pleasure. The apartment had a kitchen he didn’t use, and it was on a high floor. On the nights I stayed with him, we would look out at neighboring towers’ brightly illuminated interiors, the cleaners slowly advancing through each floor, emptying the wastebaskets. He told me that he liked the feeling of being a lumberjack going to sleep in the middle of his forest. Back then, there were no restaurants or stores open after business hours, not even a Blimpie. Joseph Pell Lombardi’s son, Michael, who grew up at 55 Liberty Street—the building next to the Federal Reserve—also remembers the streets being empty at night, with guards moving pallets of gold bars. “It all seemed incredibly casual,” Michael remembered. “There was no one around, only me, a kid, imagining how easy it would be just to take one of them.” Census figures from 1970 show that just eight hundred and thirty-three people lived south of Chambers Street. By the time I began visiting my father’s place, there were more—but not many. “The jury is still out,” Henry Robbins, an expert on real-estate trends, told the Times in 1996, in an article about living in the financial district. “The area dies at night. It needs a neighborhood, a community.” Thanks in part to Berman, the financial district now has enough population density to feel like a proper New York neighborhood. His office at 40 Wall Street is on the seventeenth floor, and he can see five of his converted towers out the window. Within just a few blocks of 55 Broad, he has turned 20 Exchange Place, 63 Wall Street, 67 Wall Street, 180 Water Street, and 20 Broad Street into apartment buildings. He is currently working on 25 Water Street, the former headquarters of J. P. Morgan, which, after the Pfizer building, will be the second-largest conversion to date in the United States, with Cetra and Ruddy helping him design thirteen hundred units. Crain’s New York Business has called Berman “the king of FiDi.” He enjoys his stature as a local potentate. He began his conversion business in the late nineties, after receiving an eighty-thousand-dollar loan from his father-in-law. For a time, Berman was an outlier as a developer, focussing on a market that others found too small or insufficiently profitable. Now he is turning away projects. David Marks, the executive at Silverstein Properties who is developing 55 Broad Street with Berman, said, “For many years—and I’m quoting Nathan—he was the quirky monster that no one really understood, and now he’s the prettiest girl on the dance floor and everyone wants a dance with him.” Berman can decide almost instantly—just by knowing the age and the location of a building and by glancing at Google Earth—if the place is ripe for conversion. “If the price per pound is right, I say, ‘Let’s go,’ ” he said. Berman, who was born in Ukraine and came to New York at the age of fourteen, is the child of a Holocaust survivor, and the niche he occupies in the city’s real-estate ecology makes sense for an immigrant with a mistrust of government. He focusses only on buildings built before certain years—1977 below Murray Street, and 1961 for the rest of Manhattan—because they can be converted without special variances. (Conversions have long been restricted in Manhattan because sudden population surges in residential neighborhoods can crowd schools and overwhelm public transport.) “Life is short,” he told me. “I don’t want to wait two or three years for rezoning.” A current zoning-change proposal, which Mayor Eric Adams supports, would allow any building in New York built before 1990 to be converted. It would add to the pool of potential apartments nearly as much office space as there is in all of Philadelphia. Berman hopes that the zoning change will become law by the end of the year. After we left 55 Broad, Berman took me on a tour of two of his other properties. We started down the street, at 20 Broad, once a part of the Stock Exchange. We briefly visited an apartment, but the showpiece was the sub-lobby level. There was a commercial-size gym replete with punching bags, elliptical trainers, free-weight racks, and rows of treadmills. Another room held pool tables, and a third was a library graced with one of Cetra and Ruddy’s modular shelving units. Nobody seemed older than thirty-five. Down the hall was a vault with heavy iron bars where bonds had once been stored. Rather than pull the huge structure out, Cetra and Ruddy had set up a co-working space in it. (“Tenants sometimes play poker there now.”) As we left, Berman took the massive door and swung it on its massive hinges, eager to show me that it still worked. We walked down Beaver and Pearl Streets to 180 Water Street, the building from which Berman had removed the core. At the entrance, he said, “I will challenge you to show me any elements in this interior where you can point out and say, ‘Gee, that’s really from the office period.’ ” I couldn’t. He boasted that he’d never lost that bet. In the elevator, we met a young resident. She had a dog and said that she had been in the building for more than five years. Berman seemed disappointed. On the twelfth floor, near another modular shelving unit, there was a bright-white machine labelled “Tulu: Your Smart Rental Store.” Using your phone, you could rent household items like a toaster or a vacuum cleaner, or buy something you’d run out of: tampons, Tide Pods, Doritos. It was a clever way to both justify small closets—“Nathan believes in very compact closets,” Ruddy told me—and monetize how people live now. “These people want to snack at night,” Berman said. Afterward, I walked out into the early FiDi night. I turned onto Exchange Place, where I passed crowds of tourists taking pictures of Kristen Visbal’s “Fearless Girl” statue. Various restaurants were filling up, from beer halls like Trinity Place to steak houses like the recently renovated Delmonico’s. Stone Street was now a sort of food court, and I could have picked up groceries at a Whole Foods just north of Exchange Place. (My father would have had to go to the Village to get groceries, if he’d wanted any.) The street life died out at Chambers Street, where government offices stood dark and empty. It was as if the original Dutch settlement had been re-created, back when Wall Street had a wall. In a 2022 Glassdoor post, a user called McKinsey Consultant asked, “Should I live in FiDi?” The responses included a lot of cheering for the rooftop pools and the great views. But a user called IBM1 advised living somewhere else. “It’s such a soulless neighborhood,” IBM1 wrote. “Don’t be swayed by the ultra luxe buildings.” It’s true that FiDi remains on the sterile side. It could use some parks, and its inhabitants seem either new to the island or temporary. All those amenities in the buildings keep people within their confines; if you have a Tulu dispensing machine in your basement, who needs to drop by a local hardware store or a pharmacy? All the same, more than thirty thousand people now live in FiDi—and at least some of them have begun to see it as a permanent home. Berman told me that, whereas more than half of his renters used to be apartment sharers, he expected the percentage at 55 Broad Street to be closer to fifteen. This suggested to him that families were moving in. He added that he recently ripped out a Ping-Pong room at 180 Water and turned it into a children’s play space. “We have sixty children in the building!” he said, amazed. One was his grandson. His son, who is the No. 2 at the firm, and his daughter-in-law moved in five years ago. “They never left,” Berman said. ♦",
    "commentLink": "https://news.ycombinator.com/item?id=40285211",
    "commentBody": "Can turning office towers into apartments save downtowns? (newyorker.com)292 points by pseudolus 20 hours agohidepastfavorite556 comments tylerFowler 14 hours agoIt's kind of odd to me (as someone who used to live there at its latest boom time) that nobody talks about Kansas City when it comes to this topic. From the ~70's until the early 2010's Kansas City's downtown was in a similar \"doom loop\" of crime, undevelopment, decaying historic buildings, etc... In that city 75% of the metro lives in suburbs, drives in to downtown for work and promptly leaves. Until about 2012 or so. Urban redevelopment kicked in, adding (free!) transit, boosting retail, arts district events, a new stadium, and crucially - *massive office to housing conversion projects*. There are tons of success stories like the historic Fidelity Tower at 909 Walnut (https://en.wikipedia.org/wiki/909_Walnut), a huge 35-story tower that sat vacant (creepy) for the better part of a decade and is now home to 159 units. Ditto with the Power & Light Building (https://en.wikipedia.org/wiki/Kansas_City_Power_and_Light_Bu...) (36 stories) - largely vacant for the better part of 20 years and now home to nearly 300 units. I could go on, every block has similar projects of 100+ year old buildings of nontrivial sizes that are now super unique apartments. I myself lived in the 30-story Commerce Tower (https://en.wikipedia.org/wiki/Commerce_Tower) for a while and it was incredibly cheap to do so (~$1100/month for 750sqft 1 bed on the 14th floor), I had a 10 minute commute by foot to my office, it was awesome. Even the more squat, broad midsize banking buildings have had major success with residential conversions. These kinds of conversions have been proven out when there is willpower to do so at the city level - people will move in and prices typically get competitive fast if done at scale. I've lived in SF for 4 years now and I'm convinced its a policy problem not an economic problem. reply ido 1 hour agoparentThis is off-topic, but I just wanted to say as a European how crazy it is to keep finding out the US is so large that even some third-tier city I ± never heard of is big enough to have a downtown full of outright skyscrapers[0]! It reminds me of reading about some minor Indian city and then looking it up in Wikipedia and seeing it has a population in the millions. [0] https://upload.wikimedia.org/wikipedia/commons/thumb/7/79/Ka... reply keiferski 24 minutes agorootparentEurope seems to do a better job at constructing buildings that can be used in various use cases, though. So when the office use case goes away, making the building into apartments is fairly simple. This is unlike the office towers mentioned in the article, which don’t really lend themselves naturally to being apartments. reply arethuza 43 minutes agorootparentprevWell Europe and the EU both have larger populations than the US and quite a bit higher population densities (even though the continent of Europe is slightly larger) so wouldn't you expect Europe to have more skyscrapers than the US? reply WJW 35 minutes agorootparentEurope just seems to go for much more mid-rise high-density neighborhoods compared to the US with skyscrapers in the city center and then sprawling suburbs that are so low-density you can barely get out of them without a car. reply weberer 34 minutes agorootparentprevA lot of it is about zoning laws. The downtown Helsinki area has 0 skyscrapers, and the closest ones are the Redi buildings about 5km away. reply loftyal 1 hour agorootparentprevWait until you see China. There are multiple cities bigger than NYC or London that most people have never heard of. https://en.wikipedia.org/wiki/List_of_cities_in_China_by_pop... reply wolverine876 14 hours agoparentprev> a similar \"doom loop\" of crime, undevelopment, decaying historic buildings, etc. That's not the doom loop in the OP, which results from office space demand decreasing due to so many working remotely: Urban theorists describe a phenomenon called the “doom loop”: once workers stop filling up downtown offices, the stores and restaurants that serve them close, which in turn makes the area even emptier. And who wants to work somewhere with no services? > every block has similar projects of 100+ year old buildings of nontrivial sizes that are now super unique apartments Per the OP (and I've read elsewhere), older buildings are easier to convert because their floors are smaller, which makes it much easier to give a windows to every apartment (a law in many/most/all places). reply jvanderbot 14 hours agorootparentIt's easy to imagine that the two doom loops are in fact connected. A vacant downtown is essentially what GP described, and the crime seemed to follow and exacerbate the problem reply wolverine876 10 hours agorootparent> the crime seemed to follow and exacerbate the problem Did it? Not in NYC. Crime is very low; it's arguably one of the safest places to be in the country. reply Supermancho 9 minutes agorootparent> Not in NYC. That is now. NYC was famously crime ridden for at least a century. Today, just as in London, crime past a certain value or a non injury car accident, is not investigated. reply IG_Semmelweiss 7 hours agorootparentprev\"Crime is very low\" Murder may be low. Are assaults low? Petty theft? What about pedestrian sentiment when walking the street? Not all crime is reported and not all threats are crimes yet threats will certainly cause someone to feel unsafe. reply wolverine876 6 hours agorootparentYou can look up the answers to some of the questions. Do you have any factual basis for your claims? > Not all crime is reported and not all threats are crimes yet threats will certainly cause someone to feel unsafe. What does that mean? We don't know anything about anything? Then maybe crime is even lower than I think. Everyone feels threatened all the time? What basis do you have for saying that crime is _____ (what?)? It's all fabricated so far. reply monero-xmr 6 hours agorootparentThe problem is it’s hard to get anything other than anecdata when discussing things that don’t come into macro statistics. I live in a large Democrat-dominated city, I have very deep connections and roots all around, and casual mentions of petty crime are common. I have observed a lot of shoplifting and I’m only in retail stores so often. There is certainly an attitude that some types of crime just occur and no one will stop it. > In its annual survey, BJS asks crime victims whether they reported their crime to police. It found that in 2022, only 41.5% of violent crimes and 31.8% of household property crimes were reported to authorities. BJS notes that there are many reasons why crime might not be reported, including fear of reprisal or of “getting the offender in trouble,” a feeling that police “would not or could not do anything to help,” or a belief that the crime is “a personal issue or too trivial to report.” > Most of the crimes that are reported to police, meanwhile, are not solved, at least based on an FBI measure known as the clearance rate. That’s the share of cases each year that are closed, or “cleared,” through the arrest, charging and referral of a suspect for prosecution, or due to “exceptional” circumstances such as the death of a suspect or a victim’s refusal to cooperate with a prosecution. In 2022, police nationwide cleared 36.7% of violent crimes that were reported to them and 12.1% of the property crimes that came to their attention. https://www.pewresearch.org/short-reads/2024/04/24/what-the-... reply wolverine876 3 hours agorootparentYou still don't have any data supporting your claims that crime is high. All you say is that it's not always reported - which is well known and has been for generations, if not forever. What shows that it's high? It's circular to say that the unknown numbers are higher, not lower, because you think crime is high. > a large Democrat-dominated city What does the political party have to do with it, unless this really is about your politics? Republican areas of the country have higher crime rates, last I checked. If you think there's a correlation between party and crime rate, feel free to show us. > (Pew data) It's great to have some data, thanks. I don't know that anything has changed, however, though I imagine numbers were different during the pandemic and immediately after. Numbers could be better now, for all we know. reply grardb 2 hours agorootparentprevWhen you say \"shoplifting,\" do you mean someone has simply walked/run out of the store without paying for something? If so, I don't really understand what that has to do with safety. Same with plenty of other crimes, such as someone jumping a subway turnstile, graffiti, etc. These things surely lead to a lower quality of life and I'd prefer that they didn't happen, but I personally wouldn't say that they make a city dangerous. reply watwut 7 minutes agorootparentprevAll those are low by historical standard. reply outop 3 hours agorootparentprev> What about pedestrian sentiment when walking the street? In other words: people who are not from New York come to the city. They falsely believe it to be a dangerous place compared to where they're from. They walk around fearfully, unable to escape their mindset. Now they would like this fear they experienced to be reflected in crime statistics. reply AndyNemmity 8 hours agorootparentprevSaying NYC being one of the safest places to be in the country is why LLMs will never be accurate. People will just say anything no matter how irrational it is. reply reducesuffering 8 hours agorootparentWant to back that up with some data? NYC is half the per capita murder rate of the US. What swath of places do you think are safer such that \"one of the safest places\" for NYC is inaccurate? On >250k population list, NYC is #20/100 safest: https://en.wikipedia.org/wiki/List_of_United_States_cities_b... On a 100k to 250k list, NYC is above average safety. reply jvanderbot 6 hours agorootparentWhen you have so many people per square mile, it slightly skews the metrics. The reality might be you're less likely to be murdered, but you might be way more likely to witness a crime. That matters. reply wolverine876 6 hours agorootparent> you might be way more likely to witness a crime. That matters. People are just reaching for possible straws. Do you have any factual basis to say that? We can make up anything and put the word 'might' or 'maybe' in front of them. I've spent lots of time in NYC and other dense, major cities, and I think I've seen one crime ($20 stolen). The interstate highway is more threatening, with the aggressive drivers. Really, go to NYC. Look at the millions of people walking around without a care, going about their days. reply jahewson 6 hours agorootparentprevSorry but moving the goalpost from “one of the safest places to be in the country” to “one of the safest big cities in the country” is not fair game. The US has over 19,000 cities and you discarded all but 91 of them. reply reducesuffering 4 hours agorootparentI've not discarded them, it's just the the largest portion of data for where people in the US are, which NYC has an objectively good per capita murder rate compared to where most people live in the US. If we want to get data for the US as a whole, please point me in the direction where we can find the safety level of the median person, besides NYC having a 3.3 murder per capita rate while the US is ~6.5. Idk what % it is, but a large portion of Americans live in those lists of 100k+ cities. reply eptcyka 2 hours agorootparentprevInstead of crime per capita, what’s the likelihood of being the victim of a violent crime in NYC? reply oblio 1 hour agorootparentAren't those inherently related? reply cabalamat 16 minutes agorootparentNot if a small number of people are victims of multiple crimes. AndyNemmity 7 hours agorootparentprevThe statement that \"NYC is one of the safest places in the country\" and the relevance of per capita crime rates are two different discussions. When we say a place is \"the safest,\" we're referring to the absolute level of safety. Per capita crime rates, while useful for comparing the likelihood of crime between areas with different population sizes, don't determine the absolute safety of a place. They normalize crime data by population to show relative safety, which is a different metric. reply kurikuri 7 hours agorootparent“Absolute level of safety” what does this even mean? Per capita crime rates seem quite relevant to this underlying property and can be well defined. reply hyperadvanced 3 hours agorootparentThere’s a psychological reason (rational or not) people equate cities with violence, and it’s this same “absolute level of safety”. When you could live in a town of 5000 where 1 person gets murdered every year, it’s very different than scaling those numbers up by 1,000. Realistically, 999 of those murders will be of/by people you will barely interact with, whereas in a small town, it’s all but guaranteed that you will know both the murderer and the victim (and they will know each other). That said, I understand the desire to frame crime in the absolute. Either you’re the 1 guy who gets unlucky and experiences a crime in your small town, or there are 999 people like you in the big city). Crime is a problem of scale, and a bigger scale freaks people out even if the coefficients are the same. reply watwut 2 minutes agorootparentIt is not just the scale issue. It is also that for your personal safety, only statistics on killings of random strangers matters. Majority of murders are among people who know each other. You personal risk is all about who your partner is, who your friends are, how much aggressive your uncle is. A guy you never met that is just about to murder his wife has nothing to do with your personal risk. satvikpendem 6 hours agorootparentprevBy that logic, somewhere with no people would be the safest area. Good factoid to know but for most people, it's not useful to live with no others around. reply outop 3 hours agorootparentprevOne of the worst and most tendentious numerical arguments I have ever seen. reply wolverine876 6 hours agorootparentprevAs any person who lives in a city knows, more people means more safety. It's the empty, dark street - especially non-residential - that you want to avoid. Also, the NYPD is always nearby if that suits you (I've never needed them personally). reply reducesuffering 7 hours agorootparentprev> When we say a place is \"the safest,\" we're referring to the absolute level of safety. Idk who you think \"we\" is. It's not in my definition of safety, nor probably the general populace to think of Stockton, California as \"safer\" than NYC, just because Stockton had 50 murders a year compared to NYC's 240... Because Stockton has a 17.77 murder rate per 100k, and NYC 3 per 100k. You bet your ass you'd be \"safer\", as in less likely to get shot or stabbed to death, living in NYC. reply AndyNemmity 7 hours agorootparentNYC hasn't had as little as 240 murders in a year since murder data was recorded. reply reducesuffering 7 hours agorootparentMy argument is clear about how ridiculous absolute values of crime pertain to how safe it is for someone. That you instead want to nitpick the exact # of murders in NYC (when I just winged a 3 per 100k at 8m population) is indicative that you can't refute it and your original assertion is incorrect. If anything, the real # in 2023 of 386 only further proves my point. That it's ridiculous to assert even that many more murders is less safe than Stockton, CA. Safety is about \"am I more or less likely to have something happen to me\" and no one is thinking that means moving to the tiniest country of 2,000 people where only 10 murders happened in the entire country last year. reply AndyNemmity 7 hours agorootparentApologies, I wasn't aware you were making up numbers. That isn't useful, or helpful in conversation in my opinion, but clearly you feel strongly that's a reasonable thing to do. I contend it is significantly safer in the majority of cities in the US than New York City. reply mgce 5 hours agorootparentLike everyone else here I'm having trouble parsing what you're trying to say. You seem to be implying per capita is a bad measure of safety. But you're not being clear why. Can you elaborate? reply Nimitz14 5 hours agorootparentprevHis argument remains valid with the correct number, proving that his made up number was close enough. You're factually wrong. reply reducesuffering 4 hours agorootparentprevI didn't realize you need the exact #'s to understand an argument. Now that I've provided NYC's, and Stockton's is 34 in 2019, does it make any difference? No. Your conclusion is without any evidence and is in fact, wrong, based on the crime metrics we can easily compare. It's quite uncanny that it's the people who most fear crime, think they're going to something like \"stay the hell away from crime infested NYC\" and proceed to move somewhere where they're more likely to be affected by crime. reply kortilla 3 hours agorootparentAffected and “victim of” are different. Higher population densities mean more people are affected by a murder (more witnesses, more people disrupted by the crime scene, more people that go ‘I go to that train stop every day, that could have been me’). Rational or not, proximity of crime freaks some people out much more than probability of being the victim. reply watwut 1 minute agorootparentIsnt it the case that the people most scared of cities are the ones who dont live there? bryanrasmussen 6 hours agorootparentprevI gotta say this subthread is weird, our society has pretty much decided to evaluate such issues as safety, worker happiness, gender equality and so forth on the basis of statistics, and all these people here are just saying no, it doesn't matter that statistically New York is safer without providing any argument why the statistics are wrong (there are a couple people who say things like the population size skews statistics but that's like the premise of an argument, not an actual argument), according to these folks it just is the case that New York is much less safe and that's it buddy! I would expect to see a mathematical argument as to why the statistics are wrong, size of population skews the statistics, cool. How does that work and why has it made New York seem safer when you contend it isn't!? There are different kinds of crime than murder - ok, show the stats as to why people suffer more from other crime in New York than the rest of the country! You're more likely to see a crime in New York than in other places with more crime because there are more people, ok sounds like a cool statistical \"paradox\", show your work. This is HN and all that stuff! I mean it is just a weird little discussion here, reminds me of twitter, although with more text and admittedly more grammatical correctness. reply wpm 5 hours agorootparentIt’s clear to me: NYC doesn’t feel safe. And while subjective experience is important, it’s hard to argue with a position someone didn’t logic themselves into. I live in Chicago and it’s much the same. I hear about carjackings and muggings and all sorts yet the one thing I see and experience nearly every single time I leave the house is being nearly run over by some impatient wanker in a car. I don’t feel that other stuff because I yanked the IV drip of fear-news-weather garbage media out of my arm and that’s really the only thing feeding this crap to people. Most big cities are the safest they have been in history. Someone wants people to feel otherwise. I wonder why. reply wolverine876 2 hours agorootparent> It’s clear to me: NYC doesn’t feel safe. What is that based on? Is there a survey? When I've been in NYC, people seem to feel very safe and relaxed, 24/7 (of course, in the city that never sleeps). reply bryanrasmussen 1 hour agorootparentI suppose it is on one hand based on media, NYC is the big bad city, it's the mafia city, if you have a crime based movie it will be in NY, LA, or maybe Chicago as the most popular places. Another thing might be that it feels unsafe due to social isolation maybe, for people who are actually there, but that is just a supposition. I think it's reasonable that they feel it's less safe. I often feel things that I know do not exactly correspond to statistical reality, one should just try to be aware of where the feelings slightly diverge and not go arguing that what one feels is objective reality. reply csomar 1 hour agorootparentprevMost crime is marginal even inside cities \"infested\" with crime. If you are personally affected (statistically improbable), the city/place is already unlivable and it'll go down quickly like some places in South America. I think there is an important distinction to make here. Crime is unlikely to affect most people, but that doesn't mean that an increase should be tolerated. In fact, it should be alerting to the people and authorities to reverse course as much as possible. Of course that doesn't mean that NYC is unsafe to visit. It's reasonably safe. Bogota is also not unsafe to visit. It's kinda safe though and requires extra precautions. > Someone wants people to feel otherwise. I wonder why. There is an ongoing campaign to show Western cities as collapsing/decaying. There is also another ongoing campaign to show that China is collapsing any minute now. Welcome to the new cold war. reply jvanderbot 10 hours agorootparentprev> seemed to In GP comment - sorry too late to edit. reply whythre 9 hours agorootparentprevAccurate crime statistics are… let’s say, ‘discouraged,’ by the NYPD and the Mayor’s Office. reply warcher 8 hours agorootparentIn the end the murder rate cannot be fudged, and it is quite favorable. https://worldpopulationreview.com/us-city-rankings/cities-wi... reply raydev 7 hours agorootparentThere's more to \"violent crime\" than just murder. reply wolverine876 6 hours agorootparentWhat do you conclude from that? Maybe the other crime is even lower. reply reducesuffering 7 hours agorootparentprevOf course there is, however murder is the best simple proxy because it's the most severe and least likely to be disputed that it's fudged. So it stands in well for quick internet comments. When you get into a multi-pronged analysis of 5+ crime metrics, and how you factor each into \"safety\" and what metric and places go more underreported, etc. well, you have yourself a research paper... reply Fomite 7 hours agorootparentprevAccurate crime statistics are discouraged by every police department and Mayor's Office in this country. reply bryanrasmussen 6 hours agorootparentWhat if there was some place with no crime? No wait, then they would discourage it because there would be no need to fund the police and what the hell does the Mayor do all day. reply TeMPOraL 3 hours agorootparentIf it was a large enough city, you could convincingly argue that there being no crime means crime prevention is massively overfunded and/or excessive in nature. reply ajross 8 hours agorootparentprevYou're going to need to elaborate beyond \"let's say\". What you wrote is basically just a conspiracy theory. In fact crime statistics reporting is an extremely mature field with well-understood methods and comparable data, and nothing one particular administration can do it really going to impact things very much. Basically, if this was such a great trick for Adams to have invented, why didn't it occur to Bloomberg or Giuliani or Koch? In fact NYC is a very safe city. That it's inconvenient for you to believe that doesn't change the facts. reply zdragnar 8 hours agorootparentprevCombine that the culture of not snitching, and you get wildly bad crime data, fueling all sorts of silly internet arguments, bad politics and bad social science. reply wolverine876 6 hours agorootparentSo how do you know the truth? Maybe crime is even lower. I'm not sure how the no-snitch culture lowers the crime rate; it would seem to lower the conviction rate. reply wernercd 13 hours agorootparentprevThe problem is the differences... IE: Soft on crime policies in large cities. I seriously doubt a lot of these larger cities that are in the \"doom loop\" will have the same results with the current differences between 20-30 years ago and today with simply turning buildings into apartments. Just look at New York where businesses are closing all over because of rampant theft. They aren't closing because people aren't there. They care closing because they can't afford to have half their wares walk out the door because New York is refusing to charge criminals because of \"justice\". The world we live in is vastly different than it was and the doom loops aren't just because of remote workers. reply hughesjj 13 hours agorootparentCitation needed on all of that. It's not just retail closing up in NYC -- the rent is ludicrous, and no one wants to start renting at a lowe rate lest their appraisal goes down and their mortgage lender/city coffers start putting the pressure on the landlord reply Supermancho 0 minutes agorootparentTakes a few seconds. All major cities (even Fargo ND) have seen increased theft. This is unsurprising due to the economies in western countries (which is all i can speak to). https://www.nytimes.com/2023/04/15/nyregion/shoplifting-arre.... sifttio 12 hours agorootparentprevCorporate real estate is a different beast. Residential real estate and corporate real estate do not mirror each other in the market. One can be in high demand while the other has excess supply. Residential landlords are also much different than dealing with corp real estate owners. The terms, length of lease, laws and many other factors are completely different. reply bombcar 12 hours agorootparentPerhaps we need to encourage (via taxes?) convertible buildings that can either be corporate or residential with relative ease, similar to how in smaller towns you often have dentists and lawyers operating out of obviously converted houses. reply choilive 11 hours agorootparentThis is primarily a building code issue for residential vs commercial construction. Office generally try to maximize square footage, this tends to result in floor plans that are very awkward to adopt into residential use, primarily because the building code virtually everywhere has some sort of \"natural light\"/window requirement. This means that purpose built residential high rises tend to be \"skinnier\" to have more windows per sq. ft of floor space. Not to mention the very expensive changes (hvac, plumbing, etc.) required to support residential use. If the building code was changed so that the requirements for office and residential use buildings were closer then it would make future buildings more easily convertible between those use cases. It does not solve the problem of the existing buildings however.. reply hughesjj 12 hours agorootparentprevI don't get how any of that is relevant when my claim is that the corporate rental rates is also too high and the financing for rentals shares the same concerns w.r.t rentable price regardless if it's residential or corporate landlords reply tylerFowler 12 hours agorootparentFwiw it's almost exclusively international developers running the conversions in Kansas City. I think Greystar might be the one with the largest footprint there. reply entangledqubit 9 hours agorootparentprevI believe that some retailer special interest group put out some numbers that did not support any real increase in shoplifting/shrinkage. Initially they made a claim otherwise but they ended up backpedaling. Oddly, the numbers around shrinkage from self-checkout seems to be persistent though. reply wolverine876 10 hours agorootparentprevCrime is historically low in many cities, including NYC. Visit and you'll see what I mean. reply wernercd 10 hours agorootparentCrime is historically low... as crimes like theft are decriminalized. It's easy to say there's less crime when you make stuff not a crime. For reference: all the stores closing because of all of the theft not being prosecuted or that's no longer enforced. IE: California where theft of under $1000 is no longer enforced. reply rovolo 9 hours agorootparentThe CA $950 threshold is when the theft switches from a misdemeanor to a felony: https://www.hoover.org/research/why-shoplifting-now-de-facto... In comparison, Texas has a $2,500 threshold for upgrading from a misdemeanor to a felony: https://www.criminaldefenselawyer.com/resources/criminal-def... reply zdragnar 8 hours agorootparentThat doesn't answer the parent's point. It doesn't really matter whether it's upgraded from a misdemeanor to a felony or not if it doesn't get prosecuted in the first place. reply bryanrasmussen 6 hours agorootparentmisdemeanors are more likely to be thrown out or never taken to court because of court/jail overcrowding. Felonies are likely to get prosecuted if you have a culprit. Misdemeanors... end up getting asked is this trip really necessary? reply wolverine876 10 hours agorootparentprevI'm talking about objective facts. You can make up reasons, but so can anyone about anything - they don't mean anything without a factual basis. Where in NY are these stores closing? reply dundun 6 hours agorootparentI'm going to fight tooth and nail when someone calls NYC unsafe, but it's going to be very difficult to argue against the store closings because of theft (as at least one factor). I've personally witnessed three blatant thefts in the last few years from my local Duane Reade (that closed down in April). Every time the clerks are like \"pretty sure that was the same guy from yesterday\". It's never violent or scary. It's just like watching a fight between homeless people in a subway station -- you look, think that's odd, and move on. > Where in NY are these stores closing? 4 different pharmacies that have closed down since the pandemic just on my path to work, including two a stone's throw from the NY stock exchange. https://maps.app.goo.gl/fJcHCgjVacP5pEuHA https://maps.app.goo.gl/kmDXnjHruMCvS2CA6 I suspect it's not all shrinkage though. I imagine continued trends where we buy more and more things via online retailers like Amazon and the growth of online/by mail pharmacies has contributed too. CVS/Duane Reade are still opening new locations too, so it can't be all that bad. reply chiefalchemist 9 hours agorootparentprevA Walgreens or CVS closed near my GF's flat in NYC. She's within 10 blocks of Central Park. The chain said it was due to too much shrinkage. It's been discussed on WNYC as well, tho those experts claimed most shrinkage is employee related. I live closer to PHL and hear similar claims / rumours. https://www.msn.com/en-us/money/markets/it-s-started-shoplif... reply gremlinunderway 8 hours agorootparentSelf-reporting on these reasons from the business itself is basically useless. No one's fact checking their claims nor do they provide evidence when making these kinds of social-narrative driven claims like \"taxes are too high!\" or \"too much theft!\". Businesses, including chains, fail all the time and the owners/managers have an inherent incentive to try and deflect any blame on environmental factors so they are looked at more favourably by corporate. reply mogiddy55 5 hours agorootparentYou can call it a \"narrative\", but then I get to shrink the \"food desert\" term into just a part of some narrative too, since I've seen personally how theft drives out grocers in MN here. One after another \"underperforming\". The progressive D.A.'s and A.G.'s, who own this project of mass downward departure from normal sentencing, do not expect things to change overnight by taking pressure off (mainly) the poor and the young. But in 5-10 years if crime is worse and society more stratified, their project can be called a failure. reply causality0 13 hours agorootparentprevI would be interested in an analysis of how refusals to prosecute are or are not affecting statistics. If you stop prosecuting a certain crime, does it appear like that crime is happening less on paper? reply talldatethrow 9 hours agorootparentI was at a hardware store when someone walked out with a compressor. I said to the clerk \"that's a 99-2003 GMC Sierra in case you want to tell them when you call it in\" And he replied \"oh we don't even call the police.. they won't do anything\" I was so angry I called corporate on the way home. Corporate told me \"oh yes that's right, that's our policy. We have insurance for that!\" I said \"that's great you have insurance, but I'd like my police department and newspaper to know my town is going to hell...\" reply pstrateman 13 hours agorootparentprevYes because the police stop arresting people for it. If you don't prosecute, it effectively stops being illegal. reply bombcar 12 hours agorootparentMost of the time. Certain crimes (eg public intoxication) often have an arrest followed by letting the person go when they’ve sobered up, so no official prosecution but not no arrest. reply pstrateman 9 hours agorootparentPublic intoxication would seem to be the only example of that though. Can you think of any other? reply wolverine876 6 hours agorootparentProtestors (whatever those charges are). Disobeying a police officer. Lots of things. The DA's resources are limited (unless all the Republican critics want to pay more taxes!). Arrest is enough of a deterrent for minor crimes. reply outop 3 hours agorootparentprevLoitering, prostitution, vagrancy. reply selimthegrim 7 hours agorootparentprevLittering, poaching reply beepbooptheory 11 hours agorootparentprevIf true, maybe. But this very point makes this whole line of argumentation unfalsifiable and in that a little limp... reply lmm 7 hours agorootparentIn the UK crime statistics are collected from a survey of people's experiences of crime that's completely independent from police/arrest records. Does the US not do this? reply pstrateman 9 hours agorootparentprevOf course it's falsifiable, start prosecuting again and see if the arrest rate goes up. reply beepbooptheory 8 hours agorootparentBut the point is that would not tell us anything about the relative frequency of the crime when it's not prosecuted! You are saying, effectively, that we know that smoke always comes from fire simply because when you light a fire, you see smoke. You can't argue that policing deters crime simply because when there is policing, crimes are prosecuted. That makes a lot of bad assumptions about the nature of crime itself that I don't think a single criminologist would follow you on. reply wolverine876 10 hours agorootparentprevDo you have evidence of that? reply zarathustreal 8 hours agorootparent..my friend, what world do you live in where civilians are gathering their own evidence, of any kind, much less evidence related to police behaviors? reply wolverine876 6 hours agorootparentYou don't need to gather your own. Researchers and journalists do it, and you can find them pretty easily ... reply x0x0 10 hours agorootparentprevLots of crime doesn't get reported. The police where I live are transparently useless. Someone tried to steal my car at bart. Even my auto insurance didn't bother asking for a police report while paying out $3k for a repair to the door. Everyone involved understands it's an utter waste of time and not a thing will happen. For at least n=1, no crime happened. reply nathan_douglas 7 hours agorootparentTwenty years ago, in a small college town, my car was broken into and the stereo stolen. I called the police out and the cop said, “okay, what do you want me to do about it?” Well, I don’t know. What should be done about this? I guess I thought my report might be tabulated, that perhaps a pawn shop or two might be called. My wife and I were living financial-aid-reimbursement-check to MGIB check to work-study check at the time. We got the one of the cheapest car stereos we could find, but it still hurt. We had just gotten it installed literally that day, and the next morning it was gone. I’m sorry to have wasted your time, officer. There’s probably a kid with a one-hitter that you could arrest on your way back to the station. reply causality0 9 hours agorootparentprevThat's very interesting. It would explain why the \"high level\" takes such as papers and articles keep saying crime is plummeting but all the anecdotal accounts are \"crime is getting worse and worse and nobody does anything about it\". reply x0x0 5 hours agorootparentWell, there's also hard data like this: https://www.cjcj.org/media/import/documents/san_franciscans_... Take a look at the chart on page 2. reply wolverine876 6 hours agorootparentprevBut that's been true for generations. It wouldn't explain some perceived surge now. What does explain it is what explains many other things that don't match facts, about the economy, vaccines, climate change, election legitimacy, Obama's birthplace, etc. Whatever the conservative message machine focuses on, generally a large portion of the population believes. reply malfist 13 hours agorootparentprevAh yes, New York, home of \"stop and frisk\" and it's other soft on crime policies. reply gottorf 8 hours agorootparentNYPD hasn't been stopping and frisking in a while. reply tylerFowler 12 hours agorootparentprevTrue - not specifically related to fleeing workers, as I understand it (wasn't there at the time) the office usage was more or less static downtown through all of that. Though, nonetheless, most of the buildings I cited (and many more) remained vacant so over the grand scale of the 150 year history or so of that city, one could say office space was largely unused. Interesting point on older buildings being easier. I would have thought quite the opposite. Commerce Tower was one of the \"newest\" buildings converted and it was built in 1965. Although, I suspect older buildings are still an untapped resource in many cities depending on what we mean by \"older\". reply masom 11 hours agorootparent\"older\" and \"newer\" is the construction type. Think of an old brick building with several stories and a window per floor vs a new steel + concrete building with windows spanning multiple floors. The \"older\" builder like the converted one in the parent post has small windows, allowing easy subdivisions. Newer buildings have windows spanning multiple floors and need to be retrofitted and on a skyscraper that comes at a huge cost. The bigger ticket item is the plumbing and ventilation, and to some extent the electrical. Ventilation is needed around the cooking area and washrooms, adding that to a building not purposed for this is challenging (where does the \"contaminated\" air go out?). It's often cheaper to bomb down the building and start over than doing a conversion on a new highrise. You'll see this often where they gut the entire structure and floors, keep a few walls/supporting structure, and build new. reply bobthepanda 10 hours agorootparenti think the best way to describe it is to consider what an office in these types of buildings looks like. consider a detective office in a movie from like the '50s. the office is small, primary illumination is from large, openable windows, maybe there's a front section for a secretary. that happens to be pretty more or less around the ideal size for an apartment as well, though for more bedrooms you probably need to merge adjacent offices. now consider the office block from Office Space. it's extremely large and dark in the middle. it is so large that there is no way you could possibly get natural light into the middle easily. in Office Space that's kind of the point, the darkness and required artificial lighting makes it super depressing and a dystopian commentary on the modern economy. who would want to live in an interior like that? reply tylerFowler 10 hours agorootparentWell.. yeah when you put that way I see what you mean. No those types of buildings wouldn't be very amenable, I suppose I had assumed we would already exclude those from consideration. Note that none of the buildings I mentioned are this way (the Wikipedia links have pictures). Although.. I did live in an old saddlery building there that is somewhat like what you're saying, the hallways were just made wide and apartments very long to ensure window access. Still, an Office Space style building will never be that. reply bobthepanda 10 hours agorootparentthe problem now is that in 2024, these Office-Space style buildings are getting into their 40s and 50s, and with the glut of new class A office space being made available in more contemporary bright, airy open styles, that's the kind of building that is going to be a struggle to fill with new office tenants or convert to residential. reply wolverine876 6 hours agorootparentprevI don't mean this in the usual snarky way, but read the OP. It goes into detail with a developer about these issues. reply matt-p 12 hours agoparentprevI think to be fair we have been doing this in Europe for quite a while, many apartments were once factories or something else 100 years ago. I think, from my understanding, the greatest challenge is in turning modern office blocks into housing. They are usually really big (10,000 -40,000 sqft) floor plates so there's very little natural light to go around and the shape of the flats needed to get window access would be really impractical. Meanwhile the slab to slab heights, floor loadings and locations mean they're not good for industrial or any other use beyond offices. reply weberer 31 minutes agorootparentThey've been doing it in the USA for quite a while as well. Most former factories are now \"The Lofts at [name]\" reply tylerFowler 12 hours agorootparentprevIndeed, I think one of the \"newest\" redevelopments was Commerce Tower which is an all-glass contemporary styled office building but it's still a building built in 1965. It probably helps that that building was also quite thin and had centrally located elevator banks & old style mail chutes that meant all the offices were around the windowed sides anyway. reply matt-p 57 minutes agorootparentYes I'm thinking of buildings probably 1980/90s onwards. We are tearing them down and rebuilding apartments in some cases I'm aware of. That seems like a real shame to be honest. I know in canary wharf for example they are turning some into children's nurseries and schools which feels \"interesting\" I'm absolutely certain if they could cost effectively be converted for much much more lucrative apartments they would be. reply epolanski 9 hours agorootparentprevAlso, you pay property taxes on the actual floor sizes. So even if you turn into apartments the areas close to windows the rest of the space is still taxed even if empty. Thus, in areas like Manhattan it's virtually impossible to convert those buildings or you need to literally cut off the center of these buildings. reply RankingMember 14 hours agoparentprevI was under the impression that Kansas City was still in a bit of a dire situation as far as crime is concerned[1], so I appreciate you highlighting some positive developments. In particular, I'm surprised and impressed they made transit free- that's something I experienced in Estonia and thought was an amazing idea considering the cost of policing turnstiles and fare collection itself plus the benefits of people moving around a city via mass transit over individual vehicular traffic. [1]https://realestate.usnews.com/places/rankings/most-dangerous... reply nox101 14 hours agorootparentI feel like free transit is a bad idea in the long run. People generally devalue thing that are free in my experience. There's also culture, transit is seen as \"the thing poor people use\" in most of the USA and making it free just seems to re-enforce that prejudice. (oh, it's free? it must be for poor people, not me). Free would also mean it's a place to just hang out. Homeless? Sleep on the free train, why not? It's free! Oh, they wake me up at the end of the line? So what, exit and re-enter. It's free and at least not too hot or too cold and I'm not getting rained on. Of course the homeless should be cared for, but if they end up in the train system even less people are going to use it. Also, it's looked at as an expense for the city so there is always a push to cut it's budget or not raise it enough to do what's needed to make it good. It doesn't help that the previous two points make the non transit using tax base see it as a waste of their taxes. I'm totally for transit. Hate driving a car. Love taking good transit in Paris, London, Berlin, Tokyo, Seoul, Singapore, etc... So if free works great! But, if free ends up making things worse for transit that would be bad. I feel like Japan did a good job by privatizing their train system and giving the companies incentives to make the train system great by having them build and run adjacent businesses (offices, retail space, apartments, stores). The more people ride their trains the better these other interests do and visa-versa. Bad trains in this system = people move to a better line run buy a better company. They may not directly think that but they do hear that station X is the new up and coming place with all the cool stuff nearby and much of that is from train company investment so their appears to be a positive feedback loop. reply ensignavenger 13 hours agorootparentKansas City Metro has stated that since going fare free they have seen an increase in \"nusiance riders\" (riders that don't follow the rules and get hostile with staff when given instructions). They are trying to find ways to combat this and have considered reinstituting fares, even just a small one. It will be interesting to continue following their experiment. reply jandrese 12 hours agorootparentAdding fares just means the nuisance riders will jump the fares. It won't stop them from riding. reply ensignavenger 11 hours agorootparentIf that were the case, the theory is that the issues would not have increased after they went fare free. The increase may have been due to something else, though. Experimentation is probably the only way we will truly find out. reply coffeebeqn 11 hours agorootparentprevIt coincided with opening the new light rail in the most popular nightlife areas. Hardly a laboratory setting reply friendzis 2 hours agorootparentYour point is nightlifers correlate with nuisance riders? reply cogman10 13 hours agorootparentprevDoes Kansas City have a police presence on the tram? I'd expect that to be more effective than a fare. reply tylerFowler 12 hours agorootparentOhhh yeah, they definitely do. Usually just on weekends or for events but you have fully uniformed & armed officers riding the loops. reply tylerFowler 12 hours agorootparentprevThe key thing that KC did in ~2014 or so is that they rebuilt a \"streetcar\" (identical to SFMUNI light rail so that name was a marketing tactic for sure) line downtown where parking has been scarce & is being eaten up by new developments (a good thing). This was the first public transit to be totally free, and to combat the idea that suburbanites wouldn't want to use it they freed up payments on parking zones _up the street_, so that for any decent sized event it became the smartest way to park and not overpay. They also took a ton of time painting the trains in city colors or with city designs, keeping them incredibly clean, doing things like putting live music at every stop on certain days etc... It became really fashionable really fast to ride the thing. They also policed it like mad on the weekends. Buses on the other hand, are a different story and carry the same stigma. Though I'm still really proud of KC for making that free as well. reply hibikir 10 hours agorootparentKC was smartin choosing lines that had actual demand, so the lines remained popular: Go to the other side of the state, and see what happened in the streetcar in St Louis. It's only running because running it seasonally at a loss is cheaper than paying back the federal government for their share of the project. No amount of pretty colors fix the fact that nobody needs to travel that route, and nowhere near enough people live near the stops reply toyg 11 hours agorootparentprevEffective commercialization of station space is indeed a positive development, but you don't really need to privatize railways to get that (even without going into the whole \"privatizing in Japan is not the same as here\", since large private companies and local authorities coordinate strongly in ways that we wouldn't consider acceptable in the West). Very dense European cities, like London and Paris, are getting more and more of that type of development too; and even in Tokyo, not all stations have a commercial development on top. It's mostly a function of density levels, which are sky-high in Japan. One clear element of the Japanese system is that stations are hugely overmanned, and staff are still paid pretty good money. That means facilities are spotless, and drifters or nuisance riders are removed promptly, making the system more appealing. This is very hard to implement in the West, where the sacred fear of unionization pushes for constant cuts, both in the number of humans involved in any task and in their remuneration levels. reply nox101 7 hours agorootparentI agree with your general points and I don't know what worked in Japan would work in the USA. I'm pretty confident what works in Europe will probably not work in the USA either though :( > That means facilities are spotless I can assure you no stations are remotely spotless. In fact I'm surprised some of them aren't considered fire hazards. Ueno, Shinjuku, Shibuya, Shinagawa, Akihabara, all have extremely messy areas. Newer stations appear clean but that's only because they're new AFAICT. Just commenting because I don't want people to ge the wrong impression. They might be cleaner than SF, NYC, Paris on average but they're not as clean as Stockholm or Singapore reply pezezin 4 hours agorootparentOf the stations that you mention, I find Akihabara to be actually quite nice and reasonably well organized (for Japanese standards). But I agree with you that the others are a mess. Shibuya and Ueno look like half the stations are falling apart, and Shinjuku is some kind of non-euclidean labyrinth, every time I go there, I get lost. If there is even a fire there, the death toll is going to be immense. reply nox101 1 hour agorootparentAkihabara, there are new and old parts of the station. The new part (lower level) are relatively clean. The old parts (upper level, Sobu-sen) are less clean (unless they've been renewed - I haven't been up in a 2-3 years) reply bobthepanda 4 hours agorootparentprevI think that you and the OP have different notions of spotless. In Japan, at major train stations I find the bathrooms reasonably clean and good to use. In New York, it smells of poop and pee because the odds are that someone probably publicly urinated, or some pipe somewhere in the subway tunnel is leaking sewage. reply nox101 1 hour agorootparentThat's generally true of all bathrooms in the USA vs Japan. I think I've seen 1 disgusting bathroom in Japan a restaurant/mall/store in the more 15+ years I've spent there where as in the USA it's like 1 of 5 that's disgusting. In other words, it's not unique to train stations. That said, the bathrooms between JR Shinjuku and Marunouchi-line used to smell pretty rancid. I haven't used them recently though. reply nomel 5 hours agorootparentprev> transit is seen as \"the thing poor people use\" in most of the USA That’s the objective reality, in most of the USA. The only way they’ll attract people that aren’t forced to use it, for financial reasons, is to remove the piss and puke from the floor, the stains of the same composition from the seats (although rock hard plastic is getting popular to compensate), and prevent, not remove, the guy with obvious drug and mental health issues, and now has his dick out, from boarding. If you want something to be attractive for the general population, it has to, literally, be attractive, or at least not repulsive. Otherwise, the shorter commutes and clean interior that personal transport gives is the obvious choice, as it currently, measurably, is. But, that can’t happen, because anything nice will be destroyed/vandalized, because anyone can board. The goal of not discriminating against anything or anyone is a choice that society has made (probably the correct one) that makes public transport literal shit. I don’t miss the few times I used it, then noped back into my car. reply warcher 8 hours agorootparentprevHonestly the places that did it successfully didn’t try to make transit cheap, they didn’t subsidize cars. Requiring amenities like parking by law seems to be working at cross purposes to spending money on transit. For desirable real estate it seems the only way to make affordable housing it to take away expensive upgrades. I think we can agree that safety is not a luxury, but parking spots, private laundry, private kitchens, even private bathrooms, these are things that many people live without every day. I remember some of those times in my life fondly even. Making it illegal by force of law to live like that seems… counterproductive to me, if our stated goal is affordable housing. reply matt-p 12 hours agorootparentprevI suspect charging a nominal but non zero fee e.g $2 on peak $1 off peak per trip probably ends up with the best of both worlds. Free ends up with some negative side effects reply bluGill 10 hours agorootparentA couple dollars pre day is nothing to most people but multiply by thousands of riders every day and it is a lot of money. There is no transit system in the world that couldn't be better with more funding so take that money. You of course need a plan for the poor but that should be a minority. reply jen20 13 hours agorootparentprevMy experience of the Kansas City tram is that everyone uses it, if they're already downtown. Admittedly I only see KC for ~5 days per year but I get the impression it's well handled. The city has changed beyond recognition in the 13 years I've been going. reply jimbob45 11 hours agorootparentprevWhy not just ban the homeless then? They’re not hard to identify and remove. reply seer 1 hour agorootparentprevI feel like this might be a double edged sword in the long run. A lot of public transit where I live (Mumbai) is subsidized, which means its not profitable to run the way it is run. That was a very popular decision at the time (help mobility of the poor), but it ends up with massive underdevelopment as the needs expand. There’s no money for private companies to invest in the infra and it waits on election cycles to push investment. I feel that if it was actually profitable to build new trains / metros / etc it would have happened much quicker. reply tylerFowler 12 hours agorootparentprevYeah crime post-pandemic there is still a major problem, although typically concentrated in poor neighborhoods as opposed to downtown. When I first moved there, the inner city was considered really dangerous and I saw such rapid gentrification that you'd see people walking their dogs in the middle night without incident just within 8 months or so. So it just sorta.. moved. I definitely wouldn't say KC has made many inroads on crime despite the massive boom it's had in the inner city core, which did increase foot traffic and makes people at least feel safer. reply Zak 11 hours agorootparentprevCrime rates on that site appear to be from 2020. reply justanother 12 hours agoparentprevTo your point, I've watched with interest the redevelopment of the West Bottoms. I don't live anywhere near Kansas City anymore, but in the 1980s and 1990s, we teenagers used a large portion of the Bottoms around the 12th Street Bridge to play hide-and-seek at night, and we never encountered another soul (people were just too scared to be in the Bottoms at night, but we were young and crazy). Just a desolate area with tall neglected brick buildings from 1900, with some alleys that were still dirt. But I'm blown away now at how small businesses are taking it over block-by-block and turning it into a kinda pleasant place. Surely this could not have been possible without some civic backing (the soil contamination in the Bottoms was simply awful and required extensive EPA cleanup and then some), but as you note, policy plus cheap prices appears to be turning it around. reply tylerFowler 12 hours agorootparentThe redevelopment of downtown did push out many of the artists and so they packed up & moved to West Bottoms. In general KC is such an arts town that people genuinely like to go where the artists go, it's a very cool vibe. West Bottoms is packed with record stores and underground (literally) event venues though the Halloween event people still take up most of the space that might be good for living/working. reply tetris11 34 minutes agoparentprev> 750sqft 70m2 for anyone wondering reply asdfman123 11 hours agoparentprevI'm hardly an expert on this, but it seems the exact same thing happened to all sorts of cities in the 2010s. Millennials wanted to live in cities. It's possible some cities handled it better than others, but still worth pointing out. reply b800h 6 hours agoparentprevOne problem with doing this - as seen in Manchester in the UK, is that the new residents will demand that their neighbouring bars and clubs be closed to cut down on noise and disturbance. It has the effect of gutting a city centre of it's culture and nightlife. reply aquova 9 hours agoparentprevI know people who live nearish the KC downtown area, and live in an old elementary school that was converted into an apartment complex. A bit weird architecturally, but they seem to like it. reply melenaboija 14 hours agoparentprevI have seen something similar happen in Birmingham, AL while living there since 2015 to 2022. reply listenallyall 8 hours agoparentprevRedevelopment of derelict downtowns has been outsourced to massive real estate corporations who have created a successful formula that they can pretty much copycat over and over. Kansas City's Power & Light district, which you mention, was built by the Cordish Companies, which cut and paste in Louisville and Baltimore, among others. https://www.cordish.com/businesses/entertainment-districts Certainly there are benefits of revitalizing urban spaces but the fact that it is entirely engineered, has little room for local entrepreneurs, and most financial benefits flow to a small cadre of real estate giants, is also somewhat concerning. reply pwthornton 20 hours agoprevConventional wisdom is thaty only certain office buildings can be converted to housing. The depth and shape of the building matters quite a bit. A lot of office buildings are very deep and would result in a lot of rooms/space without windows or access to natural light. The DC area is doing a pretty good job with conversions. A lot of these midrise buildings are a good fit for this. Although the very broad midrise buildings are a poor fit. But I wonder if we could challenge the conventional wisdom on conversions of deeper buildings. Could we come up with novel things to do with this deep interior space? reply Kon-Peki 16 hours agoparent> Conventional wisdom is that only certain office buildings can be converted to housing. If you can buy the building cheap enough, conventional wisdom can be thrown out the window. I once toured a building that had been converted from an old warehouse to residential. Huge floorplate. They had built the condo units around the edges, created a hallway, and then the inside was divided up into \"storage\" spaces. Each condo owned the space directly across the hallway. They were very large, and people had transformed them into offices, arcades, workshops, playrooms, theaters, etc. You could do just about anything you wanted with the space, and because it wasn't \"living space\" you didn't have to worry so much about noise and the property taxes were lower than they otherwise would have been. You can't sell it for a price that includes that space as \"living space\" either. Which goes back to the point - if you can buy the building cheap enough, you can make anything work. reply deltarholamda 11 hours agorootparent>If you can buy the building cheap enough, conventional wisdom can be thrown out the window. Maybe, but you can't throw out the building codes. A warehouse, certainly, can be retrofit. But office towers? Almost certainly not. Elevators are not sized for residential; electrical service not sized for residential loads (dishwasher/dryer/microwaves/ovens); HVAC not sized for residential heat loads (same as above); metering requirements means the existing electrical rooms are not large enough (they are never large enough); plumbing and sewer are not sized for residential. It goes on and on. Even if you got the building for free, you'd still want to run the numbers to see if it's still cheaper to demolish and build again. It's not entirely clear whether it is or is not. It can be done if you make huge, expansive apartments, which has to be read as \"really expensive\". There aren't that many really rich people who can drop 5 figures per month for an apartment. reply bobthepanda 10 hours agorootparentit's also possible for offices to be doing poorly but also still make more money than as a residential property. while vacancy rates are high, those vacancies are spread around, so some people are still renting in these office buildings. if you want to redevelop them you have to get the existing tenants out. reply rovolo 9 hours agorootparentprevI understand electrical and plumbing, but why does the HVAC need to be bigger/smaller? reply MrLeap 8 hours agorootparentJust conjecturing, but I assume an apartment contains more ovens than an office. Would probably make it take more cooling? reply Ekaros 8 minutes agorootparentThey also contain lot less people. Average person generates 70-100W at rest. Add that to what ever screens, computers, extra lighting. And it is not that big difference in load. reply elevatedastalt 14 hours agorootparentprev> If you can buy the building cheap enough, conventional wisdom can be thrown out the window. Conventional wisdom is that there won't be too many windows available to throw conventional wisdom out of :-) reply rgblambda 14 hours agorootparentEspecially if the windows don't open. reply chii 6 hours agorootparentThis is where you unconventionally put an LCD screen where a window should be, and put in HVAC systems to blow fresh air. reply TeMPOraL 3 hours agorootparentYou jest, but if someone makes a decent enough full-spectrum daylight simulator, and then get a stamp of approval from FDA (or whichever the authority is relevant here), they stand to make a lot of money by enabling residential use of windowless spaces. (Cue arguments on how dystopian this is. But hey, if the daylight simulator was to-spec, it wouldn't be the worst of things.) reply askl 1 hour agorootparentThere was Munger Hall [1] but they didn't actually build it thankfully. [1] https://en.wikipedia.org/wiki/Munger_Hall reply leoedin 16 hours agorootparentprevThat sounds great! The thing I love most about living in a house is the prospect of “engineering space” - places to do carpentry, electronics, home maintenance etc. Living in an apartment (assuming adequate noise isolation) is actually great otherwise. reply mitthrowaway2 15 hours agorootparentWould a space in the center of an office tower really have enough air ventilation for ordinary hobby-maker work like sanding, soldering, painting, resin molding, grinding, etc? reply pragma_x 15 hours agorootparentConsidering what some people do with basement spaces and almost no ventilation at all, I'm going to hazard a guess that a windowless interior room is not a dealbreaker. reply ElevenLathe 13 hours agorootparentPlus the fact that the commercial space that's being converted was often used for this sort of thing already anyway. Yes, this is mostly office space, but many engineering firms have an electronics lab or small prototyping workshop in their \"office\" space. reply nine_k 13 hours agorootparentprevOffices have ventilation. They are usually more densely manned than apartments. Unless they specifically tear it down during redevelopment (why would they?), there should be plenty of air circulation. reply dbuxton 3 hours agorootparentIf you read the OP you will see that in one example that’s exactly what they do do - in order to reclaim the space occupied by HVAC and turn into more apartments! reply et-al 15 hours agorootparentprevIf anything with large towers, I wouldn't be surprised if the HVAC is routed adjacent of the elevator shafts in the center of the floor. reply bombcar 12 hours agorootparentprevThe danger is that people will be tempted to use the unlivable space as living space, and then you get some massive fire that kills a bunch, and then reactionary laws that prohibit everything uselessly. reply jermaustin1 15 hours agorootparentprevNYC has a few places like this, but typically it is living space above the ground floor, and you can rent \"dont-ask-dont-tell\" space in the basement. Most people use it for storage, but I've seen a few workshops in them. reply hedora 13 hours agorootparentprevI’ve seen similar, except the building had > 10 ft ceilings for some reason (it had been a factory). The owners built a ~ 9ft “building” that was missing a wall (and ceilings) inside the space. That was where the kitchen and bedrooms were. Light came in through the open wall in the kitchen, and from where the drop ceiling in the bedroom would have been. The rest of the factory floor was hobby / office / entertaining space. It was spectacular. reply shagie 7 hours agorootparentprevThe shape isn't always right either. Pipe fixtures can be in the wrong spot. Structural pillars can't be removed. https://www.washingtonpost.com/opinions/interactive/2023/cit... https://www.nytimes.com/interactive/2023/03/11/upshot/office... Note that things such as arcades may need additional power demands that the area isn't sized for (fire risk). Workshops may need added ventilation and woodsheds would require additional power too. A theater type area could have issues with fire codes and occupancy. reply kumarsw 12 hours agorootparentprevI dunno if it's practical, but the idea of an urban \"garage\" for woodworking/storing your kayak sounds kinda awesome. reply tommychillfiger 8 hours agorootparentprevDamn, that sounds like an ideal place to live for me. I live in an apartment but also play music. I would love to be able to set up a drum kit (and maybe store an e-bike) and still be in my walkable area for cheap ish rent. reply LeafItAlone 15 hours agorootparentprevAre you willing to share where that building is? It sounds like a dream to me. reply Kon-Peki 15 hours agorootparentIt was in Chicago, west of downtown reply mortenjorck 14 hours agorootparentI read your description and thought of a building in Chicago, but there must be a few like this. The one I was thinking of is in River North along the river. It’s a beautiful building, with roomy communal spaces and vintage timber all over. I probably would have bought there if I weren’t too noise sensitive for timber floors. reply beaeglebeachh 15 hours agorootparentprevYes west of Chinatown. Cermak I think. Spice warehouse in days of old IIRC. Was used for raves decade+ ago. Place is a trip. I camped on top of it once when I was homeless. I've forgotten of that place for years. Truly magical. Thank you for the memory. I almost shake recalling it. reply Kon-Peki 15 hours agorootparentI think there are multiple of these kinds of buildings. The one I looked at would be considered West Loop these days. reply hamburga 12 hours agorootparentI’m actually looking at “West Loop” apartments right now. Would you mind sharing the address of the building? reply cozzyd 11 hours agorootparentIt sounds like maybe 165 N canal if my recollection from when I was condo shopping is correct (it was a bit above my price range, but I seriously envied the large storage space). The condo I ended up buying (a converted office building in the east loop) has a similar hall of storage rooms but they are much much smaller and not practical for anything other than storage. reply Kon-Peki 11 hours agorootparentprevSorry, it’s been a long time and I don’t remember. I know that we were looking for places that were within a 15-20 minute walk from the Loop offices where we worked. reply SergeAx 7 hours agorootparentprevOld warehouses were built very differently compared to modern office buildings. Specifically, office buildings are made to the load requirements of office spaces, with their symbolic walls and furniture. You may have trouble putting even bathtub into an apartment converted from an office. reply jrockway 17 hours agoparentprevAs someone who has spent hundreds of dollars on blackout curtains (and sticking electrical tape on every LED in the house), I'd be happy to buy an apartment where few of the rooms have natural light. I know bedrooms have to have windows so the fire department can pull you out of a burning building while you're asleep or whatever, but personally, I am not a fan of the noise and light most of the time. I just think there is so much space that you can use in a residential setting without natural light. Your movie room. Your bedroom if you feel like not following The Law as to where they're allowed to be. All your 3D printers and other maker activities. If it's space that nobody wants, I'd personally buy it at a discount if it were offered to me. reply kbenson 17 hours agorootparentIt's less about natural light than ventilation. If whatever ventilation systems the building uses breaks down, interior rooms without opening windows are a liability. reply WalterBright 16 hours agorootparentModern skyscrapers do not have openable windows, so the building ventilation system is totally relied on anyway. reply jandrewrogers 15 hours agorootparentOne of the newest and tallest skyscrapers in Seattle, Rainier Square, not only has openable windows throughout but on some high floors has massive sliding windows that open up to a sheer drop (widely recognized as a bit of a potential safety risk). reply ceejayoz 15 hours agorootparentClosed windows won't necessarily save you. https://en.wikipedia.org/wiki/Death_of_Garry_Hoy > While giving a tour of the Toronto-Dominion Centre to a group of articling students, he attempted to demonstrate the strength of the structure's window glass by slamming himself into a window. He had apparently performed this stunt many times in the past, having previously bounced harmlessly off the glass. After one attempt which saw the glass hold up, Hoy tried once more. In this instance, the force of Hoy slamming into the window removed the window from its frame, causing the entire intact window and Hoy to fall from the building. reply avarun 14 hours agorootparentThey won’t save you if you’re a complete idiot, yes. Why did this guy think this was in any way a safe thing to do? reply ceejayoz 14 hours agorootparentI’ve always wondered what he thought on the way down. reply WalterBright 8 hours agorootparentI saw a video of a daredevil who decided to film himself hanging on to the roof of a skyscraper with his fingertips. He did a couple of pullups, but exhausted himself and could not pull himself up back onto the roof. Finally, he let go. Pulling oneself up onto something by one's fingertips takes a lot of strength. Should have tried that beforehand. (It looks easy in the movies, but those stunt people are very fit.) reply winkywooster 14 hours agorootparentprevwhoa, this links to https://en.wikipedia.org/wiki/List_of_unusual_deaths, and there are some crazy deaths. one that stands out is Kurt Gödel: \"The Austrian-American logician and mathematician developed an obsessive fear of being poisoned and refused to eat food prepared by anyone but his wife. When she became ill and was hospitalized, he starved to death.\" reply kbenson 15 hours agorootparentprevSee my cousin comment about Central Park Tower. Modern commercial skyscrapers seem to not have openable windows. reply valenterry 16 hours agorootparentprevIsn't ventilation most of the time built into offices and hence much better than what you can get in most residentual buildings? reply dv_dt 14 hours agorootparentI would think that the people density that an office space is built for is higher than residential density. Even a studio apt is more individual space than many shared office layouts. reply bloomingeek 16 hours agorootparentprevMost high rise office buildings use steam (older bldgs) or heat strip for heating. If there's an electrical problem, all fans blowing the treated air stop. They use chilled water flowing through air handling units for A/C. (in winter, these same units supply the heated air, the flow of chill water is usually halted.) In a system like this, if the AHU stops for any reason, the whole floor is effected. Since chillers and associated equipment are very expensive, I would imagine the maintenance fees would be uncomfortable. (get it? Sorry.) reply Loughla 15 hours agorootparentI think maybe spread out across all owners it might be palatable. You have to figure, if your heating/AC conks out, you're into that for 20k. So that's a pretty decent number when/if you're talking about multiple flats on one floor, right? reply jrockway 5 hours agorootparentEvery big building I ever lived in in Chicago had this system but for residential use. I am sure it is expensive when it breaks, but it is also amortized across hundreds of units. I'd go so far as to say it's \"industry standard\" so isn't going to be much of an obstacle for converting office buildings to residential spaces. (In NYC, we all agreed to believe that there is no such thing as residential air conditioning that is not in the form of a window unit, however.) reply kbenson 16 hours agorootparentprevWhen it works, sure. You don't generally sleep for extended periods in an office though, so would probably notice it getting stuffy. Sleeping or bedridden people might not notice or be able to easily do something about it though if oxygen levels drop. reply valenterry 15 hours agorootparentSleeping produces less co2 than working. Therefore, if you don't feel it getting stuffy (without opening windows) at work time, then sleep time should be no issue. Besides, offices have to deal with more people than residential buildings. And air quality depends on the number of people (and what they do). reply kbenson 13 hours agorootparentThe difference when sleeping compared to resting or low activity work is that sleeping is about 60%-65% of resting from what I've found.[1] I'm not sure why we should assume that shouldn't be a problem. We could be close to a low oxygen situation prior to sleep, and then start sleeping and have hours for it to get worse. We don't generally design safety regulations around \"should\" and averages, but instead when edge cases happen, as the magnitude of the outcome is very important to take into consideration. I'm not sure what you're trying to express with your comment. 1: https://www.engineeringtoolbox.com/co2-persons-d_691.html reply valenterry 3 hours agorootparentIt's simple: if working for 8 hours with many people is not a problem for the ventilation system, then sleeping with less (residential) people should be no issue at all. reply quickthrowman 16 hours agorootparentprevOffice towers do not have windows that open, aside from some very old ones might have windows that open. Commercial building windows in general do not open at all. reply kbenson 16 hours agorootparentMy understanding is you have to fix this when changing to residential. There are building codes requires to be met for residential housing, and normally that includes openable window space for both ventilation and egress in an emergency. Maybe they'll make an exception for egress, but I doubt they will for ventilation. reply WalterBright 16 hours agorootparentWhat good is an egress window 30 stories up? reply throwup238 10 hours agorootparentThe secret to flight is throwing yourself at the ground and missing. reply kbenson 16 hours agorootparentprevExactly why I would expect them to make an exception for it. Unless the local laws have been changed specifically to allow for that situation, I doubt the laws started out that way though. I don't imagine the people designing building codes for residential living put a lot of thought to extremely tall buildings initially. reply amanaplanacanal 16 hours agorootparentprevI don’t understand building codes. It’s not safe for people to sleep there, but it’s safe to work there for eight to twelve hours a day? Something is off. reply asalahli 15 hours agorootparentPresumably because you're awake while working and can notice problems when they happen. Not so much when you're sleeping. reply MatmaRex 15 hours agorootparentprevI mean… yes? I don't see what's so confusing. In an office building, if anything goes wrong, an alarm goes off and everyone leaves, and insurance pays for damages. In a residential building, you have people sleeping, sick, possessions they might not be willing to leave behind, babies, pets… It makes sense for the safety requirements to be different. reply lazide 15 hours agorootparentprevAre you regularly alone and unconscious when at work? reply coryrc 15 hours agorootparentprevLots of tall condos have windows that can't open. reply kbenson 13 hours agorootparentI'm not sure if you're being lax in your terminology or whether you are misinterpreting my point. The problem is not that every window needs to open, it's that some windows need to open. In the building codes I've seen in the past for residential homes, that was expressed as a percentage of square feet of the room or entire building. So, are you saying there are plenty of tall condos where no windows in a specific dwelling open, or that they have some windows that don't open? If they have no windows that don't open, do you mind mentioning where, as I'd be interested in what the solution was to the problem of needing to allow for passive ventilation. reply coryrc 5 hours agorootparentThere is no requirement for passive ventilation. My friend owns a condo in Vancouver. No windows open. Also Seattle: https://www.reddit.com/r/Seattle/comments/s2s7bx/cant_open_a... Also https://www.reddit.com/r/askTO/comments/omdxmp/downtown_unit... reply kbenson 4 hours agorootparentThanks, that's fairly clear and concise, at least for the locations you noted. It does appear to be that in New York it might be required though[1], so it's possibly still a problem depending on area. I'm not going to act like I'm an expert on reading building codes or that one in general though, so I could be misinterpreting it. SF had what clearly seemed like a mechanical ventilation exception in it when I just looked, but SF and Manhattan are the only things I looked up to compare to see if I could find whether it seemed fairly universally allowed or not. 1: https://up.codes/viewer/new_york_city/nyc-building-code-2022... and https://up.codes/viewer/new_york_city/nyc-building-code-2022... reply jen20 15 hours agorootparentprevThis very much depends on where you are. I had an apartment in a high-rise building in Austin TX a few years ago that did not have openable windows of any kind (I also did not realise this until after signing the lease, which was unfortunate). I assume the building met code. reply NoMoreNicksLeft 16 hours agorootparentprevThis seems wrong... are people opening windows on the 60th floor? What about those buildings whose design precludes easy retrofitting to openable windows? I'm not being rhetorical, I'd like answers. reply kbenson 16 hours agorootparentI figured looking up tall residential building and seeing what they do would be a good indicator of norms. The tallest Residential building in New York is Central Park Tower at 98 above ground floors: The residential stories have casement windows, within the curtain wall, that can swing up to 4 inches (100 mm) outward. In addition, some condominium units have motorized windows at least seven feet (2.1 m) above the floor.[1] The condominiums start on the 32nd floor, according to the same article. If building can't easily be retrofitted to allow openable windows, then I would assume they either can't be used for residential or they could try to get some sort of exemption if they can prove it's safe. I'm mostly going off what I know about building codes and what I've read previously on the topic when it's posted and it's delved into what the actual problems are in converting to residential. 1: https://en.wikipedia.org/wiki/Central_Park_Tower reply JackFr 15 hours agorootparentprevOpening windows on high floors is great -- you don't need screens cause most urban insects stay much closer to ground level. (Although admittedly I haven't opened any windows higher than 20) reply whartung 9 hours agorootparentprev> I know bedrooms have to have windows so the fire department can pull you out of a burning building while you're asleep or whatever When we were looking for a new house we toured one that had a bit of remodeling done. What was done was the back patio was enclosed to turn it into a room. The detail was that there was an existing bedroom that had a window onto the patio. The solution, which I have to assume passed code, was simply to retain the bedroom window, even though it did not lead to the outdoors. So this house had an interior window. reply sokoloff 13 hours agorootparentprevI think in most places that bedrooms just need two means of egress plus possibly a specific ventilation requirement, which is most commonly met via a door plus a window, but could be met by two doors to two different legal means of egress (and an HRV/ERV if ventilation is required under locally adopted code). reply ldjkfkdsjnv 17 hours agorootparentprevI rented a place specifically because one of the bedrooms has no windows. Easily one of the best quality of life improvements. reply webdood90 17 hours agorootparentGreat, until it's not. You have no escape in case of an emergency. The room probably wasn't technically a bedroom. reply JumpCrisscross 16 hours agorootparent> You have no escape in case of an emergency Nobody is exiting the 60th floor of a skyscraper through the window. We don’t even have ladders that go that high on firetrucks in New York [1]. [1] https://www.fdnysmart.org/fire-trucks/ 95 ft, or about 10 stories reply BobaFloutist 16 hours agorootparentWhat, you don't keep a parachute under your bed? reply renewiltord 17 hours agorootparentprevI think the probability of that is lower than my probability of death on my motorcycle. So they're probably going to be fine. Everything is fine until it's not. The risk tradeoffs we disallow are ones where you need to be 1σ+ to be making the tradeoff because the crucial functionality there we provide is legibility in the marketplace. reply ldjkfkdsjnv 16 hours agorootparentprevHigh up in an NYC building, there is no escape regardless of windows. Also, my quality of life with good sleep is so high, its easily worth it reply giantg2 17 hours agorootparentprevThat's easily remedied - axes and specialized entry tools can be used for exiting. Yeah, if it doesn't meet the legal definition of a bedroom, it can't be listed as one. That's partially why there are so few interior rooms - lower property value vs if it was a bedroom (but mainly consumer demand for windows). reply SkyPuncher 16 hours agorootparentEven simpler, many interior walls are built with studs covered by drywall. Most able bodied adults can break drwall between the studs. It’s not a particularly strong material. reply throwway120385 16 hours agorootparentIt really depends on the drywall. If it's 3/4 inch soundproof drywall over sound insulation with services like water, electrical, and sewage you can break it but now you have to navigate the services. And many people aren't going to shove themselves through studs on 16\" centers. reply SkyPuncher 15 hours agorootparentThose services are in very limited places in interior walls. You may have an electrical line running to the outlets, but you be extremely unlikely to hit sewer or water. If you do, the gap ti the left or right is extremely unlikely to also have the same services. reply giantg2 15 hours agorootparentprevIf they're worried about not fitting through 16\" studs (let alone this entire scenario), then they should select a room with two doors. If you can't fit through studs, I find it hard to believe they're fitting through most windows (generally a more awkward position with limited dimensions too). reply jrockway 16 hours agorootparentprevYeah, you can put beds in whatever room you want. You just can't sell a room with no window as a bedroom. reply bluGill 9 hours agorootparentIn some cases you can - most of the time codes don't allow it, but there are exceptions. Check the local laws.. reply eropple 17 hours agorootparentprev> That's easily remedied - axes and specialized entry tools can be used for exiting. You keep an axe in your bedroom? reply WalterBright 15 hours agorootparentYes. Seattle is earthquake country, and I want to be able to get out if the doors are blocked or jammed from earthquake or fire. It's a fireman's axe, as that job is what they're designed for. I also keep a fire extinguisher in the bedroom. reply bombcar 12 hours agorootparentI mean you’re also an axe murder, but that’s besides the point. ;) reply WalterBright 8 hours agorootparentIf you've ever handled a fire axe, you'd realize it makes a lousy weapon. It has a long handle, with a heavy head. The idea is it can build up a lot of momentum to crash through things like doors and walls. But the long handle makes it difficult to swing in a melee, and slow to swing, and once the swing starts it will be very hard to change its arc. Hence, your target can easily sidestep it. I suppose it would be good against plate armor, but not many villains wear plate armor these days. You also have to be careful with a fire axe to not chop your foot if you miss. I don't think I've ever seen a war axe/hatchet/tomahawk anywhere near that size. For self-defense in close quarters, a baseball bat is ideal. P.S. I am no martial arts expert. reply giantg2 16 hours agorootparentprevMy bedroom has a window, so no. If your bedroom doesn't have a window and you're concerned about another exit, then sure. reply kibwen 20 hours agoparentprev> Could we come up with novel things to do with this deep interior space? Yes, here's an article with great visualizations on how developers are coring out the center of repurposed office buildings in order to create columns of natural light (and how, in certain jurisdictions, this lost square footage can then be reclaimed via new construction stacked on top of the building): https://www.nytimes.com/interactive/2023/03/11/upshot/office... reply MichaelZuo 17 hours agorootparentOr even better just give every unit multiple huge bathrooms, dens, libraries, etc... reply geodel 15 hours agorootparentWell of course one can even have whole floor for single family. It is a small matter of how much one can pay for the space of that size that need to be hashed out. reply MichaelZuo 13 hours agorootparentAn office to condo conversion is going to be luxury regardless. reply gumby 16 hours agoparentprev> But I wonder if we could challenge the conventional wisdom on conversions of deeper buildings. Could we come up with novel things to do with this deep interior space? What if every other floor were removed, so all the apartments had loft ceilings? Then light could penetrate from the upper windows deeper into the core of the building. You could even have rooms (like bathrooms or home offices) with lower ceilings and skylights. reply pnw 15 hours agorootparentImpossible to pull off on a modern office building designed with pre-tensioned concrete floors. Cheaper to demolish the building and start again. reply gumby 10 hours agorootparentThanks, didn’t know the floors were also structural supports. I thought they just hung from the central core. Oh well. reply SamBam 16 hours agorootparentprevI was thinking instead you could remove a long rectangle from the middle of each floor, so that the two remaining sides could have windows facing each other. If you swapped the orientation of the removed rectangle on each floor, it would look rather like a Jenga tower with the middle block pushed out on each level. In any case, the central problem (but maybe required step) of all these kinds of solutions is going to be losing 50% or so of the potential floorspace. reply cduzz 16 hours agorootparentThere's a good \"odd lots\" podcast about this[1]. There's another of their podcasts about how apartment zoning rules make it hard to make \"family\" apartments[2]. Basically you need two egress points, windows that can open, windows in kitches and bedrooms. All of these are directly in contradiction to modern office buildings with open floor plans, fixed windows, shared mechanical systems. So it may be possible to convert offices into apartments, but it's very expensive and you end up losing a bunch of floor space. [1]https://www.youtube.com/watch?v=HNkLcD3PKyk [2]https://www.youtube.com/watch?v=76IHpt6q9ME reply gumby 15 hours agorootparent> you end up losing a bunch of floor space. You're losing 100% of the floor space when it's an unused commercial building. reply cduzz 14 hours agorootparentI'm not sure your accountant would agree. An un or under-rented building has some potential value based on various hand-wavy factors. Things may get better next year and you've only lost a year's of potential revenue. You may be able to hand the burning bag of dog crap to some star-eyed dreamer. Taking out a loan, applying for permits, etc, locks in the loss. You get to tell your bank the asset they've got for collateral is worth a lot less, but hey you've got a plan that involves chopping the building apart so there are big holes in it so you can rent it to residents instead of commercial leases. Hey it's a brand new market and a wonderful opportunity to get in on the ground floor. reply Ericson2314 6 hours agorootparentprevCan't recommend listening to the people being interviewed in [2] enough reply MR4D 15 hours agorootparentprevI'll second this - great podcast episode! reply dexwiz 16 hours agorootparentprevThe feature you are describing is a light well. They work, but I am not sure how well in the configuration you are describing. https://en.wikipedia.org/wiki/Lightwell reply marcus_holmes 3 hours agorootparentIn Germany they're called \"hofs\" and incredibly common. Many apartments only have one view out over the hof, facing someone else's apartment 5m away. You get used to it. reply jessetemp 16 hours agorootparentprevThat’s an interesting idea. If there’s enough room between floors already, I wonder if you could squeeze in some horizontal periscope skylights without removing floors. Just need to occasionally send someone out to scrub the exterior window like in that show Silo reply hn_version_0023 16 hours agorootparentMinus the part where you’re also condemned to death I assume? reply snarf21 16 hours agoparentprevI feel like this is not that terribly different than row homes in the city. They only have windows in the front and back and the back might just be a view of the building backing up on them from the other street. These houses already have a long skinny footprint and it works fine. It doesn't seem that hard to do the same with office buildings. Additionally, amenities like a gym, laundry, community room can all be placed in the center of each floor as desired. reply irrational 16 hours agorootparentBut a row house has a front and back door. In an office building, the entry would be in the middle. You could divide each floor up into four apartments - two long apartments stretching the width of the building, say on the East and West ends. Then two smaller apartments with windows on only one side on the North and South ends of the building. reply zmgsabst 16 hours agorootparentWhy not divide it into four corner apartments? reply hibikir 15 hours agorootparentFour corner apartments work great if the floor plate is small enough to divide the floor by 4 and end up with a sensible square footage. Many office buildings are just so much bigger that this would lead to massive apartments that have few interior walls. Those are not so easy to sell. That's a reason the modern residential skyscraper is typically a narrow needle, instead of being shaped like Sears tower. reply UncleMeat 19 hours agoparentprevThis is conventional wisdom, but it always felt odd to me. Loft apartments were originally created from business spaces. In the beginning, they were low cost because of some undesirable properties but have slowly become an extremely in-demand style (huge windows and tons of light being major selling points). Why was it possible to convert industrial space into living space 50 years ago but today it is not possible? reply yardie 17 hours agorootparentLofts evolved from old factory and warehouses. This was in an era before air conditioning and fluorescent lighting. You needed lots of windows to light factories and lots of vertical space for convection cooling to work. Factories were at the edge of town and that town grew into a city. Eventually, the edge was closer to the center than the suburbs. And a new generation wanted to be city dwellers. Cheap lofts were peak re-urbanization, but now they are some of the most in demand housing because of the large space and natural lighting. reply dsr_ 19 hours agorootparentprevBecause industrial space needs room for very large machines, which means large volumes and good access to them. Office cubicle farms around a services core are made up of small volumes and poor access. reply vel0city 16 hours agorootparentprevOld (like early 1900s old) industrial spaces are radically different structures than commercial office real estate from the 80s and 90s. reply bombcar 12 hours agorootparentprevBecause buildings that were already old 50 years ago were built before elevators and other massive height building techniques. Look at old factory pictures, usually about three or four stories max. That’s much more adaptable than a gigafactory or a World Trade Center. reply seanmcdirmid 14 hours agorootparentprevthe buildings built 100 years ago to be converted 50 years ago are very different from the buildings built 50 years ago to be converted today. Improved efficiency and optimization for purpose in building design plays some role in that. reply jxf 20 hours agoparentprevNot all of the building needs to be direct residential; for example, I could definitely imagine some light retail, a computer lab, a tool library, an indoor track, a gym, et cetera. reply flanbiscuit 19 hours agorootparentIn Tokyo you have buildings with levels of retail, restaurants, bars, karaoke, etc. I'm sure Seoul and probably other dense cities have this too. Korea Town in mid-town Manhattan also does this, but that's just one little block. I'm really surprised there's not more of this in Manhattan actually. I'm sure it exists more than I'm aware of, but it should be more prevalent in a city that dense. reply _fat_santa 18 hours agorootparentprevThe problem here is zoning laws, in most places in the US you can't build businesses right next to residential. We would have to change our zoning laws and do something similar to a 5 by 1 (bottom floor is retail, top floors are residential) reply wkat4242 17 hours agorootparentThe same in Europe, zoning laws are a bitch and they're basically existing for bribes. Many local city counsellors get paid off to change the zoning laws and thus raise the price of the briber's property. reply immibis 16 hours agorootparentAt least in Berlin this doesn't seem to be true. While zoning laws are a bitch and exist for bribes, none of them prohibit intermixing offices with residential or retail with residential, and such buildings are very common. reply wkat4242 15 hours agorootparentYeah true here in Spain it's very mixed too. It's better like that I think because this way the neighborhood doesn't become deserted at night and also we can go to a local restaurant for lunch. The 'zoning' here is more building by building rather than neighborhood based. But I mean the same kind of administrative issue holds back conversions here. Most Office buildings here would be ideal. They're not that big because here it's illegal to offer office space without plenty of daylight. reply kelnos 17 hours agorootparentprevRight, these laws absolutely should change. I would hope that any municipality facing a problem like this, with lots of empty office space, would also recognize the fact that their zoning rules are a bad idea. reply onthecanposting 8 hours agorootparentSometimes offices are empty because banks that own them dictate the lease rates and building managers can't adjust price to fit the market rate. I sometimes think city councils and codes get too much heat on this issue and the 800lb finance gorilla in the room is politely ignored. reply dylan604 16 hours agorootparentprevI used to live in 100+ year old warehouse building converted to residential. It has even less windows than a glass paned high rise. They used the interior space exactly like this. One floor had a gym. Some floors converted the space to storage units available to the residents. Other floors had other shared common space. All ideas as you probably took 5s to come up with. It's really not a hard problem to solve that any developer worth their salt would not be able to solve. reply deegles 17 hours agorootparentprevI saw a picture of one of those giant suburban developments in Texas... like thousands of cookie cutter homes over a huge area. But I'm sure you could provide the same amount of living space and better amenities in a focused apartment building. There should be schools and restaurants and shops etc spread out over every floor. Or maybe like a 90's shopping mall with a 30 story apartment building on top. It just makes sense to me. reply bombcar 12 hours agorootparentYou’d be surprised. Those homes are surprisingly cheap to throw up and you don’t have to do any additional support/maintenance. And you’d mainly be missing the yards. Most of these office buildings they’re talking about should just be knocked down and new purpose-built multi-use buildings built in their place. reply onthecanposting 8 hours agorootparentprevSharing a wall with a neighbor is something I happily to pay a premium to avoid. Hearing a neighbor beat her son or listening to top volume manufactured R&B beats at 2AM when I needed to be at work at 530AM are not fond memories. It was powerful motivation to take my career more seriously. reply cooper_ganglia 17 hours agorootparentprevWhy buy a nice suburban home and invest in your future when you could instead live in a windowless former office building with hundreds of other families? reply r14c 17 hours agorootparentA home is a really silly investment (i know that goes against the prevailing \"wisdom\"). the economic opportunities of living in an urban area coupled with investing in things that are actually economically productive are more likely to benefit you in the future. does your suburb's tax base cover the infrastructure maintenance costs? is this why suburban folks are so sensitive to their property values, because any little thing could send the development into a tail spin? sounds like a pretty dodgy investment to me. reply TheAmazingRace 17 hours agorootparentA home is considered the new hotness, and if you don't own one, you're not financially smart for some reason. At least this is according to the hivemind at /r/personal",
    "originSummary": [
      "Real estate developer Nathan Berman specializes in converting Manhattan office towers into luxurious apartments to combat the city's housing shortage, maximizing residential space in outdated buildings.",
      "The pandemic has accelerated the trend of converting office spaces to apartments, notably impacting downtown areas like the financial district, increasing residential occupancy.",
      "Berman's cost-effective and eco-friendly approach has revitalized neighborhoods like the financial district, drawing in young professionals and families to live in his converted office buildings."
    ],
    "commentSummary": [
      "Converting office towers into apartments is being explored to rejuvenate downtown areas, using successful projects in Kansas City as examples.",
      "The discussion delves into urban development, crime rates, safety perceptions, public transportation, and challenges faced when repurposing older buildings for residential purposes.",
      "Participants also touch on building codes, maintenance, ventilation systems, and the advantages of mixed-use spaces in city planning, emphasizing safety regulations in high-rise structures."
    ],
    "points": 292,
    "commentCount": 556,
    "retryCount": 0,
    "time": 1715088589
  },
  {
    "id": 40284389,
    "title": "Boeing Probed for Alleged 787 Inspections Issue",
    "originLink": "https://www.theguardian.com/business/article/2024/may/07/boeing-us-investigation-787-inspections-faa",
    "originBody": "View image in fullscreen A Boeing internal memo said the problem was an instance of ‘misconduct’, but not ‘an immediate safety of flight issue’. Photograph: Mic Smith/AP Boeing Boeing faces new US investigation into ‘missed’ 787 inspections FAA examining whether employees may have falsified records after firm said it might not have properly carried out checks Business live – latest updates Julia Kollewe Tue 7 May 2024 07.04 EDT Share Boeing faces a new investigation after the planemaker told US regulators it might have failed to properly carry out some quality inspections on its 787 Dreamliner planes. The US Federal Aviation Administration (FAA) said it was “investigating whether Boeing completed the inspections and whether company employees may have falsified aircraft records”. The regulator said that while the investigation was under way, Boeing employees would reinspect the Dreamliners that had not been delivered to airline customers yet, and the company would develop an “action plan” for the planes that are already in service. Boeing’s first astronaut launch called off due to faulty valve Read more The FAA said Boeing “voluntarily informed us in April that it may not have completed required inspections to confirm adequate bonding and grounding where the wings join the fuselage on certain 787 Dreamliner airplanes”. The Boeing executive overseeing the 787 programme, Scott Stocker, wrote in an internal memo, seen by the Guardian, that the problem was reported by an employee and was an instance of “misconduct,” but not “an immediate safety of flight issue”. The memo said the company concluded that “several people had been violating company policies by not performing a required test, but recording the work as having been completed”. “We promptly informed our regulator about what we learned and are taking swift and serious corrective action with multiple teammates,” the memo added. Stocker said the company would “celebrate” the employee who spoke up. Last month, a whistleblower came forward with different quality allegations about several Boeing models and urged Boeing to ground every 787 Dreamliner jet worldwide, warning they were at risk of premature failure. skip past newsletter promotion Sign up to Business Today Free daily newsletter Get set for the working day – we'll point you to all the business news and analysis you need every morning Enter your email address Sign up Privacy Notice: Newsletters may contain info about charities, online ads, and content funded by outside parties. For more information see our Privacy Policy. We use Google reCaptcha to protect our website and the Google Privacy Policy and Terms of Service apply. after newsletter promotion The Boeing engineer Sam Salehpour claimed that the company took shortcuts to reduce production bottlenecks while making the 787. He also raised issues about the production of the 777, another wide-body jet. The FAA is investigating these allegations. Salehpour, who has worked at Boeing for more than a decade, said he faced retaliation, including threats and exclusion from meetings, after raising concerns over problem including a gap between parts of the fuselage of the 787. In January, a door panel was blown out of a Boeing 737 Max aircraft on an Alaska Airlines flight in mid-air, forcing the aircraft into an emergency landing. Explore more on these topics Boeing Airline industry news Share Reuse this content",
    "commentLink": "https://news.ycombinator.com/item?id=40284389",
    "commentBody": "Boeing faces new US investigation into 'missed' 787 inspections (theguardian.com)241 points by mindracer 22 hours agohidepastfavorite131 comments jl6 20 hours agoIs this the kind of problem that is being uncovered because of the extra scrutiny that Boeing is currently under? To put it another way, is this the kind of problem that is endemic across industry and we’re only finding out now because we are taking the time to look? One can only imagine the horrors lurking in industries lucky enough not to have had any famous failures recently. reply nolok 20 hours agoparentDepends what you mean by industry. If you mean the US big airliner industry, then Boeing is essentially it, so yes, And I presume it comes directly from the FAA delegating so much of their job to Boeing meaning Boeing had to inspect itself. The you had the bad deal grand fathered between USA and EU : FAA inspect Boeing planes, EASA has to accept that certification without checking by themselves. Airbus need to be certified by the EASA, they have much less if any delegated authority for that (though they do pay a lot into that, there is a difference between you need to pay into your regulator and you are your own regulator through your own employees). And then they need to separately be certified by Boeing. This means the surface area for Boeing screwing around is much, much larger. I don't doubt there are flaws if we looked into airbus assembly, but I highly doubt they would be systemic and part of a larger screwed process like what is being found at Boeing. reply p_l 1 hour agorootparentThe delegation of authority was a Bush Jr. policy about deregulation and making airline aviation \"more agile\" or some other BS. It involved defunding FAA efforts at control and explicit policies for more self-governance in the industry. There wasn't a \"bad deal grandfathered\" - there was a general acceptance of FAA flight certificates in many regulatory areas due to previous good performance of those, which was wrecked by 737 MAX crashes. Even then it wasn't automatic pass, it was more that local certification authorities would let you submit it in lieu of some checks but could require more. Also, apparently Airbus has wildly different policies for dealing with suppliers, which are seen as both bad (they aren't easy to work with and most importantly they mean more risk to supplier) but also good (better oversight, integration, and Airbus will actually help you deal with your suppliers, and the risk is shared) reply burkaman 20 hours agoparentprevYes, I mean obviously cargo ships are not well inspected, they lose power all the time and companies just hope it doesn't happen near anything important. The oil industry is like this, you've got bottom-of-the-barrel tankers (https://www.amazon.com/Tankship-Tromedy-Impending-Disasters-...), spills continuing for decades (https://www.propublica.org/article/chevron-will-pay-record-f...), leaking methane wells permanently abandoned by \"bankrupt\" companies, etc. You're definitely right that we're all paying extra attention to Boeing at the moment, but if you look around I think you'll find that there actually have been pretty recent famous failures in quite a few industries. reply _heimdall 18 hours agorootparentLosing power in a ship that spends almost its entire life in the open ocean is much less risky than losing power in an airplane though. Its reasonable that expectations, regulations, and maintenance schedules differ there. I have nothing good to say for oil spills though. I was interning st Exxon's upstream research department when the BP oil spill happened in the gulf. What I heard from many people that had been there for a a long time amounted to no one being too surprised given how the industry works and how much is outsourced. They also (rightfully) expected the main concern to be PR. Exxon researchers were the ones to give BP the chemicals to coagulate the oil into globs and sink it just below the surface. They were really proud of it at the time, though having family on the Gulf Coast I was less impressed with the solution or the tar balls that washed up on the beach for years. reply spixy 13 hours agorootparent> is much less risky well Baltimore thinks otherwise reply _heimdall 12 hours agorootparentMy point stands in the context of the GP comment I replied to, they were pointing out how frequently container ships lose power. Baltimore was a terrible scenario with power failure at just the wrong time, but ships do regularly lose power or have mechanical issues and very rarely cause Amy damage. If a plane loses power it falls and hits whatever is below. reply fcsp 11 hours agorootparentI agree with you, but I would like to point out that airplanes without power should glide, i.e. see gimli glider. I do wonder though if anyone has tried with recent Boeing models. reply cmurf 8 hours agorootparentThey do glide, in fact airliners have a glide ratio rivaling gliders, and substantially exceeds general aviation airplanes. reply p_l 1 hour agorootparentAirliner glide ratios fall between 15 and 20, which is comparable with cheapest immediately post-war training gliders for category B, aka the gliders you put a student pilot to learn how to make turns before the advent of two-seater trainer gliders. And that 15 to 20 glide ratio is for best possible condition. reply tstrimple 6 hours agorootparentprevI knew most planes have quite reasonable glide distances and I knew that large commercial airliners were no exception. But from a relative standpoint (GA vs CA) this seemed suspect at first. Intuitively to me the enormous mass differences would make up for the wing surface area differences and the advantage would be to much smaller / lighter planes. On further reflection makes a lot of sense. Fuel is one of the highest costs of commercial airlines. Of course they are going to be hyper-optimizing for this where they can. A 747 being fuel efficient is far more important to bottom lines than a tiny Cessna being fuel efficient and with the relative costs of these aircraft, Boeing can invest far more in research to reduce drag and optimize lift. Aircraft glide ratios certainly seem to confirm this. reply dawnerd 19 hours agorootparentprevI thought ships were a little different since they can just follow the regulations of their flagged country and skirt a lot of rules. reply cookiengineer 19 hours agorootparentWhich is the country their rules should follow? Their flag? Their company's residency? Their production location? Guess what, all are bought and sold off for cheap on the market. Oh and they pay less than 1.5% taxes, usually, if they even pay taxes. reply dawnerd 14 hours agorootparentI agree it's a tricky problem to solve without agreements in place governing it. The airline industry seems to have settled on you follow the laws of the destination country. Ships in particular though do everything they can to abuse flagging their ships with the most lax working regulations. reply paulmd 4 hours agorootparentprevThe union of the ruleset of the flag country, the source country, and the destination country seems pretty reasonable - you should have to follow all three. Obviously not how it’s currently done, it’s an industry that has been doing it by flagging out of places like Liberia or Mauritania since time immemorial, but we can clearly do better. If this causes conflicts that make certain flags legally incompatible with certain combinations of sources and destinations - good. Those flags-of-convenience can harmonize their rules to the rest of the world. The environment and human rights will benefit. reply hunter2_ 19 hours agorootparentprevWith the amount of overlapping vocabulary, I would've thought airplanes are basically just ships in the sky. Ports, captains, boarding, etc. Or are these used metaphorically, like virtual computer stuff versus physical office stuff (desktop, folder, etc.) where there's enough similarity to avoid making new words but they're operated quite differently? reply philipwhiuk 18 hours agorootparentThe need to overfly a country, vs just sail round, means that airlines need country's permission to fly into/through a country. Overflight privileges cost money. In addition protectionism has meant that you generally have to land or take off at an airport run by the flag you carry. So unlike shipping, a Malaysian airline can't fly from the US to UK. This means that airlines need to be based at their 'real' base of operations, they can't just register in a tax-shelter. reply rob74 18 hours agorootparentAdditionally, the fact that passenger airplanes carry, well, passengers, while cargo ships don't, means that airlines are automatically under more scrutiny. Further to what you wrote, some airlines have even been blacklisted by e.g. the EU because of safety reasons, like PIA (the flag carrier of Pakistan!) from 2020 to 2023. reply hunter2_ 12 hours agorootparentI guess cargo tends to go more by ship and passengers tend to go more by plane, but all 4 combinations certainly exist so no need to assume 2 variables are being compared simultaneously. reply noir_lord 19 hours agorootparentprev(tongue in cheek) isn't that what boeing did with the FAA? reply jeromegv 19 hours agorootparentprevShips and planes are so different in their regulations and safety measures, it's not even close or comparable. reply doikor 20 hours agoparentprevThe owners/economy has demanded for things to become more efficient then last year every year for decades now. If no massive technological breakthrough happens at some point the operation is as efficient as it can be while producing safe products but still you have to become more efficient for next quarterly/yearly results to keep the shareholders happy (if you can't find new clients to expand to). It is at that point that you start cutting corners and end up in situations like this. This happens in every field but obviously in most fields the cost of these failures is not big enough to really matter but when you apply it to aviation you get planes with hundreds of people crashing into the ocean. reply LargeWu 19 hours agorootparentNot even more efficient. Just cheaper. Efficiency implies there's no drop in quality. reply constantcrying 13 hours agorootparentprevThis would mean that failures should be spread evenly over an industry. Airbus clearly does not have the problems Boeing has. I also don't think you understand the current airline industry. Both Boeing and Airbus desperately want to grow, they have orders lined up for decades and just need factories and people. The issue is not a stagnant industry looking for cost-cutting it is a very in demand industry desperate to fill orders. reply p_l 1 hour agorootparentThe shareholders care about stock price and dividends, and those aren't necessarily linked to company actually growing. Ultimately, for a lot of publicly held companies, quite often the \"boss making the decision\" is an excel sheet on some analysts desk that will result in a specific pressure being applied on company. reply gruez 20 hours agorootparentprev>This happens in every field but obviously in most fields the cost of these failures is not big enough to really matter but when you apply it to aviation you get planes with hundreds of people crashing into the ocean. Is that so bad? According to the IIHS there were 43k automotive deaths in the US in 2021. Meanwhile the last passenger death in a scheduled commercial flight[1] in the US was 4 years ago. In the past decade there has only been 2 deaths. [1] this excludes flights like https://en.wikipedia.org/wiki/2022_Mutiny_Bay_DHC-3_Otter_cr..., because they're essentially flying general aviation planes which are far less reliable than airliners that people typically associate with air travel. If you include them the number goes up to 27. reply rapatel0 19 hours agorootparentAirline ------------ Total number of global flights per year: ~38.9 Million Total deaths over the last decade (3,562) -> 21.2 deaths per million trips Cars ------------ Total number of driving trips per year US: 227 Billion (using your number of 43K deaths per year) -> 1.9 deaths / million trips Wow, I'm surprised but actually airline travel is less safe then driving. Links: - https://www.statista.com/statistics/564769/airline-industry-.... - https://www.statista.com/statistics/263443/worldwide-air-tra... reply kayodelycaon 19 hours agorootparentYou're calculating per trip, which is very misleading. Each flight two orders of magnitude more people on it. You want per person per trip to actually calculate an individual's risk on a single trip. Then you need to account for distance or time. If I had to guess, that's three to four orders of magnitude more time or distance per trip. reply cge 17 hours agorootparentprevIn addition to the other points brought up, you're comparing global flight statistics with US driving statistics. Fatal commercial aviation accident rates are heavily region dependent. If I recall correctly, the number of fatal aviation accidents on scheduled US commercial flights over the last decade is zero. Meanwhile, some other regions are vastly less safe: for example, while this was some number of years ago, I remember a statistic of flights in Africa accounting for single-digit percentages of global flights and a quarter of passenger fatalities. reply tstrimple 5 hours agorootparentMost US aviation deaths are from general aviation, not commercial. As you've pointed out, commercial flights in the US are far, far safer than driving almost no matter how you want to cherry pick stats. General aviation has an accident rate equivalent to if not higher than automobile accidents. Commercial airline practices and procedures largely work and are very effective at reducing casualties. But some of these small general aviation airports don't even have full time staff coordinating landings and departures and you have to handle that yourself and hope anyone else landing at and taking off from that airport are on the right frequencies. After listening to a few too many aviation accident breakdowns and the reasons behind them, I'm honestly surprised the general population doesn't hear more about these shenanigans and the dangers surrounding the hobby. It's way too easy to mix the commercial airline safety records with general aviation and have a false sense of security around those types of flights. It's one thing when an inexperienced pilot kills themselves through neglect and incompetence, but a very different sort of impact when looking at these reckless pilots who get others killed. Think of any celebrity who has died in an aviation incident from Denver to Kobe and they are all general aviation accidents. Small planes with less experienced or complacent pilots. reply avar 19 hours agorootparentprevIt makes no sense to look at number of flights for a comparison. You need to look at passenger miles. reply hunter2_ 19 hours agorootparentYou need to look at hours invested. If I'm planning to go away for a week, I'm choosing whether to sit in a car for 12 hours or airports+airplanes for 12 hours -- these are the equivalent options for how to spend my time away from home, so between these, how much safer is one over the other? Said another way, compare \"trips by plane\" against \"a set of car trips whose average duration matches that of trips by plane.\" reply gruez 17 hours agorootparent>[...] these are the equivalent options for how to spend my time away from home No they're not. If you're in San Fransisco for instance, a 12 hour drive maybe gets you to LA, Vegas, and maybe salt lake city. It can't even get you to Seattle or Phoenix. Meanwhile even if you factor in getting to the airport 3 hours before departure, 1 hour to get to the airport, and 1 hour to get to your final destination, that leaves you with 7 hours of flight time. That's enough for a direct flight anywhere in the lower 48 states, and as well as many cities in Canada and Mexico. reply hunter2_ 11 hours agorootparentIf I have a certain amount of time to spend and want a reasonable ratio of travel time to time-at-destination, some ratio that I prefer regardless of mode of travel, those are indeed the options I can choose from (e.g., driving to the next state or flying across the country). There will be many factors leading to a decision of which option to select, and if I want travel safety to be a factor, then the safety stats need to be presented as I'm describing. reply NekkoDroid 10 hours agorootparentprevFuck, in 12 hours time I can get from Germany to USA Atlanta no problem, probably still 1-2 hours extra time. reply gbear605 19 hours agorootparentprevDeaths per trip is a bad comparison. As you can see from those numbers, Americans take about 1000 car trips per year and many fewer plane trips (hard to get exact numbers based on that since it’s global and planes can have dozens-hundreds of people). reply samatman 19 hours agorootparentprev\"Per trip\" is an utterly meaningless way of looking at the data. Some car trips last ten minutes and a 10 or 12 hour flight isn't uncommon. You know for a fact that it's more dangerous to drive from NYC to Miami than it is to drive to the grocery store and back, so why would you use a meaningless statistic like this? I would say the most sensible is \"per passenger hour\", followed by \"per passenger mile\", since in some cases one might be choosing between driving somewhere and flying there. By both of these metrics, airplanes are much safer than cars. reply tstrimple 5 hours agorootparent> I would say the most sensible is \"per passenger hour\", followed by \"per passenger mile\" Not trying to be contrarian or anything but I'm curious why. Another comment mentioned the material difference in available options between comparing an 8 hour flight to an 8 hour drive. In my mind, the only meaningful metric then would be the \"per passenger mile\". This should be emphasized by the fact that there are destinations that literally no amount of driven miles will get you to. The utility provided from 8 hours of flight versus 8 hours of driving are so drastically different I can't imagine how you'd compare them. 8 hours of flight will let you visit roughly a quarter of the world. 8 hours of driving won't even get you a quarter of the way across the US. reply triceratops 18 hours agorootparentprevAre you comparing global flights to US driving? reply 0xAFFFF 20 hours agorootparentprevIt's not so bad yet because the aviation industry comes from a place of extremely high safety standards, but it could get worse pretty fast. reply notfromhere 19 hours agorootparentprevThat’s a sentence you can say if you don’t find yourself or a loved one on that plane. reply masklinn 20 hours agoparentprev> Is this the kind of problem that is being uncovered because of the extra scrutiny that Boeing is currently under? Yes. > To put it another way, is this the kind of problem that is endemic across industry and we’re only finding out now because we are taking the time to look? That is a completely different matter. The increased scrutiny on Boeing can simply be uncovering that the wheels have been coming off for a while as result of the stock-chasing policy they’ve been following for 25 years. Previously it could be covered up because their primary regulator was too hands off but recent events have led to the FAA dedicating more resources to Boeing, and thus having a lot more opportunities to trip over malfeasance. reply hosh 19 hours agoparentprevAccording to the Netflix documentary on Boeing’s culture and engineering problems, those have been there for years. Falsifying quality reports had been going on regularly at Boeing. Quality engineers were harassed out of their positions, and this was all lead by the executives in charge. This was the stuff that former quality engineer and whistleblower was in the middle of a five day deposition when he died to a gunshot wound. Allegedly “self-inflicted”. This stuff isn’t being uncovered so much that there had been people loudly trying to tell this to the world for years now. What’s changed is that with the series of very public failures and flight safety, it’s become difficult to spin this away. In addition, the executives may be facing felonies — not for harassing whistleblowers, but in regards to flight safety. It’s galling to hear them “celebrate” the whistleblower for the issues reported in this article. Boeing used to have a culture where quality was the top concern, ahead of profits. With three whistleblowers dead in the past several months, the optics is not looking good for Boeing. Maybe this is more spin. reply TedDoesntTalk 18 hours agorootparent> With three whistleblowers dead I thought it is 2…? reply hosh 16 hours agorootparentLooks like I was wrong. The Guardian article published four days ago reports two, not three. reply constantcrying 13 hours agoparentprevThis is a Boeing specific problem. Boeing does things which no other company would dare doing, the failure on engineering the MAX is simply inexcusable. The same goes for the failure to correctly plug a door. These things don't happen by chance, they can only happen if you have a deep institutional rot in your organization. These things don't happen by chance, because the systems are designed in a way such that they can never happen. If they do happen the system has failed. The entire airline industry knows that if you are lax on safety critical issues, there will be consequences. The consequences over the last few years have focused on Boeing, because it actually is a Boeing problem. reply MuffinFlavored 12 hours agorootparentHow am I supposed to have any confidence I won't be the next casualty/news headline if I travel on a Boeing plane (which feels like the majority of planes in America)? reply kiicia 11 hours agorootparentinsert meme: that's the neat part, you don't reply _ache_ 20 hours agoparentprevYou are inverting cause and consequence. It's because Boeing has had so many failures that it has come under particular scrutiny. Failures which, from a statistical point of view, suggest that there may be a cause to look for. Other companies do not have as many (and as basic) problems. reply dylan604 18 hours agoparentprevAuto manufactures issue recalls fairly regularly. To me, this shows how easy it is to \"miss\" something, or let something questionable pass and hope for the best while knowing there is a mechanism in place to bring things back. Food industry also has recall notices frequently. Both of these industries have inspections not by the manufacturing company. Boeing weaseled their way into self inspection and certification. OF COURSE things went bad. I can think of no examples of self policing working, ever. They eye of Sauron is on Boeing, but it is of their own making. reply lenkite 18 hours agoparentprevIt is the result of the McDonnell Douglas Merger. McDonnell Douglas' focus on affordability and shareholder value replaced Boeing's passion for great planes. It is kinda funny - Boeing bought the company but it was McDonnell Douglas executives who held the rulership of the merged company Boeing's Downfall: A Tale of Corporate Culture, Greed, and Safety Compromises https://www.linkedin.com/pulse/boeings-downfall-tale-corpora.... reply p_l 1 hour agorootparentThe person who started critical changes in culture was a Boeing lifer - who also merged in McDonnel-Douglas. reply renegade-otter 19 hours agoparentprevNo, this is relatively new. This is the result of decades of the quest to \"dismantle the regulatory state\". The book Flying Blind is a thorough account of how Boeing transformed from a respectable engineering shop to a business-school-jock-run FAA revolving door. Unabated capitalism sort of doesn't work when your planes are losing pieces mid-air and Airbus is eating your dinner. Not surprisingly, there are solid arrows leading to GE and Jack Welch. reply jerf 20 hours agoparentprevIt is endemic across society. People confuse goals with results all the time, and accept that inspections are scheduled or that some box was checked in lieu of an \"actual\" inspection all the time. It is one of the major reasons that I am always very skeptical of the \"just throw more regulations at it\" solution to anything; generating paperwork and generating compliance are two very different things. The fundamental error of bureaucracy is to conflate paperwork with reality. The shocking thing is that as cynical as that may sound, things do in fact generally work. This suggests that the problem is less on the side of reality and more on the side of the paperwork and the bureaucracy. You see this one instance where the mismatch is problematic, but honestly there's immense mismatch everywhere, and generally, things work, because while in bureaucratic theory a mismatch between paperwork and reality is itself intrinsically a problem, in reality, it's only a problem if something goes wrong, and usually it doesn't. If that doesn't make sense to you, consider that there is an evolutionary process in play. The particular ways in which mismatches cause real problems tend to get squeezed out of the system by the very failures they create, whereas all the harmless mismatches can persist. And yeah, eventually some of those \"harmless\" mismatches will get upgraded to \"whoops less harmless than we thought\", but they will be the exceptions, not the rule, and calling in advance which they will be is a lot harder than meets the eye. There's kind of a variant of Gell-Mann Amnesia at work here; $YOU know that in the specific place you work, paperwork is a faint echo of reality, documentation is perpetually underinvested in, processes are constantly diverging from the paper processes, the field workers in charge of managing paperwork generally coevolve a particular \"flavor\" of compliance that passes initial checks but may have shall we say \"complicated\" relationships with reality... but $YOU think that all the other paperwork in the world is being done studiously by superhumans with unflagging concentration, impeccable ethics, and presumably, about 96 hours in their day. It seems to be built into the human condition somehow. Somehow, we are all bureaucrats at heart. reply yareal 20 hours agoparentprevBoeing used to have a safety culture and and engineering excellence culture. That changed over the past twenty to thirty years as leadership rotted the company and monetized that rot by choosing to skip safety and engineering in favor of profits. reply linuxftw 20 hours agoparentprevThanks to M&A's, Boeing is the industry (at least in the US). Then the bean counters figured out that they can outsource a lot of the manufacturing to '3rd parties' who pinky-promise they have a functional QA process. When MCAS drove 2 planes into the ground, they were able to proclaim \"It wasn't us, it was our software vendor!.\" All the airframe problems? Well, that's Spirit Aerosystems! There should be a lot of Boeing execs in prison for manslaughter. reply phonon 19 hours agorootparent> When MCAS drove 2 planes into the ground, they were able to proclaim \"It wasn't us, it was our software vendor!.\" They did not say that. reply s_dev 19 hours agoparentprevA lot of people defending Boeing are simply using the Trump playbook. Which always goes as follows: Yes it's true -- Boeing may be bad -- but even Airbus is worse! It must be the entire industry that is doing this due to increased scrutiny on Boeing -- drain the airline industry swamp! McDonnell-Douglas have no involvement with Airbus and Airbus still maintains an immaculate safety record the past few years relative to Boeing. Boeing alone was never the issue -- their partnership with McDD however has caused these issue perhaps the US gov should buy out their stake with a compulsory purchase order for the sake of national security. Boeing is only a few disasters away from a complete loss in confidence in the company at this point. reply cjk2 20 hours agoparentprevI work in a different sector. If we built planes it'd be raining bodies. If you go looking you will find. And it's almost never pretty stuff. There is however a lot of paperwork filled in that says everything is fine. reply langsoul-com 20 hours agoprevFeels like Boeing is trying to shift the blame on the individual instead of their incentive and corporate culture about missed inspections. Its not the company, but rather x individual that falsified their safety report. Don't worry, we'll fire them and everyone can forget about this. Is the feeling I'm getting. reply banannaise 19 hours agoparentIt was all the fault of this one guy not doing his job. Also, it's just a coincidence that we fired his peers who were doing their jobs, and that the one who went to the feds was found dead in his car recently. reply ActionHank 20 hours agoparentprevThey remember that VW \"it was a lone developer\" play. reply ruph123 19 hours agorootparentThen they would have forgotten that former Volkswagen CEO Martin Winterkorn was indicted on fraud and conspiracy charges. reply Drunk_Engineer 17 hours agorootparentThat was 4 years ago, and the case has not gone to trial. reply ruph123 10 hours agorootparentTrial is starting this year: https://apnews.com/article/germany-volkswagen-diesel-scandal... reply agilob 20 hours agorootparentprevwho was charged and went to prison reply onlyrealcuzzo 20 hours agorootparentAlso, that one bank employee (Kareem Serageldin) that went to jail over the financial crisis. Don't worry guys, we got him. reply tgv 18 hours agorootparentAn utter disgrace. reply TheCleric 13 hours agoparentprevExactly this. Could it be we told the safety inspector to do X inspections a day (which would be literally impossible) or be reprimanded? Nope, it's the person's fault for falsifying the inspection. No need to do a root cause analysis. reply proee 19 hours agoprevHow many organizations of similar size bamboozle regulators? When a fast food restaurant is responsible for an ecoli outbreak, regulars are quick to point out broken processes in the organization that need fixed. It's a bit of reactionary theater, where both sides are playing a game to show that they are not the source of the problem. If you are the regulator, there is plenty of motivation to show the world you are doing your job by pointing to missteps in a process. If you are the offender, there is plenty of motivation to concede to \"some\" failures in process, cut your losses, and show that you are improving. Everyone, if you could just submit your TPS reports on time that would be great. reply avgDev 19 hours agoparentRegulators are generally blinder when it comes to huge organizations. Boeing is a huge DOD contractor. They have a direct line to people at the top of the US govt., just like other huge organizations. It is a big fish, and it is hard to fry. US govt. discussed shut downs during COVID with largest organizations in the US before they even occurred. Some knew they were coming before anything was announced. It isn't some kind of conspiracy either, most governments will \"care\" more about the biggest contractors and businesses in their nation. reply stephc_int13 20 hours agoprevI sincerely hope this whole Boeing debacle will be used as wake-up call for other large corporations, or the general public. Most of them are not as safety-critical, but some of them are, thinking about Big Pharma and Chemical giants... reply bluSCALE4 20 hours agoparentI will say it's having an impact on me personally. In the tech world, we work in 2 week Sprints and Sprints have Stories. Stories are Pointed efforts rated on level of effort that need to be completed within a single Sprints. Developers can only be allotted X number of Points per Sprint. If stories aren't done, then you usually need to reflect on why and improve, typically breaking down high pointed stories into smaller ones. The problem with many organizations face is that instead of leaving a Story open and answering the why, they'll simply close the problem Story and open a new one with the missed worked. This causes problems all over the place and snowballs if not addressed. In my situation, we also cancelled Retros so concerns weren't being raised further compounding things and finger pointing. So what may have started as an over zealous developer with a well intentioned manager can become a trend leading to missed deadlines, cut corners and defective code making it to production. I started to realize that I was no different than Boeing; that I was Boeing and that hasn't sat right with me. So for better or worse, I'm going to be more vocal about things. reply GartzenDeHaes 19 hours agorootparentManagers with spreadsheets are the cause in both cases though? reply maskil 20 hours agorootparentprevExcept that people aren't dying as a result reply Gareth321 20 hours agorootparentYes, this is all about risk tolerance. If a component on a website doesn't function as expected, it rarely kills people. Flight engineering should have the lowest risk tolerance possible. This is expensive, but necessary. reply albrewer 19 hours agorootparentprevJoke's on you, he works on engineering software used in a highly safety critical industry! /s reply ItsBob 20 hours agoparentprevNot sure what you mean: there have, to-date, been no repercussions for Boeing! They're getting away with all kinds of shit here! In an ideal world, companies with a massive safety-related function should have Warhammer 40k gun servitors hovering over the execs in the event they cut corners on safety with orders to empty the clip! Boeing are really a gov extension these days from what I can see. reply humanlion87 20 hours agorootparentI wouldn't say they haven't had any repercussions. The whole 737 Max debacle cost them an estimated $20 billion (https://web.archive.org/web/20201221001329/https://www.cnn.c...). But I do agree that overall there needs to be more repercussions. Unfortunately they are a \"too big to fail\" kind of company considering how critical they are to the aviation industry as a whole. reply itsanaccount 20 hours agorootparentprevhttps://www.pewpewtactical.com/wp-content/uploads/2016/01/Ma... Clips help fill internal magazines on older weapons. A servitor drone would have a magazine. Just being pendantic but I think you'd want \"dump a mag into c-suite executives found guilty of corruption in safety critical industries.\" I'd also say since we don't currently have servitor drones we may have to do it ourselves. In an ideal world of course. reply decafninja 19 hours agorootparentIn an ideal 40k world, the c-suite would already have been lobotomized into servitors to serve penance for tech heresy. reply clarionbell 20 hours agorootparentprevI vote for the servitors. Either that or an arco-flagellant on quick dispatch for the entire room. That aside, yes, Boeing and in EU Airbus are very much intertwined with government. It's inevitable when their products are strategically important and barriers to entry high. It's unfortunate, but that's how it is. reply decafninja 10 hours agorootparentBetter to lobotomize the entire c-suite into servitors for tech heresy. reply is_true 20 hours agorootparentprevFor the military boeing is too big to fail. reply ChrisMarshallNY 18 hours agoparentprevI'm not a fan of industry \"self-regulation.\" Everyone keeps calling for it (as opposed to government regulation), but these stories keep popping up. We need to have self-regulation success stories to be highlighted, if we want to go that way. reply masklinn 20 hours agoparentprev> I sincerely hope this whole Boeing debacle will be used as wake-up call for other large corporations Why would it? Stock's still up from 2016, higher than Airbus has ever been and those who sold between 2018 and 2020 made absolute bank. It's not hurt any current board or C-suite member to say nothing of those who actually taken the strategic decisions leading to the current fallout, I've not seen anyone seriously suggesting Stonecipher should be even mildly inconvenienced. reply sidewndr46 20 hours agoparentprevI'd hope it'd be a signal that we can't allow an industry to regulate itself reply isolli 20 hours agorootparentWe already had a strong and clear signal from the financial industry in 2008... reply pjc50 20 hours agoparentprevAccountability seems to be slower in the pharma industry, but it did come for Theranos and Purdue. There doesn't seem to be the same kind of safety issue, only horrific pricing issues and a complicated supply problem for certain kinds of medications. reply jtc331 20 hours agorootparentI don’t see how we can say that pharmaceuticals don’t have serious safety issues. Consider Vioxx: > Merck withdrew the drug after disclosures that it withheld information about rofecoxib's risks from doctors and patients for over five years, allegedly resulting in between 88,000 and 140,000 cases of serious heart disease. reply inemesitaffia 13 hours agorootparentVioxx is fine. Just improper disclosures. Many patients who it was right for lost out because of the withdrawal reply admissionsguy 20 hours agorootparentprevNoting though that barring extreme cases, a safety issue with a batch of drugs would be really hard to detect. reply markus_zhang 19 hours agoparentprevUnless the whole management got enough punishment, it's only going to serve as incentive for them to keep the old way. Like, put your feets into their shoes -- wouldn't you? By saying enough punishment, I meant at least something like Enron -- Exec went to jail, regulation people got slashed and hacked, company re-structured as we cannot afford to lose it. reply karaterobot 20 hours agoprev> Stocker said the company would “celebrate” the employee who spoke up. My advice to this employee: run. reply buster 20 hours agoprevInteresting read regarding \"missed inspections\": https://pluralistic.net/2024/05/01/boeing-boeing/#mrsa reply belter 16 hours agoprevhttps://news.ycombinator.com/item?id=40278391 reply ItsBob 21 hours agoprevAt what point do we say \"This company is rotten to the core and needs shut down\"? This article alleges that they may be falsifying safety stuff! It's not like they're making lollipops ffs... they're transporting millions of people per day in things that cannot afford to have issues... ever! Sure, I understand that the media has them under a spotlight right now but even so! On another note this quote got me laughing: \"Stocker said the company would “celebrate” the employee who spoke up.\" \"Yeah, Mike, just step out the back while Tony and Big Joe celebrate you in the alleyway here\"! :) reply hnthrowaway0328 21 hours agoparentBoeing is in the position of TBTF. The best can happen is a change of management but this doesn't solve anything. Good leadership does not grow like grass, and if the soil is corrupt none grows. reply andkenneth 10 hours agorootparentIMO there needs to be personal criminal responsibility for safety related issues in an engineering org like this. We did this in New Zealand a few years back, you can get prosecuted and sent to prison for wilfully ignoring safety issues that cause serious injury or death, and that responsibility exists at every level up to the board. reply SteveNuts 21 hours agoparentprevI’m not knowledgeable in this area, is there any precedent or legal method for the US government unilaterally shutting down a large publicly traded conglomerate (purely as a hypothetical)? Not that I think they’d actually do it, I’m just curious if it’s even possible. I imagine they could bog them down with lawsuits but Boeing has a deep war chest and it would take many years. reply clarionbell 20 hours agorootparentIn principle it is possible. But shutdown isn't what you want. Because that would cause job losses, collapse of supply chain and all of that horror. You need restructuring. Replace management, take control over the direction. Keep things running while the mess is resolved. It's not going to be fast, an it's not going to look nice. But at this point there are no other options. reply brookst 20 hours agorootparentprevNot just Boeing, all of their shareholders, including institutional ones. This would essentially be an edict that the stock price is now $0, modulo liquidation plans. But I don’t think you could declare the company too rotten to operate and also sell off its operations, so enterprise value would be best case. Oh, and many of its assets in the form of planes would presumably be instantly devalued/worthless. It’s a fun hypothetical (unless you work for Boeing), but I don’t think there’s a legal precedent or a good outcome even if it were possible. reply ensignavenger 13 hours agorootparentA more practical approach would be to fine them a huge amount of money after a fair investigation and trial. Then, when the company is forced to declare bankruptcy or is on the brink of it, the US Government could buy the company out of bankruptcy, replacing all executive management and run it for a enough time to ensure that corrections have been implemented, then the Government could take the company public again and divest itself. reply pjc50 20 hours agorootparentprevThis would almost certainly fail legally on \"taking\" grounds. It also fails politically, since Boeing is a critical US defence contractor, and the US is not going to cut off its supply of parts for military aircraft. Killing Boeing is a fantasy. reply ItsBob 20 hours agorootparentprevNot sure about the US but in the UK, the government can close companies down for things like not filing taxes or failing to pay them for example. So it wouldn't be outwith the realms of possibility that a company making planes that are potentially unsafe could have their doors closed! I'm not mega pro-government intervention but there are times I'd like to think my gov has my back... this is one of them! reply GartzenDeHaes 19 hours agorootparentprevThe federal government did take over GM somehow. https://en.wikipedia.org/wiki/General_Motors_Chapter_11_reor... reply adgjlsfhk1 20 hours agorootparentprevshutting it fully down would likely be very difficult, but the FAA definitely has the authority to not certify any Boeing planes for a couple years and let Boeing go bankrupt reply swasheck 20 hours agorootparentyou’ve heard how the government certifies these things right? they allow the company to inspect and certify. reply p_l 1 hour agorootparentBecause Bush Jr. government policy ordered them to and reduced funding for inspections. reply adgjlsfhk1 20 hours agorootparentprevyes. they can stop doing that (and in fact have already walked back some of that) reply jmyeet 21 hours agorootparentprevYes, the US can nationalize a company. And it's happened before [1]. The current political climate means there's no appetite for any party to alienate actual or potential donors by doing something like this however. The one exception is banks where the FDIC can (and does) take over nonperforming banks all the time [2]. Think of it as eminent domain but for companies. [1]: https://thenextsystem.org/history-of-nationalization-in-the-... [2]: https://www.fdic.gov/resources/resolutions/bank-failures/in-... reply interdrift 19 hours agoprevAlways remember that one guy testifying that he saw employees jumping with their feet on airplane parts to insert them in somewhere. LOL. reply VyseofArcadia 20 hours agoprevI sure hope the investigation finds some fault with management for creating an environment where falsifying records was considered an option. reply swasheck 20 hours agoprevreally excited to be flying one these in 36 hours reply yareal 20 hours agoprevI believe it's time to nationalize Boeing, remove their leadership and replace them with engineers. The people who run the company should be the people who know how to build planes. It's in our economic and industrial and defense interest to have a well run, high quality aerospace manufacturer. reply tbihl 20 hours agoparent>it's time to nationalize Boeing I seem to have lost the thread between that and > The people who run the company should be the people who know how to build planes. It's in our economic and industrial and defense interest to have a well run, high quality aerospace manufacturer. As a reminder, the leadership competitions in bureaucracy select for loyalty to the bureaucracy, not the bureaucracy's stated mission. Exceptions are rare and fleeting. reply _ache_ 19 hours agorootparentI may suggest that the link between the two statements is exactly to escape from the trap you talked about. If the leadership is elected by the state rather than by internal competition, you don't end up with a group of people who are loyal to themselves but with a meritocratic leadership. reply 6510 19 hours agorootparentprevI liked the analogy with hyenas and Saint Bernard's. The Saint Bernard is high-maintenance, require lots of care and attention. They are very loyal but need to constantly be told what to do. The hyena is highly opportunistic, they see and hear everything and need to be monitored constantly or they run off with your company. They over-sell everything specially themselves. The Saint Bernard's all need their own pen or they cant write, if there is one pen in the building all of the hyenas can write. reply bdw5204 19 hours agoparentprevI'd say reversing the McDonnell Douglas merging via antitrust enforcement would be a more promising route given the issues the US government has with running anything competently. If the FAA can't hire competent independent inspectors then just let the competitor do the inspection and tie the bonuses for inspectors to finding something wrong with the competition's planes. reply p_l 1 hour agorootparentFAA can't hire because Congress and Bush Jr. took away the money to do so and instituted a policy of self-checking for corps. Reversing McDonnell Douglas merger wouldn't really work, the remaining product lines that could be separated are very much non-overlapping. reply _ache_ 20 hours agoparentprevYeah, but actually, that nationalize part isn't something in trend in USA, isn't it ? Gwynne Shotwell, engineer and COO of SpaceX maybe considered from the job of CEO of Boeing. It seems to me more USA-ish move than nationalize Boeing. Don't know witch one is the best for Boeing in the end. reply ziofill 12 hours agoprevFor a split second I thought how ironic there were 787 missed inspections. reply croes 18 hours agoprevIf companies are people in the US after how many crimes does a company get a life sentence or death penalty? reply sylware 21 hours agoprevAll this Boeing thing is now really seriously dangerous, up to life threatening. Something seems REALLY wrong. Maybe time to ramp up the type of \"services\" to understand what is going up there... reply jmyeet 20 hours agoprev [–] It's a well known phenomenon that over time profits tend to fall [1]. Investors demand growth however so the consequence is obvious: all companies will tend to try and increase prices and/or cut costs to maintain profits. Raising prices tends to lower demand. It might be the case that a lower volume at higher margin leads to a higher gross profit but often it doesn't. So raising prices tends to be limited to where demand is ineleastic or you have or can construct a monopoly or enclosure of some kind. So most companies have to resort o cutting costs. This means suppressing wages and doing what Boeing has done in spades: using subcontracting to lower costs. Being is now in the FAFO phase of cost-cutting. Since around the time of the McDonnell-Douglas merger, the engineers lost and the accountants won. Once again, this Steve Jobs quote [2] is apropos. [1]: https://en.wikipedia.org/wiki/Tendency_of_the_rate_of_profit... [2]: https://www.youtube.com/watch?v=tGKsbt5wii0 reply resource_waste 20 hours agoparent>So raising prices tends to be limited to where demand is ineleastic or you have or can construct a monopoly or enclosure of some kind. United States Medical Anyway, outside of that, this typically is a good thing. $200 flat screen tvs make it easier to live life. I remember moving out of my parents house and being amazed at how affordable the minimum products were. Yes offbrand TV with 50 hz, yes you $2/lb meat with 30% fat, yes your shower curtain was a pain in the butt to install... But making 19k/yr, I could afford it. reply GartzenDeHaes 19 hours agorootparent> $200 flat screen tvs make it easier to live life. From what I've been reading, those cheap TV's won't last more than 3 years. So those poor people will end up spending more on TV's than if they bought an old-style expensive TV. Same thing with shoes, rent, cars, etc. reply avidiax 19 hours agorootparentThat's more typical of the \"Black Friday special\". That TV cuts corners that don't make sense to cut, but the reputational damage that a super cheap low volume TV causes is minimal to none. reply inemesitaffia 13 hours agorootparentprevTV's from Hisense and TCL are fine reply resource_waste 19 hours agorootparentprev? I am still using my 200 dollar tv from 2012. If you want to repeat a fiction story about boots, that's fine, but its still a fiction story. reply constantcrying 13 hours agoparentprev [–] Total nonsense. Do you not know what is happening in the airline industry right now? Boeing is not facing a stagnant market, they have customers even now begging them to give them planes as soon as possible. What Boeing is trying to do is fill enormous demand, they actually could easily grow if they had the leadership capacity to do so. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Boeing is under a new US Federal Aviation Administration investigation for potential lapses in quality inspections on its 787 Dreamliner aircraft.",
      "The FAA is examining if Boeing staff falsified records, with Boeing addressing the issue through corrective measures.",
      "A whistleblower highlighted worries about production shortcuts and safety problems with Boeing planes, adding to the scrutiny on the company's practices."
    ],
    "commentSummary": [
      "Boeing faces scrutiny for overlooked inspections on its 787 planes, raising concerns about inspection delegation systems.",
      "Comparisons with Airbus's rigorous inspection procedures highlight potential systemic problems within Boeing and the impact of industry deregulation.",
      "Safety worries, accountability issues, and discussions on potential government actions like shutdowns or nationalization are ongoing, reflecting broader concerns about safety and quality compromises for profit reasons at Boeing and beyond."
    ],
    "points": 241,
    "commentCount": 131,
    "retryCount": 0,
    "time": 1715082110
  },
  {
    "id": 40284291,
    "title": "Hackers Crack NES Tetris, Enable Reprogramming within Game",
    "originLink": "https://arstechnica.com/gaming/2024/05/hackers-discover-how-to-reprogram-nes-tetris-from-within-the-game/",
    "originBody": "Building a better Tetris — Hackers discover how to reprogram NES Tetris from within the game New method could help high-score chasers trying to avoid game-ending crashes. Kyle Orland - 5/6/2024, 10:16 PM Enlarge / I can see the code that controls the Tetri-verse! Aurich Lawson reader comments 57 Further Reading 34 years later, a 13-year-old hits the NES Tetris “kill screen” Earlier this year, we shared the story of how a classic NES Tetris player hit the game's \"kill screen\" for the first time, activating a crash after an incredible 40-minute, 1,511-line performance. Now, some players are using that kill screen—and some complicated memory manipulation it enables—to code new behaviors into versions of Tetris running on unmodified hardware and cartridges. Further Reading This amazing glitch puts Star Fox 64 ships in an unmodified Zelda cartridge We've covered similar \"arbitrary code execution\" glitches in games like Super Mario World, Paper Mario, and The Legend of Zelda: Ocarina of Time in the past. And the basic method for introducing outside code into NES Tetris has been publicly theorized since at least 2021 when players were investigating the game's decompiled code (HydrantDude, who has gone deep on Tetris crashes in the past, also says the community has long had a privately known method for how to take full control of Tetris' RAM). Displaced Gamers explains how to reprogram NES Tetris within the game. But a recent video from Displaced Gamers takes the idea from private theory to public execution, going into painstaking detail on how to get NES Tetris to start reading the game's high score tables as machine code instructions. Fun with controller ports Taking over a copy of NES Tetris is possible mostly due to the specific way the game crashes. Without going into too much detail, a crash in NES Tetris happens when the game's score handler takes too long to calculate a new score between frames, which can happen after level 155. When this delay occurs, a portion of the control code gets interrupted by the new frame-writing routine, causing it to jump to an unintended portion of the game's RAM to look for the next instruction. Advertisement Usually, this unexpected interrupt leads the code to jump to address the very beginning of RAM, where garbage data gets read as code and often leads to a quick crash. But players can manipulate this jump thanks to a little-known vagary in how Tetris handles potential inputs when running on the Japanese version of the console, the Famicom. Enlarge / The Famicom expansion port that is key to making this hack work. Nintendo World Report Unlike the American Nintendo Entertainment System, the Japanese Famicom featured two controllers hard-wired to the unit. Players who wanted to use third-party controllers could plug them in through an expansion port on the front of the system. The Tetris game code reads the inputs from this \"extra\" controller port, which can include two additional standard NES controllers through the use of an adapter (this is true even though the Famicom got a completely different version of Tetris from Bullet-Proof Software). As it happens, the area of RAM that Tetris uses to process this extra controller input is also used for the memory location of that jump routine we discussed earlier. Thus, when that jump routine gets interrupted by a crash, that RAM will be holding data representing the buttons being pushed on those controllers. This gives players a potential way to control precisely where the game code goes after the crash is triggered. Coding in the high-score table For Displaced Gamers' jump-control method, the player has to hold down \"up\" on the third controller and right, left, and down on the fourth controller (that latter combination requires some controller fiddling to allow for simultaneous left and right directional input). Doing so sends the jump code to an area of RAM that holds the names and scores for the game's high score listing, giving an even larger surface of RAM that can be manipulated directly by the player. Advertisement By putting \"(G\" in the targeted portion of the B-Type high score table, we can force the game to jump to another area of the high score table, where it will start reading the names and scores sequentially as what Displaced Gamers calls \"bare metal\" code, with the letters and numbers representing opcodes for the NES CPU. Enlarge / This very specific name and score combination is actually read as code in Displaced Gamers' proof of concept. Displaced Gamers Unfortunately, there are only 43 possible symbols that can be used in the name entry area and 10 different digits that can be part of a high score. That means only a small portion of the NES's available opcode instructions can be \"coded\" into the high score table using the available attack surface. Despite these restrictions, Displaced Gamers was able to code a short proof-of-concept code snippet that can be translated into high-score table data (A name of '))\"-P)', and a second-place score of 8,575 in the A-Type game factors prominently, in case you're wondering). This simple routine puts two zeroes in the top digits of the game's score, lowering the score processing time that would otherwise cause a crash (though the score will eventually reach the \"danger zone\" for a crash again, with continued play). Of course, the lack of a battery-backed save system means hackers need to achieve these high scores manually (and enter these complicated names) every time they power up Tetris on a stock NES. The limited space in the high score table also doesn't leave much room for direct coding of complex programs on top of Tetris' actual code. But there are ways around this limitation; HydrantDude writes of a specific set of high-score names and numbers that \"build[s] another bootstrapper which builds another bootstrapper that grants full control over all of RAM.\" With that kind of full control, a top-level player could theoretically recode NES Tetris to patch out the crash bugs altogether. That could be extremely helpful for players who are struggling to make it past level 255, where the game actually loops back to the tranquility of Level 0. In the meantime, I guess you could always just follow the lead of Super Mario World speedrunners and transform Tetris into Flappy Bird. ARS VIDEO reader comments 57 Kyle Orland Kyle Orland has been the Senior Gaming Editor at Ars Technica since 2012, writing primarily about the business, tech, and culture behind video games. He has journalism and computer science degrees from University of Maryland. He once wrote a whole book about Minesweeper. Advertisement Promoted Comments johnsonwax Ah, little bobby tables gets a high score. May 7, 2024 at 7:08 am Channel Ars Technica Unsolved Mysteries Of Quantum Leap With Donald P. Bellisario Today \"Quantum Leap\" series creator Donald P. Bellisario joins Ars Technica to answer once and for all the lingering questions we have about his enduringly popular show. Was Dr. Sam Beckett really leaping between all those time periods and people or did he simply imagine it all? What do people in the waiting room do while Sam is in their bodies? What happens to Sam's loyal ally Al? 30 years following the series finale, answers to these mysteries and more await. Unsolved Mysteries Of Quantum Leap With Donald P. Bellisario Unsolved Mysteries Of Warhammer 40K With Author Dan Abnett Steve Burke of GamersNexus Reacts To Their Top 1000 Comments On YouTube Modern Vintage Gamer Reacts To His Top 1000 Comments On YouTube How The NES Conquered A Skeptical America In 1985 Scott Manley Reacts To His Top 1000 YouTube Comments How Horror Works in Amnesia: Rebirth, Soma and Amnesia: The Dark Descent LGR's Clint Basinger Reacts To His Top 1000 YouTube Comments How One Gameplay Decision Changed Diablo Forever How Forza's Racing AI Uses Neural Networks To Evolve 30 People Play Mario Kart 8 From Newbies to Pros Unsolved Mortal Kombat Mysteries With Dominic Cianciolo From NetherRealm Studios How NBA JAM Became A Billion-Dollar Slam Dunk Linus \"Tech Tips\" Sebastian Reacts to His Top 1000 YouTube Comments How Alan Wake Was Rebuilt 3 Years Into Development How Homeworld Almost Got Lost in 3D Space 30 People Play Super Mario Bros. Level 1-1 How Prince of Persia Defeated Apple II's Memory Limitations How Crash Bandicoot Hacked The Original Playstation Myst: The challenges of CD-ROMWar Stories Markiplier Reacts To His Top 1000 YouTube Comments How Mind Control Saved Oddworld: Abe's Oddysee Civilization: It's good to take turnsWar Stories Dead Cells: How to avoid falling to your death (and resurrection)War Stories Warframe's Rebecca Ford reviews your characters Subnautica: A world without gunsWar Stories How Slay the Spire’s Original Interface Almost Killed the GameWar Stories Amnesia: The Dark Descent - The horror facadeWar Stories Command & Conquer: Tiberian SunWar Stories Blade Runner: Skinjobs, voxels, and future noirWar Stories Dead Space: The Drag TentacleWar Stories Aliens versus Predator: The 11th hour decisionWar Stories Ultima Online: The Virtual EcologyWar Stories Blizzard answers unsolved mysteries of the Hearthstone universe Unsolved mysteries of Warframe Unsolved mysteries of League of Legends More videos ← Previous story Next story → Related Stories Today on Ars",
    "commentLink": "https://news.ycombinator.com/item?id=40284291",
    "commentBody": "Hackers discover how to reprogram NES Tetris from within the game (arstechnica.com)239 points by LorenDB 19 hours agohidepastfavorite71 comments mcculley 15 hours agoSuch exploits always remind me of the line from Stross' Accelerando about the ultimate end game for hacking: \"running a timing channel attack on the computational ultrastructure of space-time itself, trying to break through to whatever's underneath\" reply smegger001 15 hours agoparentI am fairly sure I would not want to be within the lightcone of anyone making a attempt with chance doing anything. thats sounds like a good way to trigger vacuum decay and I would rather that universe not bluescreen. reply lampiaio 13 hours agorootparent> I would not want to be within the lightcone If someone were to find an exploit to run arbitrary code using the computational ultrastructure of the universe, I wouldn't be too sure if in-game restrictions could keep us safe, though! reply smegger001 9 hours agorootparentI would be less concerned with them succeeding and more with them failing and crashing the local shard reply volemo 1 hour agorootparentLet’s hope there are backups. reply tux3 14 hours agorootparentprevI'm sure the sysadmins can restart us. They do have backups, right? reply hi-v-rocknroll 19 minutes agorootparentAll of the sysadmins were fired, and along with IT, were replaced with devops SWEs who thought replication was good enough. reply Drakim 14 hours agorootparentprevEven if you have backups, if you have never tested your backups, you don't have backups. reply skeaker 13 hours agorootparentNot to worry, the VM we're on has only been running since last Tuesday. reply kevindamm 6 hours agorootparentIf someone were to restore the universe from a faulty backup, would we even know there was anything missing? Assuming the consistency checks passed. reply Drakim 5 minutes agorootparent> Assuming the consistency checks passed. You have a lot more faith in world-ops than I do. You think the have tests and checks? fragmede 2 hours agorootparentprevso that's where my socks have been going missing! reply mcculley 14 hours agorootparentprevThe computational substrate might just be a side-effect of something else happening in higher dimensions. reply Diederich 13 hours agorootparentprevBeing involved in a vacuum decay event would not be bothersome in the slightest. reply alecco 13 hours agorootparentA very hard sci-fi novel about something like this: https://en.wikipedia.org/wiki/Schild%27s_Ladder (beware spoilers) reply Kipters 1 hour agorootparentprevthat's just the first step for getting Doom to run reply skrebbel 2 hours agorootparentprevThe program doesn't notice when it crashes reply vasco 14 hours agorootparentprevA blue screen would prove someone wrote shitty drivers so we'd know the universe is more like Windows than like OS X reply hi-v-rocknroll 11 minutes agorootparentFuturama already proved the universe is a simulation inside of a simulation written by some forgetful professor. https://youtu.be/9gWgNetp8jE reply jl6 13 hours agorootparentprevThe universe is actually more like Linux: we got it for free, but we have to figure out how it works on our own. reply tambourine_man 10 hours agorootparentYou call this free? reply smegger001 9 hours agorootparent\"Free\" as in, \"you arent paying the AWS bill\" as opposed to \"free speech\" or \"free beer\" reply Aerroon 7 hours agorootparentTo be fair, the universe offers as much free speech as is possible. You can say anything you think of. The universe ain't gonna stop ya. reply snypher 9 hours agoparentprevI'd like to point out this book is available as a free download from the authors blog: https://www.antipope.org/charlie/blog-static/fiction/acceler... reply kromem 12 hours agoparentprevWell, we just discovered a sync error, so that might be a good edge case to start on: https://www.science.org/content/article/quantum-paradox-poin... reply DarmokJalad1701 7 hours agorootparent@skdh has some takes on this: https://youtu.be/GerzZ6GDe-0?t=432 https://www.youtube.com/watch?v=Wsjgtp9XZxo reply wizzwizz4 11 hours agorootparentprevThat's not a sync error: it's just a demonstration that all collapse theories are inconsistent with some other assumptions we like to make. (There are many alternatives, the most famous of which is probably Hugh Everett III's relative state model, though none of them are completely elegant.) It hasn't just been discovered: it's been known since the 60s, and developed on-and-off since. Of course, we haven't really tested this because we haven't attempted to put humans “in a superposition”. Physicalists assume that photons are adequate substitutes for humans, in the thought experiment, but something's wrong with our intuitions, so imo we should adopt some philosophical rigour about this whole thing. reply IIAOPSW 14 hours agoparentprevI believe that's what we call \"physics\" reply Guthur 1 hour agoparentprevSpace-time? Why limit too space, meta-time is so much more interesting. reply hnthrowaway0328 16 hours agoprevGod, I love these people. I feel shameful to not have such a hacker mindset -- to do something probably useless just for the fun of it. I love these people. reply farseer 3 hours agoparentStupid question but are such hackers mostly trust fund babies? With their future secure and nothing but boredom? Coz the rest of us are too busy making a living for such exploits (no pun intended). reply Cthulhu_ 4 minutes agorootparentI feel sorry for you that you have no spare time outside of work, but work on yourself and try and improve the system instead of shitting on other people's projects or making assumptions about their situations. reply thrdbndndn 11 minutes agorootparentprev> Coz the rest of us are too busy making a living Bar some exceptions, most people are not that busy (including me and likely you). They just lack commitment. reply ZaoLahma 1 hour agorootparentprevI doubt trust fund babies would spend their days hacking an old NES game. No idea of who would do it though. My best guess is young-ish brilliant engineers in the beginning of their careers, before they've taken on too much responsibility at and outside of work. Those who still have a lot of excitement for the field with a lot of time on their hands. reply nusl 2 hours agorootparentprevNo. It's very much okay to just do things because you enjoy doing them. Surely not every single thing you do in your life has meaning, but it might have meaning for you. What's the point in dropping a strange negative comment like this anyway? Are you unhappy that others are happy? reply 0xDEFACED 13 hours agoprevHow long before someone runs Doom on Tetris? reply unwind 3 hours agoprevIf you don't want to read the full article but still wonder \"huh I thought NES carts ran from ROM?\", then yeah they do but the exploit manages to make the CPU jump into RAM that is used to store the high score table. Fantastic. reply maCDzP 12 hours agoprevI want to this with Factorio. Build a huge computer within Factorio made out of belts. Make it seg fault and break out of the game. reply Cthulhu_ 3 minutes agoparentFactorio can run Doom though: https://www.youtube.com/watch?v=0bAuP0gO5pc reply colechristensen 11 hours agoparentprevThe factorio guys are way too dedicated to squashing bugs for that. reply apantel 16 hours agoprevTime well wasted. reply freedomben 15 hours agoparentwhat do you do for fun? do you consider that time wasted? (honest question. I did used to consider fun stuff to be time wasted, but as I've gotten older and am paying the price for a long-term high stress lifestyle, I'm starting to think differently about it) reply milesvp 15 hours agorootparentFeynman has a fantastic anecdote about getting over what we might today call burnout. He was not interested in work at all in his professorship, when he noticed a wobble in a plate being thrown, and wondered about it. He spent all day working on the physics of this toy problem, and claims discovering play as the key to his recovery. If you haven't read them yet, \"Surely You're Joking, Mr. Feynman\", and \"What Do You Care What Other People Think\" were great reads in my 20s. reply jvanderbot 14 hours agorootparentThe punch line is that the equations he worked out ended up being useful for his Nobel Prize winning work. So it ended up being useful even if that wasn't the goal. reply Pearse 11 hours agorootparentMy favourite part of that story is when he shows the maths to one of the other professors they can't understand why he is \"wasting time\" on it. And then he goes on to win the Nobel prize because of it.. reply smu3l 15 hours agorootparentprevI don't think GP is suggesting the time was wasted. Rather the opposite. reply indigodaddy 15 hours agorootparentAgree that’s also how I interpreted the comment reply freedomben 12 hours agorootparentIndeed, that does seem quite plausible! Thanks. reply tombert 13 hours agoprevPeople figuring out ACE in old games utterly fascinates me. I remember seeing this in Super Mario World a couple years ago and I became a bit transfixed on how that was even possible. I mean this in the best way, and I am being complimentary, but it's going to sound like I'm being a jerk: I love when really smart people spend a lot of time and effort doing completely useless things. Is there any reason, at least immediately, to inject code into NES Tetris? No, I doubt it, but that's not the point. The point is figuring out what's possible, and figuring out what you can force some old code and a primitive computer to do. It might not be \"useful\" in the classical sense, but neither is a Sudoku puzzle or a crossword puzzle or playing NES Tetris to begin with. reply memco 13 hours agoparent> Is there any reason, at least immediately, to inject code into NES Tetris? It’s somewhat obscured by all the technical details but doing this exploit does have a practical purpose: it allows highly skilled players to play longer since they can now have a way to prevent a crash that prevents them from playing past certain points. For the average player there’s no practicality to this but for those who want to compete for the highest scores this solves a limitation and opens new opportunities for competition. reply krallja 8 hours agorootparentIf you have ACE, what’s the difference between fixing the kill screen bug and `score = SCORE_MAX`? reply memco 7 hours agorootparentI guess that's an open question that the community will decide. I can't speak for anyone in particular, but if you watch live streams of fractal, bluescuti, etc. their primary drive seems to be to have more game to play, not less. They already go far beyond the 'max' score that the UI can display. For them to \"beat\" Tetris is to play so long that there's no more game left to play not just to get the biggest number. reply vsnf 1 hour agorootparentRelatedly, this is why zfg, a famous (debateably the best) Ocarina of Time speedrunner, doesn't do Any%, and has also opted out of the 100% category. He'd rather play the game than not play the game. reply stavros 11 hours agoparentprev> I love when really smart people spend a lot of time and effort doing completely useless things It's not useless: they like doing it. Any use other people might derive from the things you don't like doing (but still do) is either a happy accident, or something that benefits you indirectly (money so you can live, recognition, etc). Doing something because you like it is the most immediate form of usefulness to the person it matters most: you. reply bawolff 12 hours agoparentprev> I love when really smart people spend a lot of time and effort doing completely useless things. Many scientific discoveries happen this way. Number theory was originally considered useless but now powers basically all public key crypto. reply tombert 11 hours agorootparentThere’s that quote Adam Savage always says that goes something like “the difference between ‘science’ and ‘goofing off’ is writing it down”. I’ve always liked that sentiment, since it sort of works to “ungatekeep” science. It’s easy to be intimidated by the seeming monolith of “science”, but fundamentally science basically boils down to “doing, testing, and measuring something” and it doesn’t really matter what that “something” actually is. reply jpalawaga 14 hours agoprevHonestly, I'm surprised that it took tetris so long to be broken! I strongly suspect this will usher in a new era of any% runs, in which the goal is to get the end scene/credits of the game to run as quickly as possible. My favourite example of this is Ocarina of Time, which has had ACE exploits for years now. The game is so totally broken, it can be \"beat\" in just a handful of minutes by manipulating the games memory and editing specific entrance warps. Perhaps most incredibly, people edit the memory with their hands, using nothing more than a couple buttons and the analog joystick. here is someone who rolled credits in just 3m: https://www.speedrun.com/oot/runs/z1l1627m reply jvanderbot 14 hours agoparentI saw a similar one Super Mario, where finding a glitch in a warp tunnel triggered an out of bounds read, and prior joysticking wrote the appropriate bytes just beyond the buffer to trigger whatever they wanted. There was another one in a Pokemon game where you had to do a bunch of buy/ sell transactions to prep memory just so, then overflow an item count to trigger a jump. Truly fantastic stuff. Someday aliens will attack and these antics will save us. reply daveofiveo 10 hours agorootparentSuper Mario World code injection: https://www.youtube.com/watch?v=hB6eY73sLV0 reply smrq 13 hours agoparentprevJust based on reading TFA, I doubt it since the ACE apparently relies on getting to the kill screen in the first place. Imagine if OoT's ACE was triggered by the end credits rolling; you can't improve your times with it because it only happens after completing a run successfully. reply CyberDildonics 13 hours agoparentprevHonestly, I'm surprised that it took tetris so long to be broken! Before this, what made you think arbitrary execution was plausible? reply naikrovek 12 hours agorootparentThings like this are common in software of the era, but few look for such things. reply CyberDildonics 11 hours agorootparentThat doesn't answer the question at all. They said specifically that they expected this in tetris before it happened for some reason. reply chowells 6 hours agorootparentThe entire nature of the kill screen in NES Tetris is that the game crashes because the game has written over its stack. reply bena 17 hours agoprev [–] Getting to the point of arbitrary code execution is always more interesting than what you do with it after the fact. To be able and take apart the game, find out when and where it does all this, then where you can manipulate things to input instructions, is a level of dedication that's admirable. reply Dwedit 17 hours agoparentSometimes even after you've come up with the exploit, you can create something really beautiful for the console to run afterwards. Pokemon Yellow: https://www.youtube.com/watch?v=Vjm8P8utT5g Super Mario World: https://www.youtube.com/watch?v=hB6eY73sLV0 reply NanoWar 4 hours agorootparentOff topic, but hey is that you, Dan from the old Ti83 World? Thanks for the huge include file and I loved your work on DQM ;-) reply bena 15 hours agorootparentprevYeah, but once you get to arbitrary code execution, everything is on the table. It's about as interesting as just creating it from scratch. The \"something\" stands on its own merits, separate from how it was created. reply jordigh 15 hours agorootparentNot everything, depends on the system. On the NES, for example, you can only mess around with RAM if you find ACE, but if the cartridge is using CHR ROM, whatever you create with ACE must still use the only tileset available to the game. You can get creative drawing graphics with a fixed set of tiles, but you'll be constrained nonetheless. There may be other constraints for other systems. I don't know the SNES architecture too well, but I assume even with ACE you're still limited in various ways to the constraints of the cartridges. reply Dwedit 15 hours agorootparentSNES does not put video memory on CHR-ROM, it's all writable RAM. The restriction is total RAM avaialble (including cartridge save ram), possibly calling functions or using data from the original ROM. reply GuB-42 11 hours agoparentprev [–] I particularly like the Super Mario World one. Arbitrary code execution is triggered by an actual shell code. As in, it is done by manipulating Koopa shells in game. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Hackers found a way to reprogram NES Tetris from within the game, enabling players to prevent crashes and introduce new behaviors.",
      "By manipulating crash and controller input mechanisms, players can influence the game code post-crash and inject instructions into the high score table.",
      "This breakthrough offers the potential to eliminate crash bugs from NES Tetris, benefiting players aiming to exceed level 255 or revolutionize the game experience."
    ],
    "commentSummary": [
      "Hackers reprogram NES Tetris, sparking debates on running arbitrary code on the universe's computational structure, touching on Windows, Linux, and simulations.",
      "Discussions delve into free will, reality, quantum physics, and philosophical implications, highlighting the significance of engaging in enjoyable projects and leisure activities for mental well-being.",
      "Unexpected benefits from apparently trivial tasks and accomplishments using arbitrary code execution in retro video games are also shared, emphasizing the value of play."
    ],
    "points": 239,
    "commentCount": 71,
    "retryCount": 0,
    "time": 1715081048
  },
  {
    "id": 40294630,
    "title": "Apple Found Guilty of Illegal Staff Interrogation and Flyer Confiscation",
    "originLink": "https://www.forbes.com/sites/antoniopequenoiv/2024/05/06/us-labor-board-rules-apple-illegally-interrogated-staff-and-confiscated-union-flyers/",
    "originBody": "FORBES BUSINESS BREAKING U.S. Labor Board Rules Apple Illegally Interrogated Staff And Confiscated Union Flyers Antonio Pequeño IV Forbes Staff Pequeño is a breaking news reporter who covers tech and more. Follow Click to save this article. You'll be asked to sign into your Forbes account. Got it May 6, 2024,07:34pm EDT Updated May 7, 2024, 01:02pm EDT TOPLINE The National Labor Relations Board ruled Monday that Apple illegally questioned staff of its World Trade Center store in New York City in 2022, affirming findings from a judge who determined employees were specifically questioned over their pro-union sympathies. Apple has not received any punishment or been ordered to pay damages by the board for the ... [+] GETTY IMAGES KEY FACTS The board affirmed the decision of administrative law Judge Lauren Esposito, who ruled last year that Apple illegally stopped workers from placing union flyers on a table in the break room of the World Trade Center store, confiscated the flyers and interrogated staff over their “protected concerted activity.” Esposito ordered Apple cease and desist from illegally questioning workers about union matters in addition to confiscating union flyers from the store’s employee break room. Monday’s ruling is the board’s first decision against Apple, according to Bloomberg, which first reported the ruling and cited agency spokesperson Kayla Blado. The board cannot impose fines or direct punishments against Apple for its violations. Apple didn’t immediately respond to Forbes’ request for comment. Get Forbes Breaking News Text Alerts: We’re launching text message alerts so you'll always know the biggest stories shaping the day’s headlines. Text “Alerts” to (201) 335-0739 or sign up here. KEY BACKGROUND Other cases against Apple are still pending, according to Bloomberg, which noted a case in which a National Labor Relations Board member accused the company of illegally excluding unionized workers from certain benefits. Several Apple stores have moved to unionize in recent years including ones in Short Hills, New Jersey, Oklahoma City and Towson, Maryland, with the latter two locations successfully establishing a union. Apple employees outside of the World Trade Center store staffers have also run into opposition while seeking to unionize. The National Labor Relations Board found in late 2022 that Apple hosted mandatory anti-union meetings at an Atlanta store where management made coercive statements against employees. FURTHER READING Apple Illegally Interrogated NYC Retail Staff, US Labor Board Rules (Bloomberg) Apple Store employees in New Jersey are trying to unionize (The Verge) Follow me on Twitter or LinkedIn. Send me a secure tip. Antonio Pequeño IV Follow Antonio Pequeño IV is a reporter who covers breaking news, with a focus on technology and online culture. He joined Forbes in 2023 and works in Los Angeles. He’s covered Elon ... Read More Editorial Standards Print Reprints & Permissions",
    "commentLink": "https://news.ycombinator.com/item?id=40294630",
    "commentBody": "U.S. Rules Apple Illegally Interrogated Staff and Confiscated Union Flyers (forbes.com/sites/antoniopequenoiv)215 points by iancmceachern 4 hours agohidepastfavorite102 comments pashsoft 5 minutes agoIt seems like most of the \"union problems\" in the US arise from workers clashing with overly-greedy, abusive employers. The employer generally has more power in this relationship, and uses that power to squeeze every last bit of value out of their workforce. The workforce has few ways to push back against this, with unions being one of the options. Maybe if other options were on the table, there would be fewer union problems? As a worker, would you join a union for protection (and pay for it) if your government provided robust legal defences for you? reply Stranger43 2 hours agoprevI wonder why US companies keep pulling stunts like this despite the data showing little negative impact of unionization to overall productivity nor company survival. It looks to be a purely dogmatic ego trip where somehow blind loyalty is demanded even when it's not actually necessary. reply whoitwas 26 minutes agoparentIf you aren't joking, it's because executives fight against employees to enrich themselves. They only consider the next quarter, not systemic change. reply bell-cot 17 minutes agoparentprevRead up on primate behavior. Top executives are alpha males, and didn't get there without lots of compulsive control-seeking, dominance-asserting, etc. behavior patterns. reply inamorty 14 minutes agorootparentDid Europeans descend from elves instead of primates? reply zimpenfish 41 minutes agoparentprevAt a meta level, if your government isn't particularly pro-union (and one side is rabidly anti-union with the potential for them to be in charge for the next four years), and you are to some extent hostage to governmental fortune (cf the current lawsuits against Apple, the reliance on outsourcing to China, etc.) there's probably a certain amount of political sense in being seen to be anti-union Just In Case. reply highcountess 11 minutes agoparentprevIt is why the collection collections of people called companies do anything, profit and derived individual personal gain of executives. The better question to ask is why the general populace falls for the same types of tricks of the de facto contemporary form of aristocracy. In other words, anything those at the top of the social hierarchy say, advocate, or support is always going to be, at best, more advantageous to those at the top than those in the 99%, and usually is even detrimental. Due to leaked documents from corporations and research itself shows that it is also why corporations push diversity, because it quite literally prevents unionization when employees have no unity. Diversity is actually just an abusive practice that is part of a policy sometimes called “strategic tension”, where conflict and friction is desired in order to prevent unity, not suppressed (seem familiar?) i.e., what an abusive husband may do to his abused wife by making impossible demands or constantly shifting the target and gaslighting about what was said. That was just scaled up to abuse whole segments of society and even all of humanity and then the professional liars in PR firms were deployed to abuse people into believing the bad is good and the good is bad. You can hate me all you want for saying it, but that does not change that I’m just the messenger and nothing changes about the objective facts, those above you are not friends, the most rudimentary measure of that being the amount of money separating you from them. reply hackernewds 21 minutes agoparentprevIs that actually true, you do see unionized pilots and police unions functioning pretty unproductively and in bad faith reply DrewRWx 13 minutes agorootparentPolice aren't labor and therefore are not a labor union. reply nicce 2 hours agoparentprevIf it saves even a little money, it will happen. reply Stranger43 2 hours agorootparentYep but that's again a dogmatic view as there is hints that anti-unionization measures are the kind of cost cutting that kills companies rather then the kind that makes them huge successes. reply sverhagen 8 minutes agorootparentIn companies with a healthy culture, maybe. But if staff unionizes, how's Amazon going to make them pee in a bottle to squeeze out a few more deliveries, without running into trouble with the union. I'm sure other companies have their own terrible anecdotes. reply Rodeoclash 1 hour agorootparentprevCompanies do all sorts of itrrational things that cost money reply komali2 2 hours agoparentprevI personally believe companies are given way too much credit in being considered rational pure-profit motive actors. They're far worse than that (a rational pure-profit motive actor is also bad because it would inevitably select for slavery): they're typically run by extremely conservative, often ivy-league people (if not in alma mater, then in worship of ivy league culture). These people are trained from the get go to associate unions with everything antithetical to their cult heroes of people like Ford or Edison. Once in an argument with a similarly minded founder, he dropped the line on me, \"unions are anti-capitalist,\" which to me really reveals the mindset that to at least him, negotiating from positions of equal power is not capitalism. reply thuridas 58 minutes agorootparentYou mean that forcing developers to work 60 hours weeks do not work? Surprise!! reply mytailorisrich 2 hours agoparentprevIf it made no difference why would employees bother? It does make a negative difference for employers with potentially higher costs and lower flexibility over time. reply gklitz 1 hour agorootparentGreat question. Ledt assume that you’re discussing a topic like working hours. Your company says you have to work. 14h each day, and you argue that if you have to do that, you’ll be sleep deprived, make mistakes and be so low productivity that it wouldn’t benefit the company, especially since they would have to pay you more with a constant hourly wage than if you worked fewer hours and was more productive. The company argues: “STFU pessent, you’ll work when we say, how long we say and exactly how we say or you’re fired this instance, now get back in line before I duct your pay for the time you’re wasting by even thinking you’re worthy of talking to someone in management!” Now in that scenario the worker has no power at all, even though it actually is as we all know today, a proven benefit to have resonable working hours for the company. So if you have a union and all the workers can collectively go “we need to talk about work hours or we strike” you force through the policy that is ultimately beneficial to the company, even though management hates the idea of there being 6hours more per day that their wage slaves are not at the work benches. Why would workers argue for fewer hours if it doesn’t end up costing the company net profits? Because they are humans, not machines, and not everything in this world boils down to pure profit driven motives. reply wddkcs 30 minutes agorootparentThis is a bad example and overall I don't see the argument. For decades companies did force workers into extremely long hours. The standardization of the 40 hour work week was in some sense a collective action effort, starting with the request of the National Labor Union in the U.S. in 1866. [1] Prior to those efforts, workers in industrialized positions were indeed working 80 to 100 hours a week on average, with little to no recourse. Beyond that, you ask 'Why would workers argue for fewer hours if it doesn't end up costing the company net profits? The workers don't care about net profits, at least at the expense of their own time. Time is money, and if the company has more of your time, you have less... [1] https://www.businessinsider.com/history-of-the-40-hour-workw... reply chii 28 minutes agorootparentprev> and not everything in this world boils down to pure profit driven motives. that's not true. In fact, the polar opposite is true - every decision is in service to profit and the bottom line. It is only laws that prevent slavery. It is only laws that prevent over-work or exploitation. I would say that it is better to have regulation over unions. Unions may or may not act in the interest of the employee - it's a hit or miss depending on the actual union in question. But as a citizen, you have some semblence of a vote on the gov't, and at least everybody gets the same gov't. reply sebtron 2 hours agorootparentprevIt makes a positive difference for the employees. You seem to imply that this is correlated to a negative difference for the employer, which is completely wrong. reply mytailorisrich 2 hours agorootparentObviously in general it makes a positive difference for employees, that was a rhetorical question... I have listed two main negative differences it makes for employers. I am hard-pressed to find any positive differences for them... on the issues this touches it's pretty much a zero-sum game so if employees gain it means employers 'lose'. If something is positive for both sides then I think the market will eventually adopt it on its own. reply sebtron 1 hour agorootparentThe purpose of unions is to make sure the employees work is sustainable, easier, pleasant if possible. On a more practical level this means enforcing work regulations, if an employer is at risk of breaking regulations (intentionally or not), unions will help them keep the line. A happier employee is more productive and less likely to leave the company on the forst occasion or do anything hostile (stealing, selling company secrets). This is not exactly news, it has been known for at least 150 years. reply mytailorisrich 1 hour agorootparentThe purpose of unions is to extract as much as possible from the employer on the behalf of its members, or at least to extract more than the employer would otherwise have given. You do not contradict anything I have written nor provide any examples of how unions might be a positive for employers... Again, if an happier, more productive employee benefits the company then the market will sort it out by itself (For instances tech companies have not needed unions to offer high pay and plenty of perks). That said, it's not an universal truth that a happier employee is a net gain. reply sebtron 10 minutes agorootparent> The purpose of unions is to extract as much as possible from the employer on the behalf of its members, or at least to extract more than the employer would otherwise have given. > You do not contradict anything I have written (...) I mean, you directly contradict me in the previous paragraph about what the purpose of unions is, can we at least agree to disagree? > (For instances tech companies have not needed unions to offer high pay and plenty of perks) That's true, some companies do well without unions, but this does not make unions universally useless. reply close04 44 minutes agorootparentprev> if an happier, more productive employee benefits the company then the market will sort it out by itself I don't think this argument in particular stands well against reality. The market is generally driven by players with concentrated power and it's very likely the interests of those players will align creating an even more one sided power imbalance. People's preferences are low entropy. Things that are in the interest of individuals rarely \"sort themselves out\" without some intervention, usually from a regulatory body. And even that's less a democratic exercise than it is a lobbying one where concentrated donations are worth more than sparse individual contributions. In the US there was a time when more people could own a house, car, and raise a family with just one family member's income. Now it's increasingly difficult to do it even with two incomes. People didn't decide to just work more and afford less. The market sorted itself to benefit those who already had more power and could influence. reply ok_dad 51 minutes agorootparentprevYou're saying \"the market\" like it's some magic word that makes everything fair. The market for employment is not fair. Companies hold most of the power in America today, for most jobs. Highly paid white collar workers may hold a slight edge, sometimes, but it's not common. One of the ways that workers can balance the power that a company has over them, is to unionize so that as a group all of the workers can affect the company's decisions. This is good for the market, because it equalizes power and allows for the synthesis of good efficiency through fair price discovery for labor and good worker conditions through the equality of power that they hold in a union. You can scream THE MARKET until your throat is dry, but it doesn't mean that an unfettered capitalism is the best thing for the country as a whole. Everyone must be thriving, otherwise we're all failing together. reply arrrg 1 hour agorootparentprevIt is perfectly possible that a given company performance can be reached by different paths, some better for employees, some worse. In fact, I think that even seems like a very plausible hypothesis, given that treating your employees worse can have advantages (reduction in costs and headcount) and disadvantages (less motived and potentially productive employees, worse retention, difficulties attracting people) for the employer, so those two may roughly cancel out under a whole lot of different conditions. So for the employer there may be no benefit (but also not really a downside) to treating employees better, but add to the mix this strong cultural idea in US business circles that unions are the worst and you get this taboo against unions and no market pressure to change that. Obviously this is a quite horrific situation to be in because we get worse outcomes for employees and unchanged outcomes for employers. So no one benefits and most people have worse outcomes. Bad all around. reply jensgk 32 minutes agorootparentprev\"If something is positive for both sides then I think the market will eventually adopt it on its own.\" In Denmark we (by law) all have 5 weeks of holiday per year (besides public holidays), paid sick leave, 37 hour work week, paid maternity care (m/f), reasonable notice of termination, reasonable rules for work environment, etc. All because of our unions. How is it in the US? reply geertj 8 minutes agorootparentThose very generous working conditions, and the mind set that it comes from and creates, I am convinced, is also why Denmark does not have an Apple, Tesla, Google, Nvidia, or SpaceX, and why the AI revolution is heavily based on the US. I’m not dinging you for it. I am ex EU (NL) myself and have lived under both systems enough time to have experienced the difference. Countries and individuals can make choices between quality of life and achievement. And while there are some short range positive correlations (more quality of life leads to better thinking and more productivity), I think the long range correlation is negative (p100 achievement will require long hours and sacrifices). Note that I am not saying one side is inherently better than the other. I’m saying it’s a choice with consequences. It is essentially a question about what you value in life. reply simiones 53 minutes agorootparentprevYou're missing the simple fact that this idea that the market will arrive at every positive thing that can be quantified is simply wrong. Markets will very very often converge on local maxima if allowed to. This is obvious if you think about it from a simple computational perspective: markets at best do a random walk search, and that can only ever find a local maximum, not the global maximum of a function. reply lukan 1 hour agorootparentprev\"If something is positive for both sides then I think the market will eventually adopt it on its own.\" Due to millions of regulations already in place, the market is not really free, but very distorted. So even if the \"free market\" would always go for the best solution, current state is nonproof for anything. Otherwise you might argue, that europe would be a proof, because unions here are strong. reply actionfromafar 41 minutes agorootparentWithout regulations the market would remain free for at most a hot minute and you would get regulation again, your own or someone elses, to paraphrase. reply felipelemos 8 minutes agorootparentprev> If it made no difference why would employees bother? despite the data showing little negative impact of unionization to overall productivity nor company survival. Employees should bother to improve their own lives, not to decrease productivity or affect company survival. reply globular-toast 47 minutes agorootparentprevYou're assuming it's a zero sum game? reply boomboomsubban 4 hours agoprevThe key section of this story is >The board cannot impose fines or direct punishments against Apple for its violations. Which makes me wonder why they spent two years working on this investigation. reply nickff 3 hours agoparentThey have a lot more power than you might think from that quote. https://en.wikipedia.org/wiki/National_Labor_Relations_Board... They can now try Apple in an administrative court, with an 'administrative judge' selected by the agency, then get a (normal) Article 3 court to rubber-stamp their 'order'. reply balls187 4 hours agoparentprevPure speculation—the doj can use that determination against apple in any future proceedings. It’s like a GAL report for a family law case; independent investigator provides a report which the court will then use to inform it’s decision. reply emsign 43 minutes agoprevI simply don't trust companies that crack down on union organizers. reply jollofricepeas 3 hours agoprevI don’t understand how so many HNers are anti-union and anti-collective bargaining. The fact that corporations and billionaires are so anti-union and engage in regular conduct like this demonstrates that unions are pro-worker and ideal for the balance needed in the US and across the globe. Balance is key between workers and owners. Without it we are left with growing wealth inequality such as… - ridiculous housing prices - stagnant wages - poor public school education - no retirement benefits - expensive public universities reply thequux 2 hours agoparentI think unions are great! I'm even a member of one, and have been known to encourage everybody else around me to unionize. Notably, though, I love in Belgium. The American implementation of unions, on the other hand, is not great. While in the US, I've run into rules like not being allowed to plug something in because only union electricians are allowed to do that, and that's just one example among many. Police unions and teacher's unions are mostly known for making bad employees unfirable outside of egregious circumstances, and teacher's unions in particular don't seem to have improved pay or working conditions much. I have no idea where the difference comes from, but for people who have only seen and interacted with the American system, I don't blame them for not liking unions reply actionfromafar 36 minutes agorootparentThe differences were made law with the Taft-Hartley Act of 1947. I.e. sympathy strikes are illegal in the US, so the only real power left to unions is jealously guarding the petty pearls available to them. reply cjk2 2 hours agoparentprevI'm anti-union AND pro labour regulation. Your government should be targeting wealth inequality with legislation and public services on all of these issues. The union tends to become another opaque organisation with a power hierarchy of its own and layers of internal politics and power struggles. The unions rarely if ever push for legislation change either because it would risk their existence. The corporations should live in fear of the government, not an independent union. Also my father was a union member for many years but was ousted after suggesting that full time management were wasting members' money on expenses. This turned into a 2 year long shit show where he was pretty much forced out of the job he had for being a non union scab. As much as being told he was a lesser human. reply therouwboat 2 hours agorootparent\"The corporations should live in fear of the government, not an independent union.\" I think that membership funded union would be much likely to protect worker rights than tax funded government. reply cjk2 1 hour agorootparentBut the union has little to no legislative power, only relying on lobbying and private legal cases, at which point you end up competing with non-union entities who have a lot more money. reply h4ckerle 1 hour agorootparentA general strike has a lot of power. Which is why they are banned in many countries. reply Stranger43 1 hour agorootparentprevIt's not not always been the case that the unions were the weaker non-state actor, in fact they used to rival political parties and business cartels in their ablity to lobby/negotiate. And in some countries that power havent completely faded away. reply pmontra 2 hours agorootparentprevThe interests of the people working inside a union might align with the interests of the workers they represent but as in any other organization they are focused on their own job and careers. The people at the bottom are probably more idealistic. When I hear about strikes in my country and their purpoted goals and the way they are made, most of the time I feel like their goal is to advance the careers of somebody in the union and not to get something for the workers. Furthermore most unions have clear links with some political parties, from left to right. They are part of the overall political play to the next elections. reply lotsofpulp 1 hour agorootparentEven the interests of the older members of the union diverge from the younger members (not unlike a country). Pretty much every defined benefit pension plan has “tiers” of benefits, where the older members voted themselves higher benefits in exchange for younger members getting less. reply whoitwas 21 minutes agorootparentprevYou're scared of monolithic unions, so give all power to government? Unless workers own the company, unions are needed to give voice to power. reply ein0p 2 hours agoparentprevPublic school education is poor in part because it’s darn near impossible to fire bad teachers though. And I don’t see how unions would improve housing affordability. I’m not against unions per se, particularly for positions which are not as lavishly compensated as a typical white collar FAANG employee. I’m merely pointing out that you need to pick better examples. Otherwise you just undermine your own argument. reply dragonwriter 1 hour agorootparent> Public school education is poor in part because it’s darn near impossible to fire bad teachers though. The funny thing is that public employment (in the US) is where unions have the least impact on firing difficulty, because employees have a property interest in their employment which means government — even as their employer — cannot deprive them of it without due process under the 5th (for the federal) or 14th (for states and subdivisions) amendments even if they are not unionized. Also, the line you push about teachers is pushed by privatizers (and sometimes administrators as an excuse for their own failures), often supported by one or two anecdotes, so often as to have become an article of faith ib sone circles, but there is no systematic evidence for it. reply walthamstow 2 hours agorootparentprevNeither of you are giving examples, only conjecture. Bad teachers are a noteworthy cause of poor schools. House prices wouldn't be so high if we had unions. Ok, prove it. Or at least give me some reasoning to go off. reply pbmonster 42 minutes agorootparent> House prices wouldn't be so high if we had unions. In Switzerland, all the large unions (railroad workers, public service workers, ect.) have so much money and so much access to cheap credit, they can (and frequently did in the past) start housing co-ops for their members. This results in large developments in the middle of the most expensive cities, where only union members have a chance to rent. And even those have year-long waiting list on every new development. Since the housing co-op is non-profit and pays rock bottom interest rates on its loans, rent is frequently around half the fair market rate. reply lupusreal 34 minutes agorootparentWhere did those unions get so much money from? reply ein0p 2 hours agorootparentprevWhat is the mechanism under which houses would be more affordable if employees had more money thanks to the unions? Real estate (and by extension rents) will take up any available slack on income. reply dragonwriter 1 hour agorootparent> What is the mechanism under which houses would be more affordable if employees had more money thanks to the unions? Employees have more money thanks to unions -> those deriving income from capital have less money -> narrower distribution of wealth -> goods (including housing) which both classes pursue are more affordable to the working class. QED. reply diffeomorphism 1 hour agorootparentprevHousing prices rose much, much faster than inflation or rises in wages. So they are obviously not driven by income. Try again. reply pera 2 hours agorootparentprevI think gp was referring to tenants/renters unions: https://en.m.wikipedia.org/wiki/Tenants_union They are quite common in the UK. reply komali2 2 hours agorootparentprev> Real estate (and by extension rents) will take up any available slack on income. Only if allowed to do so - but yes, landlord are leeches who do nothing and in return soak up every penny of income they can from their tenants. However, isn't the typical market-capitalist argument to this, \"if they could be making more money now, they would?\" Don't \"markets\" set the price of real estate? Markets don't, but it's really funny to me when people argue against good things, such as increasing minimum wage, with absurdities, like somehow a minimum wage increase is simply a subsidy for landlords. reply lupusreal 51 minutes agorootparentprev> Bad teachers are a noteworthy cause of poor schools You have that completely backward. Virtually all bad teachers start out hopeful and excited to teach kids, that's why they become teachers in the first place. They are then worn and beaten down by bad schools. Bad schools are mostly caused by bad students, who are disrespectful, disruptive, get into fights, etc. Bad students are mostly caused by bad parenting. If they aren't raised to respect adults, education, authority and their peers, then there is little that teachers can do about it. The reason \"bad schools\" have trouble keeping enough teachers despite high levels of state and federal funding is because the students burn through the teachers quickly. The teachers who hang on for a long time at those schools are often ones that develop coping strategies like not giving a shit, finding pleasure in yelling at kids, etc. reply refurb 1 hour agorootparentprevI’m not anti-union, but I’m not pro-union either. I was in a union, Teamsters in fact. It protected crappy workers, prevented any change to work processes that might save time and put a low ceiling on high performing workers. Not all unions are like that, but they certainly aren’t panaceas to all workers ills (in fact, sometimes they are the cause of new ones). reply komali2 2 hours agorootparentprev> impossible to fire bad teachers though. It's impossible to fire bad teachers (it really isn't, btw, at least not in Texas) because there are no teachers because teachers wages are pathologically low because every education department in the country is underfunded. reply nox101 1 hour agorootparentIIUC the USA spend near the top per student of all countries. Why is it other countries get better results with less money? https://nces.ed.gov/programs/coe/indicator/cmd/education-exp... reply geraldwhen 1 hour agorootparentWhite and Asian students, if stratified separately, rank in the top 10 academically world wide. Everyone else brings down the average, and it has nothing to do with teachers or how good they are. Teachers in my county are assigned to schools randomly, and yet some schools score 9/10 and others score literally 1/10 and 2/10 on great schools for academic performance. Funding between schools is the same. Teachers assigned are from the same pool. It’s not the money. It’s not the teachers. Anyone who talks about “education in the USA” without addressing the giant disparity in academic performance that begins in kindergarten for black and Hispanic students is ignoring the single largest factor on student outcomes. reply MathMonkeyMan 25 minutes agorootparentWhy do black and hispanic students score worse? reply impossiblefork 1 hour agorootparentprevYes, but if we look at teacher salaries, the average in the state of New York is 79 637. In Sweden it's around 41550 SEK per month, which implies 498 000 SEK per year, which including the tax would be 647 400 SEK per year, i.e. 59 000 USD. But Sweden has a per capita GDP of 56000 USD whereas the per capita GDP of New York State is 104343. It's not going to be incredibly competitive. reply weberer 45 minutes agorootparentYou need to subtract the tax rate, not add it. reply impossiblefork 4 minutes agorootparentNo. In order to compare a wage to GDP one should calculate the amount of money that the employee actually costs the employer. So tax is in. I need to compare the same kinds of things. The American numbers are also pre-tax. Here in Sweden though, the fully pre-tax numbers are rarely reported. Rather the wages minus a 1-1/1.3 tax, so I have to reverse that tax by multiply by 1.3. quartesixte 53 minutes agorootparentprevI am going to go out on a wild limb here, and conjecture that one of the US's biggest weakness is how they handle teachers. 1. Above average talent in any subject field do not join the talent pool for teachers in the USA. Industry pays better and has better working conditions (both real and perceived). This is going to be a major cultural hurdle but the USA needs to break away from the idea that teaching is lesser work. This will create more political will to boost teacher funding. 2. Free Market Economics. You heard me. Teachers in the USA have the unique \"opportunity\" of having to find and interview for positions. Two problems: First, you end up with talent pooling unevenly. Next, you have teachers stagnating at one school for decades. Solution: central deployment. Teachers are hired by districts, and then the district rotates teachers through all schools in the district over the course of their career. Seniority grants some preference ranking, and longer rotation periods. This also counters a third problem: Teachers finding themselves out of a job, or facing job prospects so dire that they just leave the field altogether. There is a level of security, and those who wish to proceed to administration are given opportunities towards the ends of their careers. 3. Administration as a separate career track. Administrative bloat is becoming a real problem, and both the higher pay associated + the opportunity to laterally move into the track via \"further education\" is a root cause. This makes them a separate class with competing interests, and incentives will only lead to more bloat as well as siphon away talented teachers who get burned out by teaching (mostly due to the abysmal pay). My time in Japan gave me perspective on a perhaps better way: a) School based administration (principals and vice principals) is a role only obtainable at the end of your career, on a fixed timeline. If you decided to put yourself on that track, you will most likely be in the last 6 years of your career, and your last 3 years as principal will end in retirement. This both prevents admin bloat and class distinction, while allowing skilled, senior teachers to provide guidance and pass along wisdom.\\ b) School District administration -- that is all the bureaucratic management of schools -- is a completely separate career track of professional, general bureaucrats who are assigned to school administration as part of a career rotation inside their career at government. Teachers teach, and become experts at teaching. In the end, they are rewarded by becoming Principal Teachers. Bureaucrats manage, and in the end are rewarded by becoming ... senior bureaucrats somewhere inside the halls of Government, by being a good administrator during your rotation. And thus begins a virtuous, positive feedback loop of good teachers rotating around a school district reinforcing good teaching, and then ultimately being rewarded by becoming the ultimate teacher. At the same time, administrative entrenchment is avoided because administration is an assigned, rotating task. reply lostlogin 1 hour agorootparentprevThat’s causing the same problem then. If there are no teachers to hire, you’re less likely to fire reply raverbashing 2 hours agorootparentprevIt would be better if they could fire the bad students as well reply badgersnake 2 hours agoparentprevIt’s a forum run by a VC firm. reply strken 1 hour agoparentprevAre so many of them anti-union? I don't see much evidence that they're any more against unions than the general population. It's surprising to me that nobody has pointed out this article has far fewer anti-union than pro-union comments. reply tivert 3 hours agoparentprev> I don’t understand how so many HNers are anti-union and anti-collective bargaining. You underestimate the appeal of libertarian propaganda to software engineers. It's got the oversimple but neat models we can't resist, plus our relatively advantaged economic position allows allows us to easily confuse ourselves for capitalists. reply dragonwriter 2 hours agoparentprev> I don’t understand how so many HNers are anti-union and anti-collective bargaining. HN is full of the aspirational haut bourgeois, of course they want the proletariat maximally exploitable. reply cbsmith 2 hours agoparentprev> The fact that corporations and billionaires are so anti-union and engage in regular conduct like this demonstrates that unions are pro-worker and ideal for the balance needed in the US and across the globe. I think there are a lot of things that demonstrate that unions are pro-worker and ideal for the balance needed in the US and across the globe, but that isn't one of them. Corporations & billionaires are so anti-union because they perceive unions as reducing their profits, which is not necessarily the same thing as them being pro-worker or ideal. You'll find that, outside of the defense industry, corporations & billionaires generally don't like wars, for exactly the same reason; that doesn't really suggest anything good about wars. reply wickedsickeune 2 hours agoparentprev> The fact that corporations and billionaires are so anti-union and engage in regular conduct like this demonstrates that unions are pro-worker and ideal for the balance needed in the US and across the globe. No, that's incorrect. The corporations could also be anti-nuclear war, this would not make it good for workers. A better example is to look at countries with functional unions and functional capitalism, like the Netherlands, Denmark and others. The quality of life won by union conflicts with capital owners is a measurable fact. Unions can be implemented and be useless for the workers (an example for the most part is Greece), or maybe too demanding like France. It's up to the participants of the union to shape it as a net contributor to society. The point is that each individual member has a significant say in how a union operates, meanwhile in corporations, even middle management can be completely powerless. reply mytailorisrich 2 hours agoparentprevI suspect most people here are highly educated in sought-after, high-paying jobs. There is a valid point that the net benefits of unions in such cases are low. Workers unions were created by and for factory workers, miners, etc. People who had zero leverage individually. In addition, we might be a more individualistic and competive crowd who think they are better than average and thus better off bargaining for themselves. reply komali2 2 hours agorootparent> People who had zero leverage individually. This is interesting to say considering the record amounts of layoffs we've had across the tech sector of these \"high leverage individuals\" in the thousands the last few quarters. reply mytailorisrich 2 hours agorootparentLayoffs are not a good example when discussing highly paid jobs with plenty of perks. In a historical perspective I really mean being squeezed to the limit with no possibility of complaining or of \"just moving somewhere else\". reply jajko 2 hours agoparentprevYou are trying to blame literally every public policy failure in US on unions, that's simply incorrect and they stem from many other sources. I know its convenient in some populist fashion, but unless you understand the root cause of each of those, any sort of fix has no chance of arriving. Unions make corporations extremely sluggish when they need to react quickly to sudden market shocks or changes of direction of companies, be it positive or negative (yes, includes firing people for which there is suddenly no work anymore). There are corporations that have them heavily and they are not the top of the cream when it comes to progress, agility nor salary pay. The money is the key issue here - people want extra safety/bargaining net as long as their ultra massive FAANG-like salaries are not touched. How many would agree with say 20% cut while working exactly the same, but being unionized? That's maybe how much that additional burden is to the owners/managers. I suspect not that many but maybe I an wrong. reply badgersnake 48 minutes agorootparentIt’s far easier to tell people that if we just fix this one thing then everything will be fine than try to deal with the complexity of the world and actually make things better. In the UK it’s been the EU, which we left so they moved on to immigrants. That was too hard to fix so they’ve recently decided to pick on the sick. reply flandish 2 hours agoparentprevIt’s because HN folks seem to consider themselves above working-class or soon to be above. They’re just temporarily stuck before they find their big ceo level breaks. They often demonstrate ideals that go counter to their own interests while simping for daddy ceo’s boot and money. reply ExoticPearTree 2 hours agoparentprev> I don’t understand how so many HNers are anti-union and anti-collective bargaining. I had the displeasure of working with union labor in the US, they would come in at 7 (who does that) and leave at 4 on the dot. Then worked with non-union labor: cheaper, worked as much as possible per day, got the job done faster. Money-wise it was less than union labor. Now tell me how is good for me, as a company that needs stuff done, to use union workers? reply ok_dad 1 hour agorootparent> Now tell me how is good for me, as a company that needs stuff done, to use union workers? Because in a race to the bottom, even the winner ends up in shit. The reason America is turning to crap now is because everyone is just out for themselves, and that's mostly folks like you who just want to get stuff done cheaply. Unions may not be the answer for your company, but please at least treat your workers like they matter, and pay them better than you think you should, because they are your company, they are America, and the rampant evisceration of the middle/working/blue collar class by the greedy corpos will destroy us all before anything else. I truly don't understand how businesspeople cannot understand that humans are your most important resource. You run roughshod over them for a bit more profit. It's perplexing! reply ExoticPearTree 55 minutes agorootparentY'all missing the point in this: - I hired a company to do some construction work and in that particular location, due to how the contracts were set, I could only get union work. Expensive and no over time. - In another location which did not had this kind of restriction, I hired another company that was not unionized: they worked longer hours and as a result finished faster. Also their hourly rates were lower. Point being, in a non-union market you can do a lot more faster. And to get to the financial issue, all hours were paid, so just in case it is not clear, there was never a question of not wanting to pay overtime. The question was why I should pay ~20-25% more for the same work? reply ok_dad 43 minutes agorootparentHave you ever researched the difference between the companies, the workers, how satisfied they are, how much they are paid, and why it was a quarter cheaper? You keep saying how the union affected you for your two projects, and how the union was the bad guys because you had to spend more time and money. Did you ever think that perhaps the workers at the union company could have been secure in their jobs, happy with their work/life balance and that the workers at the other company could be beaten down, pushed too hard, paid too little? In any case, your few experiences could be caused by a lot of stuff, perhaps not even the union! I don't know, without more research. Your particular story is from your perspective only, and doesn't even delve into any other reasons this could have happened, like local noise ordnances or something. This is my point: you don't care about the human cost of your entrepreneurial enterprises, and that is in my opinion the reason that America seems to be infighting and \"going downhill\" as they say. Greed, an uncaring attitude about others, and narcissistic \"me me me\" behavior is the problem, not a few workers trying to get a bigger cut. Unions aren't the enemy, those who would attempt to take advantage of workers are. America is only strong if we're all strong, not just a few tenths of a percent at the top. I am not replying any more to this thread, all I have to say was said here. reply ExoticPearTree 32 minutes agorootparentYou do know that you can switch jobs if you don't like the one you're in at the moment, right? And it is not my problem with how much someone if paid, really. I put out a request for work, companies bid, the cheapest one overall wins. That's how the free market works. And sure, believe whatever you want, but getting the best deal possible is not going to be the end of the US :) reply badcppdev 1 hour agorootparentprev> they would come in at 7 (who does that) To answer your question human beings who want work life balance and want to do things with friends and family in the afternoon. Or do you believe that a work day should ONLY be for work? reply ExoticPearTree 44 minutes agorootparentMy gripe is with the fact that somehow they got the building owner to only allow union work. And by doing this, they distorted the market in having no option but to get things done on someone else's schedule. And having this kind of restrictions is hurtful to the business. reply komali2 2 hours agorootparentprev> how is good for me, as a company that needs stuff done, to use union workers? For the company, it's very good, because unionization, or better yet, being a co-op, means your company will be more likely to be profitable in the long term. But you are correct that in these situations, your employees are more difficult to exploit for longer hours, harsher conditions, and worse pay. So yes, if you're trying to be a major shareholder-owner, unions may hurt your individual bottom line, even if they benefit the corporation as a whole. reply ExoticPearTree 51 minutes agorootparentI think you should read my initial comment again. And then the next one, answering to an angry person just like you: It is about working as much as it is needed to get work done, not about not paying. And there is nothing wrong in getting the best possible price on a contract. reply medo-bear 3 hours agoprevThose who do not move do not notice their chains. - R. Luxemburg reply lostlogin 1 hour agoparentThanks - Rosa Luxemburg was an interesting human. https://en.m.wikipedia.org/wiki/Rosa_Luxemburg reply Woshiwuja 1 hour agoprevdisgusting fascist behaviour reply sjtgraham 3 hours agoprev [–] The whole idea of the executive branch functioning as the judicial branch is a bananas concept to me. reply wolverine876 2 hours agoparentInstead of the same old talking point, what solution do you propose? Should the executive branch agency make decisions by fiat, without due process? The use of due process doesn't make them a judiciary; it's just a more fair way to make decisions within their purview. Should we move every such judgment out of the executive branch's hands? First, they have to be able to make some decisions. But the heart of this issue is, as is well known: There aren't nearly enough capacity in the judicial branch to handle all that work. It's a trick to prevent lawful and needed regulation from happening by severely cutting their capacity. reply FBT 2 hours agorootparent> There aren't nearly enough capacity in the judicial branch to handle all that work. Then appoint more Article 3 judges. It could even be the same people who are now \"administrative judges\"—but take them out of the executive branch hierarchy, and give them the independence that the constitution requires judges to have. reply sjtgraham 2 hours agorootparentprev> Instead of the same old talking point, what solution do you propose? Unnecessarily hostile opening, but full separation of powers as written in the Constitution. > Should the executive branch agency make decisions by fiat, without due process? It should not be adjudicating any kind of legal controversy. That is not the role of the executive branch. > The use of due process doesn't make them a judiciary; it's just a more fair way to make decisions within their purview. It's very often worse. Executive agencies often have legislative functions, i.e. APA rulemaking, as well judicial functions, e.g. in this matter. The SEC is another one, (see SEC vs Jarkesy currently before SCOTUS). > Should we move every such judgment out of the executive branch's hands? Yes. > But the heart of this issue is, as is well known: There aren't nearly enough capacity in the judicial branch to handle all that work. It's a trick to prevent lawful and needed regulation from happening by severely cutting their capacity. Total BS. reply tsimionescu 30 minutes agorootparentThis makes no sense. They are not adjuticating a legal issue, they are investigating whether they should start legal proceedings. What's your alternative? That they should just sue every company every year so that a judge can decide if that company is doing illegal union busting activities? reply badcppdev 1 hour agorootparentprevAnd what solution do you propose? reply cbsmith 2 hours agoparentprev [–] They aren't functioning as the judicial branch. As part of the responsibilities of the executive branch, they have to decide when there's a problem, so they can decide if they need to execute. Indeed, the next step here is to take Apple to court. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The National Labor Relations Board found that Apple unlawfully questioned employees and took away union pamphlets at its World Trade Center store in NYC, as per a judge's determination.",
      "Despite the violations, Apple has not been penalized for these actions, while additional cases on unionization issues involving Apple are ongoing.",
      "Over recent years, several Apple stores have managed to form unions successfully."
    ],
    "commentSummary": [
      "The article delves into allegations of labor law violations by Apple, emphasizing power dynamics and greed in the workplace, and the significance of unions in mitigating such issues.",
      "It compares the market dynamics of the US and Denmark, evaluates the role of unions in safeguarding workers' rights, and assesses implications on housing affordability and education quality.",
      "Furthermore, it explores the impact on tech workers and construction sectors, underscoring the delicate balance between profits and worker welfare, while emphasizing the role of the judicial system and due process in decision-making."
    ],
    "points": 218,
    "commentCount": 103,
    "retryCount": 0,
    "time": 1715145904
  },
  {
    "id": 40292181,
    "title": "Decker: A Modern HyperCard Revival with 1-Bit Graphics",
    "originLink": "https://www.beyondloom.com/decker/index.html",
    "originBody": "Decker Decker is a multimedia platform for creating and sharing interactive documents, with sound, images, hypertext, and scripted behavior. You can try it in your web browser right now. Decker builds on the legacy of HyperCard and the visual aesthetic of classic MacOS. It retains the simplicity and ease of learning that HyperCard provided, while adding many subtle and overt quality-of-life improvements, like deep undo history, support for scroll wheels and touchscreens, more modern keyboard navigation, and bulk editing operations. Anyone can use Decker to create E-Zines, organize their notes, give presentations, build adventure games, or even just doodle some 1-bit pixel art. The holistic \"ditherpunk\" aesthetic is cozy, a bit nostalgic, and provides fun and distinctive creative constraints. As a prototyping tool, Decker encourages embracing a sketchy, imperfect approach. Finished decks can be saved as standalone .html documents which self-execute in a web browser and can be shared anywhere you can host or embed a web page. Decker also runs natively on MacOS, Windows, and Linux. For more complex projects, Decker features a novel scripting language named Lil which is strongly influenced by both Lua, an imperative language popular for embedding in tools and game engines, and Q, a functional language in the APL family used with time-series databases. Lil is easy to learn and conventional enough not to ruffle any feathers for users with prior programming experience, but also includes pleasant surprises like implicit scalar-vector arithmetic and an integrated SQL-like query language. A few lines of Lil can go a long way. Decker provides a small collection of built-in interactive widgets for building interfaces, as well as a facility for defining new ones. Custom widgets and their definitions can be copied and pasted using the system clipboard, which also makes it possible to share them anywhere you can share or store text. Every deck is a toolkit of reusable parts that can be harvested and repurposed for another project. Decker is command-line friendly: when built from source, it comes with Lilt, a standalone Lil interpreter which can (among other things) read, write, manipulate, and even execute Decker documents \"headlessly\". Lilt has even fewer dependencies than Decker itself, so it can also be compiled as a cross-platform APE executable, ready for writing run-anywhere shell scripts. Would you believe there's a Lil interpreter that runs on POSIX AWK? Decks are stored in a line-oriented text format which interoperates well with existing source control tools like Git and SVN. Decker includes no advertising, telemetry, gamification, or other intrusions on user privacy and autonomy. If you like Decker, please share it with other people who might enjoy it. Build something that makes you happy. Examples Decker: A Guided Tour 5GUIs A CHIP-8 Interpreter All About Draggable All About Sound Sokoban: A Block-Pushing Puzzle Game Modules Plot: Simple Graphs for Decker Zazz: Animation Helpers for Decker Ease: Easing Functions for Decker Dialogizer: Visual-Novel Modals for Decker Puppeteer: Visual-Novel Sprite Animation for Decker Documentation The Decker reference manual The Decker document format The Lil programming language Learn Lil in 10 Minutes The Lil playground Lil Quick-reference card Lilt: the Lil Terminal Decker: Responding to Responses Additional Resources Browsable source code and a bug-tracker are available on GitHub. Decker is free and open-source, under a permissive MIT license. Periodic binary releases for MacOS and Windows are available on Itch.io. The Itch page includes a community forum for discussing Decker and sharing projects made with Decker. back",
    "commentLink": "https://news.ycombinator.com/item?id=40292181",
    "commentBody": "Decker: A reincarnation of HyperCard with 1-bit graphics (beyondloom.com)216 points by metadat 11 hours agohidepastfavorite55 comments kibwen 9 hours agoBeautiful! I realize there's a certain aesthetic you're going for here, and what I'm about to propose is a prime example of a slippery slope, but if you're willing to go juuuuust a little further from 1-bit graphics to 2-bit graphics, you might actually get legible photographs. Here's a website that I love that has a similar aesthetic; every image here has at most six colors (although each image has its own color palette): https://solar.lowtechmagazine.com/ reply RodgerTheGreat 9 hours agoparentDecker's UI is primarily black-and-white, but it actually uses a customizable 16-color palette. On the community forum[0] there's some discussion about dithering and importing colored photos. Users have taken advantage of color to make some really gorgeous projects[1]. [0]: https://itch.io/t/2668739/16-color-dithered [1]: https://crowmorbid.itch.io/desker-deckmonth reply treve 5 hours agoprevThere's so much love for HyperCard. I missed out on this, but it seems like it empowered people to make lots of small useful utilities. It somewhat seems similar in versatility to spreadsheets, but with a different utility. For the people that were around for this, I'm curious what kind of modern tools captures this feeling for you? I'm slightly younger, and I feel some nostalgic love for tools like Delphi/VB and Macromedia Flash. Imperfect tools, but they sparked creativity. Our tools got so much better, but we lost something along the way. reply surfingdino 4 hours agoparentFlash got close, but was too complex and expensive for the average user (HyperCard was initially free, but was not bundled with the Performa line of Macs). HTML captured a lot of what HyperCard offered, but web authoring tools never got as easy and still don't offer one feature that HyperCard did--consistency of the look and feel of the UI, which can be a good thing or a bad thing, but some users liked the fact that they could create a flipbook by adding two buttons (previous, next) to a page. Some even started businesses selling educational material bundled as HyperCard stacks. The HyperTalk programming language was not great though and was one of those languages that hippies liked but the average user was as lost with it as with C or Pascal. I always had a feeling that Apple tried to control what you could do with it too much, which is the opposite of what I expect a programming language and the tooling around it to do for me. If you want to get a feel of what it was like to code in it try coding something in AppleScript, you will see what I mean. reply vendiddy 2 hours agoparentprevI'm in a different generation as well. What was special about hypercard? I do remember VB and Macromedia tools and feel like today's tools are harder to use! reply kristopolous 2 hours agorootparenthttps://youtu.be/FzbHYl17x6U?si=0EbyyYdqeyU5f5qb&t=312 This Computer Chronicles episode from 1990 should help explain it. They did a number of episodes on it. Here's one with laserdiscs https://youtu.be/v9o5Ld8hpug?si=hBaB8MHBdMOKQsUE&t=979 I recommend binge watching old computer chronicles episodes if you're interested in startups and how software gets built, thrives and dies. If this is genuinely new to you, the guy with the beard created the CP/M operating system and had a tragic early death https://en.m.wikipedia.org/wiki/Gary_Kildall . The other guy is still around but seems to have fallen off posting online in the last few years. (https://twitter.com/cheifet/status/1642364464564510720 last update) Here are the episodes chronologically https://archive.org/details/computerchronicles?sort=date reply msephton 2 hours agorootparentprevIn a nutshell: HyperCard democratised software development, making it available and accessible to the masses, for free. In fact, many users were not even aware they were developing software. That's how easy and accessible it was. Early web browsers, like NCSA Mosaic, enabled people to change the page - you could edit and publish HTML right there in the same tool that you were using to view it! That was the closest we got, and it was taken away from us. reply tomcam 1 hour agoparentprevLivecode is a cross-platform commercial product that pretty much satisfies all the requirements of a HyperCard user but with 2024 tech and features reply divbzero 5 hours agoparentprevAlso, given the love, why is it that HyperCard died out and nothing similar took its place? reply dfabulich 3 hours agorootparentThere are a bunch of simple GUI builders, including GUI builders for the web, but none of them are popular, due to the sweet spot of supply and demand that Hypercard hit. When Hypercard launched, it came with every Mac, it was free, and there was nothing else like it available on the Mac. On the Mac, the alternative to Hypercard was to layout UI widgets in code, with no GUI builder at all, or eventually to pay $$$ for a professional-grade IDE like CodeWarrior. As an entry-level user with no budget, if you wanted a GUI builder for the Mac, you got Hypercard, or nothing. This created a community of Hypercard enthusiasts. Furthermore, when Hypercard launched, Macs had a standard screen resolution. Every Mac sold had a screen resolution of 512x342 pixels, so you could know for sure how your cards would look on any Mac. Supporting resizable GUIs is one of the hardest things to do in any GUI builder. (How should the buttons layout when the screen gets very small, like a phone? Or very wide, like a 16:9 monitor?) Today, Xcode uses a sophisticated constraint solver / theorem prover to allow developers to build resizable UIs in a GUI; it works pretty well, I think, but it's never going to be as easy to learn as \"drag the button onto the screen and it's going to look exactly like that everywhere.\" The last issue is the real killer for modern Hypercard wannabes: it's a small step from a web GUI builder to raw HTML/CSS. You don't have to pay big bucks to have access to professional-grade HTML, CSS, and JavaScript. Sure, they're not that easy to learn, but you can teach a kid to write interactive web pages, no problem. As a result, the demand for a simple GUI builder is lower than it was for Hypercard, and even when you do capture a user, they tend to outgrow your product, and there are a zillion competitors, so none of them can build a community with real traction. reply msephton 2 hours agorootparent> On the Mac, the alternative to Hypercard was to layout UI widgets in code, with no GUI builder at all Don't forget ResEdit, version 1.0 released December 1985. reply SeanLuke 2 hours agorootparentResEdit edited the resource fork, not code, right? reply msephton 2 hours agorootparentRight, but... the resource fork could contain code, as well as window definitions, graphics, icons, dialog boxes, menus, language translations, strings, and more. Of course, generally you'd write source code as text and hook up your interface. Resource definitions could be written as text or laid out using a GUI tool, of which ResEdit was just one. And everything would be compiled during development. But none of that was strictly necessary. Interestingly, early Palm OS worked exactly the same way - using resources. They took the concept and implemented their own version based on how the classic Mac system worked. So, you could fix a bug in an app using only a resource editor without having access to the source code. I've done this on both systems! reply ryani 1 hour agorootparentprevI think something like Unity or Gamemaker or even Scratch fills that niche now. They are a bit more game-focused than Hypercard (which was really more of an early iteration on the web), but, I think, capture a similar feeling of empowerment and creativity among nerdy young people as Hypercard did for nerdy young me. reply chrstphrknwtn 4 hours agorootparentprevI suppose Flash was the new HyperCard. Almost anyone could get an awful lot done without knowing much about anything. reply surfingdino 4 hours agorootparentAnd the results were quite awful... Steve Jobs couldn't wait to kill it. reply msephton 1 hour agorootparentThe results were fantastic, actually. The Flash era was a great time, and lots of important and influential software came out of it. The problem Steve Jobs had was that Flash was too resource/power hungry to run on the first iPhone, so his decision to disallow it was a defensive one. Interestingly, you could make iPhone apps with Adobe Air (a descendent of Flash) and such apps still run today! So there have more longevity and compatibility than apps written with the official Apple tools. Pickle's Book is one such app you might like to try out. Today's iPhones are capable of running Flash much better, and iOS is now a resource (CPU/RAM/battery) hog all by itself. So what was really achieved in the long run? reply pavlov 4 hours agorootparentprevApple’s App Store is filled with ugly apps that have poor performance. People will write that kind of software on anything that has enough programmability. So it’s not that Jobs killed Flash to save users from bad taste. It was because Flash was the most widely deployed cross-platform runtime of the desktop web era (at one time on 96% of computers!) and he didn’t want Adobe or anyone else to have that kind of power anymore. reply surfingdino 4 hours agorootparentTrue, Jobs did not want Adobe to have that kind of power, but I am also glad he killed it, Flash apps were fugly, heavy, and the runtime created security problems. reply beagle3 4 hours agorootparentBut the main reason, I think, was the flash runtime was an energy hog, and at the time (up until iPhone 4 or so), that meant using a Flash website on an iPhone would shorten battery life and put the blame on Apple. At the time, they vetoed many things for that reason. reply surfingdino 3 hours agorootparentYes, it was a problem. reply treve 5 hours agorootparentprevFor the first question, I found this: https://daringfireball.net/linked/2010/07/23/hypercard-3 reply metadat 10 hours agoprevFolks: Don't forget to try it out yourself! https://www.beyondloom.com/decker/tour.html This triggered amazingly sweet memories for me, am I alone? My only wish would be to have pinch-zooming on mobile. reply josephg 9 hours agoparentFor some reason that uses about 20% of my iphone screen. Its so tiny I can’t read the text, and they’ve disabled pinch zooming so I can’t get in closer. reply hoc 6 hours agorootparentYou can increase the zoom factor in Safari's address line, hitting that aA button and then the big A... That, combined with landscape mode seems to make it usable, at least on the Max models. reply metadat 9 hours agorootparentprevBuckle up and put on some glasses B-) Or, shudder, bookmark and visit from a desktop. Worth it. reply al_borland 9 hours agorootparentprevGoing into landscape helps. reply treve 5 hours agoparentprevCould be a firefox issue, or I don't know how it works but 'New Card' just gives me a blank screen and no way to write/draw. reply gnabgib 11 hours agoprevPreviously on HN [1] (191 points, 3 months ago, 36 comments) [0] (215 points, 2 years ago, 88 comments) [0]: https://news.ycombinator.com/item?id=33377964 [1]: https://news.ycombinator.com/item?id=38985409 reply time4tea 4 hours agoprevThere was a NASA laserdisc at school with loads of shuttle information. It was hooked up to a Mac running HyperCard, for navigation and displaying info. It was pretty cool, although I think at the time I didn't appreciate it as much. reply firecall 4 hours agoparentI remember things like that! Dont think I ever saw that one, but I'm sure there were some HyperCard Stacks floating around that were mini databases of cool things with images. Sort of an early Multimedia before the 90s Multimedia bubble :-) reply sircastor 9 hours agoprevWith respect to this project’s choices, I feel like HyperCards greatest flaw was failing to implement color when the Mac had begun to embrace it. By the time I was in high school, all the Macs in my school were color machines. But HyperCard never got true, native, built-color. reply thaumaturgy 9 hours agoparentThat's largely my recollection too. HyperCard was hugely popular, especially as an introduction to programming for kids, but the Quadra Macs and their kin introduced color and everyone wanted color. HyperCard never got a native update for it and it was the number one feature everyone wanted. There was a related product (SuperCard?) that offered color. I remember trying it but it was clunky. I don't remember why, but in any case, it never took off. There was also the matter of HyperTalk's limited API. There was a market for ... extensions, I think they were called? Some developers really dominated that space for a while. (I wonder whatever happened to J5erson? He was brilliant.) Extensions were commonly written in Pascal and could add a lot of capability to HyperTalk, and were a great stepping-stone for people that wanted to explore programming beyond HyperCard. Decker looks like it has a lot of the good things from HyperCard -- simplicity, portability. Kind of a shame it isn't using something closer to HyperTalk though. It really was a nice language, for what it was. reply gcanyon 5 hours agorootparentSuperCard still exists! https://supercard.us/ That said, it's stuck in 32-bits-land, so it must be in significant decline. reply karmelapple 7 hours agorootparentprev> There was a market for ... extensions, I think they were called? XCMDs and XFCNs - see https://en.wikipedia.org/wiki/HyperTalk#Extending_HyperTalk reply sircastor 7 hours agorootparentprevOne of my early computing regrets is not focusing enough to learn HyperTalk. I was aware of it, but my attention and focus were just a little too wild to allow me to explore it properly. I made lots of stacks that were basic games, but I struggled with sticking with it long enough to finish any. I think if I’d understood what was possible with HyperTalk I could’ve gotten into organizing programming a lot sooner. reply msephton 1 hour agorootparentStruggling to stick with it long enough to finish is a general programming problem, not specific to HyperCard. :) reply surfingdino 4 hours agorootparentprevYou didn't really miss on much. If you want to relive the experience, try to code something in AppleScript. reply firecall 3 hours agorootparentprevIndeed, Supercard was the Hypercard option with colour! And amazingly seems to still be around! https://supercard.us/ reply msephton 7 hours agoprevDecker is superficially similar to HyperCard because it's primarily 1-bit, but it's different enough to require a totally new learning curve. It's quite difficult to use, requires learning \"Lil\" a new and unique language, and is missing many HyperCard/MacPaint affordances you might be looking for if you're expecting HyperCard. The upshot is that HyperCard artists can't switch and continue to use HyperCard in a browser thanks to Infinite Mac. So close, yet so far. Similar, but different. reply gcanyon 5 hours agoparentIf the language is the most important thing for you, https://livecode.com/ has a very HyperTalk-like language and runs on modern hardware. reply msephton 2 hours agorootparentIt's not. There's isn't a single most important thing. HyperCard was an almost perfect blend of many things. That's why it's easy to spot when they're missing, like some omissions or differences in Decker. reply rapnie 3 hours agorootparentprev\"Sign up for your demo\" .. missed opportunity. reply msephton 2 hours agorootparentThey've done that for years, as well as some very dark patterns, and perplexing dishonest marketing. I tend to avoid them. reply hyperhello 5 hours agoparentprevIt's not everything it was, but HyperCard in a browser is available at https://hypercardsimulator.com reply msephton 2 hours agorootparentWith a browser I can use the original HyperCard via Infinite Mac website. I mention this in my comment. It's insanely great. reply 7thaccount 4 hours agoprevI have no idea why apple or Microsoft don't spend more effort making something like this for their OS for casual developers. VB and Python are fine, but sometimes I want something that is really tailor made for building small applications. reply firecall 3 hours agoparentWell... Apple do have Swift Playgrounds. And SwiftUI is about as easy as it gets! Although using Xcode and setting yourself up in the App Store still requires some understanding of arcane settings and Xcode IDE familiarity :-) reply msephton 2 hours agorootparentSwiftUI is a long way behind drag and drop user interface layout. But today there are different problems to solve, like dynamic screen sizes, which it make relatively easy. reply metadat 11 hours agoprevCredit: @smartmic https://news.ycombinator.com/item?id=40289036 reply anonzzzies 4 hours agoprevNote that the author also made a k interpreter [0] which has a graphical env to play around with as well. Lil takes a lot from it. [0] https://github.com/JohnEarnest/ok reply KingOfCoders 5 hours agoprev [–] This would be a great tool for coding interviews, Any idea how to share the HTML in realtime? reply msephton 2 hours agoparent [–] You'd expect people to code in a language they likely have never seen before in an interview? Would you provide the documentation? reply KingOfCoders 2 hours agorootparent [–] People have different opinions, and all can be fine, I don't claim to have the definite answer. IMHO coding is about thinking, not about the knowledge of a programming language. How do you split the problem into smaller problems? And then in even smaller ones to the level you can express solutons in code? Then what can be abstracted for future changes? I would not provide documentation, but explain how the candidate can write some code if they tell me what they want to do, perhaps write the code sketch for them. The language is so simple that any candidate can get the basics in a few minutes. And I'm interested in their thinking, not in their mastery of a programming language. If you're a good Python coder, I'd also hire you for writing Go. (To understand where I came from: This is perhaps from my personal background, I have done hundreds of interviews, I wrote code in 20+ languages and was paid for in 10+ languages over the last four decades. So I value thinking and problem solving more than the correct syntax in an interview - e.g. if you write Java code, I don't care about braces and semicolons) reply msephton 1 hour agorootparent [–] I agree coding is about thinking. I've worked for Apple, and written a GOTY game, but I don't know how I'd react if asked to write code in such an esoteric system. I could talk through the concepts, and you could write the code. Cheers! reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Decker is a multimedia platform influenced by HyperCard and classic MacOS, allowing users to generate interactive documents like E-Zines, presentations, and games with a \"ditherpunk\" look.",
      "Users can save their creations as standalone .html files, benefit from quality-of-life enhancements, a user-friendly scripting language called Lil, interactive widgets, and the option for custom widget development.",
      "Decker is privacy-focused, open-source under the MIT license, command-line compatible, and provides resources like the Lil programming language, reference manual, and a community forum on Itch.io."
    ],
    "commentSummary": [
      "The discussion delves into the accessibility and user-friendliness of early software development tools like HyperCard when compared to modern ones.",
      "Apple's choice to ban Flash on the iPhone due to performance concerns is examined, sparking a debate on alternatives such as Livecode and SuperCard.",
      "The conversation also touches on the shift from HyperTalk to contemporary programming environments, emphasizing problem-solving abilities in interviews over proficiency in programming languages, with the introduction of Lil as a straightforward coding language for interviews."
    ],
    "points": 216,
    "commentCount": 55,
    "retryCount": 0,
    "time": 1715120105
  },
  {
    "id": 40286959,
    "title": "Zed: High-Performance App Development on Linux with GPUI",
    "originLink": "https://zed.dev/blog/zed-decoded-linux-when",
    "originBody": "Zed Decoded: Linux when? Thorsten Ball Mikayla Maki May 7th, 2024 Take a look at this: Screenshot of Zed — but where are the red/yellow/green window controls? Does anything stick out? Yes, exactly, it's a screenshot of Zed running on Linux! Wait, what? Zed on Linux? Is it released yet? No, it's not, but it's taking shape, fast. At the end of January we open-sourced Zed and had zero Linux support. Now, three months later, you can compile & run Zed on Linux and actually use it. And I mean really use it — I've worked in Zed (on Zed!) the whole last week without any problems. The words \"alpha release\" seem to appear at the end of the tunnel. We're getting closer. Today we're going to talk about how Zed on Linux took shape, what the challenges were, who did the work, and what's still left to do. Companion Video: Linux when? This post comes with a 1hr companion video, in which Thorsten and Mikayla explore Zed on Linux, dig through the codebase to see how it's implemented, talk about implementation challenges, and how the open-source community helped out big time. Watch the video here: https://youtu.be/O5XVVnA2LoY Why not Linux from the start? Or: the Tricky Thing About Platforms Why didn't Zed work on Linux out of the box? That's not an unreasonable thing to ask and many of you did. After all, Zed's written in Rust, a language that's known for its cross-platform support. So if Rust programs can run on macOS, Linux, and Windows, why didn't Zed? The tricky thing about cross-platform support is that — in general and in Rust — it only works as long as you're fine with the platform being abstracted away, hidden behind an API that is the same on every platform. At Zed, though, we want to use each platform as best as we can to build a high-performance application that is and feels native to the platform. That often means talking directly to the platform, in order to use it to the best of its abilities. On macOS, for example, Zed makes direct use of Metal. We have our own shaders, our own renderer, and we put a lot of effort into understanding macOS APIs to get to 120FPS. Zed on macOS is also a fully-native AppKit NSApplication and we integrated our async Rust runtime with macOS' native application runtime. If you want your application to have this level of depth and control over its platform integration and have it be cross-platform, what you'll need to build is a framework. A framework that allows you to talk directly to the platform whenever you need, but otherwise abstract it away from you so you don't have to worry about it when you write application-level code. That's what Zed did. The framework is called GPUI and when we released it as open-source, along with Zed, it came with cross-platform support. Except not really. GPUI's cross platform support GPUI does abstract away the underlying platform and assumes there's more in the world than macOS. Here are parts of GPUI's Platform trait, which is all a GPUI application has to interact with: // crates/gpui/src/platform.rs trait Platform: 'static { // [... some methods left out to keep this short ...] fn background_executor(&self) -> BackgroundExecutor; fn foreground_executor(&self) -> ForegroundExecutor; fn text_system(&self) -> Arc; fn run(&self, on_finish_launching: Box); fn quit(&self); fn restart(&self); fn hide(&self); fn displays(&self) -> Vec>; fn open_window( &self, handle: AnyWindowHandle, options: WindowParams, ) -> Box; fn window_appearance(&self) -> WindowAppearance; fn open_url(&self, url: &str); fn on_open_urls(&self, callback: Box)>); fn register_url_scheme(&self, url: &str) -> Task>; fn prompt_for_paths(&self, options: PathPromptOptions, ) -> oneshot::Receiver>>; fn prompt_for_new_path(&self, directory: &Path) -> oneshot::Receiver>; fn reveal_path(&self, path: &Path); // [...] fn local_timezone(&self) -> UtcOffset; fn set_cursor_style(&self, style: CursorStyle); fn write_to_clipboard(&self, item: ClipboardItem); fn read_from_clipboard(&self) -> Option; fn write_credentials(&self, url: &str, username: &str, password: &[u8]) -> Task>; fn read_credentials(&self, url: &str) -> Task)>>>; fn delete_credentials(&self, url: &str) -> Task>; } This Platform offers nearly everything an application might want to do: execute work on different threads, start/stop the application, manage windows, open URLs, open system dialogues to open files and directories, cursor style, clipboard, credentials, ... and a few more things that I left out to keep this succinct. From the start, GPUI has had this Platform abstraction built-in. GPUI never was macOS-only. It was always platform-agnostic, as long as the Platform trait is implemented for the platform. And that's essentially what it meant to get Zed running on Linux: implement Platform for Linux. So, just implemented a bunch of methods and then Zed works on any platform? Well... yes, kind of. Here's what the Linux Platform implementation looked like in the middle of February. Again, in excerpts: // crates/gpui/src/platform/linux/platform.rs @ 266988adea impl Platform for LinuxPlatform { // [... some methods left out to keep this short ...] fn background_executor(&self) -> BackgroundExecutor { self.inner.background_executor.clone() } fn foreground_executor(&self) -> ForegroundExecutor { self.inner.foreground_executor.clone() } fn text_system(&self) -> Arc { self.inner.text_system.clone() } fn run(&self, on_finish_launching: Box) { self.client.run(on_finish_launching) } fn quit(&self) { self.inner.state.lock().quit_requested = true; } //todo!(linux) fn restart(&self) {} //todo!(linux) fn hide(&self) {} //todo!(linux) fn unhide_other_apps(&self) {} fn prompt_for_new_path(&self, directory: &Path) -> oneshot::Receiver> { unimplemented!() } fn reveal_path(&self, path: &Path) { unimplemented!() } //todo!(linux) fn write_to_clipboard(&self, item: ClipboardItem) {} //todo!(linux) fn read_from_clipboard(&self) -> Option { None } } Some methods implemented, some marked with todo!, others panicking with an unimplemented!(). Back in February this file had 11 todo! comments and 9 calls to unimplemented. Now it has 9 todo!s (different ones) and the word unimplemented in a string. So, yes, the work was getting rid of the todo!s and implementing the missing methods. But we all know: not all methods are created equal. While a write_to_clipboard method seems relatively straightforward to implement, things weren't as easy as the lower-case todo! and its defiantly cute exclamation mark might suggest. Which Linux are you talking about? A big challenge when building a GUI application for Linux is that there is no such thing as Linux, really. Linux is a kernel and when you install and run it, you're mostly likely doing that through a Linux distribution that gives you the rest of the operating system too: Ubuntu, Debian, CentOS, Arch, Gentoo, and so on. They nearly all differ in some aspects that are relevant for an application developer. Example: package management. Distributions not only have their own format with which to distribute applications, but they also have different ways of managing dependencies. And there's no standard across distributions (but of course there are competing standards). So far, with Zed, we've avoided the whole packaging topic and just focused on building .tar.gz archives that others can then turn into distribution-specific packages. But just scrolling through Tailscale's Packages site tells us that there's a lot of work to do that we never had to do for macOS. But even if you focus on only one distribution, say Ubuntu, you still have to decide: do you support X11 or Wayland? Both are — ignoring some technical details (don't send me angry letters) — display servers with which Linux software can draw things on the screen. X11 has been around for a long, long time and Wayland has been trying to replace it, also for a long, long time. That, in turn, makes the question a trick question: you have to support both. They're both widely used. Still used in the case of X11 and gaining users in the case of Wayland, but it's not realistic that 100% of Linux users will be on Wayland anytime soon. After deciding to support Ubuntu and X11 and Wayland, the next question is: which desktop environment or window manager are you going to support? KDE or GNOME? Qt or GTK? What about tiling window managers and users that want to turn window decorations off? Which audio server are you going to support to get audio calls working? PipeWire? Or still PulseAudio? Do you even have to worry about that or is that choice dictated by the desktop environment you chose? I could probably go on and find more \"actually, there's more than one of X\" to list, but you get the point: when building a graphical application to run on Linux, you have to make quite a few technical decisions about which platform combinations to target and how. And we haven't even touched on the most fundamental of all of these decisions yet: how do we render Zed? From Metal to... what? Even if you knew how to package your application, and how to target X11 and/or Wayland, and managed to open an application window in them — how do you draw your application in that window? You might have missed it, but in the Platform trait above, there's a method to open a new window: // crates/gpui/src/platform.rs trait Platform: 'static { // [...] fn open_window( &self, handle: AnyWindowHandle, options: WindowParams, ) -> Box; } In order to implement that, you also have to implement the interface of the thing it returns: PlatformWindow. And that packs a punch. As of right now, PlatformWindow requires 37 methods to be implemented. The methods range from hooks (on_close, on_fullscreen) to attributes (is_fullscreen, is_minimized) to bread-and-butter (content_size, mouse_position, toggle_fullscreen) to tricky (scale_factor, appearance). But there's one method on PlatformWindow that's the most fundamental of them all: draw. Another name for it could be where_the_rubber_hits_the_road. Here is its implementation for macOS: // crates/gpui/src/platform/mac/window.rs impl PlatformWindow for MacWindow { // [...] fn draw(&self, scene: &Scene) { let mut this = self.0.lock(); this.renderer.draw(scene); } } It doesn't look imposing, but it is: Scene is the result of GPUI rendering a frame and this method, draw, is what makes it appear on the screen. A Scene is the visible parts of the application — text, windows, borders, rectangles, underlines, and so on — turned into data, primitives. Here's the definition of Scene: // crates/gpui/src/scene.rs, simplified struct Scene { shadows: Vec, quads: Vec, paths: Vec>, underlines: Vec, monochrome_sprites: Vec, polychrome_sprites: Vec, surfaces: Vec, paint_operations: Vec, primitive_bounds: BoundsTree, layer_stack: Vec, } Shadows, quads, paths, underlines, sprites — that's what GPUI boils your application down to when a frame is rendered. And then, in draw, it hands that data over to the renderer. And the renderer — that's your GPU. As I mentioned above: on macOS we use the Metal APIs to talk to the GPU and when you follow the definitions there, you'll end up at a fn draw, implemented on MetalRenderer, that contains nearly the complete saga as told in our 120FPS blog post: setting up a rendering pipeline, triple-buffering, syncing up with the OS and the display, all of that. So how do we implement draw on Linux, without Metal? How do we talk to the GPU on Linux? Enter stage: the open-source hackers Less than two weeks after Zed going open-source, flying through the cloud of pull requests and issues that's been stirred up by all the excitement comes @kvark (Dzmitry Malyshau) with a by now legendary PR that makes Zed render on Linux. In less than four thousand lines of code. Sure, it didn't render all of Zed yet, but: wow. Screenshot of kvark's pull request that made Zed render on Linux The PR answers the question of how to talk to the GPU on Linux: with blade, kvark's \"rendering solution for Rust\", that offers a \"lean low-level GPU abstraction focused at ergonomics and fun\". Blade uses Vulkan under the hood, a graphics API that's similar to Metal in the level of abstraction it offers, to talk to the GPU and render Zed. After @kvark opened the PR, Mikayla, Antonio, and Nathan reached out to him to talk about the feasibility of using Blade in Zed and shared goals. It didn't take long to get to a shared understanding and, exactly two weeks after open-sourcing Zed, the PR that made it compile and run on Linux was merged. With that big TODO and question out of the way, the community jumped on the chance to remove the remaining todo!s: @witelokk added support for Wayland, @kvark continued his work on getting all of Zed to render smoothly, @romgrk added support for file dialogues, @apricobucket28 fixed scrolling and selections and many others contributed and fixed a lot of other things, with Mikayla diligently reviewing, coding, managing, leading and keeping an eye on everything. This list of contributions is incomplete and I'm sure I should've mentioned more people, but the point I want to make is this: Zed on Linux was and is an impressive open-source team effort that surprised all of us at Zed with how fast it got Zed to a working state. Zed on Linux: the abstractions Let's take a look at how it works, how X11 and Wayland and Blade are tucked away under the Platform abstraction. Here's an excerpt: // crates/gpui/src/platform/linux/platform.rs impl Platform for P { fn background_executor(&self) -> BackgroundExecutor { self.with_common(|common| common.background_executor.clone()) } fn foreground_executor(&self) -> ForegroundExecutor { self.with_common(|common| common.foreground_executor.clone()) } fn text_system(&self) -> Arc { self.with_common(|common| common.text_system.clone()) } fn run(&self, on_finish_launching: Box) { on_finish_launching(); LinuxClient::run(self); self.with_common(|common| { if let Some(mut fun) = common.callbacks.quit.take() { fun(); } }); } fn quit(&self) { self.with_common(|common| common.signal.stop()); } // [...] } Platform is implemented on LinuxClient, which itself is a trait too: // crates/gpui/src/platform/linux/platform.rs trait LinuxClient { fn with_common(&self, f: impl FnOnce(&mut LinuxCommon) -> R) -> R; fn displays(&self) -> Vec>; fn primary_display(&self) -> Option>; fn display(&self, id: DisplayId) -> Option>; fn open_window( &self, handle: AnyWindowHandle, options: WindowParams, ) -> Box; fn set_cursor_style(&self, style: CursorStyle); fn write_to_primary(&self, item: ClipboardItem); fn write_to_clipboard(&self, item: ClipboardItem); fn read_from_primary(&self) -> Option; fn read_from_clipboard(&self) -> Option; fn run(&self); } And there are two implementations of LinuxClient: one for Wayland one for X11 Both of them have an implementation of PlatformWindow too: here's the Wayland PlatformWindow and here's the X11 PlatformWindow But both of them use Blade in their draw method: // crates/gpui/src/platform/linux/x11/window.rs struct X11WindowState { // [... other fields ...] renderer: BladeRenderer } impl PlatformWindow for X11Window { // [...] fn draw(&self, scene: &Scene) { let mut inner = self.0.state.borrow_mut(); inner.renderer.draw(scene); } } // crates/gpui/src/platform/linux/wayland/window.rs struct WaylandWindowState { // [... other fields ...] renderer: BladeRenderer, } impl PlatformWindow for WaylandWindow { fn draw(&self, scene: &Scene) { let mut state = self.borrow_mut(); state.renderer.draw(scene); } } That means the rubber hits the road through these layers of abstractions: GPUI asks the Platform to open a window The Wayland LinuxClient uses Wayland to open a window, the X11 client uses X11 When rendering a frame, GPUI turns the application into a Scene It then calls the PlatformWindow to draw that Scene Both the Wayland and the X11 implementations use Blade, which uses Vulkan, to talk to the GPU, to draw the Scene That's obviously only scratching the surface. There are a lot more interesting things going in the Linux implementation — did you know that there's multiple clipboards on Linux but only one on macOS? Or here, take a look at this: Screenshot of Zed on Linux, including a native file dialog Yup, that's a native file dialog, but we don't use GTK in Zed and that clearly is GTK — so, how does that work? You'll find the answer in the companion video in which Mikayla and I dive deeper into this and also touch on the question of whether or not we should use GTK or Qt or something else (spoiler: it's complicated). So, Linux when? So what's left to do for Linux? In order to get to an alpha release: not that much, but don't quote me on that. Essentially: fix 86 remaining todo!s of various difficulty levels, get window resizing/moving to work on Wayland, and implement system dialogues for GPUI. We're close, very close. After the alpha, we'll need to add support for audio calls, drag & drop, storing of credentials, make sure the performance is consistently high, increase stability, and so on. Take a look at the Linux Roadmap tracking issue. Even though there might still be a lot to do (I don't even want to know how complicated drag & drop can be on Linux— I mean, GNOM— I mean, KDE, no, I mea—) and a lot of unknown unknowns and surprises along the way, one thing's for sure: the fact that we got to the current state of Zed on Linux in three months, with that many high-quality open-source contributions, is pretty amazing. Want to try out Zed on Linux? You need Rust, some dependencies, and depending on your patience enough CPU and memory to compile it in a reasonable amount of time. Take a look at these instructions. Have fun! Looking for a better editor? You can try Zed today on macOS. Download now! We are hiring! If you're passionate about the topics we cover on our blog, please consider joining our team to help us ship the future of software development.",
    "commentLink": "https://news.ycombinator.com/item?id=40286959",
    "commentBody": "Linux When? (zed.dev)214 points by todsacerdoti 18 hours agohidepastfavorite143 comments andy_xor_andrew 16 hours ago> On macOS, for example, Zed makes direct use of Metal. We have our own shaders, our own renderer, and we put a lot of effort into understanding macOS APIs to get to 120FPS. So they are taking the exact opposite approach of Electron (VS Code). In my mind, if you're someone who types in HN comments in a rage because your text editor (VS Code) eats up 200+ MB ram, you don't get to cry about Zed not being supported on Linux from day one, because you can't have your cake and eat it too - if you want it on your platform, you gotta wait for the shaders to be written. reply NoraCodes 15 hours agoparent> In my mind, if you're someone who types in HN comments in a rage because your text editor (VS Code) eats up 200+ MB ram, you don't get to cry about Zed not being supported on Linux from day one, because you can't have your cake and eat it too - if you want it on your platform, you gotta wait for the shaders to be written. This is certainly one takeaway. On the other hand, this very blog post points out that a community member, Dzmitry Malyshau, did the work to get the program working on the OS he uses. Malyshau does not work for Zed, which is a for-profit company; as far as I can see, he gets nothing out of working on Zed, except that he and other Linux users get to use it. Perhaps, rather than characterizing Linux users as whiny, we could take away the idea that many Linux users are willing to put in quite a lot of work to make things better for each other. reply justinclift 2 hours agorootparent> Perhaps, rather than characterizing Linux users as whiny, we could take away the idea that many Linux users are willing to put in quite a lot of work to make things better for each other. That's more an \"Open Source\" thing rather than Linux specific. A personal case in point was when I got Mellanox adapter support officially added to FreeNAS (now TrueNAS) because that's the adapter brand I had and needed it to work. Not a super huge effort as the FreeBSD driver worked (so just needed porting), but the integration process and follow up testing/advocacy/etc work wasn't exactly trivial either. reply weinzierl 14 hours agorootparentprev\"Perhaps, rather than characterizing Linux users as whiny, we could take away the idea that many Linux users are willing to put in quite a lot of work to make things better for each other.\" ... and that even if it means signing away their rights in a CLA to a for profit company. This is on another level than contributing to the Linux kernel that is GPL2 plus practically not relicensable because of the multitude of copyright holders. reply jonhohle 16 hours agoparentprevOr… use one of the existing cross platform toolkits that have existed for decades. Text Editors, traditionally, did not require shaders to run or be performant, and not require entire systems worth of RAM. reply jamil7 15 hours agorootparentCross platform toolkits, more than other software components incur massive tradeoffs. They’ve written one themselves, tailored to their needs and open sourced it along the way. I guess I don’t see the problem here. If one that’s existed for decades fits your needs better then use that. reply jauntywundrkind 14 hours agorootparentWgpu seems very very well loved & supported, is one of the most successful comings together of the graphics world in ages. I'd love to hear some actual critique of it, hear what people think are shortcomings, because it feels to an outsider like this is the fantasy land, that we're living in the better place now. https://github.com/gfx-rs/wgpu reply littlestymaar 11 hours agorootparentWhat I find interesting is that kvark, the open source contributor that made Linux port possible was the main developer on wgpu at Mozilla, yet he decided to build an alternative [1] to wgpu that he used for zed. I wonder what's the rational for that. [1]: https://github.com/kvark/blade reply jauntywundrkind 10 hours agorootparentDzmitry gave a talk at a rust gamedev meetup on Blade. This ain't my turf so apologies for inaccuracies here. It appears to be a fairly novel attempt to write a graphics library with a semi conventional looking pipeline, that under the hood ends up eskewing a lot of the Vulkan concepts. Instead of per object contexts, it uses global contexts to do most work. Instead of taking resources and binding them into descriptor sets to use across pipelines (tracking state), Blade kind of recreates resources on the fly, lets them get used, and disposed of them. Kind of interesting philosophy of a complex binding model in Vulkan/WebGPU vs a more direct diy render model? https://www.youtube.com/live/63dnzjw4azI?si=KzLPm-gBX0gDKq7H... Zed tool this work as an outside contribution. Maybe that someone did the work was good enough. I'm not sure what would make Blade a better match or not, vs wgpu-hal. reply auggierose 14 hours agorootparentprevWell, it's Rust. I am using TypeScript. I guess I am sticking with Electron. Maybe WebGPU can help me out. reply MR4D 15 hours agorootparentprevOut of curiosity, how many of them run at 120 FPS? (honest question - I've never looked into it myself) reply adastra22 15 hours agorootparentThe question doesn’t make sense. They don’t rerender the buffer every frame. I assume zed isn’t doing that either as it would be horribly inefficient. I presume what is meant is that it can handle a redraw fast enough to be in the next frame. In which case the answer is: all of them. Drawing text is not the bottleneck for a GUI program, unless you have a god awful browser stack as your rendering engine. reply LoganDark 14 hours agorootparent> I assume zed isn’t doing that either as it would be horribly inefficient. What are you supposed to do instead? Zed uses the GPU. It's not making calls to retained-mode widgets to individually reposition them, nor is it blitting into a buffer using the CPU. It's using the GPU which eats pixels for breakfast. You've been able to rerender the entire screen each frame for over a decade - just look at Windows 7 Aero, which ran on the laptops of 2009 for the exact same reason: it used the GPU! reply zozbot234 13 hours agorootparentRerendering each frame is wasteful because it keeps hardware from reaching deeper power-saving states. This includes the CPU, GPU and even the display, due to technologies such as FreeSync. On modern hardware, even removing the blinking cursor has been found to save quite a bit of power, by eliminating needless screen redraws. reply justinclift 2 hours agorootparent> ... even removing the blinking cursor has been found to save quite a bit of power, by eliminating needless screen redraws. Hadn't heard of that before, but it makes sense now you mention it. :) reply Rusky 13 hours agorootparentprevNeither Windows Aero nor Zed renders every single frame, 120 times per second. The parent comment is correct is correct that the important thing is to be able to render any given frame in 1/120th of a second, but to leave things alone when nothing is changing. reply adastra22 9 hours agorootparentprevYou render into a texture once, and then just send that to the compositor each frame as needed. reply forgotpwd16 15 hours agorootparentprevThis is the first time I see FPS mentioned for a text editor. Is this something that matters or just an pointless metric utilized as selling point? reply freqmod 15 hours agorootparentFps in itself is not important, but it is a substitute for input latency, and if your keystrokes start lagging it feels sluggish. At least historically electron based editors (like atom) has been feeling significantly more sluggish than sublime text or vim with a decent terminal emulator. reply forgotpwd16 15 hours agorootparentFor terminal emulators comparisons at least the metrics used are latency and throughput. Now those plus times to do operations (load file, search & replace, etc) wouldn't surprise me to be the comparison metrics for text editors. FPS though feels weird. reply SomeRndName11 15 hours agorootparentprevWell it may have effect on ergonomics. Some people claim they cannot use 60Hz monitor after trying 144hz. reply NekkoDroid 15 hours agorootparentWell, that is usually refering to some form of \"simulated fluid motion\", not new characters appearing and disappearing. The only case where that kinda fluid motion would matter is when you have text scrolling by at semi-fast speeds. reply ffsm8 14 hours agorootparentI count myself among the people that would consider >60hz necessary. For me, it's animations, especially if I'm dragging a window or just the mouse. On 60hz it's nauseating if I'm paying too much attention to the window I'm moving. It's goes completely away around 90-100 Hz (at least for me) reply NekkoDroid 10 hours agorootparentI'd still say that draggin a window would count as a \"simulated fluid motion\". Maybe not something you'd immediatly think of, but its still trying to convey the sense of motion. Just text appearing and dissappearing isn't something I personally could categorize ass the same type of animation. And I also count myself as someone who considers 120 at least necessary (on the primary monitor) reply LoganDark 14 hours agorootparentprevI've had the privilege of using a merely 90Hz display and the difference is still incredible. It gives me input feedback much faster so my brain does not have to do as much buffering/prediction, everything feels a lot more direct. One would think a measly 5 milliseconds wouldn't amount to much, but for input feedback it absolutely does. I do not suffer from nausea or motion sickness of any kind arising from computers or visuals in general, but I can still easily tell the difference between a 60Hz and 90Hz display. A few months ago I had the privilege of checking out the 120Hz displays on the new MacBooks and they're amazing. reply sunshowers 15 hours agorootparentprevAs far as I can tell, graphics pipelines often use frame buffering. The quicker the next frame can be drawn, the faster it feels. reply adastra22 9 hours agorootparentGUI apps like text editors don’t redraw frames every update cycle. reply jethro_tell 15 hours agorootparentprevHonest question. What could you possibly do with a text editor at 120 fps that you can't do at 15 or 30? I can't think or type that fast and I can't read scroll back that fast either so I can't wrap my head around needing something like that. reply teo_zero 14 hours agorootparentSmoother scrolling? reply jethro_tell 8 hours agorootparentI mean, yeah that's pretty smooth, I'm not even sure I could notice it past 15/30. reply ajross 15 hours agorootparentprev> Honest question. What could you possibly do with a text editor at 120 fps that you can't do at 15 or 30? More-honest-than-it-should-be answer: sell a product to Apple-ecosystem developers trying desperately to find something to justify the $3k they want to spend on a new MBP. (Typing this very comment in emacs running out of the Linux VM on a mid-range chromebook attached to a 30 Hz 4k television, btw. Come at me, as it were.) reply jethro_tell 8 hours agorootparentHeh, rocking a self built tmux/vim setup her myself, but even still I don't think I'm so cynical to think the only reason is for the marketing speak. I assume they have a reason, I just can't guess whatnot is. reply ajross 8 hours agorootparentI'm... absolutely that cynical. 120 Hz displays and the horsepower to drive them are the golden boutique speaker wire of the Gen Z tech set. reply Dylan16807 8 hours agorootparentOh come on. Fancy wires do nothing. 120Hz makes motion much smoother. It also reduces latency. Those make a big difference in many video games, or even just moving my mouse around and having it not skip two inches at a time. Your cynicism over 120Hz should match your cynicism over 4k. reply jethro_tell 4 hours agorootparentThat, doesn't make any sense 4k allows monitors to push past 24 inches, though, on a 24 or less 2k is plenty for me, but the generation of 22 inch 1080p monitors was rough on the eyes. reply Dylan16807 4 hours agorootparentIt makes plenty of sense. They're both quite useful but you can do without them. I'm not saying to be highly cynical, I'm saying to be equally cynical. You can change either side to reach equality. reply whazor 14 hours agorootparentprevChrome supports 120Hz+ https://www.testufo.com/browser.html It also seems like you can configure VSCode to go faster: https://stackoverflow.com/questions/52230196/vs-code-is-visi... But honestly, 'responsiveness' should come from not blocking the main/UI threads, and rendering in the right order of importance. reply Iwan-Zotow 12 hours agorootparentprev> Text Editors, traditionally, .. and not require entire systems worth of RAM. Emacs is looking at you reply phendrenad2 15 hours agorootparentprevIf you want a shaderless text editor, just use NotePad. But the world has moved on and people like their text to look nice. reply hvis 15 hours agoparentprevMoltenVK is also a thing. Whatever small translation overhead it incurs is probably not that important for a text editor. And then you get a cross-platform API: not just Linux, but Windows as well. Maybe also other more niche OSes as well. reply skohan 15 hours agorootparentMolten VK is amazing. When I started working with it, I was expecting a lot of caveats and compromises, but it's shockingly similar to just using Vulkan that you can easily forget that there's a compatibility tool in play. Probably you can squeeze a bit of optimization out of using Metal directly, but I think it's a more than viable approach to start with Vulkan/MoltenVK as a target, and add a Metal branch to the renderer when capacity allows (although you might never feel the need) reply duxup 16 hours agoparentprevSomeone has to pay for it, time, money, wait for it whatever. I love working with technical people who ask why and think about more optimal paths. On the other hand I'll listen to the same folks complain about some random app being stinky (I don't disagree) and wonder \"Yeah but you going to pay more for that burrito so that company can hire folks to write it natively on every platform?\" No you're not ... I know our customers at times aren't willing to pay / wait for the optimal path, and their customers aren't, so I get it. reply nemomarx 16 hours agoparentprevI suppose ideally you'd want it to be written on Linux first, if you're that kinda person - but the market is probably on Mac? reply m463 15 hours agorootparentI remember reading a long time ago (think pre-intel macs) that game developers never developed for macs. However if they did, the games would sell because the mac people wanted games and there were so few available they would buy anything. problems of a closed ecosystem. reply wmf 15 hours agorootparentBecause Macs are expensive (in absolute terms) Mac users are a self-selected group who are willing to spend money. Linux users have high standards and are not willing to spend money. For a text editor in particular Linux is going to be the toughest market because it's already pretty saturated. reply pjmlp 3 hours agorootparentprevHow do you think iDevices games are developed? reply thfuran 15 hours agorootparentprevCirca 2000, a lot of big name games actually were released for Mac. Heroes III, StarCraft, and Diablo II, for example, all got Mac releases. reply from-nibly 10 hours agorootparentprevDoing it on Linux would make people ask \"why isnt it on my distro\" a LOT sooner. Thus adding complexity while you are trying to prove the concept. reply cess11 14 hours agoparentprevOver the last few years I've had two applications that tended to rot after a week or two running, one is Firefox, which I still use for political reasons, and VS Code, the only Electron thing I've used for more than a few hours. The other being MICROS~1 Teams, which doesn't play nice with my window manager, tries to force me into identifying myself to join some video chat for a bit, and prefers to hang rather than shut down when asked nicely. Instead I join chats without video support. This is my problem with Electron applications. I'd be fine with them gobbling up a few GB if they were stable. reply zamalek 13 hours agoparentprev> In my mind, if you're someone who types in HN comments in a rage because your text editor WebGPU exists. It works with Metal. Vulkan could have also worked and MoltenVK would have bridged it to Apple. No, this is just like every other project that only works on MacOS: a mentality I really can't comprehend or explain. reply pjmlp 3 hours agorootparentDeveloped for Web browsers, targeting managed languages, and hardware specifications from 2015. reply Arnavion 16 hours agoprev>Yup, that's a native file dialog, but we don't use GTK in Zed and that clearly is GTK — so, how does that work? You'll find the answer in the companion video in which Mikayla and I dive deeper into this and also touch on the question of whether or not we should use GTK or Qt or something else (spoiler: it's complicated). Unless I missed it, the video doesn't actually answer the question. They talk 38:50 onwards about how they don't want to pull in GTK / Qt as dependencies, rather they just want to be able to read the toolkits' config and render a mimicking UI themselves. Then they show the editor opening a \"native\" file dialog, but don't actually say how exactly they did it. Anyway, I assumed they would use xdg-desktop-portal, and based on searching the repo that does seem to be the case. reply mikaylamaki 16 hours agoparentYep! That's how we did it, with the heavy lifting done by the delightfully named \"Aperture Science Handheld Portal Device\" crate: https://crates.io/crates/ashpd reply tomjakubowski 15 hours agorootparentSo Zed is using dbus to create and manage the file dialog in a toolkit independent way? Neat reply Arnavion 15 hours agorootparentYes, xdg-desktop-portal is a standard dbus interface. The interface covers file picker, print, screenshot, screencast, etc. The implementation of the interface (the \"backend\") is expected to be provided by the desktop environment in some way that makes sense for that DE. So gnome has xdp-gnome, kde has xdp-kde, etc, and there are some like xdp-gtk that are DE-agnostic but toolkit-specific. Backends can implement a subset of the interfaces, and xdp can be configured to try multiple backends in sequence until it finds one that provides the interface that the application wants. Eg xdp-wlr for wlroots-based Wayland compositors only implements compositor-specific interfaces like screencast, so users would chain it with something like xdp-gtk for other interfaces like file picker. Native applications usually use their toolkits' API for showing file picker dialogs etc, and xdp's file picker interface is primarily used by flatpak applications since that is a way for sandboxed applications to read/write outside their sandbox. But it's not impossible for native applications to also use it, as zed is doing. https://flatpak.github.io/xdg-desktop-portal/docs/api-refere... reply zozbot234 16 hours agoparentprevIf you want a GTK-based IDE experience there's always GNOME Builder. Like other modern GTK apps, it actually works quite well with custom GTK themes which provide a Win9x-like look (see the chicago95 and b00merang projects on github) if you want the better usability and higher information density that this enables. reply jchw 16 hours agoparentprevYeah, there are some caveats to using desktop portals this way, but at least it'll usually give you a native dialog. I do wonder what happens on a machine with no desktop portals, though. I suppose in the future it probably would be feasible to implement a decent file picker directly in GPUI as a fallback, so it's not that big of a deal; I reckon they could make one that's more usable than the GTK one anyways. reply eikenberry 16 hours agoprevIf Apple had just adopted the Vulkan standard instead of going off with their own proprietary graphics API this would have been a non-issue. Smacks of Canonical and their repeatedly doing one thing while ignoring the rest of the community standardizing on something else. reply mikaylamaki 15 hours agoparentUnfortunately, the renderer is only one step in supporting an alternative platform. Even if we could have used Vulkan on every platform, the windowing APIs are completely different and those do most of the things we like our code editors to support, like typing. reply cpuguy83 15 hours agoparentprevMetal predates Vulkan. But also I'm pretty sure Apple doesn't care if an app is portable. reply skohan 15 hours agorootparentIn fact they probably care that it's not easily portable reply jauntywundrkind 14 hours agorootparentprevBut AMD's Mantle predates both & was proposed & what lead to Vulkan. Apple saw the fork in the road & quickly scrambled to build their own thing. I'm kind of tired of this contention cropping up again and again. Technically true but insufficient & obfuscating more than revealing. reply cpuguy83 14 hours agorootparentSorry no. Vulkan literally did not exist. That's not obfuscation. It didn't even exist. Had Apple adopted Mantle then where would they be now? Apple did what they do, took ownership of their own stack. reply jauntywundrkind 11 hours agorootparent\"Sorry no\" whatever. Khronos was discussing what to do with Mantle as it became Vulkan. Apple was part of those industry players who were invited to the table. Apple left and did their own thing, like Apple does. These specs don't spring up fully formed. Vulkan was a collaboration long before it was named as such and released; Approaching Zero Driver Overhead/bindless was 100% clear writing on the wall for a while by then. Apple just doesn't collaborate; they demand control. That's why they didn't participate when everyone else was figuring out how to distill out Vulkan from Mantle & other close to the metal patterns that were already about. reply pjmlp 3 hours agorootparentMicrosoft, Sony and Nintendo also don't really care Vulkan exists. Besides AMD most likely only offered Mantle, because OpenGL vNext was going to be another Long Peaks failure, had it not been the case. And for what, Vulkan is already an spaghetti extension mess, with a complexity that feels like build a car out of LEGO Technics pieces, when one wants to drive down to the grocery store. reply cpuguy83 7 hours agorootparentprevApple _shipped_ Metal in 2014. Kronos started their work in 2014 and wasn't even named as such until 2015. The full spec wasn't released until 2016. reply tylerekahn 16 hours agoprevI'm curious why Zed chose Blade over wgpu/wgpu-hal. There's a bit of detail here: https://github.com/zed-industries/zed/issues/7015 But I'd be curious to read a longer writeup on the tradeoffs and how they came to their decision. reply mikaylamaki 16 hours agoparentPartially because Kvark, who has a long history in graphics programming, was enthusiastic about it, and has similar values of simplicity and effectiveness to our own. Mainly because our renderer is simple enough that we would have preferred to use Vulkan APIs directly, rather than going through wgpu. Blade is a thinner abstraction than wgpu, it's a bit more ergonomic than wgpu-hal, and it already supports our long term platform goals (Linux, Windows, and the web though via WebGL). So far, it's been running flawlessly, and it's been everything else that's the hard part! reply nasso_dev 15 hours agorootparentwgpu (and webgpu more generally) is often presented as a very good choice for a cross platform low level graphics api. but it was designed around safety/security constraints to support the web, sometimes at the cost of performance (my understanding) i heard somewhere this nice example: only big actors like AAA game engines would really benefit from the extra development effort it would take to use an even lower level api like vulkan/dx12 to squeeze the last 10% of performance that wgpu can't get you so if i understand correctly, zed, just like a AAA game engine, wants to squeeze every last bit of performance from the gpu, and so wgpu is \"too high level\" for it? and blade is \"like wgpu, but no design tradeoffs and lower level\" so its a better fit? does that mean someday zed might reach for vulkan directly one day? im assuming dx12 is gonna be used on windows anyway? i love kvark's work btw, we need more kvarks reply mikaylamaki 15 hours agorootparentRoughly yes, though I'd caveat it with the specific application of the GPU: simple, 2D, UI elements. We've already built an abstraction that allows us to fluently build up a design, and then burn it down into a collection of rectangles, paths, images, and such. We're not dealing with particle effects, lighting, 3D models, or anything else that game engines need to consider. As such, it's a lot lower cost for us to use something like Vulkan or DX12 directly. And, in fact, that was the backup plan if we didn't find something like Blade that we where happy with. reply Pannoniae 15 hours agoparentprevThis is a pretty good answer: https://github.com/gfx-rs/wgpu/tree/trunk/wgpu-hal The \"design of WebGPU\" is a problem. It's designed for the web to be secure and sandboxed so the API design reflects that. This is not a good thing on desktop. reply adwn 15 hours agorootparent> The \"design of WebGPU\" is a problem. It's designed for the web to be secure and sandboxed so the API design reflects that. The page you linked clearly states that wgpu-hal's API is extremely unsafe and skips many checks in order to reduce overhead. So while \"secure and sandboxed\" explains why wgpu wasn't chosen, it doesn't explain why wgpu-hal wasn't chosen. reply zie 16 hours agoparentprevI imagine having a great PR magically show up that used Blade had a lot to do with it. reply zozbot234 16 hours agoprevIf you care a lot about Linux support, Lapce is a very promising alternative. Much like Zed, it also is written in Rust with native GPU-based rendering and a shared focus on enabling integration with modern IDE technologies such as LSP servers, tree-sitter parsers and DAP for debugging. reply iamnbutler 15 hours agoparentyou can build zed for linux, today :) https://github.com/zed-industries/zed/blob/main/docs/src/dev... reply resource_waste 14 hours agoparentprevI didn't realize this was basically an ad for an IDE. I thought this was some component. Lol, yeah if your IDE doesnt work on Linux, the problem is your IDE, not linux. reply spudlyo 16 hours agoprevFrom what I understand Zed is a text editor that is built like a game engine, which I understand is why the marketing copy uses the term \"multiplayer\" rather than \"multiuser\". I didn't find other mention of its multiplayer capabilities, which is too bad. Has anyone used it for that purpose, I'm curious as to what the experience was like. reply mikaylamaki 16 hours agoparentWe use them every day at Zed for pair programming, we're rather fond of the features :D reply FridgeSeal 10 hours agorootparentUsed it the other day with my teammate to pair on some stuff, and it genuinely felt magic. We went from never having used it, to digging through their code together in about a minute, and have used it a few times since. Really nice UX. reply jodrellblank 14 hours agoparentprev\"multiplayer notepad\" goes back 15 years at least - https://hn.algolia.com/?dateRange=all&page=0&prefix=false&qu... it was used back with a popular website which opened a text document and anyone viewing could type, but I can't remember the name. That became a thing in Google Docs, Microsoft Office, Floobits, and lots of self-hosted and cloned sites. reply spudlyo 12 hours agorootparentYou may be thinking of Hackpad from 2014, which later was bought by Dropbox and morphed into Dropbox Paper. I guess I was thinking that a multiuser IDE is substantially different than a multiuser text editor. I did notice that Zed is by the same team that brought us tree-sitter, which is a wonderful innovation that many text editors are now using to good effect. I was wondering if were powerful enough for one person to be debugging some code in the same session that another person was refactoring. reply jodrellblank 9 hours agorootparentEarlier than that; I wanted to say EditPad but that's not it. I found it; EtherPad in 2008 is the one I was thinking of, but the history of collaborative editing apparently goes back to Doug Englebart's Mother of All Demos in 1968: https://en.wikipedia.org/wiki/Etherpad https://en.wikipedia.org/wiki/Collaborative_real-time_editor... reply KTibow 4 hours agorootparentprevA video I watched a while ago claimed that Talkomatic was the first one reply gliptic 13 hours agorootparentprevIRC was what was initially called multiplayer notepad. reply genpfault 17 hours agoprev> Zed[1] is a high-performance, multiplayer code editor from the creators of Atom and Tree-sitter [1]: https://github.com/zed-industries/zed reply keb_ 15 hours agoprevI tried it out briefly, and it seems promising so far! Zed reminds me a lot of Lite [1] and its community successor Lite-XL [2], which are also \"built like a videogame\" using a custom renderer and ui engine. [1] https://github.com/rxi/lite [2] https://github.com/lite-xl/lite-xl reply forgotpwd16 15 hours agoparent>\"built like a videogame\" A term coined Casey Muratori for this is immediate-mode GUIs or else IMGUI programming[1]. Hence the name for the very popular Dear ImGui project. GPUI used for Zed is mentioned as hybrid immediate and retained mode UI framework[2]. [1]: https://caseymuratori.com/blog_0001 (fist post on his blog!) [2]: https://github.com/zed-industries/zed/tree/main/crates/gpui reply the_duke 14 hours agoprevIf you use Nix(OS), zed is already in nixpkgs-unstable: `nix run nixpkgs#zed-editor`. I found a few issues, but overall it already works quite well and is very fast. It might even get me to give up on Neovim! reply myaccountonhn 15 hours agoprevI really appreciate how Zed tries to tailor make and ensure their app looks and feels native on Linux. One thing that scares me with it though is the lock in of the multiplayer features. I can see it forcing me to use it and not my preferred editor, because other people use Zed. Would love it if we were not only platform independent but also editor independent regardless of social pressure. reply vetrom 15 hours agoparentI am generally wary of anything that proposes to monetize the process of doing my core work. I recognize that that attitude makes marketing and selling a product to developers difficult, though. If they changed from a CLA to a DCO I'd feel less uneasy about it, but but is that something their investors and business can permit? reply myaccountonhn 8 hours agorootparentWhat I do is donate monthly to the projects I like. I'd happily pay a fixed one-time fee for them though. reply skohan 14 hours agoparentprevI use Zed as a daily driver, and I haven't once used the collaborative features reply yencabulator 10 hours agorootparent$ grep -A 999 Disable .config/zed/settings.json // Disable all genAI crap. \"features\": { \"copilot\": false, }, \"show_copilot_suggestions\": false, \"assistant\": { \"version\": 1, \"enabled\": false, \"button\": false, }, // Disable all \"social coding\" features. \"calls\": { \"share_on_join\": false, }, \"collaboration_panel\": { \"button\": false, }, \"chat_panel\": { \"button\": false, }, \"notification_panel\": { \"button\": false, }, } reply mhaberl 16 hours agoprevSerious question: I am looking at the measurements of speed on the landing page between Zed and the other editors, and they don't seem drastic. Do you actually feel the difference when typing? reply bioneuralnet 15 hours agoparentCompared with VSCode, absolutely. VSCode has a range of responsiveness from \"good enough\" to \"oh right, this is Electron\". Zed is both faster and more consistent. Sure, we're talking 10s of milliseconds, but it's surprisingly noticeable. Can't speak for other editors. I'm becoming a true convert, though I occasionally must drop down to a termianl for advanced vim features. That's high praise coming from me, as I have a high bar for adopting new tools. reply joshspankit 7 hours agoprevThe performance of their website got my attention. People who care about those details and who optimize for fast are my kind of people. reply prophesi 16 hours agoprevWhat's a valid Discord invite for Zed Linux? It seems like the link on the downloads page[0][1] needs to be updated. [0] https://zed.dev/download [1] https://discord.com/channels/869392257814519848/120467985020... reply aorth 5 hours agoprevThis looks rad. An amazing engineering effort for such a niche problem. As a Linux user who writes code, I'm excited, but I can't help wondering: what is the business model of Zed Industries? reply OptionX 15 hours agoprevStill baffled by the idea of make a code editor not only single OS but targeting mac. reply tfeldmann 14 hours agoprevZed has quickly become my daily driver for mostly python and some rust development. Awesome editor. reply timvdalen 15 hours agoprevFrom how they've been talking about Zed (on The Changelog, among other places), I expected to be wowed on my first open of the application, but the experience kind of fell flat for me. I might try it again when they have some better onboarding, though! reply FullGarden_S 13 hours agoprev> and we put a lot of effort into understanding macOS APIs to get to 120FPS If its ultimately a text editor with dead plain UI, why not simply stick to retained mode GUI and chill? I don't know how much of disk space and ram this written in Rust® text editor project demands to just build it but since its Rust™, achieving cross-platform will probably be several-folds more work than it would be in C, I applaud the devs for their work. I never tried Zed(yet) and keeping the support for vast programming languages aside, I would like to check out how this editor handles multiple workspaces and provide text editing features. I'm an Emacs guy and I have a feeling that this is not for me because even if its fast, it doesn't make me productive if I'm not using my emacs/vim key bindings. I might be wrong. reply jenadine 12 hours agoparent> since its Rust™, achieving cross-platform will probably be several-folds more work than it would be in C, Why would you think that? Rust is a more expressive and powerful language than C. If anything, I'd say it would be easier. Platform APIs are not all in C either. reply FullGarden_S 2 hours agorootparent> Why would you think that? FFI? I don't recall stating Rust is less expressive than C, the language looses its notorious \"memory safe\" feature as unsafe is called often when performing low level sys calls. > Platform APIs are not all in C either. unless you are absolutely talking about web development, this is not true at all. reply gostsamo 15 hours agoprevWhat is the accessibility story with Zed? I have a vague memory that atom had none of it. reply mikaylamaki 15 hours agoparentRight now: mixed. We provide both user defined themes and theme overrides that let people color-correct things as they need, we have a strong cultural focus on keyboard accessibility for all UI elements, you can increase or decrease the font size in the editor and the UI, and we have strong support for IME on macOS and will be extending that to Windows and Linux. However, our accessibility to screen readers is non-existent. I have ambitions to incorporate AccessKit but it's a bit of a project due to the lack of a clear guide on how to implement it. That said, I already made some progress based on the old egui PR and we should have all the pieces we need once I have time to actually do it. We also lack any way to tab through our UI elements to select each piece in turn. This one I have yet to do any thinking on, particularly as the tab key already does a lot of work in a code editor. I'm sure there's prior art here, I just haven't looked at it yet. So, piecemeal and insufficient for many cases, usable for some others. I'm very interested in improving this but we still have a lot to do. reply gostsamo 14 hours agorootparentThanks, will keep an eye on the developments there. A note, the tab key is not all that important. f6 for jumping the different parts of the screen and the arrow keys for neighbouring elements could be a good substitute for most cases. Add a few key bindings and tabless is not an issue any more. reply mikaylamaki 14 hours agorootparentThat does seem more usable for this use case. I'll keep it in mind, thank you! reply CooCooCaCha 17 hours agoprevI know it’s small thing but it annoys me so much when the blog section of a site doesn’t have a link back to the home page. Seriously, what do people think when designing headers like this? Like wow we should totally link to every other part of our site except the homepage. Because who would want to go there after they found out about a project through a blog post? reply syncsynchalt 16 hours agoparentThe logo in upper-left links to zed.dev, if that's what you mean. This is not always great for accessibility (they don't have alt=, title=, or aria-*= attributes on theortags) but seems to be a common pattern. reply CooCooCaCha 16 hours agorootparentThe logo doesn’t show up on mobile unfortunately. reply Brian_K_White 16 hours agorootparentScroll a million pages to the very bottom, and it's the totally not-visually-indicated white logo in the blue footer, it's clickable. I would not have found it or even thought to try to click on it except after reading this claim that it's there somewhere. reply dethos 16 hours agoprevSounds great. I'm eager to try it, when a release with early Linux support is published. Keep it going. reply Tubbe 15 hours agoparent`curl https://zed.dev/install.shsh` reply NoraCodes 15 hours agoprevOne question I don't think this post answers - why not use `winit`? reply mikaylamaki 15 hours agoparentSimply put: historical artifact and we want to own the stack. Zed was first created when these abstractions where a lot younger, and a text editor needs certain features, like IME support, that weren't as common back then. Further, owning the stack we're built on gives us the flexibility to change the framework whenever and however we want, which we value more than the additional work it takes to re-implement things ourselves. reply NoraCodes 14 hours agorootparentMakes sense, thank you for the thorough answer! reply adastra22 15 hours agoprevI had to click through three links just to find out what Zed is. reply riffic 16 hours agoprevam I supposed to know what zed is? reply Longhanks 15 hours agoparentWhy bother commenting instead of clicking the link and in the main navigation bar pressing the very clearly visible \"ABOUT\" button? reply metabagel 15 hours agorootparentIn all fairness, and I already knew what Zed was, the About page is a long read to find out anything more than that Zed is an editor. (As in, it says right away that they are building an editor, but then there is a long-winded discussion about history, goals, etc.) reply jasaldivara 14 hours agorootparentprevIt's just common online courtesy to add to the title a short description of the project like (Zed, code editor) to save users some valuable time. reply jodrellblank 14 hours agorootparentAnd it's in the HN guidelines not to: https://news.ycombinator.com/newsguidelines.html > please use the original title, unless it is misleading or linkbait; don't editorialize. > If the title includes the name of the site, please take it out, because the site name will be displayed after the link. > Please don't do things to make titles stand out, like using uppercase or exclamation points, or saying how great an article is. It's implicit in submitting something that you think it's important. reply beeboobaa3 16 hours agoprevWill it run on iGPU? reply paulcarroty 14 hours agoparentYep, just tested it few days ago. reply JohnKemeny 17 hours agoprevnext [14 more] [flagged] 12_throw_away 17 hours agoparentSome troubleshooting tips: - Move your mouse cursor to the \"Zed Industries\" icon in the upper-left-hand-corner, then click. - This \"hypertext link\" will instruct your browser to load the site's root \"index.html\", sometimes known as a \"landing page\". - The \"landing page\" is a great place to start if you're unfamiliar with a site or product! Here, you'll find the following text prominently displayed: \"Zed is a high-performance, multiplayer code editor from the creators of Atom and Tree-sitter. It's also open source.\" Hope this helps! reply Brian_K_White 16 hours agorootparentSome troubleshooting tips while dispensing dickface advice: There is no such logo. (If there is for you, good for you. Some information for you, different browsers get fed different content, iyt's a new thing that only started around 1993 so I can see how you weren't aware.) Actually there is one, 7000 pages down at the bottom, embedded in the footer and not looking at all clickable. \"Hope this helps!\" reply nulld3v 17 hours agorootparentprevTo be fair, the logo is like 2px tall on Firefox Android... reply iamnbutler 15 hours agorootparentthat is my bad – someone flagged this before and I forgot to fix it. A quick fix is deploying now! reply pythonaut_16 17 hours agoparentprevIt's a blog post directly on their website. Is it really unreasonable to need to click to the home page for more context? reply metabagel 15 hours agorootparentAt the time you posted this comment, there was no link to the home page for mobile devices. Also, on mobile devices it's a pain to edit the URL in the address bar. You could have gone to the About page, but there's no summary. You'd gather right away that this was about an editor, but you'd have to read quite a bit about history, goals, etc. before you find out what differentiates this editor from any other editor. reply melodyogonna 17 hours agorootparentprevPeople on the internet expect everything to be spoonfed to them. reply JoelMcCracken 17 hours agorootparentKinda wild to me that people who frequent a website that bills itself as a site for news for \"hackers\" show such a nil-level of ability to comprehend anything themselves. reply infamouscow 8 hours agorootparentThis website allows extremely vituperative people to make their points without being banned as long as they follow the rules. My account being the primary example. That this website is littered with retarded morons that cannot handle the slightest bit of pure direct argumentation is more a function of the times, where heavy-handed moderation fills in for massive gaps in their vacuous perspective, which in any other situation completely undermines one's understanding of the world outside a pseudo-intellectual bubble. The moderates here do better than any other online, IMO. And I say that knowing the moderators here do not unflag some of my comments because it's ideologically inconvenient to them personally. reply PurpleRamen 17 hours agorootparentprevMaybe because it works to often.. reply infamouscow 16 hours agorootparentWhile it's hard to separate them, derision and mockery aren't considered ad hominem insults to vacuous and lazy commentary. When you eliminate all human repercussions for acting like a moron, even if it's as small as feeling bad, one should expect to see more idiocy. reply PurpleRamen 17 hours agoparentprevThe logo in the top left leads to the startpage, explaining what zed is: [..]Zed is a high-performance, multiplayer code editor from the creators of Atom and Tree-sitter. It's also open source.[..] Not sure if I should care, but more fast modern editors are good. But if they only just now got linux-support, then I think they will still need much much love to reach a good shape. reply weberer 17 hours agoparentprevText editor. reply mgaunard 16 hours agoprev [–] Atom, like most recent editors like VS Code etc., are just clones of Sublime Text with just lots of bloat and JavaScript tech. I'll just stick to Sublime Text. reply bovermyer 16 hours agoparent [–] You're aware that Atom is not the subject of the article, right? reply bioneuralnet 15 hours agorootparent [–] I believe some of the Zed folks invented Atom back in the day. But yes, it doesn't seem like a highly relevant comment... reply mgaunard 13 hours agorootparent [–] the article doesn't tell me what Zed is. If I look around, it just says it's the new text editor from the authors of Atom. reply FireBeyond 5 hours agorootparent [–] > doesn't tell me what Zed is. If I look around, it just says it's the new text editor Huh. Did you mean to disagree with yourself the very next sentence? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The article explores the creation of Zed, a high-performing application, on Linux utilizing the GPUI framework to enable its functionality on the platform.",
      "It delves into technical choices and obstacles faced, including selecting audio and rendering systems, particularly highlighting the employment of Blade, a Vulkan-based solution, for rendering on Linux.",
      "The team is gearing up for an imminent alpha launch on Linux, enhancing features and stability, and is on the hunt for software development enthusiasts to join their workforce."
    ],
    "commentSummary": [
      "The forum discussion revolves around Zed, a text editor, covering topics such as graphics libraries, rendering efficiency, platforms, collaborative tools, Apple's Metal API, and the choice of Blade over wgpu.",
      "Users express mixed opinions on Zed's performance, features, and business model, with debates on speed, collaboration, and usability.",
      "The conversation extends to the Zed website's accessibility, comparing it to other text editors like Sublime Text and Atom, while also debating internet users' understanding and the efficacy of online forum moderation."
    ],
    "points": 214,
    "commentCount": 143,
    "retryCount": 0,
    "time": 1715095908
  },
  {
    "id": 40286632,
    "title": "New SecureDrop Encryption Protocol Enhances Source Anonymity",
    "originLink": "https://securedrop.org/news/introducing-securedrop-protocol/",
    "originBody": "May 6, 2024 This blog post is a part of a series about our research toward the next generation of the SecureDrop whistleblowing system. If you haven’t been following along, check out our previous post for some recommended context. Here, we present a proposed end-to-end encryption protocol for a future version of SecureDrop server, accompanied by a brief discussion of various aspects of the protocol and our next steps. Protocol overview We present a proof-of-concept protocol with the following core properties, which are derived from the research and threat modeling concerns previously discussed: No accounts, and therefore no user authentication; No message flow metadata, meaning messages can’t be linked together, and different types of messages are indistinguishable from one another; No changes in server state are observable externally; No ciphertext collection or information leaks via trial decryption – a given recipient receives pertinent ciphertext only. The protocol has a simple API with only three endpoints: send, fetch, and download. Sources’ and journalists’ requests to these endpoints are structured identically, to make them harder for a compromised server or network adversary to distinguish. The server’s response is unique for each request, to avoid leaking information about server state. It uses well-established cryptographic primitives (currently libsodium for proof-of-concept purposes), achieving end-to-end encryption and one-way forward secrecy for messages flowing from source to journalist. The server has access to minimal metadata, theoretically opening the door to deployment in adversarial environments. Along with this blog post, we’re also publishing: A proof-of-concept GitHub repository that demonstrates the end-to-end protocol and documents how it works; And a preliminary audit, which also contains a concise summary of the protocol. This is proof-of-concept code and is not intended for production use. The protocol details are not yet finalized. Discussion Let’s contextualize the four notable properties of the SecureDrop Protocol as enumerated above. The first property arises from the desire to maximize plausible deniability for the source in a variety of situations, including repeated snapshotting of the server. The motivation for the second property is evident, but the implementation is less trivial: it’s one thing to conceal a sender, but how does a server deliver a message to an unknown recipient? One approach is trial decryption — deliver all ciphertext to everyone, and valid recipients will determine locally which messages belong to them based on what their own secret keys can decrypt. But besides being inefficient, this would also introduce an information leak: it would allow anyone to constantly monitor the server state and know when messages are added and deleted, observe message upload patterns, and so on. In the context of a relatively low-traffic self-hosted system, this would represent a significant metadata leak, thus the need for our third property. Finally, one of our additional goals is a graceful degradation of security properties, in case of different degrees of user or server compromise. Allowing anyone on the internet to observe ciphertext would allow for “harvest now, decrypt later” attacks, so the fourth property reflects the need to guard against such possibilities. Comparison of messaging protocols 1. While there isn’t a single standard library, the implementation is straightforward. 2. New iteration of the research focuses on groups. 3. SecureDrop Protocol does not preclude scalability, but scaling to mass adoption level (i.e. millions of users) is a nonrequirement for our purposes. The following is a highly simplified comparison of real-world message delivery approaches. Trial decryption Trial decryption is a simple way to conceal recipient information. Payloads are encrypted for a given recipient, which in turn has full visibility of all the payloads in the system. As the name suggests, a recipient will discover which messages belong to them by trying to decrypt all the messages they receive. Benefits to this approach include symmetry among user traffic (all users download all payloads) and lack of account creation requirement. Limitations include the lack of scalability, the information leak mentioned above, and the ability for an arbitrary attacker to collect payloads/ciphertext. This approach would satisfy requirements 1 and 2 (lack of accounts, lack of message flow metadata), but not 3 and 4 (externally-observable server state, bulk ciphertext collection). Signal Protocol Signal relies on accounts for both message submission and delivery. Only authenticated users can submit messages, and recipients must be specified outside the encrypted payload in order for the server to deliver the pertinent payloads at the next recipient login and fetch. This message delivery approach has high delivery reliability, and is resource-efficient for the recipient, since only pertinent payloads are being downloaded and decrypted, but it would not support our transition towards a less trusted server. It satisfies properties 3 and 4 (no externally-observable server state changes, no bulk ciphertext collection), but not 1 and 2 (accounts, message flow metadata). Oblivious Message Detection/Retrieval (OMD/OMR) Oblivious Message Detection and Retrieval is a newer message delivery mechanism aimed at concealing the recipient and addressing the scalability issues of trial decryption by, in very broad terms, mixing pertinent and non-pertinent payloads, and using one or more “detector” servers to heuristically group, as well as distinguish, pertinent and non-pertinent content for a given user based on the “clues” assigned to each payload. (The idea of detecting/delivering messages in a way that makes them differentiable only by the receiver is an active research area; see also Fuzzy Message Detection.) Fuzzy message delivery is less feasible for small-scale systems like SecureDrop, where a burdensome level of decoy traffic would be required. In addition, OMR uses homomorphic encryption and other constructs that, at the time of writing, are relatively new, and pose some design challenges. The combination of the protocol's complexity and the bleeding-edge nature of some of the protocol's components gave us pause, although research in this area, particularly that focused on group messaging, is promising. SecureDrop Protocol As with OMR, in the SecureDrop Protocol a sender sends the payload and a “clue” without any authentication. The recipient anonymously asks the server for clues, and the server serves both clues and decoys on every request. Every time a message-fetching request is made to the server, the response contains a new combination of clue and decoy ciphertext. Since every request returns a different response, and since the ciphertext is indistinguishable without being able to decrypt one or more of the clues, individual requests do not leak information about server state changes. Message-fetching is a two-step operation. The recipient requests clues, and attempts trial decryption on them. For each successfully-decrypted clue, the recipient discovers a message_id. The message_id is a cryptographically unguessable value generated by the server upon message submission and kept private from all other parties. When an anonymous user discovers a message_id, they are able to fetch the corresponding message payload from the server. This two-step process is an optimization-related implementation detail, not a security measure. SecureDrop Protocol in detail Understanding SecureDrop Protocol’s message retrieval mechanism requires a basic understanding of classical Diffie-Hellman key agreement. In particular, we take advantage of the notable properties that key agreement is commutative and can be extended to any number of parties. Asynchronous three party Diffie-Hellman between sender, server, and recipient This diagram shows how the recipient of a message can agree with the server on a shared key K without learning anything about the state of the server – while preventing the server from knowing whether the recipient is indeed the intended one. Every K is obtained using: long-term key material, which later allows the correct recipient to detect their pertinent messages; per-message ephemeral key material, which introduces unlinkability between messages addressed to the same recipient; and per-request ephemeral key material via “remixed” clues, which provides unlinkability between message-fetching requests, since the clues are transformed before every request. As shown in the next diagram, this scheme allows the server to obliviously encrypt and transfer some information using symmetric key K, without knowing if the requesting recipient will be able to decrypt it. Detail of the oblivious transfer of a message_id between the server and a recipient The material that is obliviously-transferred is the message_id, a securely random, unguessable value that the server generates upon message submission. Conceptually, it can be thought of as a server-side message uuid. Its main purpose is to keep clue sizes small, rather than encrypting and remixing full message payloads. Leveraging the commutativity of multi-party Diffie-Hellman to achieve unlinkability between requests is an uncommon application. We credit this contribution to former SecureDrop team member Michael Z, who resolved what was a multi-month sticking point in our research. Sample visualization of five fetch attempts from two valid and three invalid recipients, from a server containing 3 messages (values are symbolic) Why is the server “untrusted” if it generates secret key material? The server is “untrusted” in the sense that it should learn nothing about users and messages besides what is inherently observable from its pattern of requests, and it should not have access to sensitive metadata, or sender or receiver information. What the server must know are ciphertext payloads and associated clues that need to be stored. In this regard, the server is still a “privileged” asset that should not be queryable by anyone on the internet. That is why the server contributes with an ephemeral key: to hide its state and thus render all requests unlinkable to any change of such state. SecureDrop Protocol: encryption properties Message retrieval symmetry despite source-journalist asymmetry Until now, we have commonly used the terms sender and recipient to indicate a role that a user can have. In classical messaging applications, all users can both receive messages and initiate conversations. In whistleblowing applications, the anonymized party (source) initiates communication with the trusted party (journalist). Our previous blog post outlines the difference in their endpoint requirements in detail, and describes the limitations on the type of key material that a source can have. So, while both journalists and sources will use the same message retrieval mechanism and the same submission mechanism (and in practice, the same API endpoints), and are treated indistinguishably from the server’s perspective, their key setup is inherently different. This difference of roles and setup has a significant impact on the encryption properties we can achieve, especially in three areas: forward secrecy, participant authentication, and deniability. Forward secrecy: It’s imperfect Forward secrecy requires ephemeral keys, implying an evolving state with some non-predictable, non-deterministic source of entropy. Forward secrecy requires that both parties use per-message ephemeral keys, and is commonly achieved by periodically re-generating secret-key material and then discarding it, like an automatic key-rotation procedure. Our design constraints allow for source-to-journalist forward secrecy, by rotating some journalist encryption key, but not vice versa: for usability reasons, we do not want to rotate a source passphrase, and we cannot add non-deterministic key material on the source side, since we cannot store it anywhere. We consider the anonymity of the source a higher priority than the confidentiality of the material, so under these circumstances we accept this “one-way” form of forward secrecy. Participant authentication Consider a simple Diffie-Hellman key-agreement used with per-message ephemeral keys used for symmetric message encryption. A Diffie-Hellman key agreement is performed between the secret part of the per-message key and the public component of the recipient. This kind of key agreement is unauthenticated: the recipient has no implicit or explicit proof that a particular sender sent that public key and ciphertext. This does not matter for a “first contact” message, which by definition comes from an anonymous/unknown party, but for ongoing conversations, some measure of participant authentication is required – essentially, to ensure that the source who began the conversation is indeed the one who is continuing it. The question of participant authentication is an open problem that is interlinked with deniability. We see several fairly straightforward ways of addressing it; more discussion can be found here. Deniability for sources There are multiple layers at which SecureDrop seeks to create deniability: forensic deniability on a source’s own machine; forensic deniability on the server of the existence of a given source; and message-level cryptographic deniability. The first forensic deniability property is discussed at length in our previous posts and is orthogonal to the protocol; broadly, we satisfy it by avoiding writing to disk on the source’s machine. The second type of forensic deniability is served by the lack of server-side accounts; the only way to prove that a given party exists is to gain ciphertext addressed to them (a pending message) along with their key material. Conversation-level and message-level deniability require a more detailed discussion. In cryptographic terms, deniability is often called repudiation, of which participation repudiation and message repudiation are two forms. Repudiation and authentication are both important security principles, and yet, in some ways they appear at odds with one another: how can a protocol offer both authentication and the ability to deny sending a message or being part of a conversation? The Signal Protocol does offer both repudiation attributes and implicit participant authentication under certain conditions, via a clever use of multiple Diffie-Hellman key exchanges. Broadly, the implicit authentication comes from the use of long-term identity keys, while the deniability comes from the combination of approximately-discoverable public keys (via phone number/username, on Signal’s servers) and the idea that, while both parties must agree on a shared key to facilitate their conversation, neither party must sign their individual messages. Hypothetically, a conversation between two parties could be forged by one of the parties, providing the other with a measure of deniability. By contrast, the SecureDrop Protocol cannot provide conversation-level repudiation in cases where an ongoing source-journalist conversation is desired. This is because, while journalist public key material can and must be publicly discoverable so that anonymous sources can contact them, sources do not publish their public key material, and must attach it in their first-contact message so that journalists can reply, proving their participation in at least this initial message. SecureDrop Protocol could theoretically support two modes: a true “dead drop” mode from a source that never intends to return or receive replies, and a conversational mode, in which the source expects to return and continue correspondence. In the former, the source achieves full deniability. In the latter, which is the more typical use-case, individual message repudiation can be achieved (as can participant authentication, mentioned above), but participation repudiation cannot. This issue is an area of ongoing research. Putting it all together: message-fetching and encryption The following is a simplified illustration of the message-fetching and encryption in the SecureDrop Protocol. (Ephemeral keys have been omitted for illustrative purposes). The full protocol diagram can be found on GitHub. Cryptographic primitives The proof-of-concept implementation uses libsodium for both key-agreement and symmetric encryption. That means, as of now, that our DH exchanges are powered by X25519 and our authenticated symmetric encryption is powered by XSalsa20 and Poly1305 MAC. Post-quantum security? Elliptic-curve Diffie-Hellman is not considered quantum-safe. There are various key encapsulation mechanisms we could make use of that are appropriate for encryption, however, we have not yet found a post-quantum-secure solution for the message-fetching mechanism in the protocol, which requires the commutative properties of multi-party ECDH discussed above. The multi-party mechanism we describe is used to protect anonymized metadata from leaking information about the state of the server over time, similar to a network-level correlation attack. One possibility is a hybrid model where messages themselves (including their sender information and other metadata) are encrypted in a quantum-safe manner, while the message-retrieval mechanism remains usable with less future-proofing. That said, this protocol will not transition to production-readiness until a thorough post-quantum security assessment has taken place. Post-quantum readiness is an open research topic that we do not have the resources to address alone; we are grateful for the offers of assistance with post-quantum readiness that we have received, and we welcome additional collaboration in this area! Preliminary audit In November 2023, we engaged cryptographic researcher Michele Orrù for a preliminary audit of the protocol and some of our claimed security properties, with a focus on the server state hiding and message unlinkability. The full report is available here, and a preliminary discussion of the findings can be found on GitHub. Next steps Astute readers will have wondered where the client side of this protocol takes place. We previously discussed the problem of browser-side encryption in “Anatomy of a Whistleblowing System”, and we’ll have more to say on this topic soon. Later this year, we also expect to have the results of ongoing work on formally modeling the SecureDrop protocol, proving its security properties, and finalizing the specification for audit and publication. In the meantime, we want to invite feedback from the security and cryptography community on this research. You can join our discussion list, write to us privately at , or get involved on GitHub. If you have expertise in post-quantum/quantum-resistant cryptography, we’d especially like to hear from you! Acknowledgments This research has been funded by Freedom of the Press Foundation (FPF) and is being led by SecureDrop team member Giulio B. Davide TheZero and smaury from security firm Shielder SpA have partnered in the research effort. The audit of our proposed framework was conducted by Michele Orrù. We’d like to acknowledge Michael Z for their core contributions to this work, both as a SecureDrop team member and as an independent contributor. We would also like to thank the following people: Stuart Haber Jennifer Helsby David Liu Olivia Murat Guillermo Pascual Pérez Paul Rösler Eleanor Saitta Jacob Young Appendix Additional resources and links Lua server demonstrating another example SecureDrop Protocol implementation Slide deck from SecureDrop presentation on deniability Audit report Original proof-of-concept repository based on the Signal Protocol Timeline October 2020: First commit in https://github.com/freedomofpress/signal-protocol November 2021: Working demo of https://github.com/freedomofpress/securedrop-signal-poc January 2022: Internal open discussion on the code integrity problem; JavaScript discussion and necessary steps for any end to end encryption July - October 2022: Various team meetings, discussion advances January - February 2023: Initial proof-of-concept development with Shielder, first external feedback requests October 2023: Final version of the message delivery protocol November 2023: Team members participate in IETF; initial outreach to cryptographers November 2023: Engaged Michele Orrù for the audit January 2024: Audit report, ongoing outreach to and collaboration with cryptographers",
    "commentLink": "https://news.ycombinator.com/item?id=40286632",
    "commentBody": "SecureDrop Protocol (securedrop.org)211 points by Zezima 18 hours agohidepastfavorite8 comments GalaxyNova 13 hours agoThis would be very useful for Boeing engineers. reply immibis 8 hours agoparentThey can just use Tor and HTTP upload. This protocol is more theoretical than practical. No protocol helps when you have to testify in court. reply devdao 5 hours agoprevWould you trust this system for making a report? How do journalists feel about this? Come on HN, we owe it to those working to make the world safe for democracy to engage with their ideas. reply blamestross 15 hours agoprevI'm not convinced requirements 3 and 4 are actually needed. 3) make internal state not useful to the attacker. 4) assuming the ciphertexts won't leak seems silly. Might as well hand them out. Which leads to what they call \"trial decryption\" to be a better solution. If you are that worried about the scalability of your whistleblower protocol at that level, we are trying to solve the wrong problem. reply blamestross 15 hours agoparentBitmessage solved this problem a while ago. This just adds extra centralization. reply irq-1 11 hours agorootparentFrom the Bitmessage website https://wiki.bitmessage.org/ > Security audit needed > Bitmessage is in need of an independent audit to verify its security. If you are a researcher capable of reviewing the source code, please email the lead developer. You will be helping to create a great privacy option for people everywhere! reply blamestross 8 hours agorootparentOh I don't recommend using bitmessage. It's a huge illegal content hazard. But it's design is the right set of tradeoffs for the situation this article describes. reply LorenzoGood 6 hours agoprev [–] Tor? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The blog post explores a new end-to-end encryption protocol for the upcoming SecureDrop system, enhancing source anonymity and preventing metadata correlation.",
      "The protocol prioritizes deniability, secure message delivery, and user privacy, utilizing cryptographic primitives and a straightforward API.",
      "Future developments include finalizing the protocol, tackling browser-side encryption, and gathering input from the security community, led by Giulio B. Davide TheZero and funded by the Freedom of the Press Foundation."
    ],
    "commentSummary": [
      "Debate focusing on the practicality of the SecureDrop Protocol for Boeing engineers compared to using Tor and HTTP upload is ongoing.",
      "Mention of Bitmessage as an alternative to the SecureDrop Protocol raises concerns about its security and legality, sparking further debate on the topic.",
      "Discussions also touch on the necessity of specific requirements in the SecureDrop Protocol, adding depth to the conversation on secure communication methods for sensitive information."
    ],
    "points": 211,
    "commentCount": 8,
    "retryCount": 0,
    "time": 1715094776
  },
  {
    "id": 40285476,
    "title": "Microsoft Shuts Down Multiple Bethesda Affiliated Game Studios",
    "originLink": "https://www.ign.com/articles/microsoft-closes-redfall-developer-arkane-austin-hifi-rush-developer-tango-gameworks-and-more-in-devastating-cuts-at-bethesda",
    "originBody": "HI-FI RUSH MIGHTY DOOM REDFALL ELDER SCROLLS ONLINE THE EVIL WITHIN 2 Microsoft Closes Redfall Developer Arkane Austin, Hi-Fi Rush Developer Tango Gameworks, and More in Devastating Cuts at Bethesda Staff told via email. BY WESLEY YIN-POOLE UPDATED: MAY 7, 2024 4:04 PM POSTED: MAY 7, 2024 1:17 PM Microsoft has closed a number of Bethesda studios, including Redfall maker Arkane Austin, Hi-Fi Rush and The Evil Within developer Tango Gameworks, and more in devastating cuts at Bethesda, IGN can confirm. Alpha Dog Games, maker of mobile game Mighty Doom, will also close. Roundhouse Studios will be absorbed by The Elder Scrolls Online developer ZeniMax Online Studios. Microsoft, currently valued at over $3 trillion, did not say how many staff will lose their jobs, but significant layoffs are inevitable. IGN has asked Bethesda for comment. Microsoft declined to expand further when contacted by IGN. On Redfall, the disastrous vampire co-op game will now not receive promised updates, including an offline mode and new character DLC, as Microsoft has ended all development on the game. Microsoft said Redfall will remain online to play, and it will provide a \"make-good\" offer for those who bought the Hero DLC. Signup here https://t.co/wb1R4m4emj to receive details on how eligible players can receive this credit. pic.twitter.com/69Os17kpQ8 — Arkane Studios (@ArkaneStudios) May 7, 2024 Arkane Lyon, which is working on Marvel's Blade, survives the cull, as does Bethesda Game Studios (Fallout, The Elder Scrolls, Starfield), and Machine Games (Indiana Jones and The Great Circle). Doom developer id Software is also unaffected. \"This reprioritization of titles and resources means a few teams will be realigned to others and that some of our colleagues will be leaving us. “ In an email to staff sent by Matt Booty, head of Xbox Game Studios, Microsoft blamed the cuts on a “reprioritization of titles and resources”. The email, verified by IGN, is below: Today I’m sharing changes we are making to our Bethesda and ZeniMax teams. These changes are grounded in prioritizing high-impact titles and further investing in Bethesda’s portfolio of blockbuster games and beloved worlds which you have nurtured over many decades. To double down on these franchises and invest to build new ones requires us to look across the business to identify the opportunities that are best positioned for success. This reprioritization of titles and resources means a few teams will be realigned to others and that some of our colleagues will be leaving us. Here are the changes going into effect: Arkane Austin – This studio will close with some members of the team joining other studios to work on projects across Bethesda. Arkane Austin has a history of making impactful and innovative games and it is a pedigree that everyone should be proud of. Redfall’s previous update will be its last as we end all development on the game. The game and its servers will remain online for players to enjoy and we will provide make-good offers to players who purchased the Hero DLC. Alpha Dog Studios – This studio will also close. We appreciate the team’s creativity in bringing Doom to new players. Mighty Doom will be sunset on August 7 and we will be turning off the ability for players to make any purchases in the game. Tango Gameworks – Tango Gameworks will also close. We are thankful for their contributions to Bethesda and players around the world. Hi-Fi Rush will continue to be available to players on the platforms it is today. Roundhouse Games – The team at Roundhouse Games will be joining ZeniMax Online Studios (ZOS). Roundhouse has played a key role in many of our recent game launches and bringing them into ZOS to work on The Elder Scrolls Online will mean we can do even more to grow the world that millions of players call home. With this consolidation of our Bethesda studio teams, so that we can invest more deeply in our portfolio of games and new IP, a small number of roles across select Bethesda publishing and corporate teams will also be eliminated. Those whose roles will be impacted will be notified today, and we ask that you please treat your departing colleagues with respect and compassion. We will provide our full support to those who are impacted in today’s notifications and through their transitions, including severance benefits informed by local laws. These changes are not a reflection of the creativity and skill of the talented individuals at these teams or the risks they took to try new things. I acknowledge that these changes are also disruptive to the various support teams across ZeniMax and Bethesda that bring our games to market. We are making these tough decisions to create capacity to increase investment in other parts of our portfolio and focus on our priority games. Bethesda remains one of the key pillars of Xbox with a strong portfolio of amazing games and thriving communities. As we look to the future, there is an impressive line-up of games on the horizon. In 2024 alone we have Starfield Shattered Space, Fallout 76 Skyline Valley, Indiana Jones and The Great Circle, and The Elder Scrolls Online’s Golden Road. As we align our plans and resources to best set ourselves up for success in this complex and changing industry, our teams across Arkane Lyon, Bethesda Game Studios, id Software, MachineGames, ZeniMax Online Studios and the Bethesda publishing and corporate teams will be well-positioned to build new IP, explore new game concepts, and expand on our existing franchises. - Matt Microsoft's announcement of the cuts at Bethesda come over three months after the company announced plans to cut 1,900 staff from its video game workforce, and amid a boom time for Bethesda's Fallout series following the breakout hit Prime Video TV show. The closure of Tango Gameworks hits just over a year after the launch of Hi-Fi Rush, what many considered to be one of the best Xbox games in recent years. Tweeting in April 2023, Aaron Greenberg, VP, Xbox Games Marketing at Microsoft, addressed concern that Hi-Fi Rush had failed to meet sales targets, calling the game \"a break out hit for us and our players in all key measurements and expectations.\" Greenberg continued: \"We couldn’t be happier with what the team at Tango Gameworks delivered with this surprise release.\" Hi-Fi Rush launched on PS5 as part of Microsoft's new multi-platform push in March. pic.twitter.com/KhhqgR902g — Tango Gameworks (EN) (@TangoGameworks) May 7, 2024 In a series of tweets, Arkane Lyon chief Dinga Bakaba hit out at the cuts, calling them \"absolutely terrible.\" \"To any executive reading this, friendly reminder that video games are an entertainment/cultural industry, and your business as a corporation is to take care of your artists/entertainers and help them create value for you,\" Bakaba continued. \"Don't throw us into gold fever gambits, don't use us as strawmen for miscalculations/blind spots, don't make our work environments darwinist jungles. You say we make you proud when we make a good game. Make us proud when times are tough. We know you can, we seen it before. \"For now, great teams are sunsetting before our eyes again, and it's a fucking gut stab. Lyon is safe, but please be tactful and discerning about all this, and respect affected folks' voice and leave it room to be heard, it's their story to tell, their feelings to express. \"Inside baseball, but if I read 'immersive sim curse' from the community, especially from a fellow dev, I swear to God... Please, let's talk about the *real* challenges instead of rehashing irrational anxieties of the past. \"Even more inside baseball, but with a very, very wide range, as a wise and sorely missed man said: 'Please Stop.' \" Layoffs are sweeping the video game industry, with a number of high-profile studios cutting staff or shutting down. In stark contrast to a year of blockbuster video game hits, one of the biggest ongoing industry trends in 2023 was the prevalence of mass layoffs, and they have continued into 2024. While actual figures are difficult to obtain, estimates suggest the number of workers laid off in games last year approached or exceeded 10,000. A recent GDC survey of developers suggests one-third of all game developers were impacted by layoffs last year, either directly or by witnessing them happen at their company. Wesley is the UK News Editor for IGN. Find him on Twitter at @wyp100. You can reach Wesley at wesley_yinpoole@ign.com or confidentially at wyp100@proton.me. In This Article Hi-Fi Rush TANGO GAMEWORKS JAN 25, 2023 Rate this game RELATED GUIDES Overview Tips and Tricks for Combat, Combos, and High Ratings Recommends Tango Gameworks Founder Shinji Mikami Shares Brief Reaction to Studio Closure 96 Arkane Austin Was Working on a Big Redfall Update for May Before Microsoft Ditched the Game and Closed the Studio 333 James Gunn's Superman: 3 Big Revelations From the First Photo 53 Stardew Valley Review - 2024 147 Everything Apple Announced During Its Spring iPad Event 96 Microsoft Closes 3 Bethesda Studios, Including the Developer of Redfall 1.8K How to Watch the Planet of the Apes Movies in Chronological Order 53 The X-Men Movie Timeline, Explained 130",
    "commentLink": "https://news.ycombinator.com/item?id=40285476",
    "commentBody": "Microsoft closes several large Bethesda affiliated game studios (ign.com)206 points by KTallguy 20 hours agohidepastfavorite196 comments DannyPage 19 hours agoHi-Fi Rush was a fantastic game, won major awards, showed off the promise of XBox Game Pass… and for their efforts their studio has been shut down. There have multiple instances of this in just the past few weeks; are publishers really going to just bet on their prime AAA titles (Call of Duty, Halo, GTA6, etc) and nothing else? And those games either take a lot of rotating studios and a long development cycle to release. What’s going to fill the gap? reply mattgreenrocks 18 hours agoparentThe current tech meta seems to be coalescing around this notion of \"even high-performance isn't enough to spare you from layoffs, be afraid!\" It wasn't that high-performance always guaranteed your job would continue, but at least there'd be the idea that you'd get moved to a new team if you were a great performer. To me, that just reinforces the notion that these layoffs are mostly about sending a message to workers and Wall St more than anything else. reply badsectoracula 8 hours agorootparent> To me, that just reinforces the notion that these layoffs are mostly about sending a message to workers and Wall St more than anything else. What message? reply maxrecursion 18 hours agoparentprevGame development is in a really weird place. Insanely over-saturated but almost all AAA games are extremely derivative, stale, bland games with a coat of pretty graphics Indie games are awesome right now, but they don't have the budgets to produce AAA games. So there is a huge gap. Innovative indie games with cool, new gameplay concepts, but always simple or retro graphics, and AAA games with shiny graphics on the other end but gameplay that hasn't changed in over a decade. I'm just waiting for any AAA studio to provide something new with the AAA games. Maybe AI to improve NPCs in an open world game? Anything besides the same old gameplay with new skins on it. reply somenameforme 17 hours agorootparentIt's just risk aversion. Companies want to turn video games into a factory line golden goose, but struggle to reconcile that each iteration through that factory line makes the final product relatively worse and worse, even if it continues to look better and better. Now even Call of Duty can't find a Call of Duty killer. But these same companies are terrified of trying anything new because new things do, on occasion flop. It would also entail scrapping the factory line, because creating a new game, instead of reskinning and old with a few new tweaks, is a way different beast. That said, I don't really think the stereotypes of indie games are very valid anymore. Valheim looks great, has a massive open world, and is multiplayer. [1] It also started entirely as a result of one guy's pet project, until he grabbed a coworker and then set off to make it what it became. The graphics are stylized, but I think in a broadly aesthetically appealing way, as opposed to e.g. pixel graphics which are very off-putting to many people, myself among them. Pixel graphics came from an era of CRTs with interlaced scanning, and various other visual artifacts, that naturally blurred, antialiased, and blended them. Sharp jaggy edges never really existed, and I fail to understand why that's a popular style now. [1] - https://www.youtube.com/watch?v=XSVbXgBJIuI reply raxxorraxor 1 hour agorootparentSome game concepts are fairly well developed. A shooter like Call Of Duty is such a concept, so making a competitor is far more difficult. Sure, you can make up with setting and presentation. But otherwise very true, true innovation happens in the indie world and the maximal complexity of these type of games is steadily rising due to better tools and maybe soon AI support. reply hbn 17 hours agorootparentprevAAA basically just means nice graphics at this point. You can't dump more money into a piece of art to make it better, that's why all the innovation comes from indie games. Look at Balatro, a guy made a poker roguelike and became a millionaire overnight. I think if big game studios, rather than dumping their copious amounts of money into single, giant-scope games, dealt it out amongst a variety of smaller teams for smaller-scoped projects, they'd be way better off. Everyone keeps suggesting AI NPCs. I'm sure someone's gonna take a crack at it and it'll go about as well the Humane AI pin or the Rabbit R1 before everyone realized how horrible of an idea it is. If anything it'll make for a silly novelty like the VR games where you clumsily try to perform basic tasks with VR motion controls. But in this case you argue with an in-game LLM and see how quickly you can make it get defensive or start gaslighting you with made up facts about household cleaners you can combine to make a delicious cocktail. reply dlachausse 17 hours agorootparentIt’s nice graphics, but it’s also voice acting, and robust storytelling that sets a good AAA game apart from indie games for me. Honestly even some of the indie games are getting pretty incredible graphics these days thanks to Unreal Engine. reply npinsker 17 hours agorootparentprev> I think if big game studios, rather than dumping their copious amounts of money into single, giant-scope games, dealt it out amongst a variety of smaller teams for smaller-scoped projects, they'd be way better off. This is what game publishers do, and many of them are struggling too. It’s harder than it seems to pick winning horses. (Though in this case, it may be partially because more and more skilled teams are opting to self-publish.) reply chucke1992 15 hours agorootparentprev> I think if big game studios, rather than dumping their copious amounts of money into single, giant-scope games, dealt it out amongst a variety of smaller teams for smaller-scoped projects, they'd be way better off. Big publishers tried and did not succeed much. EA, Square Enix, T2 with Private Division and so on. reply phone8675309 17 hours agorootparentprevI'd rather see studios make 100 games that each cost $2 million to make than one $200 million game. reply ProfMeowsworth 10 hours agorootparentWe have both of that, right now. reply chucke1992 15 hours agorootparentprevOh for sure. AAA games require too much effort and too much returns while indies can spend 1 year and 1 person and deliver hit being multiple time more profitable. reply Goronmon 17 hours agorootparentprevInsanely over-saturated but almost all AAA games are extremely derivative, stale, bland games with a coat of pretty graphics... People have been saying this for decades at this point. I'm not seeing it. Innovation is largely overrated. It can be a good thing, but the vast majority of games, whether AAA or indie, can't be truly innovative. And innovative doesn't translate directly to a game being enjoyable. Conversely, a game being \"derivative\" doesn't automatically make the game not fun to play. reply bart_spoon 17 hours agorootparentAgreed. In video games, \"innovation\" quickly becomes \"niche\". Microsoft actually has a wider variety of games and genres represented on the Xbox, many highly praised, but frequently gets lambasted for having no games because the the overwhelming majority of players aren't actually interested in them. Sony on the other hand is dominating, and yet its biggest titles are all somewhat similar to each other and none of them really do anything new or interesting, they simply have a lot of polish. reply dlachausse 17 hours agorootparentprevSpot on. I want Elder Scrolls 6 to basically be Skyrim in a stunning new location, with better graphics. Ditto for Forza Horizon 6 and the next installment of The Witcher. reply badsectoracula 8 hours agorootparentIf past history is any indication, TES6 will be to Skyrim fans what Skyrim was to Oblivion fans which was what Oblivion was to Morrowind fans. Daggerfall fans are split about Morrowind though and i'm not sure there are any Arena fans. reply dvngnt_ 15 hours agorootparentprevthe morrowind fans are turning in their graves reply margorczynski 14 hours agorootparentWell it got dumbed down and casualized. Don't even get me started what they did to Fallout... reply dlachausse 13 hours agorootparentTo be honest, I like my RPGs dumb and casual. Real life uses enough of my brain power and I’m trying to escape. reply mvdtnz 15 hours agorootparentprevFor me indie gaming is going through its own aggravating phase right now, but it seems most people aren't bothered by it. The quality of the games is better than ever, but every indie title now goes through Early Access, sometimes for several years. By the time the game is released the hype cycle has already finished. For people like me who play games just a couple of hours a week, I have no interest in playing an unfinished game. I have a library of games bigger than I could ever play and I will always skip the EA stuff. reply w1nst0nsm1th 11 hours agorootparentprevhttps://store.steampowered.com/app/1931770/Chants_of_Sennaar... Some independant studio or publishers have their fans base : Amanita Design, Playdead, Zachtronics, Devolver Digital, Annapurna Interractive are for me the folk to watch. reply Log_out_ 16 hours agorootparentprevEveryone waits for AAAi to arrive so that indy gaming can have that polished shell. reply Apocryphon 19 hours agoparentprevJust as with movies, it seems like we’re witnessing the death of mid-budget (AA) games. reply m12k 19 hours agorootparentConversely, the recently released surprise hit Helldivers 2 is a very prominent AA game has sold millions of copies and seems well poised to win a bunch of game of the year awards (recently concluded PSN account linking controversy notwithstanding). reply sharkweek 19 hours agorootparentSidebar: Helldivers II is not a genre I’d normally enjoy but I have absolutely fallen in love with the very rewarding gameplay loop and grind. I have never enjoyed spreading democracy more with my friends than dropping a perfectly targeted orbital strike on a bunch of unpatriotic bugs. reply danielbln 18 hours agorootparentSame here, third person coop extraction shooter? Bleh, not my cup of tea. Helldivers 2 though? 70+ hours and counting, it's just so much damn fun, with friend or with randoms. It really is the sum of it parts, and every match is different in its own ways. An amazing game, is what it is. reply highwaylights 18 hours agorootparentprevRemember: If an existing inhabitant attempts diplomacy we mustn't believe their lies. reply phone8675309 17 hours agorootparentprevHelldivers II is one of the few games of its type that it feels good to mess around in. Lately my friend group has been playing on mid level difficulty to grind out common and rare samples, and there's nothing funnier than cracking a joke about something and then that person hitting you with a 500 kg bomb and making your body ragdoll across the map. There's just something about messing around in it and being goofy that feels good that doesn't in, say, Vermintide or Darktide. reply philipov 18 hours agorootparentprevOh yeah, thanks for reminding me to refund that game, since I can't run it on geforce now, and it has denuvo so I won't install it on my own computer. reply ryanmcbride 19 hours agorootparentprevThat's been reversed this time but it won't be next time. Next time sony does this the outcry won't be as bad. It's the ratcheting effect of corporate greed. reply rdedev 19 hours agorootparentprevA regression to the mean where mean is just whatever makes most money. I can understand if they are at the risk of going bankrupt but this seems so premature reply iLoveOncall 19 hours agorootparentprevHonestly I think it might make sense. I can't remember a single AA game that was great. Actually, I can't remember a single AA game other than the ones I remember because of how bad they were. I think AA in games has, for a long time, meant \"We want to do a AAA but don't have the money or time\" and this can only end in disaster. reply glenstein 19 hours agorootparent>I can't remember a single AA game that was great I'm always skeptical of \"I can't remember ____\" as an assessment of any given historical record because, well, the average person just doesn't remember anything. Which is all well and good, you have no obligation to be ready for a pop-quiz, but snapshot moment of free-association is just not a reliable stand in for the actual record. I actually couldn't think of any AA titles off the top of my head either, but after Googling and GPT'ing a bit I came up with: Hellblade, Plague Tale, Hades, Outer Wilds, Control, Metro, Outer Worlds, Shadow Warrior 2, etc. plus the numerous others listed by other commenters. My point though is that it's fine not to remember, but that should never be our acid test for what does or doesn't exist in the historical record. reply ToucanLoucan 18 hours agorootparentAs an aside of this discussion: I do not get people who like Outer Worlds. I am a huge fan of New Vegas, I was so pumped for that game, and it was so, so bland. The combat damn near put me to sleep, the writing was atrociously heavy-handed and made me think the authors thought I was a complete fucking idiot (and I agree with them!), and the moment-to-moment gameplay was just thoroughly, thoroughly unsatisfying. I know it has a loving if smaller community and man, I wanted to love it, but I just could not. I have hope for the sequel and will definitely play it if not day one, close to it, but yeah. Outer Worlds was one of my most disappointing games of all time. reply Dystakruul 17 hours agorootparentjust a note for inattentive readers: Outer Wilds from GP's comment is a very different game compared to Outer Worlds which the parent comment mentions. (it doesn't help that they were both released in the same year, 2019) Outer Wilds: action-adventure, open world mystery game with puzzles Outer Worlds: action role-playing game, open world first-person-shooter (similar to the Fallout games) reply hombre_fatal 16 hours agorootparentI get them confused all the time and you'd be in for a ride if you got Outer Wilds while expecting Outer Worlds, but the (G)GP actually mentioned both of them. reply glenstein 16 hours agorootparentprevI mentioned both in my comment. I spaced them out in my list on purpose although perhaps that contributed to confusion. There indeed entirely distinct games, but you can make a case that both fall into the AA category. reply jicea 19 hours agorootparentprevHousemarque hadn't released any AAA games before launching Returnal on PS5, which is, in my opinion, still one of the best games on this platform. Smaller studios can innovate on gameplay and stories before creating a hit. Another example is FromSoftware. They kept iterating on their games going from KingFields to Demon Souls, Dark Souls, etc...You can't have Elden Ring without all this earned experience. reply gcr 17 hours agorootparentFromSoft is putting out some really interesting experimental bangers too. If you haven't tried the new Armored Core, I highly recommend it -- it's a great bridge from a beloved-but-niche genre (mech games) toward the mainstream. It's small, focused to a point, tells the story it wants to tell and gets out of your way. I'm still thinking about it 9 months later... Sounds like this sort of risky niche title would earn a studio closure if it came from Microsoft's corner. reply jncfhnb 18 hours agorootparentprevElden ring was the shitty mainstream version of dark souls. The open world was just a big forgettable grind of annoying mini dungeons that you probably had to look up and then didn’t enjoy, collectible items that you were never going to use 95% of the time, shitty quests that were difficult to engage in without external guides, and powerful buffs that were very useful but guarded by enemies that don’t scale; leading most players to follow the main quest to the first boss and then just run around the entire map exploring for the warp points/permanent buffs ignoring every enemy possible and then basically forgetting it was an open world game and just hitting the highlights. The memorable parts of the game are the bosses which are generally cool but very gimmicky, and the legacy dungeons. Which is… the dark souls bits. The open world was stinky garbage that made the game much worse. reply iLoveOncall 1 hour agorootparentprev> Housemarque hadn't released any AAA games before launching Returnal on PS5, which is, in my opinion, still one of the best games on this platform. Smaller studios can innovate on gameplay and stories before creating a hit. Yeah I never said the contrary. Actually you see this quite commonly with small unknown studios that release stuff like educative or mobile games and suddenly are handed a big project. > Another example is FromSoftware. Please, they've been releasing AAA since Demon Souls. They're definitely not AA games. reply jonhohle 19 hours agorootparentprevIt depends what type of game you like. There are a million AA RPGs, Metroidvanias, etc. that are enjoyable. Both RoboCop: Rogue City and Terminator: Resistance are good, budget games. Looking through recent games: Dead Cells, Bloodstained, anything from Bitmap Bureau, a lot of Way Forward titles, Altus, etc. reply aamoyg 18 hours agorootparentRobocop Rogue City was amazing. If you max out the psychology, you start giving back sassy replies to everyone and become sass cop it's great. They really built on the Terminator Engine they used earlier. reply mrangle 18 hours agorootparentprevThe list of great non AAA games would be so long, here, as to be obnoxious. A lot of game greatness is found in the explicit avoidance of AAA conventions, like extended cut scenes. Many AAA games are masterpieces, and yet few to no no AAA games are 2D platformers for example. Which is one of the all time great formats for pure fun. reply ryanmcbride 19 hours agorootparentprevIs Yakuza AA or AAA? Because every Yakuza game is great and I don't think they have that big of a budget. reply ClimaxGravely 18 hours agorootparentI've worked on some titles very similar to Yakuza and I keep espousing the way they develop their titles. I don't know for sure because I haven't worked at the studio but it really seems like they've streamlined their development to keep costs down and allow them to develop quickly via asset reuse and resisting major engine upgrades across titles in favor of a slower, more focused pace. Often in my experience in the west we tend to re-author assets, do major engine upgrades or re-implement gameplay systems across sequels when we could have iterated on existing systems and use the time we saved to work on new stuff. reply failuser 19 hours agorootparentprevHow many As does Psychonauts have? reply lelandbatey 19 hours agorootparentprevI think many of my most favorite games have been AA games. Though \"what counts as AA\" is probably a big question. Hi-Fi Rush, was a delightful game that earned every $ I spent, but didn't feel like a AAA title. Hades is a delightful game that earned every penny but which wasn't a \"AAA\" title. I wouldn't call either games \"indie\", as they both had dozens of people on the teams that made them. But I'd also guess that both games were still made by very different size teams (e.g. 2 dozen vs 5 dozen). reply iLoveOncall 19 hours agorootparentI get that \"indie\" is often viewed as the 3rd option alongside AAA and AA but I don't think that's the right definition. For me AAA and AA is about scope of the project. The 3rd option is \"small\", not \"indie\". Hades (I don't know Hi-Fi Rush) is by all means a small game, regardless of how many developers worked on it. Same for Minecraft, or many of the games that other commentors posted. You want a good measure? Check the price. AAA are $60, AA are around $40 and small games are below. PS: out of topic but I just saw that Hades 2 is out in early access. reply ang_cire 18 hours agorootparentSadly, people use indie to mean small, rather than it's actual meaning of \"independent of a publisher\", and it's one of those things that I've nearly given up on fighting. It's even more sad because the only reason that change happened was big publishers wanting in on the success of indie games as a label and concept, but by definition being excluded, so they pushed their own definition, and people gobbled up their corporate cooption. reply masklinn 18 hours agorootparent> Sadly, people use indie to mean small, rather than it's actual meaning of \"independent of a publisher\" That is way too imprecise a description for it to possibly be considered correct. Indie is about financial and creative independence aka the publisher does not drive the game. Many indies still go through publishers because they don't have the means or knowledge to handle distribution. This was even more so back when you had to distribute via physical media, but they start looking for publishers once the game is done or in good shape. For instance back when they built Bastion SuperGiant had just 7 people and it was entirely self-funded. But they went to WB for publishing, mainly to ensure getting it on XBLA would not be too much of a hassle. reply ambichook 10 hours agorootparentindie is literally a shortening of independent though, the term i believe originated from musicians who would release their music without a label to publish them reply everforward 18 hours agorootparentprevIsn’t AAA vs AA about the funding of the project, and AA vs indie is about the publisher? I don’t care for scope as part of this because scope is so heavily influenced by the type of game. Eg Call of Duty is the poster child for AAA games and has a pretty unimpressive scope compared to virtually any RPG. Even indie RPGs tend to have a broader scope; CoD has basically nothing outside of combat mechanics. Then there’s weird questions about what counts as scope too. Tabletop Simulator has a much broader scope than MTG Arena, but Arena is far closer to AAA or AA. reply iLoveOncall 2 hours agorootparentI specifically said scope of the \"project\" instead of the \"game\". You're right that CoD is simple, but it's still a massive project where most areas (graphics, networking, game engine, etc.) are infinitely more developed than a game like Stardew Valley which is a much broader experience _as a player_. reply aaomidi 19 hours agorootparentprevDeep rock galactic Dave the diver Spiritfairer Frostpunk Should i keep going? reply masklinn 18 hours agorootparentDave the diver is definitely not an \"AA\" game. It's so indie looking it was nominated for multiple indie awards (even if it's not an actual indie game). DRG is not AA either, according to its homepage GhostShip has 32 people 4 years after launching it. Same for Spiritfairer, it was built by a team of 16. reply wlesieutre 17 hours agorootparentWasn’t the case earlier in development since they didn’t have big financial backing, Ghost Ship only grew after DRG had sales > Deep Rock Galactic aired a trailer at E3 2017, then the game had a huge bump after its Steam Early Access and Xbox Game Preview launch in February 2018. Its Early Access didn't skyrocket the game \"insanely high\" like titles such as Valheim, but Pedersen said it was solid enough to know they had a success. At that time there were only 12 employees, and everyone was contracted \"because we didn't know if we'd have money the next month.\" https://gamerant.com/deep-rock-galactic-interview-ghost-ship... reply masklinn 17 hours agorootparent> Ghost Ship only grew after DRG had sales Well yes that's what I said, I didn't find what numbers they had at the time but I indicated that they had 32 people (which generally falls short of AA in the first place) 4 years after launching a successful game, so they'd most likely have had even less before then. reply ClimaxGravely 17 hours agorootparentprevI personally would consider 10-40 person teams to potentially qualify as AA. Often we also outsource code/art so the team size can sometimes be misleading. I suppose I would go by the budget. Maybe 5-10million+ IMO. It also kind of depends how they spend the money. EDIT : After some further reflection, From personal experience I'd consider a AA game one where everyone on the dev team knows each other fairly well. AAA games get so large that you don't end up knowing everyone super well by the end of the project. reply masklinn 16 hours agorootparent> I suppose I would go by the budget. Maybe 5-10million+ IMO. Which is not really useful, because we usually don't have budgets. Team size x development time might be an approximation for it, but if you assume an average salary of 80k and a development time of 30 months, by your reckoning AA is a team of 50... which is basically the low end of what's normally considered an AA team size. > From personal experience I'd consider a AA game one where everyone on the dev team knows each other fairly well. AAA games get so large Team Meat is just two people, four if you include the producer and the composer, I would very much assume they knew each other fairly well, but there's no meaningful interpretation of AA where Super Meat Boy is an AA game. reply ClimaxGravely 7 hours agorootparent> Which is not really useful, because we usually don't have budgets. The majority of the AAA/AA projects I've worked on have budgets. I'm struggling to think of a project that didn't have a budget. > Team Meat is just two people, four if you include the producer and the composer, I would very much assume they knew each other fairly well, but there's no meaningful interpretation of AA where Super Meat Boy is an AA game. Of course you would expect a small indie team of 2-4 to know each other. I'm saying that once you hit AAA size teams that no longer becomes feasible. reply masklinn 1 hour agorootparent> The majority of the AAA/AA projects I've worked on have budgets. I'm struggling to think of a project that didn't have a budget. We don't have budgets, as in the people not involved in the project don't have any access to the projects so have no way to \"rate\" on that metric. > Of course you would expect a small indie team of 2-4 to know each other. I'm saying that once you hit AAA size teams that no longer becomes feasible. How is that relevant? This here discussion is about the lower limit of AA, not the higher one. reply phone8675309 17 hours agorootparentprevWere you gaming back in the PlayStation 2 / original Xbox days? There were a TON of AA games in that era. Most of those studios have been bought up by the big studios that peddle AAA (and in some cases, claim to peddle AAAA) garbage. reply beeboobaa3 19 hours agorootparentprev\"I don't like this so it should stop existing\" reply iLoveOncall 11 hours agorootparentThat's absolutely not what I said though, just that it makes sense. Personally I wish for as many games as possible to exist, the more options the better, even if some are bad. reply 127 19 hours agoparentprevI don't really worry about it because indies are making better games than ever. reply troupo 19 hours agorootparentNot for XBox though. Mine has turned into an expensive paperweight we occasionally use to play the same 2-3 local co-op games we played 4 years ago. reply somenameforme 18 hours agorootparentIt's pretty easy to convert your TV to a PC console. Just grab a HDMI/DisplayPort cable and some USB cables for controllers. Wire your TV up, start Steam Big Picture mode, done. You now have a console with a zillion awesome games in the exact same place you had the console connected. Here's [1] the Steam page for split screen games. Currently there are 934. Can't go wrong with Earth Defense Force! And no you don't need a \"gaming PC.\" That term doesn't even make any sense in modern times, because if you have a computer from within the past decade or so, you can run the overwhelming majority of games with no problem. And I mean that literally - for instance GTA V requires an AMD HD 4870 card. That card was released in 2008! [1] - https://store.steampowered.com/tags/en/Split%20Screen reply hombre_fatal 16 hours agorootparentThanks. I had no idea you could play split screen on Steam games. I'll definitely set this up. reply SlowRobotAhead 18 hours agorootparentprevI'd really love cables from my office to my livingroom. reply somenameforme 18 hours agorootparentNot sure if you're being snark, like referring to your office at work, but we did this and it works pretty well. We ran the cables along the edges of wall/floor using little cable clamps. They're pretty much out of sight and it works great. Depending on your house you could also run them through the walls/attic/etc. There's a lot of ways to make it work without going bachelor pad. reply gopher_space 13 hours agorootparentprevI've always been surprised by the lack of cable tracking options in home electronics stores. Some installations can be done with a drill and a box knife. reply schlauerfox 16 hours agorootparentprevWiFi 6 reaches absurd speeds, no cables required. reply troupo 13 hours agorootparentYou can't HDMI/DisplayPort over Wifi reply phone8675309 15 hours agorootparentprevDo you have a smart TV or can you fit a streaming device or mini PC into your setup? If so, Steam Remote Play might fit the bill: https://store.steampowered.com/remoteplay Steam used to sell a device called Steam Link for this very purpose, but now they just release apps for common platforms: https://en.wikipedia.org/wiki/Steam_Link reply wredue 18 hours agorootparentprevI mean. The writing was on the wall there TBH. Microsoft has been consistently failing to support and engage game developers since early in the 360 cycle, then the doublespeak crook Spencer took over, and it got even worse. This whole shitting on AA was bound to happen with Xbox fanboys saying that “it is good that Microsoft is buying studios to give Sony a taste of their own medicine!” Except MS is still not giving Xbox gamers a taste of Sony medicine, because Sony gets out there and gives games chances, funds loads of “trial” titles, supports developers and studios, etc. I don’t like that Sony just does this better, as I am a PC gamer and Sonys PC game is awful. reply utensil4778 18 hours agoparentprevConsumers have been signaling for quite some time that they're perfectly content buying Skyrim and Counterstrike every six months forever. Why even bother producing anything at all when you can just put a fresh coat of lipstick on the same pig and sell it all over again? reply vel0city 18 hours agorootparentCondition Zero and Source released in 2004. Global Offensive released in 2012, eight years later. Counterstrike 2 released in September 2023, eleven years later. Counterstrike isn't the game to point to for re-releasing the same thing over and over. reply utensil4778 18 hours agorootparentCounternite, fort of duty, whatever. I care so little for this class of game that I can't be bothered to spend the brain space to keep them separate. Not saying they're good or bad, I'm just not interested in the slightest. reply eropple 16 hours agorootparentIf you can't be bothered to understand the differences, then criticizing consumers for perceiving those differences and being willing to buy them is a real interesting spot to land on. I don't play those games, either, but I at least understand the proposition that they present. reply vel0city 17 hours agorootparentprevFortnite has had a single release and is a free to play game. Once again, if you're going to point out games that just come out with a new version to sell it again, you're failing to show accurate examples. reply Apocryphon 17 hours agorootparentBethesda is also notorious for taking forever to make Elder Scrolls games. Skyrim came out in 2011 and has not had a proper sequel since. First-person RPG also isn't exactly a glutted sub-genre. Neither of the examples fit the phenomenon they're talking about! reply Night_Thastus 17 hours agorootparentI wouldn't say notorious. It's only become that way with the latest games. Morrowind to Oblivion was ~5 years. Oblivion to Skyrim was ~5 years. Fallout New Vegas (Obsidian) to FO4 (BGS) was 5 years. It's really only since then that game development times have skyrocketed as much as they have. reply Apocryphon 16 hours agorootparentThat's fair. The ancestral post's claim of \"every six months\" is still ludicrous, just not as cartoonishly so. It's not like Bethesda RPGs are licensed pro sports games or Assassin's Creed. reply chownie 17 hours agorootparentprevThe classic description for this is \"being out of touch\" with the given media. Not sure if it's something to be proud of. reply anonbanker 16 hours agorootparentThere is nothing more freeing than being \"out of touch\" with media. I got rid of my TV in 2000, and my life improved dramatically. I stopped playing AAA in 2009, and my life improved dramatically. Indie games, specifically the Godot dev scene, is where the real innovation in gaming is happening. When AAA implodes, you'll learn what you were missing all along. reply Vt71fcAqt7 15 hours agorootparentIt just sounds like you mainly like 2d platformers and open source (which is fine). But Godot isn't nearly the best engine to use for 3D, and arguably may not even the best of the open soucre engines (for example O3DE may be better, although almost no one is using it right now). Most 3D indie devs are using Unity and many are switching to Unreal now just like AAA (for example Palwolrd). I think \"out of touch\" here means making false claims about something you (not you specifically but GP) don't even care to know about. reply Apocryphon 18 hours agorootparentprevThose are all different genres of FPS reply Vt71fcAqt7 17 hours agorootparentprevIf you aren't interested enough to even know what you're talking about why do you make such specific claims about it? Fortnite is completly different from counterstrike. For example counterstrike is a team based game and Fortnite can be played single player. And counterstrike has not had a new release in which current players must buy the new game for 11 years much less every six months. Also, you are wrong when you say you are \"not saying they're good or bad\" when you said making the supposed re-releases of these games is like putting a \"fresh coat of lipstick on the same pig.\" reply gizmo385 18 hours agorootparentprevI get the point you’re trying to make, but Counterstrike is a free to play game. You might be thinking of Call of Duty? reply utensil4778 18 hours agorootparentProbably. I'm so thoroughly disinterested in this class of games that I can't be bothered to remember the difference. reply raincole 19 hours agoparentprevConsidering how many games are getting released each month, I am having a hard time comprehending what this \"the gap\" is referring to. reply filleduchaos 17 hours agorootparentThe vast majority of games that get released each month are straight-up garbage. And for the minority that aren't, I'd hope that people in a YC-affiliated forum of all places would see that a product getting released does not equal financial viability. reply willis936 17 hours agoparentprevI really don't understand why they would shutter Tango, a small / low cost house producing influential art that people actually want, while behemoths that burn cash and produce soulless products that no one wants survive. Shut down Bethesda, they're the ones with awful gameplay and writing. Don't shut down the darlings. reply dragonwriter 17 hours agorootparent“Influential art” vs “soulless products” tends to map pretty well to “niche products that return poorly compared to costs to produce while earning critical accolades” vs. “reliable cookie-cutter moneymakers”, and Microsoft isn't in the business to win critical awards for stockholders, but to make money for them. reply willis936 17 hours agorootparentI'm not arguing philanthropy. Bethesda makes a bad product that that costs a fortune to generate. It's shrunk the size of its market and its time is limited. Tango's business model is sustainable, Bethesda's is not. reply luqtas 19 hours agoparentprevwhat do you mean by \"just\"? millions if not billions of USD profit @ the 3 titles cited between the brackets? people play GTA V till these days, even Skyrim... either the way, hope it paves the way for more small studios titles... reply kkukshtel 17 hours agoparentprevThe economics of traditional AAA game development just don't make sense, and the market conditions right now are just acting as a forcing function to expedite their (current) collapse. You have insanely high development costs (50-100m on the low end to 200m+ on the high end) where the _only_ opportunity for return is to release a hit or polish the game to a hit (read, more $$$), but even being able to predict what will or won't be a hit is near impossible. So you have high burn on teams for long dev cycles (2-3 years+) that can't even really time the market because of their slow releases, with audience expectations for what a title means also insanely high, and also that their are both very good free options like Fortnite and large discounted backlogs of \"really good games\" that you're also fighting against. You also have weird calculus now where you're fighting against your own bets on live service games — spending 100m on expanding GTAV some more is likely a better return than working on GTA6. In the email announcing this from Microsoft's own words, you can see this: \"In 2024 alone we have Starfield Shattered Space, Fallout 76 Skyline Valley, Indiana Jones and The Great Circle, and The Elder Scrolls Online’s Golden Road. \" 3/4 of those titles are old games that are live services, where it's a better investment and dev cost to pump engaged players than build new audiences. It's VERY hard to beat a 5% (even more for an MS-sized deposit) return on a savings account, so closing studios that made Good Games isn't about the games at all, it's just looking at the balance sheet. Everyone always knew they were creating on borrowed time, and now that time is unfortunately up. The solution of this is to not let private companies dictate cultural production for a nation, but the US is piss poor at arts funding and all our billionaires want to squirrel away wealth overseas rather than building libraries, museums, or cultural production funds. reply Gazoche 9 hours agoparentprevI don't understand it either. Maybe there's more going on behind the scenes that we don't know about, but on the face of it seems like a really poor decision. Large publishers keep reiterating the importance of successful IPs these days, and Hi-Fi Rush was like lightning in a bottle. Here Microsoft had a new IP with critical acclaim, suitable for a large audience, and ripe for a sequel. You'd think they would cling to it for dear life, especially given how their other IPs are doing (Halo, Redfall, Starfield...) Closing the studio doesn't necessarily mean they're ditching the IP, but it doesn't bode well. reply glenstein 19 hours agoparentprevYeah, I have a hard time understanding the push to consolidate everything into a small set of core titles. Consider private label brands on Amazon, which at least maintain numerous distinct brand identites focusing on different categories. Having a portfolio of actually distinct companies with unique personalities and signature approaches to design and gameplay is exactly what you want if you are trying to maintain a thriving ecosystem. reply jonhohle 19 hours agorootparentMoney? When 90% of revenue comes from yearly sports, FPS, etc. refreshes, it’s easy for middle managers to get the idea that cutting the fat will increase margins. Unwittingly, they are missing the opportunity to find that new hit franchise that needs to be tested and refined outside of the mainstream. reply pteraspidomorph 16 hours agorootparentprev> consolidate everything into a small set of core titles This already happened in the early 2000s. If you were around back then, you might remember how everything was sequels and rehashes for a while. Diversity of ideas returned to the industry only after it became practicable to publish and monetize indie games (post-Braid). Microsoft and Embracer recently bought the whole industry. Now they might be about to light a match and set fire to the whole thing. OK, but fortunately, all the talented passionate people with the ideas and drive to create new things still exist in the world. I believe many players will find their way back to them, no matter how sufficent \"garbage\" is for the majority of people. If milking the uncaring baseline consumer was all that mattered to videogame creatives, they'd all be making ad-driven smartphone shovelware. reply dvngnt_ 15 hours agorootparentthe early 2000's was the ps2 which is the best selling console with the highest number of games shipped. i can't remember a time when there was so many different games or high quality releasing so often. there were sequels, but they was dropping every year while being improvements. we got 3 GTA games on the ps2; gta v got three playstation consoles reply busterarm 19 hours agoparentprevThey also developed The Evil Within games and Ghostwire: Tokyo, but their founder and CEO left the company last year to found a new studio. The writing was on the wall honestly. reply jarsin 18 hours agoparentprevHaven't you heard and seen the latest VC craze? AI is going to autogenerate millions of games, movies, and books all with the click of a button. reply datavirtue 13 hours agorootparentYeah, I'm hoping to find some type of image AI service that can produce passable art assets for 2D game development. Just good enough to do POC work. I have started a few gaming projects over the years and always ran into a brick wall needing assets. reply MichaelZuo 19 hours agoparentprevA single big live service game like Fortnite captures the player time that would have otherwise gone into a dozen AA games, per year. So there is no gap in available player time, as it's impossible for there to be more than 24 hours of demand per day. reply fidotron 19 hours agoprevWhat is extraordinary about the Microsoft games unit and Xbox is how immune their senior staff are to the repercussions of their bad decisions. They're certainly not taking responsibility for the failed gamepass experiment, trashing the Xbox brand, or the acquisitions they now regret since successfully closing Activision. Failing upwards has never been so conspicuously obvious as it is in modern corporate America thanks to the pervasive use of social media. reply ajmurmann 19 hours agoparentIn what regard has gamepass failed? I keep hearing nothing but excitement about it? Does it lose a lot of money? Similarly, how did they trash the Xbox brand? I've always been a PlayStation or Nintendo user so my view is quite tainted here. reply fidotron 19 hours agorootparentI refer to my other answer about gamepass. For the Xbox brand they have failed to release quality versions of every major Xbox franchise bar Forza Horizon (Halo Infinite, for example), and this mismanagement has been ongoing for so long the sales figures of the Series consoles are dire. (And the Series X is not bad by any stretch). Now they are having to release their games on their major competitor, the PS5, making the point of buying into the Xbox ecosystem . . . what exactly? And to emphasise here Sony are not exactly doing stupidly well with PS5 software and support, they just aren't actively screwing it up completely. reply falcor84 16 hours agorootparent> For the Xbox brand they have failed to release quality versions of every major Xbox franchise bar Forza Horizon They did also release Microsoft Flight Simulator in 2020, which while also available on PC was in my opinion a massive win for Xbox reply magnetowasright 9 hours agorootparentprevGive them some credit - Forza Horizon 5 was extremely buggy on release and for months after! Many non-race goals/activities, a significant part of gameplay were totally broken. I don't know if it ever got better? It could have been a quality game but I gave up. You're spot on. It's just funny to me that the lone quality release was also botched imo. reply mynameisvlad 18 hours agorootparentprevThey’ve already expressly said they’re shifting from a hardware-focused business to a software-focused one. Did you miss that and think they’re just bad at business? Hardware has always been a loss leader used to sell games, nothing more. If they can sell the games just as well on their “competitiors platforms” then why bother spending time and exorbitant amounts of money building your own hardware? Exiting the hardware space looks like a loss only if you don’t understand the dynamics of the game industry. reply fidotron 18 hours agorootparentI've actually worked at a high enough level in the games industry to know about the non-public dynamics, and that includes working with some of the people involved in this story. > Hardware has always been a loss leader used to sell games, nothing more. If they can sell the games just as well on their “competitiors platforms” then why bother spending time and exorbitant amounts of money building your own hardware? By this logic Sony and Nintendo are completely wasting their time making hardware, yet they persist in doing so. Does this make them bad at business? reply mynameisvlad 18 hours agorootparent> Does this make them bad at business? No, and nobody said as much either. It just means they get enough out of other sales (including an increase from hardware) that they have decided hardware is still worth it. Microsoft has decided otherwise. They’re both perfectly fine decisions. > I've actually worked at a high enough level in the games industry to know about the non-public dynamics, and that includes working with some of the people involved in this story. Then you should very well know how hardware has always been a loss leader intended to drive people to exclusives and first party services. It shouldn’t be a surprise that once a company finds just as successful ways of driving the same engagement, they no longer need hardware. reply fidotron 18 hours agorootparent> Then you should very well know how hardware has always been a loss leader intended to drive people to exclusives and first party services. Just like the number one console, the Switch? Sold at a loss? To drive people to play the exclusives? TBH I won't engage further in this line. reply mynameisvlad 17 hours agorootparent> Sold at a loss? I highly doubt they have recouped the R&D costs of making the console and platform at $40-80 per unit. So, yes, at a loss. Looking at the BOM and calling it a day is the shallowest way to evaluate if something is sold at a gain or loss. > To drive people to play the exclusives? Nintendo? The major game studio that famously only releases their first party titles on their own platform? Yeah I'm pretty sure that's the reason their entire hardware lineup exists and has ever existed. > TBH I won't engage further in this line. Not that you've been engaging in anything but dismissals without substance in this entire post. reply guipsp 18 hours agorootparentprevBoth Sony and Nintendo have expanded outside their traditional exclusive hardware. I don't think the parent is saying that doing hardware is useless, but rather that the writing is on the wall to some extent for pure console exclusivity. reply kaibee 17 hours agorootparentIt ain't the PS3 era where consoles are some radically different hardware stack. The latest gen, except for nintendo, are basically just x86-64 gaming PCs. reply hibikir 18 hours agorootparentprevThe hardware might be a loss leader, but the manufacturer controls access to the hardware, so every game developer for the hardware is giving them a portion of the sale price. They were Steam before Steam existed. Sony didn't make most of their money on first party games, but on third party fees. The first party games, along with the loss leaders, were there to bootstrap getting consumers to buy the console. The reason to abandon hardware is when you aren't going to get many third party sales regardless, because you are getting destroyed: In large part because your first party games and the console experience aren't attracting enough players. Microsoft hasn't whiffed with every game they released in the last 5 years, but their batting average has been very low. If your games aren't selling, it makes sense to abandon hardware, but it's even more important to downsize your studios that are releasing underperforming games. reply bogwog 18 hours agorootparentprevThat's not true. Hardware gives them the ability to create a monopoly on game publishing/distribution, which is why console games have always been so expensive. The platform holder can charge high fees to developers and demand high prices from consumers. That's nothing unique to the game industry. Microsoft has said they're trying to focus more on software, but that focus is on gamepass not games themselves. They want that monopolized platform without the burden of having to sell Xboxes (which consumers have rejected time and time again), which obviously would be very lucrative for them. It's why they've been throwing so much money at devs to entice them to put their games on gamepass, and why they spent such a ludicrous amount of money on Activision despite the Xbox business being such a dud. They believe games are destined to become like video streaming industry, and want to be the Netflix for games. Now I'm talking out of my ass here, but I think there might be some kind of internal metric at Microsoft that rewards execs that chase monopolization. It's either that, or they're really so incompetent over there that nobody has noticed how bad Xbox leadership is at their jobs. The peak of Xbox success was the 360, and I think that was mostly because Sony got the pricing completely wrong for the PS3 at launch (and even then, lifetime sales of PS3 surpassed the 360) reply jarsin 17 hours agorootparent360 + Gears of War was the best time in gaming. Look what they have done to Gears as well. Nobody even knows or hears anything about it anymore. reply bogwog 15 hours agorootparentGears 5 came out around 5 years ago. Considering the series has consistently rated very well, I doubt they killed it. They're probably just doing the modern AAA thing of spending a decade and a half working on a bloated game with unlimited budget. I bet we'll see one of those logo drops for Gears 6 soon (next showcase is in June), the game will release in a buggy/broken state with a flawless in-game store, the studio will get shut down, and then they'll announce the new Elder Scrolls game while everyone is pissed. reply Xirgil 15 hours agorootparentprevMicrosoft didn't kill Gears of War out of ineptitude or malice. It's just that consumers don't want to play cover shooters anymore. There's no market for the game's core identity. People's tastes have swung hard in the opposite direction, they want movement shooters. reply kipchak 18 hours agorootparentprevHardware is not always a loss leader, with Nintendo hardware (generally, Wii U was an exception) being the largest example. Estimates were about $250[1] for production for a $300 unit at launch if I remember right, so maybe $10 profit. Selling on someone else's hardware means they take your 30% or so storefront fee. For example selling your studio's Elder Scrolls title on Steam or PlayStation nets you $42 instead of $60. [1]https://xtech-nikkei-com.translate.goog/dm/atcl/column/15/36... reply monetus 17 hours agorootparentprevRIP SEGA - can't say that sticking with their arcade minded strategy paid off more than if they had stuck with hardware, but I am skeptical. reply etempleton 17 hours agorootparentprevI think it is popular but not growing as fast as Microsoft would have hoped. When I talk to some of my friends about it they don't seem to realize how deep the catalog is and just buy games that they could have played on GamePass, but most of those games are third party games that happen to be on GamePass. The Xbox brand issue at this point is a lack of quality first party games. They can never seem to nail a release. Even when a game is pretty good there is a caveat. - Halo Infinite was pretty good, but buggy and they struggled to release new content. - Starfield is Bethesda's most ambitious game yet with the best combat, graphics and polish from the studio to date, but the exploration loop that defines their games is broken by interplanetary travel. It is still pretty good, but just not what it should have been. - Forza Motorsport launched buggy and while technically proficient is perhaps of the most joyless games I have ever played. - Redfall was hyped as a first party release and is absolutely mid I do think that both the Xbox and PlayStation brands have put themselves in strange positions strategically. People buy consoles to play certain games and if you know that you can play those games on PC (eventually) why would you ever buy a PlayStation or Xbox? reply bombcar 19 hours agorootparentprevFrom what I understand gamepass has heavily undercut actual game sales (why buy a game for $70 when it'll probably be on gamepass at some point, and most games are a month of play or so). reply anbotero 17 hours agorootparentWith games costing $70 now, I'm thinking it several times now before shelling out the money, specially for those AAA-valued bad/low-quality games, Game Pass or not. Argument is still valid, since I’ve not bought AAA games for more than two years now, only indies or AA, even if I already beat them with Game Pass. reply hypertexthero 16 hours agorootparentprevI don’t think it failed yet, but I’m not renewing my subscription, gifted to me by a friend, for the following reasons: 1. Too many game choices without having to think about spending money for each one is very expensive when the currency is time. 2. Video games will never be Microsoft’s primary business, while video game players and makers are Valve’s. This is why Steam is so much better than Xbox/PC Game Pass. 3. It’s bloody difficult to take screenshots! reply ErneX 18 hours agorootparentprevXbox was at their prime on the 360 era, they sold almost as many consoles as Sony did PS3. Ever since then they are selling half of PlayStation and their current consoles are tracking behind the previous generation in sales launch-aligned. Even with GamePass which is undeniably great value for gamers. Seems they are now seriously considering becoming a third party publisher and release all their games or at least most on PlayStation and Switch, which honestly makes sense considering the amount of new development studios they acquired that used to release their games for every platform available, there are not enough Xbox consoles with paying customer to sustain all those studios I think. reply CSMastermind 19 hours agoparentprevI don't play games at all so I'm very out of the loop but gamepass was a failure? I have friends at Microsoft who worked on it and they all seemed to think it was going well last I checked (about a year ago). reply fidotron 19 hours agorootparent> gamepass was a failure? I'm amazed that this is a question. Gamepass is essentially so low cost as to be an incredibly costly giveaway, only it has proven that this devalues everything that touches it. For context, Hifi Rush had 3 million players on Gamepass last August, and today they shut the studio. So even when they get a break out hit they cannot justify keeping the studios around to try and do another; that's not a success. reply coffeeindex 18 hours agorootparentUntil I see actual numbers on Gamepass I won’t believe that it was a failure. It isn’t a direct comparison of “gamers are spending $10 instead of $70”. People who would not buy a game that just came out will willingly pay for Gamepass every month. Gamepass also exists on PC, so Xbox is dipping into a user base which otherwise might not buy many products from them. Yes they almost definitely make less money from some people, but they might make that back and more from people who otherwise would not be buying their games. It’s hard to use your intuition here without some numbers reply rrix2 17 hours agorootparentAnd yet here we are where a fantastically successful mid budget game published on to gamepass by a first party studio leading to huge excitement in gamepass is not enough to justify the continued existence of the studio which produced it. reply wredue 16 hours agorootparentprevGame pass was an obvious piece of gamer bait. It is current high value for consumers, but that’s all it is, and in cases where MA is just a third party vendor to the publishers, high value for consumers is bad for the publishers. If it is ever deemed a success, that value will diminish extremely fast. reply whoknowsidont 18 hours agorootparentprev>they all seemed to think it was going well last I checked ...based on what? reply BlueTemplar 19 hours agoparentprev'member Games For Windows ~~Live~~ Dead ? reply VyseofArcadia 19 hours agoprev\"Thanks for the widely acclaimed mid-budget surprise hit, Tango. Now polish up your resumes, pack up your stuff, and get out.\" It's like AAA publishers have no notion of a game studio as an organic thing that can grow. It's all just pieces on a board. reply nercury 19 hours agoparentIt's like destroying a factory when the chips it produces do not sell. reply willis936 17 hours agorootparentBut Hi-Fi Rush did well and invigorated the market. Starfield cost an order of magnitude more and only had the opposite effect. It's like shutting down your successful competitor for your screwup because you're their boss. Almost like the result of an anticompetitive practice. reply hackernewds 9 hours agorootparentMortal Kombat and FIFA sell less but general more profit. Profit margins reply ppseafield 19 hours agorootparentprevWith Tango/Hi-Fi Rush, MS decided to make it free with Game Pass at launch, which obviously hurt its sales. If you sell chips but charge people $1 for two weeks for unlimited chips, then just $10/mo for unlimited chips, you might be disappointed with direct chip sales. reply VyseofArcadia 19 hours agorootparentprevFiring an entire sports team after a loss. Although in Tango's case, not even a loss, just not a big enough victory. reply sumtechguy 19 hours agorootparentGame studios have always been very brutal in their hire/grind/fire cycle. Not sure they really want to change. reply pixl97 18 hours agorootparentMost game studios don't give 2 shits about the games themselves, it's all about the MTX. They don't care if selling the game installer is a massive money loss, it's about bringing in the microtransactions and the massive piles of cash they create (generally at the expense of the users game experience). reply Taylor_OD 18 hours agoprevSo... When does the government take action against Microsoft for lying about the Activision merger and how there wouldnt be layoffs? reply gwill 17 hours agoparentThey already broke that promise back in january with 1,900 layoffs https://www.polygon.com/24065269/ftc-microsoft-activision-de... reply chucke1992 15 hours agoparentprevLying for what? They did not promise not to close studios or laying off people. Governments should start looking at their own layoffs first too. reply bilekas 19 hours agoprevThis is quite a lot of studios to be dissolving and obviously a lot of good developers and artists. The silver lining, if any, is that usually some of those let off open up their own indie studios and release some absolute gems. So here's to hoping and good luck to them! reply etempleton 17 hours agoprevMy guess is that because of the success of Fallout show Microsoft is trying to spin up resources to make another Fallout game as soon as possible while also keeping Elder Scrolls moving forward. Bethesda has always been a one game studio, but perhaps they will try to hire up to become a two game studio or they could go beef up Obsidian to pivot into a Fallout New Vegas to or other non-numbered Fallout game. The casualties of this are two financially under performing studios. reply lupusreal 17 hours agoparentIf Microsoft plans to bring a lot of new people into a Bethesda game, I hope they invest in new engine tech and QA. Old Bethesda fans know what to expect by now, but newcomers with any modern game experience are going to be in for a rude surprise if the next Fallout game ships with typical Bethesda quality. They've had this problem with their last several releases too, but I think the TV show will make it a lot worse. reply etempleton 17 hours agorootparentStarfield was much better than past Bethesda releases, but I agree. reply nercury 19 hours agoprevCorporate prioritization is the ultimate cookie cutter, doomed to produce the most generic thing possible. reply reubenmorais 19 hours agoparentThe already existing and extremely powerful paperclip maximizers. reply oersted 18 hours agorootparentIndeed, modern public corporations are already functionally equivalent to AGIs in many ways, with a legal obligation to maximize profit by any means. Not sure why this is not a more common point in AI doomsaying discussions. reply VyseofArcadia 15 hours agoparentprevEspecially in this modern era of metrics and data-driven design[0]. It's just design by committee at scale. [0] Why you would go hard on data-driven design in a creative field instead of trusting the instincts of creators I will never know. reply burnte 18 hours agoprevI don't even know why this makes news anymore. It's so rare for a game studio to last a decade, most don't, and even successful ones will be shut down after launching a huge title in order to keep more profits for the publisher. Publishers are slave masters in the games industry. reply Apocryphon 19 hours agoprevLooking Glass, Irrational Games, Ion Storm, Arkane. Wonder if immersive sims are fated to be an over-ambitious, under-sold, mass audience-unfriendly genre made by doomed studios. reply jerf 18 hours agoparentImmersive Sim + \"I have to keep up with the latest AAA game standards for graphics\" probably is doomed. We really need to give them permission to not have to be up to that graphics standard. Permission to not have to be fully voiced would be nice too. Right now they're in an ugly place where they're still awfully large for an indie or AA studio, but an AAA studio still largely won't make anything that doesn't drive to the limit of modern graphics. reply Apocryphon 18 hours agorootparentI almost wonder if one could make immersive sims for VR, though such an experiment would probably be incredibly intensive for a relatively minuscule playerbase. reply SlowRobotAhead 18 hours agorootparentprevThere is no doubt AI won't be used to fix the fully voiced issue. That is such a no-brainer. As to graphics, my favorite games of the past 5 years were average graphics at best. (Subnautica, Outer Wilds, Hollow Knight, Hades... Red Dead 2 ok not that one) reply jerf 16 hours agorootparentI'm holding out on that one until I see it work. AI voices have certainly reached the point where I don't mind listening to them for a good period of time, and they can match the basic contours of someone's voice, but at the moment they are still missing precisely the fine details a game needs from a voice actor. And I don't know if this is just a matter of a few last tweaks or if it's a case of the last 10% taking 90% of the work. I'm not saying it's impossible, just that I'm waiting until I see it before I declare that it's here. Personally I've nearly entirely bowed out of AAA gaming. The harder they push the graphics the more everything else ends up trashed. It isn't even a lack of effort per se. It's just that if literally everything has to have pristine animations and perfect voice acting and physics-based interactions with its environments, you get less than when all you needed was a 5 frame pixel animation and a funny sound effect for some particular interaction. AI can only cut into the problem there but not solve it until it is essentially not only human-capable, but human-capable in realtime, which is literally getting to holodeck levels of computation. reply danielbln 17 hours agorootparentprevAI for voices is such a catch-22. On one hand, you don't want to open that box of pandora and/or alienate human VAs. OTOH, if the choice is between no VO, and AI VO, well, I'd be ok with AI VO, but it's a slippery slope for sure. reply mepian 19 hours agoparentprevArkane's Prey is one of my favorite games ever, shame they were \"rewarded\" like this. reply bpiche 15 hours agorootparentThe Dishonored games were great. At least they wrapped up the story before they got canned. Wonderful painterly art style and imaginative gameplay. reply SlowRobotAhead 18 hours agorootparentprevI liked it, but couldn't get past \"this is just Bioshock in space with a more mysterious plot\". Then the reveal, well, IDK. reply Apocryphon 17 hours agorootparentahem it was a System Shock 2 spiritual successor, c'mon reply mepian 17 hours agorootparentDan(ielle) Sho was initially conceived as a direct reference to SHODAN, even: https://kotaku.com/old-leaked-design-documents-show-what-pre... reply baerrie 19 hours agoprevI’m calling it now, in five years Bethesda will be making more money with Fallout and Elder Scrolls television shows than their games reply SlowRobotAhead 18 hours agoparentI think the Starfield TV show is going to have difficulties, it'll just be people switching between planets every scene. reply coolbreezetft24 18 hours agoprevI feel like AAA gaming has just gotten too big, the time between releases is way too long compared to back in the 2000s and try to hard to big massive open world or movie quality cinematic experience. I feel similar about Sony. Nintendo on the other hand seems have a good balance of putting out quality games of various ambition with a good frequency reply hbn 18 hours agoprevJust yesterday there was a poll going around on Twitter of which of the big 3 gaming companies, Nintendo, Sony, and MS is the \"least bad\" or something like that, and I saw a lot of people saying Microsoft. I figured they'd do something stupid soon to remind everyone how Microsoft operates, but didn't think it would be so soon! I can't believe these studio acquisitions still aren't being blocked. At what point will they finally acknowledge the blatant anti-competition Microsoft regularly demonstrates by buying any studio that gets too big, letting them rot, and then killing them off? reply wredue 16 hours agoparentHow on earth is MS the “least bad” in the wake of this massive anti-competitive practice of just buying up studios? reply hbn 15 hours agorootparentEvery once in a while they'll do a generous move to essentially buy the good graces of gamers -- e.g. backwards compatibility, launching new releases day one on GamePass (I don't consider that a good thing but whatever), making some custom accessible controller for the disabled, etc. But as with any shitty corporation like MS, it's not charity. It's an investment to distract everyone from the fact that they're a shitty corporation. And it works apparently! reply AdmiralAsshat 19 hours agoprevLooks like Shinji Mikami left Tango before he had to watch it get shut down. reply swozey 17 hours agoprevThe amount of layoffs the last two years is ridiculous. My company just laid off 35 people (150ish employees) and gave them a whopping 2 weeks severance for each year that they worked there. Most of the people let go had only worked here for 1-2 years. Engineers and QA. I was shocked when I heard that because I've always seen it as a great place to work and very forward thinking. That wasn't publicly disclosed, of course, I heard it from a manager coworker/friend. Now I'm petrified. reply bschmidt1 17 hours agoprevBring on the AI: - Procedural worlds - Realistic NPC conversations - Dynamic and unpredictable encounters reply WhereIsTheTruth 18 hours agoprevFTC must be happy! reply loa_in_ 19 hours agoprev [–] Anecdotally, as an avid gamer, I, personally do not miss the studios listed reply delecti 18 hours agoparentBut that clearly isn't equivalent to \"they will not be missed\", or \"the gaming industry is better without them\". Rockstar or Treyarch could disappear and I personally wouldn't care, but I also wouldn't pop into threads about them and pat myself on the back for not being bothered. reply phyllistine 19 hours agoparentprevI can't say the same, Hi-Fi Rush was incredible and I want so many more genre bending games like it. reply Apocryphon 19 hours agoparentprevNever played Prey 2017? reply HDThoreaun 19 hours agorootparentTheyve released 5 bad games since prey reply badsectoracula 19 hours agorootparentArkane Austin only made Redfall since Prey, the rest were Arkane Lyon (which didn't close). Allegedly (based on some articles) Redfall started as a pure singleplayer game ala Prey 2017 (and their older titles) but Zenimax and Bethesda wanted to add multiplayer and Games-as-a-service elements because they thought it'd increase the company's value back when they were looking for buyers Essentially the studio was forced to make a game very different from what they were known for (not just as Arkane but even people who worked there) and wanted to make for reasons outside the game's own merits, which in turn introduced a lot of production issues (they did not have the staff or know-how and had to expand in size) and predictably ended up a mess. reply HDThoreaun 19 hours agorootparentthank you for the context. reply drrlvn 19 hours agorootparentprevOnly one, Redfall. They're only closing Arkane Austin, not Arkane Lyon. See https://en.wikipedia.org/wiki/Arkane_Studios#Games_developed reply Sakos 18 hours agorootparentThis seems bizarre to me. They're closing the one that made Prey 2017. Is it because labor is cheaper in France vs in Austin, USA? Or maybe because everybody who was involved with making Prey left during the shitshow that was Redfall? Or a combination maybe? I can't imagine giving Harvey Smith the axe, though I guess he'll be one of the handful who'll be shuffled off to work on something else inside Bethesda. reply anbotero 13 hours agorootparentNot only it’s probably cheaper in France, but France has strong employee-protection laws, so closing a company there will probably be more costly on severances alone, assuming they can justify to the government why they are closing that company. reply Apocryphon 18 hours agorootparentprevHarvey Smith making a Fallout would be a sight to behold, shame Todd won't let him reply ProfMeowsworth 19 hours agorootparentprevThere are two Arkanes: Austin and Lyon. Redfall was made by Austin and I think they didn’t make any game between that and Prey (2017). https://en.m.wikipedia.org/wiki/Arkane_Studios reply failuser 19 hours agorootparentprevRedfall was the only outright bad one. And it was pretty obvious the specification for the game was bad already. reply ajmurmann 19 hours agorootparentprevDidn't like Deathloop? reply HDThoreaun 19 hours agorootparentnope reply timeon 19 hours agoparentprev [–] I would not miss 3d Fallouts. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Microsoft has shuttered multiple Bethesda studios, such as Arkane Austin, Tango Gameworks, and others, leading to extensive job cuts.",
      "The closures stem from Microsoft's strategic shift in projects and resources, resulting in the reassignment of some employees to different initiatives.",
      "Titles like Redfall will cease to receive updates, while games like Mighty Doom and Hi-Fi Rush are set to be discontinued, alongside job reductions in Bethesda's publishing and corporate divisions, aligning with a broader pattern of layoffs in the gaming sector."
    ],
    "commentSummary": [
      "Concerns are raised about game development and innovation, especially with major studios like Bethesda closing and Microsoft focusing on safe, unoriginal AAA games.",
      "The discussion delves into the differences between indie and AAA games, emphasizing aspects like innovation, team size, budgets, storytelling, the impact of AI, and the importance of diversity in portfolios.",
      "Criticisms of corporate practices, management issues, and the disposable treatment of game studios, along with debates on Game Pass and hardware's role in the industry, are highlighted in the conversation."
    ],
    "points": 206,
    "commentCount": 196,
    "retryCount": 0,
    "time": 1715090025
  },
  {
    "id": 40284164,
    "title": "Ultimate Guide to Dotfiles Management on GitHub",
    "originLink": "https://dotfiles.github.io/",
    "originBody": "Your unofficial guide to dotfiles on GitHub. Home Tutorials General-purpose utilities Tool-specific frameworks Bootstrap repositories Inspiration Tips and tricks FAQ GitHub ❤ ~/ Why would I want my dotfiles on GitHub? Backup, restore, and sync the prefs and settings for your toolbox. Your dotfiles might be the most important files on your machine. Learn from the community. Discover new tools for your toolbox and new tricks for the ones you already use. Share what you’ve learned with the rest of us. Navigating this site If you’re just starting out, before you go symlinking everything in ~/*, you might want to check out some tutorials that discuss how you can organize your dotfiles. Next, you could look through the general-purpose dotfiles utilities to find a system you can use to manage your dotfiles. Some tool-specific frameworks could save you time in customizing specific programs like your shell. Prefer to start from a popular base configuration? Check out dotfiles bootstrap repositories. Or just looking for inspiration, to see how others are doing things? Check out other GitHub users’ dotfiles or Reddit’s r/dotfiles subreddit. Contribute Want to help out? Great! Submit a feature request, open an issue, or submit a patch. Theme by orderedlist. Logo by Joel Glovier.",
    "commentLink": "https://news.ycombinator.com/item?id=40284164",
    "commentBody": "Dotfiles: Unofficial Guide to Dotfiles on GitHub (dotfiles.github.io)196 points by stefankuehnel 23 hours agohidepastfavorite60 comments kjuulh 21 hours agoThis site seems much more like an some of the awesome- repositories on github than a guide. Although it does give some extra information on the packages. To not make it took negative. I personally use Chezmoi, which I've been super happy with. The ability to maintain files in a central repository and then pull them into the file system on an update is quite nice. You can handle merge conflicts and whatnot between your local copy and the upstream dot files. You can opt your files into using go templates in your files which is useful when I need to maintain differences in configs between mac and linux, or in general for just storing secrets out of your dotfiles. There are some small annoyances but overall it is the only dotfile manager, I've managed to stick with. reply p5a0u9l 6 hours agoparentI guess I don’t quite get it. It’s not a ton of work for me to manage things with git, symlinks, and a makefile. Clearly chezmoi adds some polish and maybe additional features, but I don’t see how that outweighs learning a custom tool for this one thing. reply rcarmo 0 minutes agorootparentI just use stow and git with a Makefile as well, and I could certainly do without stow these days (I can just write a Makefile target for it, but haven’t bothered). reply DylanSp 21 hours agoparentprevSeconding Chezmoi, it's what I use, and it works well. reply frantathefranta 20 hours agorootparentThird here, it's the easiest tool to setup dotfiles that work on machines ranging from RHEL7 to latest MacOS. Haven't dealt with Windows dotfiles though. reply tedmiston 17 hours agorootparentFourthing Chezmoi. I tried so many dotfiles tools and it's the only one that does just enough, gets out of my way, and \"just works\". Being able to run `chezmoi diff` and `chezmoi apply` are game changers. reply pprotas 20 hours agoprevAll you need is a symlink script that symlinks files in ~ to ~/path/to/your/dotfiles https://github.com/pprotas/dotfiles/blob/main/symlink.sh That's it. reply jmondi 20 hours agoparentUsing the GNU stow utility makes this super easy. It even has a --dotfiles flag so you can author your dotfiles as dot-zshrc and when the utility symlinks it, it will be .zshrc. Makes it nice so your source files are not hidden but the actual dotfiles are (as expected). reply pprotas 20 hours agorootparentAlthough GNU stow is very interesting, I like this simple script because it has no external dependencies. Stow is not available on macos by default, for example. reply kdtsh 20 hours agorootparentIt is available though in every package manager for macOS except the App Store, and any user of a script like this on macOS is probably also using Homebrew, macports, Nix, or srcpkg. reply PurpleRamen 20 hours agoparentprevThis will not handle cases other targets than $HOME. The automatic deletion seems also very risky. In my own script, I move all pre-existing targets to a separate directory, so I can check later. reply pprotas 20 hours agorootparentYou can set the source and target directories to whatever you'd like, this is just what I need for my own use. The automatic deletion is for directories, to prevent recursive symlinks. Of course, you can program your own however you wish, maybe add a confirmation step before removing and/or create a backup beforehand. reply biftek 20 hours agoparentprevI use stow, which has some nice advantages vs managing symlinks yourself https://brandon.invergo.net/news/2012-05-26-using-gnu-stow-t... reply fock 20 hours agoparentprevI made it Python, added a simple json-config and mustache-templating. Very portable (where symlinks are), very flexible (also used it for termux for a bit) and very simple. reply matheusmoreira 15 hours agoparentprevI think I'm the only person insane enough to use a makefile for this. https://github.com/matheusmoreira/.files/blob/master/GNUmake... https://www.matheusmoreira.com/articles/managing-dotfiles-wi... It even supports the XDG stuff as well as variables like GNUPGHOME. reply amarshall 20 hours agoparentprevSure, maybe. Though eventually one may find lots of edge cases. What about files in the root that get deleted? What about programs that put wanted state within config dirs? What about programs that ignore symlinked files? What about when you’re hacking on your dotfiles and break everything because the active config is the config being edited? What about when you need to vary based on system? What about… I mean, yea, if it works for you, great! But all of the above are problems I did have. I grew a custom install script for years. Now I use Home Manager and the impermanence module. reply pprotas 14 hours agorootparentMy approach to coding is to tackle these types of problems once they occur, instead of thinking \"What about...\", \"What if...\". I don't have these problems you're describing, so the script works great. reply tedmiston 16 hours agorootparentprevSimilar experience here, like: What about when your editor is set to auto save when unfocusing a tab or window and saves while you were mid-edit on your dotfiles and now every new shell is crashing? etc etc reply tedmiston 16 hours agoparentprevI'm sorry, but running `rm -rf ...` in a script against $HOME is a bit too reckless for me. Maybe you haven't lost data from this yet, but this is where Chezmoi has nice guardrails and protects against, e.g., modifications or additions accidentally being made to $HOME instead of the dotfiles dir, which look like they would be silently blown out by your current process. Just my 2¢ from someone who used to do it this way and lost data because of it. reply pprotas 14 hours agorootparentWhat do you think is the risk here? I don’t see a code path to remove my home dir in the script EDIT: Unless I have a dir called \"*\" in my dotfiles, but at that point I deserve to have my home dir removed :P ZSH also asks for confirmation first reply vbezhenar 20 hours agoparentprevYeah, same: https://github.com/vbezhenar/dotfiles/blob/main/ln-all No need for those complex software, just shell script is enough for me. reply theshrike79 21 hours agoprevI've been bikeshedding with dotfiles for 20 years and ended up with Chezmoi[0] in the end. It came down to the fact that it's easy to install and easy to bootstrap as long as your dotfiles are in a public Github repo. It also integrates with a bunch of password managers to grab any secrets you might need in addition. I use the \"run_onchange_install-packages.sh.tmpl\" to install basic packages to computers I bootstrap chezmoi on, it adapts based on the operating system using chezmoi's templating system[1] It takes a few tries to remember the flow of \"chezmoi add\"ing a file after editing it so that you can push it to Github. After that it's just a matter of chezmoi update on your other computers to bring them up to date. For .gitconfig I use a .gitconfig.local file on my work computer to override my personal credentials and not have to mess with templating the file. [0] https://www.chezmoi.io [1] https://www.chezmoi.io/user-guide/advanced/install-packages-... reply josephd79 21 hours agoprevjust setup a bare repo and call it a day. https://www.atlassian.com/git/tutorials/dotfiles original thread on HN: https://news.ycombinator.com/item?id=11070797 reply kryptn 16 hours agoparentThis is how I manage my dotfiles, and it has worked pretty well for me. reply twp 15 hours agorootparentHow do you handle differences between machines (e.g. between Linux and macOS)? How do you handle secrets? reply nimih 7 hours agorootparentI can't speak for the OP, but I've used a bare git repo for a number of years to manage my dotfiles, and in the few cases where I need to handle differences between machines, I've always been able to find a simple and straightforward, albeit ad-hoc, solution: for my shell, I `source local-config.${hostname}` when it exists; for my emacs config, I have a couple `cond` blocks in my config.el file; my preferred terminal emulator (kitty) can load multiple configuration files in sequence via passed arguments, so I can write a simple wrapper script and use a per-machine override .conf if necessary; etc etc. I don't currently put secrets in any text configuration files (nor can I envision myself doing so in the near future; I generally use a password manager, ssh/scp, or magic-wormhole to move secrets between workstations and servers where I have a user account), so I don't have a good answer for that one. I imagine these solutions would scale poorly if I had a large number (dozens? hundreds?) of machines which all needed unique configurations, or if I used more tools which needed per-machine configuration, or if I needed to stick secrets in configuration files for some reason. Luckily for me, that's not the case, and my system has served me quite well as a result. reply LorenzoGood 17 hours agoprevFor me, I have found nix home manager to be the most effective solution for managing my dotfiles. The migration process was easy, and I was able to use it to replace my sim link script with it's file directives. It also integrates well with my Nixos Systems. reply ar_lan 14 hours agoparentNix + Home Manager is by far the easiest-to-manage system for dotfile management to date for me. The added bonus is: 1. No need for complicated directory structures that a symlink-like script needs to manage. 2. Dotfile management coupled with package management. Dotfiles only make sense as configuration to packages you have - why would you not tie these together? 3. Minor-to-no tweaks required to have a fully reproduced setup on any machine. Seriously, I transitioned my dotfiles from my NixOS machine at home to my work-issued M2 within... an hour? I will never not evangelize Nix. reply Cu3PO42 13 hours agoparentprevI have also ended up with Home-Manager. It's an extremely powerful tool that I am unlikely to replace in the next ten or so years, but you also need to learn a whole programming language (Nix) plus a DSL (the NixOS module system) to get the most out of it. On the upside: you do get a whole purpose-build language for configuring your system, piecing together your configurations dynamically, installing your packages and so much more. Ability to install packages is another huge thing. Home-Manager does not just configure my tools, it also installs them (including my DE), so I can get started with my custom config on a new system in a matter of minutes. reply low_key 20 hours agoprevI started managing my dotfiles in git and followed the pattern I found in Anders Knudsen's repository below. The most painful thing has been coming up with config that works across Linux and MacOS, but that's been chosen pain that can easily be avoided. https://github.com/andersix/dotfiles reply asix66 16 hours agoparentThis method, using a bare git repo, is really an elegant, efficient, and easy way to manage one's dotfiles. In the end, how you do it is personal pref. I liken it to editor wars, vim vs emacs, etc. What works for me, may not work for you. PS: these are my dotfiles (link is my repo), and I've used this method for some 5 plus yrs now and it works well for me. I really like how easy it is to spool up a new linux instance and have my config ready to go in mere seconds. I've not used branches yet, for different OS'en, so my zshrc and bashrc have conditional checks for OS. I will be looking at using branches in the future since it does sound like it would make shell rc files cleaner. reply hk1337 20 hours agoparentprevI have been keeping them in separate branches because I have had macOS with intel and arm because there were differences in the config. reply josephd79 17 hours agoparentprevyup, a bare repo. its the easiest way I think. reply diego898 21 hours agoprevGnu stow has finally been updated to fix a long standing issue so I can once again wholeheartedly recommend it again! It’s lightweight and uses symlinks. Last I checked, chezmoi has a symlink mode but it’s a bit worse. reply fforflo 19 hours agoparentI guess you're referring to this ? https://github.com/aspiers/stow/issues/65 reply diego898 16 hours agorootparentSorry - should have been specific in top-level comment. This one: https://github.com/aspiers/stow/issues/33 reply jmondi 20 hours agoparentprevCurious what the issue you were running into was? I’ve been using stow for my dotfiles for at least 8 years now and have been loving it the whole time. reply diego898 16 hours agorootparentSorry - should have been specific in top-level comment. I was referring to this one: https://github.com/aspiers/stow/issues/33 Basically, the --dotfiles option was not working with directories so you had to have things that look like this: lazygit/.config/lazygit and now it looks like: lazygit/dot-config/lazygit. Really a small issue that bugged me forever - shouldn't have made it seem like it was core a problem with stow! reply biftek 20 hours agoparentprevWhat's the issue? I've been using stow for a decade now and can't recall ever running into problems reply diego898 16 hours agorootparentSorry - should have been specific in top-level comment (cant edit now). I was referring to this one: https://github.com/aspiers/stow/issues/33 Basically, the --dotfiles option was not working with directories so you had to have things that look like this: lazygit/.config/lazygit and now it looks like: lazygit/dot-config/lazygit. Really a small issue that bugged me forever - shouldn't have made it seem like it was core a problem with stow! reply tionis 20 hours agoprevI'm currently using a git based approach for my dotfiles, similar to the one notes here[1]. I've got one significant change, though: All my dotfile management works over my cfg[2] script that helps me maintain a main branch for dotfiles for all machines and then branches that branch off of that main like 'main.arch.MACHINE_NAME' that are merged like a waterfall during sync (main -[merge]-> main.arch -[merge]-> main.arch.MACHINE_NAME). (I can also cherry pick up the waterfall) [1]: https://www.atlassian.com/git/tutorials/dotfiles [2]: https://gist.github.com/tionis/a0f23a7a33b0e289f1b03cc6ff503... reply keybored 19 hours agoprevSymlinks and Git. A remote like GitHub is then a minor bonus. I didn’t store my dotfiles on GitHub because they didn’t used to have free private repositories. (Why would I share my dotfiles?) reply gbrindisi 20 hours agoprevthe ultimate bikeshedding with dotfiles is home-manager with nix, an infinite cacophony of deep pain and enlightened pleasure reply yoyohello13 19 hours agoparentYes, I've gone through every style of dotfile management. Stow, bare git repo, custom script, and now nix with home-manager. Using Nix is definitely the best solution so far, largely because not only does it manage the configurations, but also installs the programs you need. reply rgoulter 19 hours agorootparentPlus, Nix allows for installing the same versions of those programs everywhere you're using Nix. reply yjftsjthsd-h 19 hours agoparentprevThe one thing that makes me hesitate with nix is that it only really supports Linux and Darwin; of the BSDs only FreeBSD has any support (somewhat suboptimal still), and AIUI Windows is also unsupported. reply travis51 20 hours agoprevI highly recommend dotter, especially if your dotfiles have variation from machine to machine and or if you don't need all of them at a given time. It's pretty simple and lightweight https://github.com/SuperCuber/dotter reply nirvdrum 19 hours agoprevI've had really good luck with vcsh and git-crypt. vcsh can be a little awkward to use because git commands need to be scoped to the vcsh project, but you get used to it. I like having separate git repos for separate roles (e.g., fish-common, fish-personal, fish-work). Then, depending on what I'm doing on a particular machine I can pull in just the roles I need. It may seem like overkill, but I've found it very helpful in sharing my dev config without pulling personal stuff onto a work machine, vice versa. git-crypt allows me to protect sensitive files while still tracking them in a git repo. I don't think I'd rely on it for shared projects, but it works well for me. reply epiccoleman 14 hours agoprevThis one's my favorite - has been working reliably and with barely any intervention for years now: https://github.com/andsens/homeshick My own dotfiles: https://github.com/epiccoleman/dotfiles reply botanical 19 hours agoprevI'm using yadm for some years now, which works really well: https://github.com/TheLocehiliosan/yadm reply JimDabell 18 hours agoparentI’ve tried a bunch of these out and settled on yadm as well. It’s basically a shim for Git that tracks your home directory with a bare repo stored elsewhere. reply sevagh 11 hours agoprevAm I missing something or is the use of \"GitHub\" weird here? Are they GitHub employees? None of the creators seem associated with GitHub. They're _users_ of GitHub, nothing more. reply ivanjermakov 1 hour agoparentI bet more than half developers do not differentiate between git and GitHub. reply throwaway918274 20 hours agoprev1. sudo apt/dnf/zypper install stow 2. man stow 3. git init ~/dotfiles have at'er reply shizzy0 10 hours agoprevAll you really need is one alias. alias homegit=\"git --work-tree=$HOME --git-dir=$HOME/.dotfiles.git\" reply asicsp 22 hours agoprevPrevious discussion: https://news.ycombinator.com/item?id=32632533 (256 pointsAug 28, 202261 comments) reply mehdix 18 hours agoprevJust use your home folder as a git repo, rename .git, create alias \"githome\" to git cli pointing to that directory and viola, you have all you need. reply Frictus 22 hours agoprevWhat I see is a mishmash of information. reply hammyhavoc 21 hours agoparentIs there a better resource you can recommend? reply bbor 19 hours agoprev [–] The good stuff: https://dotfiles.github.io/inspiration/ reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The guide provides tutorials, utilities, frameworks, and tips for managing dotfiles on GitHub, enabling users to backup, restore, and sync their preferences and settings.",
      "Users can learn from the community, share their knowledge, and find inspiration for organizing and customizing dotfiles on the site.",
      "Additionally, users can contribute by submitting feature requests, opening issues, or submitting patches to enhance the guide further."
    ],
    "commentSummary": [
      "GitHub discussions focus on various tools like Chezmoi, stow, Home Manager, Nix, for dotfile management, emphasizing individual preferences and concerns about data loss.",
      "Users highlight the importance of finding a suitable tool that aligns with personal needs and discuss the efficiency of different tools, including the use of branches in git-based methods.",
      "Personal preference and usability are key factors in selecting the most appropriate approach for managing dotfiles on GitHub."
    ],
    "points": 196,
    "commentCount": 60,
    "retryCount": 0,
    "time": 1715079512
  },
  {
    "id": 40289323,
    "title": "The Grateful Dead's Revolutionary Wall of Sound",
    "originLink": "https://audioacademy.in/the-grateful-deads-wall-of-sound/",
    "originBody": "The Grateful Dead’s Wall of Sound by Audio AcademyFeb 2, 2019Audio Legends A vision during an LSD trip is what inspired Owsley “Bear” Stanley, the Grateful Dead’s sound engineer’s mammoth feat of technical engineering, “The Wall of Sound”, irreversibly changing live sound and engineering for the better. It was a time when live sound problems plagued engineers, bands, and audiences equally. While rock concerts grew in size and scope throughout the 60s, audiences grew larger and louder, without the technical sophistication of amplification ever changing to meet this scenario. Screaming fans meant that low-wattage guitar amps could hardly be heard and without the help of monitoring systems, bands could barely hear themselves play. Things were so bad that the Beatles quit touring in 1966 because they couldn’t hear themselves over the audience. It was after this era that the band, the Grateful Dead, became obsessed with their sound, largely thanks to their eccentric and dedicated sound engineer. Though incredibly frustrated with the noisy, feedback-laden, underpowered situation, they did not want to give up playing live, and the Dead had Owsley on board to help solve the sound situation. Who was Bear Owsley? Owsley Stanley, left, with Jerry Garcia of the Grateful Dead The famous story goes that in 1974 the Grateful Dead drummer Mickey Hart walked on stage to find Owsley “Bear Stanley standing in front of a wall of over 600 speakers with tears streaming down his face. Whispering to the huge mass of equipment, Bear said, “I love you and you love me- how could you fail me?” This story sums up Owsley’s obsession with sound, both as a concept and as a physical thing. A former ballet dancer and craftsman from Kentucky, Owsley was also jailed twice for manufacturing and distributing LSD, the profits of which he used to finance the Grateful Dead for some time. The band which formed in the San Fransico bay area during the hippie days of the mid-1960s boasted of a huge cult following. An engineering dropout, he met the Grateful Dead through Ken Kesey, during one of his infamous ‘acid test’ parties in 1965 and became friends with them. Owsley, or ‘Bear’, as he became known, began to work with the band as their sound man and financed them with his earnings cooking acid. He was also the person who along with his good friend Bob Thomas, helped design the iconic lightning bolt logo, which would sell countless t-shirts. After a hallucinatory incident, where Owsley ‘saw’ the Grateful Dead’s sound, he became obsessed with aural perfection and started on an endless path, working with the band, to achieve it. Keeping a sonic journal of each performance of the band, he would use it to improve on his setup and mix for each concert and highlight issues to the band members. He insisted on sound checks and encouraged them to listen to tapes of their performances, so that they could hear how they sounded, and also correct him on what he was doing. Rumored to have taken inspiration Buffalo Springfield’s monitoring setup, Owsley was not satisfied with the standard speakers and amplifiers that were available to him and began modifying and manufacturing his own audio gear with his apprentice Tim Scully, eventually founding the company Alembic. In a 1969 meeting when brainstorming ideas for musical exploration and solutions for their technical problems and smoking ‘special’ cigarettes, Owsley proposed putting the P.A behind the band. This casual, stoned suggestion would change the way audio engineers thought about concert sound. This meant that the audience and the band would hear the same thing without any delays, chaotic reverb, colliding frequencies, and minimal-to-no feedback. Soon after this, Bear began his most ambitious design to date. He, Dan Healy and Mark Raizene of the Grateful Dead’s sound team collaborated with Ron Wickersham, Rick Turner and John Curl of Alembic to create the legendary Wall of sound. The Wall of Sound The Wall of Sound, 1973 The mammoth structure was massive, made up of over 600 hi-fidelity speakers that sat behind the band as they played. It used six separate sound systems which were able to isolate eleven separate channels with vocals, rhythm guitar, piano each having their own channel. Another channel each for the bass drum, snare, tom-toms, and cymbals. The bass was transmitted through a quadraphonic encoder, which took a signal from each string and projected it through its own set of speakers. The result of each speaker carrying only one instrument or voice at a time was crystal clear audio, free of intermodulation distortion. The Wall of Sound served as its own monitoring system and solved many, if not all of the technical problems that sound engineers faced at that time. This design was also the first of its kind to eliminate the need for a sound guy, at least at the front of house. In addition to monitor controls, each of the microphones had volume dials. The idea was that each band member could adjust the sound in real time. The situation of the monitors gave the band the power to control their own sound. While the system had many trials and errors, the largest version stood three stories high and about 30 meters wide, the first line array system of this scale ever assembled. High-quality audio could be heard at 200 meters, with decent sound up to another 200 meters, at which point wind started to degrade the audio. The completed Wall of Sound made its debut in 1974. Some of the early issues that surfaced was problems with feedback between the speakers and the singers’ rear-facing vocal microphones and the giant task of physically mounting the system. It was a logistical nightmare and almost made the band go bankrupt, because almost as soon as it rose the roof, it was time to pull it back down again. Eventually, the massive Wall of Sound had to be streamlined into a far more manageable and cost-effective touring rig. All the same, Owsley and the band’s willingness take ideas and execution to extreme lengths changed live sound forever. The Legacy of the Wall of Sound While parts of the wall were kept, repurposed and recycled for future touring rigs, other parts were sold off. Legend has it that friends of the Grateful Dead, Hot Tuna and Jefferson Starship quickly bought some of the highly sought-after top-of-the-range gear. While modern improvements in these technologies have the benefit of being far more powerful and lightweight than the Wall’s, in terms of experimental, unrestrained engineering, at a time where there was nothing like it ever heard, the Wall of Sound remains unparalleled.",
    "commentLink": "https://news.ycombinator.com/item?id=40289323",
    "commentBody": "The Grateful Dead's Wall of Sound (audioacademy.in)193 points by 1970-01-01 15 hours agohidepastfavorite90 comments scrumper 15 hours agoArticle doesn't mention one of the more interesting (to me) aspects which was how feedback was avoided. The solution is elegant: each vocal microphone is doubled, meaning there are two at each position. The phase is inverted on one of them, the singer sings into only one, and both are sent to the speakers via their channel's amp. The effect of that setup is that only the difference between the two microphones is amplified; common signal in both (i.e. the sound coming out of the speakers) is nulled out, but the difference signal (the voice) makes it through. It apparently wasn't quite perfect but was absolutely a lot better than wailing feedback. The thing that made it sound so good was that any given speaker only reproduces a single source, but the article touches on that. The mic arrangement I described is simply what makes it possible. reply TylerE 14 hours agoparentWhile it's true that they did that and why, I'd ultimately chalk it up as more of a flaw than a feature. Vocals never sounded great on wall of sound shows because they could never sing perfectly into one mic. This can be confirmed by listentng to soundboard tapes of the shows, and comparing them with ones a year or two either side - the full on Wall was only used for about a year. While the WoS laid much of the groundwork for how modern PAs are designed and operated, it was more of a white elephant than anything, and many of it's actual ideas were discarded. It was totally impractical to tour with and they lost money doing so. The only real technical legacy it has is of using coherent phased line arrays. Really it's whole reason for existence (getting a coherent, in phase, non-canceled signal at an extended distance from the stage) isn't even relevant, as these days secondary speaker arrays with delay lines (to sync them perfectly with the mains) is almost childs play. Literally plug and play. Modern PAs can self-tune the whole system just from playing a short burst of white noise through the system, and listening for the response. reply mannyv 8 hours agorootparent\"Modern PAs can self-tune the whole system just from playing a short burst of white noise through the system, and listening for the response\" A technology which was developed by/with the Grateful Dead, by the way. From what I understand they essentially financed the modern PA industry by spending a ridiculous amount of money on sound equipment. People don't realize how much money they made - they were pretty much the top grossing tour band for about 15-20 years, playing about 90-100 shows a year. And they could (and did) use those shows to experiment with sound in a way that probably no other band has done since. I haven't watched any D&C shows, but I expect their sound quality was just as good, if not better, than the Dead's. reply dekhn 8 hours agorootparentFrom one of the dead's main sound engineers, Dan Healy, who helped establish this technology (I wasn't able to find any further discussions, but I know I've read a few interviews where he talked about doing this at the soundboard). \"\"\"What tools do modern sound engineers have at their disposal that you didn’t? Computers and all the things that became possible at the advent of computers. It’s removed the limitations to creativity. Nowadays, in terms of concert sound, you can not only correct the sound for any room, you can ongoingly correct it in real time as the room changes and as the temperature changes and as the humidity changes. We used to do that in the ’80s and ’90s – I had a complete weather station at my mix board and we tracked temperature, humidity, barometric pressure, because they change sound dispersion, sound quality. We corrected the systems accordingly. We did long, long studies and mapped it out, and we had curves so we could predict where the sound was going. We had to do that by hand. Nowadays, it happens all by itself.\"\"\" reply dekhn 13 hours agorootparentprev(I assume you're aware, but for the larger audience)... the grateful released an album \"Two From the Vault\" which was a soundboard recording... but the original soundboard had huge phase cancellation errors due to microphone placement. To recover it, some 20+ years later, with digital tech, the sound engineers could recover the original signal using some clever FFT and phasing very similar to what you describe modern secondary arrays use to self-tune. reply TylerE 11 hours agorootparentIronically i haven't really listened to most of the official live albums much. I tend to just go straight to the board tapes, which often sound better due to having a few decades of technological advancement - many were transferred in the 90s or 2000s. Of course they didn't have then what we have now, but even consumers by then had access to software for things like mastering that would have made any 70's engineer drool - certain kinds of repairs are much more easily done digitally - back in the day cutting out a spot of stactic or a mic pop involved literal tape and razor blades. reply dekhn 11 hours agorootparentTwo From The Vault isn't a \"official live album\", it's a soundboard that was shelved for decades due to the quality of the recording. I got this album on CD when I was in college (early 90s) and didn't have access to high quality taping equipment, and soundboards from the late 60s were very rare. The audio quality is absolutely excellent (I am just relistening to it now, there's only tiny background hiss, excellent clarity on all the instruments, decent vocals, and only a bit of high-volume distortion on the guitar and bass). It's also a nice counterpoint to the original of the \"From The Vault\" series, One From The Vault, which was recorded years later under ideal conditions and the band had been practicing extensively. Much has changed from the days when we had to implement balanced binary trees of tapes (analog tape copies were lossy, so you wanted to minimize the total depth of copies). reply lb1lf 1 hour agorootparentOooh, thank you for sending me down memory lane. (I took part in lots of tape trees back in the nineties -mostly Neil Young, with a good helping of Grateful Dead and occasionally some Phish thrown in for good measure. Then CD-R became a thing and we did lots of those for blanks+postage. Good times! (I even did DAT trees, as I had three (!) DA-P1s gifted from a local radio station going out of business) I once got called down to the customs authority to explain what I was up to - they noted I got loads and loads and LOADS of seemingly innocent recording media in the mail, only to ship them out again at a later date. Nothing showed up when they inspected the packages for drugs, so if I didn't mind - would I PLEASE explain what was going on? reply greentxt 10 hours agorootparentprev> Vocals never sounded great at wall of sound shows It was Donna. reply craigmcnamara 4 hours agorootparentHarsh reply bongodongobob 13 hours agorootparentprevWell, yeah, compared to today it's not great but no one had tried anything like that before. They delayed the sound to distant speakers with tape delay. It's cool as shit and was the groundwork for how we do things today. It's like saying relay computers were dumb... Boolean logic was new and no one had ever attempted stuff like that before. reply TylerE 13 hours agorootparentNo, the whole point of the WoS was that there were no distant speakers. Everything was single sourced, to the point where each speaker only carried a single instrument. reply JohnBooty 12 hours agorootparentI know you probably know, but: each speaker only carried a single instrument. Each vertical stack of speakers only carried a single instrument; not each individual speaker. reply TylerE 12 hours agorootparentThe routing wasn't nessisarily full spectrum though. There were a lot of crossovers in use. I also believe I heard some of the precussion mics were targetting only one or two speakers. At least in the case of the speakers for Jerry, they had a a seperate McIntosh hi-fi amp for each speaker, being fed out of a Fender-derived preamp and a many-way splitter. Owsley basically bought the every one that model amp that was in stock at dealers on the west coast. Hundreds of thousands of dollars just on those amps - they were something like 2 or 3k a pop even then. The only reason they were even able to afford in the first place was that Owsley (Yeah that Owsley, who was also their primary sound engineer) had so much illegal cash from a decade of making most of the LSD consumed in the United States. Band never even paid for most it. It was more this crazy idea Owsley had and mostly paid for that they kind of rolled with. That sort of thing was more than a bit of a pattern in that camp, and was a large part of the band's downfall. It got to a point where it seemed like half of Marin county was on the payroll, and there was so much money going out that they had to tour constantly, wether they wanted to or not. The heavy touring clearly had a major toll on Jerry both physically and mentally. A two or three year hiatus around '91 or '92 would have done him (and probably some of the other guys) a world of good. reply bongodongobob 12 hours agorootparentprevThey definitely used distant speakers, but yeah, not part of the WoS. I'm just saying that was cutting edge at the time. reply dekhn 13 hours agorootparentprevNot exactly: the wall of sound was only set up on stage. However, you could hear the music extremely clearly 1/4 mile away, due to the coherence. The delay towers were used before the WoS. reply neckro23 12 hours agoparentprevThis is very similar to how noise cancellation works on cell phones. The secondary mic is typically on the back of the phone and picks up the ambient noise to be subtracted from the primary mic’s signal. reply plussed_reader 14 hours agoparentprevSince balanced cables predate the GD, this strikes me as an acoustic implementation of an EE concept. Neat! reply insaneirish 13 hours agorootparentHere's a fun one. I'm involved in maintaining the audio system for an auditorium used by a non-profit. After a flood and remodel, including replacing some audio components (like microphones), it was observed that the microphone on the main podium always had a 60 Hz hum. The hum depended on where the microphone was facing. Sometimes it was there, sometimes it was not. Being a non-profit facility, there are no fancy DSPs to notch out the hum or anything like that, so more creative solutions were investigated. It was determined through dumb experimentation that orienting an identical microphone 180 degrees to the one with the hum and setting the gain similarly would nearly eliminate the hum. Eventually, the working theory became that a relatively new large pad transformer installed across the street was being picked up by the microphones. Orienting one microphone 180 degrees from the other caused the hum to be picked up out of phase from the main, and thus could be mixed in to cancel out the main mic hum. Ultimately the real solution was simply buying better microphones, but there was a period of some months while a microphone sat off stage, pointed backwards. reply ChainOfFools 13 hours agorootparentIt's been quite a few years since I last worked in live show production, but on any show at a venue where we couldn't be sure of access to clean power, humbuckers (not the guitar pickups, a nickname for what I believe was just a dumb 60hz notch filter or ground loop isolator inlined on house power taps) were a standard pack out in the road kit. I would have expected that this decades-old and well established component of power infrastructure would have been commoditized by now and integrated into any dedicated AV performance/production space such as an auditorium. reply munificent 10 hours agorootparent> what I believe was just a dumb 60hz notch filter A simple notch filter won't fix 60 Hz hum. Or, at least, when I've tried to eliminate annoying 60 Hz hum in my own amateur recordings, it's never been very effective. The problem is that the 60 Hz hum isn't a sine wave. It's more like a square, so you've got a bunch of harmonics up the frequency spectrum to worry about too. You can try to also notch out 120, 180, 240, etc. but it starts to get weird sounding fast. reply bombcar 12 hours agorootparentprevNonprofit and church auditoriums and halls are often about fifty years out of date and are somewhat around “barely working”. reply plussed_reader 11 hours agorootparentprevThat is my kind of tickler/teaser. Thanks for the share. reply fuzzfactor 5 hours agorootparentWith the orchestra spread out on stage from left to right, and a bi-directional mic overhead with the capsule facing left-right, you get a channel which is largely the difference between what a listener at the same position would be hearing from each ear. Not exactly, but something like that. So not very listenable on its own. On the same pole facing down at the entire band, you have the omnidirectional mic trying to capture the whole thing as good as possible, suitable for live broadcast from this other channel alone. They didn't have stereo radio yet anyway. Afterward back in the studio, starting with only the two channels on reel-to-reel tape, the \"difference\" channel can be phase-inverted to an auxiliary tape, then you have three channels suitable for mixing. And with analog techniques like this they can be mixed \"down\" to 5.1 surround sound. From a single mic stand and only two live signals. reply hunter2_ 9 hours agorootparentprevThe history of balanced lines (common mode rejection, differential pair, etc.) is fascinating. Apparently the first twists (as in \"twisted pair\" to pick up external interference as similarly as possible on each conductor) were achieved not within a bundled cable, but between utility poles. Every two spans would constitute a full twist, with two single wires alternating from left to right on the cross member. But as for the acoustic implementation, even that has a long history. The Dead borrowed the idea from fighter pilot headsets, the only difference being that pilots were contending with a noisy cockpit rather than feedback. Same general idea that the unwanted sound hits both mics somewhat equally while the voice hits both mics somewhat unequally. reply itishappy 15 hours agoparentprev> common signal in both (i.e. the sound coming out of the speakers) is nulled out, but the difference signal (the voice) makes it through. What drives this? Singers and speakers are both localized sources, so I'd expect the mics to pick up similar phases for each. I bet it's distance! Falloff depends on distance to source, so there should be a larger difference in volume for closer sources. reply scrumper 14 hours agorootparentYep, you sing into one but not the other, so there's a big difference in the vocal signal, whereas spill from the speakers is going to hit both mics pretty well evenly. reply llamaimperative 15 hours agorootparentprevYeah one microphone was behind the other, though I was under the impression it was half a wavelength behind and thus created something quite akin to modern active noise-cancellation? Edit: Apparently this is not the case! reply dekhn 14 hours agorootparentSee https://archive.org/post/256492/the-betty-question-answered for more details both on WoS and the microphone setup. reply bregma 14 hours agorootparentprevWhen I saw them (back in 83, it's been a long strange trip) it looked more like one mic was on top of the other with about a 6 inch separation. reply itishappy 14 hours agorootparentprevA very literal phased array! reply dylan604 13 hours agorootparentprevHalf wave length in what frequency would be the first thing that would give me pause to this. I'm reading this after your edit, but even before I got to the edit my brain was already heading towards nope reply llamaimperative 13 hours agorootparentYeah that asterisk popping up in my head was why I did some more digging as well :) thought tbf, the Dead never seemed to care much about vocal quality so it wouldn’t have been crazy for this to work well only for a narrow band. I don’t have any intuition for exactly how narrow that band would be and how that’d sound in practice though. reply rnicholus 13 hours agoparentprevHere is a much more detailed article that covers the mics and so much more: https://www.vice.com/en/article/wnnayb/the-wall-of-sound reply JohnBooty 15 hours agoprevYou can see a partial legacy of the \"Wall of Sound\" at most concerts today - vertical line array speakers. https://www.soundonsound.com/techniques/line-arrays-explaine... For the most part, each performer plugged into the \"Wall of Sound\" had their own vertical 1xN stack of drivers. 3 drivers in a vertical line away will have less distortion than 3 drivers in a horizontal array; the horizontal drivers will suffer from comb filtering for listeners who are not located dead center at the middle of the array. (This of course assumes your audience is dispersed horizontally as opposed to floating randomly in space) Modern home loudspeakers hew to this philosophy as well to an extent. As opposed to big \"monkey coffin\" 70s speakers with a random array of drivers sprayed across the front of the speaker[1], modern tower speakers have a vertical array of 2 or more drivers whose centers are aligned in a vertical line[2]. ____ [1] https://www.reddit.com/r/BudgetAudiophile/comments/yburht/at... [2] https://www.audiosciencereview.com/forum/index.php?threads/r... reply dekhn 14 hours agoparentMany of the ideas here were explored and commercialized by Meyer Sound: https://en.wikipedia.org/wiki/Meyer_Sound_Laboratories (the founder also helped out with the Wall of Sound) reply bongodongobob 14 hours agoparentprevAnother cool thing you can do with line arrays is beam steering, you can direct the sound to a certain extent. There are tradeoffs, but I always thought that was pretty damn magical. reply bombcar 12 hours agorootparentIsn’t the audio setup in that LED ball in Vegas a bunch of beam steering? reply _kb 10 hours agorootparentYep. They’re running by a rather sizeable Holoplot [1] rig. Expands the concept of a line array to planar array along with a good helping of DSP. [1]:https://holoplot.com/ reply TacticalCoder 11 hours agoparentprev> modern tower speakers have a vertical array of 2 or more drivers whose centers are aligned in a vertical line But you cannot say that on HN. HN is the place where people believe soundbars are as good as high-end audiophile speakers! reply block_dagger 12 hours agoprevFor those wanting to listen to free legal taped audio of Dead shows, head over to https://relisten.net/grateful-dead or install Relisten app for iOS. All fan supported and open source. reply switz 12 hours agoparentHey! I created this website and have been maintaining it along with my friend Alec for the last decade. Fun seeing it pop up here on HN. Thanks for sharing! reply block_dagger 11 hours agorootparentThanks for your work! I maintain the phish.in API and caught a show with Alec over ten years ago. Good times! reply derwiki 5 hours agorootparentprevI’ve been using Relisten every day lately. Thank you so much! reply gverrilla 9 hours agorootparentprevBeen enjoying the music on your website for the last hour, very grateful for your work! :) Why those bands, and not others? What do they have in common? reply switz 8 hours agorootparentAll of these bands have agreed to allow people to tape and distribute their concerts for free. This is most popular in the jam band community, but you’ll find a plethora of other bands that allow it as well. reply DoodahMan 6 hours agorootparentprevthank you SO MUCH fam! relisten is such a great repository and i have shared it plenty. thank you too block_dagger for phish.in, much the same.. much love! reply 082349872349872 15 hours agoprevTh' Dead not only allowed taping, they encouraged it: at the shows I attended, there was invariably a small grove of microphones set up near the soundboard, in the middle of the audience. https://en.wikipedia.org/wiki/Taper_(concert) (a disadvantage to too much ethology reading: I can't remember \"Bill Graham Presents\" without thinking of baboon behaviour) reply kyleblarson 5 hours agoparentI was a bit too young to get too many shows (I saw one in 1993 and a couple of JGB after that), but I have extremely fond memories of going to the local head shop in my town that had a massive tape collection. You would bring 6 blank tapes and pick 5 from their library that they would copy for you and keep the other blank as payment. reply jMyles 15 hours agoparentprevI'd love to hear more about your experience and observations of taping at the shows. Not only did GD (and particularly Jerry Garcia and John Perry Barlow) eschew the 'intellectual property' model of music, their thought-leadership has lived on to become much of what we today consider fundamental internet technology and methodology. Early decentralized crypto-economics, peer-to-peer file sharing, and the founding of the Electronic Frontier Foundation were all developed by some combination of deadheads and musicians and tapers, particularly on an internet service called The WELL. The Green Pill Podcast had an entire episode exploring the bluegrass roots of blockchain technology; I was humbled / psyched to be a guest and play several of my songs, as well as some traditionals that GD also played. It's here if you're interested: https://www.youtube.com/watch?v=Y3s9Fu4yu7o&t=2898s reply zer00eyz 14 hours agorootparentIt's almost impossible to talk about The Dead and not talk about LSD. Owsley (wall of sound engineer) was one of the original major LSD manufactures... and a dead show was always where you went to score ACID if you lived on the east coast. This remained true well into the 90's. I'm going to guess that all of those early internet pioneers that you mentioned also have fond stories of LSD. The Dead, Bill graham, hells angles, peoples temple, Patty Hearst.... There is a continuum of culture that spills out of San Francisco to this very day. reply Liquix 13 hours agorootparent> I'm going to guess that all of those early internet pioneers that you mentioned also have fond stories of LSD. What the Dormouse Said by John Markoff is exactly that. A dive into how psychedelic counterculture made its mark on folks at Stanford, folks at XEROX PARC, Doug Engelbart, Jim Fadiman, Steve Jobs, etc. Not the most cohesive narrative but fascinating stories https://www.amazon.com/What-Dormouse-Said-Counterculture-Per... reply joezydeco 12 hours agorootparentprevIs there gas in the car? Yes, there's gas in the caaaarrrr... reply cccybernetic 14 hours agorootparentprevTo add to this, John Perry Barlow, one of the Dead's two main lyricists, co-founded the Electronic Frontier Foundation. reply dekhn 7 hours agorootparentBarlow wrote the famous \"Declaration of the Independence of Cyberspace\" (https://www.eff.org/cyberspace-independence) in 1996, famous in its time but also somewhat poignant, given the commercialization of the internet not long after. Barlow was also big on the WELL, an early influential time-sharing messaging system (https://en.wikipedia.org/wiki/The_WELL a Bay Area legends from a time when the Bay Area dominated in both technology and music). In many ways Hacker News is an intellectual inheritor of the WELL (along with Usenet and Slashdot). reply jtriangle 14 hours agoprevDave Rat, of Rat Sound/RHCP/Bassnectar/etc fame, has some very interesting takes on the wall of sound idea using modern equipment. The core of it is that speakers are bad at polyphony, so if you can avoid it, you can produce something that sounds more natural to human ears, and do so in a larger area. The way to avoid it is more speakers, more stacks/arrays/etc. You don't necessarily need an array per instrument, because modern loudspeaker arrays are indeed much better than they used to be, and modern loudspeaker processing fixes a multitude of problems. reply mrob 14 hours agoparentAnother advantage of individual speakers for each instrument is that it allows positioning them without the phase cancellation artifacts you get with a stereo setup. It's obvious with some sounds, e.g. try playing some mono pink noise on stereo speakers and move your head. Then try again after hard panning the audio to only one of the speakers. This is why adding a real physical center channel makes dialogue clearer in movies. reply hunter2_ 9 hours agorootparentThe center channel also helps by allowing the listener to localize the dialogue differently than the music/FX. When the dialogue comes from a different point in space, it doesn't need to be louder than the music/FX for sufficient intelligibility. When downmixed to stereo or mono or any other non-center-having configuration, the dialog must be mixed well above the music/FX to avoid being masked by it. Sometimes a downmix occurs without this step being given enough consideration, resulting in complaints about the mix, when the original 5.1 (or whatever) mix was perfect but an engineer wasn't involved for downmixing. This is also what Dave Rat is on about, in addition to IM and combing concerns, when explaining why a speaker per instrument sounds better than a mix: not just that it's a single point source, not just that it's doing one single job, but that all the sounds come from discrete points nowhere near each other. Just like an acoustic band using no reinforcement. reply amlib 14 hours agoparentprevI wonder how do they deal with the phasing over so many speakers? Wouldn't an array of speakers require you to do something about that? Specially so if you are stuck with 70s tech. reply JohnBooty 12 hours agorootparentdeal with the phasing over so many speakers? Each performer had their own vertical stack of speakers. So, no phase issues / comb filtering on the horizontal axis. https://www.soundonsound.com/techniques/line-arrays-explaine... reply amlib 12 hours agorootparentI was thinking more in terms of the Wall of Sound as shown in the article. Were they also doing vertical stacks per channel/instrument or were they doing arrays of speakers arranged in a grid per channel/instrument? From what I take it was the later as line arrays weren't a thing at that point, right? reply ssl-3 6 hours agorootparentWith the Wall of Sound, it worked more-or-less like this: The bass guitar gets its own pile of speakers. The lead guitar gets its own pile of speakers. The vocals get their own pile of speakers. (And so on, and so forth, until everything on the stage was covered -- maybe with some mixing on a stack to fit something else in, but probably not for things like Jerry's guitar and Phil's bass). So, yes: There were aspects of the Wall of Sound that absolutely behaved like modern line arrays do (even though \"line arrays\" as we know and use them today did not begin to gain wide popularity until somewhere around the middle of the 1990s). There's no magic necessary to implement a line array: It can just be an array of speakers that are arranged in a line. (Of course, it can also be much, much more complex than that -- but that complexity isn't an inherent part of what a \"line array\" be. It can involve things like phase-steering and per-element EQ and 3D predictions [and 3D measurements!], but it does not have to be that way in order to be a line array.) reply JohnBooty 12 hours agoparentprevThe core of it is that speakers are bad at polyphony You can look at intermodulation measurements for some popular and affordable home speakers here. The TL;DR is yeah, you're going to go from something like -60dB distortion to -40dB when doing synthetic multitone tests, but I would not remotely characterize this as being \"bad\" at polyphony nor the primary reason for the Wall of Sounds primordial \"one vertical speaker array per instrument\" design. https://www.erinsaudiocorner.com/loudspeakers/kef_r5_meta/ https://www.erinsaudiocorner.com/loudspeakers/sony_sscs3_tow... more: https://www.erinsaudiocorner.com/loudspeakers/ The way to avoid it is more speakers, more stacks/arrays/etc. If we're talking about his remarks here I'd characterize his take as \"more vertical arrays\" and not just \"more speakers.\" I realize it's a bit pedantic of me, but some people think that simply adding more speakers equals more betterer sound and it's quite far from the case. In general, multiple loudspeakers arrayed horizontally (this includes MTM center channels in home theater setups) lead to comb filtering. I'm sure you know that, just clarifying for others. If he has written about this elsewhere I'd love to read more! reply jtriangle 11 hours agorootparentI'm simplifying things significantly because I don't expect most HN'ers to know the ins and outs of pro sound. Dave Rat has a good youtube channel, he also has some sort of insider subscription thing that I've never bothered with. I wouldn't say his ideas necessarily translate to every situation, but, he really presents these ideas as tools to use in a toolbox, not as gospel. One interesting thing is that, we generally view comb filtering as universally 'bad', when, in reality, our ears do an excellent job of sorting out comb filtering when it comes to natural sounds. In fact, comb filtering is how we can locate a sound in 3d space with only two reference points (and, if you try, only one reference point moved around a little). That's remarkable, and points back to how speakers comb filter instead of mere comb filtering itself. In practice, say you have a rock band, and your sound system has two arrays with subs spaced 40ft apart. Now, you're going to get a less than ideal pattern from that in the ranges where the bass guitar and kick drum live. How do you fix it? The answer is fairly simple, you simply run bass/kick in stereo, then, you delay the bass on one side by a little, kick on the opposite side just a little, then add some kind of EQ difference to the delayed side of each, then play with the delays until it sounds right. Why does that work? Or does it really work? It's odd, because the math says \"no no, it'll sound bad\", the reality is, it can tighten up that comb to the point you don't really hear it and you can get good bass coverage out of a fairly poor system design. reply hunter2_ 9 hours agorootparentThe opposite delays is an interesting touch. Basically steering the \"power alley / valleys\" (which of course are a gradient of lobes whose actual positions vary by frequency, but when you're in a 60-80Hz valley, you're in a bad spot) of each LF instrument so that instead of being uniform, the one with kick is angled a little left while the one with bass is angled a little right, so any given member of the audience finding themselves in a valley for one is in the alley of the other. And then the chaotic smearing from EQ tweaks makes the valleys even less stark. Hmm! Definitely one of those \"it's good if it works\" type things, I suppose. #808 reply ssl-3 5 hours agorootparentprevOne of the problems with modern PA is that it is hung in stereo. It'd be better if it were a mono stack right in the middle of the stage, but we can't do that because that's where the performers are and two objects can't occupy the same space at the same time. So we do the obvious thing: We put speakers at each side of the stage, instead. This at least solves the obvious Newtonian physics issue. But there's a problem with that. Now we have two sources of audio in one room, and the sounds that many in the audience hear are a sum these two sources, mixed together acoustically -- even though those two sources arrive at the listeners' ears with different amounts of delay (due to relative distance). And when those two sources are playing exactly the same thing (an instrument or vocalist panned to the middle, say, as is quite common), that always results in predictable comb filtering for many people in the room. And it's the same comb filter across the whole spectrum, for all of the sounds (every instrument, every vocalist, every everything) that come from both sets speakers. One approach that Dave uses is to try to make sure that each side of the PA is always playing something different from the other one. This might mean using two kick drum mics (placed differently), and two microphones in front of a guitar cabinet (also placed differently), and so on. One microphone is for the left speakers, and one microphone for the right speakers. They still mix acoustically, but because they're different signals to begin with, they don't comb filter as efficiently as they would if they were identical signals. And because each of these things are all mic'd differently (with one mic per speaker-stack), one instrument's inevitable comb filtering will be different from another instrument's inevitable comb filtering. This randomizes (ish -- it can be predicted and measured) the comb filtering that is involved in the sum of the entire PA, and that can make for a better listening experience for most people. (And where two microphones can't be used, like for a vocalist, then the idea is to do something, anything different between the left and the right output channels for that singular source. It could be different EQ (what a wonderful phase-scrambler conventional EQ is), or delay, or different reverb settings -- but it should be different somehow coming from left and right speakers.) The math works fine. If comb filtering is inevitable, and it is, then it is better to have a random-ish mix of comb filters of diminished depth and that each affect individual musical elements differently than to have one singular and very brazen comb filter that affects everything uniformly for any given listener. The problems with Dave's ideas here are this: It doubles the channel count at the board (which can be worked around just by throwing more money at it), it more-or-less doubles the expenditure on microphones and snakes, and it more than doubles the complexity of getting an initial mix working well. But most importantly: It defies the conventional workflow of trying to mix sounds on a stereo PA system in a huge room like it is the same as listening to a high-end stereo system at home and sitting in the sweet spot. Dave Rat's ideas generally don't/can't combine to make a central sweet spot, or to produce a perfect stereo image: There's a left mix and a right mix, and they're both very deliberately different mixes that are each made from sources that are split up to be as different as is reasonably possible. So the biggest problem with Dave's ideas aren't that they don't make sense somehow (they do make sense), but that it goes against the conventional target. (But the conventional workflow that aims for the idea of precise stereo image as a target is kind of inherently bullshit anyway, because only a small percentage of attendees can stand in that sweet spot to hear that perfect stereo image at one time, while the other 90+% of concert-goers simply can't physically be there (because Newton, again). Most listeners aren't anywhere near centered on the stereo PA, and will hear what psychoacoustically seems to come chiefly from either the right or the left stack (because Haas effect), and will have no ability to be in a place where a good stereo image is possible to begin with. The conventional target sacrifices the auditory experience of many for the benefit of few.) reply soulofmischief 1 hour agoprevInformative article and a good read, although the author clearly has no idea how commas are supposed to work. I had to re-read the very first sentence multiple times to make sense of it, and there were plenty more comma errors throughout the article. reply anonymousiam 14 hours agoprevNot be be confused with this other \"Wall of Sound\": https://en.wikipedia.org/wiki/Wall_of_Sound Despite being an insane murderer, Phil Spector was a musical genius. reply robodan 8 hours agoprevBose sells line array speakers that will happily work if placed behind a standard mic. They are using anti-feedback DSPs to cancel the audio loop this creates. They can be used in a speaker per performer setup. What is weird is that they don't talk about this. There is a graphic on products that can do this that show the speaker behind the mic. That's it, no text at all. I think the moral of this story is that marketing is weirder than engineering... reply tkgally 6 hours agoprevAm I the only one here to have actually heard the Wall of Sound? I attended two of the 1974 concerts where it was used, at UCSB in the spring and at the Hollywood Bowl in the summer. I was seventeen. I had also attended two shows at Winterland in February, where apparently some the components of the Wall of Sound were used before the full system was rolled out the following month. One should not, of course, trust fifty-year-old memories, especially about something so subjective as sound quality. But, for what it’s worth, my recollection of the Hollywood Bowl in particular is very positive—clear, solid sound with distinct separation of the instruments, while not so loud that my ears rang afterwards (as they did after other rock concerts I attended in those days, until I started wearing earplugs to protect my hearing). reply stevehiehn 15 hours agoprevI remember chatting with a sound technician at a concert once and he told me that putting amplification in front of the performers only started happening in the late 60's (ish). Before that musicians were actually subjected to insane DB's by standing only a few meters in front of the amplification. (Don't take this is as fact, but this diagram suggests that he was correct) reply buildsjets 14 hours agoparentSome techniques require this. Ted Nugent wouldn't have gotten the crazy howling feedback out of his semi-hollowbody Gibson Byrdland had he not been standing directly in front a pair of Fender Super Twins pushing 4 15\" drivers. If it doesn't make your pants flap in the breeze, turn it up! reply dekhn 13 hours agorootparentI believe most musicians these days achieve this using a nearby monitor speaker, for example Trey Anastasio from Phish, although I believe he may have adopted newer technology (see https://treysguitarrig.com/2023/08/31/2023-summer/ for more details). He could sustain notes for a long time with his custom hollowbody (like, minutes at a time). reply standardly 13 hours agorootparentThat dude is a wizard. A lot of guitar players look down on fancy pedal setups, but watching his rig rundown video on youtube was mind blowing. I've heard Anastasio get tones and effects you just won't hear anywhere else. reply dekhn 7 hours agorootparentI hadn't seen that before (https://www.youtube.com/watch?v=kZKjKaQdW9w) but it's lots of fun. It's nice to be able to see what devices he's fiddling with to get his effects (most of the concert videos don't show the effects pedals and stomp boxes in detail). He's making heavy use of a looper- he can play some notes, capture them, and loop them continuously in the background (he uses to create a whole group of backing music that runs even when he's not playing actively). I mean if you look at this effects it's not surprising he can get all those tones. He's got multiple signal paths that get split and recombined with various effects applied to one or the other (being played thru speakers that are miked), a Leslie (rotating speaker in a cabinet that adds some interesting warbles), a bunch of different gains with distortion, along with a bunch of \"digitech\" that does real-time complicated distortion. All of those run in chains so effects compound. Some day I really want to have the time to build my own guitar similar to his as well as my own pedals. I've spent various bits of time in the background working on related things, but haven't allowed myself to really focus on it, because I tend to get a bit obsessive until I've fully explored my interests. Here's a nice example of looping to make one instrument sound like many: https://www.youtube.com/watch?v=UsBINddmVcY and here's a person who runs her harp through various effects pedals (she has a library of them): https://www.youtube.com/watch?v=C1Uv7JcnhhI reply llamaimperative 14 hours agoparentprevMust’ve felt pretty amazing, at least for a little while til the injuries started. reply STRiDEX 14 hours agoprevi found this Wired youtube video pretty similar https://www.youtube.com/watch?v=8c-gD4mwI8A sound engineer that worked on coachella and they talk about the switch to vertical stacks of speakers reply stevehiehn 12 hours agoparentVery interesting video, thx reply tom_ 9 hours agoprevDon't care about the grateful dead? Me neither. What you might like instead (as I certainly did) is this one-sided collection of endless forum posts from Stanley about his all meat diet: https://justmeat.co/archives/active-low-carber-forum-posts/ reply DoodahMan 6 hours agoprevlove how i checked HN with 5/28/77's Sugaree playing and come upon this post! love how today you can still see Bear's influence on venues across the world w.r.t the sound setup. here's to Dead&Co's Sphere run, it ain't the same of course but the music never stops (~);) reply derwiki 5 hours agoparent5/28/77 had a banging Sugaree, and is overall a top 5 GD concert recording IMO! reply pastureofplenty 14 hours agoprevWeb designers really, really need to stop putting light grey text on a white background. Good article though. reply Waterluvian 10 hours agoparentI just go into Reader Mode any time it’s supported. A lot of design work is already solved. Gosh now I’m imagining if people did this with books. At least chapter books are mostly the same. reply S_A_P 8 hours agoprevDisappointed that the components were not included in the article. I want to say they used McIntosh hifi amps that were 300 watts per channel. I’m not completely sure however. reply syngrog66 10 hours agoprevnext [3 more] [flagged] causality0 9 hours agoparentNot like it ever did anything anyway. I'd report spam then get the exact same damn e-mail the next day. reply 89vision 10 hours agoparentprevThe wall of sound sounds way more fascinating imho. reply gjmacd 14 hours agoprev [–] The most incredible sound system to hear a band play out of tune and out of key for 90 minutes. reply owenmarshall 11 hours agoparent [–] 90 minutes? At a _Dead_ concert?! That’s enough to cover a typical Playing>Uncle John’s>Drums>Space, and probably not get all the way through the reprises. You’ve easily got another two hours of jam. reply somat 5 hours agorootparentThe joke was \"I went to a Grateful Dead Concert and they played for SEVEN hours. Great song.\" reply dekhn 9 hours agorootparentprev [–] also, they were \"in tune\" and just playing in a mode, rather than a typical key. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Owsley \"Bear\" Stanley, the Grateful Dead's sound engineer, developed the groundbreaking Wall of Sound in 1974, transforming live sound engineering.",
      "Comprising over 600 speakers and cutting-edge sound technology, the Wall of Sound provided unparalleled audio quality and resolved common technical problems.",
      "Despite facing some hurdles and needing modifications for practicality, the Wall of Sound significantly influenced live sound engineering with its avant-garde and inventive approach."
    ],
    "commentSummary": [
      "The Grateful Dead's Wall of Sound was a groundbreaking but pricey and impractical sound system that impacted modern PA technology.",
      "Their innovative sound engineering experiments reshaped the live music sector and were influenced by psychedelic counterculture.",
      "The discussion involves advancements like individual speaker stacks, comb filtering, speaker arrays, and amplifier use in music performances at length."
    ],
    "points": 193,
    "commentCount": 90,
    "retryCount": 0,
    "time": 1715105189
  }
]
