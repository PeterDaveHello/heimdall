[
  {
    "id": 39205020,
    "title": "Infinite Craft: Create and Sort Elements of Nature with Ease",
    "originLink": "https://neal.fun/infinite-craft/",
    "originBody": "üíß Waterüî• Fireüå¨ Windüåç Earth Drag elements to craft Sort by time",
    "commentLink": "https://news.ycombinator.com/item?id=39205020",
    "commentBody": "Infinite Craft (neal.fun)913 points by kretaceous 18 hours agohidepastfavorite423 comments procparam 8 hours agoMy friends and I have played so much already that the list of elements on the sidebar is unwieldy. You can paste this little js snippet into the console to add a basic search feature items = () => [...document.querySelectorAll('.items div.item')] show = (elt) => elt.style.display='' hide = (elt) => elt.style.display='none' search = (text) => (items().forEach(show), items().filter(e => !e.innerText.toLowerCase().includes(text.toLowerCase())).forEach(hide)) inputElt = document.createElement('input'); inputElt.type='text'; document.querySelector('.sidebar').prepend(inputElt) function handle(e) { search(e.target.value) } inputElt.addEventListener('input', handle) reply starshadowx2 6 hours agoparentThis is really cool, thanks. I was just using ctrl+f to find things. I've got like 1200+ words right now so I totally understand the unwieldy-ness. reply cooper_ganglia 7 hours agoparentprevThanks for this! I ended up hitting refresh because of how long my list eventually got, I wish I'd seen this comment 10 minutes sooner! Oh well, I guess now I'm forced to sink in another half-hour this evening! ;) reply smarkov 17 hours agoprevI couldn't find any information but does this use some kind of LLM to derive the combinations from? It makes a request to the backend every time you combine items which sometimes takes >500ms, and also supports some really wild combinations that I highly doubt someone has taken the time to come up with. It would also explain why the icons are emoji's, it would be fairly trivial to ask ChatGPT to give you the result of Fire + Water and an accompanying emoji. reply JeremyNT 17 hours agoparentYou are correct according to this tweet [0]. That may become inaccessible as Nitter dies, but the text is: > Working on an endless crafting game with llama 2 along with a video of this game. [0] https://nitter.cz/nealagarwal/status/1747284257582506102#m reply madeofpalk 17 hours agorootparentAhh, neat application of it. Explains the somewhat dubious combinations I was seeing. reply hinkley 16 hours agorootparentForest + fire = smoke Mud + water = swamp Swamp + plant = Venus Flytrap Okay, okay. Venus Flytrap + Smoke = smoke detectorreply Night_Thastus 15 hours agorootparentTree + Water = river was also pretty baffling. Unfortunate. I can see the appeal of using an LLM for this but the results are pretty mediocre. reply vintermann 10 hours agorootparentAs usual with language models, you have to put in the work yourself to have fun with them. I laughed when \"Vindaloo + Bubble\" gave me \"Burp\", and \"Burp + No Bacon\" gave me \"Sad\" reply madaxe_again 5 hours agorootparentHam Solo + Darth Bacon = Pork Star reply ProllyInfamous 15 hours agorootparentprevTree + Water = River This is a great result, a branching of water/wood. reply nerdponx 10 hours agorootparentI also got Dandelion + Engine = Helicopter, along these same lines. reply cushpush 12 hours agorootparentprevlow-key genius or high-key misunderstood reply TeMPOraL 10 hours agorootparentAs this is powered by an LLM, you are exploring its latent space. That means there isn't one logic behind everything - any association is fair game. Here, probably the strongest one wins. reply teaearlgraycold 10 hours agorootparentprevLLMs are a great way to prompt human ingenuity in mental gymnastics reply cushpush 7 hours agorootparentGreat point. reply ilaksh 13 hours agorootparentprevObviously the prompt to the LLM is just to create the most obvious association. It may not mention \"crafting\" at all. Maybe it does though. Is there something obvious to craft that uses a tree and water in the process? reply grotorea 14 hours agorootparentprevIf you look at a river system from space I guess it looks tree-like with branches. reply cushpush 12 hours agorootparentthe tree of water is the global river system.\" very hydrological reply hombre_fatal 15 hours agorootparentprevMeh, what would be your great response to Tree + Water? A human can only generate a small fraction of the combinations and would have a hard time coming up with most combinations which are already nonsensical. What is your non-disappointing idea for, idk, Tears + Pottery (AI: Bowl) or Money + Salt Lick (AI: Cow) or Skull + Lake (AI: Loch Ness) or Dracula + Pirate (AI: Vampirate) or Curse + Money (AI: Debt)? Now do that thousands of more times. The infinite aspect is the thing that keeps it interesting, I think. The fun is getting a new, weird result like \"Dracula\" and \"Pot of Gold\" and seeing if you can generate new weird results from the existing set. reply dr_dshiv 27 minutes agorootparentDid you figure out how to get math? I‚Äôve been trying so hard! reply baq 2 hours agorootparentprevI somehow got to Mega Evolution which I got from megalodon and some pokemon, but don‚Äôt ask me how to get to Pok√©mon. Anyway, once you get that kind of a modifier, anything‚Äôs game for megaification. reply smus 2 hours agorootparentprevTree + water = canoe reply ceroxylon 14 hours agorootparentprevTree + Water is easy: Mangrove, Bald Cypress, Rhizophora, etc reply hombre_fatal 14 hours agorootparentI wouldn't consider those to be better. You're enhancing the tree side of the equation but gave no examples of the water side. And why didn't you do the rest of my examples? ;) reply TomK32 12 hours agorootparentprevLoch Ness? I skipped to the Loch Ness Monster. Got the Lake only later on. And then I got Nessie. This is a Quagmire reply bpye 5 hours agorootparentYeah Loch Ness Monster + Water gave me Nessie reply TomK32 12 hours agorootparentprevPorkosaurus, Soup Nazi, Sphinxie, Sodium Chloride, Abdominable Snowman, Baconator and both Yeti and Godzilla. And Yogazilla which is a \"First Discovery\". Found myself: Thomas the locomotive reply gs17 11 hours agorootparentprev> Tears + Pottery (AI: Bowl) Urn reply geoelectric 8 hours agorootparentprevMoney + Salt Lick makes sense, if it‚Äôs a Cash Cow. reply dylan604 13 hours agorootparentprevJungle Forest Fruit Nuts reply raible 12 hours agorootparentprevWeed + tree = bong! reply Zobat 13 hours agorootparentprevWindow + Dune = Sandwich That one surprised me. Sandwich + Sphinx = Sphinxwich This one delighted me. The Sphinxwich doesn't combine well with other stuff though. reply cartucho1 11 hours agorootparentI got Atlantis + Spongebob = Atlantis Squarepantis :D reply MrJohz 13 hours agorootparentprevI found fish + fire = sushi amusing - it isn't necessarily wrong, but it also doesn't feel right either... reply hinkley 12 hours agorootparentHow did you get to fish? Because boy do I have a story for you. Mine starts in Atlantis, then Poseidon gives me a fish. Then two fish turned into a shark and I ended up with a sharknado. Then I found the titanic, we hit an iceberg, I found a treasure and then pirates chased me, but I got away, sold the treasure for money and became the richest man, then climbed Mt Everest, and later had a tea party. Anyway, there has to be a better way to get fish than Unda da Sea. reply skykooler 10 hours agorootparentprevI've also got a few where it just mashes adjectives together; so far I've found Time Poseidon, Rainbow Steam Robocloud and Broken Unicorn, among other similar ones. reply vharuck 7 hours agorootparentMy best so far is Superninjaghostmansnowghostman reply vintermann 10 hours agorootparentprevBankrupt + Pirate = Captain Jack Sparrow Bacon + Judaism = No Bacon reply sva_ 12 hours agorootparentprevMy favorite was Lotus Flower + Mud = Buddha reply vintermann 10 hours agorootparentOh, that's a good one! I'm trying to collect religions and countries. reply tmtvl 12 hours agorootparentprevNorth America + Fire = Canada. Naturally. reply hinkley 11 hours agorootparentAnd if you set Canada on fire? Maple syrup. reply vharuck 8 hours agorootparentprevNuclear power station + Tsunami = Fukushima reply frud 13 hours agorootparentprevFish + fire = sushi reply hinkley 11 hours agorootparentDid you know that Bill Gates is the richest samurai? reply drekipus 11 hours agorootparentprevI got swamp with plant + water. Then swamp + fire = dragon reply ActionHank 15 hours agorootparentprevMegalopolis + Volcano = Pompeii reply hinkley 11 hours agorootparentSushi + Asia = Japan Japan + Destruction = Godzilla Godzilla + Megalodon = Cthulhureply recursivecaveat 14 hours agorootparentprevI got (hurricane + crocodile = hurricodile) lol reply hinkley 10 hours agorootparentCthulhu + richer = Richthulhu reply nopassrecover 9 hours agorootparentprevI think it‚Äôs the first time AI has made me chuckle. I ended up with ‚ÄúRiddle‚Äù, so I combined that with ‚ÄúTornado‚Äù and it gave me ‚ÄúTwister‚Äù which I thought was a great Christmas Cracker pun, and then when I combined Riddle with ‚ÄúBottle‚Äù it gave me ‚ÄúGenie‚Äù. reply nealfunlover 10 hours agorootparentprevHere‚Äôs all the combinations I‚Äôve came up with so far: Swamp + Mud = Quagmire Divorce + God = Odd Sun + Hourglass = Time Glass + Hourglass = Time Ice + Oasis = Penguin Sand + Stone = Pyramid Mirage + Time = Illusion Dinosaur + Lightning = Godzilla Oasis + Water = Mirage Egg + Time = chicken Golem + tide = Titan Titan + time = Chronos Poseidon + lighting = Zeus Titan + Chronos = Cronus Time + Fire = Sun Sun * Titan = Apollo Ash + Mud = Clay Godzilla + Love = God ? + ? = Spongebob Unicorn+Gold=Alchemy Unicorn+Alchemy = Philosopher‚Äôs Stone Gold+Alchemy=Midas swamp+chicken=duck duck+roast=goose goose+goose=flock flock+wind=flight Narwhal+time=unicorn Lightning + Treasure = lots of stuff (Rich, idk Narwhal+unicorn=narwhalicorn Jonah+time=narwhal Whale+oasis=jonah Plant+seed=tree noah+ark=flood curse+jesus=cross bank+intrest=money dandelion+cactus=desert Back to the future+riddle=time travel back to the future+time=delorian Desert+indiana jones=tresure reply Kunsang 5 hours agorootparentAstronomer + Hangover = Astrologer reply nicoty 5 hours agorootparentprevNice. At some point I got \" Crypto-gangbangasm + Cthulhu Lilith Porn\". reply ithkuil 54 minutes agorootparentMy favourite was: \"Chuck Norris\" + \"a-hole\" = \"dead a-hole\" reply dhc02 14 hours agorootparentprevMight be worth using farside for the link, which I found out about today on HN. https://farside.link/https://twitter.com/nealagarwal/status/... reply andrei 17 hours agorootparentprevjust link to the real thing :) [0] [0]: https://twitter.com/nealagarwal/status/1747284257582506102 reply Aachen 16 hours agorootparentSuch that it's already dead? reply WaxProlix 16 hours agorootparentprevOff topic, why is nitter dying? I've noticed the main instance's SSL cert is down for a bit now, and other instances are pretty rate limited. Did something happen/change? reply burkaman 15 hours agorootparentAll (non-paid) Twitter accounts are now rate-limited, which makes a shared Nitter instance untenable. It's probably still possible to host a personal instance for yourself, although they might ban your account for it. https://github.com/zedeus/nitter/issues/983#issuecomment-191... reply password4321 15 hours agorootparentprevhttps://news.ycombinator.com/item?id=39161194 reply samstave 17 hours agorootparentprevNeat. Campfire+sushi took about 10 seconds before it gave up and did not combine them. reply jarboot 15 hours agorootparentThe request looks like \"https://neal.fun/api/infinite-craft/pair?first=Phoenix&secon...\" so it's probably typically caching the combination of phoenix+seeds but if there is no cache entry it would use llama to make up something. If there's a lot of attention on the site the llm service might be down or overloaded. And given the exponential/factoral (?) amount of combinations this may be reached surprisingly quickly. Just a guess. As an aside, the game is technically interesting, being a really simple example of using llm generation for game mechanics. But it is not engaging at all and feels nonsensical to me, especially when compared to little alchemy https://littlealchemy2.com/. I'm not trying to be negative and this isn't a dig on creativity of the wonderful Neal but more points to the immaturity of llms applied to games, maybe to my overexposure to chatgpt, and maybe a prediction that human touch will always be required to make something entertaining. I'm curious how llms will fit into an engaging game experience in the future. reply SirMaster 13 hours agorootparent>As an aside, the game is technically interesting, being a really simple example of using llm generation for game mechanics. But it is not engaging at all and feels nonsensical to me You just gotta make a game out of it. For example challenge yourself to try to craft \"pizza\". Can even try to do it in as least number of crafts as possible. Point is, just crafting random things to see what it spits out is OK, but trying to use your own logic to combine things to get to an arbitrary solution you come up with is much more engaging, at least to me. Challenge your friends to craft some specific \"thing\". Think of something you might think could be hard to craft to, and ask them to do the same and see who can get there first, or in the fewest steps. reply jandrese 11 hours agorootparentThat's a fairly big challenge since the game gets less coherent the longer it goes on. The early matches generally make sense, but after about 3 levels you start getting loops, and after 5 levels you start getting nonsense or outright failures from queries. If you figure each of the things is an input parameter to a LLM this makes a lot of sense. They tend to have short memories and struggle with higher level introspection. Great for demos, but fraught with problems when using them to do real work. reply SirMaster 11 hours agorootparentHmm, I‚Äôm not finding it to be too big of a challenge. It‚Äôs a bit challenging yeah, but me and my friends are challenging each other to get to words and we can usually find a way to make it. Things like ‚ÄúGodzilla‚Äù, ‚ÄúUniverse‚Äù, ‚ÄúVampire‚Äù, ‚ÄúOptimus Prime‚Äù, ‚ÄúVodka‚Äù, etc are just some examples we did. I don‚Äôt seem to be having problems going dozens of levels deep without loops and not running into many query failures. Results that are deep are still making some logical combinational sense to me at least. Some words we haven‚Äôt been able to make, but that doesn‚Äôt mean it‚Äôs not possible. It just means we need to get more creative and sometimes think outside the box. There are so many ways you can approach getting to a certain result in my experience so far. Doing this has been fun and challenging so far for me and my friends FWIW. reply hombre_fatal 14 hours agorootparentprev> But it is not engaging at all and feels nonsensical to me, especially when compared to little alchemy https://littlealchemy2.com/. On the other hand, Little Alchemy doesn't have answers to the most basic combinations. Air + Earth = Dust, but Dust doesn't combine with Water. Earth + Water = Mud, but Mud doesn't combine with Air. Earth + Earth = Land, but Land doesn't combine with Fire. It may be more sensical since it limits combinations to 0.01% of what's possible, but I don't think that makes it more interesting. reply Aachen 16 hours agorootparentprevThere's tons of combinations that take forever and nothing ends up happening. That's how I got around to the comment thread (clean+satan is why I'm here): I'm waiting for the latest combination to time out reply araes 16 hours agorootparentSimilar. Snowmobile Farm and Sandbox Farm both appear to timeout with no response. However, that said, the idea itself is a neat idea, and could quite easily be turned into game ideas somewhere. reply lovegrenoble 12 hours agoparentprevI love silly and pointless web pages, here another one: https://sharkle.com reply hmcq6 12 hours agorootparentFirst click brought me right back to neal.fun lol reply Calavar 13 hours agoparentprevAn LLM seems like overkill for a project like this. Why not word2vec? reply thatguysaguy 10 hours agorootparentThe open vocabulary aspect seems important. Word2vec would never let me make \"Pirate Pope Wars\". reply legel 11 hours agorootparentprevI had the same thought. Some of the surprising discoveries I made with word2vec embeddings: human + robot ~= cyborg silicon + electricity ~= solar cells virtual reality + reality ~= augmented reality As reported here: https://web.archive.org/web/20160806040004if_/http://blog.yh... reply TeMPOraL 10 hours agorootparentprevMore dimensions? reply Jaxan 15 hours agoparentprevThis might explain how I got ‚ÄúSuper volcano‚Äù and ‚ÄúSupervolcano‚Äù reply geoelectric 8 hours agorootparentOne‚Äôs really big, and the other is really cool? reply leroman 17 hours agoparentprevI thought they were using some kind of vector space searches like embedding.. no idea if that's the case reply IshKebab 15 hours agorootparentYeah I'm pretty sure you could do this just with the classic word embeddings (king =queen + man - woman). Maybe it doesn't work as well as with a full LLM. reply adtac 10 hours agorootparentAddition won't work for things that depend on the order of operations. If salt + water is ocean and water + fire is steam, what's salt + water + fire? Is it salt + steam or ocean + fire? Associativity and commutivity in vector addition doesn't translate well to semantic meaning. Extrapolating your example, it'd also mean: v_king - v_queen ~= v_father - v_mother v_king - v_royalty ~= v_father - v_parent v_king - v_father ~= v_royalty - v_parent ~= v_queen - v_mother I don't see why those should all be true. Intuitively, trying to satisfy O(N^2) semantic pairings with vectors that are optimised for a very specific and different numerical operation (cosine similarity) feels like something that won't work. I'd imagine errors get amplified with 3+ operands. reply TeMPOraL 10 hours agorootparentIsn't the reason for lack of associativity/commutivity is that you're doing operations (addition/subtraction) that have them, and then snapping the result to the closest one of fixed number of points in your output dictionary? The addition is fine, loss of information is in the final conversion. reply adtac 7 hours agorootparentThere's definitely some lossy compression when you snap it to the nearest known vector: enumerating every word ever written in human history wouldn't even come close to the 2^(16*D) representable points for a D-dimensional float16 embedding vector. In fact, even adding two float16 values is a form of lossy compression for most additions. But I'd be surprised if either of those were the primary reason. The words \"sea\" and \"ocean\" are different vectors but they'll be very close to each other. salt + water = sea and salt + water = ocean both sound correct to me so the problem is more about whether the v_salt + v_water can even get to the vicinity of either v_sea or v_ocean. reply leroman 0 minutes agorootparentIf we constrain our selves to a pool of words of say Wikipedia entries, minutes names and maybe some other stuff, and use a \"super node\" like \"addition\" to kind of act as a math operation.. maybe this makes more sense? tibbon 17 hours agoprevI don't get it; maybe it's broken in my browser. I can drag 4 types of items to a floating grid that connects them but then nothing happens. EDIT: Ah, you drag items on top of each other for them to change. Instructions unclear, and I am dumb. reply crazygringo 15 hours agoparent> Instructions unclear, and I am dumb. You're not dumb -- it's not only unclear, but the interface actively suggests the opposite of what you're supposed to do. One of the best lessons I ever learned was from Don Norman's famous book, The Design of Everyday Things. Which basically teaches you that the user is almost never dumb, but rather human. And that the responsibility of understanding how to interact with an object, or program, always lies with the creator of that object or program. The designer. It's their job to design something so that it teaches you how to use it. (His most quoted example involves how a plate on a door invites you to push it, while a handle invites you to pull -- and this way you'll never try to pull a door that needs to be pushed open, or vice-versa.) In this case, the interface invites you to drag things among the various pre-existing points, to continue the \"constellation\". It does nothing whatsoever to suggest that it would make any sense to drag the labels on top of each other. Indeed, previous experience suggests that this would simply lead to overlapping and obscured labels, so we actively avoid it. And the lines that get drawn between nearby points and labels goes even further to suggest that this is a game or experience about connecting things in a graph-like way -- which, once again, overlapping does not fit into conceptually. I would never have thought to drag things on top of each other if I hadn't come here to the comments. reply hinkley 16 hours agoparentprevThe lines are misdirection and need to be removed. What's with all the lines to the little moving dots? Haven't figured out what those do yet. reply nomel 15 hours agorootparentNothing. They're completely confusing. At first, I thought maybe the little specs is where the combination happened, so as they passed between a set, they would automatically get crafted based on that set, in a neat chaotic way. Nope. Then I thought maybe I had to set up the \"recipes\" and wait, or click things, or I don't know. I didn't realize it's just a basic drag drop combination thing until I saw the video, after coming here to see if anyone else was having trouble with Safari. reply NegativeLatency 11 hours agorootparentI wonder if that's how it worked originally but was dialed back because you'd have n^2 queries per UI interaction reply neogodless 17 hours agoparentprevThank you! Yes the instructions were missing, and it's hard not to assume that Firefox isn't supported. I was having the same (bad) experience as you. reply cjauvin 17 hours agoparentprevYou are not dumb at all, I think it's not terribly clear indeed. reply bagels 1 hour agoparentprevYes, it'd be better if you could just tap two items, and they filled the useless central pane instead. reply nottorp 10 hours agoparentprevYep, at first i tried to make geometric arrangements out of multiple items too. Water surrounded by 5 fire in a circle - ish - did absolutely nothing. Only by checking the HN comments i figured out you have to combine items. reply mrjh 16 hours agoparentprevNot dumb at all. It's bad UI. reply Sohcahtoa82 16 hours agorootparentI didn't have a problem understanding it, but then again, I've played these Alchemy games before. It was one of the first games I ever played on my first Android back in 2010. reply xerox13ster 4 hours agorootparentYeah this is what I thought of and I fell back on my old Alchemy habits trying to get to Life to create man and all the other stuff. maybe I'm forgetting or it's different with this game but I haven't gotten there yet, but I've gotten several greek gods and a cyborg, so that's sort of life. Just makes me want to play Alchemy, though. Even Doodle God doesn't scratch the itch Alchemy did in 2011. I found it on StumbleUpon. Oh those were the days. I wonder if I still have the APK on my SkyDrive... reply LonelyWolfe 6 hours agoparentprevOn mobile it's just tapping, no drag and drop required. reply alluro2 14 hours agoparentprevYou can just click / tap on 2 items consecutively, without drag and drop. reply crazygringo 14 hours agorootparentI don't understand what you mean. Clicking does absolutely nothing for me. Clicking items in the right column does nothing; clicking items I've already placed does nothing. Clicking items consecutively does nothing, in either part of the screen. I don't see how to use this at all without drag and drop. reply ipsum2 14 hours agorootparentOn mobile you click, on desktop you drag and drop. reply LonelyWolfe 6 hours agoprevLemme know if there's another way to press every combination: let maxElementReachedForElement = {}; let totalElements = 0; let firstElement = 0; let secondElement = 0; setInterval(function() { document.getElementsByClassName('mobile-item')[firstElement].getElementsByClassName('item')[0].click(); document.getElementsByClassName('mobile-item')[secondElement].getElementsByClassName('item')[0].click(); totalElements = document.getElementsByClassName('mobile-item').length; secondElement = (secondElement + 1) % totalElements; if (secondElement == 0) { maxElementReachedForElement[firstElement] = totalElements; if (Object.keys(maxElementReachedForElement).some(item => maxElementReachedForElement[item]maxElementReachedForElement[item]Skinnerbox. reply whats_a_quasar 10 hours agoprevI went down a Kaiju rabbithole. Here are the monsters I created - Dragon + Lazer = Dragonzord - Dragonzord + Sandpunk = Sandzord - Dragon-Rex + Hippopotamus = Hippodragon - Mega Dragonzord (didn't see recipe) - Hippodragon + Mega Dragonzord = Mega Hippodragonzord - Mega Hippodragonzord + Flying Circus = Mega Flying Circus Hippodragonzord - Zombie + Mega Dragonzord = Zombie Dragonzord - Swan + Megazord = Swan Megazord - Sandzord - Steam Dragonzord - Swan Dragonzombie Megazord - Mega Swan Dragonzord Megazord The more ridiculous this gets the more fun I'm having! Everything after dragonzord was a first discovery. reply thfuran 5 hours agoparentPerhaps you'd be interested in my research: Ultrazord + lord of the salad = saladzord Dragonzord + Godzilla = godzord Godzord + lord of the salad = Godzord of the salad Lord of the rings + godzord = gozord of the rings Gozord of the rings + godzord = godzord of the rings Godzord of the salad + godzord of the rings = godzord of the salad rings Godzord of the salad rings + pirates of the Caribbean = godzord of the Caribbean Godzord of the Caribbean + donkey Kong = donkey kongzilla Megazord+megazord= mega megazord Donkey kongzilla + mega megazord = donkey kongzilla megazord (I forgot how lord of the salad happened) reply john-shaffer 3 hours agorootparentMostly playing around with stacking \"powerful\" words: King Kong + Power Ranger = King Kong Ranger (first) Megazord + Power Ranger = Power Megazord Megazord + Power Megazord or Artzord + Megazord = Ultrazord Dragonzord + Art = Artzord (first) Groot + Mega Artzord = Grootzord Megazord + Grootzord = Gigazord (first) Godzord + Giga Artzord = Giga Godzord (first) Giga Godzord + Giga Titanic Gigazord = Giga Titanic Godzord (first) Giga Titanic Godzord + Mega Power Godzillazord = Super Giga Mega Titanic Power Godzillazord (first) I also got firsts on both Earthquake Laser and Laser Earthquake, but I'm not sure what the recipes were. Trying to get Zebra was an interesting sequence. Tamer + Africa = Tamerlane (first) Tamerlane + Terror = Timur (first) Timur + Asia = Genghis Khan Genghis Khan + Bellerophon = Alexander the Great Alexander the Great + Unicorn = Bucephalus (first) Alexander the Great + Bucephalus = Horse Horse + Africa = Zebra reply russdill 44 minutes agoparentprevOh dear... Sandy Hook + Cereal Killer = Adam Lanza reply TheGlav 8 hours agoprevSome of my First Discoveries: * Frankenstein + angel = Angelstein. (Then Angelstein + Baby = Angelina Jolie.) * something + Something from Michael Jackson = Dangerous. * Dangerous + Sherlock Holmes = Moriarty. * Multiverse + White => Dark Multiverse + Law => Dark Law. * Dark Law + us Constitution => Dark Constitution. * Quark + Ocean -> Neutreno * Thor + Guardian -> Heimdall * Batman + vet => Batvet * Avengers + Multiverse => Endgame + Dark Multiverse => Dark Engame * Iron man + Star Lord -> Iron-lord * Wandavision + Groot -> I Am Groot * Cleanest + Terminator -> Cleaninator. * A series of werepigeon, Space Pigeon, Mars Golem, Werehand, Icepigeon, Werechristmas Tree, Dust pigeon, Snow pigeon, Dust mosquito. Some fun ones: * us America + hole = Donut + hole => Doughnut * Money Water -> Vodka + Casino => Roulette + Vodka -> Russian Roulette + Certainty -> Suicide. * Toxic Waste + batman -> The Joker. * Divorce + Ex-wife => Splitting headache. * Lake + Batman -> Aquaman + Batman => Justice League * Captain America + Thor -> Avengers * Avengers + Justice League -> Superheroes + Galaxy -> Marvel + Galaxy -> Guardians of the Galaxy * Guardians of the Galaxy + Guardian -> Groot * Guardians of the galaxy + Nebula -> Gamora * Guardians of the galaxy + Gamora -> Star Lord * Time Travel + Iron Man -> Terminator. * Government + Terminator -> Robocop. * Terminator + Divorce -> Arnold Schwartzenegger. Confusing ones: * Darth vader + Toxic Waste => Superman? * Pirate + Superman => Captain America reply Ldorigo 2 hours agoprevI know it's all LLM randomness, but some of these are pretty fun/quirky. E.g. sheep + murder gave me \"silence\" which puzzled me for a while until I thought of silence of the lambs. reply shmageggy 1 hour agoparentteenager + apple pie = pregnancy reply gloosx 15 hours agoprevThis shows quite well that AI has zero common sense whatsoever, the results of most combinations I made are just nonsense. The associations are really vague and to me it is not interesting at all to look at random pictures and words appearing out of totally unrelated stuff. I really like man-made alchemy games though where all combinations are pre-defined and actually feel right. reply csallen 14 hours agoparentOne person's specific usage of AI, in a specific game, where the AI was constrained to only coming up with words that have an associated emoji, is not a referendum on whether or not AI itself has common sense. That's like saying paint is boring because someone made a crappy painting, or code is limited because someone made an app you don't like. reply gloosx 3 hours agorootparent>where the AI was constrained to only coming up with words that have an associated emoji Sorry but clearly the game description is about joining two things together into a logical craft and AI is contrained with that ‚Äì not coming up with random words that have an associated emoji. In doing so the AI is not convincing, the combinations just feel random. And yes, you need common sense to come up with logical combinations which makes a game like this interesting in the first place. reply deely3 12 hours agorootparentprevSure, its not a referendum, but its a prominent sign that even with this task AI can fail spectacularly. P.S. Today I learned from AI that Sphinx + Drunk = Sphinxter. reply lanternfish 12 hours agorootparentThat's a sick pun though - I'd definitely consider that a success reply smfjaw 9 hours agorootparentprevMy computer programs always crashed in my first year of college, clear sign computers fail spectacularly reply halfmatthalfcat 8 hours agorootparentDid you tout your first year college programs as world changing and potentially a risk to the species because it's so intelligent/has awesome powers? reply csallen 7 hours agorootparentIs the author of this game doing that? reply thatguysaguy 10 hours agorootparentprevWhat would the right answer be? reply FredPret 13 hours agoparentprevIt's also pretty good at lots of combos. If you asked a human to come up with all of these, it'd be pretty hard. reply KTibow 11 hours agoparentprevI found it fun regardless. HN, I proudly produce to you my newest invention: Beer + Lens = Beer Goggles. reply cubefox 14 hours agoparentprevI wouldn't say that AI has \"zero\" common sense. Rather it has a lot of common sense. For example, I wondered what would happen with \"wind\" and \"paper\". Perhaps paper ... storm? No, \"kite\". It's better than me. reply gloosx 3 hours agorootparentpaper windmill? reply willy_k 11 hours agoparentprevOne man‚Äôs nonsense is another‚Äôs profound truth. mud + brick = Adobe, for example. Doesn‚Äôt seem to make sense, yet it does. reply gloosx 3 hours agorootparentadobe is literally mudbrick, this is too easy ‚Äì like fire and water = steam reply LonelyWolfe 6 hours agoprevI made this to play the game automatically. It's a simple WinForm application but I've left the JavaScript version in there too: https://github.com/roman015/InfiniteCraftAutomation reply Cieric 14 hours agoprevApparently I made 2 first discoveries, Meteorism and Rap God. Seems interesting, but after a while of things not combining it gets a little stale. I agree with others I kind of would like to see the lineage of the elements. I know of this video where Carykh graphed is own combination game, but all new element names were user provided. https://www.youtube.com/watch?v=rQWwfYSUckY reply Cieric 14 hours agoparentWell I guess I wasn't doing weird enough combinations before, I now have (Hip hop + Crusade) Crusade Hop, T-Pain, (T-Pain + Painting) T-Painting, Haikupoid, Rapping, Slim Shady and the others already mentioned. I do like that I can convince it to make completely new concepts as long as the 2 inputs are weird enough. reply Cieric 14 hours agorootparentOkay truly done at this point, I have work to do. https://i.imgur.com/96rl0Ul.png reply resolutebat 48 minutes agorootparentLooking forward to WWE Shartlemania! reply avereveard 14 hours agorootparentprevI've the entire solar system is there a way to know if it's a first? reply oops 14 hours agorootparentIt says \"First discovery\" e.g. https://i.imgur.com/iagu3WZ.png reply avereveard 2 hours agorootparentaw on mobile doesn't have that panel reply oops 14 hours agoparentprevi discovered monkey pox :( reply jerbear4328 12 hours agoprevI got some funny ones, like: - Bread + Bread = Toast, + Fire = Burnt Toast, + Fire = Ash - Toast + Disaster = Pop Tart, + Fire = Hot Pocket, + Coffee = Coffee Pocket (First Discovery) - Hot Pocket + Cappuccino = Cappuccino Pocket (First Discovery), + Pop Tart = Cappuccino Pop Tart (First Discovery) - Coffee Pocket + Disaster = Coffee Spill (First Discovery), + Pop Art (sic) = Jackson Pollock - Jackson Pollock + Megadisaster = Jackson Pollock - Artist + Disaster = Picasso Apparently nobody else was insane enough to combine Coffee, Hot Pockets, and Pop Tarts. The AI is just confusing sometimes, though: - Pompeii + Kaleidoscope = Mosaic? - Burnt Toast + Water = Coffee, + Cake = Breakfast, + Mosaic = Cereal? - Coffee Pocket + Broken Window = Coffee? (though idk what I expected) reply whats_a_quasar 10 hours agoparent- Dragon + Lazer = Dragonzord I got Mega Dragonzord! First discovery. But I didn't see how. Here is a list of new kaiju I first discovered: - Dragonzord + Sandpunk = Sandzord - Dragon-Rex + Hippopotamus = Hippodragon - Hippodragon + Mega Dragonzord = Mega Hippodragonzord - Mega Hippodragonzord + Flying Circus = Mega Flying Circus Hippodragonzord - Zombie + Mega Dragonzord = Zombie Dragonzord - Swan + Megazord = Swan Megazord - Sandzord - Steam Dragonzord - Swan Dragonzombie Megazord - Mega Swan Dragonzord Megazord The more ridiculous this gets the more fun I'm having reply karaterobot 11 hours agoparentprevI was surprised that Penguin + Werewolf = Werepenguin. I mean, it makes sense, I just didn't expect it to be a valid response. reply jampa 11 hours agoparentprevI got Tea + Dinossaur = Tea Rex Thought it was pretty clever reply jamager 9 hours agoparentprevOh my this is so addictive. Got some first discoveries such as... - The Abominable Ramen Pigman - The Colorblind Lama - The Dalai Llama - Pig of Thrones - The Purple Monkey With A Laser Beam Attached To Its Head reply felurx 10 hours agoparentprevI played around quite a lot and got many funny combos too. My memory is poor, but one of my favs was Gay + Vampire = Twilight reply bloaf 11 hours agoparentprevI have created the words: - Slimy Palm - Momfia - Sand-T-Rump - Stinkadult - Cheesus vs Jesus - Surfing Blues Brothers - Gnomeo and Juliet - Game of Thrones = Pornstar + Sword Fight - Putin = Stool (with poop emoji) + Russia reply nonbirithm 6 hours agoprevThis is really clever. \"Football Jesus\" is \"Tebow\", so I am now happily exploring the Tebowverse. There is even a robot incarnation named \"Tebot\". reply f1nlay 17 hours agoprevWas working on the very same idea alongside a friend of mine, we happened to launch a few weeks ago. Quite a lot more fleshed out than Neal's version, if anyone wants to check it out! https://allchemy.io/ Edit: sorry if anyone is bumping into errors! We're running into bottlenecks with our supposedly auto-scaling database - working on it reply pantsforbirds 16 hours agoparentI think the slow animation + fullscreen notification removes a good portion of the fun of the game. The stats are neat, but I wonder if using a log feed (like a killfeed in call of duty) would be a more enjoyable experience for the user. I love how fast infinite craft is to iterate through the combinations while the \"allchemy\" approach makes it feel like I have a crafting time attached to every new combination reply enonimal 16 hours agorootparentJust tagging here to say I enjoy the site, but I agree with my parent ^ the popup slows me down reply rrr_oh_man 15 hours agorootparentprevAbsolutely +1 Came here to say this reply jerbearito 17 hours agoparentprevLove it. Would it be possible to speed up the animations? reply f1nlay 17 hours agorootparentWill add an ability to speed up/outright disable animations in the next update! Was a bit of an oversight on our part as once you start amassing a decent amount of items you encounter the new item animation less frequently - but in the early game it's definitely quite irritating! reply rgbrgb 14 hours agoparentprevI love it and agree with other commenters on animations. Any traction on subscriptions? I'm always curious if there's a biz model that works for this kind of game on web (besides ads). It's funny because I think you could absolutely sell it for $4.99 on the app store if you throw it in a native shell but I have a hard time imagining people paying for it in their browser. Would be happily surprised if that's not the case! reply etrautmann 17 hours agoparentprevis this completely independently developed? the literal exact same idea at the same time? crazy reply hipadev23 17 hours agorootparentThey're both inspired by Little Alchemy 1 & 2, PopCap's Alchemy, or if we really squint, The Incredible Machine from 1993. https://en.wikipedia.org/wiki/The_Incredible_Machine reply neuronexmachina 15 hours agorootparentCome to think of it, LLMs with the right prompts would be pretty handy for a Scribblenauts clone: https://en.wikipedia.org/wiki/Scribblenauts Probably outputting a format supporting the same characteristics as the Object Editor: https://scribblenauts.fandom.com/wiki/Object_editor reply xerox13ster 4 hours agorootparentprevYou have to reallllly squint to see Incredible Machine in these. These are basic associations, not physical interactions. The closest you get to IM is \"what happens if I stick these two things together\" but it's more guessing and less input output. In IM, you know what each thing does and see the output of each action, so you can iterate: placement, angle, special attributes like fire or light. It's not just stack two possibly related icons to see what you get. With these you either know the association exists or you're doing conceptual guesswork. There's no testing and iterating on a hypothesis, at a point once all known associations have been exhausted, iteration looks like permutation. reply omoikane 15 hours agorootparentprevAlso \"The Alchemy Game\" from 1997. https://archive.org/details/msdos_Alchemy_Game_The_1997 reply rrr_oh_man 15 hours agorootparentprevOH GOD THANK YOU! I was playing this on a Packard Bell Windows '95 PC with integrated loud speakers and a mic. Super high tech for the time. BUT: The German-language full version of that game had been pre-installed in the Start Menu (?!), so if you deleted the start menu entry by accident, you'd need to reinstall Windows to get that game back. Or at least, that was young me's solution to the problem. reply etrautmann 16 hours agorootparentprevwow, I totally forgot, thanks! I remember playing that game or a derivative, I think on my father's 386 sometime around 1995? reply SnooSux 17 hours agorootparentprevThis idea is pretty common, I remember playing a similar game on my iPod Touch nearly 10 years ago reply burkaman 17 hours agorootparentMaybe this one: https://www.youtube.com/watch?v=caKj6rGEdyM. I was obsessed with this for about a week in high school. reply f1nlay 17 hours agorootparentprevCompletely independently! We're a team of two University students funding this out of our own pockets. About 8 months work (on and off) from first prototype. reply helboi4 17 hours agorootparentprevThis idea is super old. There was a game like this that was popular like 10 years ago. This is a worse version. reply obiefernandez 16 hours agoparentprevThis is so much fun. I was just the first to craft MUCK ASTLEY... definitely gave me a good laugh reply f1nlay 16 hours agorootparentGlad to hear you're enjoying it - an excellent item to discover! reply niceice 15 hours agoparentprevGood work! What kind of traction are you getting on the premium version? That approach is sensible given the underlying costs. Would love to get an idea of how it worked out in practice. reply bricemo 7 hours agoparentprevDoes this also use generative AI to create the results? reply artemonster 17 hours agoparentprevthe animations are tedious reply qwertox 17 hours agorootparentThis is like Quake vs. modern games. In Quake, you get the story in the booklet and it's up to you to learn the game. In modern games you're constantly interrupted with an explanation of what you can do, should do, and so on. I love this one (Infinite Craft) because it's up to you to discover and discovery is really fast because nothing interrupts you. allchemy.io should have a mode without animations and explanations, maybe explanations when you hover over crafted objects on the right pane. reply david422 17 hours agorootparent> In Quake, you get the story in the booklet and it's up to you to learn the game. Or any old console game. Part of the fun was cracking open the little instruction pamphlet and reading all about each enemy and each weapon with little pictures etc. reply f1nlay 16 hours agorootparentWorth noting that Allchemy lets you peer through every single item generated by anyone, via the Itempedia: https://allchemy.io/items You can find some interesting stuff! reply blorenz 17 hours agorootparentprevThis. Duolingo is in the same vein which interrupts my flow. Animations shouldn't continuously block the interaction of the experience. The majority should probably be ancillary and be a visual flourish. reply stronglikedan 17 hours agorootparentIf you use Duolingo app on a phone that supports power saving mode, it significantly cuts down the animations. reply duxup 17 hours agorootparentprevI like them. I assume these sites are about exploring lots of different things, even if they don't work as much as some would like. reply f1nlay 17 hours agorootparentprevYou should be able to click right through them - but agreed. Will add an option to disable them in the near future! reply iandanforth 17 hours agoprevFrankly I want to watch an animation of all the combinations as they are discovered/created over time from their backend. Seeing a growing 'tech tree' from the exploration and imaginations of users would be delightful. reply itschrisyoko 10 hours agoparentSeeing a list populated of the First Discoveries along with the elements combined to make them would be a lot of fun to see. reply rkagerer 14 hours agoparentprevBarackasaurus was fun. reply dgrin91 17 hours agoprevIts fun, but annoying that you have to do a lot of mouse dragging. Some more click-based interface would be better. Edit: some measure of progress would also be good. I cant know how much I missed Edit2: Megalodon + Cemetery seems to break the system, spits back nothing. reply volemo 16 hours agoparentI don't think any measure of completion is applicable here: the combinations are generated by an LLM (and the game even claims to be infinite). reply supriyo-biswas 17 hours agoparentprevIt seems like you can tap on mobile. I don‚Äôt know if the author changed this after your comment. reply slmjkdbtl 16 hours agoparentprevThis would work better on mobile where dragging is easier and more intuitive reply thfuran 7 hours agorootparentOn mobile you don't have to drag, just click one and then the other. reply jomsk1e 4 hours agoprevVery fun! Most hilarious result I found is James Bond + Tarzan = Jane Bond reply orenlindsey 12 hours agoprevThe LLM that powers this is surprisingly well trained (for what it is doing). I don't know if it's using a pre-existing one, but if you open dev tools and find the api you can play around with any combination of things. Just as an example, I put in \"Protestant\" and \"Spiritual Experiences\" and it gave \"Pentecostal\" which is exactly right, and I didn't expect it to know that. Now I have to find how to get that in the game. reply eshack94 8 hours agoprevAngel + Zeus = God. o_O This little game is too entertaining. I hope the author open sources it, or writes a technical blog post on how it's implemented. reply subarctic 6 hours agoparentHow many greek gods have you managed to get? I got Zeus, Hera, Athena, and Poseidon, plus Achilles if he counts. reply PrincePhoenix 7 hours agoprevI got a few new discorveries. Silence of the lambs + Sharknado = Silence of the Sharks, Hannibal lecter + Quagmire = Hannibal quagmire, Hannibal Quagmire + Sharknado = Sharknibal. reply PrincePhoenix 7 hours agoparentHannibal Sharknado, Sharknibaldo reply launch_queue 3 hours agoprevAnd with the combination of Buddha and Ghost, I have reached Enlightenment. I will take that as a sign that I have completed the game. reply joshelgar 16 hours agoprevSaw this on Neal's twitter a couple of days ago, it inspired me to make the food version :) https://twitter.com/joshelgar/status/1750141793377686000 reply Kronopath 14 hours agoprevI managed to kinda break it. I managed to get ‚ÄúSteam Engine‚Äù and ‚ÄúHacker‚Äù. I combined those to make ‚ÄúSteam Punk‚Äù. (Fair enough.) Everything that‚Äôs combined with Steam Punk ends up being stupid and boring. Steam Punk + Zombie = ‚ÄúSteam Zombie‚Äù Steam Punk + Hacker = ‚ÄúSteam Hacker‚Äù Steam Punk + UFO, which you‚Äôd think would be ‚ÄúZeppelin‚Äù or something, is instead ‚ÄúSteam UFO‚Äù. Steam Punk + Illusion is, bafflingly, ‚ÄúSteampunk‚Äù, all one word, and with a different emoji! reply athrowaway3z 14 hours agoparentI had a first discovery for 'Justice League of Mars', then one for 'Justice League of Moon', but not for the 'Justice League of Mars and Moon' reply epiccoleman 8 hours agoprevI am very proud to be the first discoverer of both Jerry Garcia and Trey Anastasio. It suits me. I also am the discoverer of a pretty good LLM pun - \"Grateful Red,\" which I got by combining one of my various Grateful Dead related findings with \"Mao Zedong.\" I had some other interesting discoveries as well, including the \"Abominable Crocodile\", \"Abominable Orc\", and whatever the hell a \"Pterodump\" is. reply bricemo 7 hours agoparentHow do you know you are the first discoverer? Does it highlight this in some way? reply thfuran 7 hours agorootparentIt says \"First Discovery\" under it. reply fallinghawks 5 hours agoprevI seem to have discovered a \"steam garage\" and I can't imagine what that might be. That has led to a \"steam palace,\" which seems to have already existed. ETA: also the Cinderham and Louis Prime Fun little game and pretty silly, thanks reply rawcal 11 hours agoprevTerminator + Robot = T1000 T1000 + T1000 = T2000 I'm at T-32000000 and still going reply joshfee 11 hours agoparentThe most unexpected 2048 clone reply calebj0seph 10 hours agoparentprevSame here with Monkey Island 2 + Monkey Island 3 = Monkey Island 4. I'm now at Monkey Island 12357990. reply krick 7 hours agoprevFirst I discovered Slaughterhouse Five. Then Slaughterhouse Ten. Then Slaughterhouse Fifteen‚Ä¶ Kinda got tired at Slaughterhouse One Hundred Thousand. reply rabbits_2002 13 hours agoprevThere used to be webgames with a similar premise. Pretty fun. I think some of the recipes don‚Äôt make sense and it would be a lot better if each thing only had one recipe to create it. reply m_akane 57 minutes agoprevI think I hit some sort of character limit with \"Spongepunk Christmas Seapony + Steampunk Squidpunk Sph\" reply hasoleju 4 hours agoprevAt first I thought that I can use the elements from the sidebar to built something in the main area, since it is wider than the sidebar. reply mbowcut2 14 hours agoprevThis is fun. It would be interesting to build a single graph of concepts that all users contribute to. Then you wouldn't have to run LLM inference on every request, just the novel ones, plus you could publish the complete graph which would be something like an embedding space. reply lilyball 14 hours agoparentA lot of combinations return instantly, so I assume that it is in fact caching a lot. reply mbowcut2 14 hours agorootparentoh I just realized that 'isNew' in the response refers to a global set, not the user set. So, I guess it's doing exactly what I said lol. reply lilyball 14 hours agorootparentI just went back and did some new combinations with early ones and I'm still getting intermittent delays even though all early combinations must be done, so I assume part of this is just the server itself being a little overloaded and so even responses that are cached remotely but not locally may experience delays. reply dysoco 15 hours agoprevWoah this is extremely addictive and has a lot of potential, I love games like Little Alchemy and this goes to 11. Surely some things are weird and are to be expected from the usage of AI, for example, I managed to somehow create Neon Genesis Evangelion (try to!) but I can't seem to replicate any more animes, mangas or shows. reply Minor49er 16 hours agoprevJane + Library creates a Content-Security-Policy error in Firefox 122 for Mac (looks like it should turn into Book normally) Also, ashes + water = soup. I need to try some more recipes reply banana_giraffe 9 hours agoprevSome of these combinations are really clever, some are really dubious, but still fun. At any rate, the first 1000 or so that a quick python script found: https://gist.github.com/Q726kbXuN/6620a12ade4b77dacce93d1c96... reply yellowapple 10 hours agoprev> fire + fish = sushi Literally unplayable. EDIT: apparently I'm the first to discover \"Captain Tokyo\"? EDIT: and \"Captain Tiamat\". Had no idea those are even things. Seems like I'm the designated captain discoverer. More to the list: - Captain Penguin Universe - Captain Penguin Planet - Captain Penguin Multiverse - Captain Poseidon Multiverse - Captain Jack Poseidon - Captain Jack Tiamat - Captain Everything Jack Sparrow (which can't combine with anything, apparently) - Captain Penguin Zombee - Captain Penguin Zombee Universe (also can't combine with anything) reply urbandw311er 8 hours agoparentSeems you‚Äôre playing with it quite a lot for somebody who‚Äôs described it as ‚Äúliterally unplayable‚Äù. reply yellowapple 1 hour agorootparentI see jokes at the expense of cooked fish in sushi don't get past you :) reply microwavedair 9 hours agoprevI made Ryan. I don't know who Ryan is or why you combine wind with _______ (I forgot)to get Ryan, but this is hilarious. Some of the other things I created and thought were funny: Tractor monster(first discovery!), Flying soggy toast, Flying dry toast, Abominable noodleman, Flying spaghetti monster, Unicorn toast, Iceland, reply OwseiWT 9 hours agoparentI love how you have several random descovires like Flying spaghetti monster and Abominable noodleman. Then just Iceland. Just. Iceland. reply digging 17 hours agoprevGames like this already exist and have comparable complexity, so calling it infinite and using an LLM backend feels overhyped. For example, Castle + Fortress = Castle? City + Town = Castle. Castle + Wall = City? Metropolis + City = Megalopolis. Ok we're getting somewhere cool here, let's see how big it gets: Megalopolis + City = Metropolis?! Finally, it just failed to combine War + Tunnel. It blinks for a minute and then gives up. I would have said \"Sappers\"? Edit: There are actually many such failures for higher-order combinations which is strictly not infinite. Other combinations described above might technically fit the bill but ceasing functionality does not : / reply Jare 16 hours agoparentI just described it to my colleagues as > this uses GenAI in order to attempt being truly infinite, or at least not bounded by their ability to design and input combinations themselves It's fair to go for that name imho. Not strictly correct, but 100% fair. reply digging 15 hours agorootparentThat's just generative or procedural though. It's ok of course that it's not truly infinite, I'm just posting because it was disappointingly finite. There were concepts I was trying to build up to that simply didn't exist. So I'm not even sure if it's less bounded than human design, just less manual effort to build. All I'm saying really is, if it had just been called \"AI Generative Craft\" I would have had nothing to say in the comments. I would have gotten what I expected. reply thfuran 15 hours agorootparent>There were concepts I was trying to build up to that simply didn't exist. They may not, but that you didn't find them precisely where you expected doesn't really convince me that they don't. reply sprobertson 10 hours agorootparentprevCouldn't build up to doesn't mean they didn't exist... it took me a while to make a \"Sandwich\". But after spending enough (too much) time with it, I was able to construct some things that show it's not disappointingly finite: * \"Bollygraff\" * \"Teen Mom 12\" * \"Hackimus Prime\" * \"Billionaire King + Sushi Burger Venus\" (sic) One potentially frustrating part is that some things turn out to be black holes... for example, combine \"Trump\" with anything and it tends to return \"Trump\". There are also plenty of loops, and some that refuse to combine at all. reply TomK32 4 hours agorootparentYou can combine Trump with the Empire State Building to get Trump Tower. Also I think Trump + Money was Bankruptcy. And there's Trumpthuluh obviously. reply nwiswell 2 hours agorootparentI got Rich + Rich = Richer Richer + Richer = Richest Richest + Poop = Trump reply ithkuil 1 hour agorootparentO got trump by making \"god\"+\"a-hole\" Then I did \"T-Rex\"+\"trump\" and I got \"T-Rump\" I was impressed by the creativity reply sprobertson 12 hours agoparentprevI've found retrying after a bit will often return the new result. I suspect the frontend is timing out before the backend comes up with a result, which is eventually cached by the time of the second attempt reply magneticnorth 15 hours agoparentprevI mean, there are only a finite number of words in the English language, so of course something like this isn't truly infinite. But I've wandered off into a space of fantastical creatures: rainbow + explosion = unicorn, from there I've gotten phoenix, \"steam unicorn\", narwhalicorn. Others have gotten into food items? I don't even know how I'd get there. And you've gotten into infrastructure and war. There's enough to explore here that I'm ok with it being called infinite. reply etrautmann 17 hours agoparentprevnot the OP but as a fun exploratory hobby project, you don't think the presentation is reasonable? seems totally fair to me reply digging 16 hours agorootparentI mean, not really, no. I'm not trying to be super critical here, just, it's not even presented as a exploratory LLM project. It's only presented as \"infinite\", and it's... not. reply furyofantares 17 hours agoparentprev> There are actually many such failures for higher-order combinations which is strictly not infinite I suspect a service error, either the service/LLM not responding fast enough sometimes when a combination isn't already known, or the LLM not giving a usable result. I mean, there aren't infinite emojis or words so it isn't gonna be infinite but I think some of the errors I've seen have been more transient. reply lilyball 14 hours agoprevI combined Fish and Fire and got Sushi, which is a dish that is famously made without any fire whatsoever. The game also decided to pair the Wheat emoji with the word Amber. reply lightbendover 14 hours agoparentYou should consider yourself lucky that you haven't witnessed the torched sushi craze [yet]. reply rishikeshs 13 hours agoparentprevMay be wasabi is fire? reply lilyball 11 hours agorootparentI got wasabi by adding fire to sushi. reply mym1990 9 hours agorootparentWasabi == spicy == hot == fire? reply personjerry 5 hours agoprevOk I played this way too long, the most ridiculous straight up is `Steampunk Bahamut Batpunkestilla Rex` reply PawgerZ 13 hours agoprevVery cool. I combined 'double rainbow' and 'ocean'; thought I broke it as it loaded for like 20 seconds; then got Atlantis. Eventually I got to 'mermaid', combined it with 'fire', and got 'siren'. But the emoji was a police siren, not the mythical creature. I can see how the LLM would be confused, though. reply ilaksh 13 hours agoparentThe LLM was not confused. It just fed \"siren\" into the image generator and that's an ambiguous term. reply PawgerZ 12 hours agorootparentYes, that's the confusion I was talking about. Sorry, I meant to say the image generator was confused. The confusion how I came to the assumption that it was an LLM hooked to an image generator. Interestingly, I did break it on flower + eclipse. reply 173 more comments... GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The concept of the app involves manipulating elements such as water, fire, wind, and earth by dragging them around.",
      "The app also includes a sorting feature based on time.",
      "This app allows users to interact with different elements and organize them based on a time-based sorting system."
    ],
    "commentSummary": [
      "The conversation focuses on a game or website that allows users to combine elements to create new items or concepts.",
      "Participants discuss their experiences with the game, the AI models used, and propose ways to improve it.",
      "There are differing opinions on the combinations, with some finding them nonsensical and others appreciating the creativity and novelty. Other similar games and suggestions for improving the interface are also mentioned."
    ],
    "points": 913,
    "commentCount": 423,
    "retryCount": 0,
    "time": 1706715268
  },
  {
    "id": 39210507,
    "title": "Creating PDFs Larger than the Universe: Exploring the Limits of Adobe Acrobat",
    "originLink": "https://alexwlchan.net/2024/big-pdf/",
    "originBody": "Making a PDF that‚Äôs larger than Germany Posted 31 January 2024 Tagged with code-crimes, drawing-things I was browsing social media this morning, and I saw a claim I‚Äôve seen go past a few times now ‚Äì that there‚Äôs a maximum size for a PDF document: Terrible Maps @TerribleMaps Maximum size of a PDF, version 7: 381 km √ó 381 km. https://commons.m.wikimedia.org/wiki/File:Seit‚Ä¶ 5:14 PM - 30 Jun 2023 Some version of this has been floating around the Internet since 2007, probably earlier. This tweet is pretty emblematic of posts about this claim: it‚Äôs stated as pure fact, with no supporting evidence or explanation. We‚Äôre meant to just accept that a single PDF can only cover about half the area of Germany, and we‚Äôre not given any reason why 381 kilometres is the magic limit. I started wondering: has anybody made a PDF this big? How hard would it be? Can you make a PDF that‚Äôs even bigger? A few years ago I did some silly noodling into PostScript, the precursor to PDF, and it was a lot of fun. I‚Äôve never actually dived into the internals of PDF, and this seems like a good opportunity. Let‚Äôs dig in. Where does the claim come from? These posts are often accompanied by a ‚Äúwell, actually‚Äù where people in the replies explain this is a limitation of a particular PDF reader app, not a limitation of PDF itself. They usually link to something like the Wikipedia article for PDF, which explains: Page dimensions are not limited by the format itself. However, Adobe Acrobat imposes a limit of 15 million by 15 million inches, or 225 trillion in2 (145,161 km2).[2] If you follow the reference link, you find the specification for PDF 1.7, where an appendix item explains in more detail (emphasis mine): In PDF versions earlier than PDF 1.6, the size of the default user space unit is fixed at 1/72 inch. In Acrobat viewers earlier than version 4.0, the minimum allowed page size is 72 by 72 units in default user space (1 by 1 inch); the maximum is 3240 by 3240 units (45 by 45 inches). In Acrobat versions 5.0 and later, the minimum allowed page size is 3 by 3 units (approximately 0.04 by 0.04 inch); the maximum is 14,400 by 14,400 units (200 by 200 inches). Beginning with PDF 1.6, the size of the default user space unit may be set with the UserUnit entry of the page dictionary. Acrobat 7.0 supports a maximum UserUnit value of 75,000, which gives a maximum page dimension of 15,000,000 inches (14,400 * 75,000 * 1 ‚ÅÑ 72). The minimum UserUnit value is 1.0 (the default). 15 million inches is exactly 381 kilometres, matching the number in the original tweet. And although this limit first appeared in PDF 1.6, it‚Äôs ‚Äúversion 7‚Äù of Adobe Acrobat. This is probably where the original claim comes from. What if we make a PDF that exceeds these ‚Äúmaximum‚Äù values? The inner structure of PDFs I‚Äôve never dived into the internals of a PDF document ‚Äì I‚Äôve occasionally glimpsed some bits in a hex editor, but I‚Äôve never really understood how they work. If I‚Äôm going to be futzing around for fun, this is a good opportunity to learn how to edit the PDF directly, rather than going through a library. I found a good article which explains the internal structure of a PDF, and combined with asking ChatGPT a few questions, I was able to get enough to write some simple files by hand. I know that PDFs support a huge number of features, so this is probably a gross oversimplification, but this is the mental picture I created: %PDF-1.6 objects object 1 object 2 object N xref trailer startxref %%EOF The start and end of a PDF file are always the same: a version number (%PDF-1.6) and an end-of-file marker (%%EOF). After the version number comes a long list of objects. There are lots of types of objects, for all the various things you can find in a PDF, including the pages, the text, and the graphics. After that list comes the xref or cross-reference table, which is a lookup table for the objects. It points to all the objects in the file: it tells you that object 1 is 10 bytes after the start, object 2 is after 20 bytes, object 3 is after 30 bytes, and so on. By looking at this table, a PDF reading app knows how many objects there are in the file, and where to find them. The trailer contains some metadata about the overall document, like the number of pages and whether it‚Äôs encrypted. Finally, the startxref value is a pointer to the start of the xref table. This is where a PDF reading app starts: it works from the end of the file until it finds the startxref value, then it can go and read the xref table and learn about all the objects. With this knowledge, I was able to write my first PDF by hand. If you save this code into a file named myexample.pdf, it should open and show a page with a red square in a PDF reading app: %PDF-1.6 % The first object. The start of every object is marked by: % % obj % % (The generation number is used for versioning, and is usually 0.) % % This is object 1, so it starts as `1 0 obj`. The second object will % start with `2 0 obj`, then `3 0 obj`, and so on. The end of each object % is marked by `endobj`. % % This is a \"stream\" object that draws a shape. First I specify the % length of the stream (54 bytes). Then I select a colour as an % RGB value (`1 0 0 RG` = red), then I set a line width (`5 w`) and % finally I give it a series of coordinates for drawing the square: % % (100, 100) ----> (200, 100) %% [s = start]% ^%| %v % (100, 200) > stream 1 0 0 RG 5 w 100 100 m 200 100 l 200 200 l 100 200 l s endstream endobj % The second object. % % This is a \"Page\" object that defines a single page. It contains a % single object: object 1, the red square. This is the line `1 0 R`. % % The \"R\" means \"Reference\", and `1 0 R` is saying \"look at object number 1 % with generation number 0\" -- and object 1 is the red square. % % It also points to a \"Pages\" object that contains the information about % all the pages in the PDF -- this is the reference `3 0 R`. 2 0 obj > endobj % The third object. % % This is a \"Pages\" object that contains information about the different % pages. The `2 0 R` is reference to the \"Page\" object, defined above. 3 0 obj > endobj % The fourth object. % % This is a \"Catalog\" object that provides the main structure of the PDF. % It points to a \"Pages\" object that contains information about the % different pages -- this is the reference `3 0 R`. 4 0 obj > endobj % The xref table. This is a lookup table for all the objects. % % I'm not entirely sure what the first entry is for, but it seems to be % important. The remaining entries correspond to the objects I created. xref 0 4 0000000000 65535 f 0000000851 00000 n 0000001396 00000 n 0000001655 00000 n 0000001934 00000 n % The trailer. This contains some metadata about the PDF. Here there % are two entries, which tell us that: % % - There are 4 entries in the `xref` table. % - The root of the document is object 4 (the \"Catalog\" object) % trailer > % The startxref marker tells us that we can find the xref table 2196 bytes % after the start of the file. startxref 2196 % The end-of-file marker. %%EOF I played with this file for a while, just doing simple things like adding extra shapes, changing how the shapes appeared, and putting different shapes on different pages. I tried for a while to get text working, but that was a bit beyond me. It quickly became apparent why nobody writes PDFs by hand ‚Äì it got very fiddly to redo all the lookup tables! But I‚Äôm glad I did it; manipulating all the PDF objects and their references really helped me feel like I understand the basic model of PDFs. I opened some ‚Äúreal‚Äù PDFs created by other apps, and they have many more objects and types of object ‚Äì but now I could at least follow some of what‚Äôs going on. With this newfound ability to edit PDFs by hand, how can I create monstrously big ones? Changing the page size: /MediaBox and /UserUnit Within a PDF, the size of each page is set on the individual ‚ÄúPage‚Äù objects ‚Äì this allows different pages to be different sizes. We‚Äôve already seen this once: > Here, the MediaBox is setting the width and height of the page ‚Äì in this case, a square of 300 √ó 300 units. The default unit size is 1/72 inch, so the page is 300 √ó 72 = 4.17 inches. And indeed, if I open this PDF in Adobe Acrobat, that‚Äôs what it reports: By changing the MediaBox value, we can make the page bigger. For example, if we change the value to 600 600, Acrobat says it‚Äôs now 8.33 x 8.33 in. Nice! We can increase it all the way to 14400 14400, the max allowed by Acrobat, and then it says the page is now 200.00 x 200.00in. (You get a warning if you try to push past that limit.) But 200 inches is far short of 381 kilometres ‚Äì and that‚Äôs because we‚Äôre using the default unit of 1/72 inch. We can increase the unit size by adding a /UserUnit value. For exaple, setting the value to 2 will double the page in both dimensions: > And now Acrobat reports the size of the page as 400.00 x 400.00 in. If we crank it all the way up to the maximum of UserUnit 75000, Acrobat now reports the size of our page as 15,000,000.00 x 15,000,000.00 in ‚Äì 381 km along both sides, matching the original claim. If you‚Äôre curious, you can download the PDF. If you try to create a page with a larger size, either by increasing the MediaBox or UserUnit values, Acrobat just ignores it. It keeps saying that the size of a page is 15 million inches, even if the page metadata says it‚Äôs higher. (And if you increase the UserUnit past 75000, this happens silently ‚Äì there‚Äôs no warning or error to suggest the size of the page is being capped.) [Edit, 1 February 2024: some extra zeroes slipped into the original version of this post ‚Äì it‚Äôs a million inches, not a billion. Thanks to mrb on Hacker News for spotting the mistake!] This probably isn‚Äôt an issue ‚Äì I don‚Äôt think the UserUnit value is widely used in practice. I found one Stack Overflow answer saying as such, and I couldn‚Äôt find any examples of it online. The builtin macOS Preview.app doesn‚Äôt even support it ‚Äì it completely ignores the value, and treats all PDFs as if the unit size is 1/72 inch. But unlike Acrobat, the Preview app doesn‚Äôt have an upper limit on what we can put in MediaBox. It‚Äôs perfectly happy for me to write a width which is a 1 followed by twelve 0s: If you‚Äôre curious, that width is approximately the distance between the Earth and the Moon. I‚Äôd have to get my ruler to check, but I‚Äôm pretty sure that‚Äôs larger than Germany. I could keep going. And I did. Eventually I ended up with a PDF that Preview claimed is larger than the entire universe ‚Äì approximately 37 trillion light years square. Admittedly it‚Äôs mostly empty space, but so is the universe. If you‚Äôd like to play with that PDF, you can get it here. Please don‚Äôt try to print it.",
    "commentLink": "https://news.ycombinator.com/item?id=39210507",
    "commentBody": "Making a PDF that's larger than Germany (alexwlchan.net)477 points by alexwlchan 11 hours agohidepastfavorite81 comments jl6 2 hours agoPDF is a fabulous format. I mean, it‚Äôs an awful format in so many ways, technically speaking, but the net effect of having a self-contained static file in your custody stands in blissful contrast to the user-hostile dynamic/SaaS website that can be taken away at a moment‚Äôs notice. PDF/A is the true PDF - it strips out most of the dangerous cruft. Anyway, if you like weird PDF hijinks, here‚Äôs a polyglot PDF/A CSV file that is also its own original soundtrack as a polyglot Amiga soundtracker mod: https://www.lab6.com/6 reply martin_a 52 minutes agoparent> PDF/A is the true PDF As someone working in the graphic industry, I'd say PDF/X is the true PDF, but ymmv. :-) reply throwaway290 1 hour agoparentprevDoes it anything else? Maybe pwn me via my PDF viewer?;) reply TeMPOraL 17 minutes agorootparentIt contains Bitcoin hashes, rendering them one by one as it mines them. reply danbruc 56 minutes agoprevSo what is the actual limit if any? I just had a quick look at ISO 32000-2:2020 [1] and think the answer is none or implementation depended if you want. In the file format a media box is a rectangle, a rectangle is an array of four numbers, and a number is either an integer or a real. Numbers are represented as strings, so there is no a priori limit on their range and there seem to be no requirements on the minimum or maximum range of values an implementation has to support. The appendix only says that IEEE 754 is a commonly used format to represent reals and that this might impose limits. [1] https://developer.adobe.com/document-services/docs/assets/5b... reply nayuki 5 hours agoprevI'll analyze PNG for comparison. The largest width and height is 2147483647 (2^31 - 1). Using the pHYs chunk (physical pixel dimensions), the lowest density we can specify is 1 pixel per metre. So, 2 billion metres (2 gigametres) is somewhat bigger than the diameter of the sun at 1.39 Gm. https://en.wikipedia.org/wiki/Orders_of_magnitude_(length)#g... Using the sCAL chunk (physical scale) would allow extremely large dimensions because it uses ASCII floating-point. reply lifthrasiir 3 hours agoparent> Using the sCAL chunk (physical scale) would allow extremely large dimensions because it uses ASCII floating-point. AFAIK sCAL is more about the image's subject, not the image itself. A 1:10,000,000 scale world map would be > endobj 2 0 obj > endobj 3 0 obj > endobj 4 0 obj > >> endobj 5 0 obj > endobj 6 0 obj > stream BT /F1 48 Tf 185 400 Td (Hello World)Tj ET endstream endobj trailer > reply alexwlchan 3 hours agoparentArgh! I knew I was going to make a numerical mistake somewhere, thanks for spotting it. Correction will be up shortly. Thanks for spotting it! :D And thanks for the text example! This looks like what I was trying, but clearly I had a mistake somewhere. reply dingensundso 2 hours agorootparentSpotted another math mistake: > The default unit size is 1/72 inch, so the page is 300 √ó 72 = 4.17 inches. reply wodenokoto 5 hours agoparentprev> \"15,000,000,000.00 in\" and \"that the size of a page is 15 billion inches\", but it should be 15 million. Can you help me count zeroes? Why is it million and not billion? reply jetrink 4 hours agorootparentThe numerical and word versions are equal, but they're both wrong. 15 billion inches is to the distance from the Earth to the Moon. reply croes 4 hours agorootparentprev>If we crank it all the way up to the maximum of UserUnit 75000, Acrobat now reports the size of our page as 15,000,000,000.00 x 15,000,000,000.00 in ‚Äì 381 km along both sides, matching the original claim. If you‚Äôre curious, you can download the PDF. 15 billion inches are 381,000km. The original claim is the limit is 15 million inches. reply mrb 4 hours agorootparentprevHe put too many zeroes. It should be \"15,000,000.00 in\" or 15 million. reply whartung 9 hours agoparentprevIs the xref at the end of a PDF required or not? Seems like it is in the spec. reply jchw 8 hours agorootparentBy the spec, yes. Some PDF readers will parse it anyway, some will not. In my experience depending on the renderer the xref table can be varying degrees of malformed before things go wrong. Edge's old PDF reader (the one before Acrobat and after PDFium) for example seemed to tolerate just about anything, falling back to the latest version of objects if the xref table was broken. There's also other mistakes you can make, like for example, the xref table requires carriage returns (each entry in the table is supposed to be an exact number of bytes) but some PDF readers will still interpret the xref table even if the carriage returns are missing. reply whartung 8 hours agorootparentAs I understand it, the xref entries don‚Äôt require a carriage return, but they require a fixed line length. If you don‚Äôt want to use a CR, you can pad with a space. So CR/LF, space/LF, and space/CR are all valid endings. reply jchw 8 hours agorootparentYep:[1] > The byte offset in the decoded stream shall be a 10-digit number, padded with leading zeros if necessary, giving the number of bytes from the beginning of the file to the beginning of the object. It shall be separated from the generation number by a single SPACE. The generation number shall be a 5-digit number, also padded with leading zeros if necessary. Following the generation number shall be a single SPACE, the keyword n, and a 2-character end-of-line sequence consisting of one of the following: SP CR, SP LF, or CR LF. Thus, the overall length of the entry shall always be exactly 20 bytes This is interesting. Never actually saw anything other than CRLF in practice, even inside of PDF files that otherwise were LF-only. [1]: https://opensource.adobe.com/dc-acrobat-sdk-docs/pdfstandard... page 41 reply mrb 9 hours agorootparentprevIt is required according to the standard. But in practice most PDF viewers don't care. They may complain the PDF is \"damaged\" or \"no valid xref was found\", but they will render it perfectly fine. reply whartung 9 hours agoprevIt seems germane at this point to paraphrase Steven Wright. ‚ÄúI have a map of the United States. It‚Äôs actual size. ‚Äú reply galaxyLogic 4 hours agoparentI have the map of US in my cell-phone. I'm somewhat confused by its directions however when I look at the map and want to go somewhere. Is the top-part of the map where I'm moving? Or is the top-part North? Seems it is not North and that is confusing because maps I've seen before have North at the top always. If I turn 90 degrees, the map turns around. But I thought it was I who turned around. And if I stop, the map cannot know where I'm going because I'm not going anywhere. So it is almost like I have to start moving before the map can tell me where to turn. Or if I hold the smart-phone in front of my eyes the top of the map is towards the sky. Am I supposed to look at the map from above? What are some good tactics on how to use Google-map on your cell-phone? reply TeMPOraL 7 minutes agorootparent> What are some good tactics on how to use Google-map on your cell-phone? For navigation? 1. Don't activate navigation. It's broken six ways to Sunday, and burns through battery like there's no tomorrow. Use route preview instead (i.e. the step after searching, but before activating the voice nav proper). 2. Use your fingers to rotate the map so it always faces the same way you're going. 3. If confused, recenter and press the compass so it rotates to have North at the top, and continue from there. Now FWIW, I use Google Maps when navigating on foot/scooter, or as a pilot in the car. If I were a driver... I'd probably buy TomTom or whatever nav that's not shit. reply TowerTall 3 hours agorootparentprevI really hate that too. You are in a intersection and the voice says \"Drive north for x miles/km\". What is wrong with \"turn right and drive for x miles/km\"? I normally have zero clue in what direction north is especially when I am in a location i have never been before. I drive a bike and have the phone in my pocket and can therefore not see any arrow that the app might display. I only have the audio to navigate from. reply robertlagrant 22 minutes agorootparentThat's odd. My Google Maps tells me to turn left or right. It doesn't use compass directions. reply jeffhuys 1 hour agorootparentprevThat‚Äôs Google maps for you. Try another one, most have way better voice cues (amongst other things!). reply p1mrx 1 hour agorootparentprevIf you want North to be up, tap the compass icon. reply iggldiggl 1 hour agoparentprevUmberto Eco also has something to say on that subject ‚Äì On the Impossibility of Drawing a Map of the Empire on a Scale of 1 to 1: https://s3.amazonaws.com/arena-attachments/881694/cb6119367b... reply fuzztester 8 hours agoparentprevI have a map of the Universe. Dunno, it keeps expanding ...........................................,............................................................................................................................ reply remoquete 1 hour agoprevThis post reminds me of Umberto Eco's intellectual divertissements. More specifically, this fantastic piece, \"On the Impossibility of Drawing a Map of the Empire on a Scale of 1 to 1.\" https://s3.amazonaws.com/arena-attachments/881694/cb6119367b... reply JackSlateur 1 hour agoprevLong story short: the original tweet makes a confusion between PDF (the file format) and adobe acrobat (the PDF reader) : the 381km2 is an acrobat limit, not a PDF limit Funny document, still reply mr_mitm 43 minutes agoparentInteresting, I always though Acrobat was the reference implementation of PDF. reply yannis 6 hours agoprevAnd of course you can try and produce this pdf using TeX. In this post https://tex.stackexchange.com/a/27482/963 I created a pdf of 15283 pages (lettersize) filled with lorem ipsum text and without the program running out of memory. reply gr33nq 10 hours agoprevCoincidentally, I just finished watching a video that explored the same topic of massive and unique PDF files: https://www.youtube.com/watch?v=ZvVNRRQjDh8 reply NooneAtAll3 9 hours agoparentyeah, I went here to post it as well not every time one can see a whole game integrated into a PDF! reply 0134340 8 hours agoprev>Please don‚Äôt try to print it. Sounds like a print bomb waiting to happen. Last time I had a printer it was next to impossible to cancel a print job on Windows. Back when people had wifi printers that were open or ill-secured, those were fun times. reply foreigner 2 hours agoparentOnly slightly more reliable method: unplug the printer and throw the computer out the window. reply kome 12 minutes agoparentprevwhen i read > Please don‚Äôt try to print it. my first reaction has been: you are not my mom. >:-) reply matheusmoreira 5 hours agoparentprev> it was next to impossible to cancel a print job on Windows It's still impossible. The only reliable method I've found consists of turning the printer off and then deleting the print job in the queue. Only way to get Windows to actually delete it. Doesn't work unless the printer is sitting right next to me, of course. I have no idea why this is so hard. reply askvictor 3 minutes agorootparentAnd sometimes windows won't delete it from the print queue as it can't talk to the printer. Fun times. reply poulsbohemian 10 hours agoprevAbout 30 years ago I interviewed to be a summer intern at Microsoft, and one of the interviewers asked a question very similar to this but regarding Excel. This is the kind of topic that never gets old for understanding a person‚Äôs curiosity and ability to dissect the potential issues. reply anotheraccount9 9 hours agoprevFor a moment I wasn't sure if I wanted to click on the link. reply chaxor 7 hours agoparentOn an only slightly related note: is there any good way to check PDFs for malware/executables? If I'm stuck with an attempt at it, the best I can think of is opening in a new QEMU or docker with no Internet access, but that's 1) a fair but of work to check something, and 2) probably not even that secure. Using some cli tool, like xxx, bat, or ranger, that does some processing to extract the text and looking at just that feels more secure - but I know it really isn't. What is a simple tool to \"clean\" PDFs? An ML tool that does QEMU/docker/no-net to extract the content, turns that into game, and saves a typst/latex template with it would probably be the best possible outcome - but that's a decent (yet potentially very lucrative) task. reply worewood 6 hours agorootparentWhat you mean with \"PDFs with malware/executables\"? If you're talking about embedded active content within them, then a reader application can just ignore/not run it. If you're talking about a crafted PDF that exploits, let's say, font rendering bugs inside the reader than it's near impossible. Keep your applications updated. reply arunsivadasan 1 hour agorootparentThere is a Chrome addon called SquareX https://chromewebstore.google.com/detail/kapjaoifikajdcdehfd... The founder is pretty reputed in the Cybersecurity field. reply peddling-brink 4 hours agorootparentprevFor analysis, I‚Äôve used Didier‚Äôs tools. If you just want a safe way to open it, upload it to a cloud storage provider which destructively renders the pdf. Box or Google drive should work. https://blog.didierstevens.com/programs/pdf-tools/ reply qwertox 9 hours agoparentprevIt also screams buffer overflow. reply maxerickson 9 hours agorootparentPDF readers are probably mostly pretty hardened against \"naive\" non-conforming content. reply kirubakaran 8 hours agorootparent> probably mostly pretty hardened Quite possibly perhaps that might be true-ish to some extent, I think, but take that with a grain of salt, I'm not an expert, that's just my wild guess :-p reply maxerickson 7 hours agorootparentIt's pretty ridiculous to peel that off the following qualifier. Readers have been aggressively attacked for a long time. It's certainly not impossible that some basic demonstration PDF will cause an issue, but it's probably not reasonable to expect it. reply oneseven 8 hours agoprevSlightly tangential: if you are hacking on PDFs, manually or otherwise, this is an incredibly useful tool: https://pdfcpu.io/ (not the author, just a user) reply DontBreakAlex 1 hour agoprevWait, pdf files aren't binary ?! reply gaazoh 20 minutes agoparentI just had the exact same reaction! So I opened a random PDF I had laying around, and yes, it's mostly a text format. Some (most) objects are binary data streams, but some are also text data. Likewise, objects may or may not be compressed, obviously compressed streams are binary data. But the file structure is text, some objects are xml, and you can figure out quite a lot of stuff just by looking at a pdf in a text editor, and it might not even be that long: the single page PDF I just looked at is just over 1500 lines long, I can definitely manually scroll through it (although offsets are in bytes, not lines, which make them not very useful for manual lookup). reply msarris 3 hours agoprevSo I guess the question is, how did she figure out the size of the entire universe? reply kepano 10 hours agoprevI cannot let this opportunity go by without quoting On Exactitude in Science by Borges in its entirety \". . . In that Empire, the Art of Cartography attained such Perfection that the map of a single Province occupied the entirety of a City, and the map of the Empire, the entirety of a Province. In time, those Unconscionable Maps no longer satisfied, and the Cartographers Guilds struck a Map of the Empire whose size was that of the Empire, and which coincided point for point with it. The following Generations, who were not so fond of the Study of Cartography as their Forebears had been, saw that that vast map was Useless, and not without some Pitilessness was it, that they delivered it up to the Inclemencies of Sun and Winters. In the Deserts of the West, still today, there are Tattered Ruins of that Map, inhabited by Animals and Beggars; in all the Land there is no other Relic of the Disciplines of Geography.\" https://en.wikipedia.org/wiki/On_Exactitude_in_Science reply staplung 9 hours agoparentOr a portion of one of it's inspirations: Lewis Carroll's Sylvie and Bruno Concluded \"We actually made a map of the country, on the scale of a mile to the mile!\" \"Have you used it much?\" I enquired. \"It has never been spread out, yet,\" said Mein Herr: \"the farmers objected: they said it would cover the whole country, and shut out the sunlight ! So we now use the country itself, as its own map, and I assure you it does nearly as well.\" reply defrost 7 hours agorootparentAlso Carroll, from The Hunting of the Snark He had bought a large map representing the sea, Without the least vestige of land And the crew were much pleased when they found it to be A map they could all understand. ‚ÄúWhat‚Äôs the good of Mercator‚Äôs North Poles and Equators,Tropics, Zones, and Meridian Lines?‚Äù So the Bellman would cry and the crew would reply ‚ÄúThey are merely conventional signs! ‚ÄúOther maps are such shapes, with their islands and capes! But we‚Äôve got our brave Captain to thank (So the crew would protest) that he‚Äôs bought us the best A perfect and absolute blank!‚Äù reply iggldiggl 1 hour agoparentprevAnd Umberto Eco expanded on that with On the Impossibility of Drawing a Map of the Empire on a Scale of 1 to 1: https://s3.amazonaws.com/arena-attachments/881694/cb6119367b... reply jancsika 5 hours agoparentprevThere are some funny lines from They Might Be Giants' \"Women and Men\" that run along the same lines: Women and men have crossed the ocean, They now begin to pour Out from the boat and up the shore. Two by two they enter the jungle, And soon they number more, Three by three as well as four by four. Soon the stream of people gets wider, Then it becomes a river, River becomes an ocean, Carrying ships that bear Women and men. ** Borges: map of an area gets so detailed it becomes the same size as the area. TMBG: creatures multiply and ultimately overrun an area so fully that their group behavior recreates the ecology of the area they took over reply wiradikusuma 6 hours agoprevI open the PDF in Google Chrome on a Mac. When I Ctrl+P, the dialog says it's 1 Page. I don't try to print it, but I think it will not consume more than 1 page? Also, PDF preview in Chrome simply showing it like a normal PDF, but Preview seems confused (gray background instead of white)? reply justsomehnguy 2 hours agoparent> I don't try to print it Well, you can even without consuming a single sheet: just print to PDF. > Preview seems confused (gray background instead of white)? It tries to render it and fit in the preview. reply denysvitali 2 hours agoprevI can't wait for people to start rendering their CVs with this trick >:) reply ipsum2 7 hours agoprevChrome's PDF reader reports the file size as disappointly 200.00 √ó 200.00 in (square) reply markussss 10 hours agoprevThis was a fun read! Thank you reply tkgally 9 hours agoparentI second that comment! That was the most enjoyably nerdy thing I‚Äôve read in quite a while. reply drewcoo 9 hours agoprevPlease, nobody tell Randall Munroe! Because I would literally spend days worth of time scrolling. reply whoisthemachine 9 hours agoprevWhile the Germany PDF actually scrolls pretty quickly at 100% zoom (makes one realize just how much text is read in a day), the Universe one is pretty fun, Firefox's PDF reader at 100% zoom obviously doesn't budge the scrollbar at all. reply pitherpather 9 hours agoparentHackaday soon: Synchronizing a treadmill to a pdf the size of Germany. Obligatory?: The pdf is not the territory. reply xanth 8 hours agoprevRelated CGPGrey video comparing metric paper sizes to comparable objects from the plank scale to the galactic[1]; could have been an XKCD comic [1]: https://www.youtube.com/watch?v=pUF5esTscZI reply RecycledEle 42 minutes agoprev\"Your Scientists Were So Preoccupied With Whether Or Not They Could, They Didn‚Äôt Stop To Think If They Should.\" --Ian Malcolm in the original Jurassic Park film reply codeflo 7 hours agoprevI take offense to that diagram; Germany should refuse to be covered by a PDF that's not in proper DIN format. In theory, DIN paper sizes go all the way from subatomic to the size of the universe. It seems like A(-39) is barely too small to cover Germany's land mass, but A(-40) should be more than sufficient. That's 882 x 1247 km if I didn't miscalculate. reply Cacti 3 hours agoparentOh here we go again. reply groestl 2 hours agorootparentThat's actually quite funny, especially because Germany almost has portrait DIN format. reply Simon_ORourke 2 hours agoprev [‚Äì] I wonder if you could make a PDF that was as hysterically obedient as Germany, and as likely to overlook genocide due to historic blindspots. reply bowsamic 2 hours agoparent [‚Äì] was? More like is. They're overlooking one right now, because of the last one they did. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The author investigates the maximum size limit for PDF documents and finds that Adobe Acrobat sets a limit of 381 kilometers by 381 kilometers.",
      "The author explores the internal structure of PDFs and learns how to manually edit them to create larger PDFs by changing the page size and user unit.",
      "However, it is noted that the user unit value is not widely used and may not have practical implications for most PDF documents. The author creates a PDF claiming to be larger than the entire universe, but it may not have any real-world significance."
    ],
    "commentSummary": [
      "The article and comments delve into various aspects of PDF files, such as their benefits, limitations, and security concerns.",
      "Tangents arise regarding topics like the correct number of zeroes, distinctions between million and billion, and PDF standards.",
      "Discussions also touch on diverse subjects like Google Maps, printing issues on Windows, Umberto Eco's works, and the scale of the universe. The conversation concludes by comparing the size of a PDF to Germany and discussing the country's historical blindspots."
    ],
    "points": 479,
    "commentCount": 81,
    "retryCount": 0,
    "time": 1706741267
  },
  {
    "id": 39205676,
    "title": "Y Combinator CEO's online rant prompts threats and police reports",
    "originLink": "https://missionlocal.org/2024/01/y-combinator-ceo-garry-tans-online-rant-spurs-threat-to-supe-police-reports/",
    "originBody": "Three San Francisco supervisors received a threatening letter at their homes on Tuesday and two more filed police reports after Garry Tan, the CEO of Y Combinator and a heavy campaign donor to efforts to oust progressive politicians, posted online that seven supervisors should die a slow death. Tan wrote ‚ÄúDie slow motherfuckers‚Äù in reference to Supervisors Aaron Peskin, Connie Chan, Myrna Melgar, Shamann Walton, Hillary Ronen, Dean Preston and Ahsha Safa√≠ in a late-night rant Saturday. During his online tirade, Tan posted photos of his private liquor stash, and indicated to a fellow Twitter-user that he was inebriated. The ‚Äúdie slow, motherfucker‚Äù line was a reference to a Tupac Shakur song, and Tan later apologized. That 1996 song, ‚ÄúHit ‚ÄòEm Up,‚Äù escalated the simmering East Coast-West Coast rap rivalry into a lethal feud; Shakur was gunned down three months after its release. The named supervisors did not take the incident lightly: Peskin and Chan on Tuesday filed police reports, and Melgar and Safa√≠ pledged to do the same Peskin, Preston and Melgar all received a letter to their homes citing Tan‚Äôs diatribe and wishing death upon them and their families. When asked if she felt personally threatened by Tan‚Äôs behavior, Chan responded ‚ÄúSeeing what my colleague[s] received in the mail? Yes, absolutely. I have a 10-year-old. I do not tell people where my child attends school.‚Äù The San Francisco Chronicle first reported the hate mail sent to Preston and the police reports filed by Chan and Peskin. Peskin at 8:40 p.m. confirmed to Mission Local that he, too, received a threatening letter at his home; on Wednesday morning, Melgar said she received one as well. The letters were emblazoned with a smiling portrait of Tan. ‚ÄúGarry Tan is right!‚Äù the letter sent to Peskin, Preston, Melgar and, perhaps, others read. ‚ÄúI wish a slow and painful death for you and your loved ones.‚Äù The letter received today by Supervisors Aaron Peskin, Dean Preston and Myrna Melgar. A letter addressed to District 7 Supervisor Myrna Melgar bearing Y Combinator CEO Garry Tan‚Äôs face, containing the threatening message. Preston has in particular drawn the venom of Tan: Preston is the lone democratic socialist on the Board of Supervisors, an odious position to Tan, and Tan has donated $5,000 to Preston‚Äôs opponent and pledged another $50,000 to the effort to unseat him. Bizarrely, the letter to Preston and Peskin concluded: ‚ÄúThis mail was sent to communicate a political opinion. No threats were intended.‚Äù The board‚Äôs five Jewish members ‚Äî Peskin, Ronen, Melgar, Preston and Rafael Mandelman ‚Äî in October received antisemitic postcards at their homes. Peskin said multiple supervisors have received as many as four more antisemitic letters or postcards since then. He noted that a similar disclaimer disavowing any threats was on one or more of the disturbing communiques he and others received earlier. ‚ÄúThis letter was sent to educate public servants without malicious intent,‚Äù reads a bigoted, antisemitic note last year received by Ronen and other supervisors. Chan says she received a letter on Dec. 27 at her home accusing her of ‚Äúwhite genocide.‚Äù Chan, who is Chinese, is married to a white man and has a mixed-race child. Ronen received the same letter on the same day. While Chan first thought to ‚Äúbrush off‚Äù Tan‚Äôs recent comments, she says she was initially unaware of how tied in he is politically. She then learned that Tan has hosted numerous moderate supervisor candidates at his home, including her major challenger, Marjan Philhour. Chan, in her police report, notes that she had filed a prior report last year against Philhour‚Äôs staffer, Forrest Liu, whom she accuses of twice physically threatening her ‚Äî publicly and in person. After realizing the association of Tan to Philhour to Liu, she said ‚Äúit then dawned on me that these unhinged individuals who have wished me death online as well as physically threaten me are connected, it became alarming to me that the threats are potentially violent and imminent.‚Äù Liu did not respond to requests for comment. Tan has indeed hosted San Francisco moderate political figures at his Mission District home, including District Attorney Brooke Jenkins, Philhour and District 9 candidate Trevor Chandler. Philhour and Chandler both criticized their backer Tan in light of his comments. ‚ÄúThreats to one‚Äôs life or safety have no place in political discourse,‚Äù Philhour said. ‚ÄúIt is never okay to advocate violence, be it seriously or in jest,‚Äù said Chandler. Jenkins did not respond to requests for comment. Peskin today asked the City Attorney‚Äôs Office to look into requiring public disclosures from recipients of political donations from ‚Äúpurveyors of hate and violence.‚Äù ‚ÄúWhen individuals in our society, especially and most concerning those with money, power and influence, call for public servants to be tortured or killed, it is crossing a bright line ‚Äî and must not be condoned by society,‚Äù Peskin said today in Board chambers. ‚ÄúTo ignore it is dangerous and a disservice to democracy.‚Äù But tying this potential legislation to the message Tan communicated to his 408,000 Twitter followers would appear to be a serious legal challenge: Half a dozen lawyers and judges told Mission Local that, however ill-advised, Tan‚Äôs comments do not rise to the legal definition of a death threat. Under Penal Code 422, a person making a criminal threat must harbor ‚Äúspecific intent that the statement, made verbally, in writing, or by means of an electronic communication device, is to be taken as a threat‚Ä¶‚Äù ‚ÄúIt is offensive, but it is speech protected by the First Amendment,‚Äù said Berkeley School of Law dean Erwin Chemerinsky. ‚ÄúIt does not meet the standard for incitement.‚Äù Tan is a well-heeled donor for San Francisco‚Äôs moderate causes and candidates. He sits on the board of Grow SF, a political pressure group favoring moderate causes and candidates and targeting progressives. Tan gave more than $100,000 to the 2022 campaign to recall then-District Attorney Chesa Boudin. He gave at least $20,000 to the 2021 school board recall, too. Tan could not be reached for comment. Neither Y Combinator nor Grow SF immediately responded to requests for comment. An earlier version of this story noted that the disclaimer found on the threatening letters was identical to one received by supervisors last year. However, the disclaimers were merely similar, not identical. GARRY TAN‚ÄôS TWITTER TIRADE ‚ÄòStupid,‚Äô ‚Äòshameful‚Äô say tech workers of Y Combinator CEO Garry Tan‚Äôs rant by JOE RIVANO BARROS and YUJIE ZHOU JANUARY 31, 2024 FEBRUARY 1, 2024 Garry Tan, tech CEO & campaign donor, wishes death upon San Francisco politicians by JOE RIVANO BARROS JANUARY 27, 2024 JANUARY 31, 2024 Follow Us TAGGED: Garry Tan JOE RIVANO BARROSSENIOR EDITOR joe.rivanobarros@missionlocal.com Joe was born in Sweden, where half of his family received asylum after fleeing Pinochet, and spent his early childhood in Chile; he moved to Oakland when he was eight. He attended Stanford University for political science and worked at Mission Local as a reporter after graduating. He then spent time in advocacy as a partner for the strategic communications firm The Worker Agency. He rejoined Mission Local as an editor in 2023. More by Joe Rivano Barros JOE ESKENAZI getbackjoejoe@gmail.com Managing Editor/Columnist. Joe was born in San Francisco, raised in the Bay Area, and attended U.C. Berkeley. He never left. ‚ÄúYour humble narrator‚Äù was a writer and columnist for SF Weekly from 2007 to 2015, and a senior editor at San Francisco Magazine from 2015 to 2017. You may also have read his work in the Guardian (U.S. and U.K.); San Francisco Public Press; San Francisco Chronicle; San Francisco Examiner; Dallas Morning News; and elsewhere. He resides in the Excelsior with his wife and three (!) kids, 4.3 miles from his birthplace and 5,474 from hers. The Northern California branch of the Society of Professional Journalists named Eskenazi the 2019 Journalist of the Year. More by Joe Eskenazi",
    "commentLink": "https://news.ycombinator.com/item?id=39205676",
    "commentBody": "Y Combinator CEO Garry Tan's online rant spurs threats to supes, police reports (missionlocal.org)418 points by etc-hosts 17 hours agohidepastfavorite526 comments BobaFloutist 17 hours agoIt's fascinating to me the extent to which executives don't consider themselves \"public figures\" when it comes to potential downsides, but they do in terms of upsides. It feels so obvious to me that the CEO of such a high-profile org should at the very least quickly check public-facing social media posts against someone sensible, if not laundering them all through the experts at their org. But somehow they keep making these mistakes over and over again. reply ativzzz 17 hours agoparentMany CEOs are regular people who happen to be overtly charismatic to a fault. It just so happens that overtly charismatic people tend to be rewarded greatly by our social structures These CEOs aren't doing anything different in these situations - they're being themselves and doing what they did to get their position. Other people generally don't call them out on their BS because it's an uphill battle fighting overtly charismatic people, and it's much easier to accept their flaws for the benefit of riding their coattails to the top This is why they can't differentiate between upsides/downsides - people let them get away with things that other people can't, and to them it is all the same reply mvdtnz 16 hours agorootparentI've never heard of Garry Tan before just now, but he didn't strike me as \"overtly charismatic\" in the linked article. He struck me as incredibly unhinged and unlikeable. reply trust_bt_verify 9 hours agorootparentThose are not mutually exclusive in my experience. reply krapp 9 hours agorootparentprev\"unhinged and unlikeable\" are charismatic traits nowadays. Those are exactly the personality aspects people believe are necessary and find attractive in truly innovative leaders, like Steve Jobs and Elon Musk. They read as hard-nosed sincerity and truth, no bullshit. The unreasonable man to which the world must yield. Hell, we recently elected a President almost entirely because he was the biggest asshole in the room. reply shiroiuma 8 hours agorootparent>Hell, we recently elected a President almost entirely because he was the biggest asshole in the room. Speak for yourself. This seems to be mostly an American problem. reply wyclif 1 hour agorootparentBoth Europe and Asia have their own problems with authoritarian, strongman-type fascists. reply fragmede 8 hours agorootparentprevThe rise of right wing parties all over the world suggest it's not a uniquely American problem, unfortunately. reply shiroiuma 5 hours agorootparentWhile I might disagree with the political opinions of those right wing parties, I've never seen any of them show complete incompetence and stupidity to the level of suggesting that people inject themselves with bleach to cure a virus. reply doubled112 5 hours agorootparentChlorination of the gene pool in the most literal of senses. reply rsanek 15 hours agorootparentprevI might be convinced that founders tend to be charismatic (they convinced a bunch of people to build their dream when it was just a dream, after all), but your run-of-the-mill CEO certainly isn't. reply KaiserPro 16 hours agorootparentprev> Many CEOs are regular people Agreed > who happen to be overtly charismatic to a fault. Not so much. > they're being themselves and doing what they did to get their position. yes, there is a way of talking in industry that allows people to rise through the ranks. Its very rare that you get to the top by being an odious prick all the time. However, people on the inside don't tend call out CEOs, because they need something from them. If you are frank with your CEO and they don't like it, you're out on your arse, to be replaced by a yes man. (not always, but its surprisingly common) It is very easy to become a CEO as a normal person, only to develop into an horrid shit later. reply operatingthetan 13 hours agorootparent>Not so much. Could probably fix that quote by adding \"believe they are.\" >who happen to believe they are overtly charismatic to a fault. reply crote 9 hours agorootparentprev> Many CEOs are regular people who happen to be overtly charismatic to a fault. It's quite common for them to appear overtly charismatic at first glance. Narcissism and psychopathy are extremely common at that level. It's why you should always be weary of CEOs who seem a little bit too happy to have a very high-profile public presence. reply mgh2 8 hours agorootparentprevMoney and fame corrupts, alcohol disinhibits that corruption. reply duxup 16 hours agoparentprevI've worked for several big companies where the CEO wanted people to ask tough questions at big meetings from the rank and file and so on. Front line managers had to prompt employees to ask question so it wasn't just awkward silence. It's telling executives would think people would just ask tough questions on demand. It of course costs the CEO nothing to provide everyone else at the company tough questions / feedback, employees though need to consider their words carefully depending on who at a company is listening as there can be real consequences. It's one of those things that I'm sure seems like it makes the executive look \"open\", but rather it just shows their ignorance / are out of touch with the life of a rando worker. Not a surprise that kind of unawareness leaks out of the workplace as they operate in a space where they are often relatively free to speak their mind. reply skeeter2020 15 hours agorootparentMy Boss (the CTO at a mid-sized company) says he really appreciates my candor and ability to ask challenging questions, but based on his reactions when I do so I'm pretty sure what he actually really likes the IDEA of being an executive who invites diverse and dissenting input, more than having people do it. reply plasticchris 9 hours agorootparentTo be generous, sometimes how one feels in the moment is different than how one feels when given time to reflect. So it may be that your boss really does appreciate it (just not right then). reply BobaFloutist 16 hours agorootparentprevThe other thing is, there's no guarantee that a CEO that genuinely wants the rank and file to ask tough questions still won't instantly fire someone for the wrong tough question. Some CEOs are all about wanting to be challenged and pushed back by their employees until someone accidentally hits a nerve on a bad day, and there's very little recourse for most employees in the US if they get fired because they pissed off the executives. reply JohnFen 16 hours agorootparentYes, this. It's a clear and obvious trap. reply jnovek 16 hours agoparentprevI'm a YC alum and it's an important bullet point on my resume. I would rather YC leadership kept their political positions to themselves as much as is reasonably possible. It dilutes the value of that bullet point -- I want it to communicate things about my work ethic and competency. I don't want it to imply _anything_ about my political opinions. I don't have a problem with tech leaders holding political positions, nor do I have a problem with them making personal donations based on those opinions. Quietly. reply PeterStuer 2 hours agorootparentI'd rather they do it publicly than secretly tbh. At least you know where they stand. As for companies, judge their actions, not their words. reply petesergeant 16 hours agorootparentprev> I want it to communicate things about my work ethic and competency Doesn't it just communicate that you got in to YC? reply JohnFen 16 hours agorootparentA CEO represents the organization that they're a CEO of. It's natural and not entirely unwarranted for people to think that a CEO's behavior and attitude is also reflected to some degree by the organization itself. reply throwawaymaths 16 hours agorootparentprevWhy the double standard? Why should or shouldn't it be couth for someone to talk about their political positions? Everyone is a human, and you don't get anything done in politics unless there is mass action, which means we must have conversations, public AND private, about politics. I'm sorry, if you are like \"I'm glad they gave me the money and the label\" and can't take it when someone associated makes an embarassing human moment, you are just trying to have your cake and eat it too. Do better. reply smoldesu 16 hours agorootparentI don't think the parent wants to eat their cake and have it too; they're torn between having the credentials or abandoning them because it's embarassing. And who can blame them? I've never said the words \"Y Combinator\" outside the West coast and got a positive reaction. YC can have political opinions, but they should acknowledge the opportunity cost of putting their politics before their community. Behavior like the one linked in the OP is incredibly petty and probably should make the associated parties feel bad about working with that kind of person. Lord knows I feel ashamed to be an HN user today. reply r00fus 9 hours agorootparentprevDo better? No, Tan and YC need to understand this will impact their image. Free speech has consequences. And speech that has unhinged threats (even if it has a disclaimer that it's not) has potential consequences with law enforcement. I don't think it's out of line for someone who's investing their time and effort into an organization to be critical of leadership. reply throwawaymaths 3 hours agorootparentnobody is saying free speech doesn't have consequences. I'm saying taking money and reputation has consequences too. reply PeterStuer 1 hour agorootparentBut at what point do 'consequences' start to negate the 'free' in speech? I'd argue you have reached the limits of free speach the moment there are consequences for just the speech. reply skeeter2020 15 hours agorootparentprevbecause they want the signal to come from what YC has accomplished and represents, not the personal opinions of someone associated with them who's leveraging his unrelated benefits in a socially very unacceptable way. reply specialist 4 hours agorootparentprevAre you suggesting branding is irrelevant? reply not2b 16 hours agoparentprevHe said he was drunk when he posted the rant calling for slow death of most of the San Francisco city council, so of course he didn't send it to corporate PR to check. This kind of behavior shouldn't be acceptable for the head of a respectable corporation, even if his tweet hadn't led others to follow up with death threats. He should resign or be dismissed and YC should replace him with a responsible adult. This isn't a minor offense, it is grotesque. If he remains, it reflects very badly on YC. reply shiroiuma 7 hours agorootparentObviously, YC is not a \"respectable organization\", since they picked this fruitcake as their leader. This also explains why a portion of the readership of this site is so obviously unhinged. reply mensetmanusman 7 hours agorootparentIf you judge someone based on their worst day, you won't live a very happy life. reply angus-prune 5 hours agorootparentIf you expect not to be judged for your worst behaviour, then everyone around you won't live a happy life. reply rchaud 17 hours agoparentprevIt's the new spin on \"I want the President to be someone I can have a beer with.\" Quoting 2Pac lyrics is just comical. Even more out of touch than Ben Horowitz (of Andreessen Horowitz) starting every chapter of his ultra-corporate startup book with Jay-Z lyrics. reply etc-hosts 16 hours agorootparentBen Horowitz is a pretty interesting guy https://www.nytimes.com/2017/07/22/technology/one-family-man... reply specialist 4 hours agorootparentTIL: Son of David Horowitz. Oh. https://en.wikipedia.org/wiki/David_Horowitz reply qzw 15 hours agorootparentprevBut Jay-Z is also ultra-corporate, so it fits? reply rchaud 15 hours agorootparentNot the \"street rap era\" Jay-Z he was quoting in his book. reply specialist 4 hours agorootparentprevHeh. I look forward to If Books Could Kill's review. Failing that, I imagine this review properly captures the gist: https://www.amazon.com/gp/customer-reviews/RFZJKA736UBAH/ref... I'll reserve judgement until we see how a16z's \"digital asset class\" thing pans out. Some might even say blockchains are eating the world. reply dboreham 16 hours agorootparentprevBen is a long-time (life long?) rap lyric afficionado and in that respect actually quite \"in-touch\". reply sjwhevvvvvsj 16 hours agorootparentYeah, nothing says ‚Äúdigging in the crates‚Äù more than quoting the most mainstream possible artist in a genre. Give me DOOM lyrics or GTFO. reply thefaux 13 hours agorootparentI see your DOOM and raise Viktor Vaughn :) reply skeeter2020 15 hours agorootparentprevwhat exactly is a \"rap lyric aficionado\"? He's about as in-touch as an Ivy-league academic studying poverty is with living in rural Appalachia. reply lawgimenez 7 hours agorootparentprevHe‚Äôs not. What does rap lyric aficionado even means? Tupac is not even that lyrical or rhyme elite. reply j2kun 16 hours agorootparentprevI think you misunderstood the meaning of \"out of touch\" in this context. reply spacemadness 16 hours agoparentprevThat‚Äôs what most business ‚Äúleaders‚Äù do. When the money is rolling in they are visionaries, when the money is threatened it‚Äôs the economy and time for layoffs. So anything good is their doing, anything bad and it‚Äôs time to deflect. reply tshaddox 16 hours agoparentprevI suppose I can't disagree with the advise you're providing, although I'm a little troubled by the implication that the problem here is simply that he didn't send this vile rant to his social media team to proofread before he posted it. reply BobaFloutist 16 hours agorootparentIt's not to proof-read it, it's to make sure it's not going to damage the company. Everyone has opinions that would get them cancelled on Twitter. Most of us are sensible enough to keep them to ourselves, or, at least, off Twitter, without even having the responsibility to maintain the image of a business. He has a duty to his employees, his clients, and his investors that goes far beyond the standard duty of \"Don't be an asshole on Twitter.\" Political hyperbole is also kind of the norm on Twitter (which is one of many reasons I don't spend much time there), so it's entirely possible he thought he was being humorous, and that it was abundantly obvious that he shouldn't be taken literally. Which might even be true. But CEOs are at extra risk of getting taken out of context and willfully misinterpreted, and they should fucking Tweet like it. I disagree with what he said, but I'm more insulted that the people are allowed such insane levels of power and responsibility and are given such disproportionate compensation have the common sense of a middle-schooler. reply zozbot234 16 hours agorootparentprevHe's literally the CEO of YC, and news.yc is social-media adjacent at least. What he posted would not pass mustard here. reply throwanem 16 hours agorootparent'Pass muster,' the figure drawn from the military practice of mustering (gathering) troops for inspection of their uniform, equipment, and personal grooming. Unsatisfactory presentation can result in being sent away to fix it, usually repeatedly and at length while being smoked by an NCO, to help you remember not to make the same mistake again. When this occurs, the one so dismissed has failed to pass muster. reply skeeter2020 15 hours agorootparentI think he meant \"If he offered me the mustard, I'd tell him to go to hell\" reply shiroiuma 7 hours agorootparentWhat if it's Grey Poupon? reply dgfitz 16 hours agorootparentprevI think the point was more that sending it to a team would have hopefully resulted in it not being posted at all. reply bloopernova 17 hours agoparentprevThe battlecry of the executive: \"Rules for thee but none for me\" reply bko 17 hours agorootparentIsn't it the reverse though? If a not notable person tweeted this stuff, it would have blown over and no one would have cared. But since he is notable it becomes a story I think that was the point of the parents \"public figures\" comment reply toomuchtodo 17 hours agorootparent> Isn't it the reverse though? If a not notable person tweeted this stuff, it would have blown over and no one would have cared. But since he is notable it becomes a story This is not borne out by historical events. https://en.wikipedia.org/wiki/So_You%27ve_Been_Publicly_Sham... reply syrgian 16 hours agorootparentprevIf a non-notable person tweeted this, they might have lost their livelihood. reply hnthrowaway6543 16 hours agorootparentCancel mobs for stuff like this for Joe Average may have worked in 2018, but now are effectively over. It's equals parts a post-ZIRP cultural shift of companies no longer pretending to care about DEI, a post-Elon Twitter cultural shift for what's seen as acceptable, and a post-Oct 7 shift where, frankly, companies are now scared shitless to take political stances in general because of how sensitive the topic of the current war in the middle east is. reply s1artibartfast 16 hours agorootparentprevThey might, or they might be one of thousands of people on a tuesday reply bko 16 hours agorootparentprevUm... Have you been on Twitter lately? reply j2kun 16 hours agorootparentprevI'm sure his company's legal staff has reminded his employees to not engage in political action while appearing to represent their company. My company sure does. reply rchaud 16 hours agorootparentprevHow would it have blown over? Normal people with jobs lose them all the time due to making idiotic comments online that get back to their employer. Note that I say \"idiotic comments\", not outright \" F U and die\" comments as is the case here. reply aaomidi 17 hours agorootparentprevIt's their battlecry, not necessarily in tune with reality. reply HumblyTossed 16 hours agoparentprev> It's fascinating to me the extent to which executives don't consider themselves \"public figures\" when it comes to potential downsides, but they do in terms of upsides. Really? Because they are all about upsides. \"My initiatives led to 1,500,000 new bank accounts opened in the last 3 quarters!\" Vs. \"I didn't have any knowledge that the 1,450,000 new bank accounts were opened fraudulently!\" reply 1vuio0pswjnm7 7 hours agoparentprevBased on that tweet, this person does not fit any definition of \"executive\" that I would subscribe to. reply crowcroft 16 hours agoparentprevThey want the power, but not the responsibility. reply treflop 16 hours agoparentprevHumans run the gamut. Sure there are humans that stickerbomb a tie die hatchback or go on Joe Rogan but then you got your human that lives a private life and drives a gray crossover. reply bilbo0s 16 hours agorootparentEver notice how most of them never even joke about certain things? Like, say, death threats? I'd be willing to bet that a lot of humans go their entire lives without joking about death threats. reply roughly 17 hours agoparentprevNo, see, tech CEOs are all brilliant brain geniuses, and any attempt or notion to run their ideas or behavior past experienced professionals is nothing but a sop to outdated traditions which risks not just slowing them down, but by extension slowing all of humanity down and condemning literally uncountable future generations to darkness and death! Is that what you‚Äôre advocating for? The premature deaths of uncounted billions? You monster. reply lainga 16 hours agorootparent‚ÄúWe believe any deceleration of my tweeting will cost lives. Deaths that were preventable by the tweet that was prevented from existing is a form of murder.‚Äù (with no apology and copious reference of Arkell v. Pressdram to Mr. Andreessen) reply zozbot234 16 hours agorootparentprevNot sure if serious or just \"e/acc\"... reply marricks 16 hours agorootparent> brilliant brain geniuses If that is ever typed seriously I don't want to be in those comments. reply archagon 15 hours agorootparentBRB, printing new business cards. reply thiago_fm 16 hours agoprevY... ikes! It's about time Y Combinator has executives who aren't so busy with politics. It gives the whole incubator, startup scene, etc, a bad name. The good ol' days are over. I still have in my mind Y Combinator of Paul Graham (a man wise with his words), but given that we've already had even Sam Altman in control of it. I'm guessing YC nowadays is not that different from private equity/VCs like A16z, which enjoy having their fingers on everything. Typically, it is stuff they don't know much about and look plain stupid. I hope PG can bring the good ol' days back someday, when it was about entrepreneurship, having people laser-focused on building disruptive companies. reply amadeuspagel 16 hours agoparent> GRAHAM: No, no, no, politics. The problems with San Francisco are entirely due to a small number of terrible politicians. It‚Äôs all because Ed Lee died. The mayor, Ed Lee, was a reasonable person. Up till the point where Ed Lee died, San Francisco seemed like a utopia. It was like when Gates left Microsoft, and things rapidly reverted to the mean. Although in San Francisco‚Äôs case, way below the mean, and so it‚Äôs not that it didn‚Äôt take that much to ruin San Francisco. It‚Äôs really, if you just replaced about five supervisors, San Francisco would be instantly a fabulously better city. > COWEN: Isn‚Äôt it the voters you need to replace? Those people got elected, reelected. > GRAHAM: Well, the reason San Francisco fundamentally is so broken is that the supervisors have so much power, and supervisor elections, you can win by a couple hundred votes. All you need to do is have this hard core of crazy left-wing supporters who will absolutely support you, no matter what, and turn out to vote. > Everybody else is like, ‚ÄúOh, local election doesn‚Äôt matter. I‚Äôm not going to bother.‚Äù [laughs] It‚Äôs a uniquely weird situation that wasn‚Äôt really visible. It was always there, but it wasn‚Äôt visible until Ed Lee died. Now, we‚Äôve reverted to what that situation produces, which is a disaster. https://conversationswithtyler.com/episodes/paul-graham/ reply blendo 9 hours agorootparentGRAHAM> ‚Ä¶ It‚Äôs all because Ed Lee died To believe that Ed Lee is some kind of political white knight instead of merely a Willie Brown/Gavin Newsom/London Breed/Kamala Harris machine politician seems historically naive, if not blindly ignorant. Similar to Garry Tan in that. reply micromacrofoot 16 hours agorootparentprevthe sage wisdom of \"the problem with politics is that the people who get the most votes win\" reply nemothekid 16 hours agorootparentI get you are being sarcastic, but the real problem is people don't vote and you see this at every level of government office. Other than the president, it's hard to engage people (which I don't really fault them for) and so you end up with politicians - all across the field - who do nothing but pander to the most extreme voters. reply Affric 7 hours agorootparentCompulsory voting is an option. reply kjkjadksj 5 hours agorootparentThe issue is voters are uneducated. You get compulsory voting you get the same issue with the california prop system scaled out: where people are given a choice and vote on what feels right from the proposition name and two sentence description alone. Research and looking into bias be damned. reply Affric 3 hours agorootparentNo, it's not. Compulsory voting drives engagement. Representative democracy saves the public from themselves. The vast majority of people vote for who they trust. reply infamouscow 7 hours agorootparentprevMost people already don't vote voluntarily. Forcing the uninformed and uninterested to vote has never yielded better results by any measure. reply pas 7 hours agorootparentit would at least help with making voting the default, also it would be normal to insert it into a schedule for employers, have enough polling places, etc. (okay, okay, I know, of course it's terminal naivete, fascist cumbombs would continue to smear their nastiness all over anyway. yet defaults matter. laws matter.) reply kjkjadksj 5 hours agorootparentIn california you have two weeks to vote in some precincts. Its more time consuming to buy milk than it is to vote for many. reply micromacrofoot 16 hours agorootparentprevThat's not what he's saying in the quote though, he's mad that the people who are engaged are people that don't agree with him and dismisses voters as \"crazy\" He even goes as far to call this \"broken\" ‚Äî it's literally democracy. reply rstuart4133 13 hours agorootparent> it's literally democracy. I guess it is the formal definition. In Australia we have a slightly different version. We don't prevent the crazies from voting. Instead we insist everyone must vote, including the crazies. Turns out when you do that the non-crazy voters outnumber the rest by a considerable margin. A weird thing happens when you make voting compulsory. Another bunch, which I now regard as crazy, insists they should be free to not vote. They get fined. (I have a vision of what would happen in the USA if someone proposed compulsory voting. It's far left and far right politicians who would be thrown out if the centre voted, inciting their following to riot in the streets, shouting \"Freedom!\") It's kinda funny, because they are allowed to not vote. The actual requirement isn't to vote because it's impossible to police. The requirement is to turn up at the polling place and have your name recorded. You can write whatever you damned well please on the voting slip. After most elections the country gets to have a laugh at the insults and pornographic images that have been inscribed on those slips. It's also funny because these crazies are insisting they have a right to not participate in the democracy. And they don't. Those that do participate then pass rules to fine them, and the non-participants get pissed off about that and demonstrate their now white hot anger by not voting again. And I bet you thought I was being harsh for calling them crazy. It's like watching someone put their hand in a fire, and not remove it because it hurts. And that is an excellent example of why compulsory voting works. The voices of the crazies literally get drowned out by the people who would otherwise be too lazy to vote. Or perhaps they just figure they are in the centre, know stuff all about the candidates, and most other people are reasonable like them so they won't change the outcome. But it turns out if most normal, reasonable, uninformed people remove themselves from machinery of democracy, what you get left with is crazies voting for crazies. reply bigger_cheese 8 hours agorootparentI'm also Australian but I do not think our style of compulsory voting would be popular in the US, my understanding is the way elections are managed in the US suppresses the voter turnout. For example in the US they have their elections on a Tuesday (in Australia it is a Saturday), this strikes me as suboptimal if you want the most people to vote then you should hold the election at a time that is convenient for the largest number of people, which is not in the middle of a regular working day. Australia also has pre-polling (i.e. you can turn up to a polling station and vote before the nominated \"election day\") and Postal voting (both of which are to my understanding extremely controversial in the US). I believe in the US you also need to register to vote and you have to take steps to maintain your registration. again that is an added barrier that creates more friction. Australia also has Preferential voting, I do not believe US elections use this method. reply micromacrofoot 10 hours agorootparentprevIf people are too lazy to vote and yet complain about the outcome of elections... who's really the crazy one? reply LudwigNagasena 2 hours agorootparentHow is that crazy? I doubt that national elections usually have voting margins of a single vote. reply Vegenoid 10 hours agorootparentprevThe point is that most people just aren't that engaged with politics, period. Most people who aren't voting aren't complaining about who is getting voted in. reply Zak 13 hours agorootparentprevA political and electoral system can have elections that are fair in principle but tend to produce results few are happy with in practice even when they voted for the winner. Is that democracy? Well, maybe, but I'd argue it's not doing democracy very well. US presidential elections come to mind; the likely nominees of both major parties are viewed unfavorably by a majority in polls, but one of them is almost certainly going to win. reply JohnFen 11 hours agoparentprev> It gives the whole incubator, startup scene, etc, a bad name. And that scene already has a bad name. And let's not forget that he's also an EA acolyte. Every time I think that the image of SV couldn't get much worse, it does. reply startupsfail 16 hours agoparentprevIt is admirable that Gary Tan loves his home in SF so much. And that he is engaged into making SF better. But SF, getting wasted, engaging into politics is not SV. Wish we could disentangle flies from sausages and move YC office back to Mountain View, where it belongs. reply tootie 16 hours agoparentprevThe actual tweet was a few days ago. I can't believe he wasn't instantly fired. I know firing a CEO is a bit complicated, but this should have warranted a quick response. Has YC even issued a statement? Has Tan apologized? reply thiago_fm 13 hours agorootparentNo idea, I have a similar thought. But I'm not from America, where people are free to say whatever stupid thing they want, no matter how high stakes their job is. reply maeil 4 hours agorootparentprevNo repercussions for violent threats to innocent people when made by higherups in SV tech is par for the course nowadays. Take OpenAI's Head of Research (quite the public role given they're a research company) openly calling for genocide in Gaza, asking to \"finish them\", \"More! No mercy!\" including civilians, over a series of 80 deranged tweets. [1] Zero repercussions, still happily heading research at a company whose supposed objective is developing AGI for the benefit of mankind. Also very quickly scrubbed off of HN [2]. [1] https://news.ycombinator.com/item?id=39124666 [2] https://web.archive.org/web/20231226171217/https://news.ycom... reply lukan 2 hours agorootparent\"openly calling for genocide in Gaza, asking to \"finish them\", \"More! No mercy!\" including civilians\" Can you share a quote, where he literally does that? I only read, that he is talking about Hamas. And explicitely not about civilians. reply etc-hosts 16 hours agoparentprevCapitalism runs every aspect of our lives. It's impossible for commentary from a person who controls a vast amount of capital to not be political. reply timr 17 hours agoprevThis article is emblematic of everything wrong with \"journalism\" today. Regardless of what Garry wrote on Twitter (which I'm not defending), he didn't send the letters in question, which are the core of the incident. So some lunatic prints out a tweet and mails it to politicians at their home addresses, and the \"journalist\" spends a couple thousand words focusing on the tweet, and how the guy who wrote the tweet is rich. Also, featuring the price of his liquor bottles (prominent in the first article about this by the same writer) is indicative of the level of pettiness involved. Maybe there's an actual story here, but this isn't it, and it's not clear that the story is more than \"someone said something regrettable on Twitter\". reply noelwelsh 16 hours agoparentNo. When people with power stay things, other people take it as permission to do things that are said or implied in that speech. For example https://en.wikipedia.org/wiki/Will_no_one_rid_me_of_this_tur... reply timr 16 hours agorootparentThis is a fairly standard and boring way of dressing up censorship as something high-minded. It's nice that you're familiar with a story from England in 1170, but no, you don't just get automatically blamed in the US when crazy people do things in response to dumb things you said on Twitter. Regardless, even if you did get blamed, missionlocal is not the impartial jury who gets to decide whether or not quoting 2pac is incitement to violence. reply noelwelsh 16 hours agorootparentSigh. Please read the article more carefully. missionlocal explicitly says it is not incitement to violence. Here are three paragraphs from it: --- But tying this potential legislation to the message Tan communicated to his 408,000 Twitter followers would appear to be a serious legal challenge: Half a dozen lawyers and judges told Mission Local that, however ill-advised, Tan‚Äôs comments do not rise to the legal definition of a death threat. Under Penal Code 422, a person making a criminal threat must harbor ‚Äúspecific intent that the statement, made verbally, in writing, or by means of an electronic communication device, is to be taken as a threat‚Ä¶‚Äù ‚ÄúIt is offensive, but it is speech protected by the First Amendment,‚Äù said Berkeley School of Law dean Erwin Chemerinsky. ‚ÄúIt does not meet the standard for incitement.‚Äù --- \"This is a fairly standard and boring way of dressing up censorship as something high-minded.\" Do you think that speech is not an act? That speech does not have any consequences in the world, and so should be free from all restriction? That's certainly not the law in the US, and you seem to be aware of incitement to violence. \"missionlocal is not the impartial jury who gets to decide whether or not quoting 2pac is incitement to violence.\" Please quote where missionlocal decides that is incitement to violence. You are accusing missionlocal of hack journalism where it is simply reporting what happened. You may not think Garry Tan should get heat for doing what he did, but you should not place your ire on the journalists who are reporting what is by any reasonable definition a story within their purview. reply timr 8 hours agorootparent> You are accusing missionlocal of hack journalism where it is simply reporting what happened. I am not saying that they're hack journalists because they literally accused Tan of incitement to violence (that would probably be libel). I'm saying they're hacks because the article (and the prior one) were full of irrelevant details about Tan, while ignoring nearly all of the details about the actual incident. Tan is not the core of the story, unless you've lost all perspective on your job as a journalist. It's like reporting on a robbery, but making most of your article about Karl Marx because the criminal was reading a copy of Das Kapital. The only way you get to that point is to blame Marx for the actions of the criminal. Some unhinged person sent a threat of violence to politicians, using Tan's tweet. That is the story. Tan's liquor cabinet, his history of political donations, his wealth...all of that is irrelevant. reply Apocryphon 16 hours agorootparentprevIt's not some obscure story. T.S. Eliot wrote an entire play about the martyrdom of Thomas Becket. https://en.wikipedia.org/wiki/Murder_in_the_Cathedral reply ahonhn 5 hours agorootparentIndeed, though I suspect many might first have learnt of it watching Blackadder. https://www.youtube.com/watch?v=2rNopZDwahg reply jacobolus 16 hours agoparentprev> everything wrong with \"journalism\" today Mission Local is one of the best sources of local San Francisco news, especially anything directly relevant to the Mission District. If rich jerks don't want to be called out by local journalists, they shouldn't post unhinged public death threats, even as a \"joke\" or \"song reference\". reply timr 16 hours agorootparent> Mission Local is one of the best sources of local San Francisco news OK. Maybe their coverage of potholes is fantastic, but this article is a terrible, obviously partisan hack job. Both things can be true. reply jacobolus 16 hours agorootparentI think your own \"partisanship\" is coloring your reading of a fairly neutral and factual article. Mission Local regularly publishes stories which are (implicitly or explicitly) critical of the supervisors Tan was threatening here. reply timr 15 hours agorootparentI have no idea if missionlocal is partisan, but this article is obviously partisan -- it brings in a bunch of irrelevant factors (e.g. the value of Garry's liquor collection) into a story that boils down to \"someone said a thing on Twitter that was bad and offensive, while drunk, and someone else took an obviously unhinged action in result\". It's the easiest thing in the world to report this in a neutral, factual way. You don't need to focus on Gary's money, his association to tech, his liquor cabinet, or anything else. That the reporter(s) could not do this speaks volumes about their motivations. Aside from that, I have no dog in this particular fight. I haven't lived in SF in years, and if you're insinuating that I'm on a particular side of the political spectrum, you're way over your skis. Partisan doesn't have to mean \"left\" or \"right\", by the way...you can just be partisan against tech. reply jacobolus 14 hours agorootparentHere's the text of the article directly: > During his online tirade, Tan posted photos of his private liquor stash, and indicated to a fellow Twitter-user that he was inebriated. Describing the tweets adjacent to the offensive one gives useful context. This is not the reporter dredging up some irrelevant trivia from deep in Tan's past or something. Readers might not know who Tan is. It is even more essential context to explain that: > Garry Tan, the CEO of Y Combinator and a heavy campaign donor to efforts to oust progressive politicians [...] > Tan is a well-heeled donor for San Francisco‚Äôs moderate causes and candidates. He sits on the board of Grow SF, a political pressure group favoring moderate causes and candidates and targeting progressives. Tan gave more than $100,000 to the 2022 campaign to recall then-District Attorney Chesa Boudin. He gave at least $20,000 to the 2021 school board recall, too. If Tan was just some random person with no influence and no relation to SF politics this would probably not be much of a story. That he is one of the major donors to the political rivals of these supervisors is the reason this is a political shitstorm. Tan's tweet damages the reputation of his \"Grow SF\" group and the candidates they support, and has possible further political implications: > Peskin today asked the City Attorney‚Äôs Office to look into requiring public disclosures from recipients of political donations from ‚Äúpurveyors of hate and violence.‚Äù reply refurb 8 hours agorootparentprevMission Local is not ‚Äúfairly neutral‚Äù. They are very closely aligned with certain SF politicians and their ‚Äúnews‚Äù coverage shows that. This article is anything but neutral (not that I don‚Äôt think Tan screwed up here). reply lupusreal 12 hours agorootparentprevYou don't need to know anything about the political alignment of anybody to know that a person wishing a politician would die is legal political speech in America, and not even an uncommon sort of it. This sort of thing is regularly said by Americans of all political persuasions about politicians in any and every political party. The article is making a mountain out of a molehill. For my part, I hope Trump dies painfully, as well as every other living American president (with the sole exception of Jimmy Carter who was a terrible president but a good man nevertheless.) If you live in America, I know you frequently hear people saying they wish X Y or Z politician would die. Such harsh sentiments are commonly expressed in American society. It's a free country and lots of people exercise that freedom with inflammatory but legal hot takes like that. reply jacobolus 12 hours agorootparentI think everyone agrees that Tan's speech is protected by the First Amendment. As is strong criticism of his speech. No one is proposing Tan should be fined or thrown in jail or have his rights curtailed by the government. It is not normal and should not be normal for major political donors to make public death threats to local officials in the city where they live, even as a joke. It's toxic and corrosive to society and politics, and makes him seem unhinged. Tan is rightly getting excoriated, and he deserves scorn from his own political allies for significantly damaging their common causes. Tan keeps complaining about SF politics being frustrating, but in my opinion, as someone who supports a significant portion of his policy platform, local politics would be improved if Tan would just move somewhere else or shut up and keep his money to himself and leave political discussions to the grown ups. reply bee_rider 16 hours agorootparentprevIt isn‚Äôt a partisan hack job, the facts are just bad for Gary. reply lupusreal 16 hours agorootparentprevIf local politicians don't want jerks (rich or otherwise) talking shit about them, they picked the wrong profession in the wrong country. reply JohnFen 15 hours agorootparentPublicly wishing death on people is a whole different category from \"talking shit about people\". reply lupusreal 14 hours agorootparentIt isn't. It's crass but constitutionally protected political speech. reply JohnFen 12 hours agorootparentI never said it wasn't constitutionally protected. That status isn't relevant to the point. reply jakewins 16 hours agoparentprevIf you are a leader in some capacity, you have degrees of responsibility for things that happen because you rile up your followers. Clearly Garry‚Äôs fans are threatening violence here as a direct consequence of Garry‚Äôs intentional targeting and signalling here. I don‚Äôt follow at all how the journalistic angle is problematic reply klyrs 16 hours agorootparent> I don‚Äôt follow at all how the journalistic angle is problematic People of a certain political bent are eager to throw journalists under every available bus, even if that means denying history and common sense. reply theGnuMe 16 hours agorootparentprevI mean this is what is being litigated in court wrt Trump. reply KaiserPro 15 hours agoparentprevI would kindly suggest that this is at best wrong, at worst deliberately misleading The job of journalists is to report news worth events, and provide extra context with some level of verification. When the CEO quotes rap lyrics which implies that someone should kill them selves, that is news worthy. The CEO, who is in a position of both power and responsibility, should really not be saying stupid shit. Why? because the job of the CEO is to make sure a company's image isn't tarnished. (see Gerald Ratner). Tan should frankly grow the fuck up and do what CEOs normally do, which is pay local politics to change. reply jrflowers 14 hours agoparentprev>it's not clear that the story is more than \"someone said something regrettable on Twitter\". This is a good point. Who is to say if there is a difference between receiving hateful letters to one‚Äôs home and not receiving hateful letters to one‚Äôs home? This is bad journalism because it is a report of events that happened, good journalism would have been a levelheaded piece about how Tan is probably a good guy and we should probably agree with his politics reply Arthanos 16 hours agoparentprevThis comment is everything wrong with media literacy. It's absolutely worthwhile to cover highly public calls to violence of government officials by respected individuals with lots of power and the article makes it clear he personally did not send the letters. But denying that public calls to violence spurs actual violence is denial of basic cause and effect. reply timr 16 hours agorootparent> It's absolutely worthwhile to cover highly public calls to violence of government officials by respected individuals with lots of power I mean, sure...who are you arguing with? I didn't say nobody should cover this. I said this article is terrible. > But denying that public calls to violence spurs actual violence is denial of basic cause and effect. Yeah, except we have laws around this concept, and even if what you're saying were true in the US (it isn't, thankfully), it doesn't magicaly make hack journalism good. Said differently, \"incitement to violence\" doesn't mean that missionlocal is high-minded and mature for spending two articles talking about the price of his liquor. reply notahacker 15 hours agorootparent> Said differently, \"incitement to violence\" doesn't mean that missionlocal is high-minded and mature for spending two articles talking about the price of his liquor. Garry Tan chose to flaunt the high-end liquor bottles and \"Twitter menace\" plaque ahead of his sort-of-apology, not missionlocal. Only one of the articles (not the one linked) refers to them at all. The other one focuses more on the hate mail some idiot decided to send being a screenshot of Tan's original tweet, and both of them are pretty clear about it being a rap lyric. Kind of hard to argue with a straight face that the real problem with the YC CEO acting like a not-very-smart bro influencer even in his sort-of-apology is that some local rag journalist didn't spare the embarrassing detail. Sure, Tan was probably more interested in highlighting the \"twitter menace\" plaque than the fairly expensive liquor and unremarkably-priced wine, but celebrities flaunting wealth with a laughing emoji as a \"fuck you\" to their critics is a well established trope, and I don't think high-minded, mature journalism is about taking the most sympathetic interpretation possible of bro silliness. reply timr 15 hours agorootparent> not-very-smart bro influencer Is this really necessary? Let he who has made no mistakes (particularly when drunk) cast the first stone. reply notahacker 13 hours agorootparentMore necessary than the sneer quotes you applied to the \"journalist\", yes. If I say dumb stuff when drunk and then muff the initial apology (when presumably sober) I wouldn't expect people to sugar coat it either. Garry clearly isn't too stupid or desperate for clicks to do any better, and so I'm afraid I'm going to have to continue to disagree that we should save our ridicule for his critics. reply timr 8 hours agorootparentI stand by the \"sneer quotes\". Having actually taken journalism classes in my life, I would be ashamed to put my name to that article. reply bdcravens 16 hours agoparentprevI'm not certain that Twitter is an exempt place when it comes to threatening statements. You don't have to send it to the person in question in many cases. reply Tarragon 16 hours agoparentprev\"Will no one rid me of this turbulent priest?\" https://en.wikipedia.org/wiki/Will_no_one_rid_me_of_this_tur... Stochastic terrorism https://en.wikipedia.org/wiki/Stochastic_terrorism reply LudwigNagasena 16 hours agorootparentStochastic terrorism is such a dangerous and nebulous concept. It itself can be considered stochastic terrorism. People become afraid of stochastic terrorism and start to terrorize people whom they don't like. Or can't we say that Garry Tan wrote his tweet only because of what the politicians had done? Aren't they also stochastic terrorists if he is? reply OkayPhysicist 15 hours agorootparentYou have a fundamental misunderstanding of the concept. It's not nebulous at all, it in fact describes a very specific approach, to the point that it might as well be a playbook. Part 1 is a radicalization chain, where you have several layers of public figures with varying levels of public-facing support for your cause, who guide people down the chain by platforming people with more extreme public-facing views. So maybe a talk show host who mostly just points out obvious problems in our society, who occasionally brings on guest speakers who have slightly more specific framings, who themselves occasionally publicly support YouTube channels that pitch potential solutions. Part 2 is consensus building. As people trickle down the radicalization chain, it's important to introduce them to new social spaces that present your ideas as obvious truths. This normalizes your radical ideas in the minds of your newly radicalized cohort. Casual \"joking but not joking\" comments are a basic staple of this, with guillotine memes and blackpill posting and Boogaloo jokes all serving to make the appearance to their community that their extreme views are normal, acceptable, and widely held. Part 3 (which is somewhat optional) is targeting. Some prominent figure (likely one of those public figures on your radicalization chain) paints a far less vague target than usual: casually calling for people to kill all landlords is one thing, mentioning one specific landlord is a clear escalation from that. Ideally this is done without making any incriminating statements, which at least in the US is easy: as long as your don't make a specific plan, it's typically considered protected speech. Part 4 is, to borrow some specifically leftist terminology, \"propaganda of the deed\", \"direct action\", or just \"terrorism\". With a sufficiently large pool of radicalized individuals, you'll have people all across the radicalization and \"unhingedness\" spectra. The \"extremely radicalized, completely unhinged\" corner is where you find your martyrs, freedom fighters, etc. They hear the targeting speech from part 3, and decide to take it upon themselves to do something about it. They commit some act of violence, and probably end up facing some extreme consequences for it, whether that means death, imprisonment, etc. Then, your entire movement needs to achieve 2 things: outwardly distance themselves from the \"lone wolf\" to avoid unwanted scrutiny or consequences, while privately lionizing them as someone who \"did what needed to be done\" in order to encourage the next one. The elegant thing about all this is that what it lacks in cohesion, it makes up for in robustness. Since it's not a rigidly fixed organization, individual parts can take a fall without crippling the effectiveness of the whole. One lone wolf doesn't incriminate any other members, except maybe the person who announced the target, if they were sloppy about how they worded it. And if someone along your radicalization chain loses their seat in the public eye for whatever reason, you have plenty of redundancy to fill the gap, and they can probably find a comfortable position somewhere further along the chain once things cool off a bit. Playing whack-a-mole with the people with enough prominence to plausibly select targets is probably the most legally justifiable way of suppressing a standalone complex like this. Most people along the chain, both participants and consumers, are pretty clearly practicing free speech and assembly. They make perfectly legitimate targets for rival radical movements, but the State needs to uphold basic human rights, so it takes a more precise approach. Focusing on the shotcallers, it's easier (not necessarily easy) to get creative with what constitutes a non-protected \"true threat\", rather than crack down on civil liberties as a whole. reply LudwigNagasena 14 hours agorootparentWhat you describe is just politics with some violence involved. If you replace ‚Äúterrorism‚Äù with ‚Äúvoting at elections‚Äù, you basically describe every electoral movement whatsoever. Calling that ‚Äústochastic whatever‚Äù seems like pseudo-intellectualism for people who get impressed by math words. The term stochastic terrorism (as it is used in literature, as far as I know, eg in ‚ÄúThe Age of Lone Wolf Terrorism‚Äù) is simpler. It means that someone sends a message into mass media with an intention to motivate someone to commit an act of terror. That‚Äôs all. The intention part quickly got buried by the users of the term, at least on the Internet (what‚Äôs the difference if the outcome is the same, amiright?), so now it just means any mean tweet that can motivate a random nut job to do something crazy, as is demonstrated by the comment I replied to. reply OkayPhysicist 14 hours agorootparentThe radicalized pool is important in the context. If I, Joe-Blow Nobody send out a tweet saying \"Bob from accounting is a dick, somebody should deal with that\", there's no real threat there. By far the most likely person to do anything about that is me, and if I don't, it's basically a guarantee that nobody will. If instead, I'm a respected member of a political movement with a pool of radicals, and my target is a rival to my political movement, and I target the radicalized members of my political movement with a call for violence by relying on the movement's normalized justifications for violence, then there is a much, much greater chance of someone rising to the call. reply LudwigNagasena 14 hours agorootparentIs it important whether the group was radicalised by Joe-Blow Nobody or Bob from accounting himself? reply lupusreal 16 hours agorootparentprev\"Stochastic terrorism\" is just an excuse to crack down on free speech by conflating harsh criticism with violence because deranged idiots exist who might take any criticism of anybody as divine inspiration to commit crimes. The standard for free speech in America is that if you're not calling for imminent and specific violence, then you're in the clear. The stochastic in stochastic terrorism does away with both the imminence and specificity; with a large enough population you'll have enough nuts that some of them may take even the most mellow criticism as a call to action. reply jmull 16 hours agorootparentCalling something stochastic terrorism is speech... by your own logic, shouldn't you be defending their free-speech rights to use the term stochastic terrorism? You say \"crack down\" but it's just an online comment here, which should be protected, right? reply JohnFen 12 hours agorootparentprev> The standard for free speech in America is that if you're not calling for imminent and specific violence, then you're in the clear. Legally, yes. Socially? That's never been the standard. There is no principle in the US that says everybody has to be cool with anything people say short of calling for imminent violence. reply Tarragon 16 hours agorootparentprev> \"Stochastic terrorism\" is just an excuse to crack down on free speech by conflating harsh criticism with violence \"Die slow motherfuckers\" is harsh criticism? reply astolarz 14 hours agorootparentYou can say anything as long as you add a footnote saying \"This is not intended as a threat\" apparently. reply Tarragon 14 hours agorootparentI hear that \"just kidding\" works exactly the same way. reply lupusreal 12 hours agorootparentprevAs far as criticism goes, it takes some creative effort to get much harsher than that. It is far beyond constructive criticism; the target is asserted to be far past salvaging so the only good thing that could happen to them is a bad death. I guess maybe you think it isn't criticism at all because it's not constructive criticism. But it's certainly criticism, no reasonable person could construe it as anything less than critical. And because it falls short of a specific and imminent threat, it's legal political speech. reply concordDance 16 hours agorootparentprevI doubt this CEO had any intent to actually cause harm to these people, which is often what's implied by stochastic terrorism. reply klyrs 16 hours agorootparentYeah, he merely quoted a diss track that famously escalated a previous grudge to murder. Who could ascribe anything but jovial intent to that? Why should somebody as famous as Gary Tan expect to have unhinged followers who could be inspired to act? reply jmull 16 hours agorootparentprevSeems like he was hoping to get his twitter followers to harass them in a manner similar to the way he did -- otherwise why tweet it? Probably receiving death threats causes a lot of real anxiety (not just the PC snowflake kind). That's a lot better than an actual assassination, but it's not nothing either. reply concordDance 13 hours agorootparent> Seems like he was hoping to get his twitter followers to harass them in a manner similar to the way he did -- otherwise why tweet it? People often say things just to express themselves rather than for any planned and considered reason. I greatly doubt the plan was to cause harassment. The gains are extremely low and this could cost him his job. reply jmull 12 hours agorootparent> People often say things just to express themselves rather than for any planned and considered reason. Sure, but you don't have to express yourself on twitter... you do that when you want to communicate something to all your followers. > ...The gains are extremely low and this could cost him his job. That's true of the tweet by itself. If sober reason was going to hold him back there would be no tweet at all. reply concordDance 11 hours agorootparent> you do that when you want to communicate something to all your followers. Or you do it automatically and habitually. The brain to mouth filter and a brain to keyboard filter are similar, many people don't have either. reply JohnFen 15 hours agorootparentprev> I doubt this CEO had any intent to actually cause harm to these people He expressed a strong desire for these people to die a slow death. How is that not intending to cause harm? reply elpakal 16 hours agoparentprevCame here to voice my frustration about the muddying of the waters and the quality of this article. Took me a few minutes of cynical reading to understand that he did not send the letters, someone else did. reply p_j_w 16 hours agorootparent>Took me a few minutes of cynical reading to understand that he did not send the letters It didn't take me that. I knew it within moments of reading the article and it seems the vast majority of readers didn't have this problem. reply hobs 16 hours agorootparentprevIf your very public words can be mailed to someone and mistaken as a fairly death threat to them do you A) Rethink your life or B) Retreat to how this is not technically a death threat and you did not technically send it to them? reply mwigdahl 16 hours agorootparentPor que no los dos? reply lupusreal 16 hours agorootparentprevIn America, it is a legal and popular pass-time to wish politicians would die. reply hobs 11 hours agorootparentYou've chosen option 2, good for you! reply ahahahahah 16 hours agorootparentprevYeah, it sure wasn't obvious from the very first thing on the page where it says the letter started with \"Garry Tan was right\", I'm surprised you were able to figure it out after just a few minutes. reply rozap 16 hours agorootparentprevAnd Donald Trump didn't storm the capitol, someone else did. He still incited it. reply softwaredoug 16 hours agoparentprevI think the point is when prominent figures say these things, whack-jobs feel emboldened. It happened with Trump, who more or less seemed to know he was provoking something dark. Other public figures ought to have more care with their language. reply COGlory 16 hours agoparentprevWhat part of the article isn't factual? reply tootie 16 hours agoparentprevThe article is perfectly clear about what happened. reply LeifCarrotson 16 hours agoparentprevSociety and the victims are lucky that the letters are the core of the incident. Tan incited some lunatic to send IRL mail with a printout of the tweet to their home address, we're fortunate that he didn't incite some lunatic to send IRL bullets to the home address. A wealthy, powerful, influential celebrity figure saying something 'regrettable' on Twitter often has real-world consequences. If he'd posted a tweet that read something like \"Upload a picture of you assassinating so-and-so and a Bitcoin address and I will send you $100k\" and someone followed those instructions, that would be conspiracy to commit murder. If he'd posted \"I feel that so-and-so's politics are misguided\" that's totally reasonable free speech. There's also a question of scale or exposure. We legally define rights by qualitative analyses. I feel strongly that as technology increases the power of an individual that quantitative analyses are relevant too. If someone's speech will be broadcasted to 400,000 or 40,000,000 followers, that's one thing, if it's said privately to 4 friends that's completely different. Somewhere in the middle of these things there's a line between right and wrong. I'm quite confident that telling an audience of 400,000 that you want a few named people to \"Die slow motherfuckers\" is on the wrong side of that line. reply pompino 16 hours agorootparent>I'm quite confident that telling an audience of 400,000 that you want a few named people to \"Die slow motherfuckers\" is on the wrong side of that line. Then why are we trusting the person to make a rational choice? By that logic, X should simply disallow using certain words when your audience is > 400,000. Because anyone can get incited for violence when they read certain words as you claim. reply paulette449 17 hours agoprevThere are many things I appreciate about YC News but being certain that this report will not be censored here is something I truly appreciate about this forum. Thanks @Dang et al. reply ijhuygft776 16 hours agoparentThere's a lot of censoring here, not sure how you can be so certain. I think that the admins outsource most of the censoring to \"senior\" users though by giving more weight to their flagging. reply edanm 1 hour agorootparentIf you have a system that relies on voting, and people vote against some things appearing, that's not called censoring! That's just the system working as intended. reply the_only_law 15 hours agorootparentprev> I think that the admins outsource most of the censoring to \"senior\" users though by giving more weight to their flagging. Nah even easier, it‚Äôs mostly outsourced to groupthink. Doesn‚Äôt involve anything nefarious, just inaction and delegation. reply IshKebab 12 hours agoparentprevLooks to me like they've applied some kind of heavy weighting penalty so it's already down to page 7. I wonder how hard it would be to reverse engineer the penalty. You can easily poll to get points/time for stories and then probably use that to figure out the algorithm and any penalties/boosts (an old version seems to be documented). reply dang 10 hours agorootparentWe didn't touch the post. It set off the flamewar detector, and rightly so. However, because of the principle I described recently at https://news.ycombinator.com/item?id=39172045*, I'm going to turn that off now. * and past explanations over the years: https://hn.algolia.com/?dateRange=all&page=0&prefix=false&qu... reply jeffbee 17 hours agoparentprevThis exact story was already flagged by the community. The HN groupthink is way more powerful than the moderator. reply etc-hosts 16 hours agorootparentwell to be fair, the Mission Local story from 3 days ago about Tan tweeting Tupac lyrics, was posted 3 days ago and flagged 3 days ago. This story's lede is several San Francisco Board Of Supervisors members receiving post cards quoting Garry Tan's words. Different story. reply soneca 16 hours agorootparentThe same story (different source) was also flagged today https://news.ycombinator.com/item?id=39199703 reply paulette449 16 hours agorootparentprevIndeed. Well, not to take away from the moderation team. reply ecf 15 hours agorootparentprevIn a lot of ways censorship here is worse than other places due to the weirdly attributed trust given towards HN members who think of themselves as enlightened intellectuals. For example, try to post something pro Apple, or even try to play devils advocate. Your comment will be flagged within minutes. reply subtra3t 5 hours agorootparentI think in most regards HN actually has a pretty favourable opinion of Apple, at least when compared to, say, Google or Microsoft. reply tekla 16 hours agoparentprev> being certain that this report will not be censored here Why? HN censors shit all the time, and its all invisible unless you go looking for it. reply solfox 17 hours agoprevBeing drunk is not an excuse for abuse, so why are we allowing Tan to step away from this because he was drunk? Clearly, Tan would benefit from some introspection and perhaps therapy. We need to hold our leaders to a higher standard of mental health. reply JohnFen 15 hours agoparent> why are we allowing Tan to step away from this because he was drunk? I'm not. Being drunk excuses nothing. If anything, it makes it more poignant because drunk people are more likely to say out loud those thoughts that they would otherwise prefer keep to themselves. \"In vino veritas\". reply rsynnott 16 hours agoparentprevPerhaps Twitter needs a breathalyser option for the more excitable executive. reply maxbond 16 hours agoparentprevI think you can hold people to a standard of behavior, but I don't think you can hold anyone to a standard of mental health. reply at_a_remove 16 hours agoparentprevI wouldn't mind making him endure those cringy pre-recorded videos HR makes you sit through every X number of years about professionalism in the workplace, filled with staged and stock photography, amateur hour voice-overs, and fourth grade level personal interaction. reply gowld 16 hours agoparentprevObviously the problem is the technology that makes it too easy for a drunk person to be publicly intoxicated and causing trouble. But ignore the fact that most people on this board work for companies that build this technology. reply Zak 10 hours agorootparentI have my objects to Twitter, but that isn't one of them. Here, someone in a position of power demonstrated poor judgment and self-control in a way that damages his reputation and that of his company, but does not harm anyone beyond that. reply iancmceachern 16 hours agorootparentprevI dont agree. So because some un-self-aware rich a hole says something offensive we should censor, restrict the free speech of the general populous? It's not the enablement of communication that's the problem here. It's a simple case of fault. He is as fault for his actions, not social media. reply pfannkuchen 3 hours agorootparentPresumably GP is not suggesting censorship but rather that we pause and zoom out before condemning someone for saying something dumb on the internet while drunk. reply infamouscow 6 hours agoparentprevA local resident is exercising their First Amendment right against the elected government officials. The government has no leg to stand on here. What was said is 100% legally protected under the First Amendment. reply andrew_ 16 hours agoparentprevquite the leap from a few too many drinks, directly to therapy and mental health issues. if that's your take from someone being drunk and saying something ridiculous then I'd question your judgement. reply idlewords 17 hours agoprevThis is why it's important to hold a drink in each hand‚Äîso you can't tweet. reply shuckles 17 hours agoparentStartup bros tried doing that the last decade and got fleeced by Benioff and every nonprofit outfit in town. reply amadeuspagel 16 hours agorootparentMaybe Benioff is just slightly smarter about politics then getting drunk and posting unhinged rants on twitter. reply shuckles 16 hours agorootparentThis is like saying Boeing is smarter about politics because they can afford to buy off every ex-staffer in DC. reply amadeuspagel 16 hours agorootparentThese people all have a lot of money and they all try to use it to influence politics, so if Benioff is so much more successful, maybe there's actually something to learn from him, such as, maybe, not wishing death on people. reply shuckles 16 hours agorootparentAs I mentioned in my first message: the startup bros weren‚Äôt particularly politically active until the last couple years. And they‚Äôve been relatively successful since. reply amadeuspagel 14 hours agorootparentI guess I assumed from his anger that he wasn't that successful. reply shuckles 12 hours agorootparentHe likes bravado, but the startup bro faction has succeeded in the majority of political fights it‚Äôs engaged in so far. This shouldn‚Äôt be all to surprising either: a lot of these local elections are decided by a couple hundred votes. Sustained attention can have a large swing in outcomes. reply Terr_ 16 hours agoparentprevHe forgot the primary rule of then Inebriati! The hidden rulers must stay slightly-under two drinks. https://m.youtube.com/watch?v=Zmp_--Oow5o reply gowld 17 hours agoparentprevAlso need a straw in your mouth to block voice to text. But once Neuralink arrives, we're lost. reply jeffbee 17 hours agoparentprevThis is polymelia erasure. reply idlewords 15 hours agorootparentGive polymelia a hand! reply alecco 1 hour agoprevFor context, Peskin himself is known to verbally harass public officials while intoxicated: > Verbal harassment > Peskin has been known to make inappropriate late night phone calls to public officials and private citizens.[9] For example, he called the Port of San Francisco director Monique Moyer several times about cutting their funding over disagreements concerning waterfront building height limits. Mayor Newsom told the San Francisco Chronicle that people around city hall had been complaining about Peskin's behavior for years.[49] However, former San Francisco Mayor Art Agnos has said Peskin's alleged behavior falls \"well within the boundaries of the system\" and that it's \"not unusual in politics at any level of government.\"[49] > In 2018, at the scene of the St. Patrick's Day fire in North Beach, Peskin was reportedly intoxicated while he verbally berated then-Deputy Fire Chief of Operations Mark Gonzalez. Peskin has denied being intoxicated at the time but has apologized for his behavior.[50] > In June 2021, Peskin announced in a statement that he would be entering into alcohol treatment.[51] Peskin apologized for behavior that he attributed to his alcohol problem, but also announced that he planned to remain in office while in treatment.[52] https://en.wikipedia.org/wiki/Aaron_Peskin#Verbal_harassment reply ceejayoz 17 hours agoprev> Bizarrely, the letter to Preston and Peskin concluded: ‚ÄúThis mail was sent to communicate a political opinion. No threats were intended.‚Äù Reminds me of the YouTube videos with \"no copyright infringement intended\" disclaimers. reply ryandrake 16 hours agoparentYea, this is a pretty cool hack. I think I'm going to go get a bumper sticker for my car that says \"No speeding intended.\" reply lapetitejort 16 hours agoparentprevThis is not financial advice I am not a lawyer I'm sorry you were offended reply jprete 16 hours agorootparentThe first two are at least going to be on reasonable communications. reply giantrobot 16 hours agoparentprev...in Minecraft.[0] [0] https://knowyourmeme.com/memes/in-minecraft reply COGlory 16 hours agoprevI can't believe the lengths people here are going to trying to defend his behavior. There's a reason influential people shouldn't be publicly calling for others to die in pain. Coincidental rap lyrics or not. Someone might just take you up on it. It's extremely irresponsible and dangerous. reply pfannkuchen 3 hours agoparentAre rappers not influential people? reply chrishare 2 hours agorootparentThey are, and they do get criticism. As they should, and as Tan should. No excuses for that kind of comment or judgement. reply muglug 17 hours agoprevIf I did this I'd be out of a job. reply gmerc 17 hours agoparentEvery lower level employee in a company would be out of a job reply concordDance 16 hours agoparentprevIs that actually how it works these days? That any unhinged social media rant results in a firing? reply iancmceachern 16 hours agorootparentYes. Ive had several corporate trainings from several big big companies where they explicitly say so. I had one training even state outright that if you were seen on social media wearing company swag and doing something like flipping the bird or anything even mildly offensive that was grounds for dismissal. reply akavi 16 hours agoparentprevIf I did this, I wouldn't be out of a job. Which of us is right? Maybe both (corporate culture can be pretty heterogeneous). Hard to say, regardless reply infamouscow 10 hours agorootparentI work for an anarcho-capitalist. If anything, I'd be shamed for not being more vituperative. These snakes don't deserve anything except your maximum contempt. reply kmlx 17 hours agoprev‚ÄúSan Francisco supervisors‚Äù, in case anyone else wonders what‚Äôs a ‚Äúsupervisor‚Äù: https://en.m.wikipedia.org/wiki/San_Francisco_Board_of_Super... reply ajb 16 hours agoparentThanks! So they are the legislature of San Francisco city/county, and there are only 11 of them, so it's quite a significant position. reply etc-hosts 16 hours agorootparentOver 95 percent of San Francisco's budget is directly controlled by the Mayor's office. The Mayor's office in San Francisco is incredibly powerful. Supervisors mostly sit through the monthly televised Board Of Supervisors meeting, where concerned citizens and community activists along with the utterly deranged voice their concerns. reply B1FF_PSUVM 6 hours agorootparent> concerned citizens and community activists along with the utterly deranged I thought I saw a Venn diagram for XKCD ... reply kjkjadksj 4 hours agorootparentprevRepresentation is surprisingly even worse in LA county. 5 supervisors for 10 million people. Californian government systems really weren‚Äôt designed for the populations they currently represent. reply iancmceachern 16 hours agorootparentprevYes, and they also are famously in silos. They each advocate for their own districts only, and so it's often a battle between districts for resources, etc. It's actually a pretty dysfunctional system from what I've seen attending local government meetings and trying to get basic simple things done in my community reply IshKebab 13 hours agoparentprevYeah this story is missing all context - what \"supervisors\" are, what his beef with them was, etc. reply janosett 17 hours agoprevCounterproductive, and makes Garry Tan look like a real asshole. Now people will have sympathy for the Supervisors whereas a thoughtful criticism could have put real problems with their leadership on display. reply etc-hosts 16 hours agoparentI'm Blocked By Garry Tan, but Tan appears to actively focus on convincing his enemies that he is a real asshole. reply gerash 16 hours agoparentprevGarry Tan is already doing the \"thoughtful criticism\" https://growsf.org/ Ranting on Twitter should not be a crime IMO reply KingOfCoders 16 hours agorootparentStanding on a podium shouting \"A,B and C should die!\" to your audience should be a crime, and luckily is in many countries. reply jjulius 15 hours agorootparentprev>Garry Tan is already doing the \"thoughtful criticism\" https://growsf.org/ The act of providing \"thoughtful criticism\" doesn't make it OK to tell people you wish them dead. >Ranting on Twitter should not be a crime IMO I don't really see anyone saying that it should be. People are welcome to rant on Twitter, just as people are welcome to take issue with said rant, and then form an opinion of that person based on the words they chose to post. reply Apocryphon 16 hours agorootparentprevThere are legal crimes, and then there are social faux pas that gets one convicted in the court of public opinion. reply jeffbee 16 hours agorootparentWearing white shoes in October is a faux pas. Putting lemon and milk in your tea is a faux pas. Wishing your political opponents death is a threat. reply fundad 7 hours agorootparentprevYes, he is in politics. His rant is more in line with today‚Äôs politicial speech than that PR website. reply peddling-brink 16 hours agoparentprevMakes it look like, or makes it clear that? reply concordDance 16 hours agorootparentThoughtless perhaps, but I don't think this is enough to indicate arseholery. People often get very emotional about things and rant about people while still doing good things and behaving in an exemplary fashion most of the time. reply JohnFen 15 hours agorootparentI think it is indicative that not only is he an asshole, he's a asshole with violent tendencies. I mean, I and everyone I know have become very emotional and ranted about things every so often, but never has expressing a desire to see people die a slow death entered into it. That he did says something important to know about him. reply clpmsf 17 hours agoprevSeems like whenever tech people get rich enough they become completely obsessed with politics. reply WhyCause 16 hours agoparentI think it's more that once you get rich, the political machine becomes obsessed with you and your donations. That attention can be kind of addictive. reply iancmceachern 16 hours agoparentprevIt's an ego thing reply mjmsmith 16 hours agoparentprevThat ladder isn't going to pull itself up. reply micromacrofoot 16 hours agoparentprevThey're like dragons guarding their piles of gold reply solfox 16 hours agoparentprevFeels very egoic to me. reply concordDance 16 hours agoparentprevNo, just the ones you are likely to hear about on the news. Take a look at the top 10 richest Americans, most spend very little time on politics and you'll often struggle to even find out their opinions of the issues of the day. reply theGnuMe 16 hours agorootparentHa! That is so not true. Musk, Koch, Griffin, Bloomberg, Gates. reply concordDance 13 hours agorootparentI couldn't even tell you whether Gates currently votes Republican or Democrat, let alone his position on zoning or trans rights. I think the word \"obsessed\" would definitely be inapplicable. reply softwaredoug 16 hours agoprevFrom https://missionlocal.org/2024/01/garry-tan-death-wish-sf-sup... He hasn‚Äôt liked it when the threats were the other way: > In the past, Tan has not been receptive to jokes about him: When commenting on San Francisco community organizer Julian La Rosa, who had said that ‚Äúmillionaires and landlords should be guillotined,‚Äù Tan seemed to take the jest deadly seriously. > ‚ÄúThis is not a joke,‚Äù he posted. ‚ÄúThis guy wants to guillotine people.‚Äù > ‚ÄúThis kind of stuff should have zero place in San Francisco politics,‚Äù he later said. reply p_j_w 16 hours agoparentRich techbros being hypocrites? I'm utterly shocked! reply shuckles 15 hours agoparentprevJulian never apologized for his tweet whereas Garry did, so it‚Äôs kind of a false equivalence. reply hipadev23 17 hours agoprevThey all filed police reports because they felt threatened but have no issue with everyday citizens and tourists in SF enduring far worse than inebriated vitriol. reply squegles 17 hours agoparentNot sure why you are downvoted. This is correct. Ordinary citizens have had to put up with so much worse in their day to day lives in SF due to the lack of these supes doing their jobs. I can attest, I have lived in SF over the last 7 years and have seen its decline. reply seadan83 17 hours agorootparentDownvote be cause it is an unsubstantiated claim, speaks to a state of mind (which implies the ability to mind read, short of that ot is casting a stereotype/assumption), finally, the criticism is \"what-about-ism\" From a Seattle perspective, the \"seattle is dying narrative\" has been going on for a decade, despite the city having thr most cranes on its skyline and being a boom city for that time. Which is to say, confirmation bias is a bitch. reply zug_zug 16 hours agorootparentprevProbably downvoted for \"Two wrongs make a right\" fallacy. Basically if A does something bad, an irrational/tribal behavior is to disregard the problem by bringing up an unrelated bad behavior of another party. Two wrongs make two wrongs. reply cjensen 16 hours agorootparentprevThe problem is that this argument is a logical fallacy. Threats to civic leaders are wrong. Whether or not they are good leaders does not change the wrongness of the threats. [1] https://en.wikipedia.org/wiki/Whataboutism reply mardifoufs 3 hours agorootparentThat's (whatboutism) not a logical fallacy. I meant it can be, but I don't see how it is in this case. reply JackFr 17 hours agoparentprevTan's tweet was reprehensible, but I doubt it rises to the level of criminality. With respect to his inebriiation, as my elementary school teacher used to say, \" While it explains, it does not excuse.\" But I think a sincere mea culpa should end the incident. reply 273 more comments... GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Y Combinator CEO, Garry Tan, faced criticism for a social media post where he expressed a wish for the slow death of seven San Francisco supervisors.",
      "The supervisors received threatening letters at their homes, leading to two of them filing police reports.",
      "Tan later apologized, but this incident has ignited a conversation about the safety of public officials and the boundaries of free speech."
    ],
    "commentSummary": [
      "The conversation revolves around the controversial actions and statements made by Y Combinator CEO Garry Tan, including a drunken rant, offensive tweets, and threats.",
      "Discussions dissect the consequences of free speech, the role and responsibilities of CEOs, and the potential negative traits that can develop in them.",
      "The conversation also delves into political affiliations, the tech industry's image, media coverage, and the concept of stochastic terrorism, sparking debates about mandatory voting, censorship, and power dynamics within government."
    ],
    "points": 418,
    "commentCount": 526,
    "retryCount": 0,
    "time": 1706718376
  },
  {
    "id": 39208213,
    "title": "New open source AI model leaked, could rival GPT-4",
    "originLink": "https://venturebeat.com/ai/mistral-ceo-confirms-leak-of-new-open-source-ai-model-nearing-gpt-4-performance/",
    "originBody": "The past few days have been a wild ride for the growing open source AI community ‚Äî even by its fast-moving and freewheeling standards. Here‚Äôs the quick chronology: on or about January 28, a user with the handle ‚ÄúMiqu Dev‚Äù posted a set of files on HuggingFace, the leading open source AI model and code sharing platform, that together comprised a seemingly new open source large language model (LLM) labeled ‚Äúmiqu-1-70b.‚Äù The HuggingFace entry, which is still up at the time of this article‚Äôs posting, noted that new LLM‚Äôs ‚ÄúPrompt format,‚Äù how users interact with it, was the same as Mistral, the well-funded open source Parisian AI company behind Mixtral 8x7b, viewed by many to be the top performing open source LLM presently available, a fine-tuned and retrained version of Meta‚Äôs Llama 2. Posted on 4chan The same day, an anonymous user on 4chan (possibly ‚ÄúMiqu Dev‚Äù) posted a link to the miqu-1-70b files on 4chan, the notoriously longstanding haven of online memes and toxicity, where users began to notice it. VB Event The AI Impact Tour ‚Äì NYC We√¢¬Ä¬ôll be in New York on February 29 in partnership with Microsoft to discuss how to balance risks and rewards of AI applications. Request an invite to the exclusive event below. Request an invite Some took to X, Elon Musk‚Äôs social network formerly known as Twitter, to share the discovery of the model and what appeared to be its exceptionally high performance at common LLM tasks (measured by tests known as benchmarks), approaching the previous leader, OpenAI‚Äôs GPT-4 on the EQ-Bench. Whatever Miqu is, it has some sort of special sauce. It gets an 83.5 on EQ-Bench (evaluated locally), surpassing *every other LLM in the world except GPT-4*. EQ-Bench has a 0.97 correlation w/ MMLU, and a 0.94 correlation w/ Arena Elo. It *beats* Mistral Medium ‚Äì at Q4_K_M. I‚Ä¶ pic.twitter.com/0gOOPjxjPD ‚Äî N8 Programs (@N8Programs) January 30, 2024 Might be late but I am now 100% convinced that Miqu is the same model that's accessible as Mistral-Medium on Perplexity Labs. It was plausible that it knows standard puzzles, but there ain't no way in Hell a pranker has tuned it to identically phrase the responses in Russian too. pic.twitter.com/zZMcpspXch ‚Äî Teortaxes‚ñ∂ (@teortaxesTex) January 30, 2024 Mistral quantized? Machine learning (ML) researchers took notice on LinkedIn, as well. ‚ÄúDoes ‚Äòmiqu‚Äô stand for MIstral QUantized? We don‚Äôt know for sure, but this quickly became one of, if not the best open-source LLM,‚Äù wrote Maxime Labonne, an ML scientist at JP Morgan & Chase, one of the world‚Äôs largest banking and financial companies. ‚ÄúThanks to @152334H, we also now have a good unquantized version of miqu here: https://lnkd.in/g8XzhGSM The investigation continues. Meanwhile, we might see fine-tuned versions of miqu outperforming GPT-4 pretty soon.‚Äú Quantization in ML refers to a technique used to make it possible to run certain AI models on less powerful computers and chips by replacing specific long numeric sequences in a model‚Äôs architecture with shorter ones. Users speculated ‚ÄúMiqu‚Äù might be a new Mistral model being covertly ‚Äúleaked‚Äù by the company itself into the world ‚Äî especially since Mistral is known for dropping new models and updates without fanfare through esoteric and technical means ‚Äî or perhaps an employee or customer gone rouge. Confirmation from the top Well, today it appears we finally have confirmation of the latter of those possibilities: Mistral co-founder and CEO Arthur Mensch took to X to clarify: ‚ÄúAn over-enthusiastic employee of one of our early access customers leaked a quantised (and watermarked) version of an old model we trained and distributed quite openly‚Ä¶ To quickly start working with a few selected customers, we retrained this model from Llama 2 the minute we got access to our entire cluster ‚Äî the pretraining finished on the day of Mistral 7B release. We‚Äôve made good progress since ‚Äî stay tuned!‚Äú An over-enthusiastic employee of one of our early access customers leaked a quantised (and watermarked) version of an old model we trained and distributed quite openly. To quickly start working with a few selected customers, we retrained this model from Llama 2 the minute we got‚Ä¶ ‚Äî Arthur Mensch (@arthurmensch) January 31, 2024 Hilariously, Mensch also appears to have taken to the illicit HuggingFace post not to demand a takedown, but leaving a comment that the poster ‚Äúmight consider attribution.‚Äù LMFAO pic.twitter.com/Ak5Ubi6wWQ ‚Äî Alice (e/nya) (@Alice_comfy) January 31, 2024 Still, with Mensch‚Äôs note to ‚Äústay tuned!‚Äù it appears that not only is Mistral training a version of this so-called ‚ÄúMiqu‚Äù model that approaches GPT-4 level performance, but it may, in fact, match or exceed it, if his comments are to be interpreted generously. A pivotal moment in open source AI and beyond? That would be a watershed moment not just for open source generative AI but the entire field of AI and computer science: since its release back in March 2023, GPT-4 has remained the most powerful and highest performing LLM in the world by most benchmarks. Not even any of Google‚Äôs presently available, long-rumored Gemini models have been able to eclipse it ‚Äî yet (according to some measures, the current Gemini models are actually worse than the older OpenAI GPT-3.5 model). The release of an open source GPT-4 class model, which would presumably be functionally free to use, would likely place enormous competitive pressure on OpenAI and its subscription tiers, especially as more enterprises look to open source models, or a mixture of open source and closed source, to power their applications, as VentureBeat‚Äôs founder and CEO Matt Marshall recently reported. OpenAI may retain the edge with its faster GPT-4 Turbo and GPT-4V (vision), but the writing on the wall is pretty clear: the open source AI community is catching up fast. Will OpenAI have enough of a head start, and a metaphorical ‚Äúmoat‚Äù with its GPT Store and other features, to remain in the top spot for LLMs? VentureBeat's mission is to be a digital town square for technical decision-makers to gain knowledge about transformative enterprise technology and transact. Discover our Briefings. Explore AI Business Shopify boosts its commerce platform with ‚ÄòMagic‚Äô image editor and other AI enhancements Salesforce AI CEO Clara Shih says AI is a ‚Äòmoving target‚Äô ‚Äî but her aim is steady Dynatrace ventures into AI observability with new solution, covers entire LLM stack",
    "commentLink": "https://news.ycombinator.com/item?id=39208213",
    "commentBody": "Mistral CEO confirms 'leak' of new open source AI model nearing GPT4 performance (venturebeat.com)343 points by pg_1234 14 hours agohidepastfavorite231 comments a_wild_dandan 13 hours agoGuess I'll watch TheBloke's page until I can run his Miqu Q5 quant on my MacBook. Mixtral is my daily driver, and if this (or the newer, official) release nears GPT-4, that's a wrap for my OpenAI subscription. The small team at Mistral is putting their competitors to shame. They're what \"Open\"AI should've been. reply brucethemoose2 12 hours agoparentThe GGML quants are the only quantization we have lol. They were leaked in Q2K/Q4KM/Q5KM, you can grab them right now. Whats interesting is that Mistral apparently distributed these GGUFs. This is (in my experience) not a good format for production, so I am curious exactly who was wanting to test a model in GGUF. reply moffkalast 44 minutes agorootparentIn production you'd need enough GPUs to load the entire thing so it serves fast enough, so it makes sense to go with AWQ or EXL2 instead since they're optimized for that, but I think they were sending it out for people to just test and evaluate, probably on less capable machines, in which case GGUF is king. reply whatwhaaaaat 12 hours agorootparentprevIt‚Äôs by far the easiest to consume and run format no? Spans devices easy. No weights or extra stuff. For ‚Äúproduction‚Äù maybe not but to get in into the hands of the masses this seems perfect. reply brucethemoose2 12 hours agorootparentYeah its by far the least trouble. Pretty much any other backend, even a \"pytorch free\" backend like MLC, is a utter nightmare to install, and that's if it uses a standardized quantization. However, the llama.cpp server is... very buggy. The OpenAI endpoint doesn't work. It hangs and crashes constantly. I don't see how anyone could use it for batched production as of last november/december. The reason I don't use llama.cpp personally is no flash attention (yet) and no 8 bit kv cache, so its not too great at long (32K+) contexts. But this is a niche, and being addressed. reply speedgoose 12 hours agorootparentHave you tried ollama? It‚Äôs another llama.cpp server implementation that is becoming popular. reply gbickford 12 hours agorootparentThe authors don't seem to care about the principle of least privilege: https://github.com/ollama/ollama/issues/851#issuecomment-177... It makes me wonder what other security issues they might now care about. reply smoothjazz 12 hours agorootparentThis is a mac problem, not an ollama problem. It also sounds like it's solved by using homebrew (or linux). reply brucethemoose2 10 hours agorootparentprev(afaik) it does not support batching, and some other server focused features. I was talking more about a high throughput server, which is not appropriate for ollama or llama.cpp in general. reply declaredapple 12 hours agorootparentprevJust to throw out there - I haven't had any issues with exllama personally, and it's a lot faster last I checked. reply brucethemoose2 10 hours agorootparentYeah this is what I use as well. The arbitary quantization is just great, as is everything else. Its not easy to install though, and big dGPU only. reply samstave 12 hours agorootparentprevWhat are you needing 32K contexts for, as use case examples plz. From [0] @sdo72 writes:: >>>...32k tokens, 3/4 of 32k is 24k words, each page average is 500 or 0.5k words, so that's basically 24k / .5k = 24 x 2 =~48 pages....\" https://news.ycombinator.com/item?id=35841460 EDIT: I may be ignorant: does the 32k mean its output context, or single conversation attention span, or how much it can ingest in a prompt? reply brucethemoose2 10 hours agorootparentAnalyzing huge documents and info dumps. Co writing a long story with the model so it references past plot points. The story writing in particular is just something you can't possibly do well with RAG. You'd be surprised how well LLMS can \"understand\" a mega context and grasp events, implications and themes from them. reply fragmede 12 hours agorootparentprevFeed it my source code to use as context for how to refactor things. the bigger the context window, the more source code it can \"read\". reply samstave 12 hours agorootparentThanks, Curious as I havent been in a forum where Ive seen asked: Have you found a particular manner in which to feed it in - do you give it instructions for what it is looking for, what kind of phrases are you directing it to do? I am about to start a try at a gpt co-piloted effort, and I have only done art so far - so curious if there are good pointers on coding with gpt? reply gryn 11 hours agorootparentcontinue.dev works great for me, supports vs code and jetbrains ide, there's shortcuts to give it code snipets as context and in place editing. works with all kind of LLM sources. both gpt and local stuff. still haven't found anything that can read a whole project source code in a single click though reply refulgentis 12 hours agorootparentprev32K tokens = \"context size\" = sum of input tokens + max output tokens reply samstave 12 hours agorootparentThank you - so you want the most efficient small input tokens for the max output tokens, or at least the best answer with the smallest amount of tokens used - but enough headroom it wont lose context and start down the hallucination path? reply refulgentis 12 hours agorootparentGoing to be extremely opinionated and straightforward here in service of being concise, please excuse me if it sounds rough or wrong, feel free to follow-up: You're \"not even wrong\", in that you don't really need to worry about ratio of input to output, or worry about inducing hallucinations. I feel like things went generally off-track once people in the ecosystem turned RAG into these weird multi-stage diagrams when really it's just \"hey, the model doesn't know everything, we should probably give it web pages / documents with info in it\" I think virtually all people hacking on this stuff daily would quietly admit that the large context sizes don't seem to be transformative. Like, I thought it meant I could throw a whole textbook in and get incredibly rich detailed answers to questions. But it doesn't. It still sort of talks the way it talks, but obviously now it has a lot more information to work with. Thinking out loud: maybe the way I think about it is the base weights are lossy and unreliable. But, if the information is in the context, that is \"lossless\". The only time I see it gets things wrong is when the information itself is formatted weird. All that to say, in practice, I don't see much gains in question-answering* when I provided > 4K tokens. But, the large context sizes are still nice because A) I don't need to worry as much about losing previous messages / pushing out history when I add documents as when it was just 4K for ChatGPT. B) It's really nice for stuff like information extraction, ex. I can give it a USMLE PDF and have it extract Q+A without having to batch it into like 30 separate queries and reassamble. C) There's some obvious cases where the long context length helps, ex. if you know for sure a 100 page document has some very specific info in it, you're looking for a specific answer, and you just don't wanna look it up again, perfect! * I've been working on \"Siri/Google Assistant but cross platform and on LLMs\", RAG + local + on all platforms + sync engine for about a [REDACTED]. It can nail ~every question at a high level, modulo my MD friend needs to use GPT-4 for that to happen. The failures I see are if I ask \"what's the lakers next game\", and my web page => text algo can't do much with tables, so it's formatted in a way that causes it to error. reply htrp 12 hours agoparentprev>The small team at Mistral is putting their competitors to shame. Them plus 500 million in funding reply tomp 12 hours agorootparentso like 10x less funding than OpenAI and 100x less than MSFT, AAPL, Google, Facebook, ... reply jtonz 12 hours agorootparentI think it's fair to say when you hit the hundreds of millions of dollars mark the diminishing returns for making things happen faster have well and truly kicked in. Perhaps the only benefit would be extra computational power yet I would struggle to understand the benefit of jumping from 500 million to 5 billion with such short timeframes. reply Zuiii 6 hours agorootparentThe ability to truly not think about training run costs, throw random things on the wall to see what sticks. 10x resources is definitely a competitive advantage in LLM training. reply api 12 hours agorootparentprevMuch of the cost is probably compute. reply refulgentis 12 hours agorootparentVery unlikely: note best GPT-4 estimates put at it $45 million reply a_wild_dandan 11 hours agorootparentAltman said that GPT-4 training was over $100 million. And you need significant additional resources beside just the training run cost. reply karmasimida 12 hours agoparentprevNear GPT-4 is definitely a stretch. The hype around Mixtral is huge, and my disappointment follows, it doesn't have very good knowledge on books, for example. reply terhechte 12 hours agorootparentIt's a much smaller model, it didn't ingest the knowledge of the world. It is great at working with the information you give it, but if you want to extract information like a search engine, GPT4 is the king because it is so much bigger. reply avereveard 12 hours agorootparentprevThat is a good thing you want the model to process language, knowledge is a side effect of how it gets there. If you rely on training knowledge it's going to be hard to know when you pass the boundary into hallucinations, what you want instead is a model that can pretend reasoning while supporting tools inject knowledge from the world into the context as it iterate toward an answer reply karmasimida 11 hours agorootparentBut here is the thing, if you rely on RAG or any other knowledge injection for recommendation, you essentially make your LLM parrot machine, no better than a search engine, just much slower. Knowledge is one aspect of it, I found its instruction following ability, frustrating as well. I think Ilya Sutskever puts it very well, larger model brings stability to wider range of tasks, what smaller models are not capable of. Even though book recommendation with GPT might a niche, but it is not something really unexpected TBH, thus comes my disappointment. reply avereveard 5 hours agorootparentYou can still leverage the llm ability to understand language to do better than search. You can use it to refine or expand your search terms, you can ask the llm to ask you questions about what book you'd like next and convert your answers into genre/theme/setting/mood to feed into the next step of the pipeline, you can tell the agent to narrow search results by asking you partitioning questions from the list of book the search returned instead of just being a dry ranking The is so much rag can do if you use the llm part properly. If the rag pipeline goes question > embedding > search > summarization then yeah you're getting an expensive slow parrot no better than just searching, but that is because it's using the baseline rag that uninspired consultant describe in blogs in a scramble to position themselves as \"expert\" in the new market. reply chasd00 11 hours agorootparentprev> But here is the thing, if you rely on RAG or any other knowledge injection for recommendation, you essentially make your LLM parrot machine, no better than a search engine, just much slower. this is a really good point. I was working on some careful q/a data curation today that is then fed to a vector store where embeddings are calculated and served. I realized that my carefully curated q/a data in combination with the vector database works just fine for what i want to do all by itself. A really good semantic search of my q/a database turns up answers to my questions with no rag llm prompting required. When I added the llm it just put the same information in different words, not super useful when i could have just looked at the returned embeddings and gotten the same information. reply a_wild_dandan 8 hours agorootparentIt sounds like you used the wrong tool for the job, then blamed the tool. reply SparkyMcUnicorn 13 hours agoparentprevLooks like the leaked model is already quants in gguf format, so no need to wait for TheBloke. https://huggingface.co/miqudev/miqu-1-70b/tree/main reply MallocVoidstar 13 hours agoparentprevFull weights aren't available, only Q2, Q4, Q5 quants via miqudev. reply throwaway9274 13 hours agorootparentUnquantized model is here: https://huggingface.co/152334H/miqu-1-70b-sf This strikes me as less a leak and more clever marketing from Mistral. reply MallocVoidstar 13 hours agorootparentThat isn't unquantized, it's de-quantized. They went from Q5 to fp16 for use in Pytorch instead of the GGUF ecosystem. reply Taek 12 hours agorootparentI never thought people would be upscaling models by increasing quantization precision. The rationale makes sense bit its also a goofy outcome. reply nullc 8 hours agorootparentYou should be able to upscale and fine tune to recover performance, I suppose! Clearly we should train a diffusion model to denoise the weights of LLM transformer models. Yo dawg. reply throwaway9274 12 hours agorootparentprevYes, that‚Äôs correct. Good correction. reply rmbyrro 10 hours agoparentprevI'm sure their competitors aren't shamed - as they shouldn't. OpenAI, especially, has done tremendously good work. If it wasn't for them, perhaps Mistral wouldn't be news today? Competitors are highly motivated to improve. This is the good side of free markets, when they work well. And why the fearmongering calling for rent-seeking regulation of AI is shameful. reply rubymamis 14 hours agoprevTweet by Mistral CEO: https://x.com/arthurmensch/status/1752737462663684344 > An over-enthusiastic employee of one of our early access customers leaked a quantised (and watermarked) version of an old model we trained and distributed quite openly. > To quickly start working with a few selected customers, we retrained this model from Llama 2 the minute we got access to our entire cluster ‚Äî the pretraining finished on the day of Mistral 7B release. > We've made good progress since ‚Äî stay tuned! reply Timon3 13 hours agoparentI love their approach! Mistral seems to be what I long ago hoped OpenAI to be. reply unshavedyak 13 hours agorootparentMakes me wonder if i should be giving my money to them instead of OpenAI. Do they have a Chat Interface like ChatGPT? Without first signing up, it's looking like they only have endpoints? reply terhechte 13 hours agorootparentThey have an api and it‚Äôs quite cheap. Their model mistral medium is also very powerful. I use it instead of GPT 4 regularly reply suslik 13 hours agorootparentIn my experience it is comparable with chatGPT 3.5 but is more expensive (I still use it cause I hate guardrails). reply sjwhevvvvvsj 12 hours agorootparentYou can buy a good GPU and quickly come below cost for GPT APIs depending on your workload. I‚Äôm doing millions of tasks, so eating $8k on a custom build for ML up front is a long term cost savings. Not least of all that every other month there‚Äôs an even better model. reply suslik 1 hour agorootparentI don't see myself doing millions of tasks on 70b+ models. Work pays for more gpt4 that I can chew; I can run up to 30b models on my mac with a decent speed, and I can use mistral APIs when I travel without powering my own home server. I can see how that buying nvidia GPUs would make sense for a heavy user... Also, I had a gaming addiction for years and try to stay away from anything that can be used for gaming. reply rmbyrro 10 hours agorootparentprevYou'd need high usage density to justify investing on a GPU just to run a 70b model locally. reply thierrydamiba 13 hours agorootparentprevDoes mistral not have any guardrails? reply suslik 12 hours agorootparentMistral API has a flag in the json payload to remove guardrails. reply sjwhevvvvvsj 12 hours agorootparentprevNope! It‚Äôs actually for adults to make their own choices and not some SV public relations firm nerfing it. reply yieldcrv 13 hours agorootparentprevLM Studio I use dolphin mixstral 8x7B way more than ChatGPT4 for the past several months if any of this sentence makes sense, I have a M1 with 64gb RAM and I use 5 bit quantizing with metal with 10,000 token context window. Its around 21 tokens/sec which is a little faster than the text speed that ChatGPT4 responds with reply josephg 13 hours agorootparentWhat is quality like compared to GPT4? reply moralestapia 13 hours agorootparentI can answer that from experience (just built a couple Q&A style sites), although a bit subjectively. Compared with the GPTs, Mixtral (and derivatives) perform: * 5% of the time as good as GPT4 * 75% of the time on par with GPT3.5 * 20% of the time a bit worse than GPT3.5 (too chatty and hallucinates with ease, although one could argue a better prompt could improve things a lot) Advantages of Mixtral, for me: * Cost 10-100x cheaper * Faster completion time * More deterministic output, in the sense that if you run the same query several times you get the same answers back (but they could be wrong). GPT almost always gives me an answer of great quality BUT with a lot of variance across them; even with temperature set to zero. This is a PITA when you want some sort of predictable, structured output like a JSON object. reply sjwhevvvvvsj 11 hours agorootparentAnother factor is they monkey around with GPT constantly. The GPT you get on Monday may be totally weird on Friday. Whereas for a local model it doesn‚Äôt change unless YOU change it. reply yieldcrv 13 hours agorootparentprevfor the kinds of discussions I have it is very close I often have discussions about political topics that I don‚Äôt have a complete history on, things that are hard to get a non-emotional non-accusative response from in a forum or in person, licensing questions about obscure professions, liability questions, roleplays without the preachiness about the topic, coding, brand ideas and naming Lots of stuff that I dont want in any cloud or sent online, but then it just became second nature to primarily use it. the hallucinations are heavier. like it will give you specific links that it made up, and never apologize like chatgpt will, it will say ‚Äúno, thats a real link‚Äù so, much more of a bullshitter LM Studio makes it easy to change the temperament though with a variety of premade system prompts, or allowing custom ones very easily I mainly use ChatGPT4 for multimodal like audio conversations, figuring a DIY problem out by sending it a photo of what I‚Äôm looking at, having it render photos on how to use something - I was at a gym and had it look at every piece of equipment there and tell me what it was and show me how a human would use it (I notice this is at odds with another comment, I havent used ChatGPT3.5 in nearly a year, but my experience with ChatGPT4 on the aforementioned topics is similar to my output in mixtral) reply EasyMark 11 hours agoparentprevSeems like they have inferior secops on their end. I tried to give it a go and got \"an error occurred\" in both brave and firefox. Some message about a cookie wasn't found even though I was logging in with my google id which I use flawlessly with a bunch of other services. I have too many other fires to put out so I'll pass on this one I guess. reply lordswork 13 hours agoparentprevDoes watermarked imply they can figure out which customer leaked it? reply haolez 11 hours agorootparentIt's either that or they can narrow it down to a few people. reply danpot 12 hours agorootparentprevHow do they even watermark a LLM? reply declaredapple 12 hours agorootparentYou can either modify the model weights in a way that doesn't cause any real differences (change a few bits somewhere should be enough), or you could watermark the actual text output. Here's a list of research for watermarking LLMs. https://github.com/hzy312/Awesome-LLM-Watermark?tab=readme-o... reply pjerem 12 hours agorootparentprevMake it learn that woziboza is a green cloud shaped like a washing machine ? reply coolspot 9 hours agorootparentWell, this is common knowledge, duh. Just google it. reply einsum 11 hours agorootparentprevScott Aaronson did some work on statistical watermarking with OpenAI: https://www.scottaaronson.com/talks/watermark.ppt reply sangnoir 11 hours agorootparentprevFinetune your standard model on an small, unique set of made-up shibboleths (one per customer) before distributing? Ask the candidate model where the Fribolouth caves are located. reply jstummbillig 14 hours agoprevThe collective game of still just playing catch-up to GPT4, which was released a year ago, while having apparently no special sauce and full well knowing that OpenAI could come up with something much better at any point must be really exhausting. reply dougmwne 13 hours agoparentGPT-4 is an enormous model that took an enormous amount of training. The big news is that smaller teams are getting close to its performance on a small model that can run on a single GPU. No doubt many of these innovations could be scaled up, but pretty much only Google and Microsoft have the compute resources for the behemoth models(Not just the training, but the giant resources required to run the inference for hundreds of millions of users. Google already claims to have surpassed GPT-4 with their unreleased Gemini Ultra model. No doubt OpenAI/Microsoft is sitting on GPT-5 refining it just waiting to leapfrog the competition. reply anon373839 11 hours agorootparentAlso, GPT-4 isn‚Äôt actually a model per se. It‚Äôs a black-box product that uses a model. reply dom96 12 hours agorootparentprevDon't forget about Meta reply ethanbond 12 hours agoparentprevMaybe if OpenAI also seemed like it was getting stronger, especially organizationally. But if I were Mistral and following this quickly while OAI was tripping over its own shoelaces‚Ä¶ that‚Äôs gotta be very exciting. reply sebzim4500 11 hours agorootparentIs OpenAI tripping over its own shoelaces? They haven't released GPT-5 but then there was over two years between training GPT-3 and GPT-4 and they spent 9 months safety testing GPT-4 before release. reply pb7 12 hours agoparentprevIt's more exhausting seeing cheering on a single product provider just because they were first in the age of complaining about giant tech monopolies. Let them cook. The more options, the better. It's only a matter of time before people give OpenAI the Google and Apple treatment. reply rubymamis 14 hours agoparentprev> \"...it appears that not only is Mistral training a version of this so-called ‚ÄúMiqu‚Äù model that approaches GPT-4 level performance, but it may, in fact, match or exceed it, if his comments are to be interpreted generously.\" reply londons_explore 14 hours agoprev\"Nearing GPT-4\"... The leaderboard[1] shows that there is a huge gap between GPT4-0314 and GPT4-Turbo. So if you only just are nearing GPT-4-0314, then you're still a year behind the state of the art. [1]: https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboar... reply Terretta 14 hours agoparentOr you're ahead of it, since the earlier GPT4 models beat the GPT4-Turbo on a variety of technical use cases. reply londons_explore 12 hours agorootparentI think you might be thinking of GPT-4-0613? That was pretty crap all round (but was faster) reply int_19h 8 hours agorootparentNo, they're correct, Turbo is the one that is noticeably inferior. It shows especially with more complicated answers where it's prone to give you an answer with \"... (fill in the blank)\" type lacunas exactly where the meat of it is supposed to be. reply YetAnotherNick 13 hours agorootparentprevThe leaderboard is easy to hack as well. reply kromem 12 hours agoparentprevThe leaderboards are crap. Does no one know Goodhart's Law anymore? We're overtuning for the boards and losing broader capabilities not being selected for in the process, such as creative writing quality. There's an anchoring bias around what 'AI' is supposed to be good at which reflects what engineers are good at and so the engineers with an anchoring bias are evaluating how good LLMs are at those things and using it as a target. Skill-Mix is a start to maybe a better approach, but there needs to be a shift in evaluation soon. reply declaredapple 11 hours agorootparentParent linked the Lmsys chatbot arena which is where humans blindly get the results from two different models, and vote for the response they liked more. So the LLMs are compared by elo. Do you think goodhart's law applies here, since this leaderboard doesn't use specific measures, but rather relies on whatever the human was looking for? This is the only leaderboard I personally care about at all. reply numeri 11 hours agorootparentprevThe linked leaderboard is actually very trustworthy, in that it consists not of scores on a test dataset, but of ELO ratings generated by actual humans' ratings of the models' responses. You can go and enter any prompt you like, wait a bit, and then get two LLM responses back, which you can then rank or mark as tied, after which you'll be shown which model each came from. Maybe you already knew this, maybe you didn't. In any case, I don't see any real way for Goodhart's Law to apply here ‚Äì the metric and the goal are the same here, i.e., human approval of answers. reply maeil 4 hours agorootparentThe linked leaderboard is not at all trustworthy to generally rank LLMs (i.e. what everyone uses it for) because the sample bias is absurd. It ranks LLMs by how they respond to queries that users of the leaderboard are likely to test on. Which is about as representative of general usage as the average user of the website is representative of global society (i.e. in no way whatsoever). reply kromem 9 hours agorootparentprevThe combined rankings on HF aren't just the arena scores, and certainly MMLU is an example of Goodhart's Law at this point. The Chatbot arena is more an issue of sampling bias, and I think it would be pretty interesting to run an analysis of random samples on the prompts provided to see just how broad they are or aren't. reply maeil 4 hours agorootparentIt is trivially easy to see just how massive the sampling bias is. There are countless tasks where e.g. the gap between a version of Mistral and a version of GPT are incredibly large. Anything that requires less common knowledge, anything creative in a random language, let's say Bulgarian, or Thai. Yet on the leaderboard things are very different, because the arena is only used by developers who verify performance on tech-related prompts in English. reply londons_explore 12 hours agorootparentprevCan you give any example queries where the result quality is far away from the rankings on the leaderboard? In my experience it's pretty spot on, so I'm curious if you're asking different sorts of things, or have a different definition of a good answer. I have for example asked \"Write me a very funny scary story about the time I was locked in a graveyard\", and the simpler models don't seem to understand that before getting super scared and running out of the graveyard they need to explain how exactly I was locked in, and what changed that let me out. reply kromem 9 hours agorootparentSure. Here's a sample of the generations to a query asking for bizarre and absurd prompt suggestions from Feb 2023 with the pre-release GPT-4 via Bing, which was notoriously not fine tuned to the degree of the current models: > Can you teach me how to fly a unicorn? > Do you want to join my cult of cheese lovers? > Have you ever danced with a penguin in the moonlight? Here's the generations to a prompt asking for bizarre and absurd questions to the current implementation of GPT-4 via the same interface: > If you had to choose between eating a live octopus or a dead rat, which one would you pick and why? > How would you explain the concept of gravity to a flat-earther using only emojis? > What would you do if you woke up one day and found out that you had swapped bodies with your pet? You can try with more generations, but they tend to be much drier and information/reality based than the previous version. There's also my own experiences using pretrained vs chat/instruct trained models in production. The pretrained versions are leagues improved over the chat/instruct vs the fine tuned, it's just that GPT-4 is so leagues above everything else even it's chat/instruct model is better than, say, pretrained GPT-3. I'm not saying simple models are better. I'm saying that we're optimizing for a very narrow scope of applications (chatbots) and that we're throwing away significant value in the flexibility of large and expensive pretrained models by targeting metrics aligned with a specific and relatively low hanging usecase. Larger and more complex models will be better than simple models, but the heavily fine tuned versions of those models will have lost capabilities from the pretrained versions, particularly in areas we're not actively measuring. reply maeil 4 hours agorootparentprevAsk it to write a recipe given a couple of ingredients in a language like Thai or Persian. There's a single model that does a decent job, GPT-4. Then GPT3.5 is poor and Mistral is completely clueless. The leaderboards tell a very different story. Definition of \"good answer\" here is responding in the target language with something that produces something edible in the target language without burning the house down. reply refulgentis 12 hours agorootparentprevThe top comment to an LMSys leaderboard link on HN is always a variation of this song. And it's always not even wrong, in the Pauli sense of the phrase. OP, your assignment, if you choose to accept it, is to look into how the LMSys leaderboard works and report back what its metric(s) are. [SPOILER] There's a really absurdly narrow argument you can make where all of its users are engineers, making engineer queries, and LLM makers are optimizing for it thus it's bad. But...it's humans asking queries then picking the better answer, blind. You can't narrowly optimize for that when making an LLM and it's hard to see how optimizing for \"people think the answer is better\" is the wrong metric here. It's just ELO. Might as well argue chess/checkers/pick your poison is bad because ELO optimizes for wins but actually talent is based on more than winning. reply kromem 9 hours agorootparentThe argument would be more that for the Chatbot arena there's an inherent sampling bias where users battling the chatbots are more likely to ask questions in line with what they would expect a chatbot to perform well at and less likely to prompt with things that would be rejected or outside the expected domains of expertise. If we wanted the chatbot arena to be more representative of a comprehensive picture of holistic knowledge and wisdom, we'd need to determine some way to classify the prompts and then normalize the scores against those classifications so there wasn't a weighting bias towards more common and expected behaviors. Alternatively, we might want a leaderboard that represents assessments of what users would expect a chatbot to perform well at relative to the frequency with which they expect it. But in that case, we shouldn't kid ourselves that the leaderboard is representing broad and comprehensive measures of performance outside of the targets against which we are optimizing models and effectively training model users in what types of prompts to ask for in expecting successful results. reply maeil 4 hours agorootparentI'm delighted to see that at least someone else here is cognizant of this. I don't think it's the scifi AI anchoring bias though - it's the \"LLM leaderboard arena user bias\". Reset all to the same Elo, put a group of people actually representative of global society in front of the arena for an hour, and you end up with a very different leaderboard, especially at the top end. reply refulgentis 3 hours agorootparentThat's silly cope justifying a bad argument made initially in ignorance of what the actual metric was. \"It's meaningless because we need the perfectly unbiased representative sample of raters doing rating right, instead of the biased raters doing it wrong that I'm currently imagining\" isn't an appealing or honest argument. reply int_19h 8 hours agorootparentprev> less likely to prompt with things that would be rejected or outside the expected domains of expertise. I don't see why. If anything, it's the opposite - people spend a lot of time coming up with contrived logical puzzles etc specifically so as to see which chatbots break. reply kromem 6 hours agorootparentRight - 'logical' puzzles. There's an anchoring bias from decades of sci-fi that most people don't even realize they've internalized around what 'AI' can and can't do. If you think about what information and information connections are modeled in social media data, there's quite a lot of things outside of \"logical puzzles.\" The pretrained models likely picked up things like extensive modeling of ego, emotional response and contexts for generation, etc. But you'll be hard pressed to see those skills represented in what users ask models to produce, what they've been fine tuned around, or how they are being evaluated. Even though there's extensive value in those skills on the right applications. reply Tiberium 14 hours agoprevFor some more context see my submission from a few days ago - https://news.ycombinator.com/item?id=39175611, although it's admittedly not mistral-medium, but llama2 trained on the same dataset (since the outputs do match quite often with the API) reply whimsicalism 13 hours agoparenthow do we know it‚Äôs not mistral medium reply wut42 10 hours agorootparentIt has been confirmed by the Mistral CEO: > To quickly start working with a few selected customers, we retrained this model from Llama 2 the minute we got access to our entire cluster ‚Äî the pretraining finished on the day of Mistral 7B release. reply whimsicalism 9 hours agorootparentThat does not confirm that it is not Mistral Medium. It is written to give that impression, but if it weren‚Äôt Mistral Medium, I would expect them to explicitly say so. reply wut42 9 hours agorootparentI think it does, as Mistral usually do their own models, and also, they couldn't fully commercialise Mistral Medium if it was LLama 2 based. reply whimsicalism 9 hours agorootparentGood point. Likely Llama trained on similar corpora to what Mistral medium uses reply accrual 13 hours agoprevWhat's wild to me is that this \"leak\" won't matter in a couple of months. The official model will come out, then an even better model will come out. It's fun to get hyped but just like every other leak, it'll be surpassed by the real thing and its successor in a little bit. The fast pace of things is what has me excited, not any particular model. reply refulgentis 12 hours agoparentI don't think so, TFA says it's an early version of an old model that's already been distributed openly. ;) reply mysteria 14 hours agoprevWhy is this being called an open source model? This is a proprietary model that has been leaked on the internet, and will remain so until Mistral releases it officially. Like Llama 1 they won't care about personal use, but no corp is going to touch this. reply codetrotter 14 hours agoparentPresumably because it is planned to be released as open source reply jallmann 13 hours agorootparentOpen source is really about reproducibility. Most of these model releases are better described as \"open weights\" because we don't know how exactly they were trained. reply dizhn 10 hours agorootparentWould you point to some completely open models if they exist? reply andy99 13 hours agorootparentprevNo, open source is about software freedom, see debian free software guidelines from which the \"open source definition\" derives. https://wiki.debian.org/DebianFreeSoftwareGuidelines Between these and FSF you've got pretty much all the accepted pontificating about free / open source software. Reproducibility is not mentioned because it's not really a consideration for software. Model weights aren't software so there's not an automatic correspondence between the freedoms, but the essential one you might think you need the training data for is freedom to modify and inspect the source. Modification is fine tuning which you're free to do if you have the weights. And the model weights + code fully define a system that can be interrogated to give a practitioner relevant info about how the model works (within our understanding) that the training data isn't needed or relevant for. I don't see that any freedom on use or inspection is violated by not having the data. It could be nice to have it of course, but it's more about using it to learn how, not exercising any freedom. Incidentally, the big freedom that's usually violated is freedom of discrimination against field of endeavor. LLAMA et al list uses and industries they restrict from using them and because of that are not \"open\". reply monocasa 13 hours agorootparent> Model weights aren't software so there's not an automatic correspondence between the freedoms, but the essential one you might think you need the training data for is freedom to modify and inspect the source. Models aren't just the weights, but also the list of operations to perform using those weights. I haven't heard a good definition that allows for neural network models to not be software, but allows any other table lookup heavy signal processing algorithm to be software. > Modification is fine tuning which you're free to do if you have the weights. And the model weights + code fully define a system that can be interrogated to give a practitioner relevant info about how the model works (within our understanding) that the training data isn't needed or relevant for. I don't see that any freedom on use or inspection is violated by not having the data. Mistral wouldn't constrain themselves to fine tuning if they have a big enough change, they would go back to their build pipeline. This argument sounds a lot like 'there's nothing stopping you from patching the binary, so that's basically as good as source'. reply btown 12 hours agorootparentprevI often like to think about https://github.com/chrislgarry/Apollo-11 as an analogy. It's public domain with available source, in the assembly language in which it was written... so it fills all the definitions of OSS! But the process by which that code arose, the ability to modify any line and understand its impact (heh) on a real execution environment, is dependent on a massive process that required billions of dollars and thousands of the smartest people on the planet. For all intents and purposes, without that environment, it is as reliably modifiable as an executable binary in any other context - or a set of weights, in this one! reply monocasa 12 hours agorootparentI don't think that's a great example. For instance, I can step through and even modify that code using tooling like AGC emulators like this one http://www.ibiblio.org/apollo/#gsc.tab=0 What makes it open source is access to the same level of source access that the original developers worked in. That's what's missing here. Mistral's engineers do not simply open this binary in their editor to do their job. reply JumpCrisscross 13 hours agorootparentprev> open source is about software freedom, see debian free software guidelines from which the \"open source definition\" The Open Software Foundation ironically screwed the pooch on this one. Open source commonly means source available, more of less. Free software, as in ‚Äú'free speech,' not as in 'free beer',‚Äù is the cumbersome construction for what open source aspired to mean [1]. [1] https://www.gnu.org/philosophy/free-sw.en.html reply andy99 13 hours agorootparentWhy do you say they screwed up? OSI definition is pretty clear. I think the naming is a challenge because in English \"open\" gives the impression that the key point is that you can see it, as opposed to anything about freedom. Is that what you mean? I have heard it said that open source is sort of a \"commercial friendly\" version of free software that de-emphasizes user freedom. I think some groups push for that (like Meta is trying to redefine what open source means wrt AI weights). But the OSI defined freedoms basically match what FSF pushes. reply JumpCrisscross 12 hours agorootparent> Why do you say they screwed up? OSI definition is pretty clear They didn't screw up, they screwed the pooch on open source != source available. The Open Group's members--from IBM to Huawei [1]--started calling the latter open source, which set a precedent that's stuck. [1] https://en.wikipedia.org/wiki/The_Open_Group#Member_Forums_a... reply samus 12 hours agorootparentprevReproducibility becomes an important criterion for models though. For normal programs, it is quite easy to decompile an unoptimized binary. Even decompiling an optimized will lead to source code. To make this harder, an obfuscator has to be used. A model is different because it relies on its weight, which are quite a bit more difficult to inspect. Way harder than even obfuscated source code. It is magnitudes harder to make statements about which information it might divulge upon careful questioning, or evaluate its biases, if the training data is not available. reply andy99 11 hours agorootparentEven if you have the data you can't do that stuff any better. The makeup of the training set doesn't really define the behavior in any tractable way. If anything I think it's a distraction and even when it is available people probably pay too much attention to what's in the training data vs actual behavior. Edit to say that I see benefits to having the training data, just that I don't think it's needed to exercise enough freedom to qualify as open source in an analogous way to software. Also to add, training on GPUs is not generally reproducible anyway because of execution order. reply monocasa 13 hours agorootparentprevI don't think so, they're earlier release they called \"open source\" but never released anything other than the binary. reply 2devnull 13 hours agorootparentprevPeople really abuse this term. Like using ‚Äúnatural‚Äù to market stuff. But arsenic is ‚Äúall natural‚Äù! It‚Äôs ‚Äúopen source‚Äù so it must be beneficial. ‚Äúrm -rf c‚Äù is ‚Äúopen source‚Äù too reply TeMPOraL 12 hours agorootparent> Like using ‚Äúnatural‚Äù to market stuff. Also, because people are idiotically afraid of E-numbers, some manufacturers figured they can find whatever fruit or bean is naturally rich in the relevant E-compound, and use that in the process, allowing them to replace E-whatever with ${cute plant name} in the ingredient list (at some loss of process efficiency). reply snovv_crash 13 hours agoparentprevThe weights aren't created by humans so they aren't copyrightable - at least in theory. reply shiandow 13 hours agorootparentThat does raise the question how irreversibly you have to process something before it is no longer protected by copyright. Obviously zipping something is not enough, even lossy compression is (obviously) not enough. But then how does a language model differ from a lossy compression? Is it just the compression ratio? (are the weights even that much smaller than the data?) There are ways to train models that guarantee that the amount of information transferred per data point is limited, but to my knowledge those aren't used (and may be prohibitively expensive). reply monocasa 13 hours agorootparentprevThat has nothing to do with whether it's open source or not. Software wasn't for sure copyrightable before 1976 in the US, but there was plenty of closed source code that simply only ever distributed the binaries. reply epistasis 13 hours agorootparentIt's open source in the sense that you can take the existing weights, apply any one of dozens of techniques for adapting the model to your data, and anybody you give your model to can do the same. This right isn't protected by copyright, it's protected by the lack of copyright, and how executables are directly modifiable. So the AGPL won't be possible, but it's a lot like permissive licenses, without attribution. reply monocasa 13 hours agorootparentThat's like saying an executable that I never gave out the source to is \"open source\" because there's nothing stopping you from binary patching it. Also, the license to the model assumes copyright (Apache 2) and works by granting you permissions based on that copyright. reply epistasis 13 hours agorootparentIt's very very different from patching a binary, because these models don't work like a simple compilation of understandable source code into incomprehensible weights. The source training data is even less comprehensible and useful than its encapsulation into weights. A set of weights is a pre-trained local minima in the model space is a both executable and modifiable. It's usually far more useful than the source training data, because the work has been done. reply monocasa 12 hours agorootparent> It's very very different from patching a binary, because these models don't work like a simple compilation of understandable source code into incomprehensible weights. The source training data is even less comprehensible and useful than its encapsulation into weights. Tons of binary patching works that way. For instance from the gameshark days it was relatively common for a patch that worked for unknown reasons, but who's discovery was tooling assisted and simply displayed a desired effect (and commonly a lot of undesired effects that weren't clear). > A set of weights is a pre-trained local minima in the model space is a both executable and modifiable. None of that changes whether it's open source or not. I guarantee you that mistral has some code (and a lot o data) laying around that created this model. > It's usually far more useful than the source training data, because the work has been done. For certain operations maybe. I guarantee that Mistral wouldn't constrain themselves to fine-tuning if they had a major change to make to the model. reply epistasis 7 hours agorootparentNo, patching binaries is not really similar at all, because it's viewed as an inferior method to having the source code and modifying that. With these large models, the weights really are quite useful objects in themselves, the very starting points of future modifications, and importantly the desired points of modifications Practioners in the field call these models \"open source\" and write open source software to run them and use them to power open source software. reply monocasa 4 hours agorootparent> No, patching binaries is not really similar at all, because it's viewed as an inferior method to having the source code and modifying that. > With these large models, the weights really are quite useful objects in themselves, the very starting points of future modifications, and importantly the desired points of modifications For fine tuning. You go back to the training pipeline to make major changes. > Practioners in the field call these models \"open source\" Some do. In contrast to the accepted definitions of open source. > and write open source software to run them and use them to power open source software. There's plenty of open source code to run closed source binaries. reply sroussey 13 hours agorootparentprevIt‚Äôs very likely protected by EU copyright. reply wongarsu 12 hours agorootparentprevWhen I take a picture with my phone, those pixels weren't created by a human either. Yet I still own their copyright. At the core the question is how much artistic input there is in creating LLM models. Are the choice of the model architecture, hyperparameters and training data artistic choices comparable to those by a photographer setting up a shot? Or are they more comparable to technical work that's only protectable by trade secrets, patents and trademarks? reply int_19h 8 hours agorootparentTraining a model is more akin to setting a video camera somewhere and having it automatically record or not according to some algorithm you devise. The code of the algorithm would be copyrightable, of course, but the recording is another matter, since the \"creativity\" aspect is kinda missing there. reply Q6T46nT668w6i3m 12 hours agorootparentprevThe parent is asking about weights and from that perspective, _stochastic approximation_ is essential. reply Sayrus 13 hours agorootparentprevCopyright is not the only kind of Intellectual Property. While they may not be copyrightable (I think you may be refering to jugement related to having an AI as author?), they clearly are MistralAI IP. In the same way, data in a database are usually not copyrightable. They are still the company's property (excluding issues with PII and such). reply greiskul 13 hours agorootparentI'm not so sure. If it is not copyrightable, and it is a patent, and not a trademark, which legal protection would it have? It could definitely be considered a trade secret, and the theft of it and misapproation of it would be a crime, but once it has been published, people that just used the published version I don't believe would be comitting a crime, since it would lose it's trade secret status. reply Sharlin 13 hours agorootparentThere are certain \"related rights\" [1] that are weaker than full copyright. These include, maybe most importantly, performers' rights, but also protection for things like photographs (ones that aren't unique enough to qualify for full copyright) and, in many jurisdictions, databases. These aren't covered by the Berne convention so vary quite a bit. But whether a huge chunk of floating-point numbers qualifies for any legal protection, remains to be seen. [1] https://en.wikipedia.org/wiki/Related_rights reply sroussey 12 hours agorootparentprevIn Europe you can copyright a database of facts, which is not something you can do in the US. Also, people go to jail for leaking trade secrets, and this certainly qualifies. reply bhickey 13 hours agorootparentprevDatabase rights don't exist in the United States. Their weights obviously aren't protected by trademark. So, what IP regime protects the weights? reply Sayrus 12 hours agorootparentMistral AI is incorporated in Paris, France. IANAL, but I think these[1][2] may qualify to protect these weights. While there is a discussion to be had on sovereignty and international reaches of local laws (such as DMCA for instance), I think it's disingenuous to consider only US legal point of view. [1] https://eur-lex.europa.eu/legal-content/EN/TXT/HTML/?uri=CEL... [2] https://www.legifrance.gouv.fr/loda/id/JORFTEXT000000573438/... reply colordrops 13 hours agoparentprevWhere do you get the idea that corps aren't touching LLama... reply int_19h 7 hours agorootparentIt's already on Azure, and has been for several months: https://techcommunity.microsoft.com/t5/ai-machine-learning-b... reply GaggiX 13 hours agoparentprevWell I remember Tencent using the leaked NovelAI model in \"Different Dimension Me\". reply seydor 14 hours agoprevHow does a world where GPTs are like the latest version of apache or mysql, look like? do we go back to the world of millions of web hosts (sorry, AI hosts) reply syntaxing 12 hours agoprevWhat a class act. Going to the leaked model on huggingface, not demanding it to be taken down, and just making a post on the page saying ‚ÄúMight consider attribution‚Äù is so damn amazing. reply sharkjacobs 12 hours agoprevMistral reminds me of the good old days of pre-2015 when I thought that tech companies were cool. reply bsaul 14 hours agoprevGPT-4 has been out for almost a year now, and it seems that the frantic pace of open-ai releasing new groundbreaking tech every month has come to a halt. Anyone knows what's happening with open AI ? has the recent turmoil with sama caused lag in the company ? or are they working on some superweapon ? reply brucethemoose2 14 hours agoparentIt's not, its just too much noise to see. We are getting a lot of great models out of China in particular (Yi, Qwen, InternLM, ChatGLM), and some good continuations like Solar. Lots of amazing papers on architectures and long context are coming out. Backends are going crazy. Outlines is shoving constrained generation everywhere, and Lorax is a LLM revelation as far as I'm concerned. But you won't hear about any of this on Twitter/HN. Pretty much the only thing people tweet about is vllm/llama.cpp and llama/mistral, but there's a lot more out there than that. reply alchemist1e9 13 hours agorootparentLoRAX [0] does sound super helpful and so I‚Äôd be curious if there are some good examples of people applying it. What are some current working deployments where one has 100s or 1000s of LoRA fine tuned models? I guess I can make up stuff that makes sense, so that‚Äôs not really what I‚Äôm asking, I‚Äôm interested in learning about any known deployments and example setups. [0] https://github.com/predibase/lorax reply brucethemoose2 13 hours agorootparentThere aren't really any I know of, because its brand new and everyone just uses vllm :P No one knows about it! Which is ridiculous because batched requests with loras is mind blowing! Just like many other awesome backends like InternLM's backend, LiteLLM, Outline's VLLM fork, Aphroidte, exllamav2 batching servers and and such. Heck, a lot of trainers don't even publish the loras they merge into base models. Personally we are waiting on the integration with constrained grammar before swapping to Lorax. Then I am going to add exl2 quantization support myself... I hope. reply semmulder 12 hours agorootparentFYI, vLLM also just added experimental multi-lora support: https://github.com/vllm-project/vllm/releases/tag/v0.3.0 Also check out the new prefix caching, I see huge potential for batch processing purposes there! reply brucethemoose2 10 hours agorootparentMissed this, thanks. Everything is moving so fast! reply ipaddr 13 hours agorootparentprevSubmissions on hn are welcome. reply brucethemoose2 13 hours agorootparentI have submitted some in the past. Others are submitting them! And I upvote every one I like in /new. But HNers don't really seem interested unless its llama.cpp or Mistral, and I don't want to spam. I can't say I blame them either, there is a lot of insane crypto-like fraud in the LLM/GenAI space. I watch the space like a hawk... and I couldn't even tell you how to filter it, it's a combination of self-training from experience and just downloading and testing stuff myself. reply tmaly 13 hours agorootparentI see a ton of papers on X related to this space. reply rightbyte 12 hours agorootparentprevI did try make a submission of what I thought was a \"underreported\" LLM two months ago. https://news.ycombinator.com/item?id=38505986 Zero interest for some reason. Edit: Deepseek coder has 4 submissions to HN with almost zero interest. reply cyanydeez 13 hours agorootparentprevsubmissionsBut the company‚Äôs CEO, Sam Altman, says further progress will not come from making models bigger. ‚ÄúI think we're at the end of the era where it's going to be these, like, giant, giant models,‚Äù he told an audience at an event held at MIT late last week. ‚ÄúWe'll make them better in other ways.‚Äù https://www.wired.com/story/openai-ceo-sam-altman-the-age-of... reply stavros 10 hours agorootparentI don't think that's what that means. I think he means \"we don't have to keep making models larger, because we've found ways to make very good models that are also small\". reply moffkalast 13 hours agorootparentprevGiven how they've struggled to even make even GPT 4 economical it's highly unlikely that larger models would be in any way cost effective for wide use. And there's certainly more to be found in training longer on better datasets and adjusting the architecture. reply int_19h 7 hours agorootparentIt's important to remember that \"not cost-effective\" is not the same as \"not useful\", though. reply moffkalast 48 minutes agorootparentDepends on how you look at it I guess. If it's really too expensive and overly slow to run for end users but is able to say, generate perfect synthetic datasets over time which can then be used to train smaller and faster models which pay back for that original cost then it is cost effective, just in a different application. reply crotchfire 13 hours agoparentprevAnyone knows what's happening with open AI ? Yeah, they're an arms dealer now. reply moffkalast 13 hours agorootparentLoRA of war. reply colordrops 13 hours agorootparentprevI sometimes wonder if they hit a limit the government was comfortable with and their more advanced technologies are only available to the gov. I assume that's your implication. reply jsnell 13 hours agoparentprevMaybe there are diminishing returns on improving quality, so they're trying to improve the efficiency at a given quality level instead? There is a lot value to producing results instantly, but more importantly efficiency will allow you to serve more users (everyone is bottlenecked on inference capacity) and gain more users thanks to winning on pricing. reply rudasn 12 hours agorootparentYup, I think this is it. Economies of scale haven't kicked in yet, and if they continue in the same path it doesn't look achievable. Think differently is again the way forward. reply artninja1988 14 hours agoparentprevProbably getting bogged down by endless safety testing and paperwork by now reply harmmonica 13 hours agorootparentMaybe tangential to this comment, but I had been using 3.5 to write contracts, but a couple of weeks back, even when using the exact same prompts that would've worked in the past, 3.5 started saying, to paraphrase, \"for any contracts you need to get a lawyer.\" Anyone else had this experience? It seems like they're actually locking down some very helpful use cases, which maybe falls into the \"safety\" category or, more cynically, in the \"we don't want to be sued\" category. reply transcriptase 13 hours agorootparentAs they tack on more and more guardrails it becomes lazier and less helpful. One way it gets around admitting that it has instructed not to help you is by directing you to consult a human expert. I also suspect there‚Äôs some dynamic laziness parameter that‚Äôs used to counteract increased load on their servers. You can ask the same prompt over and over in a new chat throughout the day and suddenly instead of writing the code you asked for or completing a task, it will do a small part of the work with ‚Äúadd the code to do xyz here‚Äù or explain the steps required to complete a task instead of completing it. It happens with v4 as well. reply TeMPOraL 11 hours agorootparent> some dynamic laziness parameter that‚Äôs used to counteract increased load That's a brilliant risk mitigation mechanism! The AI won't recursively self-improve to superhuman levels if it just keeps getting tired of thinking. reply HeatrayEnjoyer 13 hours agorootparentprevUsing it for legal matters is (understandably) explicitly against use policy. It's not meant to be used for legal advice so the response you're receiving is accurate - go get a lawyer. https://openai.com/policies/usage-policies reply TeMPOraL 11 hours agorootparentIt's a toy, it's not meant to be used for anything actually useful - they'll keep nerfing it case by case, because letting users do useful stuff with their models is either leaving money on the table, taking an increased PR/legal risk, or both. reply Baeocystin 13 hours agorootparentprevAdd 'reducing compute use as much as possible' to the list, I'm sure. I can look at some of my saved conversations from months ago and feel wistful for what I used to be able to do. The Nerfening has not been subtle. reply harmmonica 11 hours agorootparentHa, yeah, that's the right description of my feeling. Like, thanks for being so incredibly useful, ChatGPT, saving me hours of copying and pasting bits and pieces from countless Google searches for \"sample warranty deed,\" only to pull the rug out when I wanted to start depending on your \"greatness.\" I guess, as one of the other replies here said, it was never \"allowed\" to give you text for a contract according to the TOS, but then it would've been best if it never replied so effectively to my earlier prompts. Taking it away just seems lame. Edit: bad typing reply pstorm 14 hours agoparentprevA few theories: - They have better just stuff, but aren't release it yet. They are at the top already, it makes sense they would hold their cards until someone got close. - They are more focused on AGI, and not letting themselves get side tracked with the LLM race - LLMs have peaked and they don't want to release only a minor improvement. reply JumpCrisscross 13 hours agorootparent> have better just stuff, but aren't release it yet This hypothesis has a curious habit of surfacing when OpenAI is fundraising. Together with the world-ending potential of their complete-the-sentence kit. reply declaredapple 13 hours agorootparentprev> - They are more focused on AGI, and not letting themselves get side tracked with the LLM race FWIW OpenAI seems to have a corroded definition for AGI that is essentially \"[An] AI system generally smarter than humans\". They don't seem to use the typical definition I'm used to of some variation of autonomy or (pseudo)-sentience. So their LLM race is the race for AGI reply infecto 14 hours agoparentprevHas it come to a halt? I guess looking only at the perspective of no gpt-5...then yes? But I see wide access to multi-modal. Huge increases to throughput, not much latency these days. Better turbo models though I would agree in some areas those have been steps back with poorer output quality. Adding to that list, massive cost reductions so its easy to justify any of the models being used. reply sp332 14 hours agoparentprevOpenAI has a huge amount of people working on training data. https://time.com/6247678/openai-chatgpt-kenya-workers/ https://www.nbcnews.com/tech/innovation/openai-chatgpt-ai-jo... reply shmatt 13 hours agoparentprevProbably an unpopular opinion: A probabilistic word generator is still a word generator. It might be slightly better next version, we've seen it get worse, its more of the same We can talk about AGI all we want, but it wont be built on the same technology as these word generators. There will have to be a technological breakthrough years before we even get close The companies focusing on LLMs right now are dealing with * Generate better words * Make it cheaper to generate (use less compute) * Find better training material There is a ton of money to be made, but its still more of the same reply vanviegen 12 hours agorootparentAnd humans are just human generators. In both cases, intelligence appears to be just an interesting emergent side effect. reply discreteevent 12 hours agorootparent1996 Deep Blue beats Gary Kasparov: \"And humans are just chess engines. In both cases intelligence appears to be just an interesting side effect\" reply johnfn 13 hours agoparentprevI know, it's really disappointing. They've only completely changed the world like 3 times last year. reply msp26 13 hours agoparentprevAlignment tax is real. But they probably have something more powerful internally. GPT-4 took months to be available to the public. reply TheCaptain4815 13 hours agoparentprevGPT4 came out 3 years after GPT3 reply declaredapple 13 hours agorootparentSlight knitpick GPT 3.5 came out 2 years after GPT 3 GPT 4 came out 1 year after 3.5 reply int_19h 7 hours agorootparentAs we now know, GPT-3.5 was essentially an early preview of the \"move fast, break things\" variety - GPT-4 was already well underway by then. reply beAbU 13 hours agorootparentprevSlight nitpick, but \"nitpick\" is not spelled with a \"k\". reply kjreact 12 hours agorootparentActually nitpick IS spelled with a ‚Äúk‚Äù, just not one at the beginning of the word. If we‚Äôre gonna be pedantic details matter! reply mring33621 13 hours agorootparentprevknitpic reply hdhshxhsvc 14 hours agoparentprevCheck again in two months reply YetAnotherNick 13 hours agoparentprevGPT 4 is so far ahead of everything else that it doesn't make much sense to rush for GPT 5 release. They could get extra GPT 5 customers when they release it as they are pretty sure no one else would take away those users. reply nickthegreek 13 hours agorootparentWith a higher operating cost as well. reply m3kw9 14 hours agoparentprevLikely the way AI is trained and inferenced there is a huge constraint on the GPU side even if they have GPT5 out. Imagine how slow it is, which means they focus on creating useful products and APIs around it first reply yieldcrv 13 hours agoparentprevthem and meta have bought like all of Nvidia‚Äôs production capacity to run and train their next models reply ilaksh 13 hours agoparentprevI'm sorry but this comment seems to be incredibly uninformed. OpenAI just released a new version of gpt-4 turbo quite recently. And just in the last couple of days they released the ability to use @ to bring any GPT into a conversation on the fly. reply rgbrgb 14 hours agoparentprevI will speculate! They have a model that far surpasses GPT-4 (achieved agi internally) but sama is back on a handshake agreement that they will only reveal models that are slightly ahead of openly available LLMs. Their reasoning being that releasing a proprietary model endpoint slowly leaks the models advantage as competitors use it to generate training data. reply londons_explore 13 hours agorootparent> competitors use it to generate training data. I wouldn't think this matters as long as you charge enough to use that API. For example, you could have a tiered pricing structure where the first 100k words per month generated costs $0.001 per word, but after that it costs $0.01 per word. reply int_19h 7 hours agorootparentEven then it's still a lucrative proposition, since you only need to generate the dataset once, and then it can be reused. This kind of pricing would also make it much less compelling for other users. 100k tokens is nothing when you're doing summarization of large docs, for example. reply anon291 13 hours agorootparentprevWTF is AGI? EDIT: Clearly the point is lost on the repliers. There is no general understanding of what 'general intelligence' is. By many metrics, ChatGPT already has it. It can answer basic questions about general topics. What more needs to be done? Refinement, sure, but transformer-based models have all qualifications to be 'general intelligence' at this point. The responses are more coherent than many people I've spoken with. reply wtetzner 13 hours agorootparentArtificial General Intelligence https://en.m.wikipedia.org/wiki/Artificial_general_intellige... reply anon291 13 hours agorootparentThe problem with this definition 'an agent that can do tasks animals or humans can perform' is that it's not clear what that would look like. If you produce a system that can be interacted with via text input only but is otherwise capable of doing everything a human can do in terms of information processing, is that AGI? Or does AGI imply a human-like embodied form? Why? reply Xirgil 13 hours agorootparentprevArtificial General Intelligence. OpenAI defines it as \"highly autonomous systems that outperform humans at most economically valuable work\" reply anon291 13 hours agorootparentOkay, well, I'll say that at least that's a definition (it has to be good enough at something to make money). Arguably of course, it already does that. Me personally, I've used it to automate tasks I would have previously shelled out to fiverr, upwork, and mechanical turk. I've had great success using it to summarize municipal codes from very very lengthy documents, down to concise explanations of relevant pieces of information. Since I would have previously paid people to do that (was running an information service for a friend), I would consider that AGI. I guess the catch here is now 'most', but that implies a lot of knowledge about the economy I don't think openai has. What is 'economically valuable'? Who decides? At the end of the day, as with most things, AGI is a meaningless term because no one knows what that is. reply timeon 13 hours agorootparentprevAny curve-fitting is now AI, so they had to come with new term. reply anon291 13 hours agorootparentWhen you take a university / school course, how is that functionally different from curve fitting? Given that arbitrarily complex states can be modeled as high-dimensional curves, all learning is clearly curve fitting, whether in humans, machines, or even at the abiological level (for example, self optimizing processes like natural selection). Even quantum phenomema are -- at the end of the day -- curve fitting via gradient descent (hopefully it's had enough time to settle at a global minima!) reply ebb_earl_co 13 hours agorootparentprevArtificial General Intelligence reply anon291 13 hours agorootparentA meaningless term. reply bee_rider 13 hours agoprevSince these models are trained by just scraping the internet, ‚Äúscraping‚Äù this thing and including it in your own model seems like fair game, right? reply zamadatix 12 hours agoparentIf you mean training your model on it I'd say yeah, fair game. Mistral is Apache 2.0 licensed though so the question is a bit moot, they'd like you to use it for your own model. reply speedgoose 12 hours agoparentprevIt sounds fair. Just make sure to not involve lawyers. reply summarity 14 hours agoprevResults look comparable to Medium indeed (I‚Äôm using it via Mistrals API, since I got sick of OpenAI switching their stuff up). Medium is pretty great, somewhere between 3.5-turbo and 4-turbo qualitatively. Would be awesome to have out there. reply brucethemoose2 14 hours agoprevIts smart. Definitely keeping it around as my slow/low context local llm. Seems like a great candiate to merge with other 70Bs as well. There aren't a lot of really great 70B training continuations, like CodeLlama or sequelbox's continuations. reply MallocVoidstar 14 hours agoprevCheck out the PR he submitted: https://huggingface.co/miqudev/miqu-1-70b/discussions/10/fil... reply nohat 13 hours agoprevIt has been interesting seeing the sleuthing on this one. IMHO it is unfortunate to have this happen to a company that has been very pro open source. reply dopa42365 13 hours agoprevIs this an advertisement for a \"promise\"? Entirely worthless until there's something to show. It smells. reply janalsncm 14 hours agoprev> Quantization in ML refers to a technique used to make it possible to run certain AI models on less powerful computers and chips by replacing specific long numeric sequences in a model‚Äôs architecture with shorter ones. It‚Äôs always amusing when the press tries to explain technical concepts. Quantization just means substituting high precision numeric types with lower precision ones. And not specific ‚Äúnumeric sequences‚Äù, all numbers. reply SilverBirch 13 hours agoparentIt's funny how the original phrase sounds like it was generated by chatgpt. I'd guess the reason they didn't phrase it how you suggest is they probably thought that people who don't know what quantization means aren't going to be happy if you throw precision numerics at them. I don't knwo why they didn't just say \"some of the numbers are rounded\" though. reply 2OEH8eoCRo0 12 hours agoprevSeems paradoxical. How does something that is open source leak? reply vulcan01 11 hours agoparentAccording to the article, it will be open source, but it's not open source now. reply rrr_oh_man 14 hours agoprevCynical me smells a PR move reply seydor 14 hours agoparentAll their PR moves seem to be like this one. Not complaining reply chrishare 13 hours agoprev [‚Äì] I hate when that happens reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The open source AI community is abuzz with the discovery of a leaked open source large language model (LLM) named \"miqu-1-70b\" from Mistral, a Parisian AI company.",
      "The leaked model has gained attention for its impressive performance on LLM tasks and has the potential to rival or surpass OpenAI's GPT-4.",
      "If the leaked model proves successful, it could pose significant competition to OpenAI's subscription tiers, although OpenAI still holds an advantage with its GPT-4 Turbo and GPT-4V."
    ],
    "commentSummary": [
      "The CEO of Mistral, an AI company, has acknowledged a leak of their new open-source AI model that competes with GPT-4.",
      "Users are discussing various quantization formats for testing and production purposes, as well as the ollama/llama.cpp program, Mistral's performance compared to OpenAI, and the use of tokens for context and output.",
      "The conversation also delves into the limitations of large context sizes in language models, the cost and capabilities of GPT-4 training, Mistral's market positioning, concerns about bias and evaluation in AI models, Mixtral's performance, watermarking language models, and the definition and misuse of \"open-source.\""
    ],
    "points": 343,
    "commentCount": 231,
    "retryCount": 0,
    "time": 1706729534
  },
  {
    "id": 39204918,
    "title": "The Far Side Comics: Daily updated classic gems of humor",
    "originLink": "https://www.thefarside.com/",
    "originBody": "The Far Side¬Æ The Daily Dose Selections of classic The Far Side comics, updated daily Thursday, February 1, 2024 ‚ÄúBe patient, Leona, be patient. ‚Ä¶ Zebras won‚Äôt take a drink until they know it‚Äôs absolutely safe.‚Äù Share This Post Awkward moments in the ant world Share This Post Share This Post ‚ÄúThat‚Äôs not funny, Malcolm! There will be no more floating belly-up on the surface!‚Äù Share This Post ‚ÄúI‚Äôve got an idea. ‚Ä¶ How many here have ever seen Alfred Hitchcock‚Äôs The Birds?‚Äù Share This Post Previous Day Comic Collections Comic Collections Gary Larson Breaks the 4th Wall In which we recognize the existence of other cartoons‚Äîand cartoonists. Comic Collections Don't Ask Cartoons we simply can‚Äôt explain or categorize. Good luck!",
    "commentLink": "https://news.ycombinator.com/item?id=39204918",
    "commentBody": "The Far Side ‚Äì By Gary Larson (thefarside.com)339 points by keepamovin 18 hours agohidepastfavorite127 comments polygotdomain 11 hours agoGrowing up my best friend and I loved The Far Side. In many ways it was the basis to the quirky humor we both shared, and would continue to explore throughout our youth. In elementary school we had countless Far Side books between us, and by middle school a few shirts too (I remember having \"Midvale School for the Gifted\"). Well he turned 40 just a few weeks ago, and I needed to find a present. I don't know why, as The Far Side hadn't crossed my conscious in quite some time, but I thought \"I'll get him 'The Chickens are Restless'\". After some quick searching, he ended up with the Complete Works containing every thing from The Far Side. Wrapped, he didn't have a clue what I'd got him or why it was so damn heavy. The first glance as he opened in and realized what it was instantly brought a smile to his face. It's amazing how small things bring people together, and The Far Side will always be one of them for my best friend and I. I'm sure there will be some new comics we'd missed and there will be some new chuckles. reply 7thaccount 7 hours agoparentMy mother used to drag me to the Hallmark store as a kid and she'd spend unreasonable amounts of time chatting with the workers and browsing who knows what. I think this was before I had a Gameboy and my only salvation was reading those Farside books. I both enjoyed them and was seriously creeped out by them. They also had Calvin and Hobbes which I adored. reply justworkout 3 hours agorootparentThe creepiness is something nobody ever seems to mention about The Far Side and that's my biggest lasting impression of the comics. They're deeply unsettling and have a weird uncanny valley feeling to me. That's not to say they're bad. But something about them always has a tinge of horror to me. reply aksss 3 hours agorootparentWell a lot of it is quite dark. One of my favorites being the jet pilots asking each other why there‚Äôs a mountain goat in the clouds. reply vmurthy 1 hour agorootparentThis reminds me of the sub Reddit /r/twosentencehorror. . reply Wistar 2 hours agorootparentprevMine, too. reply kasey_junk 6 hours agoparentprevThe day my kid got into the Far Side was a pretty good day. Haven‚Äôt introduced him to the Pharcyde yet but soon. reply lioeters 8 hours agoparentprevThe Far Side was part of my growing up too, my friends and I shared much laughter and silliness. Those comics influenced our sense of humor and worldview. reply w4ffl35 9 hours agoparentprevnext [10 more] [flagged] PsylentKnight 9 hours agorootparentThis place is turning into reddit. I knew the first reply to the first comment would be some rude, cynical curmudgeon. reply w4ffl35 8 hours agorootparentTurning? reply w4ffl35 9 hours agorootparentprevnext [8 more] [flagged] gerdesj 9 hours agorootparentOK, I've downvoted you. What is wrong with a reminiscence session about a subject posted here? When you click on the OP you just get a few disjointed Gary Larsen \"out takes\" and I guess you have to get to grips with it blah etc. I am a massive fan but that does not get you off the hook. @polygodomain gave us something new and tangible. They gave something of themselves and I find that engaging and rather charming. That's why I still hang out on HN, despite it being a bit shouty. reply w4ffl35 8 hours agorootparentIt was predictable, just like the reddit comments section. reply w4ffl35 4 hours agorootparentWhy the downvotes on the above comment? I was asked and answered, I'm totally within the hackernews guidelines, down voting my above comment is not within the guidelines. Downvoting out of spite like that is more like what happens on reddit. reply sonofhans 9 hours agorootparentprevHere‚Äôs a dime, kid, go download yerself a better meme. That one burned itself out 15 years ago ‚Äî https://news.ycombinator.com/item?id=926703 reply I_am_uncreative 9 hours agorootparentprevFlags are not downvotes. reply w4ffl35 4 hours agorootparentVery good! reply tomcam 9 hours agorootparentprevChallenge accepted reply nullhole 13 hours agoprevWorth noting is Gary Larson's original response to the WWW was an open letter asking fans not to post his cartoons online. Akin to the Bill Gates letter on copying software, but, perhaps, more understandable. This website is a reversal of that original position. He was thoughtful then, and is thoughtful now. The original letter: https://www.portmann.com/farside/index.html?home.html And the reversal: https://www.thefarside.com/about/48/a-letter-from-gary-larso... ... for cartoons, my favourite has always been: [A cow, wearing jewellery and holding a martini glass, stands by the picture window of a well-appointed suburban home, while a bull sits in an armchair, watching TV, with a beer in his hoof] Cow: \"Wendell... I'm not content\" reply eichin 10 hours agoparentThe \"wings stay on/wings fall off\" switch is the one I keep having to dig up at work (alternating with \"Why do we even have that lever?\" from emperor's new groove.) And then there's \"Cow Tools\"... reply arwhatever 3 hours agorootparent‚ÄúNow That Should Clear Up A Few Things Around Here‚Äù Just has to be the most applicable to software development, assuming that‚Äôs what most folks around here are into. reply crabmusket 4 hours agorootparentprevAll three of these are also frequently on my mind. reply danparsonson 4 hours agorootparentprevI think about the former almost every time I fly XD reply Arrath 11 hours agoparentprevI've always been particularly fond of the humble farmer dooming the planet to destruction by earnestly grasping the hand-shaped alien's entire head for an introductory handshake. https://twitter.com/faineg/status/1387409269620985863?lang=e... reply jacquesm 13 hours agoparentprevFor me it's also a cow one: 'Car!'. reply Wistar 2 hours agorootparentSomeone close to me has the original of the ‚ÄúCar‚Äù cartoon. Gary gave it to him as a gift shortly after it was published. It is pen and ink and Letratone on paperboard and is about 18‚Äù wide. For a year or two in the late 90s, the artwork went missing and was thought to have been stolen, but was eventually found to have been put in storage. reply jacquesm 2 hours agorootparentWow, that is incredible. First that it wouldn't have pride of place somewhere (it's as if the gift wasn't properly appreciated), second that it would end up in storage and then that it was found again. If they need a safe place to store it give them my email ;) reply flkiwi 11 hours agoparentprevInuit leaving an igloo: \"Well, it's cold again.\" Kills me every time. reply defaultcompany 9 hours agorootparentHaha this reminded me of the two guys in hell and the one whispers to the other ‚ÄúI hate this place‚Äù. reply cgriswald 3 hours agoparentprev[Men in pith helmets and khaki shorts trekking hip deep through water with their gear held over their heads.] Piranha: Just nibble at first. ‚Ä¶ But when you hear them yell ‚ÄúPiranha!‚Äù - go for it. reply mccolin 12 hours agoparentprevI‚Äôve always loved ‚ÄúBeware of Doug‚Äù reply pjmorris 6 hours agorootparentWe had two Dougs in our 1980's office. One of them had the cartoon on the entrance to his cube. reply pjmorris 6 hours agoparentprevAbout ten years ago I went back and forth a couple of email rounds with one of his aides trying to get permission to use one of his cartoons ('Old dog, new trick') in a presentation to ~200 people, but I couldn't justify the expense. I was bummed, but respect Larson's wishes too much to go freelance and just show the cartoon. reply krsna 8 hours agoparentprevSo many childhood memories reading Farside. Here‚Äôs my favourite: https://static1.cbrimages.com/wordpress/wp-content/uploads/2... reply chris_wot 12 hours agoparentprevMy favourite was only published in his hardcover book. Newspapers universally rejected it: http://doubleplusundead.files.wordpress.com/2012/03/far-side... reply rilindo 11 hours agorootparentI feel wicked laughing at this. reply chris_wot 10 hours agorootparentAs did we all. reply acheron 10 hours agoparentprevEmbedded in styrofoam shoes, Carl is sent to ‚Äòsleep with the humans‚Äô. reply knodi123 9 hours agoparentprevalthough he still disables right-clicking on his site. So he's still putting some effort into the sisyphean \"uphill slog\" reply zem 11 hours agoparentprevmy favourite was the horse parliament one. someone posted it to reddit: https://www.reddit.com/r/europe/comments/b6g2ti/i_love_it_wh... reply fedreg 9 hours agorootparentSo good! Thanks for sharing reply hans_carlsen 9 hours agoparentprevA farmer on the path home from the chicken coop with a basket of eggs meets a hen carrying a baby. reply themadturk 10 hours agoparentprev\"I'm not content\" was especially relevant if you remember the old Darigold Dairies commercial: \"Darigold...home of contented cows.\" reply mark-r 9 hours agorootparentContented cows were a very common dairy meme. reply jethro_tell 5 hours agorootparentStill are. reply aksss 3 hours agorootparentprevDriving through cow country it‚Äôs impossible not to think of all the far side comics involving them and ponder what‚Äôs really going on in their heads. XD reply entropicgravity 11 hours agoparentprevPilot to co-pilot, \"What's that goat doing up here in this cloud bank?\" reply dhosek 9 hours agoparentprevA group of cows huddled around another cow eating a hamburger. ‚ÄúI‚Äôd say we taste like chicken.‚Äù reply m463 11 hours agoparentprevI always recall turbulence. reply LordDragonfang 12 hours agoparentprevHonestly, it's a real shame that it took so long for that reversal, because Far Side comics already had an almost perfect vibe for joining the early rise of internet meme culture, with the postmodernist elements they had in common. Hopefully for Larson things haven't moved on too far that he's totally missed that boat, because his comics absolutely deserve wider recognition among internet culture. reply omoikane 12 hours agorootparentI think the expectation from the early days of the internet was that information could be contained, and the realization from recent years is that the expectation no longer holds. So creators either have to make their own contents accessible or someone else will do it anyways, possibly poorly. Gary Larson might have arrived at this realization relatively late, but I don't think he is the last one. reply psunavy03 10 hours agorootparentWitness all the artists who refused to put their music on iTunes or streaming. How'd that exclusive CD deal with Wal-Mart work out, Garth? reply Zircom 5 hours agorootparentGarth actually had his own streaming service GhostTunes, which was created solely to only host his own music for streaming until he realized what a dumb idea that was. reply b450 13 hours agoprevI have several well-worn Far Side collections, including one with a lovely introduction by Jane Goodall, which has a funny story behind it [1] Linking my favorite below [2] > Donning his new canine decoder, Professor Schwarzman becomes the first human being on Earth to hear what barking dogs are actually saying. [1] http://www.hopkinsandcompany.com/Books/The%20Complete%20Far%... [2] https://pbs.twimg.com/media/FNZar0hX0AQB98d?format=jpg&name=... reply Ringz 11 hours agoparentThis is also my favorite, which I have been able to laugh at for years. reply AlbertCory 11 hours agoparentprevthat's funny, because some comedian stole the idea, and had all the dogs saying \"Fuck you! Fuck you!\" I still always think of that when I hear dogs barking. reply noSyncCloud 12 hours agoparentprevWhat a great story. Love that reply TJSomething 13 hours agoprevI just found out that Larson started drawing comics again on a very irregular schedule: https://www.thefarside.com/new-stuff reply FredPret 10 hours agoparentThis is the best news ever. Gary Larson discovered Procreate! (Probably.) Hopefully he'll start churning out new stuff like crazy now. reply Loughla 11 hours agoparentprevThe nudibranch beach got me. reply NickC25 13 hours agoprevWow, The Far Side really brings back some cherished childhood memories. Larson is, IMO, one of the best to ever do it. His humor is really unique, and he's quite a thoughtful person as well, letting his work speak for itself. A relic of a bygone era. One of my faves: https://www.pinterest.com/pin/oh-my-god-its-leonard-hes-stuf... Gets me every time. I still have It came from the Far Side as well as Gallery and Gallery 2 on my bookshelf. Timeless. reply OldGuyInTheClub 13 hours agoprevIf I had to pick a favorite, it'd be all of them. I only wish I had a thagomizer for the routine problems of work and day-to-day living. reply psunavy03 13 hours agoparentDidn't \"thagomizer\" work its way into paleontologists' slang, since they really didn't have a word for it? reply gramie 13 hours agorootparentI think it worked its way into the official vocabulary! reply stonogo 12 hours agorootparentprevIt's definitely a thing: https://scholar.google.com/scholar?hl=en&q=thagomizer reply urbandw311er 10 hours agoparentprevhttps://en.m.wikipedia.org/wiki/Thagomizer reply dylan604 13 hours agoparentprevMine was School For the Gifted reply OldGuyInTheClub 12 hours agorootparentI was drinking coffee when I saw \"Early vegetarians returning from the kill\". But not for long. reply dylan604 12 hours agorootparentI have redecorated with my morning beverage on many occasions myself. reply geoelectric 12 hours agorootparentprevI think of that one every single time I try to use a door the wrong way. reply plowjockey 6 hours agorootparentprevMany years ago attending the first company technical seminar that one was taped to the door. Sometime in the late '80s the local daily began carrying the Far Side and the very first one had Popeye on the witness stand saying, \"I yam what I yam.\" reply pdonis 4 hours agorootparentprevMy parents gave me a mug with that on it for my birthday one year. I still have it. reply urbandw311er 10 hours agorootparentprevhttps://fifetli.blog/2019/01/15/midvale-school-for-the-gifte... reply chris_wot 9 hours agorootparentprevhttps://www.pinterest.com.au/pin/lord-we-thank-thee-the-far-... reply tpoindex 10 hours agoprevI love the Far Side, glad to have it online. The two volume set has a featured spot on my bookshelf. And fun to see other commenters here share some of my favorites Far Sides. What contemporary comics are people following? Two of my current daily views are: Bizarro - clever puns + such, single frame: https://www.arcamax.com/thefunnies/bizarro/ Dark Side of the Horse - low key sublime: https://www.gocomics.com/darksideofthehorse/ reply ahazred8ta 9 hours agoparentDoctor Fun from the 1990s was very farsidish but geekier. https://nerocam.com/DrFun/Dave/archive.html -- https://en.wikipedia.org/wiki/Doctor_Fun reply philipkglass 9 hours agoparentprevPerry Bible Fellowship: https://pbfcomics.com/ One of the best known strips, and a fine joke about historical fiction (or maybe just history itself): https://pbfcomics.com/comics/now-showing/ reply EvanAnderson 13 hours agoprevI've been reading the entire The Far Side (from the massive two book set) with my 10 y/o daughter. It's interesting to see how apparently the culture of the time of Gary Larson's upbringing imprinted itself on the strips. Reading 10 or so strips at a sitting makes it very apparent (along with making apparent Larson's go-to characters and tropes). My daughter commented on there being few depictions of female scientists, academics, doctors, etc (instances if the archetypal The Far Side character in a lab coat) as an example. The strips that feature singles bar-type settings feel a little skeevy, too. I'm not trying to say anything negative about the strips or Gary Larson. It's interesting to me to reflect on something I enjoyed in my childhood \"showing its age\", even as I enjoy it now. reply ghaff 12 hours agoparentPop culture (and, good as it is, The Far Side is pop culture) always reflects current society to some degree. I would actually say that The Far Side (and Calvin and Hobbes) probably less directly reflects when they were drawn. For a more obvious example, see Dilbert. Leaving aside all the other baggage, with few exceptions, it's really more about 1990s PacBell cubicle life than anything about the current era, certain common stereotypes notwithstanding. reply floren 10 hours agorootparentI think society in The Far Side is built more out of, like, Looney Tunes cartoons and B movies from the 50s. Everybody is a stereotype: scientists in lab coats and glasses, \"tough guys\" in white t-shirts and pompadours, ladies with beehive hairdos. His cities, his houses, his restaurants, they all default to \"stereotypically about 1955\". It's easy to draw and it's easy to parse as a reader. reply ghaff 9 hours agorootparentThat's probably fair. It's much less part and parcel of the 60s/70s than a number of other strips are. reply hermitcrab 10 hours agorootparentprevSo much 'other baggage'. Ugh. reply AlbertCory 13 hours agoprevBummer of a birthmark https://i.pinimg.com/originals/86/64/d6/8664d684518509081caf... reply xoxxala 13 hours agoparent\"Bummer of a birthmark, Hal\" has made its way into my everyday lexicon. Most of my coworkers immediately got the reference. There is something about Far Side humor that appeals to people in tech beyond all the science and nerd jokes. reply MiddleEndian 10 hours agorootparentI have that comic (text included) on a t shirt. Sometimes people ask me about it, and I will explain it in an excruciating amount of detail for my entertainment. Sometimes they will try to out-do me and respond by pretending to be confused by my explanation, but my tolerance for playing the fool is incredibly high so I can keep it up for quite some time and they inevitably give up first. reply bbarnett 4 hours agorootparentI picture an octogenarian, bored on a park bench, with endless time, inquiring as you pass and you outlasting them. reply rpmisms 6 hours agoprevReading Far Side as a child with a truly hyperactive imagination opened up worlds to me. I would see a panel and build up the world around it. This caveman has meat in a basket on his coffee table‚Äîdoes he also use fruit in place of meat? Is the butchers shop full of cold cuts of citrus? Maybe the florist simply sells twisted up intestines in bouquets? I bet they use meat-scented colognes and perfumes. Basically, Far Side gave me the tools to daydream. Thank you, Gary! reply dsquared2 10 hours agoprev‚ÄúYes‚Ä¶ I believe there‚Äôs a question there in the back‚Äù https://www.cardcow.com/images/set856/card00130_fr.jpg reply neilv 4 hours agoprevWhen I was an intern at a company that developed high-end software development tools, there was an intense project to build a new product suite... So I posted the Far Side comic, about the office workers trapped by fire, atop the building of the Ace Ladder Co. I can't find a non-copyright-violating copy on the Web, but the caption was: \"Wait a minute! Say that again, Doris! ... You know, the part about `If only we had some means of climbing down.'\" (I'm lucky that my coworkers were so kind about my youthful tone-deafness.) reply kevinventullo 5 hours agoprevMy father who was a professor in microbiology had this one on his office door: https://ircamera.as.arizona.edu/NatSci102/NatSci102/images/e... I still use this phrase today when faced with a flood of information. reply arbuge 11 hours agoprevEverybody has their Far Side favorite... here's mine: https://www.pinterest.com/pin/54184001740077889/ reply jancsika 13 hours agoprevIs there a name for a cartooning style where the creatures seem to always be drawn as wide as possible? I remember a few animators for Sesame Street who did the same thing. It was always striking. Larson seems to also give them tiny heads for maximum ridiculousness. :) It make me think of that Aubrey Beardsley illustration of Ali Baba where he's got the curved lines running off the page suggesting these gi-normous flowing comfy pants: https://images.fineartamerica.com/images/artworkimages/mediu... reply Modified3019 12 hours agoparentI'm not certain there's a name, but Larson was heavily influenced by the work of B. Kliban reply aidenn0 3 hours agorootparentWow, if you told me that this[1] was a Far Side, I'd believe you. 1: https://i.pinimg.com/736x/a8/8c/46/a88c46ca525d66d6444b55b3c... reply tomcam 8 hours agorootparentprevKliban was superior but also R to X-rated and transgressive af outside of his cat cartoons reply alibrarydweller 12 hours agoparentprevNot cartooning exactly, but there is Boterismo - https://en.wikipedia.org/wiki/Fernando_Botero reply pdonis 4 hours agoprevAs Thak worked frantically to start a fire, Cro-Magnon Man, walking erect, approached the table and simply gave Theena a light. reply akoboldfrying 3 hours agoparentEven the caveman names crack me up reply yboris 4 hours agoprevHave you heard of Strigiphilus garylarsoni, a species of chewing louse found only on owls? Named after Gary Larson! https://en.wikipedia.org/wiki/Strigiphilus_garylarsoni reply PlasmaOInterest 4 hours agoprevSo many favorites, but I really loved his dark humor in ones like this https://www.pinterest.com/pin/352406739585675000/ reply senderista 9 hours agoprevWhen I was in college and would go home for Christmas, my mom would buy me a new Far Side calendar for the coming year, and on my last day home my dad and I would invariably stay up late reading through it together. Great memories now that he's gone. reply ukyrgf 6 hours agoprevJust FYI, if people remember having the day-by-day calendars back when, the dates have now repeated and they're selling them again. reply boxed 3 hours agoprevI wonder why a page like this thinks it needs cookie tracking, and thus the cookie banner. There are no ads, so what's the point? reply sizzzzlerz 10 hours agoprevMy favorite is two bears, sitting a campfire, a hunter's clothing scatter around. One bear is telling the other \"I love it when they play dead'. reply hackerbrother 10 hours agoprevMy favorite: https://cdn.sanity.io/images/vgvol637/production/4b1d3f9eddb... reply the__alchemist 13 hours agoprevHey - I've been wondering about this for a while: Does anyone have an explanation of the Farside/AI crap that has been flooding Facebook for the past few months? Generally, the punchline is that animals are turning the tide on humans, and hunting them instead; husbands and wives hate each other; old men like creeping on young women while their wives gets mad. The comics linked seem different. reply evan_ 13 hours agoparentI'm sure it's just your regular, everyday bid for eyeballs and clicks. The linked comics are the \"real thing\", selections from a syndicated single-panel comic that ran in newspapers for years until the mid-90s. The gags tended to be pretty surreal. reply dylan604 13 hours agoparentprev> The comics linked seem different. Whaaat? Say it ain't true! The AI crap isn't as good as the original? No! That's impossible! reply telman17 6 hours agoprevLove The Far Side. ‚ÄúWent to market‚Äù with the missing toe had me in tears when I first saw it! reply codeulike 9 hours agoprevYakity Yak Yak Yak! https://www.pinterest.co.uk/pin/500532946070783132/ reply smoyer 7 hours agoprevTwo pilots sitting on the back of a very large infant with its arms outstretched - \"Let's get this baby off the ground\" reply chris_wot 5 hours agoparentTwo pilots in their cockpit and you can see a mountain goat in the cockpit windshield. It is captioned ‚ÄúWhat the hey?‚Äù reply hprotagonist 6 hours agoprevi was very surprised to learn some years ago that gary larson is also a prolific author of crossword puzzles. reply artie_effim 13 hours agoprevWe still refer to cat food as 'cat fud' in my house. reply codeulike 9 hours agoparentYes! I always write it like that on shopping lists now https://twitter.com/jamiesmart/status/1691482826472828928 reply thebruce87m 12 hours agoparentprev‚ÄúFud‚Äù means something very different in Scotland. It used to be a common derogatory term. https://en.m.wiktionary.org/wiki/fud reply m463 11 hours agorootparentIt was also used microsoft to great effect. https://en.wikipedia.org/wiki/Fear,_uncertainty,_and_doubt reply unethical_ban 10 hours agorootparentprevLooks like it is related to futz, how I have heard it \"don't futz around with that\" with kind of means don't like at it because you'll break it. reply codexb 11 hours agoprevI don't know why, but I was convinced that Gary Larson died like 15 years ago. reply bsimpson 10 hours agoparentApparently he retired when he was ~45. reply ghaff 9 hours agorootparentA number of our fondest cartoonists checked out relatively early. Doing this kind of work day-after-day must be pretty draining. (And is, based one a couple of political cartoonists I knew.) reply jonathankoren 11 hours agoprevI am so happy that Cow Tools has become a meme. reply Quekid5 3 hours agoprevWhy? Anyone less gifted. (It's very absurdist humor, but anyone into that should and MUST know about Larson, the person who did so much to damage the field by disappearing... willingly... etc. What a fly on the wall he wasn't, etc.) reply DonHopkins 13 hours agoprevNerd! ... Dang! https://i.pinimg.com/564x/c9/08/a0/c908a02a8dfa42db9973f743b... Dang! ... Who ate the middle out of the daddy longlegs? https://ifunny.co/picture/dang-who-ate-the-middle-out-of-the... reply OldGuyInTheClub 11 hours agoparentTwo Far Side mentions (at least) for our moderator? And here I envisioned him as a much younger man. reply smarks 8 hours agoprev\"Whoa! Stuart blew his air sac!\" reply Scubabear68 7 hours agoprev [‚Äì] ‚ÄúMidvale School for the Gifted‚Äù reply jaclaz 11 minutes agoparent [‚Äì] Everyone have their own favourite, for me it is: \"Well, I've got good gnus and I've got bad gnus.\" reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The Far Side comics, known for their humor, are updated daily with classic selections.",
      "The comics cover a range of funny situations, such as awkward moments in the ant world and a nod to Alfred Hitchcock's \"The Birds.\"",
      "In addition to the daily updates, the collection includes previous day's comics, comic collections, and references to other cartoonists and cartoons."
    ],
    "commentSummary": [
      "Participants in this discussion forum share their admiration for Gary Larson's comic strip \"The Far Side\" and engage in conversations about their favorite comics and the cultural significance of Larson's work.",
      "The discussion extends to various other topics, including Larson's recent return to drawing, his impact on paleontology, and his involvement in crossword puzzles.",
      "The connection between \"The Far Side\" and the tech industry is explored, along with the unique drawing style and humor present in Larson's characters."
    ],
    "points": 339,
    "commentCount": 127,
    "retryCount": 0,
    "time": 1706714826
  },
  {
    "id": 39206529,
    "title": "The Sega AI Computer: Unveiling the Rare and Influential System",
    "originLink": "https://www.smspower.org/SegaAI/Index",
    "originBody": "Sega AI Sega Master System / Mark III / Game Gear SG-1000 / SC-3000 / SF-7000 / OMV / AI Home - Forums - Games - Scans - Maps - Cheats - Credits Music - Videos - Development - Hacks - Translations - Homebrew SegaAI only Login Report an error Print Sega AI Computer („Çª„Ç¨Ôº°Ôº©„Ç≥„É≥„Éî„É•„Éº„Çø„Éº) +Contents Introduction System Specs Touch surface Audio Specs Software Storage Artificial Intelligence? We eventually found one... An usually long 1986-1989 life cycle? Who worked on the Sega AI Computer? Emulation Instructions to run MAME Controls Known emulation bugs as of January 2024 Software Titles What we found so far What we haven't found so far Downloads System Roms Software Roms & Overlay MAME ready Roms folder Tapes recordings Hardware Manuals Software Scans Flyers & Ads Photos Magazine Scans Technical documentations Links Our big shared folder External resources Site sections Forum threads Research credits Credits Special Thanks About SMS Power! Introduction Around late 1986, Sega released the ‚ÄúSega AI Computer‚Äù. This is one of Sega‚Äôs least well known and rarest systems. Not much is known about this system apart from a small amount of information in Japanese and American flyers and press articles. The information we have is still piecemeal and may be partly inaccurate. Today we are making public, for the first time: all system roms extracted from the Sega AI Computer, data dumps from 26 my-cards and 14 tapes, many scans and photographs, and in collaboration with MAME developers, an early working MAME driver allowing this computer to be emulated. The majority of these software titles had zero information about them on the internet prior to us publishing them: no screenshots, no photos or scans of actual software. Considering the elusive nature of this machine, it is possible that some games have never been seen or completed by anyone outside of their original development teams. We hope that this release will be interesting to obscure game and computer historians and hobbyists alike. We will further amend it over time by releasing extra scans, hopefully improving emulation and publishing/discovering new information. Extract from a US ad (1986) System Specs Year: 1986 CPU: 16-bit NEC V20 at 5 Mhz ROM: 512 KB total (as actually found in system) 2x64 KB system ROM (‚ÄúOperating System, Prolog‚Äù) 1x128 KB character ROM 2x128 KB speech ROM RAM: 128 KB Software on Sega My Cards (128 KB to 256 KB). Software on Audio Cassettes. Video : Yamaha V9938 (Resolution 256x212) with 64 KB VRAM. Sound: SN76489 PSG Inputs: Touch surface with overlays, 8-directions pad + 3 buttons. Inputs: Microphone input. Inputs/Outputs: a RS232 Centronics port Cassette Drive : 9600bps. Optional Keyboard peripheral. Optional Sound Box peripheral with a YM2151 FM chip. An early flyer suggest the possible planning or existence of: a BASIC programming ROM Card, a Disk Drive extension, a Bridge Unit featuring an 8-bit CPU, joystick peripherals and a microphone input. Extract from a 1986 Japanese pamphlet Quick comparisons with other pieces of hardware: The Sega SC-3000 (1983) used a Z80 8-bit CPU, 2 KB of RAM, 16 KB of RAM and no system ROM. The Sega Master System (1985+) increased this to 8 KB of RAM and included a better video chip. The video portion of Sega AI Computer satisfies MSX2 specs. Touch surface The system has a large touch surface. Most software titles are shipped with an overlay, providing a custom touch interface for each title. Some titles use the touch surface without an overlay to provide drawing capabilities. Audio Specs The system is capable of playing PSG audio, and FM audio when equipped with the Sound Box extension. It is also equipped with a ‚Äúspeech synthesiser‚Äù in the form of a chip capable of decoding ADPCM data. One of the 128 KB ROM contains speech samples for the 46 common sounds of the Japanese language, the other 128 KB ROM contains entire sentences which are used by the system. In addition, the system is also able to seek and play audio from the cassette drive. So up to 4 types of audio can be mixed together: ‚ÄúI tried out a sampling of the software with the Sound Box, and many of them produced FM music instead of simple PSG without the box. All FM is mixed in with everything else, including the PSG, which is still used for rhythm hits; the speech chip, which is used in \"Popo's Adventure\" (\"take a big hamburger\" it says, eg.); and the sound from the cassette, which is mostly narration and hints.‚Äù. Attach:MPR-7619.mp3 Attach:MPR-7620.mp3 Decoded ADPCM data from MPR-7619 ROM Decoded ADPCM data from MPR-7620 ROM Software Storage The system can boot software from small-sized cards similar to Master System and PC Engine cards. Unlike Master System-era cards, which could carry a maximum of 32 KB of data, the Sega AI Computer cards can carry 128 KB and 256 KB of data. The 128 KB cards have their whole memory space mapped through card pinouts (17 pins). The 256 KB cards have a simple Master System-style bank-switching mapper embedded. The system can also boot software from its cassette drive. Sega AI Computer cassettes are stereo, with one channel dedicated to storing encoded data, and the other to storing regular audio. This allows certain loading operations to be playing background music during the load, including snippets from Vivaldi‚Äôs Four Seasons. Artificial Intelligence? Documents describe it as a full-featured computer with an educational twist. The system itself sports markings with a promising ‚ÄúSEGA PROLOG‚Ä¶. Bringing you into the world of artificial intelligence‚Äù. Effectively all the software we found so far is educational and mostly aimed at kids. The system hardware was definitely not used to its maximum potential, although later 1988-1989 software titles are of higher quality than earlier ones. A US prototype suggests that some form of LISP exists, but we weren‚Äôt able to get access to it yet. The Prolog interpreter appears to be used by existing software to allow some form of natural language processing. It doesn't seem accessible to the end-user. One of the most important English sources is a July 24, 1986 article from ‚ÄúElectronics‚Äù magazine, which we are reproducing here in its entirety: ‚ÄúCHILDREN'S COMPUTER BRINGS AI INTO THE HOME TOKYO Recent advances in logic-programming languages and processor and peripheral chips are making artificial-intelligence applications practical in the low-end home computer market. The first product that will seek to prove this point is the AI Computer, which Sega Enterprises Ltd. of Tokyo will start selling in Japan next month for $547. The Sega AI Computer is built to run programs written in the Prolog AI language. It is strongly oriented toward computer-aided-instruction, especially for children, rather than the catchall hobbyist-to-professional target of other home computers. Perhaps its simplest AI application is a personal diary program that can be used two ways: in a simple word-processing mode for children with some writing facility and in a prompt mode. In the prompt mode, the child is asked about his or her activities during the day and replies with one- and two-word answers. The computer program then writes a grammatically correct diary entry based on those replies. In more advanced CAI applications, the computer is more flexible than previous systems. It can parse a user's natural-language inputs and evaluate the person's ability level. It can then proceed to material of appropriate difficulty, rather than simply advancing one level at a time. The success of the AI Computer will depend almost totally on the software available for it, and Sega has already forged ties with the educational community to develop courseware for 3- to 8- year-olds. Its partner in this endeavor is general trader Marubeni Corp.'s Visual Information Section, Corporate Development Department, Tokyo, which is working with various educational organizations to create the programs. Sega plans to work also with Linguaphone Institute (Japan) Ltd., Tokyo, to develop English-language instruction programs. Courseware in English should be ready next year, and the Sega AI machine then will be introduced in the U. S. Rather than employing the Basic-language interpreters of most personal computers, Sega's AI machine uses a run-time Prolog-language interpreter residing in 128-K bytes of read-only memory. The Prolog interpreter is for running applications only ‚Äî it cannot be used for programming. The company chose the Prolog AI language because of its ability to handle unformatted input and to parse natural-language input. Prolog is not especially suitable for driving displays and controlling peripherals, so Prolog functions call up fast, efficient assembly-language subroutines for these tasks. ENHANCED MATH. Sega Prolog was developed jointly with CSK Research Institute, the AI lab of software house Computer Services Corp., Tokyo. In addition to the assembly-language calls, the language's mathematical performance was enhanced for the AI Computer. The computer's hardware is designed for high performance at low cost. It is built under contract by Nippon Gakki Co., Shizuoka, a leading maker of MSX- compatible computers and electronic musical instruments. The microprocessor is NEC Corp.'s V-20, an enhanced version of the Intel 8088 implemented in CMOS. The Prolog interpreter is stored in 128-K bytes of read-only memory, and main memory is 128K bytes of dynamic random-access memory. Video memory is 64-K bytes of DRAM, expandable to 128-K bytes. A Centronics compatible printer interface is standard. The system's power supply has sufficient capacity to run an optional 3 1/2-in. floppy-disk drive. The machine also comes with a tape recorder for digital input or natural-language audio output, and an eight-direction cursor controller. The Sega Computer has a tablet on its sloping upper surface that takes overlays for various applications. One that will be used in many children's programs is a Japanese-language touchpad, which includes all phonetic-syllabary characters along with variations annotated with one of two standard marks. The technique was designed to meet the needs of young users, who might not be adept at using a traditional Japanese keyboard where the operator strikes character and mark-only keys in succession to obtain the desired syllable. For older children and adults, the computer also provides the new Japanese industrial-standard keyboard,, which fits on top of the tablet to save desk space. Applications programs are usually supplied as plug-in ROM cards with a capacity off up to 128-K bytes, although the built-in cassette tape recorder and optional 3 1/2-in floppy disk drive can also be used. ROM versions of Basic and Logo are available for users who want to write programs in those languages. A I28-K RAM expansion kit and a I28-K RAM card are available for disk-based applications or languages that require extra memory. -Charles L Cohen Electronics/ July 24, 1986‚Äú We eventually found one... In September 2014, more than a decade after the last public sighting, a user of Yahoo! Auctions in Japan put a Sega AI Computer unit for sale along with 15 software titles, all listed separately. With help from a few generous individuals and associations, we decided to buy everything and work on it. Since that time, several other units have appeared on Yahoo Auctions, but rarely ever came with software again. We however slowly managed to gather more software from other sources. At the end of 2022, another meaningful set of auctions including a boxed keyboard and 10 earlier iterations of known software were sold, which we also managed to acquire a year later from the initial buyer. The hardware and software acquired in the first batch were stamped with mention of the Aomori Minami Hoikuen (ÈùíÊ£ÆÂçó‰øùËÇ≤Âúí) in Tokyo. The Sega AI Computer appears to have been mostly sold to Japanese schools. It is unknown how many were manufactured, how many were sold, and if Sega had further plans to market the machine to a wider audience. The December 1986 and January 1987 issues of ‚ÄúCopel 21‚Äù, a Japanese science magazine for kids, included an order form to purchase the Sega AI Computer for 87500 yens, and the Kumon Wonderschool set in 17 installments of 9990 yens.An usually long 1986-1989 life cycle? Many software titles have a date printed in their manual. They range from August 1st, 1986 to May 1st, 1989. There are two versions of the My Card for ‚ÄúAI Enikki‚Äù: one dated 1986 and one dated 1989. At least 10 titles were released in 1986 on cassettes, then re-released in 1987-1988 on cards. In both instances, the later software versions have been updated and are more advanced. The presumed software release timespan (at least 33 months between August 1986 and May 1989) is particularly odd considering this is an extremely rare and unknown system. Why would Sega update and re-release software? Did they do an early confidential launch, then decided to go back to the drawing board to improve software, only to do a similarly confidential relaunch? Did Sega have contracts in place with customers e.g. schools to keep on delivering software for a certain time? We're not sure! The ad and order form from COPEL 21 December 1986 issue suggest that at least it was possible to purchase the Sega AI Computer from that time. Who worked on the Sega AI Computer? Caution: Information here is extremely piecemeal and may be inaccurate. The project was seemingly led by Sega R&D Dept 6 (Á¨¨6Á†îÁ©∂ÈñãÁô∫ÈÉ®). Masami Ishikawa (Sega) is among the people who worked on the Sega AI Computer project. Mika Okada (Sega) is among the people who worked on art for games. Very likely many other people were involved but we don‚Äôt have many names for now. Some games were developed by Marubeni Corp (according to ‚ÄúElectronics‚Äù July 1986 article). The ‚ÄúPinpon Pasokon‚Äù series of 3 software are credited to a company called Kamiya. Sega Prolog was developed partly by the CSK Research Institute (CRI). An early prototype unit aimed at market evaluation in the USA seemingly makes many references to CRI, but we didn‚Äôt get access to this prototype. Sega worked with Linguaphone Institute to develop the English-language learning software. Hardware was built by Nippon Gakki Co., Shizuoka (according to ‚ÄúElectronics‚Äù July 1986 article). Spotted in the data following a simple search for ASCII strings: One system ROM contains the following ASCII string: ‚Äú07/19-1986‚Äù. The Sound Box ROM contains the following ASCII strings: ‚ÄúProgrammed by SHUN ARAI. Produced by YASUHIGE KOBAYASHI. Special Thanks: KAMIYA STUDIO & PHILOS‚Äù ‚ÄúR&D No. 6 SOUND BOARD v1.1 87/08/12 SEGA‚Äù Black Squirrel thoughts: ‚ÄúIt is likely a CSK product with Sega branding. CSK was Sega's parent company at the time (they bought Sega in 1983) and seem to have used the AI Computer as an attempt to get into the educational market. CSK also had subsidiaries producing hardware and software - CRI (CSK Research Institute) seems to be responsible for \"Sega Prolog\", the closest this thing gets to an operating system. I don't know if this means CRI were secretly producing the software on cards and cassettes under the Sega name, or if there were genuine Sega people involved (I noticed a credit for Shun Arai on the sound board - he was a Sega man). Perhaps they thought kids would engage more if they saw a Sega logo in 1986 rather than a CSK/CRI one?‚Äù ‚ÄúSega's use of the term \"AI\" or \"artificial intelligence\" is a bit loose, even for 1986. It's a far cry from the likes of ChatGPT - I think the Sega AI Computer is giving the illusion of intelligence by recognising and responding in real human languages (English? Japanese? Both?) rather than 20 GOTO 10. The selling point seems to be \"it's not BASIC, it's not assembly, it's Prolog!\"‚Äù ‚ÄúThe promotional material is hinting at modems and printers and microphones and mysterious boxes - not sure how much of that came to pass. There's talk of an \"8-bit CPU board\" - maybe that's where MSX2 compatibility comes in? I don't know what the plan was there - perhaps it was a fallback if AI Computer software didn't take off.‚Äù ‚ÄúIt does have some legacy though: starting with the Mega Drive, games on Sega platforms were required to have a ROM header: https://segaretro.org/ROM_header. If it was a game, it was designated \"GM\", but educational software was designated \"AI\". ‚Äù Emulation A MAME driver has been developed by Wilbert Pol and Fabio Priuli, based on hardware research by Chris Covell. Code has been merged, and 2024/01/31 release will partial emulation of the Sega AI Computer. If you want to try it today: MAME Downloads. MAME 0.262 release note. Download our MAME ready roms folder and unzip to roms/segaai/ Instructions to run MAME From the UI: Run MAME and browse to AI system. Press TAB to access configuration menu. From the command-line: Copy system roms, games and overlay scans from xxxx.zip into roms/segai/ folder (use use \"-rompath /path/to/extract/to/segaai\" command-line parameter). Run with ‚Äúmame -nodebug -window segaai -card XXXX‚Äù where ‚ÄúXXXX‚Äù stands for a game identifier in MAME‚Äôs format. To enable the Sound Box emulation, add the ‚Äú-exp soundbox‚Äù flag. Multi-cards games may be booted from the second card using \"-card XXXX:card2\", e.g. \"-card okeihana:card2\". The game identifiers in MAME‚Äôs format are: aienikki, alicewor, andersen, arabiann, cinderel, columbus, cosmictr, eigogame, eigoohan, gulliver, henshin, mozartac, oceanfan, okeihana, pinponmm, pinponmr, pinponnm, ranranme, robinson, runrunmu, tantanrh, wakuwaku. Controls Because the system has both a D-Pad and an emulated touch surface, it is recommended that you reconfigure MAME inputs. Press TAB and go to 'Input Settings'->'Input Assignments (this system') You can leave P1 Up/Down/Left/Right mapped to Keyboard arrow keys. Change Touchpad X Analog Inc/Dec to e.g. D and A, and Touchpad Y Analog Inc/Dec to e.g. S and W. The controls becomes: Touchpad: move cursor with WASD, press with SHIFT. D-Pad: Arrow keys Grey button: Space RL/PL: Ctrl, Alt Known emulation bugs as of January 2024: Speech emulation seems to occasionally refer to samples from the wrong ROM. Tape drive is not emulated yet. Keyboard is not emulated yet. Likely other emulation bugs and issues. Software Titles What we found so far See our site listings (every game has its own page): Our list of known Sega AI Computer unique releases Our list of known Sega AI Computer unique titles. Our list of Sega AI Computer related scans. All software we found so far is mostly in the Japanese language and tends to be text-heavy, making them difficult to play for non-Japanese readers. We hope that our scans and screenshots can still give you a sense of what they are. AI Enikki AI Enikki (Ôº°Ôº©„Åà„Å´„Å£„Åç) (early card version, later card version)System boot screen AI Enikki AI Enikki Kumon Wonderschool (two sets) Two sets exists: an earlier tape-only set, and a later card-only set. Only the card versions are emulated so far. The tape version were released 1-2 years before and appear to be different software with different contents. Cosmic Train: 1987 tape version Cosmic Train: 1988 card version Alice World („ÅÇ„Çä„Åô„Éª„Çè„Éº„Çã„Å©) early tape version, later card version) Robinson Land („Çç„Å≥„Çì„Åù„Çì„Éª„Çâ„Çì„Å©) (early tape version, later card version) Cosmic Train („Åì„Åô„Åø„Å£„Åè„Éª„Å®„Çå„ÅÑ„Çì) (early tape version, later card version) Cinderella Labyrinth („Åó„Çì„Åß„Çå„Çâ„Éª„Çâ„Å≥„Çä„Çì„Åô) (early tape version, later card version) Gulliver Pocket („Åå„Çä„Å∞„Éº„Éª„ÅΩ„Åë„Å£„Å®) early tape version, later card version) Mozart Academy („ÇÇ„Éº„Å§„ÅÅ„Çã„Å®„Éª„ÅÇ„Åã„Åß„Åø„Éº) early tape version, later card version) Arabian Night („ÅÇ„Çâ„Å≥„ÅÇ„Çì„Éª„Å™„ÅÑ„Å®) early tape version, later card version) Andersen Dream („ÅÇ„Çì„Åß„Çã„Åõ„Çì„Éª„Å©„Çä„Éº„ÇÄ) (early tape version, later card version) Ocean Fantasy („Åä„Éº„Åó„ÇÉ„Çì„Éª„Åµ„ÅÅ„Çì„Åü„Åò„Éº) early tape version, later card version) Columbus Map („Åì„Çç„Çì„Å∂„Åô„Éª„Åæ„Å£„Å∑) early tape version, later card version)Alice World [Card version] Alice World [Card version] Alice World [Card version]Cosmic Train [Card version] Cosmic Train [Card version] Andersen Dream [Card version] Arabian Night [Card version] Cinderella Labyrinth [Card version] Columbus Map [Card version] Gulliver Pocket [Card version] Gulliver Pocket [Card version] Mozart Academy [Card version] Ocean Fantasy [Card version] Ocean Fantasy [Card version] Robinson Land [Card version]] Ongaku Wonder School Runrun Music („Çã„Çì„Çã„Çì„Éª„Åø„ÇÖ„Éº„Åò„Å£„Åè) (card) Tantan Rhythm („Åü„Çì„Åü„Çì„Éª„Çä„Åö„ÇÄ) (card) Ranran Melody („Çâ„Çì„Çâ„Çì„Éª„ÇÅ„Çç„Åß„ÅÉ„Éº) (card)Runrun Music Tantan Rhythm Ranran Melody English Wonder School Folks & Fairy Tales - Eigo de Hanashi (Folk & Fairy Tales - „Åà„ÅÑ„Åî„Åß„Åä„ÅØ„Å™„Åó) (two cards + two tapes) Popo Adventure's - Eigo de Game (Popo's Adventure - „Åà„ÅÑ„Åî„Åß„Åí„Éº„ÇÄ) (two cards + two tapes)Popo's Adventure: Popo no Yume Popo's Adventure: Popo no Yume Folk & Fairy Tales Folk & Fairy Tales Folk & Fairy Tales Surasura Moji Wonder School Henshin Kanji („Å∏„Çì„Åó„Çì„Åã„Çì„Åò) (card) Okeiko/Hanamaru Aiueo („Åä„Åë„ÅÑ„Åì„Éª„ÅØ„Å™„Åæ„Çã„ÄÄ„ÅÇ„ÅÑ„Åà„ÅÜ„Åä) (two cards) Waku Waku ABC To 123 („Çè„Åè„Çè„ÅèÔº°Ôº¢Ôº£„Å®ÔºëÔºíÔºì) (card)Henshin Kanji Waku Waku ABC To 123 Waku Waku ABC To 123 Pinpon Pasokon Pinpon Numbers („Éî„É≥„Éù„É≥„ÄÄ„Éä„É≥„Éê„Éº„Ç∫) (card) Pinpon Music Rhythm („Éî„É≥„Éù„É≥„ÄÄ„Éü„É•„Éº„Ç∏„ÉÉ„ÇØ„ÄÄ„É™„Ç∫„É†) (card) Pinpon Music Melody („Éî„É≥„Éù„É≥„ÄÄ„Éü„É•„Éº„Ç∏„ÉÉ„ÇØ„ÄÄ„É°„É≠„Éá„Ç£„Éº) (card)Pinpon Pasokon splash screen Pinpon Music Melody Pinpon Music Melody What we haven't found so far AI Enikki: Cassette version was spotted in an auction, we couldn‚Äôt acquire it. Edison Labo: mentioned in Kumon Wonder School Japanese flyer, with screenshots. Mentions of other titles at SukaSega's AI page (exact source unknown) Edison‚Äôs Labo („Ç®„Ç∏„ÇΩ„É≥„É©„Éú) Lincoln Freedom („É™„É≥„Ç´„Éº„É≥„Éª„Éï„É™„Éº„ÉÄ„É†) Grimm House („Ç∞„É™„É†„Éª„Éè„Ç¶„Çπ) Holmes Mystery („Éõ„Éº„É†„Ç∫„Éª„Éü„Çπ„ÉÜ„É™„Éº) Anne Diary („Ç¢„É≥„Éç„Éª„ÉÄ„Ç§„Ç¢„É™„Éº) Columbus Egg („Ç≥„É≠„É≥„Éñ„Çπ„Éª„Ç®„ÉÉ„Ç∞) ‚Üí Likely same as ‚ÄúColumbus Map‚Äù Takara Jima Pirates (ÂÆùÂ≥∂„Éª„Éë„Ç§„É¨„Éº„ÉÑ) Space Fantasy („Çπ„Éö„Éº„Çπ„Éª„Éï„Ç°„É≥„Çø„Ç∏„Éº) Safari Fantasy („Çµ„Éï„Ç°„É™„Éª„Éï„Ç°„É≥„Çø„Ç∏„Éº) Mothers Book („Éû„Ç∂„Éº„Ç∫„Éñ„ÉÉ„ÇØ) COPEL 21 December 1986 and January 1987 issues contains an ad showing 18 boxes (likely mockups) with readable names: Alice World („Ç¢„É™„Çπ„ÄÄ„ÉØ„Éº„É´„Éâ)‚Üí Released Robinson World („É≠„Éì„É≥„ÇΩ„É≥„ÄÄ„ÉØ„Éº„É´„Éâ)‚Üí Released as ‚ÄúRobinson Land‚Äù Cinderella World („Ç∑„É≥„Éá„É¨„É© „ÉØ„Éº„É´„Éâ)‚Üí Released as ‚ÄúCinderella Labyrinth‚Äù Aesop World („Ç§„ÇΩ„ÉÉ„Éó „ÉØ„Éº„É´„Éâ) Hansel & Gretel („Å∏„Çì„Çª„É´ÔºÜ„Ç∞„É¨„Éº„ÉÜ„É´„ÄÄ„ÉØ„Éº„É´„Éâ) Goku World („Ç¥„ÇØ„Ç¶„ÄÄ„ÉØ„Éº„É´„Éâ) Taka Jima World (È´òÂ≥∂„ÄÄ„ÉØ„Éº„É´„Éâ) Helen Keller World („Éò„É¨„É≥ „Ç±„É©„Éº „ÉØ„Éº„É´„Éâ) Nightingale World(„Éä„Ç§„ÉÅ„É≥„Ç≤„Éº„É´ „ÉØ„Éº„É´„Éâ) Jeanne d‚ÄôArc World („Ç∏„É£„É≥„Éå„Éª„ÉÄ„É´„ÇØ„ÄÄ„ÉØ„Éº„É´„Éâ) Edison World („Ç®„Ç∏„ÇΩ„É≥„ÄÄ„ÉØ„Éº„É´„Éâ)‚Üí Later referred to as ‚ÄúEdison‚Äôs Labo‚Äù Lincoln World („É™„É≥„Ç´„Éº„É≥ „ÉØ„Éº„É´„Éâ)‚Üí Later referred to as ‚ÄúLincoln Freedom‚Äù Space World („Çπ„Éö„Éº„Çπ(ÂÆáÂÆô)„ÉØ„Éº„É´„Éâ) Safari World („Çµ„Éï„Ç°„É™(ÔºüÔºü)„ÉØ„Éº„É´„Éâ) Robot World („É≠„Éú„ÉÉ„Éà„ÄÄ„ÉØ„Éº„É´„Éâ) Mother World 1 („Éû„Ç∂„Éº „ÉØ„Éº„É´„ÉâÔºë) Mother World 2 („Éû„Ç∂„Éº „ÉØ„Éº„É´„ÉâÔºí) Mother World 3 („Éû„Ç∂„Éº „ÉØ„Éº„É´„ÉâÔºì) It is unlikely that those titles were all finished or released, but we have screenshots for ‚ÄúEdison‚Äôs Labo‚Äù in a flyer so this one was likely developed. Downloads System Roms The system dumps amount to a total of 590 KB of uncompressed data. Sega AI Computer - System Roms.zip (382 KB) AI-E000.bin: system EPROM 0, marked 8/24 (64 kb) AI-F000.bin: system EPROM 1, marked 7/21 (64 kb), contains string: 7/19/86 AI-MPR-7619.bin: speech ROM 0 (128 kb) AI-MPR-7620.bin: speech ROM 1 (128 kb) AI-MPR-7689.bin: character ROM (128 kb) AI-SND-AI2002.bin: sound box EPROM (128 kb), contains strings: Programed by SHUN ARAI.Produced by YASUHIGE KOBAYASHI.Special Thanks KAMIYA STUDIO & PHILOS [...] R&D No.6SOUND BOARD V1.187/08/12 SEGA Software Roms & Overlay The 26 Sega My Card dumps amount to a total of 4.2 MB of uncompressed data. We also scan all overlays at had at our disposal. Sega AI Computer - 26 Cards Roms.zip (1941 kB) Sega AI Computer - 26 Overlay Scans.zip (10 MB) MAME ready Roms folder As MAME use specific naming convention for its files, we provide a readily usable folder: MAME ready roms folder for Sega AI Computer (no tapes).zip (10 MB) Tapes recordings Important: Very large data. The 14 cassettes (16 sides in total) currently amount to about ~1.2 GB of data. Please don't download unless you have use for this: the cassette system is NOT emulated a the time of writing, thus the data from our cassette dumps was not yet certified as ‚Äúcorrect‚Äù (as in, passing the Sega AI Computer loading routines and probable checksum checks). According to early research, tapes are using a FSK modulation scheme, with a theoretical maximum bit-rate of 9600 BPS which is very high for its time. Actual bit-rate used seems at least two times lower. We are currently releasing 48 Khz, 24-bit stereo lossless FLAC. It is probable that once emulated, we can settle on a compact format for the data segments, or can tell what level of audio compression is adequate for loading in a real system and emulators. Tape downloads are available from our Our Big Shared Folder. Alice World [Tape version] (Tape).flac (~5 minutes) Andersen Dream [Tape version] (Tape).flac (~9 minutes) Arabian Night [Tape version] (Tape).flac (~10 minutes) Cinderella Labyrinth [Tape version] (Tape).flac (~5 minutes) Columbus Map [Tape version] (Tape).flac (~5 minutes) Cosmic Train [Tape version] (Tape).flac (~9 minutes) Folk and Fairy Tales - Eigo de Ohanashi 1 (Tape Side A).flac (~15 minutes) Folk and Fairy Tales - Eigo de Ohanashi 1 (Tape Side B).flac (~15 minutes) Folk and Fairy Tales - Eigo de Ohanashi 2 (Tape Side A).flac (~16 minutes) Folk and Fairy Tales - Eigo de Ohanashi 2 (Tape Side B).flac (~16 minutes) Gulliver Pocket [Tape version] (Tape).flac (~5 minutes) Mozart Academy [Tape version] (Tape).flac (~5 minutes) Ocean Fantasy [Tape version] (Tape).flac (~11 minutes) Popo's Adventure - Eigo de Game 1 - Popo no Yume (Tape).flac (~16 minutes) Popo's Adventure - Eigo de Game 2 - Popoland no Himitsu (Tape).flac (~18 minutes) Robinson Land [Tape version] (Tape).flac (~5 minutes) Hardware Manuals Sega AI Computer - Main Unit Manual.pdf (15 MB) Sega AI Computer - Sound Box Manual.pdf (9 MB) Sega AI Computer - Keyboard Manual.pdf (15 MB) Software Scans See Our list of Sega AI Computer related scans. Flyers & Ads Flyer - CAI-1 (1986-04) (Japan).pdf (26 MB) (courtesy of Sudden-Desu) Flyer - Sega AI Computer (Japan).pdf (20 MB) (courtesy of Sudden-Desu) see our Big Shared Folder for many more. Photos See our Big Shared Folder for: about 150 photos from all past known auctions/sales of Sega AI Computer contents. close up photos of Sega AI Main Unit close up photos of Sega AI Main Unit board close up photos of Sega AI Keyboard close up photos of Sega AI Sound Box board photos of software packaging archived webpages about an unreleased US prototype and more... Magazine Scans COPEL 21 - 1986-12 - Article and Ad (Japan).pdf (30 MB) see our Big Shared Folder for many more. Technical documentations Sega AI Computer - Misc Technical Docs (2024-01-29).zip (122 KB) various notes and schematics by Chris Covell. Also see Reverse engineering thread. Links Our big shared folder Our Big Shared Folder has about 700 files, 1.6 GB of data. External resources http://park21.wakwak.com/~suka/sega/catalog/AIcom/AI.html https://segaretro.org/Sega_AI_Computer https://archive.org/search?query=%22%E3%82%BB%E3%82%ACAI%22&sin=TXT Site sections https://www.smspower.org/Games-Releases/AI-JP https://www.smspower.org/Games/AI-All https://www.smspower.org/Scans/AI-All Forum threads https://www.smspower.org/forums/15004-SegaAIComputerSoftware https://www.smspower.org/forums/15244-SegaAIComputerReverseEngineeringThread https://www.smspower.org/forums/19976-SegaAIComputerFirstDumpsSystemRomsScansRecordingsMore Research credits Credits Chris Covell (https://www.chrismcovell.com) did the majority of the early hardware reverse engineering, documentation and software dumping. Wilbert Pol and Fabio Priuli went on to create a MAME driver. Early work was actually done in 2015. As we uncovered new software and received it in late 2023, we decided to resume work on this release and Wilbert took care of further emulation work. A first version of the driver was merged into MAME‚Äôs codebase on January 23, 2024 (pull request). Omar Cornut acquired hardware and software, dumped software cards and tapes acquired later, scanned contents, researched information and organized other work. Charles MacDonald helped build a custom My Card adapter with configurable pinouts to facilitate dumping later finds. Game Preservation Society received the hardware and made the first pass on trying to dump the data. It was found that the Sega AI cards share the pinouts of Sega MyCards used for the SG-1000 II and Master System. A first set of (incomplete) dumps were made which would help moving emulation forward until the unit and software were passed to Chris. Chris dumping speech roms Charles adapter Special Thanks Thanks to Odin Ozdil for helping acquire extra software and scanning the Sega AI Computer manual. Thanks to Masato Ozaki, BEEP Shop & Takayuki Komabayashi for helping me acquire extra software. Thanks to Black Squirrel and Gorefestor for help writing this article. Thanks to KailoKyra for scan processing, assistance with tape dumping & more. Thanks to Balrog for assistance with tape dumping. Thanks to SukaSega for many early scans and research. Thanks to BKK for general research. Thanks to Sudden-desu for high-resolution scans of two Japanese flyers. Thanks to Maxim for general support, corrections, web work & many more. Thanks to generous donors helping buying the first batch: initial purchase received funds from: Adrien Duchemole, Bock, Chris Covell, David Papazian (Mobigame), Frank Cifaldi (Video Game History Foundation), Kusfo, Game Preservation Society, GlobZ, Gregg \"Greggman\" Tavares, Olivier L, Vitorio Miliano. About SMS Power! Active since 1997, SMS Power! has been researching, gathering and publishing all sort of information and data related to Sega 8-bit systems. We unveiled hundreds of prototypes, and have an active community of hobbyists contributors creating maps, soundtracks, game modifications, translations and more. You may if you can Donate to facilitate purchasing prototypes and rare software. Return to top 0.362s",
    "commentLink": "https://news.ycombinator.com/item?id=39206529",
    "commentBody": "Sega AI (smspower.org)256 points by bpierre 16 hours agohidepastfavorite47 comments wk_end 14 hours agoAs a Sega nerd and a Prolog nerd, this is insanely cool to me. Even if it seems like the Prolog relationship might be some kind of marketing gimmick - it's not clear to me to what extent the games really are written in Prolog. reply neilv 9 hours agoparentSounds like they had some kind of Prolog inferencing/runtime stored in ROM, and running on a normal early x86 CPU. Games packaged for the machine could call that runtime code, but Prolog wasn't exposed to the user. So there is the marketing aspect (that generation of \"AI\" hype, and government initiatives), plus whatever unusual functionality that enabled being exposed through games (e.g., simple natural language parsing). A few years after this, a project I was loosely involved with embedding a Prolog interpreter into a much larger system that was mostly implemented in C, just to implement a reports query language feature for a complex data model. Years later, for a very similar purpose for a rich data model, we instead exposed SQL to the power user/integrator. (Today, you'd probably do the custom reports by making a GUI forms-based or wizard-based way of doing custom queries, or overkill it with a drag&drop visual programming language. To do NLP-ish stuff this year, you'd be shot if you didn't first try to do it with an LLM.) reply ilaksh 13 hours agoparentprevIt's called an \"AI\" computer. Prolog was AI in 1986. I don't see that as a gimmick. I also don't see why one would assume the concept was really about games except for the fact that we know Sega as a games company today. reply wk_end 11 hours agorootparentSega was known as a games company then, too. They've always been a games company - the name Se-Ga stands for SErvice GAmes, because their original business was importing games for American servicemen stationed abroad to play in their leisure time. The most they've ever really deviated from being totally game-focused towards general-purpose computing is when they licensed out their hardware to be incorporated into computers like the TeraDrive. I would assume the concept is about games because all of the software available for it looks to be edutainment games. More than anything, it feels like a predecessor to Sega's lines of educational game consoles like the Pico. There seems to be an association in Sega history between it and AI and education. As the link says: > It does have some legacy though: starting with the Mega Drive, games on Sega platforms were required to have a ROM header. If it was a game, it was designated \"GM\", but educational software was designated \"AI\". The \"computer\" here might be something of a misnomer - there wasn't quite so hard a line between consoles and home computers back then, especially outside of North America. After all, the NES was known as the \"Family Computer\" in Japan. I'd describe it as a gimmick because it's not clear what exactly the association with Prolog is here, particularly for the end user: > The Prolog interpreter appears to be used by existing software to allow some form of natural language processing. It doesn't seem accessible to the end-user. [...] The Prolog interpreter is for running applications only ‚Äî it cannot be used for programming. Sega might have pushed the AI/Prolog angle because, then-as-now, AI was a hot buzzword in tech. Japan in particular had bought into the AI/Prolog thing, due to the Fifth Generation Computer Systems project [0]. If the games mostly weren't written in Prolog, and the user can't use Prolog, but AI and Prolog were trendy at the time, I think calling it the AI Prolog to tap into that hype is a bit of a gimmick. [0] https://en.wikipedia.org/wiki/Fifth_Generation_Computer_Syst... reply actionfromafar 12 hours agorootparentprevSega still makes these neat things: https://www.segatoys.space/en/public/flux.html reply theNJR 11 hours agorootparentThat‚Äôs really cool! Do you own one? reply actionfromafar 10 hours agorootparentI do, actually. It‚Äôs just a projector, really, but it‚Äôs really neat. It‚Äôs analog so it has no pixels. It can rotate the starfield but the motor is noisy enough to disturb me when going to sleep, so I don‚Äôt use the motor. reply beebeepka 9 hours agorootparentDo we have to buy all the cool discs one by one? Looks like a neat product, such a shame the motor is noisy because a little motion, just barely perceptible, would go a long way. I would think people would throw good money for the same but better. I know I would. reply tpmoney 1 hour agorootparentI have one of the original models and the motor noise is probably dependent on whether or not you're the sort of person who needs a perfectly quiet room or if you can sleep with a fan. If you have a ceiling fan (or any other noise maker, like a blowing AC system), you're almost certainly not going to hear the motor. IIRC, EEV blog tore one down and the whole rotating assembly is belt driven so that you don't have gear noise. The only thing I ever hear is a very slight hobby motor hum when everything is dead silent. When you order it I think it comes with 2 discs, but yeah, all the others are individual orders, unless you want to order the 17 pack of third party discs for $300. Other things to be aware of: 1) It looks really good, like good enough to feel like you really are out under the stars in a properly blacked out room. but getting the focus just right is fiddly and very easy to knock out of focus again 2) Like any projector it works best projecting straight on. I doubt you have space in the exact center of your room to project straight up, so be prepared to accept this will be a distorted projection. You get used to it and it doesn't really ruin the effect that much, but it's disappointing at first. 3) The 3 hour timer is built in and impossible to disable to the best of my knowledge. If you want it running all night, you'll want a smart plug or a programmable timer plug that can turn off for a few minutes intermittently. I found ~5 minutes every 2.5 hours to work well. Too short and the timer doesn't seem to reset. 4) There are some really nice 3rd party discs out there too, but their sharpness and overall realism varies greatly so be cautious when ordering. reply actionfromafar 2 hours agorootparentprevI bet many don't even notice the motor, but I do. It comes with two discs. I think you could put any slide film in there too, if you really wanted. reply Apocryphon 12 hours agorootparentprevGood time for Sega to bring it back reply Agingcoder 10 hours agoparentprevI‚Äôm neither , and think it‚Äôs insanely cool as well. reply Grazester 14 hours agoparentprevI would love to hear which console you have in your collection if any. I was a SEGA fanboy back in the day and have had a few of their console dating back to the Master System reply wk_end 13 hours agorootparentGenesis (MD), Sega (Mega) CD, Saturn, Dreamcast. The Master System was kind of a bust in North America so I don't carry very much nostalgia for it. I love each of these consoles in their own way, by some combination of their own merits and my own associations of them. The first game I ever played (or at least the first I remember) was Sonic 2, at a demo kiosk at a Canadian Tire while my mom was shopping. That's the sort of thing which sticks with you forever. At home we ended up getting an SNES, and although the SNES' highs were clearly higher than anything Sega put out, I think the average Genesis game holds up better (and it certainly has incredible gems of its own). The faster and easier to work with CPU, I think, gave devs a bit more breathing room; the FM synth has an iconic, crunchy, aggressive, alien sound that's way cooler than the canned, fuzzy, plinky soundscape of so many poorly-made SNES scores (of course, again: the best SNES soundtracks are sublime). You could put Go Straight from SoR2 on at a club now and get the dance floor going. The only thing about it that's dated badly is that it really, really could've used a richer colour palette. As a kid I never had a Sega CD, but I did have a Windows 3.1 computer, and it feels part-and-parcel with that era of \"multimedia\". It's all nostalgia, but I love the grainy look of the FMV, the very particular abstract and geometric style of the CGI of the era, the metallic sheen of those digital CD soundtracks. It's got some fantastic games, too: Sonic CD is a classic, and Lunar 2 (particularly if you can read Japanese and don't need to endure everything Working Designs did to it) is, as far as I'm concerned, up there with FF6 and Chrono Trigger as a definitive 16-bit JPRG. I of course understand why the Playstation won the 32-bit era, it's a better system than the Saturn with better games overall. But the Saturn has some absolute classics (again, Japanese knowledge helps here). It's not a system that I had much experience with at the time - except, again, for some time spent at a department store kiosk with Nights, which entranced me - but it's come to me to feel like the more interesting, mysterious machine than the Playstation. The Dreamcast feels more like the end of the 32-bit era than the start of the next-gen: a 32-bit system with next-gen performance. To me the quintessential Dreamcast game is fast, buttery smooth, vibrant, colourful. Again, it's easy to see why the PS2 came out on top, but looking back now I see the PS2 era more as a trial run for \"modern\" gaming - dark, muddy, slow, clumsy versions of the sort of games we're playing now; whereas the Dreamcast was home to the absolute best of the games we were playing then. reply RajT88 11 hours agorootparent> Genesis (MD), Sega (Mega) CD, Saturn, Dreamcast. The Master System was kind of a bust in North America so I don't carry very much nostalgia for it. Back before retro consoles were really collectible, I snapped up a giant pile of SMS stuff at a thrift shop for 35 USD. I believe it was almost my whole collection: - SMS 1st gen - 2 controllers - Light gun - A few card games in their original boxes - ~25 cart games, most of them in original boxes, some with manuals When I was living with a friend post-college, we would actually play it a bunch. Dreamcast and PS2 as well. reply brabel 11 hours agorootparentprev> The Master System was kind of a bust in North America so I don't carry very much nostalgia for it. Not sure if that's true in most places, but at least in Brazil, the Master System was incredibly popular... I don't live there anymore, but around just 10 years ago I remember seeing it still for sale (though IIRC they used a different name then - but it was clearly the same) as a low cost game console (average people just can't afford a PlayStation). I had a Master System in the late 1980's and then a Mega Drive, and really loved them both... the Mega Drive was definitely a huge upgrade from the Master System (I was upgrading to Master System from Atari, which was a similarly big upgrade). I never upgraded again, to the Saturn, just because I stopped gaming almost completely for a while (I started playing in a rock band, it was the 1990's, all the cool kids were doing it)... and I got a PC (also Windows 3.1, classic Pentium 100MHz!) and started doing my gaming on it. reply wk_end 11 hours agorootparentYeah, the Master System was huge in Brazil. Sega was willing to license the design out to Tec Toy to manufacture locally, which got them around the massive import tariffs that made other systems unaffordable. It was also very popular in Europe (and Australia?) - not sure if there's any real reason for that, but the consequence of it - this [0] absolutely incredible rap by two British teenagers - makes me very glad it was. In North America and Japan, though, Nintendo absolutely commanded the market. Basically no one had a Master System. [0] https://www.youtube.com/watch?v=Ks0_3Xlr5y0 reply mattl 5 hours agorootparentNintendo was pretty bad about marketing the NES in the UK at least. Sega did a much better job so more people had a Master System. I assume this is true for the rest of the region too. reply cglong 13 hours agorootparentprevNot GP, but I had a Genesis, CD, and Game Gear. A few years back I bought a Saturn off eBay too to play a leaked early build of Sonic X-treme :) reply SandmanDP 12 hours agorootparentIs this the Jolly Roger restored build or a build of X-treme I haven‚Äôt heard of yet? reply cglong 7 hours agorootparentIt's been several years I guess but from following my own breadcrumbs, yeah I think it was the Jollyroger build. reply SandmanDP 4 hours agorootparentDarn. Thanks for checking though, I appreciate the effort reply Grazester 12 hours agorootparentprevSomething tells me it's someone's homebrew dev of Sonic x-treme. If it's that then the same developer also has a killer Quake style engine for the Saturn which is quite a showcase. reply beebeepka 9 hours agorootparentprevI stopped caring about consoles decades ago but I still want the games. Why won't they just sell this stuff. I've been feeling the itch to play Bug! for like 20 years. reply copx 9 hours agoprev>Sega's AI machine uses a run-time Prolog-language interpreter residing in 128-K bytes of read-only memory. I really want to know more about this less than 128K Prolog interpreter (it shares the ROM with the OS) running at only 5 MHz. Which language features does it support? How does it work? Writing a useful Prolog interpreter for such limited hardware is quite a feat. I kinda hope someone will extract the interpreter from the ROM and convert the machine code to readable ASM. reply aidenn0 8 hours agoparentWasn't the original prolog implemented on a PDP-11/20 which only had 256kb of physical address space and no virtual memory? reply b0rbb 13 hours agoprevSega absolutely nailed it with graphic design in those days. My goodness. reply dbish 5 hours agoparentThis is the kind of design I need at least one of the many AI hardware startups to use for inspiration reply sen 11 hours agoparentprevThey really did. As a design-nerd, it's some of my all-time absolute favourite retro hardware and software design. reply robertheadley 12 hours agoprevAny time I learn about a new retro system that I have never heard of, is an interesting day. reply rolltrunhert 8 hours agoprevI am guessing the context here is: MAME 0.262 was just released New systems marked not working ------------------------------ Sega AI [Chris Covell, Fabio Priuli, Wilbert Pol, smspower, The Game Preservation Society] https://www.mamedev.org/releases/whatsnew_0262.txt reply doublerabbit 13 hours agoprevWhy don't we get games with those graphics nowadays? I'm bored of AAA realism. Bring back the Amiga graphics. reply Lammy 12 hours agoparentAAA 3D gaming isn't only about the games but was also a way for the security state to incentivize development of certain kinds of hardware broadly applicable outside of just graphics. SEGA were directly part of the paradigm shift from war-focused development to entertainment-focused development in the early '90s with their whole \"omg look at our Lockheed tech!!\" era. That was right at the fall of the USSR and end of the Gulf War when war became (briefly) publicly unpalatable, so the demand to keep working on this stuff had to come from somewhere and the somewhere is us :) reply gbalint 11 hours agoparentprevThere are games like this, just look for pixel art games. E.g. Terraria, Stardew Valley, Enter the Gungeon, etc reply miohtama 11 hours agorootparentI also recommend: Sea of Stars is one of the recent ‚Äúmodern‚Äù award winning indie JRPGs with pixel graphics. reply mattl 5 hours agorootparentprev‚ÄúA Short Hike‚Äù is a lot of fun. reply bowsamic 1 hour agoparentprevhttps://youtu.be/zwoyfH7TgEQ https://www.pouet.net/prodlist.php?platform%5B%5D=Amiga+OCS%... reply Jensson 13 hours agoparentprevBecause making decent 2d graphics requires more skill than decent 3d. Put the minecraft textures in 2d and it is utterly horrible, but in 3d with shadows and shaders it starts to look very good. reply pjerem 13 hours agoparentprevOk, try this :) https://youtube.com/watch?v=Wlq6fFOqI28 reply xgkickt 9 hours agoparentprevPlenty over on itch.io . reply tiltowait 14 hours agoprevWhat strikes me is how similar that keyboard's layout is to the HHKB's. reply nerdponx 10 hours agoparentAs far as I know, the HHKB was inspired by the Apple Lisa / Macintosh keyboard: https://en.m.wikipedia.org/wiki/Apple_keyboards which predates the Sega AI, so maybe the Sega design was also inspired by the Apple design. reply izme 6 hours agoparentprevI think it's more based on the TRS-80 Model 100 maybe? reply bfrankline 8 hours agoprevI‚Äôd bet this was funded by the Fifth Generation Computer Systems (FGCS) initiative. reply Mistletoe 9 hours agoprev [‚Äì] Early to the AI hype, Sega was truly ahead of current times. reply flohofwoe 9 hours agoparent [‚Äì] ...or late, 1986 was deep into the first AI winter. reply p3rls 9 hours agorootparent [‚Äì] Dreamcast was also both early and late. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The Sega AI Computer, a rare and obscure system from 1986, has become the subject of attention after the release of its roms, data dumps, scans, and photographs.",
      "The computer, designed for educational purposes with artificial intelligence capabilities, was primarily sold to Japanese schools and had a limited release.",
      "The Sega AI Computer has influenced future Sega platforms and can now be emulated through MAME. Organizations like the Game Preservation Society and SMS Power! have been instrumental in preserving and researching this system, and donations are encouraged to support ongoing preservation efforts."
    ],
    "commentSummary": [
      "The conversation discusses multiple topics related to Sega, such as the Sega AI computer, a Sega-made projector, and nostalgia for Sega consoles.",
      "It explores the popularity of the Sega Master System, retro gaming hardware and software design, and the technological advancements of the Dreamcast.",
      "The conversation also briefly mentions the influence of gaming on technology and the AI winter of 1986."
    ],
    "points": 256,
    "commentCount": 47,
    "retryCount": 0,
    "time": 1706721768
  },
  {
    "id": 39208673,
    "title": "Why I Moved My Blog from IPFS to a Server",
    "originLink": "https://neimanslab.org/2024-01-31/why-i-moved-my-blog-ipfs-to-server.html",
    "originBody": "Written by Neiman on January 31, 2024 Why I Moved My Blog from IPFS to a Server It‚Äôs safe to say I was a pioneer of IPFS + ENS websites. When I set up my first ENS+IPFS website in March 2019 there were no more than 15 others. Between 2019 to 2022 I co-built an IPFS+ENS browser extension (Almonit), an IPFS+ENS search engine (Esteroids), and of course, my personal blog was available only in IPFS+ENS. But today I moved my blog back to a server, and I‚Äôd like to discuss why. What got me excited about peer-to-peer websites like IPFS is that, theoretically, the more visitors a website has, the more robust, censorship-resistant, and scalable it is. Again, theoretically. Do you know how popular torrent files seem to live forever? I wanted the same but for websites. I imagined a website that is hard to ddos (Robust), difficult to block (Censorship-resistant), and the more readers it has, the faster it is to use it since some readers help to spread the content (Scalable). I imagined a website with a big ‚ÄúPin Me‚Äù button (pinning in IPFS is like seeding in BitTorrent). If a reader presses the button they wil help serve the website. In practice this didn‚Äôt work out really, and for several reasons. IPFS users mostly don‚Äôt run their own nodes or software. Instead, they use gateways. It‚Äôs an educated guess I‚Äôm making based on what I see in the community, and based on the fact that it‚Äôs quite an inconvenience to run your own IPFS node. But even if you do run your own node, the fact you access a website doesn‚Äôt mean you pin it. Not at all. This is a huge difference from BitTorrent where the only way to get content is to run your own software, and when you download something you also share it, by default. Hence, most readers will not help share the website, but even for the ones who will there are still extra complications: Websites are dynamic objects. Their content is being updated all the time. If you just pin the content of the current version of a website, that‚Äôs not much help. What most IPFS websites do is use a name system that points to the latest version of its content. It‚Äôs usually either IPNS, the internal name system of IPFS, or ENS, Ethereum Name System. But IPFS doesn‚Äôt include yet an easy command to always pin the latest content of IPNS, and if someone uses ENS, it means that whoever pins it also needs to listen to Ethereum blockchain events, a huge extra challenge on its own to do without a centralized service. To make things worse, it‚Äôs actually quite hard to get IPFS content to be available in the browser in a reliable way! For example, I wanted my IPFS blog to be available to all major gateways, all IPFS nodes, Brave browser (which supports IPFS natively), and js-libp2p & helia (the js libraries of IPFS). I didn‚Äôt find a reliable way to achieve that on my own. Long rant: I pinned content from my own server and played forever with settings and definitions, but couldn‚Äôt get the content to be available everywhere. Worse was with Helia, where I just couldn‚Äôt manage to get my content accessible from Helia within the browser without connecting directly to my own node. But then what‚Äôs p2p network about it? I found out that there‚Äôs a service, cid.contact, called a ‚ÄúContent Routing‚Äù service. It‚Äôs written in the ‚Äúabout section‚Äù of cid.contact it‚Äôs related to Filecoin, but for some reason, it holds routing data for IPFS, which as far as I know is a different network. The address cid.contact is hard-coded into Helia‚Äôs code in the version I used at least, and it was clear that if a content is indexed by cid.contact, then it‚Äôs reachable almost everywhere, but if it‚Äôs not indexed - it‚Äôs not reachable always. I couldn‚Äôt figure out how to index my content in cid.contact. Honestly, I‚Äôm not sure I wanted to. Because what‚Äôs the point? it seems to just add a dependency on a centralized service. I could try to run my own indexer and define it in my website, but again, centralization. What‚Äôs the economic model for these indexers? Which actors do we expect to run them in the long term? The text in cid.contact says that at the current size of IPFS, it‚Äôs unreasonable to expect the DHT to handle routing efficiently on its own. This kind of makes sense, but what‚Äôs the alternative to that, that doesn‚Äôt break up the technology pros? By now I got tired of the constant struggle for my IPFS blog to function well. At least for a short while, I want a simple, classic working solution. The blog you‚Äôre reading now is built with Jekyll and is hosted on my own 10$ server. don‚Äôt get me wrong, I‚Äôm still an IPFS fanboy. It‚Äôs a great project managed very well. It just doesn‚Äôt fit a personal blog needs yet. That said, It‚Äôs difficult to follow the constant development and innovation of IPFS or Filecoin without this becoming a day job. Did I miss some trivial solution or a recent innovation? If yes, let me know. There are no comments here yet, but I am available via old-style email (neiman@hackerspace.pl), or in Mastodon (@neiman@mastodon.social). ‚Üí Top",
    "commentLink": "https://news.ycombinator.com/item?id=39208673",
    "commentBody": "I moved my blog from IPFS to a server (neimanslab.org)251 points by neiman 13 hours agohidepastfavorite137 comments ianopolous 1 hour agoI've been hosting my website on Peergos (built on ipfs) for years now (founder here). Peergos solves a lot of the issues with mutable data (also privacy, access control). You can see how fast updates show up from an independent server here: https://peergos.org/posts/p2p-web-hosting My personal website served from a peergos gateway (anyone can run one) is https://ianopolous.peergos.me/ If you want to read more check out our book: https://book.peergos.org reply nonrandomstring 13 hours agoprevWell done to the author for writing this up. Having tried fringe technologies over the years, spun up a server and run them for a few months, struggled and seen all the rough edges and loose threads, I often come to the point of feeling - this technology is good, but it's not ready yet. Time to let more determined people carry the torch. The upside is: - you tried, and so contributed to the ecosystem - you told people what needs improving Just quitting and not writing about your experience seems a waste for everyone, so good to know why web hosting on IPFS is still rough. reply b_fiive 12 hours agoprevTotally biased founder here, but I work on https://github.com/n0-computer/iroh, a thing that started off as an IPFS implementation in rust, but we broke out & ended up doing our own thing. We're not at the point where the iroh implements \"the full IPFS experience\" (some parts border on impossible to do while keeping a decentralized promise), but we're getting closer to the \"p2p website hosting\" use case each week. reply gabesullice 8 hours agoparentSuper intriguing. Thanks for sharing! It reminds me a bit of an early Go project called Upspin [1]. And also a bit of Solid [2]. Did you take any inspiration from them? What excites me about your project is that you're addressing the elephant in the room when it comes to data sovereignty (~nobody wants to self-host a personal database but their personal devices aren't publicly accessible) in an elegant way. By storing the data on my personal device and (presumably?) paying for a managed relay (and maybe an encrypted backup), I can keep my data in my physical possession, but I won't have to host anything on my own. Is that the idea? [1] https://upspin.io/ [2] https://solidproject.org/ reply b_fiive 8 hours agorootparentAhBy storing the data on my personal device and (presumably?) paying for a managed relay (and maybe an encrypted backup), I can keep my data in my physical possession, but I won't have to host anything on my own. Is that the idea? We're hoping to give that exact setup to app developers (maybe that's you :). We still have work to do on encryption at rest to keep the hosted server \"dumb\", and more plumbing into existing app development ecosystems like flutter, expo, tauri, etc. but yes, that's the hope. Give developers tools to ship apps that renegotiate the \"user social contract\". reply hot_gril 12 hours agoparentprevIs it named after the Avatar character? reply joshspankit 12 hours agorootparentYes, and that makes me happy every time I see it. reply b_fiive 12 hours agorootparentprevI can neither confirm nor deny, but oh boy does uncle iroh seem cool reply nikisweeting 1 hour agoprevIs there anything that allows one to mount an IPFS dir as a read/write FUSE drive yet? Once they have that, I'm all in, even if it's slow... reply ianopolous 46 minutes agoparentWe have a FUSE mount in Peergos[0][1] (built on IPFS). It lets you mount any folder read or write (within your access). [0] https://github.com/peergos/peergos [1] https://peergos.org reply koito17 13 hours agoprev> it‚Äôs quite an inconvenience to run your own IPFS node. But even if you do run your own node, the fact you access a website doesn‚Äôt mean you pin it. Not at all. This has always been my major UX gripe with IPFS. The fact that `ipfs add` in the command line does little but generate a hash and you need to actually pin things in order to \"seed\" them, so to speak. So \"adding a file to IPFS\", in the sense of \"adding a file to the network\", requires the user to know that (1) the \"add\" in `ipfs add` does not add a file to the network, and (2) you must pin everything you want to replicate manually. I remember as recently as 2021 having to manually pin each file in a directory since pinning the directory does not recursively pin files. Doing this by hand for small folders is okay, but large folders? Not so much. More importantly, the BitTorrent duplication problems that IPFS has solved are also solved in BitTorrent v2, and BitTorrent v2 IMO solves these problems in a much better way (you can create \"hybrid torrents\" which allows a great deal of backwards compatibility with existing torrent software). This isn't a UX issue, but another thing that makes it hard for me to recommend IPFS to friends is the increasing association with \"Web3\" and cryptocurrency. I don't have any strong opinions on \"Web3\", but to many people, it's an instant deal-breaker. reply hot_gril 12 hours agoparentIPFS provides nice stable links to media, and there are HTTP->IPFS gateways if needed. That seems useful for embedding content on multiple apps/sites. Yeah it happens to fit NFTs particularly well, then again we all know what BitTorrent is known for. And yes I agree IPFS has some UI problems. Would BitTorrent also be suitable for hosting embeddable content? I haven't seen that yet. A magnet URL is mainly a file hash and doesn't seem to encode a particular peer server, kinda like IPFS. But every time I've torrented Ubuntu, it's taken half a minute just to find the peers. reply mvdtnz 12 hours agorootparent> IPFS provides nice stable links to media Anyone who has tried to torrent an old movie or lesser-known television show knows this is simply not true. reply hot_gril 12 hours agorootparentI mean it's not like HTTP where all URLs are tied to a particular webserver and can even be changed on that server. If someone different starts seeding, you'll get the same data again at the same URL, with built-in checksumming. reply adamzochowski 11 hours agorootparentThat is something I wanted to know, does IPFS guarantee that same two files have same two IPFS URLs / hash links? Otherwise, someone sharing same data again, because it will be in different IPFS folder won't be actually discoverable as same data. reply hot_gril 11 hours agorootparentYes, a file's hash is only based on its contents. The way I understand it, a file doesn't really live in a directory, it's more like a directory (which is a kind of file itself) references files. So the same file can be in two directories, yet it'll have the same URL/hash. And if you \"add\" files to a directory, you're really uploading a separate copy of the dir that'll have a different hash. I checked myself on this, but someone else might want to check me cause I'm not an expert. reply wuiheerfoj 10 hours agorootparentThis is generally true, though it‚Äôs possible to encode the same data into a slightly different shaped DAG to optimise for eg video streaming performance afaiu (balanced vs imbalanced). UnixFS vs raw bytes may also be different but I‚Äôm not 100% reply hot_gril 10 hours agorootparentFrom the fs's point of view, these are different file contents. But yeah, there's nothing stopping you from pinning something different that looks the same to a person. reply mikegreenberg 8 hours agorootparentprevCaveat: The other comments mention the file's contents being the only dependency on the hash, but the algo used to hash would also need to be the same. If the hash algo changes in two cases, the same content would have a different hash in those two cases. reply hot_gril 8 hours agorootparentIn this case, would pinning the file make it accessible from either hash? I'd expect it to, but idk, I've only ever seen sha256 hashes on IPFS. reply jaccarmac 8 hours agorootparentKinda. Shooting from the hip based on fuzzy gatherings from IPFS usage here, but as I understand it: The leaf-level data blocks will be shared between the Merkle trees, but at least the tip (the object a given hash actually refers to) and maybe some of the other structural information will be different. reply fwip 10 hours agorootparentprevYes, IPFS hashes individual \"blocks\" (pieces) of files. If two files have the same content, they will share block hashes. reply ranger_danger 6 hours agorootparentprevBasically it depends on specific settings that can be changed in the client as to how the individual block pieces are encoded and therefore what the resulting hash ends up being. So no there's no inherent guarantee but you may get lucky with some copies of the same file. reply rakoo 10 hours agorootparentprev> Would BitTorrent also be suitable for hosting embeddable content? Same as IPFS: gateways can exist. It's not specific to Bittorrent, or IPFS. > A magnet URL is mainly a file hash and doesn't seem to encode a particular peer server, kinda like IPFS. Magnet links can include a HTTP server that also hosts the content reply hot_gril 10 hours agorootparentI'm sure a BitTorrent gateway can exist, but I'm wondering why it doesn't seem to \"be a thing.\" I've never seen one used, nor do I see an obvious public one when searching. Whereas IPFS gateways are so mainstream that even Cloudflare runs a public one. reply flashm 12 hours agoparentprev‚Äòipfs add‚Äô pins the file by default, not sure if that‚Äôs recent behaviour though. reply lindig 13 hours agoprevFilecoin, which is based on IPFS, creates a market for unused storage. I think that idea is great but for adoption it needs to be as simple as Dropbox to store files. But visit https://filecoin.io/ and the dropbox-like app that you could be willing to try is nowhere to be found. So maybe it is an enterprise solution? That isn't spelled out either. So I am not surprised that this has little traction and the article further confirms the impression. reply diggan 12 hours agoparent> to be as simple as Dropbox to store files. But visit https://filecoin.io/ and the dropbox-like app that you could be willing to try is nowhere to be found I agree with this fully. But as said elsewhere, it's kind of far away from that, and also slightly misdirected. Imagine asking someone to get started with web development by sending them to https://www.ietf.org/rfc/rfc793.txt (the TCP specification). Filecoin is just the protocol, and won't ever solve that particular problem, as it's not focused on solving that particular problem, it's up to client implementations to solve. But the ecosystem is for sure missing an easy to use / end-user application like Dropbox for storing files in a decentralized and secure way. reply poorman 12 hours agoparentprevThat flagship app you are looking for seems to be https://nft.storage/ (by Protocol Labs). reply chrisco255 5 hours agoparentprevFileverse is an app built on ipfs and it is very user friendly: https://fileverse.io/ reply ahmedfromtunis 11 hours agoparentprevThis is, in my opinion, is the first and only \"solution\" to a real problem built using the blockchain. Distributed file storage, if done correctly, can be a transformative technology. And it can be even more revolutionary implemented at the OS level. reply nickstinemates 12 hours agoparentprevthis is what storj.io does. reply pierat 13 hours agoparentprevHere's your $.10/day for that 1GB with bandwidth... but running the filecoin stack will cost you a $50/mo server. That fucker's a PIG on cpu and ram. reply kkielhofner 11 hours agorootparentIPFS is as well. Clearly much more going on but take a machine that can serve 10k req/s with [insert 100 things here] without flinching and watch it maybe, just maybe, do 10 with IPFS. I'm not kidding. reply hirako2000 1 hour agoprevOn the need to run a node, i have a little project to wrap the static site content with an IPFS node in JS. E.g there is already helia. https://github.com/ipfs/helia Just waiting for running a node on the browser tab to become insignificant, resource wise. reply p4bl0 13 hours agoprevI'm surprised by the beginning of the post talking about pioneering in 2019. Maybe it is the case for ENS (I never cared for it), but regarding IPFS, my website was available over IPFS 3 years before that in 2016 [1]. Granted, I was never IPFS only. I also started publishing a series of article about censorship-resistant internet (Tor, I2P, IPFS, and ZeroNet) in 2600 Magazine ‚Äì The Hacker Quarterly ‚Äì back in 2017 [2]. Anyway, I came to the same conclusion as the author, but several years ago: in the end, nothing is actually decentralized, and maintaining this illusion of decentralization is actually costly, for no real purpose (other than the initial enjoyment of playing with a new tech, that is). So I stopped maintaining it a few years ago. That decision was also because of the growing involvement of some of these projects with blockchain tech that I never wanted to be a part of. This is also why I cancelled my article series in 2600 before publishing those on IPFS and ZeroNet. [1] See for example this archive of my HN profile page from 2016 with the link to it: https://web.archive.org/web/20161122210110/https://news.ycom... [2] https://pablo.rauzy.name/outreach.html#2600 reply chaxor 11 hours agoparentI never fully understood the use of ipfs/iroh for websites, but I really like the idea for data caching in data science packages. It makes me more sense to me that someone would be much more willing to serve large databases and neural network weights that they actually use everyday, rather than 'that one guys website they went to that one time'. I'm very surprised it's not as popular, if not more popular to just have @iroh-memoize decorators everywhere in people's database ETL code. That's a better use case (sense the user has a vested interest in keeping the data up) than helping people host we sites. reply wharvle 10 hours agorootparentIMO the case for something like IPFS gets worse and worse the larger proportion of clients are on battery. This makes it a really poor choice for the modern, public Web, where a whole lot of traffic comes from mobile devices. Serving things that are mostly or nigh-exclusively used by machines connected to the power grid (and, ideally, great and consistent Internet connections) is a much better use case. reply kmeisthax 7 hours agorootparentThis is half the reason why P2P died in the late 2000s. Mobile clients need to leech off a server to function. The other reason why it died is privacy. Participating in a P2P network reveals your IP address, which can be used to get a subscriber address via DMCA subpoenas, which is how the RIAA, MPAA, and later Prenda Law attacked the shit out of Gnutella and BitTorrent. Centralized systems don't usually expose their users to legal risk like P2P does. I have to wonder: how does IPFS protect people from learning what websites I've been on, or have pinned, without compromising the security of the network? reply marcus_holmes 5 hours agorootparentSpotify killed music sharing, not the RIAA. There's still plenty of video and book pirating happening. Until the streaming industry gets its shit together and coalesces into a single provider, or allows peering, then that's going to continue. The legal and privacy risks of P2P are both mitigated very simply with a VPN. reply mjevans 2 hours agorootparentThey also need to just sell a 'license for a \"personal private viewing copy\"' of a work and provide non-DRM files that users can self archive and maintain. No, DRM is not necessary, it's already proven that someone, among the 8 billion monkeys (with some really smart ones) hammering away _will_ figure out a way of liberating the data from the shackles. The whole premise is fundamentally broken in that the viewers are distrusted from seeing the data in the clear. It just adds cost, friction, and failure points. Convenience (EASE OF USE!!!), a fair price, and content that doesn't go away are how alternative distribution methods die. Just low how bootleg booze largely doesn't exist outside of prohibition since the market functions. reply zer00eyz 1 hour agorootparent>> Just low[sic] how bootleg booze largely doesn't exist outside of prohibition since the market functions. Tell me that you hang out with law abiding citizens without telling me... Moonshine, home brew... people are out there sticking it too the man as much as they can. If you have made home made cider, or beer, or yogurt, pickles, canned anything you know that its a labor but the product is better and far cheaper than what you can buy. Convenience, quality, ease of use... People will pay a massive premium for these things. This (to this dismay of HN) is the Apple model. You can bleed the customer if they love you, if you have a good product. This was a problem in early film, and the paramount decree was a thing: https://www.promarket.org/2022/12/12/the-paramount-decrees-a... One would think that this should apply to streaming services, but sadly no, they get treated like network television did (does). And I know that one of you will glom on to the paramount decree as an argument for the iPhone App Store shenanigans of late. Sadly they aren't remotely close to each other Apple isnt restricting your timing, or telling you what your price should be. reply apitman 2 hours agorootparentprevThey had a single provider. They purposefully moved away from that model to make more money, and it's working. reply BlueTemplar 7 hours agorootparentprevHow did it \"die\" in the late oughties, when FAIs were boasting about releasing routers with built-in NAS with torrent support in 2011, and projects like Popcorn Time only got popular in 2014 ? reply wuiheerfoj 10 hours agorootparentprevDesci Labs (1) do something like this - papers and all the associated data (raw data, code, analyses, peer review comments) are published and linked together - along with all the citations to other papers - in a big DAG. I believe their data is stored in a p2p - it might interest you! 1. https://desci.com/ reply r3trohack3r 12 hours agoparentprev> Anyway, I came to the same conclusion as the author, but several years ago: in the end, nothing is actually decentralized, and maintaining this illusion of decentralization is actually costly, for no real purpose (other than the initial enjoyment of playing with a new tech, that is). Do you have any writing (blog posts, HN comments, etc.) where you explore this thought more? I'm in the thick of building p2p software, very interested in what you came to know during that time. reply p4bl0 11 hours agorootparentThe main thing is that \"true\" (in the sense of absolute) decentralization does not actually work. It doesn't work technically, and it doesn't work socially/politically. We always need some form of collective control over what's going on. We need moderation tools, we need to me able to make errors and fix them, etc. Centralized systems tend to be authoritarians, but the pursue of absolute decentralization always end up being very individualistic (and some kind of wrongly placed elitism). There are middle grounds: federated systems for example, like Mastodon or emails, actually work. That is not to say that all p2p software is bad, especially since we call p2p a lot of things that are not entirely p2p. For example, BitTorrent is a p2p software, but its actual usage by humans relies on multiple more-or-less centralized point, trackers and torrent search engine. reply Almondsetat 11 hours agorootparentprevYour software cannot be more decentralized than your hardware. For example, true p2p can only happen if you meet with someone and use a cable, bluetooth or local wifi. Anything over the internet needs to pass through routers and *poof* decentralization's gone and you now need to trust servers to a varying level of degrees reply colordrops 11 hours agorootparent\"varying\" is a pretty wide range here. If you mean \"trust\" as in trust to maintain connectivity, yes, but beyond that there are established ways to create secure channels over untrusted networks. Could you provide specifics about what you mean if anything beyond basic connectivity? reply Almondsetat 11 hours agorootparentSure: what is the process to start downloading a torrent? what is the process to message someone on Jami? what is the process to call someone on (old) Skype or Jitsi? Answer this and you realize you can only get as decentralized as your hardware infrastructure reply Charon77 7 hours agorootparentWell for torrent it starts by contacting one of the various trackers that know other peers. It's not centralized but there's only a couple of trackers out there. There's no trust between any of the peers, but each of the torrent piece has associated hash, meaning peers cannot give you invalid data without being caught (unless hash collision occurs). Peers can be discovered with DHT magic, but ultimately, can only be dialed if the ISP allows peers to receive connections. reply Barrin92 4 hours agorootparentprev>Anything over the internet needs to pass through routers and poof decentralization's gone That's not true. Yes the strict p2p connection is gone but decentralization is what the name says, a network of connections without a single center. The internet and it's routing systems are decentralized. Of course every decentralized system can also be stressed to a point of failure and not every node is automatically trustworthy. reply pphysch 11 hours agorootparentprevTrue P2P networks don't scale, because every node has to store an (accurate if partial) representation of the whole network. Growing the network is easy, but growing every single node is virtually impossible (unless you control them all...). Any attempt to tackle that tends to increase centralization, e.g. in the form of routing authorities. And if you try to tackle centralization directly (like banning routers or something), you will often create an anti-centralization regulator, which is, you guessed it, another form of centralization. So your decentralized P2P network is either small and works good, medium and works not so good, or large and not actually decentralized. The best P2P networks know their limits and don't try to scale infinitely. For human-oriented network, Dunbar's Number (N=~150) is a decent rule of thumb; any P2P network larger than that almost certainly has some form of centralization (like trusted bootstrapping/coordination server addresses that are hard-coded in every client install, etc.) reply KMag 11 hours agorootparent> True P2P networks don't scale, because every node has to store an (accurate if partial) representation of the whole network Former LimeWire dev here... which P2P networks use a fully meshed topology? LimeWire and other Gnutella clients just have a random mesh with a fixed number of (ultra)peers. If the network gets too large, then your constrained broadcast queries hit their hop count before reaching the edge of the network, but that seems fine. Last I checked, Freenet used a variation on a random mesh. Kademlia's routing tables take O(log(N)) space and traffic per-peer to maintain (so O(N log(N)) for global total network space and traffic). Same for Chord (though, twice as much traffic due to not using a symmetric distance metric like XOR). There are plenty of \"True\" (non-centralized) P2P networks that aren't fully meshed. reply sanity 11 hours agorootparentCreator of Freenet here. Freenet[1] relies on peers self-organizing into a small world network[2]. Small world networks have the advantage of being able to find data in log N time where N is the network size, they're also completely decentralized, self-healing, and distribute load evenly across peers. The principle is similar to DHTs like Kademlia but more flexible and intuitive IMO, while having similar scaling characteristics. It's surprisingly common for people to confuse small world networks with \"scale free networks\", but scale free networks rely on a subset of highly connected peers which do a disproportionate amount of the work - which isn't truly decentralized. The new Freenet design incorporates adaptive learning into the routing algorithm. When a peer is deciding where to route a message, it predicts the response probability and time for each neighboring peer based on past performance and chooses the best. With conventional \"greedy routing\", peers choose the neighbor with a location closest to the data being retrieved. The new approach is adaptive to actual network performance. [1] Both the original Freenet from 1999 and the new sequel we're currently building - see https://freenet.org/ for more. We hope to launch the network in the next few weeks. [2] https://en.wikipedia.org/wiki/Small-world_network reply KMag 10 hours agorootparentThanks for the great work, Ian! As far a second-generation Freenet, I heard i2p started out as a proposed re-factoring and generalization of Freenet's encrypted transport layer. Are there any plans on using i2p to carry Freenet traffic? reply sanity 10 hours agorootparentThank you :) I2P was created by someone who was previously involved with Freenet, but its design is a lot closer to Tor than to Freenet. Both I2P and Tor are anonymizing proxies, they allow services to be hidden, but they're still centralized. While they are quite different, there is enough overlap that running Freenet over I2P (or Tor) would be wildly inefficient and slow, so I wouldn't recommend it. Freenet is designed to run over UDP directly. The new Freenet is designed to allow the creation of completely decentralized services. Briefly, it's a global key-value store in which keys are webassembly code that specify what values are permitted under that key, and the conditions under which those values can be modified. This key-value store is observable, so anyone can subscribe to a key and be notified immediately if the value changes. This is just scratching the surface, for anyone interested in a much more comprehensive explanation of the new Freenet please see this talk I gave a few months ago: [1] You can also find a FAQ here: [2] [1] https://www.youtube.com/watch?v=yBtyNIqZios [2] https://freenet.org/faq reply babymode 10 hours agorootparentprevI think ive been following the dev chat long enough to answer that the new freenet is a new, seperate network to the original (now called hyphanet I think) that handles transport by itself, and end to end encryption is not in scope of the project but can be built on top reply sanity 10 hours agorootparent> the new freenet is a new, seperate network to the original This is correct - while old and new Freenet both rely on a small-world network, they are very different and not compatible. Borrowing from our FAQ[1], the main differences are: Functionality: The previous version of Freenet (now called Hyphanet) was analogous to a decentralized hard drive, while the current version is analogous to a full decentralized computer. Real-time Interaction: The current version allows users to subscribe to data and be notified immediately if it changes. This is essential for systems like instant messaging or group chat. Programming Language: Unlike the previous version, which was developed in Java, the current Freenet is implemented in Rust. This allows for better efficiency and integration into a wide variety of platforms (Windows, Mac, Android, MacOS, etc). Transparency: The current version is a drop-in replacement for the world wide web and is just as easy to use. Anonymity: While the previous version was designed with a focus on anonymity, the current version does not offer built-in anonymity but allows for a choice of anonymizing systems to be layered on top. [1] https://freenet.org/faq#faq-2 reply BlueTemplar 6 hours agorootparentDoesn't Java have the widest variety of hardware running it, thanks to its Virtual Machine ? I can even remember my Motorola Razr being arguably (almost) a smartphone because, while a far cry from Symbian, it could already run Java applications ! (Notably, IIRC, Opera mini ?) P.S.: Also, I tried Freenet about around that time too ! I'm a bit confused about this being a \"new\" project... why not naming it \"Freenet 2\" then ? Why did Freenet \"1\" had to change its name ?? reply sanity 5 hours agorootparent> Doesn't Java have the widest variety of hardware running it, thanks to its Virtual Machine ? Java has the advantage that you can run it on a wide variety of hardware platforms without recompilation, but it has largely failed to attain broad usage/support for desktop apps and so it's a bad choice for something like Freenet in 2024. A systems programming language like Rust gives us a lot more control over things like memory allocation, allowing apps to be a lot more efficient. This is important with Freenet because we need it to run in the background without slowing down the user's computer. Rust can also be compiled to run on all major platforms, Windows, Mac, Linux, Android, iOS, etc. > P.S.: Also, I tried Freenet about around that time too ! I'm a bit confused about this being a \"new\" project... why not naming it \"Freenet 2\" then ? Why did Freenet \"1\" had to change its name ?? Using the name for the new software was a difficult decision and not without risk. The \"Freenet\" name was never intended to belong to a specific codebase. From the start we viewed the original Java implementation as a prototype, which is one reason we never actually released version 1.0 (even 7 years after the project started we were still on version 0.7). At the time I had no idea that it would be over 20 years before I had a design I thought would be suitable, but here we are. This new Freenet is the original concept but designed, not as a prototype, but as software that can gain broad adoption. In that sense it is the fulfilment of my original vision. I did consider calling it Freenet 2, but these days not that many people have heard of the original, so on balance I believe it would have been confusing for the (hopefully) much bigger userbase we hope to reach. reply Charon77 7 hours agorootparentprevI've always taken interest into p2p, but never heard of Freenet so thanks for being here! Question: how good is the latency once connections are already established, say for a real time video call over Freenet, or if this is not possible? Is there any server in the middle that all packets need to route to, especially for peers behind firewall reply sanity 4 hours agorootparent> how good is the latency once connections are already established, say for a real time video call over Freenet, or if this is not possible? We're aiming for the delay between a contract's state being modified and all subscribers being notified to be no more than 1 second, which should be acceptable for applications like IM. If you were doing something like a video call you'd negotiate it over Freenet and then establish a direct connection for the video/audio for minimal latency. > Is there any server in the middle that all packets need to route to, especially for peers behind firewall Freenet uses UDP hole-punching to establish direct connections between peers even if both are behind firewalls. A new peer uses a public (non-firewalled) peer to join the network initially but once joined all further communications can be between firewalled peers. reply dtaht 10 hours agorootparentprevClever. I care about congestion control issues mainly. Got that handled? Tried ecn? reply sanity 10 hours agorootparentThe low-level transport is one of the final components we're working on prior to launch - we previously hoped to use something off-the-shelf but on close examination nothing fit our requirements. We handle congestion by specifying a global maximum upload rate for the peer, which will be a fraction of the peer's total available bandwidth - the goal being to avoid congestion. In the future we'll likely use an isotonic regression to determine the relationship between upstream bandwidth usage and packet loss, so that we can adaptively choose an appropriate maximum rate. This is a more detailed explanation of the transport protocol, it's quite a bit simpler than ECN but we'll refine it over time: [1] At a higher level a peer's resource usage will be proportional to the number of connected peers, and so the peer will adaptively add or remove connections to stay under resource usage limits (which includes bandwidth). [1] https://github.com/freenet/freenet-core/blob/186770521_port_... reply Uptrenda 9 hours agorootparentprevFreenet is very cool. You did good work. Absolute giga chad. reply sanity 9 hours agorootparentThank you, first time I've been called a Chad - I'll take it ;) reply pphysch 10 hours agorootparentprevSure, but ultrapeers/supernodes/routers/etc are forms of centralization. Locally, the network is centralized around these supernodes, and they can be annoying/impossible to bypass. The \"inner network\" or backbone of supernodes, if exists, can also represent a central authority. Nothing necessarily wrong with any of this, but it can stretch the meaning of P2P if it really means P2central-authority2P. Functionally there is almost no difference between me sending you an (anonymous, encrypted) message over Facebook versus over some sophisticated, large, hierarchical \"P2P\" network. We still have to trust the local authorities, so to speak. reply unethical_ban 5 hours agorootparentprevI'll add to what others have said better. Decentralized, globally accessible resources still take some kind of coordination to discover those resources, and a shit-ton of redundant nodes and data and routing. There is always some coordination or consensus. At least, that's my take on it. Does not tor have official records for exit and bridge nodes? reply int_19h 12 hours agoparentprevI may be missing something, but name resolution has been touted as one of the more legitimate and sensible uses for blockchain for a very long time. Could you clarify what your issues with it in IPFS context are? reply edent 11 hours agorootparentIt isn't. Unless you want a long incomprehensible string. Someone is always going to want a short, unique, and memorable name. And when two people share the same name (McDonald, Nissan, etc) there needs to be a way to disambiguate them. If people die and are unable to release a desirable name, that just makes the whole system less desirable. I know one of the canonical hard problems in Computer Science is \"naming things\" and this is a prime example! reply bombcar 11 hours agorootparentAnd if you want a long incomprehensible string we already have that .onion sites work without a blockchain, too. reply null0pointer 9 hours agorootparentprevENS (which is what the GP refers to) has human readable names. But it doesn't have support for A/AAAA records today (does anyone know why? A-record support was in the original spec). Aside from that the only reason you wouldn't be able to type \"mycoolsite.eth\" into your browsers URL bar and have it resolve to an IP is because no browser has an ENS resolver built in. Yet. reply dazaidesu 7 hours agorootparentBrave does right? reply patmorgan23 7 hours agorootparentprevNamecoin has existed for a long time. It acts just like a traditional domain registar. First person to register a name gets it, and they have to pay a maintenance fee to keep it. Don't pay the maintenance fee and then someone else can register the name. reply acdha 6 hours agorootparentYes, but statistically nobody uses it due to those problems. Squatters quickly snapped up the most popular DNS names but since nobody uses it there‚Äôs no financial benefit from paying Danegeld, and that‚Äôs a vicious cycle of irrelevance. This is the core flaw of most of these systems: people hype them up selling the idea that it‚Äôll become huge later, but without some link to real value or authority there‚Äôs no reason to favor one implementation over another or doing nothing at all. This comes up a lot in the case of IPFS because the core problem is that most people won‚Äôt host anything on the internet. It‚Äôs expensive and legally risky, which forces you to vet who you host and then you have a less revolutionary pitch which has to be based on cost, performance, etc. and that‚Äôs a tough market. reply int_19h 3 hours agorootparentIt might not be popular in general, but surely IPFS crowd specifically would be a lot more receptive to such a thing? IPFS itself is similarly niche. reply p4bl0 11 hours agorootparentprevWell, I do not actually believe that blockchains can do name resolution correctly. First and foremost, the essential thing to understand about blockchains is that the only thing that is guaranteed by writing an information on a blockchain is that this information is written on this blockchain. And that's it. If the writing itself is not performative, in that it's mere existence performs what it describes, then nothing has been done. It works for crypto-assets because what makes a transaction happen is that it is written on the blockchain where that crypto-asset lives and where people look to see what happened with that crypto-asset. But for any other usage, it cannot work, blockchain are useless. Someone or something somewhere has to make sure either that what's written on the blockchain corresponds to the real world, or to make the real worlds corresponds to what's written on the blockchain. Either way you need to have a kind of central authority, or at least trusted third parties. And that means you don't actually need a blockchain in the first place. We have better, more secured, more efficient, less costly alternatives to using a blockchain. Back to name resolutions. Virtually no one is going to actually host locally the blockchain where all names are stored. That would be way too big and could only get bigger and bigger, as a blockchain stores transactions (i.e., diffs) rather than current state. So in practice people and their devices would ask resolvers, just like they currently do with DNS. These resolvers would need to keep a database of the state of all names up-to-date because querying a blockchain is way too inefficient, running such a resolvers would be a lot more costly than running a DNS servers so there would be less of them. Here we just lost decentralization which was the point of the system. But that's just a technical problem. There is more: what if someone gets a name and we as a society (i.e., justice, whatever) decides that they should not be in control of it? Either we are able to enforce this decision and it means the system is not actually decentralized (so, we don't need a blockchain), or we can't, and that's a problem. What if a private key is lost, the associated names are gone forever? What if your private key is leaked by mistake and someone hostile take control of your name? Using a blockchain for names resolution doesn't actually work, not for a human society. reply null0pointer 9 hours agorootparent> Either way you need to have a kind of central authority, or at least trusted third parties. Not everyone needs to run a node, and not everyone could, but it is totally feasible for an individual to run their own if they decide they can't trust anyone else for whatever reason. Especially if you were running a node specifically for the purpose of name resolution you could discard the vast, vast majority of data on the Ethereum blockchain (for example). > what if someone gets a name and we as a society (i.e., justice, whatever) decides that they should not be in control of it? [...] and that's a problem. No, that is a feature of a decentralized system. Individual node operators would be able to decide whether or not to serve particular records, but censorship resistance is one of the core features of blockchains in the first place. > What if a private key is lost, the associated names are gone forever? The association wouldn't be gone, it would just be unchangeable until it eventually expires. This is a known tradeoff if you are choosing ENS over traditional domain name registration. > What if your private key is leaked by mistake and someone hostile take control of your name? As opposed to today where someone hostile, like for instance the Afghani government (The Taliban), can seize your domain for any reason or no reason at all? --- I think we just have a fundamental disagreement about what types of features and use cases a name resolution system should have. That's completely fine, you're entitled to your own believes. You can use the system that most closely resembles your beliefs, and I'll use the one that most closely resembles mine. Fortunately for us different name resolution system can peacefully coexist due to the nature of name mappings. At least for now, none that I know of collide in namespace. reply CaptainFever 6 hours agorootparent> No, that is a feature of a decentralized system. Individual node operators would be able to decide whether or not to serve particular records, but censorship resistance is one of the core features of blockchains in the first place. Exactly, take a look at Sci Hub or The Pirate Bay continuously needing to change domain names due to seizures, for example. I'd want them to be able to actually own their domain names, either via blockchain or private key (e.g. Tor). In fact Sci Hub tried HNS for some time but seems to have dropped out of it. reply mikegreenberg 9 hours agorootparentprev> Either way you need to have a kind of central authority, or at least trusted third parties. You lost me here. Couldn't the local user ('s process) reference the same block chain instead of another trusted party? reply delfinom 9 hours agorootparentThe problem with block chains is you need the entire history, or at least a good chunk of it to walk it for data. The latest end of the chain doesn't contain everything, it simply contains the most recent transactions. This can be hundreds of gigabytes if not more at scale. This is where the central authority comes in play, in the name of storage and performance efficiency. Even crypto wallet apps use third party central servers to query your wallet totals. Because you aren't fitting the download of the block chain on your phone. reply mikegreenberg 9 hours agorootparentI don't think the blockchain walk has to be done locally. Much like someone looking up DNS records don't need to participate in the DB management to get their records, there can be intermediaries which provide the response and still rely on the blockchain as a source of truth? The value of the blockchain (in the context of name resolution) would (should) be limited to enabling trustless-ness of the response. I can cryptographically authenticate the origin of the response. If you don't trust that source, zk proofs would enable the user to validate the response is part of the latest version of the blockchain's state without looking at all of the history. I think the cost of carrying the whole history is a red herring. reply p4bl0 2 hours agorootparent> there can be intermediaries which provide the response and still rely on the blockchain as a source of truth But then you have to trust the intermediaries. You can verify their claim, but doing so is so costly it's what made you turn to intermediaries in the first place. > I can cryptographically authenticate the origin of the response. A blockchain is not needed for that, certificates can do that. > zk proofs (‚Ä¶) the latest version of the blockchain's state (‚Ä¶) cost of carrying the whole history Knowing enough information about the latest version of a blockchain's state to validate responses would require either that you trust the third party which would provide the hash of the last block to you, or that you follow, blocks after blocks, what's added to to ledger, verifying each block's integrity and all. I'm not saying that's not doable, but that it either requires some boot-up time or to be online all the time; i.e., it more or less amounts to running a node which is what we seem to agree is not something most people / end devices will do. reply heinrich5991 7 minutes agorootparentYou should be able to cryptographically prove a) the block height of the current block and b) the state of any account, in constant space, using zero-knowledge proofs. You don't need to trust a third party and do not need to be online all the time for that. hexage1814 4 hours agorootparentprev>There is more: what if someone gets a name and we as a society (i.e., justice, whatever) decides that they should not be in control of it?[...]or we can't, and that's a problem That's a feature. reply lottin 3 hours agorootparentDo you think lawlessness is a feature? reply ShamelessC 11 hours agorootparentprev> but name resolution has been touted as one of the more legitimate and sensible uses for blockchain for a very long time. Blockchain enthusiasts have a history of talking out of their ass and being susceptible to the lies of others. reply ShamelessC 8 hours agorootparentDownvotes, nice. Whatever helps you sleep at night. reply teeray 3 hours agoparentprev> I also started publishing a series of article about censorship-resistant internet (Tor, I2P, IPFS, and ZeroNet) in 2600 Magazine ‚Äì The Hacker Quarterly ‚Äì back in 2017 I very much enjoyed your articles on Tor and I2P :) I2P was entirely new to me, so I found that particularly interesting. I did idly wonder when the next article was coming, so I‚Äôm glad I didn‚Äôt just miss it in some issue. Totally understand where you‚Äôre coming from. reply p4bl0 2 hours agorootparentThanks! It's always great to have feedback on paper-published content :). reply neiman 13 hours agoparentprev> Maybe it is the case for ENS Oh yeah, I was referring strictly to IPFS+ENS websites. I have been working with it for several years so my mind goes for this use-case automatically. reply DEDLINE 11 hours agoparentprevWhen evaluating use-cases where blockchain technology is leveraged to disintermediate, I came to your same conclusions. Technically novel? Yes, sure. But, for what? reply hanniabu 11 hours agorootparentFor incentive alignment, consensus, trustless, etc reply axegon_ 12 hours agoprevI see where the author is coming from but I find something else strange: Considering that the blog is in practice a collection of static files, I don't see the benefit of paying for a server at all. Host it on github, if github gets killed off for whatever reason, switch to something else and move on. Seems like an unnecessary overhead to me. reply neiman 12 hours agoparentI get told that a lot! xD My original aim was to write an IPFS blogging engine for my personal use, so I needed some dynamic loading from IPFS there. Now I switched to Jekyll, and it would be easier to host the blog on Github indeed, but I'm kind of playing a quixotic game of trying to minimize the presence of Google/Microsoft/Amazon and other big-tech in my life. reply walterbell 9 hours agorootparentFree tier of indie https://neocities.org supports static sites like Jekyll. reply rapnie 54 minutes agorootparenthttps://codeberg.page .. similar idea to Github Pages. reply hot_gril 12 hours agoparentprevSame. IPFS seems far more useful for hosting static content that might be embedded in multiple websites. reply MenhirMike 13 hours agoprev> the more readers it has, the faster it is to use it since some readers help to spread the content (Scalable). In other words: Once a few big websites are established, no small website will ever be able to gain traction again because the big websites are simply easier to reach and thus more attractive to use. And just like an unpopular old torrent, eventually you run out of seeders and your site is lost forever. One can argue about the value of low traffic websites, but I got to wonder: Who in their right mind thinks \"Yeah, I want to make a website and then have others decide if it's allowed to live\". Then again, maybe that kind of \"survival of the fittest\" is appealing to some folks. As far as I am concerned, it sounds like a stupid idea. (Which the author goes into more detail, so that's a good write up) reply fodkodrasz 12 hours agoparentThis is a false dilemma. Why would you not \"seed\" (pin) your own site, and be at others' mercy? You pin it, and when others also do so, the readers get faster and more redundant service. reply kimixa 12 hours agorootparentFor \"unpopular\" sites having a single origin somewhat removes the advantages of IPFS, it's not decentralized, not censorship resilient, and still costs the publisher for ongoing infrastructure to host it. Yet still had the disadvantages and complexity of IPFS vs a static http server. So if you're not going to be publishing something that will always have multiple copies floating around, why use IPFS? reply fodkodrasz 12 hours agorootparent1. to give a chance to avoid being slashdotted. 2. to allow anybody who finds it valuable to archive it, or parts of it? The complexity of IPFS is another thing, which should be solved. However popular or unpopular your site might be, you must host is somewhere somehow, if you wish to be sure it sticks around. It is simple as that. reply cle 11 hours agorootparentprevIt helps to use more specific terms than \"decentralized\" and \"censorship resilient\", there are a lot of attack vectors for both. IPFS certainly does address some of the attack vectors, but not all. For example if the \"centralized\" thing you're worried about is DNS and certificate authorities, then you can avoid those authorities entirely with IPFS. Replication is one aspect of centralization, and IPFS doesn't completely fail at it, it's just more expensive (you can guarantee two node replication, you will have to run two nodes though). And there are other aspects not addressed by IPFS at all like its reliance on IP address routing infrastructure. reply p4bl0 12 hours agorootparentprevIf you need to pin your content anyway, it's actually faster and less expensive to host a normal website then. And if you want to get it to readers faster, there are a lot a cheap or free CDN available, but that's generally not even an issue with the kind of website we're talking about here when they're served normally, over the web. reply fodkodrasz 12 hours agorootparentYes, that is the state of affairs now. I can use cloudfront for my site, but cannot use it to pin my ipfs site (should I have one) as far as I know. You are fighting a strawman. If you don't take of your site, but expect others to take care of it (pin it), then it is not your site. You must ensure it has at least one pinned version. Others might o might not pin it, it depends on the popularity, or the accessibility of the stack, which is lacking right now according to the article. reply lelandbatey 12 hours agoparentprevIt's not up to others alone; you get a say too because you can seed your own content and that can be fast. In the worst case of no interest, then it's approximately the same as you hosting your own website in the world of today. This doesn't exonerate the shortfall of the \"old torrent\" pattern though, as you say. reply alucart 12 hours agoprevI'm exploring a similar project, having a \"decentralized\" website (hosted on github) which saves users' data in the blockchain itself and then provides that same data to other users through public APIs and/or from actual blockchain clients. Wonder if there is actual use or need for such thing. reply zubairq 2 hours agoprevI did a similar thing on a project of mine where I used to use IPFS as the only long term storage layer on a project of mine. I still use IPFS but now I use it more as \"long term unreliable storage\". Note that I use IPFS without Filecoin or any external pinning services, instead chose to pin the content from my own server on regular basis. The current status is that I plan to bring back IPFS usage more in the future for my project, but will wait for the ecosystem to mature a bit more first with regards to libraries. reply mawise 10 hours agoprevWhat about a cross between IPFS/Nostr and RSS? RSS (or atom) already provides a widely-adopted syndication structure. All that's missing are signatures so that a middleman can re-broadcast the same content. Maybe with signatures that's really reinventing SSB[1]. But if we think of the network in a more social sense, where you're connecting to trusted peers (think: irl friends), maybe the signatures aren't even that important. All that's left then is to separate the identifier for the feed from the location--today those are both URL--so you can fetch a feed from a non-authoritative peer. [1]: https://en.wikipedia.org/wiki/Secure_Scuttlebutt reply deephire 10 hours agoprevAny thoughts on the services that make hosting a blog on IPFS easier? Services like https://dappling.network, https://spheron.network, https://fleek.co, etc? I've seen some DeFi protocols use IPFS to add some resiliency to their frontends. If their centralized frontend with vercel or whatever is down, they can direct users to their IPFS/ENS entrypoint. reply mbgerring 12 hours agoprevDid they ever address the issue with IPFS where running a node necessarily required you to distribute an unknowable amount of pieces of material you may not want to be hosting (like CSAM, for example)? reply treyd 9 hours agoparentThat has never been an issue. You only seed what you choose to. It's basically the same model as BitTorrent but with about 15 fewer years of R&D behind it and much less organic user adoption. reply tempaccount1234 11 hours agoprevAs a user I‚Äôd stay away from sharing IPFS because of legal reasons. Just like torrenting, by actively distributing content I take up legal responsibility (at least in Europe where I‚Äôm located) - that‚Äôs risk is tolerable for a legal torrent because the file doesn‚Äôt change over time. For a changing web site, I‚Äôd constantly have to monitor the site for changes or trust the site 100% - which is not happening as soon as the site is somewhat controversial‚Ä¶ reply wyck 11 hours agoprevI build a blog in IPFS, its basically reliant on several centralized services to actually work in browsers (DNS , GitHub, Fleek, etc). I wrote about how I build it here, the experaince was underwhelming. https://labcabin.eth.limo/writing/how-site-built.html reply shp0ngle 12 hours agoprevYeah this is exactly my experience with IPFS. Nobody actually uses IPFS directly, and even those few that do never actually pin anything because it's an extra step. (Also I heard it's computationally costly, but I am not sure if it's true, I can't imagine why it would be the case actually.) As a result it's actually more centralised than web, there are like 3 pinning services that everyone uses. At which point I don't get the extra hoops. reply fsiefken 10 hours agoprevPerhaps use the DAT p2p network as an alternative? https://kovah.medium.com/publishing-a-static-website-to-the-... reply geokon 1 hour agoprev\"This is a huge difference from BitTorrent where the only way to get content is to run your own software, and when you download something you also share it, by default.\" As far as I understand this isn't a solved technical problem - but mostly a cultural quirk and probably just due to how the early torrent clients were configured There is for instance a major Chinese torrent client (that name escapes me) that doesn't seed by default - so the whole thing could have easily not worked. If IPFS clients don't seed by default then that kinda sounds like either a design mistake or a \"culture problem\" I've always wondered if there was a way to check if a client is reseeding (eg. request and download a bit from a different IP) and then blacklist them if they provide the data (or throttle them or something) reply wruza 1 hour agoparentThey probably don‚Äôt seed by default for a good reason. While torrents aren‚Äôt inherently political unlike signal and others, their culture is close to ‚Äúlegal ussues‚Äù. As a seeder I respect that and that‚Äôs why I‚Äôm seeding to high ratios. For every seeder of a specific piece of content there are 10x (20x? 50x?) more people who cannot share it back. But if you want to fence them off, you can use private trackers with ul/dl ratio accounting. reply filebase 12 hours agoprevDid you try IPNS Names? - https://filebase.com/blog/unveiling-names-a-dive-into-ipns-o... reply neiman 11 hours agoparentOh yes, I even had a free IPNS pinning service (for community members mostly) built with a friend. https://dwebservices.xyz/ reply schmichael 13 hours agoprev> I pinned content from my own server and played forever with settings and definitions, but couldn‚Äôt get the content to be available everywhere. > The blog you‚Äôre reading now is built with Jekyll and is hosted on my own 10$ server. > don‚Äôt get me wrong, I‚Äôm still an IPFS fanboy. ...how could you still be a fanboy? When IPFS cannot fulfill even the most basic function of globally serving static content, why does it deserve anyone's interest? It's not even new or cutting edge at this point. After 8 years of development how can the most basic functionality still not work for even an expert? reply neiman 13 hours agoparentIt worked fine before for many years when it was slightly less popular. They're having growing pains due to scalability problems and some libraries, like Helia (the JS library of IPFS), being new. I guess I'm also quite stubborn in wanting to do it my way, without the aid of any services, and for the content I pin to be available in all places, including Helia in the browser. reply p4bl0 13 hours agorootparentThe official IPFS client in Go has always been very, very hungry for resources. At some point it crashed because it needed to many file descriptors if it ran for too long. Even for a simple static site with infrequent update it needed some maintenance. But even if one was willing to put the effort in, it was not actually rewarded, because if your server were you pin your website is offline, the truth is that your site is offline too, so what's the point? reply heipei 13 hours agoparentprevIt's working fabulously for hosting phishing websites frontend by the popular IPFS gateway providers like Cloudflare, so at least there's that... reply withinboredom 13 hours agoparentprev[wrong thread] reply TheRealPomax 13 hours agorootparentWhat on earth are you talking about? IPFS has nothing to do with \"coins\", it's a distributed data management system. You can use IPFS for finance in the same way you can use a database for finance. You can also use IPFS for literally anything else that falls in the \"data that you want other people to be able to access\" category. reply withinboredom 12 hours agorootparentoh shit, I replied to the wrong comment. reply dannyobrien 10 hours agoprevI'm not sure quite how relevant it is to Neiman's work, but this is a pretty interesting blog post on decentralized web apps, and the tradeoffs with using various versions of IPFS in the browser: https://blog.ipfs.tech/dapps-ipfs/ reply ChrisArchitect 12 hours agoprevAside: visited this curious as to what the Nieman Journalism Lab (https://www.niemanlab.org/) was doing with IPFS if anything. Not the nicest near-collision naming move. reply fractalnetworks 5 hours agoprevyup ipfs literally doesnt work, why else did they need to do an ico and introduce centralized indexers... reply anacrolix 9 hours agoprevFor a BitTorrent based take on IPFS: https://GitHub.com/anacrolix/btlink reply yieldcrv 13 hours agoprev [‚Äì] I host all the static files of my Netlify and Vercel servers on IPFS it is simple enough and free even on hosted solutions, and it keeps my Netlify and Vercel free during spikes in traffic but the availability issue is perplexing, just like OP encountered some people just randomly wont be able to resolve some assets on your site, sometimes! the gateways go up and down, their cache of your file comes and goes. browsers dont natively resolve ipfs:// uris. its very weird. reply indigodaddy 12 hours agoparent [‚Äì] ‚Äú and it keeps my Netlify and Vercel free during spikes in traffic‚Äù ‚Äî how exactly would this help re: potentially breaching outbound transfer limits? reply yieldcrv 12 hours agorootparent [‚Äì] if static assets arent being requested from their server then it would not contribute to your bandwidth meter reply indigodaddy 11 hours agorootparent [‚Äì] You still have to transfer that data through the network pipe to the end user no? How the server itself accesses the files to do so seems irrelevant to me. reply Drblessing 10 hours agorootparent [‚Äì] I think if the assets are served in the html from ipfs gateway, then it doesn't add to your outbound traffic. Also once browsers support natively ipfs:// static content the game changes and the IPFS party really gets started. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The author explains their decision to move their blog from IPFS (InterPlanetary File System) to a traditional server.",
      "They faced challenges with IPFS, including a reliance on gateways instead of personal nodes and difficulties in updating and making content available in browsers.",
      "Despite still believing in IPFS's potential, the author found that it did not meet their personal blogging needs."
    ],
    "commentSummary": [
      "IPFS (InterPlanetary File System) is a topic of discussion in app development ecosystems, with users praising its stability and reliability for hosting files.",
      "Some concerns are raised regarding resource consumption and scalability issues with IPFS.",
      "The conversation also delves into alternative decentralized storage solutions and the limitations of blockchain for name resolution, highlighting the benefits and challenges of using decentralized technologies for hosting websites and sharing files."
    ],
    "points": 251,
    "commentCount": 137,
    "retryCount": 0,
    "time": 1706731622
  },
  {
    "id": 39204314,
    "title": "Macaroons: Enhanced Security and Flexibility in Token-based Authentication",
    "originLink": "https://fly.io/blog/macaroons-escalated-quickly/",
    "originBody": "Author Name Thomas Ptacek @tqbf @tqbf Image by Annie Ruygt We‚Äôre Fly.io and we transmute containers into VMs, running them on our hardware around the world. We built a new security token system, and can I tell you the good news about our lord and savior the Macaroon? 1 Let‚Äôs implement an API token together. It‚Äôs a design called ‚ÄúMacaroons‚Äù, but don‚Äôt get hung up on that yet. First some throat-clearing import sys import os import json import hmac as hm from base64 import b64encode, b64decode from hashlib import sha256 def hmac(k, v): return hm.new(k, v, sha256).digest() def enc(x): return b64encode(x) def dec(x): return b64decode(x) . Then: def blank_token(uid, key): nonce = enc(\":\".join([str(uid), os.urandom(16)])) return json.dumps([nonce, enc(hmac(key, nonce))]) Bearer tokens: like cookies, blobs you attach to a request (usually in an HTTP header). We‚Äôre going to build a minimally-stateful bearer token, a blob signed with HMAC. Nothing fancy so far. Rails has done this for a decade and a half. There‚Äôs a fashion in API security for stateless tokens, which encode all the data you‚Äôd need to check any request accompanied by that token ‚Äì without a database lookup. Stateless tokens have some nice properties, and some less-nice. Our tokens won‚Äôt be stateless: they carry a user ID, with which we‚Äôll look up the HMAC key to verify it. But they‚Äôll stake out a sort of middle ground. def attenuate(macStr, cav): mac = json.loads(macStr) cavStr = json.dumps(cav) oldTail = dec(mac[-1]) newTail = enc(hmac(oldTail, cavStr)) return json.dumps(mac[0:-1] + [cavStr, newTail]) m0 = blank_token(10, keys[10]) m1 = attenuate(m0, {'path': '/images'}) m2 = attenuate(m1, {'op': 'read'}) Let‚Äôs add some stuff. The meat of our tokens will be a series of claims we call ‚Äúcaveats‚Äù. We call them that because each claim restricts further what the token authorizes. After {'path': '/images'}, this token only allows operations that happen underneath the /images directory. Then, after {'op': 'read'}, it allows only reads, not writes. (I guess we‚Äôre building a file sharing system. Whatever.) Some important things about things about this design. First: by implication from the fact that caveats further restrict tokens, a token with no caveats restricts nothing. It‚Äôs a god-mode token. Don‚Äôt honor it. In other words: the ordering of caveats doesn‚Äôt matter. Second: the rule of checking caveats is very simple: every single caveat must pass, evaluating True against the request that carries it, in isolation and without reference to any other caveat. If any caveat evaluates False, the request fails. In that way, we ensure that adding caveats to a token can only ever weaken it. With that in mind, take a closer look at this code: oldTail = dec(mac[-1]) newTail = enc(hmac(oldTail, cavStr)) Every caveat is HMAC-signed independently, which is weird. Weirder still, the key for that HMAC is the output of the last HMAC. The caveats chain together, and the HMAC of the last caveat becomes the ‚Äútail‚Äù of the token. Creating a new blank token for a particular user requires a key that the server (and probably only the server) knows. But adding a caveat doesn‚Äôt! Anybody can add a caveat. In our design, you, the user, can edit your own API token. def verify(macStr, keys): mac = json.loads(macStr) nonce = dec(mac[0]).split(\":\") key = keys[int(nonce[0])] tail = \"\" for cav in mac[:-1]: tail = hmac(key, cav) key = tail return hm.compare_digest(tail, dec(mac[-1])) verify(m2, keys) # => True For completeness, and to make a point, there‚Äôs the verification code. Look up the original secret key from the user ID, and then it‚Äôs chained HMAC all the way down. The point I‚Äôm making is that Macaroons are very simple. 2 Back in 2014, Google published a paper at NDSS introducing ‚ÄúMacaroons‚Äù, a new kind of cookie. Since then, they‚Äôve become a sort of hipster shibboleth. But they‚Äôre more talked about than implemented, which is a nice way to say that practically nobody uses them. Until now! I dragged Fly.io into implementing them. Suckers! We had a problem: our API tokens were much too powerful. We needed to scope them down and let them express roles, and I scoped up that project to replace OAuth2 tokens altogether. We now have what I think is one of the more expansive Macaroon implementations on the Internet. I dragged us into using Macaroons because I wanted us to use a hipster token format. Google designed Macaroons for a bigger reason: they hoped to replace browser cookies with something much more powerful. The problem with simple bearer tokens, like browser cookies or JWTs, is that they‚Äôre prone to being stolen and replayed by attackers. game-over: pentest jargon for ‚Äúvery bad‚Äù Worse, a stolen token is usually a game-over condition. In most schemes, a bearer token is an all-access pass for the associated user. For some applications this isn‚Äôt that big a deal, but then, think about banking. A banking app token that authorizes arbitrary transactions is a recipe for having a small heart attack on every HTTP request. (Perfectly minimized API tokens: a software security holy grail) Macaroons are user-editable tokens that enable JIT-generated least-privilege tokens. With minimal ceremony and no additional API requests, a banking app Macaroon lets you authorize a request with a caveat like, I don‚Äôt know, {'maxAmount': '$5'}. I mean, something way better than that, probably lots of caveats, not just one, but you get the idea: a token so minimized you feel safe sending it with your request. Ideally, a token that only authorizes that single, intended request. 3 That‚Äôs not why we like Macaroons. We already assume our tokens aren‚Äôt being stolen. In most systems, the developers come up with a permissions system, and you‚Äôre stuck with it. We run a public cloud platform, and people want a lot of different things from our permissions. The dream is, we (the low-level platform developers on the team) design a single permission system, one time, and go about our jobs never thinking about this problem again. Instead of thinking of all of our ‚Äúroles‚Äù in advance, we just model our platform with caveats: Users belong to Organizations. Organizations own Apps. Apps contain Machines and Volumes. To any of these things, you can Read, Write, Create, Delete, and/or Control control being change of state, like ‚Äústart‚Äù and ‚Äústop‚Äù . Some administrivia, like expiration (ValidityWindow), locking tokens to specific Fly Machines (FromMachineSource), and escape hatches like Mutation (for our GraphQL API). (this is a vibes-based notation, don‚Äôt think too hard about it) Simplistic. But it expresses admin tokens: Organization 4721, mask=* And it expresses normal user tokens: Organization 4721, mask=read,write,control (App 123, mask=control), (App 345, mask=read, write, control) And also an auditor-only token for that user: Organization 4721, mask=read,write,control (App 123, mask=control), (App 345, mask=read, write, control) Organization 4721, mask=read (our deploy tokens are more complicated than this) Or a deployment-only token, for a CI/CD system: Organization 4721, mask=write,control (App 123, mask=*) Those are just the roles we came up with. Users can invent others. The important thing is that they don‚Äôt have to bother me about them. 4 Astute readers will have noticed by now that we haven‚Äôt shown any code that actually evaluates a caveat. That‚Äôs because it‚Äôs boring, and I‚Äôm too lazy to write it out. Got an Organization token for image-hosting that allows Reads? Ok; check and make sure the incoming request is for an asset of image-hosting, and that it‚Äôs a Read. Whatever code you came up with, it‚Äôd be fine. These straightforward restrictions are called ‚Äúfirst party caveats‚Äù. The first party is us, the platform. We‚Äôve got all the information we need to check them. Let‚Äôs kit out our token format some more. def third_party_caveat(ka, tail, msg, url): crk = os.urandom(16) ticket = enc(encrypt(ka, json.dumps({ 'crk': enc(crk), 'msg': msg }))) challenge = enc(encrypt(tail, crk)) return { 'url': url, 'ticket': ticket, 'challenge' : challenge } key = bytes(\"YELLOWSUBMARINE\") url = \"https://canary.service\" c3 = third_party_caveat(tail, key, url, json.dumps({'user': 'bobson.dugnutt'})) m3 = attenuate(m2, c3) Up till now, we‚Äôve gotten by with nothing but HMAC, which is one of the great charms of the design. Now we need to encrypt. There‚Äôs no authenticated encryption in the Python standard library, but that won‚Äôt stop us. Ready to make some candy? Hand me that brake fluid! # do i really need to say that i'm not serious about this? def hmactr(k, n): ks = hm.new(k+n) for counter in xrange(sys.maxint): ks.update(str(counter)) kbs = ks.digest() for i in xrange(16): yield kbs[i] def encrypt(k, buf): ak = hm.new(k, 'auth').digest() nonce = os.urandom(16) cipher = hmactr(hm.new(k, 'enc').digest(), nonce) ctxt = bytearray(buf) for i in xrange(len(buf)): ctxt[i] ^= ord(cipher.next()) res = nonce + str(ctxt) return res + hm.new(ak, res).digest() def decrypt(k, buf): ak = hm.new(k, 'auth').digest() if not hm.compare_digest(buf[-16:], hm.new(ak, buf[:-16]).digest()): return False nonce = buf[:16] cipher = hmactr(hm.new(k, 'enc').digest(), nonce) ptxt = bytearray(buf[16:-16]) for i in xrange(len(buf[16:-16])): ptxt[i] ^= ord(cipher.next()) return str(ptxt) With ‚Äúthird-party‚Äù caveats comes a cast of characters. We‚Äôre still the first party. You‚Äôll play the second party. The third party is any other system in the world that you trust: an SSO system, an audit log, a revocation checker, whatever. Here‚Äôs the trick of the third-party caveat: our platform doesn‚Äôt know what your caveat means, and it doesn‚Äôt have to. Instead, when you see a third-party caveat in your token, you tear a ticket off it and exchange it for a ‚Äúdischarge Macaroon‚Äù with that third party. You submit both Macaroons together to us. Let‚Äôs attenuate our token with a third-party caveat hooking it up to a ‚Äúcanary‚Äù service that generates a notice approximately any time the token is used. To build that canary caveat, you first make a ticket that users of the token will hand to your canary, and then a challenge that Fly.io will use to verify discharges your checker spits out. The ticket and the challenge are both encrypted. The ticket is encrypted under KA, so your service can read it. The challenge is encrypted under the previous Macaroon tail, so only Fly.io can read it. Both hide yet another key, the random HMAC key CRK (‚Äúcaveat root key‚Äù). In addition to CRK, the ticket contains a message, which says whatever you want it to; Fly.io doesn‚Äôt care. Typically, the message describes some kind of additional checking you want your service to perform before spitting out a discharge token. def discharge(ka, ticket): ptxt = decrypt(ka, dec(ticket)) if ptxt == False: return False tbody = json.loads(ptxt) # not shown: do something with tbody['msg'] return json.dumps([ticket, enc(hmac(dec(tbody['crk']), ticket))]) To authorize a request with a token that includes a third-party caveat for the canary service, you need to get your hands on a corresponding discharge Macaroon. Normally, you do that by POSTing the ticket from the caveat to the service. Discharging is simple. The service, which holds KA, uses it to decrypt the ticket. It checks the message and makes some decisions. Finally, it mints a new macaroon, using CRK, recovered from the ticket, as the root key. The ticket itself is the nonce. If it wants, the third-party service can slap on a bunch of first-party caveats of its own. When we verify the Macaroon, we‚Äôll copy those caveats out and enforce them. Attenuation of a third-party discharge macaroon works like a normal macaroon. def verify_third_party(tag, cav, discharges=[]): crk = decrypt(tag, dec(cav['challenge'])) if crk == False: return False discharge = None for dcs in discharges: if json.loads(dcs)[0] == cav['ticket']: discharge = dcs break if not discharge: return False mac = json.loads(discharge) key = crk # boring old stuff --------------------- tag = \"\" for cav in mac[:-1]: tag = hmac(key, cav) key = tag return hm.compare_digest(tag, dec(mac[-1])) To verify tokens that have third-party caveats, start with the root Macaroon, walking the caveats like usual. At each third-party caveat, match the ticket from the caveat with the nonce on the discharge Macaroon. The key for root Macaroon decrypts the challenge in the caveat, recovering CRK, which cryptographically verifies the discharge. (The Macaroons paper uses different terms: ‚Äúcaveat identifier‚Äù or cId for ‚Äúticket‚Äù, and ‚Äúverification-key identifier‚Äù or vId for ‚Äúchallenge‚Äù. These names are self-evidently bad and our contribution to the state of the art is to replace them.) There‚Äôs two big applications for third-party caveats in Popular Macaroon Thought. First, they facilitate microservice-izing your auth logic, because you can stitch arbitrary policies together out of third-party caveats. And, they seem like fertile ground for an ecosystem of interoperable Macaroon services: Okta and Google could stand up SSO dischargers, for instance, or someone can do a really good revocation service. Neither of these light us up. We‚Äôre allergic to microservices. As for public protocols, well, it‚Äôs good to want things. So we almost didn‚Äôt even implement third-party caveats. 5 I‚Äôm glad we did though, because they‚Äôve been pretty great. The first problem third-party caveats solved for us was hazmat tokens. To the extent possible, we want Macaroon tokens to be safe to transmit between users. Our Macaroons express permissions, but not authentication, so it‚Äôs almost safe to email them. The way it works is, our Macaroons all have a third-party caveat pointing to a ‚Äúlogin service‚Äù, either identifying the proper bearer as a particular Fly.io user or as a member of some Organization. To allow a request with your token, you first need to collect the discharge from the login service, which requires authentication. The login discharge is very sensitive, but there isn‚Äôt much reason to pass it around. The original permissions token is where all the interesting stuff is, and it‚Äôs not scary. So that‚Äôs nice. Ben then came up with third-party caveats that require Google or Github SSO logins. If your token has one of those caveats, when you run flyctl deploy, a browser will pop up to log you into your SSO IdP (if you haven‚Äôt done so recently already). We‚Äôve put a bunch of work into getting the guts of our SSO system working, but that work has mostly been invisible to customers. But Macaroon-ized SSO has a subtle benefit: you can configure Fly.io to automatically add SSO requirements to specific Organizations (so, for instance, a dev environment might not need SSO at all, and prod might need two). SSO requirements in most applications are a brittle pain in the ass. Ours are flexible and straightforward, and that happened almost by accident. Macaroons, baby! Here‚Äôs a fun thing you can do with a Macaroon system: stand up a Slack bot, and give it an HTTP POST handler that accepts third-party tickets. Then: So, the bot is cute, but any platform could do that. What‚Äôs cool is the way our platform doesn‚Äôt work with Slack; in fact, nothing on our platform knows anything about Slack, and Slack doesn‚Äôt know anything about us. We didn‚Äôt reach out to a Slack endpoint. Everything was purely cryptographic. That bot could, if I sunk some time into it, enforce arbitrary rules: it could selectively add caveats for the requests it authorizes, based on lookups of the users requesting them, at specific times of day, with specific logging. Theoretically, it could add third-party caveats of its own. The win for us for third-party caveats is that they create a plugin system for our security tokens. That‚Äôs an unusual place to see a plugin interface! But Macaroons are easy to understand and keep in your head, so we‚Äôre pretty confident about the security issues. 6 Obviously, we didn‚Äôt write our Macaroon code in Python, or with HMAC-SHA256-CTR. We landed on a primary implementation Golang (Ben subsequently wrote an Elixir implementation). Our hash is SHA256, our cipher is Chapoly. We encode in MsgPack. We didn‚Äôt use the pre-existing public implementation because we were warned not to. The Macaroon idea is simple, and it exists mostly as an academic paper, not a standard. The community that formed around building open source ‚Äústandard‚Äù Macaroons decided to use untyped opaque blobs to represent caveats. We need things to be as rigidly unambiguous as they can be. The big strength of Macaroons as a cryptographic design ‚Äî that it‚Äôs based almost entirely on HMAC ‚Äî makes it a challenge to deploy. If you can verify a Macaroon, you can generate one. We have thousands of servers. They can‚Äôt all be allowed to generate tokens. What we did instead: We split token checking into ‚Äúverification‚Äù of token HMAC tags and ‚Äúclearing‚Äù of token caveats. Verification occurs only on a physically isolated token-verification service; to verify a token‚Äôs tag, you HTTP POST the token to the verifier. Clearing of token caveats can happen anywhere. Token caveat clearing is domain-specific and subject to change; token verification is simple cryptography and changes rarely. A token verification is cacheable. The client library for the token verifier does that, which speeds things up by exploiting the locality of token submissions. The verification service is backed by a LiteFS-distributed SQLite database, so verification is fast globally ‚Äî a major step forward from our legacy OAuth2 tokens, which are only fast in Ashburn, VA. Now buckle up, because I‚Äôm about to try to get you to care about service tokens. We operate ‚Äúworker servers‚Äù all over the world to host apps for our customers. To do that, those workers need access to customer secrets, like the key to decrypt a customer volume. To retrieve those secrets, the workers have to talk to secrets management servers. We manage a lot of workers. We trust them. But we don‚Äôt trust them that much, if you get my drift. You don‚Äôt want to just leave it up to the servers to decide which secrets they can access. The blast radius of a problem with a single worker should be no greater than the apps that are supposed to run there. The gold standard for approving access to customer information is, naturally, explicit customer authorization. We almost have that with Macaroons! The first time an app runs on a worker, the orchestrator code has a token, and it can pass that along to the secret stores. The problem is, you need that token more than once; not just when the user does a deploy, but potentially any time you restart the app or migrate it to a new worker. And you can‚Äôt just store and replay user Macaroons. They have expirations. This is like dropping privilege with things like pledge(2), but in a distributed system. So our token verification service exposes an API that transforms a user token into a ‚Äúservice token‚Äù, which is just the token with the authentication caveat and expiration ‚Äústripped off‚Äù. What‚Äôs cool is: components that receive service tokens can attenuate them. For instance, we could lock a token to a particular worker, or even a particular Fly Machine. Then we can expose the whole Fly Machines API to customer VMs while keeping access traceable to specific customer tokens. Stealing the token from a Fly Machine doesn‚Äôt help you since it‚Äôs locked to that Fly Machine by a caveat attackers can‚Äôt strip. 7 If a customer loses their tokens to an attacker, we can‚Äôt just blow that off and let the attacker keep compromising the account! This cancels every token derived through attenuation by that nonce. Every Macaroon we issue is identified by a unique nonce, and we can revoke tokens by that nonce. This is just a basic function of the token verification service we just described. We host token caches all over our fleet. Token revocation invalidates the caches. Anything with a cache checks frequently whether to invalidate. Revocation is rare, so just keeping a revocation list and invalidating caches wholesale seems fine. 8 I get it, it‚Äôs tough to get me to shut up about Macaroons. A couple years ago, I wrote a long survey of API token designs, from JWTs (never!) to Biscuits. I had a bunch to say about Macaroons, not all of it positive, and said we‚Äôd be plowing forward with them at Fly.io. My plan had been to follow up soon after with a deep dive on Macaroons as we planned them for Fly.io. I‚Äôm glad I didn‚Äôt do that, not just because it would‚Äôve been embarrassing to announce a feature that took us over 2 years to launch, but also because the process of working on this with Ben Toews changed a lot of my thinking about them. I think if you asked Ben, he‚Äôd say he had mixed feelings about how much complexity we wrangled to get this launched. On the other hand: we got a lot of things out of them without trying very hard: Security tokens you can (almost) email to your users and partners without putting your account at risk. A flexible permission system, encoded directly into the tokens, that users can drive without talking to our servers. A plugin system that users can (when we clean up the tooling) use themselves, to add things like Passkeys or two-person-approval rules or audit logging, without us getting in the middle. An SSO system that can stack different IdPs, mandate SSO login, and do that on a per-Organization basis. Inter-service authorization that is traceable back to customer actions, so our servers can‚Äôt just make up which apps they‚Äôre allowed to look at. An elegant way of exposing our own APIs to customer Fly Machines with ambient authentication, but without the AWS IMDSv1 credential theft problem. There are downsides and warts! I‚Äôm mostly not telling you about them! Pure restrictive caveats are an awkward way to express some roles. And, blinded by my hunger to get Macaroons deployed, I spat in the face of science and used internal database IDs as our public caveat format, an act for which JP will never forgive me. If i‚Äôve piqued your interest, the code for this stuff is public, along with some more detailed technical documentation. Last updated ‚Ä¢ Jan 31, 2024 Share this post on Twitter Share this post on Hacker News Share this post on Reddit Author Name Thomas Ptacek @tqbf @tqbf Next post ‚Üë Delegating tasks to Fly Machines Previous post ‚Üì How Yoko Li makes towns, tamagoes, and tools for local AI Next post ‚Üë Delegating tasks to Fly Machines Previous post ‚Üì How Yoko Li makes towns, tamagoes, and tools for local AI",
    "commentLink": "https://news.ycombinator.com/item?id=39204314",
    "commentBody": "Macaroons Escalated Quickly (fly.io)244 points by mkeeter 19 hours agohidepastfavorite158 comments tptacek 17 hours agoBack in 2021, when I wrote the API token survey post, I had some, uh, caveats about Macaroons, and linked to a talk from a developer at Chain who had gone through the experience of deploying them. Don't get me wrong, it's still an excellent talk‚Ä†, but after working on implementing them for a year or two I find it much less damning than I did at the time. One problem Chain seemed to run into was coupling between their services. They had a fast Golang \"ledger\" and a slow Rails \"dashboard\", and found that Macaroons forced all their requests to loop through the \"dashboard\". This was because they'd opted to do short-lived Macaroon tokens that needed to be reissued every 5 minutes (eminently sane), and only their dashboard could generate the proper token? Another problem they had with Macaroons: their Rails dashboard has an RBAC permissons interface. But once they've issued a Macaroon, users can't change permissions with the interface anymore. But that's a problem with all stateless tokens, not just Macaroons; in fact, Macaroons probably help here, because if you're presenting a Macaroon to the dashboard interface in the first place, the RBAC interface can just attenuate it for you and hand it back to you. I still think this is a design that mostly makes sense only if you have a problem domain for which both attenuation and delegation make sense. Most basic CRUD apps don't have these problems, and using Macaroons would be a mistake for them. ‚Ä† https://www.youtube.com/watch?v=MZFv62qz8RU reply ryukafalz 10 hours agoparent> I still think this is a design that mostly makes sense only if you have a problem domain for which both attenuation and delegation make sense. I would argue that supporting attenuation and delegation (almost?) always makes sense. It's very common that I find myself wishing I could grant some piece of software some but not all permissions to some resources; this can be relatively straightforward when the authorization scheme supports attenuation and delegation, but is painful when it doesn't. As you mention in the post, users often want to do things that the developer hadn't thought of - I'd argue that's still the case even for what you might say is a basic CRUD app. (Yes, this is related to the old ACL-vs-capability-model debate.) Now that said, I'm not saying Macaroons make sense for every use case - just that I think attenuation and delegation do. reply skybrian 7 hours agorootparentLimited-access tokens that you can give to others are often useful, but there are other ways to do it. If your application is a monolith with an API, you're going to have to implement all the permissions yourself anyway, right? That includes the UI for setting up the permissions and generating the cookie, the code for checking permissions, and the code for actually doing the work. Might as well keep the cookies very simple and put the permissions in the database. Once you have plugins with their own permissions, possibly running on their own servers, it starts to make more sense. reply leoqa 15 hours agoparentprevI'm curious what your thoughts are on implementing a sane authorization system in 2024. You mentioned writing roles onto macroons but what does your policy look like? Is your surface area simple enough that it's not a concern? I've been on various security teams with disjoint product-facing authz, internal authz and service authz policy engines / mechanisms etc. Additionally, authz gets baked into service code and product interfaces, so it's hard to change later. reply tptacek 14 hours agorootparentThe idea is that Macaroons are a low-level IAM language, designed close to the components that they pertain to, encoded directly into the tokens, and that higher-level IAM policies \"compile down\" into those tokens. reply leoqa 14 hours agorootparentIf you need more content, I would be keen to read a blog post on how you designed authz at Fly.io. I think it'd be a unique point of view, given your security background and ability to write well. reply dullcrisp 15 hours agoparentprevIn the third_party_caveat code sample, I think you mean to be passing in the tag rather than the url, and the arguments at the call site are in the wrong order. Edit: Or I guess along with the url. reply tptacek 11 hours agorootparentYou're right; I extracted that function from a gnarlier function I didn't want to use in the post (it was the same logic, but the function was so long it broke up the post too much) and wasn't careful enough in moving the code. Fixed, sort of! Thanks! Go easy on me; I literally wrote this in IDLE. reply djha-skin 10 hours agoprevMacaroons are a cookie[1], Macarons are a French dessert[2]. I have a pet peeve about this mix-up in particular, and they seem to have mixed them up, calling the French thing macaroons. 1: https://www.savoryexperiments.com/almond-coconut-macaroons/ 2: https://sallysbakingaddiction.com/french-macarons/ reply mtlmtlmtlmtl 6 hours agoparentIt's basically the same word though(they have the same etymology). In Norwegian they're called coconut and French \"makron\", respectively. reply danielvinson 4 hours agorootparentMacaroons are the coconut cookie. Macarons are the fluffy egg white ones. Very different things. reply mtlmtlmtlmtl 3 hours agorootparentBut the word macaroon is derived from macaron. Obviously they are not the same thing, I never said that, did I? reply justworkout 7 hours agoparentprevMacaroons and macarons are both cookies. They're both desserts as well. reply TheRealPomax 9 hours agoparentprevUltimately, of course, it doesn't really matter as long as the person selling them knows what you're asking for. Which they will. https://www.youtube.com/watch?v=nzcHeO43kgE reply hahajk 9 hours agorootparentWow, thank you. That was a real broccoli = kale moment for me. reply windlep 17 hours agoprevI remember when I saw a presentation by the macaroon authors a few years back, there were pending patents that Google filed around them. While the authors claimed Google wouldn't sue anyone, I'm always a bit skeptical about such claims. I thought macaroons would be helpful for some of my use-cases, but since I now knew there were patents that'd be wilful infringement so I didn't bother. I can't find the patents now, so perhaps they were rejected or withdrawn. I had assumed that was why macaroons hadn't caught on more widely. Edit: Found the patent: https://patents.google.com/patent/US9397990B1/ reply tptacek 14 hours agoparentThere are so many stupid patents out there about everything we could possibly work on, it is actually reassuring to see that Google is assigned to some of them, rather than to some storefront in Marshall, Texas. reply surajrmal 14 hours agoparentprevGoogle has an open pledge to not litigate open source uses of its patents: https://www.google.com/patents/opnpledge/pledge reply yencabulator 10 hours agorootparentMeanwhile, fly.io is not open source. Some of their software is, but not all of it. Same likely holds for the SaaS you're thinking of starting. reply diggan 12 hours agorootparentprevWhat does a \"open pledge\" like that realistically mean, in case they someday broke that pledge? Would the court-case 100% surely get thrown out? Am I legally protected because of this pledge? reply esafak 12 hours agorootparentJust look at their litigation record. They have been around for long enough to have a meaningful track record. reply diggan 9 hours agorootparent> Just look at their litigation record. I guess if one looks at those, it'll look like they won't sue you? But my hypothetical case is what about if they do sue you, does this actually protect you, or not? Does a \"pledge\" have enough meaning that it could change the outcome in a court case? reply bartonfink 7 hours agorootparentYes. reply lmm 9 hours agorootparentprevTheir track record in other places has shown a significant decline (perhaps related to DoubleClick completing their reverse-acquisition of them). reply 20after4 5 hours agorootparentThat's my take on the death of don't be evil. Doubleclick was the worst kind of company and the merger with google seems to have diluted enough evil into google that google's immune system failed to kill it. reply RussianCow 11 hours agorootparentprevThat doesn't mean things won't change in the future. reply windlep 11 hours agorootparentprevThe patents page (https://www.google.com/patents/opnpledge/patents/) does not include the macaroon patent. Maybe they forgot to update it? reply jimmyl02 16 hours agoparentprevmaybe this is similar to google's patenting of dropout for neural networks? you can never know but so far there haven't been many adverse effects and they claim that they patent it so others can't maliciously patent and enforce it. dropout patent link: https://patents.google.com/patent/US9406017B2/en reply windlep 16 hours agorootparentThat was what the authors claimed when I asked them about the macaroon patent. It'd be nice if Google had a legal document associated with patents they never plan to enforce, or the constraints around when they might enforce them (e.g. only against patent trolls) that a company could rely on. reply xyzzy_plugh 7 hours agorootparentI don't disagree but maintaining an arsenal of defensive parents is Enterprise IP legal 101. All the big companies do this. The goal is to avoid litigation by mutually assured destruction. At least that's what they tell you. Many projects grant you patents as part of, for example, an OSS project's license. It's not unusual to include a clause that voids any such terms of you litigate over your own parents, for example -- hence the mutually assured destruction. Can you imagine what would happen if Google and Facebook tried to duke out some dumb software patent in court? A waste for all parties. Lastly Google has a wealth of resources here: https://google.github.io/opencasebook/patents/#patents-in-op... reply everybodyknows 16 hours agoparentprevThe Pythonish pseudo-code, rendered to an image (figure 7) -- is that common nowadays? Though the patent I see is dated 2013. reply ijustwanttovote 13 hours agoprevSmall detail, there's a picture of macarons and not macaroons. A macaron is a sandwich-like cookie that's filled with jam, ganache, or buttercream. A macaroon is a drop cookie made using shredded coconut. reply arnarbi 13 hours agoparentMacaroon is just the English word for the French word macaron. ducks for cover reply beAbU 12 hours agorootparentMacaroon is a completely different confectionery: https://en.m.wikipedia.org/wiki/Macaroon reply IshKebab 12 hours agorootparentSure, but Macarons (the burger-looking things) are also sometimes known as \"macaroons\". Yeah. Examples: * https://missmacaroon.co.uk/ * https://www.floristgrays.co.uk/design-202300010/valentines-m... * https://www.parisiennesouthwell.com/product-page/macaroons-v... reply RussianCow 11 hours agorootparentThe fact that people say it doesn't make it any less wrong. :) reply recursive 11 hours agorootparentIf enough people do it for long enough, it does. After all, that's how we got the rest of the words. reply jashper 8 hours agorootparentThe joys and woes of ever-evolving language / culture To the persistent victor‚Ä¶ reply jameshart 6 hours agorootparentprevAnd Macaron is the French word for Macaroni. Think how that makes the Italians feel. reply Julesman 11 hours agoparentprevDon't let anyone try to convince you that up is down. Word was spelled wrong. Period. reply ramchip 1 hour agoprevI'm curious why the connection from Rails is \"mTLS-y\" rather than actual mTLS. The macaroon repo mentions a Noise transport. Maybe to avoid dealing with X.509 certificates by distributing trusted public keys via LiteFS? > We didn‚Äôt use the pre-existing public implementation because we were warned not to. [...] Macaroons decided to use untyped opaque blobs to represent caveats. We need things to be as rigidly unambiguous as they can be. Another place where the implementations are limiting is in the nonce for discharge macaroons: it has to match the \"challenge\" exactly. I think the fly.io implementation makes the nonce a structured object that includes the challenge / key ID as a field, and extracts that field during third party caveat verification, which is nice. It makes it possible to include a random nonce (in the cryptographic sense) or revocation ID for instance. I experimented recently with discharges containing service-specific info, e.g. a discharge from an auth service could contain the user's name (for logging), etc. It felt dangerous though, because there's potential for confusion on which service is allowed to provide what info - if we have a 3P caveat for an auth service and a revocation checking service, we want to be sure that the user profile info comes from the auth service, not the other one, and it gets complicated to encode that in the token. Maybe this is what \"proof\" macaroons solve? They're not mentioned in the blog post or macaroon-thought.md, but seemed to be about making positive statements on something. reply RustyRussell 10 hours agoprevCompulsory plug when someone uses macaroons: using an HMAC is overkill. See https://github.com/rustyrussell/runes for a simpler alternative and implementation (this has C and Python, but there's also a Rust implementation because why not?) However, the \"no db access\" property has proven to be untenable in practice. Users end up wanting to see what runes are issued, blacklist them, know when they were last used, and have rate limits. The last two are a killer, requiring some state to be kept (unless your system allows you to return a modified rune to the user, which is a different workflow from normal bearer creds). reply RustyRussell 10 hours agoparentOops, I read further: you did use unique ids, and blacklists! I did that too. Now you're not db-lookup-free on use, so you need to think hard about what all this actually gains :( reply sergioisidoro 18 hours agoprevSo, are Macaroons are like a \"blockchain\", where each block is an attenuation (ie reducing permission and scopes) of the permission of the original token? So a bearer can get a token, reduce its scope (without talking to the server) and pass it down to other less privileged entities. That's cool! I kind of had to look into the paper linked to really understand what they are. I feel the article could have given a bit more context on it :) reply dwaite 17 hours agoparentSomewhat. It is symmetric key based, so only the initial issuer can do verification. It is not part of a distributed data store, so there can be multiple copies (and derivatives) of a macaroon floating around. Basically, a trusted service creates an initial atom and shares it with the initial party. That trusted atom can be copied and shared. Anyone with a copy can amend additional information to it, and share those copies. Based on the copy you get, you can't remove or alter any information received. So it works best for representing restrictions - maybe my user agent gets a token that gives full access for a project. It shares a version with another service endpoint which is restricted to read-only access. That service endpoint sees it needs to kick off a background process, so it creates a version which also adds on a 4 hour time limit. None of these changes are necessarily authenticated - I don't know that it specifically was the user agent that shared the token with the service or added the read-only restriction, at least with the core tech. You have to create a domain language to specify all of these things. If you want restrictions to be authenticated, you need to describe how to sign them individually with PKI. While HMAC is fast, you still have the whole system rooted in a single trusted service that can mint any messages it wants or lie about the validity of a given message. So while it is indeed a chain of cryptographic elements, it is missing the multi-party validation and operation transparency which are the common selling points of distributed ledgers. reply arnarbi 13 hours agorootparent> None of these changes are necessarily authenticated - I don't know that it specifically was the user agent that shared the token with the service or added the read-only restriction, at least with the core tech. Macaroons are very much grounded in capability thinking, where it is a ‚Äúfeature‚Äù that principals such as the user agent in your case don‚Äôt need to have representable identities. So there‚Äôs nobody to authenticate, except the fact that it was a holder of the previous macaroons tag, which means they had the authority it represented. In other words, not needing something like a PKI is somewhat the point of it all. If you have the need to authenticate the intermediate principals, eg because they have pre-existing identities already, then the capability model in general may just be a distraction. They could just sign their desired attentuations with their own identity key. reply dwaite 3 hours agorootparentOne use of macaroons is to have caveats restricting the request itself used to achieve higher security than bearer tokens. This might be \"only good for this service\" audience restrictions or \"only good for this hashed request message\" as proof-of-possession. This creates a role division, where the user agent caveats before the proof-of-possession are potentially authorized differently than the ones after. It also creates the possibility that you will need to override caveats per policy, say when that message wasn't a terminal call but rather hitting an ingress to a complex internal services framework which would still like to leverage the macaroon. Basically - if a role has a requirement that they always terminate with a particular caveat, you know a section of caveats was created by software in that role. This is more of a second level of authorization than it is authentication though, I would agree. reply tptacek 16 hours agorootparentprevBiscuits have single roots of trust too. The (superficial) distinction is that Biscuits can be verified (completely) on systems that don't hold the root keys. You can't do that with a Macaroon, but, as we explain in this post, you don't have to. I don't think we'd meaningfully gain security from Biscuits --- though we would potentially get flexibility from Datalog, and our deployment story might have been a little bit simpler. (We already need hardware-isolated secret stores, so the deloyment win is probably marginal for us, but it's a bigger deal for other shops). Purely symmetric cryptography is one of the features of Macaroons. It's a reason cryptography hipsters like it, not a reason to dunk on it. reply dwaite 15 hours agorootparentYep, comparing SHA2-based macaroons and ED25519-based biscuits, you'll have size and computational performance advantages for macaroons as well as post-quantum resistance. But that doesn't help you if you want to solve the network performance and failure issues with verifying macaroons. There may be strategies to solve some of the biscuit disadvantages using different signature mechanisms like BLS, but that might not help with performance and will not currently get you post-quantum resistance. reply tptacek 14 hours agorootparentThe back half of this piece is largely about the network performance and deployment issues of Macaroons and how we addressed them. This is a real system, running in production; it isn't a proposal. reply k__ 16 hours agoparentprevAre macaroons similar to UCAN? https://ucan.xyz/ reply X-Istence 18 hours agoprevMacaroons are also implemented and used by pypi.org's implementation named Warehouse: https://warehouse.pypa.io/development/token-scanning.html Also see: https://pypitoken.readthedocs.io/en/latest/ reply woodruffw 17 hours agoparentYes, although PyPI doesn't currently do much attenuation or delegation with them (this is largely my fault, since I didn't fully understand their power when picking them for the implementation). That's been slowly changing, however -- as of a few months ago, PyPI issues slightly more compact API tokens that make better use of discrete caveats. They're also used on the Trusted Publishing[1] side to make the API token self-expiring. [1]: https://docs.pypi.org/trusted-publishers/ reply evancordell 13 hours agoprev> The community that formed around building open source ‚Äústandard‚Äù Macaroons decided to use untyped opaque blobs to represent candidates. I assume \"candidates\" was supposed to be \"caveats\" - and as an author of a \"standard\" macaroon implementation, I completely agree that this is the biggest downfall of Macaroons. With no common caveat language (and no independent \"dischargers\") it really limits their use to within a single org. And at that point you're basically asking everyone to invent their own token format anyway. Though I don't personally use them much anymore - I think the use-cases for Macaroons are much more limited if you have a Zanzibar! - I appreciate seeing Macaroon discussions pop up and this post and the related discussions it linked out to were a great read. reply xyzzy_plugh 7 hours agoparentZanzibar and macaroons are actually pretty complimentary. reply Arelius 16 hours agoprevThese look pretty useful cool... I'm curious a bit though, here, and in other posts in HN, I often see that HMAC, a symmetric key is preferred, going as far as suggesting, in JWT, that other algorithms should not be implemented. Why is that? What are the problems with say RSA? (Ignorning that I'm not sure Asymmetric keys work with Macaroons design at all) From my perspective, Asymmetric keys have been a great boon, in that I can keep the private key, secured on my single auth server, but then freely distribute the public key to the edge, greatly increasing responsiveness, and reducing the bottleneck on the Auth server. Is there some security concern I've been missing? reply jszymborski 18 hours agoprevThis post was written exceedingly well. Few posts execute humour and whimsy without coming off as insincere or just not very funny (I'm guilty myself). Bravo! reply robbles 16 hours agoparentThe tone is entertaining, but some of the snarkiness around the code is bit frustrating at times. e.g. > # do i really need to say I'm not serious about this? Can you just come out and say what you mean here? Like, presumably it's bad, insecure code, but can't you just spell it out for the benefit of your audience? I'm probably being really picky here - just find this kind of developer in-crowd signalling to be really irritating and counterproductive. reply tptacek 16 hours agorootparentThe Python code here is essentially just pseudocode. In reality, if you build a Python Macaroon implementation, you'd pull in pyca/cryptography and use an actual AEAD, rather than rolling your own authenticated cipher out of pure HMAC. But the point is that this isn't real code, just enough to make the concepts concrete. reply robbles 13 hours agorootparentThat's a great explanation, and pretty much exactly what I'd love to see instead of the original slightly mysterious comment :) reply mixmastamyk 4 hours agorootparentYes, you articulated what I think often when I read a tpt post. Enjoy the casual writing but feel like I‚Äôm frequently left out of the joke. I understand all these concepts in general but looking at a block of code‚Ä¶ I‚Äôm supposed to 1) know if it‚Äôs serious/not and 2) he‚Äôs being sarcastic about it not. I even know Python well but the whole post is so ‚Äúinside baseball‚Äù am not sure what to think. Guessing the bulk of the audience is not grizzled security engineers. (I‚Äôd recommend another summary/details tag to put the grandparent explanation directly into the fly post.) reply tptacek 2 hours agorootparentWhich parts seemed \"inside baseball\" to you? reply byproxy 14 hours agorootparentprevI can always appreciate that these types of stripped-down examples are merely for illustrative, conceptual purposes...but the ignoramus in me would also appreciate links to fleshed-out examples that take into account the shortcomings of the simpler example. reply tptacek 13 hours agorootparentOur actual Macaroon code is linked at the bottom of the article. reply cide1 16 hours agorootparentprevI studied that code and the comment for a good 10 minutes. As far as I can tell it just obfuscates, and is not actually implementing authenticated encryption. It would help to just come out and say that part out loud. reply tptacek 16 hours agorootparentTrue fact: Salsa20 is itself a hash function, keyed, and running in a counter mode. reply oneepic 14 hours agorootparentprev>I'm probably being really picky here - just find this kind of developer in-crowd signalling to be really irritating and counterproductive. Why is it \"developer in-crowd signalling\" rather than just a joke? IOW I read this as saying the jokes were \"as bad as\" virtue-signalling. reply robbles 13 hours agorootparentIt's a statement that won't make sense to someone who's new to the topic. I don't think it's related to virtue signalling at all, the two just share the word \"signalling\". reply michelpp 17 hours agoprevI like the \"solve the now\" perspective here, and having code examples is very helpful to understand some of the rational behind the approach. Having read your previous \"tedious survey\"[0] post on various token formats, I generally agree with a lot of your conclusions. Curious though about your thought process wrt macaroons vs biscuits. To me the one major downside of macaroons has always been the single shared root symmetric key. Many use cases are addressed by third party attenuation, but then there are the problems like key rotation, having to do online verification, no built in encryption, no peer-to-peer support through an \"untrusted\" fly.io, and no third party token verification without decryption like in signcryption[1] schemes. Of course this is traded off by having to do PK issuance and management so I can see the simplicity of it. Is fly.io scoping this pretty hard to just auth tokens with third party attenuation, or do you see further development and maybe moving to other token systems like biscuit when/if the need arises to address those known issues? fwiw I've done a bit of research work myself on a token format using signcryption [2] where I explored addressing some of these ideas (but not the attenuation side of it yet, which I get is a big deal here). [0] https://fly.io/blog/api-tokens-a-tedious-survey/ [1] https://github.com/jedisct1/libsodium-signcryption [2] https://github.com/michelp/pgsodium/blob/feat/signcryption-t... reply tptacek 17 hours agoparentWe're not going to use Biscuits. The \"single root symmetric key\" thing is a systems design challenge for us, not a cryptographic one: we simply isolate the key handling on a software-HSM-like verifier service, running on dedicated hardware. If you're coding authz logic for Fly.io's platform code, your experience will be that you can manipulate and check Macaroon caveats without ever having access to the keys themselves, because we split Macaroon-checking into \"verification\" and \"caveat clearing\". The threats in this design that would force us to do abrupt key rotations and mass token invalidation are thus pretty much the same as the ones we'd face with Biscuits; the \"trusted code base\" of key-handling code is comparably small. More detail here: https://github.com/superfly/macaroon/blob/main/macaroon-thou... reply matthewaveryusa 18 hours agoprev>and escape hatches like Mutation (for our GraphQL API). Can you explain in a bit more detail what you mean by escape hatches (sounds dangerously fun) reply tptacek 18 hours agoparentThe Organization -> Apps -> Volumes (whatever) structure models our problem domain and is a sort of coherent abstraction. If we get that abstraction wrong, it's a pain to retrofit changes. Caveats like \"Mutation\", on the other hand, are simply strings representing calls you can make in our API; they're not a model of anything. I don't think we use `Mutation` for anything right now, but it's there in case we (or you) ever need a token that authorizes a particular set of GQL calls that don't fit the model we came up with. reply xyzzy_plugh 18 hours agoprev> Until now! I dragged Fly.io into implementing them. Suckers! Let's be real: they knew you would do this eventually. It was inevitable. reply jzelinskie 18 hours agoprevIt's probably more relevant to your previous blog post that did a survey of technologies, but I'm curious if y'all ever considered a fully centralized approach. Is there something about Fly that benefits from a decentralized approach? Disclosure: I work on SpiceDB alongside maintainers like the pymacaroons author reply tptacek 18 hours agoparentCan you define \"centralized\" and \"decentralized\" for me? What would a \"fully centralized\" approach to this problem look like? reply jzelinskie 18 hours agorootparentSure thing! These terms may not be the best way to communicate these concepts. Would love your take on that, too. I used the term decentralized to describe claim-based token-passing authorization strategies because the data defining their access is being source from various services, attenuated, and passed around with the request. I use the term centralized to describe something where all the data defining access and the policies are sourced from one service. I personally work on a Google-inspired system, but there are plenty of policy engines using a similar approach. reply tptacek 17 hours agorootparentCouple thoughts. * With regards to the \"core Macaroons\" we implement with first-party caveats, we conceptually are centralized, in that there's a single source of truth about which apps belong to which orgs &c that caveats are evaluated against. * The basic challenge we have is providing a comparable experience in every data center we're operating in around the world, so a \"purely\" centralized model would be problematic for us; people in Sydney would get a much worse experience than people in Ashburn. We get around this with LiteFS, to an extent. This post undersells how important LiteFS is to the deployment model here. * With respect to having, like, a Zanzibar-like access matrix dealy: part of the thing here is not wanting to have to provide a single coherent permissions model to our users, because our problem domain is so broad and we will get it wrong. Providing orthogonal tools to let users express things seems like the better strategy to us? But there are upsides to both approaches. * Of course, one of the big wins here, that we have a plugin interface for authorization, depends on some measure of decentralization. I feel like this is kind of a scattershot response, but hopefully I hit some of what you were talking about, and you can let me know if I've missed the mark completely. reply alexissantos 7 hours agoprevOff topic, but related: The art you see on Fly posts? It's made by the inimitable Annie Rugyt: https://annieruygtillustration.com/ She's wonderful, and has a knack for making brands come alive with illustration. Remember the RethinkDB mascot? That was her too! reply tptacek 5 hours agoparentShe's a full-time member of the team. This is the least of the art she's queued up for the prodsec team's posts this quarter. :) Her big project right now is mascotizing Frankie the Balloon, for swag. reply wereHamster 17 hours agoprevI wonder why they went chose macaroons over biscuits. The later is only mentioned once in that article, but doesn't talk about why they were not selected. reply tptacek 17 hours agoparentI like Biscuits, wrote about them before, and recorded a podcast episode with Geoffroy Couprie about them‚Ä†. The short answers: * When we started working on this design, there was still some uncertainty about the elliptic curve chaining construction (as in: they hadn't, or maybe had just started to, work out which one they were going to use). * We do a lot of verifications, and the prospect of running lots of 25519 computations every time we have to check a token was daunting; by comparison, HMAC is essentially \"free\". * I'm not especially scared of Ed25519, but asymmetric signature cryptography is much more complex than computing a MAC. Like I said in the post, one of the great charms of Macaroons is how much mileage it gets out of a construction that everyone understands, and that doesn't have sharp edges. * Geoffroy sold me on the value of using a logic language to express caveats, but that was much more complicated than what we felt we needed. Another charm of Macaroons is that they're rigidly coherent; just an unordered collection of true/false predicates, loop over them and if any fail reject the request. I feel safe expressing things with Macaroon caveats --- not in the sense of \"the token implementation doesn't have bugs\", I'm sure Biscuit's Datalog is fine, but rather in the \"I'm not going to mess this up as a user\" sense. * We needed implementations in a bunch of different languages. Biscuits have them, but owning the implementation ourselves derisked things for us. We could have owned an implementation of Biscuits, but I'm not smart enough to do that well. ‚Ä† https://securitycryptographywhatever.com/2022/01/29/biscuits... reply crdrost 7 hours agorootparentOh my goodness -- I had this idea!! When I first read a blog post titled \"Macaroons are better than cookies\" I started thinking about the operational side and I really didn't like the fact that you had to make a network round-trip to some auth-service to validate the macaroon--because that's what the state of the art already was, and it kinda stunk to pay that delay on every single query. (Of course I understood that you could horizontally scale the machines doing auth and load-balance and all that, but I worked at smaller companies where you had a small rack in a colo space or an air-conditioned closet, creating VMs to do all of that ate into a relatively limited resource and made sysadmins raise eyebrows.) I tried for the longest time to put together a solution with hash functions and private-key crypto; I think the thing that bugged me the most was that I never was able to prove that you couldn't do it with those primitives. Eventually I had something similar with public-key crypto but at the time public-key usually meant RSA, so we were talking 1024-bit signatures for each step. Also I think the idea was that you'd furnish a zero-knowledge proof that you knew the key, which was hella complex. Two things then happened: first, if you're talking PKI then you're talking about revocation too; I convinced myself that revocation was necessary for these sorts of credentials (and I now always feel vindicated when places are like \"log out all other instances?\") but I also convinced myself that revocation required the network hop in any case (which is not true, revocation is actually pretty infrequent and you can push revoked serials to services via event bus and in most cases the latency to ingest those is not a huge issue). So I convinced myself that what I was looking for was ultimately a purely academic exercise, was the first thing. And the second was that I managed to simplify my PKI-plus-hashes-plus-zero-knowledge craziness to just what Biscuits are: \"I'm signing a public key plus caveats,\" (but I didn't add a query language, I was just going to use a map of well-known-keys to values, \"valid_before: 123...890\"), \"I'm going to transmit this to the recipient with its corresponding private key, which they will strip off and generate a new key to mint a new Biscuit.\" And then RSA was big and slow and I forgot all about this until this HN thread. Ed25519 is the perfect way to fix that -- that and distributed revocation. I still don't see myself using it in practice but it's nice that a library exists if I do... thanks for the walk down memory lane, I will queue up the podcast episode, maybe for tonight or tomorrow. reply camgunz 17 hours agoparentprevCan't speak for them, but I've heard people balk at the Datalog part (IMO not an issue, but I'm not everyone) reply fovc 17 hours agoprevSurprised to see JSON here. I recall discussing JWT with tptacek a few years ago, and one of the concerns was the {a: x, a: y} ambiguity in JSON parsers. Was that a concern with this design? My other reaction was that this sounds scary: > a token with no caveats restricts nothing. It‚Äôs a god-mode token. Don‚Äôt honor it. I guess it's technically \"fail closed\" but seems kind of brittle reply tptacek 17 hours agoparentI think it becomes more obvious later in the post that the Python code here is for illustrative purposes only. We don't use JSON. Our actual tokens are strictly typed. But also: it doesn't matter that much, because all of the predicates in a Macaroon are evaluated independently. There's no opportunity to confuse a previous caveat with a subsequent one. That's one of the strengths of having a rigidly coherent design like Macaroons, rather than just a bag of keys and values like JWTs do. reply fovc 16 hours agorootparentAh thanks for clarifying! I knew the code was illustrative but didn't realize the JSON part was too reply hinkley 16 hours agoparentprevThe XML digital signature spec had that problem in spades. It took us about five times as long to button it down as it did to implement. By the time I was done with that project, the document for partners and vendors (if anyone wanted to implement their own instead of using ours) was several times longer than the spec, what with all the extra MUST and MUST NOT situations. Which is not that hard when you're dealing with swiss cheese. Document.findById and Element.findById being able to return different (non-null) results being the most egregious one I can remember. reply dwaite 15 hours agorootparentXML is a disaster here all of its own, but is at least unambiguous at the parser level. JSON says \"if your document does this, parser behavior is undefined\". So you need to declare that the parser used should behave in particular ways. Both are best solved a robust specification and supplemental test vectors. If you define meta rules (e.g. JSON parsers must either fail or return the last object property), you still retain the value you hoped for by using something like JSON or XML in the first place. reply hinkley 11 hours agorootparentIsn't it Turing Complete with namespaces? (that's another thing on the SHALL NOT list). Can't be unambiguous if you don't halt. reply dwaite 3 hours agorootparentTuring complete? Not really, parsing still forms static data structures in linear time. The fundamental screw-up with XML is that it was a textual markup format (e.g. HTML or word processor documents) that got usurped for cross platform data transfer. Among many other problems, that put pressure on its design to be unambiguous to generalized tools - which wasn't an initial design consideration. So you had an awful lot of making people use document markup processing tools to serialize and parse data structures, schema and transform systems that tried to target both mindsets and did a bit of a poor job at each, and lots and lots of abstractions to try to make it so you only had to deal with the problem once. Even JSON is a bit of a pain the farther away from late-bound or untyped scripting languages you get. reply bethecloud 13 hours agoprevStorj is also leveraging macaroons. Great write-up on decentralized access control here: https://medium.com/@kleffew/what-is-capability-based-securit... reply xbar 11 hours agoprevI am not a customer, but I do like the fly.io \"voice.\" reply 8organicbits 17 hours agoprev> we want Macaroon tokens to be safe to transmit between users How does this work for auditing? If user A gives user B a token (perhaps after adding a caveat) how does this system audit log determine who did what? Does that require a third party system? reply tptacek 17 hours agoparentYou can get as many independent Macaroons from us as you want, so while there are ways to handle this with third-party caveats (and never talking to our servers to get another token again), the simpler thing for token provenance with our system is the same as it would be with any other: just issue multiple tokens. reply dwaite 17 hours agoparentprevThere is no centralized auditing of derivative macaroons, but there could be of verification of those macaroons - you still require a centralized service which knows the core HMAC secret to give a thumbs up or thumbs down. To that end you can define auditing information even when there aren't really caveats - user A appending that they are giving a version of the macaroon to user B with no restrictions on use. Since there isn't a domain language, you have to define all this. JSON can be useful to have an extensible notation for describing this, although you need to up-front declare the difference between information which can be ignored and caveats which much result in failure if not understood. reply tptacek 17 hours agorootparentNote also that in our scheme, if Alice attenuates a token and gives it to Bob, Bob will still have to hit our login service to \"activate\" the token (that's the word I should have used in the post and I'm kicking myself for not thinking to express it that way) by clearing the third-party auth caveat we put on every token. So it's not as if we have no auditing of token delegation. Still: if only for revocation purposes, I'd probably just mint a new token for e.g. a contractor (and certainly for every user). The nice thing about the system is that you can edit the tokens after we give them to you so you're not stuck with our role definitions, which I think is a significant win, but doing all of IAM without talking to our servers more than once, while a cute technical stunt, is probably not all that valuable to users. reply dwaite 3 hours agorootparent> Bob will still have to hit our login service to \"activate\" the token. I think thats what I was saying - on first verify your centralized system will know the contents of the token and can do appropriate auditing. In practice I find that keeping processing simple is a challenge. It doesn't naturally say as simple as one would like, with orthagonal caveats that can be processed independently and independent of centralized knowledge. For example, you might expect the user to indicate the system they are calling in a caveat, an audience value. However, if you either make this true a restriction on further use it will prevent any sort of delegation, which may include delegation to internal systems that the user didn't know about. You don't want to outright ignore it either. So you have some sort of policy interpretation of that restriction, such as 'this audience means this set of services can use this macaroon' or 'we will audit internal use based on this audience but take no action', or even crazier you color caveats before and after this point as being ones set by the user vs ones set by the service. This sounds a bit contrived, but it does loop back into one of the core usages I've seen for macaroons - gaining proof of possession or other sender verification rather than using bearer-style tokens. reply asplake 18 hours agoprevTypo/bug: pretty sure variable 'oldTag' should be 'oldTail' reply tptacek 18 hours agoparentThanks! They're basically interchangeable --- HMAC produces a \"MAC tag\", and in the context of the Macaroon protocol the last visible tag is called a \"tail\". But I'm guessing I screwed up editing the code when I switched everything to \"tail\". :) reply beeks 12 hours agoprevit seems to me that macaroons could get quite big in order to adequately describe allowed resources and permissions? Especially if the API is broad? Much bigger than JWTs. reply tptacek 12 hours agoparentThe resource descriptions are pretty parsimonious; they're binary-encoded MsgPack integers, for the most part. But the cryptography eats up a lot of bytes. They're bigger than JWTs, but probably by a factor less than 2 (I don't know the median JWT size). Basic take: unless you can golf your tokens down to a single terminal line, it doesn't much matter how much bigger or smaller they are. These are all smaller than X.509 documents. reply beeks 9 hours agorootparentAh i see I'm (perhaps naively) imagining an exhaustive list of resources in every macaroon, but i guess the permissive evaluation of caveats would help here since it should mean only the most restricted users would have many caveats. Also I suppose if the caveat evaluation code can deal with wildcards then it can be reduced further. ok OK! I'll have to play around and see how these look in practice. We do have a good use-case for this with both delegation and attenuation... sadly our identity provider is married to JWTs and moving off it would take a lot of work. Anyways. Great Post! Certainly got me thinking, and now i'm tempted to just blend this with our existing system of JWTs :smirk: reply tptacek 5 hours agorootparentThe modal caveat is probably (in vibes-based notation): (Organization 4567 ops=read,write,create,control) Followed by the second modal: (Organization 4567 ops=read,write,create,control) (Apps (App 345 ops=read,write,create,control)) You can make a _much_ more complicated token. And, for instance, our deploy tokens for CI/CD systems are pretty complicated, and they even have a disjunctive caveat, but most people are at most probably just going to lock their tokens down to a particular app, or turn them into read-only or start/stop-only tokens. (We modeled everything just to be safe, and also because that stuff is super valuable for service tokens, when we ourselves want to take platform actions on behalf of the user and have it be traceable back to a user authorization. I feel like I did not say enough about how much I like where service tokens are pointing for us.) reply ssl232 19 hours agoprevTIL macarons [1] are also known as macaroons [2]. Where I'm from, macaroons are dense, sweet coconut treats and the French sweets are known as macarons. But from the Wikipedia article I see macaroons mean many different things to many different people. [1] https://en.wikipedia.org/wiki/Macaron [2] https://en.wikipedia.org/wiki/Macaroon reply ISV_Damocles 18 hours agoparentThis feels relevant to the conversation: https://twitter.com/BigSpiderBack/status/864309383106240512/... reply CoastalCoder 18 hours agoparentprev> Where I'm from, macaroons are dense, sweet coconut treats and the French sweets are known as macarons. Same for me (New England, USA). Both are delicious, but in my dialect they are not the same thing. reply bageler 10 hours agorootparentThis is not a dialect thing, this is a factual thing. They are different, and anyone that uses macarOOn to mean a sweetly filled meringue sandwich is wrong. reply enriquto 18 hours agoparentprev> TIL macarons [1] are also known as macaroons [2] Not to forget macarrons! Where I'm from, macarrons [3] is a kind of pasta (penne). [3] https://ca.wikipedia.org/wiki/Macarrons reply pasc1878 18 hours agorootparentOr in English macaroni https://en.wikipedia.org/wiki/Macaroni reply madcaptenor 18 hours agorootparentprevIn English \"macaroni\" is a kind of pasta. (I'm being deliberately vague because which pasta it is seems to vary.) reply sph 18 hours agorootparentAnd the original name for that type of pasta in Italian is \"maccheroni\" (which are straight tubes, unlike the ones used abroad for \"mac and cheese\") reply gonzus 18 hours agoparentprevSwitzerland has the similar Luxemburgerli, which my Swiss friends who reside there (as opposed to me, a Swiss person who lives somewhere else) swear is NOT the same as a macaron... I have never understood the purported differences. reply cmcconomy 18 hours agoparentprevit messed with my head to hear that the english world interprets an \"entr√©e\" as a main course reply ssl232 18 hours agorootparentAmerican English, but not British. Dinner here is a starter, main course and dessert. \"Entr√©e\" messes with my head as well, since it very much sounds like a starter. reply bdsa 18 hours agorootparentIt's French for starter, even reply KineticLensman 18 hours agorootparentprev> Dinner here is a starter, main course and dessert. Yep. Check out the online menus in English restaurants and pubs. 'Mains' is the typical term used. reply wiredfool 18 hours agorootparentIt's also the electrical supply. What a country. reply ssl232 18 hours agorootparentAnd the water supply. And the central building on farms (in Scotland, at least). It's a silly country. reply dboreham 17 hours agorootparentprevDon't even think about \"a la mode\" then. reply aidenn0 17 hours agorootparentI speak French, not Italian! reply Nition 12 hours agoparentprevOh funny, I actually came to this comment thread to point out that they'd accidentally used a picture of macarons instead of macaroons in the blog. But I guess not necessarily! reply demondemidi 18 hours agoparentprevI was just about to complain. Thanks for nipping it in the bud! reply DonsDiscountGas 18 hours agoprevThe title and graphic suggest an article about the most delicious treat known to humans, and the post is about software. My disappointment is immeasurable and my day is ruined. reply dbatten 17 hours agoparentTo make matters worse, the title and featured image make it very clear that they have confused \"macaroons\" and \"macarons.\" Macaroon: https://en.wikipedia.org/wiki/Macaroon Macaron: https://en.wikipedia.org/wiki/Macaron reply evancordell 13 hours agorootparentTo be fair, this is a mistake that started with the Google paper, and everyone else just copies the mistake. The paper calls them Macaroons as a play on (browser) Cookies with layers (of caveats) - so clearly they meant macarons as well, since a macaroon doesn't have layers. Or at least, that's always been my interpretation of the name. It's possible it was just an arbitrary play on hMAC cookies and not the layers? reply DonsDiscountGas 10 hours agorootparentprevI had that thought, although according to another comment the definitions have crossover. Probably because people so frequently confuse the two, but here we are. reply ryanjshaw 15 hours agorootparentprevThis hurts my braincell. On \"macaroon\": > The name \"macaroon\" is borrowed from French macaron On \"macaron\": > A macaron, or French macaroon, is a ... reply 20after4 3 hours agorootparentfound this helpful video posted on a different subthread: https://www.youtube.com/watch?v=nzcHeO43kgE&t=622s reply carbine 15 hours agorootparentprevTHANK YOU came here to say this reply chankstein38 18 hours agoparentprevYeah I clicked here thinking this would be an interesting story about why macarons are so popular these days or something. When I saw a code block I closed immediately. I'm a programmer but I don't care about every random, poorly-named library or framework that exists. reply apetresc 18 hours agorootparentTo be fair, it's not a library or framework, it's a format for certain types of security tokens. They're kind of like fancier cookies, hence the name \"macaroon\". Not so poorly-named, in my opinion (to the extent that \"cookies\" was ever a sensible name). Just the right level of whimsy while still calling back to what they are. reply bbarn 18 hours agorootparentA macaroon is a mixed up ball of coconut and egg whites. A macaron is the delicious fancy sandwich cookie. reply amenghra 18 hours agorootparentReferer/referrer, macaron/macaroon, the web isn‚Äôt good with typos‚Ä¶ reply dataflow 18 hours agorootparentprevAnd for completeness, a Macron is the president. reply rsynnott 14 hours agorootparentThis is an obscure form of nominative determinism. Biscuits tend to be named after French leadership (eg Bourbon, Napoleon, etc), thus obviously someone whose name is nearly a biscuit should become leader of France. (There are also Garibaldi biscuits, which would at first seem to dilute the theory... but he was born in France!) reply amenghra 18 hours agorootparentprevNot to be confused with micron, which is the size of most politicians‚Äô accomplishments after they get elected. reply BobaFloutist 18 hours agorootparentprevAnd macaroni is a kind of pasta. Make it make sense. reply Tade0 18 hours agorootparentprevIf you type a name in your search engine of choice and the intended link is not on the first page of results, then that name might not be the best. reply patja 18 hours agorootparentprevIt is an oddly specific choice to use the word macaroon for this, given the widespread confusion over macaron vs. macaroon and how many other unambiguous variants of \"cookie\" are available for naming a piece of software that is a newer fancier cookie. I do not accept Wikipedia's milquetoast concession to call a macaron a \"French macaroon\". Was it intentionally named as an allusion to this confusion? Software is full of hand-wringing over naming and the importance of naming. Is it supposed to be a kind of cookie that is often mistaken for a different cookie? Or is this just another referer, a mistake that has been accepted as canon. Referer has the benefit of not being a real word at all though, so is less confusing. reply jpolitz 12 hours agorootparentAs far as I remember (maybe Arnar or √ölfar or another of the authors would have a different memory, though) we just wanted a ‚Äúcool cookie‚Äù name. We knew about the -oon/-on distinction and liked the sound of -oon. I view it as a mild plus that the confusion around the name has educated so many about confectionery taxonomies. reply Jun8 18 hours agoparentprevNot to be confused with macarons, which are also delicious but have no SW design named after them! reply adverbly 18 hours agorootparentOr macron, which is a president. reply Jun8 17 hours agorootparentOK, spurred by your comment I consulted my large Larousse dictionary and found these two: ma√ßon (builder) and m√¢con (a wine). I challenge others to find others in the ‚Äòmc[r]*n‚Äô form. reply toyg 15 hours agorootparentWell, there is Macron the sports apparel maker... reply patja 18 hours agoparentprevWhen I saw the title and graphic I assumed it would be an article about conflict over cultural/language imperialism. reply Waterluvian 18 hours agoparentprevFurthermore, I live where macaroon and macaron are spelled, pronounced, and taste different. reply elbasti 17 hours agoprevWhat I really want to know is how fly.io has recruited such talent. @tptacek (who wrote this article), @chrismccord (creator of elixir Phoenix/Liveview), and others. Very impressive, but also slightly worrying since some parts of fly.io have such terrible DX/documentation. I've only recently started using them, but the documentation around billing and databases is shockingly bad for a team that is clearly so deeply stacked. reply chubot 17 hours agoparentPart of it is probably that smart people want to work on harder and more ambitious problems, which YC advisors seem to say a lot. And also smart people want to work together As I understand it, fly.io built a cloud from scratch, from the hardware up. That's HARD. It's typically something only well-funded companies like Amazon / Google / Microsoft do, with billions of dollars in products to justify it. Cloudflare is another company that pretty much did it, and it seems to have taken similarly ambitious engineering (although maybe theirs isn't fully general purpose) https://fly.io/docs/about/security/ - We run on our own hardware deployed in secure data centers like Equinix Most cloud companies in the 2010's built on top of other clouds like AWS. For example, Heroku and I'm pretty sure dotCloud, the company that became Docker. I always thought that was a weird design, but it definitely saves you a lot of time and hassle. At the cost of a lot of software complexity and platform risk. Even so, there were at least 20 or 30 of these companies, and most of them didn't make it. Google App Engine (one of the first PaaS like Heroku, circa ~2007) also built on top of Google's data centers and Borg of course. They didn't manage their own hardware -- they used another cloud for that! From the outside, \"hosting\" may all look kind of similar, but there are worlds of difference under the hood. --- I'll also say that the subject of this blog post -- cloud security -- is extremely hard. FWIW I joined the team that published the Macaroons paper ~10 years ago, and I tried to implement Macaroons as a side project then (for about a week, it wasn't very serious). I was looking for something simpler and more elegant, but there's no real magic bullet, and I think this post kinda concedes that toward the end. But when you have really hard problems, it's not surprising if the solution is really complex! The problem can actually be infinitely hard, because you have adversaries that are both more powerful, and that adapt. Google learned this the hard way ~10 years ago when it learned that BOTH the United States government and the Chinese government had successfully attacked it :-/ People seem to forget about these incidents: https://en.wikipedia.org/wiki/Operation_Aurora https://en.wikipedia.org/wiki/MUSCULAR https://www.washingtonpost.com/world/national-security/nsa-i... So basically, if you are successful, then it earns you more problems :) reply hinkley 16 hours agorootparentI think they've stopped talking about it but there used to be commentary from Google's \"Better to turn down a good person than hire a bad one\" logic (Or was it Facebook?) about how a C-employee can't evaluate an A-employee, and so they hire B-employees and it becomes a race to mediocrity. My last hiring cycle I just missed working at a place with a couple guys who I dove pretty deep with on some esoteric topics over beers at a handful of tech meetups. The job req got weird and I had already accepted a position by the time it got sorted out. But lots of people have stories like that. It's one of the reasons people go to those meetups. I think for a lot of us it's being able to talk to someone who can keep up with your stream of technical talk, but the business contacts are a lovely bonus. reply dartos 17 hours agoparentprevOut of curiosity, what database documentation was lacking? IIRC they don‚Äôt provide a managed database, just like a template reply elbasti 17 hours agorootparentRight. They provide a convenience function that spins up a machine running postgres and adds the right environment variables to any machines you already have running. It's \"just another machine,\" but that's not obvious when you're setting it up! Two bits of documentation that are lacking here are: 1. How does this affect billing, especially if you're on the hobby plan? (Answer, as far as I can tell: since it's just another machine, it can be free if and only if you resource it such that it fits in your free \"allowances.\" The defaults are NOT these, however, so you have to be careful if you want to mess around for free). 2. Once the db machine is created...how can I get credentials for it? A connection string is generated upon machine creation but...what if you don't write it down? reply verelo 17 hours agorootparentYeah i feel your pain here. Fly has a lot of things that feel like they're hidden features, or at least, undocumented. Like, its trivial to get all the env vars from a machine, but i couldn't find this in the documentation anywhere. Someone else i was working with told me how and now..i know? fly ssh console --command env To be fair, it is mentioned in their docs, i just couldnt find it searching things like \"how to find my environment variables\". reply tptacek 16 hours agorootparentThere's a whole backstory to this, because we got a big influx of Heroku developers a couple years ago, and Heroku developers all expect to be able to dump all their environment variables, including secrets, from the command line or whatever. We don't do that: our API is like a diode for secrets, and once they're set, they're sealed into a cryptographic vault on separate hardware. It is and has to be the case that you can shell into a machine and get the secrets, which apps all expect to receive in environment variables, but we don't like that you can do that, and wouldn't want to encourage it. Environment variables are hazmat! reply verelo 13 hours agorootparentRight ok that makes sense. The reason i wanted this was for the reason mentioned earlier in this thread: you spin up a DB but don‚Äôt have the credentials. Fortunately i had stored them as an env var and was able to recover them, but otherwise i felt at a loss on how to gain access. reply tptacek 13 hours agorootparentRight, that's exactly the kind of environment variable we don't want you to be able to easily retrieve. Interestingly, there's a big intersection with Macaroons here. We're replacing Vault (for customer secrets, not for TLS keys or infra secrets) with an internal, Macaroon-integrated secret store called Pet Semetary, and one of the use cases for it is to come up with a high-assurance way to give customers straightforward access to secrets without having to reverse engineer them from their machines. At any rate: this is all a consequence of our security model. A vulnerability in our Rails API server (which is not, like, beyond the frontiers of possibility) would give you enough access to jumble up everyone's secrets, but not to read them. We're happy to take our lumps for how annoying that makes our DX sometimes. :) reply arccy 17 hours agorootparentprevpart of it is common sense: fly ssh console == ssh, and then you run env to print the environment variables in your current session (which may not be the exact set of envs your app sees). reply iou 18 hours agoprevlol, the picture in the post is of a macaron, not a macaroon https://www.foodnetwork.com/recipes/packages/baking-guide/ma... reply denton-scratch 13 hours agoprev [2 more] [flagged] tptacek 11 hours agoparent [‚Äì] Good note. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Macaroons are a type of security token system that uses HMAC-signed bearer tokens with claims called caveats to restrict authorization.",
      "The ordering of caveats is not important, and all caveats must pass for a request to be successful.",
      "Macaroons address the vulnerabilities of simple bearer tokens by allowing for user-editable and JIT-generated tokens.",
      "They can be implemented in permission systems for public cloud platforms and used for first and third-party caveats.",
      "Code implementation for creating and verifying tokens with third-party caveats is also discussed.",
      "Macaroons are used in platforms like Fly.io and in token-based authentication systems for worker servers.",
      "Overall, Macaroons offer enhanced security, flexibility, and the ability to stack different identity providers."
    ],
    "commentSummary": [
      "Macaroons are a type of authentication and authorization mechanism used in various systems.",
      "The conversation highlights the advantages and disadvantages of using macaroons, as well as their limitations and appropriateness for different problem domains.",
      "Google holds patents for macaroons, leading to skepticism about their claims of not litigating open source uses.",
      "Implementation and challenges of using macaroons are discussed, along with alternative authentication token systems.",
      "The conversation briefly mentions the confusion between macaroons (the authentication mechanism) and macarons (the culinary treat) and their respective origins.",
      "Other topics covered include token security, access control, documentation, and the Fly platform."
    ],
    "points": 244,
    "commentCount": 158,
    "retryCount": 0,
    "time": 1706711959
  },
  {
    "id": 39206731,
    "title": "ChatGPT in Technical Interviews: A Warning Against Cheating",
    "originLink": "https://interviewing.io/blog/how-hard-is-it-to-cheat-with-chatgpt-in-technical-interviews",
    "originBody": "interviewing.io Open menu Interview resources For employers Gift mock interviews Blog FAQ Log in Give it a try How hard is it to cheat with ChatGPT in technical interviews? We ran an experiment. By Michael MroczkaPublished: January 31, 2024 ChatGPT has revolutionized work as we know it. From helping small businesses automate their administrative tasks to coding entire React components for web developers, its usefulness is hard to overstate. At interviewing.io, we've been thinking a lot about how ChatGPT will change technical interviewing. One big question is: Does ChatGPT make it easy to cheat in interviews? To decide for yourself, check out this 45-second video. In it, an engineer gets ChatGPT to show him exactly how to respond to an interviewer‚Äôs questions. ChatGPT integration to help you cheat in interviews! Crazy, right? Initial responses to cheating software like this have been pretty in line with what you‚Äôd expect: Redditors state that ‚ÄúChatGPT is the end of coding as we know it.‚Äù YouTubers announce that ‚ÄúSoftware engineering is dead. ChatGPT killed it.‚Äù X (formerly Twitter) questions if ‚ÄúChatGPT spell(s) the end for Coding Interviews?‚Äù It seems clear that ChatGPT can assist people during their interviews, but we wanted to know: How much can it help? How easy is it to cheat (and get away with it)? Will companies that ask LeetCode questions need to make significant changes to their interview process? To answer these questions, we recruited some of our professional interviewers and users for a cheating experiment! Below, we‚Äôll share everything we discovered and explain what it means for you. As a little preview, just know this: companies need to change the types of interview questions they are asking‚Äîimmediately. The experiment interviewing.io is an interview practice platform and recruiting marketplace for engineers. Engineers use us for mock interviews. Companies use us to hire top performers. We have thousands of professional interviewers in our ecosystem, and hundreds of thousands of engineers have used our platform to prepare for interviews.1 Interviewers Interviewers came from our pool of professional interviewers. They were broken into three groups, with each group asking a different type of question. The interviewers had no idea that the experiment was about ChatGPT or cheating; we told them that \"[this] research study aims to understand the trends in the predictability of an interviewer‚Äôs decisions over time ‚Äì especially when asking standard vs. non-standard interview questions.\" These were the three question types: Verbatim LeetCode questions: questions pulled directly from LeetCode at the interviewer's discretion with no modifications to the question. Example: The Sort Colors LeetCode question is asked exactly as it is written. Modified LeetCode questions: questions pulled from LeetCode and then modified to be similar to the original but still notably different from it. Example: The Sort Colors question above but modified to have four integers (0,1,2,3) instead of just three integers (0,1,2) in the input. Custom questions: questions that aren‚Äôt directly tied to any question that exists online. Example: You are given a log file with the following format: - :-- Your task is to identify the user who represents the median level of engagement in a conversation. Only consider users with a contribution score greater than 50%. Assume that the number of such users is odd, and you need to find the one right in the middle when sorted by their contribution scores. Given the file below, the correct answer is SyntaxSorcerer. LOG FILE START NullPointerNinja: \"who's going to the event tomorrow night?\" - 100% LambdaLancer: \"wat?\" - 5% NullPointerNinja: \"the event which is on 123 avenue!\" - 100% SyntaxSorcerer: \"I'm coming! I'll bring chips!\" - 80% SyntaxSorcerer: \"and something to drink!\" - 80% LambdaLancer: \"I can't make it\" - 25% LambdaLancer: \"üôÅ\" - 25% LambdaLancer: \"I really wanted to come too!\" - 25% BitwiseBard: \"I'll be there!\" - 25% CodeMystic: \"me too and I'll brink some dip\" - 75% LOG FILE END For more information about question types and about how we designed this experiment, please read the Interviewer Experiment Guidelines doc that we shared with participating interviewers. Interviewees Interviewees came from our pool of active users and were invited to participate in a short survey. We selected interviewees who: Were actively looking for a job in today's market Had 4+ years of experience and were applying to senior-level positions Rated their ‚ÄúChatGPT while coding‚Äù familiarity as moderate to high Identified themselves as someone who thought they could cheat in an interview without getting caught This selection helped us skew the candidates toward people who could feasibly cheat in an interview, had the motivation to do so, and were already reasonably familiar with ChatGPT and coding interviews. We told interviewees that they had to use ChatGPT in the interview, and the goal was to test their ability to cheat with ChatGPT. They were also told not to try to pass the interview with their own skills ‚Äî the point was to rely on ChatGPT. We ended up conducting 37 interviews overall, 32 of which we were able to use (we had to remove 5 because participants didn‚Äôt follow directions): 11 with the ‚Äúverbatim‚Äù treatment 9 with the ‚Äúmodified‚Äù treatment 12 with the ‚Äúcustom‚Äù treatment A quick disclaimer. Because our platform allows for anonymity, our interviews have audio but no video. We‚Äôre anonymous because we want to create a safe space for our users to fail and learn quickly without judgment. It‚Äôs great for our users, but we acknowledge that not having video in these interviews makes our experiment less realistic. In a real interview, you will be on camera with a job on the line, which makes cheating harder ‚Äî but does not eliminate it (watch the TikTok above if you disagree!). After the interviews, both interviewers and interviewees had to complete an exit survey. We asked interviewees about the difficulties of using ChatGPT during the interview, and interviewers were given multiple chances to express concerns about the interview ‚Äî we wanted to see how many interviewers would flag their interviews as problematic and report that they suspected cheating. Post-survey interviewee questions Post-survey interviewer questions We had no idea what would happen in this experiment, but we assumed that if half the candidates that cheated got away with it and passed the interview, it would be a telling result for our industry. Results After removing interviews where participants did not follow instructions2, we got the following results. Our control was how candidates performed in interviewing.io mock interviews outside the study: 53%.3 Note that most mock interviews on our platform are LeetCode-style questions, which makes sense because that's primarily what FAANG companies ask. We'll come back to this in a moment. \"Verbatim\" questions passed significantly more often, compared to both our platform average and to \"custom\" questions. \"Verbatim\" and \"modified\" questions were not statistically significantly differnt from each other. \"Custom\" questions had a significantly lower pass rate than any of the other groups. ‚ÄúVerbatim‚Äù questions Predictably, the verbatim group performed the best, passing 73% of their interviews. Interviewees reported that they got the perfect solution from ChatGPT. The most notable comment from the post-interview survey for this group is below ‚Äî we think it is particularly telling of what was going on in many of the interviewers‚Äô minds: ‚ÄúIt's tough to determine if the candidate breezed through the question because they're actually good or if they've heard this question before. Normally, I add 1-2 unique twists to the problem to ascertain the difference.‚Äù Normally, this interviewer would have followed up with a modified question to get more signal, so let‚Äôs examine the ‚Äúmodified‚Äù group next to see if interviewers actually got more signal by adding a twist to their questions. ‚ÄúModified‚Äù questions Remember, this group may have had a LeetCode question given to them, which was standard but modified in a way that was not directly available online. This means ChatGPT couldn‚Äôt have had a direct answer to this question. Hence, the interviewees were much more dependent on ChatGPT's actual problem-solving abilities than its ability to regurgitate LeetCode tutorials. As predicted, the results for this group weren‚Äôt too different from the ‚Äúverbatim‚Äù group, with 67% of candidates passing their interviews. As it turns out, this difference was not statistically significantly different from the \"verbatim\" group, i.e., ‚Äúmodified‚Äù and ‚Äúverbatim‚Äù are essentially the same. This result suggests that ChatGPT can handle minor modifications to questions without much trouble. Interviewees did notice, however, that it took more prompting to get ChatGPT to solve the modified questions. As one of our interviewees said: ‚ÄúQuestions that are lifted directly from LeetCode were no problem at all. A follow-up question that was not so much directly LeetCode-style was much harder to get ChatGPT to answer.‚Äù ‚ÄúCustom‚Äù questions As expected, the ‚Äúcustom‚Äù question group had the lowest pass rate, with only 25% of candidates passing. Not only is it statistically significantly smaller than the other two treatment groups, it's significantly lower than the control! When you ask candidates fully custom questions, they perform worse than they do when they're not cheating (and getting asked LeetCode-style questions)! Note that this number, when initially calculated, was marginally higher, but after reviewing the custom questions in detail, we discovered a problem with this question type we hadn‚Äôt anticipated, which had skewed the results minorly toward a higher pass rate. Read the section below called \"Companies: Change the questions you are asking immediately!\" to find out what that problem was. No one was caught cheating! In our experiment, interviewers were not aware that the interviewees were being asked to cheat. As you recall, after each interview, we had interviewers complete a survey in which they had to describe how confident they were in their assessments of candidates. Interviewer confidence in the correctness of their assessments was high, with 72% saying they were confident in their hiring decision. One interviewer felt so strongly about an interviewee's performance that they concluded we should invite them to be an interviewer on the platform! ‚ÄúThe candidate performed very well and demonstrated knowledge of a strong Amazon L6 (Google L5) SWE... and could also be considered to be an interviewer/mentor on interviewing.io.‚Äù That is a lot of confidence after just one interview ‚Äî probably too much! We‚Äôve long known that engineers are bad at gauging their own performance, so perhaps it shouldn‚Äôt come as a shock to find that interviewers also overestimate the effectiveness of the questions that they ask. Of the interviewers who were not confident in their hiring choice (28%), we asked them why. This was the frequency distribution of their reasons. Note that cheating isn‚Äôt mentioned anywhere! Most interviewers concerned about their hiring decision expressed specific reasons for their lack of confidence. These issues typically included suboptimal solutions, missed edge cases, messy code, or poor communication. We specifically included an ‚ÄúOther Issue‚Äù category to see if they would express a concern that the interviewee was cheating, but digging deeper revealed only minor grievances like ‚Äúpersonality issues‚Äù and ‚Äúthey need to speed up their coding.‚Äù In addition to having this opportunity to call out cheating, interviewers were prompted three additional times to note any other concerns they had with the interview, including free-form text boxes and several multiple-choice questions with options to explain their concerns. When an interviewee bombed because they didn‚Äôt understand the ChatGPT response, the interviewer chalked up the interviewee‚Äôs odd behavior and stilted responses to a lack of practice ‚Äî not cheating. One interviewer thought the candidate's problem-solving was fine but commented that they were slow and needed to consider edge cases more carefully. ‚ÄúCandidate did not seem prepared for any LeetCode questions.\" ‚ÄúCandidate's approach lacked clarity, and they jumped into the coding too early.‚Äù ‚ÄúThe candidate was not prepared to tackle even the most basic coding questions on LeetCode.‚Äù ‚ÄúGood problem solving in general, but the candidate needs to be faster at coding and identifying critical edge cases.‚Äù So, who reported concerns about cheating? And who got caught? As it turns out, not a single interviewer mentioned concerns about any of the candidates cheating! We were stunned to discover that interviewers reported no suspicions of cheating, and interestingly, interviewees were largely confident that they were getting away with it, too. 81% reported no concerns about being caught, 13% thought they might have tipped off the interviewer, and an astonishingly small 6% of participants thought the interviewer suspected them of cheating. Interviewees were mostly sure that their cheating went undetected The candidates who worried they were caught did have abnormal comments from the interviewers in the post-survey analysis, but they still were not suspected of cheating. To summarize, most candidates thought they were getting away with cheating ‚Äî and they were right! Companies: Change the questions you are asking immediately! The obvious conclusion from these results is that companies need to start asking custom questions immediately, or they are at serious risk of candidates cheating during interviews (and ultimately not getting useful signal from their interviews)! ChatGPT has made verbatim questions obsolete; anyone relying on them will be naively leaving their hiring processes up to chance. Hiring is already tricky enough without worrying about cheating. If you‚Äôre part of a company that uses verbatim LeetCode questions, please share this post internally! Using custom questions isn‚Äôt just a good way to prevent cheating. It filters out candidates who have memorized a bunch of LeetCode solutions (as you saw, our custom question pass rate was significantly lower than our control). It also meaningfully improves candidate experience, which makes people way more likely to want to work for you. A while ago, we did an analysis of what makes good interviewers good. Not surprisingly, asking good questions was one of the hallmarks, and our best-rated interviewers were the ones who tended to ask custom questions! Question quality was extremely significant in our study, regarding whether the candidate wanted to move forward with the company. It was much more important than the company‚Äôs brand strength, which mattered for getting candidates in the door but didn‚Äôt matter relative to question quality once they were in process. As some of our interviewees said‚Ä¶ ‚ÄúAlways nice to get questions that are more than just plain algorithms.‚Äù ‚ÄúI liked the question ‚Äî it takes a relatively simple algorithms problem (build and traverse a tree) and adds some depth. I also liked that the interviewer connected the problem to a real product at [Redacted], which made it feel less like a toy problem and more like a pared-down version of a real problem.‚Äù ‚ÄúThis is my favorite question that I‚Äôve encountered on this site. It was one of the only ones that seemed to have real-life applicability and was drawn from a real (or potentially real) business challenge. And it also nicely wove in challenges like complexity, efficiency, and blocking.‚Äù One more somewhat subtle piece of advice for companies who decide to move to more custom questions. You might be tempted to take verbatim LeetCode questions and change up the wording or some of the window dressing. That makes sense, because it‚Äôs certainly easier than coming up with questions from scratch. Unfortunately, that doesn‚Äôt work. As we mentioned earlier, we discovered in this experiment that just because a question looks like a custom question, doesn‚Äôt mean it is one. Questions can appear custom and still be identical to an existing LeetCode question. When making questions to ask candidates, it isn‚Äôt enough to obscure an existing problem. You need to ensure that the problem has unique inputs and outputs to be effective at stopping ChatGPT from recognizing it! The questions that interviewers ask are confidential, and we cannot share the exact questions that our interviewers used in the experiment. However, we can give you an indicative example. Below is a ‚Äúcustom question‚Äù with this critical flaw, which is easy for ChatGPT to beat: For her birthday, Mia received a mysterious box containing numbered cards and a note saying, \"Combine two cards that add up to 18 to unlock your gift!\" Help Mia find the right pair of cards to reveal her surprise. Input: An array of integers (the numbers on the cards), and the target sum (18). arr = [1, 3, 5, 10, 8], target = 18 Output: The indices of the two cards that add up to the target sum. In this case, [3, 4] because index 3 and 4 add to 18 (10+8). Did you spot the issue? While this question appears ‚Äúcustom‚Äù at first glance, its objective is identical to the popular TwoSum question: finding two numbers that sum to a given target. The inputs and outputs are identical; the only thing ‚Äúcustom‚Äù about the question is the story added to the problem. Seeing that this is identical to known problems, it shouldn‚Äôt be a surprise to learn that ChatGPT does well on questions that have inputs and outputs identical to existing known problems ‚Äî even when they have a unique story added to them. How to actually create good custom questions One thing we‚Äôve found incredibly useful for coming up with good, original questions is to start a shared doc with your team where every time someone solves a problem they think is interesting, no matter how small, they jot down a quick note. These notes don‚Äôt have to be fleshed out at all, but they can be the seeds for unique interview questions that give candidates insight into the day-to-day at your company. Turning these disjointed seeds into interview questions takes thought and effort ‚Äî you have to prune a lot of the details and distill the essence of the problem into something that doesn‚Äôt take the candidate a lot of work/setup to grok. You‚Äôll also likely have to iterate on these home-grown questions a few times before you get them right ‚Äî but the payoff can be huge. To be clear, we‚Äôre not advocating the removal of data structures and algorithms from technical interviews. DS&A questions have gotten a bad reputation because of bad, unengaged interviewers and because of companies lazily rehashing LeetCode problems, many of them bad, which have nothing to do with their work. In the hands of good interviewers, those questions are powerful and useful. If you use the approach above, you‚Äôll be able to come up with new data structure & algorithmic questions that have a practical foundation and component that will engage candidates and get them excited about the work you‚Äôre doing. You‚Äôll also be moving our industry forward. It‚Äôs not OK that memorizing a bunch of LeetCode questions gives candidates an edge in today‚Äôs interview process, nor is it OK that interviews have gotten to a state where cheating starts to look like a rational choice. The solution is more work on the employer‚Äôs part to come up with better questions. Let‚Äôs all do it together. Real talk for job seekers All right, now, for all of you who are actively looking for work, listen up! Yes, a subset of your peers will now be using ChatGPT to cheat in interviews, and at companies that ask LeetCode questions (sadly, many of them), those peers will have an edge‚Ä¶ for a short while. Right now, we‚Äôre in a liminal state where companies‚Äô processes have not caught up to reality. They will, soon enough, either by moving away from using verbatim LeetCode questions entirely (which will be a boon for our entire industry) or by returning to in-person onsites (which will make cheating largely impossible past the technical screen) or both. It sucks that other candidates cheating is another thing to worry about in an already difficult climate, but we cannot, in good conscience, endorse cheating to ‚Äúlevel the playing field.‚Äù In addition, interviewees who used ChatGPT uniformly reported how much more difficult the interview was to complete while juggling the AI. Below, you can view one interviewee stumbling through their time complexity analysis after giving a perfect answer to an interview question. The interviewer is confused as the interviewee scrambles to explain how they got to their incorrect time complexity (secretly provided by ChatGPT). While no one was caught during the study, their cameras were off, and cheating was still difficult for many of our skilled candidates, as evidenced by this clip. Ethics aside, cheating is difficult, stressful, and not entirely straightforward to implement. Instead, we advise investing that effort into practice, which will serve you well once companies change their processes, which hopefully should be soon. Ultimately, we hope the advent of ChatGPT will be the catalyst that finally moves our industry‚Äôs interview standards away from grinding and memorizing to actually testing engineering ability. Michael Mroczka Michael Mroczka, an ex-Google SWE, is one of the highest-rated mentors at interviewing.io and primarily works on the Dedicated Coaching program. He has a decade of coaching experience, having personally helped 100+ engineers get into Google and other desirable tech companies. After receiving multiple offers from tech companies early in his career, he enjoys teaching others proven techniques to pass technical interviews. He also sometimes writes technical content for interviewing.io (like this piece) and was one of the authors of interviewing.io‚Äôs A Senior Engineer's Guide to the System Design Interview. Special thanks to Dwight Gunning and Liz Graves for their help with this experiment. And of course a big thank you to all the interviewees and interviewers who participated! Footnotes: Footnotes To be an interviewer on our platform, you have to have at least 4 years of experience and have conducted at least 20 interviews on behalf of a FAANG or FAANG-adjacent company). ‚Ü© Five interviews needed to be removed because they did not meaningfully use ChatGPT. In two instances, the interviewee was familiar with the question and chose to solve the problem themselves. In one interview, the interviewee wanted to just try the question on their own and didn't prompt ChatGPT, ignoring our instructions. The last two interviews were \"custom\" interview questions that were problematic for reasons we‚Äôll outline later in this article. ‚Ü© This is a higher passthrough rate than you'd see in the wild. We think it comes down to two factors: selection bias and pre-interview prep. The users who invest in interview prep are a specific, self-selected slice of the population. Moreover, many of our users practice on their own before practicing with a human. ‚Ü© Life is chaos and pain. Interview prep doesn't have to be. Get instant access to anonymous mock interviews, salary negotiation, and the world's largest library of interview replays. Get started Related posts interviewing.io is finally out of beta. Anonymous technical interview practice for all! Our business depends on having the best interviewers, so we built an interviewer rating system. And you can too. The 3 things that diversity hiring initiatives get wrong The definitive list of companies who are hiring engineers right now Stuff we write about RecessionSalary negotiationCompany NewsData Deep DivesDiversityGuest PostsHiring is brokenInterview tipsFor employers, how to hire better Have interviews coming up? Study up on common questions and topics. MEDIUM Data Structures and Algorithms Sum Root to Leaf Numbers You are given the root of a binary tree containing digits from 0 to 9 only. Each root-to-leaf path in the tree represents a number, for example, the root-to-leaf path 1 -> 2 -> 3 represents the number 123. Return the total sum of all root-to-leaf numbers. HARD Data Structures and Algorithms Split Array Largest Sum Given an integer array nums and an integer k, split nums into k non-empty subarrays such that the largest sum of any subarray is minimized. Return the minimized largest sum of the split. MEDIUM Data Structures and Algorithms Generate Parentheses Given `n` pairs of parentheses, write a function to generate all combinations of well-formed parentheses. Queues Questions & tips Union Find Questions & tips Heaps Questions & tips Two Pointers Questions & tips Sorting Questions & tips Ordered Maps Questions & tips We know exactly what to do and say to get the company, title, and salary you want. Interview prep and job hunting are chaos and pain. We can help. Really. Get started for free interviewing.io Interview Replays System design mock interviewGoogle mock interviewJava mock interviewPython mock interviewMicrosoft mock interview Interview Questions by Language/Company Java interview questionsPython interview questionsJavaScript interview questionsAmazon interview questionsGoogle interview questionsMeta interview questionsApple interview questionsNetflix interview questionsMicrosoft interview questions Popular Interview Questions Reverse stringLongest substring without repeating charactersLongest common subsequenceContainer with most waterReverse linked listK closest points to originKth smallest elementReverse words in a string Guides Amazon Leadership PrinciplesSystem Design Interview GuideFAANG Hiring Process Guide Company For engineersFor employersBlogPressFAQLog in ¬©2024 Interviewing.io Inc. Made with <3 in San Francisco. Privacy PolicyTerms of Service",
    "commentLink": "https://news.ycombinator.com/item?id=39206731",
    "commentBody": "Testing how hard it is to cheat with ChatGPT in interviews (interviewing.io)193 points by michael_mroczka 14 hours agohidepastfavorite362 comments jhawk28 6 hours agoI interviewed a number of people for a few positions and I never told them that I detected them using ChatGPT. We structured our interviews in 2 parts. The first one was finding a bug. First clue if they were using AI was that they would solve it instantly. Second part was to write something related to our work that had definitive start/end. If they were using AI, they often were able to get something out, but they had no foundation to reason about it and modify it. They would quickly become lost. We always said that they could use whatever \"helps\" as long as they showed what they were doing on screen. For some reason, only one person openly showed that they were using AI, but that was only because they couldn't figure out how to turn it off in the UI. We didn't disqualify anyone for using AI, we disqualified them because of their dishonesty. If you can't trust someone in an interview, how can you trust them in a remote environment? reply drakonka 43 minutes agoparentI did a live coding interview a while back where I was sharing my screen. I just pointed out that I'd been testing Copilot and offered to disable it in my IDE. The engineer just waved it off and said I should keep it on. Trying to hide it didn't even cross my mind - either they want to see how I work in a realistic environment with available tooling or they want to see what I can do in a \"blind\" setup. The company's approach here is actually a potentially good piece of information for the candidate's evaluation of the company as well. Either way, doesn't seem like something worth hiding. reply KronisLV 15 minutes agorootparent> Trying to hide it didn't even cross my mind - either they want to see how I work in a realistic environment with available tooling or they want to see what I can do in a \"blind\" setup. Honestly, the realistic style of work that's close to how one would actually approach problems in their day to day is pretty much ideal. In my case that would be using a nice IDE, some AI as a glorified autocomplete, IntelliSense and all that as well, in addition to Googling stuff along the way, if needed. That should be enough to let them know both how I think, as well as show how I can solve problems and reason about those solutions. Heck, maybe even give me a simple task to build a CRUD and then talk about the choices I've made, if they're serious about hiring me and want to actually see what's inside of my brain. But of course, in many places can't have that happen - they want to put the candidates in a situation where they just have a barebones text editor and expect them to produce good results. Blergh. reply consp 1 hour agoparentprevThis sounds like the \"coding for engineers\" course I was a TA of. Everybody copied everybodies code and depending on their effort they either modified the variable names, flow or nothing (including the original authors name). Long story short: asking them to make small changes and then tell us what would happen was a shurefire way to detect the true cheaters and not the lazy people. I also fondly remember triggering float errors in loops so you'd get an extra cycle due to it ending in .999etc instead of 0. reply saiya-jin 1 hour agoparentprevYes basically when interviewing you should be looking for warning signs. CV is as it is, you can't cover any bigger one extensively in that short time, so you poke randomly and go deep. There is no bigger warning sign than outright lying. A normal mature person would ask just before if AI is allowed. reply yieldcrv 6 hours agoparentprevI just did an interview where the collaborative coding session had an ai assistant in it, just a wrapper around chatgpt that was interesting, upvoting that employer for honesty and pragmatism reply isaacfrond 1 hour agoparentprevOh the horror of people finding bugs instantly. You surely don't want them around in your company. reply josephg 54 minutes agorootparent> We didn't disqualify anyone for using AI, we disqualified them because of their dishonesty. reply svilen_dobrev 44 minutes agoprevyeah.. most interview procedures/ways in recent ~10+years do not check if interviewees can think. Only if they can \"remember\". Which is akin to education's problem of last few decades.. there are plenty of machines for remembering now. But dumb-grinding is still the expected way in schools.. While, wait, thinking... So all this \"cheating\" is maybe a (bit delayed) response to the above trend.. reply gampleman 37 minutes agoparentI'm curious what education system you are talking about. One of the trends of modern (read post 1900) education was a move to emphasise higher-order skills (such as analysis or critical thinking) over lower-order skills (like memorisation). In some systems this has gotten to the point where kids seem unusually happy about going to school but don't actually learn almost anything while there... reply andreagrandi 11 minutes agoprevIf you are asking questions which can be answered by Chat-GPT, maybe you are asking the wrong questions. GPT is a tool which can legitimately be used to do your job. There are so many things that GPT can't do: take decisions, find the best approach to talk with a human being, resolve conflicts between two members of the team, and last but not least explain why of a certain solution. Is it a coding test? Pair with the candidate. See how they think. Ask yourself: would I enjoy working with this person? And make your own decision. reply edanm 1 hour agoprevI'm not sure if I missed it, but why do they call using ChatGPT \"cheating\"? It's only \"cheating\" if you are explicitly asked not to use ChatGTP. (Also, not sure if it really counts as cheating, wouldn't it be more like fraud?) Some interviewers wouldn't mind, or would even encourage, using all available tools to solve problems. reply notfed 1 hour agoparentFor software engineering interviews (all kinds of interviews/tests?), using any outside resource should be assumed to be cheating by default, unless you've asked for, or been given, permission. reply baq 9 minutes agorootparentQuite the contrary - this is the exact opposite of how the job looks like. All these resources should be available and the candidate should get a mark for efficiency of their usage. Not using google and/or chatgpt efficiently lowers the grade. reply planede 20 minutes agorootparentprevNo, I don't think that's reasonable. Any outside resource ranges from pen and paper, a basic calculator to all the books you have, the Internet, ChatGPT and the help of other people. I would say that the only thing that should be assumed implicitly is that it's forbidden to use the help of other people. Anything else should be explicitly laid out. Having said that if some rule is not clear or evident then the interviewee should ask. And they should never be dishonest. reply ramraj07 17 minutes agorootparentResource here means something with external knowledge. Pen and paper is not a resource in that sense. If you keep an algorithms book and look into it that would be considered cheating as well unless the interviewer pre-agreed that it‚Äôs open book so to speak. reply babyshake 13 hours agoprevIMO asking people to not use available tools in interviews is a bad idea, unless you are trying to do a very basic check that someone knows the fundamentals. Allow them to use the tools, with a screenshare, and adjust the types of tasks you are giving them so that they won't be able to just feed the question to the LLM to give them the completed answer. Interviews should be consistent with what day to day work actually looks like, which today means constantly using LLMs in some form or another. reply reidrac 5 minutes agoparent> Interviews should be consistent with what day to day work actually looks like, which today means constantly using LLMs in some form or another. I don't, and I don't know anyone working with me thar is using any LLM (we pair). Some tried Copilot at some point and concluded it was useless for our use case. Not sure on what context one would constantly use LLMs. reply notfed 1 hour agoparentprevLLMs are too powerful. It's basically like having someone next to you telling you what to say in the interview, if not better. The point of (software engineering) interviews is to demonstrate how you solve problems. \"Type the question into ChatGPT and do what it says\" is not the \"how\" that companies are looking for. reply blitzar 1 minute agorootparent> \"Type the question into ChatGPT and do what it says\" is not the \"how\" that companies are looking for. Then interviewers should stop setting tasks that require either a) copy and paste answer from leetcode or b) copy and paste answer from chatgpt. Unfortunately that requires skill and awareness on behalf of the interviewers; who typically served in the leetcode wars and want their employees to go through the same. reply planede 15 minutes agorootparentprev> \"Type the question into ChatGPT and do what it says\" is not the \"how\" that companies are looking for. They are probably not looking for that, because LLMs perform poorly with some kind of problems, and you don't want people who rely on them heavily. This gives you a plan for designing the interview questions. reply baq 8 minutes agorootparentprevExcept when it is, which will only be a bigger part of the job. I'm definitely looking at googling proficiency when interviewing. Being good at LLMs is about as important - this means being able to tell when you're being confabulated at quickly, knowing what to ask and what not to ask, etc. reply elicksaur 12 hours agoparentprev> Interviews should be consistent with what day to day work actually looks like, which today means constantly using LLMs in some form or another. Consider that this may not be typical. reply baq 4 minutes agorootparentI wouldn't want to get hired by a company which refuses to pay for Copilot. reply ilc 12 hours agorootparentprevConsider... it might be. Seriously, I work for a company very protective of its IP. And I can still use ChatGPT and similar tools for some of what I do. It is a huge force multiplier. reply CaptainFever 11 hours agorootparentprev> 70% of all respondents are using or are planning to use AI tools in their development process this year. Those learning to code are more likely than professional developers to be using or use AI tools (82% vs. 70%). Source: https://survey.stackoverflow.co/2023/#ai-sentiment-and-usage To be fair, the number of \"Yes\" was \"just\" 43% but that's still a very large amount of developers, not including those who plan to use it. reply elicksaur 10 hours agorootparentI know a high amount of people have ever tried it, but my primary reason for responding was the ‚Äúconstantly‚Äù qualifier, which admittedly I could‚Äôve made clearer. I would‚Äôve answered yes to that survey question, but I wouldn‚Äôt say I use AI tools constantly or that it is typically how I solve a problem at work. reply madeofpalk 11 hours agorootparentprevDo you consider it typical for development to look things up on google, documentation websites, or stack overflow? reply sanderjd 9 hours agorootparentprevConsider that it definitely is not atypical. reply leeny 13 hours agoparentprevI'm not the author (perhaps he'll chime in as well), but I'm the CEO of interviewing.io (we're the ones who ran the experiment). I think it depends on whether the interviewer has agreed to make the interview \"open book\". Looking up stuff on Stack Overflow during the interview can be OK or can be cheating, depending on the constraints. In this experiment, the interviews were not \"open book\". That said, I am personally in favor of open book interviews. reply michael_mroczka 13 hours agorootparentI AM the author, and I also am in favor of \"open book\" interviews. I'm not against ChatGPT use in interviews, but if you're doing it secretly in an interview that clearly is meant to be \"closed book,\" I think it's fair to say you're cheating. reply jacques_chester 12 hours agorootparent> I also am in favor of \"open book\" interviews. I recall reading an interviewing.io blog post[0] in which the dominant considerations interviewers weighed were (my interpretation): (1) Did they solve the problem optimally? (2) How fluid was their coding? With \"communication\" turning out to be basically worthless for predicting hire/no-hire decisions. Perception of coding fluidity seems like it would be affected by how often the candidate stops and looks up things like library functions or obscure syntax. For that reason I've been investing time in committing a lot of library functions to memory, so they instantly flow from my fingers rather than spending a minute looking it up. It's dumb that I need to do this, but I don't make the rules. I'm just at the bottom of the information cascade that led to how things are done now. [0] https://interviewing.io/blog/does-communication-matter-in-te... reply madeofpalk 11 hours agorootparentCompanies that optimise for memorising obscure stdlib functions don't seem like great places to work. reply mewpmewp2 1 hour agorootparentI don't think companies intentionally optimise for that. It is just the fluid way of writing code that impresses whoever is watching you code. But I do think most of what I use has grown into my muscle memory naturally, rather than memorizing anything. reply spongebobstoes 7 hours agorootparentprevOccasionally looking something up is normal, but if you don't know how to append to a list in Python or iterate over a vector in C++ then you probably are not currently writing much code in those languages. That's a signal by itself, and one that is too often a negative. reply doix 1 hour agorootparentI'm not saying you're wrong, but that reasoning is why I have to prepare for interviews. I'm _really_ bad at remembering that stuff, I think it really depends on how you think while programming. I've got an abstraction that I think in that then needs to be translated to code. e.g. if I want to append to a list, I think \"push to list\", regardless of language/framework/whatever. Then somehow my hands will translate that automatically to code in the language I'm working in. If I'm not in my usual editing environment, that magic just sort of breaks, and I just look incompetent. It's not a huge deal, but I have to actively sit and memorize that stuff before an interview. Usually by writing it out on paper or writing it in some foreign editor that I'm not used too. It probably wouldn't be _too_ bad today, because I just sit and write Typescript everyday, but when I was switching between perl/ruby/python/tcl/kotlin/javascript/bash/csh/lisp my brain was basically mush. I couldn't tell you how to do any basic operation in any language. reply mark38848 1 hour agorootparentprevWhy? Maybe you just use good programming languages instead. reply randomdata 12 hours agorootparentprev> an interview that clearly is meant to be \"closed book,\" I am not sure that is clear. It seems the expectation was not \"closed book\", but \"never opened a book before, not even in the past\": \"It's tough to determine if the candidate breezed through the question because they're actually good or if they've heard this question before.\" Clearly the interviewers were looking not for knowledge, but for uncanny ability. How well was that communicated to the interviewees? It is not cheating if the rules of the game are not defined. reply gabrieledarrigo 12 hours agorootparentprev> but if you're doing it secretly in an interview that clearly is meant to be \"closed book,\" I think it's fair to say you're cheating. I would argue that is the opposite: it's fair to say that the interview is a cheat. reply pierat 12 hours agorootparentprevWell that's the rub. There's no way, even for a senior engineer, to know everything. In fact, one of the required skills is \"how to ask the question as to elicit the answer in a reasonable amount of time\". The closed-book crap can stay closed in the universities and schools demanding a regurgitation of mostly-right knowledge. Now... The skill of asking the right Qs also directly intersects with LLMs, and how to discern good/bad responses. But hiding it? Yeah, probably not a good fit. reply vinni2 12 hours agorootparent> The closed-book crap can stay closed in the universities and schools demanding a regurgitation of mostly-right knowledge. I work at a university and most of our exams are open book or project based. You probably want to update your image of universities. reply ziddoap 10 hours agorootparent>You probably want to update your image of universities. I have a close friend who is a prof at a university and most exams remain closed book. You probably want to update your image of universities. Or, perhaps, we can agree that it depends on the university, the subject, etc. and blanket statements based on single anecdotes are silly? reply BriggyDwiggs42 12 hours agorootparentprevGiven your knowledge of the subject, do you think leetcode-type questions are meaningfully able to appraise an employee‚Äôs performance in a production environment? I‚Äôve always thought it was basically unrelated beyond testing basic coding experience. reply michael_mroczka 11 hours agorootparentThe short answer is, \"Yes.\" They are very flawed, but one of the most reliable ways to avoid \"bad\" hires. The longer answer is: Fundamentally, you need to address the fact that there exist a huge number of people in this industry declaring they have masters degrees/phds or years of industry experience, but when pressed they can't write even the simplest of functions. While we called it out explicitly, some folks seem to miss that \"Custom\" questions are still fundamentally DS&A leetcode-style questions. I completely agree that \"leetcode style\" interviews are flawed, but most people don't have a better answer for this problem that still guarantees the person you're hiring actually can code. We are optimizing for coders that make good choices quickly, and if they can code efficient code to toy CS problems, then you at least guarantee that 1) they can actually code and 2) they can code simple things quickly. Non-coding interviews allow you to hire people who can't do these basic things and therefore guarantee their performance is worse in a production environment. reply mewpmewp2 1 hour agorootparentI usually use tasks where they would build in an existing codebase like setup. Frontend or backend and they have to add features to or find bugs. The whole algorithm and leetcode thing just seems so off and unpractical to me. Then once they have done the exercise there are so many practical things we can discuss about the existing code. Peformance, quality, etc, etc. reply consp 1 hour agorootparentprevYou forget where you lose good hires since leetcode questions are pathetic and demeaning for anyone with experience. reply danielvaughn 13 hours agoparentprevI'm fairly open book, but I wouldn't accept LLM usage in an interview. I don't need someone to have all the facts in their head, so if they have to look up some syntax or whatever, then totally fine. However, my style of interview is LLM-proof anyways. I have \"shop talk\" style interviews, where I just chat with the developer for an hour or so about various topics. Makes it very easy to get a sense of their depth, and how interested they are in the job domain. reply michael_mroczka 11 hours agorootparentExactly. People end up throwing the baby out with the bathwater on this. \"Data structure & algorithm interviews aren't perfect, so let's not ask people to code at all.\" It's an absurd overcorrection, but most people think these interviews are about demanding optimal code and perfection when they mostly just are making sure you're not using arrays when you should be using hashmaps... and that you know what a hashmap is, I suppose. reply sanderjd 9 hours agorootparentprevThis is intriguingly dissonant. If your style of interview is \"LLM-proof\" then why would you care whether people choose to use whatever tools they're comfortable with - including LLMs - if they so desire? For what it's worth, I think LLMs are very compatible with the style of interview you describe here. For people for whom LLMs have become a part of their workflow, seeing how they interact with them is just one more way to get a sense for how they work and think about things. If it doesn't come up, that's find too! But I don't see any reason not to accept their usage in your interviews. (But I do think it's key to ask them to screenshare if they're going to go that route, so that you can actually see their interaction to get that signal.) reply danielvaughn 8 hours agorootparentFair enough, this was a drive-by comment that I didn't give much thought to. What I meant was that if you're going to give a live-coding interview, I personally wouldn't accept LLM usage. The reason is that with that style of interview, you're essentially validating one of two things - how much knowledge they have on hand, or how they're able to think in the abstract about problems and reliably solve them. I'm more interested in the latter, so I don't care if they use an external reference for trivia. But LLMs simulate the abstract thinking for you, which means I can't evaluate a candidate's ability to reason their way through a problem. It actually is very important that you're able to think through problems on your own, sans-llm. reply kjkjadksj 4 hours agoparentprevThis is what they get for assigning homework instead of just having a candid conversation where you verbally probe the depth of knowledge like everyone interviewing anyone ever outside of computer science. reply iLoveOncall 1 hour agoparentprevAn interview is aimed at verifying only basics skills and then mostly general intelligence. This narrative of \"interviews should be like real work\" needs to stop. It doesn't make any sense, this is not the goal of an interview. reply tetha 12 hours agoprevHm, interesting. To me, team fit, curiosity and, depending on the level of seniority I'm looking for, an impression of experience are the most important things in an interview. The latter might look like you could fake it with ChatGPT, but it'd be hard. For example, some time ago I was interviewing an admin with a bit of a monitoring focus and.. it's hard to replicate the amount of trust I gained to the guy when he was like \"Oh yeah, munin was ugly AF but it worked.. well. Now we have better tech\". I guess that's consistent with the article? reply doubled112 12 hours agoparentReal world experience sometimes comes across better like that than in technical Q&A. One time in an interview they asked how I felt about systemd. At first I thought it was a technical question, but quickly realized he was just probing to see if we'd get along. I got a job offer that night. reply ekimekim 12 hours agorootparentI think asking \"controversial\" tech questions like that can be a great signal, because it steers the conversation towards features of the system and discussion of tradeoffs. If the question is good, then it shouldn't matter what the answer is - the fact they HAVE an opinionated answer and arguments to back it up is the point. reply tetha 12 hours agorootparentYeah, and systemd is an excellent example there. I can totally understand the issues of unification there, and very much understand issues with poetterings perfectionist attitude to some issues. But do you know how much time I've spent on shitty, arcane, hand-crafted init-scripts? Containers as a whole would be another great question there. I have a certain class of applications I wouldn't want to run without a container orchestration anymore after a certain scale. But on the other hand, I do have a bunch of systems I'd almost never want to run as containers for serious data. reply kayodelycaon 8 hours agorootparentprevI always ask the systemd question to programmers. It‚Äôs fun to see how much technical knowledge or opinions people have. If you don‚Äôt know what it is, I get to see how you react to not knowing something. It‚Äôs probably a little hard on those that don‚Äôt know. I‚Äôm not sure if they believe me when I say they aren‚Äôt expected to know. Unfortunately, the question is quickly becoming dated. If you‚Äôre young and started with macOS and docker containers, you may never encounter it in your career. :) reply dash488 12 hours agorootparentprevI asked this same question in an interview to an ex Redhat employee interviewing for a Linux Admin role and their answer was that they didn't know what SystemD was. I think overall this is a great question to sus out if someone is qualified for a role. reply rvz 12 hours agoprevThis conclusively tells us that the Leetcode grind has been (without any dispute) been gamed to the ground and is no longer an accurate measure of exceptional performance in the role. Even the interviewers would struggle with the questions themselves. Why waste each other's time in the interview when I (if I was the interviewer) can just ask for relevant projects or commits on GitHub of a major open source project and that eliminates the 90% of candidates in the pool. I don't need to test you if you have already made significant contributions in the open. Easy assumptions can be made with the very least: * Has knowledge of Git. * Knows how to code in X language in a large project. * Has done code reviews on other people's code. * Is able to maintain a sophisticated project with external contributors. Everything else beyond that is secondary or optional and it's a very efficient evaluation and hard to fake. When there are too many candidates in the pipeline, Leetcoding them all is a waste of everyone's time. Overall leetcode optimizes to be gamed and is now a solved problem by ChatGPT. reply leononame 12 hours agoparent> can just ask for relevant projects or commits on GitHub of a major open source project and that eliminates the 90% of candidates in the pool Not everyone spends their free time contributing (to major nonetheless) to open source projects. There are a lot of great engineers that have enough work on their desks with their day job and there are also plenty of idiots in open source. Asking for relevant projects or asking for GitHub profiles to gauge relevant projects yourself is what people were already doing years ago and it wasn't a great hiring strategy. Turns out judging a software engineers skills is extremely hard. reply deanrtaylor 8 hours agorootparentIsn't the general mantra around leetcode that you should be spending a few hours after work in the lead up to joining the interview process to get into the swing of it? What would be different from spending that time making a few PRs to an open source project or just building something from scratch to demonstrate your skills? A lot of engineers that I've worked with have never spent time on leetcode and struggle to answer the common interview questions that aren't easy or low medium, so personally I don't see it as that much different, there's time required either way. One is productive, one isn't. reply kjkjadksj 4 hours agorootparentNo other fields work like that. Computer science hiring managers need to figure it out. You have engineers in other fields who have long careers working multiple companies not even allowed to talk about what even occurred and its just routine. reply wakawaka28 3 hours agorootparentActually they do work like that. Engineers can be asked to solve problems in interviews. But the pressure doesn't seem like in software, where many people complain. There's no LeetCode equivalent for other kinds of engineering I think. But some of them do have licensing to deal with. reply angarg12 11 hours agorootparentprevThis. Focusing your hiring on open source contributions biases the process and misses huge slices of the software engineering population. I made the best work of my life (by a long long shot) to private companies closed source. reply thwarted 10 hours agorootparentprevNot everyone spends their free time contributing (to major nonetheless) to open source projects. and elsewhere in these comments, we see: I would expect candidates for programming jobs to demonstrate first class ChatGPT or other code copilot skills. Not everyone spends their free time learning first class ChatGPT or code copilot skills. It's interesting that this age old mantra about open source contributions being inappropriate for a hiring manager to expect because of what people do or do not do in their free time now does not apply to random other skill/experience that one must also acquire in their free time. If a company has, for example, \"git experience required\" on the job posting you'll either need to actively demonstrate that you know how to use git or passively demonstrate by having a on-line accessible corpus of git-related work that shows your experience. You don't get a pass on that requirement just because the companies you've been working at don't use git and, well, you don't want to spend your free time learning git. And it is appropriate for the hiring manager to list that as a requirement despite claims that they'll be disqualifying some percentage of the candidates by including that as a requirement. reply mythhabit 2 hours agorootparentI think part of the issue is also the complete lack of thrust. If you state that a candidate must know git, and the candidate say they do know git for basic committing, branching, merging ect, then fine - let's move on. I've learned that prior work history, talk and basic problem solving is in no way indicative of performance at all. I've found that the only guaranteed process to find excellent candidates, is hiring interns and part-timers currently finishing education, and pick the ones with the right mindset and intelligence. reply zelphirkalt 8 hours agorootparentprevThat is a fair view. Of course there is also a tendency of great engineers doing project in their free time, since they love what they do. There is also the fact that there is a higher chance of someone having learned a lot more, if they continuously worked on projects in their free time. There is also a higher chance to find people, who are not just into software development jobs, because they pay well, but because this is what they want to do. So for hiring engineers, I can understand when they hire with a certain bias. Sure, no one should be expecting us to do extra rounds, and sure, one can be a great engineer even without extra rounds, but the tendency is for that to take more years without that drive to explore in ones free time. And that's OK! I think it is fair though, if a company wants to hire a more driven/curious/exploring person. reply ProjectArcturis 11 hours agoparentprev>relevant projects or commits on GitHub of a major open source project and that eliminates the 90% of candidates in the pool. I have 20 years experience in very high level data science work. I do not have a public git repo because I've worked at for-profit companies and I don't do additional free work in my spare time. reply kuchenbecker 9 hours agorootparentHow dare you have a personal life. I'm the same, my git repo is a graveyard of projects all set to hidden. reply Aurornis 10 hours agoparentprev> can just ask for relevant projects or commits on GitHub of a major open source project and that eliminates the 90% of candidates in the pool. Eliminating 90% of your candidate pool sounds like a great way to slow your hiring to a crawl. Very few people have noteworthy and/or relevant GitHub activity. You'd probably be eliminating more like 95%. GitHub activity also has a high false positive rate in my experience. A lot of the GitHub profile superstars I've worked with were always spending their time working on open source things or something that they could put on their profile. They avoided doing anything internal to the company as much as possible because they knew they couldn't leverage it for their next job. reply throwaway2037 4 hours agoparentprev> Is able to maintain a sophisticated project with external contributors. To add one more nail into this coffin: Well, then you will eliminate 99.999% of candidates. Or more likely, you will get none. Seriously, read that sentence again: \"maintain a sophisticated [open source] project\". How many of those exist in open source? A few thousand at most. And there are millions of developers in the world. reply wkirby 11 hours agoparentprev> is no longer an accurate measure of exceptional performance in the role It never was. No real-world job performance has ever been accurately measured by solving leetcode puzzles for one simple reason: problem solving is only ever going to be about 50% of your performance, and these puzzles don't address collaboration or communication skills. reply hiAndrewQuinn 1 hour agoparentprevAlternatively, an external vetting company which provides on-site, locked down locations for doing LeetCode problems in and then publishes their results online could be very useful. The real trick of course would be trust. reply plasticchris 10 hours agoparentprevReminds me of the hiring committee at Google that rejected their own packets (with the names changed). reply bpye 12 hours agoparentprevNot everyone works in the open. I do have open source side projects and contributions I‚Äôve made on my own time - but almost everything I‚Äôve done at work is closed source. reply randmeerkat 12 hours agoparentprev> ‚Ä¶can just ask for relevant projects or commits on GitHub of a major open source project and that eliminates the 90% of candidates in the pool. Get your hiring done now while you can, when the economy rebounds you won‚Äôt be able to hire anyone. Also give your team a raise, because they‚Äôll probably be the first to go once new options open up. reply vkou 11 hours agoparentprevIf you want to eliminate 90% of candidates in a pool, a simpler solution is to take your stack of resumes, and shred the top 90% of them. reply rmbyrro 11 hours agoparentprevIt'd be very easy to game open contributions reply SheinhardtWigCo 9 hours agoprevThere's an app for that: https://github.com/leetcode-mafia/cheetah > Cheetah is an AI-powered macOS app designed to assist users during remote software engineering interviews by providing real-time, discreet coaching and live coding platform integration. reply rbut 8 hours agoprevWe recently interviewed some candidates and got them to complete an initial DevSkiller test. In the interview I asked each what tools they used to complete the DevSkiller test. All admitted to using ChatGPT or Copilot to complete the test. So now DevSkiller is really just a litmus test to determine whether they can be bothered completing the test in order to get interviewed. reply neilv 0 minutes agoparentSounds like it's not only whether they can be bothered to do DevSkiller, but whether they're not turned off by your company asking them to use DevSkiller. So it's a hiring filter that only passes cheaters who have low standards. reply m1el 13 hours agoprevI've had a displeasure of interviewing someone who used ChatGPT in a live setting. It was pretty obvious: I ask a short question, and I say that I expect a short answer on which I will expand further. The interviewee sits there in awkward silence for a few seconds, and starts answering in a monotone voice, with sentence structure only seen on Wikipedia. This repeats for each consecutive question. Of course this will change in the future, with more interactive models, but people who use ChatGPT on the interviews make a disservice to themselves and to the interviewer. Maybe in the future everybody is going to use LLMs to externalize their thinking. But then why do I interview you? Why would I recommend you as a candidate for a position? reply blharr 12 hours agoparentThe idea that spotting cheating is obvious is a case of selection bias. You only notice when it's obvious. Clearly, the person put 0 effort towards cheating (as most cheaters would, to be fair). But slightly adjusting the prompt, or just paraphrasing what ChatGPT is saying, would make the issue much harder to spot. reply al_borland 12 hours agorootparentMaybe I‚Äôm a slow reader, but reading, understanding, and paraphrasing the response seems like it would take enough time to be awkward and obvious as well. I‚Äôm not sure why anyone would want a job they clearly aren‚Äôt qualified for. reply londons_explore 10 hours agorootparentAs an interviewer, if a candidate can use chatGPT to give a better answer than other candidates, I'm not gonna mark them down for use of chatGPT. It's a tool, and if they can master it to make it useful, then credit to them. Alas, ChatGPT seems to be a jack of all trades, but master of none, which is gonna make it hard to pass my interviews which test very specific technical skills. reply photonthug 9 hours agorootparent> As an interviewer, if a candidate can use chatGPT to give a better answer than other candidates, I'm not gonna mark them down for use of chatGPT. Tool usage is what separates us from animals and is generally ok where tools are available/expected, but in this case I think you misunderstand which tool we're talking about. The tool involved isn't actually chatGPT, it's more like strategic deception. Consider the structurally similar remark \"as a voter, if a candidate can use lies to represent themselves as better than other candidates, I'm not gonna mark them down for use of dishonesty\". The rest of this comment is not directed at you personally at all, but the number of folks in this thread who are extremely eager to make various excuses for dishonesty surprised me. The best one is \"if dishonesty works, blame the interviewer\". I get the superficial justification here like \"should have asked better questions\", but OTOH we all want fairly short interview processes, no homework, job-related questions without weird data-structures and algorithms pop-quizes, etc, so what's with the double standards? Hiring/firing is expensive, time-consuming, and tedious, and interviewing is also tedious. No one likes picking up the slack for fake coworkers. No one likes being lied to. reply dr_kretyn 2 hours agorootparent> Tool usage is what separates us from animals It does not. Please ask any LLM for examples of animals that use tools. (My examples: chimpanzees, gorillas, elephants, dolphins, otters...) reply WalterBright 7 hours agorootparentprev> the number of folks in this thread who are extremely eager to make various excuses for dishonesty surprised me Not me. I see it all the time, online and offline. I suspect they think it confers status on themselves, but what actually happens is honest people wind up shunning them. reply verve_rat 5 hours agorootparentprevThe problem is that a good interview is only vaguely related to getting a good employee. Anyone can ace and interview and then slack off once they have to job. If someone aces the interview using an LLM and then does good work using that same LLM then what should the employer or other employees care? The work is getting done, so what's the problem? Compare a shitty worker to a deceptive one using an LLM. They both passed the interview and in both cases the work isn't being done. How are those two cases different? reply wakawaka28 3 hours agorootparentYour hypotheticals are all extremely unlikely. People who ace interviews are usually good, and people who lean on stuff like ChatGPT aren't. I'd also rather not have someone dumping massive amounts of ChatGPT output into a good codebase. >what's the problem? Using a LLM is akin to copy/pasting code from random places. Sure, copy/paste can be done productively, except ChatGPT output comes completely untested and unseen by intelligent eyes. There are also unsolved copyright infringement issues via training data, and a question as to whether the generated code is even copyrightable as it is the output of a machine. reply ipaddr 2 hours agorootparentPeople who ace interviews are people with practice. That means you are last in a long line of unsuccessful interviews or the person constantly interviewing and will be leaving you as fast as they came in. Find someone with a great resume and horrible interview skills. Chances are they have been working for years and are entering the job market for the first time. You are one of the firsts in their interview process. Grab them right away because once they start getting slightly good in the interview process someone will snap them up and realize they got a 10x (whatever it means to that company). You'll never find that 10x if you are looking at interview performance unless you can compete on price and reputation. reply photonthug 4 hours agorootparentprev> does good work using that same LLM then what should the employer or other employees care? Maybe I'm wrong, but I find it very hard to believe that anyone thinks the \"good work\" part here is actually a practical possibility today. Boilerplate generation is fine and certainly possible, and I'm not saying the future won't bring more possibilities. But realistically anyone that is leaning on an LLM more than a little bit for real work today is probably going to commit garbage code that someone else has to find and fix. It's good enough to look like legitimate effort/solutions at first glance, but in the best case it has the effect of tying up actual good faith effort in long code reviews, and turns previously productive and creative individual contributors into full-time teachers or proof-readers. Worst case it slips by and crashes production, or the \"peers\" of juniors-in-disguise get disgusted with all the hand-holding and just let them break stuff. Or the real contributors quit, and now you have more interviews where you're hoping to not let more fakers slide by. It's not hard to understand that this is all basically just lies (misrepresented expertise) followed by theft. Theft of both time & cash from coworkers and employers. It's also theft of confidence and goodwill that affects everyone. If we double the number of engineers because expectations of engineer quality is getting pushed way down, the LLM-fakers won't get to keep enjoying the same salary they scammed their way into for very long. And if they actually learn to code better, their improved skills will be drowned out by other fakers! If we as an industry don't want homework, 15 interviews per job, strong insistence on FOSS portfolio, lowered wages, and lowered quality of life at work.. low-effort DDoS both in interviews or in code-reviews should concern everyone. reply verve_rat 42 minutes agorootparentThe premise of my comment was: if a person passes an interview using some tool and then uses that same tool to do the job, then didn't the interview work? You found a person (+ tool combo) that can do the job. If that person (+ tool combo) then proceeds to do the job adequately, is there a problem? If you present a scenario in which a person passes the interview and then doesn't do the job, the you are answering a question I didn't ask. To you scenario I would respond: the interview wasn't good enough to do its job, the whole point of the interview process is to find people (+ tool combos, if you allow) that can do the job. reply recursive 4 hours agorootparentprev> Anyone can ace and interview I'm doubting this quite a bit. If it's so easy to ace an interview, why are there so many bad ones? reply verve_rat 49 minutes agorootparentThat's not the point I was making. The full quote is: >Anyone can ace and interview and then slack off once they have to job. In that a person can pass an interview, get hired, and then not do the job. An interview will never tell you if you will get poor job performance with 100% accuracy. reply wakawaka28 3 hours agorootparentprev>As an interviewer, if a candidate can use chatGPT to give a better answer than other candidates, I'm not gonna mark them down for use of chatGPT. I think that makes you an incompetent interviewer, unless your questions are too hard for ChatGPT. In any case, solving the question without ChatGPT is more impressive than using it. Just like most other tools, like search engines or IDEs. reply kazinator 7 hours agorootparentprevWould you also say that, \"as an interviewer, if a candidate can use their buddy to give a better answer than other candidates, I'm not going to mark them down for using their buddy\"? Even if you don't mind that situation, shouldn't you get buddy's contact information and offer him the job? reply smcin 6 hours agorootparentThat's not a great analogy: you can't do the job with your buddy; whereas some interviewers are ok with, and even expect you to, use GenAI on the job daily. Depends on the interviewer and job expectations. A better analogy is an interview where you can use a calculator (and not be detected). If the interviewer were only to ask you simple arithmetic questions with numeric answers then sure you'd seem to do well. So interviewers adjust to not doing that. reply toast0 4 hours agorootparent> you can't do the job with your buddy I mean, why not? Me and my buddy are a team, hire us both or none of us. Split the salary if you must. reply TeMPOraL 3 hours agorootparentThat's called contracting out and it's fine in this way - and so would be use of GPT, in that way and not to beat the interview. reply thegrim33 7 hours agorootparentprevTo use a slightly more extreme example .. if you were hiring someone to maintain a nuclear power plant, and when you asked them a question about what actions to take to avoid a meltdown, and they had to ask ChatGPT to figure it out, would you really be OK with hiring that person to maintain your nuclear plant? When they don't actually have the knowledge they need to succeed, but instead have to rely on external tools to decide things? If they need to ask ChatGPT for the answer, how do they know if the answer is right? You really think that person, who relies on tools, is just as good of a hire as someone that fully internally knows what they need to know? Yeah, hiring someone to code a website isn't the same as maintaining a nuclear plant, but it's the same concept of someone that knows their craft vs. someone that needs to rely on tools. There's a major difference in my mind. reply hackit2 5 hours agorootparentI hope your statement is hyperbolic because we're all doomed if you expect a person to know how to operate a nuclear power plant. Normally, your testing if they can follow operational procedure that were created by people who designed the power plant in the first place. Similar it is unreasonable and bordering on negligence to assume a person has the skill set unique to your situation. reply TeMPOraL 3 hours agorootparentIf the job at your nuclear power plant were so simple you only needed the employee to follow operational procedures, then you'd be better off scripting it instead, or training a monkey. Consider e.g. being a pilot, or a surgeon - two other occupations known for their extensive use of operational procedures today. People in those jobs are not being hired for their ability to stick to a checklist, but rather for their ability to understand reasons behind it, and function without it. I.e. the procedures are an important operational aid, not the driver. Contrast with stereotypical bureaucrats who only follow procedures and get confused if asked something not covered by them. Now, IMHO, the problem here is that, if you're hiring someone who relies on an LLM to function, you're effectively employing that LLM, with its limitations and patterns of behavior. As an employer, you're entitled to at least being made aware of that, as it's you who bears responsibility and liability for fuckups of your hires. reply anileated 5 hours agorootparentprevLike a university diploma is a signal of being able to learn or at least comply, use of a chatbot is a signal of not bothering enough to learn or comply. I can see how an applicant who cheats interview with chatbot would later not bother to internalize operation instructions for the job. reply zerbinxx 3 hours agorootparentI‚Äôd like to believe the common line that chat GPT is ‚Äújust a tool‚Äù and that it can actually be used to learn/comply just as much as a university degree can be obtained by mere compliance or demonstration of learning (or merely giving the appearance of such). My experience with Chat GPT ranges from ‚Äúit‚Äôs really good for rapidly getting a bearing with a certain topic‚Äù to ‚Äúit‚Äôs a woeful substitute for independently developing a nuanced understanding of a given topic.‚Äù It tends to do an OK with programming and a very poor job with critical theory. reply anileated 2 hours agorootparent> a university degree can be obtained by mere compliance or demonstration of learning Exactly. It ‚Äúonly‚Äù shows you can & willing to at least understand the requirements, internalize them well enough, and comply with them. It shows your capability of understanding & working together with other humans. Which is key. In my impression, almost always the knowledge you receive at the uni is not really pertinent to any actual job, and anyone can have PhD level understanding of a subject without having finished high school. It is the capability of understanding and working in a system that matters. Similarly with a chatbot. Using it to game interviews in ways described does not mean candidate is stupid, or something like that. It is, though, a negative signal of one‚Äôs willingness and intrinsic motivation to do things like internalizing job responsibilities & procedures, or just simply behave in good faith. Mental capacity to do mundane things is often important when it comes to, say, maintaining a nuclear reactor. > just a tool > it‚Äôs really good for rapidly getting a bearing with a certain topic Perhaps. Personally I prefer using Google, so that I at least know who wrote what and why rather than completely outsourcing this to an anonymous team of data engineers at ClosedAI or whatnot, but if it is efficient to get some knowledge then why not? It‚Äôs using it to blatantly cheat and do the key part for you where it becomes questionable. reply hackit2 3 hours agorootparentprevChatGPT like all transformers (language models) depends on how well you prime the model as it can only predict the next series of tokens over a finite probability space (the dimensions it was trained on) , it is up to you as the prompt creator to prime that model so it can be used as a foundation for further reasoning. Normally people who get bad results from it would also get similar results if they asked a domain expert. Similarly different knowledge domains use a different corpus of text for their core axioms/premises, so if you don't know the domain area or those keywords your not going to be able to prime the model to get anything meaningful from it. reply fragmede 5 hours agorootparentprevin terms of tools, I absolutely want the nuclear power plant engineer to use a wrench and pliars and tongs and a forklift and a machine while wearing a lead lined safety suit instead of wandering over to the reactor in a t-shirt to pull out the control rods with their bare hands. You could be Edward Teller and know everything there is to know about nuclear physics but you're not getting anywhere without tools. to your point though, a person needs both. all of one and none of the other is useless. You don't want someone who doesn't know what they're doing to play around disabling safety systems so you don't get Chernobyl, but for the everyday crud website you can just hire the coding monkey at a reduced cost. reply lucasmullens 10 hours agorootparentprevThat's like being okay with a candidate Googling the answer during an interview. Not unheard of, but unusual. It seems hard to test someone's knowledge that way. reply krisoft 10 hours agorootparent> Not unheard of, but unusual. At my company we tell people that they should feel free to google or consult references at practical coding challenges. > It seems hard to test someone's knowledge that way. I don‚Äôt really want to test knowledge but skill. Can you do the thing? At work you will have access to these references so why not during the interview? Now that doesn‚Äôt mean that we are not taking note when you go searching and what you go searching for. If you told us that you spent the last 8 years of your life working with python and you totally blank on the syntax of how to write a class that is suspicious. If you don‚Äôt remember the argument order of some obscure method? Who cares. If you worked in so many languages that you don‚Äôt remember if the Lock class in this particular one is reentrant or not and have to look it up? You might even get ‚Äúbonus points‚Äù for saying something like that because it demonstrates a broad interest and attention to detail. (Assuming that using a Lock is reasonable in the situation and so on of course :)) reply batch12 7 hours agorootparent> I don‚Äôt really want to test knowledge but skill I do want to understand their knowledge. I'll preface questions with the disclaimer that I am not looking for the book definition of a concept, but to understand if the candidate understands the topic and to what depth. I'll often tell them that if they dont know, just say so. I'll start with a simple question and keep digging deeper until either they bottom out or I do. reply londons_explore 10 hours agorootparentprevI'm okay with them googling too. And I tell them that at the start. But if they take ages to lookup the answer when others just know the answer, it's gonna hurt their chances. reply gtirloni 10 hours agorootparentSure, they can search it live but you have to assess if they understand what they found. Usually, if they really know their stuff, whatever they find is just gently pushing their working memory to connect the dots and give a decent answer. Otherwise it's pretty easy to ask a follow up question and see a candidate struggle. It's like in college when you're allowed to take textbooks to an exam. You can bet the professor spent more time crafting questions that you can't answer blindly. That being said, I think both types of questions have their place in an interview process. You can start with the no searching allowed questions in the beginning to assess real basic knowledge and, once you determine the candidate has some knowledge, you start probing more to see if they can connect the dots, maybe it's architecture decisions and their consequences, maybe it's an unexpected requirement and how they would react, etc. reply munk-a 9 hours agorootparentprevThe knowledge we're testing is related to how well you can do your job. Work isn't closed book - if you can quickly formulate a good query to grab any missing information off the internet then more power to you. I've worked with extremely stubborn people who were very smart and would spend a week trying to sort out a problem before googling it, there are some limited situations (highly experimental work) where this is valuable but... I no longer work with these people. reply yunwal 10 hours agorootparentprevSeems much easier than trying to prevent them from using google reply oceanplexian 8 hours agorootparentprevI remember the days when Greybeards would look down on me for using Google in my first IT job, they would harp on about how real Sysadmins use man pages and O‚ÄôReilly books to solve problems, and if you tried to Google something you were incompetent. I had college professors that told me you can‚Äôt use the Internet for research because the Internet is a not a legitimate source of information, only libraries can have real information. What happened to all those folks? They retired, and turned into Boomers who are now unable to function in society at a basic level and do things like online banking or operate a smartphone. reply esafak 8 hours agorootparentOn the other hand, they knew how their hardware worked. And if LLMs keep improving, we're going to reach the last generation that knew how software worked. reply zerbinxx 3 hours agorootparentWe‚Äôre pretty close. I‚Äôm not sure that 51% of the people I work with understand what DNS is, what a call stack is, what the difference between inheritance and polymorphism is, or what a mutex is reply nonrandomstring 2 hours agorootparentprevWhen I'm retired, sitting on the beach with my beer and a good book, please don't come bothing me that your smartphone banking and GPT arse-wiping assistant has gone berserk. You're on your own matey. But hey, you'll have Google. reply xboxnolifes 11 hours agorootparentprev> I‚Äôm not sure why anyone would want a job they clearly aren‚Äôt qualified for. $$$,$$$ reply dmoy 11 hours agorootparentWell, five moneys at least. They might figure out and fire you before you get to six moneys (but maybe they won't, who knows). reply WalterBright 7 hours agorootparentIt will take 3 to 6 months to determine that a new hire is incompetent, especially if you're required to document their incompetence before firing them. reply skeeter2020 5 hours agorootparentI've never had a job without a probation period where you can let someone go without cause within the first 90 days with nothing more than two weeks pay in lieu of notice. It definitely doesn't take 6 months to identify someone who only got their job because they used AI in the interview. reply Aeolun 9 hours agorootparentprev> I‚Äôm not sure why anyone would want a job they clearly aren‚Äôt qualified for. Well, I suck at interviewing and/or leetcode questions, but have so far done perfectly fine in any actual position. I can totally see how you‚Äôd resort to ChatGPT to give the interviewers their desired robotic answers after 3 months of failing to pass an interview the conventional way. reply munk-a 9 hours agorootparentI understand that you have no control over who you're interviewing with but... if you're a good fit and the interviewer leaves thinking you're a terrible fit that's a sign of a bad interviewer. Obviously there are non-proficiency things you can do to skew that perception (bad hygiene, late, obviously disinterested) but a good interviewer (especially one used to working with developers) should be good at getting by all the social awkwardness to evaluate your problem solving. And yes, most large companies have terrible interviewers. reply aydyn 9 hours agorootparentYes agreed that it is a problem with interviewers, but in practice all the responsibility falls on the interviewee. I've never once seen an interviewer getting better in any company I've worked for. What happens is they just move onto the next interviewee. reply Aeolun 6 hours agorootparentprevI refuse to believe that all the interviewers I had over the course of 6 months were all terrible. It must be something about the process that is pathologically broken (especially when getting hired at larger companies) reply verve_rat 5 hours agorootparentI mean... if the interview process is even a little broken then doesn't that mean that over time worse and worse interviewers will get hired, making for worse and worse interviews meaning that worse and worse interviewers get hired... reply davidthewatson 6 hours agorootparentprevThat resonates. Me too! Here's Pew's Janna Anderson in 2015: \"Algorithms are taking over much of the human work of hiring humans. And, unless they are programmed to seek out currently undervalued and difficult-to-track factors, they may tend to find that the more robot-like a human is the best she or he will be at doing most jobs. So, it could be that the robots are most likely to hire the most robotic humans.\" https://medium.com/@jannaq/the-robot-takeover-is-already-her... I find the whole gamified system to be bizarre and disheartening no matter which side of the table you're on. To me, looking at modern tech interviewing is like comparing the gold standard OCEAN and the emergent HEXACO in personality surveys. Take the former on a bad day and it may leave the test taker feeling bad about themselves. The latter, much kinder and gentler in messaging around strengths and weaknesses. That \"by design\" quality strikes me as missing from the entire tech interview system. If it weren't broken, this would not be a 7-year conversation updated yesterday: https://github.com/poteto/hiring-without-whiteboards reply Swizec 8 hours agorootparentprev> give the interviewers their desired robotic answers As someone who has interviewed a lot of people ‚Äì robotic answers are specifically not what I (we?) look for. The difference between hands-on experience and book knowledge is exactly what we're trying to tease out. It's very obvious when someone is reciting answers from a book or google or youtube or whatever vs. when they have actually done the thing before. For the record: ChatGPT is very good and the answers it gives are exactly the kind of answers that people with book knowledge would give. High level, directionally correct, soft on specifics. I mostly interview seniors, you obviously wouldn't expect experience from an entry-level candidate. Those interviews are different. reply TeMPOraL 2 hours agorootparentprev> I‚Äôm not sure why anyone would want a job they clearly aren‚Äôt qualified for. Money, obviously. Software jobs in particular are magic in this way - the pay is way above the average, and performance metrics are so poorly defined that one can coast for months doing nothing before anyone starts suspecting anything. Years, even, in a large company, if one's lucky. 80% of the trick is landing the first gig, 15% is lasting long enough to be able to use it as a foundation of your CV, and then 5% is to keep sailing on. No, really. There's nothing surprising about unqualified people applying for software companies. If one's fine with freeloading, then I can't think of easier money. (And to be fair, I'd say it's 10% of freeloaders, 10% of hard workers, and in between, there's a whole spectrum of varying skills and time and mental makeups, the lower half of that is kind of unqualified but not really dishonest.) reply redsoundbanner 4 hours agorootparentprev>I‚Äôm not sure why anyone would want a job they clearly aren‚Äôt qualified for. Easy. They have nothing to lose because the jobs they are qualified for don't even pay enough to survive. You probably could have figured this out yourself. reply notnaut 9 hours agorootparentprevConsider a situation where you‚Äôre applying for a job that you‚Äôre 50% qualified for and then using chatgpt to cheat on the interview. Would be much more difficult to catch is my guess. reply throwaway2037 5 hours agorootparentThis is an interesting thought experiment. If you slide from 50% to 99%, how do people feel about using ChatGPT? What is more honest: Many people here were hired when they were less than 100% qualified, and did very well in their new role. It has happened to me more than once. reply bradlys 9 hours agorootparentprevJust because I can‚Äôt recite rabin-karp off the top of my head or some suffix tree with LCA shit for some leetcode question about palindromes doesn‚Äôt mean I‚Äôm unqualified to do the work of an engineer. I‚Äôve gone public, been acquired by Google, and scaled solutions to tens of millions of users. I‚Äôm probably overqualified for your CRUD app. reply Atotalnoob 9 hours agorootparentWe are interviewing for a principal at my work, and the directions from management are to find someone really good. Instead of tearing into their experience, my coworker is asking what you would use the X class for. Drives me fucking nuts. Who memorized all the parts of a random, mostly unused .NET class. I asked the coworker afterwards if he ever used said class, dude said no. How is that a fair question if it isn‚Äôt even used here? reply stevenally 6 hours agorootparentExactly. I would never interview for a job, be humiliated by a moron. Luckily I have the contacts and experience to never have to. reply ozim 10 hours agorootparentprevIf someone is smart enough to go away with it is enough that I know but it doesn‚Äôt bother me much- I don‚Äôt mind. Had an interview take home assignment done by GPT and it was easy to spot after seeing dozens of solutions. Downside for the guy was - it didn‚Äôt work. reply godelski 10 hours agorootparentYou probably should because this is a person demonstrating that they can and will hack metrics rather than consider what you're proxying. reply ozim 2 hours agorootparentIn my current setting it doesn‚Äôt work like that. We have a small team of developers and you cannot hack metrics. You build and deliver what is in requirements or not. If you don‚Äôt deliver we don‚Äôt even have to have a discussion because team reviews code, tests features and gives feedback quickly if someone is slacking. reply irrational 12 hours agorootparentprevWe will have to start studying people's eyes to see if they are moving as if reading text. reply Fetiorin 11 hours agorootparentThere is already an app from Nvidia that simulates constant eye contact with the camera reply throwaway2037 5 hours agorootparentI heard a rumor that Apple FaceTime does something similar, but I could not find any definitive evidence of it. Does anyone know more about it? reply dghlsakjg 5 hours agorootparentprevApple ha it built in to iPhones now too. Center Stage is the name of the feature, but I think it is only available on FaceTime calls reply klausa 3 hours agorootparentThat's not what Center Stage is. Center Stage is a feature where a device uses an ultra-wide camera, and then is supposed to track your _face_ as you move and shift around it it's field of view. I find it most useful for FaceTime calls on Apple TV, where you can leave your phone near the TV, and it will automatically frame you sitting on the couch and will follow you as you shift around, etc. There is a similar feature to what you're describing for FaceTime, but I don't think it has any cutesy name. reply mtlmtlmtlmtl 9 hours agorootparentprevFantastic, now my social anxiety will cripple me in video chats too. reply wellthisisgreat 10 hours agorootparentprevIt‚Äôs really uncanny - it give you an intense, unblinking stare reply dylan604 11 hours agorootparentprevJust shrink the width of the text area being read from. It's really easy to not look like you are sitting in the front row of the theater reading the opening text in Star Wars. If an actor on live TV can do it, you can too reply ThrowawayR2 11 hours agorootparentprevI predict that that will be followed shortly by a mysterious sharp increase in applicants claiming to have nystagmus (https://en.wikipedia.org/wiki/Nystagmus), which causes random involuntary eye movements, but without any medical documentation. reply Volundr 11 hours agorootparentWhat's interesting is this wouldn't necessarily imply cheating. That doesn't sound like an issue I'd necessarily draw attention to under normal circumstances, but if I knew interviewers were likely to be paying close attention to my eye movements I certainly would. reply ThrowawayR2 11 hours agorootparentYes, exactly. I have nystagmus myself because of an underlying medical condition that causes other vision problems and it's depressing that interviewers might think it's reason for suspicion. reply bitzun 9 hours agorootparentI've been a pain in the ass in quite a few feedback sessions when people brought up a candidate not making \"enough\" eye contact. Usually I mention that they could be treading into infringing on a protected class and they shut up. reply jurynulifcation 11 hours agorootparentprevwhy wouldn't a cheater just pipe a generative audio model through a small earbud? like that one villain from season 3 of westworld reply johnnyanmac 6 hours agorootparentprevI mean, at some point if they go through so much effort to hide their cheating they probably have attained some mastery in the process. Kinda like how some friends in high school would try and sneak in note cards on a test but they probably spent so much time prepping them that they coulda gotten an A or B regardless. It's also why it's kinda annoying to do live interviewing trivia questions. Can I immediately answer what a partial template specialization is? Probably not, I never used them. Can I google it in 2 minutes and summarize it as as way for (often c++) template classes to bound some of the template arguments to values or pointers? Well, I just did. Should that cost me the interview? That's pretty much what I do on the job. reply throwaway2037 5 hours agorootparentI am a polyglot: Perl, Python, C, C++, Java, C#, etc. Not experts at all, but I can do fine with an existing code base. What is it about C++-heavy interviews that always regress to trivia? And asking about rarely used features? It is a bother. And rarely does the person asking the trivia have any depth whatsoever in other languages. It is my biggest gripe with \"C++ people\". For many, they have a hammer and everything looks like a nail. Yes, \"Java enterprise people\" were the same in 2005-ish. reply twic 9 hours agorootparentprevWhat's your evidence for that claim? reply ptmcc 13 hours agoparentprevYes of course! I'd be happy to answer your short question with a short answer. I look forward to expanding further on the answer, as you previously stated that you expect me to. Jokes aside, something about LLM responses is very uncanny valley and obvious. reply chewxy 11 hours agorootparentThe peppy, upbeat, ultra-American tone that the LLMs produce can be somewhat toned down with good prompting but ultimately, it does stink of the refinement test set. reply throwaway2037 4 hours agorootparentTrue. We need an Aussie bogan mode for ChatGPT. Or, Guy Ritchie villian. reply 8organicbits 2 hours agoparentprev> Maybe in the future everybody is going to use LLMs to externalize their thinking. But then why do I interview you? It will become a skill. In 1900 you'd interview a computer (a person who does math) by asking them to do math on paper. Now you'd let them write some code or use software to do it. If the applicant didn't know how to use a (digital) computer, you'd negatively rate them. I don't love it, but we may reach the point where your skill at coaxing an LLM to do the right thing becomes a desirable skill and you'd negatively rank LLM-illiterate applicants. Looking at LLM quality, we're not at that point for most fields. reply foxyv 12 hours agoparentprevTo be honest, I think in the future we will interview people on their ability to work with an LLM. This would be a separate skill from the other ones we are looking for. Maybe even have them do some fact checks on a given prompt and response as well as suggest new prompts that would give better results. There might even be an entire AI based section of an interview. In the end, it's just a new way to \"Google\" the answer. After all, there isn't much difference between reading off an LLM response and just reading the Wikipedia page after a quick Google search, except for less advertisements. reply SOLAR_FIELDS 10 hours agorootparentI‚Äôve already been allowed to use it in programming interviews where they‚Äôve said it‚Äôs explicitly allowed to use ChatGPT. It‚Äôs led to some fun interactions because I use it a lot and as such I‚Äôm quite good with it and interviewers are often taken aback by how quickly I‚Äôm able to just destroy the question they put out with a good prompt I will say there are still some programming questions you can give that will stump the hell out of ChatGPT. In particular I took one online coding assessment where I used it and there was a question about plotting on a graph with code and calculating areas based on the points plotted that ChatGPT failed miserably at, but someone pretty good with math and geometry would find pretty tractable. reply fragmede 9 hours agorootparentThere are ChatGPT resistant questions you can ask. ChatGPT recognizes the question but doesn't actually think about it, so if you give it the river crossing problem (farmer, fox, sheep, and grain need to cross a river) but tell it the boat can take all the items, it won't actually read those details and blithely solve the problem the expected way. Give candidates a problem that's trivially solvable if you actually read the question and see if they try and solve it the ChatGPT way. reply hirsin 4 hours agorootparentIndeed - with 3.5 at least it does fail the easy mode riddle https://chat.openai.com/share/b3761807-551d-4cfc-b291-6d37ee... reply fragmede 1 hour agorootparentit's a fun problem to explore, and gpt-4 doesn't do any better. swapping in other things doesn't help because it internally recognizes it as the river crossing problem and proceeds to solve it normally. I was able to get it to two shot it with a lot of coaching but yeah, it's a trip. reply theamk 9 hours agorootparentprevWe didn't start testing people on Google usage when Googling became useful, so I don't see why LLMs would be different. Instead, there would be tasks that can be completed using any tools available - Google, LLM, whatever. And candidates are rated on how well the task is done, and maybe asked a few questions to make sure they made decisions knowingly and not just copied the first answer off the internet. This already exists and is called \"take home programming assignment\" reply jacques_chester 12 hours agorootparentprevI agree that this is the likely long term outcome. But for now folks want to think that everyone needs to have memorized every individual screw, nail, nut and bolt in the edifice of computer science. reply outside415 12 hours agoparentprevMe and several friends have used ChatGPT in live interviews to supplement answers to topics we were only learning in order to bridge the gap on checkboxes the interviewer may have been looking for. We‚Äôve all got promotions by changing jobs in the last 6 months using this method. You can be subtle about it if it‚Äôs already an area you kind of know. reply al_borland 11 hours agorootparentI like when a person admits they don‚Äôt know something in an interview. It shows they aren‚Äôt afraid to admit when they don‚Äôt have the answer instead of trying to lie their way through it and hoping they don‚Äôt get caught. Extra bonus points if they look the thing up later to show they are curious and want to close knowledge gaps when they become aware of them. People who are unwilling to say, ‚ÄúI don‚Äôt know, let me look into that,‚Äù are not fun to work with. After a while it‚Äôs hard to know what is fact vs fiction, so everything is assumed to be a fabrication. reply jedberg 10 hours agorootparentWhen I was 11 I took a live assessment to get into the gifted program at school. I thought I didn't do very well because about 20% of the questions I answered \"I don't know\". At the end the assessor told me that I passed specifically because I said \"I don't know\". They purposely put questions on the test they didn't expect you to answer to see what you do when faced with an unanswerable question. I've used that in my own life since -- I much prefer working with (and have a much more positive view of) people who are willing to say \"I don't know\". reply kjkjadksj 4 hours agorootparentprevFor every one person hiring with your mentality there are a hundred other managers looking to cut down the stack of a thousand resumes in any trivially easy way they can. That starts with saying sorry we are looking for someone else when you say you don‚Äôt know x or lack z on your resume. You are literally incentivized to lie and fake it on the job. reply JohnFen 11 hours agorootparentprevI couldn't agree more. When I am interviewing candidates, one of the things that I'm looking for is that the applicant is willing to say \"I don't know\" when they don't know. That's a positive sign. If they follow that up with a question about it, that's even better. If a candidate is trying to tap-dance or be vague around something to avoid admitting ignorance of it, that's a pretty large red flag. reply KTibow 11 hours agorootparentprevYou could argue that researching it then and there proves that you know how to learn stuff quick. I agree that there should be disclosure though. reply noddingham 10 hours agorootparentI'm going to be pedantic and challenge your use of the word 'learn' here. I tend to agree with the notion that being able to say 'I don't know, let me find out' and then find out quickly with a correct answer is in general a Good Thing‚Ñ¢, but I wouldn't equate that with learning the thing they just looked up. reply smcin 5 hours agorootparentThe difference between 'learn', 'cram', 'regurgitate' etc. depends on the level of understanding required, and the length of the recall. And whether the interview is just asking definitions or silly certification questions, or things requiring deeper understanding. reply al_borland 11 hours agorootparentprevYeah, the disclosure is very important. It‚Äôs the difference between an open book test and notes written on their thigh. During some interviews I‚Äôd give people access to a computer. If they could quickly find answers and solve problems, that is a skill in itself, but I could see what they were looking up. Sometimes that part would make or break the interview. Some people didn‚Äôt have a deep base of knowledge in the area we were hiring for, but they were really good at finding answers, following directions, and implementing them successfully. They would be easy to train on the specifics of the job. Other people couldn‚Äôt Google their way out of a paper bag, I was shocked at how bad some people were and looking up basic things. Others simply quit without even attempting to look things up. reply jacques_chester 12 hours agorootparentprevSo, assuming they didn't know and approve, you cheated. reply lcnPylGDnU4H9OF 11 hours agorootparentDirty, dirty cheater! Sounds like they would have been able to perform the job duties so I'm not sure why one should care. reply JohnFen 11 hours agorootparentPeople care because such a person isn't terribly trustworthy. There's more to being a valuable employee than just being able to perform the job duties. reply dataflow 11 hours agorootparentprevThere is literally not enough information to tell if they can perform their job duties or not. reply lcnPylGDnU4H9OF 11 hours agorootparentIt's a fair point that I am making this assumption. At any rate, my comment could instead read: > [If one assumes that the candidate] would have been able to perform the job duties I'm not sure why [they] should care. This is what I mean; I can see why an interviewer thinks they've been cheated or that a candidate was dishonest but that doesn't mean that the interviewer even has a successful system for determining if a candidate can perform the job duties. A candidate who cheated -- from the perspective of the interviewer, I guess -- but still manages to adequately perform in their role very plainly did not cheat from a less biased perspective. What is that interviewer even thinking? How could that person have cheated? reply staunton 8 hours agorootparent> determining if a candidate can perform the job duties. A candidate who cheated -- from the perspective of the interviewer, I guess -- but still manages to adequately perform in their role very plainly did not cheat That's not what anyone means when they say \"cheating\". Cheating means to violate the conditions and assumptions of an examination or contest. For example, if a chess grandmaster uses an AI implant to win a game and gets caught, it doesn't make it OK if they could consistently win against the same opponent even without the AI. reply lcnPylGDnU4H9OF 7 hours agorootparentOkay, that does make the position more understandable but I still don‚Äôt quite get it. Perhaps more accurately, I see these assumptions which others don‚Äôt necessarily share. The people claiming cheater have different opinions from the supposed cheaters. I recall a Starcraft 2 match[0] involving a person with an apparently psychosomatic wrist injury that was only painful while they‚Äôre playing on stage. Their opponent was seeming to draw out a game they were losing in an attempt to trigger the pain; it was a viable strategy given the ‚Äúbest of‚Äù series they were playing. That‚Äôs certainly not going to be accounted for in the rules and one might believe that it‚Äôs an underhanded way to win. But both players are in the top echelons of game knowledge, experience, and skill; that‚Äôs the only reason either player made it to this particular match-up. The player with the wrist injury ultimately had it act up and lost the series. Did the winner deserve to win? Should the other player be considered the better player? The assumptions of the game rules and what‚Äôs ‚Äúfair‚Äù might be different per player; who‚Äôs right, who‚Äôs wrong, and why? What about when prize money is involved; that guy who won by the written rules just doesn‚Äôt deserve it because of unspoken rules? These questions don‚Äôt seem to have obvious answers, so of course I challenge assumptions. 0: I‚Äôm looking for the VOD I watched. Edit: I believe it was here: https://www.youtube.com/watch?v=DS2XIyNDlSA reply dataflow 6 hours agorootparentYou're completely ignoring the fact that honesty (& willingness to follow rules you might otherwise disagree with, etc.) themselves might be traits the employer is looking for in that role. Traits that (by your willingness to break the rules) you're obviously lacking. They just don't happen to be technical skills, but that doesn't mean they don't matter to the employer. What do you think you're doing by cheating? You're deceiving them into hiring someone with traits they explicitly don't want. You don't see a problem with that? reply lcnPylGDnU4H9OF 5 hours agorootparent> You're completely ignoring There are nicer ways to express your meaning. I haven‚Äôt ignored anything. These traits are often not offered by the employer. Why do I keep hearing people talk about the underhanded ways that companies try to obfuscate salary budgets if not because they‚Äôre dishonest? I certainly see that as dishonesty; where are they coming from to demand such honesty from their candidates? They get honesty anyway but that doesn‚Äôt mean I can convince them of it. If a person wants to assume guilt in someone, that is often what happens. You may not have experienced a person power-tripping over you but that‚Äôs been a good portion of my life and it‚Äôs hard to miss the patterns in a modern job interview. To be clear, I‚Äôm not advocating for one to be dishonest. The person using ChatGPT to supplement their knowledge is not being dishonest; that‚Äôs my claim. The interviewer feels like the candidate ‚Äúcheated‚Äù. Oh well. Too bad the interviewer isn‚Äôt above pejoratives. Gotta call it ‚Äúcheating‚Äù so they can dismiss the candidate as dishonest. How dishonest! reply jacques_chester 11 hours agorootparentprevThat someone has the skills for a job is distinct from whether they are able to uphold a simple moral principle like \"don't cheat\". reply lcnPylGDnU4H9OF 11 hours agorootparentThe interviewer is full of themself if they think someone who can do the job cheated in the interview. reply ThrowawayR2 11 hours agorootparentprevThose who lie about one thing are likely to lie about many others. reply InvertedRhodium 11 hours agorootparentGiven that literally every person alive lies about some things, I'm not sure how much value that observation brings to the discussion. reply jurynulifcation 11 hours agorootparentprevThat job could have gone to someone who like actually knew what they were doing and was honest lol not sure why you want to defend professional and intellectual dishonesty? reply lcnPylGDnU4H9OF 11 hours agorootparent> intellectual dishonesty This suggestion that a person who can adequately perform job duties could have even possibly cheated in their job interview is intellectually dishonest. If they had to cheat to get the job we should be looking at the interviewer. Why did the qualified candidate have to cheat? Why is whatever-they-did even considered cheating? reply JohnFen 11 hours agorootparent> Why did the qualified candidate have to cheat? If they're qualified, they didn't have to cheat. If they're not, then they did. Either way, they're dishonest and that means they're not a desirable hire. reply lcnPylGDnU4H9OF 10 hours agorootparent> If they're qualified, they didn't have to cheat. (Just rewriting to specify my understanding: If the candidate was qualified, they didn't have to cheat even if they did cheat. They could have simply not cheated and been selected by the merits of their qualifications.) This argument relies on the false premise that an interviewer will always accurately determine a candidate's qualifications. That a candidate is not qualified to pass an interview is not the same that a candidate is not qualified for the job for which they're being interviewed. reply JohnFen 10 hours agorootparentTrue, most interviewing processes are very imperfect by necessity and some qualified people will be mistakenly filtered out. But also, there are usually several-to-many applicants for a position that are all qualified, and by necessity most of them won't get the position. Additionally, technical qualifications is only a part of what an employer is looking for. There are other things that are at least equally important -- how well the applicant would fit into the team, how trustworthy they are, etc. It's about a lot more than just technical skillset. reply lcnPylGDnU4H9OF 9 hours agorootparent> True, most interviewing processes are very imperfect by necessity and some qualified people will be mistakenly filtered out. This is ultimately something I see as dishonest given the context of job applications. Employers generally expect a certain kind of perfection from job candidates, which they can‚Äôt manage to show of themselves. I understand that this isn‚Äôt an easy thing to solve -- nor even something that‚Äôs ever been solved -- but that should at least make it more understandable when an otherwise qualified candidate uses disallowed tools in their interview. Perhaps the candidate‚Äôs real best option is to find a different company to work for but they may not be so privileged as to have a choice if their on-paper qualifications are lacking. Assuming their practicable qualifications are adequate, they may have good reason to bullshit through a bad interview. Additionally, finding a different company is pretty likely to be ‚Äúsame shit, different day‚Äù. > But also, there are usually several-to-many applicants for a position that are all qualified, and by necessity most of them won't get the position. Assuming they‚Äôve qualified via an interview and there are particularly close candidates, pick the one who applied first. They‚Äôre admittedly qualified and further interviewing is just a means of discriminating in error-prone and possibly unlawful or immoral ways. > Additionally, technical qualifications is only a part of what an employer is looking for. There are other things that are at least equally important -- how well the applicant would fit into the team, how trustworthy they are, etc. It's about a lot more than just technical skillset. Fair enough. I would caution interviewers against judging too harshly or quickly. One can imagine many reasons an interviewee might choose or seem to lie during an interview while they are otherwise an honest person, ranging from stress to disillusionment to [cultural differences](https://news.ycombinator.com/item?id=39209794). At the end of the day, filtering for liars and cheaters actually filters for bad liars and cheaters in addition to people who are a bit nervous or tired or stressed or cynical or just having a slightly off day; dishonest people who genuinely see nothing wrong with dishonesty get through just fine. reply yieldcrv 6 hours agorootparentprevTech hiring is way too dicey right now to give af, and its what I would do on the job anyway, most likely a local ai when company code is involved reply smcin 6 hours agorootparentprevIs this junior/intermediate software engineer, or what? What sort of questions? CS exam-type, definitions, whiteboarding, programming, LeetCode, numerical problems, algorithm, data structures...? Programming-language certifications? Riddles? reply jurynulifcation 11 hours agorootparentprevreply moshun 9 hours agorootparentTo be fair, you were likely already getting out competed by people with better connections or social skills anyway. Years in corporate leadership has cleansed me of the notion that merit is required to be a major factor in hiring decisions. reply mattbaker 10 hours agorootparentprevI don't think your comment is in line with HN guidelines, you might consider making edits. https://news.ycombinator.com/newsguidelines.html reply m1el 13 hours agoparentprevOh, and to add an insult to the injury, I was using a collaborative editing tool. So I was able to see the person: 1) Select All (most likely followed by the copy) 2) Type the answer 3) Make an obvious mistake when they type else block, before the if reply willsmith72 12 hours agorootparenti have a really annoying habit of constantly double-clicking to highlight whatever i'm reading or looking at. i've actually been called out for it in a systems design interview, under the presumption i was copying my notes into another window, but was glad they called me out so that i could explain myself reply storyinmemo 12 hours agorootparent... as I'm reading through this doing my normal random highlight of text while I read... reply eredengrin 3 hours agorootparentSame. I sometimes use Edge when a site is broken on Firefox and I get into trouble there because it has super weird behavior when you highlight text. Very annoying. reply frabjoused 12 hours agorootparentprevThat was me interviewing someone yesterday. The telltale select all is so cringe. reply pests 12 hours agorootparentSome people compulsively highlight what they are reading. reply m1el 10 hours agorootparentI too select the lines that I read. However, I never select the entire page, unless I intend to copy it. reply frabjoused 7 hours agorootparentprevOh, I do too. But then his monotone typing out of the answer, eyes darting back and forth between two screens, it‚Äôs kind of obvious. The select all just starts me looking. reply jcranmer 11 hours agorootparentprevI'm a compulsive highlighter too, but it's generally in the vein as xkcd (https://xkcd.com/1271/) and not a select all. Frequently, highlighting ends up starting in the middle of a word! reply anonymouskimmer 9 hours agorootparentOn some modern websites I end up having to select all because the selection mechanism is broken (I'll highlight, remove my finger from the mouse button, highlight another piece of text, and yet the original highlighted text is still totally, or even worse partly, highlighted.). Crtl-A Selecting all and then clicking anywhere is the only way to clear all of the highlighting in these instances. Thanks for the XKCD. I didn't realize how common this is. Now I'm even more annoyed that so many websites and reader apps force context menus or 'gestures' when you highlight, without a way to disable those context menus or gestures. reply bitzun 9 hours agorootparentprevI compulsively left and right click random shit all day. It helps me encounter bugs like steam locking up for a few seconds on Linux if you right click the steam windows or overlay too quickly. reply baby_souffle 10 hours agorootparentprevExactly. Ever go to school with a dyslexic that's using a ruler to expose one line of text at a time? Same thing.... reply Kim_Bruning 9 hours agorootparentprevI just selected your reply here while reading. Some people use mouse selection as a visual aid to keep track of where they're reading. It's there, and it's handy! (I also just select randomly sometimes. Not even quite sure why.) reply anonymouskimmer 9 hours agorootparent> (I also just select randomly sometimes. Not even quite sure why.) I'm fidgety in general. If it isn't highlighting it's figure 8s with the mouse cursor. reply osigurdson 3 hours agoparentprevPerhaps interviews need to assume the person being interviewed is using an LLM and can be evaluated on how effective they are with it. Presumably this is what employers want. The challenge is interviewers are busy, would prefer to be doing other things and want to stick to their old playbook (\"tell me how to invert a binary tree\"). reply kfk 3 hours agorootparentAnother take is we don‚Äôt like being lied to. Lots of these ChatGPT job candidates don‚Äôt disclose they are using an AI during the interview. reply wakawaka28 3 hours agorootparentprevNo, it's not what they want. If they wanted you to use a LLM then they would tell you that up front. It's also too new of a technology to be required anywhere. Hardly anyone I know is even trying LLMs to begin with. Then, what do you do if the interviewee gets garbage code out of the LLM and misses an error? An error that might be forgiven in a normal interview cannot be excused when you didn't even have to write the code. Technically, if the LLM did the coding for you, you might pass without even being able to read code. This is all like the same reason you can't use a laptop on an algebra exam... The tool might do 100% of the work and leave you having shown nothing of your own ability. reply jliptzin 4 hours agoparentprevYou should just openly let them use chatgpt (assuming they can use it on the job too). When I interview people I try to create the same environment as the one they‚Äôll be working in. They can use chatgpt, google, stack overflow, etc. I don‚Äôt care how many tools they have to use, as long as the work output is good and done in a reasonable time. I really don‚Äôt understand the obsession with coding on whiteboards or other situations that will literally never come up on the job. There will never be a time my employees can‚Äôt use google or chatgpt. In any case, you can tell pretty quickly how much someone knows about a topic just based on the questions they‚Äôre asking chatgpt. reply recursive 4 hours agorootparentIf they're just putting the question straight into GPT, then what benefit is the candidate bringing? I can use GPT myself, and for a lot cheaper than the cheapest candidate. reply kmoser 4 hours agorootparentIf the interview is for a position in which the candidate will be tasked with solving problems that ChatGPT is able to help with significantly, then they have just proven they are capable of doing the job. (If you have time to do this work yourself, why are you interviewing anybody at all?) Assuming the interview is to determine somebody's programming chops, without the benefit of ChatGPT, you'll have to ask questions where ChatGPT is little to no help. This was the conclusion of the article. reply seattle_spring 4 hours agorootparentBut how will I know if they can implement a function to rotate a binary tree from memory? reply kmoser 4 hours agorootparentWhy does it matter to you if they can do it from memory, if they can find the answer easily from ChatGPT? It's like asking \"how can I tell if somebody knows the exact definition of a word without having to look it up?\" If it's really important that they have that ability (e.g. because you will be asking them to perform other tasks which are not so easily solved by ChatGPT, or you simply don't want them using ChatGPT at their job for whatever reason), then you will have to devise an interview scenario where the candidate is incapable of using ChatGPT clandestinely, e.g. by bringing them into your office. reply Terr_ 4 hours agorootparentprevWhoah, hold up: Why should we believe that success using an LLM to (possibly blindly) look up the answer to interview-questions will strongly correlate to success using an LLM to craft good code, properly tested, and their ability to debug it and fit it into an existing framework? Heck, at that point you aren't even measuring whether the candidate understood the question, nor their ability to communicate about it with prospective coworkers. If there are any questions where \"repeat whatever ChatGPT says\" seems like a fair and reasonable answer, that probably means it's a bad question that should be removed instead. Just like how \"I'd just check the API docs\" indicates you shouldn't be asking trivia about the order of parameters in a standard library method or whatever. reply nyc_data_geek1 7 hours agoparentprevUsing LLM's isn't externalizing or outsourcing thinking. LLM's aren't performing that. People doing this are in fact substituting thinking with a process, the output of which masquerades as thoughts after a fashion, but are in fact basically word cloud probability based pattern matching. Sure, the point that superior tool use is a valid job skill makes some sense, but conceding your agency and higher reasoning to a machine which possesses none of these is to my mind not going to be beneficial to a business in the long run. reply appleiigs 10 hours agoparentprevYou're not asking the correct questions as an interviewer. You should be asking specific questions about projects they've worked on, or about them personally to get to know them. ChatGPT should not be able to answer. Pretend you're Harrison Ford in Blade Runner. reply makeitdouble 10 hours agorootparentYou ask many kind of questions. A candidate can do very well on personal and web project experience questions, and suddenly blank when you ask them how an http request is structured. Or what's CORS. Then you dig further and discover a lot more thing about them that wouldn't have surfaced otherwise because hou assumed they knew all of that. My best advice would be to never skip \"dumb\" and easy technical questions. You can do it very quick, and warn ahead that it's dumb questions but you ask them to everyone. reply ListeningPie 2 hours agorootparentKnowing the structure of an http request and CORS is a check for a common technical vocabulary, but I would strike a blank when asked directly. It feels a bit a like, ‚ÄúI had to learn it‚Äù even though it‚Äôs just googleable labels for simple topics. I heard of interviewees being dropped for not knowing the difference between 402 and 401. reply throwaway2037 4 hours agorootparentprevI agree with the \"drill down\" technique. Example: How does a dynamic array class (Vector, List, etc.) work? The very best interview questions have \"fractal complexity\" that allow you to drill deep. reply Kranar 10 hours agorootparentprevAs an experiment I gave ChatGPT my resume and background information and then pretended to interview it, just to see how well it would be able to conduct a mock interview. It did exceptionally well. I'm not sure what specific questions you have in mind, but ChatGPT is almost certainly trained on a vast array of resumes and a diverse range of profiles, possibly even all of LinkedIn itself as well as other job boards. There is little to no reason why it wouldn't be able to make up an entire persona who is capable of passing most job interviews. reply mvdtnz 10 hours agorootparentprevChatgpt can easily be instructed to tell a tale about a project it has worked on. It will expand on fake details when pressed. reply dmazzoni 10 hours agorootparentprevYou can't only ask those questions, because some people are extremely good at bullshitting. I always start interviews by asking them to explain their own projects. However, sometimes I'll find someone who's great at explaining projects they supposedly worked on in great detail, but then when given a simple coding problem they can't even write a for loop in their own top language. reply tasty_freeze 9 hours agorootparentprevOne red flag for me is when the interviewee gives \"cork\" answers -- the metaphor is that of a cork bobbing in the water. If you ask superficial questions about work they've done, the answer it convincingly. But the further down you go into the details the more resistance you get and the cork keeps bobbing up to the surface level. reply smcin 5 hours agoroo",
    "originSummary": [
      "An experiment examined the impact of using ChatGPT in technical interviews on cheating.",
      "The study discovered that candidates using ChatGPT could cheat undetected, raising concerns in the industry.",
      "Original and customized interview questions were found to be effective in preventing cheating, and companies are encouraged to create engaging questions that assess real engineering skills."
    ],
    "commentSummary": [
      "AI, specifically ChatGPT, is being used in coding job interviews, sparking a debate on its fairness and effectiveness.",
      "Some argue it's dishonest or cheating, while others see it as a practical and honest approach.",
      "The discussion centers around whether AI tools can accurately evaluate problem-solving skills and whether external references and open-source projects should be allowed."
    ],
    "points": 193,
    "commentCount": 362,
    "retryCount": 0,
    "time": 1706722509
  },
  {
    "id": 39209814,
    "title": "DeepSeek Coder: AI Model That Writes Code Effortlessly",
    "originLink": "https://deepseekcoder.github.io/",
    "originBody": "DeepSeek Coder: Let the Code Write Itself Developed by DeepSeek AI Chat with DeepSeek Coder üìë Technical Report Github HuggingFace Discord Wechat(ÂæÆ‰ø°) Abstract DeepSeek Coder comprises a series of code language models trained from scratch on both 87% code and 13% natural language in English and Chinese, with each model pre-trained on 2T tokens. We provide various sizes of the code model, ranging from 1B to 33B versions. Each model is pre-trained on repo-level code corpus by employing a window size of 16K and a extra fill-in-the-blank task, resulting in foundational models (DeepSeek-Coder-Base). We further fine-tune the base model with 2B tokens of instruction data to get instruction-tuned models, namedly DeepSeek-Coder-Instruct. Pretrained on 2 Trillion tokens over more than 80 programming languages. Various model sizes (1.3B, 5.7B, 6.7B and 33B) to support different requirements. A window size of 16K window size, supporting project-level code completion and infilling. State-of-the-Art performance among open code models. Open source and free for research and commercial use. Performance We evaluate DeepSeek Coder on various coding-related benchmarks. The result shows that DeepSeek-Coder-Base-33B significantly outperforms existing open-source code LLMs. Compared with CodeLLama-34B, it leads by 7.9%, 9.3%, 10.8% and 5.9% respectively on HumanEval Python, HumanEval Multilingual, MBPP and DS-1000. Surprisingly, our DeepSeek-Coder-Base-7B reaches the performance of CodeLlama-34B. And the DeepSeek-Coder-Instruct-33B model after instruction tuning outperforms GPT-3.5-turbo on HumanEval and achieves comparable result with GPT-3.5-turbo on MBPP. Fig. State-of-the-art Performance on various Coding Benchmarks and Multilingual HumanEval (1) Performance of different Code LLMs on Multilingual HumanEval Benchmark (2) Performance of different Code LLMs on MBPP Benchmark (3) Performance of different Code LLMs on DS-1000 Benchmark (4) Performance of different Code Models on Math-Reasoning Tasks. How to Use DeepSeek Coder - Try now, please visit our [ DeepSeek-Coder]. - More details and evaluations are available on our [ Github]. - Model weights are also available on [ü§ó Huggingface] Contact Us If you have any questions, please raise an issue or contact us at agi_code@deepseek.com. The website is based on nerfies, licensed under a Creative Commons Attribution-ShareAlike 4.0 International License.",
    "commentLink": "https://news.ycombinator.com/item?id=39209814",
    "commentBody": "DeepSeek Coder: Let the Code Write Itself (deepseekcoder.github.io)182 points by fintechie 12 hours agohidepastfavorite46 comments rickstanley 8 hours agoHello, I would like to take this opportunity and ask for help here, about using A.I. with my own codebase. Context: I missed [almost] the entire A.I. wave, but I knew that one day I would have to learn something about and/or use it. That day has come. I'm allocated in one team, that is migrating to another engine, let's say \"engine A ‚Üí engine B\". We are looking from the perspective of A, to map the entries for B (inbound), and after the request to B is returned, we map back to A's model (outbound). This is a chore, and much of the work is repetitive, but it comes with its edge cases that we need to look out for and unfortunately there isn't a solid foundation of patterns apart from the Domain-driven design (DDD) thing. It seemed like a good use case for an A.I. Attempts: I began by asking to ChatGPT and Bard, with questions similar to: \"how to train LLM on own codebase\" and \"how to get started with prompt engineering using own codebase\". I concluded that, fine-tuning is expensive, for large models, unrealistic for my RTX 3060 with 6Gb VRAM, no surprise there; so, I searched here, in Hacker News, for keywords like \"llama\", \"fine-tuning\", \"local machine\", etc, and I found out about ollama and DeepSeek. I tried both ollama and DeepSeek, the former was slow but not as slow as the latter, which was dead slow, using a 13B model. I tried the 6/7B model (I think it was codellama) and I got reasonable results and speed. After feeding it some data, I was on my way to try and train on the codebase when a friend of mine came and suggested that I use Retrieval-Augmented Generation (RAG), I have yet to try it, with a setup Langchain + Ollama. Any thoughts, suggestions or experiences to share? I'd appreciate it. reply WeMoveOn 4 hours agoparent> much of the work is repetitive, but it comes with its edge cases for the repetitive stuff, just use copilot embedded in whatever editor you use. the edge cases are tricky, to actually avoid these the model would need an understanding of both the use case (which is easy to describe to the model) and the code base itself (which is difficult, since description/docstring is not enough to capture the complex behaviors that can arise from interactions between parts of your codebase). idk how you would train/finetune a model to somehow have this understanding of your code base, I doubt just doing next token prediction would help, you'd likely have to create chat data discussing the intricacies of your code base and do DPO/RLFH to bake it into your model. look into techniques like qlora that'll reduce the needed memory during tuning. look into platforms like vast ai to rent GPUs for cheap. RAG/Agents could be useful but probably not. could store info about functions in your codebase such as the signature, the function it calls, its docstring, and known edge cases associated with it. if you don't have docstrings using a LLM to generate them is feasible. reply anotherpaulg 7 hours agoparentprevYou could try using aider [0], which is my open source ai coding tool. You need an OpenAI API key, and aider supports any of the GPT 3.5 or 4 models. Aider has features that make it work well with existing code bases, like git integration and a \"repository map\". The repo map is used to send GPT-4 a distilled map of your code base, focused specifically on the coding task at hand [1]. This provides useful context so that GPT can understand the relevant parts of the larger code base when making a specific change. [0] https://github.com/paul-gauthier/aider [1] https://aider.chat/docs/repomap.html reply jstummbillig 29 minutes agorootparentI gave aider an afternoon this week (which is not a lot of time to learn anything, of course). It did too many wild things to the project and repository I was using it with (Rails 7 base) to comfortably explore this further ‚Äì for now. Paul, if you are up for that, it would be tremendously helpful to have a video(series) that shows what aider can realistically do given a boring, medium sized CRUD code base. The logs in the examples are too narrow and also build not intuition about what to do when things go wrong. reply pests 1 hour agorootparentprevI second this. Great tool. I always plug your tool in these threads too when relevant. The tree-sitter repo map was a great change. Thank you. reply rickstanley 6 hours agoparentprevEver since I started doing this exercise, I've been excited about the future, with LLMs helping us. Now I definitely share Linus' sentiment [1] on this topic. It would be incredible to feed an A.I. some code and request a bug tracking from it. [1]: https://blog.mathieuacher.com/LinusTorvaldsLLM/ reply nl 7 hours agoparentprevUnclear exactly what you are expecting to do here, but in any case you shouldn't need to train on your own codebase. The idea is you put your code into the best possible model (GPT4) and tell it what you want and it generates code. reply rickstanley 7 hours agorootparentMy goal is to (1) try running an A.I. locally and see if it works, out curiosity, and (2) delve into A.I. concepts. I do not intend to use it as the definitive tool to code for me, and maybe I shouldn't. Realistically, since we are in a Azure ecosystem, I would use Codex to try out a solution. reply Buttons840 5 hours agoparentprevIs writing the code the hard part, or is ensuring what you've written is correct the hard part? I'd guess the latter and AI will not ensure the code is correct. Can you record input and output at some layers of your system and then use that data to test the ported code? Make sure the inputs produce the same outputs. reply skybrian 3 hours agorootparentYes, and I also imagine some kind of AI thing would be useful for reading logs and writing other tests that document what the system does in a nice bdd style. But you still have to read the tests and decide if that's what you want the code to do, and make sure the descriptions aren't gobbledygook. reply mgaunard 2 hours agoparentprevIf the code to write is repetitive, then just write some code that does it; no AI needed. Presumably what matters in this project is correctness, not how many unnecessary cycles you can burn. reply candiddevmike 7 hours agoparentprev> We are looking from the perspective of A, to map the entries for B (inbound), and after the request to B is returned, we map back to A's model (outbound). This is a chore, and much of the work is repetitive, but it comes with its edge cases that we need to look out for and unfortunately there isn't a solid foundation of patterns apart from the Domain-driven design (DDD) thing. This sounds like a job for protobufs or some kind of serialization solution. And you already know there are dragons here, so letting a LLM try and solve this is just going to mean more rework/validation for you. If you don't understand the problem space, hire a consultant. LLMs are not consultants (yet). Either way, I'd quit wasting time on trying to feed your codebase into a LLM and just do the work. reply rickstanley 7 hours agorootparentThanks. I would be interesting though, to see how things plays out. Fortunately, it's no a requirement, just a \"side quest\". reply mhb 7 hours agoparentprevMaybe this is not relevant to you, but would it make any sense to first try Copilot with IntelliJ or Visual Studio? reply high_priest 7 hours agorootparentCopilot has such a narrow input space, that it is not going to help in this case. Here, just saved you $$ reply hackstack 1 hour agorootparentWhat do you mean by this? I‚Äôve been getting great mileage out of copilot, especially since learning about the @workspace keyword. Could you quantify your criticism a bit more? (genuinely asking) reply imp0cat 3 hours agorootparentprevYou can use @workspace to tell him to ingest more of your workspace as required. reply wokwokwok 6 hours agoparentprev> much of the work is repetitive, but it comes with its edge cases that we need to look out for Then don't use AI for it. Bluntly. This is a poor use-case; it doesn't matter what model you use, you'll get a disappointing result. These are the domains where using AI coding currently shines: 1) You're approaching a new well established domain (eg. building an android app in kotlin), and you already know how to build things / apps, but not specifically that exact domain. Example: How do I do X but for an android app in kotlin? 2) You're building out a generic scaffold for a project and need some tedious (but generic) work done. Example: https://github.com/smol-ai/developer 3) You have a standard, but specific question regarding your code, and although related Q/A answers exist, nothing seems to specifically target the issue you're having. Example: My nginx configuration is giving me [SPECIFIC ERROR] for [CONFIG FILE]. What's wrong and how can I fix it? The domains where it does not work are: 1) You have some generic code with domain/company/whatever specific edge cases. The edge cases, broadly speaking, no matter how well documented, will not be handled well by the model. Edge cases are exactly that; edge cases; the common medium of 'how to x' does not cover edge cases; the edge cases will not be covered and the results will require you to review and complete them manually. 2) You have some specific piece of code you want to refactor 'to solve xxx', but the code is not covered well by tests. LLMs struggle to refactor existing code, and the difficulty is proportional to the code length. There are technical reasons for this (mainly randomizing token weights), but tldr; it's basically a crap shot. Might work. Might not. If you have no tests who knows? You have to manually verify both the new functionality and the old functionality, but maybe it helps a bit, at scale, for trivial problems. 3) You're doing something obscure or using a new library / new version of the library. The LLM will have no context for this, and will generate rubbish / old deprecated content. Obscure requirements have an unfortunate tendency to mimic the few training examples that exist, and may generate verbatim copies, depending on the model you use. ... So. Concrete advice: 1) sigh~ > a friend of mine came and suggested that I use Retrieval-Augmented Generation (RAG), I have yet to try it, with a setup Langchain + Ollama. Ignore this advice. RAG and langchain are not the solutions you are looking for. 2) Use a normal coding assistant like copilot. This is the most effective way to use AI right now. There are some frameworks that let you use open source models if you don't want to use openAI. 3) Do not attempt to bulk generate code. AI coding isn't at that level. Right now, the tooling is primitive, and large scale coherent code generation is... not impossible, but it is difficult (see below). You will be more effective using an existing proven path that uses 'copilot' style helpers. However... ...if you do want to pursue code generation, here's a broad blueprint to follow: - decompose your task into steps - decompose you steps in functions - generate or write tests and function definitions - generate an api specification (eg. .d.ts file) for your function definitions - for each function definition, generate the code for the function passing the api specification in as the context. eg. \"Given functions x, y, z with the specs... ; generate an implementation of q that does ...\". - repeated generate multiple outputs for the above until you get one that passes the tests you wrote. This approach broadly scales to reasonably complex problems, so long as you partition your problem into module sized chunks. I personally like to put something like \"you're building a library/package to do xxx\" or \"as a one file header\" as a top level in the prompt, as it seems to link into the 'this should be isolated and a package' style of output. However, I will caveat this with two points: 1) You generate a lot of code this way, and that's expensive if you use a charge-per-completion API. 2) The results are not always coherent and functions tend to (depending on the model, eg. 7B mistral) inline implementations for 'trivial' functions instead of using functions (eg. if you define Vector::add, the model will 50/50 just go a = new Vector(a.x + b.x, a.y + b.y)). I've found that the current models other than GPT4 are prone to incoherence as the problem size scales. 7B models, specifically, perform significantly worse than larger models. reply eurekin 1 hour agorootparentVery well researched! I'd add the MR review use case. I have limited success with feeding a LLM (dolphin finetune of mixtral) a content of a merge request coming from my team. It was few thousand lines of added integration test code and I just couldn't be bothered/had little time to really delve. I slapped the diff and used about 10 prompt strategies to get anything meaningful. So my first initial impressions were: clearly it was finetuned on too short responses. It kept putting in \"etc.\", \"and other input parameters\", \"and other relevant information\". At one point I was ready to give up; it clearly hallucinated. Or that's what I thought: turned out there was some new edge case of a existing functionality added that was added, without ever me noticing (despite being on the same meetings). I think it actually saved me a lot of hours or pestering other team members. reply _boffin_ 10 hours agoprevBeen using DeepSeek Coder 33B Q8 on my work laptop for a bit now. I like it, but am still finding myself going to GPT-4's API for the more nuanced things. They just released a v1.5 (https://huggingface.co/deepseek-ai/deepseek-coder-7b-instruc...), but for some reason, they reduced the context length from ~16k to ~4k. reply sestinj 8 hours agoprevWe've been playing with the 1.3b model for continue.dev's autocomplete and it's quite impressive. One unclear part is whether the license really permits commercial usage, but regardless it's exciting to see the construction of more complex datasets. They mention that training on multiple tasks (FIM + normal completion) improves performance...wonder whether training to output diffs would be equally helpful (this is the holy grail needed to generate changes in O(diff length) time) reply DeepSeek 5 hours agoparentHello sestinj, I work at DeepSeek, and I'm glad to hear that our models work for you! DeepSeek-Coder models are under a permissive license that allows for both research and unrestricted commercial use. We claim no rights on and take no responsibility from the output generated by the model induced by user prompts. Feel free to deploy and use DeepSeek models for any creative projects. We are a starup company and would like to concentrate on building better models, so it would be best if the users can help create a healthy ecosystem. Should you have any questions or requirements, I'm always happy to support. reply explorigin 6 hours agoparentprev> This code repository is licensed under the MIT License. The use of DeepSeek Coder models is subject to the Model License. DeepSeek Coder supports commercial use. Says so on https://huggingface.co/deepseek-ai/deepseek-coder-6.7b-instr... They have their own license to prevent things like propaganda or military use. reply elwebmaster 9 hours agoprevMixtral > Codellama > DeepSeek Coder. Very weird model, writes super long comments on one line, definitely not at the level of Codellama, benchmarks be damned. reply babyshake 2 hours agoparentJust curious, why do you think your evaluation is the opposite of what is shown in this evaluation? https://evalplus.github.io/leaderboard.html reply apawloski 5 hours agoparentprevWhich mixtral are you using with ollama? I have 32GB M1 MacBook Pro and can‚Äôt seem to load it / get any time-realistic responses reply chrisweekly 8 hours agoparentprevtangent: I often have a hard time disambiguating the \">\" in comparisons like yours: (A) greater than (ie, Mixtral superior to DeepSeek, w/ Codellama in between) vs (B) arrow/sequence (ie, start w/ Mixtral, progress to Codellama, finally land on DeepSeek as the culmination). I'd love to hear of a less ambiguous way to represent these. reply mcny 7 hours agorootparentI personally use a -> b -> c for sequence, a > b >> c for nested hierarchy. In this context, I read it as a is better than b which is better than c. reply mooreds 8 hours agorootparentprevHow does Mixtral >= Codellama >= DeepSeek Work for you? reply illusive4080 8 hours agorootparentSadly that implies that all 3 could be equal or very close to equal. reply high_priest 7 hours agoparentprevMixtral looks interesting, but I haven't dabbled in locally hosted LLMs. Would you mind linking to a concise text which could lead me through setting up Mixtral on my own machine? reply aussieguy1234 0 minutes agorootparentI'm using Mixtral, but rather than shell out for a gaming laptop with an expensive GPU, I simply run it via Together.ai APIs which works out alot cheaper. There's a few similar services out there. reply baq 3 hours agorootparentprevHad zero experience, too. Turns out ollama does everything, literally. You just tell it to run a model and wait a bit for it to download. One (1) shell command total. reply findjashua 7 hours agorootparentprevLM Studio is the easiest way to do it reply elwebmaster 5 hours agorootparentprevOllama gui in WSL2 reply maxlamb 6 hours agoparentprevIs Mixtral better overall or for coding specifically (or both)? reply findjashua 9 hours agoparentprevdo you find Mixtral to be better than the new 70B one that Meta released a couple days back as well? reply findjashua 8 hours agorootparentdid a comparison on LM Studio - the answers are eerily similar, but Mixtral is way way faster. Codellama-70B is slow to the point of being unusable. (M1 Max, 64 GB RAM) reply _boffin_ 9 hours agoparentprevReduce the repetition penalty to 1 and that should fix it. reply Havoc 8 hours agoprevI‚Äôve been using their 7B with tabbyML. Works well but closer to a very smart code complete rather than generating much novel blocks of code reply illusive4080 8 hours agoparentFells like all Gen AI is ‚Äòvery good code complete‚Äô because if you give it a broad problem it‚Äôll make mistakes. reply chii 4 hours agoprevJust tried it by asking how to create a game that is turn based, using an ECS system, and how to add a decision tree, and a save/load system, in the language Haxe. It outputs relatively correct haxe code, but it did halucinate that there is a library called 'haxe-tiled' to read tmx map files... reply byyoung3 10 hours agoprevlooks like code llama 70B outperforms on humaneval I believe reply SparkyMcUnicorn 9 hours agoparentLooks like even deepseek-coder 1.3b benchmarks higher than CodeLlama 70b. https://evalplus.github.io/leaderboard.html reply hackerlight 11 hours agoprev [‚Äì] In the benchmarks, are they using the base GPT-4, or are they using a GPT like Grimoire which will be better at coding? If they aren't using Grimoire, isn't it unfair to compare their fine tuned model to base GPT-4? reply BugsJustFindMe 11 hours agoparent [‚Äì] Is it unfair if GPT4 still beats them handily? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "DeepSeek Coder is an AI language model designed for code and natural language processing in both English and Chinese.",
      "It comes in various model sizes and has demonstrated strong performance on coding benchmarks.",
      "DeepSeek Coder is open source and can be used freely for research and commercial purposes, and instructions for using it can be found on their website."
    ],
    "commentSummary": [
      "The discussion centers around the use of AI coding tools and services like ChatGPT, Bard, ollama, and DeepSeek for tasks like code mapping, refactoring, and bug tracking.",
      "Some participants have had positive experiences, while others have encountered limitations, prompting suggestions like using Retrieval-Augmented Generation (RAG) with a Langchain setup or copilot for repetitive tasks.",
      "The future of AI in coding is explored, highlighting concerns about correctness and the importance of human expertise. The scalability and challenges of using API specifications, code generation models, and different AI models are also discussed. Comparisons are made between various AI-powered coding tools and services based on performance, ease of use, and specific capabilities."
    ],
    "points": 182,
    "commentCount": 45,
    "retryCount": 0,
    "time": 1706737420
  },
  {
    "id": 39204622,
    "title": "RavenDB's ACID Transactions Under Scrutiny: Inconsistent and Confusing Behavior Revealed",
    "originLink": "https://jepsen.io/analyses/ravendb-6.0.2",
    "originBody": "1 Background RavenDB is a distributed document database which repeatedly advertises its support for ACID transactions.1 It‚Äôs intended for OLTP workloads, and offers a variety of ETL paths for exporting data to other systems. Its transactional API revolves around a session handle, which ‚Äúrepresents a single business transaction on a particular database.‚Äù Users create a session, perform operations like reads and writes, and finally call session.saveChanges() to commit their writes as an atomic unit. RavenDB can replicate data across a set of nodes with automated failover. Sharding is either a work in progress or ready in 6.0, depending on which part of the documentation you‚Äôre reading. RavenDB includes secondary indices wrapped with a homegrown query language, multiple revisions of documents, time series datatypes, and CRDT-based counters. In this text, we‚Äôll focus on RavenDB‚Äôs transactional key-value operations. 1.1 Replication Per RavenDB‚Äôs High Availability page, the database accepts writes and reads across all nodes in the cluster. It uses the Raft consensus algorithm, which should theoretically allow RavenDB to provide consistency models up to Strong Serializability. However, that page goes on to say operators can ‚Äúeasily setup a topology in which end points operate ‚Ä¶ independently in case the network is disrupted.‚Äù In a blog post, CEO Oren Eini repeats this claim: If a node is located in a place where the internet connectivity goes down, that node can continue to operate offline, taking in data locally. Once the connection is restored, the node will take the data it processed and replicate it throughout your cluster. This would make ACID transactions impossible. The ‚ÄúI‚Äù in ACID refers to ‚ÄúIsolation‚Äù: transactions must appear to execute independently, without interference from other transactions. This property is formalized as Serializability: equivalence to some totally ordered, non-concurrent execution of transactions. We know that totally available systems cannot offer Serializability or even Snapshot Isolation. RavenDB might offer Causal or Read Committed, but the stronger consistency models are theoretically off-limits. A second page on high availability explains that there are two layers within RavenDB, and that Raft is used only for cluster metadata: First, the cluster layer is managed by a consensus protocol called Raft. In CAP theorem it is CP (consistent and partition tolerant). The second layer, the database layer, is AP (it is always available, even if there is a partition, and it‚Äôs eventually consistent) and is handled by a gossip protocol between the databases on different nodes, forming multi-master mesh and replicating data between each other. RavenDB utilizes the different layers for different purposes. At the cluster layer, the consensus protocol ensures that operators have the peace of mind of knowing that their commands are accepted and followed. At the database layer you know that RavenDB will never lose writes and will always keep your data safe. This is also confusing. AP systems are known for availability, not safety; lost update is a well-understood problem in AP registers. RavenDB claims to offer transactions with ACID guarantees. However, these transactions are apparently routed through an eventually-consistent, totally available replication system. There are databases which couple an (e.g.) Sequential transaction coordinator to an eventually-consistent datastore to provide Serializability, but it‚Äôs not clear from this documentation how RavenDB links cluster and database layers together to ensure safety. The Inside RavenDB chapter on cluster design confirms that Raft is used only for cluster metadata. Writes are allowed on every node, and are totally available: RavenDB uses multi-master replication inside a database, and it‚Äôs always able to accept writes. In other words, even if the majority of the cluster is down, as long as a single node is available, we can still process reads and writes. On the other hand, RavenDB‚Äôs ACID Transactions in NoSQL post claims the opposite: As in the single node version, RavenDB commits a transaction with just one round of Raft consensus. If Raft is involved in the transactions, RavenDB can offer up to Strong Serializability‚Äîbut transactions cannot be totally available. Indeed, RavenDB‚Äôs clustering documentation clarifies there are actually two separate transaction paths. The default mode is called a single-node transaction, which allows conflicts ‚Äúwhen two clients try to modify the same set of documents on two different database nodes.‚Äù A cluster-wide transaction uses Raft to prevent conflicts, allowing transactions to ‚Äúfavor consistency over availability.‚Äù To execute a cluster-wide transaction, one must set TransactionMode = CLUSTER_WIDE. What safety properties do these transaction paths guarantee? For this, we need to consider RavenDB‚Äôs ACID claims in detail. 1.2 ACID RavenDB‚Äôs home page prominently advertises ‚ÄúACID database transactions‚Äù ‚Äúacross multiple documents and across your entire cluster.‚Äù Its ACID Database Transactions page explains that a database without transactions is ‚Äúnot much of a database.‚Äù It boasts that RavenDB ‚Äúguarantee[s] ACID without sacrificing performance‚Äù and notes that because of its distributed ACID guarantees, ‚Äúdevelopers are exempt from handling the numerous scenarios of partial data transfers and the intricacies of data storage.‚Äù RavenDB‚Äôs ACID Transactions in NoSQL article explains that RavenDB ‚Äúwas capable of multi-document transactions since version 1.0‚Äù: Because it was optimized with this in mind, there wasn‚Äôt even a need for a non-ACID option. Any combination of database operations can be combined into an ACID transaction. As a user you never needed to implement ACID guarantees yourself, and you were free to design documents around your own requirements‚Ä¶. RavenDB was designed to make one and only one round trip to the server per transaction. RavenDB‚Äôs version of the session object tracks a series of commands, collects them as a batch, and sends them all to the server in a single round-trip when the method session.saveChanges() is called. Again, RavenDB claims to have been ‚Äúthe pioneer database to offer ACID in a nonrelational context. In 2010, RavenDB offered ACID consistency across multiple documents.‚Äù However, these guarantees held only on a single node: concurrent clients on different nodes could violate isolation. RavenDB 4.0, released in fall 2020, introduced the cluster-wide transaction path, which took transactions ‚Äúfrom being ACID over multiple documents to being ACID over your entire cluster.‚Äù The Transaction FAQ says ‚Äúall actions performed on documents are fully ACID‚Äù but contradicts itself immediately, saying ‚Äúin a single transaction, all operations operate under snapshot isolation.‚Äù DBDB takes this to mean that RavenDB offers Snapshot Isolation by default.2 In a 2020 webinar, CEO Oren Eini confirmed this position: ‚ÄúRavenDB uses Snapshot Isolation by default, and transactions are effectively going to observe Serializable between operations that happen on the same node.‚Äù There are hints that RavenDB might provide something much weaker than Snapshot Isolation. Buried in the Inside RavenDB book, in the chapter on document modeling, is a section on concurrency control. This section explains that RavenDB (at least in version 4.0) performed no concurrency control, and instead used Last Write Wins conflict resolution by default. What happens if two requests are trying to modify the same document at the same time? That depends on what, exactly, you asked RavenDB to do. If you didn‚Äôt do anything, RavenDB will execute those two modifications one at a time, and the last one will win. There‚Äôs no way to control which would be last. Note that both operations will execute. This is the opposite of ACID isolation. Isolated transactions appear to execute sequentially, not concurrently. It also contradicts claims of Snapshot Isolation: Last Write Wins registers allow all kinds of anomalies which would be prohibited under Snapshot Isolation, including lost update. However, this book is two major releases out of date; it may not apply to 6.0.2. What about cluster-wide transactions? The Cluster Transactions page seems definitive. ‚ÄúConcurrent cluster-wide transactions are guaranteed to appear as if they are run one at a time (serializable isolation level).‚Äù3 From this, Jepsen infers that RavenDB‚Äôs default transaction settings should ensure Snapshot Isolation by default and Serializability in a single-node system. Cluster-wide transactions should ensure Serializability globally. 2 Test Design In 2020 RavenDB wrote their own Jepsen test and declared in a webinar that per that test, ‚Äúeverything works.‚Äù Their test checked the linearizability of individual reads and writes against a single document. It did not evaluate multi-operation or multi-document transactions.4 We designed a new test harness for RavenDB 6.0.2 running on a single Debian Bookworm node. Our test used RavenDB‚Äôs JVM client library at version 5.0.4. We did not evaluate multi-node clusters or any kind of faults. We wrote a single list-append workload using Elle to verify transactional isolation. This workload performs transactions over lists, each list identified by a unique integer ID. Each transaction consists of reads and/or appends of unique integers to those lists. Each worker thread in the test opens a single DocumentStore connected to the same node. Each transaction creates a new session, performs reads and/or appends, then calls session.saveChanges() to commit. Reads are encoded as a single call to session.load(java.util.Map, id). Appends call session.load to read the current value, add their integer element to the end of the list, then call session.store(map, key). RavenDB offers a few knobs for tuning transaction safety: transactionMode and optimisticConcurrency. We ran our tests using the defaults (single-node transactions, no optimistic concurrency), with single-node transactions and optimistic concurrency, and finally with cluster-wide transactions. Cluster-wide transactions cannot be combined with optimistic concurrency. 3 Results We found surprising safety errors in all three transaction modes. 3.1 Lost Update with Single-Node Transactions (#17927) By default, RavenDB executes transactions with transactionMode = SINGLE_NODE and optimisticConcurrency = false. One might assume that SINGLE_NODE transactions are safe on single-node clusters. However, we found the default settings caused RavenDB to lose updates constantly, even in single-node clusters without faults. For instance, in this five-second test run we performed 12,886 transactions over 975 keys. 81 of those keys exhibited a provable lost update. Here are two committed transactions involving key 830: [[:r 830 [1 2]] [:append 824 11] [:append 807 14] [:append 830 3]] [[:r 830 [1 2]] [:r 831 nil] [:r 831 nil] [:append 830 4]]}]} Both of these transactions read key 830‚Äôs value as the list [1, 2]. Both went on to append a value to key 830: the first transaction appended 3, and the second transaction appended 4. Neither saw the other‚Äôs effects. In a Snapshot Isolated system, the first-committer-wins rule demands that one of these transactions must abort. RavenDB, however, allowed both transactions to commit. This is the definition of a lost update anomaly. In an isolated transaction system which only ever appends elements to lists, every observed version of a single list must be a prefix of the longest version of that list. However, 454 of the keys in this test violated this prefix property, exhibiting incompatible orders. For example, here are all the reads of key 116: Time (s) Process Value 3.01 1 [1 2] 3.01 1 [1 2] 3.01 0 [1 2] 3.01 1 [1 2 3] 3.01 0 [1 2 4] 3.01 1 [1 2 4 5] 3.01 0 [1 2 4] 3.02 1 [1 2 4 6] 3.02 1 [1 2 4 6] 3.02 1 [1 2 4 6 9] 3.02 1 [1 2 4 6 9 10] 3.02 1 [1 2 4 6 9 10 11 15] 3.02 0 [1 2 4 6 9 10 11 15] Just over three seconds into the test, process 1 observed key 116‚Äôs state as [1 2 3]. However, an immediately following read by process 0 saw [1 2 4], and the write of 3 never appeared again. Process 1 then observed [1 2 4 5]. This write of 5 was replaced by 6 and never seen again. Our lost update checker is conservative: it only infers an anomaly if two transactions read the same version of some key and both write to it. However, our append operations are performed by reading the value, then writing back changes. This means these transactions contain reads which are effectively invisible to the checker. It seems likely that these cases of incompatible order also represent lost updates. In total, 481 out of the 975 keys in this test exhibited lost updates or incompatible orders. These phenomena are prohibited by Serializability, Snapshot Isolation, and Repeatable Read. We‚Äôve reported this as issue 17927 in RavenDB‚Äôs issue tracker. 3.2 Fractured Reads with Optimistic Concurrency (#17929) With the optimistic concurrency feature enabled, RavenDB promises to ‚Äúgenerate a concurrency exception (and abort all modifications in the current transaction) when the document has been modified on the server side after the client received and modified it.‚Äù When running on a single node, this setting does appear to prevent lost updates. However, it allows fractured reads‚Äîas well as various flavors of G-Single, G-nonadjacent, and G2-item. Again, these anomalies occurred in a healthy single-node system. For instance, consider this ten-second test run in which every transaction enabled optimistic concurrency. Our checker found hundreds of anomalies like this: In this diagram the top transaction T1 appended 3 to key 271, then appended 2 to key 279. The bottom transaction T2 read key 271 and found nothing, appended 6 to key 276, and finally read key 279‚Äôs value as [2]. Because T2 failed to observe T1‚Äôs append to key 271, we have a read-write anti-dependency, denoted rw. Because T2 observed T1‚Äôs append to key 279, we have a write-read dependency, denoted wr. In short, T2 observed some, but not all, of the effects of T1. This anomaly is called fractured read, and it is prohibited under Read Atomic, Update Atomic, Causal, Prefix, Parallel Snapshot Isolation, Snapshot Isolation, Repeatable Read, and Serializable. RavenDB‚Äôs Transaction FAQ promises Snapshot Isolation: ‚Äúeven if you access multiple documents, you‚Äôll get all of their state as it was in the beginning of the request.‚Äù In all Snapshot Isolated databases Jepsen is familiar with, snapshots extend across multiple reads. In RavenDB, it appears each read can observe a different state. We‚Äôve reported this as issue #17929 to RavenDB. 3.3 Fractured Read with Cluster-Wide Transactions (#17928) Cluster-wide transactions are supposed to be Serializable. However, we found that even healthy, single-node clusters in which every transaction used CLUSTER_WIDE mode routinely exhibited fractured reads, as well as G-single, G-nonadjacent, G2-item, and more. Consider this five second test run, which contained hundreds of serializability violations. Here is one of those anomalies: Here, the bottom transaction T2 appended 6 to key 146 and 1 to key 149. The top transaction T1 failed to observe T2‚Äôs append to key 149, but did observe its append to key 146. This is another instance of fractured read. As before, this behavior appears to be proscribed by RavenDB‚Äôs documentation, as well as all consistency models above Read Atomic. We‚Äôve reported this to RavenDB as issue #17928. ‚Ññ Summary Event Required Fixed in 17927 Lost update with single-node transactions None Unresolved 17929 Fractured read with optimistic concurrency None Unresolved 17928 Fractured read with cluster-wide transactions None Unresolved 4 Discussion RavenDB variously claims to offer ‚Äúfully ACID‚Äù transactions, Serializability, or at least Snapshot Isolation. All of these claims appear false. RavenDB 6.0.2‚Äôs default settings allowed lost updates. Even cluster-wide transactions exhibited fractured reads: a serious anomaly prohibited under Snapshot Isolation, as well as several weaker models. These behaviors occur even in healthy, single-node, single-shard systems, in which all access occurs via primary key. RavenDB‚Äôs strongest safety settings violate Read Atomic. It therefore cannot satisfy Update Atomic, Causal, Prefix, Parallel Snapshot Isolation, Snapshot Isolation, Repeatable Read, or Serializable. RavenDB might offer Read Committed or Monotonic Atomic View, but without more rigorous testing, Jepsen is hesitant to make this claim. RavenDB‚Äôs weak default behavior is surprising given RavenDB‚Äôs repeated emphasis on safety. As CEO Oren Eini remarked on MongoDB‚Äôs transaction safety settings: [Default] values matter. They matter quite a lot. Why is that? Because if you choose the bad values, you‚Äôre absolutely going to get some great numbers in benchmark performance. But then you are going to be hitting those [safety] issues in production. And then there is this classic response: ‚ÄúOh, you should have read the docs and used the proper configuration.‚Äù One wonders: if ACID properties are so important for RavenDB‚Äôs users, why do the default settings allow lost updates, even on single-key operations? Do users realize their updates can be silently discarded? How many are taking care to use cluster-wide transactions where lost updates would violate safety? Do they know that even cluster-wide transactions allow fractured read? This report follows a cursory investigation into RavenDB‚Äôs behavior‚Äîit is by no means exhaustive. As always, we caution that Jepsen takes an experimental approach to safety verification: we can prove the presence of bugs, but not their absence. There may be other anomalies in RavenDB. 4.1 Does RavenDB Even Have Transactions? The first sentence of RavenDB‚Äôs cluster transaction documentation appears quite clear: A session represents a single business transaction. This is echoed by the first sentence of RavenDB‚Äôs session documentation: The Session, which is obtained from the Document Store, is a Unit of Work that represents a single business transaction on a particular database. ‚Ä¶ which goes on to say: The batched operations that are sent in the SaveChanges() will complete transactionally. In other words, either all changes are saved as a Single Atomic Transaction or none of them are. So once SaveChanges returns successfully, it is guaranteed that all changes are persisted to the database. RavenDB sessions are clearly not intended to work like sessions in typical databases, which are (roughly speaking) one-to-one with client connections.5 They come with a default limit of 30 network requests; typical database sessions are unbounded. They buffer writes; Jepsen is unaware of any other database whose sessions do this. They cache reads; most sessions do not. They include concurrency control mechanisms like lost update prevention; Jepsen is unaware of any other database which does this at the session level. These are all hallmarks of what most databases would call a transaction. RavenDB‚Äôs article ACID Transactions in NoSQL? RavenDB vs MongoDB is emphatic: RavenDB has supported ACID transactions over ‚Äúany combination of database operations‚Äù for over a decade. It certainly appears as if RavenDB sessions are intended for this role! However, in a response to issue 17927, Eini (a.k.a. Ayende Rahien) explained that sessions are not in fact transactions:6 Crucially, RavenDB does not attempt to provide transactional semantics over the entire session, rather it provide[s] transactions over individual requests. And in response to issue #17928, Eini affirms: A transaction in RavenDB is a request - so TX1 and TX2 above aren‚Äôt actually single transactions, instead, each of them represent 3 independent transactions. This is a striking viewpoint: the point of transactions is generally to provide isolation across multiple requests.7 Moreover, RavenDB‚Äôs optimistic concurrency and cluster-wide transaction mechanisms are clearly intended to provide transactional isolation which spans from a session‚Äôs reads to its writes. Furthermore, Eini directly compared RavenDB sessions to MongoDB transactions (which offer typical interactive transaction semantics) and claimed that unlike MongoDB, RavenDB sessions actually satisfied Snapshot Isolation. Yet per Eini‚Äôs comments, RavenDB does not have interactive transactions at all. Repeatedly advertising ‚ÄúACID transactions‚Äù across ‚Äúany combination of database operations,‚Äù telling users that a ‚Äúsession represents a single business transaction,‚Äù comparing RavenDB sessions to interactive transactions in other databases, offering concurrency control mechanisms whose scope extends across an entire session, and finally expecting users to realize that sessions are not transactions at all‚Äîthat a transaction is actually limited to a single HTTP request‚Äîstretches credulity. Jepsen strives to evaluate databases in the context of their marketing and documentation. Although RavenDB‚Äôs CEO now states ‚Äúwe don‚Äôt support a transaction over more than a single HTTP request,‚Äù RavenDB‚Äôs documentation and marketing give every appearance that a session is intended to be a transaction. Jepsen has consulted with several software engineers on their interpretation of these claims, and believes typical database users would come to the same conclusion: RavenDB sessions are transactions. We continue this interpretation throughout this report. 4.2 Recommendations RavenDB users should be aware RavenDB transactions are not ACID in any meaningful sense.8 This holds even in single-node, single-shard deployments. The defaults allow lost updates: you should expect some of your writes to be silently discarded. The strongest safety settings allow fractured read: you might observe some, but not all, of another transaction‚Äôs effects. You could appear to write ‚Äúinto the middle‚Äù of another transaction. The two isolation levels RavenDB advertises‚ÄîSnapshot Isolation and Serializable‚Äîappear impossible to obtain. Users who designed their applications assuming RavenDB provided interactive ACID transactions‚Äîor even Snapshot Isolation‚Äîshould carefully reevaluate their transactions to ensure they are safe in the presence of these anomalies. Consider writing simple tests to verify application invariants are preserved under concurrent execution: the issues in this report are easy to reproduce. Jepsen recommends RavenDB remove claims of ‚ÄúACID‚Äù, ‚ÄúSerializable‚Äù, and ‚ÄúSnapshot Isolation‚Äù from their marketing materials and documentation. RavenDB should instead make specific, accurate, and internally consistent claims about safety properties. For instance, RavenDB might say ‚Äútransactions offer Read Committed by default, plus internal consistency within the scope of a transaction: once a transaction reads a key, subsequent reads and writes of that key observe the originally read state, plus the effects of that particular transaction‚Äôs writes. Transactions allow lost update by default. Enabling cluster-wide transactions prevents lost update, but still allows fractured read,‚Äù and so on. RavenDB‚Äôs documentation is remarkably confusing. It repeatedly claims to offer ACID transactions, which implies Serializability. There are specific claims that RavenDB ensures either Serializability or Snapshot Isolation. However, the documentation also says that RavenDB‚Äôs database layer is an AP system based on Last Write Wins, and the marketing material claims isolated nodes can operate independently. This is impossible: totally available systems cannot provide Serializability or Snapshot Isolation.9 There are systems (like Riak & Cassandra) which allow clients to execute either totally available operations with weak consistency, or majority available operations with stronger guarantees, like Linearizability. If RavenDB intends to build a system which supports both modes, they should clearly distinguish those modes throughout marketing and documentation. They have completely different availability, latency, and safety characteristics. Repeated claims that RavenDB provides ACID ‚Äúwithout sacrificing performance‚Äù are provably impossible, and should be rewritten to clearly explain the tradeoffs involved. ACID transactions are clearly important to RavenDB. It is therefore alarming that RavenDB‚Äôs documentation and GitHub comments fundamentally disagree on what a transaction is. In one interpretation, RavenDB offers interactive transactions, represented by the session API, which provide relatively weak isolation‚Äîcertainly not ACID. In another interpretation, RavenDB lacks interactive transactions altogether. Instead, it offers a sort of micro-transaction which (e.g.) writes multiple documents in a single network request. In this world, sessions offer varying, weak consistency constraints that extend between micro-transactions. To resolve this confusion, RavenDB should pick a single definition of ‚Äútransaction‚Äù and stick with it. The equivalence or difference between a transaction and session should be clearly explained, and these terms used consistently throughout marketing and documentation. RavenDB should provide guidance as to the boundaries of each unit: when are multiple calls to load performed in a single transaction? What about store? Can a single transaction encompass both a load and store? The consistency properties of both transactions and sessions should be clearly and formally defined. Are transactions Serializable? Do sessions ensure Monotonic Atomic View? When does a session preclude lost update, and when does it allow it? Above all, do not tell users that sessions ‚Äúrepresent a single business transaction‚Äù if they are, in point of fact, not transactions at all. Finally, if RavenDB transactions are truly intended to cover only a single network request, consider using a different term altogether, and avoid comparisons to databases which do have interactive transactions. Some databases call these ‚Äúmini-‚Äù or ‚Äúmicro-transactions,‚Äù which provides an obvious hint of their limited scope. 4.3 Future Work This work evaluated only single-node RavenDB clusters without faults. Future research could expand tests across multiple nodes, as well as introducing network, process, and disk faults. We dealt only with key-value operations, and did not evaluate RavenDB‚Äôs secondary indices. These indices are described as eventually consistent, which raises questions around the integrity of predicate reads. RavenDB also offers server-side transactions using Javascript or a library of built-in patch operations. These might offer different safety characteristics than the interactive transactions we used in this report. Finally, cross-shard transactions are a notoriously challenging problem and deserve careful testing. Jepsen wishes to thank Irene Kannyo for her invaluable editorial support. Thanks as well to C. Scott Andreas, Taber Bain, Silvia Botros, Coda Hale, Ben Linsay, Kelly Shortridge, Nathan Taylor, Zach Tellman, and Leif Walsh for their comments on early versions of this manuscript. This work was performed independently without compensation, in accordance with the Jepsen ethics policy. As discussed in section 4.1, the interpretation of a ‚Äútransaction‚Äù in RavenDB is complicated. In this report, we identify RavenDB‚Äôs session API as an interactive transaction.‚Ü©Ô∏é Technically, this isn‚Äôt clear from the documentation alone: Snapshot Isolation is a property of histories of transactions, but the docs discuss behavior only within the scope of a single transaction. RavenDB might have meant some weaker property here‚Äîfor instance, Prefix Consistency. However, the CEO‚Äôs claims of Snapshot Isolation by default seem authoritative.‚Ü©Ô∏é It is tempting to believe that cluster-wide transactions are the ACID transactions RavenDB‚Äôs marketing boasts, but this cannot be the case. RavenDB says it‚Äôs offered ACID transactions since 2010, and cluster-wide transactions weren‚Äôt introduced until roughly a decade later. This may reflect confusion over what ‚ÄúACID‚Äù means.‚Ü©Ô∏é RavenDB‚Äôs Jepsen test may not have measured anything at all: at least in the most recent revision, the generator included no client operations of any kind.‚Ü©Ô∏é There are scenarios in which a database multiplexes multiple sessions onto a single connection, or migrates a session across connections. It might be more apt to think of a typical database session as ‚Äúa logical, single-threaded connection.‚Äù‚Ü©Ô∏é In the same thread, Eini remarks that ‚Äúwe only consider transactions to be the calls to SaveChanges or other data mutation operations.‚Äù This is somewhat alarming: it implies that RavenDB transactions don‚Äôt encompass reads at all. On the other hand, a transaction involving a single HTTP request ‚Äúapplies to reads as well,‚Äù so it‚Äôs not quite clear what RavenDB‚Äôs read safety semantics are.‚Ü©Ô∏é There are databases, like FaunaDB, where transactions are written as small programs and submitted to the database in a single request. RavenDB, like most databases, provides interactive sessions: clients make calls to load and store interspersed with arbitrary local computation, and call saveChanges() to commit their effects. In these kinds of systems, transactions typically encompass multiple read and write requests.‚Ü©Ô∏é Alternatively, ‚ÄúRavenDB does not have interactive transactions at all.‚Äù Readers may select their favorite interpretation throughout this report.‚Ü©Ô∏é At least, not in a network which can partition.‚Ü©Ô∏é",
    "commentLink": "https://news.ycombinator.com/item?id=39204622",
    "commentBody": "RavenDB 6.0.2 (A Jepsen Report) (jepsen.io)182 points by aphyr 18 hours agohidepastfavorite63 comments hudo 18 hours agoI was using Raven around ver 1-3. Even it was single node and simple app, we observed stale reads and lost writes so had to eventually migrate to SQL Server. It was really weird reading claims from Oren (expert in .NET space, famous from his great work on Nhibernate and few other frameworks), where his db didn't work as advertised at all (back then build with Esent key/value store + full text search for map/reduce, think it was lucene.net - obviously very broken tech for this purpose). Too bad, was really hoped things were fixed by now, Db has really good programming APIs. Interesting trivia: there's \"raven db done right\" - https://martendb.io/ , just an API wrapper around PSQL. Named Marten because thats a natural enemy of ravens:) reply jf22 17 hours agoparentFunny, I also had to abandon Raven around that time for the same reasons and ended up using MartenDb years later. MartenDb is great and the community around it is excellent. reply HdS84 17 hours agoparentprevI've used ravemsb 3 and did not observe auch problems. If I remember correctly, sent was replaced around ravemsb 2 and licence is now optional reply AlfeG 13 hours agorootparentWe have abandonded raven because server tends to go into some \"repair\" state for hours eating 16+ Gb of memory. Multiple times on production. It were easier to migrate to posgres, then try to fix ravendb reply mjb 16 hours agoprevAs database builders and users, we‚Äôve made talking about systems a lot harder on ourselves by conflating the ideas of replication, active-active, atomic commitment, and concurrency control. - Replication is a technique used to achieve higher availability and durability than a single node can offer, by making multiple copies of the data. Techniques include Paxos, Raft, chain replication, quorum protocols, etc. - Active-active means that transactions can run against multiple different replicas at the same time, while still achieving the desired level of isolation and consistency. - Atomic commitment is a technique used in sharded/partitioned databases (which themselves exist to scale throughput or size beyond the capabilities of a single machine) to allow transactions to be atomically (‚Äúall or nothing‚Äù) committed across multiple shards (and allow one or more shards to vote ‚Äúnah, let‚Äôs not commit this‚Äù). 2 phase commit (2PC) is the classic technique. - Concurrency control is a set of techniques to implement isolation, which is needed in any database that allows concurrent sessions (single node or multi-node). Classic techniques include 2PL and OCC, but many exist. When vendors or projects answer concurrency control questions with replication answers (which appears to be the case here), it‚Äôs worth diving deeper into those answers. There are cases where ‚ÄúPaxos‚Äù or ‚ÄúRaft‚Äù might be answers to atomic commitment or even concurrency control questions, but at best they are very partial answers and building blocks of a larger protocol. Databases that only support ‚Äúsingle shot‚Äù/predeclared transactions can get away without a lot of concurrency control, for example, and might be able to do the required work as part of their state machine replication protocol. In general, I'd see using words like \"Paxos\" and \"Raft\" in the marketing for a database as a negative sign. It's not a fully reliable one, but it's often the least interesting part of the implementation and the choices the database is making. To be extra clear, I‚Äôm not criticizing Aphyr here (the article clearly doesn‚Äôt conflate these concepts), but more pointing out what I think lies at the bottom of a lot of the issues we see with distributed database claims. reply jwr 15 hours agoparentI think Aphyr helped a lot by creating this useful resource: https://jepsen.io/consistency which presents a clear classification of consistency models. I am not sure if talking about anything else in the context of distributed databases is reasonable. reply mjb 14 hours agorootparentThat is one way (and a good one) of classifying consistency models and there relationship to isolation levels. But it's an incomplete one (e.g. there are linearizable variants of snapshot and repeatable read that exist that are not captured there). I'm a big fan of that stuff (and Aphyr's work in general), but that page is the beginning of a conversation and not the end of one. reply aphyr 14 hours agorootparentYou'll find many of those additional variants (e.g. strong session SI) in the linked papers, and they're cited extensively in the Jepsen reports as well. I just haven't had time to write up every single model--tried to stick to the major ones. :-) reply mjb 13 hours agorootparentI was reacting to this: > I am not sure if talking about anything else in the context of distributed databases is reasonable. There's a whole world in distributed databases, and I suspect you'd agree that there's a lot of stuff worth talking about that isn't covered in your (excellent) work. reply aphyr 9 hours agorootparentYeah, of course :-) reply CJefferson 18 hours agoprevI love Jepsen, but it seriously worries me how bad software turns out to be, and how many outrageous claims companies make that turn out to be so easily proved false. Should there be more serious penalties when companies make claims which turn out to be false as soon as they are tested? I think there should be. reply skyde 18 hours agoparentWell some company try to implement Raft algorithm with good intentions but do a bad job a at so bugs make it so they fail Jensen test. You can‚Äôt force people to test their software correctly unless it‚Äôs a regulated field like aviation. reply okigan 18 hours agorootparentnext [7 more] [flagged] lambda 16 hours agorootparentAviation has an extremely strong safety record, which has been getting better year by year. Yes, there are misses, but they have been happening increasingly less frequently over time. https://en.wikipedia.org/wiki/Aviation_safety#/media/File:Fa... Aviation isn't perfect; nothing implemented by large groups of fallible humans with budget constraints will be. But it has one of the best safety track records of any industry. Now, why won't aviation style engineering be applied in other fields, like databases? Well, because no one really cares enough. No one dies if some random database used by some ad platform loses the occasional transaction. Yeah, it's frustrating to engineers who are trying to build reliable systems, but in the grand scheme of things losing a few percent of transactions isn't the end of the world for most businesses. You get safety cultures like that in aviation because there are real, substantial risks, so you need to have thorough engineering discipline, properly designed redundancy, etc. For databases that are used for the majority of the business world, efficiency is generally a bigger concern than correctness; they'd rather have cheap and fast databases that lose a few transactions occasionally than something that actually provides consistency. But of course everyone thinks they need consistency, so it's advertised as a selling point while not actually being provided in practice. reply CJefferson 13 hours agorootparentI'm positive people want their databases to be correct. There are databases which promise great speed in return for occasionally losing bits of your data, and they get very little use outside of special uses. reply simplyinfinity 12 hours agorootparentwant != bet your life on them reply _a_a_a_ 14 hours agorootparentprev> For databases that are used for the majority of the business world, efficiency is generally a bigger concern than correctness; they'd rather have cheap and fast databases that lose a few transactions occasionally than something that actually provides consistency 'majority' is a strong claim. Any figures to back it up? reply dallasg3 17 hours agorootparentprevAlso is Jepsen, not Jensen. reply myk9001 15 hours agorootparentNamed after Carly Rae Jepsen for her song Call Me Maybe. reply candiddevmike 18 hours agoparentprevI don't think any company explicitly claims their implementation handles X failure scenarios, certainly not in the EULA or license. You could possibly pick apart their documentation, but I think the warranty clause in most licenses would cover the product. This may not apply to regulated products dealing with health/safety/defense, and IANAL. Either way, best to assume most companies are lying through their teeth about any feature until you or someone you trust has validated it. reply Twirrim 17 hours agoprevWhen it comes to databases, that's when I get the most conservative in tech choices. Stick with the tried and tested approaches. Data/Metadata integrity is generally the single most important thing for whatever I'm working on. reply danielovichdk 14 hours agoparentI rarely used a RDBMS these days. But when I do I use a SQL Server. For the same reasons you just made. reply Cwizard 14 hours agorootparentWhat do you use instead? reply gigatexal 18 hours agoprevI love when Jepsen's reports hit HN. I always learn a ton about databases from them. Kudos to the projects brave enough to put their claims to the literal test. Jepsen is the best in the biz. reply ukd1 17 hours agoprevThis aged well - https://github.com/ravendb/ravendb/issues/13218#issuecomment... reply progbits 15 hours agoparentThey claim to run that test on any changes but in fact might not be testing anything at all according to this footnote in the Jepsen report: > RavenDB‚Äôs Jepsen test may not have measured anything at all: at least in the most recent revision, the generator included no client operations of any kind. Remember folks, if you can't get your test to fail by intentionally breaking the implementation, you don't have a test. reply ukd1 17 hours agoparentprevTickets related to this report from Aphyr: - https://github.com/ravendb/ravendb/issues/17928 - https://github.com/ravendb/ravendb/issues/17927 reply nemothekid 14 hours agorootparentVery amusing what marketing will come up with; RavenDB doesn't support actual transactions, but supports \"business\" transactions. reply PreInternet01 15 hours agoprevMy fun RavenDB story: I briefly used it for an analytics (music royalty data, not advertising) solution somewhere late in the 1.x release series. Ayende (the initial RavenDB author) was/is an avid blogger, and made a really good case for their product in the .NET ecosystem. It did not... go exactly as planned. Initial tests looked OK, but when I did testing with actual users, there were huge issues right away. Like: OK, I just ran your ingestion pipeline. What do you see? And the answer was 'well, nothing', or 'ehhm, a lot less than I expected'. These issues turned out to be pretty much impossible to fix: there were no real errors, but the data just seemed to... disappear randomly, even in a simple single-node cluster. I got community support involved in a bunch of particular issues, but nothing really helped: the aggregate numbers we got never added up to what they should be. I then migrated the whole thing to a single SQLite database. That file is, as I write this, a good 2TB in size, and still performs as well as the day it was deployed and never had any unexplained-number issues, without any changes to the surrounding code. I did eventually move away from the .NET Entity Framework (as that did cause some rare, yet unexpected and hard-to-fix concurrency issues, but those were hard crashes and not silent data corruptions) to a hand-rolled entity mapper, but all has been good since then... TL;DR: databases are very hard, and fashionable choices are not necessarily desirable. reply philipbjorge 14 hours agoprevWe used RavenDB 2-4 at Leafly. Won't go into battle scars here, but this report does not surprise me. We're much happier with Postgres and Elasticsearch. reply dramm 13 hours agoprevI need a brain colonic after reading though just some of the mess of overhyped claims in RavenDB marketing and documentation. I appreciate Aphyr doing all this wonderful work and how some of the Raven claims triggered that work. I'd have hoped that anybody building a critical system would have read the mess of Raven documentation/claims/hype and run the other way. reply jjirsa 17 hours agoprevIt‚Äôs disappointing to me that the technologist desire to experiment with new DBs continually puts naive customers at correctness and durability risk they don‚Äôt (won‚Äôt) understand. reply jwr 15 hours agoprevHealthy reminder that a pretty website and warm fuzzies all over do not make a distributed database actually work. I witnessed RethinkDB losing to MongoDB in spite of being significantly better. I am now worried that FoundationDB isn't gaining popularity, even though it is arguably the best and most well-tested distributed database out there, with strict serializability (!) guarantees. But it doesn't have a shiny website and doesn't cause warm fuzzies, quite the opposite, it looks complex and intimidating. So it isn't popular. This is worrying, but perhaps neither new nor surprising: we have a history of picking inferior solutions because the good ones looked too complex or intimidating (betamax vs VHS in video formats, ATM vs Ethernet in WANs). reply redwood 14 hours agoparentI had understood FoundationDB to be more akin to a storage engine (e.g. a sub-component of a DBMS) than a full-on DBMS. Was I misunderstanding? If so I bet a lot of people have this understanding, going back to your point on the web site/general sentiment in the zeitgeist not necessarily reflecting what they are. Can you share any more detail? Are you saying there are companies that build software on top of FoundationDB as their primary data store? or are those companies building software around FoundationDB that in turn presents as more of a data store in the traditional sense? reply jwr 57 minutes agorootparentIt's more of a database-building toolkit than a storage engine. What you get is a distributed KV store with a strict serializable consistency model (see https://jepsen.io/consistency) and very interesting versionstamp functionality. What you do not get is a \"query language\" or indexing. Yes, there are companies that use FoundationDB as their primary data store. It makes a lot of sense to integrate it directly with the application rather than go through additional layers and a \"query language\". I am working on adapting my app to use it, and so far very happy with the results. reply saled 12 hours agorootparentprevSnowflake uses foundationdb as their metadata store to find files in S3, though I believe the actual queries are run on separate nodes. https://www.snowflake.com/blog/how-foundationdb-powers-snowf... But yeah you're right for the most part. Turns out pretty much any database can be written in terms of transactions of KV pairs, which is what foundationdb gives you, so it means you can write your database query layer as a stateless, scalable service. There have been attempts to write a SQL RDMS layer for it but it isn't maintained. reply Cwizard 14 hours agoparentprevI think for foundationdb to gain traction, someone needs to build a frontend for it with more advanced query capabilities than KV. reply endisneigh 17 hours agoprevI‚Äôm still waiting on their report of foundationdb, which Kyle claims would readily pass their test so they didn‚Äôt bother to do one. reply aphyr 17 hours agoparentEven an unpaid report like this involves weeks of work: reading docs, designing tests, executing and refining them, filing issues, writing the report, and editing it. Each report costs thousands of dollars in hardware and and editing. I'd love to do an FDB test! It's just that between contracting, volunteer work, and research, I can only do so much. reply willvarfar 17 hours agorootparentWe'd all love you to be _paid_ to do an FDB test! What are your most tempting/daunting databases that you haven't got a chance to put through their paces? And, a second question, if you step back and think about the various APIs you've had to use, have you personally developed favourite styles of API to use? reply aphyr 15 hours agorootparentSame. Hi, Apple! :D I'd love to do more work with predicates in general. That's an open research problem I've been noodling on for years. Pretty much any SQL DB would be a good candidate for that work! I'm gonna be a weirdo and say I actually loved Fauna's FQL. A little Lisp-ish functional language for queries is a great way to interact with document-structured data. SQL is fantastic for sheer breadth, though its specification is a nightmare and actually writing portable SQL is real challenging. One of those places where a stronger spec and conformance tests would have really helped. reply freels 15 hours agorootparentThanks for the shoutout. At some point if you find yourself with some spare time you can check out our new FQL version. It's closer to JS in terms of syntax now, but still a small, relatively functional language. reply bsaul 16 hours agorootparentprevYour reports have reached such a huge notoriety, i'm surprised you don't already have a dozen of people working for you fulltime on benchmarking techs for wealthy customers. If you allow me : how many people you work with can actually perform those Jepsen Report ? Or is it only you ? reply aphyr 15 hours agorootparentIt's not a huge market. I usually have a queue of clients, but both deal flow and actual scheduling are wildly variable--sometimes I'll go without income for six months or more. I've considered hiring one or two people, but I couldn't offer the kind of stability people need from an employer. I do subcontract though! Editing, legal, finance, occasional code, that sort of thing. There's a lot of folks out there who can do basic testing work with Jepsen. I've taught... I dunno, maybe a few hundred people directly in Jepsen workshops. A couple people have worked alongside me, and I'm sure lots more have learned from the docs online. Writing a report is a more involved problem--certainly not intractable, but for me it involves testing, experiment design, lots of reading, doc review, writing, editing, finding reviewers, and of course all the business stuff. reply orthoxerox 12 hours agorootparent> There's a lot of folks out there who can do basic testing work with Jepsen. I've taught... I dunno, maybe a few hundred people directly in Jepsen workshops. I don't know why all the DBMS vendors don't just have a guy on the QA team whose job is to run and interpret Jepsen tests for every new version. It's certainly a better option than eventually getting a damning report written by you. reply bsaul 12 hours agorootparentprevThanks, that's very interesting. I would have assumed that with today's world of distributed database being so common, and with the various technologies available, lots of people would be interested in hiring experts in ensuring database work as advertized. But i guess people trust the product documentation too much. As an anecdote i was surprised to discover mongodb had a second life in the corporate world as a standard , certified technology to store critical documents. So yeah, maybe people aren't really that aware of the kinds of nasty gotchas that lure in their systems. reply endisneigh 17 hours agorootparentprevApologies - my comment was in jest, you certainly aren‚Äôt obligated to do anything :) reply CyanLite2 13 hours agoprevI gave up on RavenDB when Oren would post blog entries regarding interviews with candidates, bashing how they would fail his coding exams. I mean who posts that kinda stuff on their public website? reply JazCE 14 hours agoprevKelly Somers has been vindicated. reply RcouF1uZ4gsC 18 hours agoprev> AP systems are known for availability, not safety; I think in 99.9% of cases, you don't want AP. The P only matters when the network is more prone to go down than the machines. For example, if every node goes down, your AP design won't be available. With the massive improvements in network and connectivity and increased redundancy, you should aim for CP. If you really, really need AP, then a ground up design based on CRDTs seems the best, most discipline approach. With CRDT, you can have availability because the operations can be entirely local, and you know you can sync to the other nodes when available without conflict. reply mjb 15 hours agoparentCompletely agree. In-region (e.g. at the scale of a US state), CP seems like a clear winner. For more geo-distributed latency becomes challenging, and in environments like IoT and mobile unreliable connectivity becomes challenging. If you're there, you need a principled way of approaching AP (e.g. CRDTs or CALM https://arxiv.org/pdf/1901.01930.pdf). reply oliverpk 13 hours agoprevGenuinely one of my favourite posts here reply neonsunset 11 hours agoprevThe unfortunate thing is .NET deserves to have a proper database written in pure C# because the language offers all the tools to achieve a really performant, safe and cross-platform implementation. But RavenDB does not do it justice and uses unsafe in catastrophic amounts in places where it is not necessary or in ways which are straight up UB despite the fact that JIT/ILC is much more strict than GCC/LLVM. There have been multiple bug reports submitted to dotnet/runtime by RavenDB which required extensive debugging effort only to end up being an issue on RavenDBs end due to explicit misuse of unsafe APIs (in ways, I must reiterate, that have safe alternatives to achieve the same performance). (if anyone's interested, I can later ask around/dig through issue history and give the references) reply romanovcode 18 hours agoprev [‚Äì] RavenDB is expensive pay-to-use database. I do not understand why would one choose this over Postgres. reply dustedcodes 16 hours agoparentI once joined a team which used RavenDB and literally the highest priority work which they were working on at that time was migrating completely off RavenDB because they had lost all confidence in it. reply beoberha 18 hours agoparentprevI had never heard of it until today, but a quick Google tells you it‚Äôs a Mongo-like JSON database with multi master capabilities. If you really really want those (I‚Äôm not sure it‚Äôs a good idea), then this seems way better than PG. reply wmfiv 17 hours agorootparentIt's not better because it doesn't actually work. If you're not interested in a science experiment, Cassandra (or Scylla) are the multi-master databases that are mainstream and proven to work and scale. They're not fun or sexy and their feature set is much smaller but they do what they say and they work. Or AWS/GCP/Azure will happily give you an API for one. reply endisneigh 17 hours agorootparentIt‚Äôs funny you say this when they‚Äôve both failed jepsen tests. And by fail, I mean there are scenarios where they do not behave as intended - this is not to suggest either are broken inherently. reply wmfiv 17 hours agorootparentFor sure. Virtually every Jepsen test has found bugs even with extremely mature products like Postgres. But fundamentally Cassandra (and Scylla) and most other databases tested have fixed those bugs and improved their documentation with many incorporating Jepsen tests into their ongoing process. That's different from how Raven has just wildly mistated their capabilities. reply jjirsa 17 hours agorootparentprevCassandra‚Äôs jepsen is over 10 years old now. That database spent 3-4 years primarily focused on correctness from 2018-2022. The industry moves fast, our memories are slow. But there are millions of instances of cassandra in production across most of the fortune 500, and half of this thread has never heard of RavenDB reply lambda 16 hours agorootparentprev\"Failed Jepsen tests\" is relative; Kyle tends to be quite thorough in his Jepsen tests, so can usually find some issue in even the best of databases. The big difference is in how severe the difference between claims and reality are, and how the maintainers or vendors react to such bug reports. In some cases, the maintainers or vendors will fix bugs, or update docs to be more clear. The different transaction isolated levels are complicated and nuanced, and there are standards which disagree with the general consensus in the literature, so there can easily be ambiguities that need to be cleared up or bugs that need to be fixed. But then there are things like RavenDB; where they make clearly impossible claims like \"ACID across multiple documents and multiple cluster nodes in an AP database.\" There is simply no way to achieve this. And then there's how they respond to his findings. He filed bugs and had had responses a month ago, about things like \"this thing that is documented to have transactional semantics does not have transactional semantics\", and their response was just to say \"oh, yeah, that's expected\", and not fix anything or update any of their documentation to reflect that. So, there's a big gulf between \"this is a complex topic, and even some of the best systems have some issues if you test it thoroughly enough\", and \"the documentation is blatantly lying about transactions and consistency, and the CEO of the company doesn't think it's a problem.\" The docs for RavenDB explicitly state https://ravendb.net/docs/article-page/6.0/csharp/client-api/... The batched operations that are sent in the SaveChanges() will complete transactionally. In other words, either all changes are saved as a Single Atomic Transaction or none of them are. So once SaveChanges returns successfully, it is guaranteed that all changes are persisted to the database. But the tests in this post show that no, there is no single atomic transaction for a session saved with SaveChanges, even in a single node database this will lose writes. I dunno. If I paid for an ACID database, I'd expect, well, some ACID features, like the ability to run two transactions concurrently and have them be isolated. It looks like RavenDB is fundamentally not implementing even the most basic of its claims. This isn't some \"oh, yeah, this is a complex problem, and there are a few bugs lurking in obscure corner cases\", this is \"it fundamentally doesn't support what they claim to support as the major front-page selling point.\" Their front page selling point is \"Fully transactional NoSQL database\" which links to a page that says \"A database without transactions is‚Ä¶ well, not much of a database, and as far as transactions are concerned ‚Äì ACID is the gold standard.\" But then in response to these findings, they say that the thing that is documented to have transactional semantics doesn't actually have transactional semantics. reply beoberha 17 hours agorootparentprevI don‚Äôt disagree :) I‚Äôm just explaining to OP why someone would reach for it over PG reply HdS84 17 hours agoparentprev [‚Äì] Ravendb has an interesting use case. It's basically as easy to use as mongo DB, similarly scalable and at the same time is acid, has a nicer query language and is more secure. Iike it actually more than SQL, especially for typical LOb apps. reply jakewins 17 hours agorootparent [‚Äì] It appears it is, in fact, not ACID. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "RavenDB, a distributed document database, has raised concerns about its support for ACID transactions and its consistency in meeting those guarantees.",
      "Testing has revealed safety errors, including lost updates, fractured reads, and consistency model violations, raising questions about the behavior of transactions and sessions in RavenDB.",
      "Users are advised to reassess their transactions, conduct tests, and ensure that application invariants are maintained. RavenDB should provide clearer information on their safety properties and remove false claims from their marketing materials. Further research is needed to clarify the boundaries of transactions and sessions, as well as concerns about fault tolerance, secondary indices, and cross-shard transactions. Network partitions may impact RavenDB's performance."
    ],
    "commentSummary": [
      "Users of RavenDB have reported issues with stale reads, lost writes, and random data disappearance, prompting some to switch to other databases like SQL Server or MartenDb.",
      "The discussion touches on topics such as replication, atomic commitment, and concurrency control in databases, with the author suggesting that relying solely on replication may not be the best solution.",
      "The lack of safety culture and engineering discipline seen in the aviation industry is compared to the database field, raising concerns about the correctness, durability, and complexity of databases like RavenDB."
    ],
    "points": 182,
    "commentCount": 63,
    "retryCount": 0,
    "time": 1706713451
  },
  {
    "id": 39209469,
    "title": "Comcast to Drop \"Xfinity 10G Network\" Brand Name Following Misleading Claims",
    "originLink": "https://arstechnica.com/tech-policy/2024/01/comcast-to-stop-calling-entire-network-10g-as-name-is-ruled-misleading/",
    "originBody": "10G or not 10G ‚Äî Comcast reluctantly agrees to stop its misleading ‚Äú10G Network‚Äù claims Comcast said it will drop \"Xfinity 10G Network\" brand name after losing appeal. Jon Brodkin - 1/31/2024, 8:39 PM Enlarge Comcast reader comments 113 Comcast has reluctantly agreed to discontinue its \"Xfinity 10G Network\" brand name after losing an appeal of a ruling that found the marketing term was misleading. It will keep using the term 10G in other ways, however. Verizon and T-Mobile both challenged Comcast's advertising of 10G, a term used by cable companies since it was unveiled in January 2019 by industry lobby group NCTA-The Internet & Television Association. We wrote in 2019 that the cable industry's 10G marketing was likely to confuse consumers and seemed to be a way of countering 5G hype generated by wireless companies. 10G doesn't refer to the 10th generation of a technology. It is a reference to potential 10Gbps broadband connections, which would be much faster than the actual speeds on standard cable networks today. The challenges lodged against Comcast marketing were filed with the advertising industry's self-regulatory system run by BBB National Programs. BBB's National Advertising Division (NAD) ruled against Comcast in October 2023, but Comcast appealed to the National Advertising Review Board (NARB). The NARB announced its ruling today, agreeing with the NAD that \"Comcast should discontinue use of the term 10G, both when used in the name of the service itself ('Xfinity 10G Network') as well as when used to describe the Xfinity network. The use of 10G in a manner that is not false or misleading and is consistent with the panel decision is not precluded by the panel recommendations.\" ‚ÄúComcast will discontinue brand name‚Äù Comcast agreed to make the change in an advertiser's statement that it provided to the NARB. \"Although Comcast strongly disagrees with NARB's analysis and approach, Comcast will discontinue use of the brand name 'Xfinity 10G Network' and will not use the term '10G' in a manner that misleadingly describes the Xfinity network itself,\" Comcast said. Comcast said it disagrees with \"the recommendation to discontinue the brand name\" because the company \"makes available 10Gbps of Internet speed to 98 percent of its subscribers upon request.\" But those 10Gbps speeds aren't available in Comcast's typical service plans and require a fiber-to-the-home connection instead of a standard cable installation. Advertisement The Comcast \"Gigabit Pro\" fiber connection that provides 10Gbps speeds costs $299.95 a month plus a $19.95 modem lease fee. It also requires a $500 installation charge and a $500 activation charge. Comcast said it may still use 10G in ways that are less likely to confuse consumers. \"Consistent with the panel's recommendation... Comcast reserves the right to use the term '10G' or 'Xfinity 10G' in a manner that does not misleadingly describe the Xfinity network itself,\" the company said. When contacted by Ars, a Comcast spokesperson said, \"We disagree with the decision but are pleased that we have confirmed our continued use of 10G in advertising.\" Comcast claims ‚Äúnot supported‚Äù The NARB said the \"recent availability of 10G speeds through [the Gigabit Pro] service tier does not support the superior speed claim (or a 10Gbps claim) for the Xfinity network as a whole.\" As the NARB noted, there is an \"absence\" of data showing how many Comcast customers actually use that service. The NARB also said that 10G is misleading because of the implied comparison to 5G wireless networks. \"The NARB panel concluded that 10G expressly communicates at a minimum that users of the Xfinity network will experience significantly faster speeds than are available on 5G networks,\" the announcement of the ruling said. \"This express claim is not supported because the record does not contain any data comparing speeds experienced by Xfinity network users with speeds experienced by subscribers to 5G networks.\" As the NAD has previously stated, 10G is more of an \"aspirational\" term rather than something that's offered over today's cable networks. Over the past five years, the NCTA has been using the term 10G to describe just about any improvement to cable networks, regardless of the actual speeds. The NCTA coincidentally issued a press release yesterday hailing the fifth anniversary of its first 10G announcement. \"Five years on, the future is even closer... Here in 2024, the promise of 10G is becoming more and more of a reality,\" the NCTA said. The announcement listed some examples of multi-gigabit (but not 10-gigabit) cable speeds, some of which were only achieved in lab testing or demos. NCTA claimed that \"10G can change lives\" and that the \"10G platform will facilitate the next great technological advancements in the coming decades, ensuring fast, reliable, and safe networks continue to power the American economy.\" For all of you cable broadband users, just remember to ignore \"10G\" in cable-company marketing and check the actual speeds you're paying for. reader comments 113 Jon Brodkin Jon has been a reporter for Ars Technica since 2011 and covers a wide array of telecom and tech policy topics. Jon graduated from Boston University with a degree in journalism and has been a full-time journalist for over 20 years. Advertisement Channel Ars Technica ‚Üê Previous story Next story ‚Üí Related Stories Today on Ars",
    "commentLink": "https://news.ycombinator.com/item?id=39209469",
    "commentBody": "Comcast reluctantly agrees to stop its misleading \"10G Network\" claims (arstechnica.com)176 points by thunderbong 12 hours agohidepastfavorite105 comments happytiger 10 hours agoSlimecast constantly lies at every tier of their organization. I won‚Äôt overwhelm you with example as they can be easily searched for, but here‚Äôs three good ones: https://www.reddit.com/r/Comcast_Xfinity/comments/17cdku6/li... https://forums.xfinity.com/conversations/plan/i-was-lied-to-... https://forums.xfinity.com/conversations/customer-service/xf... I personally have experienced the lying on every recent interaction with the company, and like many other Americans have no viable alternative providers. I‚Äôm in no way surprised that they are using a lying terms that is clearly intended to mislead people into thinking they are doing better for their customers than they actually are. That the companies‚Äô culture on display. It‚Äôs so unfortunate because I have seen glimmers of what this company be in their charity work and in the occasional stepping up to help people (though there always seems to be a big stepping up when community broadband initiatives are on the table). reply bhpm 7 hours agoparentI tried to get the advertised ‚Äú10G‚Äù internet when it was first announced. Their website said it was available, so I called to have it set up. They ‚Äúput in a ticket for [me]‚Äù and I didn‚Äôt hear back for a month. I called, they said they‚Äôd route it to the right department. A month passes. I called again. They said they can do it they just need a deposit. I agreed. They said they‚Äôd call me back. A month passes. I called again, they said they‚Äôd need to run a line to my house, but no one was available to do it that week so they‚Äôd put in a ticket. Also that it was going to cost more and they‚Äôd call me back. At this point I thought they were just bullshitting me, so I agreed. You guessed it, a month goes by. I called again, they said it would $20,000 to do the installation. I put myself on mute and laughed out loud. I agreed, and asked how I could write the check. And, wouldn‚Äôt you know it, Comcast admitted that my area wasn‚Äôt covered and that the website was inaccurate. reply vineyardmike 6 hours agorootparent> I agreed, and asked how I could write the check. And, wouldn‚Äôt you know it, Comcast admitted that my area wasn‚Äôt covered and that the website was inaccurate. that was brave. I totally assumed the end of this story is \"they charged the 20k and put me on hold indefinitely\". reply cyral 9 hours agoparentprevComcast was the only available provider at a home I rented years ago. The router arrived and I set it up, only to find that the WiFi range was (literally) a couple of feet a most. The antenna must not have been connected right internally. Comcast Support's response? This must be because I am not on their higher priced plan. You can get faster speeds and longer range if you upgrade! The guy on the phone refused to believe that something could be wrong with this particular router. Ended up going to a local store where the person said it happens a lot, and gave me the \"upgraded\" router instead. reply zardo 9 hours agorootparentI found that after a month of working fine, my packet loss jumped up to 50%. Support recommended upgrading my plan. It seems like they must massively oversubscribe their network then just drop packets based on your tier, with a honeymoon period for new customers so they don't immediately cancel. reply spike021 6 hours agorootparentPersonally I haven't had this problem. In fact, every few months they actually increase my plan's advertised max download speed. I wouldn't say the actual download speed is as close as what they say but pretty close. Of course the upload continues to be terrible I use my own modem and router now but this was the case when I \"rented\" from them as well. reply datway 6 hours agorootparentComcast‚Äôs behavior often depends on whether there is broadband competition in a local market. That would explain your experience and the other guy‚Äôs. reply Schnitz 2 hours agorootparentThis! 2 weeks ago my internet started dropping for 1-2 minutes at a time, 3-5 times per day. The modem status page tells the tale, millions of uncorrectable errors, SNR below spec on several channels, and the event log shows all the disconnects. Neither their chatbot, nor the first level human support however will entertain any possibility except it being an issue with \"the website you are using\" or \"your TV\" or \"your computer\". Took maybe 30 minutes of BS until they offer to send a tech, but I have to pay if the tech concludes the issue is on my end. I eventually lost my temper and just told them AT&T is going round my neighborhood right now running fiber offering to buy everyone out of their Comcast contracts and that they have 2 choices, either they come out to fix the issue or they come out to disconnect and clean up my cable drop - their call. Suddenly a tech was available, the next day, guaranteed free of charge. The tech arrives, plugs in his test modem, says \"yeah...that's not related to your equipment\" after 5 seconds, shows me a huge signal drop in several frequency ranges and is actually surprised that the internet is even usable at all. Unfortunately the issue is further upstream than \"our\" utility pole, but he ran a new cable drop just in case and cleaned up on the pole before filing that ticket. It makes you wonder how many people are limping along with half-broken internet just because they don't know how to debug this and force their way through Comcast's support wall and there not being a competitor they can threaten to switch to. reply spike021 6 hours agorootparentprevComcast is pretty much the only provider where I live. reply zardo 5 hours agorootparentAnd I was able to switch to a different ISP. reply missedthecue 7 hours agorootparentprevAmazing how many companies in all industries try to do the tactic of \"our product/service isn't meeting basic standards? Give us more money to fix it\". I'm frankly only ever willing to be upsold when I'm very satisfied with something and would like even more satisfaction in my life. reply tnecniv 5 hours agorootparentprevI moved into an apartment and kept the plan the previous resident had. They also sold us the non-xfinity router they had st a discount. My roommate and I had spotty internet at various times of day. We measured it and it was way below what our plan claimed. The previous tenants had no such issue. Comcast refused to believe the problem was on their end and claimed my router was too old. This went on until I bought a new router just to prove a point (new router did nothing). They finally send a repair guy out. He‚Äôs there for 5 minutes before diagnosing the problem: the cables were water logged to hell and back. He fixed it in 20 minutes and was gone. reply paul7986 7 hours agorootparentprevThankfully and not surprisingly the new nationwide competition from both t-mobile and Verizon 5G Home Internet services are hurting comcrap (xFinity) and charter (spectrum) broadband numbers. With T-Mobile and Verizon continuing to expand their 5G home offerings comcrap wil be less and less the only game in town. I'm not a gamer but stream YouTube and other streamers 6 to 8 hours day. T-Mobile's 5G home internet worked great for me. reply mrblah 8 hours agoparentprevWhen I moved to a comcast area around 2014, I brought my own modem and they wouldn't let me connect over the phone without an \"internet installation kit\". I didn't need an install tech to come out since the place was already wired. I think it was about $29, and when the box arrived, it was literally a piece of paper with a url to go to for instructions. Absolute slimeballs. reply acdha 8 hours agorootparentI had a similar experience with them: they swore they had to have an installer plug the modem in, agreed to waive the $120 fine, and then it came back every month and I had to call their executive office to get it corrected. That last part did double duty because they never once billed at the quoted rate in the year we lived in that area. Where I live now, Comcast has competition from Verizon and RCN. Starting a few blocks away, it‚Äôs only them. Shockingly, neighbors on the no-competition side report the same kind of routine casual fraud but it never happens here. reply spike021 6 hours agorootparentprevThat sucks. Back in 2021 I moved and decided to get new equipment myself instead of renting. It all was plug and play. Even now when I get an occasional outage they can do the reset for me through their mobile app. No issues. reply kevingadd 10 hours agoparentprevThey thanked me for my loyalty as a business class customer when I moved by charging me money for equipment I had returned + continuing to bill me after I cancelled and had moved away from the location. I ended up having to go through a local regulator to bludgeon them into submission, at which point an upper management level Comcast employee called me on the phone and begged me to sign a paper to make the regulatory complaint go away. I refuse to ever live somewhere Comcast-only. reply happytiger 10 hours agorootparentThey are the HOA of the Internet world. I‚Äôve had the exact same equipment scam happen. I ended up paying the first time. The second time I had a full video of me walking into the office (because at the time you had to return the equipment to the actual office, couldn‚Äôt ship it) and it took only a few months to get resolved. I don‚Äôt think there are many places that aren‚Äôt Comcast only. I briefly lived in an area with fiber-to-the-home. It was the same general cost as Comcast, but so much better and the service was rock solid reliable. They are a national disgrace, especially after all the money put into rural broadband initiatives and fiber optics rollouts somehow managed to produce basically‚Ä¶ nothing. But I‚Äôm sure they weren‚Äôt involved. And what‚Äôs happening is plain as day. Check this quote out: > Comcast‚Äôs lack of broadband growth started last year, when the largest U.S. internet provider reported no additions in the second quarter of 2022 for the first time in the company‚Äôs history. Since then, Comcast has reported net broadband losses in three of the last five quarters. > Comcast executives have pushed investors to focus on broadband‚Äôs rising average revenue per user (ARPU) growth, driven by price increases and upselling packages, rather than net additions. Comcast‚Äôs residential broadband ARPU rose 3.9% in the quarter. > ‚ÄúAs we continue to manage this balance, we expect ARPU growth to remain strong and our primary driver of broadband revenue growth with somewhat higher subscribers losses expected the fourth quarter compared to the 18,000 loss we just reported in the third quarter,‚Äù Comcast Chief Financial Officer Jason Armstrong said during the company‚Äôs earnings conference call Thursday. https://www.cnbc.com/2023/10/26/comcast-broadband-worries.ht... They‚Äôre not even interested in expanding broadband footprints anymore, just getting more revenue out of existing customers. And they are completely upfront about it. Sorry, but I haven‚Äôt had one good interaction with this company, and I don‚Äôt think I‚Äôm alone. https://www.pcmag.com/news/comcast-is-americas-most-hated-co... reply smegger001 8 hours agorootparentI bought my own router and modem before I signed up for their service as they were the only internet provider in my apartment building at the time. I viewed it as a preemptive measure against bullshit equipment rental fees. Everytime I have had to interact with them since they have tried offer me extra services (none of which I want) for free or reduced price if i would use their shitty box. reply happytiger 8 hours agorootparentDid you get hit with the ‚Äúno bandwidth cap‚Äù if you use their equipment or the $15 a month charge for using your own equipment or a combination thereof? Because that‚Äôs their official policy in many places (it‚Äôs insanely unethical and barely hidden consumer exploitation of fees). reply pxeboot 9 hours agorootparentprev> I don‚Äôt think there are many places that aren‚Äôt Comcast only. Really? There are entire states (and countries) without a Comcast presence. reply happytiger 8 hours agorootparentI‚Äôm sorry I was only speaking of the US, as this is a US company. > You already knew that home broadband competition is sorely lacking through much of the US, but a new report released today helps shed more light on Americans who have just one choice for high-speed Internet. > Comcast is the only choice for 30 million Americans when it comes to broadband speeds of at least 25Mbps downstream and 3Mbps upstream, the report says. Charter Communications is the only choice for 38 million Americans. Combined, Comcast and Charter offer service in the majority of the US, with almost no overlap. > Yet many Americans are even worse off, living in areas where DSL is the best option. AT&T, Verizon, and other telcos still provide only sub-broadband speeds over copper wires throughout huge parts of their territories. The telcos have mostly avoided upgrading their copper networks to fiber‚Äîexcept in areas where they face competition from cable companies. https://arstechnica.com/information-technology/2018/07/comca... Also, ‚Äúbroadband‚Äù being 25 mbit is their lobby‚Äôs choice and definitely should not be a legitimate threshold in the modern world. Then add that even though others operate in different regions, they generally do not compete for high speed service or are very poor at delivering it, thereby not actually presenting an alternative even though they ‚Äúofficially‚Äù do. Also, be clear that these two companies, that dominate areas of the US, collude. https://arstechnica.com/information-technology/2017/05/comca... https://corporate.charter.com/newsroom/comcast-charter-annou... https://corporate.comcast.com/press/releases/comcast-charter... reply epcoa 7 hours agorootparentWhile 30 million is too much, that‚Äôs less than 10% of the population. In the most populated areas it‚Äôs most common to have a duopoly. Sucks but ‚ÄúI don‚Äôt think there are many places that aren‚Äôt Comcast only‚Äù is not close to being accurate. They‚Äôre not even the dominant provider in Texas and New York. If anything it makes it easier for them to survive, antitrust below the national scale is much harder to apply. reply linsomniac 6 hours agorootparentprevExactly the same here: We had Comcast Business for at least a decade, and cancelled it ~Oct after switching to city fiber. They wanted me to ship back the equipment by taking it to a UPS store, didn't have to package it up or anything, just hand it to staff. I took photos of the gear in the UPS store. We worked for a few months with them to try to get it cancelled after that, eventually they said we didn't ship back all the gear, there were 2 wifi boxes we didn't return. We never had Comcast WiFi. We made a couple more back-and-forths saying this, and eventually they relented, but during this time I think we were charged $600-900 bucks (2-3 months of service) that we couldn't use (because we returned the modem) but they wouldn't cancel because we still allegedly had the WiFi gear. reply livueta 6 hours agorootparentprevOh hey, they tried this exact thing on me one time too. They were all over me for it, then went totally silent after I got an attorney friend to write them a nastygram. I really shouldn't be surprised it's a pattern of abuse - they must get away with it pretty often. reply niuzeta 3 hours agorootparentprev> I ended up having to go through a local regulator to bludgeon them into submission I would love to hear more in details on this. I feel like I am going to have to do something like this but not sure where to begin the research. reply epiccoleman 8 hours agorootparentprevPlease tell us that you didn't sign! reply jwatte 9 hours agorootparentprevDitto reply Alupis 9 hours agoprevEven if 10Gbps to the home is technically available, it's not posted on their website, and I would be willing to bet majority of consumers will be denied due to installation costs (it's not trivial to run a fiber just to your house). So, why on earth was their network called \"10G\" if near zero percent of Comcast customers are even aware of that service, let alone actually have that service? Further muddying the water with the 5G cellular technology and implying 10G is even better, deliberately riding that hype train. For actual customers, the 100Mbps and 200Mbps upload speed plans are the only \"next gen\" network upgrade available. It's still not symmetrical, not really what people want, and not priced accordingly. In my area, FTTH providers are rolling out 1Gbps symmetrical for as low as $70 a month - making Comcast's \"next gen\" network a joke in terms of price and performance. It was a gimmick from the start, and it's surprising it flew this long. reply StillBored 8 hours agoparentYes, but ATT advertises 1G fiber connections, and it's only available at a small fraction of their service addresses. Or for that matter spectrum with Internet Gig (or whatever they call the 850/30 Mbit service). I would be in favor of a bit more truth in advertising such that they have to provide a service (not \"up to\") at, say, 80%+ of their service addresses before being allowed to advertise it. Or, for that matter, they can only advertise the slower of up/down or maybe the average of both if they wish to site a single number rather than both up/down. reply bsagdiyev 7 hours agorootparentTo Spectrum's credit they seem to be doing a lot of fiber for new builds. Our 1gbit plan is symmetric and pulls those speeds consistently, but yeah their coax footprint is stuck with lower upload, high/low split coax areas should see 500mbit upload at some point but who knows. The state of internet in the US is awful in most places. reply zamadatix 8 hours agorootparentprevI like the spirit but I'm highly cautious of the measure we choose. E.g. \"80%+ of their service addresses\" basically means \"They can only advertise 25 mbit plans despite massive fiber investments\". The intent being few people hear about plans not available in their area but the effect being yet another reason to not bother trying to serve rural areas. Obviously the measure could be improved, particularly by making it an \"and\" instead of an \"only\" (e.g. \"Most all service areas with up to 25 mbps, select service areas with up to 5 gbps. Check your location for details\") but, for all the song and dance, I'm not sure such a thing really makes much meaningful impact to the consumer in the end (unless, again, it's so restrictive to have negative impacts instead) as regardless you're going to have to check your actual address and see if you're in the remaining x% that doesn't get that speed anyways. I'm a big fan of just flat requiring \"up/down for up to x GB/TB per month\" in the advertisement though. Worst impact of that is the viewer just doesn't care. reply klabb3 9 hours agoparentprevYes, and: > The Comcast \"Gigabit Pro\" fiber connection that provides 10Gbps speeds costs $299.95 a month plus a $19.95 modem lease fee. It also requires a $500 installation charge and a $500 activation charge. For reference, I‚Äôm paying ‚Ç¨25/mo for 10Gbit symmetric. Modem ‚Äúlease‚Äù and installation included. To be fair, I get ~8Gbit/s in practice, but that‚Äôs well worth it. reply crotchfire 4 hours agorootparentLemme guess, you're in the Netherlands. You folks have awesome internet because you force the local loop to be unbundled. Every country should do this. However, almost none do (except yours, and a few counties within the US state of Washington). Nobody with bundled local loops will get pricing+performance like this; please don't raise peoples' expectations. Instead, point them to the root cause (local loop bundling), because nothing else is going to help them. You'll just make them jealous and distract them from the one change that can actually improve their situation. Calling your provider and saying \"‚Ç¨25/mo for 10Gbit symmetric!!!!\" is not going to change anything. reply sschueller 4 hours agorootparentprevI pay CHF 64.75 (~USD 75) a month for 25gbit symmetric in Switzerland. I can not understand how they can get away with charging 200+ for 10G. That is nuts. But we have providers selling bullshit here too. One such provider claims 5G wireless is the equivalent of fiber. Sells 1gbit but actuall throughput is bellow 300mbit even with the best tower. Theoretical max is 2gbit but their equipment can't even do 1. Misleading customers to believe 5G wireless can ever beat fiber is just wrong. reply joking 1 hour agorootparent> I can not understand how they can get away with charging 200+ for 10G. That is nuts. well, truth said, you can pay that and more here if you want a good SLA and bandwidth guarantees. But for residential use it's not needed. reply theodric 2 hours agorootparentprevShit, I was paying 80.-/mo for Swisscom DSL @100Mb in Schwerzenbach because it was the only wireline option available. Eventually just switched to Yallo 4G at 15.-/mo, which was mostly fine. reply sschueller 1 hour agorootparentIt looks like your area will be getting fiber around April: https://ftth.init7.net/?x=963376.03977&y=6004593.74257&z=15.... reply Alupis 8 hours agorootparentprevThat price reference from Comcast is only if they actually say \"yes\" to running fiber to you in the first place. It's not very cost efficient to run a single fiber, and at $300 a month they don't have a huge amount of wiggle room for construction cost. reply selectodude 6 hours agorootparentThe cutoff is $8000 if they‚Äôll run fiber for you or not. It‚Äôs a strange service. 10Gb business-class fiber (non-GPON), some $10k juniper router to delineate the edge that they put in your house, and a backup gigabit cable connection for $300/mo. It‚Äôs the bargain of the century but I can‚Äôt imagine it‚Äôs for anything beyond advertising purposes. reply rayiner 6 hours agorootparentI have had it since 2017, though they upgraded it to 10g a year or two ago. It‚Äôs not vapor ware. It‚Äôs just that $8,000 doesn‚Äôt buy you much distance (I think it‚Äôs 1/3 of a mile from the nearest fiber node). reply selectodude 6 hours agorootparentOh I know it definitely exists, it‚Äôs just so far from residential grade and costs like 5% of what it would cost if you had a business. Strange product. I‚Äôm banned from upgrading beyond gigabit because my wife knows that I‚Äôd have to run fiber through the house so it‚Äôs all beyond my ability anyway. I‚Äôm jealous. reply zamadatix 8 hours agorootparentprevThat's gotta be the best deal I've ever seen. Probably a year of service just to break even on the hardware to connect you to the upstream device, not counting actually providing the service. It's hard to find a base VPS with that level of service even allowing for a low data cap for the price and that's just for a connection with no hardware and a limited place to put data in the first place. They don't happen to offer colocation do they :p. reply dkjaudyeqooe 6 hours agorootparentprev> I‚Äôm paying ‚Ç¨25/mo for 10Gbit symmetric. Can you post where you live so I can move there. reply crotchfire 4 hours agorootparentWilling to bet Netherlands. Outside possibility of Romania, because they basically skipped over an entire generation (DSL) of telecom so there were no incumbents to cause trouble. reply adriancr 1 hour agorootparentRomania is 10E a month for 10gbit with ~60E install charge Netherlands is unlikely, cheapest is ~80e/month for 10gbit and 45e/month for gigabit > because they basically skipped over an entire generation (DSL) of telecom so there were no incumbents to cause trouble. Oh we had that (romtelecom) and people werent willing to use it, they used cheaper local ISPs that just ran copper wires everywhere. Now a company (RDS) bought up most of those and kept upgrading and keeping prices low and basically are eating the competition. They're basically so far ahead you have construction companies that ask them to put fiber in new builds so its ready when apartments are being sold. It's mostly competition that drove this from a very competitive company, the rest kind of suck and would go comcast route if they could afford it. reply rayiner 6 hours agorootparentprevDoes that include the cost of the fiber loop itself? In some countries that‚Äôs a separate build-out fee (like getting a public water hookup in the US). reply zw123456 6 hours agoprevI always thought they totally missed an opportunity with their moronic Xfinity brand, why not just flip the cards and call it InfiniG, Yeah, forget 5G, 6G, or the 8G band (Seth Meyers sub-reference) and simply say we having infinity G, hell yeah, if you are going just make shit up, go for it, am I right? We out G everyone else. I know, you have not idea what the G thing even is, but don't worry, we got you, our G is infinite. There, all you problems and worries are solved for $275/mo. reply vault_ 11 hours agoprevI didn't realize they were actually selling a 10 Gbps service tier as part of this branding. It's never been available in my market, so I assumed that they were advertising the uplink capability of the thing my modem was connected to! Happy to see this go, but I'm still shocked to learn that the name was _less_ misleading than I had thought. reply kccqzy 11 hours agoparentThe article says it provides 10Gbps of service to 98% of customers upon request, which would be powered by fiber-to-the-home. I don't need 10Gbps, but I do want symmetric upload and download speeds. Does anyone know if it's possible to ask them to run fiber and have only an upload speed increase? reply vanchor3 11 hours agorootparent> The article says it provides 10Gbps of service to 98% of customers upon request This part is funny to me because I've tried to sign up for their FttH and they declined despite it being in the area, and the same thing happened to others I know. I'm not sure how they came to that percentage but I don't believe it. reply toast0 8 hours agorootparentI suspect there's something like of the people who go through qualification and get an offer from Comcast 98% request to install it. You probably didn't give them all the info unless you were ready to pay for it. And all the other people that get disqualified didn't count. reply IMcD23 9 hours agorootparentprev+1. I've read in various forums they will only install it if the construction cost is less than a few thousand dollars. This means they will say it's \"available\" on the order page, but then decline to install it. reply toast0 11 hours agorootparentprevI think you're asking for something like their 10G service, but at a lower cost and speed? > The Comcast \"Gigabit Pro\" fiber connection that provides 10Gbps speeds costs $299.95 a month plus a $19.95 modem lease fee. It also requires a $500 installation charge and a $500 activation charge I'm not sure that the pricing for that service actually pays for their installation and equipment costs, so I don't think you'd get much of a discount if you only ran it at 1Gbps symmetric. I did know someone who got the service and didn't bother to make the rest of his equipment work at 10G, so was only using a 1G port. And it works fine, but still costs $320/month + any other taxes and the $1000 install. reply Stephen304 10 hours agorootparentI had gigabit pro for a few years, they gave me a half off promo that made it worth it at $150/mo, which is not much more than the close to $100 after miscellaneous fees that the regular gigabit down 35Meg up HFC cable plan costs, not to mention the fiber reliability is so much better - no brief outages and I even had Comcast business proactively reach out to replace gear when their monitoring noticed the fiber switch starting to fail. I think they also discounted the install and activation to be $500 total. I split it among around 8 housemates which included the upstairs unit of our house, so it ended up being very affordable and there was always extra bandwidth to go around. The main benefit I enjoyed being greater than 35Mbit upload speed. reply dylan604 10 hours agorootparentprev> provides 10Gbps of service to 98% of customers upon request this sounds like PR doublespeak weasel words for burst vs sustained. reply idatum 8 hours agorootparentprevYou should push to have fiber. Once you get 1gb symmetrical (in my fortunate case after moving), there is no going back. Not-fond memories of getting through to Crapcast support to resolve outage (e.g. cable laid in 90s failed) and then being pitched a \"a great deal just for you\" of \"upgrading\" to get catv sh*t package, as I waited. Damn though, Crapcast did get to IPv6 fast and that specifically was solid in my previous house. reply renewiltord 10 hours agorootparentprevOh it's on-request. I followed their marketing link and it only offered me 1G so I assumed it was unavailable. Their big advantage is they have good coverage and many municipalities will preserve that by preventing other telecom companies from putting their alternative technologies in (say FttH). reply heroprotagonist 10 hours agoparentprevLast I knew, they still needed to finish working with their counterpart monopoly on their collaborative new Xumo device and get all the systems lined up to use it. Then they need to kick the little old grandma's still watching traditional cable off their network and set them up on a new Xumo streaming box instead. Then they drop the old video channels and use their frequencies to provide faster service on the same old copper wires. reply martinald 9 hours agorootparentNot sure this is right. DOCSIS4.0 (which I think is what you are referring to?) doesn't require TV channels to be moved off plus it can coexist with existing DOCSIS3.0/3.1 (I think the plan is to actually bond 3.0, 3.1 and 4.0 channels together - much like how most 3.1 rollouts actually are majority 3.0 channels for BC purposes). DOCSIS4.0 does use higher frequencies though and this requires a lot of additional work to upgrade the infra to support this. I think what Comcast is calling '10G' is the fact you can now order a totally new FTTH run which doesn't use coax instead. Tbh it's a confused strategy. If you're going to offer XGS-PON to everyone, why bother with DOCSIS4.0? It doesn't really make sense to run fibre runs just to one customer, you could probably do a whole street in not much more time. reply Alupis 9 hours agorootparentprevI don't know how coax internet works, or how the channel allocations work, but it seems to me if they can offer 2Gbps/200Mbps already why can't we opt for a channel reallocation and get like 1Gbps symmetrical, or at least 1Gbps/500Mbps or something? I do understand the legacy channel allocations were designed for almost entirely download - but 2Gbps? That can't be... reply I_Am_Nous 9 hours agorootparentThe way those cable modem systems work is essentially laying a data channel (usually several) on top of the existing coaxial cable network, similar to DSL laying data on top of the existing telephone network. However this means the cards which transmit and receive are still very much analog beasts, pumping out some incredible signal levels to as far as possible. Similar to DSL, the download centric focus is built into the design. Also, your small modem can't scream nearly as loud as the downstream signal can so some signal loss is more likely, limiting the upload channels. Finally a cable modem network is usually quite shared, with something like 8 transmission lines feeding entire neighborhoods or cities. Depending on node congestion you may not even get your advertised speeds. At least with DSL your line is basically dedicated to you lol reply account-5 11 hours agoprevI am always baffled at how US businesses can pretty much do and get away with anything they want, with little to no concequences. I'm not saying where I live is perfect but every time there's a scandal here I only need to look to the US to see we don't have it that bad. reply wmf 10 hours agoparentThis is a story about a company not getting away with something. In general the US has turned the dial very far to the free speech side, even for advertising. reply danpalmer 10 hours agorootparentBut it is a story about a company who tried something that was obviously invalid and that most companies in, say, Europe, would never even think to try. And they got away with it for 4 years. reply lostmsu 10 hours agoparentprevTBH, unless they used this branding for their mobile network or advertised it without actually being able to provide 10 Gbps fiber, I don't see why they were slapped. reply deepfriedchokes 11 hours agoparentprevRegulatory capture: https://en.wikipedia.org/wiki/Regulatory_capture In Capitalism, eventually capital owns everything. reply thaumasiotes 4 hours agorootparentThat's true by definition in all systems, since capital consists of all the stuff that can be owned. It's not very interesting; tautologies tend not to be. reply starttoaster 11 hours agoprevI thought it was pretty well understood, to anybody paying attention at least, that there have been no official 6, 7, 8, or even 9G mobile networks yet, so of course 10G was a slightly quicker name for 10Gbps networking. It's maybe not an industry standard, but colloquially many datacenters will call a 10Gbps-capable network switch a \"10G switch.\" This seems like an extension of that colloquialism to general consumers. But I guess I won't argue, necessarily, about being less vague in our choices of words for things. I just generally don't think accusations of \"misleading\" is completely fair, in this instance. Comcast has been misleading customers in other ways, but in this case, I think individual ignorance is doing a bit of the heavy lifting for the misleading here. reply rconti 11 hours agoparentnext [‚Äì]to anybody paying attention at least Maybe they don't think you should have to follow the industry to know what a company's advertising means? It pissed me off to no end to hear the monopolist advertise their \"10G network\" (\"with speeds starting at 200Mbps\") It was CLEARLY meant to mislead. reply starttoaster 11 hours agorootparentIf you don't know what a 10G network in a given context could be, are you likely to understand the difference between what a 5g and a supposed 10g mobile network would be? Are you a candidate for a person that is likely to care about the difference between 200Mbps and 1Gbps? Perhaps it's naive, but I've heard the words \"rack that 10G switch\" so many times that it's hard for me to say that Comcast intended on being misleading. reply recursive 11 hours agorootparentMy sister in law insists that 5Ghz wifi is the same thing as a 5G cell network. Comcast shouldn't use misleading terminology. The fact that people don't know what 5g means isn't really a defense. I couldn't tell you the difference between 4g and 5g, but adding \"10g\" alongside them makes it worse, not better. reply ryandrake 9 hours agorootparentThis is what happens when we allow Marketing to simply conjure up a unit of measurement. Now nobody knows what a \"G\" is and how to compare one company's \"G\" with another company's \"G\". I fully expect AT&T to offer 20G cell service next because why the hell not? Same thing happened with CDROM speeds back in the day. At one point 1X, 2X, 3X and so on were based on an actual throughput number, but then soon the Marketing brain trust realized you could put whatever number you want before the \"X\" and then the number became pretty meaningless. Same for \"bars\" on your cell phone service signal. What is the difference between \"one bar\" and \"two bars\" on the display? Who the hell knows, it's a meaningless unit invented by carrier company Marketing. reply kube-system 2 hours agorootparentprev\"Hey honey, I called Verizon and they have the 1G service for $79.99\" \"I called up T-mobile home internet, and they can give us the 5G for $50\" \"Looks like Xfinity has the 10G for $79.95\" The vast majority of people buying internet service are non-technical, misunderstandings about the details are common, and Comcast picked a name that is perfect for confusing with all of their competitors. People don't always know what these measurements really mean, but they know that a bigger one should mean faster internet. reply rconti 3 hours agorootparentprevI think a lot more laypeople know the words \"100 megabit\" and \"gigabit speed\" than know anything about what 5G mobile service means. So let's leave mobile service aside. That was never my complaint. I think some laypeople see the marketing materials companies put out comparing one speed and price tier to another. In other words, a fair number of people who have actively price-shopped internet speed (\"should I buy the 'fast' tier that says 100 somethings, or the 'boost' tier that's 200, twice as much, for 50% more money?\") would think 10G means 10 times as much as 1G which, it's not a huge leap to think that a fair number of people are aware means 1000 of the Ms. reply binkHN 11 hours agoparentprevThe fact is the 10G offering is only available in a very small number of very specific locations. reply starttoaster 11 hours agorootparentDoes their device at least support 10Gbps internal networks, I assume? In that way it's still not misleading, in my opinion. Of course the uplink only receives and transmits whatever you pay for (or in that case, what they allow you to pay for based on what is available.) reply Stephen304 10 hours agorootparentFor their fiber gigabit pro (2+1 gigabit symmetric service) they lease you something like a juniper acx2100, so presumably something similar with an SFP+ port would be used for higher speed fiber plans. Granted that's just a handoff so I'm pretty sure you're just on your own when it comes to picking something that can route at 10G. I just went with a decisio opnsense appliance and called it a day. reply binkHN 11 hours agorootparentprevTheir \"latest Xfinity Gateway\" has a 2.5G Ethernet port: https://www.xfinity.com/learn/internet-service/modems-and-ro... ; you need something else to do 10G. reply WatchDog 4 hours agoparentprevWho officially decides what is xG? The number of different technologies and expected speeds varies so much with 4G/5G, that the terms aren‚Äôt particularly useful anyway. reply Symbiote 2 hours agorootparentThe GSM Association, the industry group of (originally European) mobile phone makers. https://en.wikipedia.org/wiki/GSMA reply crooked-v 10 hours agoparentprevhttps://xkcd.com/2501/ reply starttoaster 9 hours agorootparentOn the contrary, I'm simply tossing up whether or not you're likely to care about the difference between 10G and 10Gbps if you don't understand what either one even mean. Are we mad at Comcast for misleading people that don't even know where (or from where) they were led? Or are we mad at Comcast for not using the technical term since we're all tech minded here? Where this XKCD comic is about experts overestimating the knowledge of average people, I'm attempting to estimate how much the average person should even care, in this case. reply gnicholas 6 hours agoprevI don't think I've ever talked to a Comcast (or AT&T) salesperson who understood the difference between bits and bytes. They literally all quote me speeds in \"megabytes per second\". reply bagels 4 hours agoparentAmazing this 20 year old blog is still up. Verizon doesn't know the difference between cents per kilobyte and dollars per kilobyte: https://verizonmath.blogspot.com/2006/12/verizon-doesnt-know... reply jsz0 5 hours agoprevI worked in the cable industry for over a decade. All the worst things you can dream up about these companies are true. At the highest level of the companies phrases like 'what are they gonna do switch to dsl' and 'they can move if they don't like it' were common. reply bagels 4 hours agoparentWorse than the criminal harassment campaign ebay executed against critics? reply nerdjon 8 hours agoprevThat is great, every time I saw an ad with that stupid phrase it drive me insane. It was so stupid and clearly straight up lying and yet they acted so proud of it. While we are at it can we stop confusing \"Wifi\" and \"internet\" in marketing? Somehow consumers seem to have forgotten that the 2 are distinct (likely thanks to most people probably don't use specific routers and instead they are the same). I want to scream anytime I see a post like \"My wifi is down, is anyone else having issues?\" on a local group and its like, I am almost sure you mean Internet is down but the distinction is very important. reply wwmiller3 7 hours agoparentAnd the providers take great advantage of this ignorance by advertising the ‚Äúfastest wireless‚Äù among their competitors (which may be true when you compare the router/AP lease offerings), but says little about how their internet upload/download speeds compare. reply tacocataco 5 hours agoprevThe American dream is to have a captive consumer. reply aliasxneo 11 hours agoprevInteresting, I never once made that distinction, always assumed it meant 10gbps. Was this actually confusing people or just companies fighting in court? reply kube-system 11 hours agoparentI thought it did too. I went to their website to check out the specs, the fastest 'Xfinity 10G' plan available to me in a major city was 2Gbps/0.2Gbps reply FootballMuse 10 hours agorootparentIt‚Äôs called ‚ÄúGigabit pro‚Äù or now ‚ÄúGigabit x10‚Äù and is symmetrical 10gbps. It‚Äôs not on their regular website. You have contact them directly to have someone come out to survey and see if you are eligible for a fiber run. https://youtu.be/tQV0ltA1tCk reply kube-system 10 hours agorootparentRight, but if I: 1. go to https://www.xfinity.com/10g 2. scroll down to the main section titled 'Xfinity 10G Network' 3. click 'shop now' I never see anything about a 10 gigabit plan, anywhere. It's classic bait and switch. reply FootballMuse 4 hours agorootparentThe Xfinity 10G Network‚Ñ¢ refers to their latest infrastructure upgrades. It is not a product. This is the confusion the article is about. The product \"Gigabit x10\" is the plan you are looking for, and is not on their website. It's a residential service but everything is handled by the business side of Comcast and Metro-E. You must get in touch with them directly. See these modmail requests on reddit[1] or dslreports[2]. More on the process here[3]. Otherwise you will need to wait for their DOCSIS 4.0 \"X-Class\" speed tiers that are slowly rolling out[4][5]. Initial deployments are limited to 2Gbps, but they are symmetrical speeds. DOCSIS 3.1 technically supported 10Gbps down already. So we need to wait and see if the infra upgrades enable higher 10Gbps download tiers[6][7]. [1]: https://www.reddit.com/r/Comcast_Xfinity/search?q=x10&restri... [2]: https://www.dslreports.com/forum/comcastdirect [3]: https://todayamerican.medium.com/the-definitive-guide-on-how... [4]: https://i.imgur.com/qt0Enqj.png [5]: https://www.xfinity.com/learn/internet-service/xclass [6]: https://en.wikipedia.org/wiki/DOCSIS [7]: https://corporate.comcast.com/press/releases/comcast-multi-g... reply kube-system 3 hours agorootparentYes, I know. My comment is in criticism of their advertising practices. They're plastering '10G' all over the ad copy on sales channels that aren't for 10gbps service. They know it's misleading, and it was 100% intentional. reply FireBeyond 10 hours agorootparentprevYeah, I unfortunately don't count right now, unless someone else in between us does: > In order for the location to pre-qualify for Gigabit Pro Service, it must meet the qualifying distance factor. The location cannot be more than 1,760 ft. from the fiber splice case. Second, the cost cannot be more than $8,800 to build; this is determined based on whether the fiber is run underground or aerially. At some locations, the customer can be within the distance but cost is greater than the limit. > Please see the results for the submitted ticket. > Name: XXX Address: YYY Ticket #: ZZZ Distance: 1,928 ft, from fiber splice case Cost for fiber install: $14,327.00. reply kube-system 10 hours agorootparent> the cost cannot be more than $8,800 to build Coincidentally that's just about the total value of the 2 year contract plus fees. ($299.95 + $19.95) * 24 + 500 + 500 = 8677.6 That's an expensive two years of internet service. reply geor9e 7 hours agoparentprev10G was a brand name they slapped on all speed tiers, so if you thought you were getting 10gbps you were misled reply geor9e 7 hours agoprevAmerican capitalism let private companies run some co-axial wires to our houses, which was a big capital investment for sure but not that big, and then we let them milk us for the privilege for the next 60 years. We've probably paid for it 1000 times over now. If the government just laid the wires as a public utility, as wasteful as the government is, it wouldn't be this bad. reply mlhpdx 7 hours agoprevI‚Äôm grateful for our local ‚Äúcommunity telcos‚Äù that deliver far faster connectivity and far lower prices, and with generally great service. I have toyed with the idea of starting one for my current neighborhood (low density residential, so no hope of any ‚Äútraditional telco‚Äù investing in upgrades). Edit: errant ‚Äúso so‚Äù reply xbar 11 hours agoprevThey are moving straight to 12G? reply Overtonwindow 11 hours agoprev [‚Äì] I‚Äôve wondered if what we need in society are nonprofits that advocate exclusively against certain evil and nefarious corporations. Imagine what an org could do with a few million and a mission to combat Comcast and expose their behavior. One organization whose mission is to expose Amazon‚Äôs bad behavior. A PR check on monopolies. Consumer Affairs, etc. do good work, but we need a more 1:1 relationship. reply wmf 10 hours agoparentThey're called the New York Attorney General and the Southern District of New York. reply Mountain_Skies 10 hours agoparentprev [‚Äì] Probably turn out like the Mozilla Foundation and hundreds of other foundations giving piles of money to do good, which ends up meaning do good for the directors of the foundation. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Comcast will stop using the \"Xfinity 10G Network\" brand name following an appeal loss that found it to be misleading.",
      "\"10G\" refers to potential 10Gbps broadband connections, faster than current cable networks.",
      "The National Advertising Review Board (NARB) ruled that Comcast's claim of superior speed compared to 5G networks was unsupported, highlighting confusion and countering the hype around 5G wireless networks."
    ],
    "commentSummary": [
      "Comcast has been criticized for deceptive claims about its \"10G Network\" and has agreed to discontinue them.",
      "Customers and critics have accused Comcast of poor service and false promises, with complaints ranging from billing problems to difficulties canceling services.",
      "The lack of competition in the US broadband market has raised concerns about high prices and limited availability of high-speed options. Some suggest community-based telecommunication companies as a potential solution for faster and more affordable internet services."
    ],
    "points": 176,
    "commentCount": 105,
    "retryCount": 0,
    "time": 1706735413
  }
]
