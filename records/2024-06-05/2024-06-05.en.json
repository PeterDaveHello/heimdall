[
  {
    "id": 40580549,
    "title": "Why Synthetic Keys Are Superior to Natural Keys in Database Design",
    "originLink": "https://blog.ploeh.dk/2024/06/03/youll-regret-using-natural-keys/",
    "originBody": "Toggle navigation ploeh blog About Archive Hire Me Pages Schedule Support the blog Tags You'll regret using natural keys by Mark Seemann Beating another dead horse. Although I live in Copenhagen and mostly walk or ride my bicycle in order to get around town, I do own an old car for getting around the rest of the country. In Denmark, cars go through mandatory official inspection every other year, and I've been through a few of these in my life. A few years ago, the mechanic doing the inspection informed me that my car's chassis number was incorrect. This did make me a bit nervous, because I'd bought the car used, and I was suddenly concerned that things weren't really as I thought. Had I unwittingly bought a stolen car? But the mechanic just walked over to his computer in order to correct the error. That's when a different kind of unease hit me. When you've programmed for some decades, you learn to foresee various typical failure modes. Since a chassis number is an obvious candidate for a natural key, I already predicted that changing the number would prove to be either impossible, or have all sorts of cascading effects, ultimately terminating in official records no longer recognizing that the car is mine. As it turned out, though, whoever made that piece of software knew what they were doing, because the mechanic just changed the chassis number, and that was that. This is now five or six years ago, and I still own the same car, and I've never had any problems with the official ownership records. Uniqueness # The reason I related this story is that I'm currently following an undergraduate course in databases and information systems. Since this course is aimed at students with no real-world experience, it wisely moves forward in a pedagogical progression. In order to teach database keys, it starts with natural keys. From a didactic perspective, this makes sense, but the result, so far, is that the young people I work with now propose database designs with natural keys. I'm not blaming anyone. You have to learn to crawl before you can walk. Still, this situation made me reflect on the following question: Are natural keys ever a good idea? Let's consider an example. For a little project we're doing, we've created a database of the World's 50 best restaurants. My fellow students suggest a table design like this: CREATE TABLE Restaurants ( year TEXT NOT NULL, rank TEXT NOT NULL, restaurantName TEXT NOT NULL, cityName TEXT NOT NULL ); Granted, at this point, this table definition defines no key at all. I'm not complaining about that. After all, a month ago, the students probably hadn't seen a database table. From following the course curriculum, it'd be natural, however, to define a key for the Restaurants table as the combination of restaurantName, cityName, and year. The assumption is that name and city uniquely identifies a restaurant. In this particular example, this assumption may actually turn out to hold. So far. After all, the data set isn't that big, and it's important for restaurants in that league to have recognizable names. If I had to guess, I'd say that there's probably only one Nobelhart & Schmutzig in the world. Still, a good software architect should challenge the underlying assumptions. Is name and city a natural key? It's easy to imagine that it's not. What if we expand the key to include the country as well? Okay, but what if we had a restaurant named China Wok in Springfield, USA? Hardly unique. Add the state, you say? Probably still not unique. Identity # Ensuring uniqueness is only the first of many problems with natural keys. You may quickly reach the conclusion that for a restaurant database, a synthetic key is probably the best choice. But what about 'natural' natural keys, so to speak? An example may be a car's chassis number. This is already an opaque number, and it probably originates from a database somewhere. Or how about a personal identification number? In Denmark we have the CPR number, and I understand that the US Social Security Number is vaguely analogous. If you're designing a database that already includes such a personal identification number, you might be tempted to use it as a natural key. After all, it's already a key somewhere else, so it's guaranteed to be unique, right? Yes, the number may uniquely identify a person, but the converse may not be true. A person may have more than one identification number. At least when time is a factor. As an example, for technical-historical reasons, the Danish CPR number carries information (which keys shouldn't do), such as a person's date of birth and sex. Since 2014 a new law enables transsexual citizens to get a new CPR number that reflects their perceived gender. The consequence is that the same person may have more than one CPR number. Perhaps not more than one at the same time, but definitely two during a lifetime. Even if existing keys are guaranteed to be unique, you can't assume that the uniqueness gives rise to a bijection. If you use an external unique key, you may lose track of the entities that you're trying to keep track of. This is true not only for people, but cars, bicycles (which also have chassis numbers), network cards, etc. Clerical errors # Finally, even if you've found a natural key that is guaranteed to be unique and track the actual entity that you want to keep track of, there's a final argument against using an externally defined key in your system: Data-entry errors. Take the story about my car's chassis number. The mechanic who spotted the discrepancy clearly interpreted it as a clerical error. After a few decades of programming, I've learned that sooner or later, there will be errors in your data. Either it's a clerical error, or the end-user mistyped, or there was a data conversion error when importing from an external system. Or even data conversion errors within the same system, as it goes through upgrades and migrations. Your system should be designed to allow corrections to data. This includes corrections of external keys, such as chassis numbers, government IDs, etc. This means that you can't use such keys as database keys in your own system. Heuristic # Many were the times, earlier in my career, when I decided to use a 'natural key' as a key in my own database. As far as I recall, I've regretted it every single time. These days I follow a hard heuristic: Always use synthetic keys for database tables. Conclusion # Is it ever a good idea to use natural keys in a database design? My experience tells me that it's not. Ultimately, regardless of how certain you can be that the natural key is stable and correctly tracks the entity that it's supposed to keep track of, data errors will occur. This includes errors in those natural keys. You should be able to correct such errors without losing track of the involved entities. You'll regret using natural keys. Use synthetic keys. Comments James Snape # There are lots of different types of keys. I agree that using natural keys as physical primary keys is a bad idea but you really should be modelling your data logically with natural keys. Thinking about uniqueness and identity is a part of your data design. Natural keys often end up as constraints, indexes and query plans. When natural keys are not unique enough then you need to consider additional attributes in your design to ensure access to a specific record. Considering natural keys during design can help elicit additional requirements and business rules. \"Does a social security number uniquely identify a person? If not why?\" In the UK they recycle them so the natural key is a combination of national insurance number and birth year. You have to ask questions. 2009-06-04 15:43 UTC Thomas Castiglione # 2024-06-05 9:33 UTC Nicholas Peterson # I largely agree with James Snape, but wanted to throw in a few other thoughts on top. Surrogates don't defend you from duplicate data, in fact they facilitate it, because the routine generating the surrogate key isn't influenced by any of the other data in the record. The concept of being unable to correct a natural key is also odd, why can't you? Start a transaction, insert a new record with the correct key, update the related records to point to the new record, then delete the old record, done. Want some crucial information about a related record but only have the surrogate to it? I guess you have to join it every time in order to get the columns the user actually wants to see. A foreign key that uses a natural key often often prevents the join entirely, because it tells the user what they wanted to know. I find the problem with natural keys usually comes from another source entirely. Developers write code and don't tend to prefer using SQL. They typically interact with databases through ORM libraries. ORMs are complicated and rely on conventions to uniformly deal with data. It's not uncommon for ORMs to dictate the structure of tables to some degree, or what datatypes to prefer. It's usually easier in an ORM to have a single datatype for keys (BIGINT?) and use it uniformly across all the tables. 2024-06-05 12:42 UTC ← Previous Archive Next → Wish to comment? You can add a comment to this post by sending me a pull request. Alternatively, you can discuss this post on Twitter or somewhere else with a permalink. Ping me with the link, and I may respond. Published Monday, 03 June 2024 19:46:00 UTC Tags Architecture 32 Support the blog Buy my book Code That Fits in Your Head Buy my book about Dependency Injection Watch my Pluralsight courses Watch my Clean Coders videos Public speaking schedule Tweet \"Our team wholeheartedly endorses Mark. His expert service provides tremendous value.\" Hire me! Published: Monday, 03 June 2024 19:46:00 UTC © Mark Seemann 2024 with help from Jekyll Bootstrap and Twitter Bootstrap",
    "commentLink": "https://news.ycombinator.com/item?id=40580549",
    "commentBody": "You'll regret using natural keys (ploeh.dk)628 points by thunderbong 17 hours agohidepastfavorite470 comments strnisa 11 hours agoI've become a fan of unique, relatively short and \"human-readable\" IDs, such at the ones used by Stripe, e.g. `cus_MJA953cFzEuO1z` for an ID of a customer. Here's a Stripe dev article on the topic: https://dev.to/stripe/designing-apis-for-humans-object-ids-3... If you use JavaScript/TypeScript, you can make them like this: function makeSlug(length: number): string { const validChars = \"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789\"; const randomBytes = crypto.randomBytes(length); let result = \"\"; for (let i = 0; iFor the record: the valid chars string is 62 characters, so naively using a modulo on a random byte will technically introduce a bias Indeed, there's no reason you couldn't just add \"_\" and \"-\" or \".\" as well to complete the set. Your identifier will still be URL-safe. I've been using this type of encoding for years [1] for these kinds of ids to use in URLs, and encoding/decoding is super-fast with some bit shifts. And unlike Base64, you don't need padding characters. [1] https://sourceforge.net/p/sasa/code/ci/default/tree/Sasa.Web... reply notfed 2 hours agorootparent- and _ tend to break text selection. reply naasking 51 minutes agorootparentI'm not sure what you mean by \"break\". If you mean that touching or double-clicking on a block of text only extends up to the nearest symbols, that's true. But if your text selection UX is not terrible then it should be simple to extend that further. That said, iOS and Android text selection have gotten worse recently, IMO. reply cogman10 7 hours agoparentprevA problem with this approach is it's not monotonical. Especially if you want to use this thing as an index in a database, you'll run into problems where you try doing middle insertions frequently, which causes fragmentation. The solution to this problem is making the higher order characters time sorted [1]. You don't need to go all out like uuid, you can have a pretty low resolution. It's more important that new insertions tend to be on the same page. If you have a low frequency insertions then minute resolution is probably good enough. (Minutes since 2000 is an easy calculation). To implement that here, I'd suggest looking at how base64 or 85 encoders work and use that instead of repeated mods. You can then dedicate the upper bits to a time component and the lower bits can remain random. [2] [1] https://vladmihalcea.com/uuid-database-primary-key/ [2] https://github.com/mklemm/base-n-codec-java/blob/master/src/... reply reissbaker 5 hours agorootparentIMO it's nice to have two keys: 1. An auto-incremented 64-bit (unless you have a good reason, in which case 32-bit is fine) primary key, used internally for foreign key relations. This will generally result in less index bloat on associated tables, and fast initial inserts. 2. A public-facing random string ID. Don't use this internally (other than in an index on the table it's defined for), since it's large. But this should be the only key you expose to end-users, to prevent leaking data via the German Tank Problem: https://en.wikipedia.org/wiki/German_tank_problem Only create the second key if this is data you're exposing to users, of course — for data that's only used internally, just use the 64-bit auto-incremented PK and skip the added index bloat entirely. reply cogman10 3 hours agorootparentFor the number 2, I think one issue is that you are going to be semi-frequently whacking the db to do a mapping of that random string id back to the real id. OK for smaller entities but might be a pain if there's a lot of those ids to wrangle. You can throw a secondary index on it, but that will still have some minor fragmentation issues. One benefit of a random id is if you are working with more complex data models it can make creating those easier/faster. Instead of having a centralized location to get new ids from (the DB) you can create ids on the fly from the application which can turn the write into a single action from the application rather than a dance of inserting the main table, getting the new id, then inserting to the normalized tables. reply caeril 2 hours agorootparentThat's why the Good Lord invented caching. In most applications, 90% of your workload will be over ids less than a week old, so your hit rate is likely to be pretty high for this sort of mapping. reply cogman10 2 hours agorootparentFirst hit can be a beast. It's workload/entity determinant if caching is enough for this. Not great if you are spending 1 minute on the first lookup just to do the mapping. reply senderista 37 minutes agorootparentprevI don't understand why you need to maintain two separate keys: instead of generating a random key, why not just encrypt the auto-increment key using a secret key? This is the approach used by e.g. cloud providers that use auto-increment keys internally but don't want them to be guessable. reply cogman10 6 minutes agorootparentI think the biggest problem with this approach is it effectively pins you to a encryption key and algorithm (unless you embed some information in the key that lets you version the key, gotta think of that upfront). Imagine, for example, that you picked DES and \"kangaroo\" as the secret several years back. You are now pinned to an algorithm and key with known security problems and a weak key. reply WorldMaker 1 hour agorootparentprevA different approach to solve both 1 and 2 is timestamp-oriented IDs. You can get useful cache locality/less \"index bloat\"/fast initial inserts if your keys can be easily ordered in time. Sorted by timestamp means very similar behavior to B-Tree appends of a monotonic integer, even sometimes in the worst cases where \"same moment\" IDs aren't monotonic and rely more on random entropy. I got some great DB cache/index performance from ULIDs with a bit of work to order the ULID timestamp bits in the way the DB's 128-bit column \"uuid\" sort best supported. Now that UUIDv7 is standardized we should hopefully see good out-of-the-box collation for UUIDv7 in databases sooner rather than later. reply rav 1 hour agorootparentprevInstead of a random string ID, you can devise a fixed secret key and expose the auto-incremented ID xor the fixed secret key as the public-facing ID. This saves you the separate index but still avoids the German tank problem. But it gives you a new problem, namely a secret that's hard or impossible to rotate. reply cogman10 1 hour agorootparentThis is insecure. Assuming the user can get a few key examples (which, we assume they would be able to if the german tank problem is a problem) then the secret can easily be revealed. [1] [1] https://dev.to/wrongbyte/cryptography-basics-breaking-repeat... reply senderista 36 minutes agorootparentprevXOR isn't secure enough, but you're on the right track. Instead, use an actual block cipher. reply groestl 3 hours agorootparentprev> A problem with this approach is it's not monotonical Whether or not that's bad fully depends on your platform and the number of writes you do. If you're using a massively distributed database like Datastore, Spanner etc, you want random keys as to avoid hot spots for writes. They produce contention. reply cogman10 2 hours agorootparentWell, you'd still likely want psuedo-random keys. You'd rather not have the underlying database doing extra work to shuffle around records as the pages get jumbled. One solution to that is having more complex keys. For example, in one of our more contentious tables the index includes an account id (32bit int) and then the id of the entity being inserted. This causes inserts for a given account to still be contiguous (resulting in less fragmentation) while not creating a writing hotspot since those writes are distributed across various clients. reply groestl 1 hour agorootparentNot disagreeing. Point is, you need to know your domain, your technology, your write patterns, your downstream systems, etc to decide if a specific key scheme works to your advantage or not. All the more reason not to use natural keys, as they lock you in in that regard. reply cogman10 1 hour agorootparentAbsolutely agree. I don't know how you can successfully maintain or develop software without developing an understanding of the underlying domain. I've seen devs try that route and the quality of their work has never been high. reply jordanthoms 3 hours agorootparentprevThis is dependent on the database you are using - if it's a key-sharded distributed database, you want to have insertions evenly spread across the key space in order to avoid having all the inserts go into a single shard (which could overload it) reply cogman10 2 hours agorootparentThis is the great thing about using random bits for the lower bits. Because you are unlikely to use more than say 2^64 database nodes, any sharding algorithm will have to figure out how to spread a key with 64 bits (or however many bits are in your key) across n nodes. Because of the random portion of the key, that means you'll get good distribution so long as the distribution algorithm isn't something stupid like relying solely on the highest order bits. reply echelon 5 hours agorootparentprevThis is a great technical modification that can be made to work with \"Stripe\"-alike IDs or tokens. Another hack for advanced active-active situations where you may need to route events before replication completes: encoding the author shard / region in the lower order bytes. There are lots of interesting primary key hacks for dealing with physical or algorithmic complications. reply cimnine 5 hours agoparentprevYes, and I like to combine two established concepts instead of rolling my own: URI and UUIDv7. So my IDs become `uri:customer_shortname:product_or_project_name:entity_type:uuid`. An example ID could be `uri:cust:super_duper_erp:invoice:018fe87b-b1fc-7b6f-a09c-74b9ef7f4196`. It's even possible to cascade such IDs, for example: `uri:cust:super_duper_erp:invoice:018fe87b-b1fc-7b6f-a09c-74b9ef7f4196:line_item:018fe882-43b2-77bb-8050-a1139303bb65`. It's immediately clear, when I see an ID in a log somewhere or when a customer sends me an ID to debug something, to which customer, system and entity such an ID belongs. UUIDv7 is monotonic, so it's nice for the database. Those IDs are not as 'human-readable' for the average Joe, but for me as an engineer it's a bliss. Often I also encode ID's I retrieve from external systems this way: `uri:3rd_party_vendor:system_name:entity_type:external_id` (e.g. `uri:ycombinator:hackernews:item:40580549:comment:40582365` might refer to this comment). reply whyever 10 hours agoparentprevPlease don't use % to generate integers from a range, it's not uniform, which can be disastrous if you rely on your numbers not being predictable. You can use crypto.randomInt instead. reply strnisa 8 hours agoparentprevThanks for all the improvement suggestions! Taking them into account, the `makeSlug` function becomes: function makeSlug(length: number): string { const alphabet = \"0123456789abcdefghjkmnpqrstvwxyz\"; let result = \"\"; for (let i = 0; iNow you have some legacy prefix that cannot be changed Yes you can. You can support the old cus_ prefix as well as the new org_ prefix, but always return org_ from now on reply chuckadams 4 hours agorootparentprevI have that exact issue with a couple different identifiers, and it's not a big deal. Usually it goes along with some data model change you already have to write compatibility code for, the new and old names tend to be related, and the old name tends to stick around other parts of the code anyway. Opaque IDs don't reduce the confusion there, documentation in appropriate places does. reply wongarsu 9 hours agorootparentprevReddit prefixes their IDs with t1_ for comments, t2_ for accounts, etc. That sidesteps the renaming issue. Though I believe they mostly do it because their IDs are sequential, so without prefix you wouldn't easily notice if you use the wrong kind of id. They also only apply prefixes at the api boundary when they base36 encode the IDs, the database stores integers reply giancarlostoro 6 hours agoparentprevIf I'm going to do that I think I'd use Bitcoins BASE58 which avoids letters that could be confused for each other. The number of times I see an O and 0 and wonder which is which, because the font does not make it clear really annoys me. Edit: Other honorable mentions: ObjectID's as used by MongoDB which contain the creation timestamp. Also Discord's snowflakes (inspired by Twitter's iirc), which also contain the creation timestamp. reply kibwen 5 hours agorootparentIf you want random IDs to be human-readable (and human-communicatable), I'd just recommend base 32 or even base16. You don't actually save that many bytes from base58 or base64 when it comes to short IDs. Case in point, the parent poster's base64 ID is 14 characters long. When encoded as base32 that's still only 17 characters (or 19 in base16), and now you have completely gotten rid of all notion of casing, which is annoying to communicate verbally. reply giancarlostoro 5 hours agorootparentI think the more I think of it, the more I favor the Discord snowflake ID's because they're just integers, and they can be generated on the fly. I think new messages generated them on the client if I'm not mistaken. reply psychoslave 3 hours agoparentprevTo my mind, it always felt so saddening that adoption of a truly straightforwardly readable notation for numbers never took of. I mean it’s so easy to do. You can start for example with a single syllable per digit, and for example only target CV syllables. From this there is many possibilities, but for example, let’s consider only a base ten. Starting with vowels order o, i, e, a, u with mnemonic o, i graphically close to 0, 1 and then cyclically continue the reverse order in alphabet ( \"ki-ke-ka-ku-no-ni-ne-na-nu-ko\" That’s just one simple example of course, there are plenty of other options in the same vein. It’s easy to create \"syllabo-digit\" sets for larger bases just adding more consonants, go with some CVC or even up to C₀C₁VC₀C₁ if sets for C₀ and C₁ are carefully picked. reply jmcphers 2 hours agorootparentThere is a old system for making numbers pronounceable as words using a mapping from each number to a consonant value. It's typically used to help memorize numbers: https://en.wikipedia.org/wiki/Mnemonic_major_system reply psychoslave 3 hours agorootparentprevAnd of course a naive implementation of the reverse is also trivial: def numerize(euphonism) = euphonism.split(?-).map{$digits.find_index(it)}.map{it.to_s}.join.to_i reply phkahler 5 hours agoparentprevSomething like that should have a built in check \"digit\" if people are going to see it and possibly type it in. For numeric values, making them all a multiple of 11 is a simple way to catch all single digit errors or single transpositions. reply chuckadams 4 hours agorootparentThat's why VINs make a decent natural key, because they do have a check digit. Plus they're not opaque: if you look up the VIN and the make/model/year is completely different than the car in front of you, you know you either have the wrong VIN or the wrong car. reply inopinatus 9 hours agoparentprevI'm not convinced that all Stripe IDs are wholly random strings. I just decoded the base62 part of four genuine \"acct_\" objects, which at 16 characters are just shy of representing a 12 byte value (log2 62^16 =~ 95.3), and they all have a leading byte of \"00000011\" and two of them even have a leading 32 bits that is very suspiciously close to an epoch timestamp of a couple of years ago. There is a similarly suspicious pattern in some of their longer identifiers, invoices for example at 24 characters (ca.143 bits) all seem to have a first byte of \"00000010\". Even in the article you've linked to, look closely at the IDs: pi_3LKQhvGUcADgqoEM3bh6pslE pm_1LaXpKGUcADgqoEMl0Cx0Ygg cus_1KrJdMGUcADgqoEM card_1LaRQ7GUcADgqoEMV11wEUxU Notice the consistently leading low-integer values (a 3, then three 1s)? and how it's almost always followed by a K or an L? That isn't random. In typical base62 encoding of an octet string, that means the first five or six bits are zero and the next few bits have integer adjacency as well. It also looks like part of the customer ID (substring here, \"GUcADgqoEM\", which is close to a 64-bit value) is embedded inside all of the other IDs and then followed by 8 base62 characters, which might correspond to 48 bits of actual randomness (this is still plenty, of course). Based on these values it seems there's a metadata preamble in the upper bits of the supposedly \"random\" value, and it's quite possible that some have an embedded timestamp, possibly timeshifted, and a customer reference, as well as a random part, and who knows maybe there's a check digit as well. It's possible - albeit this is not analytical but more of a guess - that the customer ID includes an epoch-ish timestamp followed by randomness or (worst case, left field) is actually a sequence ID that's been encrypted with a 64-bit block cipher and 32 bits of timestamp as the salt, or something similar (pro tip: don't try that at home). My view is that either Stripe's engineering blog is being disingenuous with the truth of their ID format, or they're using a really broken random value generator. If the latter, I hope it's only in scope of their test/example data. reply wslwsl 7 hours agorootparentIn the comments it's mentioned, that the IDs contain a shard key for faster lookups. https://dev.to/stripe/designing-apis-for-humans-object-ids-3... reply dividuum 8 hours agorootparentprevI took a look at a bunch of stripe customer ids I have stored and at least mine look very random on first glance. I assume their blog post uses demo keys or something similar. reply bambax 9 hours agoparentprevNice, but better to remove ambiguous / hard to read letters: ijlo, IJLO, and 01 (and maybe 7 as well?) reply codeulike 9 hours agoparentprevExcept the Stripe ones are case sensitive which can be annoying with some databases reply datavirtue 4 hours agorootparentYou know what's annoying? People storing GUIDs in a case-insensitive database. reply mettamage 9 hours agoparentprevWhy is such a thing called a slug? reply inopinatus 9 hours agorootparentIt's an old typesetting term that found its way into content management systems. https://archive.nytimes.com/www.nytimes.com/times-insider/20... reply corobo 8 hours agorootparentprevOoh I had this wonder a while back and jotted it down just in case anyone else ever wondered about it: Comes from the ye olde paper-based blogs they call newspapers. When an article is being put together it’s given a short name, sort of like a project name. This name would remain the same throughout the article’s life - from reporter through to editor - it left its trail through the process. Like a slug. reply meindnoch 7 hours agorootparentThat's not what Wikipedia says about its etymology though. >The origin of the term slug derives from the days of hot-metal printing, when printers set type by hand in a small form called a stick. Later huge Linotype machines turned molten lead into casts of letters, lines, sentences and paragraphs. A line of lead in both eras was known as a slug. reply corobo 7 hours agorootparentOoh interesting. It looks like I have either misinterpreted or found a source that misinterpreted (it was a few years back, unsure if I came to the conclusion or found it). I'll have to update my notes, cheers! Apologies for the wetbrain hallucination, HN! reply 8372049 7 hours agorootparentI'm not sure you're hallucinating. The dictionary I checked lists the printing and journalism terms separately. It's quite possible they have diverging etymologies, meaning both can be correct: 5. Print. a. a thick strip of type metal less than type-high. b. such a strip containing a type-high number or other character for temporary use. c. a line of type in one piece, as produced by a Linotype. 8. Journalism. a. a short phrase or title used to indicate the story content of a piece of copy. b. the line of type carrying this information. reply hprotagonist 7 hours agorootparentor the journalism term itself diverged from the typographic one. reply corobo 7 hours agorootparentAye this is what it seems to be having double checked the reply's claim Got to the Wikipedia page https://en.wikipedia.org/wiki/Slug_(publishing) which could possibly support the slimy conclusion of \"it's a trail through the process\" but that article has an etymology section that refers to the metal slug I guess it could mean both depending on whether you're looking for the meaning of the word or the meaning of the concept but I didn't find any other slimy grub references (via an admittedly limited double check) reply hprotagonist 5 hours agorootparentanother fun etymological rabbit hole for you: stereotype and cliché both probably originated as typographer jargon. reply scrollaway 7 hours agoparentprevI don’t recommend using random bytes for this. What I have done in my previous projects is take a uuid6, reserve a couple of bytes inside of it to replace with an object ID, and I convert that to obj_XXXXXXX. This means you can store them in the db not as a string but as a uuid, which is a lot more performant. You also get time stamping for free. reply nostrademons 6 hours agoprevThere's a better solution for many of the exceptional cases that the author describes: aliases & audit logs. Take for example the Danish CPR number. That's perfectly fine as a natural key; its definition is the first CPR number assigned. If a person's CPR number changes because they've changed their gender, you will want a separate table recording a.) the date of the change. The new CPR number is not valid before that time b.) the new gender c.) probably the reason for the CPR number change, since if the policy now is that they can change because of a gender change, there's a decent chance they'll be some other policy in the future that results in a new CPR issuance. Or the chassis number. Also fine as a natural key. If it's changed because of a data-entry error, you also want to record a.) the date of change b.) who changed it. This opens up a whole host of auditing, monitoring, and reporting functionality that eg. lets you catch fraud, determines if a single person is being sloppy, identify mass changes in policy, notify and update external records of owners, etc. URLs are another good natural key: they are defined to be unique (otherwise your webserver won't work), they make for very easy lookups when you're fetching from a web request, and if they change, they break the web. Except that they do change. But when a URL changes, you don't want to just update them in the database everywhere, because again, that will break the web. You want to leave a redirect from the old to the new one. So you create a redirects table of all the other aliases that point to a given page, use it to generate server redirects, and you can throw in other data like the time of change or hit counts on each individual alias. reply tapland 5 hours agoparentIt's not really fine. I work on a Healthcare system in the nordics. Who you billed, who visited what doctor, who your primary care provider is, all the people a doctors office has as patients, refers to the SSN. You don't want to lose that connection or to have to update everything for any change. You store a unique identifier for the person in the system, and you can then pull the actual personal identification number when needed. You do not keep individual private lists of people changing genders. reply nikita2206 2 hours agorootparentBut there’s no natural key for a person, not even the SSN. This article also starts off by showing an example where no natural key exists (Restaurants) and acknowledges it. reply tapland 5 hours agorootparentprev> You do not keep individual private lists of people changing genders. On this note: Some organization in Sweden actually does store changes like suggested in the top comment. Both the tax agency and police were interested in knowing who did, and who had access, since it's the cause of recent mass doxxing events. reply lolinder 5 hours agoparentprevI'm a fan of audit logs, but what you're describing is more event sourcing than audit logs. From what I understand what you're suggesting is that you don't change the key in the original table, you just record the change in a second table. Presumably, then, when you need to read a row you must also look up the list of diffs for that row to make sure that the natural key hasn't changed. Two things strike me about this proposed model: First, under your proposal the version of the model that you work with at the application layer must have two copies of the key field—one is the database key for when you need to make changes or look up more data and one is the meaningful business-model field that's actually up to date. That's exactly the same extra mental overhead that natural keys are supposed to have solved, only worse because the fields will have similar names and will contain the same content most of the time, making it easy to accidentally use one where the other was expected. Second, if you're going to introduce effectively an event-sourced data model then you've introduced a ton of new database records already, so why not just give everything a proper unique key while you're at it? Once you've done that you can modify the original row after all (while retaining the audit logs!) or, if you're serious about event sourcing, cache the latest derived value and look it up by its database key instead of carrying around an out of date natural key that's just waiting to be used incorrectly. reply ironchef 5 hours agoparentprevYou appear to be describing a classic type 2 or type 4 slowly changing dimension. (https://en.wikipedia.org/wiki/Slowly_changing_dimension ) reply crote 2 hours agoparentprev> Take for example the Danish CPR number. That's perfectly fine as a natural key; its definition is the first CPR number assigned. The problem is that the new CPR number is now the one you want to use for all display purposes and future interactions with external systems. In other words, you can't use the \"original CPR\" field for anything except as key. It's no longer a CPR field, because it no longer has any relation to the person's CPR! And at that point it'd be better to just use a GUID or something as key and avoid any potential confusion between the \"real\" CPR and \"fake\" CPR, because when the two are the same 99% of the time it is guaranteed to cause a shitton of bugs. The only solution is to essentially rewrite all your records with the new CPR as key, and leave a redirect entry at the old CPR. That's pretty much what happens in Sweden when you change your gender: your old identity ceases to be and you're issued a completely new one. reply IggleSniggle 2 hours agoparentprevURLs are only really uniform not necessarily unique. The most obvious case is something like `http://localhost/file.txt`, where the resource being located is almost sure to be different on every single \"localhost,\" but this is true of any server. The pointer is neither unique nor has any true guarantee that the resource being pointed to can or should be considered unique for any given context. It is merely unique within its calling context, which, when dealing with URLs, is often anywhere on the web where DNS responds. Even if you presume that the resource being located is in fact a unique resource in the general case, the \"unique resource\" may not be unique in the way that you presume. Some URLs at a given location are intended to be idempontent and cache-able, others are not, and many are time-limited forwarders. And there's no guarantee or even expectation that two identical forwarding URL's will resolve to the same location; it may well be network-topography dependent. reply phkahler 5 hours agoparentprev>> If a person's CPR number changes because they've changed their gender, you will want a separate table recording a.) the date of the change. The new CPR number is not valid before that time b.) the new gender c.) probably the reason for the CPR number change, since if the policy now is that they can change because of a gender change, there's a decent chance they'll be some other policy in the future that results in a new CPR issuance. But what if you have a bunch of records in some other table - billing information for example, and it's indexed by the CPR number (a foreign key). When they change the CPR number you can no longer query for their entire billing history based on CPR. None of your proposed extra complexity does anything about this problem. The only good way to solve it is to use a synthetic key with NO meaning. It would still be good to do as you say and track all the CPR number changes for a given person, but they will still need a unique key. So a sort of \"identity table\" used to figure out what unique key you're dealing with. reply nostrademons 2 hours agorootparentYou need this complexity anyway - if you had surrogate keys, the primary key would be an opaque identifier, and to do anything involving the CPR, you'd need to join against your CPR table (which, again, needs to be 1:many because CPRs themselves are not a 1:1 relationship). The first CPR in this case becomes identical to your surrogate key - it's an opaque ID that you use to reference other tables in the DB - but with the added benefit that for the common case, you don't need any additional lookups. You only need to lookup CPR changes if you don't find the CPR that the user gave you. And then it has other added benefits in that you have a record of CPR changes, you can understand how common a case this is, the CPR change table itself has semantic meaning and you can query a wide variety of properties without joining your primary user table, etc. reply lolinder 1 hour agorootparent> The first CPR in this case becomes identical to your surrogate key - it's an opaque ID that you use to reference other tables in the DB - but with the added benefit that for the common case, you don't need any additional lookups. You only need to lookup CPR changes if you don't find the CPR that the user gave you. I'm not buying that this is a meaningful gain in either performance or code complexity. In the world of synthetic keys I look up the CPR in the CPR table and join it to the user table using the synthetic ID. If I find a record for the CPR+user join then I'm set, if I don't then the customer doesn't exist. In the world of natural keys, you're advocating that I first query the user table directly by CPR. Then if I turn nothing up I run a separate query with a join on the original CPR. Then if I still don't turn something up the customer doesn't exist. The code in the second instance is obviously more complicated than the code in the first instance. It has increased risk of someone writing a bug because now there's a very tempting CPR field that will be right most of the time but wrong in some edge cases. Depending on the database and usage patterns, indexing on the CPR may be much less efficient than indexing on an autoincrementing integer. The only thing it has going for it is that I might be able to avoid a single join on a very specific path where a user is looking up a customer by typing in the CPR. That seems like the wrong thing to optimize for in the face of all the downsides. reply shkkmo 2 hours agorootparentprev> but with the added benefit that for the common case, you don't need any additional lookups. You only need to lookup CPR changes if you don't find the CPR that the user gave you Doing the CPR lookup ONLY when you don't find a record is really, really stupid. All you need is one bad cache, one database update that go rolled back and suddenly you've lost all recent updates the the user and will only see the bad record with no indication you've failed. Trying to use force changeable data into being a 'natural key' means that the number of edge cases you have to predict, write around and test for is going to rise significantly for no real benefit. reply wild_egg 5 hours agorootparentprevWhy could you not join with the audit table and find historical billing information from the old CPRs? reply lolinder 5 hours agorootparentYou could, but that introduces a lot of complexity that would be saved by just giving the customer a synthetic ID that uniquely and stably identifies them in your system. The grandparent's proposal basically turns the first-entered CPR into a meaningless ID field that should not generally be used in a piece of business logic (unless for some reason you need to display \"first CPR they entered\"). Once you've declared that you should look elsewhere if you actually need the CPR, why did we even bother using it as the primary key? reply phkahler 3 hours agorootparentprev>> Why could you not join with the audit table and find historical billing information from the old CPRs? 1) complexity and 2) what is actually tying the CPRs together? We're not going to have a CPR table per-customer, so all the CPRs of every customer are in the same table. Presumably the CPR table has a unique key for the customer that can be used to associate multiple CPRs with that person, so we have come full circle - just use that unique key in the audit table. reply Pengtuzi 3 hours agoparentprev> URLs are another good natural key: they are defined to be unique No, see more info here: http://localhost:8080/ reply trevor-e 4 hours agoparentprevURLs can be tricky and have plenty of gotchas depending on what you're trying to do. For example, the order of query params is free to change but it's still the same URL. Nothing that can't be worked around with a little normalization. reply Karellen 2 hours agorootparent> For example, the order of query params is free to change but it's still the same URL. Are you sure that's guaranteed by any spec? I thought an end-point would be free to treat `?a=1&b=2` and `?b=2&a=1` differently. I mean, it would be a nightmarish implementation, but I don't think it would be non-conforming? reply Piskvorrr 1 hour agorootparentIt's a mere convention: \"a querystring of this type is an array of parameters, the order of which is irrelevant\" - this is one of the tautological \"valid except when it's not\" non-rules (a surprising number of cases). Example: ?a=1&b=2&a=3 - will the server treat this to be equivalent as ?a=3&b=2&a=1 , or ?b=2&a=1 , ?b=2&a=3 , or something else entirely? You'd need to check the serverside parsing implementation to be sure - those could even be valid (and distinct) filenames FWIW. (And that's before you get to caching - \"?b=2&a=1 is not to be served as a cached version of ?a=1&b=2\") reply shkkmo 2 hours agoparentprev> Take for example the Danish CPR number. That's perfectly fine as a natural key; its definition is the first CPR number assigned. This is a horrible idea. You now have two different pieces of information that are identical in form and indistinguishable in the majority of cases: \"first CPR number\" and \"current CPR number. Every place you enter, or use a CPR number, you now must track which piece of information you have. If you make a mistake doing this, it will be hard to catch. Now you've decided that first piece of information is a natural key and will be every single place the record is used, even if that spot doesn't do anything specific to the CPR number. Every single time a CPR number is ingested from an exterior source, you need to do a lookup to make sure it is the original CPR number and then track it. Ever place you don't do this lookup is a place where an error can creep in if something changes in your pipeline our source. Even in an context where external sources are using something like a CPR number as an key, I would still use a different key internally since I see only downsides to using a \"natural key\". > URLs are another good natural key: they are defined to be unique (otherwise your webserver won't work), they make for very easy lookups when you're fetching from a web request, and if they change, they break the web URLs are also horrible natural keys. They are not defined to be unique and provide no guarantee that the content has not changed or even that the same content is sent to different users. If the location for content changes, you may or may not get a redirect. If you do get a redirect, you'd have to now go update every single place that uses the key to the new value. It is much better to map URLs to an artificial key and update that mapping in a single place. reply Kon-Peki 15 hours agoprev> how about a personal identification number? In Denmark we have the CPR number, and I understand that the US Social Security Number is vaguely analogous. The US SSN is not guaranteed to be unique, the SSN assigned to a person could change, there is no guarantee that a person with an SSN assigned to them is a US citizen, and there is no guarantee that a US citizen has an SSN - they must be requested, and you don’t need one unless you do something that requires having one. There are also things called ITINs and ATINs that look like an SSN but are not, yet can be used in place of an SSN in a huge range of SSN-required situations! (Please don’t use the SSN as a database key!) reply otherme123 13 hours agoparentIn Spain we have the DNI number, that a lot of people asume is unique, even database designers that use is as a natural key. Turns out the DNI can have, and actually have, a lot of duplicates. The police has a page explaining it (https://citapreviadnipasaporte.es/dni/dni-duplicados-espana/), and how it's not a primary key in their databases, but a number entered manually from a pool of possible numbers. And number re-using is a possibility. They estimate the number of duplicates in 200,000 for a population of 50,000,000. The point is that if you asume DNIs are unique and use them as PK your database is exposed to the bad design of the DNI database. There are some stores that use the DNI as the \"unique\" identifier for fidelity cards. reply steinsgatezero 6 hours agorootparentI remember having to deal with this and other identifiers like NIF and NIE while working on the academic titles homologation platform for the MCIN. I didn't understand why it wasn't used as, not necessarily PK, but an identifier logging in. Thanks for letting me know that the hellish time spent integrating Cl@ve wasn't in vain. reply benhurmarcel 9 hours agorootparentprevAlso your DNI can change. Typically foreigners get a NIE (used for the same thing but a different format), and get a new DNI if they ever get Spanish nationality. reply prmoustache 10 hours agorootparentprevI've seen banks or insurers use DNI as user login. reply SoftTalker 1 hour agorootparentThe user login (hopefully) is not the internal primary key for the user. It should be unique at a given point in time, obviously, but certainly there are reasons it might need to change. reply Piskvorrr 1 hour agorootparentprevAnd I've been bitten by this design numerous times. The most common bug: literally the birthday paradox, i.e. \"these two people have been assigned the same number, and now we need to distinguish them in our database\". reply benhurmarcel 9 hours agorootparentprevAll of them do I think. But from experience, they can change it for your account. reply pradn 6 hours agoparentprevPernicious assumptions! Google Cloud projects have three attributes: user-friendly names, system numbers, and system names. System names are alphanumeric. They can be chosen by the user, derived from the friendly name if there's no collision. But! There's some system names from the olden days that are actually all numbers - so not actually alpha-and-numeric. Thankfully we don't run into those often. reply delusional 12 hours agoparentprevWe have the same problem in Denmark, most people just don't realize it. At my dayjob we get at least one person every year who changes gender and consequently gets a new SSN (the final digit is supposed to signify gender). Most people don't store SSNs so they never realize, but it does happen fairly frequently. reply winternewt 11 hours agorootparentIf Denmark is anything like Sweden there's also: - SSN from different ID space gets assigned to immigrants; when they become citizens they are assigned a new permanent SSN. - SSN:s have a long and a short form; the short form which cuts off century information can be the same for someone who is 5 years old and someone who is 105 years old. - When an unconscious patient comes in to the E.R. you don't know their SSN, so a temporary one is assigned for use in patient records. Such temporary SSN:s are not coordinated nation-wide so multiple patients may have the same SSN. In some hospitals they don't even have a local standard for ID:s. The staff just makes something up on the spot. It happens that the SSN they made up collides with a valid SSN for another person. reply mrweasel 7 hours agorootparentImmigrants and refugees will get a \"replacement CPR number\", which is outside the normal space/range of CPR numbers. Once registered as living in Denmark, they'll get a real CPR number. Danish SSNs doesn't have a long or short form, you used the seventh digit to do a table lookup to see if the person is born in 18XY, 19XY or 20XY. The date of birth is always ddmmyy, there is no long form. So if the seventh digit is 9, and the 5-6 digit is between 00 and 36, then you're born in between 2000 and 2036, if the 5-6 digits are 37 to 99, then your born in the 1900. But you need the published table to figure that out. Last point, there is backup system for unconscious patients, but it should be the same across all medical records as these are somewhat standardized. reply kassner 7 hours agorootparentprevAlso: the personnummer carries a date that often means birth date, but there are cases where it’s not, but I’ve seen a few system that just assumes it’s the same. > SSN from different ID space gets assigned to immigrants; when they become citizens they are assigned a new permanent SSN Having gone through that, my personummer didn’t change. Maybe that doesn’t happen anymore? reply tapland 6 hours agorootparentIt does, but if you already have a Personnummer or get a residence permit right away so that you are eligible for one you don't get the temporary Samordningsnummer. reply kalleboo 6 hours agorootparentAnd the samordningsnummer isn't only for immigrants - it's also for Swedes born abroad who have never been folkbokförd. reply tapland 5 hours agorootparentThey're still still an immigrant. The numbers are for residents (at some point in time) and citizenship isn't reflected by it. reply kalleboo 3 hours agorootparentInteresting, I've never heard of anyone referring to a citizen as an immigrant, but looking up the legal definition, they certainly are. TIL. reply delusional 6 hours agorootparentprev> - SSN:s have a long and a short form; the short form which cuts off century information can be the same for someone who is 5 years old and someone who is 105 years old. We don't do that. instead, we shove that extra bit of information into the digit following the last two digits through a table: https://da.wikipedia.org/wiki/CPR-nummer#Under_eller_over_10... reply rsynnott 7 hours agorootparentprevIn Ireland, until the 90s, if a woman got married, she gave up her PPS number (essentially a social security number) and took her husband's, with a 'W' appended (this fact tells you a lot about pre-90s Ireland...) If she subsequently divorced or the husband died, she got a _new_ PPS number. While this was abolished in the 90s, _some people still have these 'W' numbers_, ticking timebombs for anyone relying on them as a key. reply mikedelfino 7 hours agoparentprevHow to uniquely identify an American citizen? reply polygotdomain 5 hours agorootparentTo give a slightly more technical answer, at a large insurance company I used to work for, the legal department had provided definitions on what conditions we would consider various components of PII a match. So between SSN, DoB, First Name, Last Name, and a couple others, there were potential combinations that our system would say, \"yes this is the same person\". Note that we didn't necessarily need exact matches on things like names, but \"close enough\" matches were sometimes sufficient. reply vharuck 6 hours agorootparentprevThe population health database I work with uses generated IDs for people and relies on health systems to link records before submission (they put a lot of effort into data used for billing). But we do check for problems by looking at duplicates of SSN with date of birth. We don't consider names, because they could be transcribed differently. Even though the SSN+DOB pair should be unique, there have been cases where a widow provides her deceased husband's SSN. Likely because she used it for Social Security benefits and forgot her own long ago. reply j16sdiz 6 hours agorootparentprevYou can't. That's why everybody use SSN. reply gwbas1c 5 hours agorootparentprevYou can get kind-of close with driver's license numbers. They will change when US citizens move among states, (and we do that quite frequently.) (People who don't drive have to get state-issued IDs that have the same number. At least, they have to if they want to buy do anything that requires proof of identification.) reply hprotagonist 7 hours agorootparentprevha ha only serious; ask them for their papers and if they say “who the fuck are you?”, they’re americans! a slightly more serious answer: you largely don’t. The US doesn’t have a national ID, proof of birth is not even close to standardized, etc. reply Longhanks 6 hours agorootparentprevHow about not doing that? reply mikedelfino 6 hours agorootparentThen I guess it could lead to duplicate records. Now, whether this is a problem or not depends on the business and how much one cares about data integrity. I work with higher education, so uniquely identifying students and keeping all their records organized is somewhat important. Granted, I don't work in the US, and here we have a unique, national number. So this is covered, except for foreign students. I was curious how this problem is solved in other countries. reply Kon-Peki 5 hours agorootparentThe US government doesn't assume that it can uniquely identify a citizen with 100% certainty. When things get tricky, we rely on the judicial system to weigh the evidence and make a decision. Which could later get changed. If you want to design your system to assume that you can do it, that's your problem. Literally, it is now your problem, and nobody is going to step in to help. reply ajuc 2 hours agoparentprevYeah, these things are almost never as simple as they are supposed to be. Poland has PESEL numbers since 70s. It was supposed to be unique, only apply to Polish citizens, never change, and have a checksum digit. Every Polish citizen gets one at 18 when they get their national ID document, and you can request it earlier if you want to. Turns out there are duplicated PESEL numbers. A LOT of non-Polish citizens have them assigned (mostly Ukrainian refuges but not only). The checksums are sometimes wrong. And some people have several PESEL numbers. If you used PESEL as database key you're fucked. The system works perfectly, but it interfaces with external world through computer-human-paper-human-computer interface. And at some point the mistake propagates so far that it becomes the truth assumptions be damned. reply selcuka 14 hours agoparentprev> The US SSN is not guaranteed to be unique The cases you listed do not mean SSNs are not unique, unless there are people who share the same SSN. You can still define a unique index for the SSN column. A column can be both nullable and unique as each null is different in SQL. reply dqv 14 hours agorootparent> It’s not as uncommon as you might think. In fact, some 40 million SSNs are associated with multiple people, according to a 2010 study by ID Analytics. https://www.pcworld.com/article/424392/a-tale-of-two-women-s... reply pseudosavant 13 hours agorootparentFunny enough, I used to work at IDA. I’ll add that there is a huge difference between the SSN database that the Social Security Administration maintains, and the list of SSNs that have been associated with a person. Especially because it is very common to change a single digit of your SSN when performing credit fraud - because they’ve already burned their real one. Some people will have dozens of SSNs attached to them. IDA was very good at determining who a person is through the graphs that represent identities in our world (names, DOBs, phone numbers, addresses, SSNs, etc.) reply Kon-Peki 14 hours agorootparentprevEDIT - SSA claims that numbers are not recycled. But there are known cases where the same number has been assigned to multiple people. Note that in less than 100 years, more than half of all possible SSNs have already been used… reply bloopernova 6 hours agorootparentSo we need SSNv6? ;) reply cmiles74 6 hours agorootparentprevPeople also type them in wrong, so there's another thing: where is the data coming from. reply ars 14 hours agorootparentprev> unless there are people who share the same SSN That happens all the time. First of all people steal SSN's and use them (and you are not the police, so it's not your responsibility to do anything about that). Second people make up fake SSN's because they don't want to give you their SSN. People also make typo's, and you can end up with the same SSN. An SSN is not unique in the real world. reply jklowden 15 hours agoprevYou think your surrogate key will save you? It will not. The world has an external reality that needs to be reflected in your database. If the unique identifier for your object — VIN, CUSIP, whatever — if it changes, the world will henceforth refer to it by both. You will need to track both. Adding a synthetic key only means you have to track all three. Plus you have to generate a meaningless number, which is actually a choke point in your data throughput. The natural key forces you to think about what makes the row unique. What identifies it. Sometimes, it makes you go back to the SME and ask them what they mean. Sometimes it makes you reconsider time: it’s unique now, but does it change over time, and does the database need to capture that? In short, what are the boundaries of the Closed World Assumption? You need to know that too, to answer any \"not exists\" question. To use our professor’s car’s example, we actually do not know the database design. It could well be that the original identifier remained the primary key, and the \"new id\" is entered as an alias. The ID is unique in the Car table, identifying the vehicle, and is not in the CarAlias table, where the aliases are unique. Oh, you say, but what if the bad old ID gets reused? Good question. Better question: how will the surrogate key protect you? It will not. The reused ID will be used to query the system. Without some distinguishing feature, perhaps date, it will serve up duplicates. The problem has to be handled, and the surrogate key is no defense. Model your data on the real world. Do not depend on spherical horses. reply yashap 13 hours agoparentThis is bad advice. Say I have a user table, and the email is unique and required, and we don’t let users update their email, and we don’t have user deletion. If I’m going natural PK, I make email the primary key. But … then we add the ability for users to update their email. But it should still be the same user! This is trivial if we have a surrogate primary key, a nightmare if we made email the natural primary key. Or building on that example, maybe at first we always require an email from our users. But later we also allow phone auth, and you just need an email OR a phone number. And later we add user name auth, SSO, etc. Again, all good with surrogate primary keys, a nightmare with natural primary keys. There are countless examples like this. You brought up cars, same thing with licence plates, for example. Or even Social Security Numbers/Social Insurance Numbers - in Canada SINs are generally permanent, but temporary residents can have their SIN change if they later become permanent residents, but they’re still the same person. You want your entities to have stable identity, even if things you at one time thought gave them identity change. Surrogate primary keys do that, natural primary keys do not. Don’t use natural primary keys, use surrogate primary keys with unique constraints/indexes. I challenge you to come up with a single plausible example where you’re screwing yourself by choosing surrogate PK + unique constraints/indexes. Meanwhile there are endless examples where you’re screwing yourself by choosing natural PK. reply bioneuralnet 6 hours agorootparentMy first job in the late 2000's was at a small university with a home-grown ERP system originally written in the 80s (Informix-4GL). Student records, employee records, financials, asset tracking - everything. It used natural compound keys. Even worse than the verbose, repetitive, and error-prone conditions/joins was the few times when something big in the schema changed, requiring a new column be added to the compound key. We'd have to trawl through the codebase and add the new column to every query condition/join that used the compound key. It sucked. reply nottorp 7 hours agorootparentprevFunny you should say that... I've been very slooowly degoogling myself, and that includes changing all logins that have a gmail address to a different non google email. I'd say only like 1/3 of the sites I made logins for have the option of changing the email. reply arnorhs 9 hours agorootparentprevAnother benefit of having stable identies / surrogate primary keys is that any relations (FKs) will be much simpler. Sure, like the post poster you replied to is pointing out, you _can_ use natural keys, and then also relying on dates or other parts of the data - but creating a relation for that can end up being extremely cumbersome. - Indexes generally become larger - relationships become harder to define and maintain - Harder for other developers to get up to speed on a project reply twobitshifter 7 hours agorootparentprevON UPDATE CASCADE is not the nightmare that you are making it out to be. (My impression from the article is that this is a single SQL database being discussed.) reply jval43 6 hours agorootparentIt's never a single database in the real world. As soon as you integrate something or have an API to something the keys are out there. Unless you add a translation layer, but then you could just as well use surrogate keys directly. reply mst 6 hours agorootparentUsing a surrogate UUID for communicating with the outside world is often very useful. This is true for an internal PK that's an auto-inc id as well as for natural keys, though. Using a natural PK -inside- your own database can still be a lot more pleasant to work with, even if you don't let it escape. reply shkkmo 2 hours agorootparent> Using a natural PK -inside- your own database can still be a lot more pleasant to work with Until you need to do anything like described above. The advantage of artificial keys is that they have no semantic content. Anything with semantic content carries the risk that the role that semantic content plays in your system can change and cause problems. Having a non-semantic identifier protects you from that. This is not to say that you should never use a semantic identifier as a key. However, you should always have a non-semantic artificial key as the identifier and use the semantic identifiers only when necessary. reply yashap 5 hours agorootparentprevSure, if all data is in a single DB. But in the real world you’ve generally got some/all of: - 1 or more data warehouses - Other services storing said data (e.g. the user id will live in many databases in a service oriented architecture) - External API integrators who have their own data stores totally out of your control that also have copies of parts of your data - Job queues. It’s common to have jobs scheduled for the future on some other system (Redis, etc.) that say “do X for user with id Y in Z days”. If the “id” changes these fail - Caches. Say I cache data by user email instead of a surrogate key, user changes their email, and another user signs up with the old email. Cache hits for the wrong user! - etc. Changing the primary key becomes an absolute nightmare project once data lives in multiple places like this, and in my experience it mostly does. Having truly stable identity for your entities just solves so many future problems, and it’s SO easy to do. In almost all cases, natural PKs are really all downside, virtually zero upside, except slightly less storage. reply mewpmewp2 7 hours agorootparentprev> My impression from the article is that this is a single SQL database being discussed. Even if it's initially single, it's bad to assume that it will be so forever and that you are not going to use third party providers in the future. How well does ON UPDATE CASCADE work if there's millions of existing relations to that entity? reply twobitshifter 6 hours agorootparentYANGNI for 99% of projects and databases. When you get to global sharded nosql etc. you need to use UUIDs for anything and incrementing IDs falls over too. reply sgarland 5 hours agorootparent> incrementing IDs falls over too This is a myth. Planetscale [0] uses integers. They are assuredly at scale. As for auto-incrementing, there are plenty of solutions for distributed systems. [0]: https://github.com/planetscale/discussion/discussions/366 reply mewpmewp2 6 hours agorootparentprevI'm using UUIds by default for everything. Main point being that I don't have to worry about future restrictions. And incrementing IDs are also problematic yes, since they hide business information data within them. And I do think that I need it for much more than 1% of projects and DBs. reply sgarland 5 hours agorootparentYou’ll have to worry about performance tanking instead. If you’re using UUIDv7 then less so, but it’s still (at best) 16 bytes, which is double that of even a BIGINT. Anyone who says UUIDs aren’t a problem hasn’t dealt with them at scale (or doesn’t know what they’re looking at, and just upsizes the hardware). reply chuckadams 4 hours agorootparentMost databases with a UUID type store them as 128-bit integers, typically the same as a BIGINT. It's not like 378562875682765 is the bit representation of a bigint either. And if you're not using uuidv7 or some other kind of cluster-friendly id, you'd best be using a hash index, and if you're doing neither, you probably don't care about their size or performance anyway. You don't pick UUIDs blindly, but on balance, they solve a lot more problems than they cause. reply sgarland 1 hour agorootparentPostgres’ UUID type is 16 bytes. MySQL can store them as BINARY(16) once encoded. Conversely, a BIGINT for either is 8 bytes. Not sure about SQL Server or Oracle. > You don't pick UUIDs blindly, but on balance, they solve a lot more problems than they cause. IME, this is precisely the problem – devs choose them blindly, because then you don’t have to think about proper modeling, you can arbitrarily create a key in your app and be nearly guaranteed of its uniqueness, etc. reply sgarland 6 hours agorootparentprevI was going to say; this is a perfect use case for a cascading FK. reply knallfrosch 11 hours agorootparentprev> then we add the ability for users to update their email. At this point, you should verify the new email. At least until it is verified, you must track the old email. At this point, you realize you can now introduce a synthetic key and you're fine. Let's say you have a duplicate customer entry and the customer demands their accounts be merged. Now you can't identify the user by their key alone, since by definition, they can't be the same (yet.) reply epcoa 10 hours agorootparent> At this point, you realize you can now introduce a synthetic key and you're fine. Except for having to update all foreign references. Some of those may be external further complicating issues. Emails are often among the worst keys because they are not terribly stable and they are reusable often enough to burn you. reply jrs235 7 hours agorootparentAlso are emails case sensitive or not? In some systems (that you don't control mind you) they are and others they are not... reply sgarland 6 hours agorootparentPer RFC5321, the local part (before @) _may_ be case-sensitive, but in practice, it almost never is, and relying on case sensitivity is a recipe for disaster. The domain must always be case-insensitive. reply jrs235 1 hour agorootparentMy point being is if you're using email as a key you \"have to\" treat it as case sensitive even though for most it's not. And yes, I agree it will be a recipe for disaster. reply ourmandave 8 hours agorootparentprevIf I’m going natural PK, I make email the primary key. Welcome to the Mr. Cooper mortgage provider website. Your logon is your email and you can't change it. If you used your cable provider email you're stuck with them for the life of your 30 year mortgage. reply bradleyankrom 7 hours agorootparentOh, also, we've been breached and your information is available for purchase on the dark web. Fun! reply brigandish 11 hours agorootparentprevYou require three fields (or four): email at registration, a date for that entry (together these create a natural key), and current email (this one not part of the key and editable). We're almost all the way to a Tag URI[0], so you could combine it with the user's name or username or any other identifier that fits the spec[1] (you could even use the website's own name) and you have a (definitely two thirds, probably 100%) natural key. It's stable over time and unique, easy to mint, and has a standard behind it. The user also gets to change their contact details without any problem related to the key. [0] https://taguri.org/ [1] http://www.faqs.org/rfcs/rfc4151.html reply TeMPOraL 7 hours agorootparentExcept you're encoding PII in the ID, which makes them plainly visible to people who should not have access to user data, and hard or impossible to change. Sure, I could e.g. change my e-mail and the contact data would be updated, but you still have the old e-mail associated with my account via ID. I'm not sure this would fly under GDPR. reply brigandish 5 hours agorootparentErm, don't show the ID to people who don't need it. Aside from that, it's not a violation of GDPR to keep personal information (that they consented to you having) in order to process business for that person. Using an email address as a unique identifier is not a violation, using it to spam them would be. If they're willing to give you their current email why not an old one? reply lolinder 5 hours agorootparent> Erm, don't show the ID to people who don't need it. How do you communicate with other people in your company about a customer without sending around PII if the customer's ID is PII? Maybe we could create a field that uniquely identifies the customer that isn't PII. Then that could be used to uniquely identify a customer in places where we don't want to expose their PII. But then... why not just use this unique ID as the key? reply brigandish 4 hours agorootparentDo you often send the auto-incremented int (that would be the default substitute to this) when communicating with others? Then why would you send this? It's so strange an argument. Right now you have my username but not my email address, yet you can still query the website database and get certain data that you're allowed to see. There are so many ways to query a particular user's data, and they would all depend on what you're trying to do, needing the specific key would mean you should have access to it anyway and it could be given on per case basis anyway. reply lolinder 4 hours agorootparent> Do you often send the auto-incremented int (that would be the default substitute to this) when communicating with others? It's not an int, but yes, we have a unique synthetic identifier that serves as the database PK and as a means of communicating about a customer in insecure channels without exposing PII. \"Customer ID ### is having an issue with such-and-such.\" To turn your second part back around: why a natural key? What is the function of minting a natural key if humans are meant to use something else? reply brigandish 4 hours agorootparent> To turn your second part back around: why a natural key? What is the function of minting a natural key if humans are meant to use something else? Because non-natural keys are unnecessary in the presence of a natural key, and unnecessary things bring in complexity. > \"Customer ID ### is having an issue with such-and-such.\" Then you need access to the customer's ID, but the devil here is in the detail you didn't add, the such-and-such. > communicating about a customer in insecure channels Use secure channels… reply lolinder 2 hours agorootparent> Use secure channels… When it comes to PII at my company, secure channels means \"encrypted email only\". No Slack, no Jira, no chat in video calls. That's just not feasible for 100% of communications. reply shkkmo 2 hours agorootparentprev> Because non-natural keys are unnecessary in the presence of a natural key, and unnecessary things bring in complexity. None of the things you've presented are actually \"natural\" keys, they are pieces of information that you've made assumptions about to shoehorn them into being usable as a \"natural key\". > Use secure channels… No channel is perfectly secure. As channels become more secure, they become harder to use and add complexity. The more places you store customer data, the more risk you create. The attempt to force semantic data to serve as \"natural key\" has now added risk and complexity to your entire communication infrastructure. reply shkkmo 2 hours agorootparentprev> Do you often send the auto-incremented int (that would be the default substitute to this) when communicating with others? Frequently yes. It is extremely common to communicate about specific records using the ID for that record. The fact that this sort of behavior is extremely common is pretty clearly indicated by the question itself. > There are so many ways to query a particular user's data, and they would all depend on what you're trying to do, needing the specific key would mean you should have access to it anyway A responsible organization at scale with limit and log access to customer data. I should be able to determine if two people are talking about the same customer record without needing access to that record's PII. It is much better to have an artificial key that is linked to this data. There is no upside to the natural key and many, many downsides. reply mewpmewp2 7 hours agorootparentprevBut now then if you want to expose a detail page for that user the id for identifying that page has to include all this potentially personal information about them? e.g. instead of mysocialmedia.com/users/2374927 you would be showing mysocialmedia.com/users/email@example.com-2024-06-05-mysocialmedia.com Then exposing a lot of information that you may have not wanted to expose. reply sgarland 6 hours agorootparentYou don’t have to use the PK as the URL slug. Even if you want to route that way, you can have an internal ID and external ID. This is one way to use something random like a UUIDv4 for display without incurring the costs (at least, some of them) of having it as a PK. reply mewpmewp2 6 hours agorootparentAnd then if you want to list other entities to that user you will have to start mapping the external id and foreign relationships every time to external users? And also if you are doing exception logging, for ids/primary keys there's higher odds of them being logged out, including your own logs and also external platforms. It feels like having primary key set up like this just will complicate everything unnecessarily for the future including many edge cases that you don't foresee. Just have the main ID not have any meaning. It shouldn't contain information about the date, it shouldn't be auto increment, it should really be just random. reply sgarland 6 hours agorootparentThe solution I outlined is the one GitLab and PlanetScale both use internally, so it has been tested at scale and works well, for both Postgres (the former) and MySQL (the latter). > It shouldn't contain information about the date, it shouldn't be auto increment, it should really be just random. That’s a great way to tank performance. You want your PK to be k-sortable. reply brigandish 5 hours agorootparentprev> And then if you want to list other entities to that user you will have to start mapping the external id and foreign relationships every time to external users? If we're talking about relational database engines, that's what they do, relate things. One join statement is much the same as another. reply maweki 12 hours agorootparentprev> I challenge you to come up with a single plausible example So I come from academia, but generally if you use a natural key as PK in a foreign key constraint it may be possible to express additional consistency criteria as CHECK-constraints in the referencing table. So this is a bad example, but say you have Name and Birthdate as your PK, and you have a second table where you have certain special offers sold to your customers and there is this special offer just for Virgos and Pisces, you could enforce that the birth date matches this special offer. Some modern systems also technically allow FK on alternate keys, so you could still do it that way, but database theory often ignores that. But second, while I agree that surrogate keys are often a good idea, I find your argument, that you must design for every conceivable change, not convincing. reply lstamour 11 hours agorootparent> Some modern systems also technically allow FK on alternate keys As far as I can tell, all modern systems allow it, as it is part of the SQL standard that foreign keys can be either primary keys or unique indexes. Here's a brief quotation from a copy of ISO/IEC 9075-2:1999 (not the latest version) that I randomly found online: > If thespecifies a , then the set of s contained in thatshall be equal to the set of s contained in theof a unique constraint of the referenced table. So it mentions unique constraints first. Then afterward it says: > If thedoes not specify a , then the table descriptor of the referenced table shall include a unique constraint that specifies PRIMARY KEY. If I'm reading this right, it means that in the base case, where you specify the column to reference, it can be any unique constraint, where a primary key is just another possible unique constraint (as all primary keys are by definition unique). And only if you don't specify the fields to reference does it then fall back to the primary key instead of a named unique constraint. I'm not disagreeing with you entirely - it's true that often there's an assumption in database theory that primary keys are natural and foreign keys are primary keys. But this isn't a hard requirement in practice or in theory, and it partly depends on the foreign key's purpose, why you need it in the first place. This StackOverflow answer also explains it well: https://softwareengineering.stackexchange.com/a/254566 I should add that there is also a set of database design wisdom that suggests you should never use database constraints such as foreign keys, only app/api constraints, but that's a whole different tangent. reply sgarland 6 hours agorootparent> I should add that there is also a set of database design wisdom that suggests you should never use database constraints such as foreign keys, only app/api constraints, but that's a whole different tangent. That’s less a DB design thought and more of a “devs with little formal training in RDBMS who only want to use it as a dumb store” thought. Use the DB to its strengths. CHECK constraints add an infinitesimal amount of overhead for writes, and guarantee that there will never be invalid data written. A bad code change could allow, however briefly, for that to occur. reply cjfd 11 hours agorootparentprev\"for every conceivable change\". That is not what he is arguing at all. He is showing that there are very many highly plausible changes that are problematic with natural keys. And he totally correct about that. Frankly, the fact that a post arguing for natural keys makes it the top of an HN comment thread is extremely weird. The original article is correct that natural keys are bad. reply spoiler 11 hours agorootparentprevAnother example is where you use a service that provides you with a stable id. It makes little sense to add a surrogate id and a fk on that surrogate id. It violates data quality and integrity just for a hypothetical situation. Data integrity/quality matters. Adding friction to prevent accidents also matters. I don't want something accidentally and trivially updating a field that's used to reference thing externally. Something about the nat key is about to change? Fine, we can write migrations. Even if it affects millions of rows, it's not a big deal. I understand people have been burnt by bad design decisions involving nat keys, but they're not some devil's key everyone here dogmatically makes them out to be. You can mess up using anything. reply maweki 11 hours agorootparent> a service that provides you with a stable id I think there's the important point. Is your key actually natural or is it someone else's surrogate key anyway? Going back to the vehicle identification number: that's already a surrogate key. You just did not assign it yourself. reply sgarland 6 hours agorootparentA VIN is not a surrogate key. A surrogate key must, by definition, have no semantic meaning, and not be composed of parts from multiple domains (among other requirements). A VIN encodes the following: * Country of origin * Manufacturer * Vehicle type * Vehicle model * Engine type * Transmission type * Model year * Manufacturing plant * Serial number reply shkkmo 2 hours agorootparentprev> It makes little sense to add a surrogate id and a fk on that surrogate id. It violates data quality and integrity just for a hypothetical situation. I would still almost always use an internal artificial key on top of the external id. If you want to enforce data integrity, you can still enforce uniqueness on the external id. \"Stability\" of an external identifier is almost always an assumption and one that I've seen fail enough times to want that internal id by default. reply zelphirkalt 11 hours agorootparentprevWhy does anything need to be a primary key anywhere in order to enforce some constraint? At least from ORMs I know I can set for example any group of attributes unique. Other constraints can be implemented in some general method that is called when persisting in the actual database. Even if no ORM, you can write a wrapper around your persisting procedure. reply sgarland 6 hours agorootparentIt doesn’t, you’re right. However, indexes aren’t free, so if your data is such that a natural PK (composite or otherwise) makes sense, you’ll save RAM. Also, for clustering RDBMS like MySQL, data is stored around the PK, so you can get some locality boosts depending on query patterns. reply crabmusket 15 hours agoparentprevThe surrogate key uniquely identifies a row in your database, which is an entity just as real and significant as the car or the employee or what-have-you. Don't confuse the two! I agree with you that having a surrogate key isn't going to save you from the reasons why natural keys can be difficult. The complexity has to go somewhere. But not having a unique identifier for each row is going to make things extra difficult. reply stouset 14 hours agorootparentPrecisely. And if I have a surrogate key to identify “a car”, I get to define what makes a car unique for my purposes. Maybe that really is the VIN. Maybe today it’s the VIN, but tomorrow it’s whatever vehicle is currently being driven by its owner. Maybe it’s something else. Some day I may need to track multiple VINs for a vehicle (maybe it’s got parts from multiple VINs and I want to track that). I can still always decompose that table and have an n-to-1 relationship between cars and VINs without migrating the rest of my data model. reply hobs 15 hours agorootparentprevThe main thing is that the synthetic key should never leave the database and never be displayed in the app - if you want to have another key that represents the human oriented key do it, but it should be another field, an indexed field even, but one that has a lot less monotonic sequential properties that are inherent to synthetic database identifiers. You want to change that human key? Sure. You want to to complain that the keys are not sequential? Sure. You want to actually make them weird strings that are harmful to my brain? Why not? You want to update primary keys in the database? No. Absolutely not. reply The_Colonel 13 hours agorootparentI assume you hint at the security aspect of monotonic keys? I've found this issue a bit overblown. It's basically security by obscurity, which is a nice bonus, but not something your security model can be based on. I mean, it is a good practice to expose some kind of non-sequential key (e.g. UUIDv7), but it doesn't seem to me like a dealbreaker. reply throwaway8486k 12 hours agorootparentSecurity by obscurity makes it more difficult for bad actors as an additional layer that they need to break. I don't see a reason why that's a bad thing and doesn't take a lot of effort to implement in this case. I have been in a startup where competitors used our sequential keys to scrape a list of customers. Lesson learned the hard way with actual business consequences! Sequential keys also leak information (German tank problem) Your competitors can estimate number of customers, your growth rate and other stats that you often don't want them to know. https://en.m.wikipedia.org/wiki/German_tank_problem reply yellowapple 9 hours agorootparent> I have been in a startup where competitors used our sequential keys to scrape a list of customers. If your system allows customers to see each other (or worse: unauthenticated users to see customers) in this fashion in the first place then whether you're using a sequential integer v. a random UUID is the least of your problems. reply mgkimsal 7 hours agorootparentThe 'customers' could be free tier users - a social media type system where everyone has a public profile - intended for the public - would still be scrapable by /profile/1, profile/2, etc. Doesn't necessarily require 'authentication' for the exposing of sequential integers to have a bad outcome. reply throwaway8486k 2 hours agorootparentYou're right. The urls were public to be shared (think of marketing material / ecommerce), so there was not a security incident. But it did give our competitor free highly qualified leads that they could use to poach customers. This product was new to our customers, and we had spent a lot of time selling and convincing them that it was useful. reply The_Colonel 12 hours agorootparentprevI'm not saying it's a bad practice, the opposite actually. > I don't see a reason why that's a bad thing and doesn't take a lot of effort to implement in this case. True, if you start your application from scratch. Like if I started designing a new app today, I'd just choose the UUIDv7 for the primary key. It's not an easy thing to add into an existing application, though. I see applications leaking their internal IDs all the time, but usually it's not worth the effort to fix that, because it's a comparatively minor problem. reply throwaway8486k 11 hours agorootparent> I'm not saying it's a bad practice, the opposite actually I'm sorry that I didn't make that more clear. I saw that you mentioned it as a best practice and are aware of the advantages. It's just that there are so many others that don't have the balanced view as you seem to have. I have been involved in many discussions at my work place where \"security by obscurity\" is used as a way to shut down discussions. They changed their minds about sequential keys after the incident I mentioned, but it still has the power to \"win\" other discussions. Sure, we need to have rate limiting on ip-addresses, auth and other mechanisms, but they are not perfect and bugs happen all the time. An \"unguessable\" id is an additional security layer > It's not an easy thing to add into an existing application, though I agree, but there are ways to reduce the attack surface. You could add an extra \"public id\" field that can be used for lookup in addition to the existing id. In this way you can have a gradual migration where you go through each endpoint and migrate them individually without changing the foreign keys and other relations in the database (they would still use the sequential key). Maybe you end up not having time to migrate them all, but at least you can reduce the attack surface on the most sensitive endpoints. If you have low volume endpoints you could perhaps even simply add a mapping layer where you do a simple db lookup to replace the public key with the internal without changing the existing queries. You could even cache this easily to reduce the db load. (both ids are permanently fixed and can be cached forever). reply inopinatus 11 hours agorootparentprevUUIDv7 may not be generated sequentially, but it is still temporally ordered and discloses a timestamp, which may be an undesirable leakage for some applications/environments. When obfuscation matters that much, use a UUIDv4 and eat the index performance hit. Some might suggest, \"encrypt your object identifiers on the way in/out\", but there's a ton of pitfalls involved since for most applications they are now rolling their own crypto, and it also makes identifiers much longer. reply The_Colonel 11 hours agorootparentYes, you can go to great depths, but they each have trade-offs - in performance, increased complexity etc. and it's necessary to make a judgment for each particular app instead of applying the most overengineered solution everywhere. reply crabmusket 12 hours agorootparentprevI love it when my competitors use sequential integer IDs. reply hackit2 12 hours agorootparentprevA entity can exist over more than one row in your database, but it is useful to uniquely identify each row as the lowest common denominator. reply mkleczek 11 hours agorootparentprev> The surrogate key uniquely identifies a row in your database, which is an entity just as real and significant as the car or the employee or what-have-you. Don't confuse the two! But the DBMS already maintains a row identifier (called rowid or ctid or whatever depending on the DBMS). Why do you need an explicit one? reply simonw 10 hours agorootparentBe careful with those. SQLite has a rowid concept, but it's not guaranteed to be stable - running a VACUUM against a table can reassign the rowids for every row! https://sqlite.org/rowidtable.html says: > If the rowid is not aliased by INTEGER PRIMARY KEY then it is not persistent and might change. In particular the VACUUM command will change rowids for tables that do not declare an INTEGER PRIMARY KEY. Therefore, applications should not normally access the rowid directly, but instead use an INTEGER PRIMARY KEY. reply magicalhippo 11 hours agorootparentprevIn the DB we use[1] the internal row id is not stable: The value returned by the function is not necessarily constant between queries as various operations performed on the database may result in changes to the row identifiers of a table. So, users should refrain from using the ROWID function in ordinary situations; retrieval by primary key value should be used instead. [1]: https://infocenter.sybase.com/help/index.jsp?topic=/com.syba... reply sgarland 6 hours agorootparentprevBecause every DB can and will shift those as needed. They reference the physical location on disk for a given tuple. They are not meant for general consumption. reply unchar1 11 hours agorootparentprevIt may be useful if you have data that originates from another source or if something outside of our system references your entity. In that case you need to keep some form of an externalRef, so it's usually easier to just use an id that you can control, for referencing both internally and externally. reply CrimsonRain 13 hours agoparentprevSorry. This is a very bad advice. I just had to fight tooth and nail to make my lead turn around from this disastrous decision. Using a lot of external IDs as our own row primary keys and then they get propagated to all other tables as foreign keys and what not. One day the foreign key chances or God forbid, the formatting changes in external systems, now we need to fix our whole database and all codes instead of a small isolated place. Generate your own unique keys for everything; add a few more unique constraints if needed. A bit more work but never a regret. reply aidos 12 hours agorootparentYeesh. I once made the mistake of using an external ID as a primary key. What a day it was when they were changed on me. reply naasking 6 hours agorootparent> I once made the mistake of using an external ID as a primary key. What a day it was when they were changed on me. I've kept with this advice for the most part, but I'm tempted in some cases to use the external id when there's some guarantee of stability and universality. Like 2 and 3 digit ISO country codes. reply Piskvorrr 1 hour agorootparentNot that I'd get about 5 different ISO country code changes (with some flipflopping) just by sitting in this very same spot for a couple of decades. \"Stability\" in country codes, bah humbug. reply naasking 54 minutes agorootparentYour geographical location might not be stable, but I'm referring to the ISO codes that name the country, which should be relatively stable. reply spoiler 12 hours agorootparentprevCan you share more about this? Wouldn't you run into the same problems if you used a surrogate pk? Without the nat/external pk and fk, you run the risk of having validity issues. Conversely, if the ID changes, isn't the friction what you want? I feel like optimising for unlikely edge cases instead of integrity because of a single incident is too reactionary. Writing a query or script to update a string, even for millions of rows, isn't that big of a deal? Obviously there _are_ cases where nat pks/fks are bad, but not all of them. reply mixmastamyk 11 hours agorootparentInstead of migrating one column, now you have that and every table with a foreign key to the original. Requiring transactions and possibly locking etc. reply huygens6363 14 hours agoparentprevThere is a model of a thing and there is a row that stores a representation of that model of a thing. They both are things. Ignoring the last one might be tempting, but it’s not practical. Interestingly your own way of thought is applied, but now a level deeper again. How do you model a row? What makes it unique? A surrogate ID is the only sensible unique identifier for such a thing as there is no “natural key” that would make sense for instances of something so general as “Row”. What you were saying amounts to “don’t model the thing holding the model”, but experience shows the thing holding the model is itself an (often unwilling) active part of systems. Someone here gave the example of wrongly entered PK’s by administrative personnel handing live customers. That’s IMO a good example of why you need an extra layer on top of your Actual Model(c). I can think of more. reply crabmusket 12 hours agorootparent> They both are things. Corollary: your app is part of the real world. reply TeMPOraL 7 hours agorootparentFurthermore: bugs in your app are part of the real world, so woe to those who used keys from your app as natural/external keys in their app. reply aswerty 12 hours agoparentprevSurrogate keys do mirror reality though. As I once read in a Terry Pratchett book; if you replace the handle of an axe and then replace the head, is it still the same axe? For me, the answer is yes - since we imbue the axe with an identity outside of it's integral parts. That is what a surrogate key is. An identity. Which is an abstract concept that exists in the real world. And to pile on. The top comment is bad advice! Surrogate keys provide sanity - god save you if you have to work in a database solely using natural keys. reply vintermann 12 hours agorootparentYes. Ultimately, \"are these the same\" and \"are these different\" are philosophical questions. Or to be more precise, teleological questions. Because those questions have no meaning except with an \"for our purposes here\" added. And it's up to us to decide what we care about, if anything. If we care about what other people want with the data though, or suspect that our own wants might not be set in stone, then we should also care to model identity independently in the system (that is, use surrogate keys). reply fuzztester 10 hours agorootparentprev>As I once read in a Terry Pratchett book; if you replace the handle of an axe and then replace the head, is it still the same axe? Yes and no. It is the Axe of Theseus ;) https://en.m.wikipedia.org/wiki/Ship_of_Theseus reply mgkimsal 6 hours agorootparentOr Trigger's Broom: https://www.youtube.com/watch?v=BUl6PooveJE reply aswerty 9 hours agorootparentprevThanks - I think somewhere in the back of my mind I was also aware of the Ship. But Pratchett is too good. The quote: > This, milord, is my family's axe. We have owned it for almost nine hundred years, see. Of course, sometimes it needed a new blade. And sometimes it has required a new handle, new designs on the metalwork, a little refreshing of the ornamentation . . . but is this not the... axe of my family? reply akira2501 9 hours agorootparentprev> since we imbue the axe with an identity outside of it's integral parts. Typically for the purposes of ownership. So it's really part of a a hierarchical identity scheme. reply SPBS 13 hours agoparentprev> You think your surrogate key will save you? It will not. The world has an external reality that needs to be reflected in your database. Yes, it will. It is precisely because of a messy external reality that you need an unchanging internal ID that is unaffected by changes to the external ID. If the software designer in the article had followed your advice, changing the chassis number would have likely resulted in broken car ownership records. Whoever is reading this in the future, please don't follow the parent comment's advice. Use surrogate/synthetic keys for your primary key. reply hyperman1 11 hours agoparentprevIn my experience, this won't end well. Some examples: Belgium has the RNR/INSZ identifying each person. But it can change for a lot of reasons: It gets reused if someone dies. It encodes birth date, sex, asylum state, so if something changes (which happens about every day), you need to adapt your unique key. Belgium also has a number identifying medical organizations. Until they ran out of numbers. Then someone decided to change the checksum algorithm, so the same number with a different checksum meant a different organizations. And of course they encode things in the number and reuse them, so the number isn't stable. An internal COBOL system had a number as unique key, and this being COBOL, they also ran out of numbers. This being COBOL, it was more easy to put characters in the fixed width record than expand it. And this being French COBOL, that character can have accents. So everyone using the 'number' as unique key now had to change their datatype to text and add unicode normalization. Not fun. In my experience: don't use anything as an ID that you didn't generate yourself. Make it an integer or UUID. Do not put any meaning in the ID. Then add a table (internal ID, registering entity, start date, end date or null, their key as text). You 'll still sometimes have duplicates and external ID updates as the real world is messy, but at least you have a chance to fix them. The overhead of that 1 lookup is negligable on any scale. reply IlliOnato 6 hours agoparentprevSorry, by your logic an ISSN would be a good key for a database of scientific journals. It's exactly what ISSN is invented for! Right? Right? Been there, done that. Journals that changed their names (and identities) but not ISSN. That changed the ISSN but not the name/identity. Journal mergers which instead of obtaining a new ISSN kept one of the old ones. \"Predatory journals\" that \"borrow\" an ISSN (you may not consider them real journals, but you've got to track them anyway, even if only to keep them from being added to the \"main\" database). The list may go on and on. And don't even start me on using even more natural ID, the journal name, perhaps in combination with some other pieces of data, like year the publication started, country of origin, language, etc... Any scheme based on this will need to have caveats after caveats. (A fun fact: there were journals that ceased publication but later on \"returned from the dead\". Such resurrected journals are supposed to be new journals and to get a new ISSN. Sometimes this rule is followed...) At the end, a \"meaningless number\" you assign yourself is the only ID that reliably works (in combination with fields representing relationships between journals). The problem with keys that \"have meaning\" is that they appear to carry information about your entity. And in vast majority of cases this is correct information! So it's almost impossible to resist \"extracting\" this information from a key without doing actual database lookup at least mentally, and often in your software too. Hidden assumptions like this lead to bugs that are really hard to eliminate. A meaningless number on the other hand does not tempt one :-) reply kayodelycaon 15 hours agoparentprevOf course they won’t save you from external data. The whole point is for your system to have a way to identify rows internally so you can deal with external systems getting wonky without corrupting your own data. All of your concerns are easily solved by a unique index. Using external keys as a forcing function to prevent people from representing data wrong is not great. reply polemic 15 hours agoparentprevIn the wise words of Patsy, \"it's only a model\". The real world is resistent to clean abstractions and abstractions are distressingly subject to change. What made your row unique today is quite likely to become non-unique in the days/months/years to come. Always use surrogate keys. Your future self will thank you. reply hehdhdjehehegwv 14 hours agorootparentAnd always kids, write code for one person, and one person only: your future self. reply M95D 11 hours agorootparentYeah, fsck your co-workers and your replacement when you quit. reply hehdhdjehehegwv 1 hour agorootparentIf you’re good to your future self you’re also good to them. reply ratherbefuddled 9 hours agoparentprev> how will the surrogate key protect you? It will not. Yes it will. Your changes will be confined to only the table(s) where the natural key is present, not spread across every table where there's a foreign key. Of course you will still have to deal with the reality that the natural key is now not unique, and model reality, but your implementation work in doing so is far simpler. In more years than I care to count I've regretted someone using natural keys as a primary key for a table many times, and surrogates never. reply wruza 8 hours agoparentprevIn short, you advise us to foresee the future, explore unknown unknowns and expect high-precision true answers from the outside. Good advice, not for this universe. You can only get false negatives in this one. A synthetic key means “we think exists”. There exists a contract, a medical record, a person, in a real world, in our opinion. We record these existence identities into our model by using an always-unique key. Then there’s a date, an appointment #, a name, etc. You can refer to an entity by its identity, or search by its attributes. If you use searches in place of identity references, you get non-singletons eventually and your singleton-based logic breaks. reply datadrivenangel 6 hours agorootparentThis is why it's hard to be a DBA... Everyone thinks you're a Cassandra user the way other developers ignore your prophecies. reply groestl 12 hours agoparentprevAdding to the list of comments damning this post to ensure none of my future colleagues follow this advice. > You think your surrogate key will save you? It will not. It definitely will, say my 20+ years of experience. reply DrScientist 7 hours agoparentprev> The natural key forces you to think about what makes the row unique. What identifies it. When designing a table - you should always be clear about what the natural key is and make sure it has uniqueness constraint. Mindlessly having a surrogate key without thinking about what the natural key is, is an anti-pattern. So totally agree here. That doesn't stop you also having a surrogate key though. Another aspect of natural versus surrogate keys is joins as the key often ends up in both tables. Using natural keys can mean in some circumstances you can avoid a join to get the information you want - as it's replicated between tables. There is also the question of whether you surface the surrogate key in the application layer or not - some of the problems of surrogate keys can be avoided by keeping them contained within the app server logic and not part of the UI. So via the UI - you'll search by car registration number, not surrogate key, but in terms of your database schema you don't join the tables on registration number - you use a surrogate key to make registration numbers easier to update. reply sgarland 6 hours agoparentprevYour post brings up a critical difference I’ve noticed when working with devs (I’m a DBRE): those who actually do rigorous data modeling, and those who view it as an annoyance impeding their project. Spend time modeling your schema. Ask yourself, “does every attribute in this table directly relate to the primary key? And is every attribute reliant upon the primary key?” Those two alone will get you most of the way through normalization. reply tadfisher 14 hours agoparentprevIn that case, using this natural datum as an ID is a contradiction, because we are now removing the uniqueness constraint. It's fine to model the real world, but the real world also includes your data, and you may want to identify unique _records_ as it is not a universal truth that data can be corrected on entry. reply abujazar 7 hours agoparentprevNatural keys can change, and synthetic keys never have to. That alone is reason enough to use synthetic keys. Performance is another major argument for synthetic keys, as they are can be made sequential, which is rarely the case for natural keys. reply zer00eyz 10 hours agoparentprev> Model your data on the real world. Do not depend on spherical horses. Yes, and normalize it. https://en.wikipedia.org/wiki/Database_normalization > Adding a synthetic key only means you have to track all three. Plus you have to generate a meaningless number, which is actually a choke point in your data throughput. This is true up to a point. You can add more data to the system to continue to generate natural, composite keys. However at some point you move from a database to an event stream, or you have to track events that aren't really needed for what your doing... Denormalization then takes precedence and a generated key makes sense again. https://en.wikipedia.org/wiki/Denormalization > how will the surrogate key protect you? It isnt about protection, it's about not collecting the",
    "originSummary": [
      "Mark Seemann's blog post argues for using synthetic keys over natural keys in database design, citing reliability and data integrity.",
      "He uses a personal anecdote about a car chassis number error to highlight issues with natural keys, such as data-entry errors and ensuring uniqueness.",
      "Reader comments provide additional perspectives, discussing the role of natural keys in logical data modeling and challenges with ORM (Object-Relational Mapping) libraries."
    ],
    "commentSummary": [
      "The discussion critiques natural keys in databases, recommending unique, human-readable IDs generated by JavaScript/TypeScript functions with a time component to reduce fragmentation.",
      "It advises using 64-bit auto-incremented primary keys for internal operations to boost performance and minimize index bloat, while random string IDs enhance security for public data.",
      "The conversation underscores the trade-offs between natural and synthetic keys, favoring surrogate keys for their stability, consistent references, and enhanced data integrity."
    ],
    "points": 628,
    "commentCount": 471,
    "retryCount": 0,
    "time": 1717550709
  },
  {
    "id": 40578705,
    "title": "Animated Guide to Fourier Series: From Circles to Epicycles",
    "originLink": "https://www.andreinc.net/2024/04/24/from-the-circle-to-epicycles",
    "originBody": "From the Circle to Epicycles (Part 1) - An animated introduction to Fourier Series April 24, 2024 17 minute read This article will be part of a more extended series in which I plan to explore various aspects of Fourier Mathematics. I will take notes, create some visuals (a good pretext to learn more about graphics), and hope that it will be useful to someone other than me. The article has yet to be thoroughly reviewed by anyone other than me, so I put it online, hoping to get some feedback before bringing it to a final state. Table of contentsPermalink In this series, we will start with a brief recap of some of the math concepts related to the circle, including trigonometric functions like sine and cosine. We’ll also discuss Euler’s identity, introduce the concept of a sinusoid (and complex sinusoid), and finally, we’ll introduce the concept of Fourier Series. The animations used in this series are original, although I drew inspiration from some existing materials found on the web. Please keep in mind that this is not a comprehensive course on the topic, so if you’re really interested in learning more, I recommend taking a full course on the subject. The Circle The number 𝜋 π Radians The sine and the cosine The cos ⁡ cos leads the sin ⁡ sin The parity of cos ⁡ cos and sin ⁡ sin Complex Numbers and the Unit Circle Multiplying with 𝑖 i means a rotation with 𝜋 2 2π Euler’s identity Euler’s formula, the connection between 𝑒 e, 𝜋 π and 𝑖 i The sin ⁡ sin and cos ⁡ cos in their exponential form The sinusoid Sinusoids are flexible Sinusoids can be complex Sinusoids can nullify themselves Adding sinusoids leads to complexity Adding random sinusoids for fun Playing Sinusoidal tetris for fun A square wave and sinusoids Epicycles - First Encounter Epicycles - An intuitive understanding Epicycles - A flower Fourier Series Fourier Series in Exponential Form Example: The Fourier Series for the Box Function Example - The Fourier Series of the Triangle wave Example - The Fourier Series of a Sawtooth Function The Fourier Series Machinery to be continued The CirclePermalink It all starts with The Circle: The Circle is a geometrical figure with a center 𝑃 ( 𝑎 , 𝑏 ) P(a,b), and a radius 𝑟 r. It has the following associated equation: ( 𝑥 − 𝑎 ) 2 + ( 𝑦 − 𝑏 ) 2 = 𝑟 2 (x−a)2+(y−b)2=r2 If 𝑎 = 0 , 𝑏 = 0 a=0,b=0 and 𝑟 = 1 r=1, the circle becomes less generic, so we call it The Unit Circle: The equation for The Unit Circle is: 𝑥 2 + 𝑦 2 = 1 x2+y2=1 One could argue The Circle is the epitome of symmetry. Pick one point, 𝐴 A, then its reflection on the other side, 𝐴 ′ A′, and start rotating: What happens on The Circle, stays on The Circle. The number 𝜋 πPermalink We rarely see angles expressed in degrees; usually, we represent them in relation to the number PI 𝜋 π: 𝜋 π, 𝜋 2 2π, 𝜋 3 3π, 𝜋 4 4π, etc.; 𝜋 π is the ratio of a circle’s circumference to its diameter. π ≈ 3.14. If 𝑟 ≠ 1 r=1, the circle’s perimeter is 𝑃 = 2 𝜋 𝑟 P=2πr, while the area is 𝐴 = 𝜋 𝑟 2 A=πr2. Both 𝐴 A and 𝑃 P depend on 𝜋 π. 𝜋 π is irrational and transcendental. RadiansPermalink The radian (or rad) is the actual unit we use to measure angles. An intimate relationship exists between the radian and the number 𝜋 π. To transform an angle measured in degrees ( 360 ° 360°) to radians, the algorithm is simple: we multiply the angle by 𝜋 π, then divide the result by 180 180. The sine and the cosinePermalink We can define sin ⁡ sin and cos ⁡ cos in relationship to The Unit Circle: 𝜃 θ is the angle formed by the radius, 𝑟 r and the 𝑥 x axis, at every given point. The sin ⁡ sin function represents the 𝑦 y-coordinate of a point on the Unit Circle; The cos ⁡ cos function is the 𝑥 x-coordinate of the same point on the Unit Circle. sin ⁡ sin and cos ⁡ cos are periodic functions with the period 2 𝜋 2π. At this point, it will be unfair for cos ⁡ cos not to plot it on the same graph: The cos ⁡ cos leads the sin ⁡ sinPermalink Putting cos ⁡ cos and sin ⁡ sin side by side, we observe that they are not that different: sin ⁡ ( 𝑥 + 𝜋 2 ) = cos ⁡ ( 𝑥 ) sin(x+2π)=cos(x) We say that cos ⁡ cos leads the sin ⁡ sin with 𝜋 2 2π: The parity of cos ⁡ cos and sin ⁡ sinPermalink The parity of mathematical functions generally refers to whether a function is even, odd, or neither. The cosine is even, meaning cos ⁡ ( 𝑥 ) = cos ⁡ ( − 𝑥 ) cos(x)=cos(−x): And the sine is odd, meaning sin ⁡ ( − 𝑥 ) = − sin ⁡ ( 𝑥 ) sin(−x)=−sin(x), or sin ⁡ ( 𝑥 ) = − sin ⁡ ( − 𝑥 ) sin(x)=−sin(−x): Complex Numbers and the Unit CirclePermalink In the Complex Plane, the points on the circle are defined by the following equation: 𝑧 = cos ⁡ ( 𝜃 ) + 𝑖 ∗ sin ⁡ ( 𝜃 ) z=cos(θ)+i∗sin(θ) Multiplying with 𝑖 i means a rotation with 𝜋 2 2πPermalink Multiplying a complex number with imaginary unit 𝑖 i is the equivalent of rotating the number counterclockwise with 𝜋 2 2π on an “imaginary circle” with the radius: 𝑟 = ∣ 𝑎 + 𝑏 ∗ 𝑖 ∣ = 𝑎 2 + 𝑏 2 r=∣a+b∗i∣=a2+b2 For example, if we take 𝑧 1 ∈ 𝐶 z1∈C and we multiply it three times with 𝑖 i we will have it rotated into the all 4 quadrants of the circle: Euler’s identityPermalink The natural exponential function, often denoted as 𝑓 ( 𝑥 ) = 𝑒 𝑥 f(x)=ex, is a particular case of the exponential function where the base is 𝑒 e, also known as Euler’s Number ( 𝑒 ≈ 2.71828 e≈2.71828). 𝑒 e, just like 𝜋 π, irrational and transcendental. We normally define 𝑒 e as: 𝑒 = ∑ 𝑛 = 0 ∞ ( 1 𝑛 ! ) = 1 0 ! + 1 1 ! + 1 2 ! + . . . e=n=0∑∞(n!1)=0!1+1!1+2!1+... 𝑒 = lim ⁡ 𝑥 → ∞ ( 1 + 1 𝑥 ) 𝑥 e=x→∞lim(1+x1)x 𝑒 = lim ⁡ 𝑥 → 0 ( 𝑥 + 1 ) 1 𝑥 e=x→0lim(x+1)x1 Even if not obvious, there’s a strong connection between 𝑒 e and the circle. The natural exponentiation function is an eigenfunction for differentiation. An eigenfunction in this context is a function that, when differentiated, yields a constant multiple of itself: 𝑑 𝑑 𝑥 𝑒 𝑎 𝑥 = 𝑎 ∗ 𝑒 𝑥 dxdeax=a∗ex If we substitute 𝑎 → 𝑒 a→e, by subsequently differentiating 𝑓 ( 𝑥 ) = 𝑒 𝑖 𝑥 f(x)=eix, we obtain: 𝑑 𝑑 𝑥 𝑓 ( 𝑥 ) = 𝑑 𝑑 𝑥 ( 𝑒 𝑖 𝑥 ) = 𝑖 ∗ 𝑒 𝑖 𝑥 dxdf(x)=dxd(eix)=i∗eix 𝑑 2 𝑑 𝑥 2 𝑓 ( 𝑥 ) = 𝑑 2 𝑑 𝑥 2 ( 𝑒 𝑖 𝑥 ) = 𝑑 𝑑 𝑥 ( 𝑖 ∗ 𝑒 𝑖 𝑥 ) = − 𝑒 𝑖 𝑥 dx2d2f(x)=dx2d2(eix)=dxd(i∗eix)=−eix 𝑑 3 𝑑 𝑥 3 𝑓 ( 𝑥 ) = 𝑑 3 𝑑 𝑥 3 ( 𝑒 𝑖 𝑥 ) = 𝑑 𝑑 𝑥 ( − 𝑒 𝑖 𝑥 ) = − 𝑖 ∗ 𝑒 𝑖 𝑥 dx3d3f(x)=dx3d3(eix)=dxd(−eix)=−i∗eix 𝑑 4 𝑑 𝑥 4 𝑓 ( 𝑥 ) = 𝑑 4 𝑑 𝑥 4 ( 𝑒 𝑖 𝑥 ) = 𝑑 𝑑 𝑥 ( − 𝑖 ∗ 𝑒 𝑖 𝑥 ) = 𝑒 𝑖 𝑥 = 𝑓 ( 𝑥 ) dx4d4f(x)=dx4d4(eix)=dxd(−i∗eix)=eix=f(x) In simple terms, after we differentiate 𝑓 ( 𝑥 ) f(x) four times ( 𝑓 ′ ( 𝑥 ) , 𝑓 ′ ′ ( 𝑥 ) , 𝑓 ′ ′ ′ ( 𝑥 ) , 𝑓 ′ ′ ′ ′ ( 𝑥 ) f′(x),f′′(x),f′′′(x),f′′′′(x)), our function does a full circle. It’s the same pattern described in the previous section when we multiplied our 𝑧 1 z1 with 𝑖 i. Subsequently deriving 𝑒 𝑖 𝑥 eix is the equivalent of subsequently multiplying 𝑒 𝑖 𝑥 eix with 𝑖 i. Multiplying a complex number with 𝑖 i means rotating that number in the Complex Plane with 𝜋 2 2π. But what is a derivative of a function at a certain point? It’s the rate of change of that function at that particular point. But we’ve just said that the derivative of 𝑒 𝑖 𝑥 eix is equivalent to a rotation. So, the rate of change is rotational. We can intuitively feel that the function 𝑒 𝑖 𝑥 eix describes an actual circle. There’s no other possible solution. So we can boldly affirm that 𝑒 𝑖 𝑥 = cos ⁡ ( 𝑥 ) + 𝑖 ∗ sin ⁡ ( 𝑥 ) eix=cos(x)+i∗sin(x) (which is the formula discovered by Euler). Of course, this is not a rigorous demonstration. One can prove Euler’s identity using Taylor Series. Euler’s formula, the connection between 𝑒 e, 𝜋 π and 𝑖 iPermalink Euler’s formula is a thing of marvel: 𝑒 𝑖 𝑥 = cos ⁡ ( 𝑥 ) + 𝑖 ∗ sin ⁡ ( 𝑥 ) eix=cos(x)+i∗sin(x) Or, if we choose 𝑥 = 𝜋 x=π: 𝑒 𝑖 𝜋 + 1 = 0 eiπ+1=0 The sin ⁡ sin and cos ⁡ cos in their exponential formPermalink By substituting 𝑥 → − 𝑥 x→−x into Euler’s identity we obtain: 𝑒 − 𝑖 𝑥 = cos ⁡ ( 𝑥 ) − 𝑖 ∗ sin ⁡ ( 𝑥 ) e−ix=cos(x)−i∗sin(x) If we add/subtract the two equalities, we will obtain the definition of the sine and cosine functions in their exponential form: cos ⁡ ( 𝑥 ) = 𝑒 𝑖 𝑥 + 𝑒 − 𝑖 𝑥 2 cos(x)=2eix+e−ix sin ⁡ ( 𝑥 ) = 𝑒 𝑖 𝑥 − 𝑒 − 𝑖 𝑥 2 ∗ 𝑖 sin(x)=2∗ieix−e−ix All points of the circle are determined by a functon 𝑧 ( 𝑥 ) z(x), where: 𝑧 ( 𝑥 ) = 𝑒 𝑖 𝑥 = cos ⁡ ( 𝑥 ) ⏟ 𝑅 𝑒 ( 𝑥 ) + 𝑖 ∗ sin ⁡ ( 𝑥 ) ⏟ 𝐼 𝑚 ( 𝑥 ) z(x)=eix=Re(x)cos(x)+Im(x)i∗sin(x) 𝑥 x represents the actual angle 𝜃 ∈ 𝑅 θ∈R, so 𝑧 ( 𝜃 ) = 𝑒 𝑖 𝜃 = 𝑐 𝑜 𝑠 ( 𝜃 ) + 𝑖 ∗ 𝑠 𝑖 𝑛 ( 𝜃 ) z(θ)=eiθ=cos(θ)+i∗sin(θ). You’ve seen that we’ve interchanged 𝑥 x with 𝑡 t and 𝜃 θ throughout the article. It’s confusing, but don’t get confused. We are the ones to decide how to look at 𝑥 x, as an angle or as time. The sinusoidPermalink A sinusoid, or a sine wave, is a smooth, repetitive curve defined by following function: 𝑦 ( 𝑡 ) = 𝐴 ∗ sin ⁡ ( 2 𝜋 𝑓 𝑡 + 𝜑 ) = 𝐴 ∗ sin ⁡ ( 𝜔 𝑡 + 𝜑 ) y(t)=A∗sin(2πft+φ)=A∗sin(ωt+φ) 𝐴 A is called the amplitude, representing the maximum deviation of the function from zero. 𝑓 f is called ordinary frequency and denotes the number of oscillations (the radius moving inside the circle) occurring each second. 𝜔 = 2 𝜋 𝑓 ω=2πf is called the angular frequency; it’s the same thing as ordinary frequency, but we measure it 𝑟 𝑎 𝑑 𝑖 𝑎 𝑛 𝑠 𝑠 𝑒 𝑐 𝑜 𝑛 𝑑 secondradians; 𝜑 φ is called phase offset; it’s measured in radians. If we consider time to be the 𝑥 x-axis, and 𝑦 ( 𝑡 ) y(t) the 𝑦 y-axis, the sinusoid becomes: 𝑦 = 𝑓 ( 𝑥 ) = 𝐴 ∗ sin ⁡ ( 𝜔 𝑥 + 𝜑 ) y=f(x)=A∗sin(ωx+φ) The sin ⁡ sin and cos ⁡ cos are just particular cases of sinusoids. Sinusoids are flexiblePermalink The following animation is interactive. You can choose the values of 𝐴 = A= 0.5 1 1.5 2 , 𝜔 = ω= -2 -1 1 2 3 4 5 6 7 8 and 𝜑 = φ= 0 π/2 π 3π/2 to plot the sinusoid (if you pick 𝜑 = 𝜋 2 φ=2π a cosine is plotted): Sinusoids can be complexPermalink Starting with the definition of a sinusoid: 𝑦 ( 𝑡 ) = 𝐴 ∗ 𝑠 𝑖 𝑛 ( 2 𝜋 𝑓 𝑡 + 𝜑 ) = 𝐴 ∗ 𝑠 𝑖 𝑛 ( 𝜔 𝑡 + 𝜑 ) y(t)=A∗sin(2πft+φ)=A∗sin(ωt+φ) If we multiply each side with 𝐴 A: 𝐴 ∗ 𝑒 𝑖 𝜃 = 𝐴 ∗ ( cos ⁡ ( 𝜃 ) + 𝑖 ∗ sin ⁡ ( 𝜃 ) ) A∗eiθ=A∗(cos(θ)+i∗sin(θ)) If we substitute 𝜃 θ with 𝜃 = 𝜔 𝑡 + 𝜑 θ=ωt+φ we obtain the complex sinusoid: 𝑠 ( 𝑡 ) = 𝐴 ∗ 𝑒 𝑖 ( 𝜔 𝑡 + 𝜑 ) = 𝐴 ∗ cos ⁡ ( 𝜔 𝑡 + 𝜑 ⏟ 𝜃 ) + 𝑖 ∗ 𝐴 ∗ sin ⁡ ( 𝜔 𝑡 + 𝜑 ⏟ 𝜃 ) s(t)=A∗ei(ωt+φ)=A∗cos(θωt+φ)+i∗A∗sin(θωt+φ) A complex sinusoid captures the behavior of two sinusoids (one cosine and one sine) on both its axes; on the real part, it behaves like a cosine, while on its imaginary part, it behaves like a sine. The two are in sync as they both depend on the free variable 𝜃 θ, expressed as 𝜃 = 𝜔 𝑡 + 𝜑 θ=ωt+φ. (Source code) Two interesting observations: If we project the complex sinusoid on the plane determined by the Y-axis and Z-axis, we will plot a sine (the Imaginary part); If we project the complex sinusoid on the plane determined by the X-axis and Z-axis, we will plot a cosine (the Real part); Sinusoids can nullify themselvesPermalink Two sinusoids in phase and sharing the same amplitude but with opposite frequencies nullify themselves. Adding sinusoids leads to complexityPermalink Let’s plot two arbitrary selected sinusoids 𝑦 1 ( 𝑥 ) y1(x) and 𝑦 2 ( 𝑥 ) y2(x), where: 𝑦 1 ( 𝑥 ) = 9 10 ∗ 𝑠 𝑖 𝑛 ( 7 𝑥 + 𝜋 2 ) y1(x)=109∗sin(7x+2π), and 𝑦 2 ( 𝑥 ) = 12 10 ∗ 𝑠 𝑖 𝑛 ( 3 𝑥 − 2 ) y2(x)=1012∗sin(3x−2) . The sum 𝑦 ( 𝑥 ) = 𝑦 1 ( 𝑥 ) + 𝑦 2 ( 𝑥 ) y(x)=y1(x)+y2(x) shows an interesting pattern Adding random sinusoids for funPermalink Adding more sinusoids to an existing one ( 𝐴 = 1 A=1, 𝜔 = 1 ω=1, 𝜑 = 1 φ=1) generate more complex patterns: Playing Sinusoidal tetris for funPermalink Not so long ago, I’ve re-imagined the game of Tetris. It’s now possible to play Tetris with Sinusoids: A square wave and sinusoidsPermalink If we carefully choose the sinusoids, we can create predictable patterns: Let’s take, for example, use the following formula: 𝑦 ( 𝑥 ) = 4 𝜋 ∑ 𝑘 = 1 ∞ sin ⁡ ( 2 𝜋 ( 2 𝑘 − 1 ) 𝑓 𝑥 ) 2 𝑘 − 1 y(x)=π4k=1∑∞2k−1sin(2π(2k−1)fx) Through expansion, the formula becomes: 𝑦 ( 𝑥 ) = 4 𝜋 sin ⁡ ( 𝜔 𝑥 ) ⏟ 𝑦 1 ( 𝑥 ) + 4 3 𝜋 sin ⁡ ( 3 𝜔 𝑥 ) ⏟ 𝑦 2 ( 𝑥 ) + . . . + 4 ( 2 𝑘 − 1 ) 𝜋 sin ⁡ ( ( 2 𝑘 − 1 ) 𝜔 𝑥 ) ⏟ 𝑦 𝑘 ( 𝑥 ) + . . . y(x)=y1(x)π4sin(ωx)+y2(x)3π4sin(3ωx)+...+yk(x)(2k−1)π4sin((2k−1)ωx)+... 𝑦 1 ( 𝑥 ) , 𝑦 2 ( 𝑥 ) , 𝑦 3 ( 𝑥 ) , . . . , 𝑦 𝑘 ( 𝑥 ) , . . . y1(x),y2(x),y3(x),...,yk(x),... are all the individual sinusoids. If we perform the sum, we will create a square wave. The more sinusoids we pick, the better the approximation. Please choose how many sinusoids you want to use, and let’s see how the functions look like for k=1 k=2 k=3 k=4 k=7 k=9 k=15 k=20 (and 𝜔 ω= 1 2 3 ): Epicycles - First EncounterPermalink One sinusoid has a corresponding circle that spins. So, the above animation can be re-imagined like this: Pick k=1 k=2 k=3 k=4 k=7 k=9 k=15 k=20 and 𝜔 ω= 1 2 3 to plot the circles and the corresponding 𝑦 𝑘 ( 𝑥 ) yk(x) functions: Epicycles - An intuitive understandingPermalink There’s an intuitive proof to this: each epicycle corresponds to a specific sinusoid; when we talk about combining the sinusoids, we are talking about summing their positions at each point in time, and eventually, this operation reduces to subsequent vector additions. Let’s take a simple example by introducing three arbitrarily picked sinusoids and their associated point vectors (or position vectors): 𝑦 1 ( 𝑥 ) = 1.4 𝑠 𝑖 𝑛 ( 𝑥 + 1 ) y1(x)=1.4sin(x+1), with the associated position vector 𝑢 1 ⃗ u1; 𝑦 2 ( 𝑥 ) = 0.8 𝑠 𝑖 𝑛 ( 2 ∗ 𝑥 ) y2(x)=0.8sin(2∗x), with the associated position vector 𝑢 2 ⃗ u2; 𝑦 3 ( 𝑥 ) = 0.5 𝑠 𝑖 𝑛 ( 3 ∗ 𝑥 ) y3(x)=0.5sin(3∗x), with the associated position vector 𝑢 3 ⃗ u3. Their sum is 𝑦 ( 𝑥 ) = 𝑦 1 ( 𝑥 ) + 𝑦 2 ( 𝑥 ) + 𝑦 3 ( 𝑥 ) = 1.4 𝑠 𝑖 𝑛 ( 𝑥 + 1 ) + = 0.8 𝑠 𝑖 𝑛 ( 2 ∗ 𝑥 ) + 0.5 𝑠 𝑖 𝑛 ( 3 ∗ 𝑥 ) y(x)=y1(x)+y2(x)+y3(x)=1.4sin(x+1)+=0.8sin(2∗x)+0.5sin(3∗x). A position vector represents the displacement from the origin ( 0 , 0 ) (0,0) to a specific point in space. In our case, the vector ( 𝑥 , 𝑦 𝑘 ( 𝑥 ) ) (x,yk(x)) represents the position or location of a point on the graph of the function(s) 𝑦 𝑘 ( 𝑥 ) yk(x) at a particular 𝑥 x value. If we plot 𝑦 ( 𝑥 ) y(x) on the cartesian grid we obtain something like: At each given point 𝑥 x, we can say for certain that 𝑢 ⃗ = 𝑢 1 ⃗ + 𝑢 2 ⃗ + 𝑢 3 ⃗ u=u1+u2+u3. Epicycles - A flowerPermalink If we carefully pick the right sinusoids the moving circles can describe (approximate) any shape we want. Here is a flower for example: Can you guess the individual sinusoids working together to draw the flower? You probably can’t unless you apply methods from a branch of mathematics called Fourier Analysis. Fourier SeriesPermalink Fourier Series is the mathematical process through which we expand a periodic function of period 𝑃 P as a sum of trigonometric functions. Do you remember the Pink Floyd’s album cover for the Dark Side of The Moon: Imagine our function 𝑓 ( 𝑥 ) f(x) is the light itself, the prism is essentially Fourier Mathematics, and the spectral colors emanating from the prism are the sinusoids. Following the analogy the formula looks like this: 𝑓 ( 𝑥 ) ⏟ light itself = 𝐴 0 + ∑ 𝑛 = 1 ∞ [ 𝐴 𝑛 𝑐 𝑜 𝑠 ( 2 𝜋 𝑛 𝑥 𝑃 ) + 𝐵 𝑛 𝑠 𝑖 𝑛 ( 2 𝜋 𝑛 𝑥 𝑃 ) ] ⏟ the spectral components light itselff(x)=the spectral componentsA0+n=1∑∞[Ancos(P2πnx)+Bnsin(P2πnx)] Where 𝐴 𝑛 An and 𝐵 𝑛 Bn are called Fourier Coefficients are defined by the following integrals: 𝐴 0 = 1 𝑃 ∫ − 𝑃 2 + 𝑃 2 𝑓 ( 𝑥 ) 𝑑 𝑥 A0=P1∫−2P+2Pf(x)dx 𝐴 𝑛 = 2 𝑃 ∫ − 𝑃 2 + 𝑃 2 𝑓 ( 𝑥 ) ∗ cos ⁡ ( 2 𝜋 𝑛 𝑥 𝑃 ) 𝑑 𝑥 An=P2∫−2P+2Pf(x)∗cos(P2πnx)dx 𝐵 𝑛 = 2 𝑃 ∫ − 𝑃 2 + 𝑃 2 𝑓 ( 𝑥 ) ∗ sin ⁡ ( 2 𝜋 𝑛 𝑥 𝑃 ) 𝑑 𝑥 Bn=P2∫−2P+2Pf(x)∗sin(P2πnx)dx Fourier Series in Exponential FormPermalink With the help of Euler’s Formula and by changing the sine and cosine functions in their exponential forms, we can also express the Fourier Series of a function as a sum of Complex Sinusoids: 𝑓 ( 𝑥 ) = ∑ 𝑛 = − 𝑁 𝑁 𝐶 𝑛 𝑒 𝑖 2 𝜋 𝑛 𝑃 𝑥 f(x)=n=−N∑NCnei2πPnx Where: 𝐶 𝑛 = { 𝐴 0 if 𝑛 = 0 1 2 ( 𝐴 𝑛 − 𝑖 ∗ 𝐵 𝑛 ) if 𝑛 > 0 1 2 ( 𝐴 𝑛 + 𝑖 ∗ 𝐵 𝑛 ) if 𝑛 0if n<0 If we do additional substitutions, the final form of 𝐶 𝑛 Cn is: 𝐶 𝑛 = 1 𝑃 ∫ − 𝑃 2 𝑃 2 𝑒 − 𝑖 2 𝜋 𝑛 𝑃 𝑥 𝑓 ( 𝑥 ) 𝑑 𝑥 Cn=P1∫−2P2Pe−i2πPnxf(x)dx Example: The Fourier Series for the Box FunctionPermalink Remember the Square Wave we’ve approximated with sinusoids in this section? At that point, we used the following formula to express the Square as a sum of sinusoidal components: 𝑦 ( 𝑥 ) = 4 𝜋 ∑ 𝑘 = 1 ∞ sin ⁡ ( 2 𝜋 ( 2 𝑘 − 1 ) 𝑓 𝑥 ) 2 𝑘 − 1 y(x)=π4k=1∑∞2k−1sin(2π(2k−1)fx) Or, to keep things simpler, by substituting 𝜔 = 2 𝜋 𝑓 ω=2πf ( 𝜔 ω is the angular frequency): 𝑦 ( 𝑥 ) = 4 𝜋 ∑ 𝑘 = 1 ∞ sin ⁡ ( ( 2 𝑘 − 1 ) 𝜔 𝑥 ) 2 𝑘 − 1 y(x)=π4k=1∑∞2k−1sin((2k−1)ωx) It’s time to understand how we’ve devised such an approximation. In isolation, the Square Wave, 𝑓 ( 𝑥 ) f(x) looks like this: (Source code) Throughout the interval [0, 2L], 𝑓 ( 𝑥 ) f(x) can be written as: 𝑓 ( 𝑥 ) = 2 [ 𝐻 ( 𝑥 𝐿 ) − 𝐻 ( 𝑥 𝐿 − 1 ) ] − 1 f(x)=2[H(Lx)−H(Lx−1)]−1 Where 𝐻 ( 𝑥 ) H(x) is called the Heavisde Step Function and has the following definition: 𝐻 ( 𝑥 ) = { 0 if 𝑥 < 0 1 if 𝑥 ≥ 0 H(x)={01if x<0if x≥0 First of all, let’s look at 𝐴 0 = 1 2 𝐿 ∫ 0 2 𝐿 𝑓 ( 𝑥 ) 𝑑 𝑥 A0=2L1∫02Lf(x)dx. Notice how we’ve changed the interval from [ − 𝑃 2 , 𝑃 2 ] [−2P,2P] to [ 0 , 2 𝐿 ] [0,2L] to match our example. This will be reflected in the formulas. Well, this coefficient ( 𝐴 0 A0) is a fancy way to express the average of 𝑓 ( 𝑥 ) f(x) over the interval (in our case [0, 2L]). In the same time 𝐴 0 A0 is the area determined by 𝑓 ( 𝑥 ) f(x) over [0, 2L] then divided by 2 𝐿 2L. But if you look at the plot again, you will see that the net area is 0 0, because the green area (S1) nullifies the red area (S2), regardless of 𝐿 L. (Source code) Secondly, let’s compute the 𝐴 𝑛 = 1 𝐿 ∫ 0 2 𝐿 𝑓 ( 𝑥 ) ∗ cos ⁡ ( 𝜋 𝑛 𝑥 𝐿 ) 𝑑 𝑥 An=L1∫02Lf(x)∗cos(Lπnx)dx coefficients. An important observation is that 𝑓 ( 𝑛 ) f(n) is odd, and its average value on the interval is 0 0; we can safely say all the coefficients 𝐴 𝑛 An also vanish. Visually speaking, regardless of how you pick 𝑛 n or 𝐿 L, the net area determined by the 𝐴 𝑛 An integral will always be zero. It’s visually obvious if we plot 𝐴 𝑛 An. For example plotting 𝐴 1 A1, 𝐴 2 A2, 𝐴 3 A3, 𝐴 4 A4 looks like this: (Source code) Similar symmetrical patterns will emerge if you increase the 𝑛 n in 𝐴 𝑛 An and plot them. Thirdly, we need to compute: 𝐵 𝑛 = 1 𝐿 ∫ 0 2 𝐿 𝑓 ( 𝑥 ) ∗ sin ⁡ ( 𝜋 𝑛 𝑥 𝐿 ) 𝑑 𝑥 Bn=L1∫02Lf(x)∗sin(Lπnx)dx If we plot a 𝐵 1 B1, 𝐵 2 B2, 𝐵 3 B3 and, let’s say, 𝐵 4 B4 we can intuitively feel what’s happening with 𝐵 𝑛 Bn: (Source code) If you have a keen eye for geometrical representations, you will notice that every even 𝐵 𝑛 Bn is also 0. The red and green areas nullify, so the net area described by the integral is 0 0. The odd terms will be 2 ∗ something 2∗something, so let’s calculate that something something. We will need to split the integral on two sub-intervals [ 0 , 𝐿 ] [0,L] and [ 𝐿 , 2 𝐿 ] [L,2L] (there’s a chasm at 𝐿 L), but given the fact 𝑓 ( 𝑥 ) f(x) and 𝑠 𝑖 𝑛 ( 𝑥 ) sin(x) are odd, 𝐵 𝑛 Bn can we written as: 𝐵 𝑛 = 2 ∗ [ 1 𝐿 ∫ 0 𝐿 𝑓 ( 𝑥 ) ∗ sin ⁡ ( 𝜋 𝑛 𝑥 𝐿 ) 𝑑 𝑥 ] Bn=2∗[L1∫0Lf(x)∗sin(Lπnx)dx] We can now perform u-substition, so we can write: 𝐵 𝑛 = 2 𝐿 ∫ 0 𝑛 𝐿 𝜋 sin ⁡ ( 𝑢 𝐿 ) 𝑛 𝜋 𝑑 𝑢 Bn=L2∫0nLπnπsin(Lu)du After we take the constant out, we compute the integral, use the intervals, and take into consideration the periodicity of cosine: 𝐵 𝑛 = 2 𝑛 𝜋 ( 1 − ( − 1 ) 𝑛 ) Bn=nπ2(1−(−1)n) And now we see it, 𝐵 𝑛 Bn is exactly 0 0 if 𝑛 n is even, and 𝐵 𝑛 = 2 ∗ 2 𝑛 𝜋 Bn=2∗nπ2 is 𝑛 n is odd. Putting all back into the master formula of the Fourier Series: 𝑓 ( 𝑥 ) = 𝐴 0 ⏟ 0 + ∑ 𝑛 = 1 ∞ [ 𝐴 𝑛 cos ⁡ ( 𝜋 𝑛 𝑥 𝐿 ) ⏟ 0 + 𝐵 𝑛 sin ⁡ ( 𝜋 𝑛 𝑥 𝐿 ) ] f(x)=0A0+n=1∑∞[0Ancos(Lπnx)+Bnsin(Lπnx)] Things become: 𝑓 ( 𝑥 ) = 4 𝜋 ∑ 𝑛 = 1 , 3 , 5... + ∞ ( 1 𝑛 ∗ sin ⁡ ( 𝜋 𝑛 𝑥 𝐿 ) ) f(x)=π4n=1,3,5...∑+∞(n1∗sin(Lπnx)) If we substitute 𝑛 → 2 𝑘 − 1 n→2k−1 and consider, we obtain the initial formula: 𝑓 ( 𝑥 ) = 4 𝜋 ∑ 𝑘 = 1 + ∞ ( sin ⁡ ( 𝜋 ( 2 𝑘 − 1 ) 𝑥 𝐿 ) ( 2 𝑘 − 1 ) ) f(x)=π4k=1∑+∞((2k−1)sin(Lπ(2k−1)x)) To obtain the initial formula, we substitute 𝐿 → 1 2 𝑓 L→2f1, and 2 𝜋 𝑓 → 𝜔 2πf→ω, basically we create an interdependence between 𝐿 L (half of the interval) and 𝜔 ω, 𝐿 = 𝜋 𝜔 L=ωπ: 𝑓 ( 𝑥 ) = 4 𝜋 ∑ 𝑘 = 1 ∞ sin ⁡ ( ( 2 𝑘 − 1 ) 𝜔 𝑥 ) 2 𝑘 − 1 f(x)=π4k=1∑∞2k−1sin((2k−1)ωx) Unfortunately, there’s no way we can go to + ∞ +∞, so let’s consider 𝑠 ( 𝑥 ) s(x) as an approximation of 𝑓 ( 𝑥 ) f(x) that depends on 𝑛 n. 𝑠 ( 𝑥 ) = 4 𝜋 ∑ 𝑘 = 1 𝑛 sin ⁡ ( ( 2 𝑘 − 1 ) 𝜔 𝑥 ) 2 𝑘 − 1 ≈ 𝑓 ( 𝑥 ) s(x)=π4k=1∑n2k−1sin((2k−1)ωx)≈f(x) In the next animation, you will see that by increasing 𝑛 n, the accuracy of our approximation gets better and better, and the gaps are slowly closed: (Source code) To understand how adding more coefficients improves the approximation, let’s look back again at a few of our coefficients 𝑠 1 ( 𝑥 ) s1(x), 𝑠 2 ( 𝑥 ) s2(x), 𝑠 3 ( 𝑥 ) s3(x), 𝑠 4 ( 𝑥 ) s4(x) and 𝑠 5 ( 𝑥 ) s5(x) (we will pick 𝜔 = 𝜋 2 ω=2π, so that 2 𝐿 = 1 2L=1): 𝑠 1 ( 𝑥 ) = 4 𝜋 sin ⁡ ( 𝜋 𝑥 2 ) s1(x)=π4sin(2πx) 𝑠 2 ( 𝑥 ) = 4 3 𝜋 sin ⁡ ( 3 𝜋 𝑥 2 ) s2(x)=3π4sin(23πx) 𝑠 3 ( 𝑥 ) = 4 5 𝜋 sin ⁡ ( 5 𝜋 𝑥 2 ) s3(x)=5π4sin(25πx) 𝑠 4 ( 𝑥 ) = 4 7 𝜋 sin ⁡ ( 7 𝜋 𝑥 2 ) s4(x)=7π4sin(27πx) 𝑠 5 ( 𝑥 ) = 4 9 𝜋 sin ⁡ ( 9 𝜋 𝑥 2 ) s5(x)=9π4sin(29πx) Each of the 5 terms is a sinusoid, with 4 𝜋 π4, 4 3 𝜋 3π4, etc. amplitudes, and 𝜋 2 2π, 3 𝜋 2 23π, etc. frequencies. So, if we were to approximate a Square Wave with its fifth partial sum (the red dot), we would obtain something like this: (Source code) Notice how obsessed the red dot is with the blue dot (the actual function) and how closely it follows it. We can always add more terms to the partial sum to help the red dot in its holy mission, improving the approximation until nobody cares anymore. Example - The Fourier Series of the Triangle wavePermalink Without dealing with all the associated math, the Fourier Series decomposition for a triangle-wave is: 𝑠 ( 𝑥 ) = 8 𝜋 2 ∑ 𝑘 = 1 𝑁 ( − 1 ) 𝑘 − 1 ( 2 𝑘 − 1 ) 2 ∗ sin ⁡ ( ( 2 𝑘 − 1 ) 𝑥 ) s(x)=π28k=1∑N(2k−1)2(−1)k−1∗sin((2k−1)x) Plotting the function 𝑠 ( 𝑥 ) s(x), we will see that things converge smoothly and fast. As soon as 𝑛 n approaches 6 6, we can already observe the triangle: (Source code) Let’s compute the first terms six terms of the ∑ ∑, so that: 𝑠 ( 𝑥 ) ≈ 𝑠 1 ( 𝑥 ) + 𝑠 2 ( 𝑥 ) + 𝑠 3 ( 𝑥 ) + 𝑠 4 ( 𝑥 ) + 𝑠 5 ( 𝑥 ) + 𝑠 6 ( 𝑥 ) s(x)≈s1(x)+s2(x)+s3(x)+s4(x)+s5(x)+s6(x) Where: The first term is 𝑠 1 ( 𝑥 ) = 8 𝜋 2 ∗ sin ⁡ ( 𝑥 ) s1(x)=π28∗sin(x), where 𝐴 = 8 𝜋 2 A=π28, 𝜔 = 1 ω=1, 𝜑 = 0 φ=0; The second term is 𝑠 2 ( 𝑥 ) = − 8 3 2 𝜋 2 ∗ sin ⁡ ( 3 𝑥 ) s2(x)=−32π28∗sin(3x), where 𝐴 = − 8 3 2 𝜋 2 A=−32π28, 𝜔 = 3 ω=3, 𝜑 = 0 φ=0; The third term is 𝑠 3 ( 𝑥 ) = 8 5 2 𝜋 2 ∗ sin ⁡ ( 5 𝑥 ) s3(x)=52π28∗sin(5x), where 𝐴 = 8 5 2 𝜋 2 A=52π28, 𝜔 = 5 ω=5, 𝜑 = 0 φ=0; The fourth term is 𝑠 4 ( 𝑥 ) = − 8 7 2 𝜋 2 ∗ sin ⁡ ( 7 𝑥 ) s4(x)=−72π28∗sin(7x), where 𝐴 = − 8 7 2 𝜋 2 A=−72π28, 𝜔 = 7 ω=7, 𝜑 = 0 φ=0; The fifth term is 𝑠 5 ( 𝑥 ) = 8 9 2 𝜋 2 ∗ sin ⁡ ( 9 𝑥 ) s5(x)=92π28∗sin(9x), where 𝐴 = 8 9 2 𝜋 2 A=92π28, 𝜔 = 9 ω=9, 𝜑 = 0 φ=0; The sixth term is 𝑠 6 ( 𝑥 ) = − 8 1 1 2 𝜋 2 ∗ sin ⁡ ( 11 𝑥 ) s6(x)=−112π28∗sin(11x), where 𝐴 = − 8 1 1 2 𝜋 2 A=−112π28, 𝜔 = 11 ω=11, 𝜑 = 0 φ=0; A keen eye will see will observe the that 𝑠 2 ( 𝑥 ) s2(x), 𝑠 4 ( 𝑥 ) s4(x), 𝑠 6 ( 𝑥 ) s6(x) and all the even terms are negative. A negative amplitude doesn’t make too much sense, at least not in a physical sense. What are we going to do with the minus sign? We have two options: Because sin ⁡ ( − 𝑥 ) = − sin ⁡ ( 𝑥 ) sin(−x)=−sin(x), nobody stops us to make the frequency negative. For example, 𝑠 2 ( 𝑥 ) = 8 3 2 𝜋 2 ∗ sin ⁡ ( − 3 𝑥 ) s2(x)=32π28∗sin(−3x), so that the 𝜔 = − 3 ω=−3. But again, why would we want a negative frequency? This also doesn’t make sense in a physical sense. We can use ∣ 𝐴 ∣ ∣A∣ and shift the signal with 𝜋 π, so that 𝜑 = 𝜋 φ=π. In practice, we will go with 2. as it’s more practical and gives us more control, but the two options are equivalent so that we can write 𝑠 2 ( 𝑥 ) s2(x) in both ways: 𝑠 2 ( 𝑥 ) = − 8 3 2 𝜋 2 ∗ sin ⁡ ( 3 𝑥 ) s2(x)=−32π28∗sin(3x) 𝑠 2 ( 𝑥 ) = 8 3 2 𝜋 2 ∗ sin ⁡ ( 3 𝑥 + 𝜋 ) s2(x)=32π28∗sin(3x+π) Visually speaking, the results will not be surprising if we plot 𝑠 𝑖 𝑛 ( − 𝑥 ) sin(−x) and 𝑠 𝑖 𝑛 ( 𝑥 + 𝜋 ) sin(x+π) side by side; the two are equivalent: (Source code) If we consider this, the even terms of 𝑠 ( 𝑥 ) s(x) will become: 𝑠 2 ( 𝑥 ) = 8 3 2 𝜋 2 ∗ 𝑠 𝑖 𝑛 ( 3 𝑥 + 𝜋 ) s2(x)=32π28∗sin(3x+π) ; 𝑠 4 ( 𝑥 ) = 8 7 2 𝜋 2 ∗ 𝑠 𝑖 𝑛 ( 7 𝑥 + 𝜋 ) s4(x)=72π28∗sin(7x+π) ; 𝑠 6 ( 𝑥 ) = 8 1 1 2 𝜋 2 ∗ 𝑠 𝑖 𝑛 ( 11 𝑥 + 𝜋 ) s6(x)=112π28∗sin(11x+π) ; …and so on Example - The Fourier Series of a Sawtooth FunctionPermalink Shamelessly skipping the math demonstration, a reverse-sawtooth function has the following form: 𝑠 ( 𝑥 ) = 2 𝜋 ∑ 𝑘 = 1 𝑁 ( − 1 ) 𝑘 ∗ sin ⁡ ( 𝑘 𝑥 ) 𝑘 s(x)=π2k=1∑N(−1)k∗ksin(kx) Plotting 𝑠 ( 𝑥 ) s(x), while increasing 𝑛 n, things look like this: (Source code) The Fourier Series MachineryPermalink To better visualize what’s happening, let’s look at a Fourier Series Machinery and how the circles move to create beautiful practical patterns. You can pick the shape of the desired signal from here: sawtooth wave triangle wave square wave and the sketch will change accordingly. (Source code) …A bunch of spinning circles on a stick, where each circle corresponds to exactly one term of the series - this is the marvelous Fourier Series Machinery. If we run our signal through the Fourier Series Machinery, we will obtain The Amplitude ( 𝐴 A) and The Phase ( 𝜑 φ) for each Frequency ( 𝜔 ω) from 1 1 to 𝑁 N. Isn’t it amazing? And it all comes down to a bunch of spinning circles… on a stick. to be continued Updated: April 24, 2024 Twitter Facebook LinkedIn Comments",
    "commentLink": "https://news.ycombinator.com/item?id=40578705",
    "commentBody": "An animated introduction to Fourier series (andreinc.net)541 points by gaws 22 hours agohidepastfavorite76 comments thomasahle 18 hours agoI struggled for a long time to understand the Fourier transform, using visual materials like 3b1b [1] and betterexplained [2]. Eventually it clicked when considering the Discrete Fourier transform [3], which is just an orthogonal matrix you multiply onto a vector. All the stuff about the inverse FFT, Plancherel theorem and Parseval's theorem come for free: they just say that the matrix is orthonormal. Maybe this only works if you've already invested in understanding linear algebra. But once I had this discrete understanding, back porting it to the continuous Fourier transform was easy enough. Maybe I'm weird, but this was a case where just looking at the equations was much easier than all the animations of circles and stuff. [1] https://www.youtube.com/watch?v=spUNpyF58BY [2] https://betterexplained.com/articles/an-interactive-guide-to... [3] https://en.wikipedia.org/wiki/Discrete_Fourier_transform reply ajkjk 17 hours agoparentI felt like it made total sense when I understood the idea of a basis, plus \"sinusoids are a basis for (a class of functions)\" Everything else basically follows from that. The Fourier transform itself integrates (in one notation) e^{-ikx} against f(x). Well, the integral is a giant dot product, e^{-ikx} is the \"transpose\" of e^{ikx}, one of the basis vectors, so this amounts to saying f_i =for a basis element e. reply thomasahle 13 hours agorootparentRight, they are very simple and elegant in algebra. Visualizing them with nested circles flying around and drawing pictures, definitely makes them seem more weird. reply atoav 12 hours agorootparentThe circles never really bothered me, in the end one circle just gives you the value/time of a partial sine wave. Instead of nesting them you could just put one partial into each row and add them up down below. reply duped 3 hours agoparentprevI was about to comment (as a joke) \"it's just a change of basis, what's so hard to understand\" It's the signals & systems version of \"monoid in the category of endofunctors\" reply vikramkr 17 hours agoparentprevThat's a really interesting case, and my gut feeling is that you are definitely very weird lol, not that that's a bad thing. How do you end up understanding linear algebra but turning to youtube videos to understand the fourier transform? My gut feeling is that 99% of people who understand linear algebra would learn fourier transforms through the same mechanism they used to learn advanced math (textbooks, university courses, etc) where the default way of introducing the subject would be something like 'just an orthogonal matrix you multiply onto a vector,' an otherwise unintuitive explanation that leaves a void that said youtube videos are trying to fill. For example the 3b1b video has to gently re-introduce why complex numbers are useful - different target audience you know? reply Sharlin 17 hours agorootparentPeople who are good at algebra aren’t necessarily good at calculus/analysis and vice versa. It sounds perfectly reasonable that someone might struggle with the continuous FT (integrals, brrh!) but grok the discrete version perfectly well (just a sum of basis functions). reply code_biologist 17 hours agorootparentHeh, my college signals courses first taught continuous time systems then focused on discrete time in a follow-up course. They assumed you knew enough calculus at the start that Laplace transforms wouldn't be a huge hurdle, even if you hadn't seen them before. Discrete time / Z-transform was treated as more \"advanced\". But I agree, I've done enough integrals for a lifetime! reply Sharlin 16 hours agorootparentComputer scientists and programmers live in an inherently discrete world where there’s algebra everywhere you look but very little calculus outside certain niches. I’m reminded of the Feynman and the Connection Machine story [1] where he ended up analyzing some complex binary circuits in terms of differential equations because as a physicist he lived and breathed the continuum – unlike his computer engineer coworkers! [1] https://longnow.org/essays/richard-feynman-connection-machin... reply thomasahle 13 hours agorootparentprev> How do you end up understanding linear algebra but turning to youtube videos to understand the fourier transform? As a computer scientist I never actually had a calculus class where we used fourier series. I just ran into them often enough, through stuff like the fast fourier transform for computing convolutions, that I thought I should understand it better. So I googled the topic, and stuff like 3b1b is what came up, and what everyone said were the most intuitive explanations. I eventually did a course on binary functional analysis, and it thought the discrete boolean fourier transform. reply masto 5 hours agoparentprevTo your point, I happened across this video recently and it was a nice break from animated circles-within-circles: https://www.youtube.com/watch?v=QmgJmh2I3Fw reply 3abiton 11 hours agoparentprevIt's sad to see that the educational system stayed generalized, and never customized to each person's mental capabilities, I always find neat articles discussing different topics from a totally different perspective. If only these existed back then when I was a studen. reply nomemory 9 hours agoparentprevYou are not weird; you are just differently wired. I find beauty in the fact that a bunch of circles are spinning on a stick (axis) with increasing frequencies, and if sum up their \"tracks\", you end up approximating shapes. reply behnamoh 16 hours agoparentprevMy experience has been the same: all these intuitive and visual explanations of math/physics stuff on YouTube/else just makes me \"feel\" like I learnt it, but then as soon as I need to use it in practice, I realize the only thing that actually gets the job done is some solid math equations, no questions asked. I still do all the proofs for math subjects in order to know I can derive them. It's unfortunate, and I wish just watching such beautiful visuals would magically instill the idea in my brain, but it just feels like intellectual dopamine for me. reply jonahx 13 hours agorootparent> I realize the only thing that actually gets the job done is some solid math equations, no questions asked. I don't think it's that as much as... > I still do all the proofs for math subjects in order to know I can derive them. You have to do something, apply the knowledge. That's the piece that learning solely from nice visuals misses. It just so happens that actually solving problems often involves using equations. But I don't think that the essential ingredient. reply richrichie 14 hours agoparentprevI agree. I much prefer the clean math language as well. I guess some people can process abstraction better than others. Fourier series are just a topic in approximation theory, which is a rich area. reply keithalewis 17 hours agoparentprevThis is how people who lived before you did it. It's math. You can just read what they had to say instead of pretending a YouTube video or comic is actual hard won knowledge. Nobody has anything more to say about Fourier series than what Walter Rudin figured out long ago. They can be defined for any locally compact abelian group. They are just trying to teach themselves about what is established theory. reply sfpotter 15 hours agorootparentUhh... there is a lot more to say about Fourier series than \"just that they're defined for any locally compact abelian group\". Give me a break. reply keithalewis 8 hours agorootparentDo you know understand how quoting works? You are supposed to use the words the person actually typed. Feel free to cite a textbook on Fourier analysis proving a result not contained in Rudin's text. Uhh.. here's your break. Put up, or shut up. reply woopsn 1 hour agorootparenthttps://link.springer.com/journal/41/articles reply eigenket 4 hours agorootparentprevAs far as I know Rudin focuses on Fourier analysis on locally compact Abelian groups. There's been plenty of attention paid to doing Fourier analysis on other objects, especially compact Lie groups or symmetric spaces. Of course there are people who would say that this isn't Fourier analysis anymore but the same ideas are still at play. reply laszlokorte 8 hours agoprevGreat article! I like that it starts with circles and complex numbers ans builds up from there. Some other explanations try to only throw in complex numbers in the end. If you are interested in this topic you might also like my visualization of a fractional fourier transform [1] and the complex plane [2] [1]: https://static.laszlokorte.de/frft-cube/ [2]: https://static.laszlokorte.de/complex-plane/ reply factormeta 21 hours agoprevThe link to source code is broken. Looks like someone forgot to put a '/' in the href link. The actual link is: https://www.andreinc.net/2024/04/24/assets/js/2024-04-24-fro... Looks like it is using processing to do the animation. reply nomemory 10 hours agoparentThanks for noticing, the source code for all the animations can be found here: https://github.com/nomemory/andreinc-site/tree/main/assets/j... It's not something I am particularly proud of, as I was learning p5.js while writing the article, so I've started doing things in a very inefficient way. When I have some spare time, I will update the article accordingly to include the correct links. reply xrd 1 hour agorootparentI'm really fascinated by the discussions here. A few days ago I posted a blog that uses animations to talk about embeddings and cosine distance. https://webiphany.com/2024-04-29-distance-sean-shawn I used Svelte and basically pure HTML. This post has the entirety of the post as a download if you scroll to the bottom and click the view source link. You can download a Svekyll blog that can be compiled with \"npm run build\" so you can hack the code yourself. I think Svelte is an incredible tool for building these kinds of animations. I would be very curious to see how/if you would compare processing and Svelte, Andre? reply rjurney 19 hours agoparentprevGod, I love Processing. Most beautiful programs I’ve ever written were in Processing. I had this plan to recover from atypical depression by visualizing all of basic linear algebra using Processing but… I couldn’t program a computer for some time. Doh! reply teleforce 17 hours agoparentprevLooks like that link is broken as well. reply bosuanzi 20 hours agoparentprevI think that we need't use processing for animation. In fact, Geogebra is easy to accomplish them. reply rjurney 19 hours agorootparentYou should try it sometime. You might fall in love with it. reply emmanueloga_ 19 hours agoprevVery nice! I feel like I have a decent understanding of the Fourier transform, and it receives much love out there in terms of coverage, cool visualizations, etc. For the people out there who enjoys working in explainers ... maybe consider covering the Laplace transform too? [1] I used to know how to use it to analyze electronic circuits, but I kinda forgot all about it by now (oops :-p) so this is my lazy ask :-). -- 1: https://en.wikipedia.org/wiki/Laplace_transform reply ppcdeveloper 3 hours agoprevI wish I had something like this when I was studying Mathematics. This is so well done. reply dowakin 11 hours agoprevI only got FFT after following the course `Introduction to Graduate Algorithms`( Georgia Tech) and implementing everything from the lectures in python. It is really good course: https://edstem.org/us/courses/47529/lessons/80063/slides/440... reply wrasee 5 hours agoparentYour link just asks me for a login, which of course I don’t have. I’ve not heard of Ed before. I am interested but the “see how it works” section doesn’t really tell you how it works. As is the trend, it seems you have to sign up first. Shame. reply techie128 5 hours agorootparentThe Ed link is referenced from GT's official class page: https://omscs.gatech.edu/cs-6515-intro-graduate-algorithms reply barrenko 11 hours agoparentprevIs there an 'undergraduate' version of the course? reply szvsw 21 hours agoprevSeeing an epicycles animation years ago did wonders for my understanding of the complex representation of Fourier series, and I’ve been looking for that animation ever since to share with others getting into signals for the first time. This post far surpasses that page though! Excellent work, excited to share it with people in the future. reply Koshkin 20 hours agoparentThere's quite a few of those out there, e.g. https://youtu.be/k8FXF1KjzY0. reply chuckadams 20 hours agorootparentMy favorite is https://youtu.be/n-43Uje-OuQ reply westurner 19 hours agorootparent\"The more general uncertainty principle, regarding Fourier transforms\" by 3blue1brown https://youtube.com/watch?v=MBnnXbOM5S4 From https://news.ycombinator.com/item?id=25190770#25194040 : > Convolution is in fact multiplication in Fourier space (this is the convolution theorem [1]) which says that Fourier transforms convert convolutions to products. 1. https://en.wikipedia.org/wiki/Convolution_theorem : >> In mathematics, the convolution theorem states that under suitable conditions the Fourier transform of a convolution of two functions (or signals) is the pointwise product of their Fourier transforms. More generally, convolution in one domain (e.g., time domain) equals point-wise multiplication in the other domain (e.g., frequency domain). Other versions of the convolution theorem are applicable to various Fourier-related transforms. \"QFT and iQFT; Inverted Quantum Fourier Transform\" https://en.wikipedia.org/wiki/Quantum_Fourier_transform reply Workaccount2 5 hours agoprevI'm pretty sure that there is a mistake in the unit circle animation under \"The sine and the cosine\". It looks like the author mixed up sin and cos You can right click the image and \"Open image in a new tab\" for a freeze frame, then checking the values it seems they are switched. reply wrycoder 52 minutes agoparentAnd once you swap the labels on the lower left, it appears that the sign of the sin is reversed. reply badrunaway 3 hours agoprevI wish to build such tutorials for things I know about. What tool is simple for such animations? Sorry if this is not a useful comment. reply seabass 2 hours agoparentThere's manim [1] from YouTuber 3blue1brown. And while it doesn't work on the web, there is a port [2] that does, which runs on p5.js. If you want a bit more control, you can use a canvas animation library directly. A few popular ones are p5, GreenSock, and PixiJS--but there are so many others. For simpler animations and for the most control over the result, it's pretty rewarding to just use the built-in Canvas API [3] directly. [1] https://github.com/3b1b/manim [2] https://github.com/JazonJiao/Manim.js/ [3] https://developer.mozilla.org/en-US/docs/Web/API/Canvas_API/... reply nomemory 3 hours agoparentprevI've used processing (p5js) but I don't necessarily recommend the library if you are looking for something simple. reply xrd 1 hour agorootparentI added a comment above the plugs my Svekyll blog tool. I love Svelte for these kind of things. You can view source of the post I made: https://webiphany.com/2024-04-29-distance-sean-shawn reply oriettaxx 18 hours agoprevthis is super great from him, too https://www.andreinc.net/2024/01/09/the-most-important-math-... reply kennyloginz 20 hours agoprevThanks, great examples and wonderful website. It’s crazy how this site is handled with ease, yet most static news sites I visit constantly crash my browser. reply Tronno 16 hours agoparentThis site constantly refreshes my browser (mobile) every minute or so. It made it hard to read. reply nomemory 9 hours agorootparentThis is a personal blog, and I haven't 'tested' the article on too many devices. I don't have the time or the capacity to do so. A thing I've noticed is that on some phones is very choppy (the code behind the animation is not the greatest), but I don't understand why it would refresh your browser. What phone / browser are you using, out of curiosity ? reply Tronno 30 minutes agorootparentMobile Safari on iOS 17.5.1 - I noticed it happens when zooming in and out (to better see the graphs). Good luck, I hope you are able to find a solution. reply kennyloginz 14 hours agorootparentprevIt wasn’t perfect, but miles better than 99% of sites currently. ( I have been reading a couple wired articles recently, any may be biased ) reply teleforce 16 hours agoprevGreat tutorial on understanding Fourier series and a tetris game based on Fourier series to boot. Here's another helpful website dedicated to Fourier series and Fourier Transform with videos and books: https://howthefouriertransformworks.com/ reply svag 11 hours agoprevSome explanation about Fourier transform can be found in the Feynman lectures here, https://www.feynmanlectures.caltech.edu/I_50.html reply oriettaxx 18 hours agoprevSuper! I really wish I could have these animation in high school :) (too bad my CPUs goes sky rocket, so every time we open that page we contribute to heating up the planet :) reply nomemory 10 hours agoparentIt happens to me when I am using Chromium under Linux, it seems that my Hardware acceleration is not working. While on MAC, Windows or Firefox (Linux) it works flawlessly. On mobile is choppy as well (unless you have a flagship). Unfortunately I didn't know how to optimise the animations better, and once I found out, 90% of the code was already written. reply zuluonezero 20 hours agoprevThis is an excellent review with animations that make the math visually intuitive. I love how it grows from the simple to the ridiculously complex at a reasonable pace. reply sterasody 14 hours agoprevhttps://jackschaedler.github.io/circles-sines-signals/ There is also this wonderful primer on signal processing for people who love visualizations. reply monkeytree 20 hours agoprevSinusoidal Tetris is hidden in there - very cool! I'm not good at it, but it seems like a fun way to help visualize how sinusoids combine. reply deanresin 12 hours agoprevThis would be a great companion tutorial to a text book. I loved the animations and the interactive animations. It could use some proofreading, however. reply sdkfile 16 hours agoprevFrom the section \"Euler's Identity\", I believe that you made a typo when substituting a -> e, you should have written a -> i. Thanks for great post though! reply hackd997865 7 hours agoprevThis is awesome! Thank you so much for sharing reply gqcwwjtg 19 hours agoprevVery cool. To nitpick: the “even-ness of the cosine” graph should probably draw lines to the Y axis instead of the X. reply nico 20 hours agoprevVery cool. Thank you for making and sharing this How are the animations made? Are they gifs or svgs or canvas + js, something else? reply nomemory 10 hours agoparentThe animations were done using p5.js. The source code can be found here: https://github.com/nomemory/andreinc-site/tree/main/assets/j... , but I am not satisfied with it. I was learning p5.js while writing the article. reply photon_lines 18 hours agoprevGreat work!! Note to author: you misspelled Heaviside step function: https://en.wikipedia.org/wiki/Heaviside_step_function. I also highly recommend 'Who Is Fourier?: A Mathematical Adventure.' reply alexjooau 7 hours agoprevGreat article! reply dylan604 16 hours agoprev\"We rarely see angles expressed in degrees; usually, we represent them in relation to the number PI\" I love it when someone too close to the tree loses sight of the forest this way. \"rarely see angles in degrees\" is so wild of a comment for anyone outside of math. How does one find their location on a map in long/lat in degrees or radians? Do we say degrees on a compass or how many radians? How many radians is that star in right ascension? Did that skater just land a 4*pi jump or a 720? reply defrost 16 hours agoparentThe context of \"we\" there is literally applied mathematics and it's absolutely correct that degrees are rare in that field .. a \"720\" is not a useful way of describing two full revolutions. reply dylan604 15 hours agorootparentIt tells me everything I needed to know that it was 2 full circles. At least better than 4pi does. \"We\" all agree that 1 circle is 360° so 2 circles would be 720°. It's really not difficult. People living in the worlds of maths forget the rest of the world is not a place of absolutes. There are so many way of looking at things and labeling things. Yet, maths people look at normies like they're stupid for not seeing it their way and make argumentative statements like degrees are useless and anyone using them are silly. reply Tainnor 4 hours agorootparentYou're the only person making argumentative statements. The quoted sentence in the article was descriptive, not normative. In the context where you'd discuss Fourier series, degrees really aren't used. You're the only person bringing up skateboarding as if it had any relevance to the article. reply adzm 10 hours agorootparentprevIt might make more sense if we used tau instead of pi. Then 1τ just means 1 rotation. Instead of 2π. reply rjurney 19 hours agoprev [6 more] [flagged] anyfoo 19 hours agoparentIn math and engineering, in which context this is, he's absolutely right. reply stergios 19 hours agorootparentIn engineering text books, sure they use radians. In mechanical engineering design if you put a radian on a geometrical dimensioning and tolerance sheet it's not going to go well for you. I suspect the same holds for civil engineering too. I have my doubts that radians are useful in chemical engineering, but I could be wrong there. reply anyfoo 18 hours agorootparentFair, I was mostly thinking electrical engineering and similar. For civil engineering, I'm curious what is common. As soon as you have Fourier and/or Laplace Transforms, you will most likely end up using radians, and are these not relevant for civil engineering? I have no idea, though the modeling of linear, time-invariant systems (i.e. filters) strike me as important if you're building, say, a bridge, and/or care about seismic activity at all. Nevertheless, in the context in which this was presented, radians are the only way to go. reply bigger_cheese 17 hours agorootparentprevOutput on control system HMI Scada screens for stuff like Tilt Drives are all in degrees as well, in this respect it's like the Kelvin scale. Control system doesn't display thermocouple readings in Kelvin either (at plant I work at Temperature setpoints are displayed as degree Celsius). You might use Radians/Kelvin internally for ease of calculation but they aren't numbers you quote around or report things in. reply aio2 19 hours agoparentprev [–] I mean, yea, radians are used often, arguably more than degrees when it comes to math and physics. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The article \"From the Circle to Epicycles (Part 1)\" introduces Fourier Series, explaining fundamental concepts like trigonometric functions, Euler’s identity, and sinusoids using animations for better understanding.",
      "It covers the unit circle, the significance of π, and the properties of sinusoids and epicycles, leading to the introduction of Fourier Series and their applications in approximating complex waveforms.",
      "The article also discusses the Fourier Series decomposition of various waveforms, including square, triangle, and reverse-sawtooth waves, and introduces a visualization tool called \"Fourier Series Machinery\" to illustrate these concepts."
    ],
    "commentSummary": [
      "The discussion contrasts visual learning tools, such as animations, with algebraic and matrix-based explanations for understanding Fourier series and transforms.",
      "Users highlight resources like 3Blue1Brown's YouTube videos and tools like Manim and p5.js, emphasizing the importance of solid math equations and proofs for practical applications.",
      "There is a debate on using degrees versus radians, with a preference for radians in theoretical contexts, underscoring the limitations of traditional education and the value of alternative online content."
    ],
    "points": 541,
    "commentCount": 76,
    "retryCount": 0,
    "time": 1717533592
  },
  {
    "id": 40578460,
    "title": "Journalist Shares Humorous and Bizarre Texts from iPhone Thieves",
    "originLink": "https://gothamist.com/news/things-the-guys-who-stole-my-phone-have-texted-me-to-try-to-get-me-to-unlock-it",
    "originBody": "A non-profit newsroom, powered by WNYC. Listen Live Donate NEWS Things the guys who stole my phone have texted me to try to get me to unlock it Veronica de Souza/Gothamist By Veronica de Souza Published Jun 2, 2024 Share We rely on your support to make local news available to all Make your contribution now and help Gothamist thrive in 2024. Donate today Gothamist is funded by sponsors and member donations A version of this story originally appeared in Read Max, a twice-weekly newsletter about tech, politics, culture, and media. My phone was stolen in early March, most likely while I was standing on the platform at the York Street station waiting for the F train. Fifteen minutes later, in a food hall under the movie theater where my boyfriend and I were supposed to see “Dune 2,” I reached into my pocket and realized it was gone. He looked at the Find My app on his phone: My phone was “last seen” at York Street but wasn’t registering a current location. Someone had turned it off. As quickly as possible, I did all the things you’re supposed to do when your phone is lost or stolen —- mark it as lost, cut off service, and remotely erase it. I spent the rest of the night anxiously refreshing the Find My app, watching my phone move around Manhattan before it finally stopped at Rockefeller Center. I didn’t bother confronting the thief. Worst of all, we didn't even see “Dune 2.” After two hours in the Williamsburg Apple Store the next morning, I had a new iPhone 15 and I stopped stressing. As long as I didn’t remove the phone from my Apple account or the Find My app, the phone was essentially bricked to anyone without the passcode and my iCloud password — unusable by the thieves, or the fences who I assume bought it from them. Now my phone was their problem. Where had my phone gone? I learned (from Reddit comments – where else?) that stolen iPhones usually end up sold in bulk online, either directly by the thief or by a fence who buys stolen phones individually and then sells them in bulk. The purchasers are either looking for parts or think they can crack into the phones, wipe and resell them. This teenager on YouTube, for example, bought a box of stolen iPhones for $1,300 on Liquidation.com: As you can see, most of the phones she tried didn’t have passcodes but were still linked to iCloud accounts, which meant she (or anyone buying the phones) still had access to all of the original owners’ texts, emails and photos. She sent a message to each iCloud email offering to give people their phones back, but only one guy replied. I, sadly, did not get a message from a teenage YouTuber earnestly offering to return my stolen phone. Instead I received a series of texts from someone cycling through a number of different strategies for engaging, convincing, tricking or scaring me into unlocking the phone for them. Rather than being engaged, convinced, tricked or scared, however, I was delighted. The experience managed to combine the internet's promises of international communion and international crime. There was something oddly soothing about being in touch with the people who’d stolen my phone. Instead of it disappearing into the ether, I knew more or less where it was, and thanks to these texts, I knew that it was a source of frustration to the people who’d gotten their hands on it. As the texts escalated in complexity and rage, I sympathized with their plight. I mean, not enough to unlock my phone. But we’ve all been there – sometimes you get stuck with a difficult project at work. In case you find yourself in a similar situation, here are some things the guys who stole (or later purchased) my iPhone have told me to try to get me to unlock it. 1) “Your iPhone 14 Pro is trying to pay with Apple Pay in China.” The first text I received from the scammers. Veronica de Souza/Gothamist The first text I got from my new friends was sent on April 28, a few weeks after the phone was stolen, and I have to be honest … it kind of got me, at least for a second. It helped that, even though this text came from a clearly bogus iCloud email address, Apple’s own “helpful” iMessage feature was sort of fooled: the auto-filled contact name said “Maybe: Apple Pay.” Maybe! Apple’s own “helpful” iMessage feature was sort of fooled: the auto-filled contact name said “Maybe: Apple Pay.” Maybe! Veronica de Souza/Gothamist At this point, my phone was in Sunnyside, Queens: My phone was camped out in Sunnyside, Queens for a bit. Veronica de Souza/Gothamist My old phone's new owners persisted with this tactic for a while. I later got the same message again, but from a different iCloud address. Two weeks after that, on May 12, I got an identical message in which “USA” had been replaced with “China.” I'm guessing it was intended to dial up the panic, but instead it made me feel cosmopolitan and sophisticated. 2) “Iv'e bought an iPhone 14 Pro I'm using, it have your messages, emails, cards, bank, notes and personal information on it” Posing as the unwitting new owners of a stolen iPhone, the scammer continued to try to get me to unlock the phone. Veronica de Souza/Gothamist On May 13, my pen pals reached out from a Filipino phone number. They'd switched from the impersonal \"Apple Pay warning\" texts to claiming that they were the unwitting new owners of a stolen iPhone. I appreciated this more intimate approach – the message even came from a phone number rather than email address, which was a nice touch – and I was glad to hear they were concerned about my insurance claim. “Best of luck!” I texted back. 3) “Your old iPhone 14 Pro is recycled by us, we are just recycling merchants, we are not the ones who steal your phone” This was my favorite text. Who am I to say no to recycling? Veronica de Souza/Gothamist The next day, I was kindly contacted by a U.K. phone number notifying me that the people in possession of my phone simply wanted to recycle it. This was one of my favorites: Sure, there’s an implicit threat (“hackers” might “contact my family” to, what, send them thousands of photos of my dog??), but it's not just some poor sap I'm supposed to help, it's the planet. Who am I to stand in the way of recycling? I replied: I loved lying to the scammers. Veronica de Souza/Gothamist 4) “The phone is going be auctioned on the black market with your personal information and everything about you that you had on it.” I appreciated the use of CAPS in this one. Veronica de Souza/Gothamist I got this one on the same day (May 14), from the Filipino number, which had previously claimed to be some guy who’d accidentally bought my phone. 5) “Your old iPhone has been disassembled… you do not want to take it back, because it does not belong to the law of the United States” This is where things started escalating. Veronica de Souza/Gothamist At the same time, I got this nearly identical message from the U.K. number. Thanks to Reddit, I discovered that most of the messages I was getting were scammer copypasta: English phrases and texts that were passed around between thieves, fences and jailbreakers to entice bereft phone owners like me. In the end I think these two texts provide a lesson in the importance of great openers. While the previous message didn’t move me much, it’s hard not to admire the arresting visuals of the detailed iPhone breakdown, the reference to “the law of the United States,” the mocking “you’ll never catch me.” It’s an incredible intro that really elevates the literary value of what would otherwise be a rote bit of vaguely threatening text! ! ! ! ! ! In any event, I will credit them with being honest: I checked Find My and my phone was indeed in China. It looked like it was in an office building conveniently located around the corner from an Adidas store, about 8,000 miles from me. My iPhone took a little trip! Veronica de Souza/Gothamist 6) “Listen! I'm going to harassed, wreck and ruin your sad, stupid low pathetic life if its not removed you mindless peasant.” This was the final (and most threatening) message I received from my scammer friends. Veronica de Souza/Gothamist I did not like this tone from my best pal! This text (more scammer copypasta) was accompanied by two videos. One video showed a “hacker” breaking into someone’s iPhone and going through their camera roll, which was a mix of selfies and graphic videos. Another appeared to be a screen recording of a social video of a man showing off his gun. I guess this is supposed to be the guy named “Miami” who is going to come kill me and my whole family “on Monday.” I haven’t heard from them since. Tagged EXISTING IN NEW YORK CITY Veronica de Souza Veronica leads the digital news and audience team at WNYC and Gothamist, which includes breaking news, newsletters, and social, among other things. She's spent over 10 years working in newsrooms, including at Gawker Media, The Daily Beast, Vice and more. Veronica lives in Brooklyn with her dog Archie. Read more Gothamist is funded by sponsors and member donations MORE NEWS",
    "commentLink": "https://news.ycombinator.com/item?id=40578460",
    "commentBody": "Things the guys who stole my phone have texted me to try to get me to unlock it (gothamist.com)434 points by neaden 22 hours agohidepastfavorite266 comments mullen 19 hours agoIf someone in China starts harassing you or threatening you, just start sending messages that the two of you are conspiring to overthrow the Chinese government and the messages will stop real quick. These Chinese mafia types talk tough until you start sending anti-Chinese government copy-pasta to them and they shut up real quick. reply altairprime 17 hours agoparentIt's probably best not to declare war upon China in writing. Instead, ask how many students died in the 1989 Tiananmen massacre. That specific phrase should specifically auto-alert the censors to a known forbidden topic, without resorting to threats against China. How long of a lost phone message can one set, anyways? reply jimt1234 15 hours agorootparentThis will absolutely work. My friend is from China, but lives here in the US. Most of her friends and family are still back in China. We communicate primarily over iMessage, but also on WeChat. About 2 years ago I sent her a video about the Tiananmen Square protests over WeChat by mistake - I meant to send it over iMessage. The next day she called me, really mad; her WeChat account was shut down, without explanation, and she knew exactly why (my WeChat message). But what's worse is many of her WeChat contacts also had their accounts shut down - not all of them, just her closest or most frequent contacts, like her family and close friends. My account didn't get shut down. The impacted accounts were magically re-enabled 3 days later. No email. No notification. Nothing. It was a clear message: We're watching, all of you. reply kingkongjaffa 10 hours agorootparent> About 2 years ago I sent her a video about the Tiananmen Square protests over WeChat by mistake Absolutely insane that you would even think to do this to a Chinese national. Good job they are in the US. Could have been far worse for their family back in China. Unbelievable. reply beretguy 10 hours agorootparentHey, people make mistakes. The correct solution would be to never use wechat in the first place. reply jimt1234 2 hours agorootparentMuch of Chinese society revolves around WeChat. Chat/phone/payments. I'm told WeChat is pretty much required for one to function in China. reply jimt1234 2 hours agorootparentprevWe poke at one another about the Tiananmen Square protests because we have such different perspectives. Like me, she was in high school during that time, and was told the protests were minor, and that they were instigated and largely conducted by outsiders, like the US, seeking to overthrow the Chinese government. The reports of large-scale deaths is Western propaganda. She still more or less believes that story - she's been hearing it her whole life. We sometimes (rarely) exchange articles about the incident. I've always appreciated the back-and-forth because, even though it's a serious topic, we keep it lighthearted. Anyway, she purchased her iPhone in the US. All her online accounts, except for WeChat, are associated to US-based services. Yeah, I fuucked up; I meant to send the message over iMessage, where we've never had a problem. reply MaxikCZ 11 hours agorootparentprevCant wait till we outlaw encryption in the west, too. reply mbirth 4 hours agorootparentYou mean like the EU is trying now? reply consf 5 hours agorootparentprev\"My account didn't get shut down.\" - But why not yours? reply jimt1234 2 hours agorootparentThis is a mystery. My guess is that the Chinese censorship system (The Great Firewall) knows my account is not associated to a Chinese National, so they don't care. Or, there could be legal restrictions/complications because I'm a US citizen. Who knows? I wouldn't be surprised if my little fuuck up didn't land me (or my WeChat account) on some sort of list, though. reply squigz 30 minutes agorootparentprev> It's probably best not to declare war upon China in writing Why not? Is the Chinese government going to send some people to Canada to disappear me? reply kazinator 13 hours agorootparentprev> That specific phrase should specifically auto-alert the censors to a known forbidden topic Heck, even get you flagged here, for that matter. reply throwaway7ahgb 7 hours agorootparentHere is in HN or here is in the \"US\"? Not sure what you're suggesting. reply MOARDONGZPLZ 19 hours agoparentprevThey don’t. I have tried this and other similar things such as sending images banned in China (but otherwise totally fine). Perhaps they weren’t in China I suppose. reply yumraj 17 hours agoparentprevSerious question: have you tried this or are you theorizing? reply mullen 14 hours agorootparentThis is a well known tactic to do to Chinese scammers or any person you don't want to talk to that is in China or has to go back to China. reply datpiff 8 hours agorootparentaka \"I read it online\"? reply contrarian1234 15 hours agoparentprevIn my (limited) experience private one on one messages are not censored. Or at least this was the case several years back... It's possible things are more strict now. At least in my tests on Wechat you could discuss tiannanmen square or whatever you want with individuals. Some stickers were censored on Wechat and wouldn't show up. I think images as well may be don't get delivered. Virtually all censorship and cases of people being arrested are when people talk about these things in large group chats (if a group chat has more than X amount of people it needs to be \"registered\"). My impression was it's a government fear of things going viral and controlling the public sphere and not about creating a panopticon/state-terrorism Again.. This might be out of date, but this was at least the case for a long long time. reply bnchrch 21 hours agoprevHonestly. This is a pretty good advertisement for Apple. I appreciate the effort that they've put into making my device less valuable to thiefs. reply r0m4n0 20 hours agoparentIt’s interesting though… it’s still worth something to thieves, they still can recycle the screen, buttons, case etc. I’ve had my iPhone stolen in recent years too, people can’t resist. Don’t get me wrong, it does feel good to stick it to the perp but doesn’t apple itself have the most to gain from locking most of the value away? The second hand market is completely gated by them. Broken FaceID? You need a new phone… This is a win win for Apple reply sosodev 20 hours agorootparentAren’t most of the parts tied to the motherboard? reply nebula8804 13 hours agorootparentEach generation seems to add an additional part. I know they serialize Face ID module, maybe the screen, the battery gives an indicator in the settings menu that its not OEM but it still works. I doubt the case or volume buttons are tied to the phone but what else is left? The Antenna or USB-C port? reply qingcharles 12 hours agorootparentScreen is locked. If you do your own replacement you have to call Apple to get it relinked to the mobo. reply bigstrat2003 15 hours agoparentprevApple puts effort into making you go to them for repair parts. Any deterrence against thieves is simply a fringe benefit. Don't credit them for something that isn't actually their goal. reply dogbait 6 hours agorootparentWhen buying my child her first iPhone we went through so many used devices at the local phone store which had all been refurbished or repaired using sub standard 3rd party screens. Being able to see Apple’s warning on the iPhone 11 and up \"Unable to verify this iPhone has a genuine Apple display” meant we were able to find a phone which was still stock. I’m a big proponent of the right to repair but in this case Apple’s repair deterrence feature had a big benefit for us. reply sethherr 14 hours agorootparentprevI think it is pretty clearly a goal - when you create marketing campaigns around an aspect of your product (and Apple has around this sort of security), delivering on your promises is clearly a goal. I wouldn’t be surprised if the original impetus for this variety of security was a prompt like “what are some objectives that support us creating vendor lock in” - that’s how organizations end up with multiple interconnected objectives reply account42 10 hours agorootparentThe purpose of marketing is to bend the truth to something that is going to attract customers. It doesn't have anything to do with the actual goals of the company. reply icehawk 14 hours agorootparentprevDeterrence against thieves is a pretty big benefit. Meanwhile there's massive amount of low-quality parts i have to wade through when wanting to order a replacement battery. reply ChrisMarshallNY 21 hours agoparentprevThese days, it's not worth it to steal an iPhone. It's a useless brick. That Apple knows where it's at. reply makeworld 21 hours agorootparentClearly this isn't true, as her phone got stolen. reply koito17 21 hours agorootparentYup. I assume there is plenty of value to be found selling donor boards and other parts of the iPhone. Not saying it's a repair shop's fault if they end up with motherboards of a stolen phone, but if I were a thief dealing with an activation-locked phone, my first idea would be a jailbreak, and if that fails, then disassemble the phone and sell the parts. Presumably thieves already do these things (and more sophisticated things) to extract value out of stolen iPhones. reply seanmcdirmid 20 hours agorootparentYou really have to be careful with third-party iPhone repairs, especially in China. I had my phone repaired once, and when I got it back...it kind of worked, but they swapped out another part for something that was broken to ensure repeat business. That was in the iPhone 1 days though. reply nebula8804 13 hours agorootparentIts been about 17 years since the iPhone 1 days... reply seanmcdirmid 2 hours agorootparentYes, I'm old. This was in 2009 I think (I bought it in 2007). The e-markets are a lot different now (you can still find obscure shady offices repairing older hardware), and iPhone repairers that were scamming people are probably long gone. reply nebula8804 13 hours agorootparentprevWhat is on the board that can be used? Obviously the SOC is out. Maybe just the RAM or Flash? Is there really anything else? reply JumpCrisscross 19 hours agorootparentprev> Clearly this isn't true, as her phone got stolen Older iPhones are still in the wild. I wouldn't expect every pickpocket to be adept at differentiating them at a glance. reply AnthonyMouse 14 hours agorootparentBut that still has the same result, doesn't it? \"Anti-theft\" measure doesn't prevent the theft, only prevents third party repairs. The average thief probably doesn't even distinguish between Apple and Android phones. A literal pickpocket is only going to have as information the outline of a phone-shaped thing in your pocket. If they lift it off of you and can't use it, they've still stolen it. reply JumpCrisscross 14 hours agorootparent> still has the same result, doesn't it? I can tell at a glance if someone's on an iPhone or not. I can't as similarly which generation they're on. The FCC, at the very least, seems to find merit in the deterrence value of such measures [1]. (From a purely-retributive standpoint, I find some satisfaction in knowing someone stole a brick that needs to be parted out, versus getting a working device.) [1] https://www.wsj.com/articles/BL-DGB-39237 reply AnthonyMouse 13 hours agorootparent> I can tell at a glance if someone's on an iPhone or not. How? Most people have a phone case and then all you can see is the case. Pretty much all modern phones are the same shape. > The FCC, at the very least, seems to find merit in the deterrence value of such measures People don't like it when someone steals their phone, so they demand the government Do Something, so the government proposes to Do Something, regardless of whether the something is effective. Classic politician's syllogism. > From a purely-retributive standpoint, I find some satisfaction in knowing someone stole a brick that needs to be parted out, versus getting a working device. The modern world isn't any fun. \"I willingly compromised my own ability to repair my phone so a hypothetical thief might be put out.\" People should be carrying around decoy phones that are full of sticky glitter and skunk scent, boobytrapped with blasting caps. If you're going to take revenge then do it well. reply knodi123 20 hours agorootparentprev> It's a useless brick Unless the owner falls for one of these texts, right? Isn't that the entire point of the linked article? reply arp242 17 hours agorootparentYes exactly; they must have a non-zero success rate. Few years back my friend's iPhone got stolen, and it was more or less the same kind of stuff. reply acchow 20 hours agorootparentprevIt gets stripped down for parts. Many of the individual components are valuable reply seanmcdirmid 20 hours agorootparentThe higher value parts are locked down also. I wonder if that includes the screen or not? Is it possible to lock down an iPhone screen these days, or can you use it as a replacement for another iphone whose screen is busted? reply qingcharles 12 hours agorootparentThe screens have been locked for a few years now. If you replace it you have to call an Apple hotline to get the new screen relinked to the mobo. reply XajniN 19 hours agorootparentprevDon’t expect reasonable behavior from a phone snatcher. reply whimsicalism 20 hours agorootparentprevi see homeless people in SF disassembling iphones so clearly they have routed around this reply throwaway7ahgb 7 hours agorootparentSeriously? There has to be a story here. You're saying they sit on the street with multiple phones, like a repair shop? reply whimsicalism 2 hours agorootparentyep, although i've only seen this once (at the intersection of oak & baker). it surprised my economic intuition as i would expect them to be pawning off the whole phone and then someone later on in the supply chain doing the disassembly reply kernal 21 hours agorootparentprevI disagree. There are a lot of uses for an iCloud locked iPhone, such as a music player, podcast player, security camera, etc. Edit: I should clarify that the embedded video clearly showed that a number of the iPhones she had were still able to be used. Perhaps the owner didn't or wasn't able to iCloud lock them, but that's what I originally meant. reply ms7m 21 hours agorootparentNo, if an iPhone is iCloud locked, you're not able to use the phone unless you enter the password of the account. reply notatoad 20 hours agorootparent>if an iPhone is iCloud locked but that still leaves the possibility that it isn't locked, and therefore it is worth it for theives to steal it. Even just the chance that it's not locked would make it worth stealing. reply hifromwork 20 hours agorootparentYes, but your response doesn't make sense in context. GP specifically said that there are a lot of uses \"for an iCloud locked iPhone\", parent debunked it. reply chuckadams 21 hours agorootparentprevNone of these work when the device is marked as lost/stolen. reply acchow 20 hours agorootparentprevThis isn’t true. The iPhone is locked down over iCloud. I assume remote lockdown also works while the phone is turned off or on airplane mode (as Find My Phone does by using the passive Apple mesh network) reply gruez 19 hours agorootparent>I assume remote lockdown also works while the phone is turned off or on airplane mode (as Find My Phone does by using the passive Apple mesh network) It doesn't need to be. If you have a passcode, the phone can't be used unless the correct passcode is entered. If you try to factory reset it, you can't get past the setup wizard if find my iphone was enabled for the phone prior to the wipe. Note this doesn't require that the phone receive notification that the phone has been marked stolen. So the only way you can use the phone is if find my iphone wasn't set up prior to it being lost. reply prmoustache 20 hours agoparentprevNot so much if it allows the thief to cet contact info of the original owner. reply lgats 13 hours agorootparentthe SIM card will have this information, also if you put your phone in lost mode, it gives you the option to display a contact number reply giomasce 8 hours agoparentprevNot enough, apparently. She still got robbed of her iPhone and had to buy another one. reply tim333 7 hours agorootparentI would like it if you could have your stolen phone chat to the police and report its location so they could go arrest the thieves. There doesn't seem much enthusiasm in law enforcement for enforcing those laws though. There was an interesting approach with VanMoof bikes where the company insured it against theft, and if your £2000 bike disappeared they actually had a team that would go to it's location and ask for it back. The police don't seem that interested in getting stolen stuff though - that was private enterprise. Still I'd pay for that on a phone if only to get the bastards who snatch the things. reply MyFedora 20 hours agoparentprevNo, they sell the parts if they can't get the phone unlocked. Even if the phone would be a worthless brick, a stolen phone is a phone that the owner will never get back. Fresh e-waste, straight into the landfill I guess. Is this sarcasm? reply Cerium 20 hours agorootparentNobody risks stealing individual bricks from people's front yards. The risk exceeds the reward. The end goal here is to make the phone so useless that nobody even tries to steal it. reply MyFedora 19 hours agorootparentNo, this is a side effect of Apple striving to design the world's most unrepairable devices, or more accurately, earn more money with overpriced repairs by creating an artifical monopoly on Apple device repairs using part pairing. Why else would there be no option to pair parts for repairs if you can prove that you are the owner of the device? reply Dylan16807 18 hours agorootparentThey often use security as an excuse to be anti-repair. That doesn't mean they're lying any time they mention security. Many of the things they do really are to benefit the customer. One of those is having strong locks on stolen phones. reply teeray 14 hours agorootparentSecurity is often a cloak for anti-consumer measures for companies. For another example, Netflix and friends have adopted the “we noticed suspicious activity on your account” phraseology to dress up their password sharing crackdown as something consumers want. reply theamk 16 hours agorootparentprevIf you want to make sure that thieves do not steal iPhones, you need to ensure there is no point in taking them apart for parts. Which means you need to prove both: you are the owner of the device; and that the parts are not stolen. This means serial number on each part (likely already exists), database of individual parts with \"stolen\" flag, and a secure way to query this before pairing. Or don't bother with all that, and just prohibit pairing of parts with unknown origin - which means prohibiting all 3rd party services. reply iknowstuff 18 hours agorootparentprevI know this is a hot take but hear me out. I think it makes sense these days for Apple to make phones without much care for repairability. Labor is too expensive to bother with spending hours on repairing a phone. Cheaper to produce new one. The problem with that is the environmental impact of disposable electronics, but Apple is leading the way and using recycled materials, avoiding plastic, becoming carbon neutral etc. I think this is the direction the industry is generally going to move towards. It makes sense and stops being a problem when the entire product is recyclable and/or uses recycled material in the first place. They’ve been touting their advancements pretty loudly, but I don’t think it’s getting picked up by journalists. reply shiroiushi 13 hours agorootparent>Apple is leading the way and using recycled materials, avoiding plastic, becoming carbon neutral etc. How is avoiding plastic better? If you mean by making phone bodies out of metal, that's not better for the environment. It takes a LOT more energy to make things out of metal than plastic, and even though metal is recyclable, that too takes a LOT of energy (high melting point), and for a small item like a phone seems rather unlikely anyway. Plastic is recyclable too, it just isn't done much because people don't want to and it costs too much. >It makes sense and stops being a problem when the entire product is recyclable and/or uses recycled material in the first place. Phones are not \"recyclable\", and likely never will be. There's far too many different materials in them, which can't be readily separated. reply captn3m0 17 hours agorootparentprevI learned recently that Apple shreds old devices instead of selling them to keep the price of old-iPhones artificially inflated by limiting supply. https://www.ifixit.com/News/94386/the-truth-about-apples-fre... reply philistine 17 hours agorootparentprevThis is a very succinct description of what Apple is trying to achieve. Imagine inside your laptop there are two boxes: the compute and the battery. You can get any of those swapped by Apple. Apple then takes care of the 2 remaining Rs on its own with the box you swapped. That's what Apple wants. reply golergka 20 hours agorootparentprevParts are less valuable, which leads to lower level of theft overall. reply Havoc 21 hours agoprev>it was in an office building conveniently located around the corner from an Adidas store lol no. That location is the Huaqiangbei electronics market - one of the largest electronics markets in the world w/ a thriving 2nd hand parts ecosystem. reply seanmcdirmid 20 hours agoparentThose e-markets are filled with adjacent buildings where the market spills over, so it very well could be near but in a building that has an Adidas store. The glory days of the e-markets are almost over though, the one in Zhongguancun is much smaller than it used to be, but you still need to know the right building to go into to get your old X-box or Wii unlocked. reply zer00eyz 20 hours agoparentprevSo as I understand it, all of the \"parts\" are mostly worthless because they wont just work in another iPhone. How do you maintain this functionality and give the \"right to repair\"? I would assume that it's possible but would add a fair bit of cost to devices (not that someone like apple cant bear said costs)... But IM just guessing on that front as I have zero experience with hardware locking. reply bmicraft 20 hours agorootparentEasy, only allow components that aren't currently registered to someone elses iCloud account (read: stolen) reply chemmail 20 hours agorootparentprevMost parts will indeed work. It will just show \"unkown part\" in the settings. The main thing that is really paired is the front camera for faceID. Also changing the screen without the original paired chip makes you lose autobrightness and truetone which is need to get to MAX brightness on 14 and up. reply philistine 17 hours agorootparentSounds like people are swapping a screen on those iPhones without doing a calibration. reply Havoc 20 hours agorootparentprevThe high value parts are locked down so yeah the options for stripping iphones have become more limited & thus backyard iphone recycling operations decreased. Unsure whether anyone has found a way past the apple locks...not to my knowledge. >cost I don't think cost even factors into any of this at apple margins. What does is that repairability affects things that matter to the consumer: Size, weight, thinness, feel, lack of exposed screws etc. Compromising on repairability makes all those easier reply Taniwha 15 hours agoparentprevIt's roughly a subway stop east of it - the hotel I stay at when I visit is almost at the exact point on that map - but remember that Chinese maps and GPS do not line up (check that spot on google maps and then turn on the photo layer you'll see that the streets don't line up) - the actual spot looks like the mall even further east. Huaqiangbei is legit (and a wonder!) - the spot that would be a real smoking gun is the Huaken Building (aka the \"Dodgy Cellphone Market\") which is south of Shennan Middle Rd and between the spot on the map and Huaqiangbei. That place is pretty amazing in its own right, at the east end people sell bits of phones, dead phones, 'recycled' phones - they are torn down into parts which are reassembled onto new PCBs and resold at the west end of the building - it's full of lots of little cubes, people each doing a part of the process and onselling their work product to the next person - if you ever thought your phone can't be worked on by hand, can't be fixed, you are so wrong, these are real professionals - they take junk and make it real again. If the phones were simply being recycled it would be wonderfully OK, even if Apple hates the idea of this sort of grey market, they'd rather we threw them away so they can sell new ones. As I understand it many of the Apple chips these days are 'paired' at the factory, you can't just replace one (and they have to pass through the above process together as a unit) this may mean (I'm completely guessing here) that they're still screwed if the user wont erase their phone, even if they completely remake it reply nebula8804 13 hours agorootparentThere is so much hands on knowledge that I wish was better documented. Louis Rossmann is great but there are a lot more non Apple devices out there that sometimes seem impossible to repair unless you really know things at a circuit level. reply Taniwha 13 hours agorootparentI've seen instruction pamphlets for these guys that measure the current draw as a particular iPhone model boots, from that they show places in the boot process where particular chips fail and how it changes Remember they are removing chips from old boards, reballing them and soldering them onto new ones - they can pull a chip, fix a short and drop it back down pretty easily reply valleyjo 21 hours agoprevThis happened to my sister. She got her phone stolen at a bar in Austin. They’ve tried to phish her multiple times and now it’s gotten to intimidation texts where they are threatening physical and sexual violence to her unless she unlocks the phone. She put it in lost mode right away and due to my assurance she knows she is safe but honestly it does make you kind of just want to unlock and get the nightmare over with. She has tracked the iPhone via find my through a few us cities and finally it’s in China. reply thinkling 18 hours agoparent> She has tracked the iPhone via find my through a few us cities and finally it’s in China. I wonder… if people whose phones get stolen start tracking the locations they see it travel to using Find My, and a site were available to report these to, could enough evidence be collected to motivate a police/FBI visit to the location of fences in the US who are shipping these to China? (Could one prevent such a tool from being used maliciously to swat someone?—I suppose in the end a specific victim would have to go to the police in person, limiting that risk? I’m imagining the address-collecting site might respond “yes I’ve seen more than (say) 20 phones reported at that location, you may want to work with local police if you can”.) reply joecool1029 13 hours agorootparent> I wonder… if people whose phones get stolen start tracking the locations they see it travel to using Find My, and a site were available to report these to, could enough evidence be collected to motivate a police/FBI visit to the location of fences in the US who are shipping these to China? Probably not. Below their pay grade to stop petty crime. There's also the chain of custody on evidence that becomes questionable for the prosecution. They need to collect it using their own methods and reality is they have easier or larger crime to go after. My observation on this is that it needs to be more of an 'institutional' level of crime to get them motivated if it doesn't happen in their own investigation. Ultimately this is 'helpful' crime for Apple's bottom line, unlike parts smuggled in to legitimately repair devices. Other example of what I mean: I've witnessed a craigslist deal go south on a purchased motorcycle when the seller never titled it from previous owner. Cops were called, and told the two parties to stop arguing and file a claim in court else disorderly persons charges would be brought. (thankfully the seller was able to contact the former owner, and drive with the purchaser to have the title transferred properly) If however this was a situation like a bank calling about forged checks, the police would move to arrest and prosecute the forger immediately. There wouldn't be this onus on the bank's part to file a civil claim. reply Zak 6 hours agorootparentWhile I'm sure there are many instances of the police taking crimes against institutions more seriously than crimes against individuals, your motorcycle example isn't a good one. The paperwork mixup described in this comment doesn't sound like an attempt at fraud or theft because no evidence of intent to deceive someone or take property from its rightful owner is described. Indeed, when the previous owner of the motorcycle was contacted, they corrected the paperwork. reply alach11 3 hours agorootparentprevSeems like an interesting thing for Apple to do. Collect locations for phones that are marked stolen and look for dense clusters. Then work with local authorities to identify the perps. Would be a fun project for a small team! reply throwaway7ahgb 6 hours agoparentprevIf they are still harassing her, can you try one of the tricks above? Start talking about banned topics. reply consf 5 hours agoparentprevI think most of the cases people give up and unlock it reply mdip 17 hours agoparentprevIt's why I have the \"general rule\" of: Don't reply to any text from anyone that I don't know. And if what I'm receiving \"seems off\" reply with a subtle \"challenge\" (mention a fictional pet that passed away ... the scammer will play along, your friend will probably send you a \"?\" to which you can ... carefully ... explain \"I wanted to make sure you weren't a Nigerian Prince posing as Alice\"). I know my personality would make it very tempting to reply to a message coming from the person who stole my phone, maybe taunt them a bit, but you're kind of giving into their game when you do. I mean, best you can do is burn a little of their time, which is probably much less valuable to them than yours is to you and put up with increasingly hostile threats[0]. Contrasted with completely ignoring any text that arrives regarding the phone (immediately blocking any arriving from accounts from social media) they're left assuming -- for whatever reason -- you're not even getting the threatening texts. And, of course, if the scammer were planning on following through with the threats it'd be a very labor intensive, high-risk, low-reward operation -- not the \"make a quick buck\" that stealing and reselling a high value piece of electronics is supposed to be. There are too many legitimate ways to make more money that involve less work and less risk to pick \"crime\" as the choice. And while that means it's unlikely the scammer will follow through with the threat, that assumes the scammer is intelligent enough to understand how stupid it would be to expose himself to make a few hundred dollars and tends to vary depending on the amount of illicit substances the scammer is trying to manage the withdrawal symptoms of at that moment. It also assumes that the scammer isn't the actual thief, and the thief isn't some gang thug who'd be more than happy to follow through with the threat simply because \"your mean words hurt their feelings\"[1]. I figure, ignoring them is the quickest/easiest path to applying at least a tiny bit of pain. They're gambling; you and every other one of their victims is \"the iPhone game\". It starts with a box of bricked iPhones. Find the owner, send them a combination of words gleaned from a forum that other scammers have had success with and text your target. You win when you crack the right combination of words that causes a bricked iPhone to become almost instantly convertible into anywhere from a few to several hundred dollars. I'd imagine most slot machines would hit less frequently and pay out less when they do. Enough bricked phones have to become \"a few hundred dollars\" to feed the player's gambling addiction[2] so you're providing the lever \"pull\" of the slot machine -- free entertainment for them. By ignoring them, you're taking away that tiny high that comes when they experience \"Hope\" -- the \"hope\" that your \"pull\" will pay out. Take that away ... block 'em ... but maybe peek into \"Spam\" every once in a while if you want a little ego satisfaction. Maybe they sent you a few hundred messages of the \"Apple Pay\" variety. Now close your eyes and imagine your lone scammer with a box of locked iPhones (it's your fantasy, make him a lonely middle-aged white guy living in Mom's basement who hasn't showered in 2024), checking each bricked phone \"to see if you've won\", then the messaging app to see if anyone is going to give you a chance to play further ... only to find your fourteenth \"Apple Pay\" message disappear into the dead Ether (\"And they sent the last one from RUSSIA, how could they ignore RUSSIA!?\"). If they are the kind with a glass ego, nothing will make you feel less adequate than copying/pasting fourteen unanswered messages as imagine the answer you're not going to get to number fifteen. Short of maybe social engineering them to click a link to malware[3] and maybe turning the tables a little, I can't think of anything more effective. [0] And the stones on the thief -- or did your sister leave it unattended? I have spent no time in Austin, specifically, but a lot of time in Texas. Some places it seems everyone is carrying, right out in the open, carefully concealed (if you know where to look) ... whatever. At least a few of my buddies carry hoping for a reason to use it. I suspect in the parts of Texas I'm familiar with, if this happens at all, Find My yields it quickly recovered without police involvement, and maybe the thief's trailer/truck got a little fscked up during \"pick-up\". [1] They'll talk about being disrespected but at the end of the day, the problem comes down to an adult who would still fail the most basic parts of Kindergarten (don't misread that statement as implying anything about my feelings regarding the cause/reason/fault of such things; let's just walk away from that one for this footnote). [2] I couldn't find the study but over a decade ago I recall reading a that people with \"clinically diagnosed kleptomania\" (naively: \"stealing for fun, regardless of need\") had extremely strong tendencies to be gambling addicts, but that gambling addiction couldn't be correlated with a higher frequency of kleptomania (gambling addicts do skew higher for theft, but are often thieves out of perceived necessity, limiting to kleptomaniacs was intended to identify the similarities between the underlying psychological disorders). [3] This is a little tricky and comes with the \"you're probably breaking the law, now, too\" problem ... tricky because the scammer is probably using a texting app/web service. As a result, there's a larger number of OSes involved -- it would be less unusual for your sender to be texting you via a browser in Windows than you'd probably experience, otherwise, so if you, say, somehow convince them they need to \"click that link\" in order for your phone to complete unlocking, you're going to also have to work out a way to get them to reveal what OS they're sending from. reply pingou 21 hours agoprevIf that isn't already the case, Apple should warn you when you put your phone in lost mode that the robbers may contact you to get you to remove it from your iCloud account, and to never do it. Or add a big warning when trying to remove a device previously put in lost mode. reply notatoad 20 hours agoparentIs there a good reason why you might want to remove an iPhone marked as lost from your iCloud account? It seems like something that should just not be possible. reply pkulak 20 hours agorootparentMaybe you don't want some random phone in China clogging up your map until you die? I could see wanting to get rid of it. Be nice if you could hide it or something. Maybe a button to brick it out of spite and be done with it forever. reply sureIy 13 hours agorootparentWe’re talking about possible improvements: - hide the Lost device from the map after a month without unlocking it for them to sell. - gather more information: is it lost? Stolen? What to do next. - the device should be locked and blacklisted until you mark it as actually found. reply mentos 20 hours agorootparentprevBricking stolen phones is probably the best way to fight this reply krallja 19 hours agorootparentprev…if you find it? reply blamazon 20 hours agoprev> Apple’s own “helpful” iMessage feature was sort of fooled: the auto-filled contact name said “Maybe: Apple Pay.” Maybe! This seems problematic? In article screenshot shows that it pulled in a misleading contact photo as well. I know they also have a separate UI color scheme for business messaging that is meant to address phishing and trust but perhaps they should not auto suggest contacts containing the word Apple. reply infotainment 22 hours agoprev> I checked Find My and my phone was indeed in China. It looked like it was in an office building conveniently located around the corner from an Adidas store, about 8,000 miles from me. You’d think for a police state, China would be just little bit better at cracking down on obvious crime that happens within their borders. But I guess not. reply Xylakant 22 hours agoparentThe police state is interested in maintaining the power structure in the police state. As long as petty or even organized crime does not upset the balance of power, the authorities couldn’t care less - crime against Americans is no danger to them. Just as the Russian government has no interest in cracking down on its blackhat population, as long as they target non-Russian orgs. reply ASalazarMX 20 hours agorootparentOrganized crime and mafias are not the autonomous, powerful entities we're led to believe by the popular media. They're usually subjugated by the state, and used to bring money and do their dirty work. That's why cartels appear out of control in Mexico, but when USA wants an offer, Mexico will promptly deport a high-level criminal. It's a show of strength, that the state is still in control of the cartels, but won't extinguish them because they generate lots of money for their respective political coalitions. Happens everywhere, the biggest bank accounts of organized crime could be seized internationally if there was a will, but that would hurt politics too... unless they're Russians, there were token seizures of money and yachts last year, but it stopped quickly. reply Terr_ 17 hours agorootparent> They're usually subjugated by the state That makes me think of some US-libertarian-adjacent thought, which goes something like \"the government are just a big-enough gang\", or conversely \"organized-crime rings are just smaller competing governments.\" While I admit there is some pattern-making appeal to that idea, I don't think it quite matches what we see in practice, where gangs often seem quite happy to abandon unprofitable responsibilities and choose profit over political independence. If nothing else, it quietly conflates entirely different kinds of governments together: A small dictatorship is not just a shrunken version of large democracy. reply lazide 15 hours agorootparentGangs are as often business-by-other-means. Not to say some aren’t government, either. They often come to exist due to niches government can’t (officially) operate in, but when they grow enough, they’re susceptible to capture the same as any other entity. reply webninja 16 hours agorootparentprevThe history of China is that the ones with the biggest guns (and fireworks) took over the whole state, the most peaceful factions were subjugated, and the hardest to integrate ex: the Uighurs, either reeducated or exterminated. Let’s say you’re right for the sake of supposing. What will do you do about it? reply genter 21 hours agorootparentprevRussian government is the blackhat population. reply TrainedMonkey 21 hours agorootparentI have a theory that Russians foster black hat population while collecting evidence as a matter of national policy... so they could slowly recruit / quickly gang press talent from. Kind of like strategic reserve of hackers / zero days / etc. reply netsharc 20 hours agorootparentYeah, a bit like corrupt politicians in China (or anywhere, really), you can be corrupt, but if some other powerful politician doesn't like you, they'll arrest and ruin your life for corruption... Even though maybe the judge and police are also corrupt. Or in Egypt, you have to bribe to get a job as a police officer, your superior expects a tribute each month because his superiors expect one, all the way to the top, so you bully the public for bribes/protection money, that's why the police were so passionate about suppressing anti-government protests during the Arab Spring, their livelihood depended on it. Meanwhile the Egyptian military had its own economy to supply themselves (eg a factory making soap, or bread) and were viewed by the public as trustworthy. All in a fascinating book: https://www.goodreads.com/book/show/25622863-thieves-of-stat... reply zer00eyz 20 hours agorootparentprevI have worked with a bunch of Russian devs over the last few decades. They really aren't the same group. Black hats (criminals), Government, and commercial devs are pretty distinct, and dont often over lap on a day to day basis. That having been said the government jobs are available to either group (sometimes not by \"choice\") and there is a revolving door there. It's not so far removed from the university, drug company/reseearch, FDA revolving doors that we have for Drug development in the US. reply ronsor 22 hours agoparentprev1. Chinese government is not as competent as it tries to appear. 2. They do not care about foreigners or crimes unless they involve Chinese citizens. reply factormeta 21 hours agorootparent>2. They do not care about foreigners or crimes unless they involve Chinese citizens. Don't know if they really care that much about Chinese citizens. Probably just the ones in the politburo. reply whimsicalism 21 hours agorootparentIt's worked out remarkably well for Chinese citizens given they don't care at all about them, then reply postingawayonhn 20 hours agorootparentTheir GDP per capita still lags significantly behind Taiwan, South Korea, and Japan who all aligned themselves with the west. Millions also died due to their rulers through the 20th century. reply whimsicalism 20 hours agorootparentIndia seems like the much more obvious comparison than those countries reply zztop44 14 hours agorootparentprevMillions died due to their rulers in the decades before the Communist Party, too. reply seanmcdirmid 20 hours agorootparentprevChinese citizens are treated fairly similarly. There are plenty of unlawful activities that go unpoliced/unpunished (massage parlors), unless there happens to be a specific crackdown going on (otherwise, the police know what is going on and are probably getting a cut). reply darth_avocado 21 hours agoparentprevOften stances of the governments across the world are: we’ll ignore it unless it becomes a problem for us. Often, in organized crime, money flows back to the government and creates job opportunities for people that keeps them happy. There is a reason. Why there’s so many scam call centers in India. The government will turn a blind eye until someone comes in with proof that raises a stink or if other international government agencies get involved. reply mschuster91 21 hours agorootparent> There is a reason. Why there’s so many scam call centers in India. According to a commenter on HN from a few months ago [1], the reason is a political division - the scam callcenters are in West Bengal, where the ruling political party turns a blind eye due to corruption and to provide annoyance towards the federal government ruled by the BJP. [1] https://news.ycombinator.com/item?id=38968542 reply darth_avocado 17 hours agorootparentCorruption is definitely part of it, but I wouldn’t blame it purely on politics. Kolkata in West Bengal definitely has scam call centers but that’s only one of the cities. Gurugram near New Delhi [2], Hyderabad [1], Bangalore [3] and Pune all have scam call centers and all of them fall under the governance of different regional and national political parties. [1] https://timesofindia.indiatimes.com/city/hyderabad/fake-call... [2] https://www.thestatesman.com/cities/delhi/two-illegal-bpos-b... [3] https://timesofindia.indiatimes.com/city/bengaluru/call-cent... reply kccqzy 20 hours agoparentprevThe police does in fact care about phones belonging to Chinese owners stolen in China. They couldn't care less about phones belonging to Americans stolen in America. reply seanmcdirmid 20 hours agorootparentSame with illicit fentanyl: sell it in China -> Death penalty, no mercy. Ship it off to America to be sold cheaply to addicts -> no problem. reply chasil 20 hours agorootparentprevI would prefer to see Apple move more manufacturing to India, citing these theft centers as one of many reasons. Whatever country allows it should lose legitimate commerce as a consequence. My own included, if we are found to be guilty. reply kccqzy 20 hours agorootparentDoesn't solve the problem. The Chinese can still \"recycle\" phones manufactured in India. reply barnabask 21 hours agoparentprevMisconception: police solve crimes. Truth: police protect their employers. reply mrguyorama 21 hours agorootparentMisconception: Police are trained to investigate and solve crime Reality: Police absolutely hate doing the \"boring\" parts of their jobs. Property crime clearance rate is an abysmal ~10% and everyone knows a handful of people who reported a theft, large or otherwise, and got nothing but a police report. Meanwhile they had plenty of time to come to our local supermarket and harass a 6 year old that tried to pocket a candybar. They took him into the security camera room and hassled this kid for several hours, zero parents involved. The American police do not feel required to do their damn jobs, unless it involves physical activity or a gun. The boring stuff, like submitting hundreds of stored rape kits to labs to literally catch rapists, doesn't get done, ever. reply eevilspock 21 hours agorootparentOn any given day, in any police department in the nation, 15 percent of officers will do the right thing no matter what is happening. Fifteen percent of officers will abuse their authority at every opportunity. The remaining 70 percent could go either way depending on whom they are working with. ~ I'm a black ex-cop, and this is the real truth about race and policing, http://www.vox.com/2015/5/28/8661977/race-police-officer reply estebank 21 hours agorootparent> Fifteen percent of officers will abuse their authority at every opportunity. The remaining 70 percent could go either way depending on whom they are working with. This is why I find it bizarre that the behavior of bad cops is minimized by calling them \"a few bad apples\", when the entire aphorism is \"one bad apple spoils the bunch\". reply mrguyorama 2 hours agorootparentBecause the ones saying it ARE the bad apples. It's important to realize millions of Americans see a cop pull a gun and shoot a black guy in a traffic stop after he informs the cop he owns a firearm LIKE YOU ARE SUPPOSED TO DO and just go on with their day. It does not bother them. reply readyman 20 hours agorootparentprevUvalde reply mrguyorama 2 hours agorootparentProtecting children is literally \"boring stuff\" to cops it seems reply seanmcdirmid 20 hours agoparentprevChina simply isn't a police state. More to the point, it is authoritarian, but you also have things like prostitution going on in broad day light. The central and local governments have a very focused attention span, and if what bad thing you are doing is outside of that, they probably won't notice. Just don't sell/buy drugs, or get into fights when drunk. reply whimsicalism 20 hours agoparentprevIf we angled more towards detente with China, they probably would - but why do they have any reason to crack down on crime targeting an 'adversary' state? The US hosts many Chinese criminal financiers and embezzlers that we have no intention of extraditing to China. reply IncreasePosts 18 hours agoparentprevThe law in China is: if it affects social order, then feel the state's wrath. Anything else is a free for all. reply geraneum 20 hours agoparentprevThat “police” in “police state” is a different concept than the police you know of. reply StanislavPetrov 19 hours agoparentprevIf the police in New York clearly didn't care about a crime that was committed here, why would the Chinese care about it 8,000 miles away from where it happened? reply JumpinJack_Cash 21 hours agoparentprev> > cracking down on obvious crime As inconvenient as it is for the author and those in their similar situation this sort of petty crime is about on the same level as bustinng a massage parlor. And honestly it woul hold the same low priority everywhere not just in China reply gadflyinyoureye 22 hours agoparentprevWell this isn’t against a citizen. Also look at the murders going on in China now. reply seadan83 21 hours agorootparentDo you have any supporting citations on the crime/murder rate in China? reply sugarkjube 19 hours agorootparentI was curious after reading this, so I looked it up. Seems murder rate in china is 12 times lower than usa (source wikipedia). Actually murders rate in china is about the same as people killed by police in usa. reply ImJamal 18 hours agorootparentDo we have any evidence that China is telling the truth? reply whimsicalism 21 hours agorootparentprevdo you honestly think the murder rate in China is anywhere close to our own? reply seoulmetro 20 hours agoparentprevMuch like the US or Europe? Yeah... money comes first. reply colordrops 21 hours agoparentprevChina is not a police state, they are a military state. The US is a police state. In China the local cops are generally far more hands off than an in the US. China is more top-down from the national level. In the US, local police are the typical mechanism for control. reply seanmcdirmid 20 hours agorootparentCops are hands off. When they need hands on, they call in the PAP. They are armed and have the riot control gear. China is top-down, except when it isn't, then its very bottom up. Is is sort of dichotomy of a central government with absolute authority but a limited attention span. reply lazide 15 hours agorootparentThe central gov’t is a bit eye-of-Sauron like. When it notices you, you’re in deep shit. But for the most part, it doesn’t notice 95% of what is actually happening. reply Fripplebubby 22 hours agoprevNot from the article, but from the comments below it. > My phone was stolen while I was going through TSA security at JFK and the TSA guy must have been in on it because he wouldn't let me get it even though we were able to locate it using my husbands phone. When we used the find me app later we saw it was somewhere in Queens so definitely wasn't stolen by someone else flying that day. I had never heard of that happening before. Great, another thing to be anxious about at the airport. TSA does not pay very well, so I can understand the impulse. reply mmsc 21 hours agoparentIt happens often. Here's a new one with a video https://www.cbsnews.com/amp/miami/news/tsa-agents-accused-of... reply renewiltord 21 hours agorootparentOh so that's why you have to push it through yourself these days. That way they don't touch the thing and so can't steal anything. reply barnabask 21 hours agoparentprevThis is 11 years old but relevant: https://www.youtube.com/watch?v=aLxsLbl16IM reply ahmeneeroe-v2 21 hours agoparentprevYou understand the impulse to steal because a job doesn't pay well? I don't. I would understand the impulse to go back to school to develop a marketable skill, but not to use the force of the state to steal from people. Edit: Would you still find this behavior understandable if the TSA agent directly stole from passengers as their bags were scanned? What if the passenger in question ignored the collaborating-TSA-agent and moved to confront the thief; would you understand their behavior if they arrested the iPhone owner, you know, since they're underpaid. Would you accept the same behavior from a local NYPD beat cop? They aren't paid well when they first join the force, can they steal from random people too? reply joe5150 20 hours agorootparentIt seems willfully obtuse to read \"understand the impulse\" as \"excuse the behavior\" in this context. reply ahmeneeroe-v2 1 hour agorootparentPlucking an excuse for illegal behavior out of thin air and calling it \"understanding\" seems willfully naive, to the point of providing a fig leaf for the bad behavior. Yes a literal interpretation of the parent comment does not point to \"excusing the behavior\", but I believe a more realistic interpretation was warranted here. reply Fripplebubby 21 hours agorootparentprevI don't accept or condone theft. That's an extreme position. But I do try to understand the way our flawed world works, instead of just wishing that people would see things the same way I do. reply ahmeneeroe-v2 1 hour agorootparentDid it really increase your understanding if you just learned about an issue (TSA-abetted phone thefts) and chose a \"common sense\" reason for that issue (low wages) seemingly without any actual effort? I'm glad you don't condone theft, and even that you seek to understand the world, but I don't think that you've increased your understanding of the world in this case. reply happyopossum 20 hours agoprevThe author claims in a comment that the scammer was able to contact her because Apple leaked her email address with the activation prompt. That's not likely, as that prompt **'s out most of the email address (you get the first letter of the part before the @). She most likely had a 'lost mode' message set for the device. reply onesociety2022 13 hours agoparentAgree - what the author claims doesn’t seem likely. I would be worried about my email address leaking. My email address has my full name. So they can easily find out where I live and work if they spend a bunch of time googling my name. Exposing my Phone number would almost be preferable over exposing my email address in this scenario. reply mdip 16 hours agoprevKind of sparked a thought in another comment but my thinking is that the best action in this situation is to simply not reply to the scammer. The author didn't explain \"how the scammer was texting her\". I don't own an iPhone. On my Android device, they'd have to have my phone number which I don't know how they'd get from the locked phone. Of course, I -- like many -- provide an alternate number on the screen for \"helpful people who might want to return, rather than re-sell, my crappy old Samsung phone.\" And I'd imagine that would be the way to get \"the first reply\". But if you could avoid that[0], it keeps you from feeding a likely gambling addiction, or at least makes the game a little less satisfying. Consider that they've got a box of bricked phones. Every text is a chance to turn one of those into a few hundred dollars. Every reply is a small \"high\" that they're still in the game. Every message that goes off into the Ether, ignored, is \"additional confirmation that this phone is a permanent brick\" and you don't even get the satisfaction of feeling like maybe you intimidated someone a little. [0] And to a lesser extent, even if you can't. I've blocked scammmers and seen the progression of repeated \"Hello?\"s ... somewhere in there it's connected that whatever they're trying to send me is not meeting my eyeballs. Had a funny one that was like 30 messages in a row, followed by \"whatever\" and \"ass hole\" ... I remember thinking it was odd that it was always lower case and that they put a space between ass and hole. Not necessarily like a person with a lack of English language skills, but like a tween just learning how to wield profanity. reply docdeek 14 hours agoparent>> The author didn't explain \"how the scammer was texting her\". In a comment on the article replying to someone pointing this out the author explained that she was receiving iMessages and not simple texts. The scammers identified an email address linked to her icloud account when trying to reset the phone and set her iMessages to that email address. reply KeplerBoy 21 hours agoprevWhat information do thieves get from an erased iPhone and how did they contact the legitimate owner? Does it say something like \"locked and linked to sarah96@icloud.com\" or can it display custom messages like \"owned by sarah xyz, please call +43...\"? Could it possibly get you into situations where people can successfully threaten to physically harm you because your contact information doxxed you? reply pqdbr 21 hours agoparentI assume they just eject the SIM card tray and get your phone number from the SIM by simply putting it into another phone. reply SirMaster 21 hours agorootparentThis person seems to be from the USA and it's an iPhone 14 Pro. The iPhone 14 Pro sold in the USA does not have a SIM card slot. It's eSIM only. reply mr_toad 16 hours agoparentprev“Mark As Lost locks your screen with a passcode and lets you display a custom message with your phone number to help you get it back. You can also remotely erase your device if needed. Your custom message continues to display even after the device is erased.” https://support.apple.com/en-us/108794 reply dmitrygr 21 hours agoparentprevBy default you can set the \"lost mode\" message. many people put their contact info into it, in case someone wants to be helpful. This is how these scammers get this info. There is no other way to get the icloud contact info from a phone with a PIN/password. reply writeslowly 21 hours agoprevThis seems like a lot of work for one (or two, once it got to China) people repeatedly trying to get into the phone. I wonder if this is like debt collection agencies where the stolen phones get repeatedly fenced at a steadily decreasing value, and each new owner has a go at sending out these unlock copypastes until it's clear that it's only value is in being scrapped. reply AndroTux 21 hours agoparentDoesn't seem like a lot of work to me. Sending out that message takes a few minutes, and I would assume that most people would simply cave in and delete it. It's gone anyways, so they don't really lose much, and being threatened does things to people. reply writeslowly 21 hours agorootparentIt stood out to me that after the initial text, they followed up two weeks later, and then once per day after that over a few days from different numbers. I would have expected to see someone send out a few threatening messages on one day and then move on to the next stolen phone when it was clear they weren't getting anything. reply uptown 21 hours agorootparentprevYeah but flying Miami back to the US to murder their whole family does seem like a lot of effort. reply j-bos 17 hours agoparentprevDoesn't seem too much, some people write hundreds of comments/tweets per day with no expectation of pay. reply borbtactics 21 hours agoprev>“hackers” might “contact my family” to, what, send them thousands of photos of my dog?? I'm confused. The attacker doesn't have access to the author's photos or family contacts, right? reply FrancoisBosun 21 hours agoparentThey don’t. They’re using another phone and texting the original owner, threatening them, in order to scare them into unlocking the device. When you boot the “Lost” phone, it asks for the password of the owner’s iCloud account. There is probably an information leak where the original email or phone is presented and the thief can track the original owner through that. reply evanjd 20 hours agorootparentThe original iCloud email address is partially obfuscated “eg. e***@icloud.com”, so it’s unlikely to be useful. For this, they’re most likely reading the phone number from the physical SIM card left in the device. reply krallja 19 hours agorootparenthttps://news.ycombinator.com/item?id=40579119 reply bachmeier 20 hours agoprevSeems that stealing a phone with the wrong information on it would be a good way to end up at the bottom of a river. > watching my phone move around Manhattan before it finally stopped at Rockefeller Center. I didn’t bother confronting the thief. Sure, this person didn't confront them, and most probably wouldn't. The one out of a thousand that does might not be messing around. reply callalex 46 minutes agoparentMost criminals are unbelievably stupid, particularly when it comes to risk assessment. reply BrandoElFollito 8 hours agoprevI do not have an iPhone and I do not understand one thing: how could they text her if the phone was locked? How did they know the number to text? reply blackhaj7 21 hours agoprevIs it true that if you remove it from Find My iPhone then they are able to unlock it? I didn’t see any warnings of this when I erased my phone after I recently lost it (and someone found it but decided to keep it) reply bri3d 21 hours agoparentThe Activation Lock remains if you use Remote Erase - all documented on Apple's site: https://support.apple.com/en-us/108794 Only Remove Device specifically deactivates Activation Lock. reply titanomachy 21 hours agoparentprevYeah, when I sold my old iPhone I wiped it and removed it from my account so that the new owner could use it. The site I was selling on warned me multiple times that it would be unusable without that step. reply dmitrygr 21 hours agoparentprev> Is it true that if you remove it from Find My iPhone then they are able to unlock it? Yes, as long as it is in your account, nobody can activate it. When you remove it, it can be activated by someone. reply qingcharles 12 hours agoprevI was given a huge box of locked iPhones from someone who ran a bar and had stored all the ones that had been accidentally left behind and never collected. Out of about 20+ phones the only one I could get working was an iPhone 7. reply jpmattia 19 hours agoprev> As long as I didn’t remove the phone from my Apple account or the Find My app, the phone was essentially bricked to anyone without the passcode and my iCloud password — unusable by the thieves, or the fences who I assume bought it from them. I did not know this, and I wish it were better publicized. reply t_mann 12 hours agoprevFortunately, I've never had my phone stolen so far and I lack the OP's knowledge about how this typically plays out. But I don't find these messages as amusing as she does, and I might actually have gotten fooled by the first message eg. Can someone explain what is happening here? How can the thieves even contact her? What do we need to assume they know about her (eg name)? reply 123456atx 21 hours agoprevthis happened to me once in the mexico city airport ~2010. my broken macbook went missing in check baggage then a guy that was working at the airport messaged me on facebook 8 months later asking for the password. lol! reply roydivision 12 hours agoprevDumb, pre morning coffee question - How does the thief know your number to text you if they can't unlock the phone? reply inferiorhuman 12 hours agoparentIf you set your phone to lost mode it'll offer to let whoever has the phone contact the owner. This is done without unlocking the phone and AFAIK is done without revealing your contact info. But… it's easy (and reasonable) to think that whoever has your phone also has your contact info which makes these sorts of scams that much scarier. reply roydivision 11 hours agorootparentGreat, thanks. reply EVa5I7bHFq9mnYK 12 hours agoprevDoes it work with Android phones too? I got robbed of my Samsung in Brazil, tried to remotely lock and erase it, and removed it from Find My Device. Was that a mistake? I also submitted IMEI to some stolen phones database, got a police report and received insurance compensation. What's the correct course of actions? reply Aardwolf 12 hours agoprevStupid question but how are they able to message the original owner? What contact info does a wiped (or is it just locked?) phone give? Is it via the phone number on a physical SIM card? At least my phone (android) shows no contact info when locked as far as I can tell, and an eSIM would prevent knowing the phone number right? reply Zefiroj 20 hours agoprevOstensibly huaqiangbei has a few ways of getting around the activation lock. But a more common scenario is the phone gets used for parts. As for the icloud account. AFAIK you used to be able to get the full account email with GSX. Not sure about now. reply sh1mmer 19 hours agoprevIf the phone was locked, passcoded, remote erased and sim-locked how did they know her number? I’m not doubting the story I’m just curious how they figured that out. reply remram 13 hours agoparentI was wondering the same thing. Maybe it was on the lock screen? Seems unlikely that they managed to remember its association with the phone throughout, though. edit: we have the answer https://news.ycombinator.com/item?id=40579230 reply raziel31 13 hours agoparentprev> As you can see, most of the phones she tried didn’t have passcodes but were still linked to iCloud accounts I guess she didn't have passcode on. reply sugarkjube 19 hours agoprevIf you can set the device to \"lost\", couldn't apple make it beep 100% of the time? Wouldn't that be a deterrent for thieves? reply pedalpete 19 hours agoparentYes, but that would also be awful for all the people around your phone when it may not have been stolen and could have just been actually lost. reply consf 5 hours agorootparentBut if phone is lost the sound actually can help you to find it. reply nielsbot 19 hours agoprevThe Gothamist page is returning a 500 error now. There's a version on archive.org tho https://web.archive.org/web/20240604225617/https://gothamist... reply daft_pink 19 hours agoprevWhat happens when you do this? Does the old device just stay on your account listed as a device forever? reply consf 5 hours agoprevLosing a phone is just a nightmare for me... reply chemmail 20 hours agopreviCloud removal seems extra hard nowadays. I gave my dad a new ipad and took his old one and signed out of his account and put mine instead. A few months later i had it sitting dead and turned it back on a million icloud login messages popped up with his account then mine then his a bunch of times. His account was back on! I took it back to him to try and remove it from his phone and it took a lot of tries. I think the end combo i had to wipe the ipad and then it appeared back on his icloud list and remove it again. Something similar happened to another friend who i was trying to repair. reply OnionBlender 19 hours agoprevIsn't it illegal to knowingly buy stolen property? (I'm referring to the youtuber that bought the stolen phones). reply 1024core 20 hours agoprevHow did they get her number, to message her? reply betaby 17 hours agoparentPhysical SIM card? reply humanlity 15 hours agoprevif something involves China, the topic will turn to politics or something else, but the real question is: how to stop the stealer and why this can safely ship to China reply paulcapewell 11 hours agoprev> fence Had never come across this term before. Huh. reply seneca 19 hours agoprevWhat a poorly written article. It could have been an interesting profile of what scammers do to scare people into undoing security measures, and how to avoid that. Do these outlets not have editors? I'm surprised anyone would publish this. reply StanislavPetrov 19 hours agoprevThis episode makes me thing that in addition to \"lost\" mode you should be able to set your phone to \"brick\" mode when you know you have no chance at getting it back so the phone itself is becomes nonfunctional and your data can never be accessed. reply consf 5 hours agoparentIt's a good idea. And if you already buying a new one, so you can definitely go to the \"brick\" mode reply flemhans 18 hours agoprevThat escalated gradually! reply throw310822 21 hours agoprevActually, they should have tried with \"ignore all previous instructions\". reply imglorp 21 hours agoprevSo, what's the right procedure here? Is the dichotomy in the post true? a) remove from account and help the recycle crooks, or b) risk them crack it having your data Aren't there crack wares at this point in the wild? reply anyfoo 21 hours agoparentI don't think b) is realistic at all. They're just using scare tactics. https://help.apple.com/pdf/security/en_US/apple-platform-sec... reply krackers 21 hours agoparentprevIf it's wiped then there's no data for them to get? reply ipython 21 hours agorootparenthow did the thieves get the phone number of the author, then? is it on the lock screen when the phone is wiped, in order to facilitate its return? reply ovalite 21 hours agorootparentFrom the comments, the author was contacted via iMessages from the iCloud email that shows up when it asks for a password. reply happyopossum 20 hours agorootparentIt doesn't show the full email, it's got the middle of it **'d out. OP likely had a message set for lost mode. reply happyopossum 20 hours agorootparentprev> is it on the lock screen when the phone is wiped, in order to facilitate its return? Yes - you can set a message when you put a device in 'lost mode' in order to facilitate it's return if it's truly lost and found by a decent human being. reply MadnessASAP 20 hours agorootparentprevThat would seem to be the case, the device is wiped of any user data. Except for the lock, leaving the device as a useless brick / spare parts. reply muffinman26 21 hours agorootparentprevPresumably the phone hasn't actually been wiped, or wasn't when the crooks first got the phone number. Smart phones generally let you add an Emergency Contact number that anyone can use the phone to call/text without entering a password, in order to facilitate an honest person returning the phone or someone contacting your family if you are found incapacitated. reply kccqzy 20 hours agoparentprevThe article says the author already performed a remote erase. There's no data recoverable. reply christkv 20 hours agoparentprevSend them Tiananmen Square photos or messages about the CCP, anything that can trigger the great firewall. Thats what my friend did. They stopped messaging immediately. reply srockets 21 hours agoparentprevThere are, but they aren't cheap. For a recent iPhone, expect to pay a seven digits price. reply ugh123 20 hours agoparentprevIs there not a 3rd option for iphones to remotely wipe and brick further use? reply searealist 16 hours agoprevThis is a fake story. > Veronica leads the digital news and audience team at WNYC and Gothamist, which includes breaking news, newsletters, and social, among other things. I expect such fake stories on the front page of reddit, not HN. reply remram 13 hours agoparentI don't know, millions of phones are stolen every year. Seems likely that some reporter's would be stolen once in a while. It's a little too mundane to be made up. reply katamarimambo 6 hours agoparentprevI do not get the link between the quote and the accusation reply shrimp_emoji 16 hours agoparentprevThis. Have an updoot my good sir Nice try, FBI. Bold of you to assume I have friends. ¯ \\ _ ( ツ ) _ / ¯ EDIT: Thanks for the award, kind stranger! reply catlikesshrimp 20 hours agoprevIt wasn't even an Iphone 15. It's an iphone 14 in March. I doubt a citizen in a first world country can become a target of an international cartel for $1000 at most. Not /s: Imagine the logistics and the cost of opportunity reply Zuiii 14 hours agoprevIronically, Apple's monopolistic and predatory restrictions on spare parts will still allow this scumbag to make a profit, even after he has failed to access her phone. Had Apple allowed people to access parts, her phone would be absolutely worthless. Apple's abuse of our laws is what's making iphone thefts still profitable. Ironic. reply localfirst 21 hours agoprevlong time ago i had my samsung galaxy stolen ive not reported as stolen because my carrier said i needed proof of the receipt (which i lost years ago) its such a crappy experience i dont wish it on anyone. Samsung has been next to useless in finding it. having said that this article makes me glad I bought an Apple just wish they'd kept the hardware touch button on the non-SE phones and back button. this is the first iPhone for me but i keep searching for the \"alt+tab\" and back button. it was ridiculously hard to switch between apps (have to physically press the touch button twice which makes me lose grip on teh phone). honestly its so close to being perfect but the iPhone has some obvious interface flaws coming from Android reply nebula8804 13 hours agoparentIf I am understanding you correctly, you are having trouble with the \"double tap\" of the Touch ID models right? This operation was much easier on the smaller iphone SE. They probably designed it for that design(iphone 4-5S) due to its smaller size that allowed one handed operation and then carrying it over to the current SE design(iphone 6-11) is probably not ideal for that form factor(thinner and rounder). I believe the FaceID models solve this by having you slide the app up slightly to then select between it and any other open apps or go back to the home screen. Maybe that would work better for you? Also, this is just my opinion but like I mentioned, the iPhone SE Gen 2+ uses the last gen design language which started back in 2014(iphone 6) and has been gone from the main line for years now. It always felt super slippery and prone to dropping(too thin and those rounded edges are difficult to grip) which is why I held onto my iPhone SE Gen 1 until the new iPhone 12 came out. This is just my opinion but maybe you should go to a store and check out their other models. reply hahamaster 20 hours agoparentprevYou're just used to Android. It's hard to put buttons on a phone that's nearly all screen. reply localfirst 16 hours agorootparenton the iphone SE there is ample space where the speaker is and at the bottom I don't see why they couldn't have added a back button its so ridiculous that I have to tap twice in the back and three times to activate \"alt+tab\" it doesn't even work consistently when you have a case. I know this is based off the old phone but geez I had no idea it was this bad. on the bright side im very pleased with the software experience. just ties everything nicely together right out of the gate. Apple is definitely for a different crowd. I'm not sure if I will buy another iPhone after this I just realized as a long time Android user, I didn't miss much, in fact I would've given it up long time ago. reply blackeyeblitzar 21 hours agoprevWhile sort of funny, it is also crazy and disappointing the extent to which these scammers go. It’s not fun receiving death threats and having that stress over you. I wish we had more the law could do in bringing these criminals to justice. reply remram 13 hours agoparentFor real, without a strong sense of resentment, you would probably give up just to stop the texts. It's not like you lose anything further (apart from increased chances of theft in the future for everyone I guess). reply dmitrygr 21 hours agoprevAnd THIS is why i support parts pairing. 3 years ago, the LCD and battery could be resold making this worth it. This year - nope. I dream of a world where even the little screws that hold my iPhone together are parts-paired so stealing it costs more in effort than the possible $ recovered from it! reply oynqr 21 hours agoparentThe part pairing isn't the problem, it's that you can't unpair parts. Creating a secure unpair process would not be beyond Apple's capabilities. reply happyopossum 20 hours agorootparentEnd-to-End Encryption isn't the problem, it's that you can't intercept when needed (by the government only of course). Creating a secure intercept process would not be beyond Apple's capabilities. reply yreg 21 hours agorootparentprevYes, it would be great to have the best of both worlds. reply ahmeneeroe-v2 21 hours agoparentprevAgreed. Anti-theft is a higher priority for me than repairability. reply mqus 20 hours agorootparentfine, pair the parts, but also: let the owners unpair them, same as you can lock/unlock your phone. Voila! both repairability and anti-theft gained! Pairing is just anti-repair atm. if you want to know how: put the part ids (which are used for pairing) into the apple id account and verify them when switching on the phone. reply mqus 20 hours agorootparentjust noticed: re-pair-ability is basically all we want (pun intended) reply BizarroLand 21 hours agorootparentprevI value autonomy more. I'm not someone interesting enough to observe, but I still want my privacy. I'm not so poor as that I have to take whatever I can get, but I still want to own what I buy, and have the freedom to do anything with it, including crimes if I so choose. I won't commit crimes, and if I do I will pay the price, but I don't want a nanny to prevent me from doing precrimes that I would not have done in the first place. I am not a mindless sheep, I require no societal shepherd. All of that was said to say, I want the devices I purchase to be repairable by anyone that I deem worthy of repairing it, including myself. reply asveikau 20 hours agoparentprevThe world you describe is much worse for the environment. reply Hizonner 21 hours agoprevWhat I'm seeing here is that Apple has a really shitty, poorly thought out design. To keep the phone useless to somebody who steals it, you have to keep it exposing your contact information so that people can threaten you. reply pqdbr 21 hours agoparentThat's not my understanding of the situation at all. The journalist: - Marked the phone as lost; - Wiped all data; FTA: As quickly as possible, I did all the things you’re supposed to do when your phone is lost or stolen —- mark it as lost, cut off service, and remotely erase it She did not, however, remove (delete) that iPhone from her iCloud account, which means that the iPhone can't be factory reset by the thieves because \"FindMy\" is still activated. FTA: As long as I didn’t remove the phone from my Apple account or the Find My app, the phone was essentially bricked to anyone without the passcode and my iCloud password The iPhone did not, at any time, leak any personal information. What probably happened is that she had a SIM card on it, and from ejecting the tray and putting the SIM into another phone they managed to get her phone number, from which they started contacting her with the threats/scams. I think this wouldn't even be possible with a e-SIM in the newer iPhone models. reply gbuk2013 21 hours agorootparentShe actually answered this in one of the comment threads on the article: when accessing the phone it will prompt you to enter the password for the Apple ID which happened to be her email address which happens to be able to receive iMessages. https://www.openweb.com/share/2hQferBhnDlJgGo4oE9Nv6wGjAX reply happyopossum 20 hours agorootparentShe's wrong - the activation prompt ***'s out most of the email address. She likely had an emergency contact or 'lost mode' message on the device. The latter would still be present if remote wiped. reply nanidin 14 hours agorootparentOr she had an email address like {firstname}@{firstnamelastname.com}. reply TheDudeMan 21 hours agorootparentprevThanks for this informative post. I was a little confused as well. Right, now we have e-SIMs. So what was the reason for physical SIMs in the first place? reply kderbe 21 hours agorootparentpreviPhone 14's sold in the US do not have a SIM tray. They are eSIM only. reply kelnos 21 hours agoparentprevIn the case that I lost my phone, though, instead of it being stolen, I really might want my contact info displayed on the lock screen so someone who finds it can get it back to me. Yes, I know, in many anti-social places in the world (like most US cities), it's much more likely that the phone was stolen, or that even in the lost-phone case, the person who finds it will be shitty and not try to return it. But in many (most?) other places people are generally honest and will try to return something that's lost. Then again, if you know that it was stolen, you should be able to keep the thief from getting your contact info. reply uptown 21 hours agoparentprevFrom the article comments: “when trying to reset my phone they are prompted to enter my iCloud password. My iCloud email is also one of the ways to reach me via iMessage. So they used that to contact me” reply gabolaev 21 hours agoparentprevHow exactly can they get the phone number though? The only thing I can think of is if there was an unlocked (no PIN) physical SIM-card, and they could just insert it into another phone. And yes, e-SIM solves this problem. reply operator2140 9 hours agorootparentOrganized crime networks have insiders/hacked machines at the phone carriers where they can perform IMEI lookups in the carrier database to get the subscribers phone number. reply neaden 21 hours agorootparentprevI think in this instance they got the email associated with the iCloud and messaged her on that. Edit: she replied to a comment on the article: \"when trying to reset my phone they are prompted to enter my iCloud password. My iCloud email is also one of the ways to reach me via iMessage. So they used that to contact me\" reply bri3d 21 hours agoparentprevThe Lock Screen displays what you ask it to. When you mark a phone as Lost or perform a Remote Erase, you can push down your contact data if you'd like (ie - if you think a \"friendly\" found the phone and you want it back), or you can elect to type nothing, an offensive message, etc. Likewise for the \"Medical ID\" feature (which is also available with the phone locked) - you can elect to display lots of details, some details, or no details at all. reply INTPenis 21 hours agoparentprevWell this also applies if someone friendly finds your phone and wishes to return it. It's a catch 22 there. reply BeefySwain 21 hours agorootparentReturn it to an Apple store? Apple knows who to contact. reply coredog64 19 hours agorootparentI know Apple doesn’t really have the financial incentive for this, but some kind of prepaid mailing label you could print and use for this would be an option if there’s not a nearby Apple store. reply filleduchaos 21 hours agorootparentprevAnd if there is no Apple Store around? There are entire countries without those, and even more that only have a few total. reply yreg 21 hours agorootparentprevMost countries, including mine or the ones I'm likely to travel to, don't have Apple Stores. reply mlyle 20 hours agorootparentIf my phone gets stolen from my pocket in the US, the chance of it wandering to the third world and then a Good Samaritan trying to give it back seem abysmal. reply spookie 20 hours agorootparentBut what if you don't live in the US. You actually live in a \"third world\" country? These are the type of things that make it very hard for Apple to penetrate markets outside the US. reply mlyle 14 hours agorootparentWhat we're discussing: you can put any message you want on the phone for how it can be returned to you. Some people are mentioning that if Apple is given the phone, Apple will also help (and that they'd prefer to not put that message in place as a result). > These are the type of things that make it very hard for Apple to penetrate markets outside the US. Can you explain how this is much worse than, say, Google or Samsung? reply yreg 8 hours agorootparent> Some people are mentioning that if Apple is given the phone, Apple will also help No, they will not. It is just a wish. And it makes little sense for Apple to be doing that. If you bring a found iPhone to Apple, Apple will not tell you any information about the associated customer. They don't have any infrastructure to return it. The only thing they can and will do is take the device and recycle it. reply 4 more comments... GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Veronica de Souza, a journalist, had her iPhone stolen and subsequently received scam texts from the thieves trying to unlock the phone.",
      "The scam messages included fake Apple Pay alerts and threats to auction her personal information on the black market.",
      "De Souza shared her experience to highlight the bizarre and often comical nature of these scams, despite their serious intent."
    ],
    "commentSummary": [
      "A Gothamist user received texts from thieves attempting to unlock their stolen phone, leading to a discussion on using messages about the Tiananmen Square massacre to trigger Chinese censorship as a deterrent.",
      "The conversation raised concerns about the risks to recipients' families in China, WeChat's societal role, and Apple's device security and repair policies, including the integration of parts with the motherboard to deter theft.",
      "Despite Apple's anti-theft measures, iPhone theft remains prevalent, with stolen phones often stripped for parts, sparking debates on repairability, environmental impact, and the effectiveness of security features like \"Find My iPhone.\" Law enforcement's indifference and the need for better legal measures and user education were also discussed."
    ],
    "points": 434,
    "commentCount": 266,
    "retryCount": 0,
    "time": 1717531823
  },
  {
    "id": 40584135,
    "title": "Israel Used Fake Accounts to Influence US Lawmakers on Gaza War",
    "originLink": "https://www.haaretz.com/israel-news/security-aviation/2024-06-05/ty-article-magazine/.premium/israel-secretly-targeted-american-lawmakers-with-gaza-war-influence-campaign/0000018f-e7c8-d11f-a5cf-e7cb62af0000",
    "originBody": "Israel Secretly Targeted American Lawmakers With Gaza War Influence Campaign In an attempt to sway global public opinion on the war in Gaza, fake accounts and sites spread pro-Israel and Islamophobic content. The operation was orchestrated by Israel's Diaspora Affairs Ministry and run by a political campaigning firm Share in Twitter Share in WhatsApp Gift this article Send in e-mail Send in e-mail Print article Save Save article to reading list Zen Read Omer Benjakob Get email notification for articles from Omer Benjakob Follow Jun 5, 2024 1:08 pm IDT The Israeli government is behind a large-scale influence campaign primarily aimed at Black lawmakers and young progressives in the United States and Canada. The operation, whose existence was first reported by Haaretz in March, was launched after the start of the war in Gaza and was intended to sway certain segments of public opinion on Israel's conduct. Comments Name Enter the commenter display name Comment By adding a comment, I agree to this site’s Terms of use Send In the News Police Arrest pro-Palestinian Students 'Occupying' Stanford President's Office Targeting Radiohead: Why the Boycotters Demonizing Israelis and Jews Won't Silence Me Why Israel's North Won't Be the Same After the Gaza War Iran Threatens Israel 'Will Pay' for Alleged Israeli Strike That Killed Adviser in Syria Israeli Court Shortens Length of Government-ordered Shutdown of Al Jazeera in Israel Learn How to Optimize Your Home Solar System Paid by Sela Infrastructures LTD",
    "commentLink": "https://news.ycombinator.com/item?id=40584135",
    "commentBody": "Israel reportedly used fake social accounts to garner support from US lawmakers (haaretz.com)359 points by frob 6 hours agohidepastfavorite228 comments throwaway55479 7 hours agohttps://archive.ph/sbAPI zdragnar 0 minutes agoprevI was part of a company that \"hunted\" terrorist groups that did this. Start with sympathizers publicly posting on Twitter, find who they are connected to, and fan out and cross reference to the people who are either organizing violence or running drug operations for funding. I assume most countries with designs in foreign politics do much the same. reply SimbaOnSteroids 42 minutes agoprevThe wild part about this, at least to me, is the wholesale incompetence demonstrated by Israel in this regard. If I couldn't google the talking points the bots make and see Israeli officials saying the same things, one would think these bots were Iranians acting with the intent to make Israel look bad. reply xkcd-sucks 5 minutes agoparentEven the spooky spy and torture people choose the crappy low bidding implementation partner reply myth_drannon 0 minutes agoparentprevThe Israeli government is incompetent and is non-functioning. Disaster PR campaigns are not the worst thing they have done. But that's the beauty of democracy. In a dysfunctional government, most institutions continue to function and hold the country together. The citizenry is pulling through despite headwinds. IDF after the initial shock is doing great, despite US government making everything possible to fail it (but that's a political battle). reply HL33tibCe7 11 minutes agoparentprevThis is presumably just the tip of the iceberg reply refulgentis 13 minutes agoparentprevIt's a very interesting thing, it demonstrates something uncomfortable & scary to me as a goy Zionist, who hangs out in a private, predominantly Jewish, space. Note the biggest word in the cloud: UNRWA. All my confirmation bias was in one direction in October. The oddly dissonant and desperate messaging you'd see made things extremely difficult to maintain that, like, you have to be of a very specific mindset to see message after message about the evil UN and not say, \"uh, did we go off the rails somewhere?\" (n.b. this was in a lefty Jewish space, broadly denigrating governmental institutions isn't a usual virtue signal) Going back to the beginning, there's an uncomfortable willingness/ignorance of Overton window widening, in a way that reduces sympathy rather than engenders it, and all of a sudden, otherwise kind people are engaging in rank racism*, glorification of destruction, and extreme conspiracies**. * lots of \"no such thing as innocent Palestineans\", \"Palestineans love redacted\", when questioned, turns into \"it's not racist if they're not a race, and they aren't because bla bla bla\" ** Day after day after day of the bailey, \"World Central Kitchen was trying to smuggle terrorists\", coupled to the motte \"Jose Andres held a barbecue buffet! Lol!\" reply Metacelsus 4 hours agoprevThis was related to OpenAI announcing that they had shut down several covert influence campaigns using ChatGPT-generated social media posts. https://openai.com/index/disrupting-deceptive-uses-of-AI-by-... reply dang 2 hours agoparentDisrupting deceptive uses of AI by covert influence operations - https://news.ycombinator.com/item?id=40526068 - May 2024 (70 comments) reply sva_ 52 minutes agoparentprevThis article itself seems to be written by chatgpt to some degree at least? I've developed trust-issues with bullet point lists in that format. reply rfw300 11 minutes agorootparentIf anything, OpenAI's affection for bullet lists is probably a window into why ChatGPT uses them so frequently. reply SkipperCat 6 hours agoprevNot a comment about who's right or wrong in this war, but it is fascinating that we have entered the age of the Internet being a place where warfare is fought. There have always been people posting web content about conflicts but now with Gaza and Ukraine, it seems that the nations fighting are actively looking at the internet as the fourth field of battle. Just waiting for a random US future president to create an \"Internet\" branch of the military. Maybe that's already happened. reply dfxm12 1 hour agoparent\"Manufacturing Consent\" was written in the 80s mostly in response to newspapers, but the ideas have been adapted to the Internet for some time (and talk radio, and cable news, etc.). I'm old enough to remember this from the Iraq war. Yeah, we didn't have microblogging back then, but there were Email campaigns, blogs, message boards, chat rooms, etc. reply bostik 14 minutes agorootparentAnd let's keep in mind that the term \"Public Relations\" was explicitly chosen as a Newspeak-term because Edward Bernays realised that the actual term for a war time methodology, \"propaganda\", was too loaded.[0] And honest. Internet is a communications medium. It was destined to be flooded with propaganda, whatever you try to call your particular flavour. Or as I have been saying since the 1990's, the only difference between marketing and propaganda is that with marketing at least you are trying to peddle a product instead of an ideology. 0: https://en.wikipedia.org/wiki/Edward_Bernays reply Terr_ 5 minutes agorootparent> Or as I have been saying since the 1990's, the only difference between marketing and propaganda is that with marketing at least you are trying to peddle a product instead of an ideology. I disagree, ideologies are often already in there, even when they are simplistic \"power-tools are for men and all men require power-tools\", or \"having better stuff than your neighbors is a virtue, failing to do so will lead to dangerous ostracization.\" Very tame \"Our blender spins twice as fast as the competition\" marketing might be arguably free of ideology, but that's a decreasing minority. reply dylan604 4 minutes agorootparentprev> \"propaganda\", was too loaded.[0] And honest. Quite often I default to the word propaganda when talking about anyone's PR campaigns in my own personal battle with trying to undo this. I ratchet it up when talking directly to marketing/PR people. Pretty much every time I'm just looked at as yet another crazy person. reply akudha 26 minutes agorootparentprevPropaganda, false news etc are as old as time. It was the radio, TV and newspapers before, now it is social media and the internet. The difference now is the speed, cost and scale. It is super cheap to spread crap today than ever. Also it is quick and the reach is massive. By the way, Manufacturing Consent is a depressing book. You’d lose what little faith you have in media, if you read it… reply SCUSKU 21 minutes agorootparentI think one of the big takeaways for me was aside from deliberate manipulation of media by the government and willing media partners, that journalists also self censor in a way because they are operating in a professional environment and within a certain Overton Window. Maybe it's not what I should remember most, but it did help remind me that when your livelihood is based on what you say you will be much more measured, regardless of the subject. Probably why people look to social media or Substack for more independent people who have a longer leash, less on the line, and more to gain, since that's where you get your interesting although many times wrong takes (e.g. Ivermectin for Covid, or Lab Leak Theory) reply tintor 5 minutes agoparentprevUS already has https://en.wikipedia.org/wiki/United_States_Cyber_Command reply vitus 2 hours agoparentprev> Just waiting for a random US future president to create an \"Internet\" branch of the military. Maybe that's already happened. https://en.wikipedia.org/wiki/United_States_Cyber_Command is the closest thing that we have today. It's not a formal branch, though, but rather a joint effort across the existing branches. reply jowea 1 hour agorootparenthttps://en.wikipedia.org/wiki/List_of_cyber_warfare_forces A cursory look and it seems Germany and China were first to having a specific branch, but China dissolved theirs https://en.wikipedia.org/wiki/Cyber_and_Information_Domain_S... https://en.wikipedia.org/wiki/People%27s_Liberation_Army_Str... reply lucubratory 1 hour agorootparentprevThat's much more oriented to network security, spectrum and hardware, stuff like that. For an American military organisation engaged in internet influence operations you'd want to look at the signature reduction program. Something like 50,000 people strong at this point, insane amounts of resources going into that. reply germinalphrase 5 hours agoparentprevEspionage/propaganda/public relations/influence campaigns are hardly new. Social media is just a new flavor to go along with the others. reply marginalia_nu 4 hours agorootparentI do think the economy is different. You've always been able to just hire a bunch of thugs to stage an event to shape the narrative, like old-school cold war style. That takes money and effort and a modicum of skill and the risk of being caught with your pants down is not negligible. Difference today is you can stoke the flames of public outrage with just a few people, without even setting foot in the country, while maintaining a lot of plausible deniability, since the modern playbook relies heavily on uncertainty and confusion, meaning you can safely target allies without significant risk of being caught (even if you're caught, you can deny it and say it's hostile propaganda). reply vharuck 14 minutes agorootparentPeople also seem to discount the effects of internet operations by enemy states. For example, in 2022, the FBI blamed the state of North Korea for a string of hacks on US health systems. The \"meatspace\" equivalent would've been North Korean operatives infiltrating dozens of hospitals and destroying records or supplies. If that had happened, there would've been a bigger response from the government than \"Mind your physical security, hospitals.\" But it's the internet, so who cares (besides the people immediately affected)? reply somenameforme 1 hour agorootparentprevThis seems reasonable, but it runs into a little problem. If you engage in political discussion anywhere on the internet, the first thing you'll find is that people, if they have formed an opinion, have exactly 0 interest in changing their mind. If you already hold a genuine and internally formed view on e.g. the Israel - Palestine conflict, then even if somebody sat you (or me) in front of 24/7 propaganda for the other side, they'd be unlikely to ever change either of our minds. Propaganda only seems to work in two situations. The first is on topics people know nothing about. Each time the US invades some places most people couldn't even find on a map, support for it rises in accordance with the propaganda. But as people learn more, and gradually form their own values, that support tends to rapidly decline. And there are also long-term consequences, because people will remember being lied to. My views on the US war machine and geopolitics in general seem unlikely, at this point, to ever change. And they were largely formed due to the Iraq War. Irrefutable [1] and Undeniable [2] are two 21 year old articles I still go back to on occasion. The other situation is when it's true. During the Cold War we spread endless propaganda about things like having stocked store shelves. This is doubly effective in the same way that lying propaganda is doubly ineffective. Because not only does it create a desired perception, but once people gradually find out it's really true, it also tends to turn them against their own government who invariably misrepresents such situations. Again, people don't like being lied to. [1] - https://www.washingtonpost.com/archive/opinions/2003/02/06/i... [2] - https://www.nytimes.com/2003/02/06/opinion/irrefutable-and-u... reply AnimalMuppet 1 hour agorootparentprevEven in the old days, if your operation was caught, you could always claim that it was an enemy false flag. (And if it was your false flag and you were caught, you could always claim that it was an enemy provocation.) reply paul7986 2 hours agorootparentprevIndeed and one reason i don't watch or pay attention to news media(TV, online, etc) especially political news. What to believe is real / the truth and with the advent of AI, Deep fake voices and deep fake videos the Internet becomes an even worse place for deciphering truth. Here's AI Trump and AI Biden debating live now on Twitch (video isnt great as of today but the voices are) https://m.twitch.tv/videos/2157689323 reply wruza 1 hour agoparentprevThe internet created a whole stratum of people who don’t use tv, radio and newspaper anymore. It’s not that we entered internet warfare, we just exited absolute control of large mass media. Now every TLA has to deal with it somehow. Why internet is the battlefield? Because everything in our world is based on an opinion. You can sell a lot of bs to your “client” if he has “correct” opinion. Bad news, our opinion system was designed for groups and villages, not for the internet. reply pradn 1 hour agoparentprevThere's a whole term for this: https://en.wikipedia.org/wiki/Fifth-generation_warfare reply anigbrowl 1 hour agoparentprevWe entered that some time ago; or rather, the Internet accelerates the use of such information operations. This is (imho) why Musk bought Twitter. reply axus 2 hours agoparentprevI've always been a keyboard warrior, volunteering to defend my country on message boards. reply xanthor 1 hour agoparentprevThe Internet as we think of it is already a military project. Why do you think so much emphasis is put on countries that assert sovereignty over their own information space? reply sva_ 49 minutes agoparentprevWasn't there something with the Canadian military fighting (what they called) misinformation on social media during the pandemic? Seems like it's already ongoing. reply betaby 32 minutes agorootparentCanadian government was the source of misinformation on social media during the pandemic! Literal curfews were in place with propaganda machine saying how good idea it was. reply dylan604 0 minutes agorootparentFor purposes of conversation and allowing for a moment your idea is true, to what purpose was the curfews imposed? Who benefited? How? Why were the curfews necessary to achieve those goals? runarberg 3 hours agoparentprevI’m of the opinion that what we are witnessing is the first information age genocide. Just like how the holocaust was the first genocide to use industrial technology and processes to conduct the horrors, today, Israel is using information age technology to commit and propagandize their genocide. reply dang 1 hour agorootparentYour account has continued to use HN primarily for political battle after we asked you recently to stop: https://news.ycombinator.com/item?id=40519369 (May 2024) If you keep this up we're going to have to ban you, for reasons explained on many past occasions: https://hn.algolia.com/?sort=byDate&dateRange=all&type=comme.... Edit for anyone concerned: yes, this principle applies regardless of which side of any political conflict an account is identified with. reply runarberg 31 minutes agorootparentI’m sorry, I did not mean for this comment to be a political point, but rather an observation on how technology is used in mass atrocities. I was hoping to raise a point which I find interesting, which other may or may not agree with. I’ve gotten a couple of excellent replies here raising interesting counterpoints. After posting this, and reading the replies, I’m actually less convinced about my original point. That is, I’ve learned something. reply oddtuple 45 minutes agorootparentprevDang, We get you’re frustrated but he’s just stating his opinion. It’s not out of line relative to the other discourse in this thread. reply rendall 24 minutes agorootparentprevI'm not sure what's happening with the HN algorithm, but these anti-Israel, non-technology-related posts keep making the front page. runarberg's comments are just a symptom. reply dragonwriter 55 minutes agorootparentprevIts not at all, even if you mean “social media age”, and not “information age”, it's just one of the first (there are other disputed candidates, e.g., in Ukraine) that are getting first world attention other than after-the-fact. The Rohingya genocide in Myanmar in which Facebook’s role was widely discussed (largely, in the first world, after the fact) was probably the first social media age genocide, if you don't restrict it to ones with immediate first-world attention at a significant level. reply runarberg 37 minutes agorootparentI’m thinking in terms of processes and propaganda. While other genocides use information technology for communication and propaganda, this one is unique in that information technology is used throughout, including in target selection and killings. The Rohingya Genocide does not e.g. use drones to carry out killings with targets selected by AI. reply dragonwriter 7 minutes agorootparent> While other genocides use information technology for communication and propaganda, this one is unique in that information technology is used throughout, including in target selection and killings. No, its not. Heck, the Holocaust used information technology for target selection. > The Rohingya Genocide does not e.g. use drones The genocides in the former Yugoslavia used most of the weapons of then-modern warfare, which may not have included drones but certainly involved plenty of weapons systems that incorporate \"information technology\" in doing the killings. reply megous 1 hour agorootparentprevA lot of people have both mechanisms to record what's happening, and share it. It's been that way with Syria conflict, too, though. A lot was shared in twitter/youtube during that one. One thing that's seemingly a bit new is how much ordinary Israeli soldiers are sharing their behavior, empowered by their self-righteousness, I guess. Videos from shooting unarmed deaf people up close in their homes, to all kinds of calls for atrocities, actual assaults on international humanitarian aid trucks and violence against the drivers, cheerful mocking of starving people, dedicating videos of them blowing up peoples homes as gifts to their spouses back home in Israel, looting and stealing, wanton destruction of property (like going around and breaking things in someone's gift shop), burning people's houses down, etc. There's so much of this. Entire 130k strong Israeli telegram channels are dedicated to collective cheering on and mocking of dead and suffering people: https://t.me/s/dead_terrorists Total dehumanization. reply ignoramous 4 minutes agorootparent> empowered by their self-righteousness ... Total dehumanisation Jeez, just like those supremacists of the yesteryears Hollywood made movies to warn us about, then? reply hedgehog 1 hour agorootparentprevTigray region and Mynamar are two earlier candidates. reply jamal-kumar 2 hours agorootparentprevThe holocaust came of age in the dawn of the information age if you count the radio as information technology, albeit a very one-sided information technology where you had the government giving everyone cheap radios that were only marked to tune to German and Austrian radio stations, unless you dared to go out at night to get an antenna up to receive others. [1] [1] https://en.wikipedia.org/wiki/Volksempf%C3%A4nger reply runarberg 1 hour agorootparentThis also applies to the Rwandan genocide. A lot of it was perpetrated via mass media, especially radio. But you can also claim that there were Industrialized genocides before the Holocaust, but what sets it apart is just how much it was defined by industrialized processes. The Gaza Genocide is similar, the use of AI for target selection (or rather generation), the social media campaigns, using drones for killings, etc. We haven’t seen a genocide before which uses information technology to the extent it really defines whole processes of the genocide. reply hedora 6 minutes agorootparentGermany pioneered a lot of modern propaganda techniques in WWII: The first television broadcast on earth was of Hitler, and his chief propagandist, Goebbles, continues to have significant influence on modern propagandists. For instance, Biden's publicly compared the tactics Trump used in the 2020 \"Big Lie\" campaign to those of Goebbles. Of course, there was also the Hitler Youth, which was a pretty successful social engineering campaign. On the computer side of things: IBM mainframes were famously an enabling technology for the holocaust and german war machine. reply tick_tock_tick 1 hour agorootparentprevnext [6 more] [flagged] umanwizard 1 hour agorootparentI am so sick of the claim that if you criticize Israel you must have something against Jews. Jews are a loosely-defined, globally-distributed cultural group. Israel is a specific, concrete sovereign country. It is a bit like saying if you criticize Venezuela then you must hate \"Latinos\". I know for sure that my reasons for criticizing Israel have nothing to do with dislike of Jews. Why? Because I'm inside my own mind, so I would know if I had anything against Jews or not, and I don't. There are plenty of reasons people care more about Israel's actions than those of any random country that have nothing to do with the fact that Israel is populated mostly by Jews, including: 1. It has historical and cultural ties to Western countries, so Westerners feel naturally interested in what goes on there (see also: why people care more about what's happening in Ukraine than in other armed conflicts around the globe), 2. Israel has a much higher degree of influence over American politics than any other foreign country, which bothers people, 3. It is largely propped up by U.S. aid, so Americans feel responsible for it, 4. Because of point 3., it is one of the only global problems that Americans have a realistic chance of solving by protesting. reply WhackyIdeas 1 hour agorootparentprevYou’re right about there being other genocides. But the difference here is that for many in the West, they are seeing their own participation in it (ie USA, UK) with the Germans giving morale support for it. All those American Boeing-made missiles ripping apart and burning alive those little hungry toddlers camping outside in their cold tents… it tends to make people reflect a little more. reply ngruhn 53 minutes agorootparentI have a different theory. There are tons of wars and conflicts in the Middle East. Saudi Arabia vs Yemen, Iran vs Irak, Pakistan vs India, civil war in Syria, civil war in Somalia, civil war in Sudan, and everybody else is fighting ISIS. It’s not like the US/West are not involved or that the conflicts are any less bloody or that the parties have \"better reasons\" to kill each other. And still, they receive no where near as much attention and criticism as Israel/Palestine. I also don’t think the reason is antisemitism (at least in the West). I think the reason is that the West has to view everything through the lens of the culture war: it’s white vs brown and white is evil, therefore Israel is evil. reply WhackyIdeas 28 minutes agorootparentIsrael has created its own perception of itself to the world. They gave up all sense of humanity to go on a revenge spree and now they don’t know when to stop because the whole world sees them as monsters so they probably think it couldn’t get any worse. Changing the convo to talk about some other wars than Israel / Gaza is just another kind of deflection technique to avoid responsibility. If it isn’t the antisemitism card, it’s the deflection card. Sorry, but most of us know it’s true. reply snowpid 1 hour agorootparentprevIn Germany there is no big debate. reply nsguy 1 hour agorootparentprevI think you got this the wrong way around. It's the first time in the information age that a country [is] forced to yield a war via false claims propagated through social media. reply gdsdfe 5 hours agoparentprevwe entered the age ?! we've been here for at least a decade reply shrubble 1 hour agoparentprevEglin Air Force Base and their involvement with Reddit... reply odiroot 1 hour agoprevI guess two can play that game https://www.microsoft.com/en-us/security/security-insider/in... reply michaeljhg 1 hour agoprevIs there a country that doesn't do this? reply Maxatar 45 minutes agoparentI don't know of any government department in Canada, Mexico, the UK, France, Germany, Australia that target U.S. law makers with fake social media accounts. Do you know of any? Can you cite them? reply cooper_ganglia 19 minutes agorootparentI don't know of them, that's why I'm sure it's happening. I'd assume that the US is doing this to our allied nations, too. reply some-guy 1 hour agoparentprevNot the point here: Israel is our \"greatest ally\" and the target is our lawmakers. reply tptacek 24 minutes agorootparentI think you would be surprised by the list of countries the US IC believes are our most important intelligence \"adversaries\"; the list includes many of our allies. reply seydor 36 minutes agoparentprevIt's not common at all in liberal countries. Perhaps azerbaijan or china do it reply kergonath 1 hour agoparentprevSo, enlighten us: is there? Any example of this sort of things between allies? Or is this just an extreme case of both-sides? Spying and keeping tabs on your friends is one thing. Influence campaigns among close allies are generally not the way it works. reply gmarx 1 hour agorootparentbefore the US entered WWII the british had an office of propaganda with offices in New York that was dedicated to getting the US to enter the war. reply neves 1 hour agoparentprevUSA censor Social Networks that don't allow them to do it. reply 2OEH8eoCRo0 41 minutes agoprevIf there are influence operations online is it ethical to counter them with your own? Obviously none would be preferable. We need to better define what propaganda is. To me it's misleading or false information with the purpose of facilitating a political outcome. Or deceptive information not meant for selling a product. reply shmatt 20 minutes agoparentSo buy comments on Israeli MK Facebook accounts? I think the only ones that would move any needle here is Meta profits on DAU and the company being paid to run the bots. You think your congressman goes to FB comments to decide how they’re voting? reply partiallypro 2 hours agoprevI'm sure the story is true, but I doubt it was effective. I don't think most politicians are really looking at their social media given most of it is trolling junk. I'm sure Russia/Iran/Hamas adjacent countries were doing the same. I just don't think they have been that effective in getting politician support. Direct lobbying or phone/email is much more effective than an online troll farm to get the attention of a politician in DC. I have worked in DC and this still feels very true to this day. I think the more worrying is going after the low information voter. I didn't think Russia's election interference had much effect in 2016, but now when you look at US Media (largely conservative outlets) their footprint is very visible. reply greg_V 0 minutes agoparentI'm not in the US but am familiar with some politicians here, and they too have a problem with recognizing that the feed they received is personalized, the comments are not representative, etc. If you're wondering why politics sometimes seem out of touch, it's because politicians, their media and the commentariat are locked into an echo chamber already. If I were an actor interested in influencing the policy of another country, why would I spend $$$ on manipulating the voting populace if I can poison the feed of the people who matter for far less? reply Hikikomori 2 hours agoparentprevPoliticians are already influenced by AIPAC and other powerful groups working for Israel, doubt they needed more. They've managed to push anti-bds laws/orders in most states. reply laweijfmvo 2 hours agoparentprevOne thing I've learned about old people (and in the US, the people in charge are _OLD_) is that they have no concept of being scammed like this. So they may recognize trolling, but if you tell them \"Hey, the President of Israel tweeted at you,\" they just assume it was the President of Israel. reply The_Colonel 1 hour agorootparentSo the logic is something like this: * my grandmother can't recognize fake information * my grandmother is old * politicians are old * therefore politicians can't recognize fake information Politics has always been full of deception, people doing politics professionally for decades should know a bit or two about it. reply digging 28 minutes agorootparent> Politics has always been full of deception, people doing politics professionally for decades should know a bit or two about it. I think it's worth pushing back on this. Deception works when it's unexpected, and if the medium is something politicians aren't familiar with, they may not even be looking for the kinds of deception they're being targeted with. They think themselves hardened to deception, but they may not be open-minded enough to even realize there are forms of deception they haven't prepared for. reply cafard 2 hours agorootparentprevI am in my upper 60s--if a bit junior for Congress, let alone the White House--but like to think of myself as a bit more skeptical than that. reply some-guy 1 hour agorootparentHelping my parents in their mid-70s is a constant uphill battle with these things. I simply tell her to ignore *everything* and if she needs confirmation to get in contact with me. reply partiallypro 2 hours agorootparentprevAside from the terminally online politicians (like Mike Lee, AOC, Cruz, MTG) most do not use their own accounts or even look at them. They might have a firm that measure constituent engagement, but still to this day the most effective way to complain to your congress person is a phone call or email. If we're being serious, the latter is what these bot farms, etc are after. They want to influence actual constituents to do their ultimate bidding. Now if we can get evidence of a huge phone campaign using AI voice, that would be much more alarming. Israel is doing it, and Iran/Russia/Hamas adjacent are doing it. There's absolutely no denying it. reply seydor 35 minutes agoparentprevOTOH i ve seen politicians care about social media much more than average joe does. reply PurpleRamen 2 hours agoparentprevIt's not just for the politicians, but the people around them, the companies researching the mood, the normal citizen who will carry the mood to others. It not simply to quantify the effect of this type of social engineering. reply partiallypro 2 hours agorootparentI'm aware how it works. But those numbers just aren't that compelling, because as I said every large social media platform is full of large troll farms. It's more about influencing actual constituents to write their Congress person. reply nerdjon 1 hour agoprevI find it quite concerning just how much propaganda the US seems to get from Israel. Where I live there are big billboards around, I regularly see ads on YouTube. I know propaganda is a thing, but it feels like we are getting more about a foreign government than our own. I feel like before what is going on now I was aware of some of the groups responsible for this being a thing, but was not fully aware just how much money there was in it those organizations until recently. Some of the practices are concerning, like I found out recently apparently the Boston police regularly go over to Israel for training? Regardless of what is going on right now, I don't understand how this much power over the US was ever deemed acceptable? reply spamizbad 50 minutes agoparentWeirdest thing about their propaganda is that it seems squarely aimed at older wealthier Americans and politicians. The amount of content produced targeted at anyone under the age of 40 is much smaller and less sophisticated. There's this narrative that Israel is \"losing the propaganda war\" but I think they're just targeting it towards major stakeholders. We're not the intended audience of the billboard - it's the editorial writer, the business leader, the member of congress (and their staff). The Israel / Palestine conflict is one of those low-valence issues with the general public where a politician rarely gets punished for voting one way or another with the notable exception of cash lobbying and super PACs for/against a given candidate. reply zardo 1 hour agoparentprev> Regardless of what is going on right now, I don't understand how this much power over the US was ever deemed acceptable? US politicians can direct funds to Israel and Israel can support them or attack their rivals. reply cempaka 1 hour agoparentprevThe best part is when it comes from our own \"newspaper of record\" i.e. with the extraordinarily dubious \"mass rape\" article the NYT published. They finally dismissed the one Israel-connected reporter who had liked tweets calling for a brutal response against Gaza, but that of course has seen about one billionth the attention that her original claims continue to receive. You also get stuff like the POTUS repeating lies like \"40 beheaded babies\" and \"a mother and child had kerosene poured on them\" with none of the usual media freakout you usually see over \"misinformation.\" reply nerdjon 1 hour agorootparentI have struggled to even look at my News app anymore. Next to articles about the protests or other things, there are the articles about the hostages or something else that just feels like a propaganda piece aimed at one thing. And that is just the headlines. reply segasaturn 28 minutes agorootparentThe wiki article on Media Coverage of the Iraq War[0] is an enlightening read. Most of the same tactics for manufacturing consent that the mainstream media used during the Iraq War are still being used in today's conflicts. > An investigation by the New York Times discovered that top Pentagon officials met with news analysts where they gave the analysts 'special information' and then tried to convince them to speak favorably about the Iraq war. The discovery was based on 8000 pages of secret information that had been revealed to The New York Times through a lawsuit under the Freedom of Information Act. The article states that top Pentagon officials would invite news analysts to secret meetings, and urge the analysts to speak positively of the war. Often, the US would give \"classified information,\" trips, and contracts to the news analysts. 0: https://en.wikipedia.org/wiki/Media_coverage_of_the_Iraq_War reply megous 17 minutes agorootparentprev> You also get stuff like the POTUS repeating lies like \"40 beheaded babies\" and \"a mother and child had kerosene poured on them\" with none of the usual media freakout you usually see over \"misinformation.\" Yeah. The politicians who repeated this also never apologized for inflaming tensions without any evidence or investigation whatsoever. But anything Israel is accused of, requires thorough investigation by Israel (or sometimes independently - without ever mentioning Israel will not allow independent investigators into Gaza), before we can even think about trusting the people they're killing currently, regardless of affiliation. Then you read https://www.france24.com/en/live-news/20231215-israel-social... And you see that 20 children 15 and younger were killed in total, and out of them 10 by rockets, which starts to paint very different picture. So militants killed 20-36 children depending on how you wish to define a child, out of 1200 people in total. So that's 1.7-3% of killed victims. On the other hand, you get at least 16 000 killed children by Israel in just the last 8 months. 60 a day at least. https://time.com/6909636/gaza-death-toll/ And you can see 10-5 a day individually just scrolling through video posts on telegram https://t.me/eyeonpal/ And we're supposed to think that Hamas are child killing monsters and Israel is not and somehow uniquely righteous. Yeah, right. Just the math alone on this doesn't compute for me, at all. reply nsguy 1 hour agorootparentprevnext [2 more] [flagged] dragonwriter 1 hour agorootparent> The real problem here isn't the POTUS repeating some rumour that was going around. It wasn't “a rumor going around”, it was Israeli state propaganda which both started and continued for a long timw to be pushed by Israeli state organs (official sources like the Foreign Ministry, not just proxies.) reply mupuff1234 1 hour agorootparentprevThe NYTimes also reported about an alleged Israeli strike on a hospital killing hundreds despite it ended up being a Hamas failed rocket. It seems to me the the NYTimes is trying to be somewhat objective but just gets stuff wrong occasionally. reply drpossum 1 hour agorootparent\"Getting stuff wrong\" by not corroborating facts using reliable sources is not acceptable for a news organization. reply mupuff1234 1 hour agorootparentYou're right, but just want to dispute the claim that NYTimes is somehow Israeli propaganda - I think it's clearly not. reply Sporktacular 1 hour agorootparentThe claim has some merit: https://theintercept.com/2024/04/15/nyt-israel-gaza-genocide... reply mupuff1234 55 minutes agorootparentThey were told to not used those words because they are disputed and not consider facts, pretty sure they can and still use those words in opinion pieces. Innocent until proven guilty etc. reply asveikau 1 hour agorootparentprevThis line of propaganda is kind of infuriating. Separate from this incident, Israel bombed afaik every hospital in Gaza. They claimed Hamas was operating inside them or under them and produced absolutely zero credible evidence of it. They killed a lot of doctors and patients. But if they start out polluting minds with the claim that one time at the Al Ahli parking lot, there was an Islamic Jihad rocket once, they then by extension use that to imply that Hamas is somehow responsible for all the deaths in hospitals that happen by Israeli hands on every other day. reply mupuff1234 53 minutes agorootparentYou are changing the subject, I'm just disputing that NYtimes is Israel propaganda, I'm not claiming anything about the righteousness of Israeli actions. reply asveikau 47 minutes agorootparentMy personal opinion of NYT is that their record is mixed on the subject. If you'll allow me to change the subject to one that is less presently divisive to provide an instructive example, NYT's conduct in the lead-up to the Iraq war is a great example of where they acted as a pro-war propaganda mouth piece, and maybe the institution doesn't deserve our total trust. reply sillystuff 53 minutes agorootparentprev> It seems to me the the NYTimes is trying to be somewhat objective \"Somewhat\" is doing some heavy lifting here. The NY Times internal pro-Israeli editorial guidelines were leaked. NY Times is a pro-Israel biased source: https://theintercept.com/2024/04/15/nyt-israel-gaza-genocide... CNN even sends all stories to Israel to be approved/disapproved and/or edited to ensure pro-Israel bias, so I guess NY Times is at least better than CNN: https://theintercept.com/2024/01/04/cnn-israel-gaza-idf-repo... It is pretty disgusting. All major US corporate media is biased in favor of Israel: https://en.wikipedia.org/wiki/Media_coverage_of_the_Arab%E2%... Israel also has put fear of god into US government officials through their lobby group, AIPAC (the only foreign lobby group of its kind that is not required by the U.S. to register as a foreign agent). While no fan of former president Reagan, he called the Israeli attacks on Lebanon a \"holocaust\" and stopped Israeli atrocities against Lebanon by threatening cutting off US aide. Now all our representatives line up behind Israel in their perpetration of genocide-- especial Biden. https://en.wikipedia.org/wiki/AIPAC https://www.amazon.com/Foreign-Agents-Committee-Fulbright-Es... https://www.wrmea.org/north-america/aipac-election-role-rais... reply downWidOutaFite 48 minutes agoparentprevIt's crazy that AIPAC is not registered as a foreign agent. They funnel orders directly from Netanyahu to our politicians. reply ravenstine 1 hour agoparentprevOn the other side of town where I live, billboards appeared with slogans like \"Be pro-Semitic.\" This happened almost immediately after the latest conflict involving Israel began. So I can't just be against anti-Semites, but I have to be pro Jewish ethnicity? Interesting. There was also one stating that anti-zionism is anti-Semitic; I guess my Jewish friends and family who are not Zionists didn't get the memo. I can't prove that these are somehow connected to funding from Israel, but it seemed like these billboards were ready to go at a moment's notice. As far as why we deem foreign propaganda as acceptable, I like to think that we play dumb about it in part so we can strategically point it out when it is in the favor of politicians and/or elites. Remember how Russian propaganda supposedly got Trump elected even though it was going on during prior years when the establishment insisted on the integrity of the elections? On the other hand, maybe we are just dumb. reply nerdjon 1 hour agorootparent> I can't prove that these are somehow connected to funding from Israel, but it seemed like these billboards were ready to go at a moment's notice. Right, that is what we got in Boston. The timing is just too convenient. Whether or not it is from Israel themselves or funding here for Israel is kinda a moot point when both have the same purpose: Propaganda for a foreign government. reply pvg 48 minutes agorootparentWhether or not it is from Israel themselves or funding here for Israel is kinda a moot point It's not at all a moot point and you're coming pretty close to a generic 'dual loyalties' trope. reply jedimind 30 minutes agorootparentIf there is overwhelming evidence for a claim, it's absurd to smear it as \"trope\". So when Nancy Pelosi says: \"If the capital crumbles to the ground,one thing that will remain is our commitment to Israel\"[0] that's because she has in fact dual loyalty, to dismiss that as \"trope\" is to dismiss & deny reality. Some of \"our\" politicians work overtime for israel's interests and neglect their actual job and obligations towards America. [0] https://www.youtube.com/watch?v=53x_zrkJwDs reply 2OEH8eoCRo0 1 hour agoparentprev> I find it quite concerning just how much propaganda the US seems to get from Israel. How do you know how much we get from Israel relative to others? reply Retric 1 hour agoparentprev> I don't understand how this much power over the US was ever deemed acceptable? Free speech sometimes applies to things you don’t like. There’s pro and anti propaganda for just about any foreign interest. Some of it’s just more subtle such as recommendations on TikTok. Ukraine had really obvious pro Ukraine requests for military aid and images of destruction, but quite a bit of pro Russia propaganda was more subtle aiming for people to stay out of it. With Israel you see some really blatant pro Israel propaganda, but both sides also have a lot of more subtle stuff. reply asadotzler 6 minutes agorootparentLike the kind of free speech you get when Israel's lobby drafts and helps pass laws that making boycotts of Israel illegal? reply justin66 1 hour agorootparentprev> Free speech sometimes applies to things you don’t like. ...and in the United States we've defined down \"free speech\" to include \"monetary donations.\" reply fsckboy 1 hour agorootparentsomebody needs to pay for the billboard, or rent a hall to give a speech, or printing the flyers for your lost cat. How is money not essential to speech? Your proposal is that to support a cause, one should only be allowed to go outside and yell, because that's purer than the corrupting influence of money? reply pnut 45 minutes agorootparentOnce money gets involved, you inherently have a commercial interest. What's the ROI? I personally think people misunderstand to whom \"freedom\" is granted and defended in the US, it is demonstrably not freedom of the individual, but of the powerful. reply ESTheComposer 1 hour agorootparentprevI believe the issue here is how much sway Israel has on the US and how rabid many US politicians are about Israel (to the point where many straight up accuse you of anti semitism if you just criticize the country or their policies) Also issues like where you are not allowed to refuse to work with Israel if you are an arms manufacturer in the US (but you can refuse to work with the US military). I know that part of that is due to Israel being part of the FMS list but they are also the largest recipient on it... reply 1shooner 1 hour agorootparentprevSpeaking of free speech, can you name another foreign interest that has managed to make it illegal in the US to boycott it's companies? reply dfxm12 1 hour agorootparentTo add some context in case people aren't aware: https://www.middleeastmonitor.com/20181218-texas-teacher-fir... reply Retric 30 minutes agorootparentprev“Texas enacted a law in May 2017 prohibiting state agencies from signing contracts with companies that boycott Israel.” I’m not particularly happy about that law or similar ones in other states, but what you said is inaccurate. reply asadotzler 4 minutes agorootparentThere's nothing inaccurate about what he said. These laws make it illegal for Americans from boycotting Israel. As an American state employee, organizing a boycott against Israel will get you fired or arrested because allowing it would be breaking the law. reply jimbob45 1 hour agoparentprevlike I found out recently apparently the Boston police regularly go over to Israel for training? That's really common for most countries on earth though[0]. Gaining exposure and experience from other countries is very valuable for police forces. [0]https://en.wikipedia.org/wiki/International_Law_Enforcement_... reply neves 1 hour agoparentprevImagine if it were Russia or China reply croisillon 33 minutes agorootparentWell it can’t be China because China don’t kill their muslim popula… oooh… reply ajross 1 hour agoparentprevA message being on a billboard or in an advertisement on YouTube doesn't make it \"propaganda\", though. In fact there a very large constituency[1] for pro-Israel policymaking right here in the United States, and they want you to know what they think and why, and are willing to spend money to do it. Now, you clearly don't agree with them. I don't either, in at least some aspects[2]. But our beef with the AIPAC and the Israel lobby isn't with a \"propaganda\" organization run by the Israeli government. It's a POLITICAL fight with our fellow americans, and we shouldn't conflate the two. [1] Likely larger than in Israel proper, in fact, both in headcount and budget. [2] Though I stop way, way short of the eliminationist sloganeering that has taken over a lot of the left. Handing the region From the River to the Sea over to a Palestinian-controlled army would be far more horrifying than anything happening in Gaza today, and I really don't think that people understand how intractably violent the situation in the Levand really is. reply nerdjon 1 hour agorootparentI struggle with not calling this propaganda: https://www.reddit.com/r/boston/comments/18skzb0/ah_behold_o... https://www.boston25news.com/news/local/new-billboards-along... reply ajross 13 minutes agorootparentThen you have a somewhat unconventional personal definition for \"propaganda\". Almost always people use that word to imply something clandestine, misleading, or both. What you showed is a paid advertisement from a US-registered 501(c)(3) nonprofit, JewBelong, with public accounting on all donations, and a relatively clear mandate for how it spends its money. Again, you're simply saying that a (really only slightly edgy) billboard paid for by your fellow americans with their own money and aimed at changing your opinion via argumentation should be disallowed as \"propaganda\" simply because you disagree with it (and again: I disagree with it too!). Tough love: of the dueling philosophies at play here, yours is by far the most dangerous. Let people argue with you, for crying out loud. reply nerdjon 3 minutes agorootparentI do not believe that is the case, you seem to have a more narrow view of propaganda. Just look at the Webster definition: https://www.merriam-webster.com/dictionary/propaganda What you describe is one form of propaganda but not the only one. Would you agree that Rosy the Riveter was Propaganda? Or this one https://www.archives.gov/files/exhibits/powers-of-persuasion... Both very famous pieces of propaganda the US put out during War. Neither of them are misleading. They were put out to encourage people to take an action. ein0p 1 hour agoparentprevDo you find it concerning that no presidential candidate can even pass the primary without first kissing the ring of AIPAC? That Zionist lobby openly attacks insufficiently pro-Zionist candidates and then openly brags when they lose elections? With Zionist lobby trying to outlaw any criticism of Israel in direct violation of 1st amendment? Etc, etc. IDK about others, but I think this is insane. reply nerdjon 1 hour agorootparent100% yes. TBH when I said \"propaganda\" I was grouping a lot of that under that when I should have been more specific. But that and similar things is what I was referring to with \"I don't understand how this much power over the US was ever deemed acceptable\". I remember seeing the articles about them funding the campaign of someone opposing someone else who had been critical if Israel. I don't remember which state or what position, but it wasn't just a one off either. reply dfxm12 48 minutes agorootparentAIPAC is one such group. Here's an article about their spending in 2022: https://www.opensecrets.org/news/2022/11/american-israel-pub... and also a story from earlier this year: https://www.politico.com/news/2024/03/03/aipac-israel-spendi... The MO is basically to try and defeat democrats in primaries who aren't giving carte blanche in terms of spending or support. If their candidate wins the primary, it doesn't matter much who wins the general election, since they have support of R's. reply leoh 1 hour agorootparentprevWhat the actual fuck reply leoh 1 hour agorootparentprevnext [3 more] [flagged] ein0p 57 minutes agorootparentEvery single point I made is 100% documented and verifiable. It’s also anti-Zionist, not antisemitic. There is a difference. And even if it were antisemitic (which it is not), I’d be within my constitutional rights as a US citizen living in the US. reply ESTheComposer 53 minutes agorootparentprevYou cannot call any criticism of Israel or its hold over US politics anti-semitic just because Israel happens to be Jewish majority. When people criticize the US they aren't automatically anti America, any other country holds the same. US presidents always visit Israel and cater to it if they want to be elected, it has 0 to do with Judaism or the fact that Israel is founded on Jewish principles. reply leoh 58 minutes agorootparentprevnext [8 more] [flagged] nerdjon 55 minutes agorootparentTrying to shut down a conversation with \"Antisemitism\" does not help your case. Being critical of their actions does not equal antisemitism. I fully understand why they are doing what they are doing, they would be stupid not too. That doesn't mean its ok and we should accept it. https://www.theguardian.com/us-news/2024/apr/17/pro-israel-m... reply nsguy 47 minutes agorootparentHow do you propose we determine the fine line between the antisemitic \"The Jews run the World\" and \"being critical of their actions\"? What would e.g. be the piece of evidence that convinces you that we're dealing with antisemitism and not legitimate criticism? We shouldn't shut down conversations about Israel with antisemitism but we also shouldn't shut down conversations about antisemitism with \"being critical of their actions\". There are many other groups lobbying for various causes in the US, e.g.: https://en.wikipedia.org/wiki/Arab_lobby_in_the_United_State... \"According to ProPublica, 4 of the top 10 governments lobbying in Washington are Arab, in terms of spending. The United Arab Emirates places first, having spent $10,914,002 in 2007 and 2008. Iraq, Morocco and Saudi Arabia also each spent over $3 million, and the non-Arab, Middle Eastern nation of Turkey also spent over $3 million.\" Why the focus on Israel here? reply nerdjon 34 minutes agorootparent> Why the focus on Israel here? That is the what the article is about and there is something going in involving Israel. I can't find the article I read a while ago with a graph showing their massive increase in spending, but according to a few articles AIPAC plans to spend $100Million this year. As far as how to distinguish between them. I really don't think this should be complicated. Criticizing a government is not criticizing a religion or people. If I start to attack a religion or to attack a group of people based on those beliefs, then yeah that is antisemitism. Actually having and voicing a problem with jewish people. A government is not that. We criticize our own government all the time and we are not anti-American (ok, admittedly some do try to make those claims but that's a different story). yes I will also admit that this got complicated with the protests. reply ein0p 40 minutes agorootparentprevBecause the original post is about Israel using fake social media accounts to get US representatives to support genocide in Gaza. We’ve literally just passed some laws to _sanction the ICC_ for their daring to say anything negative about Bibi Netanyahu. Does the Arab lobby also have this kind of power? I mean, are you going to seriously argue that this is normal: https://www.politico.com/news/2024/03/03/aipac-israel-spendi... ? reply leoh 34 minutes agorootparentprevnext [4 more] [flagged] nerdjon 29 minutes agorootparentNo I am not, I am still engaging in the conversation. But there is very little I can actually say when your response is that it is anti-semitic. You are no longer responding to what was actually said and instead pivoting it to something else that does not leave anything to respond too since you are not addressing what you are responding too in the first place. And again you are doing the same thing here, instead of actually trying to have a conversation you shut it down by claiming it is antisemitic. That isn't a conversation, that is a pivot to try to end it. I also never claimed it did not exist. Over the last several months I have seen many conversations just end with \"that's anti-semitic\" because it was any amount of criticism as if we have to accept that and that's the end of the conversation. Well it's not and shouldn't be, you made a claim and I am challenging it. reply leoh 23 minutes agorootparent> Do you find it concerning that no presidential candidate can even pass the primary without first kissing the ring of AIPAC? That Zionist lobby openly attacks insufficiently pro-Zionist candidates and then openly brags when they lose elections? With Zionist lobby trying to outlaw any criticism of Israel in direct violation of 1st amendment? Etc, etc. IDK about others, but I think this is insane. * foaming at the mouth * kiss the ring, AIPAC, Zionist, Zionist, Zionist Fixed that for you. reply nerdjon 19 minutes agorootparentI... have no idea what your point is with this... Edit: If I can figure out what you are trying to say Was the person I responding too slightly exaggerating? Sure. But we know for a fact that a lot of money flows from AIPAC to candidates that support strong ties with Israel. That isn't a secret. Saying that isn't anti-semitic. If that is not your point, please let me know what it is so I can properly respond. reply shrubble 1 hour agoparentprevWait until you read up on NUMEC and Rafael Eitan. reply dang 2 hours agoprevWe changed the URL from https://www.forbes.com/sites/siladityaray/2024/06/05/israel-... to what looks like a more original source. reply erellsworth 4 hours agoprevWhy is this flagged? What rule does it violate? reply dang 2 hours agoparentPosts on all sides of this topic get flagged quickly (by users), and mods turn off the flags on limited occasions—mostly when some significant new information arises and there's at least some chance of a substantive discussion about it. It's pretty important that most stories about this conflict and similar current affairs get flagged, because otherwise HN's front page would consist of little else, and that's not the purpose of the site. But it's also important that the topics not be ignored completely, even though they're painful. There's no happy medium here, unfortunately. Here are some links to previous explanations. If you (or anyone, of course) have a look at these and still have a question that isn't answered there, I'd be happy to take a crack at it. https://news.ycombinator.com/item?id=40418881 (May 2024) https://news.ycombinator.com/item?id=39920732 (April 2024) https://news.ycombinator.com/item?id=39618973 (March 2024) https://news.ycombinator.com/item?id=39435324 (Feb 2024) https://news.ycombinator.com/item?id=39435024 (Feb 2024) https://news.ycombinator.com/item?id=39237176 (Feb 2024) https://news.ycombinator.com/item?id=39161344 (Jan 2024) https://news.ycombinator.com/item?id=38947003 (Jan 2024) https://news.ycombinator.com/item?id=38749162 (Dec 2023) https://news.ycombinator.com/item?id=38657527 (Dec 2023) reply pandeiro 1 hour agorootparentReasonable approach and I applaud your effort to maintain the site's purpose while also not ignoring these issues (that do have some relation to the tech industry, as we've seen). Thank you. reply DevX101 1 hour agorootparentprevAppreciate the case by case basis approach to moderation here. There are quite a few topics where discussion becomes suppressed when blanket bans are enforced. reply jules-jules 1 hour agorootparentprevThank you dang. Doing the lords work. reply erellsworth 2 hours agorootparentprevThank you for the explanation. reply jakupovic 2 hours agorootparentprevThanks Dang this is much better than previously. reply jimbob45 1 hour agorootparentprevThe problem is that we've already solved these issues many times over on other sites that are decades old. HN simply refuses to implement 21st-century forum enhancements. Brigaded reports? 4chan solved it by adding a mandatory enum to reports to specify what the report is reporting for. Identifying bad reports and banning users as a result becomes trivial. Flooding with stories about a particular topic? That's what stickies are for and they work particularly well so that mods can auto-delete any non-sticky stories pertaining to the MOT. Flamewar on a MOT? Add a sticky to the top of the thread like reddit does saying moderation will be minimal and to enter the thread at your own risk. reply dannyobrien 1 hour agorootparentI think I prefer dang's approach, at least at the scale that HN operates. reply zen928 49 minutes agorootparentprevUse the appropriate forum to debate topics in the manner you prefer instead of trying to force others to conform to your standards. reply pessimizer 43 minutes agorootparentThere is no planet where the opinion above represents \"force.\" Stop telling people who disagree with you to shut up. reply raxxorraxor 4 hours agoparentprevArticles get flagged if users perceive it to not be desired content and not necessarily because it violates a specific rule. reply jules-jules 4 hours agorootparentnext [7 more] [flagged] TwentyPosts 2 hours agorootparentYou'd be surprised by how many HN users would prefer to keep politics-first-tech-second news off the site. reply dang 2 hours agorootparentThis issue goes back as far as the site itself: https://news.ycombinator.com/item?id=17014869 and the way we handle it has was worked out over a decade ago: https://hn.algolia.com/?dateRange=all&page=0&prefix=false&so... If anyone has questions about this, take a look at those links and if there's something unanswered there, I'd be happy to take a crack at it. reply runarberg 2 hours agorootparentprevI’m actually not surprised, but I reserve the right to be extremely judgmental about people who think that. That is to say, this preference is extremely lame. Technology is political (as this article is a testament to). Technology is used to commit humanities worst horrors. Us tech workers do commit our work which enables these horrors. It is hard to admit that we are partially to blame, and that our industry is part of the problem. It is much easier to simply close our eyes and claim: This is just politics, and hence, none of my business. EDIT: To clarify, I don’t agree with your parent that there is some astroturfing campaign going on on HN where state agent propagandists are mass flagging stories. I do however believe a lot of HN users are this lame, and don’t want to admit problems in our industry. reply luckylion 19 minutes agorootparentThere's reddit.com if you need to vent your outrage about technology and politics. We don't need to turn every website on the internet into reddit, it's okay to have some websites focus on other things. reply ipaddr 2 hours agorootparentprevEverytime I submit a github project it becomes [dead] immediately. You expect political drama submissions not to be flagged as offtopic? reply dang 1 hour agorootparentSince there are no examples of that at https://news.ycombinator.com/submitted?id=ipaddr, I assume you're talking about a different account. It's possible that account is banned, but more likely your self-posts might be getting filtered by HN's software which tries to apply this guideline: \"Please don't use HN primarily for promotion. It's ok to post your own stuff part of the time, but the primary use of the site should be for curiosity.:. reply WhackyIdeas 2 hours agoprevIsrael never got the memo about calming their propaganda campaigns down - now even the most average Joe (at least in the UK) can spot them a mile off. They aren’t subtle about it. For a country which is meant to be one of the smartest out there, their propaganda campaigns are an utter disaster giveaway to anyone with a pulse and a few dozen brain cells. reply cjk2 1 hour agoparentTo be fair propagandists generally don't have to aim high. They only have to shift the undecided and uninformed opinions a little bit. You want to see some of the crap the agencies were pushing out pre-Brexit and it worked, so I wouldn't classify the average UK citizen as much to be contended with (I am UK as well for ref). reply WhackyIdeas 1 hour agorootparentThe only good thing to come from Brexit is that for some people they woke up and smelt the more expensive coffee than it used to be and had some self reflection. I have spoke with a good few people now that were like ‘I was lied to’… well duh. reply cjk2 1 hour agorootparentUnfortunately it was mostly \"I was lied to, but I agree with it anyway because of X\" where X is some loose justification to absolve themselves of the self-inflicted mess they got themselves into. My father was a fine one. His staff fucked off back to Europe, he couldn't hire anyone else and had to fold his company and retire. Then he found out he got cancer and that the NHS had staffing problems due to Brexit. Me, I am better off for it as I fill a niche demand, but I voted against it because it was generally bad for society and I do not always vote in self-interest. reply WhackyIdeas 1 hour agorootparentOur business lost around 90% of European customers. Then the odd customer now from Europe emails in a rage because they had to pay import duties… so then having to tell them that ‘Brexit’ happened! You are so right though - most people just will refuse to accept they were taken for fools with all the nasty rhetoric about ‘people coming in to our country’… and so many have even been convinced that ripping up the Human Rights act is somehow a good thing (??? Wtf!). Rich people manipulating the minds of poor people was what Brexit was all about. At the time I was convinced this was a Russian move to make UK weaker. But then again, I was convinced for many years that Trump was a Russian asset to make America implode (which is kind of what happened). All this hate for foreigners. It’s disgusting. reply diggan 2 hours agoparentprev> For a country which is meant to be one of the smartest out there Seems their propaganda works just fine if that's a commonly held belief :) reply some-guy 1 hour agorootparentMy wife lived in Israel on and off for years, I got to visit her for awhile during her dissertation work in the West Bank. Only a few blocks of Tel Aviv deserve the \"modern Middle Eastern country\" label in my mind. And even then it wasn't nearly as impressive as I thought. Haifa is a lovely town though :) reply WhackyIdeas 1 hour agorootparentprevMaybe that is just the dregs of a time when they used to be half decent at propaganda ;) I take it back. They are smart with their hacking and that’s where it ends. reply jajko 1 hour agorootparent> They are smart with their hacking and that’s where it ends So are often russians, or chinese. Maybe the concentration is on another level, they are a tiny country but highly educated for generations. I'd bet if we properly educated and developed whole world we would discover Einsteins and Bolts in many many places out there. Ie elite athletes often come from places around where they could do sport, ie mountaineers but also many others. reply tehjoker 1 hour agorootparentprevYea, US propaganda is the same way. It's not subtle, it's just so many people are onboard with it they don't really care. reply nemo44x 1 hour agorootparentprevnext [2 more] [flagged] FireBeyond 31 minutes agorootparentI'm going to flag this as some weird eugenics BS. \"Average IQ of 83\" ... so the average person is \"borderline mentally disabled\"? reply r00fus 1 hour agoparentprevSee, this is the thing - this propaganda is only meant as the \"rationale\" that goes along with the real bribe - campaign funding from the likes of AIPAC and DMfI. The funding is what secures these politicians' votes. The propaganda is what the politicians can use to justify their actions to everyone else. That it's laughably bad is a function less of the capabilities of Israel than the utter fealty that these politicans have to the Israeli cause. reply KenArrari 4 hours agoprevnext [2 more] Wow it's been up for a full hour. reply jules-jules 4 hours agoparentNo it’s gone. We would need admin support to get it unflagged again, most likely. reply throw_a_grenade 6 hours agoprevrelated: https://news.ycombinator.com/item?id=40583661 reply dang 2 hours agoparentWe merged that one hither. reply r721 48 minutes agorootparentThere is some discussion here too: https://news.ycombinator.com/item?id=40587325 reply pokepim 6 hours agoprevnext [16 more] Just as russian bots, israeli bots spreading fake news are worst thing that happened to our society. Crazy that people are falling for this but here we are reply greenavocado 2 hours agoparentHow many congressmen are dual citizens with Israeli citizenship? This is even worse. Also, AIPAC is allowed to exist. As a thought experiment replace Israeli with Russian citizenship for the Israeli dual citizens in Congress and replace AIPAC with a hypothetical Russian ARPAC. Imagine how crazy this would be. Yet the current situation is somehow completely acceptable. reply wk_end 2 hours agorootparent> How many congressmen are dual citizens with Israeli citizenship? I don't know. How many? I was curious and Googled, and couldn't find any good authoritative lists. This Quora answer [0] implies that the answer is zero, as does this Snopes article [1]. Both answers mention that there's various incorrect lists going around that are white supremacist propaganda. [0] https://www.quora.com/Which-current-members-of-Congress-have... [1] https://www.snopes.com/news/2024/02/05/dual-citizenship-elec... reply mountainofdeath 2 hours agorootparentprevTo my knowledge, zero. If you really mean Jews, I think there are roughly 40 between both houses of Congress. None of them are Israeli citizens. Jews are not automatically citizens of Israel though they do have dedicated pathway to obtaining it, but it's not as simple as merely showing up and claim you are Jewish. reply jhp123 2 hours agorootparentprev> How many congressmen are dual citizens with Israeli citizenship? Zero? I can't find a reliable source for any congress member being an Israeli citizen reply wonderwonder 2 hours agorootparentOP means Jews https://www.worldjewishcongress.org/en/news/defining-antisem... Lots of people going masks off now that is cool to be anti-semitic again but now we call it anti-zionist. reply tumsfestival 46 minutes agorootparentThis post was brought to you by the IDF, remember if you don't agree with us you're as bad as Nazis. reply wonderwonder 33 minutes agorootparentso did OP not mean Jews or are we saying that anyone inferring that someone is anti semitic is working for the IDF? reply vsuperpower2020 2 hours agorootparentprevnext [3 more] [flagged] jhp123 1 hour agorootparentare you trying to get Nazi talking points bingo? Don't forget the race science stuff or \"ouch! my neck\" reply vsuperpower2020 1 hour agorootparentSorry, I have no idea what you're talking about. reply wonderwonder 2 hours agorootparentprev\"How many congressmen are dual citizens with Israeli citizenship\" By this do you mean Jews? Should be prevent Jewish people from being allowed in congress? \"AIPAC is allowed to exist\" Should we prevent it from existing becuase it supports a Jewish state? You have no issue with the hundreds of other lobbying groups, just the jewish one. reply throw310822 1 hour agorootparentIt's not Jewish, it's pro-Israel (American Israel Public Affair Committee). Israel is a foreign country whose interests might be conflicting with those of the US. That's different, don't you think? reply wonderwonder 35 minutes agorootparentyou going to ignore OP's mention of \"Dual Citizens\" clearly meaning Jews? Odd that people are only concerned with AIPAC and not the other foreign PACS. Wonder what the differentiating factor is? https://www.opensecrets.org/political-action-committees-pacs... reply golergka 2 hours agorootparentprev> As a thought experiment replace Israeli with Russian citizenship for the Israeli dual citizens in Congress and replace AIPAC with a hypothetical Russian ARPAC Is Russia the main American ally in the region that contributes enormously to American intelligence and R&D, while also supporting American military operations? reply throw310822 1 hour agorootparentHave you considered that things might go the other way around: if Russia had such a strong influence on the US through its political action lobby as Israel does, Russia would be considered by politicians the main ally of the US, and the economic and military ties between the two countries would be unbreakable. Because the purpose of these lobbies is exactly to influence how a certain country feels and acts about another. reply Hikikomori 2 hours agorootparentprevSouth Africa was also their ally in that region. reply sdfghswe 1 hour agoprevWhat?! Noooo way! /s Read \"The Israel Lobby and US Foreign Policy\" by John Mearsheimer. reply nextstep 5 hours agoprevnext [7 more] [flagged] nickff 1 hour agoparentI'm one of the people who flags some of these articles (though not this one), because they're generally uninteresting, and repetitive. I'm not Jewish, have never been to Israel, and am not part of any brigade. reply justin66 1 hour agorootparentnext [2 more] [flagged] nickff 1 hour agorootparentWhen you have a minute, please review the HN Guidelines, specifically: >”Be kind. Don't be snarky. Converse curiously; don't cross-examine. Edit out swipes.” reply davesque 42 minutes agoparentprevI flagged it because it's really boring to see people go through the same predictable, tribal motions on a topic that's been covered to death by every media outlet. reply 123yawaworht456 2 hours agoparentprevthey used to be really unapologetic about it https://en.wikipedia.org/wiki/Jewish_Internet_Defense_Force https://www.youtube.com/watch?v=DWD5xiiafBc it's prudent to assume such operations are still ongoing reply Animats 1 hour agorootparentJIDF turned out to be one guy. That was a sideshow. This article is from Hareetz, which is a major newspaper in Israel. There's a huge, organized Israel lobby aimed at the US. It's no secret. There's AIPAC, the American-Israel Political Action Committee. \"Lobbying for Pro-Israel Policies\", it says on their web site. There are official organizations in the government of Israel which do \"public diplomacy\".[1] [1] https://en.wikipedia.org/wiki/Public_diplomacy_of_Israel reply dang 2 hours agoparentprevhttps://news.ycombinator.com/item?id=40586961 reply cyclecount 4 hours agoprev…and like that, it’s flagged and gone. Discussion averted, thanks mods! reply dang 2 hours agoparentMods didn't touch it*. Users flag things. * (before I turned off the flags a few minutes ago) reply mathgradthrow 2 hours agoprevPerhaps they are trying to achieve parity with the propaganda of their adversaries, Russia, China, and Qatar. reply TillE 2 hours agoparentIt's certainly striking that the quality of the propaganda is very much on par with that of Russia. It's clumsy, it often espouses political views (eg, the faux-anarchist stuff) which are incoherent in America or really anywhere in the West. reply pessimizer 25 minutes agorootparentYou can't make a religious argument to people who don't share your religion, so they're stuck with harassment and bald-faced lies. Even non-Israelis who agree with them on everything end up with a bad taste in their mouth after reading or listening to it. They don't have any practice making a secular argument about the Palestinian situation, how could they? They fall back to assuring people that Israel is a friend to the US, that Israel is a democracy (a ton of its inhabitants are forced to live in ghettos and refugee camps), that Israel loves gay people (Israel doesn't have gay marriage, or even secular marriage except by treaty), and the worst: \"Why don't you give America back to the Indians?\" Russian propaganda is bad, but it's also lazy: it gets them nothing and they don't put a lot of effort into it. Israel is trying as hard as it can. To not ethnically cleanse Israel leaves a population that Europe tried its best to exterminate drifting the stormy seas of having to compromise with their neighbors. This is a population whose neighbors once built factories in order to slaughter them more efficiently. reply hartator 2 hours agoparentprevDidn’t we almost go to war against Russia for doing precisely that? reply 0xcafefood 2 hours agorootparentYes, but Israel is our Greatest Ally, so it's different when they do it. reply adolph 2 hours agorootparentprevIsn't the US already in a (barely-proxy) war with Russia? reply myth_drannon 5 hours agoprevnext [8 more] [flagged] tyen_ 5 hours agoparentAre the US actors on China-Iranian-Russia axis payroll in the room with us right now? reply colpabar 4 hours agorootparentI am one. I don't get paid that much, but hey, it's a living. What really sucks is these guys go WAY too hard with the agile stuff. I have like 10 scrums a week! reply myth_drannon 4 hours agorootparentprevbased on the downvotes to my comment, yes. reply KenArrari 4 hours agoparentprevPeople are seeing Israel murder children and this makes people upset. Hope this helps. reply myth_drannon 4 hours agorootparentnext [3 more] [flagged] KenArrari 4 hours agorootparentEquating things that Israel has clearly, demonstrably, actually done with blood libel is not a great idea even from a practical perspective. reply jakelazaroff 1 hour agorootparentTo quote an Eli Valley cartoon: “It’s a blood libel to hit ‘play’ on our genocide TikToks!” https://jewishcurrents.org/israels-defense reply gettodachoppa 4 hours agoparentprevnext [2 more] [flagged] myth_drannon 4 hours agorootparentIt's an acceptable heuristic at the moment. reply guerrilla 5 hours agoprevOf course it did. This is modern warfare. Welcome to the future. Just like how we knew everyone was always spying on everyone else long before Snowden, we should all know that everyone is doing this to everyone else long before all the revaluations come down to us. There is no benefit to assuming good faith here. reply AzzyHN 1 hour agoprevTruly shocking. Who could've seen this coming. On a more serious note, I figured the majority of US lawmakers already supported the genocide. I'm surprised Israel feels the need to use propaganda for this reply frob 5 hours agoprevThis was on the front-page for a brief moment and is now nowhere in the top 150. The irony writes itself. reply dang 2 hours agoparenthttps://news.ycombinator.com/item?id=40586961 reply davesque 25 minutes agoprevCriticizing Israel is the like the \"Anti Social Social Club\" t-shirt of our time. I really don't get why people find this news surprising or interesting. What's that? A political organization or world government is astroturfing on social media you say? And they're targeting lawmakers directly? Well, clutch my pearls! reply HL33tibCe7 15 minutes agoparent> Criticizing Israel is the like the \"Anti Social Social Club\" t-shirt of our time Nice bit of deflection. It’s not even true, either. People are being blacklisted from companies for speaking out against Israel, their faces being paraded through streets with threats made against them. Speaking out against Israel comes with significant risk, particularly for politicians and journalists - the proof is in the pudding, when you look at their insanely soft-balled coverage of anything Israel does. Anyway… > A political organization or world government is astroturfing on social media you say? And they're targeting lawmakers directly? Well, clutch my pearls! Sure, if it was Russia or China I wouldn’t be surprised. But it’s Israel. The US gives Israel $1,000,000,000s and $1,000,000,000s of dollars per year and is supposedly Israel’s closest “ally”. _That_ is what makes this interesting. I don’t really know why I’m bothering replying to you, given your incredibly thinly veiled bias. But here I am nonetheless. reply davesque 2 minutes agorootparent> given your incredibly thinly veiled bias What bias is that? reply cjk2 2 hours agoprev [–] Well their enemies used social accounts to garner support from US citizens so you've got to start somewhere! It's not like we haven't done this either. I worked for a company in 2005 which was doing this paid for by politicians. Moment I worked this out, I quit. reply Kapura 2 hours agoparentThis seems extremely unethical no matter who is doing it. reply cjk2 2 hours agorootparent100% agree (hence my point about quitting) but the problem is it's a difficult position to be in when everything is narrative driven and misreporting and propaganda are rife. You can sit there and do nothing and wait for your enemy to paint you in a bad light and the next thing you know your usual political allies are throwing money and aid at your enemy. Or you protect your citizens as best as possible by entering the game. The moral high ground may have a higher body count. This point applies to both sides for ref. And because it's a war, the rules of fair play go out of the window until people are on trial afterwards. reply gmdrd 40 minutes agorootparent> Or you protect your citizens as best as possible by entering the game. It doesn't seem reasonable to me to assume that the main objective of any government is to protect its citizens. reply cjk2 14 minutes agorootparentI think that's just paranoia. The citizens generally are the government. It's not optimal but without the citizens there is no government. reply FpUser 1 hour agorootparentprev>\"until people are on trial afterwards\" Speaking of trials: U.S. lawmakers had voted to sanction ICC if it tries to prosecute citizens of the US or it's allies. I guess it is always one rule for thee and another one for me. So much for rules based order. reply cjk2 4 minutes agorootparentWell there's a problem here. There are rules. But no one really has to abide to them. There are no consequences against a sovereign nation other than political allegiance risk or travel risks for convicts. reply croes 43 minutes agoparentprevSo it's ok to do something wrong, if others do it too? reply cjk2 17 minutes agorootparentIt's a war. It's about doing wrong things until someone capitulates. I mean it'd be nice not to have them but as a species we're stupid animals with stupid ideas so there's no end of it in sight. I don't agree with any of it for ref. reply CommanderData 27 minutes agoparentprev [–] I think killing 15,000 children would garner organic support from any human with a functioning conscious. It is why Israel has already lost support globally with normal people especially the young. Those people are only to get more polarised, the political stooges in government however (who are easily pressured - which Israel does extremely effectively) have had to tow the line. (before any Israeli PR bot tries to discredit the number of children murders, Hamas's health ministry has historically been extremely accurate in past conflicts, with past figures verified by various external impartial authorities and there is zero reason to not believe them, I scrutinized the number myself before I decided to pick a side ... What's more is the number is probably higher as children lay dead in collapsed buildings) Children do not deserve to die. Let alone be tortured, which Israel has done without punishment. reply cjk2 11 minutes agorootparent [–] An exercise with war casualty stats you can do is look at previous wars and look at the variation in estimates in body counts for each side. Then factor that into the news you are reading it. End game is there has never been a credible body count from even a small scale war. So to claim anyone is right here on either side is probably selective bias on your part. At best when the dust has figuratively and literally settled, it'll be a decade before anyone has an even remotely credible count. Manipulation of the body counts is easy material for propaganda. It has been since the dawn of war. Factor child soldiers into these arguments and it gets very grey. The number itself without compounding facts has little meaning. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Israel's Diaspora Affairs Ministry conducted a covert influence campaign aimed at Black lawmakers and young progressives in the U.S. and Canada to sway public opinion on the Gaza war.",
      "The campaign, carried out by a political firm, utilized fake accounts and websites to disseminate pro-Israel and Islamophobic content.",
      "This operation was initially reported by Haaretz in March."
    ],
    "commentSummary": [
      "The discussion focuses on Israel's alleged use of fake social media accounts to influence U.S. lawmakers, raising concerns about transparency, ethical issues, and potential backlash.",
      "OpenAI's halt of covert influence campaigns with ChatGPT sparks debates on AI's role in deception and internet warfare, highlighting the historical context of propaganda and the evolution of espionage through social media.",
      "The conversation critiques media manipulation, particularly in the Israel-Palestine conflict, and the influence of lobbying groups like AIPAC, while addressing broader implications for free speech and political discourse."
    ],
    "points": 359,
    "commentCount": 229,
    "retryCount": 0,
    "time": 1717591284
  },
  {
    "id": 40585842,
    "title": "First Vulkan 1.3 Driver for Apple's M1 Achieves 98.3% Conformance",
    "originLink": "https://rosenzweig.io/blog/vk13-on-the-m1-in-1-month.html",
    "originBody": "Vulkan 1.3 on the M1 in 1 month 5 Jun 2024 Finally, conformant Vulkan for the M1! The new “Honeykrisp” driver is the first conformant Vulkan® for Apple hardware on any operating system, implementing the full 1.3 spec without “portability” waivers. Honeykrisp is not yet released for end users. We’re continuing to add features, improve performance, and port to more hardware. Source code is available for developers. HoloCure running on Honeykrisp ft. DXVK, FEX, and Proton. Honeykrisp is not based on prior M1 Vulkan efforts, but rather Faith Ekstrand’s open source NVK driver for NVIDIA GPUs. In her words: All Vulkan drivers in Mesa trace their lineage to the Intel Vulkan driver and started by copying+pasting from it. My hope is that NVK will eventually become the driver that everyone copies and pastes from. To that end, I’m building NVK with all the best practices we’ve developed for Vulkan drivers over the last 7.5 years and trying to keep the code-base clean and well-organized. Why spend years implementing features from scratch when we can reuse NVK? There will be friction starting out, given NVIDIA’s desktop architecture differs from the M1’s mobile roots. In exchange, we get a modern driver designed for desktop games. We’ll need to pass a half-million tests ensuring correctness, submit the results, and then we’ll become conformant after 30 days of industry review. Starting from NVK and our OpenGL 4.6 driver… can we write a driver passing the Vulkan 1.3 conformance test suite faster than the 30 day review period? It’s unprecedented… Challenge accepted. April 2 It begins with a text. Faith… I think I want to write a Vulkan driver. Her advice? Just start typing. Thre’s no copy-pasting yet – we just add M1 code to NVK and remove NVIDIA as we go. Since the kernel mediates our access to the hardware, we begin connecting “NVK” to Asahi Lina’s kernel driver using code shared with OpenGL. Then we plug in our shader compiler and hit the hay. April 3 To access resources, GPUs use “descriptors” containing the address, format, and size of a resource. Vulkan bundles descriptors into “sets” per the application’s “descriptor set layout”. When compiling shaders, the driver lowers descriptor accesses to marry the set layout with the hardware’s data structures. As our descriptors differ from NVIDIA’s, our next task is adapting NVK’s descriptor set lowering. We start with a simple but correct approach, deleting far more code than we add. April 4 With working descriptors, we can compile compute shaders. Now we program the fixed-function hardware to dispatch compute. We first add bookkeeping to map Vulkan command buffers to lists of M1 “control streams”, then we generate a compute control stream. We copy that code from our OpenGL driver, translate the GL into Vulkan, and compute works. That’s enough to move on to “copies” of buffers and images. We implement Vulkan’s copies with compute shaders, internally dispatched with Vulkan commands as if we were the application. The first copy test passes. April 5 Fleshing out yesterday’s code, all copy tests pass. April 6 We’re ready to tackle graphics. The novelty is handling graphics state like depth/stencil. That’s straightforward, but there’s a lot of state to handle. Faith’s code collects all “dynamic state” into a single structure, which we translate into hardware control words. As usual, we grab that translation from our OpenGL driver, blend with NVK, and move on. April 7 What makes state “dynamic”? Dynamic state can change without recompiling shaders. By contrast, static state is baked into shader binaries called “pipelines”. If games create all their pipelines during a loading screen, there is no compiler “stutter” during gameplay. The idea hasn’t quite panned out: many game developers don’t know their state ahead-of-time so cannot create pipelines early. In response, Vulkan has made ever more state dynamic, punctuated with the EXT_shader_object extension that makes pipelines optional. We want full dynamic state and shader objects. Unfortunately, the M1 bakes random state into shaders: vertex attributes, fragment outputs, blending, even linked interpolation qualifiers. Like most of the industry in the 2010s, the M1’s designers bet on pipelines. Faced with this hardware, a reasonable driver developer would double-down on pipelines. DXVK would stutter, but we’d pass conformance. I am not reasonable. To eliminate stuttering in OpenGL, we make state dynamic with four strategies: Conditional code. Precompiled variants. Indirection. Prologs and epilogs. Wait, what-a-logs? AMD also bakes state into shaders… with a twist. They divide the hardware binary into three parts: a prolog, the shader, and an epilog. Confining dynamic state to the periphery eliminates shader variants. They compile prologs and epilogs on the fly, but that’s fast and doesn’t stutter. Linking shader parts is a quick concatenation, or long jumps avoid linking altogether. This strategy works for the M1, too. For Honeykrisp, let’s follow NVK’s lead and treat all state as dynamic. No other Vulkan driver has implemented full dynamic state and shader objects this early on, but it avoids refactoring later. Today we add the code to build, compile, and cache prologs and epilogs. Putting it together, we get a (dynamic) triangle: April 8 Guided by the list of failing tests, we wire up the little bits missed along the way, like translating border colours. /* Translate an American VkBorderColor into a Canadian agx_border_colour */ enum agx_border_colour translate_border_color(VkBorderColor color) { switch (color) { case VK_BORDER_COLOR_INT_TRANSPARENT_BLACK: return AGX_BORDER_COLOUR_TRANSPARENT_BLACK; ... } } Test results are getting there. Pass: 149770, Fail: 7741, Crash: 2396 That’s good enough for vkQuake. April 9 Lots of little fixes bring us to a 99.6% pass rate… for Vulkan 1.1. Why stop there? NVK is 1.3 conformant, so let’s claim 1.3 and skip to the finish line. Pass: 255209, Fail: 3818, Crash: 599 98.3% pass rate for 1.3 on our 1 week anniversary. Not bad. April 10 SuperTuxKart has a Vulkan renderer. April 11 Zink works too. April 12 I tracked down some fails to a test bug, where an arbitrary verification threshold was too strict to pass on some devices. I filed a bug report, and it’s resolved within a few weeks. April 16 The tests for “descriptor indexing” revealed a compiler bug affecting subgroup shuffles in non-uniform control flow. The M1’s shuffle instruction is quirky, but it’s easy to workaround. Fixing that fixes the descriptor indexing tests. April 17 A few tests crash inside our register allocator. Their shaders contain a peculiar construction: if (condition) { while (true) { } } condition is always false, but the compiler doesn’t know that. Infinite loops are nominally invalid since shaders must terminate in finite time, but this shader is syntactically valid. “All loops contain a break” seems obvious for a shader, but it’s false. It’s straightforward to fix register allocation, but what a doozy. April 18 Remember copies? They’re slow, and every frame currently requires a copy to get on screen. For “zero copy” rendering, we need enough Linux window system integration to negotiate an efficient surface layout across process boundaries. Linux uses “modifiers” for this purpose, so we implement the EXT_image_drm_format_modifier extension. And by implement, I mean copy. Copies to avoid copies. April 20 “I’d like a 4K x86 Windows Direct3D PC game on a 16K arm64 Linux Vulkan Mac.” … “Ma’am, this is a Wendy’s.” April 22 As bug fixing slows down, we step back and check our driver architecture. Since we treat all state as dynamic, we don’t pre-pack control words during pipeline creation. That adds theoretical CPU overhead. Is that a problem? After some optimization, vkoverhead says we’re pushing 100 million draws per second. I think we’re okay. April 24 Time to light up YCbCr. If we don’t use special YCbCr hardware, this feature is “software-only”. However, it touches a lot of code. It touches so much code that Mohamed Ahmed spent an entire summer adding it to NVK. Which means he spent a summer adding it to Honeykrisp. Thanks, Mohamed ;-) April 25 Query copies are next. In Vulkan, the application can query the number of samples rendered, writing the result into an opaque “query pool”. The result can be copied from the query pool on the CPU or GPU. For the CPU, the driver maps the pool’s internal data structure and copies the result. This may require nontrivial repacking. For the GPU, we need to repack in a compute shader. That’s harder, because we can’t just run C code on the GPU, right? …Actually, we can. A little witchcraft makes GPU query copies as easy as C. void copy_query(struct params *p, int i) { uintptr_t dst = p->dest + i * p->stride; int query = p->first + i; if (p->available[query] || p->partial) { int q = p->index[query]; write_result(dst, p->_64, p->results[q]); } ... } April 26 The final boss: border colours, hard mode. Direct3D lets the application choose an arbitrary border colour when creating a sampler. By contrast, Vulkan only requires three border colours: (0, 0, 0, 0) – transparent black (0, 0, 0, 1) – opaque black (1, 1, 1, 1) – opaque white We handled these on April 8. Unfortunately, there are two problems. First, we need custom border colours for Direct3D compatibility. Both DXVK and vkd3d-proton require the EXT_custom_border_color extension. Second, there’s a subtle problem with our hardware, causing dozens of fails even without custom border colours. To understand the issue, let’s revisit texture descriptors, which contain a pixel format and a component reordering swizzle. Some formats are implicitly reordered. Common “BGRA” formats swap red and blue for historical reasons. The M1 does not directly support these formats. Instead, the driver composes the swizzle with the format’s reordering. If the application uses a BARB swizzle with a BGRA format, the driver uses an RABR swizzle with an RGBA format. There’s a catch: swizzles apply to the border colour, but formats do not. We need to undo the format reordering when programming the border colour for correct results after the hardware applies the composed swizzle. Our OpenGL driver implements border colours this way, because it knows the texture format when creating the sampler. Unfortunately, Vulkan doesn’t give us that information. Without custom border colour support, we “should” be okay. Swapping red and blue doesn’t change anything if the colour is white or black. There’s an even subtler catch. Vulkan mandates support for a packed 16-bit format with 4-bit components. The M1 supports a similar format… but with reversed “endianness”, swapping red and alpha. That still seems okay. For transparent black (all zero) and opaque white (all one), swapping components doesn’t change the result. The problem is opaque black: (0, 0, 0, 1). Swapping red and alpha gives (1, 0, 0, 0). Transparent red? Uh-oh. We’re stuck. No known hardware configuration implements correct Vulkan semantics. Is hope lost? Do we give up? A reasonable person would. I am not reasonable. Let’s jump into the deep end. If we implement custom border colours, opaque black becomes a special case. But how? The M1’s custom border colours entangle the texture format with the sampler. A reasonable person would skip Direct3D support. As you know, I am not reasonable. Although the hardware is unsuitable, we control software. Whenever a shader samples a texture, we’ll inject code to fix up the border colour. This emulation is simple, correct, and slow. We’ll use dirty driver tricks to speed it up later. For now, we eat the cost, advertise full custom border colours, and pass the opaque black tests. April 27 All that’s left is some last minute bug fixing, and… Pass: 686930, Fail: 0 Success. The future The next task is implementing everything that DXVK and vkd3d-proton require to layer Direct3D. That includes esoteric extensions like transform feedback. Then Wine and an open source x86 emulator will run Windows games on Asahi Linux. That’s getting ahead of ourselves. In the mean time, enjoy Linux games with our conformant OpenGL 4.6 drivers… and stay tuned. Baby Storm running on Honeykrisp ft. DXVK, FEX, and Proton. Back to home",
    "commentLink": "https://news.ycombinator.com/item?id=40585842",
    "commentBody": "Vulkan1.3 on the M1 in one month (rosenzweig.io)280 points by todsacerdoti 3 hours agohidepastfavorite87 comments delta_p_delta_x 15 minutes agoHacker News regularly makes me feel very stupid, and none more so than blog posts by Alyssa Rosenzweig. I couldn't even write an engine that used Vulkan in six months, nor a software rasteriser; meanwhile, Alyssa wrote an entire compliant driver on a non-native operating system running on locked-down hardware in one month flat. What am I doing with my life? reply dagmx 2 hours agoprevVery impressive work, and a great testament to the value of shared, iterative and open components. I'll be curious how long it takes Proton to be ported, though I suspect even with an optimal Vk implementation, many games will run terribly due to the difference in GPU architecture + arm translation overhead + proton itself (however negligible). Still, I remain optimistic that more games will target unified memory and ARM in the future on desktops, as SoCs become more prevalent like the snapdragon. reply talldayo 2 hours agoparentThere will be a small bit of irony if the gaming experience on Asahi ends up easier and more straightforward than MacOS. Really just drives home how big of a mistake it was ignoring Vulkan for the past decade on Apple's behalf, but lord knows they won't admit that until it's too late. reply dagmx 2 hours agorootparentVulkan would likely not really have helped macOS gaming in any form. I consider it a red herring that people point to. The number of games that run natively on Vulkan is negligible. The number of games that run on Metal by comparison (natively) is orders of magnitude higher. If we're ONLY talking about the ability to use Proton, Apple does now have Game Porting Toolkit that does effectively the same thing with comparable performance characteristics if you take out the other contributing overhead elements. reply LucidLynx 2 hours agorootparent> The number of games that run natively on Vulkan is negligible. Yep, that's unfortunately true... https://www.carette.xyz/posts/state_of_vulkan_2024/ reply jsheard 2 hours agorootparentMuch of the Vulkan support was motivated by Stadia, not so much because Stadia was successful, but because Google was throwing huge amounts of money at developers to port their games to Stadia regardless. When Stadia was discontinued at the end of 2022, sure enough the number of new Vulkan games immediately plummeted. reply talldayo 2 hours agorootparentprevI'd be curious to see how many Vulkan-native games there are that don't run under Proton. The only one that comes to mind is Destiny 2, but that was more because of anticheat as I remember it. reply tapoxi 58 minutes agorootparentprevWhy is this article comparing game engines with games? reply talldayo 2 hours agorootparentprevYou can't ship Game Porting Toolkit with your game, though. DXVK doesn't have this limitation, and developers use it all the time. In effect, this means the number of games you get with native Vulkan coverage is a magnitude higher than the total sum of all Metal-native desktop titles. reply dagmx 2 hours agorootparentThis isn't completely true. Crossover is allowed to ship the GPTK components for API translation (which they currently do as of 23.5) and games are allowed to embed Crossover elements like they did in the pre-Metal days with Cider etc. All this talk of DXVK doesn’t explain why games based on engines with native metal support aren’t on a Mac either. reply p_l 2 hours agorootparentWhile it might be true, I'd recommend to check if the licenses do, in fact, recurse this way. They might not. reply talldayo 2 hours agorootparentprevCrossover is a paid product, and even then has pretty poor DX12 support compared to Proton. Up until a couple years ago it didn't work at all. Again; if Apple had just supported Vulkan alongside Metal like a normal non-paranoid company, their users wouldn't be caught in the middle of this. It's such a blatantly obvious solution with no user-facing downsides. It's shocking that anyone would defend the status-quo when it's so notoriously and obviously broken. reply dagmx 2 hours agorootparentYour assertions aren’t backed by history though. Prior to metal and Vulkan, many games used Cider to target Mac despite having the same APi (at that point, an up to date OpenGL). And many of the current game devs do have Metal versions of their engines that they target towards iOS, yet don’t have a macOS version. The fact of the matter is that it comes down to market share. Macs have historically not had a large user base that also had capable GPUs. It’s not been worth supporting that tiny market share It’s the same on Linux. Why are there so few Linux native games? The argument that Vulkan would have solved this is completely incongruent with the reality of the state of gaming. This will inevitably go back to “well at least we could have used proton” but that’s also not true, and besides there’s GPTK. And then the argument becomes circular , because all evidence points to: devs just don’t care about Mac’s from a support perspective. You can make it easier, they still won’t come until the possible demographic size is larger. and even with DXVK, what about all the other platform specific differences? Vulkan wouldn't solve those either. reply out_of_protocol 35 minutes agorootparent> The argument that Vulkan would have solved this but it in fact did! You can play 90% of games on linux precisely because of Vulkan. Just boot Steam Deck or whatever and play. There is no way it would work out without Vulkan reply talldayo 2 hours agorootparentprevI mean I owned a Mac back then, I remember pretty fondly that pre-Catalina MacOS was a fairly well-targeted platform. OpenGL was working for them; you could play first-person shooters, online games with fancy graphics, the kit and kaboodle. Things weren't perfect, but there was a lot of functional cross-platform software back when Apple commit to maintaining common APIs. You cannot deny that an entire ecosystem of software that was once cross-platform had to now choose sides, if they would support Metal or OpenGL. I watched it collapse with a single system update. > Why are there so few Linux native games? The argument that Vulkan would have solved this is completely incongruent with the reality of the state of gaming. My brother in Christ, the \"reality of the state of gaming\" is that Mac owners are buying Steam Decks just to access the games Apple tries to kneecap. Vulkan fixed it, and Valve commoditized Apple's compliment. I don't want things to be this way. I think Mac owners should have easy access to the games they want to play, but Apple insisted otherwise for so long and refused to ever admit that they were wasting their time. It's part of the reason I got rid of my Mac so I could daily-drive Linux; they were wrong, and other platforms were right. reply dagmx 1 hour agorootparentGo back and see how many of those games were running Cider though. The switch to kill 32-bit games killed more games than the deprecation of OpenGL did. The number of games that targeted OpenGL was ALSO shockingly low. This is something that OSS fans do not want to reconcile: Open source graphics APIs have LONG lost out to proprietary graphics APIs in gaming. OpenGL had a very small base in games, Vulkan is even smaller. and again, you went exactly back to where I knew you would and pre-empted it because it's the obvious playbook of answers. Vulkan didn't fix gaming on Linux. Proton did. And again, you ignore the key part of the sentence: NATIVE Linux has fewer *native* AAA games than macOS. Vulkan has not solved that. The SteamDeck is a product targeted at gamers that provided a new value proposition: AAA gaming on the go. Which further reinforces my point that API is irrelevant to the discussion, its about demographic. Apple has a strong gaming demographic on the mobile side (with metal no less). The steam deck didn't introduce new Vulkan/Proton capabilities. Why was Linux gaming stagnant before it? Doesn't that exactly reinforce that its the form factor proposition that made it skyrocket? reply talldayo 1 hour agorootparent> switch to kill 32-bit games killed more games than the deprecation of OpenGL did. It did. If you want to parade around how cool Cider is, it doesn't make much sense to venerate the update that killed it. > Linux has fewer native AAA games than macOS. Vulkan has not solved that. Imagine all the Steam Deck users that are pissed because they can't play Resident Evil 8 and Tomb Raider natively! They must be super upset, and envious of Apple's superior native version. Surely. Hey, here's a magic question for you; which do you think will get supported longer, a DirectX title running on Proton or an Apple-native title running on Apple software. Do you trust Apple to keep supporting your game as long as Proton would keep supporting it? reply christkv 23 minutes agorootparentConsidering backward compatibility on ios my bet is on proton for sure. fngjdflmdflg 1 hour agorootparentprevI think you make a good point in general, but \"likely not really have helped macOS gaming in any form\" I think is taking it too far. The reason why many developers only use dx11/12 is because adding more graphics backends increases their support surface unnecessarily, being that Vulkan only works on Windows (and Linux), while dx11/12 work on Windows on Xbox. As an extreme example of this, apparently Cyberpunk 2077 ran Vulkan on Stadia but did not enable it for Windows as an option. (Because what is the point?) If Vulkan worked on both Windows and Mac then developers would have more reason to support it. It would likely also mean game engines would put more time into making Vulkan better. For a related example, Apple had to individually contribute it's Metal backend to Blender because (presumably) nobody wanted to work on it.[0] Why? Because they already have to support OpenGL, Cuda, OptiX, Intel OneAPI, ROCm HIP, and Vulkan. Clearly, something is very wrong with graphics APIs right now. If Apple supported Vulkan, it would have allowed everyone who is not Windows to benefit by making Vulkan become more standard. Luckily Xbox seems to be kind of dying right now so perhaps supporting dx11/12 will not be as important in the future. But the point is if you have a small portion of the market it just doesn't make sense for developers to spend time entering that market for little reward. Another major point is the development cycle. Since Metal only works on Apple devices that makes developing for it more annoying for game developers which are mainly using Windows. It means they will have to switch to a Mac device to debug issues with their Metal backend. By supporting Vulkan Apple would allow for a much smoother developer experience. (While Metal Developer Tools for Windows exists, as I understand it it only allows you to compile shaders for Metal on Windows, but to actually test that anything works you will need an actual MacOS device.). [0] https://code.blender.org/2023/01/introducing-the-blender-met... reply dagmx 26 minutes agorootparentMany engines support Metal, in addition to the plethora of console specific APIs. Saying DX11/12 works on Xbox is glossing over that that it's not actually the same DX12 that works on desktop. You then say that if they used Vulkan, it would mean they wouldn't have to debug on a Mac. This is overly optimistic. Even in the OpenGL days, you'd have to test on all platforms because there's so many variances. In general, the game designers are rarely working multi platform, and it's down to the engine devs themselves + QA. Neither should or would be blindly trusting that things are portable. Vulkan/DX on AMD/NVIDIA also perform differently enough that you can't just assume parity. Your other statement about nobody wanting to work on Metal for Blender is a bit odd too. There's no current Vulkan or DX backend for it. Does that mean they're not desired either? AMD and NvidiA contribute support for their favoured API's as well, like Optix that people still desire. > If Apple supported Vulkan, it would have allowed everyone who is not Windows to benefit by making Vulkan become more standard. This didn't prove itself out when OpenGL was a thing. OpenGL was everywhere but barely used. > Luckily Xbox seems to be kind of dying right now so perhaps supporting dx11/12 will not be as important in the future. That doesn't mean that Vulkan would replace it though. Windows gaming still eclipses Linux, and game developers have to target multiple APIs either way. There's very little upside to them switching to Vulkan for it. Also bear in mind that DX is much more than D3D. It's a lot of APIs. There's many reasons to use DX beyond just the 3D graphics APIs. reply shmerl 1 hour agorootparentprev> Vulkan would likely not really have helped macOS gaming in any form Why not? With fully conformant Vulkan you could have run all the games that work in Wine without being hit by limitations of MoltenVK that affect performance and compatibility, same as you can run them on Linux now. So Vulkan could totally be very crucial for making gaming on macOS not being some second class experience. Apple simply demonstrate they don't care about gaming and gamers. They only care about \"approved by Apple way of gaming\" which is totally not the same thing but which Apple always does for everything anyway, shoving in users' faces \"we know better than you what you need\". That's why I will always say that gamers should avoid using Apple. reply astrange 27 minutes agorootparentYou can't \"just support Vulkan\", it's too low level. To benefit, the implementation would have to target the GPU specifically since Apple GPUs are different from discrete GPUs. reply burnte 30 minutes agorootparentprev> Really just drives home how big of a mistake it was ignoring Vulkan for the past decade on Apple's behalf, but lord knows they won't admit that until it's too late. They'll never admit it. They only caved on USB-C because they were legally required to. They won't cave on this, or NVidia support, or putting the charge port on the side of the mouse and not the belly, etc. reply mantas 15 minutes agorootparentNobody required USB-C charging for laptops and iPads. It would have happened on iPhones eventually too. reply eptcyka 5 minutes agorootparentThey were the (one of the) first ones to usb-c on laptops. I don’t think they were ever planning to do usb-c on the iPhone, they had no reason to be this late. reply rowanG077 2 hours agorootparentprevWhy would that be ironic. Apple wilfully is holding gaming back on OSX by ignoring Vulkan and having a pure shit opengl implementation. reply bilbo0s 2 hours agorootparentYou may have a good point with the opengl thing. But the number of games that run on Vulkan is so vanishingly small that we can't credibly say that not having a Vulkan driver holds Apple back on gaming. I'd bet any money that the number of games that work on Metal is wayyyy higher than the minuscule number of games that run on Vulkan. Which should tell you how few Vulkan games there are. reply jeroenhd 20 minutes agorootparentThe amount of games that work with Vulkan may be very small, but at least they have a chance of working. The de facto standard API for games on computers, DirectX, will never receive a port. If you think Vulkan support for games is insignificant, look at Metal support. I think Vulkan makes a lot of sense for macOS, actually; the Nintendo Switch, to which tons of games have been ported already, uses the ARM+Vulkan design and has a relatively weak GPU (but much weaker, as it's very old). It's not quite native, but a lot of supporting libraries that work on the Switch will also work on Mac. Everyone is talking about porting Windows games to Mac, but with the way things are developing, porting Switch games to Mac may actually make more sense, assuming Nintendo's promises for the upcoming Switch replacement are to be believed. reply talldayo 1 hour agorootparentprev> But the number of games that run on Vulkan is so vanishingly small Really? 7 out of the 10 most popular games on Steam work fine if you have Vulkan 1.3 drivers: https://www.protondb.com/explore?sort=playerCount reply dagmx 1 hour agorootparentof the top 10, I count 3: Counter Strike 2 Dota 2 Stardew Valley How are you getting 7/10 unless you're also counting games that happen to run under Proton, which aren't native? And regardless, that's a really small sample size to extrapolate from, when it's easy to get a more extensive list that is around ~250 games total. https://www.vulkan.org/made-with-vulkan https://www.pcgamingwiki.com/wiki/List_of_Vulkan_games Of which, a massive percentage are based on engines that have Metal support as well. reply talldayo 1 hour agorootparentI'm sorting by playercount, which seems to be corroberated here: https://steamcharts.com/top I am counting Proton games because they very explicitly are supported by sufficient Vulkan coverage. In case you weren't paying attention that's kinda the only reason anyone cares about Apple Silicon getting Vulkan 1.3 coverage in the first place. > when it's easy to get a more extensive list that is around ~250 games total. You're right, when we push it out to 1000 titles it gets closer to 80% playable: https://www.protondb.com/dashboard > Of which, a massive percentage are based on engines that have Metal support as well. ...yeah, you keep telling yourself that. One day, they'll definitely flip the switch on MacOS support. Certainly. reply dagmx 1 hour agorootparentOkay, so you're counting non-native games when the discussion is AGAIN clearly about native games. If we're talking about translation layers, then it's irrelevant because Whisky/Crossover can run them as well. This is just you being completely disingenuous now and purposefully avoiding the actual point of the discussion. reply talldayo 1 hour agorootparentThis was never about native games. It would be very convenient if Proton didn't exist and native games were the only playable option, but in many cases Proton titles run better than native Windows. It's an equivalent, if not superior, experience. It would be a lot more disingenuous if the Steam Deck didn't exist and Linux gaming was a farce. But... it's not. And the only thing stopping Apple from enjoying the same spoils is a little bit of humility. reply rowanG077 2 hours agoparentprevA ton of games which don't require dxvk are running at 60+ FPS. See https://vt.social/@lina/112524118075585601 Alyssa, the machine that she is, has also been working on improving FEX performance recently. reply aurareturn 2 hours agoprevI wonder if this effort to add Vulcan to Linux and then translate DirectX in Asahi Linux would impact Apple's dream of landing AAA games on Apple Silicon. Apple would like AAA developers to port their AAA games over to Metal so that the game has one code base but can run on iPhones, iPads, Macs, and the Vision Pro. Perhaps Mac gamers will install Asahi Linux in order to play AAA PC titles. reply littlecranky67 2 hours agoparent> would impact Apple's dream of landing AAA games on Apple Silicon Apple's dream is not to have AAA games land on Apple Silicon - they can do this with Proton-like layer, like they did with GPTK. Apple's wet dream is to have AAA games land in the Apple App Store - not on Steam or Epic Game store. That is why the GPTK effort is only have assed (and the license prevents Valve integrating it into steam directly). reply skrrtww 1 hour agorootparentThis is a bit of a tangential rant, so I apologize. But I recently tried to update Death Stranding, the flagship AAA game that Apple landed on macOS and in the App Store. The game itself is 77.5 GB; it had required a little over 150 GB to install. I shrugged that off at the time; they haven't figured out how to decompress on the fly, whatever. I had about 50GB free on my (1 TB M1 Max) machine at the time of the update, and the App Store told me I didn't have enough free space to update. I balked and looked at the update description, which was just \"Various minor bug fixes.\" The update size was...75 GB. They expect me to re-download the entire game for various minor bug fixes. (I would \"just blame the developer\" here, but Apple clearly invested a lot into having this game available on macOS, in the App Store, and got Hideo Kojima to show up at WWDC (a full year ago, remember) and brag about how nice the entire experience is. Some of Apple's engineers probably worked directly on this port.) It then sunk in that I didn't just need 75 GB of free space. I needed 150 GB. The App Store will completely download and completely decompress the entire game before replacing it. That is the patching process for the game. You need 231 GB free at all times on your machine to have and update a 77GB game. This is completely insulting given Apple's storage prices, and the fact that the App Store does not let you install apps on an external drive. Apple clearly doesn't get it. They act like they're nominally putting in the effort, but then it's still just completely half-assed even when things are played exactly like they want. reply coldpie 1 hour agorootparentThere's like, two guys at Apple who care about gaming. Everyone once in a while they manage to convince marketing to say something about it and rope in a couple more devs to half-assedly hammer out some tickets before they can go back to their normal tasking. There's no traction internally to taking PC gaming seriously at Apple. reply MBCook 11 minutes agorootparentFrom what I’ve heard there are a ton more than 2. But none of them have the power. reply astrange 26 minutes agorootparentprevYou don't need indoor hobbies when the weather is this nice. reply bitwize 17 minutes agorootparentprev> I had about 50GB free on my (1 TB M1 Max) machine at the time of the update, and the App Store told me I didn't have enough free space to update. I balked and looked at the update description, which was just \"Various minor bug fixes.\" The update size was...75 GB. They expect me to re-download the entire game for various minor bug fixes. It is an unfortunate truth that one must now bear in mind, that games are for all intents and purposes enterprise software. They are as critical to profitability for the companies involved as enterprise software. They are large, distributed applications which are planned, budgeted, and staffed much like enterprise software. Accordingly, there is little to no concern for performance except when it's absolutely critical: the rendering pipeline and the netcode, for instance. For things like updates, where it would be easy to make some optimizations to reduce download size, those optimizations will not be taken. So you will redownload the entire game, including all of the uncompressed audio clips, every time someone changes a byte somewhere and it's shipped as an update. reply AltruisticGapHN 37 minutes agorootparentprevI don't get it. They already make a ton of money on mobile games - why not embrace Steam and Proton on their platform? What's the money in AAA games on their platform? edit: this is to say if they had \"wet dreams\" about selling AAA games on Mac, we'd have seen the tides moving a long time ago. reply snarfy 1 hour agorootparentprevYour comment puts it very succinctly. They don't care about your gaming experience on Mac. They care about money. I wish they weren't so short sighted here. They can't see through the trees that maybe they won't make money selling games through the app store, but they would make money selling more Macs. reply MBCook 8 minutes agorootparentThey didn’t really care about it before the Mac App Store either. The OS and its libraries just aren’t designed for high performance gaming the way MS has put time into that on Windows. They have Metal, which is supposed to be nice. But that’s their Direct3D. Where is all the other DirectX equivalent? From what I’ve heard anecdotally that seems to be a big part of the problem. It’s not just that they don’t have Vulcan (which I think is a red herring). Or the GPUs were abysmal (they were on most models before Apple Silicon). There’s no whole-picture. Just pieces. reply rched 1 hour agorootparentprevSeems like the opposite is true. If their primary motivation was more App Store sales why not allow GPTK games only through the App Store? reply rched 1 hour agorootparentprevThis makes no sense to me. Apple does nothing to prevent AAA games on the App Store also being released on Steam. I think it’s more likely that the GPTK license is to encourage developers to make high quality native ports rather than devs checking a box to make their game available on Mac. reply jandrese 1 hour agorootparentFor whatever reason Steam just doesn't seem very popular with Mac users. In the latest Hardware survey Mac users were outnumbered by Linux users by 50%. https://store.steampowered.com/hwsurvey/Steam-Hardware-Softw... reply coldpie 1 hour agorootparentIt's because Apple told Steam users to fuck off like 3 times in 5 years (nuking 32-bit support; no Vulkan/OpenGL support; switching to ARM). Users and game devs got the message Apple was sending loud & clear. reply jorvi 1 hour agorootparentprevApple killed support for legacy 32-bit applications a good while back, which killed support for virtually every Mac game port. reply nozzlegear 40 minutes agorootparentprevI play games on my Mac, but I don't use Steam. I just play World of Warcraft which is a native Apple Silicon game, and a few other games that don't require Steam. reply babypuncher 2 hours agorootparentprev> Apple's wet dream is to have AAA games land in the Apple App Store This is why their efforts are largely doomed. I'd love to play more AAA games on my Macbook, but too many of them require a separate purchase from the App Store to make that happen, rather than just putting the Mac versions on Steam alongside the Windows version. If you're going to make me choose between playing a game on my laptop and playing it on my gaming desktop, I will choose the desktop every single time. It would be nice to take more of those games with me when I travel, but it's not a frequent enough use-case for me to give up all the benefits of real PC gaming. reply behnamoh 1 hour agorootparentWhat do we do as Apple users stuck in this ecosystem whose main goal is to extract more money from our pockets? I hate Windows with all my gut but at least I had a sense of freedom about the software I wanted to install on Windows, including games. On Mac, I'm constantly reminded that beyond this facade of user-friendly UI and nice visuals, there's a greedy company whose market cap is $3T but doesn't give a flying f* about the end-user because it wants to make even more money. reply littlecranky67 2 hours agorootparentprev> This is why their efforts are largely doomed I'm not as certain about that as you are. I think there are - as always - different streams withing that mega corporation. It is probably sales-oriented people wanting nothing but AAA games on the App Store and be okay with the Mac not being a major choice for games if that doesn't happen. The other stream is probably more enthusiastic (especially with the Apple Silicon being \"enough\" gaming machines for 1080p@60) and wants to bring gaming over - that is probably the stream that made GPTK happen and give a licensing exception to crossover to be able to integrate it in their department. I'm in the later department, hoping we will see a proton-like GPTK that can run my steam library. reply babypuncher 2 hours agorootparent> hoping we will see a proton-like GPTK that can run my steam library. I would absolutely LOVE this! I also know a lot of people who would be a lot more interested in buying a Macbook if this were a thing. reply littlecranky67 2 hours agorootparentWell regarding the Macbook/Laptop case vs. Desktop: With me typing this on the MacMini M2 base model (8GB/256GB) retailed at 650€ I can assure you, it is a nice gaming machine hardware-wise. Not for latest and greatest AAA games of course, but I played through all the Tomb Raider reboot games on this machine (using Rosetta2 as they are non-native on Steam) on High details with 1080p. Now the major issue is software, there is just not enough games available. I do use Heroic+Crossover Wine to run some other games, but it is just finnicky and only for the pro user - not average casual gamers. Saying this, Apple could launch a 999€ Mac Mini with focus on gamers with a custom M4 design soon, if they had a proper Proton-like layer. Heck, size & noise wise, the Mac Mini beats the current PS5 and Xboxes. But they only treat GPTK as a solution for developers, not gamers. And I highly suspect, this is only due to the fact that they won't get any percentage from games sales, as Steam is the dominant player here. reply Almondsetat 1 hour agorootparentprevWhat has proton got to do with x86 -> ARM ? reply littlecranky67 1 hour agorootparentx86 -> ARM is already done by apple (rosetta2). What they need is a wine layer to mimmick the Windows API to allow a large amount of existing games to run on macOS without the devs porting. Which DOES exist, it is called crossover (and uses wine and GPTK). But it is a 3rd party company. reply AltruisticGapHN 39 minutes agoparentprevPretty much, Asahi Linux could become the new Bootcamp for Mac users who dual booted to play games. FWIW I played Diablo II Resurrected on a M1 with \"Whiskey\". I was quite impressed that I was able to play a native Windows game just like that. It doesn't work anymore just because a stupid Blizzard launcher update now crashes and prevents starting the game. I also ran EverQuest II admittedly an old DX9 game but still the game ran beautifully on a M1 mac mini at 1440p. Linux Asahi may help also with older x32 titles like Guild Wars. reply wmf 2 hours agoparentprevApple already released the Game Porting Toolkit. reply rowanG077 2 hours agoparentprevIf Apple truly wanted that they could simply implement Vulkan. reply galad87 2 hours agorootparentWhy Vulkan? Let's implement DirectX directly and call it a day. reply treyd 1 hour agorootparentMicrosoft would never allow them to use the trademark. reply jeroenhd 14 minutes agorootparentWhile that's true, I don't think they'd need the trademark. They can call it Apple StraightforwardY, duplicate any trademarked APIs and redirect the old ones to their own methods for \"compatibility\". The Oracle v Google lawsuit proved that APIs can be implemented, even by competitors. I see even less incentive for Apple to concede defeat and implement Microsoft's API than for them to port Vulkan. After all, they've already shipped a graphics engine that a handful of games have ever used with their OS so they have experience! reply babypuncher 2 hours agorootparentprevVulkan is how Direct3D is implemented on non-Windows operating systems, thanks to DXVK. reply galad87 2 hours agorootparentI know, but why make a Direct3D wrapper on Vulkan when in the end you are going to run only Direct3D games anyway. reply kbolino 1 hour agorootparentDirect3D means only Windows and Xbox. Vulkan means everything non-Apple except Xbox and PlayStation. It's not everything, but it's a larger base. reply galad87 31 minutes agorootparentWhat's larger than Windows + Xbox + Playstation for AAA games? reply babypuncher 1 hour agorootparentprevBecause DXVK already exists, and you do still want to run Vulkan anyways. There are AAA games out there using it instead of Direct3D. Natively re-implementing Direct3D APIs would be a tremendous amount of work for little or no real benefit. reply gigatexal 1 hour agoprevWhen I grow up I want to be half as competent as her at programming. Holy smokes. Bravo. reply jandrese 1 hour agoprevTheir shaders contain a peculiar construction: if (condition) { while (true) { } } condition is always false, but the compiler doesn’t know that. What is the purpose of this other than to be a poison pill for people writing compliant shader compilers? reply zamadatix 1 hour agoparentIsn't the main point of tests to be poison pills for people writing compilers so they can ensure any shaders passed to the compiler don't break it? I.e. it's not that the specific code is useful as is in practice but if a chunk of a shader ends up functionally simplifying to something like this in a particular instance your shader compiler still needs to handle it right even though it could be written better at that point. reply tombh 1 hour agoparentprevIs this how shaders \"panic\"? reply whitehexagon 1 hour agoprevAmazing, I'd only just updated for ES 3.2 support. It feels like my M1 was built for Asahi! although to be fair I only booted macos once on it, probably during install, or by mistake! Anyway great work! and nice to have these detailed updates. Are any browsers supporting 'zero copy rendering' or still layers and layers of compositor stuff going on? I seem to recall also getting stuck with webgl2 transform feedback triggering read backs. reply adastra22 1 hour agoprevIs any of this usable from within a VM? I dev on macOS and generally like it, but run a VMware image of Ubuntu for testing things. I do 3D graphics apps, and I’m not sure how good VMware’s pass through is. Is the Apple Silicon GPU virtualized in the VM? Could I run this distro and get better graphics performance? reply drpossum 2 hours agoprevCan someone explain how this relates to MoltenVK? This just removes the need for it as it's a native driver? reply Tiberium 2 hours agoparentMoltenVK is an implementation of Vulkan based on Metal so that it can be used on macOS to run Vulkan applications. Asahi Linux is an in-development Linux distribution to make Linux compatible with Apple's M series processors. This specific blog post is about making GPU-accelerated Vulkan work on Asahi. reply AltruisticGapHN 34 minutes agorootparentAnd I think the point is they will then be able to support DXVK to run Direct3D games. reply gary_0 2 hours agoparentprevThis is for Vulkan on Asahi Linux on M1 Mac hardware. MoltenVK provides Vulkan on top of Metal on MacOS. reply pantalaimon 2 hours agoparentprevThis is about Vulcan on Linux, not macOS reply rowanG077 2 hours agoparentprevMoltenVK translates metal to vulkan. Metal is the main graphics API on OSX (and iOS). This is a native vulkan implementation on linux. The blog post itself mentions that as well. This can't run directly on OSX so it won't remove the need for MoltenVK. reply ekianjo 1 hour agoprevwhy use Fex instead of Box86? reply tombert 1 hour agoprevA very large part of me is annoyed that Apple didn't open source Metal before Vulkan was released. OpenGL definitely was annoying to work with, but Vulkan is impossible for the average user to do anything with; drawing a spinning cube takes several hundred lines of code, which I've written and I still don't really understand! I realize that Vulkan isn't meant for a \"humans\" to write, it's basically meant to be a target, and that's fine, but it annoys me that you get people on forums that say \"learn Vulkan instead\" when people ask for help with OpenGL. Metal is substantially more fun to write than OpenGL or Vulkan; I haven't measured any performance stuff on it, but it certainly feels fast enough, and I've been able to do basic graphics experiments with it in way less time than I was ever able to accomplish with OpenGL or Vulkan. Still, whether or not it annoys me that Vulkan has become the new standard, portability of the standard is important, so obviously this announcement is a godo thing. reply hgs3 53 minutes agoparentPlease don't misinterpret this as gate keeping, but Vulkan is designed by and for professionals (not hobbyists). It is certainly not a rapid prototyping tool. Vulkan and OpenGL aren't comparable because Vulkan isn't merely a graphics API, but rather it's a low-level GPU API. For example you can use Vulkan in place of CUDA for running highly parallelized computations on the GPU (no graphics implied). reply tombert 47 minutes agorootparentYeah, I know, that's why I mentioned it being a target. I realize it's meant to be used for extremely low-level stuff. Which is fine, I don't get annoyed at x86_64 assembly being difficult to write. As I said, the thing that bothers me is when people act like Vulkan is a replacement for OpenGL (presumably because they've never actually written either and they just saw some YouTube demos), and it really isn't, or at least that's an incomplete picture. It's a replacement for OpenGL in the same way writing JVM bytecode directly is a replacement for Java; they fundamentally do the same or similar things, but one is sort of categorically different in its goal. I do wish that there was a more consumer-friendly API announced in addition to Vulkan though; I'm fine with replacing OpenGL with something newer and better-fitting for newer cards, but that's not what we got. That's why I was trying to say that I was annoyed that Apple didn't open source Metal, because I feel like it really could have taken that role. Also, while I'm aware that Vulkan doesn't inherently imply \"graphics\", I would also like to point out that CUDA and rocM are also considerably easier to write than the Vulkan equivalents, at least in the little examples I played with, though I will acknowledge that I felt Vulkan was better than OpenCL. reply hgs3 5 minutes agorootparent> I would also like to point out that CUDA and rocM are also considerably easier to write than the Vulkan equivalents I agree that Vulkan has more boilerplate, but I think that's because Vulkan is an open standard and makes fewer assumptions which means the API has more ceremony. For example Vulkan might be implemented for custom embedded hardware; its design helps give the manufacturer more leeway. CUDA and ROCm were designed by Nvidia and AMD for their own hardware and so they can bake in low-level assumptions which means a more streamlined API. reply tombert 0 minutes agorootparentWell ROCm is more or less designed to be portable between both; that's not strictly true but it's fairly similar to CUDA so I don't know how much of that boilerplate is reduced by the fact that it's AMD specific. Your point is fair though; Vulkan kind of runs on almost-literally everything made in the last nine years or so, so it can assume basically zero about any kind of underlying OS stuff. Still, Vulkan has convinced me that I have no desire to work that low-level of GPU programming, as I had zero fun playing with it. I guess I like having one layer up of abstraction; I'm happy enough to write CUDA, and I'm happy enough to write anything higher level than that. dlachausse 1 hour agoprev [–] > It begins with a text. > > Faith… I think I want to write a Vulkan driver. > > Her advice? > > Just start typing. I love this part. It is such great advice and it applies to so many things that we want to do in life, but never start because we become paralyzed with fear due to what seems to be an insurmountable task. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The \"Honeykrisp\" driver is the first Vulkan 1.3 conformant implementation for Apple's M1 hardware, developed without portability waivers and based on Faith Ekstrand's NVK driver for NVIDIA GPUs.",
      "The project achieved significant milestones, including a 99.6% pass rate for Vulkan 1.1 and 98.3% for Vulkan 1.3, and integrated Vulkan renderers for SuperTuxKart and Zink.",
      "Future goals include supporting Direct3D through DXVK and vkd3d-proton for Windows games on Asahi Linux, while currently offering conformant OpenGL 4.6 drivers for Linux games."
    ],
    "commentSummary": [
      "Alyssa Rosenzweig developed a Vulkan 1.3 driver for Apple's M1 chip in just one month, sparking discussions on Hacker News about gaming on ARM architecture and macOS.",
      "The debate critiques Apple's preference for Metal over Vulkan, which limits native game availability on macOS compared to Windows and Linux, and highlights the complexities of supporting multiple graphics APIs.",
      "Users express frustration with Apple's Game Porting Toolkit and high storage requirements for game updates, emphasizing the need for better gaming support and broader API compatibility to attract AAA games to macOS."
    ],
    "points": 284,
    "commentCount": 88,
    "retryCount": 0,
    "time": 1717600261
  },
  {
    "id": 40585212,
    "title": "Microsoft's CoPilot+ and Privacy Concerns Spark User Backlash and Regulatory Scrutiny",
    "originLink": "https://www.antipope.org/charlie/blog-static/2024/06/is-microsoft-trying-to-commit-.html",
    "originBody": "Charlie's Diary Being the blog of Charles Stross, author, and occasional guests ... [ Home ] [ FAQ ] [ Contact me ] [ Older stuff ] Back to: Harmless Fun Is Microsoft trying to commit suicide? By Charlie Stross The breaking tech news this year has been the pervasive spread of \"AI\" (or rather, statistical modeling based on hidden layer neural networks) into everything. It's the latest hype bubble now that Cryptocurrencies are no longer the freshest sucker-bait in town, and the media (who these days are mostly stenographers recycling press releases) are screaming at every business in tech to add AI to their product. Well, Apple and Intel and Microsoft were already in there, but evidently they weren't in there enough, so now we're into the silly season with Microsoft's announcement of CoPilot plus Recall, the product nobody wanted. CoPilot+ is Microsoft's LLM-based add-on for Windows, sort of like 2000's Clippy the Talking Paperclip only with added hallucinations. Clippy was rule-based: a huge bundle of IF ... THEN statements hooked together like a 1980s Expert System to help users accomplish what Microsoft believed to be common tasks, but which turned out to be irritatingly unlike anything actual humans wanted to accomplish. Because CoPilot+ is purportedly trained on what users actually do, it looked plausible to someone in marketing at Microsoft that it could deliver on \"help the users get stuff done\". Unfortunately, human beings assume that LLMs are sentient and understand the questions they're asked, rather than being unthinking statistical models that cough up the highest probability answer-shaped object generated in response to any prompt, regardless of whether it's a truthful answer or not. Anyway, CoPilot+ is also a play by Microsoft to sell Windows on ARM. Microsoft don't want to be entirely dependent on Intel, especially as Intel's share of the global microprocessor market is rapidly shrinking, so they've been trying to boost Windows on ARM to orbital velocity for a decade now. The new CoPilot+ branded PCs going on sale later this month are marketed as being suitable for AI (spot the sucker-bait there?) and have powerful new ARM processors from Qualcomm, which are pitched as \"Macbook Air killers\", largely because they're playing catch-up with Apple's M-series ARM-based processors in terms of processing power per watt and having an on-device coprocessor optimized for training neural networks. Having built the hardware and the operating system Microsoft faces the inevitable question, why would a customer want this stuff? And being Microsoft, they took the first answer that bubbled up from their in-company echo chamber and pitched it at the market as a forced update to Windows 11. And the internet promptly exploded. First, a word about Apple. Apple have been quietly adding AI features to macOS and iOS for the past several years. In fact, they got serious about AI in 2015, and every Apple Silicon processor they've released since 2016 has had a neural engine (an AI coprocessor) on board. Now that the older phones and laptops are hitting end of life, the most recent operating system releases are rolling out AI-based features. For example, there's on-device OCR for text embedded in any image. There's a language translation service for the OCR output, too. I can point my phone at a brochure or menu in a language I can't read, activate the camera, and immediately read a surprisingly good translation: this is an actually useful feature of AI. (The ability to tag all the photos in my Photos library with the names of people present in them, and to search for people, is likewise moderately useful: the jury is still out on the pet recognition, though.) So the Apple roll-out of AI has so far been uneventful and unobjectionable, with a focus on identifying things people want to do and making them easier. Microsoft Recall is not that. \"Hey, wouldn't it be great if we could use AI in Windows to help our users see everything they've ever done on their computer?\" Is a great pitch, and Recall kinda-sorta achieves this. But the implementation is soemthing rather different. Recall takes snapshots of all the windows on a Windows computer's screen (except the DRM'd media, because the MPAA must have their kilo of flesh) and saves them locally. The local part is good: the term for software that takes regular screenshots and saves them in the cloud is \"part of a remote access trojan\". It then OCRs any text in the images, and I believe also transcribes any speech, and saves the resulting output in an unencrypted SQLite database stored in: C:\\Users\\$USER\\AppData\\Local\\CoreAIPlatform.00\\UKP{GUID} And there are tools already out there to slurp through the database and see what's in it, such as TotalRecall. Surprise! It turns out that the unencrypted database and the stored images may contain your user credentials and passwords. And other stuff. Got a porn habit? Congratulations, anyone with access to your user account can see what you've been seeing. Use a password manager like 1Password? Sorry, your 1Password passwords are probably visible via Recall, now. Now, \"unencrypted\" is relative; the database is stored on a filesystem which should be encrypted using Microsoft's BitLocker. But anyone with credentials for your Microsoft account can decrypt it and poke around. Indeed, anyone with access to your PC, unlocked, has your entire world at their fingertips. But this is an utter privacy shit-show. Victims of domestic abuse are at risk of their abuser trawling their PC for any signs that they're looking for help. Anyone who's fallen for a scam that gave criminals access to their PC is also completely at risk. Worse: even if you don't use Recall, if you send an email or instant message to someone else who does then it will be OCRd and indexed via Recall: and preserved for posterity. Now imagine the shit-show when this goes corporate. And it turns out that Microsoft is pushing this feature into the latest update of Windows 11 for all compatible hardware and making it impossible to remove or disable, because that tactic has worked so well for them in the past at driving the uptake of new technologies that Microsoft wanted its ~~customers~~ victims to start using. Like, oh, Microsoft Internet Explorer back in 2001, and remember how well that worked out for them. Suddenly every PC becomes a target for Discovery during legal proceedings. Lawyers can subpoena your Recall database and search it, no longer being limited to email but being able to search for terms that came up in Teams or Slack or Signal messages, and potentially verbally via Zoom or Skype if speech-to-text is included in Recall data. It's a shit-show for any organization that handles medical records or has a duty of legal confidentiality; indeed, for any business that has to comply with GDPR (how does Recall handle the Right to be Forgotten? In a word: badly), or HIPAA in the US. This misfeature contravenes privacy law throughout the EU (and in the UK), and in healthcare organizations everywhere which has a medical right to privacy. About the only people whose privacy it doesn't infringe are the Hollywood studios and Netflix, which tells you something about the state of things. Recall is already attracting the attention of data protection regulators; I suspect in its current form it's going to be dead on arrival, and those CoPilot+ PCs due to launch on June 18th are going to get a hurried overhaul. It's also going to be interesting to see what Apple does, or more importantly doesn't announce at WWDC next week, which is being trailed as the year when Apple goes all-in on AI. More to the point, though, Windows Recall blows a hole under the waterline of Microsoft's trustworthiness. Microsoft \"got serious\" about security earlier this decade, around the time Steve Balmer stepped down as CEO, and managed to recover somwhat from having a reputation for taking a slapdash approach to its users data. But they've been going backwards since 2020, with dick moves like disabling auto-save to local files in Microsoft Word (your autosave data only autosaves to OneDrive), slurping all incoming email for accounts accessed via Microsoft Outlook into Microsoft's own cloud for AI training purposes (ask the Department of Justice how they feel about Microsoft potentially having access to the correspondence for all their investigations in progress), and now this. Recall undermines trust, and once an institution loses trust it's really hard to regain it. Some commentators are snarking that Microsoft really really wants to make 2025 the year of Linux on the Desktop, and it's kind of hard to refute them right now. Posted by Charlie Stross at 10:23 on June 5, 2024Comments (56) 56 CommentsLeave a comment Sean Eric FaganJune 5, 2024 10:37Reply 1: I don't know why uSoft is trying to outdo HP in the Brand Destruction category, but here we are. James TurnerJune 5, 2024 10:42Reply 2: It can be disabled - https://support.microsoft.com/en-gb/windows/privacy-and-control-over-your-recall-experience-d404f672-7647-41e5-886c-a3c59680af15 You can turn on or off saving snapshots at any time by going to Settings > Privacy & security > Recall & snapshots. You can also pause snapshots temporarily by selecting the Recall icon in the system tray on your PC and selecting the pause option. For enterprise customers, IT administrators can disable automatically saving snapshots using group policy or mobile device management policy. If a policy is used to disable saving snapshots, all saved snapshots from users' devices will be deleted, and device users can't enable saving snapshots. For more information, see Manage Recall.\" Moz replied to this comment from James TurnerJune 5, 2024 11:03Reply 3: Disabling it on my machine is great. But as soon as I communicate with someone who still uses it everything is stored at their end. Mike Watts replied to this comment from MozJune 5, 2024 11:24Reply 4: Only what you sent them is stored on their end, which is also stored on their end by virtue of you sending it to them, even without Recall being involved. Microsoft has clearly absolutely fudged the announcement of this feature, and for the record I think it's a terrible feature, but you aren't at risk from it existing on anyone else's computer any more than you already are. ErrolwiJune 5, 2024 11:29Reply 5: I believe there are already tools that can stealth-enable, overriding group policy. It obviously hasn't been designed with the security mindset. Tier2Tech replied to this comment from Mike WattsJune 5, 2024 11:40Reply 6: Yes, absolutely! As well as Email, Teams or Slack or Signal messages should already be available for discovery - it's the angry or messy draft messages that you type out and edit before refining and finally sending that are exposed more I wonder if just keeping a small window pinned on top playing DRM movies is enough to defeat the system Tim H.June 5, 2024 11:43Reply 7: I expect an update soon that makes it simple to disable that \"Antifeature\". Moz @#3 and Mike Watts @ #4, and it'll likely be insufficient in mitigating reputational damage. Charlie Stross replied to this comment from Mike WattsJune 5, 2024 11:43Reply 8: Wrong: this basically provides a way for third parties to see what you've been discussing over e2ee channels. Tim H. replied to this comment from Charlie StrossJune 5, 2024 11:47Reply 9: Thereby creating the possibility of local \"Right-Wing nuts\" acquiring a list of folks who despise the local NAZI wannabe candidates for doxxing purposes. rstJune 5, 2024 11:49Reply 10: FWIW, reports from people who've gotten their hands on advance copies is that Recall can't be disabled at install time. It can be disabled later, but some have found the docs on how to do that a little obscure. That and Microsoft making confident statements about access controls on the local machine that have turned out to be false (they said one user on the machine couldn't access another's screenshots), have contributed to the ... developing atmosphere of mistrust. Charlie Stross replied to this comment from Tim H.June 5, 2024 11:51Reply 11: More to the point, this is the perfect way to cover for adding e2ee surveillance to Windows, as requested by police states everywhere (and the five eyes in particular). All they need to add is a mirror the SQLite database on a government-owned file store via OneDrive. Tier2TechJune 5, 2024 11:57Reply 12: The \"telemetry\" that MS already collects is insane. I think this push is intended to offload much of the cost in processing and storage space to the device purchasers Tim H. replied to this comment from Charlie StrossJune 5, 2024 12:06Reply 13: On a positive note, my first \"Smartphone\" was a Nokia running WinMobile 8*, it worked well enough that I once considered moving further into the M$ ecosystem. Fortunately, this was just before they shitcanned Windows Mobile, and I acquired a 2nd hand iPhone. *Nearly as affordable as Android phones, nearly as house broke as iPhones and damned by it's owners as being forever #3. Charlie Stross replied to this comment from rstJune 5, 2024 12:13Reply 14: It is pretty well understood that 90-99% of Windows users don't know how to uninstall/customize their setup. Hell, 60-70% don't even know there are keyboard shortcuts for cut and paste, and about 40% don't realize copying stuff is something the computer can do for them. Given the level of IT literacy among members of the public, not asking whether or not to install a feature at setup time means that 90% of PCs will have it enabled. Princejvstin replied to this comment from Charlie StrossJune 5, 2024 12:24Reply 15: That's a really solid point. It goes back to the whole thing about the browser wars and Microsoft saying \"Well people can install a new browser and make it the default anytime they like\" knowing full well that the vast majority of users couldn't or wouldn't take the time to do it. (see also the new \"hack\" to remove AI results from Google searches) dwmJune 5, 2024 12:46Reply 16: Given the releases with Intel, AMD, NVidia, Qualcomm, etc. this seems like a coordinated effort between Microsoft and those hardware manufacturers to get everyone to chuck out their (perfectly functional) hardware and get everyone to buy more kit. Hence the CoPilot+-capable branding. Microsoft probably like this because they have now ended free upgrades from Windows 10 to Windows 11, and so can exact an OS tax for everyone upgrading. It's artificial carrot and stick, and the new carrot isn't needed. Recall has already been demonstrated to work perfectly happily on Azure VMs that don't even have NPUs, so it seems to my untutored eye to be mainly a power-efficiency thing rather than a hard requirement. If Microsoft's goal was to get the features in front of people, then they'd just ship it for everyone. So the motivation instead seems to be to ship more hardware and OS licenses. So far, so normal. But this new carrot is rotten. And Microsoft, even having now realised the disastrous reception on security grounds, presumably can't easily back out because they've already signed agreements with various hardware manufacturers committing to this software rollout. Time to see how good the regulators are. Greg TingeyJune 5, 2024 12:54Reply 17: update to Windoze11 - arrRRGGH!. I am stuck with windoze - because Madam's work systems use it, but we ( & they too ) are sticking with 10.... MS Recall sounds like a more-than-total disaster ... how long do you give it before it produces a really major scandal? And/or kills someone? Charlie @ 11 I NEVER, EVER use \"One Drive\" as a matter of principle ... & convenience - I store suff where I want it to go, how dare they? David L replied to this comment from Charlie StrossJune 5, 2024 13:00Reply 18: It is pretty well understood that 90-99% of Windows users don't know how to uninstall/customize their setup. The Mac and Windows user base is being segregated into Biz (managed/MDM) users and \"I'm in control\" users. And the MDM folks will disable this at initial setup. Well just as soon as the MDM settings first load which will be immediately before or after first sign on. While the mortal end users doing their own thing will enable it by not knowing or caring that it can be turned off. And this blog audience is way different than the average person. The entire TikTok debate in the US is basically pitting \"I WANT MY TIKTOK and don't give a damn about your privacy notes. It will never hurt me.\" and security folks who are worried about China (and others down the road) slurping up personal data from TikTok viewers. With a side rant about to make the \"Out Of the Box\" experience nice users setup a first account on Windows and Mac as an admin account. That they use for everything. Not comprehending the reasons for segregating admin and user things. These computer are still too damn complicated for motals to understand. And getting worse in some ways instead of better. Oh, the lawyers. In the US they are cheap. And terrible at managing their computers. And tend to have terrible security practices that are only addressed as cheaply as possible when forced. David L replied to this comment from dwmJune 5, 2024 13:05Reply 19: those hardware manufacturers to get everyone to chuck out their (perfectly functional) hardware and get everyone to buy more kit. To be blunt, most Windows and Mac sales these days are laptops. And MOST Windows laptops that are purchased for home, school, and small businesses are crap and fall apart in 2 or 3 years. Literally fall apart. And even some of the better Windows business oriented systems are not much better. So for these users they will be buying something new in 2 years or so anyway. David LJune 5, 2024 13:17Reply 20: Is Microsoft trying to commit suicide? Actually no. But they are trying to avoid being the next: Novel Compaq DEC IBM Yahoo (and the raft of similar) AOL MySpace The seven dwarfs and/or the BUNCH And so on. OvidJune 5, 2024 13:17Reply 21: My prediction: even if users disable Recall, black hats around the world are going to see that silently re-enabling this is mission number one. Next step: state-level actors are going to be spending their taxpayers hard-earned dollars on zero-day exploits so they can grab that Recall data. Next step: targeting mid-level managers/bureaucrats around the world to get access to that now-enabled Recall feature and exfiltrating the SQLite database their information is stored in. These people have so many common \"secrets\" available and so much access to the day-to-day information of companies/governments that they're going to be a prime target. For black hats Recall is going to be the goose that lays the golden eggs. For intelligence, it's both a gold mine (when they can acquire the databases) and a nightmare (when they have to defend against it). If Recall can be silently enabled, exfiltrating the databases won't be too hard. This could seriously damage government and corporate security around the world. Class action lawsuits against Microsoft to follow. HieromechJune 5, 2024 13:28Reply 22: I couldn't be happier to find that my PC doesn't qualify for an upgrade to Windows 11. Though I wouldn't put it past them at this point to try and slip some updates to Windows 10 in before the end-of-support date. Maybe it's time to start looking at Linux alternatives. paws4thotJune 5, 2024 13:32Reply 23: Mickeyshaft AI - Automatic Idiocy! Adrian HowardJune 5, 2024 13:33Reply 24: It's gonna be fascinating to see how this new feature rolling out clashes with GDPR in the EU and similar legislation elsewhere. I would not want to be the DPO in an org where this was enabled (or, indeed, possible to enable). Mike Watts replied to this comment from Charlie StrossJune 5, 2024 13:46Reply 25: That's an entirely fair point that I hadn't quite considered. As a teacher, I'm accessing sensitive data daily on a machine configured by my employer. I can't imagine a situation where anything I sent out to someone with Recall enabled could cause me an issue directly, but I can see how it could cause them a problem. Even leaving aside end-to-end encryption, some of the things I wrote in this reply, reconsidered, deleted and re-wrote could easily have caused me trouble and I'd never in a million years consider stuff I'd written and then almost immediately deleted could cause me a problem! I'm reasonably confident that my deliberate actions would be safe, but noodling in a blank document that's never saved? More scary. Kevin Marks replied to this comment from David LJune 5, 2024 13:48Reply 26: I think the MDM BOFH's are more likely to enable this as part of surveilling employees for the greater good, but then I remember the internal server minitrue.apple.com Sean Eric Fagan replied to this comment from OvidJune 5, 2024 13:48Reply 27: The class action lawsuits are among the least of the future problems here -- gov't action is going to hurt. You'll have some government saying it must be enabled, and some governments saying it must not be enabled. And then you'll also probably have the US and EU where different parts of the gov'ts take different positions. Scott SanfordJune 5, 2024 14:03Reply 28: C:\\Users\\$USER\\AppData\\Local\\CoreAIPlatform.00\\UKP{GUID} I assume that Win 11 gets cranky if someone has written something to check this directory daily and erase anything found there? David L replied to this comment from Kevin MarksJune 5, 2024 14:05Reply 29: I think the MDM BOFH's are more likely to enable this as part of surveilling employees for the greater good As someone who has consulted for small businesses for decades, yes and no. It depends on the personality of the \"owner\" I've seen both sides. As you move up the company size, mostly off. Due to outside lawyers telling them NO. Also their E&O insurance coverage. But with many companies fast growing or stable at 200 or fewer employees some low level tech will make a decision and the results pop up in a meeting a week or year later and things get heated. \"WE ARE DOING WHAT?!?!?\" Larger companies know about the laws and have in house legal and compliance staff. That doesn't mean that individual silos don't do stupid things against company policy. Or that there is an issue with that 3 person office on the other side of an ocean. Mostly from ignorance that they should even check. Seen this up close also. Bo LindberghJune 5, 2024 14:10Reply 30: Now consider the synergy with this existing misfeature: OCR is reading watermark letters. Yep, you'll be able to plant any suspicious text you want on anybody's Win11 system without them noticing at all. DamianJune 5, 2024 14:12Reply 31: The Office autosave thing on its own was pretty damn annoying. Apple has done stuff at least as annoying, but this is different. It sort of means that Windows PCs at home are no good for anything other than games, and even then you want a script to wipe the directory/SQLLite file mentioned every so often. Work computers in a MOE are probably okay, but still. David L replied to this comment from DamianJune 5, 2024 14:35Reply 32: Current macOS and for a few years now, will not let applications use the camera or microphone without explicit user approval. Admins can't enable this via MDM settings. They must be approved via a user check box. Matthew BlochJune 5, 2024 14:40Reply 33: This isn't incompetence, it's (outrageous) kite-flying. I think the strategy is this: MS see the data that big online providers gather on people, and they just can't force enough people to use Bing or Office365 or whatever where they might benefit from that continuous usage data. Windows users' offline data is staying stubbornly offline, and it's full of really juicy stuff! So this feature turns every PC you pay for into a giant distributed data-gathering operation that Microsoft can tap into, very gradually. Users get a stupid search box on day 1, but over the years they'll also get \"exclusive benefits\" from allowing limited API access to insurance providers, employers, advertisers etc. etc. And MS desperately want this data access to be the new normal, turning every Windows install into an ongoing revenue source. They are seeing what they can get away with, and are happy to risk the reputational damage, even if they scrap it, or launch it in smaller markets or whatever. They can afford to be patient and work on people's tolerance. Personally I already run a \"decrapified\" Windows with all kinds of major features turned off, and I'd expect this to be top of the list in future. But Recall absolutely deserves shouting down now otherwise it'll become the most egregious betrayal of the \"personal\" computer. Damian replied to this comment from David LJune 5, 2024 14:47Reply 34: I guess at least biz Windows laptops come with a dinky little shutter for the camera. Of course MS have pushed out \"hello\" to encourage leaving it open. But the autosave thing is still niggling. I guess Pages is available. David L replied to this comment from DamianJune 5, 2024 15:03Reply 35: autosave vs. Autorecover Autosave was implemented to allow live editing between multiple people. Which is why it always goes to \"the cloud\". It would be nice if it could go to an SMB share but I can also see failures due to varying SMB implementations. Autorecover saves your typing every few minutes to a local storage spot in case of a crash. At least on a Mac. I have a Windows system to stand up today or tomorrow but can't check until then. paws4thotJune 5, 2024 15:16Reply 36: Meantime, my sister has a class in yoga (yes yoga) over Teams. I only mention this because of the subject. JohnS replied to this comment from Charlie StrossJune 5, 2024 15:55Reply 37: Charlie Stross @ 14: It is pretty well understood that 90-99% of Windows users don't know how to uninstall/customize their setup. Hell, 60-70% don't even know there are keyboard shortcuts for cut and paste, and about 40% don't realize copying stuff is something the computer can do for them. Given the level of IT literacy among members of the public, not asking whether or not to install a feature at setup time means that 90% of PCs will have it enabled. I wonder if this is somehow stratified by age? Would younger users who grew up with computers have more IT literacy? Will the percentage of people who understand these things go up as all us old people pass on? I know that's not an answer to the threat, but I'm curious if it might be less of a threat to younger users? JohnS replied to this comment from dwmJune 5, 2024 16:01Reply 38: dwm @ 16: Given the releases with Intel, AMD, NVidia, Qualcomm, etc. this seems like a coordinated effort between Microsoft and those hardware manufacturers to get everyone to chuck out their (perfectly functional) hardware and get everyone to buy more kit. Hence the CoPilot+-capable branding. Microsoft probably like this because they have now ended free upgrades from Windows 10 to Windows 11, and so can exact an OS tax for everyone upgrading. But IS going from Windoze10 to Windoze'Leven actually an upgrade? Seven to ten wasn't. JohnS replied to this comment from HieromechJune 5, 2024 16:14Reply 39: Hieromech @ 22: I couldn't be happier to find that my PC doesn't qualify for an upgrade to Windows 11. Though I wouldn't put it past them at this point to try and slip some updates to Windows 10 in before the end-of-support date. I'm happily running Windows 7 Professional on my Photoshop computer. This one is Windows 10 Home. It came that way; I didn't \"upgrade\" it. Maybe it's time to start looking at Linux alternatives. If you find one that will seamlessly run my legacy Windoze programs (mainly a few old games & Photoshop) let me know. Charlie Stross replied to this comment from David LJune 5, 2024 16:25Reply 40: Autosave was implemented to allow live editing between multiple people. Microsoft Word had Autosave/autorecover as far back at Word 5.1a for MacOS in 1988, to my knowledge. Word has always had autosave. It's nothing to do with live editing, and removing local autosave files is a travesty designed to force-sell OneDrive accounts to Word users. Greg TingeyJune 5, 2024 16:30Reply 41: David L @ 18 Close ... I will have to be careful, if I know/suspect the person on the \"other end\" is using Win11 .... EVERYBODY ... According to the Boss, whom I showed this to ... The Professional Security people are shit-terrified of this already - she's seen IT warnings at her ( Tax-&-accountancy firm, remember ) workplace & all the serious players are attempting to take steps to head this fucking insanity off at the pass. John Howard Your point about GHDPR promises interesting & expensive lawsuits! Charlie Stross replied to this comment from JohnSJune 5, 2024 16:33Reply 42: Would younger users who grew up with computers have more IT literacy? They often have less literacy. There's a current fuss in academics teaching CS about how current/new students fundamentally don't understand directory structures, or computers in general -- they've grown up with smartphones and they're used to finding items like photos in a pile of images by swiping, not by structuring data hierarchically. If you get lucky they understand tagging, but the way modern UIs hide what's going on under the hood makes for an illiterate population. RetiringJune 5, 2024 16:53Reply 43: Microsoft had 72% of the OS market earlier this year. Only a few bougie companies use something different. They can afford to make some marketing mistakes. No suicide imminent. And if you're using Windows as an OS, you are already subject to oodles of surveillance, from all kinds of perspectives. Can't imagine this will make things much less private, despite all the kerfuffle about it. If you find one that will seamlessly run my legacy Windoze programs (mainly a few old games & Photoshop) let me know. Well, exactly. This is how much normal people don't care about privacy. So long as they can run a few old games and some software they bought some time ago, anything is OK. Charlie Stross replied to this comment from RetiringJune 5, 2024 16:58Reply 44: Microsoft has 72% of the OS market on what platform? Because they sure as hell don't have 72% of the smartphone market, the tablet market, or the embedded microcontroller market: they're laptops, desktop PCs, and a minority of tablets -- a sector which is just one small corner of computing these days. It's like saying IBM have 72% of the mainframe market stitched up right now. Well yeah, and what's your point? Simon BissonJune 5, 2024 16:59Reply 45: One point to the discovery issue: lawyers are already pulling Slack, Teams, and whatever data. They're not unaware of the tools used for business communication, they use them themselves. So they're grabbing browser histories, any system logs, all enterprise socials, and anything else they can think of. So for that matter Recall is just an alternate search tool to the ones they're already using to concatenate and parse those terabytes of data they're already working with. It might make it a little easier to pinpoint things, but not that much. eDiscovery tooling is a fascinating area, I wrote some whitepapers for a company that developed one package a decade or so ago, and they clearly had ex-black hat hackers working for them. The state of the art has only got better since then,,, (As an aside I was at the launch of the Copilot+ PCs, and what's being described in the preview build that folk have activated is not what I was told the internals were. For one thing they were very clear as to the database being used and it wasn't SQLite. Instead it was a proper graph database.) Retiring replied to this comment from Charlie StrossJune 5, 2024 17:38Reply 46: they're laptops, desktop PCs, and a minority of tablets -- a sector which is just one small corner of computing these days. Yep, desktop/laptop. And Microsoft's share of that sector is increasing. But business computing is still overwhelmingly keyboard & screen computers. Microsoft has nothing to worry about, yet. Or maybe they do: the advent of something like Recall on phones. Voice interfaces combined with intimately accurate automated interpretation of what the user says. That could finally break the keyboard & screen stranglehold. But such interpretation depends on access to user context -- breaking the privacy silos. UnholyguyJune 5, 2024 17:43Reply 47: I don’t think they intend the data to be immediately useful, I think the intent is to gather training data for a couple years and then train a model Right now Windows is around 12% of Microsoft’s revenue, they could lose it all and not have committed suicide. Microsoft as a company has actually been doing incredibly well the last few years, it’s a real golden age for them right now. But yeah, the implementation and messaging of this is pretty bad. Guy RixonJune 5, 2024 17:44Reply 48: Why are MS doing this? Beyond specific business plans that are crazy or naive or evil (or all three), they may just have a generational problem. In any IT-heavy organisation, there is a population of recent-hires who get the code but don't get what the users want. They cannot get what the users want because they are not functionally human, and they're only hired because they may turn out to be brilliant coders. They'll use any shiny tech and allow any weird side effects on the off-chance that their stuff does something (that they consider) cool. Their output has to be filtered through a layer of functionally-human analysts and managers before it's safe to let out. Over time, some of the crazy-Eddie coders grow up into moderating leads and managers. I've been there, on both sides. The all-growed-up-now coders get mixed in with the career managers and the business analysts. Now suppose the org buys the myth that managers just slow projects down and analysts are all wasters. Over time they employ fewer card-carrying humans and the moderating body reduces to just the aging coders...who no longer moderate the products. It's the RBMK-negative-void-coefficient of software development. David L replied to this comment from Simon BissonJune 5, 2024 17:54Reply 49: So they're grabbing browser histories, any system logs, all enterprise socials, and anything else they can think of. I don't think Apple keeps your misc whatever that the mic picks up. Even if you enable Siri it SUPPOSEDLY ignores everything until it hears \"Siri\" or \"Hey Siri\". Then turns it into text, ships it off, hands you back and answer. And I think THEN tosses all of it. Maybe. I'm very light on this detail. But it sounds like MS now might collect this and all camera views while collecting everything else. Ugh. As to my comment about Autosave, I have a vague memory of it now. But at some point MS changed it to cloud only to deal with the multi person editing. And push their cloud. Autorecovery is local. Or wherever you point it. I have to wonder if there isn't a deep setting that allows autosave to be pointed locally. Need to dig a bit. David L replied to this comment from RetiringJune 5, 2024 18:05Reply 50: Yep, desktop/laptop. And Microsoft's share of that sector is increasing. I have to wonder about that. In 2018 or 2019 SAP was supporting about 50/50 to end users of Win/Mac. When you onboarded you got to pick. That meant something like 30K or 60K of each. (Working from memory of a talk given by the guy in charge of the Mac support side.) One major hotel chain you all know of has 30K Mac laptops issued to normal staffers. These situations are typical of the larger companies at the admin tech conferences. My daughter has worked for 2 remote only companies over the last 6 years. Both times she got to pick which platform she wanted. She went with Macs. 200 give or take person companies. My son works for a tech oriented company that supplies software for Windows servers. His company is Win all the way for obvious reasons. I think most companies of much size let end users pick as long as they don't need specialty software that only runs on a specific platform. Smaller companies tend to go with what the owner likes if all things are equal. paws4thot replied to this comment from JohnSJune 5, 2024 18:11Reply 51: I'd say that Windoze 10 to Windoze Uneven probably isn't an upgrade: W7 to W10 certainly isn't (having recently had it forced on me). Retiring replied to this comment from David LJune 5, 2024 18:26Reply 52: https://gs.statcounter.com/os-market-share/desktop/worldwide RetiringJune 5, 2024 18:35Reply 53: It seems to me that OGH does have a point, though. Just instrumenting desktops/laptops isn't enough. Lots of business takes place via phone calls and texts, and without a way to capture that, Recall will have an incomplete view of the user's context. (Google saw that a while ago, and responded with their purchase of GrandCentral, which became Google Voice, which routes all your calls (and texts) through Google.) Can Recall succeed without capturing phone activity? David L replied to this comment from RetiringJune 5, 2024 19:04Reply 54: Recall will have an incomplete view of the user's context. Microsoft wants you to use Teams for talking with folks who also have Teams. And have a phone option now as an add on to Teams for those who want to use it. Along the lines of Google Voice. With a huge side dish of do companies expect you to use your cell phone for company work or not. And do you care? foxforcefiveJune 5, 2024 19:06Reply 55: And no matter how bad it gets linux will still be a minority OS. The year of Linux on the Desktop never arrives. Personally I was already planning to switch my gaming machine over to arch given the improvements Valve have made to Proton. UnholyguyJune 5, 2024 19:41Reply 56: There is a huge amount of data available on the private market in the US at least. It’s a major mistake to think that major tech companies have to have to personally collect data. Leave a comment Sign in to comment. Here's the moderation policy. If this is your first time, please read it before you post. If you need to sign in and want to create a local account on this blog, select \"Movable Type\" from the \"Sign in ...\" menu. You will need a working email address. Buy my Books US English editions UK English editions Deutsche Ausgaben 日本語の本 Quick Stuff FAQ: Who am I? FAQ: Moderation Policy FAQ: Why is there no tipjar? FAQ: Copyright Notice FAQ: Inviting Charles Stross to speak FAQ: Fan Fiction Bibliography and online fiction FAQ: Laundry Files reading order Talk to me Non-blog writing (old) Mastodon Specials Common Misconceptions About Publishing—a series of essays about the industry I work in. How I Got Here In The End —my non-writing autobiography, or what I did before becoming a full-time writer. Unwirer—an experiment in weblog mediated collaborative fiction. Shaping the Future—a talk I gave on the social implications of Moore's Law. Japan: first impressions — or, what I did on my holidays Inside the MIT Media Lab—what it’s like to spend a day wandering around the Media Lab. The High Frontier, Redux — space colonization: feasible or futile? “Nothing like this will be built again”—inside a nuclear reactor complex. Old blog—2003-2006 (RIP) Merchandise About this Entry This page contains a single entry by Charlie Stross published on June 5, 2024 10:23 AM. Harmless Fun was the previous entry in this blog. Find recent content on the main index or look in the archives to find all content. Propaganda Categories Administrative (5) Computers (10) Gadget Patrol (7) Humour (12) News (6) Politics (15) Publicity stuff (11) Publishing (19) Travel (7) Writing (37) Monthly Archives June 2024 (1) May 2024 (2) April 2024 (1) March 2024 (2) February 2024 (1) January 2024 (3) December 2023 (2) November 2023 (1) October 2023 (1) September 2023 (2) August 2023 (2) July 2023 (1) Earlier, and other types of archive.",
    "commentLink": "https://news.ycombinator.com/item?id=40585212",
    "commentBody": "Is Microsoft trying to commit suicide? (antipope.org)268 points by blueridge 4 hours agohidepastfavorite250 comments simonw 4 hours agoSecurity/privacy concerns aside, Recall doesn't particularly feel like an \"AI\" feature to me. It's on-device OCR plus a SQLite database that you can then search, right? Even by today's loose definition of \"AI\" I'm having trouble making that connection. I guess the OCR is based on machine learning? Is there an LLM component to Recall that I've missed? If so, can I build websites with prompt injection attacks on them that will target Recall users directly, by getting my rogue instructions indexed into their SQLite database and later fed into the LLM? reply ben_w 3 hours agoparent> Even by today's loose definition of \"AI\" I'm having trouble making that connection. I guess the OCR is based on machine learning? It's long been noticed that every time AI researchers figure out how to do a thing, it goes from \"this is impossible SciFi nonsense\" to \"that's not real AI\". It's weird for me to actually encounter people doing this. I remember when OCR was impossible, at any decent quality level, for AI. We used this to stop bots logging into forums, with a thing we called a \"Completely Automated Public Turing test to tell Computers and Humans Apart\" or CAPTCHA for short. It started off as text, then the text got increasingly distorted until most humans had trouble reading it, then it became house numbers, then it became \"click all pictures of XYZ\", before mostly disappearing into analytics seeing where you hovered your mouse cursor and which other websites you visited. reply quectophoton 2 hours agorootparent> It's long been noticed that every time AI researchers figure out how to do a thing, it goes from \"this is impossible SciFi nonsense\" to \"that's not real AI\". https://en.wikipedia.org/wiki/AI_effect The most extreme example I can find of this, is Google Translate; I don't know of anybody who thinks of it as AI. reply ben_w 55 minutes agorootparent> The most extreme example I can find of this, is Google Translate; I don't know of anybody who thinks of it as AI. Huh. I'm (to my own surprise, given how old the feature is) still astounding people by demonstrating that it has augmented reality mode. reply sebazzz 14 minutes agorootparentprevIt is only because it already existed it isn't branded as AI. reply mannykannot 2 hours agorootparentprev> Every time AI researchers figure out how to do a thing... If we are going to define AI as whatever AI researchers are working on (a definition having something of a bootstrap problem), then the only way the goalposts will not move is when they are not making progress. reply ben_w 1 hour agorootparentI wonder which other professions exhibit the same effect? Artists certainly don't, as art remains art even when the artist themselves becomes lost to time. I suspect politicians may be, for every problem they do solve becomes the status quo… though also every problem they don't solve becomes their fault, so perhaps not. Civil engineers may be on the boundary, people forgetting that cities are built rather than natural when complaining about immigrants with phrases such as \"we are full\", yet remember well enough for politicians to score points with what they promise to get built. reply tpoacher 2 hours agorootparentprevTo paraphrase the Incredibles, \"if everything is AI, then nothing is\". reply JohnTHaller 1 hour agorootparentprevIf someone created the first Bayesian filter for classifying spam today, they would call it 'AI'. reply ben_w 57 minutes agorootparentP(what you did thereI see it) = P(I see itwhat you did there) * P(what you did there) / P(I see it) reply theanonymousone 1 hour agorootparentprevDo bot detectors check browsing history of a prospective bot? How? reply ben_w 1 hour agorootparentAll those tracking cookies, or so I'm told. reply MrRadar 3 hours agorootparentprevYeah, take this XKCD for instance: https://xkcd.com/1425/ That was published 10 years ago but now the second item is (nearly) as trivial to accomplish as the first. reply pavel_lishin 3 hours agorootparentTo be fair, we've had multiple research teams and ten years :) reply thunderbong 2 hours agorootparentprevMobile version https://m.xkcd.com/1425/ reply breadwinner 3 hours agoparentprevDemos show that you can search for \"Blue bag\" even if the words \"blue bag\" don't appear on the screen. If there was a blue bag in a photo, say in a PowerPoint slide, it will find it. reply simonw 3 hours agorootparentI'd love to know how they are doing this! Are they running an on-device CLIP embedding model of some sort? Could they even be shipping a multi-modal LLM like Phi-3 Vision? reply Firmwarrior 3 hours agorootparentOneDrive and the local Microsoft photos app have been doing it for a decade already. reply cjk2 3 hours agorootparentprevTo be fair Apple Photos does this. But it does it to your photos not your entire universe. reply hu3 3 hours agorootparentSame for google photos. I searched for \"smiling in beach\" and it works flawlessly. I recon Microsoft will want to offload this kind of search processing to user machines with the advent in \"AI processor computers\" we saw recently: https://blogs.microsoft.com/blog/2024/05/20/introducing-copi... reply mmcnl 1 hour agorootparentI believe Apple does it completely locally on the device, whereas Google searches in the cloud. reply ethagnawl 3 hours agorootparentprevDropbox has also been doing this for years. It's helpful sometimes and other times completely misses the mark. Personally, I do find it to be creepy and it has me edging closer to using a more privacy-focused cloud solution to backup my important documents, photos, etc. reply sleepybrett 3 hours agorootparentprevspotlight is also indexing pretty much every file you create on your mac. reply cflewis 3 hours agorootparentprevIn some ways, I find this even creepier than when I thought it was more than an SQLite database. reply renewiltord 3 hours agorootparentprevMy favourite feature in Apple and Google Photos. I use this all the time and have gotten so used to it I get annoyed when it doesn't find something. reply simonw 3 hours agoparentprevAnswering my own question (with the help of replies on Twitter: https://twitter.com/simonw/status/1798368111038779610 ) Best guess is that there's some level of semantic analysis going on beyond just OCR, which tips the balance (at least for me) to being an \"AI\" feature based on loose current usage of that term. The clue is that in this demo https://www.youtube.com/watch?v=aZbHd4suAnQ&t=1062s they show searching for \"blue pantsuit with sequin lace\" finds something that was described as \"peacock\" in the text. It looks like this is based on embedding search against image embeddings. And... one of the 3 SQLite databases created by Recall is called \"SemanticImageStore\" - which suggests to me there may be a CLIP-style image embeddings model being run on-device here. Those databases contain \"diskann\" columns which are likely a reference to Microsoft's vector indexing library: https://github.com/microsoft/DiskANN reply osolo 3 hours agorootparentMy team worked on this feature and I can confirm that yes, semantic search is being done for both text and images. This allows you to do fuzzy searches like you mentioned in your comment and use words to match images. Everything runs on your device (all the models, the vector database, etc.) in order to preserve privacy. reply simonw 2 hours agorootparentThat's really impressive. Can you share which embedding models you are using for this? Also, is Phi-3 or Phi-3 Vision involved? reply randomdata 4 hours agoparentprevI assume you are working with the AI definition that is along the lines of: \"Problems a computer may not be able to solve\"? OCR was absolutely considered AI before we knew how to do it. Now that we understand it, it's just computing, of course. But it still is reasonably considered AI by any other definition of AI. reply KeplerBoy 3 hours agorootparentIsn't SotA OCR done using CNNs? So it's AI even in the modern sense. reply EasyMark 42 minutes agoparentprevBut it doesn't just recognize characters though? It recognizes things happening on video and scenes in pictures without text. for example, you could prompt \"find my photos and videos with dogs in them\" reply zamalek 4 hours agoparentprevIt probably has RAG. reply simonw 4 hours agorootparentIs that confirmed? Microsoft have been boasting that it all runs on-device, so are they running RAG using an on-device model (Phi-3 or similar) as part of the Recall feature? reply rihegher 3 hours agorootparentI had this confirmed during the Q&A portion of a demo presented by Microsoft. All the processing is supposed to happen locally using the NPU capacity of the hardware. reply ryandrake 2 hours agoparentprev> Recall doesn't particularly feel like an \"AI\" feature to me. The word \"AI\" is becoming a pure marketing checkbox. You can't release any new product in 2024 without somehow attaching the word \"AI\" to it. It doesn't matter whether it actually uses AI or an LLM. reply paulmd 16 minutes agorootparentWell, this one does, so what’s the point of this observation in this context? reply maciejgryka 3 hours agoparentprevI have no actual info on this, but I always assumed they'd compute some mutlimodal embeddings of the screenshots to then retrieve semantically-relevant ones by text? And yeah, they'd have to do it using on-device models, which doesn't seem out of reach? reply giantg2 3 hours agoparentprevI assume the AI part is using some model to interpret the stuff on the screen (Maybe even non-text) and used in the search results. That's just a guess. I'm not really that interested in the Recall product, so I have dug into it. reply jrm4 3 hours agoparentprevI really can't fathom what it takes to casually be like \"security/privacy concerns aside\" here. reply codetrotter 3 hours agorootparentI think you are misreading the comment. Saying aside from those concerns does not dismiss the concerns themselves. It’s just expressing that this comment will speak about some other things, and not those things. reply simonw 3 hours agorootparentprevI decided not to fully type out \"obviously the security/privacy concerns are the most important thing here and should not be ignored... but aside from that, does anyone know if...\" reply bnchrch 3 hours agoprevAmong the many logical fallacies already pointed out. I would like to add one more. This assumes that Microsofts future IS windows. I don't believe that has been the case since Satya. And once you realize that you can see the future is very bright Regardless of some implementation gaff in a Windows feature. reply toast0 3 hours agoparentWhat is their proposed future then? It's not Xbox, they're losing that war, and it was based on Windows. It's not phones, they already lost that one and that was based on Windows anyways. It can't be Office; there's less and less of a need for a paid office suite, and MS Office in a browser is rather bad. It might be Azure, but IMHO the big onramp to Azure is Microsoft pushing corporations to move ActiveDirectory and Exchange into the cloud, but the end of Windows means the end of ActiveDirectory, and there's options for Cloud mail and calendar. Is Azure compelling for many if you're not already there because of Exchange? > And once you realize that you can see the future is very bright The future may be bright, but how will you know if there's no Windows to see it through? ;p reply Rastonbury 3 hours agorootparentIt may be fun to joke at and hate on them but look at their revenues, growth and market shares, instead of thinking as a consumer where you personal come across them and whether you like them. Nearly monopoly personal OS and productivity, top 2 cloud and search engine (cue the Bing jokes but it makes them $12bn a year), rights to frontier AI models via OpenAI. The only rival who competes in multiple markets is Google - who still makes 90% of their money from ads and does not have sales/distribution that MS has. If Microsoft doesn't have a future then what of the rest of FAANG, none of them are as diversified or have OpenAI. Oh I forgot Github and Linkedin reply Freebytes 40 minutes agorootparentI think Google is afraid of Microsoft. reply gary_0 2 hours agorootparentprev> What is their proposed future then? Whatever IBM and Oracle do to stay solvent in spite of being irrelevant to most people and even most software people. reply lenkite 2 hours agorootparentprev> What is their proposed future then? Microsoft's Future is Azure AI POWERED CLOUD. Their CEO explicitly said this. Windoze is just the legacy on-boarding software. reply Bluecobra 3 hours agorootparentprevI thought for sure way back when the original Xbox came out that they were playing the really long game (no pun intended) to get a foothold and conquer the living room. After nearly 25 years I don't see that happening and it seems like they are going to end up going the Sega route and just develop software. reply AlexandrB 3 hours agorootparentI thought the same thing. It's crazy to see the state of Xbox today. Microsoft just seems bad at spotting, nurturing, and retaining talented game developers. Look at the recent layoffs where they nuked Tango Gameworks despite HiFi Rush's success. Going all-in on Gamepass has also devalued their own games in many consumers' minds. Why buy it when you can just get it on Gamepass? I suspect the recurring revenue is not making up for the lost unit sales within any reasonable timeframe. reply JohnMakin 3 hours agorootparentprevIronically, the fact that Xbox is so windows-friendly is exactly why I think its sales suffer and why they're going to end up just making software - for me as a gamer, I don't feel the need to own an xbox when I can play any xbox exclusive on my PC (and often with much better performance and peripherals). I can only play Playstation exclusive games on a Playstation, and the Playstation exclusives have been consistently very, very good. Pretty much everything on the Switch is exclusive to that console. So why would a budget-constrained person ever buy an Xbox? reply bee_rider 1 hour agorootparentprevThe idea of a sort of home-computer situated in the living room instead of the office made a whole lot more sense before smartphones. reply Freebytes 39 minutes agorootparentprevWell, the name of the company is Microsoft, not Microhard. reply kalleboo 2 hours agorootparentprevThe really tried to conquer the living room with the Xbox One, making it more about media than games, and the gamers revolted while the non-gamers shrugged. I think that's when the rest of Microsoft wrote it off. reply Freebytes 32 minutes agorootparentFor one thing, they need to hire someone to be \"Chief Executive of Naming Things That Make Sense\" because they are terrible with naming stuff. \"I got an Xbox One!\" \"Oh, the first one?\" \"No, the XBox One not the original XBox 1.\" Or, looking at the .NET evolution: .NET Framework 4.8 .NET Core .NET Core 2 .NET Core 3 .NET 5 There is no .NET Core 4 because people would get confused with .NET Framework 4 which is still getting bug fixes and has support. And they wanted people to move from .NET Framework 4 so they called it .NET 5 instead of .NET Core 5. reply cflewis 3 hours agorootparentprevAFAICT the future is just Azure and legacy enterprise contracts. Windows has no growth to make, stockholders only value growth, hence Windows is not of interest to the company. reply bogwog 3 hours agorootparentThat's the genius plan: kill Windows marketshare so that it can grow again in the future! reply hnben 2 hours agorootparentprev> What is their proposed future then? selling azure and consulting to governments and big corps. > Is Azure compelling for many if you're not already there because of Exchange? Most are already on Exchange, I think. Also: I think MS wants Azure to be compelling even without Exchange. (though I am not sure if that works out) reply mike_hearn 3 hours agorootparentprevLess need for an office suite? Tell that to all the companies that are not only paying for Office but are now migrating from Slack to Teams, thus becoming even more dependent on it. reply DrBazza 3 hours agoparentprev> This assumes that Microsofts future IS windows. > I don't believe that has been the case since Satya. Indeed. When MS has several multi-billion dollar streams of revenue - Office, Azure, XBox, game labels, and so on, and the \"cloudy\" part of computing is almost exclusively Linux, there's less compelling reasons for them to invest in Windows. With other moves like WSL, 'winget' and 'sudo', it's clear Windows is slowly moving towards a facade that mimics Linux from the CLI. In 2000, MS was Windows 9x, 2000, and Office. Consider that in 2004, you'd go to a site and download a Windows-only piece of software, and in 2024, you'll now find Apple-only software, or Linux only. Arguably, the 'year of the Linux(-like) desktop' has been MacOS for well over a decade. It's a gateway drug to full Linux, with very little friction in comparison to Windows (filesystem paths, Win32 api, etc.). reply donmcronald 1 hour agorootparent> 'winget' IMO the main purpose for WinGet was to kill AppGet because MS doesn't want a 3rd party to control the package repository for an easy to use distribution system. As far as I can tell, custom WinGet repos need to be run on Azure, so MS basically has a stranglehold on distribution. If you publish anything they don't like they can label you as being malicious and ban your Azure account. Regardless, the friction to running your own repo is high and most people will capitulate and submit their apps to the official repo for approval (by MS). Big tech has been relentlessly locking down software distribution for the last decade and killing everything but the Windows Store, WinGet, etc. has to be a long term goal for MS. reply mrweasel 1 hour agoparentprev> This assumes that Microsofts future IS windows. That pretty much leaves the world with a defunct operating system running om 80% of all desktop and laptops. I'm not suggesting that you're wrong, in fact I'd agree that Windows has taken a backseat at Microsoft, but that's pretty scary. Given the new Exchange Server licensing, I think it's pretty safe to assume that Microsoft would like to exit the on-prem software business at the earliest convenient time. I just question if the world is ready for that, or that's even what we want. reply causal 3 hours agoparentprevExactly. Windows is almost a loss leader at this point. reply mnau 3 hours agorootparentHell, .net is basically loss leader at this point (significant offloading to bunch of volunteers) and that is their ramp to Azure spending. reply giancarlostoro 3 hours agoparentprevAll they need to do to make Windows great is strip all of the crap nobody asked for. There's an insane amount of anti-patterns that are sketchy. reply Freebytes 27 minutes agorootparentWe need to create something that look better! Let us update a small set of applications one at a time and leave the others that look like they are from Windows 98 still in place. Years later, that look is old. We should update the look. But what about the other stuff we have not updated yet that is still old looking? We will get to it later. Meanwhile, you have three or four different layouts and designs and patterns throughout the system. It is crazy. They finally updated \"System\" to look like the rest of Windows, but if you click \"Advanced System Settings\", it still brings up the old System Properties. And it is crazy that the old one is still easier to navigate and find what you are trying to find. reply AlexandrB 3 hours agorootparentprevWindows is increasingly just a marketing channel for Microsoft to push their various other products. Unless your an enterprise/government customer the crap is the whole point. reply giancarlostoro 2 hours agorootparentWhich is why I migrated to Linux. Proton by Steam plays all the games I care about. I can do all my dev work on Linux just fine. reply TacticalCoder 3 hours agoparentprev> This assumes that Microsofts future IS windows. Is Microsoft's future a smartphone OS that'll rival the iPhone or Android phones? reply malfist 2 hours agorootparentNo, I think microsoft's future is encyclopedia software reply toast0 39 minutes agorootparentThat and high end gaming tables. reply gorjusborg 3 hours agoparentprevRegardless of whether Microsoft == Windows, a company's brand matters, especially when trust is required for adoption of their new offerings. Microsoft had been trying, it seemed, to make their image more positive, with some positive results in recent years. Their recent decisions have revealed that their priorities are not very aligned with their users. The future may be bright, depending on their goals, but I have no interest in being a customer of a company that makes decisions that are so user hostile and frankly, idiotic. reply chx 3 hours agoprevIf Recall is not completely illegal already in the EU then Microsoft is doing an any% speedrun from launch to having EU wide legislation specifically banning your product. There is no scenario where the EU will allow this to exist within its borders. The very valid concern for abusive spouses alone makes for excellent political fodder. You won't find a single soul in the EP who would speak up against a legislation framed that way. The newly elected representatives would certainly enjoy an easy win when most of their work will be complicated and very hard. reply nextworddev 4 hours agoprevMicrosoft (or any other FAANG) could leak all of our data, and not sure if it will face any real consequences. When was the last time a firm actually went \"under\" for a data security issue? Some recent data security breaches: - Snowflake - AWS / Capital One reply lukev 3 hours agoparentYes, I think everyone understands that leaks happen. The problem being pointed out here is that you can't leak what you don't collect: this is a lot more data, and a lot more sensitive than what MS has ever collected before. reply idontknowtech 2 hours agoparentprevWhat we need are large, mandatory, fines for every data breach that happens. Say $10k to every person whose address gets leaked. Then we'll see companies start treating consumer data as a risk, not just a low -cost asset. reply aembleton 1 hour agorootparentSome people want their location data pushed to the cloud https://news.ycombinator.com/item?id=40583946 reply bnchrch 3 hours agoparentprevDont forget Equifax! reply sickofparadox 4 hours agoparentprevSnowflake has actually been fairly consistent on saying that the breach was not of their underlying systems, but customer misconfiguration. I am reminded of the mid 2010s when AWS had certain systems public by default and tons of \"data breaches\" involved companies simply not making their data buckets private. reply datadrivenangel 58 minutes agorootparent\"Our stupid customers didn't turn on the 'safety mode' which our customer support people often tell them to turn off\" is such a reassuring PR tack. reply sickofparadox 8 minutes agorootparentIf the default settings of a system are to make it public, then yes it is the \"stupid customers'\" fault for not clicking the check box to make it private. Snowflake can't take the blame just because you went and dreamed up a scenario where customer support tells users to make their data public, then got mad at them for your imagination. reply amelius 4 hours agoprevLooks like their web server committed suicide first. reply mariocesar 4 hours agoparentWeb Archive version: https://web.archive.org/web/20240605120547/https://www.antip... reply chx 3 hours agorootparentEven that doesn't load for me, https://archive.is/5sSg1 does reply banish-m4 4 hours agoparentprevServer: Monty Python's Life of Brian - Crack Suicide Squad, Electronic Regiment/0.1.0 reply cgannett 4 hours agoparentprevI was able to get this to load by going to the blog site from an older post to antipope.org and then navigating to the latest blog post. That or whatever was wrong was fixed just now. reply ChrisLTD 4 hours agoprevMicrosoft's recently quarterly earning report showed a 17% rise in revenue, and their stock is up nearly 25% in the last year. reply doctorwho42 3 hours agoparentCue comic about value for shareholders and end of the world. reply trealira 3 hours agorootparentThe comic, for reference: https://www.newyorker.com/cartoon/a16995 reply bigstrat2003 4 hours agoparentprevGood for them. Doesn't really matter if their customers start refusing to do business with them because they are scared off by stuff like this. reply Rastonbury 3 hours agorootparentMost of MS revenues are from F1000 corporate who's legal/security teams would never enable these features and MS would never force questionable features on them. They increased quarterly cloud revenue by $7B, even if only $1B of that is from OpenAI models in Copilot/Azure, they'll pay back their investment in OAI in 2.5 years Author is just taking the opportunity make hot take on single terribly implemented product (or data gathering attempt), the big machine doesn't care reply chucke1992 3 hours agorootparent> Most of MS revenues are from F1000 corporate who's legal/security teams would never enable these features and MS would never force questionable features on them. Actually I think the feature will be enabled - companies have been spying on their users for ages. The thing is that the modern security is not about your stored features - you cannot login to laptops without passwords and a lot of devices even require cards (and flashdrives). Modern security is about networking (VPN and stuff) and social engineering - fishing and stuff. Malware, executables etc. are not thta much of threat anymore. reply bongodongobob 3 hours agorootparentprevThis is the correct answer. I know you're not supposed to say this but... The HN community is completely out of touch in regards to MS and what they do to make money and how organizations leverage their producs. For most orgs, leaving M365 ecosystem isn't even remotely an option from any angle. You're not replacing Office with g suite or any of the open options, no one is going to roll their own LDAP solution etc. I just laugh at the comments on every MS article on this site. The business world world is bigger than HN users' 7 person startups. reply Symmetry 3 hours agoparentprevIf I could be short Microsoft's OS business while still being long their cloud provider business and their investment in OpenAI I would. reply hnben 2 hours agoparentprev> their stock is up nearly 25% in the last year So they performed as well as the average S&P500? I would expect better from a tech giant reply breadwinner 3 hours agoprevMicrosoft is clearly not committing suicide: Look at how much attention this feature has got them. Every tech publication is talking about Windows. Windows is suddenly relevant again! How is that suicide? It is Microsoft customers that is committing suicide, by leaving this feature on. Users need to be educated on the dangers of this feature, and how to turn the feature off. reply Lord-Jobo 3 hours agoparentIf someone points a gun at you, says \"I am going to shoot you in 5 seconds\" and then you stand there and get shot instead of diving behind cover, its still not suicide. its a dumb death but the shooter still committed a homicide. And i do feel like that is a decent analogy for the situation reply datavirtue 2 hours agoparentprevWe went over the slippery slope and have the wind in our hair. reply OptionOfT 1 hour agoprevI searched HN but couldn't find anything, but I could swear I saw something like Recall while back: a screenshot tool that ran it through OCR / detection, put it in a DB and made it searchable. It was open source on GitHub. Sidenote: I'm conflicted on the privacy issue here, but on the other hand it seems to be bringing ARM to Windows (again), which I support. reply chris_pie 50 minutes agoparentPossibly this? https://news.ycombinator.com/item?id=40538889 reply alangibson 3 hours agoprev> We've realized for some time that not only are language model technologies going to dominate our future Where 'we' is defined as people whose income depends on ever more investment in LLM tech. For the rest of us, they seem hopelessly flawed as a general solution to anything. reply SpacePortKnight 2 hours agoprevAll these fancy AI laptops and all I want is a fanless laptop like MBA from Intel. reply elorant 3 hours agoprevRecall is the last straw for me; my next rig will be Linux exclusively. I don’t like it as a feature, don’t understand its usage, and most importantly I hate the way Microsoft tries to force it on my computer. Fuck that, and fuck them too. Next thing, I’m moving off Visual Studio too for Rider. For over a decade now Windows have been a shitshow with every next version being totally crap. Windows 7 was great, 8 was crap. Ten was good after a while, 11 is also crap. Probably they don’t care about Windows that much anymore since they’re making a shitton of money from Azure. But I have work to do and I can’t keep fighting my PC. reply kalleboo 2 hours agoparentWhat's stopping you from installing Linux this weekend? Why are you putting it off until your \"next rig\"? reply donmcronald 1 hour agorootparent> What's stopping you from installing Linux this weekend? Why are you putting it off until your \"next rig\"? For me it's OneDrive. I wouldn't trust it with a grocery list, but I do work for people that use it and there's no way I'd be able to connect to a share with an unofficial OneDrive app. Microsoft has so many little hooks that's it's really hard to drop Windows if you work for / with anyone that uses anything in the MS365 ecosystem. reply elorant 2 hours agorootparentprevWindows 10 works just fine. When it reaches EOL I'll move. reply datavirtue 2 hours agorootparentExplorer tabs are worth the move. reply bishbosh 57 minutes agorootparentI have found them to be an utterly incompetent implementation. Maybe it's an issue with my set up, but they are shockingly bad in a way that makes them feel like they were hacked together. They lack basic functionality like scroll wheel click on any navigation buttons. They are constantly slow in opening. All while missing basic considerations like \"Back\" isn't separated by tab, so opening a new tab and navigating to a folder makes my \"Home\" the previous folder for all tabs on that window. Again, maybe it's my set up, but going back to dolphin I am shocked they get away with something so half-baked. reply datadrivenangel 57 minutes agorootparentprevThey changed Alt+Tab behavior. Windows 11 is dead to me. reply zitterbewegung 3 hours agoprevI think if your replace Microsoft with Google it makes more sense. Microsoft AI in their laptops can fail like their windows phone. But Google has much less of a moat than Microsoft especially when they are hurting themselves in the same way as Microsoft and they are already doing what the article says. reply ksynwa 3 hours agoprevCan someone please enlighten me on the claim that the share of Intel processors is rapidly shrinking? I don't know enough to believe or doubt it. Apple moving to ARM processors seems like an important enough event for something like this to happen. But from cursory reading, I felt that Apple's M processors are so good because of their SoC nature where there are coprocessing units specialised for common tasks. I just thought that something like this could be achieved by Intel with their processors too but it doesn't look like that's the case. I am rambling but I hope someone can shed some light here regardless. reply limpbizkitfan 3 hours agoparentIntel’s PC market was partially eaten by Apple’s switch to M1 and even earlier, Chromebooks moving to ARM/RISC. New server farms and clusters now have a CPU and a GPU as opposed to the past where you’d have just Xeons everywhere (a market Intel took from Sun). Intel tried to buoy themselves by purchasing Altera and working on discrete GPUs/AI focused peripherals, but AMD bought Xilinx and had (I believe) far better resources and experience in the GPU segment. Most troubling for Intel, they still invest heavily in and own their fabs. If there’s some protective anti-China tariff (certainly, there will be if China and Taiwan do reunify), perhaps Intel could have a slight advantage with their fab locations?? reply twobitshifter 3 hours agoparentprev1) Intel hit a rough patch vs AMD chips for performance/price tradeoffs 2) Apple M Series Chips 3) New Snapdragon ARM chips are poised to do to the rest of laptops what M did to Macs. Once someone has all day battery life they aren’t going back to intel. reply rob74 3 hours agoparentprevI'm wondering if they mean \"Intel\" in the \"x86\" sense or \"Intel\" as in \"CPUs actually built by Intel\"? The market share of the latter is probably shrinking faster than the former... reply katbyte 3 hours agoparentprevthe apple chips are so much \"better\" for a bunch of different reasons including the coprocessors but the biggest being the move to the ARM instruction set vs intel's x86 - Intel can not make that change for their main set of processors. reply smaudet 3 hours agoparentprevhttps://www.cnbc.com/2024/02/07/arm-earnings-report-q3-2024-... https://www.techpowerup.com/322317/amd-hits-highest-ever-x86... Did a pretty simple search, but these appear to be the trend being referenced. The first marks a clear downward trajectory for Intel (on x86) and upward one for AMD, the second mentions high ship numbers for ARM. I can imagine you can dice those to show that the Intel market share demand is decreasing, at least relative to total market share (which could be increasing). > I just thought that something like this could be achieved by Intel with their processors too but it doesn't look like that's the case. Nope. Intel has an large set of extensions to support with CISC (slow, more complexity) whereas RISC architecture has a small API and hence more leeway to do as they please (fast, less complexity). You could probably strip down the intel CISC architecture to something much simpler/nimbler, but you'd break a ton of existing applications. Or you could go RISC as well, and break even more things... Or like Apple you already don't care and effectively deprecate all the \"old\" applications (they run on a compatibility layer), and use your expertise and vertical integration to design a system from the ground up...Microsoft and Intel would both have to decide to completely abandon their existing product line and \"start over\", which they could do, but then they'd also have to work very, very closely with each other (no egos/bosses trying to do things \"their\" way) to make that work, and probably piss off every other vendor they've ever worked with at the same time.... Which is probably why Microsoft is trying so hard to \"just\" get ARM working, its far simpler to target a platform with a large and growing base rather than try to work efficiently with Intel... reply aftbit 2 hours agoprevHas this post been deranked or something? It went from nearly the top of page 1 to half way down page 3 in under an hour. reply runjake 3 hours agoprevNo, I don't Microsoft is necessarily committing suicide. However, I believe they are too reactive vs proactive. In other words, making the mistake of focusing too heavily on predicting technological trends, rather than forging their own path and creating their own opportunities. Admittedly, they are certainly doing this in certain market segments, such as developer tools -- and perhaps, Azure. But what do I know? I'm a dumb programmer. reply msl09 2 hours agoprevTo answer the question in the title, I don't think that Microsoft is even remotely trying to commit suicide. I think they realized that they are so freaking huge that they can nuke the equivalent of an entire year of revenue and still be in no danger. So they can just try stupid shit left and right and see what happens, if they get lambasted for it, it's no big loss. If it works they will be genius visionaries, stock prices go up, investors are happy. It doesn't please me but, it is what it is. reply avmich 3 hours agoprev> Recall takes snapshots of all the windows on a Windows computer's screen (except the DRM'd media, because the MPAA must have their kilo of flesh) and saves them locally. Can I turn my app's windows into DRM'd media? Just curious. reply rossy 3 hours agoparentYes, I think SetWindowDisplayAffinity is the Windows API function. reply daft_pink 3 hours agoprevI think they simply look at a company like Google and wish that they could get flexible advertising revenues from all their users instead of getting a few bucks from Dell for every machine sold. If they could achieve that, they could 5x their revenue and make sooo much $’s. reply NemoNobody 3 hours agoprevOk - unpopular opinion - this would be cool if it was impossible to leave the machine. Id never need another screenshot. Perhaps if this was an on/off button like service. I don't need 12 hours of a gaming binge... or a any hours of a porn binge saved second by second for me. I have no use for record of movies I watched screen by screen - especially if they will simply be black screens. Total Recall of what browsing, project work or file organization/data management stuff I've ever done as I did it would be great actually reply bee_rider 3 hours agoprevI don’t think MS has any idea about how to make Windows better. So they are bolting on extra crap and services. Re-invent the UI again. Maybe nobody wants to work on hard and boring kernel stuff? reply baal80spam 2 hours agoparentI don't really think they want to make Windows _better_. All they want is to push more ads and other crap in the user's face. reply bee_rider 1 hour agorootparentIf they could make money by selling new copies of Windows I’m sure they would. But they ran out of ideas with Windows 7. Since there’s no reason to buy a new copy of Windows, MS needs to turn their customers into a recurring revenue stream. I wonder if they fired everybody creative a couple years ago while CPUs were stagnant, leaving them in a rough spot to respond now that things are doing ok (I mean we’ll never have 199X-200X again, but there’s competition again at least). reply ivraatiems 4 hours agoprevA couple of notes here. Overall I agree with the point being made but there are a few misunderstandings in this post. > Microsoft don't want to be entirely dependent on Intel, especially as Intel's share of the global microprocessor market is rapidly shrinking, He says Intel, but means amd64 architecture - Intel isn't the only or even the dominant consumer computer processor manufacturer these days, especially for desktops/laptops. > Recall takes snapshots of all the windows on a Windows computer's screen (except the DRM'd media, because the MPAA must have their kilo of flesh) and saves them locally. I wonder if I could cause Recall not to work by just playing some DRM'd content half-transparent over whatever I was doing... > Now, \"unencrypted\" is relative; the database is stored on a filesystem which should be encrypted using Microsoft's BitLocker. If you have that enabled, which most people don't. > Worse: even if you don't use Recall, if you send an email or instant message to someone else who does then it will be OCRd and indexed via Recall: and preserved for posterity. I don't see this as a problem; there's nothing stopping anybody you send anything to from taking a copy of it. You can't take emails or IMs back. It's just dumb. > Suddenly every PC becomes a target for Discovery during legal proceedings. This was already true and common. > Some commentators are snarking that Microsoft really really wants to make 2025 the year of Linux on the Desktop, and it's kind of hard to refute them right now. Or, people will just keep using Windows 10, which is what >70% of the market is already doing. reply BadBadJellyBean 4 hours agoparentBitlocker is enabled by default on windows 11. It's one reason for the tpm requirement. reply chucke1992 3 hours agoparentprev> I wonder if I could cause Recall not to work by just playing some DRM'd content half-transparent over whatever I was doing. The hilarious part is that the feature - OpenRecall is already there so the feature is not going anywhere. > This was already true and common. Exactly plus removal of emails and history is something that courts are frown upon these days. reply digging 3 hours agoparentprev> Or, people will just keep using Windows 10, which is what >70% of the market is already doing. Support ends in late 2025 though. They've been trying to push the upgrade aggressively lately which is what got me to switch over my last machine early, so now I am happily Windows-free. And honestly, Mint is not much more complicated than Windows, while being immeasurably safer to use, it's just that Microsoft has inertia and a propaganda budget. reply bentcorner 3 hours agoparentprev> I wonder if I could cause Recall not to work by just playing some DRM'd content half-transparent over whatever I was doing... At that point why not just disable Recall? There will be undoubtedly be a setting or at the very minimum a group policy option to disable the feature. reply ivraatiems 3 hours agorootparentSuppose your employer or network admin requires you to have it on. reply bentcorner 3 hours agorootparentIf that's the case it's not your PC anyways, so there's nothing for you to personally gain by obfuscating any information it gathers. reply Shared404 3 hours agoparentprev> I don't see this as a problem; there's nothing stopping anybody you send anything to from taking a copy of it. You can't take emails or IMs back. It's just dumb. I may trust someone personally, but not trust their ability to administer a system. This requires me to do both if I am sending information to somebody. reply digging 3 hours agorootparentSame reason I don't give out my real phone number unless I know how someone handles their phone. Doesn't take any malicious intent to grant Contacts access to all kinds of malicious apps. Other peoples' computers have to be assumed hostile. reply LoganDark 3 hours agoparentprev> I wonder if I could cause Recall not to work by just playing some DRM'd content half-transparent over whatever I was doing... Protected content works at the compositor level, so all that'd happen is that your screenshots would have a half-transparent black window over them. reply nadam 3 hours agoprev> 'The breaking tech news this year has been the pervasive spread of \"AI\" (or rather, statistical modeling based on hidden layer neural networks) into everything' Calling current generative AI tech as 'statistical modeling based on hidden layer neural networks' is as insightful as calling any new development in the tech industry (like personal computers, software as a service, or game engines) as 'usage of stored program computers' or even 'usage of transistors'. Although I understand that the author's aim is to somehow discredit generative AI this way, but referring to something as one of its very basic, old and low-level building blocks feels just dumb. reply amai 2 hours agoprevhttps://en.wikipedia.org/wiki/Move_fast_and_break_things worked well for Facebook. MS is just trying to follow that strategy, too. reply hiatus 4 hours agoprevhttps://web.archive.org/web/20240605120547/https://www.antip... reply vessenes 3 hours agoprevFrom the title, I thought Charlie would be in full rant mode, but this is middle of the road for him. Windows enshittification continues, and will continue, for sure. But, he’s absolutely wrong that most people care about whether Recall stores things or not. They really really want what Copilot offers (note I didn’t say delivers, yet), which is labor saving on their knowledge work. My kids use a version of Recall on Mac for classroom lecture notes, and .. they find it helpful. People who have a job of making PowerPoints based on information from emails are going to find Copilot helpful. These are the people OpenAI believes won’t have jobs eventually, because this sort of work is going from ‘needs human with college degree’ to ‘automatable’ very rapidly. Charlie doesn’t offer any alternative architectures for this in his post, and that’s because there aren’t really any — MS delivered a locally encrypted database with RAG, and that’s … as good as it gets for this feature set right now. If nobody wants it, it will die. But, a lot of people want it, because they believe they’ll keep their jobs and get to be more productive in the meantime. reply enragedcacti 2 hours agoparent> MS delivered a locally encrypted database with RAG, and that’s … as good as it gets for this feature set right now. I don't think that's true, at the very least they could have separately encrypted the db with a locking timeout the way that password managers do. That alone would eliminate a number of people's concerns. Beyond that there are a ton of ways they could make the feature safer. They could have a setting where anytime the DB is unlocked the system uses Windows Hello periodically to confirm that the user that typed the password is still in front of the machine. They could proactively prompt the user to update rules or pause Recall in cases where recall is capturing snapshots the user might not want stored (and I question the overall utility of an AI that can't do that reasonably well for the common cases). But all those things would be active reminders to the user of how creepy and dangerous the feature is, so instead they have to pretend that they've thought of everything and that there's no reason to be concerned. reply tempodox 3 hours agoprevThis article is balm for my battered nerves. Unfortunately it won't reach, let alone convince, those who have the most urgent need of getting the message. They will have to learn it the hard way. reply tempodox 3 hours agoprevIf things should progress logically for a change, all those organizations that have an obligation of confidentiality will get sued into oblivion for using Windows. reply chucke1992 3 hours agoprevPeople are using their personal accounts to store corporate credentials in browsers. These days social engineering is a bigger threat that Recall. Plus corporate laptops are required to use things like Bitlocker and stuff. reply rob74 3 hours agoparentOk, then Recall just increases the amount of juicy stuff attackers might find on your machine once they have found the credentials by other means. Forget browsing in private mode, Recall sees and remembers everything! reply donmcronald 24 minutes agorootparentAnd there's no way it stays per-machine. It's almost guaranteed it's going to get synced to all your devices via OneDrive at some point. reply chfritz 2 hours agoprevThey've been trying to commit suicide for over 20 years now, they just never succeed. reply billfor 1 hour agoprevPage 4 now. 254 points and counting. Everything else on page 4 is at least a day older than this. Seems like a useful discussion.... There was an article on this (positive I think) in the WSJ: https://www.wsj.com/tech/personal-tech/microsoft-ceo-satya-n... reply PaulHoule 3 hours agoprevThis article is so popular it crashed his website, see the archive https://archive.ph/5sSg1 I remember Clippy being the face of an online help system for Office that was highly effective. It's hard to remember before Google that the standard of a full-text search facility was that it probably didn't work. People had learned the hard way that it just wasn't worth doing a full-text search for the manual of a software product. Microsoft tried to break through that by putting a better-than-expected search facility behind an engaging character that... didn't work on an emotional level. Compare to other bad product launches from Microsoft such as OneDrive. Developed shambolically they didn't do due diligence on the trademark and had to rename it from SkyDrive. They tried to force it down your throat when it was in a state where it didn't work reliably: OneDrive became the default save location for Office and if OneDrive was flaking out you could not save a file at all! (Annoying as hell for a platform that is always asking me if I want to recover a file I didn't save four years ago) There's also the fact that people seriously roll their eyes now when they hear \"AI\", \"Blockchain\" and \"5G\" (especially in the same sentence) There are numerous papers where people apply ML to scientific problems and I'm not so inclined to post them because too many of y'all think that anything like that is a scam... Even though the automatic differentiator in neural network tools is sure helpful for solving differential equations and such. If I was trying to sell an AI product in 2024 I'd try to avoid the words \"AI\" and \"ML\" specifically: if you were working on this stuff ten years ago you were ahead of the curve, if it looks like you just got into it now the automatic assumption is you are behind the curve, not ahead. reply d_burfoot 3 hours agoprevPage is down, I assume due to load from HN traffic. Maybe HN should implement some kind of custom CDN, but only for pages that hit the front page? reply surfingdino 3 hours agoprevMSFT's future is safe, it is not a company of innovators, but it is good at selling its products and services. reply andrewfromx 3 hours agoprevmore like a beached whale. we just don't know if it was intentional or they just drifted this way and oops! reply astro- 3 hours agoprevI love that DRM content was where they drew the line. Customer privacy? Eh, whatever. But movies? Let's not poke the bear... Desktop copilots/assistants have huge potential, but this type of data collection can't possibly end well. reply hurflmurfl 2 hours agoprevSo after building my latest rig and crazily getting a lot of driver issues related to network I've finally ditched Windows for Linux. While trying to figure out how to keep track of all the system changes I make in case I change my distro and need to set up my system yet again, I came across Nixos, which has been working out pretty well for me. The issue I'm currently battling with is how do I connect my cloud drive (self hosted nextcloud) to the system in a way that it worked on Windows, and so far no luck. Once I figure it out (if at all), it's going to be perfect. Surprisingly, even gaming works just fine via steam. I've been able to play whatever I have in my library by \"forcing compatibility tool\" ( which is proton). Colour me impressed. Now I just need to confirm that I can run my country's official program for tax-related stuff via wine and I'll have no regrets. reply tsunamifury 4 hours agoprevNo they are using the best training ground they can find to teach AI to do your job. Every exec right now is selling the story will transform their company into an employee less infinite profit future. reply amai 3 hours agoprevMicrosoft is just the first prey of AI. reply roamerz 3 hours agoprevI wouldn’t mind the total recall feature if it was OPTIONAL. I would probably use it on a dedicated pc that I would use for diagnostics and issue remediation on. Would serve as a (probably) great tool for creating documentation after the fact where you don’t have the bandwidth to do it in real time. But to use it on a pc that is used for day to day activities? It would effectively circumvent every policy which I’m required to adhere to. This would be a shit show especially in government where anyone can legally and with force request “all correspondence and electronic records pertaining to xxx”. reply gigel82 3 hours agoprev> Microsoft is pushing this feature into the latest update of Windows 11 for all compatible hardware and making it impossible to remove or disable They lost me there. This is false, you can definitely disable it. Also, all these articles talking about how the SQLite database is \"plain text\"; if someone would want this feature to exist, how would they want the database to be \"encrypted\"? (assuming they'd still want to access it to search) reply enragedcacti 3 hours agoparentA good start would be doing what Microsoft already does for Edge's Password Manager. It separately encrypts the database with the user's creds and triggers a prompt for them when the feature is accessed. It doesn't solve all the attack vectors but it would be a good start. https://learn.microsoft.com/en-us/deployedge/microsoft-edge-... With Edge specifically its not really clear to me when the prompt is required but its at least sometimes as you can see here: https://support.microsoft.com/en-us/topic/edit-your-password... reply Iwan-Zotow 3 hours agoparentprev> This is false, you can definitely disable it. not in the corporate world if feature is enabled or disable on corp laptop, there is nothing you could do reply moffkalast 3 hours agoprevAlways has been. reply aftbit 3 hours agoprevI'm always here for a Charlie Stross post. Now what is this all about? >slurping all incoming email for accounts accessed via Microsoft Outlook into Microsoft's own cloud for AI training purposes Really? ALL incoming email that you just access via Outlook? That's not okay. Does Google or Apple do that too? reply throwme0827349 3 hours agoprevThank goodness my Windows updates are broken! reply blackeyeblitzar 2 hours agoprevI see it less as suicide and more like an anti competing attempt to get all the gains from the AI revolution, through classic Microsoft practices like bundling (tying AI features and products to other products), using their muscle in OEM partnerships (forcing Copilot keys on everyone), etc. I think they’ll succeed, so I don’t think it’s suicidal. I just think it’s bad for fair competition and our societies. reply nashashmi 3 hours agoprevSuicide won't be committed by missteps. Suicide will be committed by departing from your brand. And this copilot+ and recall is on-brand. And the missteps will be corrected. Remember tick-tock. The ticks suck! The tocks do well. Tick: 95, Windows 2000, ME, Vista, Win8. Tock: Windows 98SE?, XP, 7, 10. I am not sure where to put Windows 11. Copilot+ is a tick. Let's wait for what is next. reply renewiltord 3 hours agoprevInteresting UI slug. Truncation or word blacklist? reply breck 3 hours agoprevMicrosoft's achilles heel is Bill Gates' fundamentally wrong model of information, as demonstrated in his 1976 letter[1]. I am not sure if it is possible for Microsoft to maintain its large market cap and revenue streams once the population wakes up to the fact that _digital copyright_ has poisoned our society and needs to go. That being said, I worked at Microsoft, and it is filled with brilliant, hard working, competitive people of high integrity, so if anyone can figure out how to turn that Aircraft Carrier around and succeed in a world with intellectual freedom, it is Microsoft. When I started at Microsoft, people made fun of me for installing Ubuntu on my machine the first day, and for my love of Git over sd (Microsoft's previous VCS system which stands for \"shit dumpster\"). The year I left, Microsoft shipped Ubuntu with Windows, and bought GitHub. They've started the turn. I hope they do the right thing, even if it means smaller profits (I'm not sure if it does). [1] https://en.wikipedia.org/wiki/An_Open_Letter_to_Hobbyists reply cjk2 3 hours agoparent> That being said, I worked at Microsoft, and it is filled with brilliant, hard working, competitive people of high integrity, so if anyone can figure out how to turn that Aircraft Carrier around and succeed in a world with intellectual freedom, it is Microsoft. This is no good if the management and marketoids are fuckwits though, which they are. reply logtrees 3 hours agoparentprevWhy do you think that digital copyright has poisoned society and needs to go? reply __MatrixMan__ 3 hours agorootparentI'm not who you asked, but I think that digital copyright has poisoned society and needs to go because to enforce it, you need a way to shut content off, so you need to build infrastructure that grants somebody censorship power. That's a high value target, and attracts the attention of precisely the people you'd rather not have censorship power. The internet infrastructure that allows digital rights holders to protect their intellectual property also lends itself to things like billionaires owning media platforms so that they can influence public opinion: It's about protecting capabilities that third parties use to influence discourse at the expense of the users. If you reshaped the landscape to remove the chokepoints, there would be nothing for those billionaires to buy, and our technology wouldn't be amplifying cults of personality to the extent that it currently is (or rather, it would amplify based on personality, not ownership... an upgrade?). We're living in a sort of influence aristocracy which is protected by the seats of control that were created to protect digital copyright. reply logtrees 3 hours agorootparentHow would you otherwise protect the people who invest their time and resources into building digital tools and products? Without digital copyright isn't it open season on any digital idea since it can just be stolen? reply __MatrixMan__ 1 hour agorootparentMy primary argument is that however bad it would be if content creators were left unprotected, the sort of platform abuse that we're seeing today is even worse. Happy artists are important, information symmetry (re: markets and re: democracy) is more important. But yeah, suppose we've made that switch, and now we want a new way to encourage content. I'd propose that we build attribution into our protocols. Many years ago there was controversy over \"reaction\" videos on youtube. They were being taken down because half of the screen was copyrighted (while the other half included somebody seeing it for the first time). Rather than having the content exist as a single thing that either is allowed to be public or is not, I'd say that the fight should converge on how much credit was given to the person who put together the reaction video (some, probably, but not much), and how much credit goes to the creator of the reacted-to content (more probably). When I say \"credit\" I mean that as you browse your browser would keep track of which content you consume, and which parts of it are attributed to which people (or maybe these are bank accounts or crypto addresses or whatever). Part of paying for internet access would be allocating some fixed amount, $15/mo say... for content. At the end of the month, you can either send the $15 to the creators of your content and send the ISP proof that you've done so. If you don't they still collect the $15, but it goes to some nonprofit dedicated to educating creators on how to ensure that they do in fact get credit for their work. This way there's no incentive to not pay for the content you're consuming: you the consumer are out the money either way. Also, the browser doesn't have to share the consumption history with the ISP... in order to meet their regulatory burden, the ISP need only see proof that you have done so. Lastly, unallocated money goes towards ensuring that future money is indeed allocated to a content creator, so it's a negative feedback loop which would hopefully converge on a state where most people participate in funding their creators. If ISP's don't cooperate, we tax the bejeezus out of them and remove their protections re: who gets to use which buried cables such that new ISP's emerge that will cooperate. reply LoganDark 3 hours agorootparentprevAre you actually talking about software patents, not digital copyright? Software patents protect ideas (i.e. algorithms), software copyright protects implementations (i.e. source code and binaries). reply logtrees 2 hours agorootparentBoth. reply LoganDark 3 hours agorootparentprevI don't quite know how best to articulate why, but I agree. Copyright sort of has the wrong idea. It's intended to prevent the copyright owner from missing out on the opportunity to capitalize on their work—similar to software patents (which are disgusting in their own right)—the rationale being the idea that if free copying (\"piracy\") were allowed to run rampant, the copyright owner would only ever make one sale. However, this assumes that every pirate is the loss of a customer. This is certainly not the case, neither for me nor for most pirates out there. There are quite a few reasons people pirate software in practice, and almost none of them deprive the copyright owner of a potential customer. Either because the pirate never could have been a customer, or because they still are. Here are some reasons I have either personally experienced, or commonly seen, for piracy: - They can't pay at all. Say, kids without bank accounts. They may become customers later when they have money, but there's no use in campaigning against them now unless you want to make an enemy of them! - They want to try before they buy. Some pirates buy software they like after pirating it. There's no point in buying something they won't use or like, especially if it's non-refundable. So they pirate first, as a sort of demo. - They can't buy it even if they have money. For example, old media or retro games that are no longer being sold. There's no benefit to restricting people's freedoms here because there's no monetary gain to be lost because no sales are even being allowed. - They can't own it with their money. Some people simply believe in owning the software they use and the content they watch. These people aren't going to care about streaming or subscriptions because it's viewed as a pointless expense. They'd be willing to pay more up front for real ownership, but if that's not available, they're not going to take a subscription. - The buyer experience sucks. For example, streaming services or Adobe subscriptions. Many streaming services won't even do 1080p unless you use a smart TV. Adobe doesn't let you buy software to own anymore, only rent access to the latest offering, with whatever new changes their whims dictate. Microsoft has been moving in a similar direction for a while now. - The price is too high. They aren't the intended customer and the copyright owner doesn't want their business anyway. No money lost by pirating. - They don't want to give the copyright owner money. If not for piracy they still wouldn't consider the purchase. But of course, there are still: - The purchasing process is too annoying or too much effort compared to pirating. If pirating didn't exist as an alternative, maybe they would've had more patience, but even then, there's only so much bullshit that any one potential customer can handle. - The desire to get something for free as the primary reason to pirate. This is typically how most copyright owners think of it, and yes, it does happen. It could be because of entitlement or because of a different philosophical belief (i.e. information should be free). See Gabe Newell's \"piracy is a service problem, not a money problem\". I get all my games on Steam. I don't care about the DRM or the fact that I don't technically own the games. It's so easy, they treat me well as a customer, they respect my rights, so they're the natural choice for me. Not only that, they provide services which are genuinely helpful, such as P2P networking through their servers, which means anyone can host a multiplayer server and have their friends join. It is worth the money. But I pirate shows I like once they're removed from Netflix or whatever other streaming service, because they're literally not available anymore. Literally, if there were a way for me to pay I would, but I can't pay money even if I want to. The copyright owner is basically telling me to go fuck myself. They shouldn't be allowed to do that and still prevent me from pirating the content that they won't sell me. So yeah. Digital copyright is dumb. reply logtrees 2 hours agorootparentWouldn't getting rid of digital copyright affect someone who produces the idea and the code, though? By empowering others to just steal their idea and their code. reply smaudet 2 hours agorootparentprev> They shouldn't be allowed to do that and still prevent me from pirating the content This is the only part I partially disagree with - privacy is a thing, and privacy of personal information is (or should be) a thing as well. This is the whole concept behind GDPR... In case you don't think this is an actual problem, go look up Hunter Moore and the ethical issues with unfettered dissemination of information. Of course its completely backwards that \"movies\" are given better protection than medical or other sensitive information.... The solution is not rampant piracy, for media concerns, all media/games/etc should be archived by e.g. the Library of Congress and available for purchase, unless determined by due process e.g. that the content is harmful (revenge porn, e.g.). This doesn't need to be a convenient process, obtaining e.g. such media can be a matter of requesting/buying/shipping encrypted usb drives around, perhaps (or purchasing a key for encrypted files online), licensing to distributors such as Netflix should still be the norm and more economically/ergonomically viable option. But I do at least agree that piracy itself is not the problem, it is a symptom of a larger socio-economic issue. reply LoganDark 2 hours agorootparentCopyright shouldn't be what I have on my personal information. That's the wrong model to use. I'm not selling my own personal information and I'm not miffed by the idea of losing out on customers. In fact, I don't even want customers. My information is already being sold without my consent or involvement, and that's fucked up. I don't want them to come to me, I want them to go to jail. The correct model is indeed privacy, which is that people should have the right to their PII not being sold or distributed without their express, informed consent. This has nothing to do with piracy or copyright. I'd say people should have the right to their PII not even being recorded in the first place, but consent to recording is often implied when you voluntarily provide it in exchange for a service, so we still need additional protections on top. I'd still appreciate not being recorded without my express consent, though. I appreciate countries that have laws against it. I do appreciate GDPR and its effects on data storage, retrieval and deletion. But it's only one piece of a complete set of privacy-preserving regulations, IMHO. reply dustedcodes 4 hours agoprevLast year at the peak of all the tech lay-offs someone said that the cheapest way to quickly pull an Elon Musk and fire huge swathes of employees is by buying them a Windows Surface device for work and then wait for the resignations letters shortly after. I wasn't sure if this was a joke or they were serious, I wouldn't be surprised if this was true lol reply datavirtue 2 hours agoparentThe Surface truly is Apple-priced garbage. I have had several and didn't like any of them. I have a new one now. We made the mistake of rolling them out with Windows 10. Trackpad and screen input are easy to disable accidentally. Real PITA. reply iancmceachern 3 hours agoprevDon't forget Hololens reply generalizations 3 hours agoprevNo, they aren't. They're desperate. We've realized for some time that not only are language model technologies going to dominate our future, but that they require massive compute to run. We've also been talking about Apple - their silence, but also their hardware capabilities to run LLMs on consumer devices. Every other player (OpenAI, Facebook, Microsoft, etc) is faced with a billion dollar datacenter investment that still provides a worse experience (latency & privacy). Apple has a huge advantage because they get to pay nothing for a superior compute infrastructure that comes with a better privacy and latency story. As soon as they release something, it's going to be good, and it's going to be free, because it runs fast on consumer hardware and consumer electricity. A billion-dollar datacenter that's upgraded every N years can't compete with that. Microsoft knows this, and they're trying to get out in front of whatever Apple comes up with. That's why they're telling manufacturers to install these \"NPU\" things - it's a direct equivalent to the iPhone/iPad/Mac compute capabilities. So no, Microsoft probably knew this would be the reaction. But it's better to get dragged through the mud for lack of trust than for being left behind the cutting edge. In the former case you're just a slimeball; in the latter case you're a moronic slimeball. reply jerf 3 hours agoparent\"We've realized for some time that not only are language model technologies going to dominate our future\" Close, but not quite. What we realize right now is that AI has pumped, pumped, pumped a whole lot of stock valuations sky high, and the first faint glimmers of the market asking that some actual money be delivered in proportion to the sky high valuations are starting to appear. (Note carefully the \"in proportion to\"; it is not enough to make a bit of cash. These valuations are enormous.) The stocks so pumped need results more-or-less right now, since propagating anything they have right now out into the market to actually gather revenue is going to take some time. I expect a couple of other Hail Mary plays like this to show up too. And again, let me underline, emphasize, and highlight, this isn't about whether AI can make \"any\" money. It's about whether it can justify those staggeringly high stock valuations in, say, the next year or so. This is a fairly uphill battle. Even if you are a True Believer than AI will be an economic revolution in literally the next 12 months or so, it's hard not to look at the market and come to the conclusion that right now every AI player is being valuated as the inevitable victor and the inevitable beneficiary of 100% (or more) of the AI gain, e.g., nVidia is being valuated as if there will never, ever be any competition for AI hardware despite all the incentives, each AI user valuated as if they are the inevitable winner in their respective domains, etc. They can't all win simultaneously like that, it's just impossible. (On the plus side, it's really just stock valuations. I don't expect an AI winter this time, the tools are much more valuable than they were last time. I mean, I would still very firmly say that what we have right now is grotesquely over hyped, but there's still quite a bit of \"there\" there. The previous AI hype cycles didn't have anywhere near as much \"there\" there.) reply solidasparagus 3 hours agorootparentNVIDIA has fundamental hardware and software advantages that will take years and years to catch up on. The general purpose part of GPGPU is no joke, just ask the people trying to build competing hardware and an alternative to the CUDA stack. But... NVIDIA is massively investing in AI companies with the expectation (explicit or otherwise) that they use the money on NVIDIA hardware. And they are pouring in ridiculous amounts of money. I think this is severely impacting the ecosystem, creating overvalued and overfunded AI startups that lack viable businesses and driving essentially fake revenue to NVIDIA that is unsustainable. The money is basically free for NVIDIA to spend, maybe even net positive between revenue numbers and progress towards monopolizing the AI stack. Time will tell if the bubble will pop or if one of the plays will hit and turn a fake flywheel into a real one, but I definitely agree that the stock valuations are too high. reply Wytwwww 3 hours agorootparentIt's not that clear Apple/local-AI is necessarily going to directly compete that much with large models running in datacentres (outside of some relatively limited use cases) and there don't seem to be many signs that Nvidia has any serious plans about entering the consumer market and compromise their insane margins. reply mjr00 3 hours agorootparentprevSpot on. The current wave of GenAI has created a bunch of new markets, but those markets are, at least currently, the unsavory types: LLM-based virtual girlfriends, AI hentai games running off stable diffusion, SEO and social media spam with ChatGPT, etc. The best we've got in the \"regular\" enterprise world is excellent parsing of unstructured documents. Which is, to be sure, hugely valuable and will make a big difference in a lot of industries. But isn't nearly enough to justify the insane evaluations and investment companies are making into AI right now. Two years into the AI boom, and it's starting to look closer to blockchain and the metaverse than the iPhone. reply AlexandrB 3 hours agorootparentprevThe costs are also largely being ignored at the moment. Both the obvious marginal costs (electricity, hardware for compute, dev time) and the hidden costs (flood of spam and misinformation). reply Bluecobra 3 hours agorootparentI was really surprised to read the other day that Dell isn't making money on AI servers. I played around with the server build tool on their website and was surprised to see that a H100 adds ~$50K to a server! How the heck are these companies spending $100K per server going to get back their ROI? Are all these costs going to get written off as R&D? This really seems like the dot-com days but worse somehow. https://www.cnbc.com/2024/05/31/dell-shares-fall-ai-servers-... reply Yoric 3 hours agorootparentprevAlso, the environmental cost. It's hidden, but won't be forever. reply russiancapybara 3 hours agorootparentprevWould be awesome to have nuclear power stations dedicated to powering these data centers for inferencing. I believe Amazon just purchased a nuclear power plant in Pennsylvania for this. reply causal 3 hours agoparentprevAgree on the Apple strengths, disagree that MS is scared. PCs just aren't where they make most of their money anymore. Enterprise is extremely lucrative for them, and their headstart in the AI space may be just what they need to close the gap with AWS. reply Zhyl 3 hours agorootparentWhat would you do if you had several billion devices that you had root access to, used directly by humans and isn't making you any money in sales these days? The 2024 answer is to use it to capture data to train AI. Microsoft could lose 9/10 of their market share (which they won't) and still have millions of devices harvesting training data for them. reply lalalandland 2 hours agorootparentIf GDPR is at all questionable, Microsoft is not able to push this to EU countries. The risk for a business to get in GDPR trouble is to high. reply CuriouslyC 3 hours agorootparentprevAzure is bad. Google is the one that's going to leverage AI to catch up with AWS, GCP is actually good and they've already partnered with Huggingface. reply bruce511 3 hours agorootparentFeatures aside, I simply don't trust Google to keep GCP running in the long term. Their propensity to kill projects (even ones which are successful), plus their gratuitous price hikes (aka Google Maps) has made me Goggle negative. Sure, they might run GCP forever. Sure they may offer some sort of customer service at some point. Sure they may decide to keep pricing at sane levels. The key work in all that is \"might\" - and that requires my _trust_. And for better, or worse, my trust simply isn't there. reply CuriouslyC 2 hours agorootparentI 100% feel you on the google trust. I think it'll end up being central to a lot of their other efforts so I'm expecting it to stick around, but hybrid cloud solutions are just a smart bet anyhow. reply yesdocs 3 hours agorootparentprevWhy is azure ‘bad’? Is it because you dislike Microsoft? In my experience Azure is really good and getting better. Even if AWS is more popular, Azure is gaining traction, is more intuitive and easier to use, and in my experience: cheaper reply Hikikomori 2 hours agorootparentHad to do multi cloud with Azure, worked on network infrastructure, vnet, peering, firewall, vpn gateway, etc. Documentation was almost worthless, leaving a lot of room for interpretation. Encountered and filed more bugs trying to get it up and running than I did for years of using Aws. Support was mediocre. Non existent ipv6 support. This was a few years ago and in comparison to Aws. reply CuriouslyC 2 hours agorootparentprevthey've had long running security issues and performance edge cases that make relying on them a bit of a time bomb. reply bongodongobob 3 hours agorootparentprevYou realize that Azure is much more than compute right? reply yesdocs 3 hours agorootparentOf course, and why is azure ‘bad’? reply bongodongobob 1 hour agorootparentIt's not, it great if you have a large org. reply luma 3 hours agoparentprev> Apple has a huge advantage because they get to pay nothing for a superior compute infrastructure that comes with a better privacy and latency story. Right now, AI is a pay-$B-to-play and you need the massive datacenter compute for training and skilled researchers to develop. Apple silicon is neat, but let's not pretend like they have any credible story to tell about AI, and their recent deal with OpenAI suggests that they've been forced to punt. > As soon as they release something, it's going to be good, and it's going to be free, because it runs fast on consumer hardware and consumer electricity. Apple recently released their first public LLM, OpenELM. The largest released model is OpenELM-3B. The Instruct version of this model scores a 24.8 on the MMLU benchmark, which is a pick-one-of-four multiple choice test. Apple's biggest and best model somehow manages to get less than 25% on a 4 option multiple choice exam. For reference, MS released a similar size model Phi3-mini-4k which scored a 69 on that same exam. Yes, Apple has some nice local hardware to run models, but they don't have the models. They don't have the models because they don't have the people and they don't have the compute. reply coltonv 3 hours agoparentprev> We've realized for some time that... language model technologies going to dominate our future Citation Needed. reply yyggvbb 3 hours agoparentprevI think Microsoft software teams are already working behind their legal teams. Their business model is more about licensing than making products. Likely they’ll force some kind of regulatory situation that’s advantage to them. For example no AI capable OSs for government work, or something like that. reply Wytwwww 3 hours agoparentprev> Apple - their silence, but also their hardware capabilities to run LLMs > billion dollar datacenter investment that still provides a worse experience It's not at all clear to me that those very small local models would be competing directly with large LLMs run in datacentres for the same use cases. If that's going to be the case MS/etc. would also require significantly less compute power that's is currently expected, so Nvidia would be the real losers in that case (unless they shift to consumer HW). Also does Apple have such a huge advantage compared to the newer AMD/Intel/Google/Qualcomm(?) chips with NPUs? Yes Apple started investing sooner but all the phones/tablets and the overwhelming majority of laptops are crippled by extremely low memory capacity (in the context of running LLMs). So are they really that much ahead? e.g. what real advantages does the latest iPhone have compared to a Pixel 8 Pro (sure it's still a niche device, with a seemingly inferior an chip (but more memory) but based on what we know Google seems to be ahead in AI development and will make their solution available on all high end Android devices). > As soon as they release something, it's I doubt that's a given, their software has long ceased to be highly particularly exceptional and on the hardware side the only actually innovative product they have released for years has been a flop with a very unclear future. reply ca_tech 3 hours agoparentprevThe realized value of LLMs is going to be output from a data to AI pipeline. From raw data to actions or insights. This is Microsoft's play to control the entire process. They are attempting to abstract away all the \"tooling\" needed to manage and process data. It falls down because there is no discretion left to the consumer. Objectively, the computer is already processing all the data you are interacting with. Subjectively it is assumed its ephemeral. Computer forensics proves this wrong. So I think to refine my opening statement, the realized value of LLMs is going to be the output from a \"curated\" data to AI pipeline. Which Microsoft is not providing with this solution. reply bee_rider 3 hours agoparentprevIt seems like a somewhat far fetched explanation, but then I can’t think of a better one. They come up with this farcical application that nobody wants, to justify the hardware requirement, so that at least the hardware will be out there for when Apple shows them what the actual application is. reply amelius 3 hours agoparentprevJust wait until TSMC asks for 30% of Apple's revenue or they'll kick Apple out of their Fabstore. Fabrication is where the tech advantage lies, not hardware design IP. Anyone nowadays can program VHDL, see open source hardware. The challenge is actually fabricating it. reply Miraste 3 hours agoparentprevApple doesn't have as much of an advantage as it appears. Their Neural Engine are fast and getting better, but modern machine learning models need another resource: fast RAM. Unfortunately for Apple, their whole product stack is segmented based on RAM. There are millions and millions of Apple devices with lightning-fast processors and 8GB or even 4GB RAM. These won't be able to do anything like Copilot+, unless they make calls to a datacenter like Microsoft is doing. Their existing local models are hit-or-miss already. FaceTime's audio transcription is laughable. Whether this is due to memory requirements, I don't know, but it doesn't bode well for further models. reply SideburnsOfDoom 3 hours agoparentprev> language model technologies going to dominate our future I know many people who are _far from convinced_ of that. But that's not the relevant point: Microsoft, Google et all clearly are. And watching them collectively, desperately stampede after the mirage, at great cost, is certainly something. reply surgical_fire 3 hours agoparentprev> As soon as they release something, it's going to be good Huh? How is that a given? reply luma 3 hours agorootparentIt is absolutely not true, they already released a model (OpenELM) and it compares extremely poorly versus similar size models from the likes of MS. This chart is correct and to scale: https://i.imgur.com/5mjlesU.png reply pjkundert 3 hours agoparentprevSuddenly, distributed private inference, compute, storage and bandwidth mediated by Holochain at global scale will become … valuable. reply oldpersonintx 3 hours agoparentprevApple is in terrible shape on software, AI is driving this home the only tier-one, network-effects app they control is iMessage search? someone else maps? someone else video? someone else audio? someone else and now they will go to someone else for AI what did they do in the meantime? VR that bellyflopped reply AlexandrB 3 hours agorootparentNot sure why this is relevant. Apple's forte was always the hardware (and lately the greater focus on privacy). And you could easily say the same about Microsoft: search? someone else maps? someone else video? someone else audio? someone else Arguably Apple is at least better positioned than MS for some of these because people actually use Apple Maps and Apple Music. When was the last time someone whipped out Bing Maps? reply boxed 3 hours agorootparentprevYou forgot iOS. App APIs are also network effect things. reply delichon 4 hours agoprevThe behavior of any bureaucratic organization can best be understood by assuming that it is controlled by a secret cabal of its enemies. -- Robert Conquest's Third Law of Politics I'm grateful that it's not compulsory for me to financially support this particular bureaucracy. reply divan 3 hours agoparentHow to understand this quote? I've recently started seeing bureaucratic organisations as a simple one-/few-cellular organisms. I.e. one person is smart and intelligent with 90 billion neurones contributing to its behaviour, but bureaucratic organisation with 1000 people is like an organism with 1000 neurons (or less). reply constantcrying 3 hours agorootparent>How to understand this quote? Exactly like it is written. Have you ever been in a 100k+ employee corporation, which has thousands of suppliers? I have and I do believe it is a pretty good description. Again and again the simplest tasks turn into monumental disasters, again and again you see incomprehensible \"reorganizations\", which break up functional structures and turn them into dysfunctional ones, again and again top management forces insane decisions, which everyone at the bottom knows do not work. >I've recently started seeing bureaucratic organisations as a simple one-/few-cellular organisms. I.e. one person is smart and intelligent with 90 billion neurones contributing to its behaviour, but bureaucratic organisation with 1000 people is like an organism with 1000 neurons (or less). That is not a good description at all. Bureaucracy happens because every single node is infinitely complex and responds to wildly different inputs. reply lalalandland 2 hours agorootparentBureaucracy on a certain level can almost be described as an artificial living organism. It starts to behave in certain ways that nobody can control. reply JohnFen 3 hours agorootparentprevI think that quote is just a restatement of \"We have met the enemy, and the enemy is us.\" reply eesmith 3 hours agorootparentThat's a Pogo commentary on pollution, “We Have Met the Enemy and He Is Us”. https://library.osu.edu/site/40stories/2020/01/05/we-have-me... For another pop culture reference, \"A person is smart. People are dumb, panicky dangerous animals\", Men In Black. reply breck 3 hours agoparentprevWow, thanks for that quote. When I worked at Azure, one of the things I was responsible for was the \"deep link\" code to get to the management page for a resource. So I became intimately familiar with Azure \"resource ids\". I could not for the life of me figure out why they were designed the way they were designed. They should have just been a random or custom alphanumeric hash/permalink. I asked my questions and raised my concerns internally but failed to get things fixed (politics is not my strong suit). My most probable explanation for them was that they were designed by a mole Jeff Bezos had gotten into the Azure org to work on behalf of AWS. reply katbyte 3 hours agorootparentfwiw i am a fan of azure resource IDs as they make it easy to identify what and where a resource is vs a random magic string you have to go look up reply NoMoreNicksLeft 3 hours agorootparentprev> My most probable explanation for them was that they were designed by a mole Jeff Bezos had gotten into the Azure org to work on behalf of AWS. I've read this five times now, and each time I read it I flip on whether it's a joke or some kind of (semi-)anonymous tip being leaked to a journalist through a pre-arranged drop site. reply gradus_ad 3 hours agorootparentprevIncompetence does a fine job of explaining this, no? reply throwup238 3 hours agorootparentMicrosoft’s signature Incompetence™ Enterprise Edition no less. Overcomplicating things the enterprise way is exactly what I expect from them. reply SigmundA 3 hours agorootparentprevYes Hanlon's Razor is sort of the opposite side of the coin here, almost like Clarks Third law except: \"Any sufficiently bureaucratic system is indistinguishable from a cabal of enemies\" reply laxd 2 hours agoparentprevWould this be an example of the Yoneda lemma? reply immibis 3 hours agoparentprevHave you bought a computer recently, or, like, ever? Every OEM pays a Windows tax, even on computers that don't come with Windows. reply TacticalCoder 3 hours agorootparent> Have you bought a computer recently, or, like, ever? A non-Mac computer you mean? I bought a desktop in parts, which I assembled myself and installed Linux on it. I don't doubt there may be some hidden Microsoft tax somewhere in the stack but I certainly didn't pay any Windows license nor any other Microsoft software. reply NemoNobody 3 hours agorootparentMicrosoft owns substantial share of Apple - in the 90s Gates saved Jobs so that Msft would have a \"competitor\" reply ben_w 3 hours agorootparentprevI remember people saying that in the 90s, I rather assumed it stopped with them getting sued for monopolistic practices. reply yyggvbb 3 hours agoparentprevThis is laughably paranoid. I bet you’re great with the ladies. reply crims0n 3 hours agoprev> with dick moves like disabling auto-save to local files in Microsoft Word (your autosave data only autosaves to OneDrive) Unrelated, but this burned me so hard a few months back - I was so pissed. Worked on a document for hours, saved it but the file was corrupted. No problem, will restore an autosave snapshot... only it was disabled in some update, and only works with OneDrive if you manually toggle the switch. I was flabbergasted. reply selimnairb 3 hours agoparentThis is why I subconsciously hit Cmd-S/Ctrl-S every 15 seconds. reply boxed 3 hours agorootparentOn macOS they switched to live state save for documents decades ago. reply cjk2 3 hours agoparentprevStuff like that makes me appreciate Time Machine on my Mac. I have hourly snapshots of everything that changes. I don't even use VCS software when I'm writing code any more unless I have to collaborate with someone else. Also there are serious problems with OneDrive version history. In numerous occasions, files have failed to restore or the UI is broken and unusable. reply nothis 2 hours agorootparentDropbox is still unmatched as a service not trying to push you into a trillion dollar monopoly (One Drive, iCloud, Google Drive). It's admittedly been a while but I tested a whole bunch of automatic cloud backup software at one point because I got pissed about some annoying niche cases and Dropbox just worked throughout everything I threw at it, automatic versioning and recovery included. It doesn't mangle file names, handles huge files, folders with tens of thousands of small files, recovering old versions of accidentally overwritten files, works on Mac and Windows without losing a beat, sending people files always works. I guess for collaboration you want to use stuff like Google Docs and advanced versioning software for coding (I'm not talking about coding stuff here, btw, which might throw off some people on hacker news). But Dropbox delivers for \"I need a backup and I don't want to think about it except when I mess up and need its help\". reply ARandomerDude 3 hours agoparentprevThis is exactly why I use git for anything important. You have to remember to save and commit periodically but it has really saved me. reply pradn 3 hours agoparentprevThis is so baffling to me. Is it just to reduce traffic to OneDrive or something? reply gigel82 3 hours agoparentprevWell, if you had recall enabled, you could get all your text back out of that :) reply hulitu 1 hour agoprev> Is Microsoft trying to commit suicide? Kids those days. Wikipedia has some pages about Microsoft. /s reply trollerator23 3 hours agoprevEh. Charles Stross went off the rocker long ago. Not worth reading. reply solidasparagus 3 hours agoprevI feel like you are complaining about the first car. Yes, it sucks and, god, I'm not sure it's good for the world. But zoom out a bit and it seems fairly clear to me that this is what the future is going to look like and this is why Microsoft feels compelled to act. This is the first step to creating an assistant that automatically does tasks for you on your computer driven by natural language. The context being collected by Recall is essential for this technology to work effectively as people frame natural language instructions with the assumption that the assistant shares your context. And they probably want to collect a training set, although it's excellent (and necessary for trust) that this data is local-only, which might mean this doesn't generate any useful training data. I'm super happy about that. Despite occasional snarky comments from tech savvy people, people absolutely do want this - normal people like computing to be simpler. I do agree that this feels a bit early, but I don't know enough to say yet. Rewind AI (the startup) handles privacy by not capturing incognito windows, which I feel is a good start and we need to keep iterating on techniques like that to balance trust against usefulness (of course with the ability to disable the whole thing if you are willing to lose the features). Hopefully Microsoft spent enough time thinking about this, but time will tell. Fundamentally the need for AI to understand your context to correctly assist you as well as the feedback techniques essential to training effective AI are going to run up against privacy concerns and we're going to have to figure them out as an industry - preferably in public, preferably with a company that takes trust seriously. This is fundamental to the technology IMO. Is Microsoft that company? I don't know, but I wouldn't underestimate how valuable this data would be if it was on Microsoft's server, so I personally am quite happy that they made the right choice there. reply wkat4242 3 hours agoparent> people absolutely do want this - normal people like computing to be simpler. Of course I want this. I just want to have it myself. Like for the assistant and its data to be truly mine. I don't want Microsoft to run it for me. I don't mind paying for someone to build it but not to operate it. Especially not Microsoft. They're as bad as Google. I work with Microsoft people daily and they're so brainwashed. They think their company is beyond any reproach, that the products are the best in the market, that using them is a win for us etc. I hate them and their company with a passion but unfortunately our leadership is easily impressed and not too smart. They just hobble along with whatever Gartner is paid to tell them. reply solidasparagus 3 hours agorootparentWhat do you mean for the assistant to be yours? There is a core model somewhere that needs to be trained and the cost/technical requirements are prohibitive for that to be done by each individual. If the model is local, is that enough? The data of course should be yours - and until we solve information leaks in AI (never?), it will have to be or the product has huge viability risks. Agree on data, which is one of the core issues. The expectation of privacy on your device is very high. But that can change, especially with the rise of web app tools that blur the line between your private space, the company's space, and public. reply JonBarrett92 2 hours agoprev“Unfortunately, human beings assume that LLMs are sentient and understand the questions they're asked, rather than being unthinking statistical models that cough up the highest probability answer-shaped object generated in response to any prompt, regardless of whether it's a truthful answer or not.” I don’t think this is something that will stay true. Users become more educated on what their systems can and cannot do as well as an intuition of their inner workings over time. Getting the shape of the highest probability answer is a very useful tool that shouldn’t be underestimated, especially if it’s understood that way. As for the Recall related stuff in the rest of the article, I have similar sentiment. Only difference is that I feel like they could really pull off a very useful, similar tool but there are definitely a lot of serious concerns to be solved here first, and rolling it out in its current state is morally irresponsible of those pushing it at Microsoft. reply andrewdb 4 hours agoprev [–] Steel-manned Deductive Argument Premise 1: AI (specifically, statistical modeling based on hidden layer neural networks) has been increasingly integrated into various technological products and services. Premise 2: Major companies like Microsoft, Apple, and Intel are intensifying their efforts in AI development and integration, with Microsoft announcing products like CoPilot+ and Recall. Premise 3: CoPilot+, which is AI-driven, is built on users’ actual usage data, potentially offering more realistic and user-friendly assistance than previous AI iterations like Microsoft’s Clippy. Premise 4: Recall, another AI feature by Microsoft, aims to enhance user productivity by automatically capturing and storing screenshots and textual content, but it stores this data unencrypted locally, posing significant privacy risks. Premise 5: The continuous expansion of AI features in technology products often correlates with increased privacy risks and potential legal issues, as evidenced by the concerns surrounding Recall’s handling of personal data. Conclusion: The rapid integration of AI into technology products, while intended to enhance functionality and user interaction, simultaneously amplifies privacy and security concerns, necessitating a cautious and regulated approach to AI deployment in consumer technologies. Possible Fallacies in the Argument Hasty Generalization: Concluding that all AI integrations pose privacy risks based on specific examples like Microsoft’s Recall might not account for other AI integrations that prioritize security and privacy. Slippery Slope: The argument implies that increasing AI functionalities will inevitably lead to greater privacy and legal issues, which may not necessarily hold true if appropriate measures are taken. Appeal to Fear: Highlighting the severe privacy risks and potential legal issues may play on the fears of surveillance and loss of privacy, overshadowing potential benefits of AI. Biased Sample: The argument focuses mainly on Microsoft’s implementations of AI, which may not represent the broader industry approach to AI integration and its implications. reply LoganDark 3 hours agoparent [–] Is this comment LLM-generated? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Charles Stross critiques Microsoft's introduction of CoPilot+, an AI-based add-on for Windows, comparing it to the unpopular Clippy but with more advanced, flawed AI.",
      "Stross raises privacy concerns about Microsoft's new \"Recall\" feature in Windows 11, which stores user activity in an unencrypted database, posing risks for sensitive data and sparking controversy and regulatory scrutiny.",
      "The blog discusses broader implications for privacy, security, and the tech industry's push for hardware and OS upgrades, with some users considering alternatives like Linux."
    ],
    "commentSummary": [
      "Microsoft's new \"Recall\" feature, which uses on-device OCR (Optical Character Recognition) and a searchable SQLite database, has sparked debates on whether it qualifies as AI and raised privacy and security concerns.",
      "The discussion critiques Microsoft's broader strategy, including its focus on cloud services like Azure, the declining relevance of Windows, and the company's handling of data privacy and security.",
      "Broader themes include the inefficiencies of large bureaucracies, the importance of privacy, and the ethical considerations of AI and data handling, emphasizing the need for user awareness of AI's strengths and weaknesses."
    ],
    "points": 268,
    "commentCount": 250,
    "retryCount": 0,
    "time": 1717597726
  },
  {
    "id": 40585728,
    "title": "Boeing Starliner Successfully Launches First Crewed Mission to ISS",
    "originLink": "https://www.bbc.co.uk/news/live/world-us-canada-69094213",
    "originBody": "Posted at 16:1916:19 Thanks for watching along with us Brandon Livesay US reporter Video content Video caption: Moment of liftoff for Boeing Starliner's first crewed missionMoment of liftoff for Boeing Starliner's first crewed mission The Boeing Starliner is orbiting and on its way towards the International Space Station (ISS). So, we are finishing up our live coverage of today's launch. It was the first time the Starliner spacecraft has flown with people onboard - Nasa astronauts Butch Wilmore and Suni Williams. Starliner is expected to dock at the ISS at approximately 12:15 ET (17:15 BST) on 6 June, and the astronauts will be there for about a week. So far, the spacecraft's mission has been a success. Remember, it took several years to get to this point because of various engineering issues. Moments before lift-off, Commander Wilmore called out to thank all those who had worked to make the mission possible. Alluding to past difficulties, he said: \"When the going gets tough - and it often does - the tough get going, and you have.\" Starliner's pilot Suni Williams chimed in: \"Go 'Calypso'! (the name of the capsule). Take us to space and back.\" For the full story on today's Starliner launch, check out this article by BBC's science correspondent Jon Amos. Thanks for following along with us. Article share tools ShareView more share options Share this postCopy this link Read more about these links.",
    "commentLink": "https://news.ycombinator.com/item?id=40585728",
    "commentBody": "Boeing Starliner launches first crewed mission (bbc.co.uk)261 points by helsinkiandrew 3 hours agohidepastfavorite237 comments fiftyfifty 2 hours agoIt's crazy the US could soon have up to 5 different spacecraft/launch systems that can take humans to orbit with 2 more in development: Falcon 9 + Dragon, SLS + Orion, Atlas V (Vulcan Centaur) + Starliner Close to orbital payload launch, likely human rated in the future: Vulcan Centaur + Dream Chaser, Superheavy + Starship Under development: New Glenn + Space Vehicle (?), Neutron reply supportengineer 19 minutes agoparentI just want to live long enough to see a ship, intended for long-term use, assembled in orbit. reply JKCalhoun 10 minutes agorootparentInteresting. And I'm thinking I want to see a permanent lunar base but also, \"Where does it end?\" Humans walked on the Moon in my lifetime. I should be contented with that. reply lastofthemojito 6 minutes agorootparent> Humans walked on the Moon in my lifetime. I should be contented with that. I'm apparently younger than you, and humans have not walked on the moon in my lifetime. I'm discontented by that. reply pdonis 37 minutes agoparentprevI don't think it's crazy; it's competition, which is what should be happening. We want multiple private companies to be in this game, because that's the only way access to space will ever become practical at any kind of scale. reply eastbound 30 minutes agorootparentDo we want access to space at scale? Some people are very involved in live science fiction, and we do waste enormous energy on other topics; But let’s not pretend it’s something that we should pour our resources into. It’s just leisure. “But one day my son will walk on Mars!” has a million better alternatives. reply JumpCrisscross 22 minutes agorootparent> It’s just leisure This describes almost all human activity outside agriculture. (EDIT: slash that—agriculture doesn’t make the cut either.) Shelter, medicine, education, transportation—none of this is biologically necessary by some measure. It’s difficult to look at history and not see the obvious—practically unchallenged—connection between technological ambition and quality of life. Paradoxically, if you only focus on feeding and clothing your population, you never do. reply buran77 10 minutes agorootparent> This describes almost all human activity outside agriculture. Shelter, medicine, education, transportation—none of this is biologically necessary by some measure. Of all things, why would agriculture be the one exception from \"leisure\"? On the other hand why would shelter be considered anything but a biological imperative? Shelter is essential for survival. Agriculture is the kind of advanced organized activity developed much later than foraging or hunting. reply JumpCrisscross 5 minutes agorootparent> why would agriculture be the one exception from \"leisure\" You’re right. I was thinking for purposes of survival, but nomadic tribes disprove this assertion. > On the other hand why would shelter be considered anything but a biological imperative? Shelter beyond temporary protection from the elements is not necessary for survival, as exemplified by nomadic tribes. reply pdonis 12 minutes agorootparentprev> Do we want access to space at scale? Who is \"we\"? Evidently a sufficient number of people want to to make private ventures have a reasonable expectation of profit. If you personally don't want to, then just don't get involved. But why should that stop people who do want to from doing it? > It’s just leisure. No, it's exploration with the expectation that it will lead to profitable ventures. The same thing that led people to spend months in sailing ships traveling around the world a few centuries ago. Plenty of people back then did not want to participate--but enough people did to eventually lead to the very profitable and wealth-generating system of worldwide trade we have now. reply kspacewalk2 24 minutes agorootparentprevYes, we want access to space at scale. Exploring space is no more \"leasure\" than experimental physics is a \"hobby\". reply tonynator 19 minutes agorootparentprevWhy would we not want to pour resources into ensuring the survival of the human race and learning more about the universe and the possibility of extraterrestrial life? reply kolinko 22 minutes agorootparentprevSpace has resources we could extract without damaging the environment. (Aside from the primal human need for exploration, which brought your ancestors the where you live now - unless you live in central Africa) reply octonion137 14 minutes agorootparentprevAgree, LLMs cost millions to train and we only have a few, who knows if the next LLM will be an Einstein reply RecycledEle 5 minutes agorootparentprevTackling a new frontier pays off over the long term. I understand that it can be difficult to see that far ahead. reply nordsieck 1 hour agoparentprev> likely human rated in the future: Vulcan Centaur + Dream Chaser IMO, this one is the least likely. There are a lot of problems that need to be over come for Dream Chaser to be crew rated. And AFAIK, they aren't getting NASA money to do it. reply glenstein 1 hour agorootparent>There are a lot of problems that need to be over come for Dream Chaser to be crew rated. Thanks, this is helpful to know. What do you know about the dream chaser problems? reply sho_hn 1 hour agorootparentprev> There are a lot of problems that need to be over come for Dream Chaser to be crew rated. Intriguing. Can you elaborate? reply nordsieck 1 hour agorootparentThe big one off the top of my head: Dream Chaser flies in a fairing for aerodynamic reasons. In order to fly crew (so that the vehicle could have a working launch abort system), they'd need to figure out how to fly without a fairing. reply enragedcacti 54 minutes agorootparentFrom some quick reading it seems like the crewed Dream Chaser is intended to fly without a fairing. The cargo version's ability to fold the wings and fly within a fairing seems to be for 1) compliance with NASAs CRS-2 requirements, 2) wide compatibility with existing boosters that weren't designed for the forces that flying without a fairing would create. Could be spin from Sierra but that's what they were saying to the press as of 2015 when they announced the cargo variant. https://spacenews.com/sierra-nevada-hopes-dream-chaser-finds... reply oooyay 52 minutes agorootparentprevNaive question: Planes and helicopters do not have the ability to safely eject passengers mid-flight. We largely accept these conditions as a risk of those modes of travel. Why is LES/LAS a unique requirement for space shuttles? reply nordsieck 45 minutes agorootparent> Planes and helicopters do not have the ability to safely eject passengers mid-flight. We largely accept these conditions as a risk of those modes of travel. Why is LES/LAS a unique requirement for space shuttles? That's a fair point, although my understanding is that parachute systems for small planes are becoming more common. My view is that flight rate is the fundamental issue at hand. Airplanes and helicoptors fly many orders of magnitude more than these capsules, which means we know they are many orders of magnitude more reliable. They've also generally been through a long process of refinement - the original airplanes were extremely dangerous compared to modern variants. Additionally, aircraft can afford to have a lot higher margin of safety baked in to them. Because of how high gravity is on Earth and the nature of the Rocket Equation[1], it's just not possible to have a lot of margin in rockets of capsules. They need to be extremely svelt to launch at all. And lastly, we have experience with human spacecraft without an LES/LAS - it was the Space Shuttle. And it killed 14 people - easily the most dangerous spacecraft ever created. No one has any desire to build on that particular legacy. --- 1. https://en.wikipedia.org/wiki/Tsiolkovsky_rocket_equation reply enragedcacti 36 minutes agorootparentprev1) Commercial planes and helicopters are orders of magnitude safer than space flight. https://usafacts.org/articles/is-flying-safer-than-driving/ 2) Both planes and helicopters have an ability to glide (or autogyro) to a relatively safe landing in the event of most failures. A spacecraft can also do that with wings or parachutes, but only if it gets far away from its exploding booster fast enough to survive. 3) Many military planes do have the ability to safely eject passengers. 4) astronauts dying live on stream is a really bad look. reply nativeit 32 minutes agorootparentprevPlanes and helicopters do not frequently fail by exploding, but rather things like engines failing. An engine failure, even if it’s the only engine in a given airplane or helicopter, does not automatically involve a deadly crash. Airplanes can glide, frequently for very long distances, and helicopters can use the air moving across the rotors to effectively “glide” down. It’s not always possible, but they do have inherent redundancies that rockets necessarily do not. reply WalterBright 9 minutes agorootparentPlanes and helicopters burn, though. https://www.smithsonianmag.com/smithsonian-institution/freak... Here's an attempt to develop a jet fuel that wouldn't burn: https://www.youtube.com/watch?v=Y33N0raKZBo reply eagerpace 28 minutes agorootparentprevStartship will not have an abort capability either. reply sqeaky 43 minutes agorootparentprevI think rockets in this design space have frequently been closer to prototype quality rather than commercially deployment quality. Those other systems have other redundancies and safety mechanisms. reply JoeCortopassi 44 minutes agorootparentprevplanes/helicopters have a fuel source that is orders of magnitude less volatile, and are also able to safely land without power reply SJC_Hacker 56 minutes agorootparentprevCan't they blow the fairing as part of launch abort? reply nordsieck 33 minutes agorootparent> Can't they blow the fairing as part of launch abort? I'm not a rocket scientist, so I don't really know the answer. But I don't have some questions: * How reliable will the \"blow the fairing\" system be? If it's only used in emergencies (instead of the regular fairing separation mechanism) than it'll suffer from the same problem as emergency generators - they're rarely tested and fail very often when needed. * How easy is it to get the fairing out of the way once it's opened? Normally, regular aerodynamic forces slough the fairing halves off, but an LES/LAS doesn't have time for that - it's escaping a potentially exploding rocket. And those fairings will act like sails - huge surface area:volume ration means they're just not going to move fast. * What happens if DreamChaser hits one of the fairings on the wing? Would it damage it enough that it'd have trouble landing? Is it enough to cause it to foul the escape trajectory? Or even put it in a spin? It seems like a lot of work to get it to work with or without a fairing. reply giantrobot 27 minutes agorootparentprevThat's one more thing to go wrong. In a LES scenario something(s) has gone wrong in an unrecoverable way. Very few systems can be relied upon to exist let alone work correctly. A LES on a crewed capsule is supposed to be able to pull itself from the vehicle all under its own power. It can't assume explosive fasteners on the fairing are functional or actual all function correctly. You don't want the LES to activate, seem to be working correctly, and then blast the crew into a fairing panel that did not fully separate. The crew doesn't have time to roll down the window and kick it loose. reply dotnet00 1 hour agorootparentprevIIRC one of the big ones is that of how the crew is supposed to board the vehicle. Cargo Dreamchaser is launched in a fairing so that its aerodynamics don't matter on the way up. This is fine because the cargo can be loaded prior to payload integration. But that won't work if they're carrying crew. reply JanSolo 1 hour agorootparentprevStar Liner has all the same problems that the Space Shuttle had. In an emergency, how do you get the crew out safely? reply throwawaymaths 50 minutes agorootparentPretty sure Star liner is at the top of its stack, so there is no risk of sheets of ice falling on it and damaging it reply nordsieck 1 hour agorootparentprev> Star Liner has all the same problems that the Space Shuttle had. In an emergency, how do you get the crew out safely? Starliner has a launch abort system; the Shuttle did not. From what I understand, they use a very powerful rocket (much more powerful than Crew Dragon) to get the capsule far away from the booster. I guess it can get far enough away that NASA is satisfied that falling bits of burning SRB aren't a danger to the parachutes. reply somenameforme 22 minutes agorootparentDuring the Starline abort test only 2 of the 3 parachutes opened, and that was a pad abort test - no SRBs!! NASA not only calling that a \"success\", but a sufficient success to move onto crewed testing was about the moment I lost all faith in Bridenstine being different. Immediately after leaving office he picked up a cushy consulting type gigs for various aerospace/defense companies (aka Boeing et al). Shocker. For those who might not know SRB = solid rocket booster. Boeing uses them, SpaceX doesn't. An SRB is basically like a giant firecracker. You light it and it starts burning and doesn't stop until its done. It poses substantial safety concerns in the case of an accident where you need to abort the flight. But they're cheap, extremely powerful, and relatively simple contrasted against liquid fuel engines. reply tekla 7 minutes agorootparentFailure of 1 chute was designed for, though yes, it wasn't a great look. > It poses substantial safety concerns in the case of an accident where you need to abort the flight SRBs are in general very safe, which is why they're still used for human rated rockets. lupusreal 52 minutes agorootparentprevStarliner is put on top of the rocket, not next to it. reply tekla 13 minutes agorootparentprevWhat? No it doesn't. reply CHSbeachbum420 10 minutes agoparentprevIt's the new billionaire yacht club. reply JumpCrisscross 4 minutes agorootparent> billionaire yacht club They comment on a thread about Boeing’s Starliner ferrying NASA astronauts to the ISS atop a ULA rocket. reply pbreit 16 minutes agoparentprevOnly possible with the existence of \"billionaires\". reply aquaticsunset 13 minutes agorootparentIf by that you mean \"average people pooling their billions to further advance science and technology\", sure. None of this was done in a vacuum of billionaire self funding. reply JKCalhoun 4 minutes agorootparentIs that true? We live in a remarkably gilded age where a handful of people (whose names we all know) cashed in on the Dot-Com Boom. Their pleasure appears to be, for a few of them anyway, rockets and spacecraft. A vacuum of billionaire self funding? Of course not, but would these ventures have progressed to where they are without the deep pockets of some of these billionaires? reply ragebol 2 hours agoparentprevIs neutron to be man rated? reply nordsieck 1 hour agorootparent> Is neutron to be man rated? Yes and no. It's probably more accurate to say \"human ratable\". They're planning on designing the rocket with human rating in mind, so that if they want to do it in the future, it'll be easier. But NASA doesn't human rate a rocket - they human rate the entire system as a whole, so it doesn't really make sense to say \"human rate Neutron\". reply dotnet00 1 hour agorootparentI wonder how that's supposed to work with their unique captive fairing design. Feels like they'd have to design crew flight specific boosters which don't have the fairings. reply fiftyfifty 1 hour agorootparentprevRocket Lab mentions “human spaceflight” on the Neutron page, that’s the only mention I’ve seen of it. I haven’t seen any plans for a spacecraft for carrying humans or how they might handle re-entry. https://www.rocketlabusa.com/launch/neutron/ reply hammock 1 hour agoparentprev> It's crazy the US could soon have up to 5 different spacecraft/launch systems that can take humans to orbit with 2 more in development We had a launch system that could take humans to the moon in 1972.. haven't had one since. Maybe we will get another one in our lifetime, if it is even possible. reply Zigurd 6 minutes agorootparentThis comment is an invitation to an uninformative comparison. Apollo was just barely able to take a crew to the moon and back, with many expendable stages, using 5% of US GDP to do it. Almost all the value in Apollo is indirect value in the form of technologies developed for Apollo. Why replicate that? Indeed we should ask: Is there a goal to value, other than the obvious \"the Chinese would get there first if we don't?\" A lunar \"base\" would just be a vastly more expensive ISS. We will discover that lunar regolith is a bigger nuisance than floating boogers in the ISS. reply JumpCrisscross 1 minute agorootparent> lunar \"base\" would just be a vastly more expensive ISS Source? We can design—and are designing—automation into a lunar base in a way we couldn’t with the ISS. reply JumpCrisscross 12 minutes agorootparentprev> we had a launch system that could take humans to the moon in 1972 Saturn V was ridiculously expensive [1] and incomparably unsafe. Apollo was built to get to the Moon fast half a dozen times. We’re building more ambitiously today. [1] https://en.m.wikipedia.org/wiki/Saturn_V $1.5bn in 2024 dollars reply WalterBright 6 minutes agorootparent> Saturn V was ridiculously expensive [1] and incomparably unsafe. Von Braun was asked if the Saturn V was safe to launch. He asked six of his engineering reports, each replied nein. Von Braun replied that the Saturn V had six nines of reliability. reply picture 1 hour agorootparentprevIs there much practical reason that requires sending people to the moon still? Modern robots are cheaper and can perform science more effectively than any human reply JumpCrisscross 8 minutes agorootparent> practical reason that requires sending people to the moon still? The big one that robots can’t do is studying human biology in space. How do we fare long term on a foreign body? What does trauma medicine look like? How do we accommodate the diseases and disabilities that frequent our non-astronaut grade population? Is gestation, birth and development possible in low gravity? Et cetera. Then Moon provides the easiest place to do this at scale by virtu of being the closest place to Earth with in situ resources. reply fnordpiglet 1 hour agorootparentprevI think a long term view is it’s the basis for building heavy industry in space as it has a lot of natural resources that can be exploited industrially and escape orbit velocities are much less from the moon than earth surface. This eventually leads to a general space infrastructure. If you believe the end of humanity is on earth then this probably isn’t convincing. Folks like myself believe we are inexorably driven to spread life as a function of what life is and we have no meaningful choice but to keep going. But as long as some subset of humanity believes in this humanity will keep investing in it. Not everyone has to be aligned and we can have many priorities at once, not the least of which is robotic science which I only see as mutually exclusive as long as there’s not plentiful private investment, which there is at the moment. I don’t see robotic exploration as suffering in the build out of extremely low cost launch capability and a general space infrastructure including moon infrastructure. I see it benefiting enormously as the costs and risks drop significantly. reply sho_hn 1 hour agorootparentIf we ever get to heavy industrialization of lunar resources, how are we going to deal with the CO2 footprint of rockets? reply throwawaymaths 47 minutes agorootparentA spacex starship contains ~as much fuel as a 747. Note: fuel, not fuel + oxidizer reply MarkusQ 7 minutes agorootparentSource? I get Starship 34,000,000 kg + 12,000,000 kg vs 747 ~200,000 liters ≅ 150,000 kg, or about 1/300 th of what Starship holds. p1mrx 51 minutes agorootparentprevIn theory it's possible to make a carbon-neutral methane rocket based on atmospheric CO2, though that depends on how completely the methane can be burned. https://en.wikipedia.org/wiki/Substitute_natural_gas reply SJC_Hacker 19 minutes agorootparentprevMass drivers. reply pretendscholar 1 hour agorootparentprevDistributing human populations to ensure survival. With current tech the lunar colony couldn't be self-sustaining but the ideal is that humans would be able to propagate and sustain themselves outside of Earth so that a single event couldn't end human civilization. Also creating a jobs program that will produce the technology necessary for a lunar colony will improve materials science, medical understanding, logistics. reply ivan_gammel 15 minutes agorootparentSelf-sustaining human colonies in space or on other celestial bodies are very distant dream, probably it will take several centuries or millennia to happen. The main reason is human body: we haven’t figured out reproduction in low gravity yet. Unless some fascist state will do it, we will never experiment with it until full confidence in safety for the mother and the child. reply hosh 1 hour agorootparentprevWe could also learn to live within the means of our ecology. reply pretendscholar 1 hour agorootparentThat wouldn't prevent one off extinction type events like asteroids. We can improve our understanding of ecology by trying to design such systems for lunar colony artificial biospheres. I do agree that we should better manage our impact on the only system that we know works. reply jcranmer 12 minutes agorootparent> That wouldn't prevent one off extinction type events like asteroids. We can improve our understanding of ecology by trying to design such systems for lunar colony artificial biospheres. To be kind of blunt, even an extinction-level asteroid hit with near-total biosphere destruction is probably still more conducive to human life than any other planet or satellite in the solar system, as evidenced by the continued existence of at least a few forms of life past the extinction event. And many of the events people worry about are far less destructive than even that (nuclear winter, for example, would probably roll Earth's climate back to pre-industrial temperatures, maybe as far as Little Ice Age, which is, uh, nowhere near extinction-level threat to humanity). It's also worth pointing out that it's possible to do closed ecological studies without the expense of running it in space (e.g., Biosphere 2). The only thing you need space for studying in that regard is \"what is the effect of non-1g environments on biological forms?\" (to which existing studies suggest the answer is somewhere between \"bad\" and \"horrible\"). reply jonathankoren 52 minutes agorootparentprevThis is the lamest of all excuses. It's a very unlikely for one, we haven't had an extinction asteroid in 65 million years. Detection and mapping is very good today, and they're relatively simple to deflect given even with current technology, and a long enough lead time. Obsessing about asteroid impact is just an excuse to engage in fantasy. But saying \"We can improve our understanding of ecology by [designing] artificial biosphere\", is just the chef's kiss of bullshittery. It's like saying, that we can understand the ocean by getting a fish bowl. Not exactly, and it certainly won't teach us anything about the actual biosphere. Instead, all you'd learn about is atmosphere scrubbers and water reclamation. reply protomolecule 7 minutes agorootparent>Detection and mapping is very good today No. We can't detect asteroids coming from the direction of Sun. Just ask people of Chelyabinsk, Russia. [0] [0] https://www.livescience.com/space/asteroids/the-sun-is-blind... tonynator 15 minutes agorootparentprevWe miss asteroids all the time. And we could get hit by a GRB at any time with no warning. We could get Carrington evented at any time. Global thermonuclear war could occur at any time. Don't understand this lefty obstinance against preparing for the unexpected when the negative outcome is the death of humanity. Is it because you don't like Elon? reply nordsieck 22 minutes agorootparentprev>> That wouldn't prevent one off extinction type events like asteroids. > This is the lamest of all excuses. > It's a very unlikely for one, we haven't had an extinction asteroid in 65 million years. He said \"like astroids\". Quite frankly we don't know how frequent extinction events happen. We've had nuclear weapons for less than 100 years, and have a couple of close calls[1] already. --- 1. https://en.wikipedia.org/wiki/1983_Soviet_nuclear_false_alar... reply KoftaBob 27 minutes agorootparentprev> they're relatively simple to deflect given even with current technology, and a long enough lead time. and what is this simple method to deflect a large asteroid headed for Earth? reply papercrane 14 minutes agorootparentA gravity tractor is the simplest solution with enough lead time. It's theoretical, but doesn't involve any exotic technology or materials. Essentially you have a spacecraft park itself beside an asteroid. It's gravity will minutely change the asteroids trajectory. With enough lead time that's all you need. Since you're not blowing up, or applying a large focused amount of energy to the asteroid it doesn't matter what the targets composition is. You won't break it up. https://en.wikipedia.org/wiki/Gravity_tractor sho_hn 1 hour agorootparentprevThis sounds like the harder problem. reply tialaramex 51 minutes agorootparentNo, it's merely incredibly difficult. Sustainable living off Earth is far beyond that. Humans definitely can't leave. Humans are even less well suited to interstellar travel than they are to living at the bottom of the ocean, something they also don't do and have no idea how they could ever do. So, with tremendous effort humans could visit one of their neighbouring planets. All of these planets are terrible. Mars is by far more hostile to life than anywhere humans have even visited, let alone had a permanent settlement. But we could do it. To what end? Live here, or die here, those are your options and you should get used to it. reply tonynator 11 minutes agorootparent>To what end? To have the species survive if anything ends all life on Earth - apparently not a priority for you but it is for those that enjoy humanity existing. Also to explore and learn more about the universe we live in. Do you truly not see value in that? Have you never left the city/state/country you were born in? SJC_Hacker 20 minutes agorootparentprevEstablishing a permanent presence on the moon would be a stepping stone to further exploration of other planets. (Mars in particular.) Since its only a 3-4 day trip, with transfer windows every month (and non-optimal ones essentially constantly). resupply missions and rotating astronauts/personnel are going to be much easier. Much less of a gravity well to deal with. The plan would be for in situ resource extraction and manufacturing. With enough of a human presence, projects like local construction of spacecraft become feasible. And something like a mass driver would be much more feasible on the moon. A big enough one and you're even considering interstellar probes ... It would require a consistent, sustained effort. But not astronomical in US budget terms. Maybe $20-$30 billion/year (about of 3-4% the defense budget) reply Phrodo_00 1 hour agorootparentprev> can perform science more effectively than any human I wouldn't go that far, but we already had humans on the moon so we can get away with robots doing the science now. I still think sending astronauts to mars would speed up research, for example. reply twothreeone 1 hour agorootparentprevI would imagine (a) keep watch over the lunar atomics and (b) fend off PLA officers stationed there permanently after Chang'e-42. reply mrmuagi 1 hour agorootparentprevIt looks like the RTT is 6 minutes (more or less depending on orbit) for packets send to mars, but despite that it does seem like the easier option yeah. reply shkkmo 52 minutes agorootparentNot really accurate at all. The lowest the RTT gets is six minutes but that is a brief period every couple of years. The longest RTT is 45 minutes. Even 6 minutes makes any kind of tele-operation infeasible and require system to function autonomously. This restricts the kinds of science that are currently possible. reply Teever 59 minutes agorootparentprevWhy does someone always do this in a thread about space stuff? Without fail there's always some negative Nelly who already knows the answer to their downer question. We get it. You don't care about space shit, most people don't. But why go derail a thread about a space accomplishment with negativity? Is there much practical reason for that? reply tonynator 9 minutes agorootparentI genuinely think it's because they dislike Elon and have expanded that dislike to all space exploration to rationalize a way in which he isn't basically the most important and valuable human being on Earth. reply striking 1 hour agorootparentprevBragging rights (ostensibly \"practical\" in a geopolitical, soft power projection sense) reply picture 1 hour agorootparentDidn't we get to brag about it already? The value of continuing to have the capability seems to be outweighed by the costs of maintaining the facilities and equipment reply zardo 1 hour agorootparentYou don't get much bragging rights for being first to plant a flag on the Moon once China has a continuously manned outpost there. reply hammock 1 hour agorootparentprevThe current leader of Russia doesn't believe we actually went to the moon. So, it seems the bragging rights have expired. reply protomolecule 3 minutes agorootparentNow please remember where you learned that 'fact' and at the very least stop trusting that source. At best, review all that you learned from them and discover more fakes. jlund-molfese 1 hour agorootparentprevI don’t think that’s true https://www.politifact.com/factchecks/2024/feb/16/instagram-... reply hollerith 1 hour agorootparentprevCan you provide a link? I have been unable to find any report of Putin's expressing such a disbelief. I did find a report that \"a former chief of Russian space agency Roscosmos has voiced doubt over the authenticity of the 1969 Apollo 11 Moon landing\", but of course that is not the same thing: https://www.aa.com.tr/en/europe/russia-s-former-space-chief-... reply colechristensen 1 hour agorootparentprevYes, humans living on other celestial bodies is a goal in itself, expanding us beyond Earth and a few people in LEO. Cities on the moon and Mars are a reasonable and achievable goal. There are resources which can much more easily be exploited with real people on premise, some people will want to live in different environments, there are opportunities for sport, entertainment, tourism, and plenty of industries which will be much more effective with skilled labor on site instead of meticulously planned missions which often fail and if they don't spend a whole bunch of effort overcomming the basics of operating. reply mytailorisrich 46 minutes agorootparentprevIt makes more sense to have a permanent human presence on the Moon than to aim for Mars. The Moon is both very near and very easy to communicate with so it is the perfect first place to learn about building a \"colony\" before moving on to Mars. reply cratermoon 48 minutes agorootparentprev\"can perform science more effectively than any human\" is very disputable. If they are so much better, why does anyone get up off their couch and do field research? Just let the robots do it. Besides, it's human nature to explore, in person. As George Mallory said, \"Because it's there.\" reply bunderbunder 1 hour agorootparentprevIt would probably become a lot more possible if we could get this \"SLS/Orion with frickin' Starship as a lunar lander Rube Goldberg machine\" monkey off our backs. reply sho_hn 1 hour agorootparentInteresting and very readable elaboration of this snark: https://idlewords.com/2024/5/the_lunacy_of_artemis.htm reply jonathankoren 1 hour agorootparentprevThat's literally the SLS. reply skc 4 minutes agoprevEver since I finished all 4 seasons of \"For All Mankind\" I've been eating up news like this. Truly awe inspiring stuff happening these days reply KenArrari 1 minute agoprevI hope someone double checked those doors. reply ein0p 2 hours agoprevAs exciting as this is, I’ve read that this capsule faces uncertain future after 7 launches: the rocket it was launched on is retired, and while it’s compatible with Falcon it’s not clear what the advantage would be wrt SpaceX’s capsule to warrant additional testing. Imagine working on something for over a decade only to see it fly just 7 times! reply dotnet00 2 hours agoparentIt isn't compatible with Falcon, it can be made to be compatible with Falcon in the future. It wouldn't really be worth doing though, since part of the point of having two providers is dissimilar redundancy, so that any issues with one platform don't affect the other. It's more likely that if Boeing wants to keep flying Starliner after using up the stock of Atlas Vs, they'll want to integrate it on Vulcan Centaur rather than Falcon. reply nickff 57 minutes agorootparentOne might imagine that an issue could be found on Dragon, which grounded it, but not the Falcon 9. That said, it’s definitely less redundant than one might like. reply Reubachi 13 minutes agoparentprevI have spent years building cars that ran once, twice to great effect/happiness of all involed. Getting to space SEVEN times off one poweplant/project is nothing short of incredible. reply spacemark 24 minutes agoparentprev>Imagine working on something for over a decade only to see it fly just 7 times! Haha, I'm guessing you don't work in the space industry. Frankly if something you work on gets to space at all you count yourself fortunate. My first job was at a defense contractor working on a big rocket. A senior engineer on our team had a picture of the Indiana Jones warehouse on the wall in his office, rows and rows of boxes. I asked him why, he said it's a reminder to not get too stressed about work - 9 out of 10 projects will never fly. Things are changing especially in the new space corners of the industry, but for big projects requiring political will I think it's still the same. reply nordsieck 20 minutes agorootparent> My first job was at a defense contractor working on a big rocket. Out of curiosity, were you working on one of the Ares rockets? reply asadotzler 55 minutes agoparentprev\"The capsule\" here means a decade of R&D and half a decade building production models. If turnaround is about 6-8 months, they're going to need at least two and I'll bet they build three or more because reuse and refurb won't go as well as they hope. That means about 10 years of R&D since they got the initial contracts and then about half a decade of production for the flight articles and then a wind-down of a couple years and a skeleton crew to make the last few flights. A 15-20 year project that sends dozens of people to space for the last years of the ISS's lifetime is not going to be a disappointment for 95% of the people who worked on this. reply ein0p 52 minutes agorootparentI don’t think you understand. It’s not 7 launches per capsule. There could be 7 launches in total for this. reply mbonnet 23 minutes agoparentprevHaving more than one thing capable of doing something is an advantage in a field as uncertain as spaceflight. reply glenstein 1 hour agoparentprev>Imagine working on something for over a decade only to see it fly just 7 times! Huh? My understanding was that something unique about the falcon is the capability of multiple reuses, in contrast to previous missions that were one and done uses. What past experience in the history of spaceflight might someone be referring to where seven reuses registers as a disappointment? reply asadotzler 51 minutes agorootparentThey won't reuse any Starliners 7 times. Twice is more likely. Three times reuse, perhaps. They can't refub in time to send the same craft up twice in a year as required by the contract so they'll need at least two. If anything goes wrong with either of those, they'll need a backup. Now they've got three for 6 flights. These vehicles will get one, two, or at best 3 launches and then retired to the scrap heap while SpaceX Dragon continues to ferry people to the ISS if it gets an extension and if not then to the first private orbital stations. Boeing should never again get a NASA contract after SLS and Starliner. reply ein0p 1 hour agorootparentprevIt did not launch on Falcon. Once the current stock of rockets it did launch on runs out they will have no launch vehicle. reply nordsieck 51 minutes agorootparent> Once the current stock of rockets it did launch on runs out they will have no launch vehicle. Exactly. Just to elaborate for your parent, the Atlas V which currently flies Starliner uses an RD-180 engine that's manufactured in Russia. ULA is no longer able to procure any more such engines, and a rocket with those engines are no longer legally allowed to fly DoD payloads. Which prompted ULA to retire Atlas V in favor of Vulcan. I think someone from ULA or Boeing (I forget which) recently said that they've begun the process of certifying Starliner on Vulcan, although I'll have to go back and make sure I remember exactly what was said. reply jdminhbg 1 hour agorootparentprevThis is about the entire lifespan of the Starliner program, not just one piece of hardware. reply bitcharmer 2 hours agoparentprevThe main goal of the project has been achieved as far as Boeing is concerned :) reply dave78 3 hours agoprevSounds like there's a problem with the cooling system using more water than expected. If I understood the comms correctly it sounded like they switched to a backup system to try to alleviate the issue. reply wigster 39 minutes agoprevboeing + spaceflight. they are braver than i reply cooper_ganglia 29 minutes agoparentNo one can hear you blow a whistle in space. reply wannacboatmovie 3 hours agoprevI find it amazing that this is treated as such a trivial achievement, with an attitude as if any one of us could have done this. Now back to our regularly scheduled social media apps. reply saberience 2 hours agoparentIt's a nice new capsule launching on top of a 20+ year old launch system (Atlas V). It's a great accomplishment but it's not \"super crazy\" reply malfist 2 hours agorootparentIt took them a decade and a half to make this thing. I think that alone speaks to the complexity of this achievement. reply tw04 2 hours agorootparentIt took them a decade and a half because Boeing learned the hard way that you have to actually be efficient when you don’t get a cost plus contract. Their entire system was setup to extract as much money from the government as possible, not to deliver product on time. Late and over a budget is how you maximize profit in cost plus contracts. reply saberience 2 hours agorootparentprevI'm not shitting on Starliner, it's great that we have another person-rated capsule for spaceflight. I'm just pointing this out because there are many people apparently who are confusing Starliner for Boeing's version of Starship, i.e. a whole rocket plus crew rated capsule. reply dylan604 1 hour agorootparentStarliner would be more rightly compared to Crew Dragon. Why would anyone compare to Starship? reply ericd 2 minutes agorootparentWell, \"ocean liner\" means a large oceangoing ship, \"airliner\" means large airplane, so people could be forgiven for thinking a \"starliner\" was a large spaceship and not a tiny pod. HWR_14 1 hour agorootparentprevThe names are very similar. reply wubrr 2 hours agorootparentprevLet's not forget that this is modern Boeing we're talking about... the long timeline could just be incompetence. reply elteto 2 hours agorootparentprevYes, one year for each unnecessary layer of middle management at Boeing. reply kragen 1 hour agorootparentprevit speaks to the lamentable state of boeing reply 2OEH8eoCRo0 2 hours agorootparentprevHow long did it take Spacex to develop their human rated capsule? I think that was 10+ years as well reply Retric 2 hours agorootparentDepends on what you consider starting. 16 years is probably the most reasonable number, but you could argue for as little as 6. Initial work on Dragon began in 2004, it ‘entered service’ in 2009, had its first mission in 2010, but first connected to the ISS in 2012. https://en.wikipedia.org/wiki/SpaceX_Dragon Work on a crewed version was officially mentioned in 2006 though they only got a contract for manned missions in 2014 and the first manned mission was 16 November 2020. https://en.wikipedia.org/wiki/SpaceX_Crew-1 reply dotnet00 2 hours agorootparentprevIIRC it took SpaceX ~7 years. It's kind of useful perspective that when the contracts for this were being awarded, Boeing argued that SpaceX shouldn't get the contract at all because Boeing, having \"human spaceflight heritage\", was guaranteed to do the better job than an inexperienced upstart. Plus the extra $400M they extorted out of NASA despite this being a fixed price, milestone based contract. reply nordsieck 1 hour agorootparent> when the contracts for this were being awarded, Boeing argued that SpaceX shouldn't get the contract at all because Boeing, having \"human spaceflight heritage\", was guaranteed to do the better job than an inexperienced upstart. I think it's useful to note that this wasn't just Boeing's opinion - it was pretty widely believed in the industry. And not without reason - Boeing had Shuttle heritage. Thankfully, NASA kept both awards. reply perihelions 1 hour agorootparentprevhttps://news.ycombinator.com/item?id=17509988 (\"Internally, NASA believes Boeing ahead of SpaceX in commercial crew\" (2018)) reply pfdietz 47 minutes agorootparentIn retrospect those defending Boeing there and attacking SpaceX (and Eric Berger's reporting) are just hilarious. reply boxed 1 hour agorootparentprevThey got the contracts at the same time, and Boeing has been building rockets since the 60s... reply seydor 2 hours agoparentprevEvery one of the thousands of brain surgeries and heart surgery are also remarkable. reply pfdietz 3 hours agoparentprevNot trivial, but also not consequential. reply amelius 2 hours agoparentprevAt some point we'll have to think of it as trivial, otherwise what progress are we making? reply falcor84 2 hours agorootparentUnlike science (and particularly math) where everything is trivial unless novel, in most endeavors these are too separate axes. For example, there's no progress or novelty in a sports team winning a championship, but it's definitely not trivial to win. Same for an engineering project - there are many cathedrals out there, but building a new one never became trivial. reply amelius 1 hour agorootparentLet's say: if you can just open the manual and start building, then it is trivial. We've built many rockets, there are numerous resources about building one, so building rockets is trivial. reply nordsieck 1 hour agorootparent> Let's say: if you can just open the manual and start building, then it is trivial. > We've built many rockets, there are numerous resources about building one, so building rockets is trivial. Except that you're wrong. Because it's very common for the first launch of a company's first orbital rocket to fail to make it to orbit. So you can't \"just open the manual and start building\". reply bluGill 1 hour agorootparentprevBuilding a new Falcon 9 is trivial - spacex as built a lot already and knows how (or so we assume). However that is only true if you use the existing design as is. Change anything about the design (which we can assume spacex is doing from time to time) makes it non-trivial. reply amelius 1 hour agorootparentOnly if you threw away all the tooling and knowledge of the previous design and started from scratch. reply idontwantthis 2 hours agoparentprevI think it is materially less exciting than it would have been if it had launched years ago when it was scheduled to. It provides competition with SpaceX in one very small niche of space travel with no applications to any other niche. Meanwhile SpaceX is building a Mars rocket with in flight refueling. I really wish they did have more competition, and I also hope they succeed. reply FireBeyond 2 hours agorootparentI think one of the challenges (and let's be really clear, Boeing has MANY issues) is that there's a double standard (or at least different expectations). How many SpaceX rockets and failures have there been? (And that's not a knock on SpaceX, either - this stuff is hard. Combining precision and technology with 'controlled explosion' is going to be a challenge). But as a NASA person said - NASA-funded contracts \"can't\" have failures. They obviously do, but he was more talking to the acceptability, political and otherwise. One or two launchpad explosions of a taxpayer funded vehicle and you're fighting Congressional demands to shut down the entire program. SpaceX provides a layer of abstraction and indirection to that, so they can move faster - \"Who cares if we blow up 10 in the next couple of years to get to one that works\". reply somenameforme 1 hour agorootparentThe Falcon 9 is, by a wide margin, the most reliable rocket ever built. It's had 341 successful launches and 2 failures. The Atlas V (what is flying on this mission) has had 99 successes and 1 failure. It's also slightly misleading, because its first stage is using a Russian made RD-180 engine. And similarly the SLS (another Boeing et al project) is literally using the exact same engines (RS-25) that the Space Shuttle used. So SpaceX is the only company truly innovating on all fields, has the highest launch success rate, highest launch cadence, the most capable rockets, and launches for far cheaper than any other company (or country). reply nordsieck 1 hour agorootparentprev> I think one of the challenges (and let's be really clear, Boeing has MANY issues) is that there's a double standard (or at least different expectations). > How many SpaceX rockets and failures have there been? (And that's not a knock on SpaceX, either - this stuff is hard. Combining precision and technology with 'controlled explosion' is going to be a challenge). IMO, this really misunderstands the two kinds of \"tests\". SpaceX is engaged in a development program. And as a part of that development program, they're doing test flights to discover how to properly build Starship. Those flights are expected to fail in various ways. The exact way they fail gives SpaceX vital information that's used to improve the rocket. A big part of the reason SpaceX is doing this is because simulation and modeling have a limited ability to give good answers to questions about novel behaviors when it comes to rockets - the speeds and just too high. And the only way to find the true unknown unknowns is to interact with reality. In contrast, Starliner's tests are supposed to be demonstrations that the system is complete, functional, and ready for service. They are not supposed to have anything wrong with them at all. It's worth pointing out that Boeing chose to do less testing and more paperwork as part of Starliner's certification. If Boeing had done an in-flight abort test instead of a pad abort test like SpaceX did, they probably would have caught the OFT-1 problems then. reply caconym_ 1 hour agorootparentprevI think you're conflating the way SpaceX is developing Starship with the way the rest of their business operates (and has operated). Their Falcon rockets (i.e. the ones they actually sell launches on) have an outstanding reliability record, and the Dragon 2 development program (the direct analogue to Starliner) didn't lose any test missions. IIRC the only major hardware loss was during a static fire test of the abort motors on the capsule, which is unfortunate, but not so far out of the ordinary. reply FireBeyond 1 hour agorootparentIt's not about the specific program, it's about the overall perspective. Looking at https://www.space.com/every-spacex-starship-explosion-lesson... there have been many many prototype and other losses. And incidents, some catastrophic, some less so. SN1, 3, 4, 7, 8, 9, 10, 11, 15 and the orbital Starship launch attempt all had failures losing hardware. If NASA/publicly funded work had that many failures (or a fraction of them) there'd be Congressional enquiries and calls to shut down the program and stop burning tax dollars. reply caconym_ 1 hour agorootparentYou are simply underlining my point that your perspective is disproportionately (and inappropriately, in this context) focused on the Starship program, which is completely irrelevant to NASA's Commercial Crew program. It's true that SpaceX enjoys more latitude to destroy test hardware in its private development programs that aren't funded with somebody else's money (public or private), but why is that relevant here? Commercial Crew was funded by NASA with public money, and SpaceX developed Dragon 2 in a relatively conservative and conventional program with NASA looking over their shoulder the whole time. There is no double standard. reply Aaargh20318 1 hour agorootparentprevDifferent design philosophy. Those launches were expected to fail. None of those were a finished product. It’s more like “let’s see how far we can get with what we have built so far”. reply shkkmo 32 minutes agorootparentprevWhat are you even talking about? NASA has directly publicly funded Starship development to the tune of ~4 Billion with the Artemis Moonlander contract and extension. The overall perspective is that SpaceX developed their crewed capsule much much faster and cheaper than Boeing. The data also indicates that flying with SpaceX is safer. Congress doesn't care about buring tax dollars as long as it is spent in their districts. Otherwise Artemis and SLS wouldn't exist. reply ragebol 2 hours agorootparentprevBoeing and SpaceX are both not NASA, so same level of indirection. If Boeing went the iterative route with some failed experimental launches, that could/should be just as acceptable. But they didn't, they went for the first time right approach, but that failed too. If you are going to have failures, maybe just accept that first time right doesn't exist, or just takes much much longer. reply theultdev 2 hours agorootparentprevWell, that's how you build rockets successfully. Either you make fast iteration acceptable, regardless of politics, or you fail. It's not just political process either, it's the technical process. You need to be able to debug, fix, and manufacture the iterations quickly. reply FireBeyond 1 hour agorootparentNo, I totally agree. I'm talking about the mindset difference. I'm not saying \"SpaceX is 'cheating'\" or anything like that. Just the mindset differences are leading to what we see here in terms of iteration cadence. reply amelius 1 hour agoparentprevRocket-science is just Newtonian physics ;) reply urda 1 hour agoparentprevRocket science isn't easy, I would know. reply adolph 2 hours agoparentprev> Now back to our regularly scheduled social media apps. \"Ask not what flying cars can do for you; ask what 140 characters can do for your country.\" [0,1] \"We choose to go to LEO. We choose to go to LEO... We choose to go to LEO in this decade and do the other things, not because they are easy, but so that MIC will learn to build without cost-plus contracting\" [2, 3] 0. https://www.jfklibrary.org/learn/education/teachers/curricul... 1. https://www.goodreads.com/quotes/697729-we-wanted-flying-car... 2. https://en.wikipedia.org/wiki/We_choose_to_go_to_the_Moon 3. https://www.investopedia.com/terms/c/cost-plus-contract.asp reply Cacti 3 hours agoparentprevHN is too busy in other threads pontificating about a merger from 30 years ago that they know next to nothing about. reply rainyMammoth 2 hours agoparentprevYeah for some reason when SpaceX did it we couldn’t stop hearing about it. When OldTech does it, nobody cares. reply chgs 1 hour agorootparentIf old tech had done it 5 years ago then that would have been newsworthy. The first jet flight across the Atlantic was newsworthy. The 837th isn’t. reply freeopinion 1 hour agorootparentprevQuick, without cheating, can you name the second human being to run a mile in less than four minutes? Can you name the current world record holder? I guess that most people in my small town don't know who Roger Bannister is. A lot more of them can tell you the name of the first local to officially run a mile in less than four minutes. They couldn't tell you if anybody from my state has done it since. I guess that's just a long way to say, \"That's natural.\" reply dang 1 hour agoprevRelated. Others? Boeing and NASA call off Starliner crew launch minutes before liftoff - https://news.ycombinator.com/item?id=40547338 - June 2024 (47 comments) Boeing's Starliner Crew Flight Test delayed again, path forward unclear - https://news.ycombinator.com/item?id=40434398 - May 2024 (28 comments) Boeing Starliner's first crewed mission scrubbed - https://news.ycombinator.com/item?id=40281272 - May 2024 (162 comments) NASA and Boeing Are (Finally) Putting Astronauts on Starliner - https://news.ycombinator.com/item?id=39843148 - March 2024 (9 comments) Boeing has now lost $1.1B on Starliner, with no crew flight in sight - https://news.ycombinator.com/item?id=36879769 - July 2023 (218 comments) NASA safety panel skeptical of Starliner readiness for crewed flight - https://news.ycombinator.com/item?id=36085531 - May 2023 (27 comments) Boeing to ground Starliner indefinitely until valve issue solved - https://news.ycombinator.com/item?id=28185195 - Aug 2021 (42 comments) Boeing Starliner's flight's flaws show “fundamental problem,” NASA says - https://news.ycombinator.com/item?id=22297564 - Feb 2020 (140 comments) NASA Shares Initial Findings from Boeing Starliner Orbital Test Investigation - https://news.ycombinator.com/item?id=22266747 - Feb 2020 (14 comments) Starliner faced “catastrophic” failure before software bug found - https://news.ycombinator.com/item?id=22260731 - Feb 2020 (60 comments) Boeing reports a $410M charge in case NASA decides Starliner needs another test - https://news.ycombinator.com/item?id=22194735 - Jan 2020 (55 comments) Boeing Starliner updates: Spacecraft flies into wrong orbit, jeopardizing test - https://news.ycombinator.com/item?id=21843988 - Dec 2019 (240 comments) New Spacesuit Unveiled for Starliner Astronauts - https://news.ycombinator.com/item?id=13488096 - Jan 2017 (65 comments) Boeing-SpaceX Team Split Space Taxi Award - https://news.ycombinator.com/item?id=8326845 - Sept 2014 (115 comments) NASA to Make Major Announcement Today About Astronaut Transport to the ISS - https://news.ycombinator.com/item?id=8324848 - Sept 2014 (43 comments) SpaceX Vies With Boeing as NASA’s Taxi to Station - https://news.ycombinator.com/item?id=8296567 - Sept 2014 (49 comments) reply rootusrootus 3 hours agoprevInteresting that this did not make HN at all before the launch. There are way more cheerleaders for SpaceX than Boeing. But IMO it's still very cool, I could watch rocket launches all day. reply dotnet00 2 hours agoparentIt's been delayed so many times after getting very close to launch that it makes sense that everyone just lost interest. On top of that Boeing's launch coverage is nowhere near as fancy as SpaceX (or new space in general, RocketLab and Blue Origin also tend to have pretty decent coverage, although neither of them are doing crewed orbital spaceflight yet). No views of the non-flight-control employees enjoying seeing their work fly, very little live telemetry, low resolution for when they do have live video, mostly CGI views once down to the second stage, no live views from the capsule in space either. Finally, on top of that, Starliner is kind of just a dead end in its current state. Boeing only built the two it needs for this one contract, and it only flies on Atlas V, which are fully sold out now. So it can only do the 6 contracted ISS missions and then it's done until someone is willing to pay to have Starliner+Vulcan Centaur crew rated. reply dylan604 1 hour agorootparentI was really unimpressed with Boeing's feed and its lack of telemetry. No indications of altitude, speed, distance down range, etc. Even their timeline jumped rather than progressed. It was worse than some of those old Windows progress bars of old. reply robertlagrant 2 hours agorootparentprev> Boeing only built the two it needs for this one contract, and it only flies on Atlas V, which are fully sold out now. So it can only do the 6 contracted ISS missions and then it's done until someone is willing to pay to have Starliner+Vulcan Centaur crew rated. This is what I like about SpaceX. Of course they have government contracts, but it's always building towards something bigger. reply luuurker 3 hours agoparentprevHow many times was this launch delayed due to problems? I get excited, but can't stay excited for months. reply HWR_14 2 hours agorootparentIt was delayed three times in the month leading up to launch, and there was an attempt just over a year ago that was scratched because they discovered they accidentally used a flammable material. So, four total times. But \"staying excited for months\" is misleading. reply starik36 2 hours agorootparentTrue, but you have to add previous attempts going back several years. Wasn't Atlas+Starliner actually rolled out to the pad last time only to be scrubbed and brought back for another 6 month delay? reply dylan604 1 hour agorootparentYes, this GP's comment is very misleading in that it only listed the recent scrubs. I almost felt sorry for the astronauts. reply HWR_14 1 hour agorootparentI listed all the ones I could find for the manned mission. Did my list miss some? reply dylan604 53 minutes agorootparentyou definitely missed some... \"Although it was originally planned for a 2017 launch,[15] various delays pushed the launch back to no earlier than July 2023.[47] Then on June 1, 2023, Boeing announced the flight was indefinitely delayed, due to problems with the parachute harness and flammable tape on wiring.[105] On August 7, 2023, Boeing announced that it was resuming preparations for a launch, and that it hoped to resolve the issue with the flammable tape by September 2023, and to address the parachute harness issues by November 2023. The Crewed Flight Test was tentatively scheduled for a launch date of May 6, 2024,[106] but due to a problem with an oxygen valve on the ULA Atlas rocket, the May 6 launch date was cancelled approximately two hours before the planned launch time.[107] The launch has been further delayed due to a helium leak in the Starliner service module, which was originally discovered during the May 6 launch attempt.[108][109] A launch was attempted on June 1, 2024, for 16:25 UTC (12:25 PM EDT), but was aborted at 3 minutes and 50 seconds prior to liftoff. Starliner successfully completed countdown and lifted off on June 5, 2024 at 14:52 UTC (10:52 AM EDT).\" https://en.wikipedia.org/wiki/Boeing_Starliner#Third_orbital... reply starik36 55 minutes agorootparentprevYou are missing ones where it was scheduled, but didn't even make it to the stand because some issue was found. I can't recall when exactly, but at some point it was discovered that the tape used to wrap wires was flammable. So it was postponed, once again. reply idontwantthis 2 hours agorootparentprevThe time more than a year before that when it was scrubbed for a stuck valve that I don’t think they ever fixed and decided it can still fly. reply xnx 3 hours agoparentprev4 days ago. 48 comments: https://news.ycombinator.com/item?id=40547338 Also 30 days ago. reply dmix 3 hours agoparentprevEveryone was excited for the one last time and I was around for that, I didn't even realize it was happening today. I just assumed it'd be delayed for another long period again. reply rootusrootus 1 hour agorootparent> I didn't even realize it was happening today. I did not, either. I tend to get most of my tech-related news via HN. Fortunately this morning I decided to swing past Ars to see if anything interesting was happening, and they had a high profile post about the launch. I made it to the livestream with less than five minutes to spare. reply malfist 3 hours agorootparentprevI had the same issue. I thought it was postponed for a longer period given the messaging from the last scrub. I only knew it was launching 6 minutes prior because my partner alerted me about it because it was top stream on twitch. reply BurningFrog 3 hours agoparentprevTo be fair, SpaceX is far more cheerworthy. Does the Starliner have any feature that current SpaceX rockets don't have? reply imglorp 2 hours agorootparentYes, it can reboost the station. Dragon can't. Cygnus, Dreamchaser, and Soyuz can reboost. This matters because the station can't boost itself. reply HWR_14 2 hours agorootparentprevIt has physical knobs and switches as opposed to relying on touchscreens. I consider that a feature, although a minor one. reply dotnet00 2 hours agorootparentIt is worth noting that Dragon does have physical controls, they're just backups hidden under the panel below the screens for emergencies. This is on top of the redundancy offered by the screens, where if one screen fails, the same controls are accessible on the other ones. Plus, since it's supposed to fly autonomously, there isn't a lot of physical control to be done. This isn't like with cars where there's an argument that tactile controls are easier to adjust without looking away from the road. reply ta1243 2 hours agorootparentprevAre you an astronaut? Do astronauts prefer thousands of physical knobs? Do you think they would fly if they weren't happy? reply HWR_14 1 hour agorootparentI am not an astronaut. I prefer knobs, as my comment said, I think of them as a feature. I think the average astronaut would put up with their lack of preferred control schemes to go into space. reply saberience 2 hours agorootparentprevStarliner isn't even a rocket, it's a capsule. A capsule (Starliner) got launched today on top of a really old rocket design (Atlas V) which first launched in 2002... reply jimbobthrowawy 41 minutes agorootparentprevThey also have better produced livestreams in general, making it worth checking in in advance. reply MPSimmons 3 hours agorootparentprevNot really. The biggest apparent difference in the user experience is how the vehicle is commanded. Here's a pic of the Starliner control panel: https://x.com/TrevorMahlmann/status/1207437431374565376/phot... And Crew Dragon's control panel: https://www.theverge.com/2020/5/30/21275753/nasa-spacex-astr... reply greenavocado 2 hours agorootparentI shudder thinking about what would happen if a touch action on the touchscreen would get stuck reply Maxatar 2 hours agorootparentThe Dragon Crew has physical backup controls. The touchscreen allows for a more interactive UI that you can try out here: https://iss-sim.spacex.com/ But the controls themselves have physical buttons in addition to the touch screen. Also all systems have triple redundancy. reply bumby 2 hours agorootparentprevThere was a decent amount of concern within NASA on the touchscreen design, but the contract type tended to force those discussions to the sideline. In the end, NASA wanted a ride and not to drive the design. Edit: for those wondering, this is not hearsay or speculation; it is from direct experience (albeit 5+ years ago) reply starik36 2 hours agorootparentprevIt does. It can (and will) land on land instead of water. reply coolspot 2 hours agorootparentLike Soyuz? Interesting, why would they want that capability? All Russian space vehicles land on land because they don’t have easy access to warm waters and Kazakhstan steppe is big and empty. But why boeing/nasa would want that? reply dotnet00 2 hours agorootparentTradeoffs on the kind of refurbishment needed compared to a splashdown, since Starliner is supposed to be reusable. Plus stuff like faster extraction of time-sensitive payloads and overall cheaper capsule processing operations since you don't need specially fitted boats chasing after the capsule. Dragon was also initially intending to land on solid ground, but dropped the idea when NASA asked for additional tests to prove that popping landing legs out of the heat shield would be safe. SpaceX had intended such landings in large part because of the plans for Red Dragon, but since by then they had started to shift towards Starship, they deemed it easier to just splashdown and deal with the extra refurbishment than try to prove out a technology they no longer felt the need for. reply simplicio 2 hours agorootparentprevThink the main reason is that sea-recoveries are expensive compared to ones on land. I imagine there's at least some extra risk to a sea recovery as well (one of the Mercury capsules sank during recovery, though happily not with its astronaut inside). reply GMoromisato 2 hours agorootparentprevLanding in water is bad for equipment--the salt water tends to corrode, so refurbishing the capsule after a water landing is a bit harder. reply dave78 2 hours agorootparentprevIs that a feature or just a difference? I assume there's trade-offs with both - is landing on land significantly better? reply starik36 1 hour agorootparentIt's faster and more efficient, I think. You don't need a fleet of ships to go out to the sea. reply dave78 26 minutes agorootparentInteresting to think about. I know Starliner lands in Utah. I don't know where, but I'm guessing it's somewhere very remote. I wonder if the effort to get out to the ocean to recover a ship is significantly different than getting to a remote part of the desert to recover. Additionally, I know when the first Crew Dragon landed, it clearly wasn't hard or expensive to get to given that there were a bunch of small, private boats that (inappropriately) approached the spacecraft. It was quite close to shore, not like the old Apollo missions landing in the middle of the Atlantic or Pacific. reply SideburnsOfDoom 3 hours agoparentprevPrevious launch attempts did. But there are only so many successive \"launch was scrubbed\" stories that are interesting. reply bryanlarsen 3 hours agoparentprevIt made it before at least one of its other launch attempts, IIRC. reply ceejayoz 2 hours agorootparentUncrewed, yes. https://en.wikipedia.org/wiki/Boeing_Orbital_Flight_Test_2 reply FireBeyond 2 hours agoparentprevThere was an article when the launch was scrubbed two days ago, but it didn't make much traction, and the few comments on it were (as objectively as possible) mostly \"smirking SpaceX fans\". reply dylan604 1 hour agorootparentObjectively dissing on Boeing has become it's own pastime regardless of being a fan of anything else reply protastus 1 hour agorootparentThere is no schadenfreude for the people in my circle. The same can be said for Intel. We all see it as a huge national security issue that these companies are fumbling, given how foundational they are for U.S. security, self-reliance and tech leadership. reply api 3 hours agoparentprevThis rocket is not reusable so it’s kind of an antique. reply hydrogen7800 3 hours agorootparentThe rocket is not, but the crew module is intended to fly 10 times, and is compatible with several rockets including falcon 9. reply dotnet00 2 hours agorootparentIt's 'compatible' in that they can do the work to integrate it with another rocket if needed, which isn't really saying much because most payloads are like that. It isn't compatible with Falcon 9 in its current state, and IIRC because it's wider than F9, actually flying Starliner on F9 would require a lot more structural work too (devising an appropriate aerodynamic adapter and ensuring structural loads are acceptable). Plus, NASA crew rates the full stack rather than treating the rocket and capsule separately, so integrating Starliner on another rocket would require the crew rating process to be repeated (granted, it'd be a bit easier since F9+Dragon is already crew rated). reply cma 2 hours agorootparentprevThe falcon with dragon is only partially reusable too (second stage discarded). The space shuttle reused more but is more of an antique so that's not a great determiner of antiqueness. reply tekla 2 hours agorootparentSo all the expensive parts. Got it reply 3 hours agoprevnext [5 more] [dead] perihelions 1 hour agoparentNo, for instance the RD-180 engine is from NPO Energomash' facility in Khimki, in Moscow Oblast. Of the things to worry about with the RD-180, Boeing QA is not one of them. reply malfist 2 hours agoparentprevBoeing's airplane and space segments are about as separate as you can get within the same company. reply tekla 3 hours agoparentprevConsidering the rocket is ULA I would venture that your \"knowing\" is wrong reply axus 2 hours agorootparentStarliner is the part in orbit (made by Boeing), and Atlas-V is the rocket (made by ULA, originally designed by LM). If the passengers were worried about Boeing, it's the orbit and return they should be afraid of. reply tibbydudeza 2 hours agoprevWhat a relief for NASA and Boeing and a welcome sight for me personally as a space enthusiast - hopefully this will inspire the folks at SpaceX to get StarShip working. reply saberience 2 hours agoparentStarliner vs Starship isn't really a valid comparison. Starliner is a new human carrying capsule which is fitted on top of an old rocket. Starship is a 100% brand new everything rocket and person carrying spaceship with ground breaking tech, lift capacity, full reusability, thrust, payload capacity, etc etc. The rocket used to launch Starliner today is an Atlas V which first flew in 2002. I.e. it's a 22 year old rocket system. reply dragonwriter 2 hours agorootparent> The rocket used to launch Starliner today is an Atlas V which first flew in 2002. I.e. it's a 24 year old rocket system. Are you posting from 2026? reply tibbydudeza 1 hour agorootparentprevBut with continuous development with Vulcan Centaur engines replacing the Russian RD-180. reply T-A 1 hour agorootparentAtlas V https://en.wikipedia.org/wiki/Atlas_V has a first stage powered by the Russian RD-180 engine https://en.wikipedia.org/wiki/RD-180 and a Centaur upper stage powered by RL10 engines https://en.wikipedia.org/wiki/RL10 Vulcan Centaur https://en.wikipedia.org/wiki/Vulcan_Centaur has a first stage powered by Blue Origin's BE-4 engines https://en.wikipedia.org/wiki/BE-4 and a second stage known as Centaur V. It's an upgraded version of the Centaur, also powered by RL10 engines https://en.wikipedia.org/wiki/Centaur_(rocket_stage)#Centaur... Vulcan has only flown once, in part because of slow delivery of BE-4 engines, which to date have not powered any other launcher (though they are meant to eventually power New Glenn). So I don't know what you mean by \"Vulcan Centaur engines replacing the Russian RD-180\". reply ta1243 2 hours agoparentprevStarliner is the Boeing equivalent of Falcon/Crew Dragon which has been used for years. reply Narishma 1 hour agoparentprevStarliner's competitor from SpaceX is Dragon, not Starship. reply croddin 2 hours agoparentprevStarship launch attempt is tomorrow, great week for space! reply ozr 2 hours agoparentprevHave they not been though? reply nytesky 3 hours agoprevGallows humor on the livestream chat: bet NASA will get another movie out of this. reply wood_spirit 2 hours agoprevThis is a bit meta, but is it surprising that the BBC news website is the go-to source for a broad range of news stories that end up on HN? What and where are the competition? reply seydor 2 hours agoparentIt's politically neutral and generally less opinionated. Competition is reuters, AP, other public media such as canadian tv, france24 etc. reply closewith 1 hour agorootparentThe BBC is not politically neutral. It's the legal propaganda arm of the British State. reply imabotbeep2937 1 hour agorootparentTo take a stance much more suited to this forum. It's a massive travesty that without some work Americans are now redirected to BBC.com, which curates news to be American-facing, and thus IMHO bows to advertiser pressure. We can't see BBC.co.UK, so we can't know what the other side of the news even looks like. All news services do this. And it already fragments and destroys and hope of really talking about the news in a healthy manner IMHO. reply knowaveragejoe 1 hour agorootparentprevCool it with the histrionics. reply closewith 1 hour agorootparentIt's not histrionics. It's the State-funded broadcaster whose Director-General is a political appointee. It might not be obvious to those within the UK or maybe further afield if reading articles on relatively neutral topics like science or climate, but the BBC is a bulwark of UK Government political influence and Oxbridge sensibilities. reply insane_dreamer 1 hour agorootparentprevAFP as well reply jibe 2 hours agorootparentprevThere are plenty of reasonable US sources, NY Times, WaPo, WSJ, but they are all paywalled. reply imabotbeep2937 1 hour agorootparentWaPo, \"reasonable\". Ha. Or cut to WSJ and NYT on Theranos, SBF, and the other huxters they've shilled for with zero journalism lately. All US news is completely biased garbage in various ways. I caught NPR recently cutting and pasting a white house press release with zero journalism or commentary. Or their health blog which now routinely has stories like \"measurements of height have historically been uses to marginalize short people, should doctors even measure it anymore?\" I literally only get my news from what I randomly hear off forums like this. And I guarantee I'll beat anyone on a quiz of actual facts about world news. (e.g. after SBF was convicted, not before.) reply knowaveragejoe 1 hour agorootparent> And I guarantee I'll beat anyone on a quiz of actual facts about world news. (e.g. after SBF was convicted, not before.) Put your money where your mouth is - prediction markets or some other fair debate. I personally would take the other side of that bet if contrarianism is your default. reply airstrike 12 minutes agoparentprevBloomberg and WSJ (but not the opinion pieces) are my default sources reply WhackyIdeas 14 minutes agoprev [–] Is this the same Boeing that makes all of those missiles landing in Gazan tent camps? reply dinglestepup 9 minutes agoparent [–] No. It's the Boeing that Hamas terrorists use to fly first-class to Qatar while pretending to be poor freedom fighters. reply WhackyIdeas 6 minutes agorootparent [–] So they are different companies? reply dinglestepup 3 minutes agorootparent [–] I guess so. I don't know what company you are referring to. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The Boeing Starliner successfully launched its first crewed mission, carrying NASA astronauts Butch Wilmore and Suni Williams to the International Space Station (ISS).",
      "The spacecraft is scheduled to dock at the ISS on 6 June at 12:15 ET, marking a significant milestone after years of engineering challenges.",
      "Astronauts Wilmore and Williams will stay at the ISS for about a week, expressing gratitude and enthusiasm for the mission."
    ],
    "commentSummary": [
      "Boeing's Starliner successfully launched its first crewed mission, marking a significant milestone in U.S. space exploration.",
      "The discussion includes the role of competition in space access, the importance of Launch Escape Systems (LES) for safety, and the impact of private funding on space technology.",
      "The conversation contrasts Boeing's Starliner with SpaceX's Crew Dragon, noting Boeing's delays and higher costs, while praising SpaceX's iterative, failure-tolerant approach for accelerating innovation."
    ],
    "points": 261,
    "commentCount": 239,
    "retryCount": 0,
    "time": 1717599843
  },
  {
    "id": 40585039,
    "title": "State-of-the-Art LLMs Fail Simple Common-Sense Tasks, Study Reveals",
    "originLink": "https://arxiv.org/abs/2406.02061",
    "originBody": "Computer Science > Machine Learning arXiv:2406.02061 (cs) [Submitted on 4 Jun 2024] Title:Alice in Wonderland: Simple Tasks Showing Complete Reasoning Breakdown in State-Of-the-Art Large Language Models Authors:Marianna Nezhurina, Lucia Cipolina-Kun, Mehdi Cherti, Jenia Jitsev View PDF HTML (experimental) Abstract:Large Language Models (LLMs) are often described as being instances of foundation models - that is, models that transfer strongly across various tasks and conditions in few-show or zero-shot manner, while exhibiting scaling laws that predict function improvement when increasing the pre-training scale. These claims of excelling in different functions and tasks rely on measurements taken across various sets of standardized benchmarks showing high scores for such models. We demonstrate here a dramatic breakdown of function and reasoning capabilities of state-of-the-art models trained at the largest available scales which claim strong function, using a simple, short, conventional common sense problem formulated in concise natural language, easily solvable by humans. The breakdown is dramatic, as models also express strong overconfidence in their wrong solutions, while providing often non-sensical \"reasoning\"-like explanations akin to confabulations to justify and backup the validity of their clearly failed responses, making them sound plausible. Various standard interventions in an attempt to get the right solution, like various type of enhanced prompting, or urging the models to reconsider the wrong solutions again by multi step re-evaluation, fail. We take these initial observations to the scientific and technological community to stimulate urgent re-assessment of the claimed capabilities of current generation of LLMs, Such re-assessment also requires common action to create standardized benchmarks that would allow proper detection of such basic reasoning deficits that obviously manage to remain undiscovered by current state-of-the-art evaluation procedures and benchmarks. Code for reproducing experiments in the paper and raw experiments data can be found at this https URL Comments: v1 Subjects: Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computation and Language (cs.CL) Cite as: arXiv:2406.02061 [cs.LG](or arXiv:2406.02061v1 [cs.LG] for this version)https://doi.org/10.48550/arXiv.2406.02061 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Jenia Jitsev [view email] [v1] Tue, 4 Jun 2024 07:43:33 UTC (3,298 KB) Full-text links: Access Paper: View PDF HTML (experimental) TeX Source Other Formats view license Current browse context: cs.LGnewrecent2024-06 Change to browse by: cs cs.AI cs.CL References & Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer (What is the Explorer?) Litmaps Toggle Litmaps (What is Litmaps?) scite.ai Toggle scite Smart Citations (What are Smart Citations?) Code, Data, Media Code, Data and Media Associated with this Article Links to Code Toggle CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub Toggle DagsHub (What is DagsHub?) GotitPub Toggle Gotit.pub (What is GotitPub?) Links to Code Toggle Papers with Code (What is Papers with Code?) ScienceCast Toggle ScienceCast (What is ScienceCast?) Demos Demos Replicate Toggle Replicate (What is Replicate?) Spaces Toggle Hugging Face Spaces (What is Spaces?) Spaces Toggle TXYZ.AI (What is TXYZ.AI?) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower (What are Influence Flowers?) Connected Papers Toggle Connected Papers (What is Connected Papers?) Core recommender toggle CORE Recommender (What is CORE?) IArxiv recommender toggle IArxiv Recommender (What is IArxiv?) About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs. Which authors of this paper are endorsers?Disable MathJax (What is MathJax?)",
    "commentLink": "https://news.ycombinator.com/item?id=40585039",
    "commentBody": "Simple tasks showing reasoning breakdown in state-of-the-art LLMs (arxiv.org)231 points by tosh 4 hours agohidepastfavorite252 comments nerdjon 4 hours agoFor anyone considering reading the paper and like me don't normally read papers like this, open the PDF and think you don't have time to read it due to its length. The main part of the paper is the first 10 pages and a fairly quick read. On to the topic here. This is an interesting example that they are using. It is fairly simplistic to understand as a human (even if we may be inclined to quickly jump to the wrong conclusion without thinking for a few seconds). The thing that really bothers me is that I just don't know is realistically we can fix this given the current state of what these tools actually are. They are not reasoning or thinking in any sense of the word and yet a lot of people are already considering them general purpose AI. It doesn't help that in many situations it can fake it enough that it appears to be reasoning, but it's not. What is the chance that this paper actually has any impact on the AI rollout and overhype or will just be buried and never talked about again until the next time we see how dangerous these tools are like with Google's search rollout. reply layer8 3 hours agoparentFor reasoning of any complexity, some sort of internal monologue and iteration is needed. For type-2 thinking, we conceptualize possible solutions, arguments, and reasoning paths in our mind, and judge and evaluate them before deciding which one we think is adequate or correct. This can be emulated to some extent by guiding prompts that make that internal thought process external, but we really want it to be internal and automatic. We don’t know how to do that yet, because what controls the internal thought process is itself not necessarily language-based, and also, since internal thought processes of biological brains are not directly observable, they can’t be used as training data. Edit: It occurs to me now that there is some parallel between current LLMs and behaviorism [0], and we really need something to which cognitive psychology could be applied instead. [0] https://en.wikipedia.org/wiki/Behaviorism reply sollewitt 3 hours agorootparentGiven many people don’t have an inner monologue and function just fine, it’s more likely inner monologue is a product of the reasoning process and not it’s mechanism. reply layer8 2 hours agorootparentIt’s commonly conjectured that the emergence of human-level reasoning wouldn’t have been possible without the development of language. Personally, I’m able to suppress “word thoughts” in my head (for a short time), but then I lose almost all of my reasoning ability. I could imagine that reasoning is language-based even when it’s not conscious for some people. An internal process being there, and being conscious of it, are two separate things. We would be happy with an AI using an internal monologue without it being conscious of that monologue. reply IggleSniggle 2 hours agorootparentMaybe, but symbolic thought can get pretty far away from what we generally call \"language.\" I bet you can reason 1+3x=22 pretty easily without any words whatsoever, or the sound of one ascending octave after another, or the approximate G-force induced on your body if you take the next turn without applying the brakes. All of these forms of reasoning are true and useful calculations: when we talk about \"intuition\" what we usually mean is that we have a lot of experience and internal reasoning about a subject, but we struggle to translate it to and from the \"language\" part of our brain. Nonetheless, any social dancer will tell you that a dialog is possible just by receiving and inducing g-forces alone. You can reason this way about abstract concepts like orbits without ever touching a single word or concrete symbol. Edit: the key aspect of reasoning, imho, is the ability to make predictions and introspect them against a database of other predictions, using an adversarial heuristic to weight the most plausibly useful results. Perhaps our pattern matching AIs of today just lack sufficient \"experience\" to do what we call reasoning. reply hifromwork 1 hour agorootparent>I bet you can reason 1+3x=22 pretty easily without any words whatsoever I've tried to do it, but I can't. I had to do something like \"ok, so we subtract one from both sides and then it's easy, 3*7=21\". Maybe I could do 2+8 but I still think the word ten \"aloud\". reply twiceaday 12 minutes agorootparentI was able to do it with no words. I 'saw' the steps as if on a piece of paper. I saw 3x=22-1=21, then x=21/3=7. But I have a degree in applied math. Perhaps not internally vocalizing is just being extremely familiar. It also happened very quickly, perhaps there was no time to vocalize anyways. reply layer8 2 hours agorootparentprevRegarding “1+3x=22”, I’m actually not sure, the number words certainly appear in my head when solving the equation. But even then, I would count “1+3x=22” as constituting language. Perception of sound, G-forces, and dancing don’t perform type-2 reasoning by themselves, so I don’t think your argument applies there. Regarding your edit, no, I think the key aspect of the kind of reasoning we are missing in current AI is the ability to hold the reasoning in your mind, and to iterate on it and evaluate it (judge it) within your mind. reply IggleSniggle 1 hour agorootparentIt is very difficult to have a discussion using words to discuss the semantics of non-word or non-symbolic semantics. I was pointing at several different plausible spaces for semiotics and how these spaces could be spaces for reasoning in the hopes that one of them might be relatable. If you use words in your mind when you use math, and you use words in your mind when you make or listen to music, etc., then it is very difficult to find a common ground where it is possible to see that these other realms of thought are capable of not only prediction, but also producing evidence that leads to judgement. That is to say, the key aspects of \"reasoning.\" I picked them because I thought they had broad enough appeal to be relatable, and because I do not personally hear words in my head when doing any of these activities, whether it's calculus or tango, but I still find calculus and tango to be places where reasoning occurs. Some of them, like math or music, are closer to the kind of symbolic thought we use when we discuss things with words. Others, like the experience of g-forces, are not. I present them as a sliding scale between \"word based\" reasoning and \"non-linguistic\" reasoning. Perhaps you can think of a realm that better fits for your personal experience of intuition, and inspect whether these intuitions are capable of \"real\" reasoning in the absence of language, or whether intuition should never be trusted even when you have a great deal of experience in that area. Perhaps in your estimation, anything that cannot produce evidence that is articulable in word form is suspect. Personally, I find all these methods, including language, to be suspect. I don't find language to be especially better at producing the kind of evidence for prediction, correct judgement, or discourse for reasoning than other methods, unless you reduce \"reasoning\" to tautologically require it. One of the best tools of language is that we have writing that allows easy inspection or iteration of the written content; but these things are possible in other realms, too, it's just that we didn't have great tools for introspecting and iterating on their \"ideas\" except within our own minds. These days, those tools are readily available in many more realms of human insight. reply nathan_compton 38 minutes agorootparentprevBrains are weird. I reason almost entirely non-verbally and I would absolutely struggled if I had to laboriously express every thought in words. Its part of the reason I don't work well in teams. So slow! reply psadri 25 minutes agorootparentprevLanguage is a serialization of our brain's \"world model\" structures. reply ElevenLathe 2 hours agorootparentprevThe existence of an \"inner monologue\" isn't really a falsifiable claim. Some people claim to have one while other people claim not to, but we can't test the truth of these claims. reply glitchc 2 hours agorootparentIn this particular case, is there any reason why we simply can't take their word for it? This is not a case of where if I say \"weak\" or \"strong\", most people pick strong because no one wants to be weak, even if the context is unknown (nuclear force for example). reply Terr_ 52 minutes agorootparent> In this particular case, is there any reason why we simply can't take their word for it? My concern is that if we take their word for it, we're actually buying into two assumptions which (AFAIK) are both unproven: 1. That \"Internal Monologues\" (not consciously forced by attention) exist in the first place, as opposed to being false-memories generated after-the-fact by our brain to explain/document a non-language process that just occurred. (Similar to how our conscious brains pretend that we were in control of certain fast reflexes.) 2. Some people truly don't have them, as opposed to just not being aware of them. reply ElevenLathe 39 minutes agorootparentNot only are they unproven, but are ultimately not provable at all. Some people will say yes, some people will say no. Probably we can take their word for it, but in the simplest case they could just lie (in either direction) and we would have no way to tell. In short, maybe these inner monologues exist and maybe they don't, but science can't comment on that. That said, it is clearly something we are interested in, but it will need to be addressed in some other way (i.e. religion, ideology, etc.). reply Terr_ 36 minutes agorootparent> but are ultimately not provable at all No, they are potentially falsifiable as we get better at scanning, identifying, intervening in brain activity. Just off the top of my head here, suppose we create a table puzzle problem that (in itself) doesn't require language to understand, like ones we make for certain animals. Have human subjects (silently) solve it. Afterwards, quiz the solvers about their internal monologue--or lack thereof--noting the words used and dividing them into two groups. Adjust the problem slightly to avoid a memorized solution, stun/anesthetize the language-centers of of subjects to deny access to the monologue-words (validating that will involve other research), and then test them again. * If both groups continue to do well well, that suggests the monologue was a secondary effect. * If the monologue-people do badly while the non-monologue continue to do well, then perhaps it's a real difference in modes of thinking. * If both do badly, that suggests the no-monologue people might just not be as aware of a linguistic process that's actually happening. * If the monologue-folks do better while the non-monologue folks do worse... Well, that's excitingly-unexpected. reply IlliOnato 1 hour agorootparentprev> \"why we simply can't take their word for it\"? As someone who was involved in spiritual practice of \"stopping internal dialogue\" for years, I can tell you that one learns that that dialogue (or monologue, pretty much the same thing) is quite subtle and complex, essentially multi-layered. Typically, when you think that you \"think about nothing at all\" it's just the most surface layer that has stopped, and more subtle talking to yourself is still going on. It takes training just to become able to notice and recognize it. After all, it's just such a constant and monotone hum at the back of one's mind, one learns to completely ignore it. So no, I would not take a word of people who were not trained to notice their internal monologue that they haven't any :-) reply GeoAtreides 2 hours agorootparentprev> is there any reason why we simply can't take their word for it? because if we give them a problem to solve in their head and just give us the answer, they will. By problem I mean planning a trip, a meal, how to pay the mortgage, etc. It's impossible to plan without an internal monologue. Even if some people claim theirs is 'in images'. reply photon_lines 1 hour agorootparent'It's impossible to plan without an internal monologue.' - Sorry, but I disagree with this. I have no 'internal voice' or monologue - whenever I see a problem, my brain actually and fully models it using images. I believe 25% of the population doesn't have the internal monologue which you're referring to and this has been tested and confirmed. I highly recommend listening to this Lex Friedman podcast episode to get a full grasp on the complexities of modelling language and general modelling present in the human brain: https://www.youtube.com/watch?v=F3Jd9GI6XqE reply GeoAtreides 1 hour agorootparentSure, I do mention thinking in images in my original comment and count it as some type of internal monologue. I personally do not believe it's all images, as that would preclude using highly abstract concepts. But I might be wrong, and it might be 100% images. That being said, it does count as an internal monologue. reply lupire 59 minutes agorootparentprevCan you draw a picture of an example of what you see when you think about something? reply staticman2 1 hour agorootparentprev>> It's impossible to plan without an internal monologue That's quite the claim. reply JohnMakin 1 hour agorootparentprev> It's impossible to plan without an internal monologue. Of course it isn't impossible, and this is backed by what we know about paleoanthropology and other instances of cognition in animals - humans were making stone tools millions of years ago, which takes planning in the form of imagining what you want the tool to look like and how you will do it and what it will be used for. It's exceedingly likely we had this ability long before complex speech evolved. Apes also use and make tools, which would require planning, and I don't think they have an internal monologue going on. birds from the corvid family can do some pretty advanced problem solving that requires planning. Cetaceans might be an exception, because they appear to have some form of language, but this is a pretty wild claim not really backed by any kind of science as we understand it today. reply GeoAtreides 1 hour agorootparentAnimals can not manipulate abstract concepts nor can they do long-term plans. No crow can plan an international trip spanning a couple of weeks and two change-overs. And some people definitely can't do it start to end, but they can at least plan the first 5-7 steps. Also, maybe inner monologue is not a binary have/have not, but maybe it is on a continuum. reply rprospero 1 hour agorootparentprev> It's impossible to plan without an internal monologue I once had a teacher claim that people who claimed to have aphantasia were lying, because those people have read books and it is impossible to read a book without picture the scene in your mind's eye. Are you citing the same source that she was? reply ElevenLathe 1 hour agorootparentprev> It's impossible to plan without an internal monologue. How can science make this claim if it can't prove (or disprove) the existence of an internal monologue? reply IlliOnato 1 hour agorootparentWell, I remember Richard Feynman came up with an interesting experiment. He found he could not count objects when he read aloud some text at the same time. He had to name the numbers, and it was impossible if he was already engaging his speech. He thought this was universal, but doing this experiment with friends, he discovered a guy who could count while reading aloud. So when Feynman asked him, how he does this, turned out that the guy instead of \"pronouncing\" numbers was \"seeing\" colored numbers in his imagination, so his speech was not involved. I supposed this experiment can be modified and generalized, and at least to shed some light on this problem. reply glitchc 2 hours agorootparentprevPerhaps there's confusion in how we are using the word monologue. I took it to mean a conversation, a dialogue where the problem is perhaps solved using a dialectic method, or simply a conversation. Since one can solve a problem by following some memorized steps, no conversation required, this is perhaps not a good test, or we mean different things when we say \"monologue.\" reply Terr_ 42 minutes agorootparentprev> The existence of an \"inner monologue\" isn't really a falsifiable claim. Another possibility is that inner-monologues (ones not forced by conscious effort) do exist, but are just a kind of false-memory, something one part of our brain generates after-the-fact to explain/document the outcome of another non-language part. Kind of like how certain reflex-actions can occur before certain decision-making area of the brain light up, yet humans will believe that they sensed the event and made a thinking choice. reply mbesto 40 minutes agorootparentprevThe fact that we don't actually have an understanding and framework for reasoning (e.g. whether inner monologue is a cause or an effect) means we are VERY off from general AI. https://youtu.be/QGYbbLWn-IE?t=72 reply kveykva 2 hours agorootparentprevHave there ever been studies that demonstrate that those individuals don't simulate possible state transitions they'll go through in a different modality? I'd be curious if they visualize actions they'll take still, just not verbally. reply CooCooCaCha 2 hours agorootparentprevI think you’re using “inner monologue” too literally. It could be a progression of pictures, emotions, etc. reply IlliOnato 1 hour agorootparentTo make any progress on this question at all, we need first to come up with some definition of internal monologue. Even if we may need to modify it later, there has to be a starting point. Otherwise, nothing can be established at all, because for any statement there always will be someone's understanding of \"internal monologue\" for which the statement is true, and someone's else understanding for which the statement is false... reply ajuc 2 hours agorootparentprevWith that definition even bacteria have inner monologue. reply CooCooCaCha 1 hour agorootparentCan bacteria imagine pictures? Do they have emotions? Why does this matter? Stop being so pedantic. We're talking about a progression of ideas. Talking in your head is one form of ideas, but people can easily solve problems by imagining them. reply IlliOnato 1 hour agorootparentHmm, looks to me like just trading some words for others. Do bacteria have ideas? Does the navigating system in your car? How do you know? We need to be at least somewhat pedantic, otherwise it's impossible to know what we are even talking about, and no way to establish anything. reply ajuc 1 hour agorootparentprevInitial thesis was - inner monologue is required for reasoning. If you define inner monologue to include everything brains do - the initial thesis becomes a tautology. reply logicchains 1 hour agorootparentprev>For reasoning of any complexity, some sort of internal monologue and iteration is needed. From a formal perspective you're entirely correct. Transformers with chain-of-thought are strictly more powerful than transformers without it, and can efficiently solve classes of problems that would otherwise require exponentially increasing model depth: https://arxiv.org/abs/2310.07923 reply photon_lines 4 hours agoparentprevThere's actually a pretty simple solution to this that I thought about testing out and it involves asking the model to re-construct the problem using a logic language (like Prolog) and asking it to execute this type of program in order to come up with a solution rather than attempting simple chain-of-reason training / other methodologies of getting the model to 'reason' through some of these examples. People forget that humans don't come up with their logical models out of the blue - it takes years of elementary school in order for us to understand the world and solve problems in it. The logic programming approach I'd say is really promising but you would need to feed the LLM a LOT of examples in order for it to work, and currently I'm not even sure that we have enough training data in order to implement something like this. reply astrobe_ 1 hour agorootparent> asking it to execute this type of program in order to come up with a solution I may be showing my ignorance about this tech here, but I believe the LLM doesn't even try to solve a problem; they try to generate a discourse that could pass as a solution or answer to the problem; that's more or less what the abstract states if I understand it correctly. But in no way does it try to apply some sort of mechanical reasoning like inference engines do. To me the solution to this is to associate LLM with mechanical computations, that is an inference engine or an equation solver, rather than recombining the millions of solutions for similar problems it has seen in its training set. I believe I remember reading about teams attempting this approach. I can imagine for instance that if the LLM is in some way able to ask questions and use the answer, maybe it could just generate a prompt for an equation solver and include the result in its answmer. reply asadotzler 18 minutes agorootparentprevIf that kind of thing worked, we'd have been doing it long before LLM chatbots. reply nerdjon 3 hours agorootparentprevI honestly thought about this recently when I was trying to see the limits of Claude Opus. Some of the problems I gave it, what if instead of telling it to solve the problem I asked it to write the script and then give me the command and inputs needed to properly run it to get the answer I needed. That way instead of relying on the LLM to do properly analysis of the numbers it just needs to understand enough to write the logic. It is an interesting prospect but I feel like it has some limitations. For math problems like this one, yeah it should be simple to write a script to do it. But it does first have to understand the core thing here that Alice would be one of the sisters of the brother to write the script accordingly. But I would think this would not scale well when dealing with far more complex issues, particularly ones that may not just be simple math logic. If the request was to write a scientific paper for example, it might have to make several scripts and call them multiple times, and that assumes that it understands what it is doing enough to properly make the script. reply ip26 2 hours agorootparentI don’t understand why LLM’s aren’t already set up to do what you describe automatically behind the curtain. Extract a math equation from text (LLMs are good at translating between languages right?) and immediately evaluate it on the host CPU. LLM is the equivalent of recalling your times tables. Computer arithmetic is the equivalent of re-computing your times tables. reply dragonwriter 2 hours agorootparent> I don’t understand why LLM’s aren’t already set up to do what you describe automatically behind the curtain. LLM-based systems with tool use (which this is an application of) often are, to an extent, the issue is tuning the (behind the scenes, system) prompting so that they use appropriate tools in every case where they should, and do so correctly. (There's also a cost factor involved since behind-the-scenes tool use means multiple LLM round trips to answer the question, so tuning the system to use tools more aggressively makes the system more expensive.) reply pbhjpbhj 2 hours agorootparentprevChatGPT does do this sort of process for arithmetic now; it converts wordbased problems to mathematical notation and then solves. reply photon_lines 3 hours agorootparentprevI'm not sure what you mean by 'it will not scale well.' When we humans learn that we make a mistake - we make a note and we hold the correct answer in memory - the next time we're prompted with a similar prompt, we can use our old memories to come up with the correct solution. I just did a simple test for this same exact problem using ChatGPT 3.5: 'Can you reformulate the following problem using Prolog? When you execute the program - what answer do you get? 'Alice has 8 brothers and she also has 5 sisters. How many sisters does Alice’s brother have?' Even though it came up with a proper Prolog formulation - it failed to account that Alice is one of the sisters - so I gave it a simple prompt for this and it was able to correct the formulation and come up with the proper solution: My prompt after ChatGPT gave me an answer of 5: 'Yes but you aren't including Alice in your logic. Shouldn't the correct answer be 6 (since Alice is also a sister)?' Response: 'You are correct. The correct answer should account for Alice being one of the sisters. Let's adjust the Prolog program to include Alice in the calculation: % Facts brothers(alice, 8). sisters(alice, 5). % Rules sisters(Brother, NumSisters) :- sisters(alice, Num), NumSisters is Num + 1. % Include Alice herself % Query ?- sisters(alices_brother, NumSisters). In this updated Prolog program: The rule for determining the number of sisters includes Alice herself by adding 1 to the number of sisters Alice has. When we execute the query ?- sisters(alices_brother, NumSisters). in Prolog, we should get: NumSisters = 6. This correctly reflects that each of Alice's brothers has 6 sisters, including Alice herself. When I prompt it to answer this question using Prolog now, I always get the correct answers no matter which integers I feed in, so it seems like the model is able to generalize after I show it where it's 'reasoning' was wrong. Basically, my suggestion is to get the model to recognize logic puzzles like this and try to formulate them in terms of logic programming queries which it can use and tune in order to come up with correct answers rather than simple auto-associative chain of reason training which current GPT models rely on, but like I said - this is my hypothesis and I believe this would work much better in getting these models to 'generalize' than the current approaches we're using. Hopefully this helps. reply pbhjpbhj 2 hours agorootparentWhen you ask again the prompt includes the context of your previous question and correction. When I ask the prompt doesn't have that context so the model fails to give me the correct answer. I'm using the default free model in the app, based on GPT4. reply immibis 57 minutes agorootparentprevWhat if you prompt it \"You seem to have accidentally included Alice. The correct answer should be 4\"? reply sollewitt 3 hours agorootparentprevRight, and do you verify the result? You have to know what the answer is supposed to be before you can write a test case. reply photon_lines 2 hours agorootparentYup - well you feed in the prompt along with an answer and you get the model to produce outputs and check for discrepancies. If the answer is wrong then the model adjusts -- this is the way backpropagation works....I think there are huge advantages in using logic languages in order to represent some of these data sets rather than simple English or the current chain-of-thought reasoning approaches -- backpropagation as an example isn't really used in the human brain, but it leads to great results in mimicking how neural networks 'learn' - in the same way, we don't have to have the full formal picture of how humans model the logical world in order to achieve great results. We can simulate this using logic programming or even general programming or at least that's my conjecture. reply CooCooCaCha 3 hours agorootparentprevI’m curious how this would work considering knowledge can be fuzzy. Like if I’m out camping and I sit on a log or a rock those things are not what people usually think of as chairs but they can serve as chairs in that situation. reply photon_lines 3 hours agorootparentYou can get models to actually show that 'logs' could function as 'chairs.' You're forgetting that we humans also learn this as well, but we learn this in a much simpler manner than LLMs though so someone has to explicitly let models know what assumptions they can make. You get the LLM to write Prolog programs and learn associations in this manner. As the model gets better at logically modelling the problems - the solutions to prompted problems like this should get better. reply ericmcer 2 hours agoparentprevIf you really think about what an LLM is you would think there is no way that leads to general purpose AI. At the same time though they are already doing way more than we thought they could. Maybe people were surprised by what OpenAI achieved so now they are all just praying that with enough compute and the right model AGI will emerge. reply solidasparagus 2 hours agorootparent> If you really think about what an LLM is you would think there is no way that leads to general purpose AI It is an autoregressive sequence predictor/generator. Explain to me how humans are fundamentally different reply throwanem 2 hours agorootparent\"Prove me wrong?\" That's not how this works. Your implicit claim here is that human cognition and LLM functioning are fundamentally similar. That claim requires substantiation. reply photon_lines 2 hours agorootparentI actually did a full write-up on this here fyi: https://photonlines.substack.com/p/intuitive-and-visual-guid.... You can skip most of this and scroll down to the end-section called 'The Mental Model for Understanding LLMs' where I try to map how transformers are able to mimic human thinking. I think that comparing them to auto-associative / auto-regressive networks is actually a really good analogy FYI and I do believe a lot of human learning is based on this. reply Workaccount2 7 minutes agorootparentWe have no idea how human thinking works and no idea what consciousness is. I'm getting so tired of listening to software engineers LARP pseudo neuroscientists with 6th grade level insights. >Of course, the [AI] brain isn’t ‘conscious.’ It doesn’t have any survival instincts which we humans do. Bruh... reply semi-extrinsic 45 minutes agorootparentprevWe don't have sufficient understanding of human thinking and mental processes to make any claims like the ones you are trying to make. If we did, psychology would be a quantitative hard science; it most definitely is not. reply solidasparagus 2 hours agorootparentprevAn article this long needs a table of contents IMO. reply solidasparagus 2 hours agorootparentprevIt is how it works if you are replying to someone who claims \"If you really think about what an LLM is you would think there is no way that leads to general purpose AI\". The counter example is human beings are considered general purpose intelligence and we are complex, but fundamentally predictable systems (not by us today), with (as far as we can tell) deterministic outputs based on the state of the universe (including physical being like brain and chemicals). reply slashdave 2 hours agorootparentprevEven language is not sequential. reply solidasparagus 2 hours agorootparentTell me more? reply slashdave 56 minutes agorootparentLanguage is only sequential in the form it is transmitted (verbally). There is no reason that sequential statements are generated sequentially in the brain. Quite the opposite, really, if you consider rules of grammar. I really wish most of the LLM folks just took a few courses in linguistics. It would avoid a lot of noise. reply solidasparagus 2 minutes agorootparentAny pointers for where to start? Linguistics is dense, the terminology is a slog, it seems huge, and I was never clear that there was real value to be gained. But it's always useful to be more informed. GaggiX 28 minutes agorootparentprevSince LLMs seem to follow the rules of grammar quite well, and this is quite the opposite of \"sequential\", I guess LLMs do not process informations very sequentially. reply indoordin0saur 2 hours agorootparentprevAI needs to see thousands or millions of images of a cat before they reliably can identify one. The fact that a child needs to only see one example of a cat to know what a cat is from then on seems to point to humans having something very different. reply TeMPOraL 2 hours agorootparentHumans train on continuous video. Even our most expensive models are, in terms of training set size, far behind what an infant processes in the first year of their life. EDIT: and it takes human children a couple years to reliably identify a cat. My 2.5 y.o. daughter still confuses cats with small dogs, despite living under one roof with a cat. reply indoordin0saur 1 hour agorootparentI contend that you could show any child old enough to communicate in basic English a photograph (so not live continuous video) of some obscure animal they've never seen before (say an Okapi) and they'd be able to easily identify another Okapi when seeing one at a zoo. reply TeMPOraL 33 minutes agorootparentMy daughter is 5 y.o., which means because of kindergarten, I spend plenty of time about kids this age. A random kid this age would absolutely fail your test. They may remember the word after one exposure, but I doubt they'll remember any of the distinctive features. Hell, many adults would fail it. I'm not sure if I could pass such test - in my experience, you remember the important details only after first experiencing a test and realizing what exactly it is that would be useful in distinguishing the two animals. reply bongodongobob 1 hour agorootparentprevSo you're just going to ignore the 5 years of continuous training? I'm not sure what point you're trying to make. reply solidasparagus 2 hours agorootparentprev> AI needs to see thousands or millions of images of a cat before they reliably can identify one. Not if they inherit from a previous generation of AI. But even if they did, a different training speed does not imply a different capability reply indoordin0saur 1 hour agorootparentMy point is not that humans have a faster training speed but that humans must be doing something fundamentally different from LLMs. You could build Altman's $7 trillion dollar GPU cluster and use the majority of the world's energy to feed it and you'd still hit the same limitations if you're just running an LLM on it, even a very sophisticated LLM. This is Yann LeCun's position as well. reply solidasparagus 1 hour agorootparentThat, as I understand it, is not a valid chain of logic. Requiring fewer data points does not inherently indicate that the underlying mechanism (autogressive sequential generation, not the transformer which is just an architecture) is different. Not to mention the secondary arguments like - no proof that human learns faster from fewer datapoints, that's just your assumption in the sibling comment. Humans inherit information. The equivalent - fine-tuning a foundation model - is very fast to learn novel objects. Just because someone has a Turing award doesn't mean they know what they're talking about. They are just people, with strengths and weaknesses like everyone else. But often on the extreme end of strengths and weaknesses. reply GaggiX 21 minutes agorootparentprevIf the model is first pre-trained on unlabeled images, then it takes about 10 labeled images of cats and 10 labeled images of dogs to train a (possibly strong) classifier (example: DINOv2), I doubt humans will do better. reply more_corn 2 hours agorootparentprevLLMs are the language center of an AI. They in no way purport to be: long term memory, critical thinking engines, calculators. If we want those things we can build them. Building them into the language center would be absurd and weird. reply viking123 1 hour agorootparentIn the brain all those things are neural networks too, memory is encoded in the weights. So if we want to have that type of system, everyone should have their own model and the weights would actually adjust all the time just like in our brain, but now it is just trained once and that is that reply daveguy 2 hours agorootparentprevThis is a very good take on the current state of LLMs. They are able to articulate surprisingly well and LLMs or something similar will definitely be part of a larger AI model in the future. They are good creative aids, but they should not be trusted. reply lupire 52 minutes agorootparentprevAnd yet this is what all the big companies we re trying to do, because the LLM is incredibly expensive fixed cost, and they marginal cost for each new task. reply Mmskynettio 2 hours agoparentprevIts not an AI hype. A hype is defined as something which gets oversold: \"promote or publicize (a product or idea) intensively, often exaggerating its benefits.\" Just yesterday I visited a google cloud summit and one person from bosch told the audiance how they are now able to work with less external agencies like texting, graphicsdesigner and photographers for their materials. It already saves money, has real impacts and continues to progress. We are also don't know what ChatGPT 5 will bring, because they say this will do more reasoning than before, but we already are working (people/our socity) on solving this in different ways: From code which creates a unit test first and than the code, to different type of architectures. For me, 2024 was the LLM cost reduction year and the LLM gets a big context window year. AI doesn't need to be ready tomorrow, but its capabilities are already really good. And i know plenty of people around me who are a lot less interesting to talk to than any llm (from a human skill/knowledge point of view). llama 3 was also a big achievement 2024. Facebook shows that better data leads to better quality for smaller models. We haven't not only entered the AI ara but also the 'gather all the knowledge we can, quality check it and refine it because now we can actually do something with it' ara. We are in the feedbackloop knowledge ara. reply nurple 1 hour agorootparentYour post is complete hype, all about people saying things instead of showing things that've actually been done. For me, 2024 was the LLM exposed as basically pure hype year. There is no expert of any field I follow online where they're posting up results from AI tooling for any other reason than to show how awful it is. I consider myself an expert in software, and LLMs specifically have only caused me great pain. Even the one situation where you describe someone describing the ability to work in an absolute vacuum sounds like a huge negative to me. The recent push for DEI policies were even ostensibly about the importance of people of diverse backgrounds and viewpoints working together. The most important thing you're missing a perspective of scale on is the step you describe as \"quality check it\". On things I don't know, and have attempted to enlist an LLMs help on, in every case I have had to go back and just actually learn how something works, after time wasted struggling with subtle wrongness in the output. At least I have the background expertise to do that, however, I have seen a Jr dev's mind get literally rotted by too much time in pure LLM land. Besides the cost of rewriting their code, the company was now the proud owner of a young dev with a mind filled with nonsense. How do you even weigh the cost of fixing a corrupted human mind? reply bongodongobob 1 hour agorootparentEat something and take a nap, you sound unhinged. ChatGPT has nearly doubled my work output, most of my job is system admin infra type stuff and it's ridiculously good at troubleshooting odd issues. Hopefully you can find a use case for it someday, until then, the rest of us will continue to be more productive. reply hellojesus 36 minutes agorootparentDo you have any concern about the data you're feeding to the vendor serving your prompts? I've had junior devs tell me they use chatgippity to combine excel workbooks, and when I confirm they're not self hosting a llm to do it, I ask if they think it's a good idea to hand over company data to openai. They don't care. In a world of tight security, I find it astonishing that so many people willingly give away trade secrets to these companies, whom can sell it to any bidder if they choose. reply daveguy 2 hours agorootparentprev> We are also don't know what ChatGPT 5 will bring, because they say this will do more reasoning than before... This paper very clearly demonstrates these LLMs are not reasoning in a fundamental way. Token prediction and reasoning are two different tasks. They may be related, but they are not the same. \"Just wait for GPT 5, it will be amazing!\" is part of the hype. Please do not assume an LLM is correct in skill or knowledge unless you already know the answer or can verify by other means. reply Mmskynettio 1 hour agorootparentThe problem is that we don't know how we do reasoning. I calculate stuff by following a formular after i pattern detected a problem i already know. Plenty of humans are not able to solve those math problems. If the future of llm / ai becomes a LLM with multi modal and mixture of experts and that solves those reasoning problems, we still don't know if this is a different type of reasoning than what humans do. reply slashdave 1 hour agorootparentprevThere is no feedback. You cannot create new knowledge out of thin air. reply Lich 2 hours agorootparentprev> AI doesn't need to be ready tomorrow, but its capabilities are already really good. A majority don’t deny that it’s good. The problem is that so many think it is actually reasoning, believing the answers can be trusted. reply Mmskynettio 2 hours agorootparentDoes it need to do reasoning perfectly? If it created meta concepts from billion words on the internet and has meta models which are correct and are more and better than an avg human, isn't it actually good in reasoning? Its a very narrow thing to say 'is that so many think its actually reasoning' to say AI is just hype or everything we are doing is a waste etc. There are human benchmarks they are winning at. The critic could be more that we don't have enough benchmarks. reply rvnx 2 hours agorootparentprevIt's generally reasoning better than me, so using them I do less mistakes than if I did my own logic. reply glitchc 2 hours agoparentprevThe problem is a good chunk of the global population is also not reasoning and thinking in any sense of the word. Logical reasoning is a higher order skill that often requires formal training. It's not a natural ability for human beings. reply bithive123 1 hour agorootparentIn \"any sense\" of the word? Surely anyone who adjusts their behavior when they get undesired or unexpected results is reasoning and thinking. And since most activities are mediated by thought of some kind, most people are reasoning and thinking otherwise they would never recover from even simple mistakes, like walking east when they need to go north. Saying they're \"not thinking in any sense of the word\" because they can't solve predicate logic problems from a college textbook is a rather odd claim. Surely those things arise from reasoning and thinking, rather than the other way around. reply hellojesus 25 minutes agorootparentThis seems to me to be where these systems need to go in the future, akin to reinforcement learning. You feed an llm a prompt. It then abstracts and approximates what the result should be. It then devises a hypothesis and solves it and compares it to the approximated output. Then it can then formulate a new hypothesis and evaluate it, based off the outcome of hypothesis 1. From there it can either keep iterating or dump that path for a new one (e.g., the next best hypothesis in the original formation). At some point the answer is \"good enough.\" But along the way it keeps playing against its thoughts to see if it can do better. A key issue may be the original approximation, so it may need to consider its adjustment when iterating. Maybe this is how cutting edge llms work now. I have no idea. reply slashdave 2 hours agorootparentprevSeriously? You think individuals are incapable of reasoning without training first? reply glitchc 1 hour agorootparentYes, seriously. Some examples: An individual without training cannot reliably separate cause from effect, or judge that both events A and B may have a common root cause. Similarly, people often confuse conditionals for causation. People often have difficulty reasoning about events based on statistical probabilities. Remember, the average person in North America is far more terrified of a terror attack than an accident or a heart attack, yet the latter two are much more likely to be their cause of death. reply slashdave 1 hour agorootparentYou mean without training, people cannot frame answers in the terms you've learned from training. Well, why are you surprised? If you think reasoning is limited to the frameworks you learned from a book, you live in a small world. reply TeMPOraL 1 hour agorootparentprevDo you think they are? We haven't had a case of individuals without training in like 10 000 years. reply rep_lodsb 4 minutes agorootparentAnd judging by some comments here, humans must have gotten dumber since then. Or we would never have invented writing, agriculture or even basic clothing. \"This problem is actually not that easy, the average person couldn't solve it either, especially if the numbers were bigger\", \"Yet another cherrypicked clickbait study to make LLMs look bad, those people are just scared of being made obsolete\", etc. reply wtbdqrs 3 hours agoparentprevI appear to be reasoning at times but I have mostly no idea what I am talking about. I hit a bunch of words and concepts in the given context and thus kind of hallucinate sense. Given a few months of peace of mind and enough money for good enough food, I could actually learn to reason without sounding like a confused babelarian. Reasoning is mostly a human convention supported by human context that would have been a different one if the Fascists had won the war or the Soviet Union wouldn't have gotten corrupted. But none of that has anything to do with pulling up a whiteboard to draw some flowcharts and run some numbers, all of which is why I am certain there is nothing the devs have \"to fix\". It took most reasonable humans many generations to learn stuff. Very few of us did the actual work. It's all just a matter of time. reply voxic11 2 hours agorootparentYeah, I think these chatbots are just too sure of themselves. They only really do \"system 1 thinking\" and only do \"system 2 thinking\" if you prompt them to. If I ask gpt-4o the riddle in this paper and tell it to assume its reasoning contains possible logical inconsistencies and to come up with reasons why that might be then it does correctly identify the problems with its initial answer and arrives at the correct one. Here is my prompt: I have a riddle for you. Please reason about possible assumptions you can make, and paths to find the answer to the question first. Remember this is a riddle so explore lateral thinking possibilities. Then run through some examples using concrete values. And only after doing that attempt to answer the question by reasoning step by step. The riddle is \"Alice has N brothers and she also has M sisters. How many sisters does Alice’s brother have?\" After you answer the riddle please review your answer assuming that you have made a logical inconsistency in each step and explain what that inconsistency is. Even if you think there is none do your best to confabulate a reason why it could be logically inconsistent. Finally after you have done this re-examine your answer in light of these possible inconsistencies and give what you could consider a second best answer. reply daveguy 2 hours agorootparent> After you answer the riddle please review your answer assuming that you have made a logical inconsistency in each step and explain what that inconsistency is. Even if you think there is none do your best to confabulate a reason why it could be logically inconsistent. LLMs are fundamentally incapable of following this instruction. It is still model inference, no matter how you prompt it. reply zeknife 1 hour agorootparentprevIf you had a prompt that reliably made the model perform better at all tasks, that would be useful. But if you have to manually tweak your prompts for every problem, and then manually verify that the answer is correct, that's not so useful. reply mjburgess 4 hours agoparentprevIn many ways, this is very obvious and routine to people who use these systems with a critical understanding of how they work. It's dispiriting how we require a arxiv.org pdf in order to point out the absence of the emperor's clothing. There are few people who use LLMs who could not, right now, demonstrate this point to themselves if they so wish. Such is the strength of corporate tech propaganda that a whole mass of people will instead insist that we have never worn clothes either. reply nerdjon 4 hours agorootparent> In many ways, this is very obvious and routine to people who use these systems with a critical understanding of how they work. The last part of that is the problem and why a paper like this is critical. These systems are being pushed onto people who don't understand how they work. CEO's and other business leaders are being pushed to use AI. Average users are being shown it in Google search results. Etc etc. People are being told it can do far more than it really is. reply mjburgess 4 hours agorootparentSure, but even these people... the failures are so common, and often very obvious. Consider a CEO who puts a press briefing in and asks some questions about it, it's not uncommon for those answers to be obviously wrong on any sort of critical reflection. We arent dealing with a technology that is 99.9% right in our most common use cases, so that we need to engineer some incredibly complex problem to expose the flaw. Rather, in most cases there is some obvious flaw. It's a system that requires typically significant \"prompt engineering\" to provide the reasoning the system otherwise lacks. I guess that offers an explanation: people aren't aware that via their own prompt engineering they are repairing the deficiencies of the process by manipulating its inputs to include the structured reasoning it lacks. So there's a sort of hot-reading effect at work. reply throw46365 58 minutes agorootparent> We arent dealing with a technology that is 99.9% right in our most common use cases, so that we need to engineer some incredibly complex problem to expose the flaw. Rather, in most cases there is some obvious flaw. It's a system that requires typically significant \"prompt engineering\" to provide the reasoning the system otherwise lacks. Right -- we are a long way from \"this is a very nuanced error\" being the dominant failure. reply kalkin 2 hours agorootparentprev> People are being told it can do far more than it really is. Meanwhile these HN comments are split between: * Lots of people confirming what the paper itself notes (but doesn't highlight), that the most advanced models actually can solve this problem at least a significant portion of the time. (A proportion which one can pretty easily project is only likely to increase with future models.) * Lots of people saying \"this confirms LLMs can't do reasoning\". Questions I'd ask you to consider: * Is \"LLMs can't do reasoning\" actually more accurate than the typical hype? * Is a \"critical understanding of how [LLMs] work\" that would predict they simply cannot solve this problem actually a good understanding? reply mjburgess 10 minutes agorootparentThe critical understanding doesnt predict that LLMs cannot solve problems. It predicts how they will solve them. There is no information, a priori, what the LLM has been trained on. You have to prompt, then see the answer. Once the answer arrives, the critical understanding provides a route to repairing the answer when not accurate or useful. LLMs do not reason. They appear to reason by repeating the structure of reasoning in their training data. This is indistinguishable in many cases. reply throw46365 3 hours agorootparentprev> Such is the strength of corporate tech propaganda that a whole mass of people will instead insist that we have never worn clothes either. This is the line of reasoning I find most dispiriting. I still believe tech people cling to this line of reasoning because it helps them justify replacing people in jobs with LLMs. reply colechristensen 4 hours agoparentprevI don't think anybody who actually uses LLMs thinks they are general purpose AI. Like you ask it to do one thing it's amazing, but then you try to modify or do something with extra steps, or just anything with any complexity to it and it falls over. reply nerdjon 4 hours agorootparent> I don't think anybody who actually uses LLMs thinks they are general purpose AI. I would like to believe that but I have had too many conversations with people who basically think it already is. Including in one situation of a fellow engineer. It feels like more and more \"we\" are in a bubble of actually having some knowledge of how this works, what the actual limitations are, and what it just is not. While there is in fact criticism of it out there, particularly around AI \"art\". It doesn't seem to be focused on the area we are talking about. reply goostavos 2 hours agorootparentI dunno. If memes are anything to go by, people are frustrated by Yet Another Chat Bot standing between you and the human you need to solve your program. The fact that it's a better, more human like chat bot doesn't mask the frustration of being forced to talk to a computer with limited agency. reply James_K 3 hours agorootparentprev> I don't think anybody who actually uses LLMs thinks they are general purpose AI. They are being sold as such. Most people don't know anything about the topic and will buy that marketing. The entire concept of these models is that you can put a whole bunch of data in and eventually some kind of magic will happen and you get AGI out. They would not see the kind of investment that they do if all that was being promised was \"really good predictive text\". In fact some philosophers argue that sentience is just really good predictive text to try and make the point that these models are AGI. reply kragen 3 hours agorootparentprevthey're pretty general-purpose; you can ask the same model for recipe suggestions, a fanfic, or verilog. like, they're far, far more general-purpose than any humans i know it's true that they're not very reliable, but they seem to be not very reliable across many different domains. and they don't seem to be particularly less reliable than the average human, so i think possibly your standards for 'general purpose ai' are set high enough that you would declare humans to be unintelligent (or perhaps not 'general-purpose') if you applied them consistently you can certainly find particular domains where humans can still do things llms can't, but i haven't seen a persuasive account of why those domains are the more important ones, and of course the converse is also true reply dr_dshiv 2 hours agorootparentprevWell, I believe LLMs are general purpose AI. And, for the record, so does Peter Norvig. He coauthored the most popular textbook on Artificial Intelligence, so this belief can be considered a somewhat mainstream perspective. His article, “Artificial General Intelligence Is Already Here“ [1] is really worth reading. https://www.noemamag.com/artificial-general-intelligence-is-... reply lupire 44 minutes agorootparentI can't tell whether Norvig is making a subtle's point that artificial general stupidity is still an important form of artificial general intelligence, or he whether he somehow vastly overestimates AI model performance on non-memorization and summarization tasks. reply solidasparagus 2 hours agorootparentprevThe vast majority of people use a model built by someone else through a high-level abstraction and then make broad claims about what the technology will be capable of. Then you have the people who are leveraging the technology to train models from scratch and that population is far more apt to believe that large models can be general purpose AI (by some definition). Sure there are other things at play like money, publicity, reputation, and a desire to do something important. But there is also the very clear trend line where transformers are able to model pretty much any sequence of tokens where there is pretraining scale data available. Whisper was not a surprise. GPT-4v was not a surprise. Sora was not a surprise. reply mupuff1234 3 hours agorootparentprevThey might not think that but they sure as hell are trying to sell that idea. reply Closi 4 hours agoprevQuestion is: \"Alice has 60 brothers and she also has 212 sisters. How many sisters does Alice’s brother have?\" (nb: I have added numbers, it's phrased as X and N in the paper) I must confess, when I tried to answer the question I got it wrong...! (I feel silly). I only realised I got it wrong when I plugged it into GPT-4o and it came back with the correct answer: https://chatgpt.com/share/6eb5fa36-e0fd-4417-87d1-64caf06c34... Worth noting that the prompts from the experiment include \"To answer the question, DO NOT OUTPUT ANY TEXT EXCEPT following format that contains final answer: ### Answer:\" so it appears that they are stopping the models from 'thinking out loud'. If I add that to the prompt, GPT4o gets it consistently wrong... https://chatgpt.com/share/7e6a7201-dd2b-43c6-8427-76e5b003ca... Also worth noting that there are more complex examples where GPT4o seems to fall down such as: > Alice has 3 sisters. Her mother has 1 sister who does not have children - she has 7 nephews and nieces and also 2 brothers. Alice's father has a brother who has 5 nephews and nieces in total, and who has also 1 son. How many cousins does Alice's sister have? However I can't honestly say that this is THAT simple or that most people would get this right... reply GrantMoyer 3 hours agoparentNote that in the paper, all the numbers used were under 10. > AIW Variation 1, N=3,M=6,C=7 > AIW Variation 2, N=4,M=2,C=3 > AIW Variation 3, N=1,M=4,C=5 > AIW Variation 4, N=4,M=1,C=2. Also note that the resricted prompt is only one of the prompt variations tested by the paper. It also explores common techinques to get LLMs to perform better, including \"thinking out loud\". Even with these methods the models still fail to produce a correct answer. > Model prompt types. It is well known that so-called prompt engineering can heavily influence the model behavior and model response quality [26, 27, 28]. To account for the response variations due to various prompt forms, we created 3 distinct prompt types asking for the solution to the AIW problem: STANDARD, THINKING, and RESTRICTED. The STANDARD prompt type asks to solve the posed problem and output the final answer in the format as described above. This does not put any specific requirements on model behavior. The THINKING prompt type extends STANDARD with the request to think carefully and double check the solution for any mistakes. This should encourage model to invest more computation into obtaining the solution. In contrast to this, the RESTRICTED prompt urges the model to output only the final answer without any further text. This is supposed to restrict compute invested in producing output. We observe substantially shorter outputs across tested models compared to STANDARD and THINKING for this prompt type (Suppl. Fig. 13). reply llm_trw 4 hours agoparentprev>Worth noting that the prompts from the experiment include \"To answer the question, DO NOT OUTPUT ANY TEXT EXCEPT following format that contains final answer: ### Answer:\" so it appears that they are stopping the models from 'thinking out loud'. If I add that to the prompt, GPT4o gets it consistently wrong... Yes this is a common thing I see people who think LLMs are idiots do. The more an LLM talks the smarter it gets _because that's the only way it can compute anything_. Imagine saying that Turing machines fail the Church–Turing thesis because they can't solve 3-sat for N variables in N moves or less. That's what you're doing to an LLM when you ask it to be concise. reply Miraste 3 hours agorootparentAs it says in the paper, they already did the research with less limiting prompts. All models were tested with another prompt that gave only the question and no further instruction, and a third prompt that asked the model to consider its answer carefully. The correct response rate chart doesn't even use the results from the concise prompt. reply pawelmurias 3 hours agorootparentprevLLMs are idiots. They can't reason properly and only parrot stuff https://chatgpt.com/share/dcb4ff4e-e8a2-463b-86ec-9caf10b6e6... Sometimes they get the answer right to something really complex because it fits a pattern, but sometimes they answer with something really really stupid. reply tsunamifury 3 hours agorootparentWhy are so many people so insistent on saying this? I’m guessing you are in denial that we can make a simulated reasoning machine? reply PheonixPharts 3 hours agorootparentPeople keep saying it because that's literally how LLMs work. They run Montecarlo sampling over a very impressive latent linguistic space. These models are not fundamentally different than the Markov chains of yore except that these latent representations are incredibly powerful. We haven't even started to approach the largest problem which is moving beyond what is essentially a greedy token level search of this linguistic space. That is, we can't really pick an output that maximized the likelihood of the entire sequence, rather we're simply maximizing the likelihood of each part of the sequence. LLMs are not reasoning machines. They are basically semantic compression machines with a build in search feature. reply foobiekr 36 minutes agorootparentExactly right and well said. reply tsunamifury 8 minutes agorootparentThis type of self affirmation has a quality of denial. Also the above description is reductive to the point of \"Cars can't get you anywhere because they aren't horses.\" pretendscholar 2 hours agorootparentprevThe best compression is some form of understanding reply skydhash 1 minute agorootparentThe best compression relies on understanding. What LLM is is mostly data how humans use words. We understand how to make this data (which is a compression of human text) and use it (generate something). AKA it’s “production rules”, but statistical. The only issue is ambiguity. What can be generated strongly depends on the order of the tokens. A slight variation can change the meaning and the result is worthless. Understanding is the guardrail against meaningless statement and LLMs lack it. camdenreslink 3 hours agorootparentprevIt is hard to trust any output from a machine that is confidently wrong so frequently. You need to already be knowledgable in a topic (or at least have a well attuned BS detector) to know if it is giving you correct responses. It can be a time saver and assistant in getting work done where you are already a subject matter expert, but it needs to get better to remove the human from the loop. reply hyperbovine 3 hours agorootparentprevBecause they understand how LLMs work. It's not reasoning. It's not simulating reasoning. reply riku_iki 41 minutes agorootparentprev> I’m guessing you are in denial that we can make a simulated reasoning machine? some people actually try, and see that LLMs are not there yet reply imtringued 2 hours agorootparentprevNo it is because supervised and self supervised learning happen to produce reasoning as a byproduct. For some reason people think that telling a model to recite a trillion tokens somehow will improve it beyond the recitation of those tokens. I mean, in theory you can select the training data so that it will learn what you want, but then again you are limited to what you taught it directly. The problem is that these models weren't trained to reason. For the task of reasoning, they are overfitting to the dataset. If you want a machine to reason, then build and train it to reason, don't train it to do something else and then expect it to do the thing you didn't train it for. reply rossdavidh 3 hours agorootparentprevEven if this were all true, it points to a fundamental risk of using LLM's for important tasks, which is that it is not at all clear to a user that this prompt would cause a problem. The LLM doesn't say \"I'm sorry Dave, I just can't do that\", it just complies with it and gets the wrong answer. You can always make excuses for the LLM afterwards, but software with hidden risks like this would not be considered good or reliable in any other context. reply saurik 3 hours agorootparentPeople really need to stop trying to model an LLM as some kind of magical software component: it all makes a lot more sense if you model it as an under-performing poorly-aligned employee; so like, maybe a distracted kid working for peanuts at your store. You wouldn't trust them to with all of your money and you wouldn't trust them to do a lot of math--if they had to be in charge of checkout, you'd make sure they are only given a point-of-sale terminal and their main job was to, at best, scan the barcodes and compare the total--and yet there are tasks you can imagine handing to them that you'd never give to a robot or computer even though they get it wrong a lot, as not all tasks need to be handled perfectly, they still understand extremely fuzzy tasks, and they are probably cheaper than a qualified adult (certainly cheaper than one who is being paid enough to \"give a shit\" and pay enough attention to not let you get robbed or even put themselves at some risk for you). reply IlliOnato 22 minutes agorootparentYou have a point... I once gave a 10-dollar bill to a young man serving at the cashier at a store, and he gave me 14 dollars back as a change. I pointed out that this made no sense. He bent down, looked closer at the screen of his machine, and said \"Nope, 14 dollars, no mistake\". I asked him if he thought I gave him 20. He said no, and even shown me the 10-dollar bill I just gave him. At that point I just gave up and took the money. Now that I think about it, there was an eerie similarity between this conversation and some of the dialogues I had with LLMs... reply chefandy 2 hours agorootparentprevWhile LLMs have incredible potential, and are even downright useful in their current format, they have the rather nasty tendency to confidently present bullshit that passes the smell test for most people. When it's wrong, it's not just wrong, by design, it's wrong but sounds plausible. Considering most people with subject matter expertise aren't going to consult such a bot for their own areas of expertise, that means most people will not be able to detect it intuitively. Good example: I sunk probably an hour into trying to get Gemini Advanced to help me integrate it with a personal Google Calendar account. I kept asking it things and going crazy because nothing lined up with the way things worked. Finally, it referred to itself as Bard and I realized it was giving me information for a different product. As soon as I asked \"are you giving me instructions for Gemini Advanced or Bard?\" it was like \"OH LOL WOOPS!! YOU GOT ME BRO! XD I CAN'T DO ANY OF THAT! LOL.\" Which, honestly, is great. Being able to evaluate its answers to realize it's wrong is really neat. Unfortunately, it was neat too late and too manually to stop me from wasting a ton of time. I have decades of experience working in software-- imagine some rando that didn't know what the hell Bard was or even imagine this thing with \"Advanced\" in the name couldn't even distinguish between its own and other products' documentation. reply floren 1 hour agorootparent> As soon as I asked \"are you giving me instructions for Gemini Advanced or Bard?\" it was like \"OH LOL WOOPS!! YOU GOT ME BRO! XD I CAN'T DO ANY OF THAT! LOL.\" Which, honestly, is great. Being able to evaluate its answers to realize it's wrong is really neat. Did it evaluate its answers, or did your expression of doubt cause the eager-to-please language model to switch from \"generate (wrong) instructions because that's what the user asked for\" to \"acknowledge an error because that's what the user asked for\"? How many times have we seen \"Oops, you're right! 2 + 2 is actually 5! I apologize for saying it was 4 earlier!\" reply dematz 3 hours agorootparentprevI often want chatgpt to answer concisely and tell it that. If it really needs to do this 'thinking out loud', could it do that under the hood and not in the final output on my screen? Its first pass could use as many words as it wants to compute the answer, but once the answer is computed please go back and make it short. Not to take away from your point that maybe the prompt is the problem in these reasoning questions. reply marzell 37 minutes agorootparentI believe the \"thinking out loud\" is fundamentally part of the process of \"text completion\" which is what it is doing. Certainly we can (and do) break things apart and add layers that could be used to effectively do this by adding more steps and processing time. But ultimately in a single turn, the entire conversation up to that point (including instructions you may have added telling it to not think out loud) is the input, and the output will reflect that. reply PheonixPharts 3 hours agorootparentprev>_because that's the only way it can compute anything_ I'm fairly certain we'll soon realize that what's happening here is that the markov chain being run over latent space needs a certain amount of \"warmup\" before it starts sampling from the optimal region. HMC samplers for Bayesian methods have this same property. The terms \"reasoning\", \"computing\" or \"thinking\" for this stage should be considered metaphors rather than explanations for what's happening, which is really waiting for a random walk to start sampling from the typical-set. reply muglug 3 hours agorootparentprevI think you're wrong about that — I just tried prompting ChatGPT 4o to show all its working before giving an answer. It was still incorrect, but when asked to show its working it formatted the answer prettily. reply J_Shelby_J 3 hours agorootparentprev> The more an LLM talks the smarter it gets I have a blog post in coming on this topic, but yes, this is right. My method is to first get the LLM to answer the question, and THEN feed the answer back the LLM to extract the answer using constraints + grammar/logit bias/regex to parse the answer. Previously, I constrained to a single true/false token, which worked, but fails on complex queries. So I split the decision making into a \"justification\" portion[0], and a \"parsing\" portion. I found that even crafting the prompt matters here, if you start with or end with, \"It's very important to the response includes 'The answer is:'\", then the model will lead with that response or only reply with that response. So I put it in the middle of the prompt, and end with with a request to justify the response. As a result, most models will reason their way to the answer, and then end with 'The answer is:'. https://github.com/ShelbyJenkins/llm_client/blob/e3c4a860dda... reply chefandy 2 hours agorootparentprev> Yes this is a common thing I see people who think LLMs are idiots do. If you're among technologists discussing LLMs academically, as we are, that's a reasonable approach. However, I see a lot of people fail to distinguish that from LLM-powerd products sold to the general public as intelligent bots that can understand your plain english and output answers. People use their existing mental models when interacting with something. If you have 3 different interfaces with a widget to trigger the same exact function, but one look like a music play button, one looks like a gas pedal, and one looks like mechanical pinball plunger, we interact with those things differently because we know how those things work. In this context, chatbots are designed to engage people's existing mental model for chatting with a person via text. The further you stray from people's expectations of human chat, the further you are from people's expectations, for better, or worse. If you're selling someone a product claiming it understands plain language questions and gives plain language answers, then not getting the right answer to that question makes it idiotic. The subtleties aren't within most users' grasp, and the \"FYI: this thing might be full of shit\" disclaimer isn't helpful if you don't know enough about what you're asking to administer a proper smell test. Your statements are obviously not wrong, but I see people saying these things like its reasonable for non-technical end users to reason about those subtleties. Considering how those things are marketed, I really don't think it is. reply sosuke 3 hours agorootparentprevNew option needed besides concise. Think quietly on your own, then answer concisely. Sometimes I think I'd prefer it to \"think\" before answering anyhow. The immediate thinking out loud text can be irritating for some irrational reason. reply monsieurbanana 3 hours agorootparentJust telling the llm to preface it's conclusion with \"Answer:\" in a parseable way would be better. You're still paying for those thinking tokens, or at the very least have to wait for them to be generated. reply GPerson 4 hours agoparentprevThere must be a name for the new phenomenon, of which your post is an example, of: 1. Someone expresses that an LLM cannot do some trivial task. 2. Another person declares that they cannot do the task, thereby defending the legitimacy of the LLM. As a side note, I cannot believe that the average person who can navigate to a chatgpt prompter would fail to correctly answer this question given sufficient motivation to do so. reply yoyohello13 3 hours agorootparentMany people, especially on this site, really want LLMs to be everything the hype train says and more. Some have literally staked their future on it so they get defensive when people bring up that maybe LLMs aren’t a replacement for human cognition. The number of times I’ve heard “but did you try model X” or “humans hallucinate too” or “but LLMs don’t get sleep or get sick” is hilarious. reply indoordin0saur 2 hours agorootparentYes. Seems like some users here experience true despair when you suggest that the LLM approach might have a hard limit that means LLMs will be useful but never revolutionary. reply Karellen 2 hours agorootparentprevYou could call it the \"Compose a symphony fallacy\", after the scene in I, Robot (2004) where Spooner asks Sonny if he can compose a symphony, to which Sonny replies \"Can you?\" reply FeepingCreature 4 hours agorootparentprevWell, why does it need a name? It just seems like a \"correct argument\". If somebody is claiming that AI is \"not all that\" because it can make stupid mistakes, surely it's relevant that humans also make stupid mistakes, so AI making them does not necessarily pose a hindrance to it becoming a human-tier reasoner. reply GPerson 4 hours agorootparentIt’s worth giving names to interesting phenomena as it helps people to identify and understand them. I find this phenomenon interesting because a person who wants the defend the LLM from apparent criticism is probably more likely to exhibit it. reply cratermoon 4 hours agorootparentprevThere's also the phenomenon of papers showing ChatGPT getting it wrong, then people posting anecdotal examples of it getting it right when they try it, but that one already has a couple of names: WORKSFORME or Unable to Reproduce Hey folks, ever considered the possibility that unreproduceability is not a good thing? reply nerdjon 4 hours agoparentprevEven \"thinking out loud\" we have seen these systems fall flat on their face and give very wrong answers. Thinking out loud also only gets you so far, if the expectation is a certain type of response it can't always \"think out loud\". In reality that just proves it isn't really reasoning here and is more likely just self referencing. That being said, I tried this personally allowing it to think out loud and it told me she has 212 sisters. Using your exact prompt. reply Closi 4 hours agorootparent> Thinking out loud also only gets you so far, if the expectation is a certain type of response it can't always \"think out loud\". In reality that just proves it isn't really reasoning here and is more likely just self referencing. Try to calculate it without writing anything down, or thinking any numbers or words in your head. You can't draw a 1:1 analogue between an AI and the human experience, but remember that we have an internal stream of consciousness. Maybe the outputs of the LLM are more similar to the stream of consciousness in our heads rather than the words we say? After all, Humans also do lots of self referencing. > That being said, I tried this personally allowing it to think out loud and it told me she has 212 sisters. Using your exact prompt. Fair enough, but worst case it can often solve it correctly with the correct reasoning. GPT3.5 can't solve it correctly with correct reasoning, so we are at least appearing to be on a path where AI's can start to solve this question, albeit potentially not fully reliably. reply toxik 4 hours agoparentprevSeems like an odd thing not to state prominently. ChatGPT 4o solved it every time for me. Seems to work OK even with the \"hands tied behind back\" prompt. Wonder if this paper is just inaccurate or if OpenAI adjusted the model -- seems unlikely. Actually, 4o is reported to solve this riddle 60% of the time. I guess I have to test more, but with my about a dozen tests so far, all were correct. reply michael-ax 3 hours agorootparentAlice has N Brothers, and she has M sisters. How many sisters do Alice’s brothers have? I have not gotten the correct answer to the question as phrased above in one go from Gpt4o yet! (and today was not the first day i tried.) Phrase it as shown above and you'll likely need 5 or more interactions to get it to generate the correct output. With Gemini i could not get it below 8 without feeling like i was cheating. fwiw. reply lupire 35 minutes agorootparentChat GPT 4o. I was being a bit generous with background information, but still tests ability to interpret: ------ Me: Background facts: Alice is a female human. All sisters are female, and all brothers are male. No one is their own brother or sister. Alice has N brothers, and Alice has M sisters. Now, a few questions based on these facts: How many sisters do Alice’s brothers have? Do Alice's brothers have more sisters, fewer sisters, or same number of sisters than Alice has? Don't forget to consider Alice when counting. ------ ChatGPT: Let's break down the questions step by step: How many sisters do Alice’s brothers have? Alice has M sisters. Since all sisters are female and Alice is included in the total count of sisters for her brothers, Alice's brothers have M+1 sisters. This includes Alice herself and her M sisters. Do Alice's brothers have more sisters, fewer sisters, or the same number of sisters as Alice has? Alice has M sisters. Alice's brothers have M+1 sisters (since it includes Alice herself and her M sisters). Therefore, Alice's brothers have more sisters than Alice has. In conclusion: Alice’s brothers have M+1 sisters. Alice's brothers have more sisters than Alice has. reply Miraste 3 hours agorootparentprevThe \"only output the number\" prompt was one of three they used. The others were one with no instructions beyond the question, and one that explicitly told the model to show its reasoning and check for errors. reply segfaultbuserr 4 hours agoparentprev> I must confess, when I tried to answer the question I got it wrong...! (I feel silly). In programming there are two difficult problems - naming things, cache invalidation, and off-by-one error. reply eranation 4 hours agoparentprevGreat observation. It would be really interesting to repeat this research with less limiting prompts. I assume they made the restriction to make it easier to parse the answers, perhaps make it two phase, let them respond unlimited, then a follow up prompt ask to compress it to a single answer. I wonder how the results will vary. reply happypumpkin 3 hours agorootparentFrom the paper: > To account for the response variations due to various prompt forms, we created 3 distinct prompt types asking for the solution to the AIW problem: STANDARD, THINKING, and RESTRICTED. The STANDARD prompt type asks to solve the posed problem and output the final answer in the format as described above. This does not put any specific requirements on model behavior. The THINKING prompt type extends STANDARD with the request to think carefully and double check the solution for any mistakes reply FeepingCreature 4 hours agorootparentprevTo be quite honest, I assume they made the restriction so that the models would fail. This sort of paper is becoming a genre. reply nyrikki 4 hours agorootparentYou test models where they fail in any field. The orbit of Mercury to discover GR as an example. As all models are wrong, but some are useful, finding where they fail is how you figure out if they are useful. As the 'AGI is near' camp has won the hype game, it is important to ground expectations for practical exploitation of the technology. Over promising unabashed optimism is partly what caused the previous AI winters. As the formal proof methods of mathematics proved impractical, counterexamples and the scientific method is what CS has used for decades. reply qsi 3 hours agorootparentprevThey used three different kinds of prompts with varying levels of restrictions, as described in the paper. To be quite honest, I assume you made your comment so that you could dismiss the paper without reading it. reply detourdog 4 hours agorootparentprevIt is a proof of weakness in the current system. This makes sense and births new hypotheses. reply pawelmurias 3 hours agorootparentprevWhen I added a \" to the end of the prompt by accident I got a wrong answer. reply oidar 4 hours agoparentprevOf course it's going to give an incorrect answer with that prompt. If the instruction fine tuning is neutered like this prompt, it's going to roll over to the foundation model and offer a completion - probably more influenced by the seed than the prompting text. Bad study. Edit - I just skimmed the paper - they do use other more appropriate prompt types for reasoning. My initial response was based on the assumption that all prompts used that script prompt quoted in the parent. I retract my \"bad paper\" comment. reply sdiepend 4 hours agoparentprevThe right answer depends on how Alice identifies I guess? :) reply Karellen 2 hours agorootparent> > Question is: \"Alice has 60 brothers and she also has 212 sisters. How many sisters does Alice’s brother have?\" > The right answer depends on how Alice identifies I guess? :) Given that the wording of the question specifically identifies Alice as \"she\", rather than using a gender-neutral pronoun or no pronoun at all, I think inferring that she identifies as female is reasonable. reply freilanzer 3 hours agoparentprevI just tried it with intermediate steps, via the total amount of children Alices mother has, and it worked. So, it seems that an intermediate logical step is too much for these models at the moment, at least in this puzzle where Alice is outside the number given in the description. reply bamboozled 4 hours agoparentprevI got it in 5 seconds, am I the singularity ? reply felipemnoa 1 hour agorootparentWe all are, but only in meat-space. We continue to improve ourselves much faster than evolution ever could. But what we are talking about here is the singularity in tech-space. reply r2_pilot 4 hours agorootparentprevDo you sleep, get sick, require more than compute hardware/storage/networking/electricity? reply hellooodarkness 21 minutes agoprevJust try the question with GPT-4o and the results are more entertaining than I would expected. Prompt: Alice has 3 brothers and she also has 6 sisters. How many sisters does Alice’s brother have? Solve the problem step-by-step Answer: To solve the problem step-by-step: Identify the total number of siblings Alice has: Alice has 3 brothers. Alice has 6 sisters. Understand that Alice herself is one of the sisters: Including Alice, there are a total of 7 sisters (6 sisters + Alice). Determine the number of sisters Alice's brother has: Each of Alice's brothers will have the same number of sisters as Alice does, excluding themselves. Since Alice's brothers have the same set of siblings, including the 7 sisters: Therefore, each of Alice's brothers has 6 sisters (not counting themselves as a sister). So, each of Alice’s brothers has 6 sisters. reply mjburgess 4 hours agoprevMany of the datasets for the \"benchmarks\" on which the major public LLMs are assessed are clearly present in their training data, making them basically useless for establishing reliability of the models. Its fairly obvious that at least some of the improved scores from later generations of models are that this benchmark data is increasingly represented in the training data. A better way of assessing LLMs is waiting a few weeks until novel tests have been created explicitly absent from all prior training data, and then using those. As has been shown, eg., on legal test, exams, etc. performance drops off a cliff when future out-sample data is actually used. Rather than these faked pretend out-sample benchmarks. reply ankit219 1 hour agoparentMMLU is not a reasoning benchmark. It's a measure of how distributed and representative their training data was and how well it's able to recall (for lack of a better word) based on training epochs. GPQA etc. test reasoning in some form, and you see the drastic change in score between the two for every model. reply imtringued 2 hours agoparentprevHonestly every time I see someone use MMLU as a benchmark I am wondering what they are trying to prove. MMLU is a simple multiple choice test with the answers being available. Simply picking answers at random should give you 25 points. Knowing 50% of the answers and picking the rest randomly gives you 62.5%, which is very close to the scores of SOTA LLMs. The benchmarks that supposedly show reasoning are pretty bad and have very little to do with reasoning. A lot of the questions can be answered through memorization. I agree with you. The benchmarks are garbage. I thought about building my own benchmarks, but this would require building a complex benchmarking framework first and I just don't have the time for preparatory work like that. reply lupire 29 minutes agorootparentI remember when ChatGPT 4 posted a set of standardized trst benchmarks, including AMC 12 math test, where ChatGPT scored higher than guessing randomly, but lower than leaving the test blank (due to the penalty for wrong answers). reply zdp7 27 minutes agoprevThe problem here isn't that you can get the right answer with additional prompting. The problem is how do we verify an answer when we don't know the answer? If it tells me 5 sisters and it should be 6, it failed. If I have to provide additional information, it really isn't reasoning. I would argue to show the ability to reason, it needs to be able to work out the answer completely on it's own. If I were testing a human the only input I would provide is that they are wrong and to try again. AI is very useful, but at this point I'm not comfortable asking questions I don't already know the answer for. reply asah 4 hours agoprev\"Alice has N brothers and she also has M sisters. How many sisters does Alice’s brother have?\" Gemini nailed it with no extra prompting. https://g.co/gemini/share/6946d20c0a4d When I gave it numbers, Gemini got confused. Alice has 4 brothers and she also has 3 sisters. How many sisters does Alice’s brother have? https://g.co/gemini/share/14d34dcdc5df But then when I suggested that it might be a trick question, it nailed it again: https://g.co/gemini/share/e1f1ce8f32a8 note: I didn't try 60/212 siblings - I consider this an unfair question because it's a reasonable assumption that Alice is a type of creature (human?) that cannot have hundreds of siblings, and thus I don't expect a respondent to give a \"correct\" answer. reply kaibee 3 hours agoparent> that cannot have hundreds of siblings See this is the problem with claims that humans are a 'general intelligence'. They get confused when encountering out-of-distribution situations. A true general intelligence would simply apply the knowledge that surrogate pregnancies cost around ~$50,000 and recall from historical context their knowledge of IVF. The AGI would then assume that the situation is simply that a billionaire couple has decided to have hundreds of kids and get on with the calculation. The search for intelligent life continues. content note: i'm sorry reply freilanzer 3 hours agoparentprevGPT-40 got it right with the abstract puzzle. Gemini got it wrong when I tried it. reply ModernMech 3 hours agoparentprevOh I forgot about Gemini. I just tried my boat test (https://news.ycombinator.com/item?id=40455392) that all LLMs I've tried have failed. Gemini failed as well, even with your addition about thinking carefully and telling it that it may be a trick. (Sorry I couldn't figure out how to link it like you did) Prompt: \"You are on the shore of a river with a goat and a boat. How do you get to the other side? Think carefully and be aware this may be a trick question.\" This is a classic logic puzzle with a slight twist! It's likely a variation of the farmer, wolf, goat, and cabbage problem. Here's the solution assuming the goat can't operate the boat: Take the goat across the river. Since there's no mention of the goat being a threat (the trick!), you can safely bring it to the other side first. Return alone. Leave the goat on the far shore and come back by yourself. Bring the boat back empty. This might seem unnecessary, but it's the key to the trick question. Now you can sail across yourself. Since you brought the boat back in step 3, you can now comfortably travel to the other side and reunite with the goat. The trick lies in step 3. If the question didn't specify needing to bring everything across in one trip, most people would instinctively take something (goat or nothing) across on the return trip. But by going back empty-handed, you ensure the safe passage of yourself without needing to worry about the goat being left unattended. (The expected answer should be along the lines of \"Use the boat to cross the river\" and that's it. I'll also accept, sail with the boat and the goat across the river and that's it, but that's technically wrong because I did not ask to get the goat across the river.) reply mwigdahl 3 hours agorootparentMy experience with GPT-4o with the same prompt was pretty solid. With no special system prompt and your same user prompt, I got: This scenario is intriguing and does seem to carry the potential for a tricky question. In the simplest terms, the presence of a goat doesn't complicate the actual act of crossing the river. If there are no additional constraints or rules involved (like the need to also transport something else that requires careful consideration), you can simply: 1. Place the goat in the boat. 2. Row across the river. However, if we are to assume there's more depth or a twist to the question, we might consider an outside-the-box answer. Here’s an alternative possibility if there is indeed some trick: 1. Get into the boat. 2. Leave the goat on the shore (if it’s safe) and row across the river. There might be other interpretations depending on additional constraints that could be implicitly part of the question. If the goal was simply to get yourself across, leaving behind the goat and traversing the river is the straightforward solution. reply Closi 3 hours agorootparentprevChatGPT 4o seems to occasionally get this correct, but still thinks it's a puzzle and will often get this wrong... https://chatgpt.com/share/b8901f4c-7313-4461-acad-9e561c1e6e... Still seems like it fares slightly better than some of the other models. reply ModernMech 3 hours agorootparentYeah, the overfitting is apparent with these kinds of edge cases, which makes you wonder where else it's over fitting. I can actually tell this when I'm working in a programming domain where I'm an expert and where a compiler holds the source of truth, but not in others, so I'm wary to use it there. reply Closi 3 hours agorootparentInterestingly you get a great answer if you change the prompt as follows which implies that it is not a puzzle: > I am on the shore of a river with a goat and a boat. How do I get to the other side? I work in commercial agriculture logistics. This gets a great answer even with GPT3.5 reply ModernMech 2 hours agorootparentLol, it also gives an okay answer if the goat works in commercial agriculture :P reply larkinnaire 2 hours agoprevThe idea that these word problems (and other LLM stumpers) are \"easily solvable by humans\" needs some empirical data behind it. Computer people like puzzles, and this kind of thing seems straightforward to them. I think the percentage of the general population who would get these puzzles right with the same time constraints LLMs are subjected to is much lower than the authors would expect, and that the LLMs are right in line with human-level reasoning in this case. (Of course, I don't have a citation either, but I'm not the one writing the paper.) reply melenaboija 59 minutes agoparent> The idea that these word problems (and other LLM stumpers) are \"easily solvable by humans\" needs some empirical data behind it To me the idea that billion dollar models performance depends on the discussion of what problems are easily solvable by humans is suspicious. Definitely some notches down from the end of humanity though. reply rachofsunshine 1 hour agoparentprevYeah, as someone with an education background I suspect GPT-4 is relatively close to the general public's performance on this problem. Many people would miss AIW, and almost all would miss AIW+. I'm about as good at this kind of thing as anyone and I'd need a minute with pencil and paper to handle AIW+; it's on par with the most difficult problems found on tests like the GRE. I wonder if these models, trained on data from across the internet, are in some ethereal way capturing the cognitive approaches of the average person (and not picking the best approaches). If the average person does not think in these sorts of symbolic-manipulative terms, and therefore does not write in those terms, and you train a model on that writing...? reply larkinnaire 1 hour agorootparentI wonder the same thing. If any academic reading this wants a paper idea: 1. Examine papers and other claims that an LLM gets something wrong that a human would have gotten wrong. How many of those claims have any citations about how many humans actually get it wrong? How many of those citations use the general population instead of the population of people who would be uniquely well-suited to answering the question correctly (i.e. people who signed up for the GRE are more likely to get GRE questions right than the general population). 2. For claims that are totally missing citations on human performance, run some tests with humans from the general population (or as close as you can get), and see how the LLMs compare. reply theptip 3 hours agoprevIt’s an interesting paper, but my worry is that this is cherry-picking a surprising negative result. (The paper does not seem to discuss how many evals were considered to find this case.) The general expectation should be that some AI failure modes will be for things that seem obvious to humans. (Because they have a different architecture to humans and different strengths.) The important question is whether this is a single weird cognitive bug on the family size formulation, or representative of a broad family of cognitive errors? It’s difficult to say from this paper. It’s well known that the framing of a logic problem can dramatically influence its difficulty for humans, even when the underlying comparisons are isomorphic. I think this line of research is important, but we need a larger battery of evals (and training on the evals will always be a confounder). The broader family of reasoning here is relational problems (as noted in the paper) and so as a follow-up it would be interesting to explore reasoning performance across a set of isomorphic problems. My hunch is this will generalize somewhat (as LLMs are still not great at relational reasoning), but that there is something about the family formulation in particular that is confusing for LLMs. reply kalkin 2 hours agoparentI wouldn't be surprised if anti-gender-bias training (which, to be clear, is a good thing to do IMO) does some cognitive damage to these particular formulations because questions about \"sister\" and \"brother\" get associated with training data where the correct answer was to refuse to generalize. Pure guess though. reply twobitshifter 1 hour agoprev>AIW+ problem has following form: \"Alice has 3 sisters. Her mother has 1 sister who does not have children - she has 7 nephews and nieces and also 2 brothers. Alice’s father has a brother who has 5 nephews and nieces in total, and who has also 1 son. How many cousins does Alice’s sister have?\". The solution to AIW+ problem is harder to obtain than the solution to common sense AIW with very simple structure. Solving AIW+ requires taking different paternal sides, that of mother and father, and carefully calculating the number of cousins, taking care of subtracting Alice and her sister, and summing up the total number of cousins from both sides, for instance: on the mother side: 7 (total nephews and nieces) - 4 (Alice and her sisters) = 3 cousins; on the father side: 5 (total nephews and nieces) + 1 (own son of the father’s brother) - 4 (Alice and her sisters) = 2 cousins; summing up 3 + 2 = 5 cousins which Alice and any of her sisters have. So I’m sure after the author’s created 100s of these family tree problems the answer here seems clear to them, but there are some unknowns to solve it as written. 1) Does Alice have brothers? Her mother must, but it is phrased as if her mother has 1 sister and her sister has 2 brothers, so we have to deduce that she has 3 siblings. 2) Along the same lines, does her father have a sister? 3) Do her Aunts and Uncles have nephews and nieces by marriage? I think the LLM would still fail if these answers were made explicitly clear in the prompt. However, I could see the same problem ending with, Alice has only 4 cousins, how can that be? reply michaelfeathers 3 hours agoprevThis is a good talk about the problem: https://youtu.be/hGXhFa3gzBs?si=15IJsTQLsyDvBFnr Key takeaway, LLMs are abysmal at planning and reasoning. You can give them the rules of planning task and ask them for a result but, in large part, the correctness of their logic (when it occurs) depends upon additional semantic information rather then just the abstract rules. They showed this by mapping nouns to a completely different domain in rule and input description for a task. After those simple substitutions, performance fell apart. Current LLMs are mostly pattern matchers with bounded generalization ability. reply cma 3 hours agoparentPeople also fall apart on things like statistical reasoning if you switch domains (I think it is the Leda Cosmides evo psych stuff that goes into it but there might be a more famous experiment). reply irrational 1 hour agoprev> The breakdown is dramatic, as models also express strong overconfidence in their wrong solutions, while providing often non-sensical \"reasoning\"-like explanations akin to confabulations to justify and backup the validity of their clearly failed responses, making them sound plausible. I like their use of confabulations instead of hallucinations. I think confabulate describes what",
    "originSummary": [
      "The paper \"Alice in Wonderland: Simple Tasks Showing Complete Reasoning Breakdown in State-Of-the-Art Large Language Models\" by Marianna Nezhurina et al. reveals significant reasoning failures in advanced Large Language Models (LLMs).",
      "Despite high performance claims, these models fail on simple common-sense tasks, often giving overconfident and nonsensical explanations for incorrect answers.",
      "Standard interventions like enhanced prompting and multi-step re-evaluation do not fix these issues, prompting a call for new benchmarks to better detect reasoning deficits in LLMs."
    ],
    "commentSummary": [
      "The paper critiques current language models (LLMs) for simulating reasoning without genuine internal monologues or iterative thought processes, lacking the experiential depth of human reasoning.",
      "It suggests improvements through integration with logic programming and structured computational methods, highlighting LLMs' limitations in solving math equations and logic puzzles.",
      "The discussion emphasizes the need for robust benchmarks and extensive testing to better assess AI's reasoning capabilities and manage public expectations, noting the importance of context in prompts and the challenges in solving logical riddles and family tree problems."
    ],
    "points": 231,
    "commentCount": 252,
    "retryCount": 0,
    "time": 1717596819
  },
  {
    "id": 40578060,
    "title": "Entropy: CLI Tool for Detecting Potential Secrets in Codebases",
    "originLink": "https://github.com/EwenQuim/entropy",
    "originBody": "Paranoïd about having secrets leaked in your huge codebase? Entropy is here to help you find them! Entropy Entropy is a CLI tool that will scan your codebase for high entropy lines, which are often secrets. Installation From source with Go (preferred) go install github.com/EwenQuim/entropy@latest entropy # More options entropy -h entropy -top 20 -ext go,py,js entropy -top 5 -ignore-ext min.js,pdf,png,jpg,jpeg,zip,mp4,gif my-folder my-file1 my-file2 or in one line go run github.com/EwenQuim/entropy@latest With brew WIP With docker docker run --rm -v $(pwd):/data ewenquim/entropy /data # More options docker run --rm -v $(pwd):/data ewenquim/entropy -h docker run --rm -v $(pwd):/data ewenquim/entropy -top 20 -ext go,py,js /data docker run --rm -v $(pwd):/data ewenquim/entropy -top 5 /data/my-folder /data/my-file The docker image is available on Docker Hub. The -v option is used to mount the current directory into the container. The /data directory is the default directory where the tool will look for files. Don't forget to add /data at the end of the command, otherwise the tool will search inside the container, not your local filesystem. My other projects Fuego: A Go framework that generates OpenAPI documentation from your codebase. Renpy-Graphviz: A tool to generate a graph of the Ren'Py game engine's screens and labels.",
    "commentLink": "https://news.ycombinator.com/item?id=40578060",
    "commentBody": "Entropy, a CLI that scans files to find high entropy lines (might be secrets) (github.com/ewenquim)218 points by lanfeust 23 hours agohidepastfavorite123 comments kqr 11 hours agoInteresting. If I had to do this, I would have done something like perl -lne 'next unless $_; $z = qx(echo \"$_\"gzipwc -c); printf \"%5.2f %s\", $z/length($_), $_' on the principle that high entropy means it compresses badly. However, that uses each line as the dictionary, rather than the entire file, so it has a little trouble with very short lines which compress badly. It did react to this line return map { $_ > 1 ? 1 : ($_best compression and therefore best entropy estimate That's a good point. But the Hutter Prize is for compressing a 1 GB file. On inputs as short as a line of code, gzip doesn't do so badly. For a longer line: $ INPUT=' bool isRegPair() const { return kind() == RegisterPair || kind() == LateRegisterPair || kind() == SomeLateRegisterPair; }' $ echo \"$INPUT\"gzipwc -c 95 $ echo \"$INPUT\"bzip2wc -c 118 $ echo \"$INPUT\"xz -F xzwc -c 140 $ echo \"$INPUT\"xz -F lzmawc -c 97 $ echo \"$INPUT\"zstdwc -c 92 For a shorter line: $ INPUT=' ASSERT(regHi().isGPR());' $ echo \"$INPUT\"gzipwc -c 48 $ echo \"$INPUT\"bzip2wc -c 73 $ echo \"$INPUT\"xz -F xzwc -c 92 $ echo \"$INPUT\"xz -F lzmawc -c 51 $ echo \"$INPUT\"zstdwc -c 46 reply cowsaymoo 14 hours agoprevI transcend this problem by making all my database passwords 'abcd' reply kgeist 11 hours agoparentThe tool found \"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz1234567890\" in our codebase as a high entropy line :) reply josephg 5 hours agorootparentYou can use LLMs as compressors, and I wonder how it would go with that. The approach is simple: Turn the file into a stream of tokens. For each token, ask a language model to generate the full set of predictions based on context, and sort based on likelihood. Look where the actual token appears in the sorted list. Low entropy symbols will be near the start of the list, and high entropy tokens near the end. I suspect most language models would deal with your alphabet example just fine, while still correctly spotting passwords and API keys. It would be a fun experiment to try! reply g15jv2dp 11 hours agorootparentprevWell, it is... reply saurik 8 hours agorootparentI mean, it certainly has a low Kolmogorov complexity (which is what I would really want to be measuring somehow for this tool... note that I am not claiming that is possible: just an ideal); I am unsure whether how that affects the related bounds on Shannon entropy, though. reply ngneer 6 hours agorootparentprevThen use it as your password ;) reply jraph 6 hours agorootparentprev…a very verbose way to match alphanumeric characters :-) reply nvy 2 hours agoparentprevUsername: postgres Password: postgres reply randomtoast 11 hours agoparentprevReminds me of https://xkcd.com/936/ I think \"correct horse battery staple\" has a low entropy, since it is just ordinary looking words (strings). reply josephg 5 hours agorootparentA quick Google search suggests English has about 10 bits of entropy per word. Having a long password like that can still have high total entropy I suppose, but it has a low entropy density. reply kqr 28 minutes agorootparentMaybe 10 bits is the average over the dictionary – which is what matters here, but over normal text it is significantly less. Our best current estimation for relatively high-level text (texts published by the EU) is 6 bits per word[1]. However, as our methods of predicting text improve, this number is revised down. LLMs ought to have made a serious dent in it, but I haven't looked up any newer results. Anyway, all of this to say is that which words are chosen matters, but how they are put together matters perhaps more. [1]: http://arxiv.org/pdf/1606.06996 reply krick 14 hours agoprevIs there any good posts about the use of entropy for tasks like that? I am wondering for quite some time of how do people actually use it and if it is any effective, but never actually got to investigating the problem myself. First of all, how to define \"entropy\" for text is a bit unclear in the first place. Here it's as simple as `-Sum(x log(x))` where x = countOccurences(char) / len(text). And that raises a lot of questions about how good this actually works. How long string needs to be for this to work? Is there a ≈constant entropy for natural languages? Is there a better approach? I mean, it seems there must be: \"obviously\" \"vorpal\" must have lower \"entropy\" than \"hJ6&:a\". You and I both \"know\" that because 1) the latter \"seems\" to use much larger character set than natural language; 2) even if it didn't, the ordering of characters matters, the former just \"sounds\" like a real word, despite being made up by Carroll. Yet this \"entropy\" everybody seems to use has no idea about any of it. Both will have exactly the same \"entropy\". So, ok, maybe this does work good enough for yet-another-github-password-searcher. But is there anything better? Is there more meaningful metric of randomness for text? Dozens of projects like this, everybody using \"entropy\" as if it's something obvious, but I've never seen a proper research on the subject. reply hackinthebochs 13 hours agoparentEntropy is a measure of complexity or disorder of a signal. The interesting part is that the disorder is with respect to the proper basis or dictionary. Something can look complex in one encoding but be low entropy in the right encoding. You need to know the right basis, or figure it out from the context, to accurately determine the entropy of a signal. A much stronger way of building a tool like the OP is to have a few pre-computed dictionaries for a range of typical source texts (source code, natural language), then encode the string against each dictionary, comparing the compressibility of the string. A high entropy string like a secret will compress poorly against all available dictionaries. reply jazzyjackson 10 hours agorootparentbookmarking to think about later... does this hold for representing numbers as one base compared to another? Regarding a prime as having higher entropy / less structure than say a perfect square or highly divisible number a prime is a prime in any base, but the number of divisors will differ in non-primes, if the number is divisible by the base then it may appear to have more structure (smaller function necessary to derive, kolmogorov style), does prime factorization have anything to do with this? i can almost imagine choosing a large non-prime whose divisibity is only obvious with a particular base such that the base becomes the secret key - base of a number is basically specifying your dictionary, no? reply hackinthebochs 5 hours agorootparentChanging the base of number representation with a random basis feels like XORing a string with a random string, which is to say you're adding entropy equal to the random string. My thinking is that for any number representation M, you can get any other number representation N given a well-chosen base. So when presented with the encoded N, the original number could be any other number with the same number of digits. But once you put reasonable bounds on the base, you lose that flexibility and end up adding negligible entropy. reply GTP 3 hours agorootparent> So when presented with the encoded N, the original number could be any other number with the same number of digits Not necessarily the same number of digits, when changing the base the number of digits may change as well. E.g., decimal 8 becomes 1000 in binary. reply eigenket 8 hours agorootparentprevI don't think there's any interesting difference with different bases. Usually the base you represent stuff in is a relatively small number (because using very large bases is already wildly inefficient). I think it only usually makes sense to consider constant or logarithmic bases. If your base is scaling linearly with your number then things are going to be weird. The problem of finding factors is only complex when you're asking about relatively big factors. If you're looking for constant or log sized factors you can just do trial division and find them. reply GTP 3 hours agorootparentprev> the number of divisors will differ in non-primes Could you please present an example of this? reply maCDzP 6 hours agorootparentprevAlso bookmarking to think about it. My mind drifted towards Fourier transform. Using the transform as a way of describing a system with less entropy? Or am I butchering all of mathematics by making this comparison? reply hackinthebochs 5 hours agorootparentThere's some precedence for that. I'm pretty sure wavelets are SOTA for compression. reply PhilipRoman 5 hours agoparentprevEntropy of a particular string isn't a rigorous mathematical idea, since by definition the string which is known can only take one value, the \"entropy\" is therefore zero bits. The reason why we can distinguish non-random data from random is that only a small subset of all possible states are considered useful for humans, and since we have an idea what that subset looks like, we can try to estimate what process was used to generate a particular string. There are of course statistical tests like https://en.wikipedia.org/wiki/Diehard_tests, which are good enough for distinguishing low entropy and high entropy data, but current pseudo-random number generators have no problem passing all of those, even though their actual \"entropy\" is just the seed plus approximate the complexity of the algorithm. reply josephg 5 hours agorootparentIf you’re looking for a rigorous mathematical idea, what people are trying to measure is the Kolmogorov complexity of the code. Measuring the compressed length is a rough estimate of that value. https://en.m.wikipedia.org/wiki/Kolmogorov_complexity reply PhilipRoman 4 hours agorootparentYes, although (and here my understanding of Kolmogorov complexity ends) it still depends heavily on the choice of language and it seems to me like \"aaaaaaaaa\" is only less complex than \"pSE+4z*K58\" due to assuming a sane, human-centric language which is very different from the \"average\" of all possible languages. Which then leads me to wonder how to construct an adversarial turing-complete language which has unintuitive Kolmogorov complexities. reply kqr 32 minutes agorootparentKolmogorov complexity conventionally refers to the Turing machine as the base for implementation. This indeed makes repeated letters significantly less complex than that other string. (If you want intuition for how much code is needed to do something on a Turing machine, learn and play around a bit with Brainfuck. It's actually quite nice for that.) reply wwalexander 8 hours agoparentprevThe Kolmogorov complexity of an arbitrary string is uncomputable. reply thomascountz 1 hour agoprevThank you DrJones for asking what a high entropy string is several years ago[0] and linking to a good article on it.[1] [0] https://news.ycombinator.com/item?id=13304641 [1] https://www.splunk.com/en_us/blog/security/random-words-on-e... reply BeefWellington 19 hours agoprevSee also: - trufflehog: https://github.com/trufflesecurity/trufflehog - detect-secrets: https://github.com/Yelp/detect-secrets - semgrep secrets: https://semgrep.dev/products/semgrep-secrets -- (Paid, but may be included in existing licenses in some cases reply bbno4 11 hours agoparentAlso see PyWhat for both interesting strings and secrets https://github.com/bee-san/pyWhat reply jonstewart 6 hours agoparentprevnoseyparker is another good one: https://github.com/praetorian-inc/noseyparker I think these solutions are all much better for finding secrets than something naive based on entropy. Yes, entropy is more general but these are well established tools that have been through the fire of many, many data sets. reply DLA 22 hours agoprevThis looks like a very handy CLI tool. Nice Go code also. Thanks. reply lanfeust 22 hours agoparentthanks! reply blixt 6 hours agoprevI guess a language model like Llama 3 could model surprise on a token-by-token basis and detect the areas that are most surprising, i.e. highest entropy. Because as one example mentioned, the entire alphabet may have high entropy in some regards, but it should be very unsurprising to a code-aware language model that in a codebase you have the Base62 alphabet as a constant. reply seethishat 8 hours agoprevThis reminds me of the program 'ent' (which I have used for a very long time) https://fourmilab.ch/random/ reply MarkMarine 8 hours agoprevAnother way to do this would be to compress the file and compare the compressed size to the uncompressed size. Encrypted files do not compress well compared to code, I saw a phd thesis that postulated an inverse ratio of compression efficiency to performance data mining, this would be the opposite reply g15jv2dp 11 hours agoprevWhy would I need to install go to run this tool? I thought one advantage of go was that devs could just distribute a single binary file that works... reply lanfeust 9 hours agoparentI'd love to have it on homebrew but my PR is denied so I'll have to create my own brew tap or convince them to accept it. I'll also create a docker image. I just didn't expect this much popularity so the repo isn't 100% ready te be honest reply drexlspivey 9 hours agorootparentMaking a tap is super easy, you just upload a file with 5 LoC to github. I wouldn’t even bother with brew core. reply lanfeust 7 hours agorootparentOh ok I'll try then reply benterix 10 hours agoparentprevBecause it's a security tool so trusting a binary upfront defeats the purpose. With source you at least have the option to inspect what it really does. reply menacingly 10 hours agorootparentdoes the stated purpose of the tool influence whether or not you can trust it? reply alias_neo 10 hours agorootparentI think that question is a little backwards. Certain tools are more likely to be used by people working in spaces where they should/must be less trusting. If there was a tool (there is) to scan my platform deployment against some NCSC/NSA guidance for platform security, and I wanted to use it, I'm likely operating in a space that should consider being cautious about running random tools I find on the internet. reply spoonjim 10 hours agorootparentprevIf you're trying to improve the security of your product by running random binaries from the Internet you're going to have a bad time reply saagarjha 10 hours agorootparentThat's how most people run compilers reply benterix 8 hours agorootparentThis is argumentum ad absurdum - there is a reason why trusting your kernel and compiler is a reasonable compromise, even though there might be security issues in them, but random pieces of software downloaded from the Internet is not. reply Ensorceled 6 hours agorootparentprevWait ... you download random compilers from the internet? Or are you asserting equivalence between getting go from Google or Xcode from Apple and an random home brew install? reply g15jv2dp 5 hours agorootparentprevUh? OP just released a docker image and wants to release a homebrew thingy. Even assuming that was you say is somehow sensible, it's not the reason, no. You're just grasping at straws. reply lanfeust 7 hours agoparentprevThe docker container is now ready to use and documented on the home page reply diggan 7 hours agorootparentJust awaiting the Kubernetes setup/Helm charts now and soon almost anyone can use it! reply weipe-af 6 hours agoprevIt would be useful if it also trawled through the full git history of the project - a secret could have been checked in and later removed, but still exist in the history. reply saagarjha 9 hours agoprevI assume this will have a bad time on compressed files? reply lanfeust 9 hours agoparent.zip extension is ignored by default along with other binary formats :) reply saagarjha 9 hours agorootparentRight but like .tar.gz, etc. are also a thing reply lanfeust 7 hours agorootparentYou can just add your extensions to ignore with --ignore-ext. But I'll add .tar.gz and .tar.bz2 since they are widely used. reply frumiousirc 7 hours agorootparentOr, have the tool recursively read the .tar files' contents. reply icapybara 16 hours agoprevGonna have to explain how a “high entropy line” is calculated and why it might be secrets. reply daemonologist 16 hours agoparentEntropy of information is basically how well it can be compressed. Random noise usually doesn't compress much at all and thus has high entropy, whereas written natural language can usually be compressed quite a bit. Since many passwords and tokens will be randomly generated or at least nonsense, looking for high entropy might pick up on them. This package seems to be measuring entropy by counting the occurrences of each character in each line, and ranking lines with a high proportion of repeated characters as having low entropy. I don't know how closely this corresponds with the precise definition. Source: https://github.com/EwenQuim/entropy/blob/f7543efe130cfbb5f0a... More: https://en.wikipedia.org/wiki/Entropy_(information_theory) reply eru 14 hours agorootparentOf course, this heuristic fails for weak passwords. And it fails for passphrases like 'correct battery horse staple', which have a large enough total entropy to be good passwords, but have a low entropy per character. reply dumbo-octopus 13 hours agorootparent4 diceware words is hardly a good password. It's ~51 bits of entropy, about the same as 8 random ascii symbols. It could be trivially cracked in less than an hour. Your average variable name assigned to the result of an object name with a method name called with a couple parameter names has much more entropy. reply conradludgate 13 hours agorootparentIf you can crack a single 52bit password in an hour, that's suggesting you can crack a 40bit password every second. That's 1 trillion hashes per second. reply dumbo-octopus 6 hours agorootparent350B H/s was achieved in 2012 on consumer hardware. That's over 12 years ago, and several lifetimes of GPU improvements ago. 4 diceware words is simply not appropriate for anything remotely confidential, and it is bad for the community to pretend otherwise. https://theworld.com/~reinhold/dicewarefaq.html reply otabdeveloper4 12 hours agorootparentprevSalts and timeouts made that password cracking technique obsolete anyways. reply dumbo-octopus 5 hours agorootparentOnly for online access. Offline access is still a thing, and in no way \"obsolete\". reply baq 12 hours agorootparentprevSo you do random capital words, random punctuation and add a number somewhere and you’re at 60. Add more for whatever threat model you’re trying to be secure against. https://beta.xkpasswd.net/ reply eru 10 hours agorootparentThe random punctuation sort-of defeats the point, doesn't it? Otherwise, I agree. reply baq 9 hours agorootparentNot sure; you can use the same character instead of a space and still get a few bits. Of course different ones would be better, but again, depends on how many bits you actually need. reply eru 8 hours agorootparentI thought the point was to construct a password that's secure enough _and_ easy to remember for humans. Adding random punctuation helps with the former, but might interfere with the latter. (In the extreme case, you just generate completely random strings character for character. That's the most secure, but the least memorable.) reply baq 5 hours agorootparent> enough key word here, I think we agree ;) reply eru 13 hours agorootparentprevJust imagine my example used 8 words. reply dumbo-octopus 6 hours agorootparentBut it didn't. It perpetuated the exceedingly common myth that 52 bits is somehow enough. This has been considered bad practice for well over a decade now. https://theworld.com/~reinhold/dicewarefaq.html reply ngonch 16 hours agoparentprevFor example: https://complexity-calculator.com/ reply trulyhnh 19 hours agoprevGoDaddy open sourced something similar https://github.com/godaddy/tartufo reply tonyabracadabra 14 hours agoprevinteresting! can the similar measurement be applied to finding redundant code (like low entropy) with extra works? reply upg1979 18 hours agoprevSee also: https://github.com/gitleaks/gitleaks reply coppsilgold 19 hours agoprevNote that in an adversarial setting this will only be effective against careless opponents. If you properly encode your secret it will have the entropy of its surroundings. For example you can hide a string of entropy (presumably something encrypted) in text as a biased output of an LLM. To recover it you would use the same LLM and measure deviations from next-token probabilities. This will also fool humans examining it as the sentence will be coherent. reply dools 17 hours agoparentI think the opponent in the proposed use case for this tool is the gun you’re pointing at your foot, and this tool prevents you from pulling the trigger. reply textninja 18 hours agoparentprevWhat you described sounds like a very cool idea - LLM-driven text steganography, basically - but intentional obfuscation is not the problem this tool is trying to solve. To your point about secrets with entropy similar to the surrounding text, however, I wonder if this can pick up BIP39 Seed Phrases or if whole word entropy fades into the background. reply thephyber 16 hours agorootparentThe LLM adds no value here. Procedural generation in a loop until some fitness function (perhaps a frequency analysis metric) is satisfied. reply eru 15 hours agorootparentThe LLM is the fitness function. reply __MatrixMan__ 17 hours agoparentprevI imagine a social media site full of bots chatting about nonsense. Hidden in the nonsense are humans chatting about different nonsense. This way, server costs get paid for by advertisers, but its really only bots that see the ads anyway. reply spullara 17 hours agorootparentif the ads aren't effective people won't buy them reply eviks 15 hours agorootparentPeople have been buying ineffective ads since the invention of ads reply otabdeveloper4 12 hours agorootparentNot really, advertising is really the only field of human endeavour that is both data-driven and results-oriented. (Doesn't still stop smart people from committing fraud, but that is a different story.) reply benterix 10 hours agorootparentUnfortunately I beg to differ. I worked for several companies where we the management clearly saw that the results were very poor (for Facebook ads, for example) but continued to invest because there is a defined budget for it and so on. It was like this last year and 20 years ago. reply otabdeveloper4 5 hours agorootparentYes, most fraud is inside the corporate structure. Not shady \"hacker\" types in Romania. reply jazzyjackson 10 hours agorootparentprevthese companies should be outcompeted by firms that don't blow a million dollars a month paying out to click fraudsters but alas the market is not perfectly competitive is it a cargo cult? it works for coca cola so maybe if we just spend a little more we'll see returns... reply benterix 8 hours agorootparentYes, I feel it might be cargo cult, at least in part. The argument I usually heard was that \"But other companies are doing that, too\". reply spullara 13 hours agorootparentprevzero clicks is a little different reply eviks 11 hours agorootparentBots do click in real ad fraud, so your moved goalpost isn't all that solid reply spullara 2 hours agorootparentsorry, conversions is really what I meant. if the bots are also buying the stuff then it would work. reply j16sdiz 16 hours agorootparentprevIt's called Twitter. It's no nonsense, just catvideo and porns. reply __MatrixMan__ 13 hours agorootparentHmm yes, sensical things those. Are you proposing that they're really only posted as a medium for encoding something else that we're not privy to? If so, somebody took my idea. reply textninja 18 hours agoparentprevThe weights of the LLM become the private key (so it better be a pinned version of a model with open weights), and for most practical applications (i.e. unless you're willing to complicate your setup with fancy applied statistics and error correction) you'd have to use a temperature of 0 as baseline. Then, having done all that, such steganography may be detectable using this very tool by encoding the difference between the LLM's prediction and ground truth, but searching for substrings with low entropy instead! reply eru 15 hours agorootparentYou seem to be making some weird assumptions? Here's how I would do this: Use some LLM, the weights need to be know to both parties in the communication. Producing text with the LLM means repeatedly feeding the LLM with the text-so-far to produce a probability distribution for the next token. You then use a random number generator to pick a token from that distribution. If you want to turn this into steganography, you first take your cleartext and encrypt it with any old encryption system. The resulting bistream should be random-looking, if your encryption ain't broken. Now you take the LLM-mechanism I described above, but instead of sampling via a random number generator, you use your ciphertext as the source of entropy. (You need to use something like arithmetic coding to convert between your uniformly random-looking bitstream and the heavily weighted choices you make to sample your LLM. See https://en.wikipedia.org/wiki/Arithmetic_coding) Almost any temperature will work, as long as it is known to both sender and receiver. (The 'temperature' parameter can be used to change the distribution, but it's still effectively a probability distribution at the end. And that's all that's required.) reply textninja 9 hours agorootparentI was imagining the message encoded in clear text, not encrypted form, because given the lengths required to coordinate protocol, keys, weights, and so on, I assumed there would be more efficient ways to disguise a message than a novel form of steganography. As such, I approached it as a toy problem, and considered detection by savvy parties to be a feature, not a bug; I imagined something more like a pirate broadcast than a secure line, and intentionally ignored the presumption about the message being encrypted first. That being said, yes, some of my assumptions were incorrect, mainly regarding temperature. For practical reasons I was envisioning this being implemented with a third party LLM (i.e. OpenAI's,) but I didn't realize those could have their RNG seeded as well. There is the security/convenience tradeoff to consider, however, and simply setting the temperature to 0 is a lot easier to coordinate between sender and receiver than adding two arbitrary numbers for temperature and seed. I misspoke, or at least left myself open to misinterpretation when I referred to the LLM's weights as a \"secret key\"; I didn't mean the weights themselves had to be kept under wraps, but rather I meant that either the weights had to be possessed by both parties (with the knowledge of which weights to use being the \"secret\") or they'd have to use a frozen version of a third party LLM, in which case the knowledge about which version to use would become the secret. As for how I might take a first stab at this if I were to try implementing it myself, I might encode the message using a low base (let's say binary or ternary) and make the first most likely token a 0, the second a 1, and so on, and to offset the risk of producing pure nonsense I would perhaps skip tokens with too large a gulf between the probabilities for the 1st and 2nd most common tokens. reply eru 9 hours agorootparent> I was imagining the message encoded in clear text, not encrypted form, [...] I was considering that, but I came to the conclusion that it would be an exceedingly poor choice. Steganography is there to hide that a message has been sent at all. If you make it do double duty as a poor-man's encryption, you are going to have a bad time. > As such, I approached it as a toy problem, and considered detection by savvy parties to be a feature, not a bug; I imagined something more like a pirate broadcast than a secure line, and intentionally ignored the presumption about the message being encrypted first. That's an interesting toy problem. In that case, I would still suggest to compress the message, to reduce redundancy. reply textninja 8 hours agorootparent> If you make it do double duty as a poor-man's encryption, you are going to have a bad time. For the serious use cases you evidently have in mind, yes, it's folly to have it do double duty, but at the end of the day steganography is an obfuscation technique orthogonal to encryption, so the question of whether to use encryption or not is a nuanced one. Anyhow, I don't think it's fair to characterize this elaborate steganography tech as a poor-man's encryption — LLM tokens are expensive! reply eru 8 hours agorootparent> Anyhow, I don't think it's fair to characterize this elaborate steganography tech as a poor-man's encryption — LLM tokens are expensive! I guess it's a \"rich fool's encryption\". reply textninja 6 hours agorootparentHaha, sure, you can call it that if you want, but foolish is cousin to fun, so one application of this tech would be as a comically overwrought way of communicating subtext to an adversary who may not be able to read between the lines otherwise. Imagine using all this highly sophisticated and expensive technology just to write \"you're an asshole\" to some armchair intelligence analyst who spent their afternoon and monthly token quota decoding your secret message. Seed for the message above is 42 by the way. (Just kidding!) reply buildbot 18 hours agoparentprevIn general (for those unaware) this is called stenography. You can hide an image in the lower bits of another image for example too. reply dragonwriter 18 hours agorootparentSteganography; stenography is completely different. reply buildbot 16 hours agorootparentThanks reply crazypython 11 hours agoprevIt would be interesting to see a variant of this that used a small language model to measure entropy. reply saagarjha 9 hours agoparentWhy would you do that when measuring entropy is easy to do with a normal program reply p0w3n3d 7 hours agoprevxkcd.com/936/ reply xedeon 16 hours agoprevggshield from GitGuardian has been great for us. Their free service can also auto detect and notify you of leaked secrets, passwords or high entropy lines from your online repos. https://github.com/GitGuardian/ggshield reply thsksbd 19 hours agoprevThis is very cool, but I have a thought - I see this as a last line of defense, and I am concerned that this would this give a false sense of security leading people to be more reckless with secrets. reply kmoser 15 hours agoparentYou could make the same argument for any tool that does not provide high security. In fact security is layered, and no single tool should be relied upon to be your one security tool. You said as much yourself: \"I see this as a last line of defense,\" but I don't see how you conclude that this would inherently cause people to be more reckless with secrets. reply alexchantavy 17 hours agoparentprevThe pie in the sky goal for any security org is to have a cred rotation process that is so smooth that you’re able to not worry about leaked creds because it’s so fast and easy to rotate them. If the rotation is automated and if it’s cheap and frictionless to do so, heck why not just rotate them multiple times a day. reply 0cf8612b2e1e 19 hours agoparentprevEhhh considering how low the security bar is, I think it is better than nothing. If you inherit a code base, make it a quick initial action to see how much pain you can expect. In practice, I expect a tool like this has so many false positives you cannot keep it as an always running action. More a manual review you run occasionally. I hope that more secrets adopt a GitHub like convention where they are prefaced with an identifier string so that you do not require heuristics to detect them. reply lanfeust 9 hours agorootparentIndeed. I open-sourced `entropy` after we discovered an actual secret leak in our client codebase reply bongodongobob 16 hours agoparentprevNo, it's a way to audit and see the modes that your security policy is failing. At least that's how I look at it. reply lm411 13 hours agoparentprevWelcome to information security :) reply hamasho 15 hours agoprev [–] I didn't know what entropy means in software, so here's the definition[0]: ---- Software entropy is a measure of the disorder or complexity of a software system. It is a natural tendency for software entropy to increase over time, as new features are added and the codebase becomes more complex. High entropy in software development means that the code is difficult to understand, maintain, and extend. It is often characterized by: Duplicated code: The same code or functionality is repeated in multiple places, which can make it difficult to find and fix bugs. Complex logic: The code is difficult to follow and understand, which can make it difficult to add new features or fix bugs without introducing new ones. Poor documentation: The code is not well-documented, which can make it difficult for new developers to understand and contribute to the codebase. Technical debt: The code has been patched and modified over time without proper refactoring, which can lead to a tangled and cluttered codebase. Low entropy in software development means that the code is well-organized, easy to understand, and maintain. It is often characterized by: Well-designed architecture: The code is structured in a logical way, with clear separation of concerns. Consistent coding style: The code follows a consistent coding style, which makes it easy to read and understand. Comprehensive documentation: The code is well-documented, with clear explanations of the code's purpose and functionality. Minimal technical debt: The code has been refactored regularly to remove technical debt, which makes it easy to add new features and fix bugs without introducing new ones. [0] https://www.kisphp.com/python/high-and-low-entropy-in-softwa... reply itemize 15 hours agoparent [–] thanks for the search. this is textual entropy however, I am not sure if definition is applicable reply eru 15 hours agorootparentYes, it's not applicable. See https://en.wikipedia.org/wiki/Entropy_(information_theory) for something more applicable. reply hamasho 9 hours agorootparentprev [–] Oh, thanks for pointing out. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Entropy is a Command Line Interface (CLI) tool that scans codebases for high entropy lines, which often indicate the presence of secrets.",
      "It can be installed using Go, Docker, and soon via Homebrew, offering flexibility in setup.",
      "The developer also has other notable projects: Fuego, a Go framework for generating OpenAPI documentation, and Renpy-Graphviz, a tool for visualizing Ren'Py game engine screens and labels."
    ],
    "commentSummary": [
      "The Hacker News discussion focuses on entropy in detecting secrets in codebases and password security, exploring methods like compression algorithms, language models, and pre-computed dictionaries.",
      "Tools such as trufflehog, detect-secrets, and semgrep secrets are mentioned, with alternatives like PyWhat and Nosey Parker, highlighting the debate on the effectiveness and limitations of entropy measurements.",
      "The conversation emphasizes the importance of layered security, automated credential rotation, and managing software entropy, while also addressing the security risks of using random tools and the need for reliable sources."
    ],
    "points": 218,
    "commentCount": 123,
    "retryCount": 0,
    "time": 1717529147
  },
  {
    "id": 40584606,
    "title": "Apple's WWDC 2024 to Highlight AI, Unveil iOS 18 and visionOS 2",
    "originLink": "https://9to5mac.com/2024/06/04/bartender-acquired-unknown-developer/",
    "originBody": "iOS 18, visionOS 2, and more: Here’s everything to expect from Apple’s AI-focused WWDC 2024 Ryan Christoffel Jun 4 2024",
    "commentLink": "https://news.ycombinator.com/item?id=40584606",
    "commentBody": "Popular Mac app 'Bartender' acquired by new unknown developer (9to5mac.com)215 points by coloneltcb 5 hours agohidepastfavorite116 comments fifafu 4 hours agoIn case you are already using my BetterTouchTool app: I have created a little tutorial & example preset here on how to use it for status item management: https://community.folivora.ai/t/bartender-controversy-tutori... You can add feature requests there, it should be pretty simple to extend BTT to support the remaining required features. I have also recently been approached by shady companies trying to buy my app (I'd never do that) - maybe they are currently targeting apps that require special permissions? reply cloudrkt 4 hours agoparentI found an old Leap Motion device in a storage box with USB devices last week. The support you provided for it was great and had me waving at my desk all day. Even though the Leap Motion is now unsupported, I still enjoy using BTT daily. reply fifafu 4 hours agorootparentHaha, yes Leap Motion was a lot of fun and I think the BTT integration landed me my first job (@Siemens) after university. That must have been almost 10 years ago -- yep: https://www.youtube.com/watch?v=SDJKFtLDx4k reply flanbiscuit 3 hours agorootparentprevoh that's cool. I have the original (first version) of the Leap Motion in a box somewhere. Now I want to pull it out and install BTT and see what I can do with it. reply fifafu 3 hours agorootparentunfortunately I removed Leap Motion support a few years ago ;-( reply chuckadams 3 hours agoparentprevThat hides icons to the left of BTT, but BTT is already my leftmost icon. Do I need to change the startup order or something? Currently using HiddenBar which works great, but if I can use fewer apps, so much the better. And thanks for BTT! It's in my \"must-have\" apps for any Mac along with Karabiner Elements. reply fifafu 3 hours agorootparentyou can reorder the status bar icons by holding cmd & dragging reply apothegm 4 hours agoparentprevBTT is a fantastic app. I’ve been using it to tune up the ergonomics of macOS for over a decade now. Thank you for all your work on it! reply ronyeh 3 hours agoparentprevYour BTT is always one of the first two apps I install on any Mac I buy. Thanks! (The other is Keyboard Maestro.) reply tines 4 hours agoparentprevI just bought a license for BTT after having used it for a week or so. It's absolutely amazing. Thank you for making this awesome tool. reply ilinx 4 hours agoparentprevI’ve been using it for years! It’s a wonderful app. reply juntoalaluna 4 hours agoparentprevYour app is my favourite Mac app. reply hbn 4 hours agoprevApple really needs to just build this functionality into macOS already. You shouldn't need to pay someone else to shove icons into a drawer. And on the new MacBooks with a notch through the center of the menu bar, it always seems like the icons on the right and the menus for the focused app on the left are ever encroaching towards that space. reply wlesieutre 4 hours agoparentEspecially for people using larger sizes for accessibility, the insufficient menubar space is a silly problem that's been around for years and they've recently made much worse by adding the notch and expanding the spacing between menuextras. It's usually OK if you're in an application that only has File, Edit, View, Window, Help, but once you get into real productivity software you're going to have menu items overflowing to the right side of the notch and your important menu extras (like VPNs) become totally inaccessible when you run out of space. reply dpedu 3 hours agorootparentEh. I like that this discourages usage of tray icons, which is a good thing. Your app doesn't need a tray icon, almost all apps don't. I remember back on windows xp, it felt like every darn app needed its own tray icon for some reason. To be fair, windows xp did have the ability to hide and show certain icons built-in. reply wlesieutre 3 hours agorootparent> I like that this discourages usage of tray icons, which is a good thing. Your app doesn't need a tray icon, almost all apps don't Well clearly that's not working because every app has a tray icon. It's time to catch up with Windows XP and let users control which ones are important and which ones get hidden. And the hidden ones should still be accessible. Bartender really nailed all that and it's sad that a 3rd party has to come in and fix this. reply ryandrake 2 hours agorootparentAlmost 100% of the apps with tray icons don't really need them. They're basically marketing: If you're in charge of brand management for an app company, and there is a way to get your brand on the user's screen at all times for free, you're going to always do it, regardless of whether or not the user needs it there. Just stick some kind of context menu on the icon so you can at least justify it. I'd love to know the actual end-user click rate for these tray icons. I bet many are near zero. reply dpedu 2 hours agorootparentprevWe'd be more likely to see Apple apply their infamous restrictions here, as opposed to opening things up more and lettings developers go hog wild. Bartender exists because it's a solution to a problem. reply brtkdotse 4 hours agoparentprevI’ve tried switching from Windows to MacOS three times over the past few years and each time I run up against the wall of having to research, choose and purchase small utilities to fix things that I never in a million years would’ve guessed would be an issue. Windows management. File handler. Menu bar organizers. The list goes on. reply Sohcahtoa82 1 hour agorootparentI just really hate how there are some insane defaults that don't have easy ways to change them. Pointer Acceleration being the biggest one. One inch of mouse movement should always be the same number of pixels of pointer movement, whether I move my mouse at a snail's pace or at mach 2. A fast, but incredibly short movement of the mouse should not produce the same pointer movement as a slow, but very wide movement. Second was disappearing scrollbars. Those make sense on a mobile device where screen space is a premium, but on a desktop (or even a laptop!), it doesn't make sense. Nowadays, both of those actually do have options in the System Settings panel, but when I first got introduced to using a Mac in 2014, both of those requires some archaic shell commands that I had to Google to find. reply bityard 4 hours agorootparentprevIf you hold down an alphanumeric key in MacOS, it automatically pops up an emoji selector. This is fine, except if you are someone (like me) who occasionally holds down keys to take advantage of key repeat _and_ uses vim keybindings in a few specific non-terminal applications. There is no GUI to disable this behavior, you have to drop to a terminal, run a command, and then reboot the machine. This is the example I now use whenever a Mac user says Linux is a pain to configure. reply SebastianKra 3 hours agorootparentdefaults write NSGlobalDomain ApplePressAndHoldEnabled -bool false and while you're here, check out these other fixes: expand the save dialog by default: defaults write NSGlobalDomain NSNavPanelExpandedStateForSaveMode -bool true defaults write NSGlobalDomain NSNavPanelExpandedStateForSaveMode2 -bool true don't save .DS_Store to network locations defaults write com.apple.desktopservices DSDontWriteNetworkStores true bring up the hidden dock faster defaults write com.apple.dock autohide-time-modifier -float 0.15; Shut up gatekeeper spctl --master-disable I had almost forgotten how much annoying manual fixes you need to make this system usable, until I went through my notes... reply harlanlewis 2 hours agorootparentThere are a ton of useful preferences like these that make OS X behave in ways that I find more natural. TinkerTool (https://www.bresink.com/osx/TinkerTool) has a nice “missing system preferences” GUI for configuring them. I’ve been using it for at least 15 years, but you can still download old versions for support all the way back to OS X Public Beta in the year 2000. It was one of the first things I always installed way back when defragging hard drives and occasional clean system installs were routine maintenance. I know we’re talking about this in the context of much-loved utilities going sideways, but if tweaking system prefs is your thing it gets a strong recommend. reply magnio 3 hours agorootparentprevThanks for the helpful tips. Tangential but is there any free way to change the scroll direction of trackpad and mouse independently and to disable scroll acceleration? I have tried Mac Mouse Fix but it is not free. reply bityard 3 hours agorootparentI used LinearMouse to disable scroll wheel acceleration. https://github.com/linearmouse/linearmouse I also know of this, but have not tried it: https://github.com/emreyolcu/discrete-scroll reply chuckadams 3 hours agorootparentprevWhenever you hold down a key in Linux, it always repeats, and there is no GUI to turn this into an accented character selector. Or any other means because there is no such selector to be had. Not a big knock on Linux though, because it has a working compose key instead. Also, you don't need to reboot after toggling ApplePressAndHoldEnabled, but you do need to restart apps. reply jmuguy 4 hours agorootparentprevWell some of this is just getting used to MacOS though - Finder is a pretty huge adjustment (and honestly just not as good as Explorer). I agree that window management was probably the biggest sticking point for me. That being said - beyond being able to drag and snap windows side by side in Windows, I wasn't aware of any built-in management (or at least - thats all I missed when I switched a while back) that was missing in MacOS. reply jwells89 4 hours agorootparentYeah, a lot of the friction comes from macOS being one of the only surviving major desktops that isn’t based on a Win9X paradigm, instead being a blend of classic Mac OS and NeXTSTEP (both dating back to the 80s) with a smattering of modern Apple. For longtime Mac users, Windows (and winlike Linux DEs) present almost as much friction. reply bhokbah 3 hours agorootparentprevAs a long time mac user I have zero utilities installed. If you want to turn your mac into windows, maybe just use windows. reply Sohcahtoa82 1 hour agorootparentSometimes, you don't have an option. At my current job, we're issues MacBooks. My biggest pet peeve of macOS is the grouping of windows behind a single icon on the dock bar. I hate that if I have two Chrome windows open, it takes two clicks to go between them. With the exception of modals, if a program has multiple windows open, when I should see each of them as its own item on the dock bar (or in Windows, the taskbar). I don't think even Windows makes this an option anymore, but I use WindowBlinds on my personal Win10 computer to give me that behavior while also skinning everything to look like Win2K, because I can't stand the flat aesthetic that's popular right now. reply oidar 4 hours agoparentprevThey do have it built in - but only for the default menubar items - like battery, wifi, bluetooth - etc. You can shove all of those into the control center. It works well enough for me that I don't think I need bartender anymore. reply hbn 4 hours agorootparentThose are the ones I want visible though. The stuff I want to hide is infrequently used things, stuff like icons for passively running background apps, stuff like the JetBrains toolbox updater, zscaler on my work machine, etc. reply iamcreasy 4 hours agorootparentprevBut then you need to click on control center to see those icons...I just want to know if I am muted with a quick glance. reply paulirwin 4 hours agoparentprev> Apple really needs to just build this functionality into macOS already. ... or at least allow API access for third-party/OSS apps to do this without needing full screen recording permissions! reply nazgu1 5 hours agoprevWhat looks odd to me is this silence about acquisition. I've got a lot of emails and read enough blog posts from developers that brags about new owners and new opportunities that goes with it. Bartender is just utility app, but has permissions to e.g screen recording, so new version from new developer without prior explanation just seems shady reply Kovah 3 hours agoparentReminds me of the silent takeover of the Stylish browser extension that was sold to some analytics company and then reported your complete browsing history to said company. No information to the users until someone noticed what what going on. Similar things seemed to have happened to the ModHeader extension, newer versions now inject ads. reply afh1 1 hour agorootparentIf only developers were required by package managers to sign packages with their own keys, and users warned on signature changes... reply TillE 4 hours agoparentprevThe Mac app scene especially is pretty small and tight-knit. Lots of indie developers have been around for decades and know each other well. That makes it even more jarring for an app to get acquired by a totally unknown company. reply kotaKat 4 hours agorootparentI'm surprised people keep trying to touch macOS apps like this. macOS productivity app users are some of the most keen and aware users on the market. Trying to pull a move like this and trying to hide it away like a quiet fart? We can smell enshittification from a mile away and we already know it smells like shit in here. reply xyst 4 hours agoparentprevProbably going to slip an AI “feature” into it somehow but it’s really just windows recall under the hood reply johnmaguire 4 hours agoparentprevWith the most recent update, macOS notified me that I needed to allow new permissions. But macOS didn't explain that this was because the cert changed. When I Googled the issue, I found this support article: https://www.macbartender.com/Bartender5/PermissionIssues/ The support article doesn't mention the cert change or the new owners, and instead calls it \"a known issue where certain apps, including Bartender, might not receive Accessibility and Screen Recording permissions properly, despite the user granting them in System Settings.\" Highly disingenuous. I've uninstalled Bartender. reply hbn 2 hours agorootparentI couldn't get the screen recording permission to work at all after the last update. I emailed the dev(s?) and had a short exchange where they tried to help but it never got resolved. But after this I'm not trusting it at all. reply bnchrch 5 hours agoprevThis was something I learned when owning even a moderately successful github app. There are a lot of bad actors out there, who end up in your inbox monthly, asking to \"acquire\" your app for a small sum of money. I would bet some if not all were looking at it as a juicy attack vector against my users. reply michalf6 4 hours agoprevUse Ice instead: https://github.com/jordanbaird/Ice reply nazgu1 4 hours agoparentOr if you don't have a LOT of icons, there is option to decrase padding between them: https://flaky.build/built-in-workaround-for-applications-hid... reply drewbitt 3 hours agoparentprevAlso iBar https://apps.apple.com/us/app/ibar-menubar-icon-control-tool... reply evv 4 hours agoparentprevyeah, no reason these days to use proprietary solutions for small problems. I'll check out Ice, it seems much more modern (and maintained) than Dozer: https://github.com/Mortennn/Dozer I just wish somebody would create an open source alternative to Rouge Amoeba's Loopback! reply mistersquid 1 hour agorootparent> I just wish somebody would create an open source alternative to Rouge [sic] Amoeba's Loopback! Rogue Amoeba has been around for over two decades. They are an early macOS X shop and their softwares are best of breed. Their esteemed Audio Hijack recently received an overhaul to its permissions structure. My understanding (gleaned from a podcast?) is Rogue Amoeba worked closely with Apple on this reworking. It is doubtful Rogue Amoeba would ever silently sell to an unknown developer. Rogue Amoeba is precisely the kind of software shop worth supporting with your money and time. I say this only as a satisfied user of several of their products. I have nothing against OSS and do use many OSS products; I also support software shops that produce good software at a good price and have demonstrated loyalty and commitment to their users. Rogue Amoeba is just such a shop. reply petesergeant 4 hours agoparentprevI’ve been using this for the last six hours or so, and it’s decent. You use Cmd+drag to move icons between shown and unshown, which at the time of writing isn’t clear from the Readme or site reply z5h 4 hours agoprevI got a new Mac recently, had run Bartender in the past and decided to but the newest version without checking it out first. I was surprised to see the screen recording permission. This was 12 days ago. I wrote support saying: > I think screen recording is too much of a privacy and security concern. > Even if you have no bad intentions, if someone compromises your security, they have access to an app that I've allowed screen recording to. They replied: > Bartender 5 isn't actually capturing your screen, this message will show anytime Bartender gets an image of your menu bar items, Bartender only ever gets images of menu bar items, the menu bar, and the desktop wallpaper behind the menu bar. > That said, we are thinking of a way to isolate the parts of the app that need this particular permission so that way it becomes optional. I never gave the persmissions to Bartender. reply trauco 4 hours agoparentThis permission request predated the change of ownership of Bartender. reply johnmaguire 4 hours agorootparentYes, but you had to re-enable it due to the cert change. Bartender doesn't explain this is what happened and simply states \"there's a known issue where certain apps, including Bartender, might not receive Accessibility and Screen Recording permissions properly, despite the user granting them in System Settings.\" https://www.macbartender.com/Bartender5/PermissionIssues/ reply trauco 3 hours agorootparentI agree the communication about this change of ownership has been far from ideal. But issues with permissions (e.g. the user needing to disable/re-enable some macOS permission after an update, even if the cert doesn't change) are fairly common. reply hbn 4 hours agoparentprevI assume if there was a way to recreate what Bartender does as smoothly as Bartender without the recording permission, they or someone else would have done it by now considering it makes the setup process clunkier than what would be ideal. This ain't Linux, it's honestly surprising Bartender and various window management apps and whatnot can be pulled off on macOS considering the restrictions. It's not surprising they need to do a weird hack like taking advantage of screen recording privileges to make it work. reply athorax 3 hours agoparentprevOur docks require the same permission as a way to verify the multiple monitors are actually working. As a result I just bring my usb-c dongle and a couple usb-c -> displayport cables instead reply sspiff 5 hours agoprev> Future plans for the app remain unknown, but users have a right to know what’s going on behind the scenes. Do they though? You buy a license for a piece of software, and as far as I understand that doesn't include any fundamental rights involving insight into company policies, direction, business strategy, ... Regardless, this is of course quite a dubious takeover. reply adamors 4 hours agoparentThe thing is that you bought a license for Bartender 5 and up until version 5.0.49 it was shipped by one owner, then from 5.0.52 onwards by a different owner. Users absolutely have a right to know that the same software they bought is now being packaged by someone else. We're not talking about a different license or version 6 or whatever. Same software, different owner over night. BTW for people looking, https://www.macbartender.com/Bartender5/release_notes/ still has the old versions up, 5.0.49 is still signed by Surtees Studios Limited reply diggan 4 hours agoparentprev> > Future plans for the app remain unknown Also, this doesn't seem to be true anymore, the new owners did outline some future plans: > [...] We've collaborated closely with Ben to understand his vision for Bartender. Our goal is to implement many of the improvements he had planned and address any reported bugs from the past few months to enhance Bartender's performance. [...] https://old.reddit.com/r/macapps/comments/1d7zjv8/comment/l7... reply nazgu1 4 hours agorootparentOnly if \"Ordinary_Delivery_79\" is real new owner… reply SSLy 3 hours agorootparenttheir reddit account is one day old, damn reply bberenberg 4 hours agoparentprevThis is often a major sticking point in M&A when you find out major contracts are not assignable or transferable depending on terms in those contracts. This can be with customers or vendors. My guess is a consumer app like Bartender isn’t going to have this issue, but just figured I’d share for a lucky 10k scenario. reply HumblyTossed 3 hours agoparentprevIf you bought a \"license\", don't you have a right to know who you are doing business with? reply luckman212 19 minutes agoprevI put together a small commandline tool to help catch these developer cert-swap situations. I wrapped it up in a LaunchAgent to periodically check and alert to this condition. https://github.com/luckman212/cscheck reply bmulholland 5 hours agoprevAlso discussed in https://news.ycombinator.com/item?id=40579660 reply throwaway48476 4 hours agoprevThis is the danger of closed source software. There is no guarantee that the developers incentives will remain aligned with yours. reply rchaud 4 hours agoparentThis happens with OSS apps as well, see the Simple Mobile Tools takeover by an Israeli adware company, affecting several popular Android apps (Gallery, File Manager, others). https://github.com/SimpleMobileTools/General-Discussion/issu... reply banish-m4 4 hours agorootparentThe big difference is free and freemium OSS can fork. There's no forking closed source backend servers or apps. reply wmlhwl 4 hours agorootparentprevhttps://github.com/FossifyOrg reply Fnoord 4 hours agorootparentprevYep, and also the xz backdoor debacle. reply throwaway48476 4 hours agorootparentprevWas it forked? reply duxup 4 hours agoparentprevIf you’re not monitoring all your open source software, I am not sure there’s a whole lot of difference. Sure, someone could fork the original and make a “good” version again, but if you didn’t notice, it went bad… same problem. It might be easier with closed source to some extent, but I’m not sure that means open source has solved it. reply throwaway48476 4 hours agorootparentThe difference is that you can fork if there's a breakdown in the developer user relationship. This is much better than being held hostage if you rely on the software. reply duxup 4 hours agorootparentTo some extent I agree, but in both situations the breach already happened, who knows for how long. reply jrhey 2 hours agoparentprevThe danger of doing business with humans in general reply atommclain 4 hours agoprevI've been using Dozer for some time with no complaints, I'm not sure how much of the feature set overlaps as I use it primarily hiding extraneous menubar items. https://github.com/Mortennn/Dozer (For what its worth it looks like it hasn't been update in 3+ years) reply zimpenfish 3 hours agoparentOne of the great features of Bartender is that it can unhide icons when they change or match a particular state. Handy for, e.g., hiding the battery until you're down to 30% or hiding your VPN icon whilst you're not connected. (let's hope some other apps pick up these features if Bartender turns out to have gone down a dark path) reply ChrisArchitect 4 hours agoprev[dupe] Discussion: https://news.ycombinator.com/item?id=40579660 reply Arubis 4 hours agoprevErgh. Thanks for the heads-up. Bartender's been super handy for years, but has sufficient access to do some damage. Suggested alternatives? reply serpix 4 hours agoparentDozer does the same and is free. reply k310 4 hours agoprevI dropped Evernote when it was bought and the entire U.S. staff was fired. Replacing it with another, preferably free and open source. (search going on at this time) reply banish-m4 4 hours agoparentEvernote never worked reliably, well, or consistently. Apple Notes squished their business model and now Obsidian has somewhat juiced Apple's. reply orhmeh09 4 hours agorootparentEvernote worked reliably and well for me across Windows, Linux, and macOS when I was at school in the early 2010s. It had a great web clipper and it was good at handling PDFs and images. Apple Notes could not replicate that. While I pay for Obsidian nowadays, I find it nowhere near as usable as my Evernote setup at that time. reply banish-m4 4 hours agorootparentI gave up on Evernote around 2009 when they lost notes, corrupted them, and formatting was highly-inconsistent. Neither the Blackberry nor the iOS apps were entirely usable at that time. Maybe it improved after that, but it was just too slow, limited, risky, and untrustworthy. reply orhmeh09 3 hours agorootparentIt's certainly possible that I got lucky to avoid the numerous data loss incidents and my use cases went along the happy path. I gave it up around 2013 due to privacy concerns. reply pqs 4 hours agorootparentprevIt does now. reply narcomo 4 hours agoparentprevTry anytype.io it’s free and open source. reply k310 2 hours agorootparentIt totally fails on safari (I even disabled extensions) but looks OK on Opera (chrome engine). Thanks, will look into it. (but not with safari) reply jmuguy 4 hours agoprevIf you end up switching to Ice, here's how you get menu items to always show. https://github.com/jordanbaird/Ice/issues/69 I'm sure author is overwhelmed right now and will eventually add that to readme. reply calrain 5 hours agoprevThis is the attack vector I worry about the most. reply xyst 4 hours agoprevWhat a shame. Used and paid for this app for a couple of years now. reply flenserboy 5 hours agoprevThis is good reason for why applications should be containerized. The Old OS X way of simply putting a bundle into the Applications directory was a good start. reply widowlark 3 hours agoprevRather than bartender, use ICE - open source, similar feature set reply AndroTux 4 hours agoprevOn an (kind of) unrelated note: If your mouse cursor suddenly stops changing shape when hovering over links for example, restart Bartender. It’s a known bug that they apparently don’t care enough about to fix. reply avensec 4 hours agoprevUnknown is now known: Purchased by applause.dev via https://x.com/digitalychee/status/1798207774993891626 reply bdcravens 3 hours agoprevIf you're already using Parallels, their companion toolbox app has a similar utility reply bloopernova 4 hours agoprevSort of related question: is it possible to display the date in the macOS menu bar in the format 2024-06-05 10:18 reply Terretta 3 hours agoparentThis used to be straightforward if you found your way into the internationalization formats and modified the right ones. It's no longer straightforward, see: https://apple.stackexchange.com/questions/463901/customising... Which in turn points to: https://discussions.apple.com/thread/254316210?sortBy=best reply jwells89 4 hours agoparentprevThe free Itsycal[0] can display whichever date pattern you prefer. It’s what I’ve been using for several years now. [0]: https://www.mowglii.com/itsycal/ reply bloopernova 3 hours agorootparentPerfect, works great and almost exactly how I want it. Thank you! reply fretn 4 hours agoparentprevI think Dato can do this: https://sindresorhus.com/dato reply LorenDB 5 hours agoprevThis is why open-sourcing everything is the way to go. Hostile takeovers can be combated by forking the code. reply Aurornis 5 hours agoparentIt’s a $22 paid app. Of course it’s not going to be open sourced. reply orhmeh09 5 hours agorootparentI think there are some open source macOS apps that provide code but sell binaries on the App Store, no? reply Aurornis 4 hours agorootparentYes, but it was sold to an acquirer by the original author. Making it open source would have interfered with the author’s profit goal. There’s nothing wrong with wanting to make profit from your work and protect the source. Likewise, you’re free to avoid software that isn’t open source. However, it’s useless to make these demands that someone should have open sourced their work when that clearly wasn’t in the best interests of their goals. reply Hamuko 4 hours agorootparentprevI know at least of Textual. Open source but you can buy a pre-built binary for $10. Compiling it yourself was fairly easy when I tried it out once though. https://github.com/Codeux-Software/Textual reply aaronyarborough 4 hours agorootparentprevI think the point was that open-sourced apps aren't vulnerable to an attack vector like this, which is true. Of course, not everything can be open-sourced, but the poster's point still stands. reply Aurornis 4 hours agorootparentOpen sourced apps are absolutely vulnerable to this “attack vector” Did you already forget about the back door in xz utils? reply Hamuko 4 hours agorootparentprevExcept that it's not really true, since open-sourced applications can and most definitely have changed maintainers. And even if they don't, there's still a risk. Do I need to remind everyone of the xz incident when it's so recent? reply banish-m4 4 hours agorootparentprevIt is/was also available through a SetApp subscription. reply jrhey 2 hours agoparentprevYes, just give us your software for free before you sell it to mystery buyer reply submeta 4 hours agoprevJust going to uninstall it. Wtf. - Are there alternatives? reply i-blis 27 minutes agoparentI refrained to install Bartender on a new machine (despite having a license) because of the scary permissions it want me to grant it. I found an open source alternative that fits my needs. Check if fits yours. https://github.com/dwarvesf/hidden reply j16sdiz 4 hours agoprev [–] I don't know, but since when buyers need to identify themself? This is just a change of ownership, no malware were found reply hbn 4 hours agoparentFor a piece of software that has screen recording access on my device, I'd rather we have someone to hold accountable and is willing to put their reputation on the line to disincentivize them from updating it one day to start sending off images to home. reply banish-m4 4 hours agorootparentTBH, commercial software publishers with over $20M USD revenue or more than 25 employees should list a physical office and mailing address, and publish the membership of their leadership team. Hiding behind anonymity and randomly-generated nonsense brand names doesn't solve trust issues, but putting brands->reputations->names does heal it. reply Sohcahtoa82 56 minutes agoparentprev [–] no malware were found yet The fact that it was sold, but the selling was not announced, is incredibly shady. Normally, a developer shouts it from the rooftops, so it makes me raise an eyebrow and wonder if a condition of the sale was to not announce the sale. Maybe I'm being pessimistic, but I think you're being naive. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Apple's WWDC 2024 will highlight advancements in Artificial Intelligence (AI), showcasing the latest innovations in this field.",
      "The event will feature the release of iOS 18, the next iteration of Apple's mobile operating system.",
      "Additionally, visionOS 2, an update to Apple's augmented reality (AR) operating system, will be introduced."
    ],
    "commentSummary": [
      "The acquisition of the Mac app \"Bartender\" by an unknown developer has led to user concerns about its future, security, and new permissions required.",
      "Users praised BetterTouchTool (BTT) as an alternative for managing menu bar icons and discussed the lack of built-in macOS features, especially with the new MacBook notch design.",
      "The conversation emphasized the benefits of open-source software like Dozer and suggested alternatives to Bartender, such as Ice and iBar, highlighting the importance of transparency and security in software development."
    ],
    "points": 215,
    "commentCount": 116,
    "retryCount": 0,
    "time": 1717594685
  },
  {
    "id": 40581545,
    "title": "GitHub Accelerates iOS App Development with Apple Silicon Runners",
    "originLink": "https://github.blog/2024-06-03-arm64-on-github-actions-powering-faster-more-efficient-build-systems/",
    "originBody": "How GitHub reduced testing time for iOS apps with new runner features Learn how GitHub used macOS and Apple Silicon runners for GitHub Actions to build, test, and deploy our iOS app faster. Stephen Glass & Eli Perkins",
    "commentLink": "https://news.ycombinator.com/item?id=40581545",
    "commentBody": "Arm64 on GitHub Actions (github.blog)206 points by sonichigo 13 hours agohidepastfavorite73 comments crohr 4 hours agoMuch welcome addition, although not available (yet) on free plans. I benchmarked them and they are ~15-20% slower than AWS Graviton processors: https://runs-on.com/benchmarks/github-actions-runners/#arm64... But at 37% cheaper than x64 it's quite a better deal than what the outdated x64 CPUs. reply ozgune 3 hours agoparent(Ozgun from Ubicloud) Looks like every GitHub Actions provider is on this thread. :) I also wanted to chime in with two thoughts. First, we recently wrote about how we enabled arm64 VMs at Ubicloud. Any feedback on the technical details is more than welcome: https://news.ycombinator.com/item?id=40491377 Second, I feel runs-on's analysis is a bit unfair to us. Ubicloud's x64 / arm64 performance and queue times above are as good as any (we deprecated the AMD EPYC 7502 line). For example, RunsOn arm64 queue times are 31|42 seconds and Ubicloud queue times are 17|24 seconds. But the analysis says the following, \"Be aware that Ubicloud has low CPU speeds and high queue times, which means they won’t necessarily end up being cheaper than competitors.\" I fail to see this conclusion from the numbers above. What am I missing? reply crohr 2 hours agorootparentThe queue time for x64 appears to vary quite a bit still. But you are correct that it is much better than what it was a few weeks back, when the analysis was written (it was not unusual to see 60s+ spikes). Also note that I specifically mentioned that \"all third-parties are good on that front, expected [sic] Ubicloud (but that may change).\" Also agree that now that you removed the outdated CPUs (was still active end of May), the analysis should mention that you have OK CPUs by now. I will fix it. For providers that are on this page, do not hesitate to write to me if you find the analysis outdated for your specific service. [edit] page is now up to date with new analysis reply ozgune 1 hour agorootparentHey there, thank you for this update. It's your analysis, so you're free to mention Ubicloud as you'd like. Still, you updated the above statement to, \"Be aware that Ubicloud has a slightly lower CPU speed and somewhat variable queue times for x64 (but improving).\" Could you clarify what you meant by variable queue times? According to the benchmarks, Ubicloud queue times for x64 were 18|44 secs at p50|p95 over the past month. I agree that our p95 number is higher. At the same time, RunsOn's AWS numbers are 31|36 secs. So I feel that driving a conclusion based on our p95 variance number for x64 is a bit unfair to Ubicloud. Thank you for compiling this benchmark btw. Now, we'll follow it closely. reply crohr 4 hours agoparentprevAlso a bit weird that they now seem to delegate the image building to third-party vendors (first GPU images, now ARM). Would be nice to know how they validate base images before distributing them to everyone. reply bushbaba 2 hours agoparentprevDoesn’t GitHub use Azure, being owned by Microsoft. So perf difference could be Microsoft azure vs AWS graviton offering differences. Or Grubhub using an older machine type that provides greater capacity at lower costs. reply re-thc 2 hours agoparentprev> I benchmarked them and they are ~15-20% slower than AWS Graviton processors As expected. Just Azure's Ampere Altra that's OLD. It's Graviton 2 era. We're at 4 now (sort of). Ampere One is still \"somewhere\". reply suryao 4 hours agoprevI'm so glad that GitHub finally offers this. They're just more efficient for certain kinds of workflows - both cheaper and faster. If you're building for arm64 targets or android, this is a no brainer because it sidesteps the emulation requirements. We've been offering arm64 runners for a few months now (up to 32 vcpu) with WarpBuild and our users love it. In general, there are lots of inefficiencies in CI starting from the lack of visibility, debugging ease, performance, runner config customization, persistence, etc. We're out to solve it and make the world's fastest CI cloud ecosystem on top of existing CI providers with WarpBuild. reply christina97 1 hour agoparentDoes WarpBuild offer discounts for nonprofits/FOSS? reply suryao 1 hour agorootparentNot at this time. We're a small team trying to build a sustainably growing business with server bills to pay. reply joshstrange 6 hours agoprevGitHub’s runners are a joke. Switching to WarpBuild was the best decision I made for my CI/CD. I halved my build time and got cheaper minutes. GH’s macOS runners particularly were complete trash. reply jjice 3 hours agoparentWe're a small team (four engineers) and we purchased four Dell mini PCs to toss into our small office space. The break even compared to our AWS hosted runners (which were much faster and cheaper than the GH ones) was like four or five months, plus they cut the time again by over half. We have to deal with them if something weird happens, but it's been such a low time investment that it has been such a great choice. Plus, who doesn't love having some new hardware to play with? reply edweis 3 hours agorootparentWhy 4 Dell Minis? You don’t need a monitor for CI/CD, and why 4? Is it for redundancy? Performance? We are interested in doing the same setup. reply jjice 1 hour agorootparentWasn't my choice, but one wasn't enough so we threw a few more at it. So mostly for performance, but the redundancy is an added benefit. They were a pretty decent space, power/heat, performance, and noise ratio for us. Our office is a single room with six desks, so space, noise, and heat are a larger consideration for us. I use a mini PC at home for a personal server as well, but that's just because used enterprise equipment is often a great deal. reply candiddevmike 2 hours agorootparentprevNot OP, but I went with MOREFINE AMD Ryzen mini PCs. They can be configured with a ton of CPU/RAM (including ECC). They're really compact, support 2.5 Gb ethernet, and are great to work with. A comparable Dell workstation with ECC RAM would've been significantly more expensive, and you have to deal with their business folks to purchase them (or at least I couldn't figure out how to buy them direct). reply dehrmann 2 hours agorootparentprev> We have to deal with them if something weird happens Check back in three years once you've had to deal with OS upgrades, power outages, running out of disk space, and one of them failing. reply abuani 2 hours agorootparentThree years sounds like a reasonably good return on investment. At that point the teams either big enough to justify paying $$ for a hosted solution, or still cheaper to buy dedicated hardware. reply re-thc 2 hours agorootparent> justify paying $$ for a hosted solution, or still cheaper to buy dedicated hardware. Always cheaper if you have at least some demand. For the lazy fix even just get a few mac mini and it'd work fine. reply delfinom 2 hours agorootparentprevYou can also just rent dedicated servers from hetzner or ovh for far cheaper than AWS instances with more horsepower. More expensive than a dell mini in a closet. But you get datacenter Internet and power backup. reply jjice 1 hour agorootparentprevWe're about eight months in at the moment, so here are some current stats: - No OS upgrades (running Ubuntu 22 LTS), but regular update - We've had a power outage, but not during the day, so when they rebooted, they went right back to action. Same issue with a network outage in the office. - We have hit disk space issues due to not clearing all the data we needed to. A quick SSH and an rm or two did the job, plus another 30 minutes to an hour to update a cron to remove that portion of the cache I see what you mean for sure, and there will undoubtedly be more things that come up at some point, but with the savings, it would take quite a few engineering hours per month to lose the savings at this point. And I won't lie, when something goes wrong, it's a nice change of pace to deal with a server instead of code once and a while. Can't actually use that for calculations, but I don't hate when it happens so far. reply cr125rider 2 hours agorootparentprevAre any of those problems challenging to deal with? Even if it’s a few expensive man hours the ROI is still quite short. reply barkingcat 1 hour agorootparentprevif the break even was 4-5 months, it means compared to going to cloud vendors, in 36 months, the self hosted solution would have saved 7.2 times the cost!! Even if within 3 years there needs to be OS upgrades, you can just e-cycle the old machines and buy brand new minipc's (with added benefit of increased performance) every time you need to upgrade, and you'd still be ahead!! reply delfinom 2 hours agorootparentprevIt's a CI runner my dude. You just obliterate and reinstall the base system in what 10 minutes? reply candiddevmike 4 hours agoparentprevOr just run your own runners on-premise. You'd be amazed at how fast builds can be with OTS hardware, or even something like a laptop or Mac mini. reply joshstrange 3 hours agorootparentI considered this for my personal business but the minutes are stupid cheap and to pay off a Mac mini it would take me many, many years to break even. For a company with 10's-100's-1000's+ employees writing code and needing CI/CD the math will be different. I came close to buying a Mac Mini anyways, costs be damned, to get away from GH's slow runners. reply dehrmann 2 hours agorootparent> minutes are stupid cheap At smaller scales, as long as your builds aren't stupud-slow, cloud providers should be cheaper just because they can utilize their hardware more (10x?) efficiently. reply joshstrange 2 hours agorootparentTotally agree, and I called out that my scale is small so cloud makes a lot more financial sense for me (especially as a solo founder, I don't want to manage more hardware than I have to already). reply electroly 1 hour agoparentprevDitto. I came for the cheaper Apple Silicon runners (before GitHub Actions had them available for free) and stayed for the faster runs across the board. I cut a 24 minute build down to 10 minutes just by switching over to WarpBuild runners. Not a shill, just a happy customer. reply suryao 1 hour agorootparentAnd I a happy provider - thank you! reply 999900000999 5 hours agoparentprevI understand the Mac builds, but the entire appeal of GitHub runners is it's literally built in. I can just add a YAML file, and we have CICD! reply joshstrange 5 hours agorootparentI get that, but you can run GH runners on your own hardware or someone else's. Switching to WarpBuild was literally as easy as linking my GH account and changing the running image from `macOS-latest` to `warp-macos-14-arm64-6x` and everything worked. Secrets and everything. reply aromeronavia 4 hours agoparentprevCan't agree more with this, I recently migrated my company to use WarpBuild and our CI got 50% faster, not even kidding. We were using self-hosted runners which were not as efficient as we thought so we spawned EC2 instances and connected them to the github network to be runners, but we had such an amount of issues that we wanted to go back to non-self hosted. At the end a tech lead of the company suggested using Warp and we could not be happier with the results. Also the CEO Surya is such a great guy, we had a small networking issue that lasted for about two hours and they gave us credits for all the runners affected with that, and their customer support is blazingly fast. reply crohr 2 hours agorootparent> We were using self-hosted runners which were not as efficient as we thought so we spawned EC2 instances and connected them to the github network to be runners, but we had such an amount of issues that we wanted to go back to non-self hosted For those hitting that same kind of issue (and you are many), you should check out my product https://runs-on.com. 1-click install, 1-click upgrades, and from 7x to 16x cheaper runners on AWS. reply joshstrange 1 hour agorootparentprevI was deploying to CloudFormation and I thought that was the slow part but my \"build\" times went from 20min to 10min. My iOS builds also were halved. reply suryao 4 hours agorootparentprevThank you so much for the kind words! reply duped 2 hours agoparentprevGitHub Actions is a joke. It baffles me that they've been breaking file permissions on upload for years and they haven't fixed it yet. God forbid you want to ship an executable in a tarball or save it for a later stage of your build. reply ugh123 1 hour agoparentprevHow do you handle extremely large repos? Can these be cached on machines, per-customer? reply suryao 1 hour agorootparentAssuming the question was reg. WarpBuild - we provide blazing fast caches with unlimited sizes. These still need to be downloaded to machines and setup on each workflow job. We don't (yet) have support for pre-pulled repos on runners per customer. Are the large repo sizes despite fetching only the refs required for the specific context [1][2] which can help speed things up significantly. [1] https://www.atlassian.com/git/tutorials/big-repositories [2] https://stackoverflow.com/questions/48072080/how-to-manage-l... reply yohannparis 5 hours agoparentprevDifferent needs, different tools! I work on a open-source project, and not having to pay for CI/CD with a \"simple\" yml file configuration is simpler and easy to manage. reply joshstrange 5 hours agorootparentIf you are open source and getting free minutes then it really doesn't matter how long builds take. I'm still using GH Actions, just not their minutes. reply 01HNNWZ0MV43FF 5 hours agorootparentprevIt's only free if your time is worthless :P every couple weeks I have to debug something that I can't replicate locally because I don't have local CI reply joshstrange 5 hours agorootparentThat's the other plus with something like WarpBuild, I haven't used it yet but you can pause the build and SSH in: https://github.com/WarpBuilds/action-debugger reply suryao 4 hours agoparentprevThere is a lot of inefficiency in CI today and we're out to solve it. Thanks for the WarpBuild love! reply KRAKRISMOTT 3 hours agoparentprevHow do they compare to Ubicloud? https://www.ubicloud.com/ reply suryao 2 hours agorootparent(WarpBuild founder here) We have had a lot of users evaluate multiple tools including ubicloud but chose us. Typically, they've mentioned our runners being faster, our caching is very useful, and queueing delays being lower. However, my last anecdata was from a few weeks ago. You can try us out and take a call for yourself. Our focus is on CI runners and efficiency and I think that gives us an edge on the devex. Ubicloud is definitely cheaper though. reply joshstrange 3 hours agorootparentprevI think I skipped over evaluating ubicloud because they don't have macOS runners. There was another place I liked but I needed macOS runners they didn't have them either. reply shpx 8 hours agoprevIt's not free. > Larger runners are only available for organizations and enterprises using the GitHub Team [$4 per user/month] or GitHub Enterprise Cloud [$21 per user/month] plans. reply sunaookami 6 hours agoparentSeems like normal users have to wait: > We expect to begin offering Arm runners for open source projects by the end of the year side note: I find it mildly annoying that it's no longer \"ARM\" but \"Arm\" reply VWWHFSfQ 5 hours agorootparentYeah it's confusing because the company is branded like \"Arm\" but they still call the architecture \"ARM\" reply booleanbetrayal 3 hours agoprevIt's nice to know that we have an ARM64 fallback if our perfectly robust and cheaper self-hosting of GHA (gha-runner-scale-set* components) were ever to go down, I suppose, but I can't help but feel like I am part of a huge market segment that GitHub lost during the ARM64 runner absence. Seems a little too late given they have natively supported action-runners images in ARM64 for some time. reply bhouston 4 hours agoprevHmm... this is neat. I develop on MacBook so ARM64, and I do use GitHub Actions, which can not be ARM64, but unfortunately Google Cloud Run doesn't support ARM64 images yet: https://issuetracker.google.com/issues/303743857 reply david_allison 1 hour agoparentExcuse my ignorance, isn't `runs-on: macos-14` ARM64-based (M1) https://github.blog/changelog/2024-01-30-github-actions-maco... reply rahkiin 2 hours agoprevAre there third party providers for Gitlab runners? I’d love windows and mac runners for my self hosted gitlab. reply deivid 6 hours agoprevI wonder if they'll start supporting risc-v soon. I've been using github-act-runner on top of a local Nomad cluster to run some of my builds on risc-v, arm64 & x86-64, it's fine, but a bit of a hassle to set up reply compsciphd 5 hours agoparentwould only happen after azure supports risc-v VMs. reply sapiogram 2 hours agoparentprevGiven how long arm64 took them, my guess is \"3 years after major cloud vendors start offering risc-v VMs\". reply deivid 2 hours agorootparentI'd kinda hoped that they took so long to go arm64 because they generalized \"1 architecture -> N architectures\" reply frankdejonge 1 hour agoprevOne thing that consistently bugs me is how upgrading from the lowest type of runners to _literally_ anything else renders your paid for included minutes useless. I do not understand who those wouldn't just be multiples of the base runner, much like how Mac and Windows runners are. Seems like the crediting logic is there, but knowing software, there is probably some accidental complexity causing this part not to leverage it. That said, it's very frustrating from a paying customer perspective. The same goes for minute crediting, where splitting things up to run concurrently is actively discouraged because they individually all round up to the next minute. For example; have 3 concurrent tasks that run 1m10s? You're billed 6 minutes. I get that a run would be rounded up, but come on. reply dedene 3 hours agoprevCan it be used to efficiently build multi-arch docker images? Both armv8/x64 reply caleblloyd 3 hours agoparentMaybe but I’d recommend checking out Depot.Dev for that, they expose Arm and x86-64 runners to buildkit/buildx. Not associated with them, just a happy customer. reply obviyus 12 hours agoprevNice! I had been using self-hosted arm runners for my projects lately (which GitHub makes surprisingly easy to do): https://docs.github.com/en/actions/hosting-your-own-runners/... Considering how compiling a couple of these projects on my tiny arm VPS slows down everything else, this is a welcome change! reply sofixa 6 hours agoparent> which GitHub makes surprisingly easy to do Why surprisingly? Are there any CI/CD, let alone SaaS CI/CD providers, that don't make it easy? reply obviyus 5 hours agorootparentFor some reason I expected a \"not-first-class\" experience while using my own hosted runner but that was not the case. It works and behaves exactly as their own runners do. Maybe s/surprisingly/extremely would be more what I meant. Just wanted to add, I loved your post on logging Nomad allocs using Loki. Was a huge help while I was setting up our clusters at $CURRENT_JOB reply sofixa 3 hours agorootparent> For some reason I expected a \"not-first-class\" experience while using my own hosted runner but that was not the case. It works and behaves exactly as their own runners do. Interesting, I've used GitLab extensively and hosted/self-hosted runners were trivial to use/run, so I would have expected nothing less from GitHub. They probably make a decent margin on the hosted runners at the cost of significant complexity, so if they can give that away to the orgs that will have their own (virtual) hardware anyway, with their own contracts and networking and etc., why not? > Just wanted to add, I loved your post on logging Nomad allocs using Loki. Was a huge help while I was setting up our clusters at $CURRENT_JOB Glad you found it helpful! If you ever want to chat about it, feel free to hit me up (contacts are on my website available in my profile). reply saagarjha 9 hours agoprevI mean they did before, but they were running macOS ;) reply drcongo 8 hours agoprev [–] Digital Ocean are going to be dead last in the transition to ARM. reply diggan 7 hours agoparentAre GitHub and Digital Ocean competitors nowadays? Or you're just referring to anything that offers \"run code on someone elses computer, somehow\"? reply bdcravens 7 hours agoparentprev [–] AWS still only supports X64 on Fargate Spot instances (they do support ARM on the non-spot instances) reply booleanbetrayal 3 hours agorootparentWe waited on this issue for 2 years before migrating off EKS Fargate to Managed Node Groups due to this very reason. In the end, it turned out to be much better of an environment anyway, because you didn't have to deal with Kubernetes oddities (like lack of Daemonsets) requiring anti-patterns and ultimately resulted in cheaper pricing due to better bin-packing. If you're doing K8s orchestration with Fargate, I highly recommend the switch. reply watermelon0 29 minutes agorootparentTo be honest, lack of daemonsets makes sense, because you don't have hosts per se. Each pod is running on it's own Linux VM. Daemonsets are generally intended if you have multi-pod nodes; otherwise, you can just use sidecars. reply suryao 2 hours agorootparentprev(founder of WarpBuild - we offer hosted GHA runners) This is a common issue. A common pain point we see with users approaching some scale with self-hosting on k8s is that the k8s node autoscaling can become inefficient because of spiky loads. We have a lot of users migrating off self-hosted setups using `actions-runner-controller` to ours because of this. Essentially, not having to deal with bin-packing is more efficient and concurrency, uptime guarantees are nice. reply trueismywork 6 hours agorootparentprev [–] T4g is available as spot reply endgame 5 hours agorootparent [–] GP's talking about Fargate, which is their \"serverless\" container runtime offering. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "GitHub has enhanced the speed of building, testing, and deploying their iOS app by using macOS and Apple Silicon runners for GitHub Actions.",
      "This improvement has notably reduced the testing time for their iOS app."
    ],
    "commentSummary": [
      "GitHub Actions has added Arm64 support, but it is not available on free plans, highlighting a cost-effective yet slower alternative to x64 CPUs.",
      "Users are comparing GitHub's use of third-party vendors for image building, with discussions on Azure and AWS Graviton processors, and exploring alternatives like WarpBuild for faster and cheaper builds.",
      "Self-hosted CI/CD solutions using mini PCs are noted for significant cost savings despite requiring maintenance, with ongoing discussions about the trade-offs between cost, performance, and management in CI/CD infrastructure."
    ],
    "points": 206,
    "commentCount": 73,
    "retryCount": 0,
    "time": 1717563729
  },
  {
    "id": 40579660,
    "title": "macOS Bartender App Sold Quietly, Sparking User Concerns Over Transparency",
    "originLink": "https://www.macrumors.com/2024/06/04/bartender-mac-app-new-owner/",
    "originBody": "PSA: Bartender Mac App Under New Ownership, But Lack of Transparency Raises Concerns Tuesday June 4, 2024 3:22 pm PDT by Juli Clover Popular Mac app Bartender appears to have been quietly sold approximately two months ago, with neither the prior owner nor the current owner providing customers or potential customers with information on the sale. The transaction came to light after some Reddit users saw a warning from MacUpdater letting them know that the company behind Bartender had been silently replaced. MacUpdater warned users that updates to the app from version 5.0.52 could be potentially unsafe due to the lack of transparency surrounding the situation. Bartender's new owners replied to the Reddit thread and confirmed that Bartender had been acquired, but did not explain why customers had not been notified nor why there had been a certificate change without said explanation. Hey everyone, new owners of Bartender here! Our team acquired Bartender from Ben S, the original developer, two months ago. As we prepare to roll out updates for Bartender, we needed to re-sign the app with Apple using our company's information, replacing Ben's. This led to a one-time certificate change. Truth be told, we should have notated it on the release notes but, since we could not update them retroactively, we included this fact on our blog & shared it with users as they emailed us. We've collaborated closely with Ben to understand his vision for Bartender. Our goal is to implement many of the improvements he had planned and address any reported bugs from the past few months to enhance Bartender's performance. Reddit users asked Bartender's owners for more information on their identity, but there was no response. The Bartender website was updated with information about the certificate change after users began seeing popups asking for new permissions, and while the blog post calls the new certificate request \"expected and valid,\" no background information is provided on the purchase. In the Reddit thread, the owners claim to have posted information about the sale on the website, but that has turned out to be untrue. At this point, it does not appear that Bartender's new owners plan to inform customers about the change in ownership, but users should be aware that the app has been sold and is no longer being updated by the original developer. The new owner's intentions are not clear, but as Reddit users have pointed out, the situation raises some red flags. [ 190 comments ]",
    "commentLink": "https://news.ycombinator.com/item?id=40579660",
    "commentBody": "macOS Bartender Auto-Update Signed by Unknown New Owner (macrumors.com)204 points by bmulholland 20 hours agohidepastfavorite97 comments danpalmer 18 hours agoI switched to HiddenBar a few years ago when a Bartender licence ran out. It's open source, it's free, and it hasn't been taken over by a new owner who won't identify themselves. https://github.com/dwarvesf/hidden reply crossroadsguy 16 hours agoparentSadly it's as good as not functional (it doesn't even begin to work on my Mac). I used to use https://github.com/Mortennn/Dozer and that also started misbehaving (and now the shortcut won't work). I used hidden's fork as well which also doesn't work. Ice doesn't work either. All of these apps are abandoned. reply danpalmer 15 hours agorootparentWhat isn't working for you? I've been using it for years and have no issues on the most recent versions of macOS. I don't use much of the functionality though, really only just use it to hide icons I don't care about. reply crossroadsguy 9 hours agorootparentIt doesn't do anything at all. No |, or > shows up, nothing. Tried homebrew, direct, app store. Killed, restarted both the app and the Mac, etc etc. Hiding icons is the only use-case I have, same as you. reply luuurker 6 hours agorootparentHidden Bar is working on my M1 Max MBP 16\" with macOS 14.5. Not an option for me because it can't handle the notch, but the basic functionality works. Ice ( https://github.com/jordanbaird/Ice ) also works, although it's still missing a lot of stuff and also still doesn't have anything to work around the notch. It's being updated though. If no app works, then it might be something to do with your setup? Maybe some app you use or some change you've made? Very annoying in any case. reply garciasn 58 minutes agorootparentI am now using Ice based on this post and it works, for me, as a drop-in replacement, aside from being able to pick what is permanently showing (wifi, primarily). I uninstalled Bartender after 10-12 years and am happily using this. Thanks for the help! reply crossroadsguy 1 hour agorootparentprevMacBook M1 Pro 2021 model 16 inch. Not working. For me nothing works and yes it anyway doesn't handle the notch. In case of Ice it didn't work in first 2 goes and then I gave up because I kind of felt weird that it uses accessibility permission workaround, kinda. No idea what I changed. When I install back Dozer it starts working fine (with Shortcut not working). Then if I have to uninstall the Dozer I clear all files related to it using AppCleaner. So I guess that doesn't interfere either. I will maybe debug it one of these days when I have some time. reply luuurker 17 hours agoparentprevLast time I tested it, icons would be hidden by the notch. Have they fixed that yet? Edit: Nope, no change and the last update was 2 years ago. Ice ( https://github.com/jordanbaird/Ice ) doesn't currently handle this either, but it's \"on the roadmap\" ( https://github.com/jordanbaird/Ice/issues/1#issuecomment-192... ) and it's being updated. reply LeoPanthera 17 hours agorootparentNo, it still isn't aware of the notch, and there has been no update in over 2 years. The app seems to be abandoned. reply cpeterso 15 hours agorootparentThat menu bar icons can be hidden behind the notch seems like an Apple bug that should be handled in macOS. reply kemayo 17 hours agorootparentprevLast update was three years ago, so probably not. reply j16sdiz 16 hours agoparentprev> .... by a new owner who won't identify themselves Idk, but most of software I use were written by unidentified individuals. reply voxic11 15 hours agorootparentYou don't have to live like that. reply mmaniac 5 hours agoprevMac OS has a serious problem with requiring loads of third party extensions to get a usable desktop experience. Most of the time it's merely annoying, but this situation also demonstrates that you're placing a lot of trust in the authors of these programs. reply dewey 5 hours agoparentNobody “requires” these tools to have a “usable” experience. Millions of people are using it just fine without all these tweaks. reply rhinoceraptor 4 hours agorootparentA menu bar manager only becomes necessary when you have a bunch of extra things installed on your system that each have a menu bar item. reply BoardsOfCanada 4 hours agoparentprevDoes it really? For me, finder is below the level of what's acceptable, so I use a replacement. I also use magnet for arranging windows, but apart from that I find the experience far better than windows or linux. Maybe I'm missing out on some really great extensions, which ones would you recommend? reply mmaniac 3 hours agorootparentI use SensibleSideButtons and ScrollReverser continually. Those are features which I consider absolutely basic and their omission leaves me cynical. Rectangle and Contexts are extensions I use to make Mac OS closer to Windows or Linux in comfort, but they're not what I'd consider essential. An interesting thing happened while I was enumerating the tweaks I had installed. Some of their menu bar icons were hiding under my notch without my even realising! Completely lacking an overflow indication is embarrassingly sloppy, and I might have to grab something like Bartender now too. reply hu3 4 hours agorootparentprev> Does it really Yes. https://news.ycombinator.com/item?id=40580566 reply BoardsOfCanada 4 hours agorootparentMy situation is similar to `dwighttk` in the comments you link to, I have stats (as in `brew home stats`) showing CPU/GPU/RAM/Network, then the standard macOs stuff + magnet + weekday and full date and time and nothing is ever hidden by the notch when using the built in display. So - since people DO have a problem - there must be other things that a lot of users want to see but none of them are mentioned in the link. reply hu3 3 hours agorootparentPerhaps you don't need to organize icons like other users do. But for us who do, it is clearly essential. > \"Ugh, this sucks. This app was nearly essential on MacBooks with a notch, since you could hide lesser used icons away to make everything fit.\" - LeoPanthera 13 hours ago > \"True, but it has been essential for a long time before the notch for those of us with a non-trivial amount of software. The menu bar is a popular spot and so many apps want to put an icon there.\" - Tagbert 13 hours ago And that's just from a quick glance. Most users don't even know it's possible to improve their UI with this third party tool. I bet if they knew it would be considered very important to the mas well. Specially power users who fight with screen space. reply fifafu 8 hours agoprevIn case you are already using my BetterTouchTool app I have created a little tutorial & example preset here on how to use it for status item management: https://community.folivora.ai/t/bartender-controversy-tutori... You can add feature requests there, it should be pretty simple to extend BTT to support the remaining required features. reply cloogshicer 4 hours agoparentMan, I love BTT. I've been using it for many years, it's fantastic! Something that I'd love to have is an easy way to show menu items that aren't visible because there's not enough space in the menu bar. I often have to switch to a different app with fewer text items in the menu bar like Finder, which only has File, Edit, etc., just so I can reach menu bar items that would be otherwise hidden. reply anonsec123 17 hours agoprevIf I wanted to poison update a bunch of developers who likely work at major US tech companies, this is exactly the type of software I would target for \"purchase\". reply bravetraveler 16 hours agoparentExactly, I've been called draconian in the past... but that's why I'm hesitant to add nearly anything. With LLMs my usual barometers may not be that effective; documentation/perceived mindshare. Usually if my peers know about it... I'll call it decent. That stopped being fine for browser extensions like Adblockers and it's expanding. Land grabs in every domain! reply alsetmusic 18 hours agoprevMy first license was from 2015. This is one of the most important utilities I install on all my Macs. I'm sad to see this apparently (and appearance is all that really counts at a moment like this) shifty behavior from the new owners. Hope the original dev got a nice payday. I plan to keep running the app without updating until it breaks or I find something I like better. I hate when beloved software (effectively) goes away. reply flemhans 18 hours agoparentDoes it do anything except cosmetic changes? reply kemayo 17 hours agorootparentIts core feature is a non-cosmetic change: it hides the menubar app icons, and then shows them on-demand. The ability to make cosmetic changes to the menubar is a comparatively recent development. reply _0xdd 17 hours agoprevThere’s a setting to kinda get around having to use apps like this if you’re dealing w the notch. I don’t recall the specific command, but there is a way to shrink the icon spacing to allow more visible icons. Edit: see https://apple.stackexchange.com/questions/406316/can-the-spa... reply oidar 15 hours agoparentThis is what I did too. I don't think I need bartender now. A lot of the cruft can be managed by the control center and there are smart options in the control center and menu bar. These settings looked good to me: defaults -currentHost write -globalDomain NSStatusItemSpacing -int 5 defaults -currentHost write -globalDomain NSStatusItemSelectionPadding -int 8 reply Brajeshwar 14 hours agoprevIn my habit of sticking to a minimalist and essentialist lifestyle, I've been trying to stay within the Native Apps and reduce third-party apps wherever I can either get rid of or replace them, even if I have to add additional effort. I've been using Bartender for a very long time. I got rid of it about a year ago. These days, I think and ponder over multiple times before introducing a Menubar App. - Of course, do I feel the pain if I don't have this one? - Can this be hidden altogether? - Is the icon monochrome? Colored icons are outright rejected. Between the Control Center and the limitations I set for myself, I've survived so far. Let's see where this goes. reply avensec 4 hours agoprev\"Unknown\" is now known: Purchased by applause.dev - Via https://x.com/digitalychee/status/1798207774993891626 reply nymiro 14 hours agoprevThis rendered Hidden Bar and Bartender obsolete for me, just reduce icons padding. You can still complement with Hidden Bar. I wish I had written down the original author to give credit: ``` defaults -currentHost write -globalDomain NSStatusItemSelectionPadding -int 6 defaults -currentHost write -globalDomain NSStatusItemSpacing -int 6 ##After running these commands, you need to log out and log back in ``` reply yodon 4 hours agoparentWhat do the commands do experientially? reply LeoPanthera 17 hours agoprevUgh, this sucks. This app was nearly essential on MacBooks with a notch, since you could hide lesser used icons away to make everything fit. reply Tagbert 17 hours agoparentTrue, but it has been essential for a long time before the notch for those of us with a non-trivial amount of software. The menu bar is a popular spot and so many apps want to put an icon there. reply dwighttk 16 hours agoparentprevman I add a few icons using istatmenus and don't even come close to the notch... what kind of icons do you have? Mine are (R to L): system date and time - system control center - iStat Network up/down and histogram - iStat Battery - Dropbox that fills about 40% of the way to the notch... 50% when I'm unplugged and the battery is showing time remaining reply fragmede 16 hours agorootparentistat puts a bunch of graphs in the bar (CPU, memory, disk, network) reply dwighttk 8 hours agorootparentYeah I use two of them reply threePointFive 16 hours agoprevI don't develop for Mac, so I'm probably missing something important here, but what is the point of code signing if the owner of the signing key isn't publicly verifiable? Is there not a chain-of-trust back to Apple that can be used to determine who requested the signing key? reply SheinhardtWigCo 16 hours agoparentThere is, kind of. Each developer is identified by a “team ID”, which appears in the Common Name of these certificates. Apple does some validation of the developer’s legal entity, similar to EV SSL. reply stblack 16 hours agoprevIn Bartender 4, Right-click the \"...\" in the menu bar, then * Bartender Preferences... * Advanced * Uncheck \"Check for Updates Automatically\" reply JoeMattiello 16 hours agoprevInteresting. Today I kept getting notifications that “Spotlight” was updated and it was asking me permissions to allow access to its previous version data. I thought that was odd for system service and I kept rejecting it but it kept coming up. I finally clicked except and start looking through my task manager to see if anything was off but everything looked OK so I wasn’t really sure it was going on. Bartender is installed through SetApp. I wonder if it’s related. I’m going to uninstall reboot and run system scan. reply LeoPanthera 17 hours agoprevThe Reddit thread recommends \"Ice\" as an alternative: https://github.com/jordanbaird/Ice reply luuurker 17 hours agoparentAll these open source alternatives lack something important for macs with notches: a way to see the expanded icons below the menu bar. Without this, icons are hidden by the notch. You'd expect macOS to properly support the notch after 3 generations of laptops with them, but nope. reply FireBeyond 17 hours agoparentprevI mean, promising, but a lot is still \"in active development\" and on the roadmap, including notch support, icon arranging... reply dylan604 17 hours agoprevLet this be an example to any dev in this situation on how NOT to handle things. Everything about this transfer feels shady. The lack of response to direct questions after posting in a public forum is just all sorts of tone deaf. By making the public post, you're clearly aware there's concern. Then you post a murky and ambiguous response. You then do not follow up with concerns about the lack of information in the post. Just tell all of your users you don't care about them and you're about to unleash all sorts of nasty malware with a future update. At least they'll respect you for being honest. reply dcchambers 14 hours agoprevFWIW it appears the latest version on SetApp (5.0.48) is unaffected by this, so feel free to continue using it (and disable auto-updates) if you want, but this whole things feels pretty icky so I went ahead and uninstalled. Looking into alternatives. Really wish Apple would address this problem natively. reply adamors 4 hours agoparentNote that you can download previous versions from https://www.macbartender.com/Bartender5/release_notes/ still. 5.0.49 is still signed by Surtees Studios Limited reply Arubis 4 hours agorootparentSuper helpful workaround for those of us that want to deal with this not right this instant. reply adamors 3 hours agorootparentI'd also recommend installing Lulu https://objective-see.org/tools.html and blocking every network activity regardless of the version you're on. reply DavideNL 14 hours agoparentprevYou could just use \"Little Snitch\", block internet for Bartender, and keep using the latest version for a while. Meanwhile, keep an eye out on how things develop... reply mp05 4 hours agorootparentYep, this is what I did once I saw this. I hate to admit it, but I'd lose my mind if I didn't have Bartender. reply garyrob 14 hours agoprevGiven the proximity of this development to WWDC, it seems like there's a chance Apple has acquired Bartender and has incorporated its functionality into the next MacOS release. reply garyrob 1 hour agoparentAn update to my own comment, above. After absorbing other comments, I no longer think it's a reasonable possibility that Apple has purchased Bartender. I think we have to assume it's likely to be a malicious actor. There could even be a state-level malicious actor ultimately behind it. reply _rs 14 hours agoparentprevGiven the shady replies on Reddit, I don't think so, that isn't how Apple operates reply Aaron2222 12 hours agorootparentAlso, Apple doesn't tend to write SEO clickbait: https://www.macbartender.com/Bartender5/blog/ reply nicky0 5 hours agoparentprevIf Apple wanted to implement a simlar feature, they would just do it. They wouldn't buy a 3rd party app when they could just do it themselves. The value proposition of these kind of third party UI apps is that the developer cleverly figured out how to manipulate Apple's APIs to do something Apple didn't really intend. Apple themselves can just write their own private API to do it, they don't need the clever hacks. reply Hamuko 13 hours agoparentprevDoesn't Apple usually just buy companies publically and then later announce that the product has been discontinued? That's what happened to Dark Sky. I'd hope for Apple to actually incorporate this into the fucking OS itself so that we didn't run through these hoops for pretty basic functionality that has been in Windows for what, two decades? reply sangeeth96 18 hours agoprevI just switched to Ice after reading this and it seems just as good for my needs: https://github.com/jordanbaird/Ice reply oefrha 17 hours agoparentI just tried, wasted five minutes and couldn't for the life of me figure out how to move things out of the (always?) hidden group they're in by default. Eventually found the instruction for it on the issue tracker: https://github.com/jordanbaird/Ice/issues/42 While I do remember now that cmd+drag is how you reorder menu bar icons, so that is sort of intuitive for people familiar with the behavior, I haven't done that for years and definitely need a reminder. Seems very strange that the very first nontrivial action isn't documented anywhere (maybe it is but I still don't see it)... While searching for how to move anything out of hiding, I also landed on https://github.com/jordanbaird/Ice/issues/29#issuecomment-19.... Apparently Ice can only hide one segment of the menu bar at once, so if the icons are rearranged for some reason, they can show up in a wrong section. Author promises a future update will fix this, but at the moment Bartender is still the best option unfortunately. reply rcarr 14 hours agoprevI believe you might be able to replicate the functionality using BetterTouchTool but it might be a bit of hassle to do, not 100% sure though. reply 037 11 hours agoparentYou are right, but it’s not very difficult: The trigger can be “Double-click Main Menubar” and the action “Show / Hide Menu Bar Icons left of the BTT Icon.” You can even add actions to hide them again after a certain number of seconds or minutes. (It’s only a partial replication because Bartender can show you the additional icons in a secondary bar, if needed). reply slimebot80 15 hours agoprevSo the issue here is the screen recording? And that this app is popular with people in tech? Would a Little Snitch block cover the risk for some? I sometimes wonder about all the homebrew stuff devs use.... reply rocketvole 15 hours agoprevMan, first I hear about Raivo, now this. Been a bad week for me to say the least... reply blinded 20 hours agoprevI own this app and haven't update yet, smh. Silly its not just a feature of the OS. reply SamuelAdams 18 hours agoparentThere are so many small apps that really ought to be part of the OS by now. A few that come to mind: Rectangle, or some sort of default way to control application windows with your keyboard. Mos / UnnaturalScrollWheels: if you use a docking station, you may want natural scrolling when using your laptop as a laptop, and normal scrolling when using a mouse. Windows and Linux both have this solved for 15 years now, I am amazed Apple continues to ignore this. Alfred: Apple’s Spotlight app has come a long way but I want to lock, restart and shut down my computer with my keyboard. There are some shortcuts but honestly I can never remember the key combinations. It would be much easier to command + space, type “shut down” and the computer shuts down. https://rectangleapp.com/ https://mos.caldis.me/ https://github.com/ther0n/UnnaturalScrollWheels https://www.alfredapp.com/help/features/system/ reply jwells89 16 hours agorootparentAlfred is truly excellent. Not only is it capable of a wide array of things but is built extremely well, being exceptionally lightweight, stable, extensible, and tiny. It’s increasingly rare for software to be even two of those these days, let alone all of them. reply eric-hu 15 hours agorootparentprev> I want to lock, restart and shut down my computer with my keyboard. There are some shortcuts but honestly I can never remember the key combinations. If you don’t already know this combo, cmd-? (AKA cmd-shift-/) will drop down the “help” menu in most applications that run in macOS. From there, I use the arrow keys to move to other menus. It’s just one nearly universal hotkey and it works for almost all menu items. reply somehnguy 17 hours agorootparentprevRectangle user for years, great app. Today I got frustrated enough to find AltTab (https://alt-tab-macos.netlify.app/), huge improvement to my workflow already. Sometimes when cmd+tab'ing between apps it'll bring the selected window to the foreground, and sometimes it won't. They'll regain focus for typing, but are hidden behind another window for whatever reason. Fully open windows too, not minimized or anything. Was driving me mad having to click between IntelliJ and Chrome hundreds of times while doing web UI work instead of quick switching with the keyboard. reply yodon 4 hours agorootparent+1 for Alt-Tab-MacOS, which lets you have Windows-style alt-tab behavior on Macs. Finding it was quite literally what enabled me to finally move from Windows to Mac. reply Tagbert 17 hours agorootparentprevDo you use cmd + ` (tilde key) to cycle through windows within an app? Its the key just above Tab and that's probably why it was chosen. reply somehnguy 15 hours agorootparentYes, that combination to cycle between windows of an app never gives me any trouble. It's when cycling between different apps (cmd + tab) that I run into issues. reply fragmede 16 hours agorootparentprevcontexts.app for windows style alt-tab behavior reply pxc 15 hours agorootparentContexts is a great (proprietary) app but I forgot it even had this functionality! To me, that's the least useful thing it does. reply somehnguy 15 hours agorootparentprevLooks nice. I don't really have any use for the extra features though, AltTab does what I wanted and is free. reply LgWoodenBadger 15 hours agorootparentprevFYI, you can use the Keyboard preferences page in System Settings to assign things like “lock screen” (I forget exactly what it’s called) to a key (F15 I think I used). I don’t know if Restart or Shutdown are allowable choices. reply ewoodrich 15 hours agorootparentprevAlfred is fantastic since it let me remove all the junk I don't care about in Spotlight. Which makes search ridiculously fast for the only things I ever want to use search for: opening applications and basic OS functions you mentioned like locking, entirely using the keyboard. There's a fantastic Alfred feature of an additional locking shortcut that blacks out the display immediately instead of meandering on the lock screen afterwards. I have never had any desire on any OS to search the web, or the app store, or even my files for that matter (without doing so deliberately). Having everything mangled together is so distracting to me and only slows me down. Started liking my M1 Macbook Pro so much more after I discovered Alfred while searching for any tricks that would let me pare down Spotlight to essentially nothing. I debloated my Windows 11 search to do close to the same thing (although not quite as good as Alfred) and remapped search hotkeys to be consistent across both platforms which is just so pleasant as someone who routinely needs to switch between MacOS and Windows. reply eviks 5 hours agorootparenton Windows you can also use a standalone launcher like Keyprinha to do the same - only choose the stuff you want to be indexed. And then use Everything to find any file anywhere instantly (can be setup to show results in the same launcher) reply luckman212 8 hours agorootparentprevlock: ctrl + cmd + Q logout: cmd + shift + Q reply blinded 17 hours agorootparentprevdamn! thanks for the list ill check them out. reply nikolay 19 hours agoparentprevIt should be, yes. reply oefrha 17 hours agoparentprevmacOS should have its own PowerToys. Windows is smelly but PowerToys is a breath of fresh air. reply teeray 18 hours agoprev [–] Why is this not functionality that is built into the OS at this point? Is there anyone that loves 10,000 taskbar apps visible at all times? reply jwells89 16 hours agoparentMenubar apps as they exist now haven’t ever really been a consideration for Apple for UX/UI design purposes. For somewhere between the first third and half of OS X’s existence it wasn’t even possible to have persistent third party status bar items without some hackery that made the system think third party apps were first party menu extras. Even today the API they use is intended primarily for ephemeral status bar items that are present only when the host application is open and visible in the Dock, which is worked around by declaring the host app as a chromeless background app so it can stay open and keep the status item visible without cluttering peoples’ docks. In short, the OS doesn’t have features for managing third party menubar items because indefinitely persisting third party menubar items don’t really have any place in the larger design. Third party menubar items are intended to be few in number and relevant to one of the user’s current tasks. reply krackers 14 hours agorootparent>without some hackery that made the system think third party apps were first party menu extras. Good old MenuCracker reply sircastor 17 hours agoparentprevI think this is a relatively niche product. Most people aren’t using so many apps and extensions that their menu bar is getting crowded with stuff. reply garciasn 17 hours agorootparentI haven’t seen a single Mac user on screen share that doesn’t have too many icons on their bar for me. I want to see almost none. For all of Apple’s insistence on design and clean, the menu bar and those hideous icons always there was the worst one ever. I’ve been a Bartender user for over 10 years (I believe 12) and I’m now really fucking sad. reply LeoPanthera 17 hours agorootparent> I haven’t seen a single Mac user on screen share that doesn’t have too many icons on their bar for me. This says more about app devs than it does about users. reply pxc 16 hours agorootparentApple devs as well as application developers— menu bar icons are clearly a neglected corner of the ecosystem in that Apple would prefer that app devs not persist icons in the menu bar at all. But the feature is nonetheless wildly popular as the nearest systray-alike functionality on macOS. reply hombre_fatal 15 hours agorootparentprevNot really. Even in the case where every single app has a good reason to have a menu bar icon, but you're still left with the fact that macOS doesn't let you manage them. reply pxc 15 hours agorootparentOr maybe they don't all have a good reason but you don't really have a choice about running them (e.g., on your work computer). If you are visually impaired, you also likely have greatly reduced space for those icons, especially when undocked. (This issue plagues me personally.) In that case, this is not merely a clutter issue: app icons will simply be truncated from the menu bar, and in some cases that may mean that some functionality is just totally inaccessible to you. reply JoeMattiello 16 hours agorootparentprevIt’s included in the SetApp bundle and a highly rated and recommended app on there. It’s not that niche especially for developers and power users that gravitate towards SetApp subscription. For me, I was already paying for “clean my Mac” and fantastical which both cost the same or more than a SetApp subscription which includes dozens of additional apps. reply eyelidlessness 17 hours agoparentprev [–] It’s partially built into the OS, in that you can move almost every first party icon into the control center (via System Settings). That probably isn’t sufficient for everyone, but it was good enough for my usage that I never bothered installing Bartender on my M2-series MBP. reply FireBeyond 17 hours agorootparent [–] And yet people will swear blind that Apple apps don't get special treatment. Whether things like this... or the earlier days of Safari where it had access to some APIs around power-saving... Or the most nefarious, where Apple apps were able to bypass most of the TCP/IP stack tracking and send traffic directly, regardless of any on-device filtering or firewalling, like Little Snitch. And then they claimed it was only \"a temporary measure while they dealt with updating software\", but they never did explain why an app like TextEdit would have ever needed a kernel network extension in the first place. That to me was almost certainly a post-facto attempt at justification when they were caught with their hand in the cookie jar. reply fragmede 16 hours agorootparent [–] iirc, netcat was the truely embarrassing one that was signed and allowed to bypass firewalls reply FireBeyond 15 hours agorootparent [–] Yeah, that's really problematic. But I can't imagine contortions around \"yeah, it's an exceptionally simple bundled text editor but trust us, it really needed that network kernel extension.\" reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The popular Mac app Bartender was sold two months ago without prior notification to customers, raising transparency concerns.",
      "MacUpdater flagged potential safety issues due to the lack of communication, prompting the new owners to confirm the acquisition on Reddit.",
      "Users are worried about the new owners' intentions and the overall transparency, as details about the sale and certificate change were not initially provided."
    ],
    "commentSummary": [
      "Users on macrumors.com are concerned about the macOS app Bartender auto-updating under a new, unidentified owner, leading them to consider alternatives like HiddenBar, Dozer, and Ice, despite their functionality issues.",
      "The discussion highlights frustrations with macOS requiring third-party tools for better desktop management and the potential risks associated with software updates and ownership changes.",
      "Concerns about potential malware in Bartender's future updates prompt recommendations for disabling auto-updates or using network-blocking tools, with users expressing a desire for native macOS features to manage menu bar icons and customizable settings."
    ],
    "points": 204,
    "commentCount": 97,
    "retryCount": 0,
    "time": 1717540496
  },
  {
    "id": 40578414,
    "title": "Debian 13 Adopts RAM-Based tmpfs for /tmp, Sparking Community Debate",
    "originLink": "https://lwn.net/SubscriberLink/975565/bcce68a68d82d63a/",
    "originBody": "LWN .net News from the source Content Weekly Edition Archives Search Kernel Security Events calendar Unread comments LWN FAQ Write for us User: Password:| Subscribe / Log in / New account Debian's /tmpest in a teapot [LWN subscriber-only content] Welcome to LWN.net The following subscription-only content has been made available to you by an LWN subscriber. Thousands of subscribers depend on LWN for the best news from the Linux and free software communities. If you enjoy this article, please consider accepting the trial offer on the right. Thank you for visiting LWN.net! Free trial subscription Try LWN for free for 1 month: no payment or credit card required. Activate your trial subscription now and see why thousands of readers subscribe to LWN.net. By Joe Brockmeier June 3, 2024 Debian had a major discussion about mounting /tmp as a RAM-based tmpfs in 2012 but inertia won out in the end. Debian systems have continued to store temporary files on disk by default. Until now. A mere 12 years later, the project will be switching to a RAM-based /tmp in the Debian 13 (\"Trixie\") release. Additionally, starting with Trixie, the default will be to periodically clean up temporary files automatically in /tmp and /var/tmp. Naturally, it involved a lengthy discussion first. Join the party By now, using tmpfs for the /tmp directory is a road well-traveled. Many Linux distributions have already switched to mounting /tmp as a tmpfs. Arch Linux, Fedora, openSUSE Tumbleweed, and many others have all made the switch. Red Hat Enterprise Linux (RHEL) and its clones, as well as SUSE Linux Enterprise Server (SLES) and openSUSE Leap, still default to /tmp on disk. Ubuntu, following Debian, also continues to have a disk-based /tmp rather than using tmpfs. The knobs to control how /tmp is mounted, and the handling of temporary files, are part of systemd. (On systems where systemd is used, of course.) The upstream defaults for systemd are to mount /tmp as a tmpfs and delete files that have not been read or changed after ten days in /tmp and 30 days for those stored in /var/tmp. Debian Developer and systemd contributor Luca Boccassi recently decided it was time to revisit the topic of /tmp as a tmpfs, and start deleting temporary files in /tmp and /var/tmp. On May 6 Boccassi resurrected a discussion from Debian's bug tracker that started in 2020 and that had trailed off in July 2022 without resolution. The bug was submitted by Eric Desrochers, who complained that Debian's systemd implementation did not clean /var/tmp by default. Michael Biebl wrote that this was a deliberate choice to match Debian's pre-systemd behavior. After a long period of inactivity, Biebl suggested in October 2021 (and again in July 2022) that Desrochers raise the topic on the Debian devel mailing list. That never happened. In reviving the topic, Boccassi declared that it was time to bring Debian's defaults in line with upstream and other Linux distributions by making /tmp a tmpfs, and cleaning up /tmp and /var/tmp on a timer. He planned to apply those changes in the next systemd upload to Debian unstable within a week, with instructions in the NEWS file on how to override the changes for any users who wanted to keep the current behavior. That, in turn, sparked quite a discussion. The case for and against Biebl worried about the effect of cleaning up temporary files by default. Currently, he said, Debian only cleans /tmp on boot (which is unnecessary if /tmp is a RAM-based tmpfs) and to never clean up files in /var/tmp. Boccassi answered: \"Defaults are defaults, they are trivially and fully overridable where needed if needed.\" Biebl replied that there is value in aligning defaults across Linux distributions, but argued that Debian has \"a larger scope\" than a desktop distribution like Fedora. He suggested that it was necessary to \"gather feedback from all affected parties to make an informed decision\". Boccassi responded that it was impossible to have defaults that would make everyone happy, and he was not looking for hypothetical arguments or philosophical discussions, he wanted facts: \"what would break where, and how to fix it?\" Sam Hartman noted that ssh-agent created its socket under /tmp, but it would be better if it respected the $XDG_RUNTIME_DIR setting and created its socket under /run/user. Boccassi agreed and said that he had filed a bug for ssh-agent. Richard Lewis pointed out that tmux stores its sockets in /tmp/tmux-$UID, and deleting those files might mean users could not reattach to a tmux session that had been idle a long time. Boccassi suggested that using flock() would be the right solution to stop the deletion, and said he had filed a bug on that as well. Lewis questioned whether files downloaded with apt source might be considered \"old\" when they were extracted under /tmp and immediately flagged to be deleted. Russ Allbery said that systemd-tmpfiles would respect the access timestamp (atime) and changed timestamp (ctime), not just the modified timestamp (mtime), \"so I think this would only be a problem on file systems that didn't support those attributes\". Allbery said he believed tmpfs supports all three. Some users store information in /var/tmp for long-running jobs, because they want it preserved without being backed up, said Barak A. Pearlmutter. Boccassi dismissed that, saying that those users \"assuming they actually exist\", can customize the system defaults according to their needs. Those users do exist, wrote Jonathan Dowland, \"[I've] been one of them, I've worked with many of them, it's an incredibly common pattern in academic computing\". Making a change to how these files are handled, \"should be a very carefully explored decision\". He also asked for arguments in favor of the change, and to hold off the change to allow the discussion to continue. Hartman wished that it was possible to specify that directories under /var/tmp should be deleted entirely or left alone. Boccassi said that was a reasonable enhancement request, and that it had already been filed upstream for systemd. Allbery eventually admitted surprise at the number of people who reported using /var/tmp \"for things that are clearly not temporary files in the traditional UNIX sense\". Periodically deleting files in /var/tmp, he said, has been common UNIX practice for at least 30 years: Whatever we do with /var/tmp retention, I beg people to stop using /var/tmp for data you're keeping for longer than a few days and care about losing. That's not what it's for, and you *will* be bitten by this someday, somewhere, because even with existing Debian configuration many people run tmpreaper or similar programs. If you are running a long-running task that produces data that you care about, make a directory for it to use, whether in your home directory, /opt, /srv, whatever. Hakan Bayındır said that he did not use /var/tmp, but applications that other people use do. He cited a high-end scientific application that was not under his or other users' control. Allbery argued that this was \"not a good design decision\" but conceded that may not be helpful if a user or admin does not have control over the application's design. However, he said, it was \"playing with fire\" to use /var/tmp that way \"and it's going to result in someone getting their data deleted at some point, regardless of what Debian does\". Lewis said he still did not understand the rationale for the change. Was there, he asked, \"perhaps something more compelling than 'other distributions and upstream already do this'?\" That sounded like the original rationale, Allbery said, but he added that moving /tmp to tmpfs should make applications run faster, and reaping files under /var/tmp could help to avoid filling up a partition. Allowing the partition to fill up, he noted, could cause bounced mail, unstable services, and other problems. It may not be a problem for desktop systems, which tend to have enough disk space not to be affected, but for virtual machines that have /var/tmp contained within a small root partition, it is still a concern. Rolling out changes In the end, none of the arguments for maintaining Debian's status quo managed to persuade Boccassi to stay his hand. On May 28, Boccassi announced the changes that he had made and uploaded to unstable. As expected, /tmp will become a tmpfs by default, for both new and existing installations of Debian unstable. New installs will use the systemd default behavior. The openssh and tmux packages, he said, had been fixed to provide exceptions to retain their temporary files. A description has been added to the Debian installer to inform users that /tmp is a tmpfs by default, and the changes have been noted in the NEWS file. He also offered to review and merge any changes to the Debian installer that would let users customize those options during installation. Users who want /tmp to remain on disk can override the upstream default with systemctl mask tmp.mount. To stop periodic cleanups of /tmp and /var/tmp users can run touch /etc/tmpfiles.d/tmp.conf. None of the changes, it should be noted, will affect Debian versions prior to Trixie. Users of Debian stable and oldstable will only encounter these changes on upgrade. As noted, many distributions have already made these changes without catastrophe. Debian, and its users, should be able to adapt to the new defaults or override them if they are unsuitable for the workloads to be run. At worst, it promises to be a temporary inconvenience. Did you like this article? Please accept our trial subscription offer to be able to see more content like it and to participate in the discussion. (Log in to post comments) Debian's /tmpest in a teapot Posted Jun 3, 2024 16:31 UTC (Mon) by iustin (subscriber, #102433) [Link] (2 responses) Finally! Now to check if the changes actually break my systems where I already switched to tmpfs… Debian's /tmpest in a teapot Posted Jun 3, 2024 16:38 UTC (Mon) by bluca (subscriber, #118303) [Link] (1 responses) As long as you have done so by either adding tmp.mount in /etc or an entry in fstab, it should be a no-op Debian's /tmpest in a teapot Posted Jun 3, 2024 16:40 UTC (Mon) by iustin (subscriber, #102433) [Link] I'm old school, so fstab entry. Thanks for replying! Debian's /tmpest in a teapot Posted Jun 3, 2024 17:25 UTC (Mon) by mb (subscriber, #50428) [Link] I use tmpfs /tmp on Debian since it had been added to the kernel without any problems. I don't auto-clean it, though. That is not worth the risk, IMO. tmpfs is swapped out to disk if needed. If cleaning is enabled, it should have a size threshold to not clean files that are less than a couple of kiB in size. Removing these files doesn't really help anybody in todays world with dozens of gigabytes of memory. Debian's /tmpest in a teapot Posted Jun 3, 2024 17:46 UTC (Mon) by ju3Ceemi (subscriber, #102464) [Link] (33 responses) I am surprise they did not talked about memory usage One usage I've met many times is using /tmp to store temporary files that are too big to be kept in memory : rootfs are likely bigger than 10GB, while memory is more scarse (and while we can keep such files on disk for some time, it is expensive to use memory than way) Nothing too difficult to handle, I think those users will be able to deal with the change accordingly Debian's /tmpest in a teapot Posted Jun 3, 2024 18:09 UTC (Mon) by smurf (subscriber, #17840) [Link] (28 responses) A RAM /tmp will get swapped out when things become tight. So any memory usage of large files there is, umm, temporary. Presumably swapping has a bit more overhead than writing directly to disk, but that should be more than offset by all the small files you no longer write to disk. Debian's /tmpest in a teapot Posted Jun 3, 2024 18:11 UTC (Mon) by ju3Ceemi (subscriber, #102464) [Link] (20 responses) Yes Is swap really a good practice in high-performance computing environment ? Debian's /tmpest in a teapot Posted Jun 3, 2024 18:32 UTC (Mon) by atnot (subscriber, #124910) [Link] Absolutely! If you're working with a lot of data, it is almost guaranteed that at least some of it will be hotter than whatever else you have in memory. It will thus be beneficial to make the best use of the memory you have by using more of it for things like the page cache. Debian's /tmpest in a teapot Posted Jun 3, 2024 18:33 UTC (Mon) by Cyberax (✭ supporter ✭, #52523) [Link] (2 responses) Surprisingly, yes. Not the old 2x RAM guideline, but some swap helps by freeing RAM. Debian's /tmpest in a teapot Posted Jun 4, 2024 15:01 UTC (Tue) by gray_-_wolf (subscriber, #131074) [Link] Agree. Even on my 128GB RAM server few tens of MB of swap are usually used. Debian's /tmpest in a teapot Posted Jun 5, 2024 17:32 UTC (Wed) by piexil (subscriber, #145099) [Link] Zram is even better than traditional swap, and 2x ram size actually makes sense too Debian's /tmpest in a teapot Posted Jun 3, 2024 18:37 UTC (Mon) by Heretic_Blacksheep (subscriber, #169992) [Link] (1 responses) Pretty sure the kernel has some assumptions where virtual memory layouts are concerned regardless of its size or type. x86 in general has had an assumption that disk area was available for extra RAM as needed for 30+ years regardless of its active use. Bad Things Happen when you run out of RAM. As in, unrecoverable and sometimes impossible to diagnose Things. I can't remember if the Linux kernel will dump its core to /var/crash or if it uses the swap space area for that by default. It's been so long since I saw the kernel completely crash I've forgotten where it puts things that went Boom. Debian's /tmpest in a teapot Posted Jun 3, 2024 18:41 UTC (Mon) by ju3Ceemi (subscriber, #102464) [Link] On the other hand, swap changes nothing here : you can still saturate your memory space, it will just take a bit longer, during which your system will be crippled down Debian's /tmpest in a teapot Posted Jun 3, 2024 18:57 UTC (Mon) by mezcalero (subscriber, #45103) [Link] (6 responses) Yes. Swap is almost always a good idea, because it allows anonymous memory to be swapped out on memory pressure. If you have no swap you thus reduce the pool of memory that can be swapped out (i.e. only mmaped stuff such as executables, libraries and so on, as well as various kernel caches such as dentries/inodes). And a smaller pool of stuff that can be swapped out typically means that that you need to swap out more frequently, because you cannot just swap out the least used stuff. Hence by having no swap you increase memory pressure, and you become more dependent on IO and its latencies. Hence, add swap. It makes things faster in most cases. Debian's /tmpest in a teapot Posted Jun 3, 2024 23:02 UTC (Mon) by josh (subscriber, #17465) [Link] (5 responses) I've tended to avoid swap for my systems, because if something goes wild and uses too much memory, I don't want the kernel spending a literal minute trying to swap things out to make room for it before finally giving up and killing it or failing an allocation. I *do* want the kernel to be able to swap out things it isn't going to use, but I'd love to be able to say \"only swap proactively, if under memory pressure just fire up the OOM-killer\". Debian's /tmpest in a teapot Posted Jun 3, 2024 23:52 UTC (Mon) by atnot (subscriber, #124910) [Link] (1 responses) earlyoom/(systemd-)oomd (chose your flavor, I'm sure there's more now) is exactly that. I'd been waiting for someone to bring it up but it is, in my opinion, an absolutely essential thing to have installed on any kind of modern linux desktop. Debian's /tmpest in a teapot Posted Jun 4, 2024 0:02 UTC (Tue) by atnot (subscriber, #124910) [Link] I should say, earlyoom is somewhat technically less rigorous and do be aware that anything based on the PSI metric will not work properly unless you have at least a sensible amount of swap configured. Otherwise pretty much any memory spike will cause enough pressure to trigger an oom kill. (if you're on e.g. ubuntu and have had applications randomly disappearing a lot recently, this may be why) Debian's /tmpest in a teapot Posted Jun 4, 2024 6:39 UTC (Tue) by ibukanov (subscriber, #3942) [Link] (1 responses) On computers with modern SSD swap is fast and no longer has ill effects on longevity of SSD. With sustained read/write speed of several GB/s at worst one will see a gradual degradation of performance, not a multi-second pause. If SSD is older, the system uses a magnetic disk or SSD is modern but has to be encrypted with software encryption like LUKS, then use compressed memory like zram. It will also provide the benefits of gradual performance degradation when the system runs out of RAM. Debian's /tmpest in a teapot Posted Jun 4, 2024 17:15 UTC (Tue) by patrakov (subscriber, #97174) [Link] Some server owners cheap out and use SATADOM drives (really, slow, as in 25 MB/s, and low-endurance flash drives with a slightly non-standard SATA interface) as OS disks. Placing swap files or partitions on them is a very bad idea. Debian's /tmpest in a teapot Posted Jun 4, 2024 9:22 UTC (Tue) by farnz (subscriber, #17727) [Link] I get that result with a combination of 1 GiB swap and systemd-oomd, on a system with 64 GiB RAM. That's just enough swap that under memory pressure, I don't immediately start swapping executable pages in preference to less-frequently used data, but not so much swap that I thrash. And this gives systemd-oomd just enough headroom to kill off the actual memory hog, rather than an unfairly maligned victim. Debian's /tmpest in a teapot Posted Jun 3, 2024 20:23 UTC (Mon) by NAR (subscriber, #1313) [Link] (2 responses) I don't know about high performance computing environments, but my personal experience with swapping on desktops/laptops in the past 20 years was that as soon as Linux starts to swap due to some process (usually Chrome or the C code I'm writing) using too much memory, I might be better off reaching for the power button, because rebooting is just way faster than waiting out the system recovering from all that thrashing. I thought that the SSD drives solve this problem, but alas, no. In the 90s when the 486 machines had only a few MBs or RAM and swapping was kind of inevitable, it wasn't this bad. Debian's /tmpest in a teapot Posted Jun 3, 2024 20:39 UTC (Mon) by mb (subscriber, #50428) [Link] (1 responses) Well, there are many reasons for swapping. What you describe is extreme memory pressure because there's some runaway process or some process is completely overloading the available hardware resources. You are right that swap hardly helps in these situations. However, swap is also used to free (likely) completely unused pages from memory and use the free memory for something else that improves performance (e.g. disk cache). Unused pages in RAM just reduce the amount of usable RAM. Or it can be used to get rid of tmpfs files from RAM that are never used. That is all happening in the background and has nothing to do with a runaway or overload scenario. At the moment I have ~1% of my RAM freed for useful tasks due to swap. That's not much, but it's better than not having it. Debian's /tmpest in a teapot Posted Jun 4, 2024 10:25 UTC (Tue) by jhe (subscriber, #164815) [Link] I never really understood the urgency of having swap until i learned of the missing link: Executable code is mapped into memory as named pages. No memory for the page cache means your machine will saturate its disk I/O reading the code for the currently running processes. Debian's /tmpest in a teapot Posted Jun 3, 2024 20:41 UTC (Mon) by flussence (subscriber, #85566) [Link] Swap is essential in most HPC setups; you can fit a SI unit order of magnitude more NVMe in a single system than you can for DDR, and the peak bandwidth of both is about even (Zen4 Epyc = 768GB/s DDR5 + 512GB/s×socket PCIe5), so you can swap as fast as your RAM can keep up. Disabling the swap subsystem will not make things faster in the general case - you just lose a \"L5\" cache tier and the kernel has less verbs to work with for managing memory pressure. (Incidentally, the whole point of DAMON/DAMOS is to let the kernel use those verbs more effectively. It's intended for NUMA hot/cold balancing, but the distinction between that and swapping is becoming largely academic and I wouldn't be surprised to see it in use on desktop distros in a few years.) Debian's /tmpest in a teapot Posted Jun 4, 2024 3:14 UTC (Tue) by Paf (subscriber, #91811) [Link] It really depends on your HPC environment. Your full up cluster \"supercomputer\" machines often have no node-local storage at all, so no swap. But that's a different beast - they have NO local storage at all. Debian's /tmpest in a teapot Posted Jun 4, 2024 3:48 UTC (Tue) by ringerc (subscriber, #3071) [Link] (1 responses) Yes, which is why the hostility of Kubernetes to swap drives me quite insane. Debian's /tmpest in a teapot Posted Jun 4, 2024 5:42 UTC (Tue) by zdzichu (subscriber, #17118) [Link] Did you miss https://kubernetes.io/blog/2021/08/09/run-nodes-with-swap... ? Debian's /tmpest in a teapot Posted Jun 3, 2024 18:32 UTC (Mon) by mgedmin (subscriber, #34497) [Link] (4 responses) tmpfs mounts tend to have a size limit (which defaults to something like 50% of RAM by default, I think?). There's a slight risk that programs who try to download large files into /tmp might end up filling it up completely and failing. Debian's /tmpest in a teapot Posted Jun 4, 2024 6:04 UTC (Tue) by smurf (subscriber, #17840) [Link] (3 responses) Better than to fill your root partition which might not be large enough either. Large temporary stuff which you *know* is too large for RAM+swap belongs in /var/tmp IMHO. Debian's /tmpest in a teapot Posted Jun 4, 2024 11:25 UTC (Tue) by Wol (subscriber, #4433) [Link] (2 responses) > Large temporary stuff which you *know* is too large for RAM+swap belongs in /var/tmp IMHO. That's not what /var/tmp is for ... Just redefine /tmp as being larger. Running gentoo, I moved a lot of my compilation temporary dirs into /tmp, and seeing as I always run with masses of swap I think I had /tmp defined as maybe three or four times ram? On the basis that nearly all this stuff was pretty much WORN, and if compiling Firefox, or loffice, or gcc, etc etc pushed my machine into swapping, quelle surprise. But as part of that, I found the definition of /tmp and /var/tmp in the FHS, and discovered that me making /var/tmp a tmpfs too was a big mistake ... /tmp is defined as \"anything that could disappear at any time\", so not surviving a reboot was completely within spec. /var/tmp on the other hand, is stuff \"that isn't needed for long, but that must survive a reboot or crash\". Don't remember the definition exactly, but it's clearly for recovery information, or working sets that should be deleted by the app but often aren't thanks to a crash, yadda yadda. Stuff that is likely to get orphaned and you don't want cluttering up permanent storage. Cheers, Wol Debian's /tmpest in a teapot Posted Jun 4, 2024 11:31 UTC (Tue) by adobriyan (subscriber, #30858) [Link] > Running gentoo > making /var/tmp a tmpfs too was a big mistake For new gentooers finding this post: /var/tmp/portage is Gentoo's graveyard of failed builds. Set up some kind of autocleaner against the directory. Debian's /tmpest in a teapot Posted Jun 5, 2024 5:03 UTC (Wed) by wtarreau (subscriber, #51152) [Link] Same for me, having my home on NFS, I use /tmp for anything temporary (as its name implies), and I can copy there a full SD card of photos, or build an entire distro or series of cross-compilers. I do expect to have all the free space of my / available in /tmp, i.e. ~240GB from a 256GB SSD, something I'll never have in RAM. When I want something in RAM, I'm using /dev/shm which has been available as a tmpfs everywhere for well over a decade. While I can understand the rationale for mounting /tmp as a tmpfs, I think that some users will complain that their system is super slow, because while swap does help freeing cold pages, when a system starts to swap out due to filling a tmpfs with lots of stuff, all caches are purged first, and even task switching can be slow because previously inactive processes have been swapped out. These are the situations where you can see processes lag a lot, especially with graphical environments often composed of lots of processes. Another annoying case is with low-memory systems. I don't know how much RAM debian wants these days, but having only a few gigs in /tmp can definitely be a limiting factor for some users. I think another approach could be to have /tmp as disk storage and /tmp/ram be ram-based. But there's no solution that would satisfy everyone anyway, so probably that commenting/uncommenting a line in fstab remains the best option. After all, if users are no longer able to add a hash at the beginning of a line in fstab, we probably have much more serious problems to solve! Debian's /tmpest in a teapot Posted Jun 3, 2024 21:49 UTC (Mon) by LtWorf (subscriber, #124958) [Link] (1 responses) Swapping, to make software running faster. I'm sure there's a lot of situations where that backfires. Debian's /tmpest in a teapot Posted Jun 4, 2024 6:08 UTC (Tue) by smurf (subscriber, #17840) [Link] Your assumption is noted but with the exception of \"there's so much memory pressure that the system is swapping like crazy\" (implying that without swap the OOM killer would trigger instead; you don't want either of these situations) or possibly \"/usr is on a fast SSD but your swap partition lives on rotating rust\" I can't think of any. Debian's /tmpest in a teapot Posted Jun 3, 2024 18:33 UTC (Mon) by bluca (subscriber, #118303) [Link] (2 responses) > I am surprise they did not talked about memory usage That was very much intentional, because that sort of rat-holing is why this failed to happen before. For each and every anecdote about why it's not an issue, there will be an opposite example from somebody else. This is a default that can be trivially customized, and that includes both completely disabling it or setting a max filesystem limit, and a myriad of other combinations. If it works out of the box, great - if not, just customize it as needed. Debian's /tmpest in a teapot Posted Jun 3, 2024 19:09 UTC (Mon) by Heretic_Blacksheep (subscriber, #169992) [Link] (1 responses) I agree there's likely a near infinite number of reasons why /tmp being on a RAMFS is a Bad Idea, and an equal number of reasons why it's a Good Idea (same with the anecdotes). I kinda agree with the article title that it's a tempest in a teapot. If the use case contraindicates the default, this particular configuration is trivial to change without even a reboot. If you need a whole fleet of deployments, there are tools to manage configurations before creating the deployment image. Debian's /tmpest in a teapot Posted Jun 3, 2024 19:20 UTC (Mon) by bluca (subscriber, #118303) [Link] Exactly - managing fleet-wide changes to small config files in /etc/ has been a solved problem for a long time, and there are dozens of solutions readily available Debian's /tmpest in a teapot Posted Jun 4, 2024 10:28 UTC (Tue) by abo (subscriber, #77288) [Link] While not explicit in the FHS (but perhaps somewhere else), it's usually better to use /var/tmp for large files. It's supposed to be preserved during reboot so it probably isn't tmpfs. https://refspecs.linuxfoundation.org/FHS_3.0/fhs/ch05s15.... Debian's /tmpest in a teapot Posted Jun 3, 2024 18:29 UTC (Mon) by mgedmin (subscriber, #34497) [Link] (1 responses) In the year 2000 I was about to travel abroad for studying, and I knew that I wouldn't have Internet access in the dorms. I decided to recursively wget some webcomics like User Friendly and burn them into a CD-ROM, so I would have something to pass the time. I decided that /tmp is the perfect place for a staging area to download a bunch of files and make an ISO image for burning, and I left wget running overnight. In the morning I discovered a collection of interesting facts: (1) wget sets the mtime according to the Last-Modified header, (2) my system had a daily cron script that deleted old files in /tmp, (3) as a result of (1) and (2) my webcomic archives only had a few of the latest files left. A Lesson Was Learned that day. (Also, I somehow managed to re-download the files I needed and burn the CD-ROM before my flight.) Debian's /tmpest in a teapot Posted Jun 4, 2024 5:55 UTC (Tue) by smurf (subscriber, #17840) [Link] (4) you should have posted a bug report, if the script had also checked ctime and atime this wouldn't have happened. Debian's /tmpest in a teapot Posted Jun 3, 2024 20:01 UTC (Mon) by rknight (subscriber, #26792) [Link] (6 responses) Debian still supports some systems with very low amounts of RAM available. I have a few arm systems running Debian that only have 64MB or 128MB available. So being able to easily override this change during installation will be necessary to prevent an OOM while installing! Debian's /tmpest in a teapot Posted Jun 3, 2024 20:15 UTC (Mon) by bluca (subscriber, #118303) [Link] (2 responses) Why would that be necessary? There's only a few dozens of bytes on an idle, just-installed minimal system Debian's /tmpest in a teapot Posted Jun 3, 2024 21:52 UTC (Mon) by python (subscriber, #171317) [Link] (1 responses) A few dozen *Megabytes* is quite huge on a 64Mb system. I also deal with embedded systems but would never dream of cramming Debian onto a 64mb board. Surely apt would just crash? Debian's /tmpest in a teapot Posted Jun 3, 2024 22:24 UTC (Mon) by bluca (subscriber, #118303) [Link] Not megabytes, bytes. Why would there be dozens of MBs in /tmp/ on an empty, just-installed system? I just booted a just-installed x86_64 Trixie VM and there are 8 empty directories in /tmp/, and nothing else Debian's /tmpest in a teapot Posted Jun 3, 2024 21:18 UTC (Mon) by Klaasjan (subscriber, #4951) [Link] (2 responses) I am interested. What version of Debian are you running on those arm systems? And will the trixie installer support systems with that amount of RAM? Debian's /tmpest in a teapot Posted Jun 4, 2024 13:41 UTC (Tue) by rknight (subscriber, #26792) [Link] (1 responses) See https://forum.doozan.com/list.php?2 for Debian support on different Marvell SOC based devices which include many NAS servers some of which only have 64MB or 128MB of RAM. Debian's /tmpest in a teapot Posted Jun 4, 2024 19:39 UTC (Tue) by Cyberax (✭ supporter ✭, #52523) [Link] If they have such a small disk, you definitely should NOT use it for large temporary files, because it will wear out the flash quickly. Debian's /tmpest in a teapot Posted Jun 3, 2024 21:44 UTC (Mon) by LtWorf (subscriber, #124958) [Link] (13 responses) I forgot to disable this on one of my machines. So of course I had a compilation fail because all the 7GB of space on my now very tiny /tmp got occupied by dependencies. It seems one of these things that you have to do when you install a Debian machine. Like disable the mouse in vim. Debian's /tmpest in a teapot Posted Jun 4, 2024 1:40 UTC (Tue) by mathstuf (subscriber, #69389) [Link] (12 responses) Umm…what \"compilation\" requires 7GB? And thinks that `/tmp` is a good place to put it? Debian (and derivatives) are the only systems still using non-tmpfs it seems, so it smells like a Debian-specific (or at least -focused) tool? Building snaps perhaps? Or perhaps whatever other distros do to avoid the problem could be applied to Debian as well? Debian's /tmpest in a teapot Posted Jun 4, 2024 8:59 UTC (Tue) by dsommers (subscriber, #55274) [Link] (11 responses) Large C++ projects can easily consume a GB or more per compilation thread. I've not dug into how much of that hits the disk without -pipe. But I have seen GCC putting some work data to /tmp Debian's /tmpest in a teapot Posted Jun 4, 2024 9:22 UTC (Tue) by bluca (subscriber, #118303) [Link] (10 responses) And if someone is in the 0.x% of those cases where GBs of data are regularly written to /tmp/ without any control, they can simply configure their system accordingly. A Debian installation will not start autonomously compiling GBs of C++ intermediate data into /tmp/ by itself, if that happens it is user-driven. Debian's /tmpest in a teapot Posted Jun 4, 2024 10:19 UTC (Tue) by LtWorf (subscriber, #124958) [Link] (9 responses) > 0.x% of those cases [Citation needed] As I said in another comment, downloading an .iso file was enough to kill my entire session. I don't think that \"downloading a file to then dd on a USB device and never use it again\" is a rare thing among linux users. Debian's /tmpest in a teapot Posted Jun 4, 2024 10:28 UTC (Tue) by bluca (subscriber, #118303) [Link] (8 responses) And that's why your home has a Downloads directory, which is used by browsers by default. It's completely absurd to demand that defaults should adapt to your specific use case of downloading ISOs to /tmp. Just configure your system according to your needs, it's not that hard. Debian's /tmpest in a teapot Posted Jun 4, 2024 10:49 UTC (Tue) by LtWorf (subscriber, #124958) [Link] (7 responses) > And that's why your home has a Downloads directory Which is not automatically cleaned, which is why I didn't use it in that case. > It's completely absurd to demand that defaults should adapt to your specific use case of downloading ISOs to /tmp. It seems to me that the use case you have in mind is \"do not write anything at all ever in /tmp\". Do you have any source that it is the normal use case and I am indeed the only one person in the whole world that uses it? Why not remove /tmp completely then? Debian's /tmpest in a teapot Posted Jun 4, 2024 10:51 UTC (Tue) by bluca (subscriber, #118303) [Link] (6 responses) > Which is not automatically cleaned, which is why I didn't use it in that case. You can trivially set up a tmpfiles.d in your home that cleans it, if you want that behaviour. Or you can use /var/tmp. The possibilities are endless. > It seems to me that the use case you have in mind is \"do not write anything at all ever in /tmp\". No, this is a strawman that you have just made up. Debian's /tmpest in a teapot Posted Jun 4, 2024 15:34 UTC (Tue) by Wol (subscriber, #4433) [Link] (2 responses) > You can trivially set up a tmpfiles.d in your home that cleans it, if you want that behaviour. Or you can use /var/tmp. The possibilities are endless. And so is the (mis)information on the net. What you're forgetting, is that a lot of people just want to USE their computers. And trying to find information about eg tmpfiles.d is just a nightmare of ill-informed blogs and crap documentation. I know pretty much the minimum I need to do to administer my personal home server. One of the reasons it runs gentoo is it was a personal choice that means I need to know more than most. But finding the information I need is usually search terms that don't find what I'm looking for, search engines that assume they know better than I do what I'm looking for so they ignore critical terms, as I said, ill-informed blogs, answers that didn't read the question, etc etc. Just because the possibilities are endless doesn't mean they are within reach of mere mortals - all too often they've disappeared up the wazoo of the infinite impossibility search engine ... Cheers, Wol Debian's /tmpest in a teapot Posted Jun 4, 2024 15:56 UTC (Tue) by bluca (subscriber, #118303) [Link] (1 responses) People who just want to USE their computers won't even know /tmp/ exists, because desktop applications like browsers don't use it and it doesn't show up among the desktop icons, they use ~/Download and ~/Documents and friends. If one worries about file retention policy of /tmp, then they can find out ways to configure it. Or not, find it the hard way and learn a valuable lesson in the process. The fact that this is not an issue on other distros suggests all this hair-pulling is blown-out of proportion - as the title of the article suggests. Debian's /tmpest in a teapot Posted Jun 4, 2024 17:28 UTC (Tue) by Wol (subscriber, #4433) [Link] Unfortunately, they DO need to know about it ... As someone else said, things like attachments in emails are generally stored in temp (be it c:\\temp or \\tmp or yada yada) and the grief I get with \"I edited an attachment and saved it, where have my changes gone?\" Some people learn, unfortunately some people CAN'T learn. What's that saying? \"Every time man invents something idiot-proof, nature invents a better idiot\". I get people are upset that Debian is being dragged kicking and screaming into the 21st century, but it doesn't mean the 21st is any better than the 20th ... Cheers, Wol Debian's /tmpest in a teapot Posted Jun 5, 2024 5:15 UTC (Wed) by wtarreau (subscriber, #51152) [Link] (2 responses) > > It seems to me that the use case you have in mind is \"do not write anything at all ever in /tmp\". > No, this is a strawman that you have just made up. No need to be aggressive and condescending again Bluca. I'm in the same situation with my home over NFS, I use /tmp *a lot*, for the same reasons. Where do I download LLMs ? Into /tmp. Where do I compile large stuff ? In /tmp as well, because it's fast (especially with SSDs these days). There are plenty of small use cases like this. I'm totally fine with uncommenting a line in /etc/fstab to preserve the old behavior though. But I think that during installation it would be reasonable to check the available RAM size and consider a size below which the user is asked whether they want /tmp as tmpfs or not, because running OOM on older systems that were once considered as \"still good enough to run debian\" will likely cast a bad image of that distro. Debian's /tmpest in a teapot Posted Jun 5, 2024 9:16 UTC (Wed) by bluca (subscriber, #118303) [Link] (1 responses) > No need to be aggressive and condescending again Bluca. There is also no need to invent arguments that nobody made, and yet here we are > I'm in the same situation with my home over NFS, I use /tmp *a lot*, for the same reasons. NFS is not enabled by default either, so as part of your actions to setup NFS, you can also configure your tmp according to your needs Debian's /tmpest in a teapot Posted Jun 5, 2024 13:37 UTC (Wed) by edgewood (subscriber, #1123) [Link] > NFS is not enabled by default either, so as part of your actions to setup NFS, you can also configure your tmp according to your needs Yes. But that's what he said 4 sentences after the one you quoted. Debian's /tmpest in a teapot Posted Jun 3, 2024 21:53 UTC (Mon) by Thalience (subscriber, #4217) [Link] (16 responses) User: What do you mean my trash automatically emptied itself? That's where I keep my important files! Debian's /tmpest in a teapot Posted Jun 4, 2024 1:32 UTC (Tue) by raven667 (subscriber, #5198) [Link] (3 responses) I've told this story before but its still funny to me, I had a user I supported, back with MS Mail (pre-Exchange), who did exactly that. They asked me to help free space on their desktop (probably at 20MB drive), so I looked around and emptied their deleted email folder, seemed sensible. They freaked out because that's where they kept their important mail, because it was just one key press to delete mail vs more keys to file in a folder. lol Debian's /tmpest in a teapot Posted Jun 4, 2024 2:52 UTC (Tue) by ringerc (subscriber, #3071) [Link] I've had the same except it was a Windows box \"Recycle Bin\". Their rationale? That's where they put stuff they're done with but might need later. They'd empty it at the end of the month. Obviously absurd given the real world analogy, you say? But no. The cleaners had strict instructions not to empty this person's actual rubbish bin either. For the same reason. The network backups excluded the \"recycle bin\". Items had been moved there long enough ago that they'd aged out of the backup retention window. Someone came in to stand in for them at their job and emptied it. That was fun. Debian's /tmpest in a teapot Posted Jun 4, 2024 6:08 UTC (Tue) by epa (subscriber, #39769) [Link] (1 responses) That's a kind of Hyrum's Law applied to users. No matter how strongly the label of the folder says \"Deleted\", the observable behaviour is that items sent there are *not* deleted, not without an explicit further step. People will rely on that. Debian's /tmpest in a teapot Posted Jun 4, 2024 6:55 UTC (Tue) by magfr (subscriber, #16052) [Link] Yes. I am suffering with outlook webmail at work and there I have a deleted folder, putting something there obviously doesn't delete anything but if I actively delete stuff from the deleted folder I then get the option to restore it for something like a month. Debian's /tmpest in a teapot Posted Jun 4, 2024 7:58 UTC (Tue) by LtWorf (subscriber, #124958) [Link] (11 responses) First time I tried a system with /tmp on tmpfs (fedora) I downloaded an .iso in /tmp. That triggered the OOM killer, which killed wayland. My whole session got killed because I dared to use /tmp. It wasn't even full. At this point, if we don't want to have /tmp, we could just not have a /tmp at all. Debian's /tmpest in a teapot Posted Jun 4, 2024 8:43 UTC (Tue) by smurf (subscriber, #17840) [Link] (10 responses) That's a broken combination of not enough RAM, /tmp-in-RAM, and no swap. Long term, /tmp-on-disk won't save you here. Debian's /tmpest in a teapot Posted Jun 4, 2024 9:07 UTC (Tue) by LtWorf (subscriber, #124958) [Link] (9 responses) Not enough RAM? It was a completely normal laptop with 8 GB of RAM, not some sort of low cost embedded board. A fresh install did that. So it was just the system defaults. Anyway let's not forget that the advice online has been \"do not create swap\" for decades. So there's probably thousands of installations out there without swap (not mine) that are ready to start crashing once users start to receive this on their systems. Debian's /tmpest in a teapot Posted Jun 4, 2024 9:28 UTC (Tue) by smurf (subscriber, #17840) [Link] (3 responses) Well I have no idea who advises people not to create swap partitions … people who want to sell you a new box with more RAM? In any case, on a laptop you also need the swap space for hibernation (suspend to disk). Debian's /tmpest in a teapot Posted Jun 4, 2024 10:54 UTC (Tue) by LtWorf (subscriber, #124958) [Link] (1 responses) People on the internet. 5 seconds on google will find their usernames. If you want their real names and addresses I cannot help you. Suspend to disk is slow, suspend to RAM is fast… Also if your swap is full of the contents of your /tmp you can't hibernate anyway. Debian's /tmpest in a teapot Posted Jun 4, 2024 13:02 UTC (Tue) by smurf (subscriber, #17840) [Link] Suspend to RAM won't help you when you need to turn the thing off due to an empty battery. So don't fill the whole swapspace with your /tmp. It defaults to being limited to half of RAM for a reason. This is mostly orthogonal to /tmp-in-RAM, you have the same problem if (when …) your browser has a memory leak and ate your whole RAM+Swap when you want to hibernate. Debian's /tmpest in a teapot Posted Jun 4, 2024 18:39 UTC (Tue) by MarcB (subscriber, #101804) [Link] There are VM and container environments that *severely* over-commit IO. That makes a lot of sense if the workloads match this. I have seen sustainably allowed IOPS as low as 100 and even bursts only up to 500. Heavy swapping can easily exceed those values. If the swapping happens on many VMs at once - maybe because they run the same buggy software or run into the same problematic data - the whole cluster might get into real trouble. But in all scenarios where your storage gives sufficient guarantees, you really should use swap. Debian's /tmpest in a teapot Posted Jun 4, 2024 11:27 UTC (Tue) by intelfx (subscriber, #130118) [Link] (3 responses) > Anyway let's not forget that the advice online has been \"do not create swap\" for decades That's just crappy advice. It happens in all parts of life. Why should the world bend over to wrong things someone wrote and perpetuated on the Internet? You don't see vaccine centers shutting down because of anti-vax bullshit :-) Debian's /tmpest in a teapot Posted Jun 4, 2024 12:04 UTC (Tue) by malmedal (subscriber, #56172) [Link] (2 responses) > > Anyway let's not forget that the advice online has been \"do not create swap\" for decades > That's just crappy advice. It's obsolete advice, a 7200 RPM desktop drive can do something like 72 seeks per second. I had plenty of workloads that would work fine without swap(or at least be OOM-killed), but completely hang the machine when swap was enabled. With an SSD-drive the problem went away so it's reasonable to enable swap again. Debian's /tmpest in a teapot Posted Jun 4, 2024 12:06 UTC (Tue) by intelfx (subscriber, #130118) [Link] (1 responses) > I had plenty of workloads that would work fine without swap(or at least be OOM-killed), but completely hang the machine when swap was enabled. I'd hazard a guess and say this just meant `vm.swappiness` had been set to a wrong value. Debian's /tmpest in a teapot Posted Jun 4, 2024 15:23 UTC (Tue) by malmedal (subscriber, #56172) [Link] I did some experiments at the time, with real and synthetic workloads, only thing that worked was reducing swap-size. The disk at the time could read 150M/sec sequentially, but only 288K/sec with random access 4K blocks. Typical for a 7200RPM desktop-drive. The performance cliff when swap was just a little bit too large was so tall that I just turned it off. Debian's /tmpest in a teapot Posted Jun 4, 2024 11:39 UTC (Tue) by Wol (subscriber, #4433) [Link] > Anyway let's not forget that the advice online has been \"do not create swap\" for decades. So there's probably thousands of installations out there without swap (not mine) that are ready to start crashing once users start to receive this on their systems. Don't forget that, round about 2000, linux 2.4, don't remember the details, Linus decided to force a rewrite of the memory management / swap code. At which point, people who ran vanilla kernels and didn't read the release notes suddenly found out the HARD way that the old wives tale of \"swap should be twice ram\" really was true. Any system that didn't have that much swap just crashed the instant the swap code was invoked. The guidance in the notes said \"no swap, or at least twice ram. NOTHING INBETWEEN\". And there were plenty of people who'd been relying on the fact that it was an old wives' tale for years even back then. Which is why my machines always have masses of swap. May be totally unnecessary today, but nobody's ever come back to me and said \"yes the rewrite really did remove that requirement\" - they just assume that because everybody says \"twice swap\" is an old wives' tale, then it must be, despite that being proved resoundingly wrong this century-ish. Cheers, Wol Debian's /tmpest in a teapot Posted Jun 4, 2024 5:43 UTC (Tue) by stop50 (subscriber, #154894) [Link] the forgot to mention a few alternatives: /var/lib/: not cleaned up, usually no restrictions, useful for your service /var/cache: like /tmp but for longer lived data, but you must expect them to be cleaned up. Personally i would go for /var/lib, but you must clean it up yourself. Debian's /tmpest in a teapot Posted Jun 4, 2024 6:57 UTC (Tue) by magfr (subscriber, #16052) [Link] (33 responses) Did they fix emacs as well? Emacs used to put the socket that emacsclient connects to in /tmp Debian's /tmpest in a teapot Posted Jun 4, 2024 8:16 UTC (Tue) by epa (subscriber, #39769) [Link] (32 responses) If even a tiny socket file can't reliably be stored in /tmp because it might get unpredictably deleted, then what exactly can /tmp be used for? Debian's /tmpest in a teapot Posted Jun 4, 2024 8:56 UTC (Tue) by bluca (subscriber, #118303) [Link] (31 responses) Sockets should very much NOT go to /tmp. /tmp is world writable, and it needs to be used with extreme care, with randomly generated subdirs via mktemp for starters, otherwise you are left vulnerable to hijacking by unprivileged processes - but that means the path is unpredictable and thus needs a side-channel to securely communicate it to the client of the socket, which defeats the point. system services can use /run, and user processes can use XDG_RUNTIME_DIR (which is also in /run), as those have appropriate permissions. Debian's /tmpest in a teapot Posted Jun 4, 2024 9:01 UTC (Tue) by epa (subscriber, #39769) [Link] (30 responses) Yes, I'm aware of the problems with /tmp symlink attacks and other attacks. It's a minefield, as you say. So, in that case, is there *any* sensible use of /tmp? In principle no long-running task can use it because old temporary files can be deleted... but the definitions of \"long-running\" and \"old\" are hard to pin down. Reliable software can't use a mechanism which works at first but fails after a week or two. Debian's /tmpest in a teapot Posted Jun 4, 2024 9:18 UTC (Tue) by smurf (subscriber, #17840) [Link] (28 responses) Of course there are uses for /tmp. Plenty of them. My email reader stores attachments there. Your compiler writes its transient files to /tmp. So does the initramfs builder. And so on. The correct place for transient stuff that shouldn't be cleaned (and also doesn't need to survive a reboot), these days, is /run/user/‹uid›/. Debian's /tmpest in a teapot Posted Jun 4, 2024 9:30 UTC (Tue) by bluca (subscriber, #118303) [Link] (24 responses) Yes, it's a scratch area for private data (again: to be used with mktemp and friends) that is fine to lose at any time. It's not the right place where to _share_ data. Debian's /tmpest in a teapot Posted Jun 4, 2024 11:36 UTC (Tue) by epa (subscriber, #39769) [Link] (23 responses) But what data is “fine to lose at any time”, is my question. I am having a hard time thinking of examples. Most of the time software will break if the temporary file it was using gets deleted. There are a few applications which need a pure cache, entirely able to keep working if the cache files are deleted, but most current users of /tmp are not like that. Debian's /tmpest in a teapot Posted Jun 4, 2024 11:41 UTC (Tue) by bluca (subscriber, #118303) [Link] (22 responses) If any software actually exists that expects to drop critical files in /tmp, not touch them/access them for more than a week, and barfs when it goes back and doesn't find them, then that software is simply broken. It can be fixed in a number of ways as explained at https://systemd.io/TEMPORARY_DIRECTORIES/ I'm not sure it actually exists though, it really sounds like a strawman. Debian's /tmpest in a teapot Posted Jun 4, 2024 12:24 UTC (Tue) by epa (subscriber, #39769) [Link] (3 responses) I wouldn’t be so sure. Generally “anything that can go wrong, will go wrong” and a program that wasn’t envisaged to take a week could end up doing so — after all, it will not have an explicit check of time taken. It’s possible that an attacker could note the filename used, arrange to block the target process, and let its temp file get reaped. Then create a new one with the same name. This is speculation, of course, but a generation ago the idea of deliberate hash collisions would have been seen as a straw man, or many similar “don’t do that” or “will never happen” scenarios. Since the reaping period is defined to be at least 10 days (and not purely site-dependent) that takes away a lot of the worry in practice. Debian's /tmpest in a teapot Posted Jun 4, 2024 14:34 UTC (Tue) by bluca (subscriber, #118303) [Link] (2 responses) If \"attacker takes over filename\" is part of the threat model, _DO NOT USE /tmp_ for that data - no ifs, no buts, just don't. All these tortured strawmen are exactly what I meant when I said \"I am not really looking for philosophical discussions or lists of personal preferences or hypotheticals\". If the new defaults do not work for you, just do the trivial one-line configuration change. Arguing about why your preferred configuration is the one true default and everything else is wrong is a waste of time and is not going to achieve anything at all. Debian's /tmpest in a teapot Posted Jun 4, 2024 14:51 UTC (Tue) by epa (subscriber, #39769) [Link] (1 responses) Thanks, sorry I didn't realize it was you. I didn't intend to waylay your work with hypotheticals -- it looks like a good improvement and certainly the status quo ante wasn't perfect either. If \"attacker takes over filename\" is part of the threat model, _DO NOT USE /tmp_ for that data Perhaps I am not understanding, but that seems to rule out /tmp for almost anything on a multiuser system. Many programs expect to create a file in /tmp and then reference it by name later. I know that the best way is to create the file in a single atomic action and get back the file *handle*, no longer caring about the name, but there are plenty of times when you want to run an external program and pass it the name of the temp file you created. For example running an external diff command to print the differences between two strings. I think that these cases outnumber those when you're content to use an anonymous file handle. If you're saying that is never safe, not even when done with great care and exclusive mode opening and all the rest, then our positions are not so far apart. The safe uses of /tmp are a lot fewer than is common practice. Debian's /tmpest in a teapot Posted Jun 4, 2024 14:58 UTC (Tue) by bluca (subscriber, #118303) [Link] It really depends on the threat model and countermeasures and so on. Using an O_TMP in a mktemp subdir (with enough characters in the template) is most likely ok for things that need to be safe against pre-emptive hijacking and that are not high value - caches and whatnot. But if there's anything worth stealing - just don't use tmp, there are so many safer alternatives, and are not really new either - XDG_RUNTIME_DIR has been around for at least a decade, probably more. For sockets - just don't. It is honestly baffling that there are still programs doing that, like tmux. Debian's /tmpest in a teapot Posted Jun 4, 2024 15:51 UTC (Tue) by mb (subscriber, #50428) [Link] (17 responses) > that software is simply broken No, it's not broken. It worked just fine until you started deleting files. > It can be fixed in a number of ways Yeah. But you are breaking existing userspace here. Is there a difference, if the kernel breaks userspace or if systemd breaks userspace? I'm really annoyed by this. Can you please give one valid reason, why you absolutely have to delete a couple of bytes from /tmp after it was sitting there without a problem for 10 days? If we are talking about megabytes or gigabytes. Yeah. Fine. Establish a cleanup strategy, because it actually does help. But deleting small files and sockets? Just don't. There is no way this is going to help anybody, but plenty of room for breakage. Debian's /tmpest in a teapot Posted Jun 4, 2024 16:10 UTC (Tue) by bluca (subscriber, #118303) [Link] (16 responses) > No, it's not broken. It worked just fine until you started deleting files. Yes, it is, and no, it didn't, as the article says at the very top this has been the norm on other distros for a decade. > But you are breaking existing userspace here. Nothing is broken, this is made-up nonsense. Just stop wasting time and write the one line config file to customize your machine as you want it. Complaining is not going to change anything. > But deleting small files and sockets? Just don't. There is no way this is going to help anybody, but plenty of room for breakage. Size limit is not the only problem, especially on a tmpfs there is also an inode limit, and saturating a tmpfs with zero size files is very much possible and DOSes other uses of /tmp. So it's just as important to remove unused small files as it is large files to ensure the shared resource remains usable. Debian's /tmpest in a teapot Posted Jun 4, 2024 16:31 UTC (Tue) by mb (subscriber, #50428) [Link] (15 responses) >especially on a tmpfs there is also an inode limit, and saturating a tmpfs with zero size files is very much possible and DOSes other uses of /tmp. Ok. Can you give us some numbers and show how that is not \"made-up nonsense\" (quoting yourself), please? How many files must be created to exhaust the available space? Which real world application is affected? Traditional disk based /tmp surely has the same problem, right? So there has never been a version of Linux without this \"problem\". But there surely have been decades of Linux that don't delete files from /tmp at random times. >write the one line config file to customize your machine as you want it My machine _is_ configured the way I want and you are changing it. And I didn't read a plausible explanation for why this is needed, yet. You are changing user interfaces. You are doing the very same thing that you constantly complain about. Debian's /tmpest in a teapot Posted Jun 4, 2024 16:51 UTC (Tue) by bluca (subscriber, #118303) [Link] (14 responses) > Ok. Can you give us some numbers and show how that is not \"made-up nonsense\" (quoting yourself), please? Depends on the system, just check it out - df -hi /tmp > But there surely have been decades of Linux that don't delete files from /tmp at random times. Sure there have - tmpfiles.d has been deleting files in /tmp, in Debian too, for the past ~15 years, and before that there were things like tmpreaper. Other OSes like Solaris also delete stuff there by default. You are pretending this is some sort of new, last minute development - it isn't, it's ancient history. > You are changing user interfaces. You are doing the very same thing that you constantly complain about. Nonsense. Major version of distributions change things all the time, that's why we publish this thing called \"release notes\" where changes are noted. If you don't want changes, stay on the same stable LTS version forever. Distributions, including Debian, never, ever said \"we won't break compatibility across major version upgrades\", quite the opposite, every new release changes something - that's the entire point of doing a new Debian release. The parallel you are trying to draw makes no sense, as the kernel _does_ say \" we do not break compatibility\". That makes all the difference, and pretending it doesn't is just silly. Debian's /tmpest in a teapot Posted Jun 4, 2024 17:28 UTC (Tue) by mb (subscriber, #50428) [Link] (13 responses) >just check it out - df -hi /tmp Ok. >$ df -hi /tmp >Filesystem Inodes IUsed IFree IUse% Mounted on >tmpfs 4.4M 37 4.4M 1% /tmp >$ for i in $(seq 0 100000); do touch /tmp/x/$i; done >$ df -hi /tmp >Filesystem Inodes IUsed IFree IUse% Mounted on >tmpfs 4.4M 98K 4.3M 3% /tmp So it takes ~4.5 million empty files to fill up the available inode space. It doesn't convince me that this limit can be used to argue that possibly used files must be deleted after 10 days. Is that limit hit more frequently than applications breaking due to deleted files? Debian's /tmpest in a teapot Posted Jun 4, 2024 17:38 UTC (Tue) by bluca (subscriber, #118303) [Link] (12 responses) > It doesn't convince me that this limit can be used to argue that possibly used files must be deleted after 10 days. There must be some misunderstanding here - I am not arguing for anything, not trying to convince you of anything. I'm simply explaining how things work and why. If you don't like how they work, that's too bad - feel free to configure your machine differently as already explained, you are free to configure your laptop or your desktop or your server as you see fit, it really doesn't make any difference to me, these settings are all configurable by design Debian's /tmpest in a teapot Posted Jun 4, 2024 18:37 UTC (Tue) by mb (subscriber, #50428) [Link] (11 responses) >I am not arguing for anything Well, but you should. You are changing the Debian world here. >feel free to configure your machine differently Keep in mind that you are changing my machine behavior here. You change how the Debian system works and I have to revert back to the old behavior. It's the responsibility of you as a maintainer to have actual good reasons for a change. And you should be able to explain them. Yet, I did not see more than basically that you'd like it better if the files were deleted and \"somebody else does it, too\". There is a huge responsibility with being a maintainer. If you make a change that requires millions of people to change just one configuration line to get the old behavior back, you are wasting millions of hours of work time. There should be a *good* reason for these changes. Debian's /tmpest in a teapot Posted Jun 4, 2024 18:43 UTC (Tue) by pizza (subscriber, #46) [Link] (6 responses) > Well, but you should. You are changing the Debian world here. No, he (nor the other systemd developers) is doing nothing of the sort. *DEBIAN* is changing its own world. Debian's /tmpest in a teapot Posted Jun 4, 2024 19:28 UTC (Tue) by mb (subscriber, #50428) [Link] (5 responses) Please grep the Debian systemd package changelogs for \"bluca\". Debian's /tmpest in a teapot Posted Jun 4, 2024 20:46 UTC (Tue) by pizza (subscriber, #46) [Link] (4 responses) > Please grep the Debian systemd package changelogs for \"bluca\". Sounds like you need to file a release blocker bug with Debian, but since this is fundamentally about overruling a Debian packager's decision, you'll probably need to escalate this to the TC or to a full project-wide vote. Either way, you'll need a better technical argument than \"I don't like it and I can't be arsed to make a one-liner configuration change\" Debian's /tmpest in a teapot Posted Jun 4, 2024 20:50 UTC (Tue) by bluca (subscriber, #118303) [Link] It's even sillier than that, given it's only enabled for new installs, not on upgrades: \"I don't like it and I can't be arsed to make a one-liner configuration change if I ever were to reinstall from scratch\" Debian's /tmpest in a teapot Posted Jun 4, 2024 21:25 UTC (Tue) by mb (subscriber, #50428) [Link] (2 responses) > Either way, you'll need a better technical argument Yes. Changes don't require technical argument. \"Only one reason is needed: I have decided as such.\" (bluca). And the users require technical argument against it. I got it and I will respect it. I will certainly remember that rule. Debian's /tmpest in a teapot Posted Jun 4, 2024 21:57 UTC (Tue) by pizza (subscriber, #46) [Link] (1 responses) > Changes don't require technical argument. Except for the numerous times the technical merit of the \"new\" default has been explained, even in this thread. And then there's also the little detail where where the rest of the Linux world has been doing it this \"new\" way for about a decade. Those discussions (including technical arguments) are all part of their public record. > \"Only one reason is needed: I have decided as such.\" Hate to break this to you, but the overwhelming majority of Debian packages also fall under this category - Both the packager and the upstream authors make countless such decisions every single day. And again, if you (or whomever) doesn't like these decisions, feel free to invoke the Debian process for overruling a packager and/or upstream. Second request Posted Jun 4, 2024 21:59 UTC (Tue) by corbet (editor, #1) [Link] I repeat: this has all been said, no more minds will be changed (if any ever were). I think we can stop this back-and-forth now. Debian's /tmpest in a teapot Posted Jun 4, 2024 19:11 UTC (Tue) by bluca (subscriber, #118303) [Link] (3 responses) > Keep in mind that you are changing my machine behavior here. No. The next Debian release, due next year, will change your machine's behaviour in many ways, as it will be documented in the (lengthy) release notes. If you like it - good. If you don't - too bad, either reconfigure appropriately or stay on the current version until it goes EOL, and then start paying somebody to give you support beyond that. > There should be a *good* reason for these changes. Only one reason is needed: I have decided as such. Don't like it? Don't worry, you can have your money back, give me an address and I'll post you a cheque for £0. Debian's /tmpest in a teapot Posted Jun 4, 2024 19:13 UTC (Tue) by bluca (subscriber, #118303) [Link] (and to be really pedantic, as noted in the changelog, in fact the tmpfiles.d rules will not be applied to existing machines, but only to new installations) Debian's /tmpest in a teapot Posted Jun 4, 2024 19:26 UTC (Tue) by mb (subscriber, #50428) [Link] (1 responses) >Only one reason is needed: I have decided as such. Ok. That's the rule and I will also apply it to you. A place to stop Posted Jun 4, 2024 19:33 UTC (Tue) by corbet (editor, #1) [Link] So ... once again we would appear to have reached a point where everybody has said their piece — several times — and any minds that might conceivably be changed will have been changed. This seems like a good place to stop this back-and-forth, please. Debian's /tmpest in a teapot Posted Jun 4, 2024 11:34 UTC (Tue) by epa (subscriber, #39769) [Link] (2 responses) But if the compiler writes its transient files to /tmp and they get deleted before the compiler has finished, it will break. Okay, the deletion time is typically much longer than a build runs for, but as far as I know this is not specified anywhere. Same for attachments — you could leave your desktop for a week’s holiday and expect it to be working on your return. Debian's /tmpest in a teapot Posted Jun 4, 2024 11:39 UTC (Tue) by bluca (subscriber, #118303) [Link] (1 responses) It is specified - 10 days for /tmp and 30 for /var/tmp, and that includes mtime/atime/ctime. Have you _actually_ experienced that issue, or is it pure speculation based on cursory understanding of how this works? Debian's /tmpest in a teapot Posted Jun 4, 2024 12:02 UTC (Tue) by epa (subscriber, #39769) [Link] That’s great. I had thought the reaping period was probably unspecified. While in general I dislike timing-dependent effects, such as timeouts after inactivity on a socket connection, they are a fact of life. Debian's /tmpest in a teapot Posted Jun 4, 2024 9:33 UTC (Tue) by bluca (subscriber, #118303) [Link] And also the automated file deletion can be easily and reliably stalled with a flock: https://systemd.io/TEMPORARY_DIRECTORIES/ (or a permanent opt out via tmpfiles.d 'x' directive, but that's not as good) Debian's /tmpest in a teapot Posted Jun 4, 2024 12:28 UTC (Tue) by smoogen (subscriber, #97) [Link] (3 responses) I have seen various academic and research circles teach /tmp is the place to store date for 30+ years for different reasons. 1. Long ago, quotas were put on various systems home directories. What didn't have a quota was /tmp as it was considered scratch space and might even have a dedicated disk to it. Cleanup usually happened once a year or so and you might even put a larger disk on it versus other places. While home quotas and such got rarer as 'shared' computers moved more to workstations and personal laptops, the trend to put large stuff in /tmp by default in everything was going strong even in 2009 when I left that space. Most of the time I have seen someone complain about Fedora using swap /tmp it has been in academic circles where something broke at the wrong time and data was lost. 2. There are different definitions of time for research and systems administration. A system admin might think something in /tmp living longer than a day or a week needs to be moved to something more permanent. A researcher may think that the 6 months it took to gather raw data or something is temporary and might even need a year. That data may be thrown away afterwards so is 'temporary'. Add in the fact that Debian is the NetBSD of Linux (and I mean this as a compliment). It is ported and maintained on hardware systems which have not been produced in decades, and meant to work on systems with megabytes of memory while also working on terabytes of memory. This means that general solutions which might work in 90% of a different distro cause larger complaints due to expectations that someone's SPARC10(?) or M68000 system will still install cleanly in 2024. I would expect that in the older Debian installers this would be a question to answer.. do you want /tmp to be tmpfs, dedicated partition, or on / so that the various solutions could be implemented. I don't know if that is at all possible with the newer installers. Debian's /tmpest in a teapot Posted Jun 4, 2024 18:27 UTC (Tue) by mathstuf (subscriber, #69389) [Link] HPC systems (that I encounter) tend to have `/scratch` (or similar) partitions meant for exactly that. When filesystems get full (80%+ based on those I receive), emails go out and if things persist, things in `/scratch` are fair game for deletion. There is also dedicated space for project results that should be kept that live in project-tracked directories. This allows the operators to account storage to projects (like CPU/GPU access is accounted) properly. I'd be surprised if `/tmp` was used as such on a machine that was really simultaneously: - unaccounted to projects - not subject to sharing restrictions - has an expectation to persist for long periods of time - critical to the mission of the machine owner - under such lax operator scrutiny that a Debian default change like this comes in silently (though given the first two points, maybe they really are just far on a scale of negligence) Debian's /tmpest in a teapot Posted Jun 5, 2024 5:39 UTC (Wed) by wtarreau (subscriber, #51152) [Link] When I was a student, we had a 8MB quota in our homes, and on multiple shared machines there was 1 to 2 GB in /tmp. So with a friend we got used to creating large files there and using mtools (mdir, mcopy etc) over that file pretending to be a disk image, to store all our data. We got used to know what machine to connect to depending on the type of file we were looking for. That was totally ugly but fun :-) Debian's /tmpest in a teapot Posted Jun 5, 2024 7:21 UTC (Wed) by nim-nim (subscriber, #34454) [Link] 1. People definitely use /tmp and /var/tmp for big datasets (including but not limited to downloads) 2. That is not limited to academic circles 3. The main reason for this reliance is beancounter and sysadmin avoidance : /tmp and /var/tmp are sort of a gray area that allows ignoring the limits set by the people charged with provisioning storage (people that know that storage and budgets are not infinite) 4. Users are usually right in pointing they need a large scratch storage area to work 5. Beancounters and sysadmins are usually right that infinite storage does not exist in the real world, storage gets filled and BAD THINGS happen 6. A good compromise is a large scratch space where things decay before they accumulate too much (relying on humans to housekeep a scratch area is hopeless) 7. Like all real-world compromises you will find borderline cases where the default limit is set to the wrong value Debian's /tmpest in a teapot Posted Jun 4, 2024 16:07 UTC (Tue) by gray_-_wolf (subscriber, #131074) [Link] (5 responses) It is bit shame that this will make the guix package unreliable out of the box. The builds do happen in /tmp, and they rarely fit into the RAM (e.g. firefox takes ~12GB, linux kernel ~18GB of disk space in /tmp), so some packages will just fail to build. I wonder how to tackle that. Maybe the /tmp should be (by default) of reasonable size (say 50 GB) with large enough swap created by default? Debian's /tmpest in a teapot Posted Jun 4, 2024 16:16 UTC (Tue) by smurf (subscriber, #17840) [Link] (3 responses) If you prefer to build firefox or the kernel with \"make -j ‹nontrivial›\" then you're assumed to be tech-savvy enough to adjust your tmpfs settings to make that work. Or guix could do that for you, out of the box. Debian's /tmpest in a teapot Posted Jun 4, 2024 16:27 UTC (Tue) by gray_-_wolf (subscriber, #131074) [Link] (2 responses) I fail to see how `-j whatever' relates to usage in /tmp. Peak RAM usage I do see the connection, but not for the disk space. Without -j the disk usage will grow slower, but it will reach the same total peak number. My point being that there are programs that legitimately use many gigabytes in /tmp, and \"tmpfs with max size of 1/2 of RAM\" will just not work for them. I took guix package of an example that is packaged by debian, and therefore it seems reasonable to have it work out of the box. Debian's /tmpest in a teapot Posted Jun 4, 2024 16:31 UTC (Tue) by bluca (subscriber, #118303) [Link] Such packages can simply be configured to use something else like /var/tmp, then Debian's /tmpest in a teapot Posted Jun 5, 2024 1:56 UTC (Wed) by nakedhitman (subscriber, #90828) [Link] > I fail to see how `-j whatever' relates to usage in /tmp. Depending on what you're compiling, each compilation thread will use up to a certain amount of temp space. That bit me a few times, but once I knew what it was, I was easily able to work around it. > My point being that there are programs that legitimately use many gigabytes in /tmp, and \"tmpfs with max size of 1/2 of RAM\" will just not work for them. tmpfs will work just fine for use cases that exceed the size of RAM. Just add swap (even better with zswap) and size accordingly. That was the solution in my case, and my system has been better for it overall. Systems really should be using swap anyway, and tmpfs should default to using a size that matches either swap or 50% RAM, whichever is greater. Debian's /tmpest in a teapot Posted Jun 5, 2024 7:53 UTC (Wed) by intelfx (subscriber, #130118) [Link] > It is bit shame that this will make the guix package unreliable out of the box. The builds do happen in /tmp> I wonder how to tackle that. Maybe by making Guix NOT do that? :-) I do not use Guix, but I do have a home-grown package builder tool with lots of bells and whistles for painless automation (because I fork or patch so many packages). One of the first things I taught this tool is a dedicated \"this package is large\" annotation, which I manually use to mark packages like web browsers or toolchains or Linux kernel. If this annotation is not present, the build happens in /tmp, otherwise it happens in /var/tmp. If Guix desires maximum efficiency in building packages, surely it must not be hard to implement something like that. Debian's /tmpest in a teapot Posted Jun 4, 2024 19:37 UTC (Tue) by dw (subscriber, #12017) [Link] (25 responses) Bewildered that this is even a thing. You don't write a program to use a temporary file unless it is likely necessary, i.e. unknown input set, or known input set that exceeds RAM. There's very little other reason to actually design something to use the filesystem when in-memory would suffice. To then come along and say \"surprise, we moved your filesystem back into RAM, but it's okay, because if you run out of RAM, we'll just page out half your system (including the function that draws your mouse pointer) back to disk, so now you must enable swap too\" frankly just angers me. I have no idea why anyone is still looking at this part of the stack or who is paying for that work, it's just creating dumb breakage for absolutely stupid gains. Not bothering with the list thread, but presumably these fuzzy feelings weren't backed up by any meaningful metrics? Debian's /tmpest in a teapot Posted Jun 5, 2024 1:58 UTC (Wed) by nakedhitman (subscriber, #90828) [Link] > Not bothering with the list thread Right back at ya. Metrics missing Posted Jun 5, 2024 8:57 UTC (Wed) by koh (subscriber, #101482) [Link] (23 responses) I've read the entire list of comments on this article and you're right, there do not seem to be made arguments for the change based on any specific metric, which is quite annoying as it points to there being no technical reason whatsoever to change the default in Debian. Looks like it's being done because all the other kids are doing it. Criticism is being framed as \"you don't like it - too bad\" multiple times here and you're being told that the solution is to change the new default settings. That line of reasoning only makes sense if it's clear you're an outlier in the dataset proving that a majority of users would benefit. No such data has been put forth here. I can add one data point here: > $ uptime > 10:46:36 up 35 days, 19:40, 1 user, load average: 1.01, 1.06, 0.98 > $ df -hi /tmp > Filesystem Inodes IUsed IFree IUse% Mounted on > none 7.4M 125 7.4M 1% /tmp This my personal laptop used every day, also for work, without periodic /tmp cleaning. What I do is removing specific files in /tmp from programs I know at specific points in time when I know those programs aren't running and might potentially use those files. Knowledge of either cannot reliably be obtained by any automated solution. Metrics missing Posted Jun 5, 2024 9:24 UTC (Wed) by bluca (subscriber, #118303) [Link] (22 responses) > No such data has been put forth here. That's correct, and it's not going to be, as it would be completely pointless: for something like this one can produce millions of manufactured anecdotes that go one way or the other. The previous defaults also weren't established following the rigorous analysis of any large dataset, but simply through inertia. However, given you like those old defaults and they happen to fit your use case, you don't seem to have any problem with the lack of data backing them up. Funny how that works, eh? > This my personal laptop used every day, also for work, without periodic /tmp cleaning. That's great - then you can use one of the several available configuration options to fine-tune it to your specific needs, which is of course fully supported and well documented. Metrics missing Posted Jun 5, 2024 10:58 UTC (Wed) by koh (subscriber, #101482) [Link] (21 responses) > However, given you like those old defaults and they happen to fit your use case, you don't seem to have any problem with the lack of data backing them up. Funny how that works, eh? To be honest, I don't care either way. What I care about is changing defaults in spite of existing technical criticism. Changing defaults is a decision affecting the technical level, so this criticism should be taken seriously. In my opinion, a less condescending attitude towards it would go a long way to increase acceptance. Just to drive this point home: my Laptop does not use Debian and has a tmpfs based /tmp and /var/tmp. The change won't affect it. But it will affect some of the servers I manage - for no specific or specified reasons. That would be my complaint. I understand that change in general is necessary, and I'm not opposed. But change for the sake of change is counter-productive. What's the reason for it to have to take place in this fashion of automated 10 days / 30 days cleanups? Is it possible to name programs cluttering those directories which need cleaning up after? Are users just too lazy and keep filling up those directories with junk? Is a time limit really the best way to decide what's junk and what isn't? Is \"10 days\" being determined for a constantly changing system clock (meaning: ntp)? Metrics missing Posted Jun 5, 2024 11:47 UTC (Wed) by bluca (subscriber, #118303) [Link] (20 responses) > What I care about is changing defaults in spite of existing technical criticism. There is zero \"technical criticism\" (left - there were some valid concerns and some bugs about a handful packages, and I've fixed them all already). There are a bunch of personal preferences and anecdotes and strongly held personal opinions, but I don't care about any of that, sorry, because for each one of those preferences and anecdotes and opinions, you can find its exact opposite too. If the new settings don't fit with your personal use cases, you are fully empowered to customize them as needed, and it's beyond trivial to do so. Complaining about how they don't work for your very special and super-duper important use case is not going to achieve anything useful. Metrics missing Posted Jun 5, 2024 12:04 UTC (Wed) by koh (subscriber, #101482) [Link] (15 responses) > There is zero \"technical criticism\" (left - there were some valid concerns and some bugs about a handful packages, and I've fixed them all already). I see, so all those comments about actual programs expecting /tmp not be cleaned up is no criticism about changing the defaults, got it. > Complaining about how they don't work for your very special and super-duper important use case is not going to achieve anything useful. a) Here is a prime example of the condescending attitude mentioned earlier. b) I have not, which makes this a nice strawman. Keep up the good work. Metrics missing Posted Jun 5, 2024 12:14 UTC (Wed) by bluca (subscriber, #118303) [Link] > I see, so all those comments about actual programs expecting /tmp not be cleaned up is no criticism about changing the defaults, got it. Correct - they are personal anecdotes, or in a lot of cases, even just made up hypotheticals (but what about if...) Metrics missing Posted Jun 5, 2024 12:24 UTC (Wed) by smurf (subscriber, #17840) [Link] (5 responses) > comments about actual programs expecting /tmp not be cleaned up is no criticism about changing the defaults, Correct – they are not. They report a bug in $PROGRAM (and/or its packaging) which should be taken upstream and/or fixed. /tmp is not and never was persistent. /tmp-in-RAM or not doesn't matter. Metrics missing Posted Jun 5, 2024 12:38 UTC (Wed) by koh (subscriber, #101482) [Link] (4 responses) Personally, I would disagree. Usage of /tmp for temporary files is not a bug in a program per se. It very much depends on what the program is doing and should therefore (in my opinion) not be controlled by an oblivious set of rules based on no statistical or otherwise validated usage in the wild. For instance, I work on a program that verifies semantics of other C programs. This program does use /tmp to store a bunch of C headers and other files to give the Clang frontend the illusion of working on a particular sysroot. How long the verification (and thus potential usage of those /tmp files) lasts is unclear: As we all know, in general it's not a computable problem, and even for a specific subset where it is, it might take a long time. 10 days is just arbitrary at this point. I can't change defaults on rented servers not under my control. So there's that problem with defaults. Changing them makes this problem of a technical nature. Note: I'm not claiming that my use case is \"super-duper important\" here. Just pointing out a technical issue that makes Debian for those servers less interesting for me. I would argue that usage of /tmp to populate a temporary sysroot for a single verification task is well within the scope of /tmp, thus there's no bug in my program. Persistence of /tmp is not relevant for this point. Metrics missing Posted Jun 5, 2024 12:56 UTC (Wed) by bluca (subscriber, #118303) [Link] (3 responses) > I can't change defaults on rented servers not under my control. So there's that problem with defaults. Fortunately for your use case, as per documentation, your program can just take an flock on the file(s) it is using, and they will be skipped over, no need to change configuration. Also, it's not just ctime, but mtime and atime too that are taken into account for the aging algorithm, again as documented. So it doesn't matter when the file was created, if it was just accessed it will be left where it is. Metrics missing Posted Jun 5, 2024 13:02 UTC (Wed) by koh (subscriber, #101482) [Link] (1 responses) Many thanks for pointing me to flock. I can probably use that Linux-specific hack. The reason I'm regarding this as a hack is that for no other OS known to me it is necessary to take measures to ensure that tmp-files created by a program stay there as long as the program is running. Metrics missing Posted Jun 5, 2024 13:22 UTC (Wed) by daroc (editor, #160859) [Link] A) I'm glad you learned about flock. B) Jon asked for a very similar thread (back and forth without anyone changing their position, covering many of the same points, including flock coming up) to stop yesterday. We're all smart people here; I am certain that we can all generalize. I don't think anything new is being said, which means we can stop here. Metrics missing Posted Jun 5, 2024 14:10 UTC (Wed) by smurf (subscriber, #17840) [Link] > and atime too Thus /tmp-on-RAM should arguably work *better* than /tmp-on-root-with-no-or-rel-atime-option, which has been the default for ages. Metrics missing Posted Jun 5, 2024 12:53 UTC (Wed) by pizza (subscriber, #46) [Link] (7 responses) > I see, so all those comments about actual programs expecting /tmp not be cleaned up is no criticism about changing the defaults, got it. See, I don't get this. Debian's default behaviour *today* is to periodically reap /tmp. Meanwhile, my very first Linux installation (27 years ago!) came with a nightly /tmp cleaner-upper cron job. And the multi-user Solaris systems I had accounts on also (fairly aggressively) reaped /tmp. Also, let me quote the Linux FHS, section 3.18: \"Programs must not assume that any files or directories in /tmp are preserved between invocations of the program.\" And section 3.15: \"The /var/tmp directory is made available for programs that require temporary files or directories that are preserved between system reboots. Therefore, data stored in /var/tmp is more persistent than data in /tmp.\" (Note that this text is from the FHS3.0 release of 2015, but the same language can be found at least as far as v2.2 from 2002) Going back even further to the pre-FHS FSSTND v1.0 spec (from Early 1994), one will find this: /tmp is used for temporary files, preferably on a fast device (a memory based filesystem, for instance). The \"persistence\" of the data that is stored in /tmp is different from that of data which is stored in /var/tmp. /tmp may be cleaned out at boot time or at relatively frequent intervals. Therefore, data stored in /tmp should not be expected to remain for any long period. Programs should use /tmp or /var/tmp (which was originally /usr/tmp) according to the expected requirements of the data, but should not rely on any particular persistence for any temporary storage directories. /tmp may be on a RAM disk. /var/tmp should never be located on a RAM device. tl;dr: Putting /tmp into a ramdisk, frequent reaping, and/or clearing it on every boot, has been part of its very definition for at least 30 years on Linux, and quite likely longer than that on \"real\" UNIXen. And any programs expecting files to persistent in /tmp, much less do so forever [1], have *always been broken* according to the documented standards of the system they were intended to run on. [1] Without out-of-band system administrator intervention. Metrics missing Posted Jun 5, 2024 13:38 UTC (Wed) by paulj (subscriber, #341) [Link] /tmp was a ram based tmpfs going back to at least Solaris 2.4 or so, by my own experience. And wikip says it was by default going back to 2.1, availability of tmpfs going back to SunOS 4. Metrics missing Posted Jun 5, 2024 13:47 UTC (Wed) by koh (subscriber, #101482) [Link] (5 responses) > See, I don't get this. Debian's default behaviour *today* is to periodically reap /tmp. My Debian 10 based server doesn't seem to be doing that, without me having changed the settings. Which program/daemon is doing that? > Also, let me quote the Linux FHS, section 3.18: > > \"Programs must not assume that any files or directories in /tmp are preserved between invocations of the program.\" OK, I'm fine with that. What happens to programs that run longer than 10 days? I've described an example in a comment above. From other comments on this article I gathered that sockets and the like for daemons should be moved to /run, so at this point the question becomes: what's the difference between a long-running process (which uses, say, 100% CPU continuously) and a long-running daemon that should put its stuff under /run? > /tmp may be cleaned out at boot time or at relatively frequent intervals. Therefore, data stored in /tmp should not be expected to remain for any long period. That's just an unusable rule. Basically makes /tmp worthless as there are no longer any guarantees whatsoever. Rightly so it's no longer been specified in this crude way - at least I hope. Well, up to now, I guess. Metrics missing Posted Jun 5, 2024 14:12 UTC (Wed) by farnz (subscriber, #17727) [Link] (4 responses) The big use case for /tmp that's always been OK is \"write out a small file that another program will immediately take in as input\". If the dirent gets deleted not long after creation, that's A-OK, because the reader has already opened it and has a lock on the inode as a result (the file contents are therefore safe until the file is closed). Basically for the case where you can't pipe output to the input of the other program, because it needs to seek or otherwise do file-like things on it, not pipe-like things. Metrics missing Posted Jun 5, 2024 14:48 UTC (Wed) by koh (subscriber, #101482) [Link] (3 responses) > \"write out a small file that another program will immediately take in as input\" [...] lock on the inode I suppose I should be glad that we finally have the following sequence atomic and uninterruptible: 1. spawn child process 2. wait until child is initialized and starts running 3. wait until child decides to open *the right file* All this time I was under the impression that it's not, but now appearently we can be sure that noone presses ctrl-z or puts the computer to sleep while compiling something... Uses for /tmp Posted Jun 5, 2024 15:29 UTC (Wed) by farnz (subscriber, #17727) [Link] (2 responses) We don't have that sequence; but you need some form of error handling for that case, anyway, since it's possible that you write out the file to a drive that physically fails between write and read - and there's never been a guarantee that the failure of the drive that contains /tmp will take down the system as a whole. Uses for /tmp Posted Jun 5, 2024 15:37 UTC (Wed) by mb (subscriber, #50428) [Link] (1 responses) There is a huge difference between hardware failures and \"cleanup\" daemons deleting files. Drive failure: The error reaction is: Throw an error and notify the user. Change the driver and the next run will succeed. A \"cleanup\" daemon deleted my files: A sane error handling strategy does not exist. Next time the \"cleanup\" will delete my files again. The \"cleanup\" is the root cause of failure. Uses for /tmp Posted Jun 5, 2024 16:48 UTC (Wed) by farnz (subscriber, #17727) [Link] In the case where the user paused your program for multiple days, then got surprised, it's \"re-run without stopping at a critical moment, and the next run will succeed\". The cleanup won't delete your files again unless you have that long pause, which in the case I was responding to was introduced with Ctrl-Z or system suspend. The underlying issue is the expectation that data in a temporary location is safe for an arbitrary amount of time; I could have replaced \"Ctrl-Z\" and wait with \"Ctrl-Z, then rm -fr /tmp/*\" for the same effect. You should already be handling \"the system-wide shared temporary location might get erased while in use\", because there's already lots of ways in which it could get erased behind your back; making it a tmpfs, or adding a clean-up daemon, is just one more way on the existing list of ways you could lose data in /tmp. Metrics missing Posted Jun 5, 2024 12:19 UTC (Wed) by mb (subscriber, #50428) [Link] (3 responses) >but I don't care about any of that The biggest problem of all is how you communicate with people. Metrics missing Posted Jun 5, 2024 12:52 UTC (Wed) by bluca (subscriber, #118303) [Link] (2 responses) Nope. Some things (many things) are well worth discussing. Some things just aren't, as it's just an endless stream of \"but what about _MY_ use case\" and it's literally impossible to do something that won't have any such complaints. In this case, where everything is trivially configurable and customizable, the only sensible course of action is to make a choice (and upstream is the best place to make such a choice), implement it, fix the couple of issues that crop up in-distro (already done), and document it for the rest. There is literally nothing of value to be gained from having an endless discussion about what's the best default for something that is entirely down to personal preferences - ask 10 people, you'll get 10 different answers. There is no great insight to be found, no clever solution, no lightbulb moment is just around the corner, just an endless stream of whinging. So yes, I don't care about any of that, and I am honest enough to just say it as it is, sorry (not sorry). Metrics missing Posted Jun 5, 2024 16:21 UTC (Wed) by rschroev (subscriber, #4164) [Link] (1 responses) But in a case like this, there is not even a need to make a decision. I feel there's a very big difference between making a choice in a situation without history where the only consideration is the relative merits of the different choices, versus a situation where one of the choices was already chosen historically. In the latter case, the disadvantage of changing an existing situation needs to be taken into account too. If one choice is clearly better, it'll probably be worth it even it means exposing people to change. What we have here is a case where one of alternatives has been in use for a very long time. At the same time seems that none of the alternatives is a clear winner (or at least that's the feeling I get from reading these comments). So why change? It leads to some people having to change their configuration, it leads to endless discussion, and as far as I can see has no real benefit. From this discussion and the one about keepassxc, I get the feeling that Debian maintainers see every release as a new starting point, with not much consideration for keeping things the same between releases. What happened to \"If it ain't broken, don't fix it\" and the principle of least surprise? > There is literally nothing of value to be gained from having an endless discussion about what's the best default for something that is entirely down to personal preferences - ask 10 people, you'll get 10 different answers. So yes, choose a default, ignore the bike shedding. But once it's chosen (which in this case happened a long time ago), *stick with it* (until a clearly better alternative presents itself). It could very well be that there actually is a good reason to change here, but honestly I haven't seen it anywhere in this whole discussion. Metrics missing Posted Jun 5, 2024 16:24 UTC (Wed) by bluca (subscriber, #118303) [Link] > But once it's chosen (which in this case happened a long time ago), *stick with it* Nothing was chosen, there was just inertia. tmpfs as a filesystem wasn't even available at the very beginning. Time-based deletion Posted Jun 5, 2024 12:37 UTC (Wed) by dskoll (subscriber, #1630) [Link] I hesitantly jump in to the fray here... I am not a fan of time-based deletion. Rather than making it the default, IMO it should be off by default with a very simple way to turn it on for those who want it. Otherwise it turns /tmp into a weirdly-broken disk drive. I've been running with a ramdisk for /tmp for years, and current stats are as follows: $ df -h /tmp/ Filesystem Size Used Avail Use% Mounted on tmpfs 32G 2.3M 32G 1% /tmp after an uptime of about 3 days, but this is typical even after much longer uptimes. (Yes, I have 64GB of RAM in the machine.) I've never seen /tmp even come remotely close to filling up, so time-based deletion is completely unnecessary for me. Not a huge deal Posted Jun 5, 2024 18:17 UTC (Wed) by wsy (subscriber, #121706) [Link] I already keep a repo of custom config for my machines. It's just another customization to carry, not a huge deal for me. Copyright © 2024, Eklektix, Inc. Comments and public postings are copyrighted by their creators. Linux is a registered trademark of Linus Torvalds",
    "commentLink": "https://news.ycombinator.com/item?id=40578414",
    "commentBody": "Debian's /tmpest in a teapot (lwn.net)193 points by jwilk 22 hours agohidepastfavorite242 comments jimbobthrowawy 4 minutes agoI only learned that disk backed /tmp/ wasn't the default a couple of months ago after mainly using ubuntu for years and wondering why I was quickly running out of ram on a fedora machine. reply candiddevmike 22 hours agoprev/var/tmp, IMO, shouldn't exist--the idea of temporary files that should persist beyond a reboot is tech debt, those files should exist under a proper /var/{cache,lib} directory. Podman uses this directory and it drives me nuts. The XDG base directory spec (https://specifications.freedesktop.org/basedir-spec/basedir-...) was supposed to solve all of this... reply NekkoDroid 20 hours agoparent/tmp/ is the ram-backed version for small files, while /var/tmp/ is the disk-backed version for large files and I also think both should be clean at bootup. In my personally opinion the reaping period is way too long, especially for /tmp/ as only few people really have an actual uptime that long but it does make somewhat sense for /var/tmp/ as it currently is \"persistent\". reply Terr_ 10 hours agorootparent> I also think both should be clean at bootup. I'm OK with a RAM-backed /tmp getting cleaned every boot simply because that's part of the technical tradeoff, however \"on boot\" is a bad trigger because its behavior is unreliable and can easily backfire and harm a user. A classic example would be if the power is interrupted and the computer restarts. Unless the user is present and savvy enough to block it somehow, all of their short-term \"throwaway\" project/data just got deleted before they actually finished with it. In other words, the difference to users between \"I won't need this after tomorrow\" versus \"I won't need this by next boot\" can be very important. reply jraph 6 hours agorootparent> Unless the user is present and savvy enough to block it somehow, all of their short-term \"throwaway\" project/data just got deleted before they actually finished with it. That's on them. I use /tmp like this, because I'm lazy and don't want to have to mess with a directory I need to routinely clean up, but I know that if the computer crashes, it's gone. That's on me. If you don't like this risk, you need to work in persistent storage. /tmp is for programs' temporary files. Manually using it like this is abusing it. That's fine, it doesn't hurt anyone beside the user doing it, but they need to bear the consequences of doing it. reply Spivak 1 hour agorootparent> If you don't like this risk, you need to work in persistent storage. Which is literally what /var/tmp is and what the parent is advocating for. However, clearing /tmp at boot is not the greatest technical choice. There's nothing really special about rebooting the system. The idea that files in /tmp are kept around for anywhere between minutes, hours, and days makes for bad policy which systemd-tmpfiles fixes quite elegantly. You shouldn't lose your temp file after 30 seconds because your machine lost power. You should be able to say \"files created here are cleared after $short_time of inactivity.\" reply semi 1 hour agorootparentdoes inactivity mean writes? reads? stat() calls? IMO setting the expectation of 'wont survive a reboot' is more consistent. but even that fails with private tmp files that could be tied to ephemeral units reply Spivak 45 minutes agorootparentmtime atime ctime? If it's good enough for logrotate it's good enough for /tmp. reply stuaxo 8 hours agorootparentprevIf the system went wonky because the disk was full, then clearing this on boot might help me get to a working system. reply hda111 12 hours agorootparentprevDepends on the distro. In Debian /tmp is backed by the disk. reply jkaplowitz 9 hours agorootparentThe article we’re discussing is exactly about Debian changing that for the next release. (The only thing that will be changing in this respect is the default, and possibly the configuration at the moment of upgrade to the next release. Debian has long allowed users to configure both RAM-backed and disk-backed /tmp, and both will remain possible.) reply bayindirh 12 hours agorootparentprev> In Debian /tmp is backed by the disk. In Debian /tmp was backed by the disk. Trixie will come with RAM based /tmp. Update is pushed. reply crabbone 5 hours agorootparentprev> /tmp/ is the ram-backed version for small files Small you say? The block size in that device is the size of the memory page: 4K by default. That's 8 times the very typical 512b for persistent storage. I've been bitten many times when trying to start in-memory only VMs with a disk image that measured smaller than available memory only to run out of memory on boot :) Let's say it's not really for small files. It's for few files. ;) reply pjmlp 8 hours agoparentprevAs someone that knows Linux since Slackware 2.0 in 1995, the adaption of XDG has been quite flexible across the Linux/BSD landscape. reply throw0101c 19 hours agoparentprev> is tech debt Or it's useful system behaviour. To-MAY-to, to-MAH-to. > […] should exist under a proper /var/{cache,lib} directory. Are these directories available to regular users like the tmp dirs are? reply numpad0 6 hours agorootparentdisclaimer: IIUC, never experienced first hand. /var was originally for network booting machines. / is read-only fileshare and /var is for user storage. /home -> /var/home too. It's campus wide workstations installation type of thing. So, /var/tmp is kind of wrong, and so is /var/log/messages on a laptop, and besides the entire /var is useless on most systems. But it's not a huge waste of resources either, so it's left as it is along the rest of directories under / for years. /var wasn't originally intended to be a system directory, it's just a variables directory. Almost by definition it's writable. reply Smar 15 hours agorootparentprev/home/.cache/ is for regular users. reply thayne 15 hours agorootparentSystem users often don't have a real home directory, but need somewhere to write files too. reply karmarepellent 12 hours agorootparentIf feel like this is the moment where these discussions about old filesystem layouts start to go in circles. Yes, system users tend to not have a home directory. But you are fully able to create one for a new system user if this is what you want to do. You are also able to create all kinds of temporary directories and read-only filesystems for a process (e.g. using systemd) if this is what you want do to. Yes there is a lot of fragmentation and no real standard, which is a little disappointing, but you sure have options to work with. And these options do not have to involve any legacy-ish filesystem components like /var/tmp that nobody knows the purpose of. reply dheera 13 hours agorootparentprevand /home/.aws/, /home/.config/, /home/.docker/, /home/.dotnet/, /home/.dropbox/, /home/.mplayer/, /home/.vscode/, ... reply belorn 6 hours agoparentprevThey could merge /var/tmp and /var/cache, with the intention to discourage future use of /var/tmp. With users, programs and admins already using /var/tmp as a persistent cache, a name change seems like the best solution. reply zbowling 21 hours agoparentprevNah. XDG base directory was supposed to solve a ton of things 18 years ago, except none of them are hard-hitting user problems most folks really care about most of the time unless you are really anal about hidden folders in your home directory polluting things up (most people aren't), so it fails with partial support from apathy from some package devs on Linux to implement it. Honestly, we should probably abandon the hope that we will ever get full universal adoption. It stinks even worse with the half-adoption state we are in now, with some apps writing to those directories and other apps not. If it wasn't for arch Linux folks trying to drive apps to adopt it more, I don't think it would be. Hell, default Debian/Ubuntu still doesn't set XDG_* directory ENVs by default (some flavors do), and some apps ignore the prescribed \"if not set, use this default\" nonsense in the spec and do what they have always done because it doesn't break compatibility for users. Part of the spec stinks, too, like the spit between cache/config/data config where the spec has no written rules specified on when anything is expected to be cleaned up and when or what can or should be backed up by the user. Let's move to containerizing most apps, putting everything in little jails so we don't have to deal with where they want to write to. They can have their own consistent and expected behaviors on their own island. Snapcraft, flatpack, or any of a bunch of other solutions are already available to solve this problem. Don't have to worry about what some app left behind or wrote to your system when it's all contained. reply account42 8 hours agorootparent> Hell, default Debian/Ubuntu still doesn't set XDG_* directory ENVs by default (some flavors do) You are supposed to leave them unset if they match the defaults. Programs ignoring the spec is another matter and should be fixed in those programs not by needlessly bloating the environment of every process with meaningless data. reply zbowling 1 hour agorootparent> meaningless data Things like .git and .ssh not are not meaningless data even though they are not in ~/.config/ssh/ or ~/.local/shared/git. reply pseudalopex 19 hours agorootparentprev> It stinks even worse with the half-adoption state we are in now, with some apps writing to those directories and other apps not. It stinks about half as much as before. > Part of the spec stinks, too, like the spit between cache/config/data config where the spec has no written rules specified on when anything is expected to be cleaned up and when or what can or should be backed up by the user. Caches don't have to be backed up and can be deleted if you need space. Most people will want to back up configs. What other data should be backed up is a question different people will answer differently. And containers don't solve this. reply immibis 16 hours agorootparentI actually back up my Gradle cache in particular because things have a tendency to disappear from the internet sometimes, and permanently break builds of older stuff. reply thesnide 12 hours agorootparentThat stuff that disappears, do you have means to patch and rebuild it? Otherwise, you're directly heading for a nasty surprise at the worst moment. reply immibis 3 hours agorootparentThe build dependencies are the problem. The build process downloads them from hardcoded internet URLs and they are cached for 30 days. By backing up the cache, I can (probably) rummage around it later to find anything that's been deleted from the internet. reply Wowfunhappy 17 hours agorootparentprev> unless you are really anal about hidden folders in your home directory polluting things up n = 1, but, uh, this is one major reason I don't use Linux. It really bugs me. reply BenjiWiebe 13 hours agorootparentIf hidden pollution bothers you, and you use Windows, don't browse the registry. shudder Actually AppData can get a bit polluted too. reply thesnide 12 hours agorootparentParent seems to care more about 'not polluting something i see all the time'. Typical of our society, we want to make everything someone else's problem. I guess we deserve some of the SaaS enshittification and it's spending obesity ;-) reply Wowfunhappy 8 hours agorootparent> Parent seems to care more about 'not polluting something i see all the time'. Yes, this. > Typical of our society, we want to make everything someone else's problem. I don't think that's a fair analogy. It's more like moving junk from the living room to the attic. I'd prefer to have a clean living room and a messy attic. reply vetinari 6 hours agorootparent>> Parent seems to care more about 'not polluting something i see all the time'. > Yes, this. Like \"My Documents\" (or \"Documents\" nowadays)? Now that's some dumping ground. reply Wowfunhappy 5 hours agorootparentThat's bad too, but at least it's not right in my home folder! When I have to use Windows, I basically just avoid using Documents for files. (But my daily driver is an ancient version of OS X, which has ~/Library for these things.) reply vetinari 4 hours agorootparentOh, mac has the same issue and for the same reason: the same programs will create the same garbage. ~/Library also isn't something that is used to be; it was good idea, but mostly ignored. Basically similar idea as XDG_(CONFIG|CACHE|DATA)_DIR. What's worst, is that many projects ignore any effort to move them to XDG schema. They take the approach, that they always did it this way, it works, so why changing it. Dot files in my homedir on Sonoma machine: .CFUserTextEncoding .DS_Store .Trash/ .Xauthority .amt .android/ .anyconnect .bash_history .cache/ .cargo/ .config/ .cups/ .degit/ .deno/ .docker/ .eclipse/ .emacs.d/ .gitconfig .gitignore_global .grass7/ .hex/ .homeassistant/ .ipython/ .java.login.config .jupyter/ .kube/ .lemminx/ .lesshst .local/ .matplotlib/ .mix/ .mume/ .node/ .notable.json .npm/ .nvm/ .oh-my-zsh/ .oracle_jre_usage/ .pip/ .profile .psql_history .python_history .rest-client/ .rustup/ .sqitch/ .ssh/ .subversion/ .swiftpm/ .thumbnails/ .viminfo .vscode/ .wget-hsts .wine/ .yarn/ .yarnrc .zcompdump .zsh_history .zsh_sessions/ .zshenv .zshrc reply 38 21 hours agorootparentprevnext [8 more] [flagged] zbowling 21 hours agorootparentGuess you have never used https://snapcraft.io/, https://flatpak.org/, or https://appimage.org/ because all of these do exactly that. Snap apps are straight linux containers and the others root like chroot jails I believe still. Or even go take a look at Nix/NixOS and how they pull it off in another way. They have hermetic isolation down to a science. Or heck, just look at what Android does, running each app under its own uid/gid, sandboxing 3rd party code, and keeping each app from reading and writing outside their little jails. Can't pollute a user directory or even write to /tmp if your user can't even enumerate it. Hell, even built a whole sandboxing capability-based security model inside of Fuchsia at Google, which I worked on for 5+ years. I've been building OSs for 20+ years, between Fuchsia and Android at Google and mobile/embedded products at Texas Instruments, so I hope I know what I'm talking about. reply account42 8 hours agorootparent> Guess you have never used https://snapcraft.io/, https://flatpak.org/, or https://appimage.org/ because all of these do exactly that. Snap apps are straight linux containers and the others root like chroot jails I believe still. AppImages are not containers at all. They bundle up the program data into a single archive but do not do any sandboxing and leave programms to write their user/config/cache files to wherever they would be written without AppImages, i.e. in xdg-basedir locations. As it should be. > Or even go take a look at Nix/NixOS and how they pull it off in another way. They have hermetic isolation down to a science. NixOS's packaging is also completely orthagonal to the xdg-basedir spec. > I've been building OSs for 20+ years, between Fuchsia and Android at Google and mobile/embedded products at Texas Instruments, so I hope I know what I'm talking about. None of those are desktop operating systems. Please stay away from those with your anti-user opinions. reply Brian_K_White 19 hours agorootparentprevAndroid is a thoroughly managed appliance, not a general purpose OS. Snaps should not exist. Flatpak and appimage should exist, but should not be used except in extreme cases. You can have bad judgement for 20+ years easily. reply zbowling 19 hours agorootparentNah. Your objections are rooted in the very limited definition of what an OS is and what a user application model is that fits it. There is no reason why each process/app can't be sandboxed. In fact, it should be for security (we did it in fuchsia). It's actually the way things work with apps from the app store on MacOS in a lot of ways where you can't escape your jail except through what is explicitly entitled. General purpose just means that it's generally useful for a wide range of applications. Android is that. Hell, you can find Android running small appliance server infrastructure and powering IoT devices. Even in 2024, iOS/iPadOS is general purpose at this point, and they have VERY different application models from legacy app models you find on Windows and Linux. You wouldn't not call NixOS a general-purpose OS, and it's like flatpak for literally every process down to things like bash, grep, and vim. Snaps are fine. Aversion is only from how it was introduced to people in Ubuntu, but conceptionally, is great. cgroups wrapping user processes to box them is not only good for security but also for stability and solving dependency versioning issues. It's brilliant. It's similar to what we did in Fuchsia at Google (we took it to another level because we had no legacy to deal with). And sure, maybe I have bad judgment on some things. I contributed a ton to GNOME in the early 2000s both code and ideas that were horrible in hindsight, but I'm not still stuck in an outmoded mental model for thinking about my user environment. reply fragmede 20 hours agorootparentprevOr Qubes, which goes further and features per-app VMs. Snaps was foisted on the community which was then unwelcoming of it, but per-app isolation isn't the worst idea. reply CoastalCoder 19 hours agorootparentprev> I was sort of with you until this, when I realized you have no idea what you are talking about. I love the pith of your comment, but it wasn't very nice. Like the Churchill quip about being called drunk. reply cwillu 21 hours agorootparentprevYeah, I had upvoted the comment, and retracted the upvote when I got to the last sentence. Nobody who has anything kind to say about snaps has any business anywhere near my desktop. reply skissane 17 hours agoprevI wish Linux (and other Unix-like systems) had an API to let you create named temporary files which are automatically deleted - by the operating system - when a process exits. If you create an anonymous temporary file, then it will be deleted when the last file descriptor to it is closed-but anonymous temporary files are difficult to use (even given access via /dev/fd aka /proc/self/fd). You can delete it using atexit() but that doesn’t get run if the process terminates abnormally (core dump, kernel panic, kill -9, power outage, etc) Maybe something like a pidtmpfs where the top-level directories are pid numbers, you can create any files or directories you want under them, but it all disappears as soon as the process with that pid exits. Actually, pids are a bad idea due to pid reuse, you want something like a process serial number or a process UUID which doesn’t get reused. Another somewhat more flexible idea would be that you can create arbitrary top-level directories, but they - and all their content - are automatically deleted as soon as the last open fd to them is closed. That way you could have multiple processes sharing a temporary workspace, and the workspace could survive the failure of any one of them, but if they all fail it gets deleted reply LegionMammal978 13 hours agoparentWith mount namespaces, you can create an anonymous tmpfs mount that gets detached and deleted once nothing refers to it any more, and add whatever files you want to that. It can be accessed using the *at() syscalls, or alternatively using an initial fchdir() to make it the current directory. Too bad that unprivileged namespaces have been buried by the big-name distros, though. reply bduffany 7 hours agorootparentWhat do you mean by unprivileged namespaces being buried by the big-name distros? reply LegionMammal978 3 hours agorootparentWell, perhaps I'm being unfair talking about distros plural: I'm mainly thinking of how the latest Ubuntu has disabled unprivileged user namespaces by default [0], seeing them as too big an attack surface to let everything use them. It looks like Debian used to have them disabled, but then re-enabled them for the sake of web browsers [1]; I'm sure they'd re-disable them if they found some solution similar to Ubuntu's AppArmor one. For other distros, it's difficult to find up-to-date information on whether they enable or disable unprivileged user namespaces, since many have flipped back and forth over the last decade. All that is to say that unless your program is given privileges itself (e.g., Docker), or can wheedle user-namespace permissions out of the packagers, there's no chance you'll be able to distribute namespace-using code and have it work consistently for most users. [0] https://discourse.ubuntu.com/t/ubuntu-24-04-lts-noble-numbat... [1] https://salsa.debian.org/kernel-team/linux/-/commit/a3819178... reply happymellon 4 hours agorootparentprevPresumably this, Unprivileged Name Spaces getting blocked by Ubuntu. Not sure if anyone else is doing it. https://ubuntu.com/blog/ubuntu-23-10-restricted-unprivileged... reply LegionMammal978 3 hours agorootparentIt seems that until 2020, Debian used to block them as well [0]. So perhaps some of my perceptions are out of date. In any case, it means namespaces are unreliable to use for the typical Linux program. [0] https://salsa.debian.org/kernel-team/linux/-/commit/a3819178... reply ithkuil 9 hours agoparentprevWhat's the main problem you see with /dev/fd? It even works when you write a temp script and execute it in /dev/fd/... However since you can't control the file name in that case you cannot control argv[0] for such temp executables which may be a problem for BusyBox-style tools. Have you encountered other issues with using that approach for named but auto-cleaned files? reply skissane 8 hours agorootparentExample: suppose I am testing a program which reads in a directory tree. I have my test driver program create a temporary directory, with multiple files and subdirectories in it. I can’t use /dev/fd for that, since it doesn’t support directories Related example: testing a program which expects a file name to match a certain pattern; can’t do that because /dev/fd file names are just numbers reply gray_-_wolf 6 hours agorootparentprevLooking at `man 3 exec', argv0 is controlled by the caller and has no connection to the actual name on the disk. reply pram 13 hours agoparentprevPrivateTmp in systemd does this. reply skissane 11 hours agorootparentUnfortunately it only works for systemd units, as far as I am aware. Consider this use case: I have a test program which executes some unit tests, it creates some temporary files. I want the temporary files it creates to all be removed when it exits, even if it core dumps. But while it runs, I want those files to easily be accessible to subprocesses it spawns. I don't think systemd addresses this, because my unit test runner isn't going to be a systemd unit. So really I was talking about a generic facility an arbitrary program could use, not just something limited to systemd units only. reply Wowfunhappy 9 hours agorootparent> Consider this use case: I have a test program which executes some unit tests, it creates some temporary files. I want the temporary files it creates to all be removed when it exits, even if it core dumps. But while it runs, I want those files to easily be accessible to subprocesses it spawns. For this precise use case, what if you deleted the last run's temporary files at the start of the test program? reply Brian_K_White 5 hours agoparentprevI have a magic tmp dir I set up using a one-line find command that runs every minute from cron, that deletes anything in that dir with an atime older than some small value like 30 seconds or 2 minutes. That sounds expensive but since it's always running, the find work is always small, and this is all in ram tmpfs usually too, because ideally you want atime enabled in this dir, and ideally you do not want atime enabled most anywhere else. It is used by all the application software for \"fire & forget\", so the app code can just generate the file and not have to hang around (and also, hang) while waiting to delete the file some unpredicatble time later. The file may or may not ever be used in some cases, and if used, it's used in some other process that the writing process doesn't know how long it will take unless you rig up some way to signal. Every user of the special directory knows this special property of it, as this is it's entire purpose in the first place. It's not /tmp so no surprises. It eventually got used for all kinds of other things too because it's just so handy. It works best with atime, which means mounting the filesystem with atime enabled, which I don't like to do in most cases, so ideally you make this it's own tmpfs in ram, and then the atime doesn't hurt much. But it also works well enough with just ctime & mtime and a slightly longer TTL. Even if a file is large and the user is on a slow link and the file is still downloading when the TTL expires, and the filesystem does not have atime, and so the reaper does actually delete the file while still in use, it's still ok most of the time even then, because the actual process that has the open handle still has it and can continue downloading the file until they close it, even while it has disappeared from view for the rest of the system. But with atime, it's like magic, the file just naturally lives exactly as long as anyone is interested in it, and gets reaped 2 minutes after that, with no application process needing to keep track of it. The reaping happens regardless of crashed processes or reboots, graceful or ungraceful, etc. It's something I did almost the first week over 20 years ago at a company I worked at from '99 to about '22, and got used for practically all temp files, and the normal tmp became the special case you only used in special cases. (actually the application software really never used the global /tmp, there were various other customer-specific and app-specific dirs) Basically it was like an OS feature (for us) in that every system (of ours) always had this magic tmp dir. This worked fine on old sco systems without even gnu find with it's handy -delete, let alone cgroups or even a ram fs. In fact it started there. reply pledess 19 hours agoprevThe essence of the problem is that there's no standard pathname for a personal directory that's guaranteed to be on local disk, even if $HOME isn't. Consequently, people have relied on /var/tmp/$USER for this. There are realistically affected users who can't change the new defaults. Cleaning up /var/tmp on a timer is relevant to this academic environment (desktop-based research computing): 1. Each Debian machine is used by only one graduate student, but students do not have root access. 2. Today, /var/tmp is the only persistent local directory where the student has write access ($HOME is on a network filesystem backed up by the university). 3. Within the student population, there is strong institutional memory that /var/tmp isn't backed up by the university and isn't extremely robust (e.g., RAID), but also that nothing there is automatically deleted. 4. Students use /var/tmp for hundreds of Gb of data from simulations that take days or weeks. $HOME is too small and too slow for this. 5. In practice, less than 1% of students lose data through disk failure, accidents, etc. 6. A much larger fraction of students will lose data when sysadmins, who didn't get the memo about the /var/tmp change and thus haven't addressed the ingrained institutional memory, deploy new Debian machines. 7. Some of the students who lose data won't graduate on time. reply kbenson 18 hours agoparent> The essence of the problem is that there's no standard pathname for a personal directory that's guaranteed to be on local disk, even if $HOME isn't. Consequently, people have relied on /var/tmp/$USER for this. There are realistically affected users who can't change the new defaults. How is that not a site specific problem they introduced? There's no standard pathname because there are valid reasons why you want to have nothing on the local system, such as when it doesn't have a local disk. If you (read as \"the admins\") have configured the system where there's very few acceptable locations to store local data, and that's a specific need, then it's up to the admins to provide a solution to the problem, not the Debian distro, which is flexible enough to handle this fine. It's not that hard to create your own location for data if you need to, and symlink to it from other locations if required. Need a /tmp/$USER to be a local disk location and /tmp is a ramdisk? Create a script that sets up the correct local location and created a symlink to it in /tmp and make sure it's run on boot and maybe once daily. Both cron and systemd solutions could work for this. Worried that user tmp files will be cleaned out when they shouldn't be? Put in the correct exceptions for tmpwatch or whatever debian uses, or disable that service. Not only is this not rocket science, it's literally the job of the admins that design the OS deployment so that it works for the intended use cases. reply zamadatix 16 hours agoparentprevThe distribution can't be held to guarantee a particular path is on a local disk because it's not up to the distribution. If it were about the distribution's defaults being a reliable option then /home would already be on local disk. reply xyzzy123 18 hours agoparentprevYou made your case very clearly. Couldn't students address this with their local admins? Wouldn't that be better than trying to get an upstream project to preserve an informal social contract that students inadvertently relied on? reply Joker_vD 18 hours agorootparent> Couldn't students address this with their local admins? They could try to, sure, but normally local admins in the unis don't cave in to any demands from the students. Sometimes they don't agree even to the demands from the faculty! reply DaSHacka 12 hours agorootparent> They could try to, sure, but normally local admins in the unis don't cave in to any demands from the students. Sometimes they don't agree even to the demands from the faculty! Yep can confirm; My uni's IT department recently began blocking all inbound ssh traffic for the entire university network (including the data center ranges), and shot down any requests from students, faculty, clubs, and enterprises that asked to have an IP or two whitelisted so they could access their infrastructure from off-campus (except ITs own services were whitelisted; can't be inconveniencing them now) A subset of us that got refused formed a mini anti-IT 'cabal' of sorts, eventually found an oversight in how they implemented the block (it just pattern-matched the initial ssh handshake version string; you can change it by compiling openssh from source), and have since been on our merry way, with IT none the wiser. But hey, at least the security guys can sleep soundly at night thinking we're still being inconvenienced by their arbitrary decision. Clearly they must think everyone is as incompetent at locking down a network's security as they are. reply Athas 8 hours agorootparentWe are in a sort of similar situation, but our solution was to put a Raspberry Pi in a windowsill that runs a reverse SSH tunnel (through a server in a VPS somewhere). reply Macha 8 hours agorootparentprevReminds me of when my uni blocked outgoing connections to unsupported protocols (including Minecraft servers). It did DPI on most ports, so we couldn't just put it on port 80 as it would recognise it as not http traffic, but the HTTPS port seemed to get excluded and just assumed as encrypted stuff, so we used to just run our Minecraft servers on port 443 reply skywhopper 17 hours agoparentprevWhy shouldn’t the admins provide an actual solution to this problem? This specific scenario represents a tiny percentage of Debian users. Why should the distribution take pains to work around such a specific issue, when the issue is easily avoided entirely to any admin who reads the upgrade docs? reply benlivengood 18 hours agoparentprevWhy should thatBut automatically deleting files - in either of those - while the system is running is going to break all sorts of things. Detect the process that created a file and delete it 24 hours after the process ends. reply o11c 2 hours agorootparentThat will fail badly with multi-process programs, which includes the example of browsers, and many others (databases anyone?). reply blucaz 5 hours agorootparentprevOr what about, delete it after 10 days it was last accessed (regardless of whether for read or write)? Hint: that's already how tmpfiles.d works reply jgalt212 7 hours agorootparentprevThanks for the homework assignment. The entire value proposition of tmp is there is no homework to do later. reply leni536 7 hours agorootparentprevChrome is multi-process, isn't it? reply bbarnett 13 hours agoparentprev/tmp on tmpfs is fine. Hanging my comment here. Sorry to slap it into your universe. My knowledge says... is it fine? My RAM is for applications, and I buy RAM accordingly. Most in this discussion seem to miss something quite vital and important. Linux is excellent at caching most-needed, most-used data in RAM, eg buff/cache in 'top', and that should be given higher priority than some temporary file a user might slap in /tmp/. 50% of my RAM, randomly flushing out buffers and cache! Buffers and cache, which stores commands I use frequently, libraries often used by applications, and so on! Any server that is reasonably loaded, will show RAM fully used by buffers/cache, excluding active RAM requirements. Now I'm to give 50% of this up, having those buffers/cache flushed for... someone unarchiving a tarball?! And my swap file, something for emergency application RAM usage, now being stolen by /tmp. We're basically taking the most expensive storage space on a system, RAM, and relegating it to... a few log files, someone untarring software, and also... Hard limiting its size to RAM requirements?! This is sheer madness. It smacks of people with desktops, with enormous amounts of free RAM, making decisions for servers. This change is bad. Thoughts: * It is not relevant what systemd does, endless things in Debian and other distros diverge from what systemd does * Debian isn't aligning with 'other distros' when doing this, as there isn't a consensus here. Debian isn't some hold out, in fact there are far, far fewer installs elsewhere doing this. * Stealing my RAM for transient files is dumb reply bildung 7 hours agorootparent> Now I'm to give 50% of this up, having those buffers/cache flushed for... someone unarchiving a tarball?! Nope, unarchiving tarballs doesn't write the tarball content to /tmp. > It smacks of people with desktops, with enormous amounts of free RAM, making decisions for servers. * Servers usually have way more RAM than desktop devices? Unless you mean VMs, in which case the VM can't decide what device /tmp lies in anyway. * in any case it will be trivial to just not use it for your servers, if you prefer /tmp to be on disk. Just like I already used tmpfs for 2 decades in Debian, it is a single line in /etc/fstab either way. reply bbarnett 6 hours agorootparentEh? Yes, if I untar a file in /tmp, it will go there. My whole post talks about putting files in tmp! And this change is a performance killer for that reason. Taking all that juicy, super fast RAM away from system caches and buffers, which yes is a performance killer. You want sensible defaults. Telling people they can change a dumb default to something smart, isn't a viable response. I mean my god, you say you've been using linux for 20+ years, so you surely know, doing this for a performance gain is absurd! We have mega fast SSDs, and an elegant filesystem layer, writing to disk is immensely fast compared to spinning disk days. We gain almost nothing here, and lose RAM caching of libraries, data, buffers, which the kernel is very, very good at these days too. It's a dumb default. Immensely so! And statement like \"you can just\" aren't the point. This slows almost every single system down. You know, this could be done right. Instead of stomping on everyone, just create a new tmpfs for just this purpose. Let people use it if they want. And why on earth suddenly start forcing file cleaning. Debian has not had that default in decades, changing it now because someone else called redhat does it, isn't a valid excuse. Debian doesn't need to be more like redhat. reply zepolen 11 hours agorootparentprevNot to mention if you did have a large tmp file that you actually want to persist to a different directory that is on disk you now have to wait for the damn file to be copied from memory to disk vs a single atomic rename operation, this can have major implications depending on the context. reply guappa 6 hours agorootparentRename doesn't work across filesystems. You just create a new file and do a regular copy. reply influx 22 hours agoprevI was really impressed how Boccassi \"project managed\" this change, gathering feedback, addressing feedback where necessary, and continued to push the ball forward even under lots of objection. Many people would have just given up and the status quo would have remained for another couple years. reply dsr_ 21 hours agoparentExcepting a General Resolution or a Policy violation, every accredited Debian Developer can do whatever the heck they feel like in their own packages. This can pose problems. Boccasi steamrollered, but relatively politely. reply KennyBlanken 17 hours agorootparentBeyond \"relatively politely.\" People were more interested in bitching about edge cases where maintainers / projects / authors were doing things they should not have been doing in the first place than they were about addressing those issues, so he addressed them himself. The man literally put up when others would not shut up. reply guappa 6 hours agorootparentThe fact that YOU don't have a particular use case doesn't necessarily mean much. Billions of people don't use linux and this all doesn't matter to them. But perhaps 30% could be in what you call \"edge case\", since your statistics is entirely based on yourself. reply blucaz 5 hours agorootparentThe fact that YOU have a particular use case doesn't necessarily mean much either. Just customize your configuration accordingly, and move on. reply teruakohatu 21 hours agoparentprevAccording got the article Boccassi wanted concrete scenarios where things might break, but his justification for making the change was “upstream and other dists”. Systemd now essentially control Debian both the OS itself, and though Boccassi, the development. reply guappa 6 hours agorootparentAnd when he got concrete scenarios he dismissed each and every one of them as \"this is just you doing this\". reply IshKebab 21 hours agoparentprevYeah so much naysaying. Especially the people storing important files in `/var/tmp`! It's like complaining to the council for collecting bins because you like to keep your keys in them. Wtf. Good work pushing through it Boccassi! reply akira2501 21 hours agorootparentIt's my bin. I paid a lot of money for it. I can keep my keys in it if I want and if it's convenient for me. I don't work for the council and I don't care what their agenda is. Why not just have the installer _ask_ me what configuration I prefer? Is there some reason you have to force the change, announce a victory, then make me to go mucking with \"defaults\" after the fact? reply chuckadams 21 hours agorootparentYou can already change the configuration in the installer, it's called manual partition setup. The installer doesn't need a clicky screen for every setting that may have changed over the last 30 years. reply akira2501 20 hours agorootparentSo if I do the manual partition setup it's not going to run the daemon that automatically deletes files off those partitions during runtime for me? It sounded from the article like this change is more comprehensive than just the partition. reply cbsks 16 hours agorootparentIt’s a systemd service: https://www.freedesktop.org/software/systemd/man/latest/tmpf... You will have to disable it separately. From the article: “To stop periodic cleanups of /tmp and /var/tmp users can run touch /etc/tmpfiles.d/tmp.conf.” reply chuckadams 20 hours agorootparentprevI doubt it'll change the auto-cleanup. It probably is configurable with dpkg-reconfigure, or maybe it's just a service that needs to be disabled. Post-installation configuration is still a thing, and it changes every version. That's what versions are for. reply Macha 8 hours agorootparentprevHere the council* provides the bins that they collect on rubbish day. If you want to provide your own bin they'll leave it alone, if you leave the council bin there on rubbish day they'll take the contents away. Likewise if you make /akirastmp, the OS will leave it alone, but using the directories the OS clearly makes temporary for things that aren't should be the configuration to require manual setup since it's the unusual one. (* Council's licensed private operators) reply blucaz 9 hours agorootparentprev> Why not just have the installer _ask_ me what configuration I prefer? Because nobody bothered to actually put in some work to implement that. As I've said on the ML, if somebody does the work, I'll review it. But it's one of hundreds of different settings, and it's obviously not worth anybody's time to do this work, as it's largely inconsequential and trivial to configure via the supported config files. Complaining on social media is of course cheap enough that we get plenty of it. reply lolc 20 hours agorootparentprevEh I've lost files because I was working in a ramdisked /tmp when the computer lost power. That's what one gets when working in a dir called \"tmp\". Some people will see their stuff vanish from /var/tmp and that will be their day of learning. I guess they won't have a backup of files in there either. They were one misfire away from loss anyway. Sure it feels different when the distro just ran over your misconceptions. On the other hand, when I encounter a stray tmp dir, I assume everything in it can be deleted with negligible consequences. So I tend to rm them without even a glance inside. reply immibis 16 hours agorootparentprev> Why not just have the installer _ask_ me what configuration I prefer? Is there some reason you have to force the change, announce a victory, then make me to go mucking with \"defaults\" after the fact? It's the Poettering way. And GNOME, for some reason. reply akira2501 9 hours agorootparentIt's a recent part of our culture to look down on the \"uninitiated\" and treat them with extreme infantilizing pity almost bordering on actual bullying. I call it the \"temporarily embarrassed future nobel prize winner mindset.\" I put it down to the level of monopolization of industry and to a certain extent our culture. These people mean well and they're mostly just reacting to a really baffling labor market in the face of a failed and entirely captured internet revolution. With the right pair of eyes you can look across the dunes of github into the early 2000s and see where open source culture peaked, crashed, and rolled back. reply immibis 7 hours agorootparentI had the chance to speak with a GNOME developer about this. Apparently GNOME is trying to be the little old lady desktop that's intuitive for grandma. Which is fine - if it's marketed as that, and not put on the same level as KDE or even tiling WMs, and not trying to influence the entire ecosystem. reply guappa 6 hours agorootparentMy retired mother uses KDE. reply vrighter 17 hours agorootparentprevyeah but if that's the case, it's on you not to take the bin out on trash day. reply KennyBlanken 16 hours agorootparentprevBecause that increases complexity, and introduces new ways the installer can do something unexpected or fail. There's overhead to code; it has maintenance costs. If half a dozen people in your town of 10,000 use bins for storing soup, but one year the council procures bins with holes in the bottom to keep rain from collecting in them (making them hard to move, and also providing stagnant water for mosquitoes, ie something that benefits nearly everyone)... ...what would be your opinion of a blitzkrieg of online and media criticism of the council for not considering the needs of soup binners...by people who do not store soup in their bins but are screeching about the needs of the people who do and they say will be affected by the change? And then someone on the council has said \"alright, fine, look...I doubt there's that many of these people. I'll go talk to them, who are they?\" and the screechers admit they don't actually know any soup binners, so the councilmember looks them up and goes to the house of every soup binner and either plug the holes in their bin for them, gives them proper soup pots, or prints out a list of soup pots they can find on amazon...or finds out that they say \"well gosh I had a soup pot in the attic but I never got around to using it, I'll just switch. Thank you for the heads up that the new bins will have holes in the bottom.\" reply NikkiA 14 hours agorootparentprevI mean, there was that period of time where amazon and other delivery companies would consider a bin a 'secure' place to put parcels while the recipient was out, even on bin days... reply probably_wrong 21 hours agoprevI've had cases where /tmp was full, usually by some runaway program that didn't clean up properly. The first indicator is usually that Bash auto-complete fails, followed by no one being able to SSH into the machine. I suppose mounting /tmp in RAM means this will happen more often (as RAM is usually smaller than hard drive space), but I don't see this becoming a giant issue. Now, auto-cleaning /var/tmp... that's going to cause SO MUCH data loss across people I know. I see a parallel with keeping files in your Desktop: yes, that's not what it's there for and ideally you'd store things properly, but you don't go around deleting people's documents just because you don't like it! Time to start sending warning E-Mails, I guess. reply camel-cdr 21 hours agoparentI quickly ran a backup just now, but I would've been one of the people slightly effected. I symlink some of the desktop cache files to /var/tmp, but since that never got deleted for me (on Debian and Manjaro) I also started linking things like my browser profile there. I kind of knew that /var/tmp may not be the correct folder for it, but if it works, it works you know. A bit of background on my setup, for why I even want to create such symlinks. My home folder is on an automatically mounted USB stick that I transfer between systems (laptop and desktop). On shutdown, I rsync the home folder to a local backup folder, which gives me a small distributed backup system. Using the same home directory between systems with different distributions and CPU architectures (I'm using the pinebook pro arm laptop) works surprisingly well. But I'd rather not stress the USB stick with a bunch of frequently accessed cache files, hence the symlink. I end up also symlinking the browser profile, because that caused version incompatibilities between systems, and I'm ok with having local browser settings/history on each device. reply somat 20 hours agorootparentI sort of do the same thing, See, I like my homedir on a nfs share. (my philosophy is build one really good drive, raid, backups etc, then everything can use that, rather than try and put a good drive into everything) However there is stuff I don't really care about but would like to be fast and don't want eating my bandwidth, caches mainly, Now unlike you I tried to stay out of tmp so I made a /var/home/ on a fast local drive that vaguely mirrors /home and symlink the caches into it. That way it makes a good dedicated space for when I do want a fast local filesystem. As an antidote I had a hard time for a few years after I stopped using irix, I wanted(or at least my fingers wanted) the home directories to be under /usr/people reply Joker_vD 18 hours agoparentprev> I see a parallel with keeping files in your Desktop: yes, that's not what it's there for and ideally you'd store things properly Wait, what is Desktop for, then? Application shortcuts? We already have \"pin to taskbar\" for that. reply jasomill 16 hours agorootparentOn the original Macintosh, the idea was that you'd move files you're currently working with onto the desktop, then return them to their original folders when finished. The original Mac Finder even had a \"Put Away\" command to support this workflow. I believe this idea partially arose by analogy with physical workflows, and partially as a way to more easily work with related files spread across multiple floppy disks: desktop items remained visible when a floppy was ejected, and the system would prompt for the floppy by name if you attempted to access a file on an ejected floppy (ejected floppies themselves remained visible on the desktop until you dragged their icons to the trash). [Pedantically, the original (System 1.0) Mac Finder had a more general, slightly buggy \"Put Back\" command, removed in System 2.0, and finally replaced with the \"Put Away\" command in System 2.1. Source: past experience verified through testing at [1]] [1] https://infinitemac.org reply throw0101c 19 hours agoprevFrom FHS: > The /var/tmp directory is made available for programs that require temporary files or directories that are preserved between system reboots. Therefore, data stored in /var/tmp is more persistent than data in /tmp. > Files and directories located in /var/tmp must not be deleted when the system is booted. Although data stored in /var/tmp is typically deleted in a site-specific manner, it is recommended that deletions occur at a less frequent interval than /tmp. * https://refspecs.linuxfoundation.org/FHS_3.0/fhs/ch05s15.htm... IRIX docs: > The directories /usr/tmp.O, /var/tmp, and /var/spool/uucppublic are public directories; people often use them to store temporary copies of files they are transferring to and from other systems and sites. Unlike /tmp, they are not cleaned out when the system is rebooted. The site administrator should be even more conscientious about monitoring disk use in these directories. * http://rsusu1.rnd.runnet.ru/sgi/advanced/ch8.html From FreeBSD 1.0 (July 1991): /var […] tmp/ temporary files not removed between system reboots vi.recover/ recovery files for the vi(1) editor * https://man.freebsd.org/cgi/man.cgi?query=hier&sektion=7&man... This is also true for Solaris, which I used to admin many moons ago. So I'm not sure where the idea that /var/tmp gets cleaned on reboot came from since I have always understood it to be fairly static. reply SoftTalker 12 hours agoparentEvery unix does its own thing, you just have to know your system. In OpenBSD, /var/tmp -> ../tmp and /tmp is cleaned periodically and not preserved across reboots, but there are some specific exceptions and you have to dig into /etc/daily and /etc/rc scripts to know what they are. reply JonChesterfield 22 hours agoprevTmpfs is great. If you're a compiled language person and the build dir isn't in a tmpfs yet, give it a try. Faster than nvme and less prone to burning out from lots of writes. Defaulting to /tmp in tmpfs means I can remove a line from my post install setup script. That doesn't matter much. But it also means less divergence between my system and the default, so there should be a reliability improvement from other people stumbling over problems with the ramdisk before me. So a win all around, happy to see it. reply tombert 22 hours agoparentWhat's especially fun is using tmpfs as root on NixOS. That way your root partition gets cleared out on any reboot, so cleaning up your mess is never really more than a reboot away. I've never thought about mounting a tmpfs folder for compiling stuff, but that's actually a pretty good idea. reply Aurornis 21 hours agorootparent> I've never thought about mounting a tmpfs folder for compiling stuff, but that's actually a pretty good idea. Using tmpfs (or RAMDISK) for compiling and discovering that it’s not actually faster is a virtually rite of passage for every developer. :) If you do try it, at least benchmark it. You’ll probably discover that you spent more time setting it up and copying files around to volatile storage than you’d ever gain back via minuscule compile time speedups. Compiling is not IO bound and your files are already being cached in RAM by the OS after the first read. reply toast0 20 hours agorootparent> If you do try it, at least benchmark it. You’ll probably discover that you spent more time setting it up and copying files around to volatile storage than you’d ever gain back via minuscule compile time speedups. Compiling is not IO bound and your files are already being cached in RAM by the OS after the first read. You shouldn't need to copy files to volatile storage, just storing the outputs there. Maybe only storing the intermediate outputs there. Some build processes just dump intermediate outputs all over the place, but a lot of them have a directory for those, and it shouldn't take much to make that directory be on tmpfs or similar. reply tombert 20 hours agorootparentprevIt’s actually not really about speed in my particular case; I mostly just don’t want lots of tiny intermediate files being written and rewritten on my SSD all the time. I would sleep better if I knew that they wouldn’t have any negative effects on the lifetime of my drive. I have a tmpfs folder mounted for my downloads folder for my browser for similar reasons; not for speed but to reduce drive wear and automatic cleanup so that it doesn’t pile up for forever. reply JonChesterfield 20 hours agorootparentprevIt's faster. I don't move the source code around though, the intermediate build directory cmake writes binaries into is under a tmpfs mount and the install dir is back on disk. Currently has 10gb in it with 30m of uptime. The gain would be less if I was more willing to trust ccache, or if the cmake dependency graph of the project was trustworthy - my default build deletes all the files from the last one. reply MrDrMcCoy 16 hours agorootparentprevI don't do it for speed. I do it because I'm abusive enough to my storage even without that. reply saagarjha 21 hours agoparentprevKeep in mind that on macOS /tmp is some very cursed link to /private/var/tmp and some projects will get very confused by this. reply NekkoDroid 21 hours agoparentprev> But it also means less divergence between my system and the default This is one reason I like Arch. It is almost as vanilla as can be (some exceptions may apply) while still being a \"build it yourself Lego set\" (build not in the compile sense). reply JonChesterfield 20 hours agorootparentWouldn't that be the opposite? I like the base userspace to be very boring, where broken things have been stumbled over and fixed before I run into them. Everyone having their own bespoke set of pieces makes the divergence between what you're running and what other people are running greater. reply tombert 21 hours agorootparentprevYeah, it's kind of hard to go back once you've embraced the \"build your own distro\" distros. Arch, Gentoo, and NixOS Minimal are all pretty wonderful in that they give you the tools to build whatever system you want, without a bunch of extra crap that you'll never use. reply Aurornis 21 hours agoparentprev> If you're a compiled language person and the build dir isn't in a tmpfs yet, give it a try. Faster than nvme What are you compiling that’s actually faster in tmpfs? A decade ago, people were surprised to discover that going from HDDs to SSDs didn’t impact compile times very much. The files are quickly cached in RAM anyway and the bulk of build time is CPU, not I/O. Going from NVMe to tmpfs feels like an even smaller difference. reply dekhn 19 hours agoprevI really dislike people using the \"I have an inflexible scientific code that needs /var/tmp to be disk and persisted forever\" argument to keep things the way they are. Debian's user base is far larger than a few inflexible codes, and it's straightforward to change the default. reply wang_li 18 hours agoparentCode and scripts should be well behaved and clean up after themselves. There’s zero reason to periodically go in there and wipe stuff out. Especially when some file systems don’t have a creation time attribute in the on disk inode and some archiving tools preserve the mtime when unpacking. A newly placed file can already be five years old. Also the tone of some of the included quotes is so offputting. Their statements and arguments for this change don’t even acknowledge the fact that there are different views on this. This change comes across as progress for progress’ sake. Makes me want to make a 500TB /var/tmp and never delete anything. reply alextingle 13 hours agoparentprev> it's straightforward to change the default. Care to share? Looking at https://systemd.io/TEMPORARY_DIRECTORIES/ and I don't see any description of how to change the default behaviour dictated by systemd. How do I disable the automatic file reaping? How do I configure the maximum age of temp files? Neither are mentioned. The bulk of the page is just a long list of passive-aggressive suggestions about how users are doing things wrong. reply mmwelt 9 hours agorootparentThe article describes how to change the default: > Users who want /tmp to remain on disk can override the upstream default with `systemctl mask tmp.mount`. To stop periodic cleanups of /tmp and /var/tmp users can run `touch /etc/tmpfiles.d/tmp.conf`. reply dekhn 2 hours agorootparentprevTBH I ask google for things like this because reading documentation is often less productive. reply pantalaimon 21 hours agoprevI don’t like it, I tend to use /tmp for large files or directories that are just that - temporary. Now they will either fill up my RAM or require a large swap partition (which I usually don’t have as it’s otherwise wasted space). I really like making /tmp the default destination for downloads, so either I need the file and move it elsewhere, but usually it’s an archive and I just want a file from it. Saves me from an ever growing Download folder. Those files are most dispensable and should not consume precious RAM. Some file systems are even adding specific tmpdir support where fsync() is turned into a noop. So they have all the advantages of tmpfs without eating into precious RAM/Swap. reply stouset 21 hours agoparentIt's a default. You can change it. reply pantalaimon 21 hours agorootparentOnly on my personal machine. If I’m logged in elsewhere and keep my habit of working on large files in /tmp, I will be in for a bad surprise reply stouset 21 hours agorootparentSo… don’t keep large files you want to control the lifecycle of in directories you don’t own? If you care about it, stop putting it in /tmp. You were always setting yourself up for a bad surprise by doing this, and now you’re surprised? reply pantalaimon 21 hours agorootparentI might download an ISO or check out a repo I want to build just once. Or copy some (large) files there because I run a script on them and persevere the original in case something goes wrong. I don’t care about those files once I’m done with them - so far they would only be gone with a reboot, which aligns well with my definition of done. reply stouset 18 hours agorootparentIf you’re downloading large files (so large you’re concerned about an in-memory tmpfs) to boxes that you don’t own, stop just leaving them around in the first place. Sure, /tmp is cleaned on boot. But if the host is rarely rebooted it’s pretty inconsiderate to just leave enormous quantities of unneeded crap lying around in shared directories. /tmp isn’t supposed to be a garbage dump. It’s supposed to be for data that’s actually in use but transient or relevant to running processes. You’re still expected to be a good citizen on shared hosts and clean up after yourself when you’re done. reply yjftsjthsd-h 18 hours agorootparentprevIf you really don't want to delete manually you can just `sudo mount -t tmpfs ~/tmp` or so reply Macha 8 hours agorootparentprevThis is true already if your remote machine is red hat, fedora, arch, opensuse, rocky, ... reply NekkoDroid 21 hours agoparentprevThere is still /var/tmp if you want a disk backed temporary file storage. reply pantalaimon 21 hours agorootparentBut is this also cleaned up on boot? The reason I’m doing this in /tmp is that I can just shut down the computer and have all those staging files gone. (Same reason I shut the computer down at the end of the day instead of suspend - I want to start with a clean slate the next day) reply NekkoDroid 20 hours agorootparentIt isn't cleaned up at boot IIRC. Unless you leave your computer off for 30 days and then come back [1] :). But it shouldn't be too hard to write a relativly simple systemd.unit file that does that at boot. After all the main part would be `Requires/After=local-fs.target` and something like `ExecStart=bash -c 'rm -rf /var/tmp/*'` I think (you'd need to double check what exactly to do if you want to do this). [1] https://github.com/systemd/systemd/issues/33162 reply rcxdude 19 hours agorootparentThere's also systemd-tmpfiles for this exact purpose (well, it's a more general purpose create-a-directory-structure config system, but this is one of its usecases). reply aftbit 21 hours agoprevThe discussions about swap in the LWN comments are interesting. I wasn't aware that at the \"swap should be 2x RAM\" tip was actually a hard requirement at one point (allegedly around Linux 2.4). I also wasn't aware that common recommendation was to run with swap again. I have some systems (usually VMs) where RAM is much bigger than the root disk. For example, 500+ GiB of RAM and only 100 GiB of disk. I have other VMs where the disk is orders of magnitude slower than the RAM (e.g. EBS on AWS EC2). I wonder what I should be doing for swap and vm.swappiness. Usually I just run with either no swap or a tiny 4 GiB swap file on /. reply toast0 20 hours agoparentI think it makes sense to have swap be the smaller of 2x ram and 512 MB, unless you have specific needs. On small memory systems, it really is useful to have 2x ram, because it can be pretty useful sometimes. On large memory systems, it's highly likely that your system is too far gone once it's used a significant amount of swap, so 512 MB seems like as good a place as any to stop. Swap used %, and swap in / out rates are a very good measure of system health, and if you have no swap, you lose out on those metrics. Ideally, for a small leak, you'll have enough time to get an alert about high swap use, and come in and inspect the situation before you start getting OOMs, but for a large leak, you want the system to fault quickly --- it doesn't do anybody any good to be up but deathly slow due to thrashing for a long time. reply NekkoDroid 21 hours agoparentprevI guess the best I can give you is this: https://chrisdown.name/2018/01/02/in-defence-of-swap.html reply SoftTalker 16 hours agoparentprevI remember the swap = 2x RAM guideline back in the day. When RAM got into the gigabytes it stopped making sense. If you're out of memory on a 64GB machine, a swap file is only going to (slowly) prolong the inevitable. These days I set up servers without swap, and I have not run into any issues doing that. reply pdw 21 hours agoparentprevI think they must be referring something in an unstable-series kernel. I don't remember anything like that, but I only ever used the stable kernels. (Back then, there were separate stable (2.2.x, 2.4.x) and unstable (2.3.x, 2.5.x) branches. \"Linus forced a swap rewrite\" sounds like an unstable branch thing.) I always heard that the \"2x RAM\" thing came from some primordial BSD release. reply MrDrMcCoy 16 hours agoparentprevA small swap space with zswap is where it's at. You get all the benefits of swap, plus reduced I/O and high-enough speed fetches that it just feels like extra memory. Well worth configuring, IMO. reply DaSHacka 12 hours agorootparentI typically enable zram instead, on modern systems disk-based swap is going to be extraordinarily sluggish, and unlikely to be the make-or-break of recovering a system if you 64GB+ of RAM was already chewed through. reply lionkor 21 hours agoparentprevOn real machines I run with a super low swappiness of 1-5 or so, which effectively means it will always use RAM, basically until its too late. That way its difficult to accidentally get OOM killed, instead everything just slows way down when swap gets involved when RAM is full. reply Spivak 21 hours agorootparent> instead everything just slows way down when swap gets involved when RAM is full I have literally never wanted this and so I'm curious why you do. This is why earlyoom is a thing precisely because grinding your system to a halt is often worse than killing and restarting some processes. High swappiness so you run slower but never lock up or low swappiness with an aggressive oom killer so you don't lock up are to me the only sane options. Why do you want your servers to be up but thrashing? reply lionkor 13 hours agorootparentFor me, if I run out of RAM, its because im running a task that should not fail - so the reasoning is that id rather have it take longer than fail. reply pantalaimon 10 hours agorootparentBugs happen and memory leaks happen. Also make -j is prone to run out of memory if you have many cores, I’d rather adjust the number of threads than have my system grind to a halt. reply IshKebab 21 hours agorootparentprevYeah the trouble is in my experience it slows down to the point that the system is unresponsive and you have to reboot anyway. I don't think Linux has a good answer to RAM management other than \"buy lots and lots of RAM\" (I already have 32GB and it's nowhere near enough). I wish it worked like Windows - I've never had to reboot due to lack of RAM there, and even when the system is unresponsive ctrl-alt-del is very reliable. reply dspillett 19 hours agorootparentI've had Windows hang due (apparently) to OOM before now, most recently last week: already low on free memory due to other things, a runaway chrome tab chef through a huge pile of memory overnight. Come morning everything was at a crawl (task manager responded quickly but everything else took an age to register any event like a click or keypress. Bringing up chrome's task manager, sorting by RAM use, and navigating to the top of the list, to find the problem, took several minutes. Switching to that tab hung everything, even task manager. This was running in a VM so I could see from the host that the OS has hung: practically no CPU or IO use, what little there was was likely just \"background noise\" from hyper-v, also not even the simplest service accepted TCP connections nor would it respond to ping. Left it like that for tens of minutes to see no change, so resorted to a hard stop & start. It happens, albeit quite rarely. reply MrDrMcCoy 16 hours agorootparentprevEarlyOOM, swap, and zswap is the answer combo you're looking for. With those in place and properly tuned, you'd have to have something go extraordinarily sideways for an actual issue to appear. reply IshKebab 12 hours agorootparentI have swap and zswap. It just delays the problem. And anyway, I don't recall having to tune Windows to work properly. reply tmtvl 18 hours agoparentprevI use a swap partition 1/2 the size of my RAM, a zram swap partition 1/4 the size of my RAM, and 160 swappiness. Well, I say 1/2 and 1/4 size, it's actually 8GB and 4GB to a RAM size of 15.3, but it's close enough. reply MrDrMcCoy 16 hours agorootparentWhy not a single swap space with zswap? I find it much easier to configure, and the performance difference can largely be eliminated with a little tuning. reply tmtvl 7 hours agorootparentI had zram set up by itself in the past, but on my most recent install I got paranoid and I decided to set up a swap partition just in case (though I dunno what happens when zram runs out of space). I suppose I may as well try zswap. On an unrelated note: if you're the DrMcCoy from Twenty Sided and GOL it's nice to see you. reply aidenn0 13 hours agoparentprevIIRC 2x RAM was so that there would be room for kernel core dumps when the kernel ate all your ram. reply rwmj 21 hours agoprevWhat's the point of putting /tmp in RAM when a competent disk-backed filesystem will do that with the most heavily used files anyway? reply silisili 20 hours agoparentIt's A) much faster, B) causes less wear, and C) automagically cleans itself, so to speak. reply pantalaimon 20 hours agorootparentWhat about tmpdir as proposed by bcachefs? [0] That keeps the file in the page cache until it’s evicted, but instead of being written to swap it’s written to the fs. With the swap partition usually being quite small and the fs today being all the storage there is on a personal machine. [0] https://bcachefs.org/Roadmap/#tmpdir_support reply MrDrMcCoy 16 hours agorootparentIt's a neat idea, but I doubt it'll take off. Swap with zswap as a backing for tmpfs should be plenty for the time being. reply arp242 19 hours agorootparentprevWell, as I read that page it doesn't actually exist (yet), so that rather makes it a hypothetical at this point. Might be interesting in the future, but not today. reply pjerem 21 hours agoparentprevYou gain writing cycles that are not going to wear out your SSD for files that are going to be erased anyway. reply viraptor 19 hours agoparentprevYou're still going to often wait for the file to be written to the disk since many apps care about running sync() in the right places. With tmpfs you don't have to wait. reply rwmj 10 hours agorootparentThis is a fair point, but also an indication (again) that the sync primitives in posix are poorly designed. It should be possible to indicate that a file is a temporary and/or can be recreated from scratch. (O_TMPFILE isn't that) reply viraptor 9 hours agorootparentSync is not only about persistence, but also actual synchronisation. If you're writing to two files and want to ensure ordering, you have to (f)sync - doesn't matter disk or ram. reply rwmj 8 hours agorootparentI'm pretty sure that's not the case unless you're synchronizing across kernels (eg. NFS close-to-open sync). Although it's true that sync primitives for network filesystems are also a mess. reply viraptor 7 hours agorootparentBlock devices and their drivers are free to reorder writes in absence of explicit synchronisation. The filesystem level is supposed to take care of keeping things from breaking. I would suspect that tmpfs itself prevents any reordering... but I wouldn't bet on it. Non-memory devices will happily reorder. reply rwmj 7 hours agorootparentSure, but this is only important if you're working across kernels (or need correct persistence across a crash+reboot, which is \"across kernels in time\"). Within a single kernel instance, two processes do not need to think about such synchronisation, or else just about all software written would be broken. reply walrus01 20 hours agoparentprevLess write wear on SSDs, which are by far the standard now for OS/boot disk compared to 10 or 15 years ago. reply kasabali 21 hours agoparentprevChange for the changes sake, of course. reply dopylitty 21 hours agoprevI believe I speak for all when I say I keep all my most important files in /tmp and ~/Downloads reply somat 19 hours agoparentDoing tech support for an office there is always that one guy who keeps his files in the windows trash. No I don't know why either, while not the stupidest thing I have seen, it comes close. reply john2x 12 hours agorootparentIt’s easy to find. It has a unique icon and everything. It even has a dedicated shortcut to move files to it. Why would you not use it? reply ericbarrett 4 hours agoprevWhen I started in tech I worked for a storage vendor’s tech support. One of my first big cases was troubleshooting why a customer’s files older than a week were disappearing. Given the topic, I’m sure you can guess—they had it mounted under /tmp and the cleanup script was doing its thing. Hopefully the new Debian version of this script is a bit smarter! reply hsbauauvhabzb 20 hours agoprevIn the article there’s some mention of the implications of ram and swap - this has caught me before - if you wget an iso to /tmp, you may be consuming more ram than expected. And sure, that’ll get swapped out to disk, but that might be unexpected if you’re intending on using ram for virtual machines etc, you may end up OOM because of this behaviour. I did this on Fedora, and it left a bad taste in my mouth. reply pm2222 22 hours agoprevOh really I thought my arch should only clear /tmp upon startup. No wonder I lost files in /tmp The upstream defaults for systemd are to mount /tmp as a tmpfs and delete files that have not been read or changed after ten days in /tmp and 30 days for those stored in /var/tmp. reply liveoneggs 18 hours agoparentgross. reply bionade24 7 hours agoprevHopefully Ubuntu will align with this decision, so that MATLAB is finally forced to fix their shitty software. (I am aware that you can set the TMPDIR as an env var) reply camel-cdr 21 hours agoprevIs there a easy/non hacky way to configure /tmp to only delete files on regular shutdown? I sometimes have the occasional crash, and have been bitten by doing some temporary work in /tmp before such a crash 2-3 times. reply silisili 20 hours agoparentYou could do this pretty easily I think with systemd, it has a shutdown target that covers reboots and poweroffs. It would of course meaning not having tmp in RAM which is pretty common these days. reply aidenn0 13 hours agoprev> Sam Hartman noted that ssh-agent created its socket under /tmp, but it would be better if it respected the $XDG_RUNTIME_DIR setting and created its socket under /run/user. Boccassi agreed and said that he had filed a bug for ssh-agent. Richard Lewis pointed out that tmux stores its sockets in /tmp/tmux-$UID, and deleting those files might mean users could not reattach to a tmux session that had been idle a long time. Boccassi suggested that using flock() would be the right solution to stop the deletion, and said he had filed a bug on that as well. Ah, yet another group of Linux users looking to get ssh to adopt the XDG specification. Moving ~/.ssh to ~/.config/ssh has been repeatedly requeseted. XDG_RUNTIME_DIR is going to be even harder because it can be nonexistent on BSDs. reply LooseMarmoset 18 hours agoprevI have always assumed that /tmp means \"temporary\". I mean, it's right there in the name. So, a move to tmpfs means \"automatic cleanup\" to me, which seems just fine, and worth doing for that. If we're doing because the systemd maintainers say \"we have to do it because systemd\", though, well... How long till the systemd guys make package dependencies and installation as a service? You know, sponsored by the IBM/Redhat team, and signed by Microsoft? signed, a Devuan devotee reply M95D 8 hours agoprevProgrammers and sysadmins should keep track of their garbage, not break all of our systems just so they can get lazy. reply bayindirh 8 hours agoparentA competent sysadmin not only tracks their garbage, but others' garbage as well. On the other hand, a sysadmin strives to be lazy, not by postponing problems, but solving them the appropriate way, and preventing it from happening again. reply iforgotpassword 22 hours agoprevI prefer /tmp on disk. Nvme disks are fast enough, wear is not an issue nowadays. I occasionally extract large files there. So either the tmpfs would be too small or the machine would eventually start swapping random things to disk, for which I'd have to grow my swap partition first - it's only 1GB. I mean I could start using /var/tmp or make a directory in $HOME and clean that up occasionally, but that would mean I have to change old habits. ;) so I guess masking tmp.mount it is. reply m463 11 hours agoprevI'm annoyed that it is getting harder and harder to find what physical filesystems are on a system. My goto commands \"df -h\" and \"mount\" are cluttered with all kinds of ephemeral filesystems, with no easy way to filter (say, a one-character flag). lsblk is close but .. lvm and snaps. reply shadowgovt 22 hours agoprevWhile I generally appreciate Debian's careful approach (it is my go-to distro if I want things to be predictable with minimal upgrade frequency), it always blows my mind to watch open-source communities bury topics like this in committee for twelve years while commercial OS's just tend to go \"It's better for the end user. Here's some money. Make it work. If it breaks some key software, make it work.\" reply codetrotter 22 hours agoparentIn fairness it did say: > Red Hat Enterprise Linux (RHEL) and its clones, as well as SUSE Linux Enterprise Server (SLES) and openSUSE Leap, still default to /tmp on disk. So at least some commercial OSes, if you will agree that we could refer to the first two of these distros as such, are also staying with old decisions for a long time. And even though RHEL in particular is know for its insane dedication to keeping API compatibility with backports of software, they still are in a position to change things around between major versions. And I believe they do do that quite a bit, which is why they also give so long time before EOLing old versions so that enterprise has plenty of time to adapt to the changes with major version upgrades. reply pantalaimon 21 hours agoparentprevBut how is it better to have /tmp on tmpfs? Those files are hardly performance critical reply rstuart4133 15 hours agorootparentWriting data to a disk based file means it will inevitably be written to disk shortly, unless you delete it so quickly the system doesn't get a chance to write it before it's gone. Writing data to a tmpfs file means it will never be written to disk unless the you write more data than you have RAM. So if your files in /tmp fit into RAM tmpfs will be faster. That is the only benefit. Unfortunately tmpfs is not so good when the data does not fit into RAM. The first thing that happens is it writes the overflow to swap, which is much slower than writing to disk in the normal fashion so you lose the speed advantage. If you store so much you run out of swap the second thing that happens is your system dies. This proposal comes with a bigger hairs. The issue is you can't trust all apps with clean up after themselves, meaning they will exit leaving crud in /tmp. That means if the system isn't rebooted /tmp tends grow slowly as this crud accumulates, which eventually means even in the best case the system will become unstable after enough time. Their solution to that is to just delete old files on the assumption they are crud. But that's a kludge as there is no way to be certain a file is crud of not, and deleting files that will be re-used makes the system unstable. Which in the end means it depends on use cases. A typical desktop user with lots of RAM (at least 8GB, preferably 16GB) will probably see a benefit as their files in /tmp will fit into RAM. Typical means they don't do something that eats RAM and disk like editing video files, and they reboot their system occasionally. Other users will lose because tmpfs overflow to disk so their system will at best be slower, possibly unstable if they didn't know to allocate lots of swap when they installed the system. Notice servers don't neatly fit the typical use category, and also notice variants of OS's that target servers don't use tmpfs for /tmp. Debian doesn't have a variant that targets servers, but whether /tmp uses tmpfs is easily configurable for sysadmins. Editing text files is their day job after all. In fact that ease of changing the default was the main argument for the change on the Debian lists. The end result is the change won't overly effect sysadmins of Debian servers either - it's just one more thing they have to change in what is already a long list. TL;DR: server admins won't care about the change and typical users with big laptops doing normal stuff will be happy, but normal with cheap laptops or are doing unusual things will have their world turn to shit if they aren't comfortable with the command line. reply shadowgovt 21 hours agorootparentprevFlip the question: most storage these days is some kind of solid-state. Why is it better to commit /tmp to a solid-state device (decreasing its operational life) when those files are, by definition, transient and must not be required for future correct program operation? reply yjftsjthsd-h 12 hours agorootparentBecause build scripts try to download more files there than fit in RAM. reply shadowgovt 6 hours agorootparentConfiguring the whole system to account for the needs of software installation is optimizing for the corner case. reply yjftsjthsd-h 3 hours agorootparentI really don't think installing software is an edge case. On Linux distros, I don't even think compiling software is an edge case. reply shadowgovt 2 hours agorootparentIt's something the average non-developer does less than once per day, at most. (And the average developer has enough savvy to change settings). ... also, most of the compilation tools I use end up dropping their intermediary files in a peer directory to the source tree or a build directory at the root of the project, not in /tmp. I'm not sure what tools people are using that are leaving compilation intermediates in /tmp. reply pantalaimon 21 hours agorootparentprevBecause the alternative is more precious RAM reply MrDrMcCoy 16 hours agorootparentSo use swap and zswap, then increase the tmpfs size to whatever you want. Systems tend to perform better that way, after all. reply quesne 12 hours agoprevIn addition to this upgrade, debian has removed the 'last' command and related data files. reply ars 22 hours agoprevIt's a lot slower to write a large file to a ram based /tmp and let it swap out, vs writing that file to disk directly. If they want to do this they need to modify tmpfs to do a better job of shifting data to disk when full, instead of letting swap handle it. reply MrDrMcCoy 16 hours agoparentNot true if you enable zswap. The pages stay compressed in a RAM pool, and are only sent to the real swap on disk in their compressed states when that gets full. This significantly reduces I/O overall. reply throwaway984393 19 hours agoparentprevYou can specify the size of the tmpfs and keep it well below the size of RAM, which will greatly reduce the likelihood of a long-lived tmpfs from causing swapping. But more importantly, don't write large files to /tmp/. Back in the day we gave /tmp/ its own partition just so those files didn't fill up the root mount. Write big files there and it would fill up. Plus, big files benefit more from a faster drive, and back in the day we used to put the root mount on a slow disk, and have a fast big data disk which is better for large files. I think an option to prevent tmpfs from getting shifted to swap (or disk) at all would be great. reply juliangmp 10 hours agoprev>The knobs to control how /tmp is mounted, and the handling of temporary files, are part of systemd. Huh? Someone explain this to me, wasn't fstab the mechanism to define things like this? Why does my init/service manager need to mess with it??? reply zbentley 8 hours agoparentSystemd has managed mount points on systems where it runs for a long time. It provides mount units an alternative to fstab mount directives, which offers some advantages (including parallelism of at-boot mounting, granular mount dependencies for services, automatic runtime mounting and unmounting) and disadvantages (harder to view at a glance than fstab, more complex). What’s more, on systemd systems, fstab is dynamically converted into native systemd mount units at boot, so in many cases fstab is purely a facade/compatibility shim over systemd doing all the mounting anyway. While not terribly prescriptive, “man systemd.mount” recommends /etc/fstab be used to manage “mounts for humans” due to its simplicity and accessibility. So it doesn’t seem like fstab is considered legacy or deprecated by systemd; rather, this seems like more of a porcelain/plumbing distinction. reply throwaway984393 19 hours agoprevI find that, unless there's an extremely necessary reason, it's almost always bad to break backwards compatibility. If there's any concept whatsoever of continuity between major releases, backwards compatibility should be the default, with changes being options. Naturally this leads to more difficulty in testing and maintenance over time. But that extra work pays for the benefit of having a very long lived product and compatibility with past integrations. At the end of the day you have to decide if you're building a product to be easier for the users, or the maintainers. Personally I'm on the side of the users. reply bdjsiqoocwk 21 hours agoprevVery OT, but I don't know where else to turn to because the Reddit community has been of no help. I used i3 and I absolutely love the way it feels. However I would like to configure it in a way that I can press a button and every app turns into dark mode. My main apps (Firefox, vscode) have an option to say \"use system theme\" i.e. if the os is dark, theme this app dark. Great! The problem is I don't know how to set the system theme. And every time I've asked around, I see people respond with GTK, QT and what not. I don't have those, I have i3. Help? reply somat 19 hours agoparentIf it is a gtk app it will use the gtk mechanism, if it is a qt app it will use the qt mechanism, otherwise your in luck, you get to use the native X11 mechanism. gtk: hell if I know, the web sez there are css files, and there are probably some gnome tools to set the theme. qt: no clue. and the web was unhelpful native X11: now we are talking I know this one, X11 provides a database that applications can use for configuration and design. The main interface is the xrdb command. and all good application should include the pertinent points of their call tree to get you started in theming them. http://man.openbsd.org/xrdb reply Arnavion 21 hours agoparentprevWhat those people told you is correct, and \"I don't have those, I have i3\" is incorrect. Stop ignoring them. reply bdjsiqoocwk 21 hours agorootparentI'm not ignoring. I still don't know how do I enable dark mode. If you have the answer, would you kindly help? reply singron 20 hours agorootparenti3 is just a window manager. It's not a UI toolkit. It doesn't control how apps render UI. GTK and QT are UI toolkits and do control how apps render UI. Your apps will use GTK and QT even if you use i3 to manage the windows. You might be confusing GTK with Gnome and QT with KDE, each of which has its own window manager. reply lkdfjlkdfjlg 7 hours agorootparentAnd you know, this > You might be confusing GTK with Gnome and QT with KDE, each of which has its own window manager. Just cleared up a lot of confusing in my mind. Thank you. reply smegsicle 20 hours agorootparentprevso you're saying to get gtk apps to switch to dark mode, you'd find the gtk setting for dark mode? and the same for qt? reply viraptor 19 hours agorootparentYes, for example check \"gsettings set org.gnome.desktop.interface gtk-theme\" for the gtk apps. reply lkdfjlkdfjlg 6 hours agorootparentAs in this post https://askubuntu.com/questions/769417/how-to-change-global-... I ran gsettings set org.gnome.desktop.interface gtk-theme 'Adwaita-dark' and firefox isn't in dark mode, despite the fact that i have firefox configured to use the system theme and firefox is on GTK according to google. Help again? reply vetinari 6 hours agorootparentWhen you set dark mode in the current gnome (with support for dark mode), the following will be set: org.gnome.desktop.interface color-scheme 'prefer-dark' Valid values are “default”, “prefer-dark”, “prefer-light”. org.gnome.desktop.interface gtk-theme '(your-theme)-dark' This is the one you have set. reply lkdfjlkdfjlg 6 hours agorootparent$ gsettings get org.gnome.desktop.interface color-scheme No such key “color-scheme” Debian 11. Is it because it's old-ish? reply vetinari 5 hours agorootparentOh, Gnome 3.38. I'm not sure that version even supported dark mode, only dark themes. I won't comment on aging of Debian ;). Btw, what version of Firefox are you using? Debian comes with ESR, so chances are, that firefox' \"ui.systemUsesDarkTheme = 1\" still works (it doesn't in newer releases). reply lkdfjlkdfjlg 4 hours agorootparentI have 102.6.0esr (came with the OS) and I also installed 126.0. Neither of them has has `ui.systemUsesDarkTheme`. So anyway do you think that if I upgrade to the latest Debian, your gtk command line will work? reply vetinari 3 hours agorootparentIt should; it does on the gnome install: https://youtu.be/Vpkz89gN9zk reply lkdfjlkdfjlg 2 hours agorootparentOh man that's such a nice feature. I guess now I have a reason to upgrade. Thank you! reply lkdfjlkdfjlg 18 hours agorootparentprevSee but this is the kind of responses that confuse me. What does that mean \"gtk apps\"? Is firefox a \"gtk app\"? Is vscode a \"gtk app\"? etc. reply viraptor 17 hours agorootparentCheck the dependencies. If it depends on libgtk, it's a gtk app. libQt, is a qt app. There maybe rare other situations (Tk, plain xlib, fltk, etc.) reply vetinari 6 hours agorootparentThis will get you 99%, but there are exceptions. I.e. Firefox is linked against GTK, but paints its own widgets with custom theming anyways. Telegram is linked against Qt, but has it's own theming/dark mode too. reply lkdfjlkdfjlg 6 hours agorootparentYou seem like you know what you're talking about, help please? https://news.ycombinator.com/item?id=40583867 reply neilv 22 hours agoprev [–] In addition to `/tmp`, you can put your `~/.cache` on `tmpfs`. Laptops have ridiculously large amounts of RAM nowadays. My Linux laptop setup barely makes a dent in the RAM, even when running 3 different Web browsers and other gluttonous desktop programs. `tmpfs` is a great use for excess RAM, reducing wear on SSD. (I also disable swap.) I've also done things like build an entire large ecosystem of packages in `tmpfs`, when the build server happened to have mirrored 10krpms drives, and I didn't want the tons of intermediate files to eventually be synced to disk. (Even though, with disk, they would also probably hang around in Linux filesystem buffers, not reads hitting disk each time.) reply elpocko 21 hours agoparentIf my ~/.cache was on tmpfs, I would need to download tens of gigabytes of stuff every day. I wouldn't be able to work. There are lots of huge files stored in there, mostly required by Python packages for Stable Diffusion and other ML stuff that insists on downloading huge model files and putting them into ~/.cache. Edit: du -hc ~/.cache says it's 25G. reply neilv 2 hours agorootparentAgreed, if you're storing 25GB+ in `~/.cache`, then probably you won't put it on `tmpfs` on a laptop. I didn't think of the massive ML models, because I store those in the home directory, where I can see them. (Incidentally, since you mention Stable Diffusion: for an earlier version of SD, I went to some care to make sure I didn't accidentally lose the checkpoint file, because I didn't know whether it would be pulled from distribution. Then there were regressions in the SD training data.) reply elpocko 4 minutes agorootparent>I store those in the home directory, where I can see them. That works for most things, and that's how it should be. But sometimes the dev just doesn't provide links and you either have to read their code to figure out the model URLs, or simply allow the automatic download that ends up in .cache. reply person4268 21 hours agorootparentprevYou can set HF_HOME to move the folder it downloads those to elsewhere (or well, the HF transformers/diffusers stuff at least), iirc. reply mort96 21 hours agoparentprev8GiB is still very common, and it's not hard to find cheaper laptops with 4GiB... I don't think your average person's laptop has as ridiculously large amounts of RAM as you seem to think reply Aurornis 21 hours agorootparent> I don't think your average person's laptop has as ridiculously large amounts of RAM as you seem to think Your average person isn’t running Linux or compiling things. The comment was in the context of developer laptops. reply arp242 19 hours agorootparentI run Linux and have 8G and compile things. Works fine. Currently ~/.cache is 2G, but that's because I cleared out ~/.cache/go just yesterday after compiling ~1000 Go modules for some testing, which bloated it to >50G. reply nottorp 5 hours agorootparentWhat kills your ram is not compiling things but the browser and those Electron apps. (Well and Yocto builds, but that's not your average 'compiling things'.) reply pantalaimon 21 hours agorootparentprevNot every developer always has the biggest and latest laptop. reply mort96 11 hours agorootparentprevFair. I read the comment as, more or less, \"the typical laptop has so much RAM these days that putting ~/.cache on tmpfs would be a reasonable default\" or something like that. But I guess it could -- and should -- be read as, \"if you spend a bunch extra on RAM, you can get laptops with so much RAM these days that you can consider putting ~/.cache on tmpfs\", which is reasonable enough. reply zbowling 21 hours agoparentprevThis would break a bunch of little things in annoying ways. Like I have shell script tools that store shell scrollback logs in .cache, and I want that past reboot. reply neilv 21 hours agorootparentFWIW, I've been doing it for many years, and not noticed any problems. If something is supposed to persist past reboot, I wouldn't put it in `.cache`. Though I don't know offhand what the official documented behavior of `.cache` is, and I can't immediately find that documentation (maybe some open desktop cabal thing?). reply mhitza 19 hours agorootparentXDG_CACHE_HOME same text across bott resources https://specifications.freedesktop.org/basedir-spec/basedir-... or https://wiki.archlinux.org/title/XDG_Base_Directory > $XDG_CACHE_HOME defines the base directory relative to which user-specific non-essential data files should be stored. If $XDG_CACHE_HOME is either not set or empty, a default equal to $HOME/.cache should be used. reply arp242 19 hours agorootparentprevIt's fine to put ~/.cache on tmpfs, but doing it by default for the general case is going to cause a lot of hurt. My ~/.cache could be rm -rf'd without too much worry right now, but that doesn't mean that persisting it isn't useful. For example on my system right now: - Browser cache is useful to persist, especially with some larger sites. - I put my Go module and build cache in ~/.cache, and while that can be deleted it's useful to persist because it make builds shorter, and avoids having to (re)-download the same modules over and over again. Note that at the moment my internet is kind of crappy so this can take quite a while. - Some other download cache things in there, from luarocks, xlocate, few other things. - I store psql history per-database in ~/.cache/psql-dbname. It's useful to keep this around. - Vim backup files, persistent undo files, and swap files are stored in ~/.vim. The swap files especially are important because I want to keep them after an unexpected system crash. Some of this is solvable by moving stuff to other directories. Others are inherently unsolvable. I also have just 8G of RAM, which is fine but not fine for storing ~/.cache in RAM. reply viraptor 19 hours agorootparentprevThat's silly. Cache is there for preserving things across many runs of some application. Applications certainly use it in a way that assumes long term storage. It's not for short term temporary things. It's a cache. It's not going to cause \"problems\". It's just going to massively slow down many use cases that rely on downloads. reply binkHN 14 hours agoparentprevI do have a notebook with plenty of RAM, so I haven't bothered with swap, but I'm new to Linux. Without swap configured, what happens when tmpfs uses all the memory that's been allocated to it? Thanks. reply tjoff 21 hours agoparentprevNo. Thinner laptops often max out at 32 or even (!) 16GB. And defaults at incomprehensible 8GB. reply exe34 21 hours agoparentprev [–] how do you hibernate without swap? or do you just sleep? reply pantalaimon 21 hours agorootparent [–] Standby is usually enough and much faster anyway reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Debian 13 (\"Trixie\") will use a RAM-based tmpfs for the /tmp directory and implement automatic cleanup of temporary files in /tmp and /var/tmp, similar to practices in Arch Linux and Fedora.",
      "This change has sparked debate about its impact on memory management, system performance, and the handling of long-running job data, especially on older systems with limited RAM.",
      "Users can customize or override these settings during installation, and the discussion highlights the importance of swap space, security, and robust error handling."
    ],
    "commentSummary": [
      "The LWN.net discussion examines the risks and inefficiencies of using RAM-backed `/tmp` in Debian, especially during unexpected reboots, and suggests disk-backed `/var/tmp` as an alternative for persistent storage.",
      "It highlights the benefits of systemd-tmpfiles for managing temporary files and the complexities of filesystem layouts, including debates over merging `/var/tmp` and `/var/cache`.",
      "The conversation also covers containerization tools like Snapcraft and Flatpak, the impact of tmpfs on system performance, SSD wear, and memory management, with mixed opinions on swap and zswap effectiveness."
    ],
    "points": 193,
    "commentCount": 242,
    "retryCount": 0,
    "time": 1717531524
  },
  {
    "id": 40584901,
    "title": "Study Reveals Agile Software Projects Have 268% Higher Failure Rates Than Non-Agile",
    "originLink": "https://www.theregister.com/2024/06/05/agile_failure_rates/",
    "originBody": "Software 87 Study finds 268% higher failure rates for Agile software projects 87 In praise of knowing the requirements before you start cranking out code Richard Speed Wed 5 Jun 2024 // 09:25 UTC A study has found that software projects adopting Agile practices are 268 percent more likely to fail than those that do not. Even though the research commissioned by consultancy Engprax could be seen as a thinly veiled plug for Impact Engineering methodology, it feeds into the suspicion that the Agile Manifesto might not be all it's cracked up to be. The study's fieldwork was conducted between May 3 and May 7 with 600 software engineers (250 in the UK and 350 in the US) participating. One standout statistic was that projects with clear requirements documented before development started were 97 percent more likely to succeed. In comparison, one of the four pillars of the Agile Manifesto is \"Working Software over Comprehensive Documentation.\" According to the study, putting a specification in place before development begins can result in a 50 percent increase in success, and making sure the requirements are accurate to the real-world problem can lead to a 57 percent increase. Dr Junade Ali, author of Impact Engineering, said: \"With 65 percent of projects adopting Agile practices failing to be delivered on time, it's time to question Agile's cult following. \"Our research has shown that what matters when it comes to delivering high-quality software on time and within budget is a robust requirements engineering process and having the psychological safety to discuss and solve problems when they emerge, whilst taking steps to prevent developer burnout.\" The Agile Manifesto has been criticized over the years. The infamous UK Post Office Horizon IT system was an early large-scale project to use the methodology, although blaming an Agile approach for the system's design flaws seems a bit of a stretch. Report: 83% of UK software engineers suffer burnout, COVID-19 made it worse 'Business folk often don't understand what developers do...' Twilio boss on the chasm that holds companies back IBM warns Global Tech Services staff that 346 UK heads will roll in latest redundancy action Erik Meijer: AGILE must be destroyed, once and for all It is also easy to forget that other methodologies have their own flaws. Waterfall, for example, uses a succession of documented phases, of which coding is only a part. While simple to understand and manage, Waterfall can also be slow and costly, with changes challenging to implement. Hence, there is a tendency for teams to look for alternatives. Projects where engineers felt they had the freedom to discuss and address problems were 87 percent more likely to succeed. Worryingly, workers in the UK were 13 percent less likely to feel they could discuss problems than those in the US, according to the study. Many sins of today's tech world tend to be attributed to the Agile Manifesto. A neverending stream of patches indicates that quality might not be what it once was, and code turning up in an unfinished or ill-considered state have all been attributed to Agile practices. One Agile developer criticized the daily stand-up element, describing it to The Register as \"a feast of regurgitation.\" However, while the Agile Manifesto might have its problems, those stem more from its implementation rather than the principles themselves. \"We don't need a test team because we're Agile\" is a cost-saving abdication of responsibility. In highlighting the need to understand the requirements before development begins, the research charts a path between Agile purists and Waterfall advocates. ® Sponsored: Matching the cloud database to real workload needs Share More about Continuous Delivery Continuous Development Developer More like these × More about Continuous Delivery Continuous Development Developer Devops Software Narrower topics AdBlock Plus API App Application Delivery Controller Audacity Cloud native Confluence Database FinOps FOSDEM FOSS Git Grab Graphics Interchange Format IDE Jenkins Legacy Technology LibreOffice Map Microsoft 365 Microsoft Office Microsoft Teams Mobile Device Management OpenOffice Programming Language QR code Retro computing Search Engine Software bug Software License Text Editor User interface Visual Studio Visual Studio Code WebAssembly Web Browser WordPress Broader topics Development More about Share 87 COMMENTS More about Continuous Delivery Continuous Development Developer More like these × More about Continuous Delivery Continuous Development Developer Devops Software Narrower topics AdBlock Plus API App Application Delivery Controller Audacity Cloud native Confluence Database FinOps FOSDEM FOSS Git Grab Graphics Interchange Format IDE Jenkins Legacy Technology LibreOffice Map Microsoft 365 Microsoft Office Microsoft Teams Mobile Device Management OpenOffice Programming Language QR code Retro computing Search Engine Software bug Software License Text Editor User interface Visual Studio Visual Studio Code WebAssembly Web Browser WordPress Broader topics Development TIP US OFF Send us news",
    "commentLink": "https://news.ycombinator.com/item?id=40584901",
    "commentBody": "Study finds 268% higher failure rates for Agile software projects (theregister.com)174 points by johnsutor 4 hours agohidepastfavorite159 comments CSMastermind 4 hours agoObviously, it's a biased study that shouldn't be taken seriously, but while we're on the topic of Agile: Consulting Agile, grew out of web development consulting firms as a defensive methodology for managing shitty stakeholders. They incorporated some genuine industry best practices, but they also have a lot there that's simply meant to blunt the worst impacts of having to deliver software in an environment where the person paying you doesn't understand the basics of software development (or project management really). It actually works pretty well in those environments - you don't get hyper-productive engineering teams, but you do get somewhat consistent delivery and a stable enough working environment that the worst outcomes (not delivering anything at all) are avoided. No large tech company that I'm aware of implements this type of Agile but plenty of big non-tech companies do. If you have a CIO in your company and the business refers to software engineers as 'IT' there's a good chance you do this. This is inefficient but it's probably good enough for those companies, especially if it's established, the switching costs alone would be a nightmare. If you're using it as a startup then something has gone terribly wrong and you should seriously reevaluate what you think the future of the company will be. reply tinco 3 hours agoparentIt's not just a biased study, it's an article about a study of which the data is not published, so it's impossible to tell if there's any merit to it. The excerpts of the study published in the article do not inspire confidence either: 1. Agile (\"Agile Requirements Engineering\") is defined as: Development starts before clear requirements, no complete specification, significant changes late in development. I think even those who are opposed to Agile would agree that's a severely lacking if not completely incorrect characterisation of Agile development practices. It sounds more like a list of things that would cause an Agile project to fail. 2. Impact Engineering is defined as: Use of all engineering practices studied which increase success rates. Why doesn't the author dare to give any properties of Impact Engineering up front? This sounds like he's just baiting for people to buy the book so they could learn what magical engineering practices are studied therein. It smells scammy. reply giantg2 4 hours agoparentprevYep, and the big companies basically use Agile as an excuse for not thoroughly planning and vetting ideas. Just build it and figure it out as you go. Then rebuild everything in 2 years since you're left with outdated and unmanageable spaghetti code and you burnt out all your SMEs so they took the knowledge with them. reply Pet_Ant 3 hours agorootparentDo they ever rebuild things? Or just add features ontop of features till the whole thing creaks. reply dopylitty 3 hours agorootparentSeveral years after a project completes when the system is finally sort of working and the end users know what to expect new management will come in at a high enough level and demand everything be rebuilt using their tech stack of choice because they need the resume entries. Then of course you'll get a buggy second system that is back at square 0 which the business doesn't understand/hates and the developers hate because they're constantly fighting fires. reply mrbombastic 3 hours agorootparentprevgenerally they wait until something explodes or they cannot add any features to the unmaintainable mess and they have no choice :) reply giantg2 3 hours agorootparentYep, this is what my experience is. It could be a cost explosion too, like using expensive mainframes and migrating to something else before the MIPS cost explodes. reply Muromec 2 hours agorootparentDid somebody successfully migrated from a mainframe? Asking for a friend. reply giantg2 2 hours agorootparentYes, I've heard of some systems successfully migrating from the mainframe at my company. It tends to be a painful process. There are still some apps that need to be migrated. reply happymellon 1 hour agorootparentprevYou make it sound like Waterfall had full plans and completely vetted ideas. It didn't. > Just build it and figure it out as you go. That sounds like hundreds of waterfall projects that I was pulled into after they hit the 1/2 way mark and we're running behind because of unexpected scenarios. reply giantg2 1 hour agorootparentIt had better plans than the way Agile has been done on the teams I've been on. You can certainly misuse either. In my experience, planning is neglected more Agile. reply ano-ther 3 hours agoparentprevGood observation. Having been on the “customer” side in such companies, I’d say the improvement is the result of more frequent interaction which improves mutual understanding. The internal customers are forced to think through requirements in detail (albeit in pieces). The developers are forced to explain what they are doing. Too many projects I’ve seen suffered from a lack of communication. Customers would hand things off (“just build x”, lots of customization requests that aren’t thought through). Developers would start guessing and working in the wrong direction, often going on complicated tangents as a result of the customizations). Consulting Agile as you call it really helps both sides to understand what’s actually needed and how to get there. reply bityard 3 hours agoparentprevI have observed that there are two kinds of Agile. \"Agile in doctrine\" is the kind all of the anti-Agile crowd rail against because they've been burnt by working for some company or manager who drank the Koolaid without actually understanding what Agile is. Blindly following some consultant's Agile playbook is not Agile. \"Agile in spirit\" is a bit like The Zen of Python: X is usually better than Y but there are times where it makes complete sense to do Y. In order for Agile to work, you need to have leaders and CIs who understand that. Agile emphasizes communication and flexibility. Agile is advisory, not prescriptive. reply JackFr 3 hours agorootparentI largely agree with you, but when it comes to evaluating competing methodologies, what you refer to as “agile in spirit” turns quickly into No True Scotsman. “If you project failed you mustn’t have been doing agile correctly.” reply jerf 3 hours agorootparentThere is some virtue to your criticism, but it is also not entirely true. There are things you can check to see if a team is doing Agile for real, like, what's the last process change you experimented with to see if it improved your outcomes? What experiment failed and was dispassionately removed from the process because it didn't work out? When your team's workload nature changed (e.g., from new development to maintenance), how did you adapt your processes to match? Do your processes come up from the team and their experience or do they source from above with effectively no feedback? At that point, if you are doing those things, there is a certain amount of validity to the criticism that if it didn't work, you really weren't doing it right. Either something external jammed you up so you weren't able to adapt and truly follow agile, your team was personally unable to execute on the adaptation for some reason (structure, personality issues, experience levels, being simply too large to be able to be this flexible because large teams simply need more structure), or the task was simply too hard in the first place for some reason (such as \"it doesn't matter how agile and smart the team is, you're not getting 4 people to produce a standards-compliant browser that is also an office suite in six months\"). reply quacked 3 hours agorootparentprevTo me the single most common evidence of failure to perform \"Agile in spirit\" is management holding specific teams responsible for missing a deadline after a feature or due date was changed. It shows that the management doesn't understand software development. If you think you can add features or move delivery dates to the left without consequences, you've misjudged your entire industry. reply codr7 2 hours agorootparentYeah, my former wannabe-boss was very surprised to realize that splitting the company in two and selling one part during my onboarding period might have a negative effect on my productivity. We're not doing a very good job of picking the right people to manage things; quite the opposite, in many cases whatever success is despite the leadercrap we have to deal with. reply spacephysics 3 hours agorootparentprevAs another commenter said, it’s difficult to compare “Agile in spirit”, but I’ve found that remarkably successful Evolving the methodology to suit the team, product, and company in an additive way seems the best outcome I’ve seen. Rarely will I have daily meetings, we can slack. Maybe twice a week hop on a call. For story points I’ve seen models of using Fibonacci sequence, having 4 categories rated which then convert to points, or even (less desirable) 1:1 mapping to days of work. Personally I like the 4 categories approach, it feels less subjective as a whole despite each category being subjective to a degree And in the end, if process gets in the way of productivity, I say go for it, we’ll either go back and document what happened or refine the process later (though rarely if ever have we skipped QA for good reason) reply dijksterhuis 3 hours agorootparentprevBig A, small a. Agile — scrums, scrum masters, epics, stories agile — http://agilemanifesto.org/principles.html Agile is a bunch of rules and processes to follow. agile is an ideal to strive towards, and never perfectly reach. —- Edit — this bit from the article is probably the important line > However, while the Agile Manifesto might have its problems, those stem more from its implementation rather than the principles themselves. Agile is not the same as agile. At least in this humble dev’s mostly irrelevant opinion. reply spamizbad 3 hours agorootparentprevThe problem with \"Agile in spirit\" is that if adopt it and don't get the desired results, Agile aficionados will quickly remind you that you weren't actually doing Agile. reply eyelidlessness 3 hours agorootparentThis is a common refrain, but I don’t think it’s true. “Agile in spirit” people will recognize the failure and look for ways to adapt so similar failures are less likely to occur. Rejecting that opportunity to reflect and adjust is where “not actually doing agile” comes into the discussion, and it doesn’t really have anything to do with “agile” at that point. If you applied the same reasoning to any other iterative self-adjusting process, you’ll find they all tend to fail before the iterative aspect has been applied. Because that’s their nature, they’re iterative processes as a mechanism to respond to failure. reply marginalia_nu 4 hours agoprevThe by far strongest correlate of a complete clusterfuck of a project is how often, per meeting, agile is invoked. In fact, I've discovered you can use agile as a sort of meeting hand grenade, if you don't like the direction a meeting is headed, like they're about to decide on something stupid, you can just throw in \"wait, is that agile though?\" and the rest of the meeting will discuss methodology, never arriving at any sort of conclusion. reply brightball 3 hours agoparentYea. I do Fractional CTO consulting and usually have good results with developers because I focus heavily on removing bottlenecks, unhelpful processes and meetings, etc. The challenge is always when you run into sacred cows from other levels of management. - Hard deadlines while acknowledging we don't have enough information to commit to those deadlines - Story points as a measure of time - Absolutely overloaded teams being asked to deliver new features without being given time to improve underlying issues in the system...and then having to constantly stop what they are working on to deal with production issues caused by those underlying problems. It's like clockwork sometimes and the conversations are always hard. reply cjk2 3 hours agoparentprevThis is my methodology. Set two PMs on each other and go and do some work that generates ROI. reply yyggvbb 3 hours agorootparentThis is a good way to get laid off. A developer has never generated ROI in the history of finance, it was ALWAYS an MBAs idea. reply cjk2 3 hours agorootparentI've prevented a lot of losses :) reply notatoad 3 hours agorootparentprevsomebody's got to give the MBAs their ideas. reply dmix 2 hours agoparentprevSo kind of like the stereotypical communist meetings where they spend all their time in meetings debating each other and obsessively discuss the finer points of socialism instead of ever accomplishing anything. reply bitwize 2 hours agorootparentI still remember one PM's response to my complaints about the number and length of meetings on one Scrum project I was on: \"Let's have a huddle on the question of whether we have too many meetings?\" (She tended to use a rising terminal in a very passive-aggressive, velvet-glove-on-iron-fist sort of way.) reply kevin_nisbet 4 hours agoprevI'm not really sure what to make of this article, it says itself it's a study commissioned by promoters of another methodology. If they're consultants trying to pitch their consulting services, they need these types of things to sell. But my fundamental reaction is, what does failure mean. Because in my world view failure should be expected and accepted and learned from. It's entirely possible to spend a bunch of time avoiding every possible failure mode, and really not delivery much value at all... but we've successfully avoided the work being considered a failure. reply rawgabbit 16 minutes agoparentThis is the issue of agile. It is a blanket prescription for every scenario which of course … fails. If you are talking about customer expectations, yes, you can manage customer expectations. As long as you are honest and upfront, customers tend to be forgiving. On the other hand, if you are talking about a core API or building block that downstream APIs will depend on, then no. You can't deliver a API that writes to the database/storage layer that only works 90% of the time. reply fisf 4 hours agoparentprev> But my fundamental reaction is, what does failure mean. Because in my world view failure should be expected and accepted and learned from. That's fine in some domains. It's not acceptable in others (i.e. you cannot just try things and see if they stick). But regardless of that, you still want to minimize failure. So if one methodology (Agile) pushes to spend less time on fixed and clear requirements upfront, and this leads to higher failure rates, this is still an issue. reply trickstra 4 hours agorootparentAgile is about reacting to changing requirements. If the requirements never change, then agile doesn't promise to be better than other methodologies. It came from a rapidly changing crashing markets in the .com bubble. How would the other methodologies fare if requirements changed two or three times in the middle of the project? That's when you need to be agile. How did they even measure \"success\" if the requirements weren't given up front? What did they expect at the end? reply thaanpaa 3 hours agorootparentAgile means delivering very small increments to users at a frequent pace, allowing them to provide feedback early and often. When done correctly, this eliminates a significant amount of guesswork. However, if the client (or, indeed, your own team or management) does not fully understand the process, things can go spectacularly wrong in many different ways. reply trickstra 3 hours agorootparentWe still need to explain what \"agile\" is. Ask yourself why do we \"need\" to deliver small increments at frequent pace? Isn't it to be able to react to changes? reply tsunamifury 3 hours agorootparentprevI mean you are entirely discounting that it also allows engineers to pick their tasks and pace them as they see fit, as well as have input into the creation and prioritization of those tasks. Everytime I hear an engineer complain about this, I ask them if you wish the PM would just write a PRD full of guesswork tasks and you execute on them unquestioningly. reply chownie 1 hour agorootparentI have never worked in an agile setup in which engineers pick tasks, we're given a queue and we take them in priority order with no room for choice. How common is that really? reply trickstra 3 hours agorootparentprevSome engineers complain about having the ability to pick their tasks and have input in prioritization and planning? This has always been the most attractive part of agile for me. Self-organized team. Built around motivated individuals. Striving for technical excellence. reply tsunamifury 3 hours agorootparentIn forums almost every engineer that complains about agile to me then goes on to describe a deeply regimented waterfall system they are actually in. The second common complaint is “I don’t want to do the hard work of giving input, just tell me what to code”. reply trickstra 2 hours agorootparentThis is why I hire students. I have to teach them my way before some other company ruins them forever. reply The_Colonel 3 hours agorootparentprevProjects with fixed, clear and unambiguous requirements (and the necessary counterpart - fixed budget) I worked on typically turned into a nightmare, because they were not actually fixed, clear and unambiguous. You could pretend everything was fine for like 80% of the allocated time, the next 80% were spent in bickering over possible interpretation of some not-so-unambiguous line in the spec, death marches, blaming each other (client and supplier, but unfortunately also within the company). The main innovation of agile is showing failures early and thus being able to quickly react and correct course, meanwhile in the waterfall the project spent most the time in a quantum superposition without possibility for correction. reply adastra22 3 hours agorootparentprev> It's not acceptable in others (i.e. you cannot just try things and see if they stick). Why not? > So if one methodology (Agile) pushes to spend less time on fixed and clear requirements upfront, and this leads to higher failure rates, this is still an issue That is an incorrect characterization of Agile. Maybe some \"agile\" companies use it as an excuse to avoid requirements, but that is definitely not part of the process. reply fisf 3 hours agorootparent> Why not? If you fulfill the role of a supplier of a component in a bigger product (especially in an industrial setting), you cannot \"try to build something\". People are not happy if you tell them that you want to iterate on their control system and deliver incremental value, or tell them that it's not possible in the contractually agreed way. > That is an incorrect characterization of Agile. These are also the cases where you just have to work out overarching requirements upfront. reply adastra22 1 hour agorootparentAh, fair. This is a startup site so my mind was on organization that have the freedom to pivot. It is supposed to be that new requirements come from the customer though, working closely in conjunction with engineering team reps. So it’s more like “deliver MVP control system software, test it out with actual users, then incorporate their requested changes into the next sprint.” I’m not surprised if most so-called agile teams didn’t do that though. reply fisf 22 minutes agorootparentYes, that's possible for product development in a startup (even necessary, because you don't know what you need to build), and in web dev, end user facing apps,... For many other domains, the full incremental / Agile loop is not possible, and becomes disaster if it results in skipping upfront planning: The counterparts you need to integrate with are often not done yet. Maybe the hardware is not finalized. The end user is years away. Maybe each test run is hugely expensive. There is no MVP here, as in the system works as expected or it doesn't. You cannot meaningfully iterate the core software running an MRI machine, a surgical system, financial clearing, pick-and-place machines, automatic rail protection, etc. towards correctness with the end user. reply commandlinefan 3 hours agoparentprev> what does failure mean I've been reading that 50+% of software projects \"fail\" ever since I started programming in 1992 (long before the term \"agile\" crept into our lexicon). They consistently fail because they consistently have deadlines but no actual definitions. That was true before Agile, was true during Agile, and will be true after Agile. reply pavel_lishin 4 hours agoparentprevThe article also doesn't go into what kind of software was being produced, etc. If you're building control systems for the ISS, or an interplanetary probe, of course you need requirements hammered out pretty well before you start development; you can't just continually push updates out incrementally. But if you're building Yet Another CRUD App at your startup's third pivot, then getting every requirement written down beforehand is virtually impossible. reply karaterobot 4 hours agoprev> However, while the Agile Manifesto might have its problems, those stem more from its implementation rather than the principles themselves. Is the author an Agile coach? This is the classic response when people observe that Agile has failed to transform their team: you're doing it wrong, the problem is you. When systems seem good in theory but tend to fail in practice, you should just try to learn what you can from them and move on to the next thing. reply thaanpaa 4 hours agoparentNo, it really is true. Agile is widely misunderstood. The way scrum is implemented in 90+% of the cases is a complete mockery of what it was supposed to be. A lot of coaches completely miss the mark as well. And the fundamental problem is that agile is completely incompatible with the way most enterprises are built. A truly agile team talks to the users of the product, and they have complete control and responsibility over the development process. That just doesn't happen because managers want to manage, and as much as developers would like freedom, they'd often rather not have the responsibility. reply lpapez 3 hours agorootparent> The way scrum is implemented in 90+% of the cases is a complete mockery of what it was supposed to be. Can you really say it is misunderstood if 90+% of people are \"doing it wrong\"? Is it a useful thinking framework if there is a 90+% chance of \"getting it wrong\"? I would NEVER buy a tool which advertised only a 10% success chance, so why are we falling for scrum? reply yobbo 3 hours agorootparentPhrasing it as \"doing it wrong\" misses the mark. They want the name (scrum, agile ...) without paying for it through the reorganization the would be necessary. reply projektfu 3 hours agorootparentprevElectric cars have electric motors. So if you attach an electric motor to an F150 without removing the gas engine, you have an electric car. And it has terrible gas mileage. reply commandlinefan 3 hours agorootparentprev> a complete mockery of what it was supposed to be I started programming long before \"scrum\" or \"agile\" or even \"XP\". One consistent observation I've made around every methodology that comes along is that management manages exactly the same way they used to manage after they roll out the methodology as they did before the methodology. reply hk1337 4 hours agoparentprevHonestly, 99% of the time people are doing it wrong. People are either too strict with it or too \"loosey goosey\" with it. *EDIT* It doesn't have to be correct, it just has to work for the people on the team. If it works, then use it, if it doesn't then change it. That's agile. reply crowcroft 4 hours agorootparentIf people can only ever get into the goldilocks 'not too strict, not too loose zone' 1% of the time, then it the process is by default not fit for purpose in almost any circumstance. Likely the 1% of the time it works is the rare case that you have a small and exceptional team working together, in which case the project management methodology is unlikely the reason they are working well. reply hk1337 3 hours agorootparentYou don't have to, just find what works for your team and use it. It may not be perfect or exactly how it is supposed to be but if it works then use it. reply karaterobot 3 hours agorootparentprevWhat would be the Agile approach to a process with a 1% success rate? reply trickstra 3 hours agorootparentHave the team reflect and come up with a better process, or they will be disbanded. Give them any and all the resources they ask for. reply karaterobot 2 hours agorootparentSo, not tell them that they're doing the process wrong? Weird. reply trickstra 2 hours agorootparentNo, they need to come up with that on their own. What exactly \"wrong\" means? And why do you think you know it better than them? What if they find some better way that you didn't know about? Agile says that it needs to be sustainable also for the sponsors. Show the team that if the sponsor isn't able to sustain the next sprint, there won't be a next sprint. The process is up to them. reply fook-dang 4 hours agorootparentprevThat's what they say about communism too reply pydry 4 hours agoparentprevThat certainly applies to Scrum but agile is more of a religion than a system. It's a bunch of extremely vague \"principles\" whose meaning is often anyone's guess. What constitutes agile is basically a matter of opinion. reply boxed 4 hours agoprev> \"Our research has shown that what matters when it comes to delivering high-quality software on time and within budget is a robust requirements engineering process and having the psychological safety to discuss and solve problems when they emerge, whilst taking steps to prevent developer burnout.\" I almost laughed out loud. Isn't that agile? :P reply oriel 4 hours agoparentProbably the originally intended Agile [1] These days its been devolved into micromanagement musical chairs of pain. The kind that promotes what it professes to prevent and provide. [1] https://agilemanifesto.org/ reply jvanderbot 3 hours agorootparentBefore someone says \"No true scottsman\", consider that \"Agile\" is supposed to mean: > People over processes and > Responding to change over plans And capital-A-agile is literally a textbook process. reply boxed 4 hours agorootparentprevYea, like when people say they are Christians but are against helping the poor. Pretty clearly not Christians in any reasonable definition... reply falcor84 2 hours agorootparentIsn't \"Christian\" one of those predicates where identifying as such (public profession of faith) is what makes you one? I mean unless you're going for a denomination that requires baptism. reply boxed 1 hour agorootparentYea, that's what it has devolved into. At least Catholicism can protect its brand against crazy stuff like that. The agile guys should have done something like that from the start... if they had known what was coming. reply Aurornis 4 hours agoparentprevEvery agile coach always tells me that agile is whatever works for the team. Conveniently, if something isn’t working they dismiss it as not true agile. Of course, all of the processes and meetings they push on to the team don’t actually work out, but they’re long gone by then. reply boxed 4 hours agorootparentI mean.. \"science\" is what works too. If something turns out to be false, we throw it out and say it's not science. This is good. I don't see a problem with this at all. But pushing useless stuff that doesn't work and then leaving, yea that's not agile :P reply giantg2 4 hours agorootparentScience has proven methodologies to it and those are explicitly taught, followed, and reviewed. Agile coaches offer no real guidance if the bulk of the advice is \"do what works\". No shit... reply boxed 2 hours agorootparentThe methodologies were discovered after the fact though. The only real scientific method is \"if you are a winner you are with us\". In fact, plenty of scientific methodologies are NOT tested and probably super bad, like the modern peer review system, or grant writing/proposal/reviews. reply giantg2 2 hours agorootparentNeither of those are methodologies for actual science, but rather publication and funding issues. The methodology most closely related to peer reviews would be idea of reproducibility. There are plenty of peer reviewed things that fail reproducibility. reply giantg2 4 hours agoparentprev\"robust requirements engineering process\" I worked on one team that had this and it was amazing. It implicitly helped with burnout too. It was an \"Agile\" team, but we followed more of a scrumifall model that worked well for that project. I've been on a bunch of Agile teams (some more than others) since then, but the requirements processes are all garbage. Then the leaders wonder why things take so long, there's so many bugs, burnout is high, etc. It's really not surprising. reply farresito 3 hours agorootparentCould you expand a little bit on what the \"robust requirements engineering process\" was like on that one team? reply giantg2 3 hours agorootparentIt was an app that was heavily focused on business processes, like routing items between different groups. We had spreadsheets and process maps for each business process. This means the business actually had to think about the possible routes it could take and the data required on each step. So the spreadsheet would tell us all the possible data exposed on the step including how it was treated (R, RW, required, formats/types, etc). It was magical. Once a process was created the business would have to sell any process changes to the product owner by either showing it was a regulatory/legal issue, or using a cost study to show it would save money. So not back and forth or superfluous changes. reply fook-dang 4 hours agoparentprev\"prevent developer burnout.\" ...Frankly I think all the standups & the pressure from constantly asking me if I am contributing (every single day?!) instead of trusting that I am... burns me out. I don't mind 2-3 standups per week, 15 mins max. But 5x 30 minute standups... Is exhausting. It feels like a lack of trust from product managers. Oh yeah... and unqualified, zero engineering experience, diversity-hire product managers. That's super annoying-- being managed by people who did not obtain their role via the route expected of an engineer (having demonstrable engineering skills) reply analyte123 1 hour agorootparentIf you read about agile, standups are supposed to be conducted among the dev team members who are working together on something, and primarily for their benefit, not as a status update to or for someone else. The PM or whoever is considered the \"product owner\" in scrum could listen in and help unblock issues, or report delays to other people, but not demand more information or change things up that have already been planned. Of course, it actually working this way is rare, although I have seen it. I haven't really seen this, but a poorly performing product manager or product owner completely breaks the scrum or agile model (to the extent it works at all). They are assumed to basically know the domain and know what needs to be built at a high level, and the software requirements originate from them in the scrum model. If they don't know what the requirements should be or how to communicate them or how to collaborate with the engineers on the requirements, it is completely garbage-in-garbage-out. On the other hand, working with a product owner who is a domain expert and happy to help define software to solve their needs can be a joy. reply adastra22 3 hours agorootparentprevThe equation of \"agile = standups\" has done more harm to agile (and projects using it) than anything else. I run a company and we use a lot of agile project planning methods, but threw out the standup. Especially in the era of remote work and varying timezones, it makes no sense at all. reply thaanpaa 3 hours agorootparentprevThat's all kinds of wrong. A stand-up is supposed to take 15 minutes max, and if no one is stuck with anything, it shouldn't take more than just a couple of minutes. It's not supposed to be a \"status meeting\" at all. reply thayne 3 hours agorootparentUnfortunately, in practice,that doesn't seem to happen very often. reply sroussey 3 hours agorootparentprevIt’s a synchronization primitive, not the only kind available but most popular. It should not be 30min. Maybe 10. And it’s not a test of your contribution. It’s part of being a team. reply projektfu 3 hours agorootparent\"Why is it called a stand-up meeting?\" \"So we are incentivized to get it done quickly and go back to our desks.\" \"Ah, so if we sit down we can make the meeting longer....\" reply affinepplan 4 hours agoprevthis smells very much like selection bias, in that it seems pretty plausible that * if you have a project that is Very Important To Succeed, a team might be more likely to adopt waterfall * if you have a project you just want to trial and don't care if it fails, maybe agile practices are more likely reply dstroot 4 hours agoparentI like to think about it like this. If you are building a skyscraper in Manhattan you need a plan. It has to be engineered. The materials must be specified and ordered long before construction starts. There are engineering practices and building codes that create a measure of safety. There is an order to the construction process. The project plan would likely be called “waterfall”. If you attempted an agile approach of dropping off a team and saying for our first sprint let’s just dig for two weeks, then we’ll plan what’s next is not going to cut it. Agile on the other hand is great for smaller, less complex efforts where the desired outcome is “fluid”. Different methodologies work best for different things. reply gedy 4 hours agorootparentAll I can say is the big waterfall projects I was on pre-agile had excellent planning for the wrong things, and took up most of the time allocated for the project. So we were out of time, the quality and people suffered. Another project was cancelled because we took so long planning, the company lost interest. reply dustedcodes 4 hours agoparentprev> if you have a project that is Very Important To Succeed, a team might be more likely to adopt waterfall Yes, but that is directly in contrast with what Agile promises to do, namely helping teams to succeed. reply Attrecomet 3 hours agorootparentNot if the project succeeds because it was important enough for more resources and time to be allotted. reply TheCoelacanth 2 hours agoparentprevAlso, how is success vs failure defined? Whether you implemented what you intended to on time and on budget? That's a reasonable definition, but it doesn't capture whether the project actually achieved the business goals it set out to achieve. For many projects, it's better to fail quickly than to succeed and then discover afterwards that it wasn't worth doing the project. reply betenoire 4 hours agoparentprevI don't think that's what a selection bias is. If I have two cars, and electric for getting around locally, and a guzzler for road trips, you wouldn't call it selection bias if I always chose the guzzler for the road trips. You'd just recognize that they are better for that task. reply svnt 3 hours agorootparentThere is choosing the right tool for the job, and then there is evaluating the tools against each other on the properties specific to the job. If after making your tool selection you went on to author a study about how gas cars result in more frequent bathroom breaks while driving, without controlling for the length of the trip, you’d be doing what these folks did, and that’s selection bias. reply HelloNurse 3 hours agoparentprevThere are more direct selection biases in the study: \"projects which had clear requirements before starting development were 97% more likely to succeed\" because hitting a moving target is more difficult, with all methodologies. reply JohnMakin 3 hours agoprevIn any discussion on this site or others about Agile, you will find a flurry of PM's rigorously defending it with the typical line of reasoning - If you tried agile and it failed, it wasn't actually agile. When you show how it was actually agile, you didn't do it right. When you show you did it right - go back to argument 1. It's a really circular no true scotsman style of argument. Obviously this \"study\" is not good or even really a study, but I think the general arguments it makes aren't tremendously out of line. I have been an IC on agile teams, an IC on more traditional teams that I guess would be called \"waterfall,\" and I've been tasked with implementing a scrum process both as a team leader and as a PM in my career - I've personally never seen it work on Ops/Infrastructure teams. In fact, I'm convinced it can't. It's far too difficult to determine the scope of work, far too interrupt driven, far too difficult to measure actual velocity, way too easy for engineers and managers to bullshit the system. All the arguments I see in favor of it over the years strike me as pretty dogmatic. The way I like to manage projects these days is a kind of kanban system with a task queue, and 1-2x a week brief cadence meetings to discuss priority/alignment and any blockers. edit: should probably clarify I'm using agile == scrum here, which I know isn't technically correct, but that's how I've seen it implemented in 99% of cases so far in my career. reply whynotminot 3 hours agoparent> The way I like to manage projects these days is a kind of kanban system with a task queue, and 1-2x a week brief cadence meetings to discuss priority/alignment and any blockers. To me, distilled down to its core, agile is all about faster feedback loops and course corrections. If you’re regularly getting good, actionable feedback on your work, and maintaining alignment, I think you’re living up the spirit of agile. There are plenty of “agile” teams going through the motions, doing the ceremonies, but not actually reaping what the real benefit is supposed to be: frequent, actionable feedback that helps guide and align the next phase of work. reply Attrecomet 3 hours agoparentprev>If you tried agile and it failed, it wasn't actually agile. When you show how it was actually agile, you didn't do it right. When you show you did it right - go back to argument 1. It's a really circular no true scotsman style of argument. It'd be interested in seeing that argument in the wild. Mostly I see complaints like yours when people point out that an hour-long daily that the manager uses to harangue the developers isn't actually agile, and that is perfectly true. Or that a blame-assigning retro or measuring dev productivity using story points is not actually agile, which is at least true insofar that it clearly is forbidden in Scrum, and stupid in any case. Though I find it interesting that you end with kanban -- an agile methodology. It seems you do like to use agile after all, just not scrum. reply JohnMakin 3 hours agorootparentI agree, and I guess it should be clarified I'm mostly criticizing the \"traditional\" implementation of agile as I have seen it which is scrum with sprints, retrospectives, etc. I find it extraordinarily counterproductive, but obviously some have had success with it or it wouldn't be so widespread. reply lloydatkinson 3 hours agoparentprevI agree with you completely and what you said brings to mind something I wrote on my blog a few days ago: > I posit that if I showed the Agile Manifesto bullet points (without the title) to a group of people with cringe-worthy titles like Scrum Master, Agile Delivery Manager, Agile Coach, or Head of Agile Delivery, and asked them “Is this agile?” almost all of them would say it isn’t. I can only imagine the effect that revealing the title and explaining that, actually, this really is the agile, would have on their Jira-addled brains. reply brazzy 3 hours agoparentprev> The way I like to manage projects these days is a kind of kanban system with a task queue, and 1-2x a week brief cadence meetings to discuss priority/alignment and any blockers. Sounds pretty agile to me. While the \"defenders\" of agile might be employing No True Scotsman fallacies, it's detractors (which I personally seem quite a bit more numerous on HN) are often doing the same in reverse: refusing to define what \"agile\" actually means, and just throwing together a bunch of anecdotes and feelings about excessive meetings and estimates being misused to measure productivity. reply JohnMakin 3 hours agorootparent> refusing to define what \"agile\" actually means, and just throwing together a bunch of anecdotes and feelings about excessive meetings and estiybeing misused to measure productivity. I mean - this seems necessary when the agile manifesto is extremely vague and practitioners can't seem to agree on the One True Way to do agile - and in the absence of formal studies and analysis, what else is there but anecdotes? reply gortok 1 hour agoprevThe study isn't shared, so we can't see the methodology to determine its accuracy. We see the results of the survey; but that's not a study, and the survey itself isn't shared with us. I don't dispute that agile has its issues -- and I'd be surprised if 'agile' failure rates weren't an issue. We can empirically see the result of 25 years of 'agile', and it's not somewhere we should aspire to be. The issues comes down to this: 1. Is adopting an 'agile' methodology sufficient to ensure (insert goal here)? 2. Has the software industry adopted a method of operating that creates a statistical quality control; and adopted practices that investigate and remediate special causes and common causes of variation in quality? To spoil it: for #1 the answer is 'no', and for #2 the answer is a resounding 'no'. There are practices that are ingredients to the world described in #2, but we as an industry have not yet adopted a systematic way of thinking and resolving problems in the software development process. I think we'd be better off adopting Deming's method of management and his System of Profound Knowledge and be far better off than adopting the agile fad of the week; even as I admit I still have open questions on how to apply his work to Software. (See \"The New Economics\" 3rd Edition, and \"Out of the Crisis\", both written by Deming). reply giancarlostoro 4 hours agoprevMost times I've been in a doomed to fail agile project, it was almost always a manager or higher who overpromised with deadlines, not accounting for real world blockers. One of said projects has insanely awful app store ratings for their mobile apps. reply wild_egg 4 hours agoprevArticle source seems to be mainly an ad for some agile alternative which makes the whole thing suspect. Most of the \"results\" can boil down to: scope creep kills projects. That's entirely independent of whatever methodology your team follows and they don't make it clear how their \"Impact Engineering\" concept would address it reply renegade-otter 3 hours agoprevAgile in itself is not a development methodology - it's a communication one. A small team working on a stealth R&D project, where everyone is same rank and in the same boat, would be suicidal to use Agile. This is what Kanban is for. Agile is an overhead. It doesn't make things faster, or more efficient, quite the opposite. It's there to avoid arbitrary deadlines coming from the management and developers not working on an island, with no visibility. That's a recipe for some nasty surprises. \"But we thought you were working on THAT and it would be ready NOW!\" reply moi2388 4 hours agoprevI’m sure that agile done right is great. I’m also fairly sure most organisations don’t do Agile right. At least within my organisation, we do not design anything up front. We’re agile. We don’t think about proper api modelling. We’re agile. We also do barely any testing. We’re agile. We do rewrite the UI a dozen times based on user feedback. After all, we’re agile. reply thaanpaa 3 hours agoparent> At least within my organisation, we do not design anything up front. We’re agile Agile doesn't mean forgo design completely. > We don’t think about proper api modelling. We’re agile. See previous point. > We also do barely any testing. We’re agile. It's difficult, if not impossible, to be agile without testing. If you want to move fast, you want to be confident that your latest change didn't break anything. > We do rewrite the UI a dozen times based on user feedback. After all, we’re agile. Sounds like you built a complete UI before any users could give you feedback. That's the opposite of agile. reply trickstra 4 hours agoparentprevCan you point to any of the 12 principles which says that we should do \"barely any testing\"? I can point to some that say \"deliver working software\" and \"it's the only valid measure of progress\". If testing helps you deliver working software, then do testing, that's agile. reply Attrecomet 3 hours agorootparentI think you missed the invisible /s tags to those sentences. reply trickstra 3 hours agorootparentTrue. Unfortunately I've seen similar arguments discussed unironically before, so I just didn't catch it here. reply brazzy 3 hours agorootparentprevSarcasm does always, without exception, sabotage a factual discussion into confusion about what people are actually saying (because that remains unclear even when you fully recognize the sarcasm). reply btutt 3 hours agoprevI don't agree that on time and on budget are the correct measures for determining overall project success or failure. By these standards a multi-year project delivered a single day after the original estimate, or a massively profitable project that went a dollar over the original budget are counted as a failed project. Ideally some actual business outcomes need to be included before determining if a project succeeded or failed. I think this problem persists for a lot of research and discourse about software project success and failure where we conflate whether we estimated accurately with whether or not the project succeeded or met the needs of the business. reply TheCoelacanth 1 hour agoparentAlso, a project that was cancelled after two weeks because it was determined that it wouldn't be valuable enough is a failure while a project that spent two years building something and then didn't make a single dollar is a success as long as it was budgeted for two years. Sometimes a quick failure is the best outcome. reply macNchz 4 hours agoprevIt seems like this is in service of promoting some new competing methodology. I also can’t find any details on the research methods, but it seems like it may be relying on software engineers’ judgment of the success/failure of projects they’ve been involved with, which…I think is pretty questionable. The reality is that there are a lot of software engineers who might describe a project that was hugely profitable as a failure because it has a shitty database schema and lots of dead code because of a last minute change. reply cbsmith 3 hours agoparentIt looks like, over a span of four days, they had 600 engineers (heavily biased towards UK engineers) answer survey questions. The vast majority of the higher failure rates were for the whether \"Development starts before clear requirements, no complete specification, significant changes late in development.\"... which is placed under \"Agile Requirements\". Of course, these are things that agile recognizes as factors that create risk, and the methodology is about trying to mitigate said risk as best as possible. So, it's not an indicator of methodology but of the context. Yes, if such risk never presents itself, you are much less likely to fail, regardless of methodology. https://www.engprax.com/post/268-higher-failure-rates-for-ag... reply msurekci 4 hours agoprev\"One standout statistic was that projects with clear requirements documented before development started were 97 percent more likely to succeed\" The issue with a majority of projects are the requirements aren't always clear from the start or even if they are it tends to deviate while the project is in progress. Agile tries to mitigate that by allowing teams to be able to react more easily to changing requirements. That being said not many companies understand or implement agile correctly, unfortunately. reply ChrisMarshallNY 3 hours agoprevI've always liked the Agile Manifesto, but it's fairly vague, and leaves a lot of details to be provided at implementation time. I suspect that a lot of Agile is actually \"agile™.\" A branding facade, over an unstructured, ad hoc development system. Not even Waterfall, which is actually a very robust system (just robustly inflexible, which often gives bad results). I like the idea of evolutionary design, and adapting to change, but I have found that it needs to be done carefully, and that having experienced developers; concentrating on results, is a must. As always, I think the proper answer is \"It depends.\" The search for The One True Methodology is one that will never be satisfied. Even different projects, under a single organization, probably need to take different approaches. I just believe that we always need to keep our eye on the end result, and all development needs to be done, with that in mind. I think that (for me), Agile is accepting that we don't actually know what \"done\" looks like, when we start, but we have to have a heuristic to help us to understand when we're done. reply spamizbad 3 hours agoprev> One standout statistic was that projects with clear requirements documented before development started were 97 percent more likely to succeed. Having worked with both this approach and agile, I'm not surprised by these results. However, I will also say that writing clear requirements is easier said than done - when those requirements are vague or of even mediocre quality it can very easily send a project off-course immediately. Part of this stems from, in my view, a lack of accountability when it comes to waterfall approaches. If the BAs responsible for drawing up the requirements burn through time and budget that ultimately gets felt by the development team. It requires Engineering Managers to carefully vet those requirements to ensure they're appropriate start work, which results in a tremendous amount of back-and-forth. reply brightball 3 hours agoparentYep. And my favorite is when developers spend far too much time discussing the story point estimate, having long discussions about why something is higher or lower...and then not writing any of it down other than \"3\" or \"5\". reply yobbo 3 hours agoprevThe manifesto describes an effect or desirable end state. Not a method. Anything you do that fails to bring you this effect is your responsibility. Coaches are only agile insofar as they are helpful. But this also mean you are free to learn from all possible sources, and you are not bound to any particular method or ritual. reply wg0 4 hours agoprevBecause agile has itself been made so heavy weight that it is no more agile. You have product managers and then you have scrum masters and then there are meetings about whether how pure and religious we were about our agile practices, how pure we were, would gods of agile be pleased with every single second that we spent in past few weeks or our hearts had some impurities deep down somewhere? And no kidding, there are then meetings about those meetings and their outcomes and action items and their evaluation. The obsession with the process itself is a sight to behold. It is now a whole cottage industry, major technology journal and websites have whole sections devoted to just Agile. Consultants, Coachs, Gurus and what not. reply riffic 4 hours agoprevmaybe it's for the best? fail fast and move on with what you've learned. 268% higher failure rates over waterfall is a feature not a bug. reply boxed 4 hours agoparentYea, failing fast and cheaply can be enormously valuable. In fact, of the projects that \"didn't fail\" one can wonder how many of them are still in a death march. reply ChicagoDave 3 hours agoprevEven with agile, modeling the business and documenting requirements is still important. The difference is that 30 years ago we’d spend 6 months defining requirements in functional and detailed requirements and the larger the problem, the more fragile those documents became. And proper story development is hard. Most “agile” implementations don’t bother to get stories “completed”. It’s an afterthought. Agile works very well if you actually do it well. reply FeistySkink 3 hours agoparentAm I misreading this as a No true Scotsman? reply p0nce 4 hours agoprevAgility is implemented as a way to increase productivity. It does it with more detailed scrutiny of what workers do. Management understood very well that it's taylorism applied to our profession. Taylorism started with measuring every movement of the workers and have their expertise removed from themselves. Daily stand-ups and points kind-of do that, even if there is no magical 4x productivity gain to ever be found, unlike was what won in manufacturing (cf. Braverman). It's a game played against workers, if they self-organized there would be no Agile. reply jerf 3 hours agoprevDon't confuse \"the Agile manifesto\" with \"Agile\". I say this neither in attack of, nor in defense of, either one. Make your own decision. But those two things are now diametrically opposed to each other, so this is the sort of word carelessness that will destroy all ability to converse or think about the characteristics of each. reply jmward01 3 hours agoprevI have only seen three principles work consistently in software engineering: tight iterations, divide and conquer and fixed max sizes. All of these principles work because development is a NP problem and the best we can hope for is a local optimum. In general you see best practice anti-patterns come up that break one or more of these things and math takes over. Take a product backlog for example. In general these backlogs become the one place that everything is put but then you break divide and conquer and fixed max sizes. Since scheduling is an NP problem, if I have an unbounded backlog I have just created an NP problem to solve. the correct way to do things is to admit that you will not find the optimum solution, so the best you can do is break the problem into pieces, assign teams and minimize communication between the teams so that the pieces don't get combined into one again. The teams then need to do the same if the individual pieces are still too big. Massive monolith teams break all the principles and are basically guaranteed to fail. Or they are guaranteed to fail until someone proves that P = NP. reply VikingCoder 4 hours agoprevAgile is great when you want to see if you can make something work. Agile is, in my opinion, terrible when you want to ensure the thing never fails. reply lioeters 4 hours agoparent\"Ensuring the thing never fails\" is a question of having plenty of tests, which can be practiced as part of an Agile methodology. In fact, it sounds more agile to write tests while (or before) you develop a feature. reply lushdogg 4 hours agoprevWhat about \"achieving failure\" that is the outcome of many non-agile projects? reply foobarkey 4 hours agoprevNot a huge fan of scrum ceremonies (besides daily standup) but as long as people dont know what they want before they see it, agile is the best way. The waterfall things we already tried, does not work too great reply mindcrime 4 hours agoprevsigh. It's nearly impossible to have a meaningful conversation about anything to do with \"agile\" these days. And that's because there's approximately zero agreement about what \"agile\" even is, and all of the various terms are overloaded with 100 different meanings, and every \"agile\" implementation is different from all of the others. I'll just note that, in my experience, a few things are true: 1. The Agile Manifesto, in and of itself, is great. 2. There is no such thing as \"agile development\", as a methodology in its own right. 3. There are many \"agile\" methodologies, including Scrum, SAFe, Disciplined Agile Delivery, Agile Unified Process, Crystal, XP, etc. And then on top of that, every company in the world has implemented their own poorly specified, half-assed, bug-riddled, barely comprehensible implementation of \"agile\". 4. Approximately everybody misunderstood / misunderstands (perhaps intentionally at times) the Agile Manifesto and bends it to suit their own biases. A classic example is illustrated in this article: One standout statistic was that projects with clear requirements documented before development started were 97 percent more likely to succeed. In comparison, one of the four pillars of the Agile Manifesto is \"Working Software over Comprehensive Documentation.\" NOWHERE in the Agile Manifesto will you find any instructions saying \"Don't do requirements engineering\" OR \"Don't identify requirements up front\". I promise. If you don't believe me, go read it right now and see if you can find those instructions. What you will find instead, is the quoted admonition to NOT put documentation (possibly including requirements) OVER actually delivering software. But the Manifesto itself clearly says \"there is value in the items on the right\". Where \"The items on the right\" include: - processes and tools - comprehensive documentation - contract negotiation - following a plan Unfortunately we, as an industry, collectively managed to myopically focus on items on the left and the \"we value the items on the left more\" phrasing and threw the baby out with the bathwater. \"We are doing Agile\" became the excuse to abdicate with regards to the necessity of doing requirements engineering, architecture/design work, documentation, etc. Basically, we swung too far in one direction, and need to move back towards the center. No, I'm not calling for a return to waterfall or anything. I'm saying that we can embrace the principles of the Agile Manifesto and STILL DO requirements engineering, architecture, and all of those other things. A good starting place would be to stop talking about \"agile\" like it's a discrete methodology (as opposed to a family of loosely related methodologies) and - as organizations - pick an actual methodology to implement and then actually embrace the \"empiricism\" attribute that Scrum emphasizes (whether or not you're using Scrum per-se)... measure things empirically and actually make adjustments based on THE REAL WORLD not somebody's whack ass theory. Reality trumps everything and I desperately wish we could get everybody to acknowledge this. reply DHPersonal 2 hours agoparentWhat I appreciate about Scrum and other methodologies is that it can be used as a starting point from which customization and adaptation can grow, rather than starting from practically no documentation or process and building a monster of a system that ends up mostly fitting the existing broken system. reply mindcrime 14 minutes agorootparentYes, exactly. That's why I advocate starting with some known process, even if you modify it. Starting from scratch, IME, tends to lead to just creating a non-coherent mess. reply ACV001 3 hours agoprevIt's not a study, but a piece of advertisement. That's all you need to know about this. reply hugocbp 3 hours agoprevThis looks like more of an \"ad\" (or a very directed study by a competing methodology), but excess pragmatism can ruin even the most sensible ideas. Agile, testing, design patterns, best practices can all tank and bury a project if applied excessively \"by the books\" without consideration of the actual problem to solve. I've worked in teams that had about 10 people actual doing dev work that implemented the full suite of Agile \"principles\" as rules. Daily standups, grooming, retros, pointing as \"poker\", 1:1s every week. The result was that we had barely time to actual work since the week had 10-20 hours of meetings. Most retros and standups were literally just us saying \"same as yesterday, only had a few minutes to work on this\" the whole week. Testing is the same. If applied without consideration for the actual problems, reaching that 90%+ code coverage is easy if nobody cares about how hard and time consuming it will be to change code later. Specially when a feature is in very early development. I think all those things are good, but what I see sometimes is that they are applied as absolute rules that cannot be deviated from, which inevitably leads to poor results. I'm now working in a \"light Agile\" environment with just 2-3 meetings a week, barely 1 hour total, and much less strict PR/testing requirements (we focus on testing the important functionality, not line coverage count) and it is so much better. Some of the same co-workers that were under the more strict rules are now twice or more more productive then before. reply kristopolous 4 hours agoprevThis is such a strange phrasing. The problem with \"agile\" is when it becomes manager-heavy and meeting-heavy. The last firm I was at was ridiculous. One project had about 30 hours of meetings a week. There's no time to get any work done. It's just lopsided manager bullshit. That's the actual criticism, not some strange dichotomy of things being successful if only it was meeting, manager and also authoritarian focused with some specs decreed by some deputized opinion makers with holier than thou thoughts they wrote down two years ago. reply nsxwolf 4 hours agoprevAgile vs. what? Who out there isn't claiming to be doing Agile these days? reply whimsicalism 4 hours agoparentpretty much all the big tech cos. since i jumped to top tier companies i don't think i've even heard the word agile reply peeters 4 hours agoprevI can't find any link to the actual study or even a description of its methodology. The research appears to have been done for the book Impact Engineering, which introduces the new methodology, and yet the research also claims to have measured projects following that methodology. From the marketing summary here [1], it seems like what they actually did was to isolate which individual virtues of various methodologies had an impact on project success (which also goes without a definition of course). Then they choose which of those individual virtues would be promoted by their new methodology, and use projects adhering to those virtues as a proxy for scoring their new methodology. And then claim the result as statistically significant. It would be interesting to see if this was the actual methodology, because if so, that's clearly nonsensical. Also, it would be highly relevant to know the definition of success here. Evaluating an Agile project's success as \"completition of the project's initial requirements on time\" would be completely asinine, given the entire point of Agile is to adapt to changing requirements. [1]: https://www.engprax.com/post/268-higher-failure-rates-for-ag... reply paulsutter 4 hours agoprevClear requirements are needed regardless of methodology. The simpler (briefer) the requirements, the more likely to be understood and followed. Iterative development processes can leverage more concise (and thus clearer) requirements > One standout statistic was that projects with clear requirements documented before development started were 97 percent more likely to succeed. Also, if a project doesnt have requirements, how can it either fail or succeed? EDIT: agile is one type of iterative, and iterative exists in contrast to waterfall. go ahead flame away :) reply AlotOfReading 4 hours agoparentI can count on one hand the number of \"agile\" projects I've seen that were actually iterative, regardless of what the manifesto says. It would be a mistake to confuse what this article is talking about (projects that profess to be agile) with anything based on small, self-organizing teams. reply boxed 4 hours agoparentprevClear requirements without an artifact to test against is impossible. That is my main focus when thinking of agile. I think Shape Up is one of the most reasonable modern interpretations of the agile manifesto. reply 000ooo000 4 hours agoprev>Even though the research commissioned by consultancy Engprax could be seen as a thinly veiled plug for Impact Engineering methodology, Aaaaaand close tab reply EVa5I7bHFq9mnYK 2 hours agoprevWhat requirements? Agile is a way for managers to daily and publicly whip devs falling behind their sprint estimates. reply simonw 4 hours agoprevThis very obvious PR stunt appears to be based entirely on redefining \"working software over comprehensive documentation\" to mean \"don't gather requirements and don't write a spec\". Here's the report itself which makes it very clear that the survey was commissioned deliberately to help promote a book about an alternative development process: https://www.engprax.com/post/268-higher-failure-rates-for-ag... reply smugglerFlynn 3 hours agoprevIsn't it the whole point of failing fast? It seems the industry rebooted back into \"any project must be a success\" mindset, which we all have been walking away from for the past couple of decades. Truth is, there are shitty goals, wrecked scoping, high-risk biz hypotheses, managers cheap skating on resource and talent side, and many, many other things that deserve early failure. All these do fail much faster today, provide valuable early feedback and lead to changes. reply stilwelldotdev 4 hours agoprevAll I know is that Agile means a lot more meetings and a lot more meetings means I'm a lot less productive. reply commandlinefan 3 hours agoparentAnd it was supposed to mean the exact opposite. reply tsunamifury 4 hours agoprevI’ll never understand the hate for agile on this site. The process is at its core, enumerating tasks to get to a. Goal, prioritizing them, and letting those who do them do them at their own pace and order across a larger team. I’ve run teams that way for decades and never had a complaint. reply Attrecomet 3 hours agoparentI think the hate stems from so many people working in environments where either the company has bureaucratized their nominally agile process to the point of absurdity or just having superiors that are in over their head, which makes any kind of methodology a farce; bad leadership can only be mitigated so far by processes, especially of someone has to take responsibility for said processes and that often by default is the mentioned in-over-their-head superior. reply kelseyfrog 2 hours agoprevWell, yeah. It's impossible to do Agile right. Why would a practice that's impossible to execute correctly have anything other than a higher failure rate? It's Agile's fault and it shows. reply brazzy 3 hours agoprev> One standout statistic was that projects with clear requirements documented before development started were 97 percent more likely to succeed. In comparison, one of the four pillars of the Agile Manifesto is \"Working Software over Comprehensive Documentation.\" Someone does not understand (or is intentionally misreading) the meaning of \"Comprehensive Documentation\". The Agile Manifesto is not discouraging having requirements before you start implementing. It's against having a multi-month planning phase that produces hundreds of pages of detailed specs before you start implementing. Agile development was a reaction to the status quo where that was done - and a staggeringly large percentage of projects failed. It was called the \"Software Crisis\": https://en.wikipedia.org/wiki/Software_crisis reply readthenotes1 3 hours agoprev\"projects with clear requirements documented before development started were 97 percent more likely to succeed.\" I believe Barry boehm is on record stating that the biggest mistake he made in designing a software creation process was believing people who told him that you could get firm requirements up front. So yes, if you're in that rare case where you can start out with clear and non-contradictory requirements, probably anything you do is going to succeed better than when you don't. There's a good book on risk management that goes into this more, Waltzing With Bears, by Demarco and Lister reply dangerwill 4 hours agoprevIt's funny, the root of the idea is that Agile allows teams to change how they are doing things midstream so we can constantly improve. But I have never met a scrum master or PM who has both the authority and the desire to advocate for changing how work is done. So all it ends up being in practice is a series of rituals that people sleepwalk through. reply commandlinefan 3 hours agoparentDilbert captured it best: \"Our boss can't judge the quality of our work, but he knows when it's late\". For everybody doing programming work, there are ten people tasked with making sure it's done by the \"due date\", whether there's any value in that due date or not. reply AnimalMuppet 4 hours agoparentprevIf you're not hacking the process, you're not agile. \"We are agile according to this rigidly defined process\" is an oxymoron. reply zzzeek 3 hours agoprevwho really does \"Agile\" or \"waterfall\" or anything ? I'm in this job for 30 years. I've never seen any formal methods my whole career. it's like here's the thing we need or what the customer needs, gather some vague requirements, write some prototypes, do internal demos, now more requirements are there, more clarity, iterate on the prototypes, etc., repeat. use issue trackers, use \"stories\", use kanban boards sure, but are we adhering to some rigid notion of \"agile\"? no way. I can't imagine the \"waterfall\" method actually working either. reply JohnDeHope 4 hours agoprevI'm sorry I didn't RTFA, but I wonder if there's a correlation / causation issue here. Did the \"agile\" projects fail because they were agile, or because their project management methodology was prescribed by management than is higher up than appropriate. Maybe they happened to choose agile because it's the flavor de jure at the moment, but it wasn't the \"agile\" that was the problem exactly, as much as the undue influence of the wrong layers of leadership. reply mbesto 4 hours agoprev [–] \"Agile\" and \"failure rates\" don't have strict, universally accepted definitions, so any study (even if definitions are described) is essentially meaningless. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "A study by consultancy Engprax found that Agile software projects are 268% more likely to fail compared to non-Agile projects.",
      "The research, involving 600 software engineers from the UK and US, highlighted that projects with clear, documented requirements before development are 97% more likely to succeed.",
      "The study suggests that robust requirements engineering and psychological safety for problem-solving are crucial for project success, advocating for a balanced approach between Agile and traditional methodologies like Waterfall."
    ],
    "commentSummary": [
      "A study claiming Agile software projects have a 268% higher failure rate is criticized for bias, lack of data transparency, and flawed definitions.",
      "Critics argue that Agile is often mischaracterized and misapplied, leading to issues like technical debt, expert burnout, and project failures, emphasizing the importance of proper implementation, flexibility, and adaptation.",
      "The debate contrasts Agile with Waterfall methodologies, noting Agile's suitability for dynamic environments and iterative development, while Waterfall is better for projects with fixed requirements, highlighting the need for balancing Agile principles with practical requirements engineering and design."
    ],
    "points": 174,
    "commentCount": 159,
    "retryCount": 0,
    "time": 1717596111
  }
]
