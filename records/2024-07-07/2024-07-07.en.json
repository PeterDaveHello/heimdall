[
  {
    "id": 40897205,
    "title": "YouTube embeds are bananas heavy and it’s fixable",
    "originLink": "https://frontendmasters.com/blog/youtube-embeds-are-bananas-heavy-and-its-fixable/",
    "originBody": "YouTube Embeds are Bananas Heavy and it’s Fixable July 1, 2024 TL;DR: YouTube Embeds are like 1.3MB in size with no shared resources between multiple embeds. Using aWeb Component is more like 100k, does share resources, and sacrifices no functionality. You can put a YouTube video on any website. They help you do it. Under the Share menu right on youtube.com there is an option toEmbed and you’ll see bit of HTML with anin it. s are never wonderful for performance, but they make sense for protected third-party content. This is what I’m getting as I write:Code language: HTML, XML (xml) If I were Team YouTube, I’d get loading=\"lazy\" on there to help with performance right away. No need for videos that aren’t even visible on the page to load right away. Code language: HTML, XML (xml) Plus I’d put some inline styles on there to keep the video fluid and maintain the original aspect ratio. Or you could target these and do that yourself in CSS. Here’s assuming the videos are the standard 16 / 9 aspect ratio: iframe[src^=\"https://www.youtube.com/embed/\"] { inline-size: 100%; block-size: auto; aspect-ratio: 16 / 9; }Code language: CSS (css) But… let’s not keep this HTML at all. I’m sure you read this blog post title, but let’s put a point on it: On a page with literally nothing at all on it other than a YouTube Embed, we’re looking at: 32 requests 1.3 MB of data transfer 2.76s to load the page on my current WiFi connection Zach Leatherman, equally exasperated by this, noted: The weight also grows linearly with every embed—resources are not shared: two embeds weigh 2.4 MB; three embeds weigh 3.6 MB (you get the idea). Wow. Looks like sizes are up a bit since Zach last looked as well. The Appearance & Functionality This is what you get from a YouTube Embed: You see a “poster” image of the video You see the title of the video You see a big play button — click it to play the video This is very little UI and functionality, which is fine! We can absolutely do all this without this many resources. Why is it this way? 🤷♀ I don’t think we have any good answers here. In fact, I heard from a little birdie who ran it up the pole that they have tested lighter embeds and found them to reduce engagement. 😭 I’m just gonna straight up say I don’t believe it. It’s like when Google told us that taking up half the screen with AI generated answers led to people clicking on third-party results more, but then refused to show data or allow us to track those clicks ourselves. And hey — sometimes there are unexpected results in testing. That’s why we test instead of guess. But because this is so counterintuitive and offtrack for so many other similar performance testing situations, this bears deeper scrutiny. It would benefit from an opening of the methodology and data. Like if you tell me that if you hit people with a stick and they smile more, I’m gonna want you to stop until we can look at what’s going on there. I really wish I could find a good link for this, but there is a famous story from YouTube engineers way-back-when who made a much lighter video page and put it into testing. They found, quite counterintuitively, that average page load times went up. But with a deeper look, they found that the lighter page was able to reach more people, including people on low-power low-internet-speed devices who were able to actually use YouTube for the first time, and them using it much more slowed those averages. That’s awesome! The speed of using the site was up relatively for everyone. The metric of the average page load speed was a red herring and ultimately not meaningful. How do we know that’s not the same kind of thing happening here? Remember the implications of all these resources isn’t just a little inconvenience. YouTube is so enormous we’re talking incredible amounts of wasted electricity and thus carbon output. Pulling a megabyte of data off every single YouTube Embed would be an incredible win all around. I might even say not improving this is environmentally negligent. The Solution is to Replicate the Embed Experience Another Way. There are Open Source Web Components That Do It Well. With a little dab of irony, Google’s own performance champion Paul Irish has had a web component doing just this for years and years and years: lite-youtube-embed The pitch is solid: Provide videos with a supercharged focus on visual performance. This custom element renders just like the real thing but approximately 224× faster. Two hundred and twenty four times faster. Which of course involves much less data transfer. And I’d like to be very clear, also does the exact same thing as the default embed: You see a “poster” image of the video You see the title of the video You see a big play button — click it to play the video You lose nothing and gain tons of speed, efficiency, and default privacy. Using Lite YouTube Embed Link up the JavaScript to instantiate the Web Component Use it You could install it from npm or copy and paste a copy into your own project or whatever. Or link it from a CDN: import \"https://esm.sh/lite-youtube-embed\";Code language: JavaScript (javascript) That’s like this: But the best way to use it is right in the README: Use this as your HTML, load the script asynchronously, and let the JS progressively enhance it. Play Video: Keynote (Google I/O '18)Code language: HTML, XML (xml) With async loaded JavaScript, note the background-image is put into the HTML so it can all look right before the JavaScript loads. Alternatives Shadow DOM version (more protected styling, more annoying to style) Do it yourself Raymond Camden: Building a YouTube Embed Web Component (both vanilla and WebC flavored) Adrian Roselli: YouTube and Vimeo Web Component Mux:(matchesDOM APIs) React Port & Next.js Official Version",
    "commentLink": "https://news.ycombinator.com/item?id=40897205",
    "commentBody": "YouTube embeds are bananas heavy and it’s fixable (frontendmasters.com)236 points by surprisetalk 6 hours agohidepastfavorite135 comments charrondev 5 hours agoI work on a community forum platform and we detect YouTube embeds and replace them proxied thumbnails that don’t load until clicked. Just because one person in a thread shares a YouTube video doesn’t mean everyone else loading that page should have to download 1MB+ of YouTube’s JavaScript bloat and have their IPs tracked by google. reply btouellette 2 hours agoparentI've got a site that is basically infinite scroll of mostly YouTube, SoundCloud, and Reddit embeds and had to do this for YouTube for it to even be functional. Using the YouTube provided thumbnails though since I'm not too concerned about tracking. reply pier25 3 hours agoparentprevIs there a similar solution for SoundCloud? Their player is also super bloated. reply diggan 3 hours agorootparentYou can basically do this with any embeds (granted they don't do a lot of global fuckery, which is a lot of them). Make sure the embed code gets evaluated within a function, then call that function when the user clicks on your \"proxy\" image. You might have to do a replace of the DOM elements as well, so the embed code gets what its expecting. reply paulirish 1 hour agorootparentprevThere are a few \"facade\" solutions listed at https://developer.chrome.com/docs/lighthouse/performance/thi... But.. AFAIK nobody has made one for Soundcloud. reply mindhunter 5 hours agoparentprevDo you also cache or proxy the thumbnail? Google can also track them when hotlinking it. reply tczMUFlmoNk 3 hours agorootparentGP says \"proxied thumbnails\", so it sounds like yes. reply mmmmmbop 5 hours agoprevThe author says they don't believe that a lighter version has been shown to reduce engagement. I, on the other hand, fully believe that. The recommended lite-youtube-embed project page has a demo of both lite and regular players [0], and the lite version takes noticeably longer to start playing the video. Every additional millisecond of load time will reduce engagement, and here the difference is more on the order of hundreds of milliseconds or more. [0] https://paulirish.github.io/lite-youtube-embed/ reply skybrian 4 hours agoparentI suspect you’re right, but on the other hand, I think it’s useful to think critically about whether starting the video faster is worth it if it makes the web page that it’s embedded in load slower. The “every millisecond counts” argument applies to the web page too. If the user bounces off the web page, they won’t get to the video anyway. Also, maybe it’s fine if people don’t want to play the video? Personally, I appreciate it when a web page includes a summary, so that I can avoid watching a video. (I prefer not using YouTube for anything other than listening to music or occasionally watching a movie.) Video can be a useful tool, but consider whose interest it’s in for you to encourage your audience to watch more TV. Is it really serving your users? Even when I do want to watch a video, it’s selective. One thing I find rather frustrating about YouTube’s redesign (on desktop) is that it devotes so much screen real estate to promoting videos other than the one you’re actually there to watch. I’d prefer fewer distractions. reply Stratoscope 4 hours agorootparent> One thing I find rather frustrating about YouTube’s redesign (on desktop) is that it devotes so much screen real estate to promoting videos other than the one you’re actually there to watch. I’d prefer fewer distractions. The F key is your friend. It puts the video full screen. You don't even have to find the full screen icon at the bottom right of the video, just hit F. reply brookst 1 hour agorootparentBut I don’t want full screen, I want it to take up the full window that I have allocated to the browser, while still allowing me to multitask in other windows. reply nulbyte 50 minutes agorootparentI love Firefox's picture-in-picture mode for this exact reason. I think there is an extension for Chrome also. reply roelschroeven 1 hour agorootparentprev> One thing I find rather frustrating about YouTube’s redesign (on desktop) is that it devotes so much screen real estate to promoting videos other than the one you’re actually there to watch. I’d prefer fewer distractions. Theater mode (shortcut 't') is a bit better. But yes, I too would like a mode where the video fills the whole browser window. reply nnf 5 hours agoparentprevI would much rather wait a few hundred milliseconds for a video to start during the few times I decide to watch an embedded video than to wait for the full video player to load every single time I visit a page with an embedded video that I'm never going to watch. Similarly, I would much rather have every stoplight I approach be green for me rather than having every light be red but for not very long. reply vasco 4 hours agorootparentThese things are not optimized for what we prefer but for what leads us to behave in a way that maximizes a particular metric, for youtube it's global watch time. reply moqmar 19 minutes agorootparentAlso, ads. I guess the shorter the time is between clicking the play button and the ad starting, the more people will have \"seen\" more of the ad before deciding that the video isn't worth watching the ad. reply Retric 3 hours agorootparentprevGlobal watch time isn’t something non Google website owners care about. Remember, Google benefits from making the web worse. reply FredPret 2 hours agorootparentDo they? They have the biggest browser, an online office suite, a cloud, and a giant ad network. They want to maximize internet use, or they should want to. reply kibwen 1 hour agorootparentUnfortunately no, Google wants to maximize their profit, and they'll enshittify the web in that pursuit until it collapses. They have Android, they don't need the web to exist. reply klyrs 1 hour agorootparent> They have Android, they don't need the web to exist. If not for apple, I'd agree. reply kylebenzle 1 hour agorootparentprevDid you even try the example? Obviously not. It's closer to 4 seconds difference, PLENTY of time for me and a lot of people to click away. It doesn't help a discussion to ignore the topic at hand, create a straw man just to easily vanquish him. Who are you even talking to here, just yourself? reply tjoff 4 hours agoparentprev>Every additional millisecond of load time will reduce engagement This is something people believed in the 90s. None of the megacorps give a damn about that as is evident by their behavior. If it doesn't matter for them it doesn't make sense for you to optimize that on their behalf. It is a non-issue for this usecase. But please do care about it for the rest of your stack. reply qwery 3 hours agorootparentIn the 90s, there was no \"engagement\" and \"content\" just meant the content of the thing you were talking about, but I digress... > None of the megacorps give a damn about that as is evident by their behavior. The rumour (and extrapolation) the discussion is based on is that Youtube prefers their bloated player to an unknown alternative because it makes the videos play faster, which drives \"engagement\". That is, in this case, the \"megacorp\" literally does care about that. > it doesn't make sense for you to optimize that on their behalf This is certainly true, but I don't think that's what the parent comment was suggesting. reply froggit 1 hour agorootparent> In the 90s, there was no \"engagement\" and \"content\" just meant the content of the thing you were talking about, but I digress... Sometimes i think back at this idea from the 80s when i need some perspective: \"Productivity soars when a computer and its users interact at a pace ( I remember when the old youtube player would just load and buffer the entire video, making replay ability very easy since you didnt need to redownload it. Somehow we regressed. When did that change? I already considered youtube's UX to be so hostile I'll go for (literal, not metaphorical) years between intentionally watching 2 videos off there. It's also possible i just didn't notice as the data transfer from there is impressively fast via google fiber (likely not coincidental). > Google takes everything that works PERFECTLY FINE and turns it into a steaming pile of … I am gonna stop right there. Google's SDLC in 4 steps: 1) \"Acquire\" software idea (invent/buy/steal/kill/etc). 2) Dev to critical mass (unlimited money cheat). 3) Enshittify (Ads team trounces Dev team because capitalism). 4) Sunset before mob descends with fire and pitchforks. reply stemlord 2 hours agoparentprevI thought we were past the era of counting in milliseconds for load time now that half the web insists on using cloudflare security checks reply qwery 4 hours agoparentprevI'm not seeing such a difference, but it is there. I'd be surprised if it was as high as 100ms. Obviously different computer environments[0] will have an impact here. I would be much less likely to notice it as \"slow\" if it didn't show me a spinny-spin. It's advertising that it's slow! I agree that the click-to-playback lag time would have such an effect, but how significant it would be is unclear. It would take an entity the size of, say, Youtube, to begin to measure this sufficiently. [0] Firefox, 2(?) year old laptop, i7-1185G7, windows 11, updating Edge (in 32-bit mode) 24/7, haven't rebooted for a few weeks reply divbzero 3 hours agorootparentSame here: While the difference in speed is noticeable, I would be surprised if it’s much more than 100ms on this specific machine (Safari, 1 year old laptop, Apple M2, macOS Sonoma). reply cogman10 3 hours agoparentprev> Every additional millisecond of load time will reduce engagement, I do not believe this. Humans can't tell the difference between 1 and 10ms. I'd love to see the study that actually proves this assertion. I suspect, it's never been done for embedded videos just webpage load time. But further, we are talking about embedded videos that you have to click to start anyways. Presumably, the person clicking the video has a desire to watch it and thus can stand that the video takes an extra 300ms to load. reply lelandfe 5 hours agoparentprevIt also seems like it takes two clicks to start a video? Is that a bug? reply _wire_ 3 hours agoparentprev> Every additional millisecond of load time will reduce engagement LOL! What's engagement?! Half the embeds I see don't work because the content is censored or rotted. For content that plays, the rush for my attention includes an overwhelming dynamic of at least three parties with vested commercial interested in the occupation of my mind cramming unrequested and unwanted advertorial content into my nervous system. Blocking unrequested content and keeping a healthy distance from tracking adds many seconds of delay to access of requested content, and the requested content typically has a cognitive half-life of a few seconds to minutes. And the requested content itself typically contains order 10,000x milliseconds of insipid attention mgmt jingles and branding setup. Then to finish it off. Even the most high-minded productions waste minutes of egress begging for \"likes, subscribe, comments,\" reading off lists of sponsors with silly handles, admonishments for upsells, and cappers to \"hit the bell, it's so, so, important\", immediately following which the player bot resets to cramming a new unrelated vid into my sockets. Engagement?! Pfft. It's an incursion. reply knome 5 hours agoparentprevDoes the potential lesser engagement with videos matter in the face of those videos causing a delay in loading the page that displays them? You'd need to check per-video engagement drop against people not bothering to engage with the site in the first place. reply estebank 3 hours agorootparentThis is an example of the tragedy of the commons: the watch time effect is tracked by YouTube, which maximises for it, but the drop off of visitors to the site is something YouTube doesn't \"care\" about (doesn't track it directly, doesn't optimize for it, etc.). reply tantalor 5 hours agoparentprevWhy is it slower? It feels same to me (Pixel phone) reply maxloh 5 hours agorootparentIf I understand it correctly, the library displays a thumbnail and defers loading the YouTube embedded player iframe until you click on it, thus improving responsiveness (page load speed is not affected by its iframe however). reply squigz 2 hours agoparentprev> Every additional millisecond of load time will reduce engagement Is there any data on this? reply dubcanada 5 hours agoparentprevI don't think the point of this is to replace highly critical videos. It's to replace videos like installation instructions which may only be needed by 10% of your users. Not only that but 250ms is the average reaction time of a human, you don't notice an extra 5 milliseconds. If a video is required on your website for engagement you probably shouldn't be hosting it on YouTube anyways. reply qwery 3 hours agorootparent> Not only that but 250ms is the average reaction time of a human, you don't notice an extra 5 milliseconds. Please stop repeating this sort of thing as a simple fact. Time and latency are difficult things to reason about and simple explanations sound particularly convincing when one lacks an intuitive understanding of the subject. Perceived latency is not the same thing as \"reaction time\". What reaction was measured? How? From what stimulus? Your reaction time number does not support your claim that humans can't notice a 5ms difference in lag. In any case, you are misunderstanding and misrepresenting the comment you replied to. When you are talking about an organisation like Youtube (size, money, mercenary, malicious, etc.) and discussing metrics like this, individual milliseconds is not an unreasonable unit to use. Consider the volume of the data. Nobody is saying that if something takes 5ms longer to load that no single human being will be capable of waiting for it anymore. Further, your 250ms is perfectly in the range of the parent comment's order of magnitude of hundreds of milliseconds. reply lotsofpulp 5 hours agorootparentprev>Not only that but 250ms is the average reaction time of a human, you don't notice an extra 5 milliseconds. If this is true, then why are online first person shooters noticeably worse when playing with a 250ms ping connection compared to a 5ms ping? 250ms ping is basically unplayable. If I recall correctly. I stopped playing video games many years ago, because my college’s internet connection didn’t offer low enough latency to be able to play. reply ImprobableTruth 4 hours agorootparentLatency is magnitudes more critical when it's something where you have to react. reply thfuran 4 hours agorootparentprevThe difference between 5 ms and 250 ms is a lot more than 5 ms, so there's no contradiction. reply lotsofpulp 4 hours agorootparentOops, yes I misinterpreted that. I was thinking how can 250ms be the average reaction time when it’s too slow of latency to play a video game, wouldn’t average reaction time have to be lower since people do notice lag at 250ms pings? reply Filligree 4 hours agorootparentHumans can notice and characterise much smaller intervals, down to somewhere around five milliseconds. We still aren’t capable of reacting faster than about 250. However, if you have latency of 250ms then your total reaction time isn’t 250, it’s 500. reply mattkrause 16 minutes agorootparent“Reaction time” isn’t really a single value: it depends immensely your own attributes (age, experience, level of alertness or fatigue), on what you’re reacting to and how you react to it. Under certain (admittedly very specific) conditions, people can view an image, categorize it, and indicate the category with eye movements, all within 120ms. Here’s one demonstration: https://www.sciencedirect.com/science/article/pii/S004269890... reply LocalH 3 hours agorootparentprevWith aids, we can perceive and notice even smaller intervals. I play a lot of Fortnite Festival, which is the Rock Band-style mode added near the end of the year. This game, unlike any previous game in the genre, has \"perfect\" judgements for note hits. The window for a perfect judgement is something around 20 or 30ms. The game also gives you an average offset from \"dead on\" for each song, measured in milliseconds. Since the perfect judgement is immediate feedback, it enables players to perceive when they are just a few milliseconds off and correct for it. I regularly get average offset results of +/- 3ms or better, including plenty of 0ms average offsets (this is of course averaged across all notes in a song, which I am playing on a plastic guitar on Expert difficulty). I'm nowhere close to the best player either, there's one player who recently got one of the most impressive full combos of the Metallica song One that could ever be done - they hit all notes without mistake, they got 100% perfect judgements, and they got the #1 leaderboard score, meaning that not only did they hit all notes within the 20-30ms \"perfect\" window, but they also \"squeezed\" overdrive activations within that window to activate and hit the first note as late as possible, and hit the first note after that overdrive activation would end as early as possible to still get it under the extra 2x score multiplier that overdrive brings. The game genre also overcomes the relatively huge (in the context) human reaction time by providing you gems to read before the strikeline (or \"now bar\"), so that you can basically internally correct for your reaction time, similar to how people reading sheet music can perform in lockstep rhythm when everyone is skilled. It's amazing what different forms of augmentation can do to help paper over the inherent shortcomings in our senses. reply mattkrause 3 minutes agorootparentA key difference here is that you’re able to anticipate and plan upcoming actions, because you’re familiar with the general structure of music and/or the specific song. Even the experts couldn’t respond to an unpredictable stimulus in 30ms; instead, they’re choosing between (say) a 330 ms response and a 340 ms one. This is, of course, still crazy impressive. bongodongobob 9 minutes agorootparentprevRhythm is a completely different beast though because you can anticipate. Most musicians would be more accurate than the average person but wouldn't do any better in a \"click the mouse when the screen flashes red\" type test. dangus 5 hours agoparentprevThis is a good point. Google doesn’t care about the total page load time of your website, they care about the load time of their video. reply qwery 4 hours agoprevOn not believing the reduced engagement rumour and the suggested 'lite-youtube-embed'[0]: I am not surprised to hear that a different player will be treated differently by users. You just need it to look slightly different or behave slightly differently and it's completely alien and not to be trusted. The lite-youtube-embed as demoed (even with the title visible) doesn't let me click through to the actual youtube page. There's no link. It could even appear as a no-right-click-esque attempt to prevent me from going to the actual source of the \"content\" -- it's hostile. Of course, this specific feature could be added easily enough, but it points out a bigger issue. I almost never play embedded video, but when I do it might as well be the normal youtube experience. If you've wrapped it in what looks like yet another layer (for an unknown purpose), I'm going to be less likely to click on it. You're asking me to contend with youtube/google and another unknown entity. This article's \"nothing sacrificed\" is an example of the mistaken belief that developers know how their (or other) software is used by users. You don't and you can't. Not really. You're always guessing. To be clear, I also want less videos everywhere, less google in everything, less megabytes of javascript in everything, etc. Please stop embedding youtube videos everywhere, to vote for a better web. [0] https://paulirish.github.io/lite-youtube-embed/ reply lulzury 33 minutes agoprevIf you have a small site or just don't want any requests to Google for privacy reasons, consider just straight-up downloading the video and embedding it in a video tag. There are yt_dlp wrappers for most popular languages (even js). I'm pretty sure even embedding their poster thumbnail results in them getting your IP and other information so consider downloading that as well (from https://img.youtube.com/vi/[TAG]/hqdefault.jpg). reply sva_ 28 minutes agoparentI feel like some content creators might not be very happy about that reply dmonitor 22 minutes agoparentprevdoing this on a broad scale is a great way to get sued reply xigoi 19 minutes agorootparentSo how about disallowing YouTube embeds and requiring users to upload videos? reply donohoe 4 hours agoprevOne way to help reduce overall weight of embeds (and improve the UX imho) is to block the ads - if you are able to leverage \"Content Security Polices\" on your pages. Example META tag:More info: https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Co... reply 8organicbits 2 hours agoparentThat's clever, well worth its own blog post. reply Aurornis 5 hours agoprev> I don’t think we have any good answers here. In fact, I heard from a little birdie who ran it up the pole that they have tested lighter embeds and found them to reduce engagement. We’re clearly missing some huge part of the story. Obviously, faster load times should improve engagement. So if engagement went down with lighter embeds, that implies that some feature or functionality had to be sacrificed. Yet this blog post claims nothing is sacrificed. Something is missing from this story. reply dceddia 4 hours agoparentI recall an article where Facebook or Uber or some other big tech co found something similar when they added some optimizations, and it turned out that it was being accessed by an entirely new market that couldn’t even load the page before. I wonder if it’s a similar thing here. Maybe the lighter embed is small enough that a huge swath of people is actually able to load a video now, but they bounce because playback is too slow on their device/connection. It would shows as a higher percentage of “person stopped watching video” instead of (nothing, because they never fully loaded the player). reply qwery 4 hours agoparentprev> Obviously, faster load times should improve engagement. I don't think it's this clear-cut. People can have different levels of expectation and tolerance for loading times of different things, and the page (post/article/whatever) is a different thing to the video embedded in it. The load time of the rest of the page is also not necessarily limited by the loading time of the embed (depends what you're looking for). Arguably, the overall page load time will mask the content (embed) load time. Finally, and perhaps most importantly, the loading time of the embed is a different thing to the loading time of the video. The metric to look at may be click-to-playback lag. reply user070223 5 hours agoprevUser side solution click 2 load for ublock users(note that chrome is transitioning to manifest v3 and might not work) is the following thanks to yokoffing/filterlists https://raw.githubusercontent.com/yokoffing/filterlists/main... (Betterfox creator, he has other useful filters on github) !||youtube-nocookie.com^$3p,frame,redirect=click2load.html,domain=~bing.com|~google.com|~duckduckgo.com|~video.search.yahoo.com ||youtube-nocookie.com/embed^$3p,frame,redirect=click2load.html,domain=~bing.com|~google.com|~duckduckgo.com|~video.search.yahoo.com ||youtube.com^$3p,frame,redirect=click2load.html,domain=~bing.com|~chatreplay.stream|~google.com|~duckduckgo.com|~video.search.yahoo.com reply schiffern 4 hours agoparentClick2load is an improvement, but embeds still suck. All I want is a plain link, so a while back I got fed up and wrote a short userscript to just rewrite the page. Works surprisingly well for how simple it is. // ==UserScript== // @name Youtube UnEmbed // @version 0.1 // @description Converts embedded Youtube iframes into links // @match *://*/* // @exclude *://*.youtube.com/* // @exclude *://*.reddit.com/* // @exclude *://looptube.io/* // @grant none // @run-at document-idle // ==/UserScript== (function() { 'use strict'; const SITE = \"https://www.youtube.com\"; //m.youtube Invidious etc const LINK_TO_TIMESTAMP = true; const SHOW_PREVIEW_IMAGE = false; const replaceEmbeds = () => { document.querySelectorAll('iframe').forEach((frame) => { const frameURL = frame.src || frame.dataset?.src; if (!frameURL) return; const match = frameURL.match(/(^https?:)?\\/\\/(www\\.)?youtube(-nocookie)?\\.com\\/embed\\/([a-zA-Z0-9\\-_]{11}).*?(\\?.*((t|start)=([\\dsmh]*)))?/i); if (match?.length == 9) { const newURL = SITE+\"/watch?\" + ((LINK_TO_TIMESTAMP && match[8]) ? \"t=\"+match[8]+\"&\" : \"\") + \"v=\"+match[4]; const elem = document.createElement(\"a\") elem.href = newURL; if (SHOW_PREVIEW_IMAGE) { let img = document.createElement(\"img\"); img.src = \"https://i.ytimg.com/vi/\"+match[4]+\"/mqdefault.jpg\"; img.alt=\"Preview image of Youtube video\"; // 320 x 180 preview. For more resolution options see // https://medium.com/@viniciu_/how-to-get-the-default-thumbnail-url-for-a-youtube-video-b5497b3b6218 elem.appendChild(img); } else { elem.innerHTML = newURL; } frame.outerHTML = elem.outerHTML; // common lazyload hiding methods elem.style.display = \"block !important\"; elem.style.opacity = \"100% !important\"; elem.style.background = \"transparent !important\"; const parent = elem.parentElement; if (parent) { parent.style.display = \"block !important\"; parent.style.opacity = \"100% !important\"; parent.style.background = \"transparent !important\"; } } }); }; replaceEmbeds(); setInterval(replaceEmbeds, 3000); })(); reply Featherknight 3 hours agorootparentGreat script idea! keeping this. Note, this does not prevent the initial loading of the embed itself, just replaces the embed with the link. The total transfer size of a page is still the same. If only there was a way to prevent the loading AND keep the link replacement. reply schiffern 2 hours agorootparentThanks. Despite being Not A Project I did edit to add optional image previews and reuse the regex object, so grab the final version if you want. I should have mentioned this is paired with uBlock Origin to block Youtube iframes (and indeed all iframes) globally. At the time I was writing it to unbreak embedded videos. https://github.com/gorhill/uBlock/wiki/Dynamic-filtering:-qu... https://github.com/gorhill/uBlock/wiki/Blocking-mode https://github.com/gorhill/uBlock/wiki/Blocking-mode:-medium... From uBO My Rules tab: * youtube-nocookie.com * block * youtube.com * block * ytimg.com * block youtube.com youtube.com * noop youtube.com ytimg.com * noop To block all iframes (except sites you whitelist, see links above): * * 3p-frame block reply KomoD 3 hours agorootparentprevI suggest using MutationObserver instead of just running the replaceEmbeds function every 3s. reply schiffern 3 hours agorootparentIntentionally left as an exercise for the reader. ;-D If you do, share it back! I'll switch to your version. Remember I threw this together in about half an hour, and maybe that amount of cleanup to post here. \"Works for me\" is the order of the day. The extra debugging alone would be longer than the whole project! Besides, the function that runs is so light it doesn't really seem worth it. It could even make performance worse, since for reliability you need to observe mutations across the entire DOM, which could occur a lot more often. So for performance you want to add some debouncing too, adding yet more complexity to what's supposed to be a 'quick and dirty' fix. reply ethanol 5 hours agoprevNow we only need to force bloggers to stop using GitHub Gist embeds. Hugo (and probably other static site generators) has built-in support for code snippets with syntax highlighting, and more dynamic sites can rely on highlight.js which removes dependence on third-party services. It's just insane, using heavy iframes for small code snippets. https://gohugo.io/content-management/syntax-highlighting/ https://highlightjs.org reply withinboredom 4 hours agoparentI remember when I worked at Automattic and discovered that the gist's heart emoji was actually served by WordPress and not GitHub. They fixed it within a few weeks, but it was like that for years... reply knallfrosch 1 hour agoprevThe page is broken on ios/safari with text size not set to 100% I don't usually care, but when you hand out advice to other sites and call your little text blog \"Frontend-Masters\", it better work. reply divbzero 3 hours agoprev> The weight also grows linearly with every embed—resources are not shared: two embeds weigh 2.4 MB; three embeds weigh 3.6 MB (you get the idea). Why aren’t these resources retrieved from cache? Shouldn’t the same-origin policy should allow for use of cached resources since they are all loading from www.youtube.com? reply 015a 2 hours agoprev> If I were Team YouTube, I’d get loading=\"lazy\" on there to help with performance right away. No need for videos that aren’t even visible on the page to load right away. This would hurt advertising impressions. reply GuB-42 3 hours agoprev[Ignore this, everything is ok now] For me the website is slow as to border on unusable. I have one core at 100% on Firefox 115 / Debian 11, I guess there is a busy loop somewhere in the JS. Works fine on Chrome(ium). I know it is usually frowned upon to comment on the website itself rather than the content, but considering the nature of the website, I think it is relevant. Edit: Looked in a bit more detail and it looks CSS-related, not JS-related as removing the main style sheet fixes the problem. It happens even in safe mode (no extensions). Possibly a Firefox bug (version is 115.12.0esr, from the Debian 11 repository), but it doesn't happen anywhere else. Edit2: Updated my system, rebooted, etc... it fixed the problem. So either I had something messed up with my system, or the author fixed it, anyway, everything is ok now. reply tambourine_man 3 hours agoparentI know it’s frowned upon and even in the guidelines, but I never understood why. This is Hacker News, the meta discussion on how a site works (or doesn’t) is as relevant and its content. reply 8organicbits 2 hours agorootparentI think HN is aiming for thoughtful and novel comments on the content of the article; complaints like \"page loaded slowly\" are shallow. The parent comment feels more on-point since the article is about optimizing websites and the post is itself a demo. I'm seeingreply xd1936 5 hours agoparentIt looks like this solution is essentially this, but a little nicer looking (on hover animation, for example). reply phh 5 hours agoparentprevAnd this has the extra benefit that you don't give your website data to Google for free reply donatj 5 hours agoprevI am not understanding how Bananas play into this? Is that a term for curly braces I am unfamiliar with or something with? Is code being \"banana heavy\" a term for code having a lot of scopes thus being bloated? I'm grasping at straws here. reply cricalix 5 hours agoparentSlang for crazy or deranged. In this case, \"bananas heavy\" would probably equate to \"crazily heavy\", which would then equate to \"crazily large for the functionality\". https://english.stackexchange.com/questions/74581/why-does-b... Has a reasonable-reading accepted answer. reply tedunangst 5 hours agoparentprevBananas are crazy. reply poikroequ 4 hours agorootparenthttps://www.youtube.com/watch?v=BZ6Ev46i2to reply pseudalopex 5 hours agoparentprevWhy not look it up? It means crazy. reply donatj 4 hours agorootparentI googled \"bananas heavy\" but I didn't get much reply spencerchubb 4 hours agorootparentprevPerfect use case for chatgpt or other llm reply pseudalopex 4 hours agorootparentAre you serious? It's the primary use for a dictionary. And did you try ChatGPT? It suggested bananas heavy meant bananas are physically heavy because they are ripe. reply spencerchubb 48 minutes agorootparentYes I'm serious. Here's the prompt I would use: \"what does bananas mean in this sentence: YouTube embeds are bananas heavy and it’s fixable\" Put it in google, and it doesn't give anything useful Put it in an LLM, and you get a correct answer because LLMs can understand full sentences reply marcosdumay 2 hours agorootparentprevYet, when I put \"What's the meaning of \"bananas heavy\"?\" in perplexity, it explains it perfectly well. One of the easiest things for a LLM to \"memorize\" is the dictionary. And it makes for a much more flexible one. It even suggest additional queries where it explains why \"bananas\" is used on that phrase. The only problem I found on the query is that perplexity seems to be de-emphasizing its sources, what IMO is bananas. It just pushes the site into direct competition with ChatGPT and removes most of its value. reply jondwillis 4 hours agorootparentprevdictionaries do not provide much context that would un-confuse someone who has not come into contact with a colloquialism. reply pseudalopex 3 hours agorootparentThe context is bananas means crazy. Dictionaries provide this context. reply donatj 3 hours agorootparentIt would not have helped me in this situation. My assumption was \"bananas heavy\" was a phrase I was unfamiliar with, so I was trying to look that up. \"Bananas heavy\" is not in the dictionary, I fully understood that bananas can mean crazy, but what I do not understand is the sentence and the context. I never would have used the word with that meaning like this. Who would say something like this? I still don't understand why a person would say \"bananas heavy\" to mean \"YouTube embeds are bloated\". It just sounds so awkward and bizarre. reply popcalc 3 hours agorootparentbananas == crazy; (x heavy) == crazy heavy reply ZeljkoS 3 hours agoprevNot mentioned in the article, one of the main reasons to ALWAYS do this is SEO. Regardless if users will play the video or not, web crawlers will not play the video and Google will penalize your SEO ranking if you use official Google's YouTube embed :D We implemented our own thumbnail image trick on TestDome homepage a few years ago (https://www.testdome.com/). Thumbnail is from: https://img.youtube.com/vi/gPQQg4yZqt8/sddefault.jpg reply II2II 5 hours agoprevTangent: I find tremendous irony in the video used as an example. It is for the remake of a game that is about problem solving and story telling, yet the remake requires vastly more resources than the original. Yes, there are reasons for the increased resource usage. On the other hand, there are reasons to use the more resource intensive version of the YouTube embed. I suppose the moral of the story is that software will grow to consume the resources available to it. Often, if not most of the time, there will be benefits to that increased resource usage. Yet that won't prevent people who prioritize factors like efficient resource usage from seeking alternatives they view as better. reply dangus 5 hours agoprevMy favorite part of the article: > They found, quite counterintuitively, that average page load times went up. But with a deeper look, they found that the lighter page was able to reach more people, including people on low-power low-internet-speed devices who were able to actually use YouTube for the first time, and them using it much more slowed those averages. That’s awesome! The speed of using the site was up relatively for everyone. The metric of the average page load speed was a red herring and ultimately not meaningful. reply demorilo 4 hours agoprevWhat is the problem of sites hosting their own videos? reply Aachen 2 hours agoparentTweakers.net mentioned in their podcast a few weeks back that they had this, but that people hated the player and it was a lot of work maintaining the back-end hosting thing. I don't understand what they need beyond Nginx with an FTP drive (or whatever WYSIWYG tool they also use for uploading images with the news article) and add `` into the article. Browser does the rest of the work. In the Flash era this was different but that's been a while So I don't know the answer to our question either but according to them we're overlooking something reply ksec 5 hours agoprev>TL;DR: YouTube Embeds are like 1.3MB in size with no shared resources between multiple embeds. Using aWeb Component is more like 100k, does share resources, and sacrifices no functionality. I would even go further and wonder why lite-youtube still requires 100K? And why they are not shared across different sites. reply KMnO4 5 hours agoparentIn a nutshell: You can track load time of your site. If (e.g.) NBC homepage has videos of X, Y, and Z on their homepage, you can simply embed those same videos on your site. Does your page takeAnd why they are not shared across different sites. Cross site resources caching has been disabled in all major browsers for a few years now, because it was used for tracking. reply jsheard 5 hours agoparentprevThe lite version still has to download the thumbnail, which is somewhere around 100-150k by itself for the 720p version. YouTube does provide smaller thumbnails you could use instead but with modern screen resolutions you probably want the big one. reply cuu508 4 hours agorootparentDoes the 100K figure include thumbnail? Edit: I guess yes, the unminified JS file here is 10KB and much of it is code comments https://github.com/paulirish/lite-youtube-embed/blob/master/... reply dangus 4 hours agoparentprev> I would even go further and wonder why lite-youtube still requires 100K? Just turn off your computer and you’ll be using 0K. reply occz 5 hours agoprevTo put this into context, the player element might load 1.3 mb but loading a video can easily be in the range of 50 mb. reply masklinn 5 hours agoparentWhich is only relevant if 100% of users load the video every time they hit the page. Because the 50M is paid by those who watch the video while the 1.3M is paid by everyone opening the page. reply justsomehnguy 5 hours agoprev> The Solution is to Replicate the Embed Experience Another Way. No, The Solution is to publicly shame Google Inc for wasting the resources. When it would be on WSJ and The Guardian front page then, maybe, you would see the result. Replacing the embed a one server/page is like pissing in the ocean - feels great, does nothing. reply zinekeller 5 hours agoparent> No, The Solution is to publicly shame Google Inc for wasting the resources. When a different part of Google publicly shames YT devs for this exact thing (https://web.dev/articles/embed-best-practices#use_click-to-l...), you know that there's little hope for change. reply QuinnyPig 5 hours agoparentprevThe front page of Hacker News remains one of the only effective places to file bug reports against Google products. reply haliskerbas 5 hours agoparentprevNo one gets a promo for maintenance work. Especially at places like google. reply stevekemp 5 hours agorootparent\"I saved XXTb of bandwidth with this one simple change!\" reply popcalc 3 hours agorootparentCould be resume fodder. reply veeti 5 hours agoparentprevGoogle PageSpeed will gladly shame you for using the slow YouTube embed. reply troupo 4 hours agoparentprev> The Solution is to publicly shame Google Inc for wasting the resources. They don't care/internal company policies are misaligned. Here's Chrome's engineering leader saying that 2.4 seconds to load a page on Reddit is fast actually: https://x.com/addyosmani/status/1678117107597471745 Here's him also writing a huge article on using third-party scripts to improve Youtube embeds because that's apparently better than fixing actual Youtube embeds: https://web.dev/articles/embed-best-practices BTW, do you know that Youtube's desktop site loads an 2.5 MB CSS file (and 11 MB of Javascript)? reply coldtea 5 hours agoparentprev>When it would be on WSJ and The Guardian front page then, maybe, you would see the result Good luck with that. Not to mention WSJ and The Guardian themselves are babanas heavy for no good reason - like every other media site. reply Havoc 4 hours agoprevI'm seriously considering just setting up a service that downloads everything in my preferred channels overnight and re-serves it locally. It'll increase their cost and decrease their revenue but frankly I'm just getting fed up with their UX. It's just sooo fuckin bad I do wonder whether YT staff actually uses it. e.g. lately the thumbnails have a popup overlay that pops into existence at last second over the thumbnail, basically hijacks your click and navigates you away from yt and to a support.google.com page that discusses paid product placement. No google thats not why I clicked on a video thumbnail. reply Larrikin 4 hours agoparentHow do you filter out what you actually want? There are no tools to sort through subscriptions that I know of. I am subscribed to some channels that put out tons of contents where I usually don't want to watch it, but the stuff that I do want to watch is high priority, channels where everything is priority, and channels where it's content I just want to watch occasionally when it happens to show up and I'm in the mood. I learned long ago that anything I want to watch more than once should be saved locally when a record label made a group I liked delete all their content to promote a new album, but I need a solution for filtering the one time views. reply Havoc 1 hour agorootparent>How do you filter out what you actually want? I wouldn't. Bulk download it all and decide locally. Not something I'd usually do because it is wasteful and not being a good net citizen but sufficiently annoyed that I'm willing to colour outside the lines here. reply l72 4 hours agoparentprevI previously used FreshRSS to follow youtube channels I was interested, but I've switched to Tube Archivist[1] + the Jellyfin Plugin [2]. Now my youtube channels are automatically downloaded to my server and I can watch them through Jellyfin, like any other show. I never have to deal with YouTube's terrible UX anymore. [1] https://www.tubearchivist.com/ [2] https://github.com/tubearchivist/tubearchivist-jf-plugin reply QuadmasterXLII 4 hours agoparentprevyt-dl, chron, and onedrive or icloud together achieve this reply pacifika 5 hours agoprevA tiny script Click to toggle the display css attribute prevents it loading as the page loads reply IshKebab 5 hours agoprevIMO YouTube embeds are a bad experience anyway, especially on mobile. Just put a poster image with a link to the actual YouTube video. reply schiffern 1 hour agoparentIf your mobile browser can run userscripts I threw something together to rewrite Youtube embeds into links. Using the preview sounded cool so I added it. Change SHOW_PREVIEW_IMAGE to true. https://news.ycombinator.com/item?id=40898049 reply mrkramer 3 hours agoparentprevOn mobile you can tap \"Watch on YouTube\" and it will deep link and open the video inside your YouTube mobile app. reply difosfor 5 hours agoprevShouldn't class lty-playbtn be lyt-playbtn? reply paulirish 4 hours agoparentlol, yes. good eye. ;) https://github.com/paulirish/lite-youtube-embed/issues/65 reply jeffbee 3 hours agoprevWhen I navigate to the test page[1] by the author of lite-embed, the standard variant transfers 82k. According to browser stats, the largest element \"base.js\" has been cached for several days. I imagine that the authors of YouTube who are undoubtedly as sensitive to the costs of transferring bytes as anyone have analyzed the problem with browser caches taken into account. https://paulirish.github.io/lite-youtube-embed/variants/yt.h... reply didgetmaster 5 hours agoprev [–] Get the climate change activists involved. How many of these videos have to play before the wasted electricity is equivalent to a car running for a year? reply phh 5 hours agoparentIn my business area (ISP), when doing carbon accounting, we have \"scope 3\" which includes the electric consumption we incur the electric consumption of our devices at user's home. A first step towards reducing those externalities [1] is mandating that they power consumption Youtube incurs is accounted for. -- Another Youtube thing that made my computer screams is \"theater mode\", eating so much CPU for eye candy on so many devices ought to be at least declared. [1] To explain the externality: Google probably does that because they make maybe +0.5% revenues by click to video, at almost 0 cost for them. Since the whole extra cost is paid for by the user (by requiring a bigger computer, by consuming more electricity). The price for the user to view a page without the embed and with it can be something like +20% at no gain for them. It's so small than no sane user make those computations, but if you start accounting like a company, you would see the difference. (Plus obviously all the ecological externalities) reply jeffbee 3 hours agoparentprev [–] This must be intentionally hyperbolic or you have no concept of how much energy is needed to run a car. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "YouTube embeds are resource-heavy, with each embed around 1.3MB and 32 requests, leading to slow load times and high data transfer.",
      "A Web Component like \"lite-youtube-embed\" by Paul Irish can reduce the size to about 100k, sharing resources and improving performance significantly.",
      "Implementing this solution involves linking the JavaScript, using the component in HTML, and loading the script asynchronously for better performance."
    ],
    "commentSummary": [
      "A community forum platform has replaced YouTube embeds with proxied thumbnails, which load only when clicked, reducing the need to download large JavaScript files and preventing Google tracking.",
      "Similar solutions have been implemented for YouTube, SoundCloud, and Reddit embeds to enhance functionality and minimize bloat.",
      "There is debate over whether lighter embeds might reduce engagement due to slower video start times, but many believe that improving overall page load time is more important for user experience and site performance."
    ],
    "points": 236,
    "commentCount": 135,
    "retryCount": 0,
    "time": 1720355875
  },
  {
    "id": 40896102,
    "title": "Malloc broke Serenity's JPGLoader, or: how to win the lottery (2021)",
    "originLink": "https://sin-ack.github.io/posts/jpg-loader-bork/",
    "originBody": "How malloc broke Serenity's JPGLoader, or: how to win the lottery Keywords: #serenity #bug I got the chance to investigate an interesting bug in SerenityOS this week. It was related to the decoding of JPG images in the operating system. For some reason, when a JPG image is viewed, it comes out like this: Lenna, showing up with incorrect colors. Weird, huh? Also seems like a simple confusion of RGB vs. BGR. And sure enough, making the following change on JPGLoader.cpp: - const Color color { (u8)block.y[pixel_index], (u8)block.cb[pixel_index], (u8)block.cr[pixel_index] }; + const Color color { (u8)block.cr[pixel_index], (u8)block.cb[pixel_index], (u8)block.y[pixel_index] }; context.bitmap->set_pixel(x, y, color); makes the image show up correctly. Case closed! …not. Why did this even break in the first place? The last non-reverted change to JPGLoader.cpp is reported by Git to be over a month ago: Commit log at the time of JPGLoader being broken. And I remembered very well that JPG images worked just fine about a week or two ago, as I had set a JPG image as my background and would’ve noticed if it looked wrong. Well, time to bisect! I didn’t know when to start, so I picked the last 1000 commits (where images showed up correctly), and started bisecting. Bisect hell Please skip to the next section if you’d like to avoid C++ whining. SerenityOS, being an operating system project that focuses on doing its own thing, also has its own standard library called AK (which stands for Agnostic Kit). This library is analogous to C++’s STL, but is more readable due to not having to support a myriad of different operating systems and not having to contort oneself to conform to hideous coding standards. One of the nice things about having the standard library in the same repository as its users is that making changes is very easy as the change propagates to everyone who pulls from master. However, this is a double edged sword when it comes to C++; because everyone includes the standard library (even if you don’t include it, your includes will), and because C++’s template system means that everything that’s templated has to include the definitions in the header as well, this means that anytime someone touches AK in a commit, the entire operating system has to be rebuilt (~3400 files at the time of writing). ccache, while being useful in many situations, cannot handle this case. Additionally, due to the breakneck pace of the SerenityOS project, someone ends up touching AK at least once every 100 commits or so. As a result, during the 1000 commits I ended up bisecting for, I had to build SerenityOS from scratch about 4-5 times on a 2011 laptop with Sandy Bridge Mobile. While this isn’t the fault of the project, I’m still mad. Bisect results So, after bisecting 1000 commits, rebuilding the OS from scratch several times and pulling my hair out because I didn’t understand how bisect worked, I finally found the commit that broke JPG images. Drumroll please… f89e8fb71a4893911ee5125f34bd5bbb99327d33 Author: Gunnar Beutner AuthorDate: Sat May 15 10:06:41 2021 +0200 AK+LibC: Implement malloc_good_size() and use it for Vector/HashTable This implements the macOS API malloc_good_size() which returns the true allocation size for a given requested allocation size. This allows us to make use of all the available memory in a malloc chunk. For example, for a malloc request of 35 bytes our malloc would internally use a chunk of size 64, however the remaining 29 bytes would be unused. Knowing the true allocation size allows us to request more usable memory that would otherwise be wasted and make that available for Vector, HashTable and potentially other callers in the future. Uh, sorry, what? But it was. Building the commit right before this one showed the image correctly: Lenna, before it was broken. Initial discussion with other developers made me think that either JPGLoader or something else up the chain is depending on the capacity of a Vector and writing directly into it when it really shouldn’t. So I began hunting down possible causes. A surprising discovery The commit seemed to touch the two main container types: HashTable (which HashMap depends on) and Vector. Both are used in the JPGLoader code, and either could be the cause of the problem here. I picked HashTable at random, removed the offending line: new_capacity = max(new_capacity, static_cast(4)); - new_capacity = kmalloc_good_size(new_capacity * sizeof(Bucket)) / sizeof(Bucket); auto* old_buckets = m_buckets; and rebuilt the system, while joking around in chat about how this can’t possibly be the problem. …but then it fixed the issue. What? How? Why does the HashTable capacity being different matter?! HashTable isn’t even a contiguous stream of data you can write to, so you shouldn’t even be able to assume its capacity! Before I present the full story to you, I’ll have give a brief background on how JPGLoader used to work. Non-deterministic serial component iteration That’s really the most appropriate title I can give this section. JPGLoader previously would read information about a JPG component from the “Start of Frame” section of the JPG file into a struct called Component, and then store that in a HashTable. Of course, the order in a JPG file for each component should always be Y, Cb and Cr, so the Component struct would idiosyncratically carry a serial_id, which was the position of the Component within the file. The reason the Components were in a hash table was that they would then be checked against the component ordering in a “Start of Scan” section to make sure all the components in the SOS section are in the expected order. Why this code was written this way instead of just checking against the ID by linearly iterating over the Components, I have no idea. Anyway, these components would then be iterated over during the different decoding stages of JPGLoader, during which the component information would be used to perform transforms on macroblocks. Getting close When I added some debug prints to see how the components were read, I saw this in the commit with the broken colors: ImageDecoder(33:33): Looking at component 0 ImageDecoder(33:33): Looking at component 2 ImageDecoder(33:33): Looking at component 1 ImageDecoder(33:33): Looking at component 0 ImageDecoder(33:33): Looking at component 2 ImageDecoder(33:33): Looking at component 1 ... And when I checked out the previous commit, I saw this: ImageDecoder(33:33): Looking at component 0 ImageDecoder(33:33): Looking at component 1 ImageDecoder(33:33): Looking at component 2 ImageDecoder(33:33): Looking at component 0 ImageDecoder(33:33): Looking at component 1 ImageDecoder(33:33): Looking at component 2 ... The final piece of the puzzle: During the discussion of this bug with CxByte at my wit’s end, we ended up manually messing with the order of the components to see what would happen, and got this message: ImageDecoder(32:32): Huffman stream exhausted. This could be an error! ImageDecoder(32:32): Failed to build Macroblock 3277 …ah. Of course. It’s a stream. The bug So, here’s a quick rundown of the bug: Someone used a HashTable to store objects that should be ordered, then iterated over it using the basic HashTable iterator The hash of the component IDs in the JPG files were passed into int_hash for hash table bucket selection Not only did they get just the right value to be in order, they got inserted into a HashTable with just the right amount of buckets to be in the correct order This caused the Huffman stream to be read in the correct order for each component, thereby masking the bug This bug was masked since JPGLoader’s inception by sheer luck until someone messed with the size of the HashTable The fix And finally, at the end of about 10 hours of debugging, here is the commit that fixed this monster of a bug: a10ad24c760bfe713f1493e49dff7da16d14bf39 Author: sin-ack AuthorDate: Mon May 31 15:22:04 2021 +0000 Commit: Linus Groh CommitDate: Mon May 31 17:26:11 2021 +0100 LibGfx: Make JPGLoader iterate components deterministically JPGLoader used to store component information in a HashTable, indexed by the ID assigned by the JPEG file. This was fine for most purposes, however after f89e8fb7 this was revealed to be a flawed implementation which causes non-deterministic iteration over components. This issue was previously masked by a perfect storm of int_hash being stable for the integer values 0, 1 and 2; and AK::HashTable having just the right amount of buckets for the components to be ordered correctly after being hashed with int_hash. However, after f89e8fb7, malloc_good_size was used for determining the amount of space for allocation; this caused the ordering of the components to change, and images started showing up with the red and blue channels reversed. The issue was finally determined to be inconsistent ordering after randomly changing the order of the components caused Huffman decoding to fail. This was the result of about 10 hours of hair-pulling and repeatedly doing full rebuilds due to bisecting between commits that touched AK. Gunnar, I like you, but please don't make me go through this again. :^) Credits to Andrew Kaster, bgianf, CxByte and Gunnar for the debugging help. Final thoughts Sometimes the simplest problems might point at big mistakes within. I could’ve probably fixed this by just swapping the order of the arguments right then and there, and it would’ve worked; until someone else came along and changed the order again. Thankfully, now we will be able to look at tubas with correct colors in peace. A tuba with the correct colors. Source: music123.com Thanks Thanks to CxByte, Gunnar, Andrew and Brian for their help with debugging this, and their helpful tips. Gunnar in particular was the one who uncovered this bug, and despite my satirical jab in the commit message helped uncover this very interesting bug, so he’s the one who made this post possible. Also, thanks to the person who introduced this bug (the commit log gets a little fuzzy, so I’m not quite sure who did) and hope he buys a lottery ticket. :^) And thank you for reading. I’ll probably post sometime in the future, but work’s been keeping me busy. But maybe I’ll find another bug to suck me into a rabbit hole. Stay tuned! ← prev next →",
    "commentLink": "https://news.ycombinator.com/item?id=40896102",
    "commentBody": "Malloc broke Serenity's JPGLoader, or: how to win the lottery (2021) (sin-ack.github.io)177 points by fanf2 10 hours agohidepastfavorite143 comments dale_glass 9 hours agoThis is one of the reasons why many hashtable implementations introduce a random component into the algorithm. The order of elements changes on every run, so if you accidentally rely on the order, it's going to go wrong sooner rather than later. It also very nicely prevents security issues, since if the hashing algorithm is fixed, it can be exploited for denial of service by coming up with keys that all fall into the same bucket. reply zarzavat 8 hours agoparentMany implementations these days also go the opposite way, guaranteeing that hash tables always iterate in insertion order. I prefer this because it means I don’t have to decide whether I need an ordered map or an unordered map. Often if I think I need an unordered map it turns out to be wrong for some subtle reason. reply mort96 8 hours agorootparentJavaScript is probably the most notable example of that. It used to not have a guaranteed iteration order, but browsers implemented it in such a way that the iteration order was the insertion order, and then that eventually got standardized because websites started depending on it. For general purpose hash maps in standard libraries, I think you ought to either randomize the iteration order so that it's different every time, or guarantee an iteration order. Leaving it unspecified but predictable in practice is a recipe to fall victim to Hyrum's Law (https://www.hyrumslaw.com/). reply masklinn 7 hours agorootparentJavascript is kinda weird as the numerical keys have their own special ordering. Python is probably the better known one, as it went through \"arbitrary but deterministic\" (before 3.3) to \"wilfully non-deterministic\" (from 3.3 to 3.6) to \"insertion ordered\" from 3.6, the latter of which was initially an implementation detail of improving the hashmap but was then made into the language spec starting 3.7. reply zbentley 3 hours agorootparentPerl also changed its behavior, but in the other direction: a random seed was used per interpreter start after 5.18. https://www.perlmonks.org/?node_id=1005122 reply masklinn 1 hour agorootparentThat's actually the first python transition I mentioned, per-process hash randomisation had the effect of making map iteration non-deterministic. I believe in both cases this was in response to the hashdos concern / attacks. reply Yoric 7 hours agorootparentprevI remember that it actually broke a critical algorithm I was testing at the time. Fun times. reply pimeys 8 hours agorootparentprevRust has HashMap with random order and BTreeMap which is ordered by the key. Additionally one can use IndexMap crate if wanting to keep the order of insertion in the map. The issue with the latter is how much memory it can waste in the worst cases. A good example is the serde_json library, if enabling the ordering of the maps. If you deserialize JSON into its dynamic Value enum, the resulting object can be many times bigger than the original string. For immutable data that can fit to the CPU cache, utilizing a sorted vector can be many times faster and uses less memory compared to the maps. reply masklinn 7 hours agorootparent> A good example is the serde_json library, if enabling the ordering of the maps. If you deserialize JSON into its dynamic Value enum, the resulting object can be many times bigger than the original string. Deserialized non-trivial objects are generally larger than the original serialised value. IndexMap should not generally be significantly larger than a HashMap though, unless the key and value are very small (sub-word). reply pimeys 7 hours agorootparentWe did measure significantly bigger memory usage with IndexMap and needed to revert back to HashMap eventually. Deserializing into a defined struct does not waste as much memory as Value does. Especially due to the recursive nature of the Map variant, which can hold another Map. reply masklinn 2 hours agorootparent> We did measure significantly bigger memory usage with IndexMap and needed to revert back to HashMap eventually. That is strange and I’d assume the maintainers would be interested in the information. By my reckoning HashMap would be consuming about capacity * 10/9 * (8 + sizeof key + sizeof value) while indexmap should be consuming capacity * 10/9 * 8 + capacity * (8 + sizeof key + sizeof value). Unless indexmap reuses hashbrown directly in which case you’d get something like capacity * 10/9 (16 + sizeof key) + capacity * sizeof value. reply aranke 8 hours agorootparentprevPython also, similar story. reply tucnak 5 hours agorootparentprevWhat's it with the Americans calling their ideas/best practices \"laws\"? Ridiculous. Hey, check out Ilya's law: if there's an idea, some American would always make it a law in his own name. You can cite me on that. reply mort96 2 hours agorootparentAmericans? This is using \"law\" to mean something akin to \"scientific law\", such as Newton's three laws of motion or Mendel's three laws of inheritance; it's a description of what will happen. People were using \"law\" in this way since before the USA even was a country lol reply kortex 3 hours agorootparentprevIt probably started with Zipf's law in the 1930s and then Murphys law a few years later. reply dkersten 8 hours agorootparentprev> Often if I think I need an unordered map it turns out to be wrong for some subtle reason. Huh. This hasn’t been my experience. I very rarely need maps to be ordered. In recent years, the only case I can remember is when serializing to TOML and wanting the keys to be written in a specific order. There have been the occasional other case where insertion order is what I wanted, but I almost never need ordering in map keys. > I prefer this because it means I don’t have to decide whether I need an ordered map or an unordered map. I’m the opposite, I prefer to be given a choice so I can make the tradeoffs when I want to or need to. If you don’t want to choose, you are free to always choose ordered map, but even if ordered map is the default, there should always be a choice to use unordered map. It’s been very rare that I started with the wrong one and had to change. When I write python or JavaScript I typically don’t care and will just use whatever is the default, but when I write C++, I very much do care and the vast majority of cases use phmap’s flat_hash_map, which has superior space and speed over std::map and std::unordered_map. For ordered maps I use tsl::ordered_map but that still comes at a cost over flat_hash_map and its unordered variants. reply zarzavat 3 hours agorootparentAs a rule I’m happy to sacrifice cycles for determinism, because non-deterministic bugs are disproportionately wasteful of developer time. As much as possible I want my code to give the same results from one run to the next. Some sources of non-determinism are unavoidable, but e.g. unordered maps and unstable sorts both have deterministic alternatives that are almost as performant. Maps are such a common data structure that eliminating unordered maps has a big impact on whole program reproducibility. reply saagarjha 3 hours agorootparentPicking a tool without thinking about it is a surefire way to get runtime bugs. reply kccqzy 1 hour agorootparentprevI don't understand this reasoning. If there is a subtle reason wouldn't you take the time to think through it carefully? Are you in such a rush that you don't have time to decide the required data structure (ordered vs unordered)? Or do you have insufficient control of downstream software that you fear for unknown bugs caused by this? And since insertion order is often related to some other ordering in the input, you are comfortable that downstream software completely rely on this ordering even when it's undocumented? Genuinely curious because this kind of reasoning is alien to me. reply thinkharderdev 8 hours agorootparentprevIsn't that just a different data structure? How do you preserve insertion order in a hash map? reply layer8 6 hours agorootparent> How do you preserve insertion order in a hash map? You enhance the stored elements to also be the nodes of a doubly linked list. The overhead is rarely critical in practice. It can be made more efficient if the hash map doesn’t need to support deletion. reply Sesse__ 4 hours agorootparent> The overhead is rarely critical in practice. Depends; you add two extra pointers for each element, so your int → int hash table balloons in size. reply layer8 4 hours agorootparentI repeat: This is rarely critical in practice. Of course there are cases where it becomes critical, but it’s a perfectly good default. reply thinkharderdev 6 hours agorootparentprevAh yeah, I've implemented LRU caches this way (hash map with an intrusive linked list overlayed on the values) but didn't put 2 and 2 together :) reply JonChesterfield 7 hours agorootparentprevKind of? It usually means you've compromised the data structure somehow but occasionally it shows up incidentally. For example, if you append the keys/values to an arena instead of inline in the hash you get a different set of performance tradeoffs. However insertion order is then available by walking the arena. Appending to an arena in the background is a decent choice for variably sized data, as opposed to heap allocating everything one at a time. That probably has to store the size of each item, hence a forward iterator over the arena at zero cost. Minor quibbles around deleting and tombstones notwithstanding. reply IshKebab 6 hours agorootparentprev> I prefer this because it means I don’t have to decide whether I need an ordered map or an unordered map. Well only if you happen to insert your elements in order. If you want a proper ordered map like `std::map` in C++ or `BTreeMap` in Rust then you are out of luck (at least in Python and Javascript). reply anal_reactor 6 hours agorootparentprevAt my previous company my boss made me angry after I already handed in my two weeks notice so I stopped caring, and I wrote code that depended on the insertion order into a map. Of course I didn't document it. Have fun guys. reply tzot 6 hours agorootparentYour ex-boss made you angry and you left code that will make the life more difficult for whom? Only your ex-boss or more people? Who gets punished for what you perceive as one person's mistake? reply anal_reactor 6 hours agorootparentIt was a company where 80% of employees were interns, I was one of them, the other 20% were people who couldn't get hired elsewhere, with just a handful of those who actually knew what they were doing. There was no leadership. I agree that I handled the situation unprofessionally, but I feel excused, considering the circumstances. Whether I'd do it again depends on who'd need to clear up this mess. If it's people I care about because I got to know them - I'd keep my cool. But if it's some abstract \"organization\" where I was just a random cogwheel with zero connection to other cogwheels, then you can't expect me to care about anything that doesn't include \"me\". reply saagarjha 3 hours agorootparentWe can expect a lot from you, but I guess you’re always free to let people down. reply anal_reactor 2 hours agorootparentIf I counted each time people let down me I'd definitely broaden my vocabulary of huge numbers. reply CoastalCoder 5 hours agorootparentprevVengeful behavior can be gratifying in the short term, but in the long term I've never felt good about it. Maybe you'll find the same. reply tedunangst 6 hours agoprevThis seems like a case where a little more debugging would have saved time over brute force bisection. The logging to print component orders had to be done eventually anyway. reply elteto 9 hours agoprevKudos on the debugging but also on that commit message. It managed to condense the cause and the fix into a couple of paragraphs. reply russfink 7 hours agoprevThis isn’t Gunnar’s fault. The problem was whomever stored ordered data in a hash file. I have been in this business for decades and I have run into the situation where changing the shape of memory uncovers bugs. Every time it causes many hours and days of debugging. If programming weren’t hard, they wouldn’t need us to do it. (I’m not sure how much longer that phrase will hold up under large language models.) reply powercf 2 hours agoparent> This isn’t Gunnar’s fault. The problem was whomever stored ordered data in a hash file. Yes. Even if it were, I don't think it needs to be mentioned in the commit message. Gunnar improved something, which triggered problems with old broken code. For his efforts he gets: > Gunnar, I like you, but please don't make me go through this again. :^) reply caboteria 7 hours agoparentprevAs long as LLMs are trained on code that has bugs, they'll suggest code that has bugs. reply chrisjj 7 hours agoparentprevIndeed. And contrary to the title, the fault isn't malloc()'s either. reply AgentOrange1234 3 hours agoprevIt is frustrating to me that, given such a gender disparity in our field, when we could literally choose any other image to demonstrate, we continue to use a centerfold. It is well past time to be aware of this and stop tolerating it. reply kmeisthax 2 hours agoparentGiven SerenityOS's stance on gender-neutral language[0], I think it would be safe to say the use of the Lenna image was deliberate. There's been backlash against using cropped pin-ups as test images since at least the 90s[1], this isn't something they would have just now discovered. [0] https://github.com/SerenityOS/serenity/pull/24647 [1] https://youtu.be/yCdwm2vo09I reply jenadine 20 minutes agorootparentWhat's SerenityOS's stance? They merged a PR to replace males pronouns https://github.com/SerenityOS/serenity/pull/24648 If I understand correctly, there was just a social media mob calling the project transphobic over the use of a couple of \"he\" in the documentation, that is uncalled for. reply Pikamander2 2 hours agoparentprevCounterproposal: Keep the Lenna image, but balance it out by adding a second test image with some opposite-flavored eye candy: https://i.imgur.com/TIMyJsW.png reply throwawayk7h 2 hours agorootparentThis is honestly a good idea (with an actually attractive model instead). I expect Lenna will never die at this point, and honestly, why try to expnge her from history. The best way to fight a meme is with another meme. reply wojciii 2 hours agorootparentprevOr you know .. some attractive male model selected by female developers. reply amelius 1 hour agorootparentWhy can't we have personalized research papers, where the images are chosen based on the reader's personal preferences? reply TowerTall 2 hours agoparentprevThe Lenna (or Lena) picture is one of the most widely used standard test images used for compression algorithms. http://lenna.org/ reply bleuarff 2 hours agorootparentYes, but the model has asked that the community stops using this image. It should be pretty easy to find an image that can serve as a standard, and that no one objects to. reply ashleyn 2 hours agorootparentThis is the thing that makes it pretty open and shut to me. Image library devs can still choose to put the whole political thing aside to respect the wishes of the photo's subject. Does anyone have to, well, not really, but it'd be the nice thing to do. reply vfclists 1 hour agorootparentprevAny sources on this? reply kchr 8 minutes agorootparentYes, for example the interview with her in this video: https://youtu.be/yCdwm2vo09I reply vfclists 1 hour agoparentprevSounds like anger finding an issue to fixate on, if it matters that much to you get some therapy. I doubt whether the lady in question is as bothered by it as much as you are. I am more upset by the brutality being inflicted on the innocents in Gaza and the fact in my country the UK, families with both parents in work still struggle to pay their bills. reply Sparkle-san 1 hour agorootparentLena has publically stated: “But I retired from modeling a long time ago. It’s time I retired from tech, too.” But that aside, the rest of your argument is just the fallacy of relative privation. reply vfclists 1 hour agorootparentThat statement in no way implies she is particularly bothered by it and I'm sure if it is an issue for her she will ask Playboy to fix the problem for her. That image is over 50 years old and is her immortality. You guys need to find another outlet for your communist ideology of seeking some real or imagined social flaws as a means of venting your shrewish tendencies. reply kchr 3 minutes agorootparentWhy should she need to go through a legal process instead of kindly asking the tech community to move on and use a different image? reply ddtaylor 4 hours agoprev> As a result, during the 1000 commits I ended up bisecting for, I had to build SerenityOS from scratch about 4-5 times on a 2011 laptop with Sandy Bridge Mobile. While this isn’t the fault of the project, I’m still mad. I think SerenityOS has some folks that help each other out with resources and PCs for testing purposes. reply Ygg2 9 hours agoprevNeeds [2021] in title reply jcelerier 9 hours agoprev> I had to build SerenityOS from scratch about 4-5 times on a 2011 laptop with Sandy Bridge Mobile. I mean, this is like trying to do Windows Vista development with a computer released in the timeframe between Windows 3.1 and Windows 95 reply elteto 9 hours agoparentMaybe in terms of time, but not in terms of actual performance. CPUs haven’t changed that much since 2011 (relatively speaking), but between Windows 3.1 and Vista we got x64 and multicore CPUs everywhere became the norm. reply nicoburns 9 hours agorootparent> CPUs haven’t changed that much since 2011 (relatively speaking) Perhaps not relatively speaking, but my 2021 CPU is 10x faster than my 2015 CPU on workload which parallelise (which compiling generally does). reply mort96 8 hours agorootparentGood, let's look at hard numbers! Windows 3.1 came out in 1992. One of the highlights in the CPU world in 1992 was the launch of the Intel DX2 (https://en.wikipedia.org/wiki/Intel_DX2). It used an 800nm process node, ran at up to 66MHz, had 8k of cache, and was usually coupled with either 4 or 8 MB of RAM. Windows Vista came out in 2007. That's the year Intel released their Core 2 Quad (https://en.wikipedia.org/wiki/Intel_Core_2). It was a quad core, manufactured on a 45nm process node, running at up to 3.5GHz, with 256k of L1 cache and 8M of L2 cache. In this era, computers often had around 2 GB of RAM. So we're talking 4x the number of cores, 50x the clock speed, 256x the RAM, 1024x the cache. Benchmarks comparing the two are extremely difficult to find, because they're from completely different eras of computing; but I think it's pretty safe to say that your 10x is completely insignificant in comparison. reply jodrellblank 59 minutes agorootparentAn intel chip from 2011: https://ark.intel.com/content/www/us/en/ark/products/52210/i... Equivalent chip today: https://ark.intel.com/content/www/us/en/ark/products/236784/... 4 cores/4 threads up to 14 cores/20 threads. Max memory supported 32GB up to 192GB. 3.7Ghz turbo up to 5Ghz. 6MB cache up to 24MB+11.5MB L2. Memory bandwidth 21GB/sec up to 76GB/sec. AVX2. Faster GPU. It’s not so dramatic but it’s not nothing; 5x the threads, nearly 4x memory bandwidth, 1/3rd higher clock speed, 4x the cache, much higher bus bandwidth I think ~5x? reply adrian_b 5 hours agorootparentprevWhile I agree that the increase in speed per socket was greater in the 15 years between 1992 and 2007 than in the following 15 years from 2007 to 2022, your comparison for the cache size is not correct. A motherboard with a 486 CPU of 1992 would have had an L2 cache memory with a size between 64 kB and 256 kB, made with discrete SRAM chips. During the year 2000, the second generations of Intel Pentium III and of AMD Athlon were the first to have an L2 cache memory integrated in the CPU. When first launched in 1999, both Pentium III and Athlon still had external L2 cache memories. External L2 cache memories had been the norm in all motherboards except in the cheapest models, starting already with 80386DX, before 1990. So the L2 cache memory of 2007 was only around 64 times the size of that of 1992. The increase in IPC (instructions per clock cycle) was huge between 1985 and 1995, i.e. 80386 => 80486 => Pentium => Pentium Pro. After that, the increase in IPC has been continuous until the AMD Zen 5 and Intel Lunar Lake of 2024, but at a much slower pace. From 1995 to 2003, there was a huge increase in clock frequency, from 0.2 GHz to 3.2 GHz, i.e. 16 times, while in the next 20 years the clock speed has increased less than 2 times. From 2005 (AMD dual core) until today the greatest speed increases have been provided by either increasing the number of cores per socket or the width of the SIMD execution units. For consumer CPUs (i.e. non-server) Intel has provided a sequence of throughput doublings in the sequence Core 2 (double SIMD throughput vs. previous Athlon X2) => Nehalem (4 cores/socket) => Sandy Bridge (double SIMD throughput) => Haswell (double SIMD throughput), but after that the following throughput doublings in consumer CPUs have all been provided by AMD, with the increase of the number of cores per socket to 8 then 16, and now with the double width of the SIMD units in the desktop variant of Zen 5, i.e. Granite Ridge. So the increase in throughput per socket (in personal computers) between 2004 and 2024 has been of 256 times, due to increases of core count or SIMD width. For comparison with this 20-year improvement, the increase in clock frequency from 1985 to 2003, during 18 years, had been from 16 MHz to 3.2 GHz, i.e. of 200 times. I do not know the exact increase in IPC between 1985 and 2003, as that would require the choice of a benchmark program, to be run both on an 80386 and on a Pentium 4 or on an Opteron, but it might have been around 20 times. The increase in IPC from 2003 to 2024 might be of at most 6 to 8 times, when accepting an increase of 10% to 20% every 2 to 3 years. So overall, with a doubling of the clock frequency from 2004 to 2024, there would be an increase in the throughput per socket for personal (non-server) computers of around 4000 times both during the 19 years from 1985 to 2004 and during the last 20 years. This corresponds on average to a little more than a doubling of the throughput per socket (in personal computers) every 2 years, during the last 40 years (i.e. from an Intel 80386 @ 16 MHz to an AMD 9950X). reply mort96 3 hours agorootparentThanks, this is good context; I had no idea that the L2 cache used to be on the motherboard. And yeah, my comparison is completely missing IPC, but that's difficult to quantify... ideally we'd have something like Geekbench results from both, but I struggled to find comparable benchmarks. reply SSLy 6 hours agorootparentprevClock speed is irrelevant as a comparison point between highly out of order micro-architectures with execution ports approaching dozen by now. reply adrian_b 4 hours agorootparentFor the throughput of a computer the clock frequency is at least as important as the number of cores, the IPC (instructions per clock cycle) and the amount of work done by one instruction. Were it not for the fact that increasing the clock frequency increases the power consumption more than the throughput, clock frequency would have been the most important factor, because increasing any of the other factors increases the throughput by less than their increment, due to various inefficiencies or because not all applications can benefit from those improvements. For the computer user only the total throughput matters, not how it is achieved. reply mort96 6 hours agorootparentprevCome on. We're not comparing 2.1GHz and 2.4GHz here. We're comparing 66MHz and 3500MHz. That difference is significant regardless of execution ports and other micro-architectural details. I'm not saying that the Core 2 Quad is 50x more powerful because it has 50x the Hz, or that the Core 2 Quad is 200x more powerful because it has 4x the cores @ 50x the Hz, or that it's 1024x more powerful because it has 1024x the cache, or anything like that. I'm trying to illustrate the extreme evolution of the microprocessor from the early 90s to the late '00s. reply accrual 9 hours agoparentprevNice comparison. Indeed, the developer's CPU is about 13 years old. Vista was released internationally in early 2007, so a 13 year old CPU at release would've been released in 1994, about a year after the original Pentium was released. But many were still using their trusty 486 DX2-66 CPUs. Quite impressive that a CPU from 13 years ago can still work on modern projects today when the same wasn't quite as true back then. And a CPU released today will (hopefully) be able to work satisfactorily beyond 2037. 8) reply jeffbee 3 hours agoprevFor whatever its worth, if we wait long enough C++ will include the equivalent of `malloc_good_size`. https://github.com/cplusplus/papers/issues/18 reply Zardoz84 9 hours agoprevHumm... Perhaps it's a good time to ask to stop using Lenna image. She asked to stop using it. reply mort96 8 hours agoparentThat's the first thing I noticed too... but it seems very on-brand for the SerenityOS project. reply sph 3 hours agorootparentAre you guys still going on about that because they don't want to create a political space in their open-source project? Stop putting politics in everything you touch. Once upon a time, before being an activist poseur was in vogue, open source was all about code, not idiotic posturing for terminally online people. Oh, the beautiful, inclusive movement of brigading and \"with us or against us\" black-and-white thinking. Very tolerant indeed. reply c-hendricks 1 hour agorootparentAm I wrong, or is \"stop putting politics in _____\" just people saying \"I will politicize _____\"? reply kmeisthax 2 hours agorootparentprevRMS has been an \"activist poseur\" since the very beginning - i.e. before \"open source\" was even a thing. Do people just never read stallman.org? reply bowsamic 8 hours agorootparentprevYeah that's kind of their thing, right? The brand of the SerenityOS community is a safe haven for developers who don't want to worry about ethical or code of conduct issues. And for the most part it seems to work okay. It's like the stupid line about \"keep politics out of x\" which is of course impossible, but in the case of SerenityOS it kind of works for them. reply mort96 8 hours agorootparentWell the issue is that they seemingly want to be a safe haven for moderate bigots and regressive types as long as they're polite about it, exactly in line with the \"keep politics out of x\" thing. Their definition of something \"political\" and deserving of scorn seems to include something as minor as using gender-neutral pronouns in documentation, while assuming that the reader is male is \"apolitical\" because that's how things used to be. It seems to be a space designed to avoid anything which the typical 90s white male geek type wouldn't perceive as political. Which in itself is a highly political goal. \"Using Lenna is on-brand for SerenityOS\" is not meant as a compliment. reply tredre3 3 hours agorootparent> safe haven for moderate bigots and regressive types That's an interesting interpretation. Considering that the project has several trans developers and even a handful of (biological) women I'm going to suggest another interpretation: They're not trying to be a safe haven for bigots, they just don't want the endless CoC/pronouns bikeshedding that plagues too many projects these days. reply ashleyn 1 hour agorootparentWhen it comes to inclusion, I look at where the rubber actually meets the road. If a project welcomes diverse contributors and doesn't foster an environment that alienates them (which, in part, you can evidence by their continued presence and contributions), that seems way more worth its weight than academic quibbles about the language used in documentation. Are cis and trans women contributors speaking up about any problems? If not, then is the issue really important? I'd rather see a project not actively alienate cis and trans women developers - a higher bar to actively meet than you'd think - than to be religiously attentive to \"microaggressions\" and the like while doing very little to actually accommodate them as contributors. reply bowsamic 7 hours agorootparentprevWell honestly I can't really bring myself to care about it. After all, it's not like we're going into the SerenityOS Discord and seeing Nazis. It's a very slight rebellion, and seems to serve well to defuse tension and actually create a space with less hate than a lot of the spaces which do enforce more modern ethical views. reply mort96 7 hours agorootparentI just find it sad, more than anything. SerenityOS and Ladybird are interesting projects which I wish I could respect. But instead they seem insistent on regressing back to a dark age in tech where casual sexism was (even more?) commonplace. I can't respect that. reply oooiu 7 hours agorootparentprevnext [7 more] [flagged] mort96 7 hours agorootparentWhat you're calling \"nonsense\" is being baseline decent human beings. That's not a headache, but if it was one, it'd be one that's worth the cost. reply oooiu 6 hours agorootparentnext [5 more] [flagged] mort96 6 hours agorootparentAll ethics is subjective (or at best intersubjective), you're not pointing out anything new or interesting there. Considering the concept of non-male users to be \"too controversial\" to accept does not meet my threshold for \"basic human decency\". reply oooiu 6 hours agorootparentnext [4 more] [flagged] mort96 6 hours agorootparentI say \"female\" when I mean female and \"non-male\" when I mean non-male. In this instance I mean non-male. Please spare me the fake outrage over precise terminology. reply oooiu 6 hours agorootparentnext [3 more] [flagged] mort96 6 hours agorootparentI don't understand what you're trying to do. Anyone who could possibly be genuinely convinced by a sexism accusation already knows about the existence of intersex people and non-binary people, so they already know that the term \"non-male\" refers to a different group of people than \"women\" (regardless of whether we're talking sex or gender). This conversation is boring, you're not getting anything out of it, and I'm not getting anything out of it. How about we leave it here? reply oooiu 6 hours agorootparentnext [2 more] [flagged] mort96 6 hours agorootparentCool beans. reply bowsamic 7 hours agorootparentprevIt works for them but I don't think it's appropriate everywhere, and there is a cost, if not immediately certainly in the future reply cedws 7 hours agoparentprevShe was paid for it. reply zogrodea 8 hours agoparentprevThis is very fair, to respect someone's wishes with regards to how their image is used. I'm sad to see this comment (at the time of posting) be downvoted. It doesn't imply guilt on the author of this interesting article or others who used this image without knowing her wishes or anything. I don't understand the response. reply xyst 2 hours agorootparentIt’s probably downvoted because it doesn’t add anything to the debugging story. At some point, I’ll probably watch the “Losing Lena” documentary. But I wasted a few minutes to determine the comment was not related to the story reply userbinator 8 hours agoparentprevIn 2021, she didn't. reply mort96 8 hours agorootparentI don't understand people who enter comment sections just to confidently state something so easily disproven? Losing Lena came out in 2019. reply userbinator 8 hours agorootparentI don't understand people who enter comment sections just to virtue-signal their manufactured outrage. reply mort96 7 hours agorootparentThat's a funny way to write, \"Oh thanks for pointing that out! I should've double-checked my facts before stating them so confidently.\" Regardless, I forgive you. reply asveikau 4 hours agorootparentprevSeems like you may be virtue signaling in the other direction? I hate that term \"virtue signaling\". I most often see it when people simply don't understand the opinions of the other person. \"It's impossible that you disagree with me, so I will accuse you of holding inauthentic positions\". reply bowsamic 8 hours agorootparentprevHow exactly did you decide that the concern was insincere? reply timeon 7 hours agorootparentprevDo you often talk with just buzz words? reply userbinator 9 hours agoprevI got Deja Vu upon seeing \"Alien Lenna\" and sure enough... I've seen and commented on this before: https://news.ycombinator.com/item?id=27374942 (2021) reply riedel 8 hours agoparentIn 2024 should also be grown up enough to not use old Playboy centerfolds to find malloc bugs... reply timschumi 7 hours agorootparentApart from the fact that this probably isn't common knowledge, this article is from 2021 (which the OP failed to disclose). Why not be mad at IEEE for a change? They apparently only managed to ban use of the image in April of 2024. reply cinntaile 8 hours agorootparentprevIt's an image used within imaging since the 70s. It's used because everybody uses it. It being part of an old Playboy centerfold isn't the relevant bit here. reply ck45 7 hours agorootparentLena said “Once upon a time, I was the centerfold of Playboy,” says the former model in the new documentary Losing Lena. “But I retired from modeling a long time ago. It’s time I retired from tech, too.” See https://www.sfgate.com/news/article/How-a-Nude-Playboy-Photo... reply viraptor 6 hours agorootparentprevSometimes we keep doing things because we don't stop and think, but it's good to stop and consider if we should continue or change. \"Because we've always done it that way\" / \"because everyone's doing it\" can mask many nasty things we wouldn't start doing if given a completely fresh context. reply cinntaile 4 hours agorootparentThat's very true, but when looking at all the source materials I don't consider this to be one of those nasty things that absolutely need changing. If I was releasing imaging research or any other kind of publicly accessible data I would not use the image just to avoid the angry mob, but I think this is blown out of proportion. reply f33d5173 6 hours agorootparentprevWhy kill whimsy? Do malloc bugs need especially serious images, like concrete walls or men in business suits? reply kmeisthax 2 hours agorootparentThe Lenna image isn't whimsical, it's a cropped nude photo. The people against it don't want image processing to be boring, they want it to be more inclusive. There are hundreds of other whimsical examples that would not be alienating to a good chunk of women. reply userbinator 8 hours agorootparentprevEnough with that woke crap. reply leononame 8 hours agorootparentAfaik, Lena herself said she'd like her image to stop being used as a test image. And IEEE already retired its use. Even if you think it's woke, there's good reason to respect the model's wish reply userbinator 8 hours agorootparenthttps://www.wired.com/story/finding-lena-the-patron-saint-of... Lena doesn’t harbor any resentment toward Sawchuk and his imitators for how they appropriated her image; the only note of regret she expressed was that she wasn’t better compensated. In her view, the photograph is an immense accomplishment that just happened to take on a life of its own. “I’m really proud of that picture,” she said. reply g15jv2dp 7 hours agorootparentShe later said she wanted people to stop using the picture. https://finchcompany.com/projects/losing-lena-trailer/ https://www.theguardian.com/technology/2024/mar/31/tech-publ... > Forsén herself has also suggested that the photo should be retired. In 2019, she said she was “really proud” of the picture and she re-created the shot for Wired magazine, which called her “the patron saint of JPEGs”. But later that year, the documentary Losing Lena spearheaded the latest effort to encourage computer science to move on. “I retired from modelling a long time ago,” Forsén said on its release. “It’s time I retired from tech, too. We can make a simple change today that creates a lasting change for tomorrow. Let’s commit to losing me.” reply AlexandrB 7 hours agorootparentDoesn't it seem weird that she was proud of the picture when interviewed in a neutral context (Wired), but wanted it removed (not very strongly might I add) when interviewed by documentarians making a film with the express purpose of trying to get her picture removed? In the movie quote she even alludes to the title of the film: \"Let's commit to losing me\". It basically sounds like she gave them the soundbite they wanted. reply cinntaile 5 hours agorootparentImo the reasonable thing to do would be to assign a higher credibility to her opinion in the Wired article higher than her opinion in the activist documentary. reply chrisjj 4 hours agorootparentMore reasonable would be to consider how much each was paying her. reply g15jv2dp 5 hours agorootparentprevWhat's weird here? She can be proud about the picture and think it's a thing of the past that needs to stopped being used. reply hhh 7 hours agorootparentprevhttps://www.sfgate.com/news/article/How-a-Nude-Playboy-Photo... This is what’s cited on Wikipedia about the cessation of use. reply FeepingCreature 7 hours agorootparentprevThe anti-woke, respectful move would be to find another Playboy centerfold. reply Jamie9912 7 hours agorootparentprevNot really, if she understood well what the photograph was being used for at the time, you can't retrospectively wish against it. That's like saying Oh I don't want to be a pornstar anymore, take down all my content thanks. reply nkrisc 7 hours agorootparentThat’s not what she’s saying. It’s a very simple and reasonable request. Choosing to not respect her wish is essentially choosing not to out of spite for her since the effort to respect it is essentially nothing. reply Jamie9912 6 hours agorootparentIt is NOT reasonable by any stretch of the imagination reply biorach 7 hours agorootparentprev> you can't retrospectively wish against it She absolutely can. And we, collectively, can choose to respect that wish by using a different test image in future. And why not? It's no real burden to make the change. reply petee 7 hours agorootparentI agree, being a decent person is an active choice we should all strive for. The burden here is that a number of people are so afraid of being \"woke\" that they'd rather double down being scummy than just find a different jpeg. If it was their daughter I'm sure they'd have a different opinion reply josefx 5 hours agorootparent> If it was their daughter I'm sure they'd have a different opinion Are we back in the 60s where a father has to sign off on the daughters job application? We are talking about a woman who willingly signed up for a playboy photoshoot, had been aware of the image being used and circulated for decades with no issues. reply biorach 3 hours agorootparent> Are we back in the 60s where a father has to sign off on the daughters job application Strawnan bs. No one advocated anything like that. > We are talking about a woman who willingly signed up for a playboy photoshoo Yep. And decades later asked it to not be used anymore. You can waste as much time with long winded arguments as you want. Or you could just be decent and not use the image. Your call. reply josefx 2 hours agorootparent> And decades later asked it to not be used anymore. Then how are her parents even remotely relevant? > You can waste as much time with long winded arguments as you want Brought to you by the people who bring this argument up every time the image is used. reply Jamie9912 6 hours agorootparentprevIt's unreasonable, by principal. Just like how beyonce tried to get her ugly image removed from the internet. reply foldr 1 hour agorootparentI mean, I don't think Beyoncé should have (or does have) any legal recourse in that kind of situation, but publishing unflattering photos of people just to make fun of how they look is a fairly crappy thing to do. The decent thing to do in that situation would be to refrain from publishing the image unless there were public interest grounds for doing so. reply biorach 1 hour agorootparentThe whole dynamic of this discussion is weird. There's a bunch of people coming up with long winded arguments, not-really-relevant examples and other guff. And there's a bunch of us repeatedly saying \"why not just be decent?\" reply petee 7 hours agorootparentprevHow could she possibly have known what the internet would become, or how vast? Nobody could have \"understood\" how their photo could be widely disseminated like today. At the end of the day its a stolen photo, and immoral to continue to use against the express wishes of the subject, no matter how you want to justify it -- she asked, so just respect it instead of finding ways to justify being a jerk. reply Jamie9912 6 hours agorootparentMy understanding is that this photo was consensual and not stolen reply petee 7 hours agorootparentprevNot being an asshole isnt woke: being a decent person is a choice. A human being asked people stop using their photo, so just do it, without mental gymnastics of why their opinion doesn't matter. Let me go dig up a photo of your family thats publicly available, you'd be ok if that becomes the new Lena? reply wwoovvo 7 hours agorootparentnext [4 more] [flagged] smcl 6 hours agorootparentPetee has simply said that we should respect someone’s pretty reasonable preferences. You’ve popped up out of nowhere utterly furious for seemingly very little reason. If this is how you behave I think most of us would far prefer to have Petee as a coworker than you. If you’re having a lot of conflicts with your colleagues about this, the problem might not lie with your colleagues… reply petee 7 hours agorootparentprevCheck the mirror. Nobody forced you to read this; but clearly you have an opinion yet only yours matters. You can happily go back in your bubble and pretend everyone in the world is ok with your level of mediocrity. reply amake 6 hours agorootparentprevYou are the asshole here. reply bowsamic 7 hours agorootparentprevWhat do you mean by that word? Does it mean insincere? EDIT: Why the downvotes? I'm genuinely curious. I see the word thrown around a lot but I can't get a grasp on what it means reply tzot 5 hours agorootparentI assume you were downvoted because downvoters would not believe your question was genuine. It is a fact that in the past groups of people have been ostracized, ignored, paid less, acknowledged less, respected less than today based on their race, gender, sexuality, country, profession etc. This has been raised as an issue and for some years —perhaps decades— a counter-motion has been going on: openly promote/respect/acknowledge people that were previously demoted/non-respected/unacknowledged. The exaggerated examples of these counter-motions are called “woke”. Imagine that we would like to promote the role of ants in the environment because they were largely ignored in the past, so someone makes a movie where an ant beats by sheer physical strength a lion; that would definitely be “woke“. There are cases where people can disagree whether something is “woke”; for example, think a woman who travels in time to a patriarchal society centuries ago where women were considered property and part of the background and yet she acts in an independent, outspoken, audacious way to men around her without anyone punishing her. That could be called “woke”, but it depends on one's sense of exaggeration. Reactions against such exaggerations is called “anti-woke”. A great example IMO of a humorous “anti-woke” statement is the image included in the following link, which is a poster for an imaginary documentary: https://knowyourmeme.com/photos/2440971-netflix reply amake 4 hours agorootparent> The exaggerated examples of these counter-motions are called “woke”. This is your definition. I doubt you will find any agreement on what \"woke\" means, because right-wingers use it to refer to anything and everything that they dislike. reply amelius 2 hours agoprevTL;DR: > Someone used a HashTable to store objects that should be ordered, then iterated over it using the basic HashTable iterator reply selimnairb 7 hours agoprevnext [8 more] [flagged] joshstrange 6 hours agoparentThis [0] is probably what you’re thinking of. I thought the same thing when I saw the image but I wasn’t remembering it quite right. That said, it’s clear the model herself would prefer not to be used anymore and there isn’t anything special about the image so I don’t see why we shouldn’t respect her wishes. EDIT: Someone else in the thread said this is from 2021 but I can’t tell since neither the URL nor the page itself give a date. I’ll never understand people hiding/not showing the publish date on blog posts. [0] https://news.ycombinator.com/item?id=39885977 reply layer8 6 hours agorootparent> Someone else in the thread said this is from 2021 but I can’t tell since neither the URL nor the page itself give a date. The Git commits in the article indicate the date. reply joshstrange 6 hours agorootparentAhhh, duh. Thank you for pointing that out. My eyes jumped right over that and I was looking for metadata at the top/bottom of the post. reply layer8 6 hours agorootparentI often Ctrl+F for 20[012] or 19[9] when looking for a date. :) reply joshstrange 6 hours agorootparentThat’s a good trick, I might have considered that if I wasn’t on mobile. On desktop “Find in page” is second nature to me but is such a slog on mobile. reply layer8 6 hours agorootparentYeah, it’s a bit of a pain on mobile. reply trallnag 5 hours agoparentprevDefine \"people\" reply Umayanga 3 hours agoprev [–] No best it is me best you are interesting way not in the calss room way this way are reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "A bug in SerenityOS's JPG image decoding caused incorrect color display due to an RGB vs. BGR confusion.",
      "The issue was traced back to a commit implementing `malloc_good_size()`, which altered memory allocation and affected data structures like `HashTable` and `Vector` used in `JPGLoader`.",
      "The fix involved ensuring deterministic iteration over components in `JPGLoader`, highlighting how simple changes can uncover significant underlying issues."
    ],
    "commentSummary": [
      "Some hash table implementations introduce randomness to prevent reliance on element order and enhance security, while modern implementations often guarantee iteration in insertion order.",
      "JavaScript and Python have standardized insertion order in their hash table implementations, with Python adopting this in version 3.7.",
      "Rust offers different hash table options (`HashMap`, `BTreeMap`, `IndexMap`), and Perl introduced random seeds for hash tables post version 5.18."
    ],
    "points": 177,
    "commentCount": 143,
    "retryCount": 0,
    "time": 1720341723
  },
  {
    "id": 40895672,
    "title": "BeaconDB – An Alternative to Mozilla Location Services",
    "originLink": "https://beacondb.net/",
    "originBody": "beaconDB public domain wireless geolocation database 177,176 networks 4,126 beacons 6,106 towers 30 countries ethically sourced: opt-in only data collection privacy friendly: published information is obfuscated to protect transmitters and contributors abuse resistant: updating existing data requires information only available in physical range of a beacon contribute beaconDB has recently started to accept submissions. to add coverage for your area you can use the following apps on your phone: NeoStumbler: supports cell towers, wifi networks and bluetooth devices download on F-Droid in the Settings tab, set the endpoint to https://beacondb.net Tower Collector: only supports cell towers download on F-Droid or Google Play in Upload Preferences, enable support for custom MLS services and set the address to https://beacondb.net/v2/geosubmit data you submit will be aggregated and shared under a public domain license. for more information on how your data is handled, see the privacy notice. usage beaconDB is experimental and should not be used by critical services if you own an Android phone running the latest preview version of microG, you can easily give beaconDB a spin without needing to install anything. in microG Settings on the Location page, pressing the three dots in the top right lets you set a custom service URL. you can set this to https://beacondb.net/ to give beaconDB a try. as beaconDB is starting from scratch there is likely no wifi coverage for your area. if beaconDB can't estimate your location using wifi, it will fallback to an approximate cell tower location sourced from MLS' final data dump. note that submissions will take at least 5 minutes to become available in the beaconDB API. developers beaconDB hosts an endpoint at https://beacondb.net/v1/geolocate which is compatible with Ichnaea's request format. if your software has a large amount of users, please don't use this as a default location service. beaconDB infrastructure is not yet capable of handling a large amount of requests. data dumps are currently not available as I'm still researching the measures I need to take to protect the privacy of both contributors and AP owners. source code on Codeberg chat on Matrix and IRC bug tracker privacy notice made by Joel Koen",
    "commentLink": "https://news.ycombinator.com/item?id=40895672",
    "commentBody": "BeaconDB – An Alternative to Mozilla Location Services (beacondb.net)170 points by joelkoen 12 hours agohidepastfavorite42 comments cimm 9 hours ago> ethically sourced: opt-in only data collection Good on them but how does this work? If my neighbour scans my WiFi network and uploads it to BeaconDB I didn’t exactly opt-in, did I? The privacy policy mentions you can add ‘_optout’ to the WiFi name, so it’s more opt-out instead of opt-in? reply joelkoen 9 hours agoparentThis line refers to opting in to using your device to collect this data. Apple and Google are taking advantage of their global user coverage by using their devices to collect this data without their consent. Your WiFi network is broadcasting its presence 10 times a second in all directions. It is well known that you should not put sensitive information in your network SSID, for example, as anybody nearby can pick that up. Hence, you can opt out here instead. reply FireInsight 9 hours agoparentprevThe person collecting the data opted in to doing it, heh. As far as the data collectors are concerned, your wifi is out in the public. reply fc_on_hn 9 hours agoparentprev> If my neighbour scans my WiFi network and uploads it to BeaconDB I didn’t exactly opt-in, did I? To clarify: all phones doing geolocation are already uploading your AP macaddr to remote location services, but BeaconDB will *not* publish this information in cleartext. Any data dump will contain only non-reversible cryptographically hashed data or aggregated data. reply kevincox 8 hours agorootparentA MAC address is only 48 bits and some of the bits are restricted. It is well within the range of brute force to reverse all of the hashes. reply joelkoen 8 hours agorootparentYou can truncate the hash to cause collisions, meaning that one MAC address does not map to one location. This requires the client to be aware of multiple physically nearby MACs in order to get a location, as it then needs to estimate which \"possible\" locations are most likely. This is a really interesting problem, and I've loved thinking about it recently. If you're keen on it too I'm happy to discuss further, feel free to reach out. reply userbinator 8 hours agorootparentprevTo put that into perspective, 48 bits is 256T, which is roughly the number of bits in a 32TB hard drive. reply landdownsundar 8 hours agorootparentprevAbsolutely right, great point. That's why I only use Windows addresses now. Can't break those with brute force! reply petre 4 hours agoparentprevYou can opt to hide your SSID and use 5GHz WiFi which doesn't reach too far, gets attenuated through walls, so it's basically kind of useless as a geolocation beacon. reply dangoodmanUT 7 hours agoprevThe author doesn't seem to have an open source mobile app or anything that would allow them to source the data from devices themselves. I'm curious where the data was collected from, esp. if it was opt-in (at the collecting device) reply joelkoen 7 hours agoparentI haven't built any apps for contributing to beaconDB as of yet. The website links to NeoStumbler and TowerCollector, which are Android apps that can be used to collect this data. reply luuurker 6 hours agorootparent> TowerCollector The developer might be open to add other services since MLS is being retired: https://github.com/zamojski/TowerCollector/issues/223 Doesn't hurt to contact them/make suggestions on this issue. reply joelkoen 6 hours agorootparentJust commented on that issue, thanks! reply dangoodmanUT 7 hours agorootparentprevThanks, based on the copy I thought it was recently opened to contribution, and the original dataset had come from somewhere else. reply dangoodmanUT 7 hours agorootparentI am curious what would cause such a distributed user base to contribute to this though? reply joelkoen 7 hours agorootparentDistributed referring to the community not yet recognising one specific software as \"the go to\"? Or distributed physically? reply dangoodmanUT 6 hours agorootparentPhysically! Like how so many users from all over the place decided to contribute to this reply joelkoen 6 hours agorootparentIt is rather surprising how many people have started contributing already. I believe that people want to support alternatives to big tech so they aren't completely reliant on these providers, and beaconDB is currently the only database not owned by big tech. Not 100% sure that answers your question :) reply dangoodmanUT 6 hours agorootparentGotcha, I guess I was asking whether people specifically opted in to contributing to beaconDB, sounds like that's the case reply denysvitali 9 hours agoprevLast time I looked into something like this for GrapheneOS it wasn't possible to provide a custom location service. It would be awesome to have this on GrapheneOS - so I'm very happy if someone knows a way to do this without using microG (I use the sandboxed GMS) reply a2800276 11 hours agoprevWasn't the main issue with MLS that they got patent trolled/sued by Skyhook? Anyone know the patents involved and how beacon DB is avoiding the issues? reply k__ 7 hours agoprevIs there a reason the API doesn't return the locations of the access points so the clients can calculate their positions by themselves? reply joelkoen 7 hours agoparentThis is planned to help clients cache data locally, which would improve the privacy of the client and reduce server load. I would like to implement this over the next few days. I have not yet found any clients that have implemented making use of such data, please let me know if you have found one or are developing one. reply k__ 7 hours agorootparentAh, okay. I was just thinking if there were any technical constraints preventing this. Because you mention Ichnaea API compatibility, and I didn't know if that spec even allows that. reply yxOverKill 8 hours agoprevThis is such a cool project. Always glad to see problem solvers filling the void left by MLS. (Unrelated, but the design looks great!) reply joelkoen 7 hours agoparentThank you, this means a lot! reply FireInsight 11 hours agoprevReading the MLS retirement issue[1] it seems that multiple established organizations (e foundation, Graphene) are also interested in providing an alternative service. Does this mean that we're now in a situation where multiple open source location service providers are competing, or is this the only publicly accessible project in this space for now? This project is cool and all, but seems to just be a one person effort with not a lot of engagement on GitHub[2]. Are you in talks with other people with similar goals to expand and collaborate on the project? Having the backing of an existing developer community could really bring this to the next level. 1) https://github.com/mozilla/ichnaea/issues/2065 2) https://github.com/beacondb/beacondb Edit: the actual project seems to be on Codeberg[3], where there is a bit more engagement from others than the primary dev. 3) https://codeberg.org/beacondb reply joelkoen 10 hours agoparentbeaconDB is currently the only publicly accessible project, but I am currently discussing working together with various other projects and organisations. The project was originally on GitHub, but it has now moved to Codeberg. reply jrexilius 4 hours agorootparentHow is this different from WiGLE? reply dangoodmanUT 7 hours agoprevIs this only offered as an API? E.g. you can't dump it and analyze locally? reply dangoodmanUT 7 hours agoparent> data dumps are currently not available as I'm still researching the measures I need to take to protect the privacy of both contributors and AP owners. Ah reply joelkoen 7 hours agorootparentYes, I really want to be able to release data dumps as this opens up a lot of great opportunities. I'm also worried that people may have lost trust in a service like MLS now that it has shutdown and abandoned all of the data contributors had collected. At the moment, there simply isn't enough data to anonymise contributions. reply disparate4927 8 hours agoprevReally nice, hopefully more software switches to this, I'm 100% gonna contribute reply jacooper 4 hours agoprevHope GrapheneOS adds support for this soon, as currently their non-Google GPS Provider is basically hopeless unless you are outside. reply acheong08 11 hours agoprevApple probably has one of the largest databases. Their API is unauthenticated and not rate limited either. Can be used for both APs and cell towers: https://github.com/acheong08/apple-corelocation-experiments reply saagarjha 10 hours agoparentSoon: their API is authenticated and rate limited reply acheong08 8 hours agorootparentThat’ll break a lot of older devices. Unlikely reply jacooper 4 hours agorootparentIt's a Metter of time, they are waiting till these devices reach EOL. reply chaz6 5 hours agoprev [–] As nobody has yet mentioned it, there is also WiGLE [1] which has tracked over a billion unique networks. [1] https://wigle.net/ reply jrexilius 4 hours agoparent [–] I was just going to ask, what ever happened to WiGLE and why build a clone of it rather than add to it? reply acheong08 4 hours agorootparent [–] WiGLE severely rate limits their APIs and don’t even allow normal people to pay for more access. They refuse to provide a data dump since they sell it for enterprise. No academic access either. People literally spend their time mapping APs and they don’t even get anything in return reply iJohnDoe 2 hours agorootparent [–] The couple of times I did a lookup it was woefully outdated as well. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "beaconDB is a public domain wireless geolocation database covering 177,176 networks, 4,126 beacons, 6,106 towers across 30 countries, with ethically sourced, opt-in data collection.",
      "The database is privacy-friendly, obfuscating information to protect transmitters and contributors, and requires physical proximity to update data, making it abuse-resistant.",
      "Developers can use the endpoint `https://beacondb.net/v1/geolocate`, compatible with Ichnaea's request format, but it's not suitable for high-traffic applications; source code is available on Codeberg."
    ],
    "commentSummary": [
      "BeaconDB is presented as an alternative to Mozilla Location Services (MLS), focusing on ethically sourced, opt-in data collection.",
      "Unlike Apple and Google, BeaconDB does not publish cleartext MAC addresses, instead using hashed or aggregated data to enhance privacy.",
      "BeaconDB is the only non-big tech database for location services, aiming to provide an open-source solution and discussing potential collaborations."
    ],
    "points": 170,
    "commentCount": 42,
    "retryCount": 0,
    "time": 1720333550
  },
  {
    "id": 40897506,
    "title": "A reawakening of systems programming meetups",
    "originLink": "https://notes.eatonphil.com/2024-07-07-systems-meetups.html",
    "originBody": "Home Notes Popular RSS Subscribe July 7, 2024 A reawakening of systems programming meetups talks This year has seen a resurgence in really high quality systems programming meetups. Munich Database Meetup, Berlin Systems Group, SF Distributed Systems Meetup, NYC Systems, Bengaluru Systems, to name a few. This post summarizes a bit of disappointing recent tech meetup history, the new trend of excellent systems programming meetups, and ends with some encouragement and guidance for running your own systems programming events. I will be a little critical in this post but I want to preface by saying: organizing meetups is really tough! It takes a lot of work and I have a huge amount of respect for meetup organizers even when their meetup style did not resonate with me. Although much of this post talks about NYC Systems, the reason I think this post is worth writing is because so many other meetups in a similar vein popped up. I hope to encourage these other meetups and to encourage folks in other major metros (London, for example) to start similar meetups. Meetups I used to attend a bunch of meetups before the pandemic. But I quickly got disillusioned. Almost every meetup was varying degrees of startups pitching their product. The last straw for me was sitting through a talk at a JavaScript meetup that was by a devrel employee of a startup who literally gave a tutorial for their product. There were also some pretty intelligent meetups like the New York Haskell Users Group and the New York Emacs Meetup. But not being an expert in either domain, and the attendees almost solely appearing to be experts, I didn't particularly enjoy going. There were a couple of meetups that felt inclusive for various skill-levels of attendees yet still went into interesting depth. Specifically, New York Linux User Group and Papers We Love NYC. These meetups were exceptional because they were language- and framework-agnostic, they would start broad to give you background, but then go deep into a topic. Maybe you only understand 50% of what was covered. But you get exposed to something new from an expert in a domain. Unfortunately, the pandemic happened and these two excellent meetups basically have not come back. A couple of students in Munich The pandemic ended and I tried a couple of meetups I thought might be better quality. Rust and Go. But they weren't much better than I remembered. People would give a high level talk and brush over all the interesting concepts. I had been thinking of doing an in-person talk series since 2022. If I put together a systems/databases/distributed systems meetup in NYC (a physical meetup, not Zoom), who'd be interested (in attending, or presenting, or helping me organize, or donating space)? No promises! — Phil Eaton (@eatonphil) September 27, 2022 But I was busy with TigerBeetle until December of 2023 when I was messaged on LinkedIn by Georg Kreuzmayr, a graduate student at Technical University of Munich (TUM). Georg and his friends, fellow graduate students at TUM, started a database club: TUMuchData. We got to talking about opportunities for collaboration and I started feeling a bit embarrassed that a graduate student had more guts than I had to get back onto the meetup organizer wagon. A week later, with assurance from Justin Jaffray that at least he would show up with me if no one else did, I started the NYC Systems Coffee Club to bring together folks in NYC interested in any topic of systems programming (e.g. compilers, databases, web browser internals, distributed systems, formal methods, etc.). To bring them together in a completely informal setting for coffee at 9am in the morning in a public space in midtown Manhattan. Trying something new! If you're a dev in NYC working on (or interested in) systems programming, grab a coffee and come hang out at 1 Bryant Park (indoor space) this Thursday 9AM - 9:30AM. See post for details and fill out the Google Form or DM me!https://t.co/A4bzcPGy6x pic.twitter.com/n1ECMd59ev — Phil Eaton (@eatonphil) December 11, 2023 I set up that linked web page and started collecting subscribers to the club via Google Form. Once a month I'd send an email out to the list asking for RSVPs to this month's coffee club. The first 20 to respond would get a calendar invite. And about the same time I started asking around on Twitter/LinkedIn if someone would be interested in co-organizing a new systems programming meetup in NYC. Angelo Saraceno immediately took me up on the idea and we met up. NYC Systems We agreed on the premise: this would be a language- and framework-agnostic meetup that was focused on engineering challenges, not product pitches. It would be 100% for the sake of corporate marketing, but corporate marketing of the engineering team, not the product. NYC Systems was born! We'd find speakers who could start broad and dive deep into some interesting aspect of databases, programming languages, distributed systems, and so on. Product pitches were necessary to establish a context, but the focus of the talk would be about some interesting recent technical challenge and how they dealt with it. We'd schedule talks only every other month to ease our own burden in organizing and finding great speakers. Once Angelo and I had decided to go forward, the next two challenges were finding speakers and finding a venue. Thanks to Twitter and LinkedIn, finding speakers turned out to be the easy part. It was harder to find a venue. It was surprisingly challenging to find a company in NYC with a shared vision that the important thing about being associated with a meetup like this is to be associated with the quality of speakers and audience we can bring in by not allowing transparent product pitches. Almost every company in Manhattan with space we spoke with had a requirement that they have their own speaker each night. That seemed like a bad idea. I think it was especially challenging to find a company willing to relax about branding requirements like this because we were a new meetup. It was pretty frustrating not to find a sympathetic company with space in Manhattan. And the only reason we didn't give up was because Angelo was so adament that this kind of meetup actually happen. It's always best to start something new with someone else for this exact reason. You can keep each other going. In the end we went with the only meetup that did not insist on their own speaker or their own branding. A Brooklyn-based company whose CEO immediately got in touch with me that they wanted to host us, Trail of Bits. How it works To keep things easy, I set up a web page on my personal site with information about the meetup. (Eventually we moved this to nycsystems.xyz.) I set up a Google Form to collect emails for a mailing list. And we started posting about the group on Twitter and LinkedIn. Very pleased to share the first NYC Systems Talks are taking place next Thursday Feb 22nd 6PM. Hosted by @trailofbits, with @paulgb and @StefanKarpinski speaking. Space is not infinite, fill out the Google Form if you can attend and would like an invite!https://t.co/jNssr5v1kJ — Phil Eaton (@eatonphil) February 15, 2024 We published the event calendar in advance (an HTML table on the website) and announced each event's speakers a week in advance of the event. I'd send another Google Form to the mailing list taking RSVPs for the night. The first 60 people to respond got a Google Calendar invite. It's a bit of work, sure, but I'd do anything to avoid Meetup.com. It is interesting to see every new systems programming meetup also not pick Meetup.com. The only one that went with it, Munich Database Meetup, is a revival of an existing group, the Munich NoSQL Meetup and presumably they didn't want to give up their subscribers. Though most others use lu.ma. The mailing list is now about 400+ people. And in each event RSVP we have a wait list of 20-30 people. Of course although 60 people say Yes initially, by the time of the event we have typically gotten about 50 people in attendance. At each event, Trail of Bits provided screens, chairs, food, and drink. Angelo had recording equipment so he took over audio/video capturing (and later editing and publishing). After each event we'd publish talk videos to our @NYCSystems Youtube. Network effects In March 2024, the TUMuchData folks joined Alex Petrov's Munich NoSQL Meetup to form the Munich Database Meetup. In May, Kaivalya Apte and Manish Gill started the Berlin Systems Group, inspired by Alex and the Munich Database Meetup. I want to start a Berlin Database/Storage systems group, where we have regular meetups, discussions and talks. WDYT? @mgill25 @mehd_io @ClickHouseDB @SnowflakeDB @awscloud @GoogleDE @TUBerlin Can I get some support? Who else would be interested? #Databases Thanks… — Kaivalya Apte - The Geek Narrator (@thegeeknarrator) May 15, 2024 In May 2024, two PhD students in the San Francisco Bay Area, Shadaj Laddad and Conor Power, started the SF Distributed Systems meetup. We’re super excited to be organizing a new SF Distributed Systems meetup NEXT WEEK! Our first meetup features @julianhyde and @conor_power23 presenting work on extending SQL and applying algebraic properties, sign up at https://t.co/d2lLDaQ5iJ — Shadaj Laddad (@ShadajL) May 15, 2024 And in July 2024, Shraddha Agrawal, Anirudh Rowjee and friends kicked off the first Bengaluru Systems Meetup. Are you ready, Systems Enthusiasts of Bengaluru? Speaking at our first-ever meetup on 6th July, we have:@simsimsandy with \"Learn about the systems that power GenAI applications\" and @vivekgalatage with \"The Browser Backstage: Performance vs Security\" (talks linked below!) — Bengaluru Systems Meetup (@BengaluruSys) July 4, 2024 Suggestions First off, don't pay for anything yourself. Find a company who will host. At the same time, don't feel the need to give in too much to the demands of the company. I'd be happy to help you think through how to talk about the event with companies. It is mutually beneficial for them to get to give a 5-minute hiring/product pitch and not need to do extensive branding nor to give a 30-minute product tutorial. Second, keep a bit of pressure on speakers to not do an overview talk and not to do a product pitch. Suggest that they tell the story of some interesting recent bug or interesting recent feature. What happened? Why was it hard? What did you learn? Focusing on these types of talks will help you get a really interesting audience. I have been continuously surprised and impressed at the folks who show up for NYC Systems. It's a mix of technical founders in the systems space, pretty experienced developers in the systems space, graduate students, and developers of all sorts. I am certain we can only get these kinds of folks to show up because we avoid product pitch-type talks. Third, finding speakers is still hard! The best approach so far has been to individually message folks in industry and academia who hang out on Twitter. Sending out a public call is easy but doesn't often pan out. So keep an eye on interesting companies in the area. Another avenue I've been thinking about is messaging VC connections to ask them if they know any engineers/technical founders/CTOs in the area who could give an interesting technical talk. Fourth, speak with other organizers! I finally met Alex Petrov in person last month and we had a great time talking about the challenges and joys of organizing really high quality meetups. I'm always happy to chat, DMs are open. New post telling a bit of the history behind https://t.co/NEh1tm8v3Q; why it only exists due to folks like @georg_kreuzmayr and @ngeloxyz; the explosion of systems meetups around the world; and encouragement and suggestions for future organizers!https://t.co/dwe4TtmXKK pic.twitter.com/ZMLkVYdZDJ — Phil Eaton (@eatonphil) July 7, 2024 Feedback As always, please email or tweet me with questions, corrections, or ideas! Subscribe Enter your email if you'd like to be kept in the loop about future articles! You can expect 2 to 4 messages per month depending on how motivated I'm feeling. :) Cheers, Phil Subscribe",
    "commentLink": "https://news.ycombinator.com/item?id=40897506",
    "commentBody": "A reawakening of systems programming meetups (eatonphil.com)164 points by paulgb 5 hours agohidepastfavorite65 comments __turbobrew__ 2 hours agoI was on the board of directors for a local Linux Users Group and can echo the difficulties in finding a venue. In a city full of empty office spaces on a weekday night it was virtually impossible to find a stable meeting location. Without a stable meeting location you are much less likely to establish a core group since people do not like to have to learn how to get to different venues. Mozilla hosted us at one point but the political tides changed there and we got kicked out. I think having strong buy in from someone in the C-suite of a company is the only real way to do it. Otherwise you are beholden to the changing winds of the company. In the end we are now in a stable location in the local library, but only because we had someone on the inside. Previously when we approached the library we were stone walled without someone to grease the wheels. I do believe that it should be a mandate for the municipality to provide meeting space for local non-profit and special interest groups. In many locations you can no longer pool some money with your buddies to buy a piece of land to build your clubhouse on. In my city there are many such clubs which were formed 50 years ago — the sailing club, the badminton club, the lawn bowling club, etc — but land is too scarce in many places now and we need the municipalities to pick up the slack. reply floren 53 minutes agoparent> In the end we are now in a stable location in the local library, but only because we had someone on the inside. Previously when we approached the library we were stone walled without someone to grease the wheels. I ran into this sort of problem when trying to put together an explicitly non-profit tech meetup (talk about your projects, nothing commercial allowed, no cost to attend) at my local library; because I wasn't a \"community group\" but rather just somebody trying to assemble a loose group of individuals from around the Bay, they wouldn't let me do it. reply packetlost 1 hour agoparentprev> I do believe that it should be a mandate for the municipality to provide meeting space for local non-profit and special interest groups. I never realized this was something that was needed so bad, but yeah. In my city we have a bunch of privately owned venues that you can rent, but the cost ranges from reasonable to very not reasonable. To be fair, a lot of my local technical meetups just hang out at one of the quieter restaurant/bars in the area and it seems to work ok for them. The downside is you don't have any way to present anything. Another group I regularly go to is a local DefCon chapter that rents out space above a \"barcade\" (arcade + bar) and aside from being a bit hot in the summer, the vibes are great and the community doubly so. reply bonestamp2 1 hour agorootparentI know some groups meet at a city library (many of them have meeting rooms that can be booked for free or for a small fee). We had this same problem and ended up starting a (non-profit) makerspace where we now host several meetups. It's win/win really. We are all about building community and bringing people together with similar interests is a great way to do that. It has also helped us get more paying members. reply Aeolun 40 minutes agoparentprev> I do believe that it should be a mandate for the municipality to provide meeting space for local non-profit and special interest groups. Japan has these. There’s local community centers basically everywhere which can be used as a meeting space for a token fee (I think it was like $5/hour last I checked). My local center in the middle of Tokyo is just three floors of rooms that people can use for whatever they need. reply citizenpaul 1 hour agoparentprevWith GPS its not so much the location but thr change in time commitment. The new location is never 5 min from the preious but 30-45 min difference. People in these groups often have busy lives so if your 3hrs suddenly becomes 4hrs its simply less of a headache to cut the unnecessary thing rather than change 50 other things in your schedule. Eventually this scheduking effect shakes off all thr people in the group with other time commitments. reply theamk 1 hour agoparentprevThat's what \"community centers\" are for! A town next to mine has one of those, and any town resident can rent a room for a reasonable amount of money ($20 to $70/hour depending on room size). So as long as at least one group member is from the right town, the room is guaranteed. reply floren 51 minutes agorootparentBut unless you're going to ask for contributions, this means the organizers are footing the bill every time -- and in my opinion, once you start collecting money you've doubled the complexity. reply jazzyjackson 37 minutes agorootparentprevBut can you really guarantee it at the same time every week? reply neilv 37 minutes agoparentprevEven when a company offers meeting space, they can be overbearing about it. I was involved a tiny bit in reviving a local interest group of high-powered programmers. Another person, who did all the actual hard work, of meeting organizing, got meeting space at a Big Tech company... Unfortunately, as a Big Tech (and in the surveillance capialism space), it was complete with signing in with security in the lobby, getting a guest badge for the visit, going to whatever presumable compartmentalized area, and the company presumably doing all the snooping things they could think of... Every time I considered going to a meeting, the idea of going out of my way to be Big Tech's b-word, before I even got into the meeting, was a turn-off. I like your community centers idea. The public libraries here have some meeting rooms. Sometimes someone at a university department can open up meeting space to the public. Someone else mentioned makerspaces, which is an idea I hadn't thought of, but maybe complementary to makerspaces, as well as great marketing for adding members. reply aijfedklmnop 1 hour agoparentprevI wouldn't shy away from some exclusivity in order to keep the venue size reasonable. reply Aurornis 4 hours agoprev> I used to attend a bunch of meetups before the pandemic. But I quickly got disillusioned. Almost every meetup was varying degrees of startups pitching their product. The last straw for me was sitting through a talk at a JavaScript meetup that was by a devrel employee of a startup who literally gave a tutorial for their product. My favorite local meetups fell to problems like this. It was easy to filter out the DevRel people trying to advertise. The hard problem was filtering out people who were only interested in presenting something so they could show another presentation on their resume or personal brand website, with no interest in engaging with the meetup. These people would want to only show up for the one meetup where they got to present. They’d present some simple content designed to make them look good, with little regard to educating or discussing things. They’d often have some excuse for needing to leave quickly after presenting, some times before any QA time. And they always needed a video recording of themselves speaking. For a while we had these to stream to remote viewers, but if the camera gear wasn’t available they’d panic and spend a lot of time improvising a way to record it with their phone even if delayed the presentation. Getting the recording of themselves speaking was the primary goal, not actually speaking to the group. After this happens enough times, the core members realize they’re being used as audience props for someone’s career advancement and they stop coming. The meetup collapses. I hope my local meetup groups can have a little resurgence like this where people are primarily interested in the meetup, not the self promotion opportunity. reply ssivark 1 hour agoparentThere seem to be a few obvious tweaks to try: 1. (Apart from the first few meetings, for bootstrapping) presenters must have attended a few meetings before they get a chance to present. 2. No mentions of one's employer / product beyond the intro slide. There's nothing special about these two suggestions; the general idea is that the organizers ought to be sensitive to what is happening, and responsive in trying to maintain a healthy atmosphere. Someone trying to organize a meetup might find some helpful ideas in the following book: https://www.goodreads.com/book/show/49766350-get-together reply eatonphil 39 minutes agoparentprevI get what you, and other commenters here, are going for. But getting speakers is still hard in the first place. And as I mentioned in the post, I see high quality talks as 100% marketing. They're just marketing for your engineering team, rather than for your product. I think we'd have trouble attracting the high quality speakers we do today if we set restrictions some commenters have suggested such as attending the meetup multiple times, not mentioning the company, etc. We're pretty picky with speakers we invite but also quite flexible once we've invited someone we think will be good. I think it's worked out pretty well for the audience and speakers. At least, that is what they all tell me. reply danenania 22 minutes agorootparentI like your approach and think it sounds very balanced. Creating and preparing for a high quality talk is quite a lot of work, so it’s maybe a bit unfair to expect people with jobs or startups to do them for purely altruistic reasons. A talk that is mostly substance but contains a quick plug here and there has always been reasonable to me. It’s also a lot more effective for marketing than just giving a blatant product pitch, so it should be a win-win. reply atmosx 3 hours agoparentprevCommunities without some kind of curation don't survive in the long run. That should be, historically at least, clear by now. reply bonestamp2 1 hour agoparentprevIt can be a chicken/egg problem. If you run a meetup, you need to meet regularly to create a community and keep members engaged. If you want to meet relatively regularly then you need something to meet about. Sometimes it's hard to get someone to present, so you have to fallback to people who need more incentive to present. In a perfect world, a great sponsor would provide a free space for everyone to meet and great community members would spend hours preparing amazing content to teach the community something without gaining much for themselves. But, that combination is pretty rare these days unfortunately. reply 1oooqooq 2 hours agoparentprevthis is a problem of the job market. most celebrated talks are from people well employed for life so they can talk shit at some system or protocol. from upnp to the persistent compiler worm talks. those folks didn't expect to depend on their resume padding every 3 to 8 months to live. likewise the people organizing the talks didn't depends on the status sending devRel people to expect sponsors. this is exactly what doctorow is yapping about lately. but everyone only accept small points of the big picture. reply tombert 3 hours agoparentprevYeah, it annoys me that a lot of functional programming stuff kind of fell into cryptocurrency startups. There definitely is interesting enough tech behind crypto, but a lot of these talks end up kind of devolving into why their product is amazing and better than all the other coins and why you should invest in their ICO. reply cachvico 3 hours agorootparentHaskell? reply tombert 1 hour agorootparentLast meetup I saw it was an OCaml meetup. reply hiAndrewQuinn 3 hours agorootparentprevCardano? reply griftrejection 2 hours agoparentprevThe solution is simple: vet the talks, vet the presenter, and don't focus on technology that is popular for grifting. Sorry, but I've been to enough JS/TS meetups to know what's happening. But that world is full of marketing and people who want funding. Compare this to a BSD meetup or a 2600 meetup. Nobody's trying to sell you stuff there. reply zeristor 6 minutes agoprevIn London there was Skillsmatter's Code Node. Skillsmatter had trouble getting continued funding, however CodeNode itself is used occasionally for some meeting tech things apparently, but not the continualy hosting 5 days a week of tech meetups it was so loved for. reply toyg 2 hours agoprev> although 60 people say Yes initially, by the time of the event we have typically gotten about 50 people in attendance That's actually pretty good. Most free events (of any kind) will typically see attendance rates around 30-50% of RSVPs. reply screye 4 hours agoprevI'm surprised that universities don't open their doors to such events for everyone. 'Systems' is fairly industry-focused. So, academia-industry partnerships on seminars seems like a great idea. It's a shame (and imo, mind boggling) that SF proper doesn't have a tier 1 university. While Stanford and Berkeley are very close, the lack of a grounding institution makes SF culture feel 'dispersed'. reply azinman2 2 hours agoparentIt does have UCSF which is tier 1; it’s just medically focused. reply weinzierl 4 hours agoparentprevThe mentioned TUMuchdata is held at an university. I think a lot depends on the individuals behind such activities. I don't know anyone behind TUMuchdata and had no contact to the department for years but the chair is still the same as when I studied there and I think he might be exactly the person that would permit or even foster such activities. reply Aurornis 4 hours agoparentprev> I'm surprised that universities don't open their doors to such events for everyone You have to be very careful about letting people use your facilities to host events. Unfortunately, grifters will use any opportunity to do something on a university campus as a way to imply they are associated with the university. Even self-help author Tim Ferriss recommended this trick in his “Four Hour Work Week” book: He advised using university campuses to speak so you could leverage their credibility for your brand (I can’t remember the exact details) It became such a problem that universities really can’t risk letting random groups come use their facilities. It doesn’t take long before someone abuses it to say they “Lectured at ” or “Gave a speech at ” reply weinzierl 3 hours agorootparentThe mentioned TUMuchdata requires a signed form, in German, handed over in person (not email) to join their community. I think this is not required to just attend one of their events. The events are at the university campus which is quite off, and makes it relatively inconvenient to get there if you are not at the university anyway. So, I guess they found a viable filter. reply pointy_hat 25 minutes agorootparentYou can attend all events without signing a form, I’m pretty sure no one would also mind if you volunteer without being a member. The form is only a thing in case you’d like to support a registered nonprofit and has no influence on your participation. (EDIT) there are also events in the city, Munich Database Meetup, which are less frequent. reply Aurornis 3 hours agorootparentprevMy closest community college allows venue rentals. The filter is that few people think it’s prestigious to speak at the local community college. reply II2II 1 hour agorootparentI work at a government facility that offers venue rentals. The rental contract states the facility name must not be used in order to avoid the scenario where someone tries to make a private event sound like it is endorsed by the government. Oddly enough, the facility name gives the impression it is affiliated with an unrelated institution. This results in interesting conversations where one starts by dissuading grifting based upon the implied affiliation, and ends by telling them they are barking up the wrong tree when they try grifting based upon the actual affiliation. If these people did not exist, many institutions would be far more generous with their space. That is especially true of colleges and universities, where there is often a strong desire to foster positive relationships with the community. reply WanderPanda 3 hours agorootparentprevAlso that level of hustle is not (yet) common in Germany I believe reply musicale 3 hours agorootparentprevMany US universities seem happy to rent out their facilities over the summer, but you have to pay. reply mechanicker 4 hours agoparentprevWould love UC Berkeley to revive innovation and collaboration as seen during the BSD days. reply musicale 3 hours agorootparentSo RISC-V isn't enough for you? ;-) More seriously, it does seem like there were a number of interesting systems research and development collaborations in the 1980s: BSD at Berkeley, Athena at MIT, Andrew at CMU, etc. Currently it seems like the interest, funding, opportunities, and incentives for academic researchers are largely for short-term projects and AI/ML rather than long-term, ongoing systems projects. The modern funding and publishing landscape seems to emphasize speed and quantity over quality and impact. Moreover, it seems that companies with deep pockets (Microsoft, Apple, Nvidia) may be less likely to collaborate with and/or fund academic projects as IBM and DEC did in the 1980s. It could be that those partnerships weren't hugely beneficial for AT&T, IBM and DEC's businesses. reply aijfedklmnop 1 hour agoparentprevSystems have been cleaved in two for who knows what purposes. Systems have always deserved a proper segregation from product, that's been lost in this new paradigm. reply musicale 3 hours agoparentprevMany universities have seminars that are open to the public, online or/and in person. reply mattrighetti 1 hour agoprevI recently moved to London and I wanted to start participating in these kind of events, where’s the best place to look for them? reply mgaunard 55 minutes agoparentThere are regular C++ meetups, which I expect would cover systems programming. It seems however cloud/web people have a different idea of what systems programming is. reply twic 13 minutes agorootparentIs that ACCU, or someone else now? reply paulgb 1 hour agoparentprevI haven’t been to one yet, but from afar I hear great things about the London Future of Coding meetups. reply billfor 2 hours agoprevThe old DECUS meetings were fun. https://en.m.wikipedia.org/wiki/DECUS reply walterbell 4 hours agoprev> I started feeling a bit embarrassed that a graduate student had more guts than I had to get back onto the meetup organizer wagon. Inter-generational perpetual embarrassment engine for innovation. reply weinzierl 4 hours agoprev> First off, don't pay for anything yourself. Find a company who will host. At the same time, don't feel the need to give in too much to the demands of the company. Or go the same route as the mentioned TUMuchdata[1] which apparently kicked off the activities in the post. They are simply meeting at their university. I know at least one Rust Meetup near them, that deliberately decided to meet at the public library (with permission) and forgo any company sponsorships. [1] The pun here is that TUM is the common abbreviation for Technical University Munich. reply torontopizza 2 hours agoprevI started a Toronto meetup with people here on HN and the Fediverse. Running a stable IRL meetup is a lot of work! reply kewbish 2 hours agoparentCurious if you have a link to your event page? reply devdao 1 hour agorootparentToronto's Tech Pizza Mondays https://social.linux.pizza/@Techpizzamondays If you're here, you belong and you are invited and welcome reply vsgherzi 1 hour agoprevIve been struggling to find good meetups in the san diego los angeles area. meetup.com seems pretty dead.. any suggestions? reply sausajez 3 hours agoprevStrange you say this, I just got together with a bunch of people and revived an old user group that I used to run, seems like there's a itch to get out and socialise again reply shae 2 hours agoprevWho's up for one in Boston? I'll organize if I can find some speakers. reply rahuldave 1 hour agoparentCount me in on organizing as well. And not just for systems but ml/llm adjacent infra and databases as well. Things like arrow and lancedb for example reply tstack 1 hour agoparentprevThere’s a handmade meetup in Boston you might want to check out — https://handmadecities.com/meetups/ reply otras 1 hour agoparentprevJust the other day, I was checking to see if there were any active ones in the Boston area. I'd definitely be up for one, and I'd be more than happy to do a talk and/or help organize. reply kaizoku20 2 hours agoparentprev+1 reply demondemidi 3 hours agoprevMeetup.com was on fire with maker, programming, and tech meetups in Portland in 2010-2014. Some were huge (I recall the auditorium for Puppet Labs wall-to-wall one point). Then ... poof. All gone, even before the pandemic. And it was super diverse topics: everything from NodeJS & Rust, and HTML1.0 to startups, manufacturing, and IoT hacking: heck, there was meetup for RF circuit enthusiasts! I just checked and there's still CTRL-H going strong, but not much else. From what I heard from two regular organizers is that it is just a LOT of work to run a consistently solid meetup, and eventually exhausting. I can see that: I remember thinking, \"I could help but do I really want to use my small amount of free time for this?\" So, hats off to people who ran those awesome meetups (Thubten, I'm looking at you!). reply walterbell 3 hours agoparent> Meetup.com was on fire with maker, programming, and tech meetups in Portland in 2010-2014. Some were huge (I recall the auditorium for Puppet Labs wall-to-wall one point). Then ... poof. All gone, even before the pandemic. [2017] Acquired by WeWork [2018] Founder steps down as CEO [2019] New pricing model [2020] WeWork sold it to AlleyCorp [2024] Bending Spoons announced it will acquire Meetup https://en.wikipedia.org/wiki/Meetup reply itqwertz 2 hours agoparentprevI also used to go to meetups every week in Portland during that time! It was great, free beer and pizza definitely helped me when i was struggling to get my career off the ground. Janrain had a great meetup space (old Nike basketball court with risers) with awesome tech topics that exposed the vast world of tech to a noob like me. Puppet Labs, Urban Airship, and New Relic also had top-tier meetups. Intel in Hillsboro was usually worth the MAX trip out there. Tons of swag, food, and opportunities for employment. The real issue with these meetups is that the money dried up. Most of the meetups had recruiters who were desperate to hire and sponsorship helped fund these meetups. Now that WFH is widely acceptable, there’s little incentive to court talent locally. Zoom meetups feel inorganic and lifeless, although they cost very little. There was a gold rush feel during the 2010’s, but I feel like that time is over. Cost-cutting, outsourcing, and the AI hype are leading to software becoming a less prestigious career than it once was. I don’t see this optimism coming back any time soon. reply 01HNNWZ0MV43FF 3 hours agoparentprevExhausting plus everyone has to make a meetup account plus meetup started charging more and more plus why not just use Facebook since I'm the only one without a Facebook account Maybe a federated alternative will take off soon. It would be nice if email was federated in practice reply shagie 2 hours agorootparenthttps://joinmobilizon.org/en/ ( https://en.wikipedia.org/wiki/Mobilizon ) Also from a few days ago - Radius – A Meetup.com alternative https://news.ycombinator.com/item?id=40717398 reply mlhpdx 1 hour agoparentprevI would love to attend a systems-oriented meetup in Portland, and would be willing to organize/operate it though I’m quite bad at that kind of thing. reply dec0dedab0de 2 hours agoparentprevsame thing happened in philly about the same time. reply convolvatron 4 hours agoprev [–] just tried to look - SF distributed systems meetup was a one-shot affair? reply msgilligan 42 minutes agoparent [–] I'm hoping that is not the case. I signed up in the hopes there will be more. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "There has been a resurgence in high-quality systems programming meetups in cities like Munich, Berlin, San Francisco, New York City, and Bengaluru in 2024.",
      "The NYC Systems Coffee Club, started in December 2023, focuses on engineering challenges and has grown rapidly, inspiring similar meetups globally.",
      "Key advice for organizing meetups includes securing a host venue, focusing on technical content, finding speakers through industry connections, and collaborating with other organizers."
    ],
    "commentSummary": [
      "Systems programming meetups are experiencing a resurgence, highlighting the community's renewed interest in low-level programming and system architecture.",
      "Organizing such meetups faces challenges, including finding stable venues, which often require strong internal support or municipal backing to secure consistent locations.",
      "The quality and focus of meetup content are crucial, with some groups struggling to balance between genuine technical discussions and self-promotional presentations by speakers."
    ],
    "points": 164,
    "commentCount": 65,
    "retryCount": 0,
    "time": 1720358437
  },
  {
    "id": 40892812,
    "title": "Where are the good resources for learning audio processing?",
    "originLink": "https://news.ycombinator.com/item?id=40892812",
    "originBody": "I&#x27;m trying to program a harmonizer, like Jacob Collier&#x27;s one built by MIT&#x27;s Ben Bloomberg. I am looking for good, accessible resources on pitch shifting (whilst still sounding natural) and other terms I&#x27;ve heard like format shifting.Where are some good resources for this for somebody with extensive programming experience but no experience in audio processing?",
    "commentLink": "https://news.ycombinator.com/item?id=40892812",
    "commentBody": "Where are the good resources for learning audio processing?144 points by SamCoding 23 hours agohidepastfavorite43 comments I'm trying to program a harmonizer, like Jacob Collier's one built by MIT's Ben Bloomberg. I am looking for good, accessible resources on pitch shifting (whilst still sounding natural) and other terms I've heard like format shifting. Where are some good resources for this for somebody with extensive programming experience but no experience in audio processing? bbloomberg 2 hours agoHello :) all of the resources mentioned here are great! One step I’d add to the learning part (and it’s what we did when building Jacob’s) is to spend a lot of time trying out existing implementations to determine what you like and don’t like. For example, many of them don’t have great low end. Some are “sluggish” and need external enveloping. Getting a sense for what’s out there can help to provide a North Star when you write your own. Some classics are the Eventide H3000, IZotope Vocal Synth, TC Voice Live, Antares Harmony Engine, and Soundtoys Little Alterboy. reply i_am_proteus 21 hours agoprevhttps://ccrma.stanford.edu/~jos/ Not much to say that Julius doesn't... open course materials for (almost) everything you might need in audio processing. reply planewave 20 hours agoparentSecond anything from CCRMA, the inventors of FM synthesis and still the one top programs in the country/world. reply worstspotgain 18 hours agorootparentThirded! CCRMA and its people are awesome (way more so than the rest of Stanford.) reply anymouse123456 17 hours agorootparentThis comment helped get me over the barrier to take a closer look... reply geokon 18 hours agoparentprevI've used it as a reference or to gain a different perspective on something I'm familiar with - but it's generally way too terse to learn from Ex: https://ccrma.stanford.edu/~jos/mdft/Bessel_Functions.html reply worstspotgain 18 hours agorootparentThat doesn't look terse to me, though it does require familiarity with the subject. \"The last expression can be interpreted as the Fourier superposition of the sinusoidal harmonics of [expression], i.e., an inverse Fourier series sum. In other words, [expression] is the amplitude of the k-th harmonic in the Fourier-series expansion of the periodic signal x_m(t).\" Many of the concepts are hyperlinked for reference. With the required familiarity, I would much rather read this than something that took seven pages to get to the point - say by assuming that the reader is unfamiliar with a premise out of an abundance of caution. reply cozzyd 17 hours agoparentprevJulius is a national treasure. I learned immensely from his class and textbooks... reply fxtentacle 20 hours agoparentprevGreat link, thanks :) reply an_aparallel 20 hours agoprevHey - one of the industry standard time stretching library is \"elastique\" by Zynaptiq (licensed, not open source). Used by Ableton, FL Studio etc. If you want to peak into some source code - you can look into Rubberband library: https://breakfastquay.com/rubberband/ Rubberband is one of the time stretching/pitch shifting algorithms used in Reaper. You can download reaper trial and listen to the results with different parameters to see how you can tweak the code and if that gets any results you're happy with: https://www.reaper.fm/ reply IndySun 10 hours agoparent>Hey - one of the industry standard time stretching library is \"elastique\" by Zynaptiq. The company is Zplane, not Zynaptiq. Easy mistake, there is a little overlap. https://licensing.zplane.de/ reply dsp_person 19 hours agoprevI find [1] a good reference. A con is the examples are in matlab, but it's clear enough between the text and matlab code to write your own implementation. Also [2] is a decent book for overall dsp concepts. [1] DAFX - Digital Audio Effects (Second Edition) Edited by Udo Zölzer https://dafx.de/DAFX_Book_Page_2nd_edition/index.html [2] Understanding Digital Signal Processing, Richard Lyons reply a-dub 18 hours agoparentlyons is a good intro but maybe a bit handwavey at times (although my copy is an edition from the 90s). consider maybe backing it up with one of the textbooks like oppenheim (the classic) or manolakis (one that i think i remember liking). reply cushychicken 18 hours agoparentprevZolzer’s book is the best out there on the topic that I know of. reply planewave 20 hours agoprevProfessor Puckett, inventor of Max and PureData (the two top visual programming languages for DSP) has a book, The Theory and Practice of Electronic Music, with interactive examples written in PD, this one probably has an example exercise for a pitch shifter [0] I often recommend also Music and Computers originally out of Columbia. [1] [0]http://msp.ucsd.edu/techniques.htm [1]https://musicandcomputersbook.com/ reply a-dub 18 hours agoparenthe gives awesome talks as well! reply bwanab 18 hours agoparentprevVery cool. I've used PD a lot in the past, but I didn't know about his book! reply geekraver 3 hours agoprevhttps://www.audiolabs-erlangen.de/resources/MIR/FMP/C0/C0.ht... might have something reply jarmitage 17 hours agoprevReal-time audio programming 101: time waits for nothing http://www.rossbencina.com/code/real-time-audio-programming-... C++ for Real-Time Audio Programming: https://learn.bela.io/tutorials/c-plus-plus-for-real-time-au... reply bwestergard 20 hours agoprevYou should learn Supercollider, starting with Eli Fieldsteel's tutorials. reply zengid 20 hours agoprevPretty good Audio Developer Conference talk on it here: https://www.youtube.com/watch?v=fJUmmcGKZMI reply Archit3ch 20 hours agoparentAlso check out the blog (https://signalsmith-audio.co.uk/writing/2023/stretch-design/) and corresponding library on GitHub. reply rhizome31 10 hours agoprevGreat channel on fundamentals of digital audio: https://www.youtube.com/@akashmurthy reply Chrupiter 21 hours agoprevWhen I wanted to make a python application to separate a song into the source instruments I used this: https://www.coursera.org/learn/audio-signal-processing. I studied signal processing as a Computer Engineer student but I didn't really get it at the time, with that course I understood what I could do in practice. reply bwanab 20 hours agoprevIt's not going to directly teach you how to build a harmonizer, but this guy has a series of incredible videos on audio processing that might be helpful: https://www.youtube.com/playlist?list=PL-wATfeyAMNoirN4idjev... reply octetta 17 hours agoprevThis dude has nice content too : https://youtube.com/@lantertronics?si=eXjrDn8zlHwQogdX reply a-dub 20 hours agoprevhttp://www.ee.columbia.edu/~dpwe/ https://github.com/librosa/librosa reply fxtentacle 20 hours agoprevAudio is half art, half science. That's why I'd try to find someone with experience. Back in university, I heard lectures on FFT and its applications to audio signal processing. So open access university courses would be the second place I'd look. The approach I always try first is to ask people I know if they can recommend a conference/meetup. For example, the annual JUCE events appear to be chock full with VST plugin developers. There's also private schools like SAE where you (or your employer) can pay for you to have an hour with one of their lecturers to ask questions. reply peterbmarks 16 hours agoprevIf you are interested in Apple platforms check out https://www.audiokit.io reply cageface 17 hours agoprevThe Will Pirkle books have a lot of good info and code to get you started: https://www.willpirkle.com Audio programming is a lot of fun but it's the most challenging domain I've ever worked in. You have to be very careful with what you do on the audio thread. No locks, no memory allocation etc. Messing this up can result in some really ugly audio artifacts. reply MaysonL 14 hours agoprevCheck out officehours.global – a lot of audio people hang out there. reply mikewarot 15 hours agoprevGnu Radio can easily handle audio I/O as well as it does IQ signals from SDR front ends. It's cross platform and you just build flow graphs, which then can be executed. reply grobibi 21 hours agoprevhttp://blogs.zynaptiq.com/bernsee/time-pitch-overview/ Not sure if it's useful. It's probably going to involve granular synthesis. reply ivanjermakov 17 hours agoprev2021 (120 comments): https://news.ycombinator.com/item?id=27273706 reply aj7 20 hours agoprevUse LabView as a calculation engine to do experiments. The advantage is you get system-like diagrams. reply the__alchemist 19 hours agoprevI would pick up a microcontroller dev board that has a mic built in (Eg one of the STM32 discoveries). Also get a \"codec\" dev board. (Or alternatively, use the MCU's onboarod DAC). Get it to receive audio, process it using DSP, then output it, and/or save to memory. This will really force you to understand it. reply spacechild1 17 hours agoparentBad advice. I have no idea how using a microcontroller would help someone understand pitchshifting algorithms. reply p1esk 18 hours agoparentprevWhy not just use a regular laptop for this? There’s a ton of low level sound processing libraries for every OS. reply checker659 10 hours agoprevAudio Anecdotes series reply Ylpertnodi 15 hours agoprev [–] www.airwindows.com may help. reply squeaky-clean 13 hours agoparent [–] Chris @ airwindows is super nice to release all his plugins and source code for free. But the code quality is really bad. That doesn't matter if you're using a plugin in the production of a song and it works well. But for learning dsp, it's a bad resource. reply mottosso 10 hours agorootparentI was curious what you meant and went to have a look. At first all seemed well, until I got to the actual audio processing part. :) https://github.com/airwindows/airwindows/blob/master/plugins... Then again, maybe this is the norm for audio engineers? Not my field. reply djaychela 2 hours agorootparentprev [–] Not looking for an argument, but can you give some pointers as to what is bad about the code? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The user is seeking resources to program a harmonizer similar to the one created by MIT's Ben Bloomberg for Jacob Collier.",
      "They need accessible information on pitch shifting and format shifting that maintains natural sound quality.",
      "The user has extensive programming experience but lacks knowledge in audio processing, indicating a need for beginner-friendly yet comprehensive resources."
    ],
    "commentSummary": [
      "A user is seeking resources to program a harmonizer similar to Jacob Collier's, created by MIT's Ben Bloomberg, focusing on pitch and format shifting.",
      "Recommended resources include open course materials from CCRMA at Stanford, industry-standard libraries like Elastique by Zplane, and books such as \"DAFX - Digital Audio Effects\" by Udo Zölzer.",
      "Additional suggestions include tutorials, YouTube channels, and community forums like Officehours.global for networking and further learning."
    ],
    "points": 144,
    "commentCount": 43,
    "retryCount": 0,
    "time": 1720295988
  },
  {
    "id": 40893866,
    "title": "A free minimalist daily habit tracker",
    "originLink": "https://rdht.vercel.app/",
    "originBody": "Hello.I was looking for a simple and clean habit tracker. I went through a few apps but felt they were missing something. I also had some time on my hands, so decided to make my own and share it in case anyone else likes it.Some of the supported features:Streaks based, track and beat your longest streaksFully useable offlineFreezes (similar to Duolingo&#x27;s streak freezes)Visual map for tracking consistencyPause the app when you need a break or will be awayFor anyone curious about the tech stack: - React for frontend - Dexie cloud for storage and syncing - Vercel for hostingLink: [https:&#x2F;&#x2F;rdht.vercel.app&#x2F;]",
    "commentLink": "https://news.ycombinator.com/item?id=40893866",
    "commentBody": "A free minimalist daily habit tracker (rdht.vercel.app)117 points by pixd 20 hours agohidepastfavorite51 comments Hello. I was looking for a simple and clean habit tracker. I went through a few apps but felt they were missing something. I also had some time on my hands, so decided to make my own and share it in case anyone else likes it. Some of the supported features: Streaks based, track and beat your longest streaks Fully useable offline Freezes (similar to Duolingo's streak freezes) Visual map for tracking consistency Pause the app when you need a break or will be away For anyone curious about the tech stack: - React for frontend - Dexie cloud for storage and syncing - Vercel for hosting Link: [https://rdht.vercel.app/] timnetworks 1 hour agoAdd a thing for bad habits too, and run the streak inverted. The more times you don't click the better. reply tbihl 12 minutes agoparentMaybe just put 'refrain' in front of bad thing for the time being. reply coreyburnsdev 1 hour agoprevSurprised nobody mentioned the two low resolution images on the front page. (my habits and the email input at the bottom). Curious why you wouldn't just put the actual component there rather than a fuzzy-screen shot of them? reply RockRobotRock 12 hours agoprevLike the design. Personally I think the barrier for a project deserving a domain is pretty low. \"Try for free\" is ominous. If you have a paid plan, please have that on the home page. Otherwise, I wouldn't use that wording. reply tempcommenttt 12 hours agoparentThat. When a page says “try” and “for free” in connection, I always research the hidden real long time cost before trying. If I can’t find it, I usually leave or Google to find out if the costs have been mentioned somewhere else. reply pixd 11 hours agoparentprevThanks for the feedback, I updated the text. My intention behind \"try for free\" was to keep the barrier for trying out the app low. But I didn't realize that it would also imply a hidden price or some premium level reply criddell 5 hours agoprevThis is really nicely done. It looks great and is very responsive. I have some things I track that aren’t daily so it doesn’t work for me, but I like that you didn’t build an app with a billion options. Are you on the free plan for Dexie? How many users do you think you can support with the 100MB limit? reply ziggyzecat 11 hours agoprevNeat. I like it! +1 for not requiring an account to try. Needs debugging for iPad display sizes and on screen keyboards. Might be helpful: [1] https://stackoverflow.com/questions/2593139/ipad-web-app-det... [2] https://developer.mozilla.org/en-US/docs/Web/API/Visual_View... reply nerbitz 15 hours agoprevUnusable on iPhone 13 mini. Couldn’t log in as the OTO entry screen was not visible. reply pixd 15 hours agoparentThanks for the feedback! I'll have to debug for safari, not sure what the issue is reply lawgimenez 12 hours agoparentprevAlso on iPhone 12, otp not visible. reply davewasthere 9 hours agoprevQuick bug. As an unauthed user, add a habit. Then log in. Habit has disappeared. (but can create habits, which then sync just fine to other devices) I guess I was expecting my unauthed user habit to persist after logging in. reply bukacdan 10 hours agoprevWould love to see the code, any plans on open sourcing? reply apantel 2 hours agoprevThis seems quite broken on iPhone (iPhone 12, iOS 17, Safari browser). Trying to add a task ends up blowing the model up too big and it’s impossible to scroll to or even see the button to add the task. reply tremarley 8 hours agoprevThose are features I’ve always been looking for in a tracker reply constantinum 9 hours agoprevOn a side note, The only habit tracker that has worked me is BEEMINDER. reply jackstraw14 8 hours agoparentEver made a side bet with yourself to remove the middleman? reply kecupochren 10 hours agoprevYou can use 16px font size on mobile so the browser won't zoom in to the text field. reply FrancoisBosun 4 hours agoprevI use an iPhone 12 mini. In portrait mode, the site was nearly unusable. I would add a habit, and the keyboard covered up the description field. There was no way for me to scroll the page to get the description field. Fortunately, the iOS keyboard has a next field button which I could click and hello me to entered the habit description. I too like the clean design and wish you good luck. reply edgarvaldes 15 hours agoprevNice and clean. I would love a collapsed view of the habits. How does the offline mode works? Any monetization plans? reply gibolt 12 hours agoparentAgreed. Being able to see more than 1.5 tasks on a page would be an improvement. It would also be nice to have some indicator (at the top) that all tasks for the day have been completed. reply pixd 15 hours agoparentprevThat's a great idea, thanks. The data is stored locally on the browser using IndexedDB so everything works offline. If you login, it'll sync your data with a remote server. I haven't really thought about monetization as this primarily started as just a tool for myself. Might have to give it some though but not sure what I would monetize reply getcrunk 15 hours agoprevGreat idea! Doesn’t work well on mobile safari reply pixd 15 hours agoparentThanks! I'll spend some time debugging on safari and see what the issue is reply thomasfromcdnjs 15 hours agoprevThis is perfect, I've been wanting something like this forever. reply pixd 15 hours agoparentThat's great to hear. Thanks! reply purple-leafy 14 hours agoprevNice. I see the GitHub style tracker alot lately reply BolexNOLA 5 hours agoprevAs someone with ADHD “streak freezes” are a thing I never knew I needed, wow. I’ve tried so many gamified habit things and the anxiety over potentially missed dopamine is just so present no matter how old or “wise” I become. Excited to try this out reply elflaune 10 hours agoprevI just want to put out Loop Habit Tracker ( https://github.com/iSoron/uhabits ). Its open source, ad free and has the option to start the day at 3 am. I am very happy with this solution. reply user_7832 3 hours agoparentLoop is an all time classic. Just wish it supported other platforms too. reply skkap 13 hours agoprevNice work on this habit tracker! Love the clean design and the GitHub-like year widget. It's awesome that it works without registration! I am also building a habit tracker ( https://mygoodweek.com ). Similar story, none of the existing options were satisfactory~ It tracks habits automatically from Google Calendar events and lets you check habits manually too. It would be cool to get your thoughts! reply kmarc 11 hours agoparentThis looks like a product a lot of effort and thought went into, including a consistent graphical design. Congrats! The style that was chosen, to me is deeply unsettling. These Well-lit, colourful, rounded, smooth 3D objects remind me of a chaotic, visually complex and triggering while trying-to-be-baby-safe environment, and indeed puts me into thinking the exact opposite of what I'd want to achieve with this product (which is eliminating chaos, and get my adult daily life sorted out). Note that this is rather a weird angle and not a criticism of your product. I'm pretty sure it's only me, or at most a handful of people who are triggered into this state of mind. I found it interesting. reply ziggyzecat 11 hours agorootparentI didn't even try to dive into why I left the page within 20 seconds. > visually complex and triggering while trying-to-be-baby-safe environment resonates perfectly, though. On a side note: sorting adult daily lives out needs to embrace and make space for the inner child. I observe it everywhere at the moment: parents, old friends, even in my younger brother and myself. An unhealthy kind of stress, burnout and depression build up if you don't. reply kmarc 10 hours agorootparentNo science here, but... Isn't it munching on ice cream at 10pm, video games, and other indulges are for that? Also at work, I am (thankfully) able to tinker a lot while delivering value, and not under constant pressure to just grind, in a dry, too serious environment. I think I do embrace (quite often) the inner child in me, but I definitely separate the \"productivity\" of my life from that: things that need the diligence, I do take seriously, and maybe this is what gives me this dissonant feeling when I look at this design language in the context of making me more productive. reply skkap 11 hours agorootparentprevThat is very interesting feedback, thank you! You might not be alone though, from time to time we get similar feedback, and in fact, we are currently rethinking branding to make it more calm and positive. reply stanislavb 12 hours agoparentprevLooks beautiful. I will get it featured on SaaSHub's newsletter https://www.saashub.com/my-good-week. Cheers! reply skkap 11 hours agorootparentThank you, very kind of you! reply lobsterthief 7 hours agoparentprevLooks slick, but I’d love to be able to see a demo (or more screenshots) before signing up. reply stared 5 hours agoprevThank you for sharing - especially with the green flag of \"no account required\"! I wanted to go habit tracing in a minimalist way, and ended up using checkboxes in Obsidian daily notes (and take them anyway, previously in Evernote) and plugins (there are a few, I use a combination of https://github.com/pyrochlore/obsidian-tracker and https://github.com/hedonihilist/obsidian-habit-calendar). reply superultra 6 hours agoprevThis is really cool. I don’t think I have diagnosable ADHD, but I have noticed that gamification features like “streaks” often cause more problems in building habits than helping them. My daughter, who is diagnosed with severe ADHD, eventually had to “give up” some of the streak based apps like Duolingo because they are, imho, preying on our desire to complete “streaks” only to increase in-app activity for their own platform. I’ve been really loving Llamalife - https://llamalife.co - they don’t do habits but they organize a lot of the schema around people who get overwhelmed easily. reply thih9 5 hours agoparentAfter reading this comment I installed llamalife and immediately abandoned it when it asked me to sign up and accept T&Cs as the first thing after opening. There was no support for sign in with apple either. I love that the app from this HN submission is usable without creating an account. reply 8organicbits 13 hours agoprevI'm always up late and I'd like to get a last minute habit checked off before I go to bed, but so many tools move to the next day at midnight. I ended up building my own minimal chore tracker[1] because I couldn't find any that aligned to that schedule. I really think everyone has slightly different needs and chore/habit trackers make a great minimal programming+design project. [1] https://alexsci.com/blog/personal-apps/ reply georgebcrawford 5 hours agoparentHi there, I thoroughly enjoyed reading your blog post - especially the part about single-purpose software. It's a recurring tension in my digital/physical existence. reply codazoda 5 hours agoprevI've been working on a minimalist daily habit tracker that I designed for counting calories as part of my own weight loss journey. It's called Quick Calories. The PWA is free, it doesn't require an account, has no advertising, and works offline. Link: [https://calories.joeldare.com] reply yunusefendi52 15 hours agoprevLooks good. How do you \"done\" past days? Sometimes I don't actually miss the habit but not have the time to open screen. I also created similar habit tracker app for mobile https://play.google.com/store/apps/details?id=com.yedev.habi... The unique thing is that I use google drive to sync habits reply pixd 15 hours agoparentI like the design, very clean. For the past days, if you click the 3 dots next to the checkbox, there's an option to add checks for previous days using the calendar reply diimdeep 14 hours agoparentprevLooks like derivative of open source https://github.com/iSoron/uhabits reply RockRobotRock 12 hours agorootparentThis app is fantastic! I wish it was on iOS. reply podviaznikov 10 hours agoprev [–] love the UI. I made habit tracker visualizer based on Apple Reminders https://public.me/anton/daily reply teitoklien 9 hours agoparent [–] you need to focus on how you'll stop eating pastries man, I'll hope you succeed soon, based on that tracker list Good luck ! Regards, Pastry Hater reply podviaznikov 9 hours agorootparent [–] thank you:) not so easy with pastries reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "A new habit tracker app has been developed by an individual dissatisfied with existing options, featuring streak tracking, offline usability, streak freezes, a visual consistency map, and pause functionality.",
      "The app is built using React for the frontend, Dexie cloud for storage and syncing, and hosted on Vercel.",
      "The developer has made the app available for public use, showcasing a personal project turned into a community resource."
    ],
    "commentSummary": [
      "A new free minimalist daily habit tracker has been created to address gaps in existing apps, featuring streak tracking, offline use, streak freezes, a visual consistency map, and a pause function.",
      "Built with React and Dexie cloud, and hosted on Vercel, the app has received feedback for improvements, particularly for iPhone users, including suggestions for bad habits, design tweaks, and bug fixes.",
      "Users appreciate the clean design and offline functionality, with suggestions for a collapsed view and better mobile compatibility; there are no monetization plans yet."
    ],
    "points": 117,
    "commentCount": 51,
    "retryCount": 0,
    "time": 1720305433
  },
  {
    "id": 40895441,
    "title": "Why Italy Fell Out of Love with Cilantro",
    "originLink": "https://www.atlasobscura.com/articles/what-are-italian-herbs",
    "originBody": "Why Italy Fell Out of Love With Cilantro Coriander went from ancient staple to persona non grata. by Andrew Coletti July 1, 2024 Why Italy Fell Out of Love With Cilantro Coriander or cilantro has a long history in Italy. Alamy In This Story Destination Guide Italy 104 Articles 1020 Places When you think of Italian herbs, cilantro (also known as coriander) is probably not the first one that comes to mind. Yet crack open the fifth-century Roman cookbook Apicius, and you’ll find it included in 18 percent of all recipes. Roman chefs prized both the citrusy seeds and pungent leaves of the plant they called coriandrum for sauces, salads, roasts, and flavored beverages, among other dishes. Compare this with Pellegrino Artusi’s Science in the Kitchen and the Art of Eating Well, published in 1891 and often considered the foundational text of modern Italian cuisine. Coriander leaves are absent from the book’s nearly 800 recipes, and the seeds show up in just four desserts. Artusi also warned readers to beware of buying cinnamon powder from unscrupulous merchants who “throw in handfuls of coriander seeds to increase the volume with a cheap ingredient.” From this reference, we can infer that there was no lack of coriander in late 19th-century Italy. But at some point between Apicius and Artusi, Italians largely stopped cooking with it. “In dishes, I would be surprised to ever find [coriander] in something from 1700 onwards,” says Karima Moyer-Nocchi, a culinary historian at the University of Siena in Italy. Moyer-Nocchi explains that while coriander is not entirely absent from Italian cuisine today, its uses are far more limited than in past centuries. “Predominantly in central Italy, porchetta is going to be prepared with slightly crushed coriander seeds,” says Moyer-Nocchi. “Around the time when people are slaughtering their pigs, you’ll find big bags of coriander at the supermarket.” The leaves, on the other hand, “are absolutely not being used,” she adds. “I have to drive 45 minutes to a grocery store in another city to find it, or grow it myself.” The boneless rolled pork roast porchetta is seasoned with coriander seeds in some regions of Italy. Pedro Angelini/CC BY 2.0 Atlas Obscura Trips Flavors of Italy: Roman Carbonara, Florentine Steak & Venetian Cocktails View This Trip Native to Europe, Asia, and North Africa, coriander has a long and widespread history of human cultivation. Latin coriandrum, the source of many modern names for the plant, was borrowed from the Ancient Greek koriandron or koriadnon. The Romans developed a taste for the ingredient through the extensive Greek influence on their cuisine. During the Roman era, Moyer-Nocchi says, “coriander is being grown locally in Italy, whereas other spices are coming in through the trade routes.” Coriander was also imported due to high demand. Pliny the Elder wrote in the first century that the herb was extensively grown in Roman Egypt. Archaeologists have found coriander seeds alongside those of other herbs like dill and fennel at Roman sites throughout Europe, including Britain. The frequency with which these seeds are found, and the fact that they have been discovered in remote settlements far from centers of power, demonstrates that coriander was consumed by all levels of Roman society. Aside from food, coriander was cultivated for medicinal purposes like soothing stomach aches, and for food preservation thanks to the seeds’ antibacterial properties, which is why they are still used in some of Italy’s regional salt-cured meats. Moyer-Nocchi describes a combination of factors that contributed to coriander’s decline after the fall of Rome. One was that the former empire absorbed influences from Germanic tribes to the north like the Visigoths, “who don’t have that tradition” of cooking with coriander. Another was that coriander’s local availability made it less elite than other spices. “Culturally, it’s not an expression of anyone’s wealth,” says Moyer-Nocchi. Instead, Asian spices like cinnamon and cardamom, imported from afar at great cost, became medieval status symbols. “Numidian chicken,” named for a region in Roman North Africa, is one of many dishes in the ancient cookbook Apicius made with coriander. Carole Raddato/CC BY-SA 2.0 Moyer-Nocchi explains that medieval Italians divided spices into two categories: “sweet” and “strong.” Powdered blends of sweet spices—including sugar—were used in a majority of dishes, but “coriander is put over into the strong spices with pepper,” she says, “so it’s going to be used less.” Coriander leaves fell even further out of fashion than the seeds because their distinct flavor clashed with the trendy imported ingredients of the time, such as rosewater. In 1544, physician and botanist Pietro Andrea Mattioli described the leaves as smelling like bed bugs or stink bugs, a comparison echoed by later authors. Coriander leaf was already mostly absent from Italian cuisine by the Renaissance, but the seeds continued to be used as a spice. They were also coated in sugar to make confetti, or “comfits” in English. These were chewed at banquets as an after-dinner mouth freshener and digestive, similar to mukhwas, the mixture of sweetened whole spices chewed in South Asia today for the same purpose. At festive celebrations, coriander comfits were thrown and scattered, giving rise to the English word “confetti” for the paper particles that later replaced them. In modern Italy, paper confetti is still called coriandoli, meaning “coriander seeds,” while confetti usually refers to a different kind of comfit, the sugared almonds given out at weddings and communions. Italy enjoyed a reputation as a center of culinary innovation and refinement until the end of the 16th century, says Moyer-Nocchi, when France replaced it as Europe’s trend-setter. “And that’s where spices just fall by the wayside,” she says. French chefs of the 17th and 18th centuries deliberately set themselves apart from the earlier Italian tradition by focusing on fresh herbs instead of dried spices and specific ingredient pairings instead of sweet and strong spice blends. As Italian chefs looked to the example of the French, “Italy frankly lost its culinary identity with the utter dominance of France for the next two centuries,” says Moyer-Nocchi. And when a distinct Italian culinary identity emerged with the unification of the modern nation in the 19th century, long-abandoned coriander was not revived, but left behind. Coriander seed is sometimes used with other spices in cavallucci, a Christmas pastry from central Italy. Nemo bis/CC BY-SA 3.0 Modern Italians see coriander as a foreign ingredient that separates them from other groups of people; what Moyer-Nocchi calls a “culinary marker.” “That comes down to a very basic sort of [idea], ‘What are the flavors that are going to express my identity?” she says. These days, “coriander just doesn’t fit into the culinary grammar of how Italians choose to express themselves.” Moyer-Nocchi points out that coriander is not the only herb whose popularity has ebbed and flowed in Italy over the centuries. Marjoram was once widely used, but “no one necessarily associates that with Italy anymore,” she says. On the other hand, some of the flavors modern Italians use to express themselves have not actually been “Italian” for very long. Basil, which originated in Asia, has only been part of Italian cuisine for a few hundred years. “It’s very young, and yet seems so Italian,” Moyer-Nocchi says. From Thailand with chilies to Belgium with chocolate, many modern nations have embraced once-foreign ingredients, folding them into their culinary identity until their absence becomes unthinkable. The curious history of cilantro in Italy shows that the reverse is also true. Sometimes, an ingredient becomes so unpopular that we forget it’s been there all along. Gastro Obscura covers the world’s most wondrous food and drink. Sign up for our email, delivered twice a week. Read next A Gastro Obscura Guide to Houston Superb kolaches, Tex-Mex, Viet-Cajun crawfish, and more in Space City. herbsfood historyspicesrenaissanceromanmedievalfood Want to see fewer ads? Become a Member. Most Read Stories 1 renaissance Why Italy Fell Out of Love With Cilantro Coriander went from ancient staple to persona non grata. Andrew Coletti 2 national parks Dozens of Alaska Rivers are Turning an Eerie Orange The cause is just as concerning as the color. Charis McGowan 3 children The Anxious History of the American Summer Camp The annual rite of passage has always been more about the ambivalence of adults than the amusement of children. Ashley Stimpson 4 crops Why Jesus Never Ate a Banana About 69 percent of the global diet is \"foreign,\" says a study that pinpoints the origin of 151 food crops. Frank Jacobs, Big Think 5 poop How to Poop in the Woods Be #1 at going #2 when nature calls in nature. Ashley Stimpson View More Stories Want to see fewer ads? Become a Member.",
    "commentLink": "https://news.ycombinator.com/item?id=40895441",
    "commentBody": "Why Italy Fell Out of Love with Cilantro (atlasobscura.com)108 points by pepys 13 hours agohidepastfavorite116 comments curl-up 11 hours ago> In 1544, physician and botanist Pietro Andrea Mattioli described the leaves as smelling like bed bugs or stink bugs... Incredible that neither this, nor the other linked article, even mention the fact that this stink-bug/soap taste is genetically predetermined for some people [1], so above quote can by no means be taken as an argument that cilantro \"became unfashionable\" so authors started describing it negatively. Instead, the real story is probably much more interesting, as the changes in cilantro popularity could probably be connected to waves of migrations and general genetic pool changes in a particular place. [1] https://en.wikipedia.org/wiki/OR6A2 reply meowface 9 hours agoparentI think you may have falsely pattern matched this particular reference. I love the taste and smell of cilantro and eat it all the time, but the first time I encountered a stink bug I thought \"oh, it smells like cilantro\". It doesn't taste like soap to me. I think the stink bug-smell has no relation to the soap-taste, which is indeed caused by that gene. I think stink bugs' scent probably smells similar to cilantro for everyone without that gene. reply DebtDeflation 7 hours agorootparentI would tend to agree. Cilantro does not smell like bugs to me. It does however taste like soap. So I always tell restaurants no cilantro and often end up having to pick it out of dishes that I didn't expect to contain it. reply bloomingeek 5 hours agorootparentFor me, when I first tried cilantro, it tasted like soap also. This was when my wife and I discovered street tacos several years ago. As time went by and I used hot sauce to mask the cilantro, I developed a taste for it and now I don't mind it at all. I no longer taste soap, but a kind of sweetness. reply glenngillen 5 hours agorootparentOh, that's interesting. I recall reading something years ago about how humans typically don't like brassicas because the bitterness is overwhelming. But through regular introduction the tastebuds have a form of plasticity (I forget the actual term) to them and they'll eventually overcome the bitterness. I wonder if the \"cilantro tastes like soap\" is a similar phenomenon. reply washadjeffmad 4 hours agorootparentSounds like childhood taste preference against bitterness changes with susceptibility to alkaloids [1]. I disliked cilantro for the same reason as a child, and I still have an aversion to cruciferous vegetables because of the saliva compound that makes them taste objectionable (they're also being bred to contain less sulfur, so Brussels sprouts today probably are better than you might remember them being). I can detect a very low threshold of even the mildest cabbage in anything, which has made me a target for Korean women throughout my life. Tasting like soap doesn't mean you can't enjoy cilantro, though, and while I still don't favor the fresh leaves plain, I use them liberally as an ingredient and dry fry them with the stems for my Sichuan and Mexican cooking. [1] https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4654709/ reply thfuran 5 hours agorootparentprevModern cultivars produce less of the glucosinolates that give brussel sprouts their bitterness. I'm not sure whether there has been a similar change in other brassicas, but sprouts were the most notoriously bitter and now actually are less bitter than they were in the 70s. reply curl-up 8 hours agorootparentprevThis is very interesting, thanks for pointing it out! I really dislike cilantro, and to me it always tasted strongly like stink-bugs (not that I ever tried eating one...). I never really understood the dish-soap reference though. So if this is correct, then the whole point of my original post is completely wrong. I'll have to look into this further. reply Modified3019 8 hours agorootparentIt definitely some kind of spectrum of response, rather than just either or. Cilantro to me isn’t something I would generally describe as soapy. If it’s real heavy there can be a slight soap aftertaste that comes through, though at that point the cilantro essence itself is just too much. I do find the comparison of the essence of cilantro as similar to stinkbugs a bit more apt. There’s this hard to describe chemical smell that some insects give off that isn’t cilantro exactly, but is in some sort of similar class. Kind of how we group sour things together. I can tolerate some cilantro without noticing much, but if it’s heavy in a dish it’ll become repulsive and ruin it. For me it’s fine when treated like a spice, not a salad. Apparently Methoxypyrazines are found in stinkbugs and cilantro, and are responsible for a lot of “vegetal” smells. Likewise it seems that stinkbugs can give of “trans-2-dodecenal” [0] which I guess can be written as “(E)-2-dodecenal” (I am not a chemist) which is found in cilantro and has a chemical citrus peel type of smell. [1] [0] https://www.researchgate.net/publication/359322168_Expressio... [1] http://www.thegoodscentscompany.com/data/rw1005071.html reply Tagbert 2 hours agorootparentprevFrom the first time I was exposed to cilantro I thought it smelled like stink bugs. I don't really get the soap taste but I rarely eat enough to really experience that, either. Oddly, I liked coriander seeds long before I was exposed to the herb. They have a very different taste to me. reply the__alchemist 3 hours agorootparentprevI am guessing that stink bug can be categorized with bitter almonds, formaldehyde, and ammonia: Scent analogies that are not as universal as their users imagine! reply qwery 7 hours agorootparentprevI love coriander. I didn't like it when I first encountered it and if pressed would have probably adopted any negative description of it. I doubt I would have described it as soap-like[herbalessence] or stink buggy without being \"primed\" though. I don't know where you get your stink bugs from, maybe stink bugs vary around the world[daftpunk]? I'm not disputing the genetic thing here, by the way. It's just interesting in general and in how knowledge of the fact changes how people talk about taste[sense]. When people like the flavour, they can generally just do so. You can say \"I like coriander\" without justification, but \"I hate coriander\" is a statement that requires justification (apparently). Wikipedia's explanation tells me that the flavour of coriander is actually -- chemically -- like soap in some sense, it's just that there's a genetic variation that determines if you like that or not. Citation needed, of course. Maybe the flavour of coriander is actually like soap. And the people who like coriander are simply the people who like soap? Anyone who denies the similarity between the two is lying because they don't want to be seen as a soap-muncher. Soap-muncher. [herbalessence] Which soap anyway? As I'm sure you know, most \"soap\" today isn't what people called \"soap\" decades ago. Maybe some people even have coriander scented soap. [daftpunk] All the stink bugs I know live along the eastern coast of Australia. [sense] Taste here means both what food tastes like and general having of preferences with or without reasoning. reply mekoka 11 minutes agorootparent> Wikipedia's explanation tells me that the flavour of coriander is actually -- chemically -- like soap in some sense [...] > Maybe the flavour of coriander is actually like soap [...] What the Wikipedia article says is that coriander contains some aldehydes, which some people find to taste like soap, based on genetics. I tried to find whether the taste of actual soap, which coriander lovers can also separately and distinctly identify, is also caused by the same aldehydes. I found nothing. How did you establish that chemical link? From what I gathered the taste of actual soap is on its own separate pathway with different chemical compounds, which most people seem to agree with, regardless of our appreciation for coriander. reply tines 27 minutes agorootparentprev> I doubt I would have described it as soap-like[herbalessence] or stink buggy without being \"primed\" though. I thought this way until a few days ago, when I smelled some new soap in the shower and the thought popped unbidden into my head, \"Man, this smells like cilantro!\" reply kurthr 4 hours agorootparentprevOn the one hand, I think hate vs like does require some sort of justification. You can like something without really caring, but hate means that you REALLY care. I can barely tolerate cilantro, because I grew up with it in salsas and sauces. At the same time I can and will pick 1mm specks out of a sauce, if there aren't very many. Many people who \"like\" cilantro, often don't even notice it's presence, while I will immediately notice even tiny amounts. People confuse it for parsley? Bleagh! I had never heard the stink bug smell correlation, and though I agree it is similar, the strong soapy stevia like taste is just much worse compared to the smell, which I can tolerate. reply qwery 1 hour agorootparentThat's fair enough, I probably should have contrasted 'love' vs. 'hate'. The difference in magnitude wasn't intentional. I meant to point out that the negative is more often treated as \"wrong\" or something to fix while the positive is more often simply accepted. I think this is true in general, at least in western/english conversation. But the coriander conversation is notable because apathy or plain dislike for the flavour can be \"backed up\" with the definitely true genetic explanation. Although in that sense, you actually \"can't not like coriander\". reply kosolam 3 hours agorootparentprevMind if I ask, how did you encounter the taste of a soap?? It boggles my mind, can there be so many people tasting soap? reply DidYaWipe 3 minutes agorootparentEver had beer? eddd-ddde 2 hours agorootparentprevI don't remember when's the last time I tasted soap, but it's just something you don't forget. At least in one of my chem classes we where taught bases vs acids and we definitely tasted soap then. Also, showering. reply bookofjoe 2 hours agorootparentI do. It was around 1955 when I was 7 years old. I said a bad word and my mother literally took a bar of soap and shoved it in my mouth. I still vividly recall how horrible it tasted and felt. reply cozzyd 1 hour agorootparentprevEver not rinsed a glass enough after washing it? reply emidln 1 hour agorootparentprevAs an aside, why add footnotes with misc words/brands rather than numbers? reply qwery 47 minutes agorootparentHappy to answer, I hope this quick list will be satisfactory despite its messiness: - numbers make the order (more) significant, making reordering the text more costly - compared to numbers, words are often easier to spot and jump to - compared to numbers, the words have some connection to the point (not saying I do this perfectly!) so you don't have to remember which (number) footnote you were looking for - ADHD - herbalessence because coriander is a herb, and Maybe some people even have coriander scented soap - daftpunk because 'Around the World (around the world)' reply gopher_space 1 hour agorootparentprevIt's a fun style of writing OP probably grew up reading, Terry Pratchett is a good example. They're asides. reply cassepipe 3 hours agorootparentprevI think MinuteFood settled it : https://www.youtube.com/watch?v=RZtPynXsFas reply nox101 4 hours agorootparentprevIs the stink bug smell thing a different gene than the taste gene? Cilantro just tastes like another leafy vegetable to me. Sure it's unique but zero negative anything. I had a pile of it yesterday at a Mexican restaurant. I've had cilantro salad at several restaurants in Japan as well. https://www.google.com/search?q=%E3%83%91%E3%82%AF%E3%83%81%... reply educasean 2 hours agoparentprevI keep hearing this, but my personal experience refutes this. I first encountered cilantro at the age of 15 and I immediately thought my soup bowl has unwashed soap in it. I did not eat cilantro-included dishes for years after, but fast forward 2 decades and I love cilantro now. I can't really detect the soapyness that once bothered me so much. I don't think cilantro preferences are as set in stone as the story alludes. reply JohnFen 1 hour agorootparentI hope not. I can't stand cilantro. Even a tiny amount renders food completely inedible to me. I wouldn't say it tastes like soap to me (although the first time I heard that, I understood). It tastes more like strongly-flavored dust. I hate this fact because cilantro has become fashionable and has made eating out into a bit of a gamble. I'd be thrilled if I started liking it, or at least stopped hating it so much. reply crazygringo 1 hour agorootparentI'm so confused. Is this whole thread about fresh cilantro leaves/stems, as I thought? Or the ground form of its seeds which is totally different and generally called coriander? (At least in the US.) Just because I've never heard of anyone refer to any kind of leaf as tasting like \"dust\". And I thought the \"soapy\" thing was exclusively a reaction to the leaves, not ground coriander. reply wkat4242 1 hour agorootparentWhere I'm from we call the leaves coriander too. reply crazygringo 38 minutes agorootparentAre you in the US? Just curious if there's regional variation in e.g. supermarket labeling. Because if you look at nationwide labeling, cilantro seems to refer exclusively to the leaves, and coriander exclusively to the seeds, and I've never seen anything different within the US: https://www.target.com/s?searchTerm=cilantro https://www.target.com/s?searchTerm=coriander https://www.wholefoodsmarket.com/search?text=cilantro https://www.wholefoodsmarket.com/search?text=coriander And if both terms can be used for the leaves, is there anywhere that calls the ground seeds cilantro? reply wkat4242 32 minutes agorootparentNo I'm in Europe. In the Netherlands we call the fresh herb coriander too, the term Cilantro doesn't exist. I think it's the same in Spain though it's a bit murkier there due to many Mexican restaurants calling it Cilantro. I think in Ireland they called everything coriander too though i don't recall exactly. But I'd never heard the term Cilantro till i went to a Mexican restaurant in Spain :) reply rpeden 8 hours agoparentprevI vaguely remember reading something similar about stevia. Anecdotally, I know some people who think it's a tasty sweetener, but for me it just tastes bitter and awful. When I add it to coffee, it ends up tasting like I added a mixture of Splenda and powdered graphite. reply bloomingeek 5 hours agorootparentI really tied to adopt stevia into my diet, gave it several months, however the rumbling in my stomach turned me away from it. I also experienced some discomfort in the upper intestinal area. When I looked up stevia before trying it, there were warnings some could experience these symptoms. reply jfengel 6 hours agorootparentprevStevia in its pure form is quite bitter and awful. The stevia sweetened products you get are chock full of bitter blockers. Some people still tolerate it better than others. I'm not sure if there's a genetic link or if it's just what you're used to. reply OJFord 7 hours agorootparentprevIsn't Splenda also a sweetener? Or is Stevia supposedly so much better that you're saying to you it tastes like a worse sweetener plus graphite, or something? reply peeters 6 hours agorootparentI think they're saying it does the job of making it taste sweet (artificially anyway) but comes with an unpleasant flavour. reply rpeden 2 hours agorootparentRight - that's what I was getting at. I find stevia tastes like what I'd get if I took that wood + yellow paint + HB graphite taste I used to get when I absentmindedly chewed the end of my pencil in elementary school and then mixed in some Splenda. reply vasco 10 hours agoparentprevTo be honest that is one of the most repeated \"TIL\" I see on the internet, so I was happy to not have to read it again! reply curl-up 10 hours agorootparentI agree that it's refreshing for an article on cilantro to not be about that. But they make some very misleading implications by completely ignoring it. reply gassit 10 hours agorootparentWhat implications? reply curl-up 10 hours agorootparentFull quote: > Coriander leaves fell even further out of fashion than the seeds because their distinct flavor clashed with the trendy imported ingredients of the time, such as rosewater. In 1544, physician and botanist Pietro Andrea Mattioli described the leaves as smelling like bed bugs or stink bugs, a comparison echoed by later authors. I read this as \"Cilantro fell out of fashion due to it's incompatibility with other popular flavors at a time, and since people love following recent trends, they started describing it in a negative way\". In other words, people saw it as \"stink-bug-like\" because the fashion changed. This happens a lot with food (e.g. Jell-O texture now being repulsive to a lot of people) but in this case, it has nothing to do with fashion, because to a lot of people cilantro does actually have such taste, and this shift in popularity is much better explained by the shift in genetics. reply rsynnott 4 hours agorootparent… I mean, coriander leaves (cilantro) was barely a thing at all in Ireland 50 years ago, and is now all over the place, primarily as a component of Indian and Mexican food, which are quite popular. I’m going to bet on fashions/familiarity, there, rather than a spate of sneaky gene therapy. reply gassit 9 hours agorootparentprevBut that’s your implication the genetic change was widespread enough to effect an entire cultural change. What evidence is there of that? reply curl-up 9 hours agorootparentNo, I am just saying that the article is omitting a very important fact that would go against the argument it implies. I am not saying that it was necessarily genetically driven, I am simply saying that I find it strange that this other perspective was not even mentioned, especially since this genetic predisposition to disliking cilantro is such a widely known fact, and since the author they cite chose to describe cilantro in exactly the way everyone with this genetic predisposition (including myself) chooses to describe it. If an article about the lack of dairy in east Asian cuisine never mentioned the high frequency of lactose intolerance in those regions, it would be equally misleading, even if there were many other factors resulting in this lack of dairy (primarily different agricultural practices). reply wkat4242 1 hour agoparentprevIt's weird, I find the taste really fresh and fruitythe changes in cilantro popularity could probably be connected to waves of migrations and general genetic pool changes in a particular place It's not impossible, but without data it's pure conjecture. The first study I found isn't particularly supportive, unless there's been massive immigration from East Asia that I'm not aware of: https://flavourjournal.biomedcentral.com/articles/10.1186/20... reply pmontra 9 hours agoparentprevWell, half of my friends like cilantro, the other half say it tastes like dish soap. I'm in Italy and by the way I'm sowing it today (too late, probably.) reply hnbad 8 hours agoparentprevI think the \"it was native and abundant so nobody used it because it didn't demonstrate wealth\" explanation is far more plausible given that the same can be observed with many native plants and spices in other parts of Europe. TFA mentions that spices were categorized as \"sweet or strong\" and given that coriander was seen as \"strong\" it competed with fancier imports. This preference likely \"trickled down\" making the spice seem less appealing to the masses, especially with increased social mobility in more recent history. reply m463 8 hours agoparentprevI think a hunk of ginger in some food tastes like soap. reply wheels 10 hours agoprevIn general, most cuisines and culinary trends are far more modern than we tend to assume. Until the last 3-ish centuries: Italy didn't have tomatoes or basil. Nor was there corn for polenta. India didn't have chilies. The Irish and Germans didn't have potatoes. reply PestoDiRucola 6 hours agoparent> Nor was there corn for polenta Before corn, polenta was made with barley: https://historicalitaliancooking.home.blog/english/recipes/a... reply antisthenes 4 hours agorootparentIn my mind, polenta was just another word for porridge, which means it could be made with literally any available grain. It's just a roughly milled grain, boiled until a certain consistency. reply veunes 1 hour agoparentprevThe history of global cuisines is a testament to the dynamic nature of food culture reply fuzztester 5 hours agoparentprevAFAIK India didn't have corn either. Like chilies and potatoes and a few other plants, it came from the Americas. reply umeshunni 2 hours agorootparentArticle on Indian food before modern European contact: https://contingentmagazine.org/2020/06/25/curry-before-colum... reply steveoscaro 3 hours agorootparentprevAnd the list goes on of the food products that only existed in the Americas - chocolate, coffee, hot peppers. But of course it goes both ways. I just had traditional breakfast in an isolated Zapotec village in Mexico. But of course the cheese wasn’t part of it until the Spaniards arrived. reply wincy 2 hours agorootparentYou’re off on coffee, a few months ago I too assumed it came from the Americas because it grows so well here, but was surprised to learn it’s actually an old world thing. https://www.ncausa.org/About-Coffee/History-of-Coffee reply asveikau 1 hour agorootparentThe name coming from Arabic is a big hint. And then the species called C. arabica. reply annexrichmond 3 hours agoparentprevIndia also didn’t have tomatoes until it was brought over by the Portuguese, at least this is what I learned in my travels there. reply icelancer 10 hours agoparentprevYeah, I always find this fact + people saying \"America\" has no unique food culture to be quite funny in tandem. Of course the United States does have unique food culture beyond that juxtaposition, but still. reply Footingerm 9 hours agorootparentBut its true. Europe has a refined food history even if certain foods are young. There was some type of pizza before and it evolved in what pizza is today. Alone the amount of cheese types is huge. What did the USA invent in comparision? What is specific to USA? Even plenty of typical USA Food was invented in Europe. reply jogjayr 5 hours agorootparentBreakfast cereals, hot dogs, hamburgers, toaster pastries, hot pockets, cornbread, jambalaya, grits, cranberry sauce, jerky, the chocolate chip cookie, pumpkin pie, the waffle ice cream cone, milkshakes, Coca-cola, peanut butter (although I learned today the first peanut butter patent was issued to a Canadian in Montreal). Some of these were invented in North America, but before the USA was founded. If you do some research you can find much more. It amounts to a considerable contribution to the culinary arts. Chocolate chip cookies alone are worthy of a lifetime achievement award. reply jltsiren 1 hour agorootparentWhile most of the others sound North American, jerky is not. Dried meat is a pretty universal concept, and jerky specifically is something Europeans got from the Inca. Even the word itself is borrowed from Quechua. reply jogjayr 1 hour agorootparentThanks! > Dried meat is a pretty universal concept Irrelevant in my opinion. Putting something on bread has probably existed since the day after someone invented bread. But we still give Italy credit for inventing pizza. Just like startups, the implementation is more important than the idea. reply Footingerm 1 hour agorootparentprevEarly version of hot dog was invented in germany Hamburger might also been created in germany Cornbread: Native Americans I give you breakfast cereals, toaster pastries, hot pockets, grits, pumpkin pie, milkshake. Coca-cola? do we now start to listen all types of drink recipetes? So pure cultural, usa invented easy foods. This has very little to do with cultural foods like cheese, or the million types of sausages and breads and etc. It does not amount to a considerable contribution to the culinary arts reply dahart 10 minutes agorootparentThe list of US-specific and US-influenced food is pretty long, and includes lots of ‘slow’ foods. Why are you basing your argument on an incomplete list of examples? https://en.wikipedia.org/wiki/List_of_American_foods And what do you mean that so-called ‘easy food’ isn’t a contribution to food culture? It’s trending globally (for better or worse), and relates closely to food supply economics. > Cornbread: Native Americans Native American foods count, why wouldn’t they? > This has very little to do with cultural foods like cheese https://en.wikipedia.org/wiki/List_of_American_cheeses reply jabroni_salad 1 hour agorootparentprevLouisiana really is this country's best kept secret. reply anon373839 8 hours agorootparentprev> What did the USA invent in comparison? Well, there’s California cuisine: https://guide.michelin.com/sg/en/article/features/california... reply rapsey 8 hours agorootparentprev> What did the USA invent in comparision? What is specific to USA? Even plenty of typical USA Food was invented in Europe. Low and slow style BBQ reply ToxicMegacolon 5 hours agorootparentBarbeque originated in the Caribbean reply TeaBrain 2 hours agorootparentThe regional BBQ culture of different sauces and meats in the US has nothing to do with what the word was used to describe in the Caribbean. What was described as BBQ hundreds of years ago in the Caribbean, as a way of cooking fish, wouldn't even be considered BBQ in the US. reply rapsey 4 hours agorootparentprevOh they were the first to cook meat on fire were they. /s reply OJFord 3 hours agorootparentprevFwiw I'm not American and get a good laugh out of /r/shitamericanssay sometimes, but I can think of: 'tex-mex', California roll sushi, deep dish pizza, bagels as sandwiches (I think? Not the bagel itself, but using it as a bun), different uses of okra than I'm aware of elsewhere like stewed/chowders/soup, some of that stuff in the south in general actually spicy shellfish chowders etc. I think there's a good argument is evolved from what came before it elsewhere. reply Almondsetat 9 hours agorootparentprevWhat is there to \"invent\" exactly? First of all, humans invent recipes based on the locally available raw foods, nothing special about that. Secondly, humans reinvent the same recipes time after time because we are all the same. Each culture invents their own stuffed dumplings, flat bread with toppings, etc. and they believe it's soooo unique and local reply darkwater 8 hours agorootparentYour strokes are too broad. Obviously different groups of humans in different times and locations are going to invent similar foods. But, for example speaking of flat bread, how you actually execute it and the differences in texture, thickness, crunchiness, what you put on top of it and what order can make two dishes that are the same macro idea but end up as two completely different things. reply Almondsetat 7 hours agorootparentCooking something for 15 minutes more or 15 minutes less can cause massive differences in texture and taste, and we are not even talking about the potentially big differences that having slightly different ingredients can cause. reply sva_ 6 hours agorootparentprevAnd curiously most older cultures have their own fermented food. reply veunes 1 hour agorootparentYep! Fermented foods have been a cornerstone of traditional diets in many cultures (Kimchi) reply brianfryer 5 hours agorootparentprevCajun cuisine and multiple types of BBQ come to mind. reply hnbad 8 hours agorootparentprevhttps://en.wikipedia.org/wiki/Tex-Mex would be an obvious candidate? I'm not sure how you'd make a claim like \"plenty of typical XYZ food was invented elsewhere\" when all food is similar to or a variation of or build upon something that came before unless we literally evolve new ways of ingesting nutrients. reply akdor1154 6 hours agorootparentprev> What did the USA invent in comparision? Clamato juice? reply api_or_ipa 2 hours agorootparentCanadians have elevated Clamato juice with the Caesar, the purest nectar of the gods. Mexicans come close with micheladas and Americans have bloody Mary’s but nothing compares to a proper Caesar. I think I might need to have one today. reply ginko 10 hours agoparentprev> The Irish and Germans didn't have potatoes. True, but fwiw I wouldn't consider potato to be particularly important in German cuisine. Sure you'll see it used quite a bit, but many of the more iconic dishes don't really need them or can be replaced like with bread dumplings for instance. reply morsch 29 minutes agorootparentI tried to find some data, and I have to say I'm surprised how low potato consumption is in Germany[1]: 59 kg per capita per year. Metaphorically unfocusing my eyes while looking at the linked map, that seems like it's about average for Europe. Another, more recent, statistic[2] bears this out, 54 kg, and also has surprising (to me) details. Of those 54 kg, fully two-thirds -- 38 kg -- are processed potatoes (the article names potato chips/crisps, ready-to-eat potato salad and, of course, fries) and just 16 kg are \"real\" fresh potatoes. Finally, things used to be different[3]: in 1950, the per-capita consumption was 186 kg! [1] https://landgeist.com/2021/12/21/potato-consumption-in-europ... attributed to FAO [2] https://www.bmel-statistik.de/ernaehrung/versorgungsbilanzen... German dept. of agriculture [3] https://www.statista.com/statistics/539799/per-capita-consum... it doesn't say, but I doubt people were eating a lot of potato chips in 1950 Germany reply jajko 9 hours agorootparentprevThe more east you go from Germany, the more important potatoes are to general population. Especially in the past, but even now they are more popular as side dish compared to ie rice or pasta, maybe due to bigger 'filling the stomach' effect that also lasts longer. reply littlecranky67 8 hours agorootparentPotatoes play a major role in german cuisine, a fact that you can also observe through architecture by looking at old houses or farms in Germany: The \"Kartoffelkeller\" (=\"potato cellar\") is a common storage room under a house with no windows/light, for long term storage of potatoes after the harvest. Often there would be slides under trapdoors to be accesible from outside the house, so you can fill the cellar with the potato harvest right from the tractor. People would get their basements filled to have enough potatoes to make it through the winter. Another cultural fact: \"Kartoffelferien\" (=\"potato holidays\") are still used by some elderly to describe the school holidays around october, because children needed to help with the potato harvest around that time. reply bleakenthusiasm 8 hours agorootparentprevAlso rice doesn't grow anywhere near Germany. Today that's not really s factor anymore, because it's so easy to shop, but my parents both grew up sticking very much to a local and seasonal approach to cooking, because everything else was new to them. They eat what they always knew best, so 5-6 days of the week the starchy side were potatoes. Rice is way more filling by transportation effort, but potatoes have been around their entire lives and in my dad's case also what his parents grew on their farm. reply hnbad 8 hours agorootparentOn a related note: German beans are different. You'll find canned kidney beans everywhere because combining them with sweet corn, bell peppers and onions with a seasoning overpowered by vinegar is a popular cheap side salad (often called \"Mexico salad\") and you'll find Heinz beans in tomato sauce but otherwise it's white beans or green beans. I was happy to discover canned pinto beans at my local supermarket but they were only available in a hot tomato sauce (branded as \"chilli beans\") - I only just found out the overpriced exotic Italian Wachtelbohnen collecting dust in the shelf next to them are pinto beans too. Heck, I'm nearly 40 and I've met Germans my age who were intrigued (or put off) by couscous because it's so exotic and they've never tried it before. I've talked to people running kebab joints (Dönerbuden) who said that they stopped offering lamb meat because the Germans didn't buy it and the few Turkish and Arab people who frequented them weren't enough to justify the overhead. reply rico_0803 7 hours agorootparentWhat German did you meet, that didnt know couscous? Granted, im a fair bit younger, but all of my friends and family know couscous and eat it fairly regularly (im german too). Which is to say: couscous is very well known in germany in general reply hnbad 8 hours agorootparentprevYes, they can be substituted but they're definitely a staple in good German Hausmannskost. Maybe not so much in the South: Swabia subsists almost entirely on Spätzle noodles and Bavarians seem to prefer various kinds of Knödel. But a good Rhineland Sauerbraten for example would normally be served with potatoes and a good Bauernomelette demands some crispy fried potato slices as well. Semmelknödel (but also often offered alongside potato dumplings) are a more common sight at special occasions or buffets. Many a young family's weekly rotation features spinach, fried eggs and potato mash alongside fish sticks, fried potatos with fried eggs and onions are a popular hearty lunch or late breakfast, and cooked potatos or mash are the default addition to some meat or sausage to the point a common microwave TV dinner still consists of Nuremberg sausages, sauerkraut and mash. reply micwag 7 hours agorootparent> Yes, they can be substituted but they're definitely a staple in good German Hausmannskost. Maybe not so much in the South: Swabia subsists almost entirely on Spätzle noodles and Bavarians seem to prefer various kinds of Knödel. While the classic northern \"Salzkartoffeln\" are basically non existent in Swabia, potatoes in general play a big role in traditional Swabian cuisine. Whether its \"Schupfnudeln\" (finger noodles), as salad, Knödel, fried potatoe slices, Hitzkuchen/Blootz/Dinnete (Pizza with potatoes instead of tomatoe/cheese), Kachelessen/Griebaschnecken/Schlanganger (various potatoe and milk dishes), Gaisburger Marsch (stew), \"sour eggs\" (potatoes and eggs in a vinegar sauce). But unfortunately most of those dishes are not really cooked anymore. (For \"sour eggs\" thats a good thing, this tasteless sour graybrown dish can die in hell for all I care.) reply CoastalCoder 6 hours agorootparentIn 2012 I was on the Tyrol valley near the Austrian Italian border, and IIRC the small restaurant we visited had potato pizza on its menu. reply agys 1 hour agorootparentprevYour list now makes me want to try to cook some german dishes! Thanks! reply wasmitnetzen 7 hours agorootparentprevYou forgot Kartoffelsalat, which is a traditional dish both in Bavaria and Swabia (and beyond). reply worstspotgain 10 hours agoprevI'm somewhat surprised that the article's list of conjectures didn't include one: that Italian Parsley may have just been viewed as superior for all use cases (which IMHO it kinda is, outside of Mexican recipes.) reply hrkfmud50k 4 hours agoparentDon't forget asian food! I feel the opposite: Cilantro is superior in all cases except perhaps Italian food, and even Italian food I am skeptical that Cilantro could be better if not for a bias from tradition. Parsley tastes so bitter and one-dimensional and you have to remove the leaves from the stem, whereas Cilantro is aromatic, fresh, and you can eat the stems making it trivial to prepare. I even go so far as to substitute cilantro for leafy greens in salad. reply karaterobot 1 hour agoprevThe article doesn't really answer the \"why\" part. Basically, it sounds like there were a lot of small things, mostly changing culinary influences. Of course, that doesn't answer the question of why they lost their taste for cilantro but not other herbs. Don't get me wrong, I liked learning the history, but it's a bad headline. reply zeristor 3 minutes agoprevI just came here to say \"Coriander\" reply OutOfHere 1 hour agoprevIt's entirely their loss. Coriander/cilantro is very good for sound mental health. Refer to PMC10385770. If you complain of a soapy taste, you're either using it wrong, or it's your genes, possibly both. reply miguel_rdp 1 hour agoprevIn Portugal it’s used quite a lot in the south in pork dishes, in a kind of bread soup (açorda) and also in a famous clam dish (bulhão pato-style clams). In the north it was very hard to find until 2 or 3 decades ago, and parsley is used a lot more. reply swasheck 2 hours agoprevdidn’t read tfa but my daughter has always hated cilantro saying it tasted like stink bugs. about 12 here’s ago she was diagnosed with celiac disease. It was kind of crippling as a family to figure out how we could holiday together outside of packaging and carrying along foods. We discovered Italy is extremely celiac friendly as it is a nationally recognized disease. I wonder if there’s any correlation between celiac and or related auto immune diseases and celiac palate. reply croisillon 9 hours agoprev“medieval Italians divided spices into two categories: sweet and strong” it’s funny because german language sometimes has this division as well: süß and pikant ; although pikant in a modern sense means a bit spicy reply PestoDiRucola 6 hours agoparentIt's the same in modern Italian: dolce (sweet) and piccante (spicy/strong) reply IshKebab 48 minutes agoprevCoriander, if you forgot what cilantro is. reply atmosx 10 hours agoprev> says Karima Moyer-Nocchi, a culinary historian What a fascinating profession. In Italy of all places! My guess is they are sought after at cocktail parties. reply petre 4 hours agoprevThe leaves taste like dishwashing shampoo. But the seeds are great. We mix them with green and black pepper. reply Havoc 7 hours agoprevI have that silly gene that makes Cilantro taste like soap so the less cilantro is in food the better for me. And yeah, tastes distinctly and strongly like soap. https://en.wikipedia.org/wiki/OR6A2 reply n2j3 5 hours agoprev [–] s/Italy/Greece. Virtually absent from mainstream cuisine reply OJFord 1 hour agoparent [–] Leaves maybe, the seeds are certainly used? And the leaves at least in dolmades comes to mind (not the vine leaf wrap obviously, but in the rice inside)? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Coriander, once a staple in ancient Roman cuisine, has nearly disappeared from modern Italian cooking, especially its leaves.",
      "The decline began post-Rome, influenced by Germanic tribes and the preference for exotic spices like cinnamon and cardamom.",
      "Despite its historical presence, coriander is now viewed as a foreign ingredient in Italy, unlike other herbs such as basil."
    ],
    "commentSummary": [
      "Italy's dislike for cilantro dates back to 1544 when botanist Pietro Andrea Mattioli described its leaves as smelling like bed bugs.",
      "The aversion may be genetically influenced, as some people perceive cilantro as tasting like soap due to a specific gene.",
      "The decline in cilantro's popularity in Italy is also linked to changing culinary trends and migration patterns, illustrating the complex interplay between genetics, culture, and personal preference in shaping food tastes."
    ],
    "points": 108,
    "commentCount": 116,
    "retryCount": 0,
    "time": 1720329769
  },
  {
    "id": 40895167,
    "title": "\"Out of Band\" network management is not trivial",
    "originLink": "https://utcc.utoronto.ca/~cks/space/blog/sysadmin/OutOfBandManagementNotTrivial",
    "originBody": "Chris Siebenmann :: CSpace » blog » sysadmin » OutOfBandManagementNotTrivial Welcome, guest. \"Out of band\" network management is not trivial July 6, 2024 One of the Canadian news items of the time interval is that a summary of the official report on the 2022 Rogers Internet and phone outage has been released (see also the CBC summary of the summary, and the Wikipedia page on the outage). This was an extremely major outage that took down both Internet and phone service for a lot of people for roughly a day and caused a series of failures in services and systems that turned out to rely on Rogers for (enough of) their phone and Internet connectivity. In the wake of the report, some people are (correctly) pointing to Rogers not having any \"Out of Band\" network management capability as one of the major contributing factors. Some people have gone so far as to suggest that out of band network management is an obvious thing that everyone should have. As it happens I have some opinions on this, and the capsule summary is that out of band network management is non-trivial. (While the outage 'only' cut off an estimated 12 million people, the total population of Canada is about 40 million people, so it directly affected more than one in four Canadians.) Obviously, doing out of band network management means that you need a dedicated set of physical hardware for your OOB network; separate switches, routers, local network cabling, and long distance fiber runs between locations (whether that is nearby university buildings or different cities). If you're serious, you probably want your OOB fiber runs to have different physical paths than your regular network fiber, so one backhoe accident can't cut both of them. This separate network infrastructure has to run to everything you want to manage and also to everywhere you want to manage your network from. This is potentially a lot of physical hardware and networking, and as they say it can get worse. (This out of band network also absolutely has to be secure, because it's a back door to your entire network.) When you set up OOB network management, you have a choice to make; is your OOB network the only way to manage equipment, or can you manage equipment either 'in-band' through your regular network or through the out of band network. If your OOB network is your only way of managing things, you not only have to build a separate network, you have to make sure it is fully redundant, because otherwise you've created a single point of failure for (some) management. If your OOB network is a backup, you don't necessarily need as much redundancy (although you may want some), but now you need to actively monitor and verify that both access paths work. You also have two access paths to keep secure, instead of just one. Security or rather access authentication is another complication for out of band management networks. If you need your OOB network, you have to assume that all other networks aren't working, which means that everything your network routers, switches, and so on need to authenticate your access has to be accessible through the OOB management network (possibly in addition to through your regular networks, if you also have in-band management). This may not be trivial to arrange, depending on what sort of authentication system you're using. You also need to make sure that your overall authentication flow can complete using only OOB network information and services (so, for example, your authentication server can't reach out to a third party provider's MFA service to send push notifications to authentication apps on people's phones). Locally, we have what I would describe as a discount out of band management network. It has a completely separate set of switches, cabling, and building to building fiber runs, and some things have their management interfaces on it. It doesn't have any redundancy, which is acceptable in our particular environment. Unfortunately, because it's a completely isolated network, it can be a bit awkward to use, especially if you want to put a device on it that would appreciate modern conveniences like the ability to send alert emails if something happens (or even send syslog messages to a remote server; currently our central syslog server isn't on this network, although we should probably fix that). In many cases I think you're better off having redundant and and hardened in-band management, especially with smaller networks. Running an out of band network is effectively having two separate networks to look after instead of just one; if you have limited resources (including time and attention), I think you're further ahead focusing on making a single network solid and redundant rather than splitting your efforts. (5 comments.) Written on 06 July 2024. « Gtk 4 has decided to blow up some people's world on HiDPI displays These are my WanderingThoughts (About the blog) Full index of entries Recent comments This is part of CSpace, and is written by ChrisSiebenmann. Mastodon: @cks Twitter @thatcks * * * Categories: links, linux, programming, python, snark, solaris, spam, sysadmin, tech, unix, web Also: (Sub)topics This is a DWiki. GettingAround (Help) Search: Page tools: View Source, Add Comment. Search: Login: Password: Atom Syndication: Recent Comments. Last modified: Sat Jul 6 22:24:49 2024 This dinky wiki is brought to you by the Insane Hackers Guild, Python sub-branch.",
    "commentLink": "https://news.ycombinator.com/item?id=40895167",
    "commentBody": "\"Out of Band\" network management is not trivial (utcc.utoronto.ca)105 points by DanAtC 15 hours agohidepastfavorite70 comments Animats 12 hours agoIn the entire history of the Bell System, no electromechanical exchange was ever down for more than 30 minutes for any reason other than a natural disaster. With one exception, a major fire in New York City. Three weeks of downtime for 170,000 phones for that.[1] The Bell System pulled in resources and people from all over the system to replace and rewire several floors of equipment and cabling. That record has not been maintained in the digital era. The long distance system did not originally need the Bedminster, NJ network control center to operate. Bedminster sent routing updates periodically to the regional centers, but they could fall back to static routing if necessary. There was, by design, no single point of failure. Not even close. That was a basic design criterion in telecom prior to electronic switching. The system was designed to have less capacity but still keep running if parts of it went down. [1] https://www.youtube.com/watch?v=f_AWAmGi-g8 reply benjojo12 11 hours agoparentThat electro mechanical system also switched significantly less calls than the digital counterparts! Most modern day telcos that I have seen still have multiple power/line cards/uplinks in place and designed for redundancy. However the new systems can also just do so much more and are so more flexible that they can be configured out of existence just as easily! Some of this as well is just poor software, on some of the big carrier grade routers you can configure many things but the combination of things that you can figure may also just cause things to not work correctly, or even worse pull down the entire chassis, I don't have immediate experience on how good the early 2000s software was, but I would take a guess and say that configurability/flexibility has had a serious cost on reliability of the network reply tsimionescu 9 hours agorootparentThe expectation should be that, as you switch more and more, so that the cost of a 30 minute pause gets higher and higher, the situation would improve, and a more modern system might have been expected to boast that it never had a break lasting more than, say, 30s outside of a natural disaster. reply kbenson 1 hour agorootparentI don't know where you get that expectation from. These are arbitrary engineering constraints informed by business decisions. If they decided that people could deal with up to 30 minutes of service interruption and set that as a goal, they would engineer with that in mind, regardless of how many people. If they used total combined user hours of interrupted service, then they would engineer around reducing possibly outage times for a system as it handled more people (or scale differently, with more systems). I don't think there's any sort of expectation that it would definitely go one way though, as you say. It's all business and legal constraints providing engineering constraints to build against. reply atoav 11 hours agorootparentprevAnd part of the reason why it is software is because people keep saying it is \"just\" software. Unreliability is unreliability even of it comes through software and we ahould treat broken software as broken, not as \"just a software error\". reply _kb 7 hours agorootparentThe core value proposition of software is the ability to implement most system designs relatively quickly and efficiently, including very bad ones. reply varjag 5 hours agorootparentprevNot sure about the calls, since apparently millennials and zoomers never pick up. reply macintux 6 hours agoparentprevA relatively famous example of the extent to which Indiana Bell went to avoid disrupting telephone service: rotating and relocating its headquarters over a few weeks. https://en.wikipedia.org/wiki/AT%26T_Building_(Indianapolis) reply bcrl 4 hours agoparentprevBell Canada had a major outage on July 17th 1999 when a tool was dropped on the bus bar for the main battery power that ignited the hydrogen from the batteries in one of the exchangeds in downtown Toronto. The fire department insisted that all power in the area be shut down which lead to the main switch that handled long distance call routing for all 1-800 numbers being offline for the better part of a day. reply gosub100 4 hours agoparentprevthat was back before the phone system was defiled by robo-dialers and scammers. reply Scoundreller 12 hours agoprevOne thing that was fascinating about the Rogers outage was on the wireless side: because \"just\" the core was down, the towers were still up. So mobile phones would try to make a connection to the tower just enough to connect but not be able to do anything, like call 9-1-1 without trying to fail-over to other mobile networks. Devices showed zero bars, but field test mode would show some handshake succeeding. (The CTO was roaming out-of-country, had zero bars and thought nothing of it... how they had no idea an enterprise-risking update was scheduled, we'll never know) Supposedly you could remove your SIM card (who carries that tool doohickey with them at all times?), or disable that eSIM, but you'd have to know that you can do that. Unsure if you'd still be at the mercy of Rogers being the most powerful signal and still failing to get your 9-1-1 call through. Rogers claimed to have no ability to power down the towers without a truck-roll (which is how another aspect where widespread OOB could have come in handy). Various stories of radio stations (which Rogers also owns a lot of) not being able to connect the studio to the transmitter, so some tech went with an mp3 player to play pre-recorded \"evergreen\" content. Others just went off-air. https://www.theregister.com/2022/07/25/canadian_isp_rogers_o... reply wannacboatmovie 10 hours agoparent> Supposedly you could remove your SIM card (who carries that tool doohickey with them at all times?) In sane handsets (ones where the battery is still removable), that tool was and still is a fingernail, which most have on their person. I believe the innovation of the need for a special SIM eject tool was bestowed upon us by the same fruit company that gave us floppy and optical drives without manual eject buttons over 30 years ago. reply rzzzt 10 hours agorootparentYou could operate the ejection mechanism by hand both on optical and floppy disk drives with an uncurled paperclip (or a SIM card ejection tool were they to exist at that point in time). But I wouldn't ascribe the introduction of the motorized tray to the fruit company, it was the wordmark company: https://youtu.be/bujOWWTfzWQ reply pgraf 9 hours agoparentprevSounds like a problem that should be (rather easily) fixable in the Operating System, no? If the emergency call doesn’t go through, try the call over a different network. This would also mitigate problems we see from time to time where emergency calls don’t work because the uplink to the emergency call center was impacted either physically or by a bad software update. reply sidewndr46 5 hours agorootparentVery few phones have the calls handled by the OS. It usually is handled by something called the baseband, which is more like firmware reply Scoundreller 8 hours agorootparentprevYes, seems to be a deficiency in the gsm spec as this affected most/all devices. I didn’t think this failure mode was even possible. reply throw0101d 7 hours agoparentprev> (The CTO was roaming out-of-country, had zero bars and thought nothing of it... how they had no idea an enterprise-risking update was scheduled, we'll never know) How does one know ahead of time if any particular change is \"enterprise-risking\"? It appeared to be a fairly routine set of changes that were going just fine: > The report summary says that in the weeks leading up to the outage, Rogers was undergoing a seven-phase process to upgrade its network. The outage occurred during the sixth's phase of the upgrade. * https://www.cbc.ca/news/politics/rogers-outage-human-error-s... It turns out that they self-DoSed certain components: > Staff at Rogers caused the shutdown, the report says, by removing a control filter that directed information to its appropriate destination. > Without the filter in place, a flood of information was sent into Rogers' core network, overloading and crashing the system within minutes of the control filter being removed. * Ibid > In a letter to the CRTC, Rogers stated that the deletion of a routing filter on its distribution routers caused all possible routes to the internet to pass through the routers, exceeding the capacity of the routers on its core network. * https://en.wikipedia.org/wiki/2022_Rogers_Communications_out... > Rogers staff removed the Access Control ListFootnote 5 policy filter from the configuration of the distribution routers. This consequently resulted in a flood of IP routing information into the core network routers, which triggered the outage. The core network routers allow Rogers wireline and wireless customers to access services such as voice and data. The flood of IP routing data from the distribution routers into the core routers exceeded their capacity to process the informationFootnote 6. The core routers crashed within minutes from the time the policy filter was removed from the distribution routers configuration. * https://crtc.gc.ca/eng/publications/reports/xona2024.htm These types of things happen: > In October, Facebook suffered a historic outage when their automation software mistakenly withdrew the anycasted BGP routes handling its authoritative DNS rendering its services unusable. Last month, Cloudflare suffered a 30-minute outage when they pushed a configuration mistake in their automation software which also caused BGP routes to be withdrawn. * https://www.kentik.com/blog/a-deeper-dive-into-the-rogers-ou... reply vitus 4 hours agorootparentRouter config changes are simultaneously very commonplace and incredibly risky. I've seen outages caused by a single bad router advertisement that caused global crashes due to route poisoning interacting with a vendor bug. RPKI enforcement caused massive congestion on transit links. Route leaks have DoSed entire countries (https://www.internetsociety.org/blog/2017/08/google-leaked-p...). Even something as simple as a peer removing rules for clearing ToS bits resulted in a month of 20+ engineers trying to figure out why an engineering director was sporadically being throttled to ~200kbps when trying to access Google properties. Running a large-scale production network is hard. edit: in case it is not obvious: I agree entirely with you -- the routine config changes that do risk the enterprise are often very hard to identify ahead of time. reply Scoundreller 29 minutes agorootparentthe report stated: https://crtc.gc.ca/eng/publications/reports/xona2024.htm > \"this configuration change was the sixth phase of a seven-phase network upgrade process that had begun weeks earlier. Before this sixth phase configuration update, the previous configuration updates were completed successfully without any issue. Rogers had initially assessed the risk of this seven-phased process as “High.” > However, as changes in prior phases were completed successfully, the risk assessment algorithm downgraded the risk level for the sixth phase of the configuration change to “Low” risk\" > Downgrading the risk assessment to “Low” for changing the Access Control List filter in a routing policy contravenes industry norms, which require high scrutiny for such configuration changes, including laboratory testing before deploying in the production network. Overall, the lack of detail of the (regulator forced) post-mortem makes it impossible for the public to decide. It's a Canadian telecom: They'll release detail when it makes them look good, and hide it if it makes them look bad. reply immibis 6 hours agorootparentprevBTW: anyone who wants to really experience how complex internet routing is, go and join DN42 (https://dn42.dev). This is a fake internet built on a network of VPN tunnels and using the same routing systems. As long as you're just acting as a leaf node, it's pretty straightforward. If you want to attach to the network at multiple points and not just VPN them all to the same place, now you have to design a network just like an ISP would, with IGP and so on. reply sidewndr46 5 hours agoparentprevIt is likely no one at the C-Level knew the update had any risk reply Scoundreller 26 minutes agorootparentit was \"high risk\", and then downgraded to \"low risk\" as this several week and phase long project proceeded without initial issue. reply gavindean90 13 hours agoprevI’m reminded of when an old AT&T building went on sale as a house, and one of its selling points was that you could get power from two different power companies if you wanted. This highlighted to me the level of redundancy required to take such things seriously. It probably cost the company a lot to hook up the wires, and I doubt the second power company paid anything for the hookup. Big Bell did it there, and I’m sure they did it everywhere else too. Edit: I bet it had diesel generators when it was in service with AT&T to boot. reply yaantc 11 hours agoparent> I bet it had diesel generators when it was in service with AT&T to boot. 20 to 25 years ago I visited a telecom switch center in Paris, the one under the Tuileries garden next to the Louvre. They had a huge and empty diesel generators room. They had all been replaced by a small turbine (not sure it's the right English term), just the same as what's used to power an helicopter. It was in a relatively small soundproof box, with a special vent for the exhaust, kind of lost on the side of a huge underground room. As the guy in charge explained to us, it was much more compact and convenient. The big risk was in getting it started, this was the tricky part. Once started it was extremely reliable. reply tonyarkles 4 hours agorootparent> by a small turbine (not sure it's the right English term) That's the right English word yes. And that's pretty cool! reply Scoundreller 12 hours agoparentprev> Edit: I bet it had diesel generators when it was in service with AT&T to boot. That's where AT&T screwed up in Nashville when their DC got bombed. They relied on natural gas generators for their electrical backup. No diesel tank farm. Big fire = fire department shuts down natural gas as wide as deemed necessary and everything slowly dies as the UPS batteries die. They also didn't have roll-up generator electrical feed points, so they had to figure out how to wire those up once they could get access again, delaying recovery. https://old.reddit.com/r/sysadmin/comments/kk3j0m/nashville_... reply thakoppno 13 hours agoparentprevhttps://www.realtor.com/realestateandhomes-detail/13229-Sout... Listing removed a couple weeks ago. reply woleium 13 hours agorootparentCrypto Collective eh? reply transcriptase 13 hours agoprevIt’s trivial when you have the resources that come from being one of Canada’s 3 telecom oligopoly members. Unfortunately the CRTC is run by former execs/management of Bell, Telus, and Rogers, and our anti-competition bureau doesn’t seem to understand their purpose when they consistently allow these 3 to buy up and any all small competitors that gain even a regional market share. Meanwhile their service is mediocre and overpriced, which they’ll chalk up to geographical challenges of operating in Canada while all offering the exact same plans at the exact same prices, buying sports teams, and paying a reliable dividend. reply Scoundreller 12 hours agoparentIt's worse than that: 2 of the 3 telecom oligopoly members share (most) of their entire wireless network, with one providing most towers in the West, and the other in the East. I'm sure those 2 compete very hard with each other with that level of co-dependency. reply 1992spacemovie 13 hours agoprevThere is OOB for carriers and OOB for non-carriers. OOB for carriers is significantly more complex and resource intensive than OOB for non-carriers. This topic (OOB or to forgo) has been beat to death over the last 20 years in the operator circles; the responsible consensus is trying to shave a % off operating expenses by cheaping out on your OOB is wrong. That said it does shock me that one of the tier-1 carriers in Canada was this... ignorant? Did they never expect it to rain or something? Wild. reply ChuckMcM 3 hours agoprevReminds me of a data center that said they had a backup connection and I pointed out that only one fiber was coming into the data center. They said, \"Oh its on a different lambda[1]\" :-) [1] Wave division multiplexing sends multiple signals over the same fiber by using different wavelengths for different channels. Each wavelength is sometimes referred to as a lambda. reply goatsi 13 hours agoprevWhen I see out of band management at remote locations (usually for a dedicated doctors network run by the health authority that gets deployed at offices and clinics) it's generally analog phone line -> modem -> console port. Dialup is more than enough if all you need to do is reset a router config. Not 100% out of band for a telco though, unless they made sure to use a competitors lines. reply no_carrier 11 hours agoparentHere in Australia, POTS lines have been completely decommissioned, UK will be switched off by end of 2025 and I'm assuming there's similar timelines in lots of other countries. reply vladvasiliu 8 hours agorootparentThey're on the way out in France, too. New buildings don't get copper anymore, only fiber. However, as I understand it, at least for commercial use, the phone company provides some kind of box that has battery-backing so it can provide phone service for a certain duration in case of emergency. reply tonyarkles 4 hours agorootparentThe tricky part with that is that, at least in Canada, the RJ11 ports on the ONT are generally VoIP. They provide the appropriate voltages for a conventional POTS phone to work but digitize & compress the audio and send it along to the Telco as SIP or whatever. That works fine for voice but you're probably going to have a hard time using a conventional POTS modem over that connection. I've never tested it and am honestly pretty curious to see how well/poorly it would work. reply hujun 7 hours agorootparentprevnow there are LTE modems reply jethro_tell 4 hours agorootparentIf you are the lte network, it gets a little tough to do oob that way, especially if you’re basically a monopoly in many service areas. reply ralferoo 7 hours agoprevFrom TFA: > If your OOB network is your only way of managing things, you not only have to build a separate network, you have to make sure it is fully redundant, because otherwise you've created a single point of failure for (some) management. I'm not sure I necessarily agree with that. You can set up the network in such a way that you can route over the main network as a backup if your OOB network was down but the main network was up. Obviously, it's not quite as simple as sticking a patch cable between the two networks, but it can be close - you have a machine that's always on your OOB network, and it has an additional port that either configures itself over DHCP or has a hard-coded IP for the main net. But the important thing is that you never have that patched in, except for emergencies like your OOB network cable being severed but you still have access to the main network. If that does happen, you plug it in temporarily and use that machine as a proxy. There's no real reason for extra redundancy in the OOB, because if your main uplink is also severed, there's not really much you're going to be usefully configuring anyway! reply siebenmann 7 hours agoparentIn a lot of environments, you can at least choose to restrict what networks can be used to manage equipment; sometimes this is forced on you because the equipment only has a single port it will use for management or must be set to be managed over a single VLAN. Even when it's not forced, you may want to restrict management access as a security measure. If you can't reach a piece of equipment with restricted management access over your management-enabled network or networks, for instance because a fiber link in the middle has failed, you can't manage it (well, remotely, you can usually go there physically to reset or reconfigure it). You can cross-connect your out of band network to an in-band version of it (give it a VLAN tag, carry it across your regular infrastructure as a backup to its dedicated OOB links, have each location connect the VLAN to the dedicated OOB switches), but this gets increasingly complex as your OOB network itself gets complex (and you still need redundant OOB switches). As part of the complexity, this increases the chances an in-band failure affects your OOB network. For instance, if your OOB network is routed (because it's large), and you use your in-band routers as backup routing to the dedicated OOB routers, and you have an issue where the in-band routers start exporting a zillion routes to everyone they talk to (hi Rogers), you could crash your OOB network routers from the route flood. Oops. You can also do things like mis-configure switches and cross over VLANs, so that the VLAN'd version of your OOB network is suddenly being flooded with another VLAN's traffic. (I am the author of the original article.) reply knocknock 13 hours agoprevMy previous org OOB used a data only SIM card from a different service provider. Curious why that wouldn't be a good solution? reply solatic 13 hours agoparent1. The risk, when you use a competitor's service, of your competitor cutting off service, especially at an inopportune time (like your service undergoing a major disruption, where cutting off your OOBM would be kicking you while you are down, but such is business). 2. The risk that you and your competitor unknowingly share a common dependency, like utility lines; if the common dependency fails then both you and your OOBM are offline. The whole point of paying for and maintaining an OOBM is to manage and compensate for the risks of disruption to your main infrastructure. Why would you knowingly add risks you can't control for on top of a framework meant to help you manage risk? It misses the point of why you have the OOBM in the first place. reply tonyarkles 4 hours agorootparentMaybe 10-15 years ago there was a local Rogers outage that would have had the #2 failure you're describing. From what I recall, SaskTel had a big bundle of about 3,000 twisted pairs running under a park. Some of those went to a SaskTel tower, some to SaskTel residential wireline customers and some of those went to a Rogers facility. Along comes a backhoe and slices through the entire bundle. reply synack 13 hours agoprevWith launch costs dropping, I wonder if there’s a market for a low bandwidth “ssh via satellite” service. Could use AWS Ground Station to connect to your VPC. reply rlt 12 hours agoparentWhy not Starlink? ~$100/month/site is pretty low cost. reply synack 12 hours agorootparentIf this is for use during outages, I want to know exactly what network path is used, ideally with as few hops as possible. Starlink can’t guarantee that. reply yusyusyus 11 hours agorootparentwhy? from my pov, once i’ve bought the service from the provider, their job is to deliver however they can; not my business, not my problem. my problem is making sure my redundancy (if required) isnt fate sharing. reply erincandescent 10 hours agorootparentBecause in networking, if you buy two uplinks and don't check the paths they're taking, fate demands that the fiber seeking back hoe just took out that one duct it turns out both of your \"redundant\" lines go down reply yusyusyus 10 hours agorootparenteven with KMZs supplied, this still happens. complications in some cases. but an IP product (like starlink), i dont see the same equivalence. at what point does fate sharing analysis end in such a scenario? reply dsr_ 7 hours agorootparentIt does not end! That's the point. If you want a reliable separate path, you must test it, and you must be prepared to spend time and money on fixing it. The tests include calling up the engineering manager for the separate path and verifying that it has not been \"re-groomed\" into sharing a path with your primary -- monthly or quarterly, depending on your risk tolerance. Operations work does not end because the world keeps changing. reply yusyusyus 4 hours agorootparentit certainly ends in somewhere resembling cost-effective. \"reliable\" has its meanings in context, and backhoe issues aren't so much of a problem architecturally for starlink. they have incentive and capability to get that traffic off the shared fate should it occur (even if that extends up to starlink serving one of their IP transit providers for OOB). that's why i question the wisdom of being overly concerned with starlink's particular paths. reply erincandescent 7 hours agorootparentprevFor transit I would want to know the path I'm taking up to the point the supplier has redundancy From there the worst that can happen generally is that the packets spiral the wrong way around the continent reply vitus 5 hours agorootparent> the wrong way around the continent I see you haven't met Google's production backbone network(s)... We intentionally didn't connect the Middle East and India (due to a combination of geopolitics and concerns around routing instability), so any traffic between the two would go the long way around the world, incurring a 200+ ms RTT penalty. (There was a ThousandEyes report back in 2018 that gave us a black eye. See pages 20-22 of https://pc.nanog.org/static/published/meetings/NANOG75/1909/...) Agreed entirely on your point that if you're buying multiple redundant links, you're responsible for making sure that they're actually relying on different underlying fiber spans. reply immibis 5 hours agorootparentprevFancy seeing you here. Can I know why you guys hate me? I remain banned from FIX and AfRA. reply jen729w 10 hours agorootparentprevRight, and it’s not as if you’d own the wired line anyway. That’d be leased just the same way your Starlink connection would be. reply immibis 5 hours agorootparentprevBecause it turns out your Starlink connection goes to a ground station in your city which is connected to your network which is the one that is broken. So you can't manage it through Starlink when it's broken. reply dkbrk 9 hours agorootparentprevI'm pretty sure I saw it mentioned that if the source and destination are Starlink dishes then packets will be routed by the satellites directly to the destination dish without going through any ground stations. That means Starlink can, in fact, guarantee communications during outages (so long as the Starlink network itself isn't down). You just need to have Starlink service at both the send and receive sides and the communication effectively acts as a direct link. reply walterbell 13 hours agoprev> hardened in-band management What would this look like in practice? Management interfaces like BSPs don't have a great security track record. reply stingraycharles 13 hours agoparentI can only assume it’s based on VLAN for security (and probably dedicated ports assigned to VLANs so regular ports are never able to access the VLAN), but other than that, I have a hard time envisioning in-band management that doesn’t lock you out when the network goes down. It would protect you against things like DDoS attacks, and you can even assign dedicated (prioritized) access for these management ports. It’s an economical decision I suppose. reply ianpenney 13 hours agoprevHam radio. Meshtastic. Knowing your neighborhood. reply kkfx 13 hours agoprevApart from Rogers et alike, the main OOB/LOM issue is that's mostly only very old iron very few know, finding people who knows and finding non-hacky homegrown and not much tested solutions it's damn hard. reply jeffrallen 11 hours agoprevI love ChrisO so much, and it's funny but often he's talking about something I'm currently working on too. Thank you to Chris and to whoever posts his articles here. reply pharos92 13 hours agoprevI disagree, Out of Band Network Management (OOBM) is extremely trivial to implement. Most companies however don't see the value of OOBM until they have a major fault. The setup costs can be high, and the ongoing operational costs of OOBM infrastructure and links is also significant. I've built dozens of OOBM networks using fibre and 4G with the likes of Opengear. In instances, often deploying OOBM ahead of infrastructure rollouts so hardware can be delivered to site directly from factory, rather than go through a staging environment which adds time, cost and complexity. reply 1992spacemovie 13 hours agoparentOOB for carriers is significantly more complex; especially when you may be the only realistic access option in certain locations. However, given the rise of Starlink I think it becomes closer to \"trivial\" when the math becomes $100/mo/location + some minimal always-on OOB infrastructure on prem + cloud. Even in heavy-monopoly situations, you can usually guarantee the Starlink to Internet path due to the traffic bypassing the transport carriers on the ground (bent pipe to LEO sat) and landing at IXPs/near telco houses which egress direct to transit carriers. reply godelmachine 13 hours agoparentprevWe have a major incident wherein our firewall was totally down last month. The director at the end suggested that we need to have RS232 cable for out of band communication for such eventualities in the future. Makes one realize the reliability of RS232 in today’s day and age. reply kjellsbells 5 hours agoprevI worry that this misses the point a little. All the OOB in the world will not help you if you cannot reach the management entity (eg IP-enabled PSU, terminal server, etc). It is also insufficient to protect against second order thundering-herd-type problems (e.g.: you log in, stop a worker process, and upstream, traffic is directed away from the node to the others, and starts causing new problems). In telco operations, every MoP should have: an unambiguous linear sequence of steps, a procedure to verify that the desired result has been achieved, and a backout plan if things do go bad. This is drilled into you at every telco I ever worked at. Rogers' cardinal sin on the day of the outage was that they didn't have a backout plan at each step of the MoP. More structurally, networks have a dependency graph that you ignore at your peril. X depends on Y depend on Z, and so on. And yes, loops are quite possible! OOB management is an attempt to add new links to the graph that only get used in a crisis. These kind of pull-it-out-when-you-need-it solutions are fine, but have a tendency to fail just when you need them. For one, they don't get exercised enough, and two, they may have their own dependencies on the graph that are not realized until too late. So, what would this Internet rando prescribe? First order of business is to enumerate the dependency graph. I would wager that BGP, DNS, and the identity system are at or near the very top. Notice the deadly embrace of DNS and ID: if DNS is down, ID fails. Next, study the failure modes of the elements. In the Rogers outage, a lack of route filters crashed a core router. That's a vague word, \"crashed\". Are we talking core dumps and SEGVs? Are we talking response times that skyrocketed, leading to peers timing out? Rogers really need to understand that. Typically in telco networks when nodes get \"congested\" like this there are escape valves built into the control plane protocol, eg a response that says \"please back off and retry in rand(300)\". They need to have a conversation with Cisco/Juniper etc and their router gurus about this. Finally, the telco industry (or what's left of it) needs to do some introspection about the direction it is pulling vendors. For the last 15 years, telcos have been convinced that if only that can ingest some of that sweet, sweet cloud juice, their software costs will drop, they can slash operations costs, and watch the share price go brrr. Problem is, replacing legacy systems with ones cobbled together by vendors from a patchwork of kubernetes and prayers is guaranteed not to lead to the level of reliability that telcos and their regulators expect. If I'm a Rogers' operations manager and my network dies, I don't want to hear that some dude in India has to spend the next week picking through a service mesh and experimenting with multus to decide if turning if off and on again is gonna work. reply 1992spacemovie 55 minutes agoparentAll great points- it sounds like you have a similar cultural awareness of the telco space. I'll reply to a few things that caught my brain's attention: > All the OOB in the world will not help you if you cannot reach the management entity (eg IP-enabled PSU, terminal server, etc). In _healthy_ OOB situations, all of the adjacent OOB infrastructure should be reachable, even if the entire core IP network is completely tanked. The only scenario where this would not apply in my eyes would be a power outage that whacks an entire site including the OOB gear. But in that scenario OOB doesn't help you. > Next, study the failure modes of the elements. In the Rogers outage, a lack of route filters crashed a core router. That's a vague word, \"crashed\". Are we talking core dumps and SEGVs? Are we talking response times that skyrocketed, leading to peers timing out? Rogers really need to understand that. Typically in telco networks when nodes get \"congested\" like this there are escape valves built into the control plane protocol, eg a response that says \"please back off and retry in rand(300)\". They need to have a conversation with Cisco/Juniper etc and their router gurus about this. Typically the \"crash\" is memory exhaustion due to incorrectly configured filtering between either routing protocols, or someone blasting a BGP peer with a large number of unexpected routes. As a former support engineer for BIGCO-ROUTER-COMPANY (either C.. or J..), I can't tell the number of times I've seen people melt down a large sized network due to either exceeding a defined prefix limit (limiting number of routes allowed), or accidentally nuking an ACL controlling route-redistribution, and either cratering all connectivity (no routes), or dump all routes unrestrictedly (no filter), with the latter resulting in memory exhaustion. Luckily, everyone these days working with big routers are culturally conditioned to do change-commit confirmation - if you make a change that blows the box up and isolates it, it will automatically revert the change after a defined period of time. > Finally, the telco industry (or what's left of it) needs to do some introspection about the direction it is pulling vendors. For the last 15 years, telcos have been convinced that if only that can ingest some of that sweet, sweet cloud juice, their software costs will drop, they can slash operations costs, and watch the share price go brrr. Problem is, replacing legacy systems with ones cobbled together by vendors from a patchwork of kubernetes and prayers is guaranteed not to lead to the level of reliability that telcos and their regulators expect. If I'm a Rogers' operations manager and my network dies, I don't want to hear that some dude in India has to spend the next week picking through a service mesh and experimenting with multus to decide if turning if off and on again is gonna work. I think your perception of the quality of a K8 telco stack is a bit off to be candid. They are not cobbling together random stacks from unvetted vendors/sources. Nearly every telco K8 stack these days is using an off the shelf K8 vendor, and off the shelf K8-compatible services on top, again from (reputable) vendors. At the end of the day this was a failure of culture and management. The technology is a side conversation. reply bigcat12345678 14 hours agoprevWho said it is trivial?... Edit: The article take a title and describe some straightforward technical and business investments to make oob management network work. reply benreesman 13 hours agoprev [3 more] [flagged] clhodapp 12 hours agoparent [–] Western Electric! The Western Electric -> Western Digital substitution seems to be common for whatever reason reply benreesman 12 hours agorootparent [–] +1 for the correction, Western Digital is in fact not the brand that most of this happened under. Thank you Sir or Madame for keeping me honest in my strident claims! reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The 2022 Rogers Internet and phone outage, affecting over 12 million Canadians, highlighted the importance and complexity of \"Out of Band\" (OOB) network management.",
      "OOB management requires dedicated hardware and infrastructure separate from the main network, ensuring redundancy and security but adding significant cost and complexity.",
      "Smaller networks might find it more practical to focus on robust, redundant in-band management rather than maintaining a separate OOB network."
    ],
    "commentSummary": [
      "\"Out of Band\" (OOB) network management is essential for maintaining network reliability, especially in modern digital systems that are prone to misconfigurations and failures.",
      "The Rogers outage underscored the critical need for robust OOB management, highlighting the importance of better planning and risk assessment to prevent core network failures.",
      "Effective OOB management involves using dedicated, redundant systems, often leveraging different providers or technologies such as LTE or satellite to ensure reliability."
    ],
    "points": 105,
    "commentCount": 70,
    "retryCount": 0,
    "time": 1720324841
  },
  {
    "id": 40896873,
    "title": "Execute JavaScript in a WebAssembly QuickJS sandbox",
    "originLink": "https://github.com/sebastianwessel/quickjs",
    "originBody": "This TypeScript package allows you to safely execute JavaScript code within a WebAssembly sandbox using the QuickJS engine. Perfect for isolating and running untrusted code securely, it leverages the lightweight and fast QuickJS engine compiled to WebAssembly, providing a robust environment for code execution.Features- *Security*: Run untrusted JavaScript code in a safe, isolated environment.- *File System*: Can mount a virtual file system- *Custom Node Modules*: Custom node modules are mountable- *Fetch Client*: Can provide a fetch client to make http(s) calls- *Test-Runner*: Includes a test runner and chai based `expect`- *Performance*: Benefit from the lightweight and efficient QuickJS engine.- *Versatility*: Easily integrate with existing TypeScript projects.- *Simplicity*: User-friendly API for executing and managing JavaScript code in the sandbox.",
    "commentLink": "https://news.ycombinator.com/item?id=40896873",
    "commentBody": "Execute JavaScript in a WebAssembly QuickJS sandbox (github.com/sebastianwessel)104 points by sebastianwessel 7 hours agohidepastfavorite32 comments This TypeScript package allows you to safely execute JavaScript code within a WebAssembly sandbox using the QuickJS engine. Perfect for isolating and running untrusted code securely, it leverages the lightweight and fast QuickJS engine compiled to WebAssembly, providing a robust environment for code execution. Features - *Security*: Run untrusted JavaScript code in a safe, isolated environment. - *File System*: Can mount a virtual file system - *Custom Node Modules*: Custom node modules are mountable - *Fetch Client*: Can provide a fetch client to make http(s) calls - *Test-Runner*: Includes a test runner and chai based `expect` - *Performance*: Benefit from the lightweight and efficient QuickJS engine. - *Versatility*: Easily integrate with existing TypeScript projects. - *Simplicity*: User-friendly API for executing and managing JavaScript code in the sandbox. jitl 5 hours agoHi, I’m the author of the underlying quickjs-emscripten runtime library. I like your ergonomic kind of “standard library” for quickjs-emscripten :) Did you try running in the browser or with a bundler? I think accepting the variant name as a string you pass to import(variantName) dynamically may not play well with Webpack et al. EDIT: SECURITY WARNING: this library exposes the ability for the guest (untrusted) code to `fetch` with the same cookies as the host `fetch` function. You must not run untrusted code if enabling `fetch`. Library should come with a big blinking warning about what is safe and unsafe to enable when running untrusted code. It’s not a “sandbox” if the sandboxed code can call arbitrary HTTP APIs authenticated as the host context! The reason quickjs-emscripten is low-level and avoids magic is so I can confidently claim that the APIs it does provide are secure. I generally reject feature requests for magical serialization or easy network/filesystem access because that kind of code is a rich area for security mistakes. When you run untrusted code, you should carefully audit the sandbox itself, but also audit all the code you write to expose APIs to the sandbox. In this case a comment from an other HN user asking about Fetch cookies tipped me off to the potential security issue. More reading: Figma blog posts on plugin sandbox security: - https://www.figma.com/blog/how-we-built-the-figma-plugin-sys... - https://www.figma.com/blog/an-update-on-plugin-security/ Quickjs-emscripten README: https://github.com/justjake/quickjs-emscripten reply AlexErrant 2 hours agoprevThere are many ways to sandbox Javascript, both serverside and browser-side. Are there any ways to \"sandbox\" DOM access? I.e. give untrusted 3rd parties access to a DOM element in a predefined spot? AFAIK the only tech that allows for this is iframes, which are unfortunately heavy and slow. I'm writing an app that can host plugins, and unfortunately, I think giving plugins DOM access means they can now literally do literally _anything_. reply spankalee 1 hour agoparentSalesforce does this with a combination of web components, with a patched up ShadowRoot so that code with a reference to the shadow root can't walk into the rest of the document, and a secure evaluator function related to SES (Secure EcmaScript) to limit the globals the untrusted script has access too. The secure evaluator is wild. I think this is the heart of it: https://github.com/Agoric/realms-shim/blob/v1.1.0/src/evalua... There's also an idea for isolated web components to solve this in the platform: https://github.com/WICG/webcomponents/issues/1002 reply cxr 1 hour agorootparentYou can also check out the discussion for Figma's earlier work on their plugin system, which is what inspired jitl (above) to create quickjs-emscripten. Previously: How to build a plugin system on the web and also sleep well at night.2019 August 22. 89 comments. reply cxr 2 hours agoparentprevThe closest thing I know of is Allen Wirfs-Brock's jsmirrors prototype, but he never got to speccing out anything for DOM (and never really intended to as far as I know). Just capabilities for JS-the-programming-system. You could look at jsmirrors for inspiration and take a crack at some sort of \"dommirrors\" yourself, but it's big undertaking. (There's a roundabout way to go about using jsmirrors as-is to kind of achieve what you want, but it's not ergonomic.) That being said, giving access to the DOM, even mediated/simulated, is almost certainly not what you really want. Figure out what you _actually_ want to allow the other side to do, and then just give them a capability that lets them do it. (For example, to let them add a button somewhere, you might think you need to give them an anchor point (parent element) where they can insert it and let them use `document.createElement` to make the DOM node that they're going to put there. But you don't actually want that—for them to have access to `document.createElement`, etc. What you want is for them to have an add-button capability. So give them that—go implement `addButton`.) Moar:PS: don't listen to anyone who comes along and says that this is what CSP is for. It's not. (If we're being accurate, even for what CSP really is for, it's poorly designed, user-hostile junk and should never have been implemented or extended as far as it has been.) It's dangerous to depend on it. reply emurlin 4 hours agoprevInteresting approach! As an author of another JS sandbox library[1] that uses workers for isolation plus some JS environment sanitisation techniques, I think that interpreting JS (so, JS-in-JS, or as in this case, JS-in-WASM) gives you the highest level of isolation, and also doesn't directly expose you to bugs in the host JS virtual machine itself. Since you're targeting Node, this is perhaps even more important because (some newer developments notwithstanding) Node.js doesn't really seem to have been designed with isolation and sandboxing in mind (unlike, say, Deno). From the API, I don't see if `createRuntime` allows you to define calls to the host environment (other than for `fetch`). This would be quite a useful feature, especially because you could use it to restrict communication with the outside world in a controlled way, without it being an all-or-nothing proposition. Likewise, it doesn't seem to support the browser (at least, running a quick check with esm.sh). I think that that could be a useful feature too. I'll run some tests as I'm curious what the overhead is in this case, but like I said, this sounds like a pretty solid approach. [1] @exact-realty/lot reply jitl 4 hours agoparentI’m the author of the underlying quickjs-emscripten library. It supports the browser (specifically tested with ESM.sh), as well as Cloudflare Workers, NodeJS, Deno: https://github.com/justjake/quickjs-emscripten?tab=readme-ov... It has APIs for exposing host functions, calling guest functions, custom module loaders, etc: https://github.com/justjake/quickjs-emscripten?tab=readme-ov... API docs for newFunction: https://github.com/justjake/quickjs-emscripten/blob/main/doc... reply brigadier132 2 hours agorootparentWow cloudflare workers support is actually super cool. How does it limit memory usage? reply jitl 1 hour agorootparentThe quickjs interpreter C code counts the bytes it's allocated, and refuses to allocate more if over the limit. It decrements by the allocation size when freed. This malloc function is used everywhere the interpreter allocates memory: static void *js_def_malloc(JSMallocState *s, size_t size) { void *ptr; /* Do not allocate zero bytes: behavior is platform dependent */ assert(size != 0); if (unlikely(s->malloc_size + size > s->malloc_limit)) return NULL; ptr = malloc(size); if (!ptr) return NULL; s->malloc_count++; s->malloc_size += js_def_malloc_usable_size(ptr) + MALLOC_OVERHEAD; return ptr; } reply FpUser 24 minutes agoprevCPU got too fast so let's run interpreter inside interpreter. reply jitl 5 hours agoprevi wouldn’t say “performance” as an advantage of running JS in QuickJS. QuickJS isn’t competitive at all with the host JS VM, although I guess it’s faster than older C interpreters, or an interpreter implemented in JavaScript. reply math_dandy 3 hours agoparentI suppose you get performance benefits if the the time it takes to start up a nodejs process dominates the execution time of the script. This is probably the case for a decent proportion of “serverless function” type scripts. reply jitl 1 hour agorootparentThis library expects to run inside a Javascript runtime like NodeJS, so you're always going to pay for the enclosing Javascript runtime to start. reply brigadier132 6 hours agoprevVery cool. Since this is compiled to wasm can this run in the browser? It would be interesting if it could and still make fetch requests without attaching cookies to the request. reply devwastaken 24 minutes agoprevYou cannot throw things into wasm and call it safe. It is wholly irresponsible. You need to do the work to ensure it's safe to a theory and in practice. reply brigadier132 22 minutes agoparentquickjs with wasm can be considered secure before different system apis are introduced reply owenpalmer 8 minutes agoprevThis is an xkcd waiting to happen reply waldrews 1 hour agoprevYes! Now, we just have to run this inside a browser, which will run inside a container, which will run inside a VM, which will run on an emulation layer... reply djaouen 44 minutes agoprevIf you’re on wasm, why not just use Elixir and be done with it? reply anonymousd3vil 4 hours agoprevso we come to a full circle reply degurechaff 3 hours agoparentnow we just need wasm intepreter in js. so we can run this package to run javascript to run wasm intepreter and so on.. reply TheRealPomax 4 hours agoparentprevSpiral. Not circle. Having something run in itself is as old as the need to safely run something in order to see if it's going to blow up or not, though. If people aren't trying to write things so your general use programming environment can virtualize itself, it's basically not used seriously enough yet. reply bluelightning2k 6 hours agoprevOh very nice! Guessing there isn't support for node modules but still very cool. reply jiripospisil 4 hours agoprevSee also https://bytecodealliance.org/articles/javy-hosted-project reply revskill 5 hours agoprevDoes it support browser ? reply dualogy 4 hours agoparentPick one of the wasm runtimes written in JS listed at https://github.com/appcypher/awesome-wasm-runtimes and find out =) reply 7 hours agoprev[deleted] jojobas 5 hours agoprev [–] I thought the whole purpose of WebAssembly was not to execute any JavaScript. reply brigadier132 5 hours agoparentWell you'd be mistaken. The point of WebAssembly is to run any language that compiles to WebAssembly in a secure sandbox. reply Muromec 4 hours agoparentprevWe will keep adding indirection layers until it feels like a computer from 60ies and costs to run as much. And employs all our friends so we don't have to introduce UBI or any god-forbidden communist ideas. reply mpalmer 4 hours agorootparentMaking sandboxed execution about your anticapitalist politics, talk about layers of indirection! You're right, it makes for inefficient discourse as well. reply anon115 3 hours agoparentprevmoew meoew moewm meow moew meow XDD the point of wasm is to play video games on the browser. reply mirekrusin 5 hours agoparentprev [–] Too late, he didn't know. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "This TypeScript package allows secure execution of JavaScript code within a WebAssembly sandbox using the QuickJS engine, ideal for running untrusted code safely.",
      "Key features include an isolated environment, virtual file system, custom node modules, HTTP(S) fetch client, chai-based test runner, and efficient performance.",
      "The package is versatile and easy to integrate with TypeScript projects, offering a user-friendly API for code execution and management."
    ],
    "commentSummary": [
      "This TypeScript package enables secure execution of JavaScript code within a WebAssembly sandbox using the QuickJS engine, ensuring isolation of untrusted code.",
      "Key features include a virtual file system, custom node modules, a fetch client, a test runner, and a user-friendly API, supporting environments like NodeJS, Deno, and Cloudflare Workers.",
      "A security warning notes that enabling `fetch` can expose the host's cookies to untrusted code, potentially compromising the sandbox's safety."
    ],
    "points": 104,
    "commentCount": 32,
    "retryCount": 0,
    "time": 1720352245
  },
  {
    "id": 40895935,
    "title": "From the Transistor to the Web Browser, a rough outline for a 12 week course",
    "originLink": "https://github.com/geohot/fromthetransistor",
    "originBody": "From the Transistor to the Web Browser Hiring is hard, a lot of modern CS education is really bad, and it's hard to find people who understand the modern computer stack from first principles. Now cleaned up and going to be software only. Closer to being real. Section 1: Intro: Cheating our way past the transistor -- 0.5 weeks So about those transistors -- Course overview. Describe how FPGAs are buildable using transistors, and that ICs are just collections of transistors in a nice reliable package. Understand the LUTs and stuff. Talk briefly about the theory of transistors, but all projects must build on each other so we can’t build one. Emulation -- Building on real hardware limits the reach of this course. Using something like Verilator will allow anyone with a computer to play. Section 2: Bringup: What language is hardware coded in? -- 0.5 weeks Blinking an LED(Verilog, 10) -- Your first little program! Getting the simulator working. Learning Verilog. Building a UART(Verilog, 100) -- An intro chapter to Verilog, copy a real UART, introducing the concept of MMIO, though the serial port may be semihosting. Serial test echo program and led control. Section 3: Processor: What is a processor anyway? -- 3 weeks Coding an assembler(Python, 500) -- Straightforward and boring, write in python. Happens in parallel with the CPU building. Teaches you ARM assembly. Initially outputs just binary files, but changed when you write a linker. Building a ARM7 CPU(Verilog, 1500) -- Break this into subchapters. A simple pipeline to start, decode, fetch, execute. How much BRAM do we have? We need at least 1MB, DDR would be hard I think, maybe an SRAM. Simulatable and synthesizable. Coding a bootrom(Assembler, 40) -- This allows code download into RAM over the serial port, and is baked into the FPGA image. Cute test programs run on this. Section 4: Compiler: A “high” level language -- 3 weeks Building a C compiler(Haskell, 2000) -- A bit more interesting, cover the basics of compiler design. Write in haskell. Write a parser. Break this into subchapters. Outputs ARM assembly. Building a linker(Python, 300) -- If you are clever, this should take a day. Output elf files. Use for testing with QEMU, semihosting. libc + malloc(C, 500) -- The gateway to more complicated programs. libc is only half here, things like memcpy and memset and printf, but no syscall wrappers. Building an ethernet controller(Verilog, 200) -- Talk to a real PHY, consider carefully MMIO design. Writing a bootloader(C, 300) -- Write ethernet program to boot kernel over UDP. First thing written in C. Maybe don’t redownload over serial each time and embed in FPGA image. Section 5: Operating System: Software we take for granted -- 3 weeks Building an MMU(Verilog, 1000) -- ARM9ish, explain TLBs and other fun things. Maybe also a memory controller, depending on how the FPGA is, then add the init code to your bootloader. Building an operating system(C, 2500) -- UNIXish, only user space threads. (open, read, write, close), (fork, execve, wait, sleep, exit), (mmap, munmap, mprotect). Consider the debug interface you are using, ranging from printf to perhaps a gdbremote stub into kernel. Break into subchapters. Talking to an SD card(Verilog, 150) -- The last hardware you have to do. And a driver FAT(C, 300) -- A real filesystem, I think fat is the simplest init, shell, download, cat, ls, rm(C, 250) -- Your first user space programs. Section 6: Browser: Coming online -- 1 week Building a TCP stack(C, 500) -- Probably coded in the kernel, integrate the ethernet driver into the kernel. Add support for networking syscalls to kernel. (send, recv, bind, connect) telnetd, the power of being multiprocess(C, 50) -- Written in C, user can connect multiple times with telnet. Really just a bind shell. Space saving dynamic linking(C, 300) -- Because we can, explain how dynamic linker is just a user space program. Changes to linker required. So about that web(C, 500+) -- A “nice” text based web browser, using ANSI and terminal niceness. Dynamically linked and nice, nice as you want. Section 7: Physical: Running on real hardware -- 1 week Talking to an FPGA(C, 200) -- A little code for the USB MCU to bitbang JTAG. Building an FPGA board -- Board design, FPGA BGA reflow, FPGA flash, a 50mhz clock, a USB JTAG port and flasher(no special hardware, a little cypress usb mcu to do jtag), a few leds, a reset button, a serial port(USB-FTDI) also powering via USB, an sd card, expansion connector(ide cable?), and an ethernet port. Optional, expansion board, host USB port, NTSC TV out, an ISA port, and PS/2 connector on the board to taunt you. We provide a toaster oven and a multimeter thermometer to do reflow. Bringup -- Compiling and downloading the Verilog for the board",
    "commentLink": "https://news.ycombinator.com/item?id=40895935",
    "commentBody": "From the Transistor to the Web Browser, a rough outline for a 12 week course (github.com/geohot)102 points by signa11 11 hours agohidepastfavorite44 comments bluish29 9 hours agoI wonder what is the fate of this ambitious plan? Giving that there are only two commits, the first was in 2016 and then another one (converting from txt to markdown). Also, I found this [1] the funniest way of reaching out (comment on a commit) Did he actually do that, is there somewhere where this is available? In this case, it would be more appropriate to link to that. [1] https://github.com/geohot/fromthetransistor/commit/bc3e63e2a... reply loufe 7 hours agoprevFor the hardware to software part, anyone interested in some \"passive\" learning of these fundamentals might consider one of two excellent games I've played: -Turing Complete -Shenzen I/O I'm sure they both work on Windows and Linux, likely Mac as well for both. They are fun dynamic puzzles that help build out a framework of understanding software-hardware control for those with no experience. reply xlii 6 hours agoparentI'm not sure if Shenzen I/O is a good recommendation - as with all Zachtronics games sooner or later it gets very puzzle like and gets difficult very fast. Turing Complete on the other hand is focused on the exploration/learning experience and - for example - doesn't show leaderboards until very late in the game. Playing both - Shenzen I/O is great, but Turing Complete has a spot in my \"best games ever\" list. reply FredPret 6 hours agoparentprevThey are both excellent and work on Mac via Steam reply XorNot 5 hours agoparentprevOkay I didn't know about Turing Complete, that looks amazing. reply cpldcpu 8 hours agoprev>Building an FPGA board This seems to be a bit odd? This is already a more tedious hardware project to debug, but when it is about learning the basics, building a much simpler circuit would provide more insight. It's also a bit questionable why building hardware should be part of a full stack digitial systems course? It's very good knowledge for certain, but seems like a sidetrack for me. reply rezaprima 8 hours agoprevThis fork [1] has considerable progress instead [1] https://github.com/andrewhummus/fromthetransistor reply akie 9 hours agoprevThat's a very ambitious 12 weeks. If you would be aiming at complete beginners this looks more like a year of work. reply AJRF 7 hours agoparentGeohot has a bit of an elitest mindset, despite not really being able to follow through himself. I'd say to anyone starting their interest in the field, don't get put off by this time frame, just keep at it and you will win. reply Tade0 8 hours agoparentprevMy whole undergrad curriculum was essentially about this, so I would say 3.5 years would be a sufficient amount to study it in depth. reply eysgshsvsvsv 8 hours agorootparentI would say 4.27 years would be a sufficient amount to study intensively. reply barrenko 8 hours agorootparentI'll assume this is only slightly a joke. reply passion__desire 6 hours agoparentprevPeople need 4 years of sleep for that kind of dense material to sink in. reply 082349872349872 10 hours agoprevcompare https://www.nand2tetris.org reply llm_trw 8 hours agoparentAlso: https://mitp-content-server.mit.edu/books/content/sectbyfn/b... reply bazoom42 4 hours agoparentprevAlso http://nandgame.com Inspired by Nand2tetris, but web-based. reply userbinator 9 hours agoparentprevAlso https://www.charlespetzold.com/code/ reply Almondsetat 7 hours agoprevThis is not just a rough outline for the course, in 12 weeks the course itself will never amount to more than a rough outline of its topics reply shepherdjerred 2 hours agoprevMost of the geniuses in computer science would struggle doing all of this in 12 weeks. reply teleforce 8 hours agoprev>Hiring is hard, a lot of modern CS education is really bad, and it's hard to find people who understand the modern computer stack from first principles Ever heard of Electrical and Computer Engineering degree courses, it covers from transistor physics, digital design including HDL, CPU microprocessor, embedded system, operating systems, programming, database web development, mobile applications and machine learning. Heck it also covers Engineering maths fundamentals for your DSP, control systems and wireless systems courses. It's like drinking from fire hose, but it's a very comprehensive, interesting and relevant degree program to be completed in four years duration. Both Lisa Su and Jensen Huang completed their degree in this engineering field. reply ThalesX 7 hours agoparentYou go study Computer Engineering and then to get hired for some CRUD work on a legacy system you have to solve a LeetCode Medium and a Hard. reply Almondsetat 7 hours agorootparentYes, and you go study mechanical engineering and you can end up working on an oil tanker, a Ferrari, a Mars rover or a garage door motor reply 42lux 7 hours agorootparentA friend of mine is designing rear view mirrors for Porsche for about 15 years and now he's seen as some sort of rear view mirror expert and he can't find anything else. It's hilarious. reply passion__desire 6 hours agorootparentThen some engineers have to code in dancing Maybach. Free market. eh. https://www.youtube.com/shorts/-51HzSpGVxc reply ThalesX 7 hours agorootparentprevI hope you've brushed up on year 3 curriculum before going to fix that garage door motor so you can answer some random medium / hard questions from Finite Element Analysis in Solid Mechanics or Vehicle Powertrain, Noise and Vibration. reply ImPleadThe5th 6 hours agorootparentprevHey me! Ive been doing web services for the last 5 years since I graduated in CE. Now I really want to break back into imbedded and don't know where to begin. Anyone have advice? reply ksec 7 hours agoparentprevI was going to say just that. I think we have this debate for a long time ( at least 25+ years ) unfortunately CS won and CE lost. If only could Computer Engineering course just offer an additional one year materials for all the additional CS stuff. Big Tech also have a preference on CS as well since a lot of them also came from CS background. reply userbinator 9 hours agoprevWhat are the numbers after the languages supposed to be? Hours? Minutes? 10 hours for blinking an LED seems far too long, while 2500 minutes for an OS seems far too short. reply gibybo 9 hours agoparentThey look like they could be lines of code to me. reply LAC-Tech 9 hours agoprevAmbitious, but I like the idea. I feel like it would be too much to absorb for most juniors, but for your average senior like me with gaps, it may prove useful. reply signa11 9 hours agoprevthere is this video from YT:https://www.youtube.com/watch?v=COr13MfGnHk which might be instructive as well. reply robxorb 6 hours agoprev(2018) reply mihaic 6 hours agoprevRarely have I seen such garbage getting so many github stars. It seems to be all signaling of how clever the guy who wrote it is, without any actual substance to back it up. There is literally no way for anyone to get anything meaningful out of such an information dump. Even if you could memorize all the information, internalizing it takes years. It's pretty much the reverse of Peter Norvig's best article, Teach yourself programming in 10 years [1]. If you haven't read it, I'd recommend you do to cleanse your palette. [1] http://norvig.com/21-days.html reply GuB-42 5 hours agoparentI agree with the \"no substance\" part. However, I think such a course is not for beginners, instead it would be for experienced programmers who already have an idea about how computers work and want to fill up some gaps so that they have the whole picture. These 12 weeks can be part of that \"10 years\" progression. It addresses the \"computer\" part, and adds a language that isn't mentioned to the list: Verilog, a hardware definition language. As for the \"guy\", he is George Hotz, a controversial figure, who likes to think of himself as very clever indeed, but going up and down the abstraction ladder is, I think, one of his strengths. So I won't dismiss his article too quickly. reply andruby 5 hours agoparentprev> all signaling of how clever the guy who wrote it is This has unfortunately been my experience with a lot of geohot’s work. reply isotypic 4 hours agoparentprevSection 3 Building a ARM7 CPU is in my view the best example of this - building a pipelined processor is exactly the sort of thing that sounds impressive, but in truth teaches you exceedingly little for the effort it requires. * Architecture wise, implementing a core in verilog versus simulating it in a higher level language are nearly the same from a learning perspective, there really aren't any special. insights from hardware when just starting out. And doing this in verilog just makes the entire process more obtuse because its hardware. * Microarchitecture wise, a simple pipelined processor teaches you basically nothing about how a modern processor (i.e. superscalar, out-of-order) functions from the perspective of a person higher on the stack. If you were to be using it to springboard to writing an OoO superscalar processor, it would be far better to be given a well-designed multistage core and the assignment be to add pipelining - this avoids the student implementating the multistage core of the processor, which is really mostly straightforward implementation of simple but large state machines, and instead focuses purely on the pitfalls of pipelining while also giving a model of good verilog/processor writing practices. Another thing which is sortof weird is worrying about the type of RAM - that's getting just way into the weeds of processor design and teaches you nothing except stuff like \"how to interface with dual port versus single port RAM blocks\". And given the time constraints it makes more sense to just simulate memory anyways. reply BaculumMeumEst 5 hours agoparentprevIf it were posted anonymously it would have been ridiculed or ignored. reply atoav 5 hours agoparentprevWhen I, an educator who has a understanding of the topics, read the outline I immediately felt sorry for the people who would be part of that curriculum. This outlines looks smart to people who already dealt with all or most topics involved, but it would be soul-crushing to most students. That being said, this is mostly an issue of how the material is organized and which parts are emphasized in which way. In a way it is weird to pinpoint what is wrong with it. It is at the same time too niche and to broad at the same time in weird way. reply udev4096 5 hours agoparentprevI think the point of it is to let everyone decide their own learning journey instead of providing a set of books or any resource links reply mihaic 5 hours agorootparentIf we want to let people decide on their learning journey, I think something like this is even more harmful, since it creates an unrealistic standard. It's the equivalent of an Instagram lifestyle for software/hardware engineering. I've met many world class developers, and I'm sure that even after years of experience almost none could handle such a course even if dedicating themselves full time to it. Building a C compiler in Haskell is just one part of a 3-week section, and if you don't know Haskell you're really dead if you have to pick it up on the fly. It feels like an A/B test, to see how much bullshit can pass, or maybe the guy really is delusional, I don't really know. reply udev4096 4 hours agorootparentYou're right. Giving a set of topics is in itself doing the opposite of \"deciding their own learning journey\" reply hypeatei 5 hours agoparentprevAgreed, I'm really lost on what this even is. reply alexnewman 5 hours agoparentprevA positive spin is people are so excited to learn this stuff that a syllabus got thousands of github stars. I think a lot of his work is great from a pedagogical form. I often learn more about ML by reading tinygrad, even if i've never used it in production. We should be celebrating this guy's efforts reply barrenko 8 hours agoprev [–] Flagging this as it's essentially a 5-year-old outline of a project. It would be great if g. ever really did it. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The course covers a comprehensive journey from understanding transistors to developing a web browser, providing a hands-on approach to learning hardware and software integration.",
      "Key projects include building an ARM7 CPU, a C compiler, a UNIX-like OS, and a text-based web browser, using languages like Verilog, Python, Haskell, and C.",
      "The curriculum emphasizes practical skills such as emulation with Verilator, FPGA board design, and implementing essential system components like an MMU, Ethernet controller, and TCP stack."
    ],
    "commentSummary": [
      "A 12-week course titled \"From the Transistor to the Web Browser\" by geohot on GitHub has generated discussion due to its ambitious scope but minimal updates since 2016.",
      "Users debate its feasibility, suggesting alternative learning tools like \"Turing Complete\" and \"Shenzen I/O,\" and argue it may be too dense for beginners, better suited for experienced programmers.",
      "Critics question the course's practicality and the author's intentions, viewing it as overly ambitious and lacking substance compared to comprehensive Electrical and Computer Engineering degrees."
    ],
    "points": 102,
    "commentCount": 44,
    "retryCount": 0,
    "time": 1720338876
  },
  {
    "id": 40897518,
    "title": "Mongo but on Postgres and with strong consistency benefits",
    "originLink": "https://github.com/event-driven-io/Pongo",
    "originBody": "Pongo Pongo - Mongo but on Postgres and with strong consistency benefits. Getting Started Install Pongo as an npm module and save it to your package.json: npm install @event-driven-io/pongo Read also introduction article on my blog for more context. Example You can use Pongo syntax with explicit typing about supported syntax: import { pongoClient } from \"@event-driven-io/pongo\"; import { v4 as uuid } from \"uuid\"; type User = { name: string; age: number }; const connectionString = \"postgresql://dbuser:secretpassword@database.server.com:3211/mydb\"; const pongoClient = pongoClient(connectionString); const pongoDb = pongoClient.db(); const users = pongoDb.collection(\"users\"); const roger = { name: \"Roger\", age: 30 }; const anita = { name: \"Anita\", age: 25 }; const cruella = { _id: uuid(), name: \"Cruella\", age: 40 }; // Inserting await pongoCollection.insertOne(roger); await pongoCollection.insertOne(cruella); const { insertedId } = await pongoCollection.insertOne(anita); const anitaId = insertedId; // Updating await users.updateOne({ _id: anitaId }, { $set: { age: 31 } }); // Deleting await pongoCollection.deleteOne({ _id: cruella._id }); // Finding by Id const anitaFromDb = await pongoCollection.findOne({ _id: anitaId }); // Finding more const users = await pongoCollection.find({ age: { $lt: 40 } }); Or use MongoDB compliant shim: import { MongoClient } from \"@event-driven-io/pongo\"; import { v4 as uuid } from \"uuid\"; type User = { name: string; age: number }; const connectionString = \"postgresql://dbuser:secretpassword@database.server.com:3211/mydb\"; const pongoClient = new MongoClient(postgresConnectionString); const pongoDb = pongoClient.db(); const users = pongoDb.collection(\"users\"); const roger = { name: \"Roger\", age: 30 }; const anita = { name: \"Anita\", age: 25 }; const cruella = { _id: uuid(), name: \"Cruella\", age: 40 }; // Inserting await pongoCollection.insertOne(roger); await pongoCollection.insertOne(cruella); const { insertedId } = await pongoCollection.insertOne(anita); const anitaId = insertedId; // Updating await users.updateOne({ _id: anitaId }, { $set: { age: 31 } }); // Deleting await pongoCollection.deleteOne({ _id: cruella._id }); // Finding by Id const anitaFromDb = await pongoCollection.findOne({ _id: anitaId }); // Finding more const users = await pongoCollection.find({ age: { $lt: 40 } }).toArray(); How does it work? Pongo treats PostgreSQL as a Document Database benefiting from JSONB support. Unlike the plain text storage of the traditional JSON type, JSONB stores JSON data in a binary format. This simple change brings significant advantages in terms of performance and storage efficiency. Pongo uses the following table structure for storing collections: CREATE TABLE IF NOT EXISTS \"YourCollectionName\" ( _id UUID PRIMARY KEY, data JSONB ) Essentially Pongo takes MongoDB api and translates it to the native PostgreSQL queries. It is a similar concept to Marten and AWS DocumentDB (see here or there, they seem to be using Mongo syntactic sugar on top of AuroraDB with Postgres). E.g. the MongoDB update syntax: const pongoCollection = pongoDb.collection(\"users\"); await pongoCollection.updateOne( { _id: someId }, { $push: { tags: \"character\" } } ); will be translated to: UPDATE \"users\" SET data = jsonb_set(data, '{tags}', (COALESCE(data->'tags', '[]'::jsonb) || to_jsonb('character'))) WHERE _id = '137ef052-e41c-428b-b606-1c8070a47eda'; Or for query: const result = await pongoCollection .find({ \"address.history\": { $elemMatch: { street: \"Elm St\" } } }) .toArray(); will result in: SELECT data FROM \"users\" WHERE jsonb_path_exists( data, '$.address.history[*] ? (@.street == \"Elm St\")' ); Storage The binary format of PostgreSQL JSONB means that data is pre-parsed, allowing faster read and write operations than text-based JSON. You don't have to re-parse the data every time you query it, which saves processing time and improves overall performance. Additionally, JSONB supports advanced indexing options like GIN and GiST indexes, making searches within JSONB documents much quicker and more efficient. Moreover, JSONB retains the flexibility of storing semi-structured data while allowing you to use PostgreSQL's robust querying capabilities. You can perform complex queries, joins, and transactions with JSONB data, just as you can with regular relational data. Contrary to common belief, JSON document data is structured. JSON has structure, but it is not enforced for each document. We can easily extend the schema for our documents, even for specific ones, by adding new fields. We should also not fail if the field we expect to exist, but doesn't. This flexibility, performance, and consistency combination makes PostgreSQL with JSONB a powerful tool. There are benchmarks showing that it can be even faster than MongoDB. Check more in: JSON Types Documentation JSON Functions and Operators PostgreSQL, JSONB and GIN Indexes by MongoDB vs PostgreSQL JSONB Benchmark How to JSON in PostgreSQL Is Pongo an ORM? It's not. It's focused on effective handling of the document data specifics. Node.js ORMs have capabilites to handle JSONB, e.g. DrizzleORM has a good support for that for basic operations. Yet, they're all but limited to querying, usually for advanced ones you need to fallback to JSONPath or JSONB functions (so raw SQL). As you saw above, this syntax is not super pleasant to deal with. That's why Pongo aims to do it for you. Is it production ready? What's there it's safe to use, but it's far from being 100% compliant with MongoDB. Pongo is a fresh project, so some stuff can be missing. Pongo is a community project, so once you find something, we encourage you to send us a GH issue or Pull Request extending the support or test coverage!",
    "commentLink": "https://news.ycombinator.com/item?id=40897518",
    "commentBody": "Mongo but on Postgres and with strong consistency benefits (github.com/event-driven-io)98 points by oskar_dudycz 5 hours agohidepastfavorite48 comments cpursley 1 hour agoThanks, just added Pongo to the NoSQL section of my \"Postgres Is Enough\" gist: https://gist.github.com/cpursley/c8fb81fe8a7e5df038158bdfe0f... reply 314156 4 minutes agoprevhttps://docs.oracle.com/en/database/oracle/mongodb-api/mgapi... Oracle database has a MongoDB compatible API too. reply pipe_connector 3 hours agoprevMongoDB has supported the equivalent of Postgres' serializable isolation for many years now. I'm not sure what \"with strong consistency benefits\" means. reply zihotki 2 hours agoparentOr is it? Jepsen reported a number of issues like \"read skew, cyclic information flow, duplicate writes, and internal consistency violations. Weak defaults meant that transactions could lose writes and allow dirty reads, even downgrading requested safety levels at the database and collection level. Moreover, the snapshot read concern did not guarantee snapshot unless paired with write concern majority—even for read-only transactions.\" That report (1) is 4 years old, many things could have changed. But so far any reviewed version was faulty in regards to consistency. 1 - https://jepsen.io/analyses/mongodb-4.2.6 reply vorticalbox 40 minutes agorootparentThat is for mongo 4.x but latest stable is 6.0.7 which has note More resilient operations and Additional data security. https://www.mongodb.com/blog/post/big-reasons-upgrade-mongod... reply endisneigh 2 hours agorootparentprevThat’s been resolved for a long time now (not to say that MongoDB is perfect, though). reply throwup238 2 hours agoparentprev> I'm not sure what \"with strong consistency benefits\" means. \"Doesn't use MongoDB\" was my first thought. reply Izkata 2 hours agoparentprev> MongoDB has supported the equivalent of Postgres' serializable isolation for many years now. That would be the \"I\" in ACID > I'm not sure what \"with strong consistency benefits\" means. Probably the \"C\" in ACID: Data integrity, such as constraints and foreign keys. https://www.bmc.com/blogs/acid-atomic-consistent-isolated-du... reply lkdfjlkdfjlg 2 hours agoparentprev> Pongo - Mongo but on Postgres and with strong consistency benefits. I don't read this as saying it's \"MongoDB but with...\". I read it as saying that it's Postgres. reply zulban 3 hours agoprevNeat. When I migrated a project from mongo to postgres I took a similar approach, except I only implemented the mongo feel I needed within my own project instead of building a proper library as done here. I was surprised how much performance improved despite using a hacky wrapper. https://blog.stuartspence.ca/2023-05-goodbye-mongo.html Personally tho, I plan to just drop all similarity to mongo in future projects. reply hdhshdhshdjd 42 minutes agoprevI use JSONB columns a lot, it has its place. It can fit certain applications, but it does introduce a lot of extra query complexity and you lose out on some ways to speed up query performance that you could get from a relational approach. Which is to say JSONB is useful, but I wouldn’t throw the relational baby out with the bath water. reply Squarex 3 hours agoprevHow does it compare with FerretDB[0]? [0] https://www.ferretdb.com/ reply aleksi 3 hours agoparent(I'm FerretDB co-founder) As far as I can tell, Pongo provides an API similar to the MongoDB driver for Node that uses PostgreSQL under the hood. FerretDB operates on a different layer – it implements MongoDB network protocol, allowing it to work with any drivers and applications that use MongoDB without modifications. reply Keyframe 39 minutes agorootparentEven monstache? reply Zambyte 3 hours agoparentprevThe posted project looks like a client that connects to pg but behaves like Mongo, where Ferret is a server that accepts Mongo client connections and uses pg as backend storage. reply oskar_dudycz 1 hour agoparentprevYes, I'm using MongoDB API in Pongo to keep the muscle memory. So, it's a library that translates the MongoDB syntax to native PostgreSQL JSONB queries. reply karmakaze 3 hours agoprevWhat makes mongo mongo is its distibruted nature, without it you could just store json(b) in an RDBMS. reply anonzzzies 2 hours agoparentSo how easy is it to distribute it? I don’t have experience with it but the tutorials look terrible compared to, say, Scylla, Yuga, Cockroach, TiDB etc. Again, honest question? reply theteapot 34 minutes agorootparentDoes \"distributed\" mean sharded or just replicated? In either case it's a bit quirky but easy enough. > Scylla, Yuga, Cockroach, TiDB etc. You have experience \"distributing\" all these DBs? That's impressive. reply rad_gruchalski 1 hour agorootparentprevPongo seems to be a middleware between your app and Postgres. So it will most certainly work absolutely fine on YugabyteDB, if one’s okay with occasional latency issues. One could optimise it more for a distributed sql by implementing key partition awareness and connecting directly to a tserver storing the data one’s after. reply oskar_dudycz 1 hour agorootparentYes, as long as database has support to JSONB and JSON path syntax (so PG 12 >= compliant) you should be good to go :) reply rad_gruchalski 4 minutes agorootparentIt could work: https://docs.yugabyte.com/preview/explore/ysql-language-feat.... reply darby_nine 3 hours agoparentprevbut then you wouldn't have the joy of using the most awkward query language invented by mankind reply richwater 3 hours agoparentprev> store json(b) in an RDBMS I actually did this for as small HR application and it worked incredible well.jsonb gin indexes are pretty nice once you get the hang of the syntax. And then, you also have all the features of Postgres as a freebie. reply eddd-ddde 2 hours agorootparentPersonally, I much better like postgres json syntax than whatever mongo invented. Big fan of jsonb columns. reply zihotki 2 hours agoparentprevBut RDBMS'es are often also distributed. So what is mongo now? reply marcosdumay 2 hours agorootparentPeople don't usually distribute Postgres (unless you count read replicas and cold HA replicas). But well, people don't usually distribute MongoDB either, so no difference. In principle, a cluster of something like Mongo can scale much further than Postgres. In practice, Mongo is full of issues even before you replicate it, and you are better with something that abstracts a set if incoherent Postgres (or sqlite) instances. reply zozbot234 44 minutes agorootparentPostgres supports foreign data wrapper (FDW), which is the basic building block for a distributed DB. It doesn't support strong consistency in distributed settings as of yet, although it does provide two-phase commit which could be used for such. reply williamdclt 14 minutes agorootparent> strong consistency in distributed settings I doubt it ever will. The point of distributing a data store is latency and availability, both of which would go down the drain with distributed strong consistency reply brabel 2 hours agorootparentprevOften?? In my experience it's really hard to do it and still maintain similar performance, which kind of voids any benefit you may be looking for. reply lkdfjlkdfjlg 2 hours agoparentprev> What makes mongo mongo is its distibruted nature, without it you could just store json(b) in an RDBMS. Welllllllll I think that's moving the goalposts. Being distributed might be a thing _now_ but I still remember when it was marketed as the thing to have if you wanted to store unstructured documents. Now that Postgres also does that, you're marketing Mongo as having a different unique feature. Moving the goalposts. reply thfuran 2 hours agorootparentIt doesn't really seem reasonable to accuse someone of moving goalposts that you've just brought into the conversation, especially when they were allegedly set by a third party. reply coldtea 1 hour agorootparentParent didn't \"just brought them\", they merely referrenced the pre-existing goalposts used to advocate for Mongo and reasons devs adopted it. reply salomonk_mur 3 hours agoprevWhat would be the advantage of using this instead of simple jsonb columns? reply joshmanders 3 hours agoparentIt uses JSONb under the hood. Just gives you a very \"mongo\" feel to using PostgreSQL. Not sure how I feel about it. CREATE TABLE IF NOT EXISTS %I (_id UUID PRIMARY KEY, data JSONB) reply wood_spirit 3 hours agorootparentCan they make it use uuid7 for ids for better insert_becomes_append performance? reply lgas 2 hours agorootparentYes reply oskar_dudycz 1 hour agorootparentYes, I'm using JSONB underneath and translating the MongoDB syntax to native queries. As they're not super pleasant to deal with, then I thought that it'd be nice to use some familiar to many MongoDB API. Regarding IDs, you can use any UUID-compliant format. reply lopatin 2 hours agoparentprevjsonb isn't web scale. Mongo is web scale. reply digger495 1 hour agorootparentI see what you did there reply imnotjames 3 hours agoparentprevLooks like it natches the mongo node API reply ramchip 3 hours agoprevHave you tried it with CockroachDB? reply oskar_dudycz 1 hour agoparentI did not, but I'm not using any fancy syntax so far besides JSONB operators. If it won't work, then I'm happy to adjust it to make it compliant. reply joeyagreco 3 hours agoprevGood work! I would like to see a section on the README outlining the benefits of Pongo reply oskar_dudycz 1 hour agoparentThanks, I'll try to cover that, good call! reply revskill 53 minutes agoprevGenius. reply posix_monad 1 hour agoprev [–] Does MongoDB have serious market share compared to DynamoDB (and similar clones from Azure, GCP) at this point? reply dudeinjapan 15 minutes agoparent [–] Totally. Many of the biggest tech companies are using for core use cases. Stripe uses a modified version: https://stripe.com/blog/how-stripes-document-databases-suppo... We use MongoDB’s cloud offering called Atlas as our core DB at TableCheck. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Pongo Pongo is a tool that allows using MongoDB-like operations on PostgreSQL, leveraging strong consistency benefits.",
      "It translates MongoDB API calls to native PostgreSQL queries, utilizing JSONB for efficient read/write operations and complex queries.",
      "Pongo is not a traditional ORM (Object-Relational Mapping) but focuses on document data handling, offering flexibility and performance, though it is not fully MongoDB compliant."
    ],
    "commentSummary": [
      "Pongo is a new project that provides a MongoDB-like API on top of PostgreSQL, leveraging JSONB columns for strong consistency benefits.",
      "The project aims to combine the ease of use of MongoDB with the robust consistency and reliability of PostgreSQL, translating MongoDB syntax to native PostgreSQL queries.",
      "This development has sparked interest due to its potential to offer a familiar MongoDB experience while utilizing PostgreSQL's advanced features, appealing to developers looking for strong consistency in their database operations."
    ],
    "points": 98,
    "commentCount": 48,
    "retryCount": 0,
    "time": 1720358532
  },
  {
    "id": 40896110,
    "title": "Scientists re-emerge after a year in Mars simulation project",
    "originLink": "https://www.dw.com/en/scientists-re-emerge-after-a-year-in-mars-simulation-project/a-69585838",
    "originBody": "ScienceUnited States of America Scientists re-emerge after a year in Mars simulation project 17 hours ago17 hours ago The two women and two men spent the last 378 days in Houston's Mars Dune Alpha habitat, designed to mimic Red Planet conditions. They spent the year conducting \"Marswalks\" and operating under \"additional stressors.\" https://p.dw.com/p/4hySE Copy link As they left the habitat, the four volunteers were visibly emotionalImage: JOSE ROMERO/AFP Advertisement After a year, four scientists in the United States on Saturday ended an experiment that simulated life on Mars. To loud applause, the four volunteers left NASA-built Mars Dune Alpha, where they had spent the past 378 days completely isolated from the outside world. The 160-square-meter structure at the Johnson Space Center in Houston was designed to mimic conditions on the Red Planet. The habitat is a 3D-printed facility, complete with bedrooms, a gym, common areas, and a vertical farm for growing food. The structure also features an outdoor area, separated by an airlock. The space is filled with red sand and is where the team donned suits to conduct their \"Marswalks.\" Life as on Mars: NASA unveils Mars Dune Alpha Four volunteers on 160 square meters: For one year, they are to test what life on Mars might look like. The U.S. space agency NASA expects this to provide important information for possible Mars missions. Image: Go Nakamura/REUTERS As realistic as possible Starting in June, four volunteers will move into NASA's Mars Dune Alpha house to test life on the Red Planet. For this purpose, they will live for a year in a converted hangar on the research grounds of the Johnson Space Center in Houston, Texas. Here, several rooms, an outdoor simulation area and, above all, lots of red sand await the future participants. Image: Go Nakamura/REUTERS Isolation, the main problem of possible missions In a first experiment, scientists want to test how humans cope with prolonged isolation and stressful situations. This should help NASA assess what \"resources\" are needed for future astronauts to survive a Mars mission, says Grace Douglas, who heads the Chapea program responsible for the experiment. Image: Go Nakamura/REUTERS Little space on Mars The volunteers cannot take much luggage with them. They will live in small bedrooms in the house on the research site during the year. The house is built entirely using 3D printing. This is one of the technologies NASA is considering as a possibility for constructing buildings on other planets or the lunar surface, says project manager Grace Douglas. Image: Go Nakamura/REUTERS Space for experiments In addition to the bedrooms, \"Mars Dune Alpha\" has two bathrooms, a medical care room, an area for relaxing and several work areas. During their time, the researchers will regularly test how the volunteers react to stress — for example, when water runs low or equipment breaks down. Image: Go Nakamura/REUTERS Matching boots for Mars landscape Participants exit the 3D house via an airlock to a replica of the Martian environment. With lots of red sand, life on Mars is to be recreated as realistically as possible. Image: Go Nakamura/REUTERS Fit in isolation Suspended from harnesses to mimic the lower gravity on Mars, the fictional astronauts will simulate trips outside on treadmills. They will collect samples, gather data and continue to build infrastructure. \"We really can't have them just walking around in circles for six hours,\" joked Suzanne Bell, director of the Behavioural Health and Performance Laboratory. Image: Go Nakamura/REUTERS Growing vegetables on Mars In addition to a weather station, the research center also offers a vertical farm for growing lettuce and other plants. Here, participants can grow vegetables to be self-sufficient during the time. Image: Go Nakamura/REUTERS Important insights for future missions A total of three long-term experiments are planned at the site. The hope is to obtain \"important information\" for a longer stay in isolation. However, Nasa is still at the very beginning of its preparations for a trip to Mars. First, the space agency is concerned with the Artemis mission: For the first time in half a century, humans are to travel to the moon again. Image: Go Nakamura/REUTERS 8 images 188 images What did the scientists do? Anca Selariu, Ross Brockwell, Nathan Jones and team leader Kelly Haston have spent the last year growing vegetables, conducting \"Marswalks\" and operating under what NASA calls \"additional stressors.\" These included communication delays with \"Earth,\" including their families; isolation; and confinement. As they left the habitat on Saturday, the four volunteers were visibly emotional. \"We can do these things together,\" Brockwell said. \"We can use our senses of wonder and purpose, to achieve peace and prosperity and to unlock knowledge and joy for the benefit of everyone in every part of planet Earth,\" he added. What space exploration missions await us in 2024? What is the goal of the mission? The mission was the first in a series called Crew Health and Performance Exploration Analog (CHAPEA). Its goal is to help NASA prepare to send humans back to the moon and, one day, to Mars. Julie Kramer, NASA's director of engineering, said the project \"gives us an opportunity to learn all these critical things about these complex systems, and it's going to make going to Mars and back a lot safer.\" Additional CHAPEA missions are planned for 2025 and 2027, she said. A year-long mission to simulate life on Mars took place in a habitat in Hawaii in 2015-2016. NASA participated but did not lead the mission. As part of its Artemis program, the US plans to send humans back to the Moon to learn how to live there long-term. This will help prepare for a trip to Mars sometime in the late 2030s. dh/rmt (AFP, dpa) Send us your feedback Your feedback Advertisement",
    "commentLink": "https://news.ycombinator.com/item?id=40896110",
    "commentBody": "Scientists re-emerge after a year in Mars simulation project (dw.com)91 points by _____k 10 hours agohidepastfavorite59 comments polairscience 9 hours agoI've been following these efforts closely. For these projects, crewed endurance space efforts, I strongly believe the most difficult component is the human. It's really difficult to be isolated in a high stress environment. And we don't adequately simulate or study what that might truly be like. I know that this was about nutrition but I just don't think nutrition is going to be the hard part of these missions. I say this with experience. Living and working doing science in the Arctic and on ships is grueling. It's grueling, and not anywhere near as difficult or unpredictable as being in space. The things that happen in that pressure cooker are really hard to explain to people who haven't lived it. It's not ethical or easy to do the kinds of simulations that would actually be useful. How do you simulate \"your colleague is gravely wounded and on life support. now you have to work for 90 hours straight to fix whatever mamed them\". Oh, also, you have 9 months of mission left with one less crew. It was mediocre at best but \"for all man kind\" highlights just how weird things might get in these places. The only analogous efforts I can imagine are the adventures of sea-fairing people of centuries past. Maybe we should invent time travel and do some sociological studies. reply somenameforme 1 hour agoparentColonization will entail a pretty large population of people across all fields of expertise. You will also have all supplies and materials necessary for any sort of contingencies. You're not going to have the electrician trying to improv as a doctor. I think it's highly probable that many people will die, but not in particularly grueling ways, but far more mundane. Around 600 people have gone to space (which I'll define as at least LEO). During this (including training), about 30 people have died. A death ratio of 5% is ridiculously high, and most of the incidents have been far from remarkable. Fire in a capsule, oxygen valve failure, heat shield failure, don't use solid rocket boosters, and so on. It'll be the same stuff with Mars, except the death ratio will likely be substantially higher. It'll again be from just incidents that will look stupid in hindsight. And they will have been, but when you're working to prevent a billion bad outcomes, a few will always manage to wiggle through the cracks. All things considered I think Mars colonization will probably be far easier than the guys who were engaging in civilization building in the past. Settlers in times past were dealing with hostile natives, hostile waters, unknown diseases, unpredictable weather, even mundane things like rats getting into your rations, and had very little ability to measure or adapt to what they were dealing with. For instance Vasco de Gama (Europe to India guy) took 3 weeks to cross the Indian ocean the first time. On his way back, it took 3 months. By the time he returned, about 70% of his crew was dead. Colonizing Mars will be absolutely brutal but we will have quite far fewer unknowns. Of course the issues will will have to deal with will be far harsher than anything anybody else ever had to manage, but they are known - and so that really simplifies the process. reply germinalphrase 1 hour agorootparent“ All things considered I think Mars colonization will probably be far easier than the guys who were engaging in civilization building in the past.” Those explorers of eras past were going places that not only sustained ample plant and animal resources (in quantities that would surprise modern people) but entire human populations. There is almost nothing on Mars that will sustain us. reply somenameforme 50 minutes agorootparentNot entirely. I'd look up things like the 'Starving Time' at Jamestown. [1] Jamestown lost 90% of its population in a single winter, and was left resorting to cannibalism and other such things. In the past you were constantly preparing for winter. And if something went wrong, either with your crops, water, surprise weather, or whatever else - you died. And that's if disease, natives, weather, or a million other issues didn't get you. And that's after you got there. Crossing the sea meant you were completely subject to the whims of the weather, often sailing into the literal unknown, all the while your rations were slowly dwindling and your crew was starting to rot from the inside out dying rather unpleasant deaths - scurvy wouldn't be entirely resolved until the 20th century! With Mars we won't really have to deal with any of this, because the issues and variables will be mostly known ahead of time, and the supplies per settler will also be dramatically higher meaning a far greater window of time to reach self sustainability. [1] - https://en.wikipedia.org/wiki/Starving_Time reply germinalphrase 32 minutes agorootparentJamestown is notable because of the deprivation, not because every settlement was a Jamestown. Certainly, we understand the variables and an identical journey on earth would be dramatically safer (though still not without peril). Even still, these explorers were able to harvest resources during their journey as well as immediately upon arrival at the destination. With Mars, there’s nothing there. No fertile soil. No potable water. No known life. It’s an actively hostile environment. Colonizers to Mars will be exceptionally reliant on carrying the totality of their survival resources. Sure - if we spend the next centuries terraforming Mars robotically (or some other scheme), we could change the math on this - but I have a hard time swallowing the argument that crossing to, and colonizing, Mars is anything but orders of difficulty harder than the feats of those explorers. I’m a nerd and want to believe, but it’s fanatical thinking for… a long time. reply 9dev 1 hour agorootparentprevPeople arguing in favour of mars colonisation always kind of skip the time between now and a future in which we’re able to ship an entire populace to another planet. We will need to send smaller crews first. Not just to do the science and validation necessary, but also because it would be plain unethical to send a large group of people without knowing what effect that kind of travel will have on them. And all of that in spite of knowing how that venture would even be technologically feasible. If you haven’t, I can highly recommend reading A city on mars by Kelly & Zach Weinersmith. reply somenameforme 6 minutes agorootparentWell this is literally the point of Starship, so the timeframes are probably quite a lot closer than many may realize. And due to the extensive amount of labor involved, a fairly large group will be absolutely necessary. But I'd emphasize that colonizing Mars, or anywhere, will likely literally never be safe, and no amount of \"science and validation\" can really change that. Huge numbers of people still regularly die in the most mundane of things, like roofing - where about 1 in 2000 workers lose their lives per year. Over a career of a few decades, those are some pretty unpleasant odds. And on Mars roofing would look like the safest job imaginable. We're going to be doing absolutely everything from scratch - industry, construction, manufacturing/processing, and more. And doing it all in conditions unlike anything on Earth, and where if you're exposed to atmosphere - even briefly, you're dead. There's going to be an unavoidably high death rate. If you want to wait until things are safe then I think that's perfectly understandable. But in turn you should also understand that people have different perspectives on life. reply tired-turtle 1 hour agorootparentprevThe settlers were ipso facto the hostile group, as they were settling another people’s land. Otherwise, we’d call them visitors. (edit) My intention wasn’t political correctness but that words are important. “unwanted” is more fitting than “hostile,” as it matches the intention of the settlers. reply nxobject 3 hours agoparentprevIt's a little bit unfair to say that this mission was just about nutrition – apparently there were simulated stressors, although I wish there were more details about what there. That being said, you're right to say that people weren't being pushed psychologically to the \"real\" limit that you'd get on long-term isolated missions... I guess that's where The Right Stuff of ye olde Space Age comes in. [1] https://www.nasa.gov/humans-in-space/chapea/about-chapea/ reply yayr 9 hours agoparentprevthere is this review of \"for all man kind\" https://www.imdb.com/title/tt7772588/ why should it be any different on mars than on earth? > This show has a cool premise, that being what if the space race never ended. It's a sort-of alternate reality and it does a good job of weaving in actual historical events with where the timeline diverged. The main problem is that I feel like the show is being pulled in two directions. In one direction, there is the tension of the space race, engineers scrambling to be the first on the moon/mars and dealing with all manner of technical issues in a realistic-ish way. That part of the show I enjoy. Then, for some reason, the show also throws in a bunch of trite interpersonal drama and stupidity. Like inter-marital affairs, people leaking NASA secrets to the soviets, and a CLEARLY unstable drug-addicted astronaut being given solo control of a super important mission. It's like the showrunners thought the show couldn't stand on it's own without dumb drama, as if there couldn't organically be issues and drama in the context of Frigging SPACE. The first season does this better, but by the 2nd/3rd seasons most of the issues come not from unforeseen difficulties of life on the moon/mars but idiots. It really makes me wonder if they just aren't sure who their audience are. The people who like the technical stuff are not going to like the artificial drama, and vise-versa. Pick a lane, show, and stick with it. reply basementcat 8 hours agorootparentI would argue that sometimes \"dumb drama\" is a good mirror of humanity. Inter-marital affairs really do happen (including in the highest levels of public policy as well as the astronaut corps), people do disclose \"secrets\" and pilots (including astronauts) are known to deceive medical professionals to avoid being \"grounded\". https://en.m.wikipedia.org/wiki/Lisa_Nowak https://www.space.com/spacex-employee-lawsuit-sex-discrimina... https://www.orlandosentinel.com/2019/04/30/after-video-of-sp... https://www.washingtonpost.com/politics/2023/08/27/faa-pilot... reply nxobject 3 hours agorootparentMy other favorite example of dumb drama at the highest levels: ex-director of the CIA, David Petraeus, was forced to resign after being caught by the FIB in an extramarital affair with his biographer. [1] You'd certainly think that the ex-military head of a spycraft organization would know to be a little more disciplined... and less vulnerable to blackmail. [1] https://en.m.wikipedia.org/wiki/Petraeus_scandal reply jowdones 3 hours agorootparentMy expectation is the exact opposite, CIA chief would be a libidinous drug addict, but my view has been shaped by this character: https://americandad.fandom.com/wiki/Avery_Bullock reply exe34 8 hours agorootparentprevWhere there are humans, there will be dumb drama. It's practically a law of human nature. reply Filligree 7 hours agorootparentprevThe drama isn’t even done that well. Like you say, it’s stupid; the people are on average just worse than the sort of people who would actually be there. I wanted the technical sci-fi to be worth it, but it wasn’t. There really isn’t much catering to an engineer audience. reply polairscience 6 hours agorootparentAs somebody who has spent a lot of time with high-quality academically pedigreed humans in far out places, I assure you that they're very capable of dumb drama. And, again tho I hated the show, everyone will be watching just as they were in for all man kind. reply yayr 5 hours agorootparentmaybe it should be mandatory to watch all episodes of \"Big Bang Theory\" on a trip to mars for educational purposes ;-) reply mensetmanusman 6 hours agorootparentprevPeople have many facets of intelligence, and no one hits them all across the board. Where there are blind spots, there will be drama. reply wyldfire 1 hour agorootparentprev> The people who like the technical stuff are not going to like the artificial drama, and vise-versa. Pick a lane, show, and stick with it. I like the show as-is. Maybe I'm not in love with every single subplot but in general the show is pretty great. IRL people have motivations that drive them to do greedy, irresponsible things -- even astronauts. reply BurningFrog 4 hours agoparentprev> It's really difficult to be isolated in a high stress environment. Mars seems more like a high boredom environment. It's a dead planet where nothing more than the occasional feeble dust storm happen. reply TheCraiggers 3 hours agorootparent> Mars seems more like a high boredom environment. Something tells me that the first few thousand people sent to Mars won't be bored. There will be tons of things to do in the pursuit of making a habitable space. Besides that, you have the constant death factor. I know we all live with risk 24/7. But, unless we're doing something outside our comfort zone, our brains largely ignore it. Living on Mars will not be like that for a settler. There are real risks that some piece of equipment will fail and kill you all. You'll likely be well aware of that because you'll be working on it. (Or worse, that idiot over there holding the spanner the wrong way is working on it.) reply russfink 3 hours agorootparentShould we pursue such a simulation in an undersea environment instead? Real stress, real risk, real isolation - but could pull someone up if super urgent. Would also simulate the long voyage to get there. reply OnlyMortal 2 hours agorootparentThat might be a good idea if space suits are available. It would be risky, tbf. reply ETH_start 3 hours agorootparentprevMars' environment is extremely hostile to life, with no breathable air, temperatures more extreme than what's found in Antarctica, and no water or food resources that can be foraged. As if all this were not enough, radiation levels on its surface are also 20 times higher than what's found on Earth. Adding to the difficulties imposed by the harshness of the environment and the paucity of its resources, a habitat there will be more remote than any human habitat that has existed in centuries, perhaps in history, in terms of difficulty of resupplying it. Simply surviving without the vast support network of civilization at hand is a constant struggle on Earth, and all of this will be exponentially more challenging on Mars. reply BurningFrog 2 hours agorootparentYou make a good case for why being on Mars is dangerous. My point is not that it's safe, but that there isn't much to do there, and very little unforeseen happens. I assume no one goes to Mars without the resources to live safely and feed themselves. If we're talking about some adventurers hijacking a Starship, I fully agree that they're in for a world of stress :) reply SoftTalker 2 hours agorootparentprevYeah I think humans living on Mars is a complete fantasy, at least in this century. Will humans visit Mars? We'll probably attempt it. But living there permanently? Not going to happen. reply ETH_start 1 hour agorootparentIt would certainly be difficult, but if we do manage to create viable permanent settlements on Mars that would massively expand the space where humans can live, since it would require significant upgrades in a whole suite of technologies necessary for human colonization of extraterrestrial environmens. Even a technology as seemingly unrelated to Mars settlement as autonomous robots I believe would see significant advances as part of any effort to create a permanent human presence on Mars. reply SoftTalker 42 minutes agorootparentBest case, the technology will allow us to continue to live on a warmer earth, which will still be vastly easier than living on Mars. reply cubefox 2 hours agoprevOne thing I don't understand: In over two decades of ISS operation, there apparently was never time to do a zero-g pregnancy experiment on mice or other small mammals. Zach/Kelly Weinersmith mentioned this in an interview with Sean Carroll on Mars colonization: https://youtube.com/watch?v=dJqr_cCi9tM If it turned out that mice can't properly reproduce in the zero-g environment of the ISS, it would be very likely that (much heavier) humans can't properly reproduce in the low-g environment of Mars. Which would be a very important thing to figure out. Perhaps more important than the mostly psychological Mars simulation project. reply quotemstr 2 hours agoparent> One thing I don't understand: In over two decades of ISS operation, there apparently was never time to do a zero-g pregnancy experiment on mice or other small mammals. Coincidentally, I was looking for research into zero-g pregnancy myself a few days ago and was surprised just as you are. I did, however, find https://www.audubon.org/news/the-amazing-story-cold-war-spac.... The Soviets did some good research on this subject and were able to successfully hatch birds at least. reply cubefox 51 minutes agorootparentInteresting. Here is more detail on how those Soviet quail experiments went: https://finchwench.wordpress.com/2011/09/06/cosmoquails/ Unfortunately, the chicks often came out deformed and many died early. Though birds are not closely related to us (our last common ancestor lived more than 300 million years ago), so I really would like to see ISS experiments on whether mice fare any better, or perhaps even worse. Overall, I'm pessimistic about colonizing Mars. In the interview mentioned before, Kelly and Zach Weinersmith also mention other problems, like the necessity to live underground in caves, due to radiation. Building an underground city on Earth would be vastly easier than on Mars, yet nothing like it has ever been tried. I think the most likely thing that will happen is this: In about 20 years, NASA will use Starship (a SpaceX rocket currently in development) to send a few US Americans to Mars, they will plant the flag of the nation, and, having beat the Chinese, that will be mostly the end of humanity's Mars aspirations. Basically like the moon missions: Once the novelty is gone, people will likely lose interest in earlier aspirations of establishing a permanent base. Especially if the ISS mice turn out crippled. reply quotemstr 2 minutes agorootparentI actually did the math on the Mars problem a little while ago. It wouldn't be that hard to build a big circular railway on Mars that would let the cars experience 1G. (Maybe a few hundred meters diameter?) Think giant luxury Gravitron. You'd stop the train every few days to allow for resupply and entry or exit. It'd certainly be a different society in which pregnant women were expected to sequester themselves in high-G trains for the duration of their pregnancies, but there's no law of nature stopping this approach working. reply accrual 9 hours agoprevI found this quote from the article especially heartfelt. To me it doesn't just convey a hopeful message, it also shows the attitude and inner strength of someone who just endured the closest thing we have to living & working on a new frontier. It's a powerful message when read through that lens. > \"We can do these things together,\" Brockwell said. \"We can use our senses of wonder and purpose, to achieve peace and prosperity and to unlock knowledge and joy for the benefit of everyone in every part of planet Earth,\" he added. reply timonoko 7 hours agoprevI bet they did not give even a minute thought about the people who live and die in tiny boxes on desolate icefields. There was weird customs and rules to make this life possible, as for example Peter Freuchen documented. This should be the starting point, imho. Those cultures were honed in 10000 years just for Mars survival perfection. reply coffeebeqn 3 hours agoparentIt’s still an order of magnitude more chill off a life than Mars. You’re surrounded by drinkable water and breathable air and some critters to hunt reply Ekaros 3 hours agorootparentAlso how simple it is to remove yourself from some situation. Something is getting on nerves. You can just put on some clothing and walk outside. Stand there for 15 minutes or half and hour and get to decompress... No such opportunity on other objects... reply oersted 6 hours agoparentprevA lot has been written by and about Peter Freuchen. Could you provide a more specific reference for these customs and rules? I'd like to read-up on it. reply timonoko 6 hours agorootparentI read the \"Vagrant Viking\" fifty years ago. And the book was probably heavily censored in translation because ungodly family habits. So I am not sure where I got all the ideas from. There was somebody else married to eskimo too, interesting because he made an Umiak suitable for outboard engine. reply HPsquared 6 hours agoprevBrowsing the modern web would be a nightmare from Mars, with a 30-minute ping and all the HTTP back-and-forth that modern pages require. I guess you would have to use an Earth-based remote desktop. reply skellington 6 minutes agoparentLogging in to your bank account to pay the Mars mortgage would take like 4 hours. reply ianburrell 2 hours agoparentprevI have been thinking recently that Mars and maybe the Moon would need to use asynchronous messages. Basically, email but with multiple message types. For browsing the web, they would send a message requesting a page, and get back the messages for the page in 30 minutes. They would probably send a request for whole site instead of round-trips for links. This means that interactive JavaScript sites would never work. There would need to be way to download app or web app that could send messages to update state. Or way to install local server that can sync with Earth. reply hughesjj 5 hours agoparentprevWith a fat enough pipe and big enough cache web browsing could be doable with push based caching. Counterstrike or zoom meetings? Not so much. reply the8472 1 hour agoparentprevBuffering... please wait reply jrnichols 2 hours agoparentprev...only to run into a page that drops a nasty overlay on your browser window because you have Adblock enabled. reply cdelsolar 6 hours agoparentprevThat would still have a 30 minute ping. You should just have a Martian Internet and then sync up the two every few hours or something. reply gruez 5 hours agorootparent>You should just have a Martian Internet and then sync up the two every few hours or something. the \"internet\" is just a network of computers. \"sync up the two\" would involve replicating the servers and coming up with some sort of consensus protocol. Needless to say, that's non-trivial to implement and needs to be done for each site individually. If there was a martian colony it might be economically feasible for some sites to do this, but if it's just a few astronauts the most feasible way of implementing internet browsing would be something like archive.today, where you send a url to some server on earth, the server runs a headless browser and takes a screenshot, and sends the captured contents back to mars/spacecraft. reply ironhaven 4 hours agorootparentprevThe question is whether it will be like Usenet with nntp or a BBS with xmodem reply thfuran 4 hours agorootparentprevIt would have a 30 minute ping for rendering, but all the round trips required for loading the page would be a few orders of magnitude shorter. reply HPsquared 6 hours agorootparentprevProbably not possible for the first several years, with limited bandwidth. Maybe they could use sneakernet for periodic updates. reply TheOtherHobbes 4 hours agorootparentMost internet traffic will be predicted/generated by AI by then, so they'll only need to send a few tiny deltas. reply supersparrow 5 hours agoparentprevMaybe one day we’ll see relay stations in space. One spaced every x miles transmitting the internet from earth. Ping should be minimal. reply thfuran 4 hours agorootparentThe delay is from the time of flight of light between Earth and Mars. Relays can only add additional processing time without shortening that, though they could potentially help with signal integrity. You also can't just freely place a relay between earth and Mars and have it stay between them — both planets are orbiting the sun and the relay also would be, with a period that doesn't match either planet. reply lolinder 5 hours agorootparentprevCannot tell if /s, but just in case this was serious: light takes between 3 and 22 minutes to travel from Mars to Earth and back, which means that ping cannot be less than between 6 and 44 minutes depending on orbits. Robust caching would help but still be nothing like what we experience on Earth (particularly the interactive portions of the web like, say, HN). reply withinboredom 4 hours agorootparentYou forgot to include the fact that there is a point where mars is behind the sun, which makes for a period where the ping will be ~2 weeks. reply Jtsummers 3 hours agorootparentBeyond line-of-sight (BLOS) communications are used all the time with satellites and other relays so people can communicate with others outside their immediate LOS. You need relays that extend the visibility of the planets with respect to each other. This is non-trivial, but not much harder than the work already going on to land things on Mars that requires (or desires) comms back to Earth. reply HPsquared 4 hours agorootparentprevRelay stations at a third location would fix this. reply lintalist 9 hours agoprevCHAPEA Mission 1 Egress Event https://www.youtube.com/watch?v=mNezVXznaHQ reply pseingatl 3 hours agoprevHow much law do we take with us? marslegalcode.org reply yayr 9 hours agoprev [–] I could not really find a TLDR or dive into the mission results... Is there something already? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Four scientists completed a year-long Mars simulation in Houston's Mars Dune Alpha habitat, a 3D-printed structure designed to replicate Martian conditions.",
      "The mission, part of NASA's Crew Health and Performance Exploration Analog (CHAPEA), aims to gather insights for future Mars missions and long-term space exploration, including the Artemis program.",
      "The team conducted \"Marswalks\" and dealt with stressors like isolation and communication delays, with additional CHAPEA missions planned for 2025 and 2027."
    ],
    "commentSummary": [
      "Scientists have concluded a year-long Mars simulation, focusing on human endurance in isolated, high-stress environments.",
      "The project underscores that psychological and social dynamics are more challenging to manage than nutritional needs.",
      "Discussions include technical and ethical considerations, potential high mortality rates, and the necessity for extensive preparation and smaller initial missions for Mars colonization."
    ],
    "points": 91,
    "commentCount": 59,
    "retryCount": 0,
    "time": 1720341858
  },
  {
    "id": 40896212,
    "title": "\"AI\", students, and epistemic crisis",
    "originLink": "https://miniver.blogspot.com/2024/07/ai-students-and-epistemic-crisis.html",
    "originBody": "Miniver Cheevy some things I think are interesting More pages ... Home Design Public interest Esoteric Cultural politics Stale blogroll 06 July 2024 “AI”, students, and epistemic crisis Weird interaction with a student this week. They keep coming up with weird “facts” (“Greek is actually a combination of four other languages”) that left me baffled. I said let’s look this stuff up together, and they said OK, I’ll open a search bar, and they opened … Ch*tGPT. And I was like “this is not a search bar” and they were like “yes it is, you can search for anything in here”. The thing that made me feel crazy is like, every kid that’s using this as a browser is getting new bespoke false “facts”. This isn’t “a widespread misconception about X that stems from how it’s taught in schools.” Each individual kid is now hooked into a Nonsense Machine. With the “widespread misconception about X” you can start at a baseline. Like, OK, in tenth grade we talk about X thing from history, and that leaves us with some misguided concepts about X, but we can correct that as students get broader understandings of the world. But with this, each child is getting unique wrong facts they are sure are correct … because they did what we told them to do! They “looked it up”! They got it from somewhere! It’s not a kid making up a belief on hearsay and assumption … it’s something they think they learned. This kid was extremely combative with me, and I understood why. I was sitting in front of him telling him that the internet, a computer, technology, all these supposedly authoritative things … were wrong. And that I, one person, was right. He basically couldn’t believe me. He decided that I was simply a teacher who’d made a mistake. He could check it, after all! He could look it up! He could find the real facts. I obviously hadn’t done that, I was just an adult who’d decided I was smarter than him. Hence the defensiveness. Like I said: I understood. It was so fucking rough. I did my best, but I am one person trying to work against a campaign of misinformation so vast that it fucking terrifies me. This kid is being set up for a life lived entirely inside the hall of mirrors. Transcribed from Twitter. The author took it down because of harassment, so I am not going to point to who they were. Not that I know anything about them anyway. So you have to make your own tricky call about whether and how it is relevant. No comments: Post a Comment Older Post Home Subscribe to: Post Comments (Atom) Me User experience strategy / research / design Hermeticist Why “Miniver Cheevy” - - - - - - - - - - - - miniver@gmail.com - - - - - - - - - - - - Mastodon Xitter My games on Itch.io minivercheevy@ProtonMail.com Facebook Instagram LinkedIn Tumbr LiveJournal feed of this blog Useful indices Scott Alexander Siskind of Slate Star Codex The San Francisco tech boom crisis Understanding American politics Cultural appropriation Social class categories Judeo-Christian Trans athletes Buy nice stuff Neoliberalism Fake sources Neoreaction Ron Paul Quillette ADHD Guns Topics public interest cultural politics culture fun esoteric geekkultur design popkultur the crisis cinema tech BS science art comix the horror ideas obits stories resource israel-palestine urban design antifa Great Americans definition game meme Trumpism me Best of cultural politics Why I insist that I am a feminist Esoteric cultural appropriation Open letter to PantheaCon Heinlein’s sexism Cuteness & punk Our little secret Introvert cards Feminism (n.) Consent Askew Best of politics politics THE IMPORTANT ISSUES Authoritarian psychology and the liberal democratic ethos The Crocodile Epiphany and the case for socialism Sarah Palin, doubleplusgood duckspeaker Fascism, authoritarianism, totalitarianism The livelihoods of far right cranks What, if not liberal democracy? American political reälignment Learning from Vietnam Zimmerman's certainty Architecture + politics Liberty & capitalism Independence Day Martin Luther King Political spectrum Online discussion Whatever it takes Bundy vs Occupy Free speech (n.) Democracy (n.) Charlottesville Korman's Law Nonviolence Hiroshima Antifa (n.) Markets Senate Power Best of esoteric The pagan sensibility Santa Claus is real Possession Black heart Swamp Charity Best miscellany Decision-based evidence-making Steve Jobs Warhol Advice Brazil EULA Entertainments Jingle Jangle: A Christmas Journey What The [Bleep] Do We Know? Kingdom Of The Crystal Skull The perfect media franchise Justice League (Unlimited) The Trial of the Chicago 7 Some Kind Of Wonderful Star Trek: Into Darkness Hamlet, Buffy, Spock Inglorious Basterds The Marvel Movies X-Men: First Class Last Tango In Paris Django Unchained Game Of Thrones Person Of Interest Star Wars midrash The Craft: Legacy Midnight Mass Our little secret Sucker Punch WandaVision Black Mirror Grindhouse Interstellar Lost Boys Sunshine The Wire Fearless Sleepers Skyfall Rome Archives July 2024 (2) June 2024 (3) May 2024 (5) April 2024 (7) March 2024 (4) February 2024 (6) January 2024 (10) December 2023 (3) November 2023 (8) October 2023 (2) September 2023 (2) August 2023 (6) July 2023 (3) May 2023 (6) March 2023 (8) February 2023 (4) January 2023 (2) December 2022 (1) November 2022 (10) October 2022 (1) September 2022 (2) August 2022 (2) July 2022 (6) June 2022 (4) May 2022 (3) April 2022 (1) March 2022 (1) February 2022 (1) January 2022 (4) September 2021 (2) August 2021 (1) July 2021 (4) June 2021 (1) May 2021 (9) April 2021 (2) March 2021 (1) February 2021 (5) January 2021 (5) December 2020 (6) November 2020 (4) October 2020 (2) September 2020 (4) August 2020 (6) July 2020 (1) June 2020 (5) May 2020 (1) April 2020 (1) March 2020 (2) October 2019 (1) February 2019 (2) January 2019 (2) December 2018 (2) October 2018 (1) July 2018 (1) June 2018 (2) May 2018 (2) December 2017 (2) August 2017 (1) July 2017 (1) February 2017 (1) January 2017 (1) December 2016 (5) November 2016 (1) August 2016 (1) April 2016 (2) March 2016 (4) January 2016 (1) December 2015 (1) November 2015 (1) October 2015 (1) September 2015 (1) August 2015 (5) July 2015 (4) May 2015 (1) March 2015 (1) February 2015 (3) January 2015 (4) December 2014 (3) November 2014 (6) October 2014 (8) September 2014 (5) August 2014 (8) July 2014 (13) June 2014 (10) May 2014 (8) April 2014 (7) March 2014 (10) February 2014 (9) January 2014 (16) December 2013 (29) November 2013 (12) October 2013 (14) September 2013 (10) August 2013 (15) July 2013 (10) June 2013 (13) May 2013 (9) April 2013 (16) March 2013 (27) February 2013 (25) January 2013 (28) December 2012 (30) November 2012 (27) October 2012 (13) September 2012 (25) August 2012 (15) July 2012 (7) June 2012 (10) May 2012 (8) April 2012 (6) March 2012 (4) February 2012 (8) January 2012 (13) December 2011 (10) November 2011 (3) October 2011 (4) September 2011 (2) August 2011 (2) July 2011 (10) June 2011 (8) May 2011 (3) April 2011 (3) March 2011 (7) February 2011 (3) January 2011 (1) December 2010 (4) November 2010 (4) October 2010 (1) July 2010 (3) June 2010 (6) May 2010 (7) April 2010 (8) March 2010 (3) February 2010 (4) January 2010 (11) December 2009 (3) November 2009 (3) October 2009 (7) September 2009 (9) August 2009 (8) July 2009 (9) June 2009 (32) May 2009 (32) April 2009 (17) March 2009 (4) February 2009 (3) January 2009 (2) December 2008 (12) November 2008 (29) October 2008 (58) September 2008 (11) August 2008 (24) July 2008 (36) June 2008 (37) May 2008 (7) April 2008 (3) March 2008 (17) February 2008 (2) January 2008 (4) December 2007 (2) October 2007 (4) September 2007 (23) August 2007 (34) July 2007 (42) June 2007 (33) May 2007 (34) April 2007 (24) March 2007 (15) February 2007 (45) January 2007 (55) December 2006 (37) November 2006 (30) October 2006 (49) September 2006 (42) August 2006 (50) July 2006 (45) June 2006 (48) May 2006 (53) April 2006 (50) March 2006 (39) February 2006 (33) January 2006 (39) December 2005 (36) November 2005 (45) October 2005 (15) September 2005 (5) August 2005 (32) July 2005 (33) June 2005 (33) May 2005 (36) April 2005 (43) March 2005 (51) February 2005 (35) January 2005 (47) December 2004 (49) November 2004 (48) October 2004 (58) September 2004 (40) August 2004 (50) July 2004 (51) June 2004 (41) May 2004 (58) April 2004 (46) March 2004 (66) February 2004 (70) January 2004 (34) January 2000 (11) Stale blogroll A Future Worth Thinking About S.H.A.M.E. Project Ta-Nehisi Coates Crooked Timber James Fallows Rick Perlstein Making Light Weekly Sift The Baffler Hullabaloo Ian Welsh þ miniver.blogspot.com .... Powered by Blogger.",
    "commentLink": "https://news.ycombinator.com/item?id=40896212",
    "commentBody": "\"AI\", students, and epistemic crisis (miniver.blogspot.com)89 points by ColinWright 9 hours agohidepastfavorite121 comments sieste 8 hours agoLike probably many people here I still remember having to find facts in books in libraries, before the internet made this skill mostly redundant. Then, as a student I remember having to put together facts from various (internet) sources into a coherent narrative. Now chatbots can just generate text and that skill seems less valuable. I use both the internet and GenAI extensively now. But I feel that having gone through the \"knowledge work\" activities without the crutches puts me in a better position to assess the correctness and plausibility of internet sources and AI in a way that kids who grow up using them constantly don't have. I feel quite privileged to be in that position, that I wouldn't be in had I been born 10 or 20 years later. I also feel sorry for kids these days for not having the opportunity to learn things \"the hard way\" like I had to. And I feel extremely snobbish and old for thinking that way. reply mrcartmeneses 8 hours agoparentIt’s something reflected in the conversations I have with my academic friends. I’m told that every essay is written in the same “voice” and that although they are usually a simulacra of an academic paper, they say nothing. The sad part comes when we reflect that these students are not learning the deep thinking skills that comes with academic writing reply ryandv 7 hours agorootparentThere are two views of writing: one, as the production of a literary artifact that has value in its own right and stands alone as the embodiment of a complex idea itself; on the other hand, as a process and tool for thought, where the literary artifact is merely meant to represent the cognitive work that went into its production. From the latter perspective, the bulk of the value is not derived from the output of the process of writing, but rather from the understanding and insight that was gained during its production - \"it's the journey, not the destination\". With generative AI we are now shortcutting directly to the destination while eliding all the knowledge and understanding we are supposed to be gaining on the way. It's deemed sufficient to merely produce and exchange the symbols of understanding without needing to possess any real underlying wisdom (the first view above), because it's through the exchange of these abstract symbols, irrespective of whether or not there is anything behind them, that we can play and win certain social games. This is merely the natural continuation of trends that began during the dawn of the Internet era, when we started to consider pixels on a screen an accurate map of reality. reply more_corn 3 hours agorootparentprevThen they’re using gen ai wrong. You can dump your research into the context window and ask it to outline the material. What you get out is a well organized story with a beginning middle and end, incorporating all relevant concepts from the research. You can then fill in with the details based on your research. Gen ai can be used to help students think and write more clearly. They’re going to use it. Give them the training and guidance to use it correctly. reply weinzierl 8 hours agoparentprevI also lived through these phases and it makes me feel very, very much the same. On the other hand I cannot not help thinking that this is similar to the arguments brought forward when the internet was new. How could correctness and plausibility be established if you don't have established trustworthy institutions, authors and editors behind everything? And yet it turned out mostly fine. Wikipedia is alive, despite its deficiencies, Encyclopedia Britannica not so much. So, is it only old people's fear? reply LittleCloud 4 hours agorootparent> On the other hand I cannot not help thinking that this is similar to the arguments brought forward when the internet was new. How could correctness and plausibility be established if you don't have established trustworthy institutions, authors and editors behind everything? Not long ago I had the same viewpoint as you! But thinking back now — it dates me but I definitely lived a childhood without Internet access — probably the optimistic belief before our age of \"misinformation\" is that, in the marketplace of ideas, the truth usually wins. Goes along with \"information wants to be free\" — remember that slogan? For us that grew up learning things \"the hard way\" so to speak, that made perfect sense: each of us, as should have the capability to discern what is good or bad as, individual independent thinkers. Therefore, for any piece of information, there should be a high probability, in the aggregate, that it is classified correctly as to its truth and utility. I think, that was to some extent, even a mainstream view. Here's what Bill Clinton's said in 2000 advocating to admit China to the WTO: (https://archive.nytimes.com/www.nytimes.com/library/world/as...) Now there's no question China has been trying to crack down on the Internet. Good luck! That's sort of like trying to nail jello to the wall. (Laughter.) But I would argue to you that their effort to do that just proves how real these changes are and how much they threaten the status quo. It's not an argument for slowing down the effort to bring China into the world, it's an argument for accelerating that effort. In the knowledge economy, economic innovation and political empowerment, whether anyone likes it or not, will inevitably go hand in hand. I would say, what we have since learned after some 20 years, is that in the marketplace of ideas, the most charitable thing we can say that the memes with the \"best value\" win. \"Best value\" does not necessarily mean the highest quality, but rather there can be a trade-off between its cost and the product quality. Clearly ChatGPT produces informational content at a pretty low cost. The same can be said for junk food, compared to fresh food: the overall cost of the former is low. Junk food does not actively, directly harm you, but you are certainly better off not eating too much of it. It is low quality but has been deemed acceptable. There are examples where we can be less charitable of course. We all complain about dangerous, poorly manufactured items (e.g. electronics with inadequate shielding etc.) listed Amazon, but clearly people still buy them anyway. And then, in the realm of politics, needless to say, there are many actors bent on pushing memes they want you to have regardless of their veracity. Some people on the marketplace of ideas \"buy\" them owing to network effects (e.g. whether they are acceptable according to political identity, etc.) in the same way that corporations continue to use Microsoft Windows because of network effects. We also probably say nowadays Clinton has been ultimately proven wrong by the government of China. Survival of the \"fittest\" memes if you like: evolution does not make value judgements. If you ask me, maybe our assumption of de-centralized truth-seeking was itself, not an absolute truth, to begin with. But it took years to unravel, as humans, collectively speaking, atrophy from disuse of the research and critical thinking skills before technology dropped the barriers of entry to producing and consuming information. reply posix86 7 hours agoparentprevYou're probably at an advantage now, but I think the effort/reward hardly pays out for the newer generation. They'll learn how to deal with this with less effort & time. Remember that the new generation doesn't just have different tools; they're also much less experienced & mature, just like we were.You can only really compare yourself to them in the future where they're at the place you're at now. reply ballenf 7 hours agoparentprevMakes me wonder how someone would have found the answers before the printing press. Write to or visit the most knowledgeable person you could get an introduction to or who would answer you? Then go to their contacts or recommendations? Then draw a conclusion from all the responses? And even this scenario assumes a working postal system, availability of paper and pen. But if you didn't have those, you may not have heard of Greek to even ask the question. reply more_corn 3 hours agoparentprevThe problem is that gen ai has no notion of fact and will just as happily confidently and incorrectly assert falsehoods. Teaching an entire generation of students with gen ai and no verification of facts will be a disaster. reply watwut 7 hours agoparentprevI think that chatbots will lead to less information available, not more. Because they make writing information more useless and demotivating. So what we are looking toward is less written information overall available. And when chatbots don't know, they lie. Kids will learn the hard way. Possibly harder then we did. reply EVa5I7bHFq9mnYK 6 hours agoparentprevOk, fellow boomer. And who do you think is the populace most likely to fall for Nigerian princes and believe in 5G Bill Gates vax chips? reply allsummer 6 hours agorootparentThat there are lots of people who believe in 5G Bill Gates vax chips is itself fake news. It is well poisoning to pre-empt criticism of billionaires with too much power and free time to meddle in African population growth and pandemic response. Supported by \"smart\" people who want to feel good and trust the science on 5G safety. There are Microsoft patents for microchips to track body activity to reward in cryptocurrency and subsidiaries who wanted to microchip vaccine passports into the hands of immigrants. reply EVa5I7bHFq9mnYK 4 hours agorootparentThat's what I talk about:) reply acnyy 4 hours agorootparentprevYes, real or invented conspiracy theories are routinely amplified and exaggerated by the powerful in order to link critics of $THING to undesirable people. Mention the word \"elites\" and you are a Nazi by association. Works every time. reply acnyy 4 hours agorootparentprevPeople who read academic publications and do not see much of a fundamental difference between tattoos that contain information like a barcode and RFID chips that could contain the same information: https://news.rice.edu/news/2019/quantum-dot-tattoos-hold-vac... Or perhaps people who have been confused by conspiracy sites like npr.org and make the mental leap that this would be used for other purposes: https://www.npr.org/2018/10/22/658808705/thousands-of-swedes... reply anal_reactor 6 hours agoparentprev> And I feel extremely snobbish and old for thinking that way. Old people usually make correct assessments given their knowledge and experience. They know how to maximize their expected gains and play safe. The problem is, real life often rewards those who take risks, and make seemingly wrong decisions, that later turn out to be good. For example, as a kid I loved playing video games, while my grandma yelled at me for not wanting to help her with work at the farm. She had absolutely no way of predicting that playing video games, which were essentially just toys, would teach me the right skills at the right time, allowing me to move up the social ladder. At the time, forcing the god damn lazy kid to milk the god damn cow was the sensible thing to do. reply huimang 8 hours agoprevI feel like this is a bit overblown. Growing up we heard, ad nauseam, that \"wikipedia is not a reliable source\". People just need to state the same thing about LLMs- they aren't reliable, but can potentially point you to primary sources that -are- reliable. Once the shininess of the new toy wears off, people will adjust. reply shepherdjerred 2 hours agoparent> wikipedia is not a reliable source Wikipedia is as good as anything else. As an example, I frequently use Wikipedia to read about history, computer science topics (e.g. how to implement an algorithm), or scientific topics. The exception is current events, but even then, I suspect that Wikipedia is not any more biased than the news. I'm open to having my mind changed, though. reply Perceval 33 minutes agorootparent> Wikipedia is as good as anything else. Encyclopedias – including Wikipedia – are not acceptable sources for college-level work certainly. They are tertiary literature, which can provide an overview to someone trying to get a toehold into a subject, and which can hopefully point them toward primary and secondary sources. But tertiary sources are not typically allowable citations for college research. reply nebulous1 8 hours agoparentprevI think it's pretty clear that as a tool they can be used very effectively if you have at least some understanding of their limitations. I will say that wikipedia, as things stand, is way more accurate than chatgpt. So much so that comparing them doesn't even make sense to me. reply cheschire 7 hours agorootparentUntil you can quantifiably prove Wikipedia is more accurate than ChatGPT, I’m not sure how you can say GP’s comparison doesn’t make sense. reply nebulous1 6 hours agorootparentOkay, I can't defend that because in reality I'm talking purely from my subjective experience. I will say that in my experience it isn't close at all. However, I will also say that a lot of the things that chatgpt gets wrong for me, wikipedia just won't contain at all, but then chatgpt seeming confident when it's wrong is basically the whole problem. reply sieste 5 hours agorootparentOne way of looking at it is that Wikipedia has a transparent and auditable way of correcting and updating false information, which makes it inherently more reliable and trustworthy than tensor weights finetuned by unreproducible human feedback. reply cheschire 6 hours agorootparentprevWikipedia has well documented and explored issues related to vandalism, bias, and misinformation. But don’t get me wrong, I also view ChatGPT conversations as being on par with pub chats as far as “confident facts” go. I think our difference may be in how we view Wikipedia. reply ninkendo 5 hours agorootparent> Wikipedia has well documented and explored issues related to vandalism, bias, and misinformation. Last I read about this, the error rate in Wikipedia was actually lower than in the Encyclopedia Britannica, by a measurable amount. This was a while back, and admittedly it only counted articles where there was an equivalent article in both (which probably gives a better picture of Wikipedia, as those kinds of “boring” articles have less vandalism…) but it’s not immediately a given that Wikipedia is just objectively bad at being accurate. reply cheschire 7 hours agoparentprevWell even before Wikipedia… remember calculators being banned? I think you’re right, we will adjust. Curriculum development will start to include new checkpoints and controls. reply rsanek 2 hours agorootparentAren't calculators still banned in primary education? From what I know, you cannot use them during most tests. reply roenxi 7 hours agoparentprevAnd the LLMs are still improving at a brisk rate. If they were outperforming teachers by the end of the decade that'd be well within expectations. Anyone pretending that human authority figures routinely get things right is defining correctness using circular logic. reply sieste 6 hours agorootparentCurrent LLMs are lacking introspection, plausibility checking, consulting external sources, and belief updating in the presence of new evidence. All of these you'd need to replace human teachers and it's not clear that the next token prediction paradigm can ever emulate these features reliably. So \"outperforming teachers\", while not impossible, is a very optimistic expectation as it requires more than mere improvement on existing methodology. reply jdietrich 2 hours agorootparentMaybe I was particularly unfortunate, but none of my human teachers had any of those features. There was one inviolable source of truth in my classrooms - the examination syllabus and the accompanying textbook. reply watwut 7 hours agoparentprevWikipedia is massively more reliable then ChatGPT. reply i_am_proteus 8 hours agoprevI've seen this happen, too, including student incredulity that ChatGPT can be wrong, and recalcitrance when guided to find proper sources. Up to the point of arguing for a higher grade based on the correctness of LLM output. reply atoav 8 hours agoprevThe problem with not writing yourself isn't that people didn't do the writing themselves, the problem is that they didn't do the thinking themselves that is a prerequisite for writing. Now of course like any tool this can be used without falling into that trap, but people are lazy and the truth is that if you don't absolutely have to do it yourself most people won't. reply xg15 7 hours agoprevI would have thought this problem was easy to solve: \"Yes, look it up, but remember your source, there is a lot of bullshit on the internet. Especially don't trust AI tools, we know those often return nonsense information.\" (Actually, didn't ChatGPT have a disclaimer right next to the prompt box that warns against incorrect answers?) So I'm more surprised (and scared) that students don't just use LLMs for sourcing but are also convinced they are authoritative. Maybe being in the tech bubble gave a false impression here, but weren't hallucinations one of the core points of the whole AI discurse for the last 1.5 years? How do you learn about and regularly use ChatGPT, but miss all of that? reply jdietrich 2 hours agoparentChatGPT says \"ChatGPT can make mistakes. Check important info.\" directly under the prompt box. If people will blindly trust a source that explicitly states that it isn't a reliable source, then they've got much bigger problems than AI. reply BlaDeKke 7 hours agoparentprevAs we grow older, we learn a lot of facts that we can use to test the correctness of ChatGPT and identify its hallucinations because we have the knowledge to do so. However, a young person who is just starting to understand the world and gather knowledge might not have enough information to notice these hallucinations. reply xg15 7 hours agorootparentYup, no question here why he believes ChatGPT's initial statements. I'm more baffled that when the teacher corrects him, he goes on and defends ChatGPT. reply relaxing 7 hours agoparentprevAre these tools being promoted and sold as fallible implements? Or are they being hyped as super-human intelligence? Which takeaway is an impressionable child going to latch onto? One who wasn’t in on the last 1.5 years of discourse? reply auggierose 8 hours agoprevYou could probably make the same argument for search bar vs. peer-reviewed publications. Of course, the search bar (which is also AI, by the way) can help you to get to the peer-reviewed publications. But the same is true for ChatGPT. The problem is that ChatGPT sounds like presenting objective facts. But maybe the lesson here is that just because something sounds right, it isn't necessarily right, and that is something that should be taught in school. Of course, that undermines the function of school to produce obedient citizens. reply lionkor 8 hours agoparentIt also undermines the way schools work to teach kids. In order to not have to explain everything, almost all lessons are mostly teaching you some axioms, even if they really are disputed, or have caveats, etc. Good teachers make clear where there is an axiom, and where something is just being simplified or assumed for the sake of saving time. I am a product of the German school system, I'd say I wasn't indoctrinated too much, so its not entirely broken, but with these new \"\"\"tools\"\"\" maybe we need a reform anyways. reply par1970 8 hours agorootparent> In order to not have to explain everything, almost all lessons are mostly teaching you some axioms, even if they really are disputed, or have caveats, etc. Good teachers make clear where there is an axiom, and where something is just being simplified or assumed for the sake of saving time. What exactly do you mean by the word “axiom” here? reply algorias 7 hours agorootparentSomething assumed to be true without proof. Think of it as one layer of abstraction above the model under discussion. Like a hyperparameter. In later years, students get taught the same topics again, with the hyperparameter tuned to be more realistic. reply lionkor 8 hours agorootparentprevSomething that is taught to be a self-evident or universally recognized truth reply par1970 7 hours agorootparentDoes “self-evident” just mean that anyone who knows the sentence’s meaning can determine that it is true without any need to gather empirical data? eg, All bachelors are unmarried. eg, If X is a triangle, then X has three sides. eg, the world is round or it is not the case that the world is round. And does “universally recognized” just mean that most people believe the proposition is true? reply lionkor 7 hours agorootparentIn this context an axiom is a nonlogical axiom, in other words an assumption, one that is not to be questioned or discussed reply par1970 6 hours agorootparentOkay. So, in your original comment are you asserting that teachers are mostly telling students to believe propositions without giving any epistemic justification for those propositions? reply lionkor 20 minutes agorootparentYes, as part of teaching one topic, teachers have to tell students to \"not worry about\" some other related topic and just take it as given fact, even when that's not technically true reply puttycat 8 hours agoparentprev> the search bar (which is also AI, by the way) It's AI in the older sense of Machine Learning, not in the currently widespread sense of an LLM, which is the source of the problem that the author is discussing. reply auggierose 8 hours agorootparentFace palm. reply bamboozled 8 hours agoparentprevA good way to learn this is to build something, wood working is an awesome way to quickly find out, looking, feeling and even someone taking a measurement isn’t enough, you need to take two :) reply llm_trw 8 hours agoparentprevI had that same argument, the teacher who told me to not trust non peer reviewed articles ended up flooding her Facebook was with pro-brexit lies a decade later. Turns out critical thinking is not outsourcing your thinking to a third party, be it a peer reviewes journal, google search results, or chatgpt. reply superfist 8 hours agorootparent\"not trust non peer reviewed articles\" - this is such naive advice. It is not black and white, peer reviewed articles only increase chance that information included in article is legit because it was verified in some formal process. I wonder how many times people giving such simple advices mention how often peer reviewed articles are retracted or can't even be replicated and how this vary accross disciplines. reply immibis 8 hours agorootparentAnd, like all formal processes, it has been gamed. Many journals are full of high quality material; others are full of peer reviewed bullshit. reply lionkor 8 hours agorootparentprevWhat is a 'pro brexit lie'? reply Angostura 8 hours agorootparentImmigration will be reduced We will be able to negotiate excellent trade deals outside the EU effortlessly The EU need our trade so badly we will be able to negotiate a seamless trade deal with them … and many others. reply lionkor 8 hours agorootparentinteresting, thank you! I hadnt followed it when it happened. reply werdnk 8 hours agorootparentprevI think believing the first lie that conservatives would really reduce immigration is the product of trusting politicians too much. Another faction of the pro-Brexiteers would think of the system in terms of a uniparty. The fundamentalists like Farage have no chance of getting elected as PM in the British system, though he did get an MP seat now. Would things change if Farage were PM? I'm cynical, probably not. reply bostik 1 hour agorootparentThere was a fundamental lie / delusion at the heart of the claim. The second thing a trade deal with another country implies \"allow more of our nationals to migrate, and make it easier for them to come over\". (The first thing is obviously: \"let our products in\".) And this can be seen from the raw numbers too. Before brexit, the annual net migration to UK was about 300k. After brexit, the annual net migration to UK is ~700k.[0] (745k in 2022, 680k in 2023.) And quite a bit of that from the fresh trade partners. The best way I have heard anyone describe the disaster that's Brexit was as \"nostalgic self-immolation for the population who still dream of the Britain where faces were white, passports were blue, and the map was Imperial Pink.\" 0: https://commonslibrary.parliament.uk/research-briefings/sn06... reply auggierose 8 hours agorootparentprev£350m a week for the NHS reply auggierose 8 hours agorootparentprevIndeed, you could apply the same argument to peer-reviewed articles, although at the moment they are still much (much!) more reliable than ChatGPT. reply kukkeliskuu 8 hours agorootparentEver heard of replication crisis? reply immibis 8 hours agorootparentStill better than ChatGPT. reply tatrajim 6 hours agoprevAnd, on a related note for education, AI is quickly obviating the need to master and plumb the depths of foreign languages. Dating myself, doubtless, but as an undergraduate, it was an unalloyed joy to study ancient Greek and read Plato and Euripides in the original, however haltingly. And later Korean, Japanese, and Chinese beckoned providing a lifetime of rich understanding of life outside the confines of English. For Americans, at least, perhaps ours is the last generation that will seek rewire our understanding of reality through linguistic hacking. reply ryandv 1 hour agoparentPresuming that those languages exist within OpenAI's training dataset. Try to have a conversation in Rinconada with ChatGPT; my last attempt led to its admission that it had no proficiency in this language. You will have to find a native speaker, and their numbers are dwindling as Tagalog and English are being favored in education - the latter thanks to American linguistic imperialism. The AI does not circumscribe all of reality; not all of reality is captured on the internet. reply nathansherburn 6 hours agoparentprevAI will always be far better at languages than I'll ever be and I expect it to get much better very quickly. But I'm still learning my partner's language and don't think I'll stop any time soon. I think it's interesting and fun in and of itself. It's also a great scaffold for learning about another culture and learning to respect and understand folks from different walks of life. reply A_D_E_P_T 8 hours agoprevThe great problem with ChatGPT is that it's a sycophant and aims to please. If you ask it about something it doesn't know, right then and there, it will concoct a fiction for you. It won't say \"I don't know,\" or \"I can't help with that.\" If you coach it to respond to something in a certain way, it'll respond that way for you as its top priority. If you ask it to review a text, it'll usually find a way to give you at least a 7 or 8 out of 10. (Though, interestingly, rarely a 10/10 score. You can upload excerpts from some of the great works of literature and philosophy and see ChatGPT give them an 8/10, just as it gives an 8/10 to your essay or blog post.) Practically the only way to get a halfway critical response is to add the words \"be critical\" to your prompt. A more circumspect and less obsequious ChatGPT would solve a lot of problems. reply rchaud 5 hours agoparentChatGPT is the search engine equivalent of those streaming sites that won't directly admit they don't have the show you explicitly searched for, but will instead show you a list of other shows they think will placate you, and pretend they're indistinguishable. reply jodrellblank 7 hours agoparentprev> “If you ask it about something it doesn't know, right then and there, it will concoct a fiction for you. It won't say \"I don't know,\"” It doesn’t know that it doesn’t know. Texts it was trained on rarely go “On the History of the Greek Language, by Andros Dimos; ahem what is the history of the Greek language? I don’t know. The end.” reply A_D_E_P_T 7 hours agorootparentHmmm... well... Let me give you an example. The other day I asked for a summary of the short story \"The Palace at Midnight\" by Robert Silverberg. GPT-4o responded: \"The Palace at Midnight\" by Robert Silverberg is a science fiction short story that was first published in 1980. It is part of Silverberg’s \"Majipoor\" series, but stands alone in terms of its setting and characters, distinct from the expansive world-building typical of the series. The story is set in a dystopian future in India and revolves around themes of memory, loss, and the haunting allure of the past. The narrative follows the protagonist, Edward, and his group of friends who are survivors of a nuclear war that has devastated much of the world. . . All of which was complete nonsense. \"The Palace at Midnight\" is a short story of a few pages that has nothing to do with Majipoor, was published in 1981, and was set in San Francisco. It doesn't know -- it knows that it doesn't know -- so it just makes something up that seems superficially credible. Because it aims to please and be helpful; altogether too helpful. reply jodrellblank 7 hours agorootparent> “it knows that it doesn't know” No it doesn’t; it isn’t an intelligence, it’s a token generator based on patterns in the inputs it was trained on (and those are explanations and answers, not millions of people writing “I don’t know”). reply A_D_E_P_T 5 hours agorootparentRight, but it may be a fixable problem, even under the circumstances. These token generators have vast databases at their disposal and have crawled the entire public-facing internet. They should be able to assign confidence values to their drafts/statements, and reject low-confidence drafts/statements before putting them in front of users. They could do this by fact-checking their drafts in an adversarial way. For instance, a cursory check of isfdb.org would have given ChatGPT better information than I was provided with. reply zertar 7 hours agoparentprevInteresting. Copilot on the other hand is more like a conceited, passive aggressive brat who uses some polite phrases but aborts conversations if you contradict it too much. The only smooth conversations with Copilot are the ones where you allow it to regurgitate \"facts\" and act in a submissive manner. reply joaquincabezas 8 hours agoprevso it’s the classical “it’s on the internet so it’s true” but on steroids. I remember a US student in the early 2000s citing a geocities website as source for the FACT that aliens created the pyramids of Egypt reply pitt1980 7 hours agoprevOne thing I’ve noticed, is that at least in the free version, if you ask chatGPT for sources outside itself, ‘can you give me a link to somewhere on it internet where it says that?’, it won’t do that. It is very much a black box in terms of letting you track its logic. Seems like future versions should be much more transparent in terms of letting you track the logic of why it’s telling you what it’s telling you. reply allsummer 7 hours agoparentYou can just ask it \"What scholars or source material books could I check out to verify these linguistics facts?\". Many facts and answers are gathered/aggregated from many different (sometimes conflicting) sources. It won't link the internet page where it found the information, because it didn't find the information on a single internet page. reply pitt1980 6 hours agorootparentAt on point I asked it for academic papers that said a particular thing, as best I can tell, only about 60% of the papers it cited actually existed reply shzhdbi09gv8ioi 7 hours agoparentprevThey trained on copyrighted material and scraped stuff they shouldn't have. That's why they \"cant\" show sources. reply elforce002 7 hours agoparentprevBrave AI search engine gives you various sources if you want to dig deeper. reply Kiro 8 hours agoprevAm I the only one who almost never experience any hallucinations when talking to ChatGPT? I have to really push it into a corner with trick questions and conflicting instructions about obscure topics in order to trigger hallucinations. That it would just come up with random false facts about something as common and \"basic\" as the history of the Greek language sounds like a made-up issue. reply nebulous1 8 hours agoparentWell, I can't say that you're the only one, but I can say that I don't even interact with it that frequently and I've had numerous instances of it giving me false information. Enough times that I'm surprised to see you say otherwise. reply watwut 7 hours agoparentprevI don't need corner. I asked him for valid IISExpress config file. Chatbots are great systetising in areas where there is a strong open source like culture - a lot of people writing tutorials and on stack overflow. The moment you move into area that is not popular to write about, they start making stuff up. And it sux massively. I would much rather seen \"I don't know\" then what ChatGPT produces currently. reply allsummer 6 hours agorootparentStart small, like asking him to review your grammar. reply cheschire 8 hours agoparentprevI know it feels like 4o has been out forever but go back to the 3.5 model without internet access for a while, which is all that a lot of students can afford. It’s kind of terrible. But yeah ever since I could start adding “and include an explanation with citations” to my prompts I’ve not had an issue with hallucinations more than once or twice. reply hayley-patton 6 hours agorootparentDoes it give real citations? I've heard it gives bogus citations, though can't remember how well it went last time. reply cheschire 6 hours agorootparentI haven’t had any issues, but I’m not using it to do “real” research so the veracity of sources has been less of an issue for me. I typically ask, then spot check sources. If I’m using it to make purchase decisions or scheduling my day then I use it purely to find me source pages. It’s really terrible at calculating the cost of services. reply drivingmenuts 7 hours agoparentprevHallucinations come in all sizes. I wouldn't worry about the big hallucinations - those are easy to spot. I worry about the small, incremental hallucinations - the ones you don't notice until several have happened. It might not be possible to even recognize that sort of thing yourself, if you've been effectively gaslighted by AI. Since there's no actual knowledge on the part of the AI, it doesn't know that it's doing it, so it's not malicious (the designers are a separate question). But it is still entirely possible, and even somewhat likely. reply csantini 8 hours agoprev> Not as wrong as the author thinks. From Britannica.com: \"Greek language, Indo-European language spoken mostly in Greece. Its history can be divided into four phases: Ancient Greek, Koine, Byzantine Greek, and Modern Greek.\" reply earl_gray 7 hours agoparentIf I had to suggest where the “combination of four languages” idea came from, it would be from Homeric Greek (the language the Iliad and Odyssey were written down in). This was genuinely a complete mess, formed of a hodgepodge of different dialects. From wikipedia: “[Homeric Greek] is a literary dialect of Ancient Greek consisting mainly of an archaic form of Ionic, with some Aeolic forms, a few from Arcadocypriot, and a written form influenced by Attic.” I’m not sure if this is a plausible explanation as I don’t have much experience using LLMs. reply allsummer 6 hours agoparentprevWhat happened was either the teacher is severely biased against ChatGPT and fabricated the fact to fit their narrative. Or ChatGPT gave the correct answer, but the student interpreted it wrong. I do believe the students keep coming up with weird (correct) facts, and that this can be scary for a teacher who is stuck at a search bar. reply moritzruth 8 hours agoparentprevThe language evolution is commonly divided into four stages. isn’t the same as Four distinct languages had an influence on the development of this fifth language. reply werdnk 8 hours agoprevIn the current system, where students can anonymously report teachers (most of whom do not have tenure and are afraid) it will be hard to change anything. Otherwise, you could do a mixture of very strict exams without multiple choice and large individual projects (no group projects). If you only do exams, people who don't do well thinking in a room crammed full of people at 8am will be disadvantaged. reply padolsey 8 hours agoprevI don't buy into the rhetoric about misinformation but the author touches on a real concern. I blame ChatGPT and other LLM clients for not surfacing their fallibility more clearly. They should highlight claims in their UX and allow options for the user to \"research\" or \"verify\" in their own way, without relying on that very flakey single one-shot inference. The big honchos (Anthropic, OpenAI) need to make it clearer that their output is, at best, an informed guess. A tiny disclaimer doesn't cut it. reply LUmBULtERA 8 hours agoparentJust to be clear, the \"tiny disclaimer\" is directly below the prompt box and says \"ChatGPT can make mistakes. Check important info.\" So it is interesting that anyone would take a hard stance that their prompt results are true facts without checking any important info (particularly after someone highlights this if they somehow missed it). reply visarga 7 hours agorootparentAll you need to do is ask it to perform a web search. Or you can sample a few answers at high temperature. I think young people growing with LLMs will know all the ins and outs of their limitations. But it's a rapidly evolving field, maybe in 5 years we won't doubt LLMs very often. reply deadbabe 8 hours agoprevI really don’t see any other solution to this kind of problem except for one: LLMs must become perfect, and never be wrong. Relying on kids to do cross referencing and deeper fact checks into everything they ask an LLM is just not going to happen at scale. reply rchaud 5 hours agoparentWhat's so hard about following the Wikipedia model of citing sources? Even 20 years ago, it was clear to college students that they cannot cite \"Wikipedia\", but they can cite the academic literature it referenced. It would take seconds to figure out of the ChatGPT citation is real or made up. reply tomhoward 7 hours agoparentprevMaybe it’s the opposite; kids need to learn that LLM’s can bullshit just like every other person and institution can bullshit, and the most important skill they can have is verification of information, no matter where it’s coming from. reply deadbabe 2 hours agorootparentIf everybody bullshits what is the point of not bullshitting yourself? reply lionkor 8 hours agoprevwell point the student at the little disclaimer that says that it may sometimes be incorrect! Of course they changed it multiple times, it used to say that you shouldnt take what it says at face value, now it says it may sometimes be inaccurate, soon it'll say \"learn more\" and link to a page about how their model is super accurate but potentially, one in a million, makes mistakes. Im worried about these students because they will be in power in a few decades. Its already a shitshow with people who didnt grow up as AI iPad toddlers. I feel like parents are failing here, more than anything else. You can't stop these companies from doing this if it drives up the stock price, equally you cant vote against it effectively because a majority of the voting population either doesn't care, doesn't know, or asks ChatGPT what to vote for. Of course Plato was right and the only way we can fix it is to have philosophers in charge, not demagogues. Good luck with that, maybe in another 2000 years we'll be smart enough to make that happen! reply adammarples 8 hours agoprev\"I am one person trying to work against a campaign of misinformation so vast that it fucking terrifies me. This kid is being set up for a life lived entirely inside the hall of mirrors.\" This is a little hyperbolic and instead maybe the kid can learn that chatgpt.com is not a reliable source. It even says at the bottom of the page \"chat gpt can make mistakes\". Lots of things are not reliable sources. Wikipedia is not a reliable source. Teachers are supposed to teach this, and teach cross referencing at the very least, not freak out. reply lionkor 8 hours agoparentNot sure if I would teach cross referencing to kids with parents who clearly dont care, for minimum wage ;) reply pnut 7 hours agoparentprevBuddhist philosophy suggests this entire reality is a hall of mirrors. reply pietmichal 8 hours agoprevThis sentiment reminds me of my teachers who complained that Wikipedia is not a reliable source to learn from. reply puttycat 8 hours agoparentWikipedia and LLMs are completely different entities that work in completely different ways. reply auggierose 8 hours agorootparentAnd yet, much of the same criticism applies to both. reply hyperhopper 8 hours agorootparentWikipedia is generally right and you can check it's sources. Also everything there was written by a human. LLMs are often wrong and you cannot check their sources. Also since it is generated you can trick it into spitting out falsehoods intentionally. They are not even close. reply auggierose 8 hours agorootparentYou can also check the sources of LLMs, just ask them for it, and then check that. An LLM is simply more flexible and more powerful than Wikipedia, and thus you have to be more cautious with regards to its results. \"Generally right\" is not the same as \"reliably right\", and therefore if you really need to rely on a fact for something important, I would trust neither Wikipedia nor LLMs. reply jacooper 8 hours agorootparentprev> LLMs are often wrong and you cannot check their sources. Depends on which LLM you are using, perplexity and copilot both cite their sources. reply ClumsyPilot 8 hours agoparentprevWikipedia is a marvel of the commons and did wonders for education. hostility to it from academia was surprising reply gitfan86 8 hours agoprevPerplexity links to sources. All he has to do was show the student the same search on perplexity. I do feel sorry for professors like this that cannot adapt to technology changes at the rate they are happening reply nebulous1 8 hours agoparentWhy would the teacher (this is almost certainly not third level education we're talking about here) use a different AI when they already have primary sources? Your suggestion makes no sense. reply gitfan86 7 hours agorootparentThe goal of teaching is to help students learn. By showing the student Perplexity the student would learn the value of sources and how he can get to those sources using a \"search\". It is unrealistic to think that there is any value in teaching students the Dewey Decimal system for finding sources. reply Semaphor 8 hours agoprevIf they only trust ChatGPT, why not just show them? From asking leading questions, to simply asking ChatGPT what the chances are of it making mistakes and hallucinating, there are tons of options. Considering this is a transcription of a deleted twitter post, my internet radar of \"yeah, that happened\" gives a lower chance this is true than your average ChatGPT answer. reply lionkor 8 hours agoparentI'm sure it happened, I've had conversations with younger people that went like that. They pick this up before they start learning critical thinking reply Semaphor 8 hours agorootparentAnd you didn’t get the idea to just show them, as I said, either? I’m not saying it’s impossible for little kids not to know about it, I’m saying it’s very straightforward to show them why that’s wrong by using the one tool they seem to believe. reply relaxing 7 hours agorootparentYou don’t work with kids as public school teacher. reply Semaphor 6 hours agorootparentPlease explain what’s wrong about my reasoning then? reply lionkor 8 hours agorootparentprevI fully agree with you, yeah. What helped for me is demonstrating that it will say whatever to seem smart, once thats demonstrated its easier to see that its not facts, its just convincing language. reply visarga 7 hours agoparentprev> they only trust ChatGPT And somehow this is an improvement over the previous status quo. ChatGPT is pretty well read and on important issues rarely says stupid things. Better than believing some influencer. reply allsummer 7 hours agoprev [–] This is a problem for this particular teacher (who sees their students surpassing them in understanding and using AI), but of course it is projected to be a problem for the student. No student is ever hurt by the introduction of a more advanced knowledge system. We heard similar laments decades ago, with: Students just believe the first 10 search results of Google. Those students are now the teachers of today, starting at the search bar. I'd go so far as saying (if version other than 3.5 was used) that ChatGPT was correct and has far more linguistics knowledge than this teacher ever will. \"Greek is actually a combination of four other languages\" is not an answer that ChatGPT will ever give, but something a teacher makes up to claim Ch*tGPT is a Nonsense Machine. ChatGPT: Greek has evolved in stages from Mycenaean Greek (Linear B script) through Classical Greek, Hellenistic (Koine) Greek, Byzantine Greek, and Modern Greek. It has been influenced by ancient Near Eastern languages, Latin, Turkish, Italian, and French. If there really is an epistemic crisis, then it already existed and ChatGPT merely reflects it, not caused or contributed to it. reply rchaud 5 hours agoparent [–] > who sees their students surpassing them in understanding and using AI This would be the equivalent of my saying that a basketball coach is obsolete because NBA 2K is available. ChatGPT output some garbage, and the student doesn't understand why or how it can be wrong. Presumably this is where a professor would attempt to help the student develop some critical thinking skills. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "A blog post from July 6, 2024, discusses a student's use of ChatGPT as a search engine, resulting in the student believing false information.",
      "The author highlights the difficulty of combating AI-generated misinformation, as each student may receive unique incorrect facts, complicating the correction process.",
      "The post reflects on the broader issue of misinformation's impact on education and was removed due to harassment."
    ],
    "commentSummary": [
      "Students today depend significantly on AI and the internet for information, contrasting with previous generations who conducted manual research in libraries.",
      "Educators are concerned that this reliance may hinder students' ability to critically assess information, resulting in essays that often lack depth.",
      "While AI tools are useful, they can also generate false information, sparking a debate on integrating these tools into education while fostering critical thinking skills."
    ],
    "points": 89,
    "commentCount": 121,
    "retryCount": 0,
    "time": 1720343169
  },
  {
    "id": 40894082,
    "title": "OpenAI was hacked year-old breach wasn't reported to the public",
    "originLink": "https://www.tomshardware.com/tech-industry/artificial-intelligence/openai-was-hacked-revealing-internal-secrets-and-raising-national-security-concerns-year-old-breach-wasnt-reported-to-the-public",
    "originBody": "Tech Industry Artificial Intelligence OpenAI was hacked, revealing internal secrets and raising national security concerns — year-old breach wasn't reported to the public News By Anton Shilov published 5 July 2024 Hackers have hacked away any perception of security around the latest AI code. Comments (29) (Image credit: OpenAI) A hacker breached OpenAI’s internal messaging systems early last year, stealing details of how OpenAI's technologies work from employees. Although the hacker did not access the systems housing key AI technologies, the incident raised significant security concerns within the company. Furthermore, it even raised concerns about the U.S. national security, reports the New York Times. The breach occurred in an online forum where employees discussed OpenAI's latest technologies. While OpenAI's systems, where the company keeps its training data, algorithms, results, and customer data, were not compromised, some sensitive information was exposed. In April 2023, OpenAI executives disclosed the incident to employees and the board but chose not to make it public. They reasoned that no customer or partner data was stolen and the hacker was likely an individual without government ties. But not everyone was happy with the decision. Leopold Aschenbrenner, a technical program manager at OpenAI, criticized the company's security measures, suggesting they were inadequate to prevent foreign adversaries from accessing sensitive information. He was later dismissed for leaking information, a move he claims was politically motivated. Despite Aschenbrenner's claims, OpenAI maintains that his dismissal was unrelated to his concerns about security. The company acknowledged his contributions but disagreed with his assessment of its security practices. The incident heightened fears about potential links to foreign adversaries, particularly China. However, OpenAI believes its current AI technologies do not pose a significant national security threat. Still, one could figure out that leaking them to Chinese specialists would help them advance their AI technologies faster. In response to the breach, OpenAI, just like other companies, has been enhancing its security measures. For example, OpenAI and others have added guardrails to prevent misuse of their AI applications. Also, OpenAI has established a Safety and Security Committee, including former NSA head Paul Nakasone, to address future risks. Other companies, including Meta, are making their AI designs open source to foster industry-wide improvements. However, this makes technologies available to American foes like China, too. Studies conducted by OpenAI, Anthropic, and others indicate that current AI systems are not more dangerous than search engines. Federal and state regulations are being considered to control the release of AI technologies and impose penalties for harmful outcomes. However, this looks more like a precaution as experts believe that the most serious risks from AI are still years away. Stay On the Cutting Edge: Get the Tom's Hardware Newsletter Get Tom's Hardware's best news and in-depth reviews, straight to your inbox. Contact me with news and offers from other Future brands Receive email from us on behalf of our trusted partners or sponsors By submitting your information you agree to the Terms & Conditions and Privacy Policy and are aged 16 or over. Meanwhile, Chinese AI researchers are quickly advancing, potentially surpassing their U.S. counterparts. This rapid progress has prompted calls for tighter controls on AI development to mitigate future risks. Anton Shilov Contributing Writer Anton Shilov is a contributing writer at Tom’s Hardware. Over the past couple of decades, he has covered everything from CPUs and GPUs to supercomputers and from modern process technologies and latest fab tools to high-tech industry trends. MORE ABOUT ARTIFICIAL INTELLIGENCE China's AI model glut is a 'significant waste of resources' due to scarce real-world applications for 100+ LLMs says Baidu CEO AI models that cost $1 billion to train are underway, $100 billion models coming — largest current models take 'only' $100 million to train: Anthropic CEO LATEST 2D quantum cooling system reaches temperatures colder than outer space by converting heat into electrical voltage SEE MORE LATEST ► SEE ALL COMMENTS (29) 29 Comments Comment from the forums CmdrShepard Still, one could figure out that leaking them to Chinese specialists would help them advance their AI technologies faster. If anyone actually bothered to read some research papers on AI, they would have known by now that many (if not most) authors of those research papers are Chinese nationals -- there's nothing to leak when they are the ones leading and publishing the AI research. Reply WINTERLORD Although I'm a proponent of privacy an such I really think these big AI companies should do there own security however I think there should be a second line of defence severity an that the gov should help ensure that these companies are secure either through fisa or somthin similiar running in the background Reply bit_user The article said: Also, OpenAI has established a Safety and Security Committee, including former NSA head Paul Nakasone, to address future risks. When I read this part, I had a major flashback to the security team in the mini-series Devs (2020). https://www.imdb.com/title/tt8134186/ OpenAI seems to have several parallels to the fictional company at the center of that series, other than the fact that they're dealing with quantum computing and not AI. Reply bit_user CmdrShepard said: If anyone actually bothered to read some research papers on AI, they would have known by now that many (if not most) authors of those research papers are Chinese nationals -- there's nothing to leak when they are the ones leading and publishing the AI research. But I think OpenAI isn't publishing its research, so we don't know how far ahead of academia they are. Reply jp7189 CmdrShepard said: If anyone actually bothered to read some research papers on AI, they would have known by now that many (if not most) authors of those research papers are Chinese nationals -- there's nothing to leak when they are the ones leading and publishing the AI research. Quantity does not equal quality or meaningful advances. I'm not necessarily being specific to Chinese research, but with all the attention AI is getting, it's harder to find the jewels in a sea of meaningless regurgitation. Find a good repo on github today and tomorrow it'll have countless forks. Reply zsydeepsky jp7189 said: Quantity does not equal quality or meaningful advances. I'm not necessarily being specific to Chinese research, but with all the attention AI is getting, it's harder to find the jewels in a sea of meaningless regurgitation. Find a good repo on github today and tomorrow it'll have countless forks. Quality-wise, the current best open-source LLM on huggingface leaderboard is Qwen2, which is from Chinese company Alibaba: https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard Reply bit_user zsydeepsky said: Quality-wise, the current best open-source LLM on huggingface leaderboard is Qwen2, which is from Chinese company Alibaba: https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard I don't know if they still do, but they at least used to have some R&D offices (including for AI) in the USA. Reply vanadiel007 Just look at the other AI article: China has been filing way more patents for AI than anybody else over the past 10 years. They are likely well ahead of other major players. In my opinion, rather than seeing them as the enemy, we should see them as an ally. We are fooling ourselves if we think we can sanction them into \"submission\" as most things we use on a daily basis are made in China. We have seen this during Covid, where the world came tumbling down on us because we could not produce the basic things we needed, and Covid prevented shipping them in like we usually do. We should not make the same mistake with AI, and end up with a toddler AI version while China has the adult AI version. And it's already happening with EV vehicles as we speak, where China can offer them for a quarter of the cost that the domestic producers are offering theirs for. We will be left behind technologically if we do not work with them. Reply bit_user vanadiel007 said: In my opinion, rather than seeing them as the enemy, we should see them as an ally. Just because you're nice to someone doesn't make them a friend. Turning a blind eye to IP theft and trade practices like dumping doesn't mean they'll allow you to do the same. It's just seen as a sign of weakness and makes you a target ripe for exploitation. vanadiel007 said: We are fooling ourselves if we think we can sanction them into \"submission\" That's not the only outcome. Every time there's an article about sanctions leaks, people seem all too ready to decry the sanctions as pointless and ineffective, but I doubt the sanctions would have so many detractors if they weren't actually having an effect. vanadiel007 said: most things we use on a daily basis are made in China. It didn't used to be that way and it needn't be, in the future. vanadiel007 said: We should not make the same mistake with AI, and end up with a toddler AI version while China has the adult AI version. It'll be another Tiktok situation, where they keep their crown jewels locked up tight and merely rent them to us - perhaps even in some impaired capacity. They won't be giving them away, or even selling them at a price worth paying. vanadiel007 said: And it's already happening with EV vehicles as we speak, where China can offer them for a quarter of the cost that the domestic producers are offering theirs for. Because dumping and they sewed up the rare earth metals supply & processing chain. Reply jp7189 zsydeepsky said: Quality-wise, the current best open-source LLM on huggingface leaderboard is Qwen2, which is from Chinese company Alibaba: https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard Yes, indeed it is. I was disputing the \"quantity\" part of the original argument without disputing the country of origin part. Reply VIEW ALL 29 COMMENTS Show more comments",
    "commentLink": "https://news.ycombinator.com/item?id=40894082",
    "commentBody": "OpenAI was hacked year-old breach wasn't reported to the public (tomshardware.com)84 points by lightlyused 19 hours agohidepastfavorite26 comments bastard_op 18 hours agoI've worked with/for a lot of org over the past few decades, and personal experience proves there are a _lot_ of incidents that go unreported. The usual is that if there's no logs saying something bad actually happened, there's certainly nothing to say that it did, even though some terribly guessable credentials were used for ages on something publicly exposed. I know, they know, but told in no uncertain terms to drop it. Nothing to see here, move along. Work to be done, money to be made. reply ChrisArchitect 18 hours agoprev[dupe] Actual article: https://www.nytimes.com/2024/07/04/technology/openai-hack.ht... More discussion: https://news.ycombinator.com/item?id=40887619 reply CamperBob2 19 hours agoprevIt's hard enough to report issues to OpenAI. Not surprising that information coming out of the company is equally constrained. Right now my ChatGPT4 history is full of chats I didn't create, on subjects ranging from corporate governance to Roblox scripting to somebody's math homework. It will be only a matter of time before this bug causes them to leak sensitive personal data. I spent 10 minutes looking for a way to report it, but they have successfully insulated themselves from any contact with their (paying) customers. Pretty annoying, and not something you expect from a supposedly security-savvy company... although that expectation is certainly changing. reply righthand 19 hours agoparentSerious question: What gave you the impression the company is security savvy? reply moralestapia 19 hours agorootparentNot OP but probably all their marketing bs about AI safety and how they're saving the world by not destroying it (th-thanks ...). They can't even do basic auth properly so ... reply gunapologist99 18 hours agorootparentIt's good to see that we are safely protected from the other side of the political aisle. reply talldayo 18 hours agorootparentWith business partners like these, who needs competitors? reply paulryanrogers 17 hours agorootparentprevDon't follow. Can you explain the political connection? reply righthand 19 hours agorootparentprevI guessed that but also AI safety doesn’t seem like a security promise to me so I thought I’d clarify. reply chillingeffect 18 hours agorootparentprevbtw fwiw my passwd in chatgpt is all lowercase lol reply Algemarin 18 hours agoparentprev> It's hard enough to report issues to OpenAI. Not at all. OpenAI follows basic accepted standards for security reporting. This is like complaining that you can't find if a website doesn't want specific directories crawled because you don't know about the existence of a robots.txt. Specifically, OpenAI has a security.txt [0], which is: > an accepted standard for website security information that allows security researchers to report security vulnerabilities easily [1] Whenever attempting to find where to report a security issue, the easiest thing to do is always check if the website has a security.txt file. [0] https://openai.com/security.txt [1] https://en.wikipedia.org/wiki/Security.txt Here's their security.txt: -----BEGIN PGP SIGNED MESSAGE----- Hash: SHA512 # # .d88888b. # .8P\" \"9bd888b. # .8P .d8P\" `\"988. # .8888 .d8P\" , 98. # .8P\" 88 8\" .d98b. 88 # .8P 88 8 .d8P\" \"98b. 88 # 88 88 8P\" `\"8b. \"98. # 88. 88 8 8\"8b. 88 # 88 \"98.8 8 88 \"88 # `8b. \"98., .d8 88 88 # 88 \"98b. .d8P\" 8 88 d8\" # 88 \"98bP\" .8 88 .d8\" # \"8b ` .d8P\" 8888\" # \"88b., .d8P\" d8\" # \"9888P98b. .d8\" # \"988888P\" # Contact: https://bugcrowd.com/openai Acknowledgments: https://bugcrowd.com/openai/hall-of-fame Policy: https://openai.com/policies/coordinated-vulnerability-disclosure-policy Hiring: https://openai.com/careers/search?c=security Canonical: https://openai.com/.well-known/security.txt Encryption: https://cdn.openai.com/security/disclosure.asc.pub # You may also email us directly. Contact: mailto:disclosure@openai.com -----BEGIN PGP SIGNATURE----- iHUEARYKAB0WIQQ5fYPd6Hi19rZDZ+kKj1HZ7OnINQUCZbiKWgAKCRAKj1HZ7OnI NS9+AQCTx4vlrCp+Urd1fa/lAQ3dcV8VNHOxA4JnxD0TH2nxwQEAuqoxenxPZWeD +IsSikn4em/LEheOeAakGDzZedcu1QE= =rMRk -----END PGP SIGNATURE----- reply CamperBob2 18 hours agorootparentLOL. ... that was a joke, right? So only people who have heard of the security.txt convention are expected to find this information easily when they need to report a bug? reply ballenf 17 hours agorootparentThis came up with my first search \"openai security\": https://trust.openai.com At the bottom is a link to report an issue. Seems like there are multiple ways to report issues. And they come with the potential for bug bounties. And so many companies don't follow the security.txt standard that it puts OpenAI well ahead of most companies. reply CamperBob2 17 hours agorootparentThis came up with my first search \"openai security\": https://trust.openai.com At the bottom is a link to report an issue. Did you click the link? What happened when you did? reply boulos 19 hours agoparentprevHuh? I just went to OpenAI.com and there is a little \"Security\" link in the bottom pile that points to https://openai.com/security-and-privacy/ . Under \"Reporting security issues\" it points you to a bug bounty page: https://bugcrowd.com/openai with a bunch of explanations. I'm guessing if you also just send an email to security@openai.com it'll go to someone. Using Bugcrowd just seems like a nice way to also run a bug bounty as part of their normal intake. reply upwardbound 19 hours agorootparentOpenAI seems to have, unfortunately, outsourced the triaging of bug bounty reports to people who don't seem to understand security well enough to recognize issues. As an example, I've been trying to get OpenAI to fix the fact that \"eval()\" is used incorrectly in one of their Cookbooks in a place where the correct function would be \"json.loads()\". https://cookbook.openai.com/examples/how_to_call_functions_w... https://news.ycombinator.com/item?id=40474451#40474452 The bug bounty report was closed with a message saying: Upon reviewing your report and consulting with the OpenAI team, we have determined that this feature is operating as intended. This means it does not constitute a valid sandbox escape. The Code Interpreter environment is securely sandboxed to support code writing and execution, including shell operations. Any code execution within this environment falls outside the scope of our program ... As you have not demonstrated a valid sandbox escape or RCE, we're closing this submission as Not Applicable. This shows a fundamental misunderstanding of basic coding, as the eval() I pointed them to is completely unrelated to the Code Interpreter environment. So, the report is incorrectly considered \"Not Applicable\", without any real further ways to try to get them to fix it. I tried contacting the Cookbook authors directly, but never heard back. reply CamperBob2 19 hours agorootparentprevI saw that. I can't be arsed to create an account on a third party 'bug bounty' site, or to waste time guessing email addresses, or to download a security.txt file I've never heard of. Sorry. Their loss, not mine. If they make it hard for me to help them, they can't be too surprised when I give up trying. reply skywhopper 19 hours agoparentprevWho has claimed OpenAI is security-savvy? reply ilrwbwrkhv 19 hours agoprevYa I hope people are not putting any sensitive information when using Chat GPT. Anything that can get stolen will get stolen. Just a matter of when not if. On device LLMs with no network transmissions are the only way to keep things safe if you really care. reply righthand 19 hours agoparent> Ya I hope people are not putting any sensitive information when using Chat GPT. The ship has sailed, OpenAI wants you to put everything in their system. It makes them more valuable. They know there is no repercussions because their base will blindly advocate for them regardless under the guise of “the best llm”. reply cqqxo4zV46cp 19 hours agoprevPost headline has been editorialised yet still terrible clickbait. > OpenAI’s internal messaging systems early last year, stealing details of how OpenAI's technologies work from employees. Although the hacker did not access the systems housing key AI technologies, […] Enough said. It’s completely normal to not disclose a breach if there’s no proof or great likelihood that customers were implicated. A poorly written article regurgitating the NYT story with uninformed alarmist shitty podcast tier ‘analysis’. Jog on. reply doe_eyes 18 hours agoparent> It’s completely normal to not disclose a breach if there’s no proof or great likelihood that customers were implicated. A bit more complicated than that for public companies. But OpenAI is private, so yeah, they most likely don't have to. It's still an interesting scoop for a journalist, though. reply skywhopper 19 hours agoparentprevSounds like they got access to email and Slack; that’s the gateway to a lot of other things. Fact is, OpenAI was booming at the time of this hack and they had every incentive to play down the severity internally. The hackers may not have gotten access to the “systems housing key technologies”, ie no SSH access to the production VMs (although I’m not sure I would trust that OpenAI’s auditing of such access was foolproof) but that doesn’t mean they couldn’t have done a lot of other damage, gathered all sorts of source code and secrets, or put a backdoor in somewhere. All in all, given the claims they are making and the level of trust they demand from their customers, they ought to have been far more open at the time. reply tux3 19 hours agoprevAs someome who hoped that OpenAI would be consistently candid, this certainly comes as a disappointment. If the internal culture is to keep problems under wraps to maintain appearances, this seems like it might backfire at some point. reply TaylorAlexander 18 hours agoparentDo not waste your energy thinking that companies like this will be “consistently candid”. That’s not what they’re here for, and it’s clear from other events in their history that they have no interest in this. reply uyzstvqs 19 hours agoprev [–] > OpenAI's systems, where the company keeps its training data, algorithms, results, and customer data, were not compromised Article just rambles about some unnamed uninformed AI-phobes being concerned about US national security in relation to China because of some unknown OpenAI internal information that might have leaked. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Hackers breached OpenAI's internal messaging systems early last year, exposing sensitive information but not accessing key AI systems, raising security and national security concerns.",
      "OpenAI disclosed the incident internally in April 2023, stating no customer data was stolen and the hacker likely had no government ties; however, the breach has led to heightened fears about foreign adversaries, particularly China, advancing their AI technologies.",
      "OpenAI has since improved its security measures, established a Safety and Security Committee, and the incident has prompted discussions on federal and state regulations to control AI technology release."
    ],
    "commentSummary": [
      "OpenAI experienced an unreported hack last year, leading to user concerns about transparency and security practices.",
      "The breach involved access to internal messaging systems but did not compromise key AI technologies.",
      "Users discussed difficulties in reporting bugs and frustrations with OpenAI's standard security reporting practices."
    ],
    "points": 84,
    "commentCount": 26,
    "retryCount": 0,
    "time": 1720308293
  },
  {
    "id": 40893749,
    "title": "What causes migraines? Study of 'brain blackout' offers clues",
    "originLink": "https://www.nature.com/articles/d41586-024-02222-x",
    "originBody": "NEWS 05 July 2024 What causes migraines? Study of ‘brain blackout’ offers clues The blinding headaches are poorly understood — a mouse study suggests that the content of spinal fluid is a trigger for pain. By Miryam Naddaf Twitter Facebook Email About one-third of people who suffer from migraines experience a phenomenon known as aura before the headache.Credit: Tunatura/Getty For a billion people worldwide, the symptoms can be debilitating: throbbing head pain, nausea, blurred vision and fatigue that can last for days. But how brain activity triggers these severest of headaches — migraines — has long puzzled scientists. A study1 in mice, published in Science on 4 July, now offers clues about the neurological events that spark migraines. It suggests that a brief brain ‘blackout’ — when neuronal activity shuts down — temporarily changes the content of the cerebrospinal fluid, the clear liquid that surrounds the brain and spinal cord. This altered fluid, researchers suggest, travels through a previously unknown gap in anatomy to nerves in the skull where it activates pain and inflammatory receptors, causing headaches. “This work is a shift in how we think the headaches originate,” says Gregory Dussor, a neuroscientist at the University of Texas at Dallas in Richardson. “A headache might just be a general warning sign for lots of things happening inside the brain that aren’t normal.” “Migraine is actually protective in that way. The pain is protective because it’s telling the person to rest and recover and sleep,” says study co-author Maiken Nedergaard, a neuroscientist at the University of Copenhagen. No-pain brain The brain itself has no pain receptors; the sensation of headaches comes from areas outside the brain that are in the peripheral nervous system. But how the brain, which is not directly linked to the peripheral nervous system, triggers nerves to cause headaches is poorly understood, making them difficult to treat. Scientists working with a mouse model of a particular type of headache, called aural migraine, set out to explore this. One-third of migraine sufferers experience a phase before their headache known as aura that has symptoms such as nausea, vomiting, sensitivity to light and numbness. It can last five minutes to an hour. During aura, the brain experiences a blackout called a cortical spreading depression (CSD), when neuronal activity shuts down for a short time. Studies of migraines have suggested that headaches happens when molecules in the cerebrospinal fluid drain from the brain and activate nerves in the meninges, the layers that protect the brain and spinal cord. Nedergaard’s team wanted to explore whether there are similar leaks in the cerebrospinal fluid that activate the trigeminal nerve, which runs through the face and skull. The nerve branches join at the trigeminal ganglia at the base of the skull. This is a hub for relaying sensory information between the face and jaw to the brain, and contains receptors for pain and inflammatory proteins. Nerve bundle The authors bred mice that experienced CSDs and analysed the movement and content of their cerebrospinal fluid. During a CSD, they found that the concentrations of some proteins in the fluid dropped to less than half their usual levels. The levels of other proteins more than doubled, including that of the pain-transmitting protein CGRP, which is one target of migraine medications. The researchers also discovered a previously unknown gap in the protective layers around the trigeminal ganglion, which allows cerebrospinal fluid to flood into these nerve cells. So they tested whether spinal fluids with different protein concentrations activated the trigeminal nerves in control mice. Fluid collected shortly after a CSD increased activity of trigeminal nerve cells — indicating that headaches could be triggered from pain signals sent from these activated cells. But the fluid collected 2.5 hours after CSDs didn’t have the same effect. “Whatever is released in the cerebrospinal fluid is degraded. So, it’s a short-lasting phenomenon,” says Nedergaard. “It really shows this nice potential interaction between how something changing in the brain could impact the periphery. There can be a crosstalk between these two components of the nervous system, and we should be more aware of it,” says Philip Holland, a neuroscientist at King’s College London. Dussor suggests that future studies should explore why the proteins in spinal fluid that hit the trigeminal ganglion result in headaches and no other type of pain. “This is going to raise a whole lot of interesting questions in the field, and it’s going to be probably the source of a lot of new research projects.” doi: https://doi.org/10.1038/d41586-024-02222-x References Rasmussen, M. K, et al. Science 385, 80–85 (2024). Google Scholar Download references Reprints and permissions Latest on: Brain Neuroscience Jobs Staff Scientist in Computational Metabolomics A position as a Staff scientist in Computational Metabolomics is available at the SciLifeLab Metabolomics Platform. Umeå (Kommun), Västerbotten (SE) Umeå University (KBC) Group Leader in Functional Genomics APPLICATION CLOSING DATE: August 15th, 2024 Human Technopole (HT) is an interdisciplinary life science research institute, created and supported by... Milan (IT) Human Technopole Faculty Positions & Postdocs at Institute of Physics (IOP), Chinese Academy of Sciences IOP is the leading research institute in China in condensed matter physics and related fields. Through the steadfast efforts of generations of scie... Beijing, China Institute of Physics (IOP), Chinese Academy of Sciences (CAS) Faculty and Research Positions, Postdoctoral Recruitment Jointly sponsored by the Hangzhou Municipal People's Government and the University of Chinese Academy of Sciences. Hangzhou, Zhejiang, China Hangzhou Institute of Advanced Study, UCAS Postdoctoral Research Scientist: DNA Replication and Repair in Haematopoietic Stem Cells An exciting opportunity has arisen for a highly motivated Postdoctoral Research Scientist to join Professor Chapman’s Group, to investigate how DNA... Oxford, Oxfordshire University of Oxford, Radcliffe Department of Medicine",
    "commentLink": "https://news.ycombinator.com/item?id=40893749",
    "commentBody": "What causes migraines? Study of 'brain blackout' offers clues (nature.com)84 points by kungfudoi 20 hours agohidepastfavorite72 comments burritosnob 15 hours agoI suffered from migraines for over a decade. They were infrequent at first but, at the peak, I was getting 1-2 a week and felt like I was just \"surviving\" in life. I saw countless doctors and specialty clinics, none of which offered long term relief other than drugs. For me, Rizatriptan helped take the edge off pain but I always had more migraines than pills could safely relieve in any given month. In the end, most months I had to choose whether I would get relief in the moment or save the 1-2 remaining pills for the following week(s) before a refill. I had every blood / allergy test available and none ever showed any issues. I thought there may be a correlation to gluten but there were times where gluten (bread) helped with the nausea side effects. Ultimately, I gave up gluten, coffee and sugar all at once. Within weeks I felt better and suffered no migraines for over a year until I decided to test the waters with a flour tortilla. Within a few hours I was bed ridden with a migraine for 2-3 days. I have worked sugar back in to my diet in moderation but gluten and coffee are still out. I have only had that one migraine in 3+ years. Anyone suffering, the absolute best advice I ever got was to keep a food / pain log. Do it every day no matter what. It may take a week, months or a year but it will uncover something that will help reduce frequency. Also, listen to your body. I had numerous doctors tell me I didn't have any food allergies and to focus on other areas for relief. Every single one was wrong and I could have had a cure years previous. reply cfn 10 hours agoparentMy migraines stopped when I went carnivore which meant no gluten and sugar as well (though I kept the coffee). I'm now in a more keto diet but still no gluten at all and I've only had a couple of light migraines in a year or so (used have them a monthly or more frequently). It is interesting to know your case as it is a smaller dietary change than mine. reply jraph 12 hours agoparentprevI've heard coffee can relieve migraines, including from a friend that has migraines and doesn't like coffee, and other people with migraines and these people don't know each others. I've not read on the topic, at this point I wouldn't rule out the placebo effect, and it seems in your case coffee wasn't helping you, but it would be interesting to know if coffee caused migraines in you without the gluten. Now, going on without coffee is nice too! reply cfn 10 hours agorootparentAt one point, if I drunk a coffee just before a migraine started it would go away. Nothing scientific here but it happenned too many times (both had the coffee and didn't) to establish the result for me. reply lormayna 9 hours agorootparentSame for me: I can recognize the migraine coming by some symptoms, then I usually drink coffee and migraine will usually don't show up. reply TN1ck 10 hours agoparentprevSince I dropped caffeine, my migraines also became much less frequent and when I have some caffeine, I am at risk for getting one. It seems closely related to sleep for me and caffeine (even in moderation) just greatly affects my quality of sleep. reply balder1991 4 hours agorootparentSame for me. As a Brazilian, I’ve always drank coffee without much care, but eventually I started limiting it to mornings. Then I noticed if I ever forget to drink coffee (or had decaf coffee instead) I’d have a headache in the afternoon. My solution was to remove caffeine completely, now I only drink decaf coffee and this particular problem disappeared after a few days. Now about auras, it’s hard to say if there was any effect as they’re more rare for me. This thread is the first time I’ve read people say they drank coffee after the symptoms show up and it can block the headache that comes later on. reply Luker88 9 hours agoparentprevI just started noticing this and I am in the exact same spot, no allergies, lots of migraines. Thank you for posting this 'cause I though I was making things up by noticing things when all tests were negative. edit: though it seems to me that coffe(+iburprofen when things are bad) helps. I started connecting it to food when I noticed I started getting pain on my lower back that slowly creeps up on the course of half a day or so and becomes a migraine. When it starts stopping and I massage my neck down to half my back I get tons of relief when I press on the right nerve/muscle/whatever. reply 29athrowaway 14 hours agoparentprevNot all flour or sugar are made equal. I wonder what happens if you try flour or sugar from places where some questionable herbicides are banned. reply eastbound 10 hours agorootparentWhen I read stories like his, I always wonder what we’re doing with our wheat that so many people become completely wheat-intolerant in just 15 years. It is clearly more than just fashion, clearly people can diagnose it themselves and it happened in the anglo-saxon world first. reply tardy_one 8 hours agorootparentI'm not really that convinced that it isn't mostly social.. There were more intolerant than diagnosed until the 90s and that has now just flipped with the social overcompensation. It's really very easy to buy in to the dominant social narrative as an explanation for one of the many undiagnosable health incidents we experience in a lifetime, and the placebo effect confirms whatever is socially helpful. Most of these social changes now originate or become big from the US first because it dominates international media. reply victorbjorklund 7 hours agorootparentprevOr do people in a rich society like ours just have more opportunity to notice problems like this vs if you live in a society where life is more hard? It's like it seems we have more people today identifying as gay vs 100 years ago. Is it really more gay people today or do we just have a more tolerant socity that enables more people to identify as gay? reply takklz 15 hours agoprevI have gotten migraines (a visual aura followed by a horrible headache) on and off for years. Some years I’d have a couple, some years I’d have none at all. I have always tried to trace it back to a common theme. For the longest time I thought that common theme was caffeine.. to the point that I quit caffeine for like a year. Unfortunately, I still ended up with a migraine! One day I realized something, maybe my migraines had nothing to do with caffeine but everything to do with electrolyte balance. I recalled the many days I’ve had migraines and besides caffeine, the other common theme was that I had drank a significant amount of water. Sparkling water especially, I can pretty much just guzzle all day because I never get bored or sick of it. Sense then, I’ve made sure that if I drink a lot of water I also throw in some LMNT or similar electrolyte powder in there. I’ve been able to enjoy coffee again without the migraine. YMMV! reply deckplecksetter 10 hours agoparentIt was similar for me. I'd get migraines a few times a year. I couldn't find any food correlation, but it seemed to often happen the day after doing strenuous activity, sweating a lot and drinking a lot of water. I started to suspect it was related to lack of electrolytes, so I began taking electrolyte solution after running or any other sweaty activity. In the few years since then, the migraines have been much less frequent, and about half as intense. reply yr1337 17 hours agoprevIt's cool to see some progress on aural migraines. In the past I tried zolmitriptans for it but they don't do much, same with milder pain killers. My migraines started at 20, two decades ago. I remember the first one like it was yesterday. I was studying CS and was coding on a dark-mode CRT monitor with a big fat window reflection on the screen. It always happen the same way, with the characteristic jiggles, some kind of colorful, moving patterns of light that start in the center of my vision, slowly expanding to my entire field of view, making me temporarily blind. Then 30 mns after the onset of that, gone. 30 more minutes and the headache + strong nausea start, and immense fatigue. At that point I'm out of commission for a good 12h. Meaning I better be close to my bed, in total darkness. It really sucks. I used to have them every 3 months on the dot, like on a timer. As I'm getting older I'm having less and less of them, going about a year without one. And they are a lot milder now. I have a lot less of the jiggles, no nausea and the headache is not so bad. I'm still fatigued but usually recover the same day. I consider myself lucky. It seems to run in my family. I've tried finding a trigger for the migraines but I have no clue. I used to associate them with bright lights as there is usually a bright light present when they happen, but it needs some other element. Stress and fatigue appear to be factors, as are tanins contained in red wine and beer. But sometimes none of these elements were present. I have a personal non-scientific theory that serotonin levels play a role. High levels like I think I had when I was younger tend to trigger migraines. As I'm getting older and grumpier, I probably have low serotonin and less migraines... reply halfmatthalfcat 17 hours agoparentSame, with aura, but without bad headache symptoms beyond a 3/10 on the pain scale. I use to have them monthly, almost on the dot. Unrelated to them, I started taking Magnesium supplements and now rarely ever have aura migraines. Kinda crazy how well it worked for me. Also could never really figure out the “precipitating factors” but bright light was almost always the direct trigger. reply jballer 14 hours agoparentprevThanks for sharing. I have the same “bright lights” hypothesis. If I see something bright I close my eyes until it feels like it’s passed. Seems to work, but who knows until I get my next one. I’m also realizing in my late-30s that I have at least some form of dyslexia - I used to think Ambien made me hallucinate letters dancing on the page, but it’s my eye muscles and happens most when I’m tired. reply majiy 10 hours agoprevStrange how this has so many different kind of triggers for different people. Makes me wonder if this really one phenomenon, or multiple with similar symptoms. For me, it took years to identify two triggers: Relieve from stress, and overly exhausting sports or sauna. I used to have a stressful job, and regular intensive training on sunday mornings, and would very often get migraine at exactly 4pm. Since changing the job and doing moderate sports, it became much more infrequent. Also, I have never had an optical aura. But I have observed yawning, and an increased sensitivity in my nasal passages (basically I feel every breath much more detailed, but not painful) as forewarnings. reply elric 6 hours agoparent\"Triggers\" are a red herring, there's way too much focus on them. The trigger is merely the final straw, the real problem is everything that came before. reply balder1991 3 hours agoparentprevMany people say one cause may be electrolytes imbalance, which would explain the migraines after intensive sports. reply belZaah 8 hours agoprevI had headaches diagnosed as migraines since I was a kid, for 40+ years. It got to a point where I had scale 9 headaches for most days of some weeks and it was a tough going. Lots of doctors, some pain relief, but ultimately, no help. Then, in desperation, I tried drinking water. Headaches had been accompanied by the feeling of hunger/thirst and I thought to drink until it went away. I brought home a six-liter pack of mineral water and drank it all in 24 hours. And no more headaches. Nowadays I always have a bottle of water at hand consuming at least 1.5 liters a day. Nights are punctuated by visits to the loo, but I can live with that. Reading other comments, I suppose migraines are a systemic issue, where something in the body is out of balance which manifests itself as an headache. And the medical science is really not very good with systemic issues: heart surgery has made great strides, but where nervous system meets the digestive system meets the limbic system etc, they struggle. Also, I think there’s an interesting aspect of level of detail of sensitivity here. Turns out my bouts of hunger were actually thirst. Since then, endurance sports have introduced me to an even wider gamut of ways the body might ask for different things. So maybe it’s about our ability to recognize and therefore correct imbalances in the body? reply kimixa 17 hours agoprevI semi-regularly get migraines, maybe a handful a year. Nearly every time it's associated with a night of insomnia - I do everything the same, no coffee or alcohol or anything, no late work or screens. The same bedtime routine, I just.... don't sleep. I'm not sure how to draw the line of cause/effect between the two. Is the lack of sleep causing a migraine the next morning? Or is the insomnia some symptom of whatever then becomes the symptom of a migraine. Or even if such a line exists, I think the association is strong enough the two are related, but I know memory and the brain making things fit patterns can be misleading. reply aliasxneo 16 hours agoparentI, of course, cannot provide medical advice, but I have had chronic migraines since I was a teenager. Over the years, I've identified several triggers and tend to categorize them; lack of sleep is common. Of course, a complete night of restlessness is fraught with many problems, but I can almost always guarantee that a migraine will be one of them. reply ianburrell 16 hours agoparentprevMigraines can have symptoms in advance of the headache. It is called the prodrome and insomnia is one of the possible symptoms. The insomnia probably doesn't help, but it is likely an advance symptom instead of the cause. reply instagib 9 hours agoprevMy favorite annoying quote from a book is once you have migraines figured out, they change. I had one solution that worked, it stopped, switched, rinse and repeat. I get lightning shocks in my head at times, it’s not great. Long list of tried and failed medications. Almost everything on the list 0. Seems different for everyone. X drug works for some then causes memory loss for others. If you have over 15 migraines per month, you can get Botox every 3 months. It lasts 1-3 months for me with an average of 2. Lasts meaning 1-3 per week. Not lasts means near daily migraines. $2-3k without insurance and it’s about 40 painful shots. Supposedly 2/3 people refuse Botox due to side affects (droopy eye 1/50 chance) or injection pain. Migraines are worse than 40 shots. The new cgrp migraine prevention versions are available and highly advertised. You need to fail 2-3 classes of drugs to get approval depending on insurance and the syringes are currently hard to get but they last one month with a self subcutaneous injection. Herbs: haven’t had much luck with anything except menthol variants and smelling/rubbing it on temples which is backed by a few papers. The people I talk to usually say they used to get migraines then got divorced and they went away. Changed jobs to be less stressful led to less migraines. Getting sick, weather changes, being overheated, pressure changes barometric/altitude, and not sleeping well causes more migraines. Food triggers led me nowhere. 0. https://www.drugs.com/condition/migraine.html?page_all=1 reply elric 6 hours agoparent> Getting sick, weather changes, being overheated, pressure changes barometric/altitude, and not sleeping well causes more migraines. Food triggers led me nowhere. My neurologist told me that living like a hermit was the best migraine preventative. Simply never leave your cave, don't change your routine ever, don't go places with different weather, etc. Of course that wasn't meant to be taken literally, but I've definitely noticed that any significant change to my routine can result in migraine. reply lormayna 9 hours agoprevIn the past I had very frequent migraines, I suspect caused by stress and irregular sleep (I worked for years in-call and making lot of night works). Migraines will usually come in the weekend and take sometimes more than 24 hours to end. They started with irritability, light, noise and smell sensitivity, then visual aura and then migraine. I had a consultation with a neurologist and he suggested, before starting the drug therapy, to try agopunture with exactly those words:\"For somebody is working and if it's not, you just got some needles in your body and you are not getting hurt\". I was very skeptical, but it works: after couple of sessions my migraine are almost gone and very less intense. When migraines increases intensity and frequency, I immediately book a new agopunture session and I am fine for 12/18 months. I suggest anybody that is struggling with migraine to try agopunture, for me was a game changer. reply shartshooter 11 hours agoprevI (late 30’s male) grew up with migraine and ultimately discovered that they were triggered by caffeine mixed with dehydration and/or chocolate. Having discovered this I’ve been able to control them completely, but realize that not everyone’s triggers are the same. I carry sumatriptan just in case but have never felt as though it’s made a difference. I’m always hopeful for more solutions to migraines for those who suffer reply deweller 5 hours agoparentI was getting migraines regularly most Mondays until I finally realized that it was tied to chocolate. I ate chocolate on weekends. I stopped eating chocolate and they went away. Now I never get migraines unless I have chocolate. It is likely the caffeine. I don't consume any other caffeine. reply lrasinen 11 hours agoparentprevI have a massive chocolate craving in the recovery phase, although I suspect it's just the body going after high-energy food. reply JohnKemeny 10 hours agoparentprevNote that there are theories that migraines start long before an actual attack, and that with this migraine comes certain cravings. I'm other words, one might crave chocolate and coffee due to the migraine, and only later get symptoms, thereby causing us to believe that there's a causation. Post hoc ergo propter hoc. reply balder1991 3 hours agorootparentI’ve read enough people say they went coffee free and it helped that I don’t think the craving thing holds up, but I can see it happening in a different symptom: some people say the trigger is like bright lights, but I actually think the upcoming headache may be the one causing the light sensitivity, which would lead us to “remember” the bright lights. reply varsock 6 hours agoprevI have pain/food logs that uncovered nothing about my migranes. I started keeping a sleep log and discovered that on days I snooze my alarm and go back to sleep I have migranes. It takes some effort, but not \"snoozing\" my alarms (even 10 minutes) and getting up immediately has cured me of my migranes. I share this in the event someone suffers this unknowingly. reply elric 13 hours agoprevLet me just copy/paste what I commented the last time this article was posted: > The pain is protective because it’s telling the person to rest and recover and sleep Try telling that to people who have pain so intense they end up in a cycle of vomiting and sleeping for a period of 72 hours. > the brain experiences a blackout called a cortical spreading depression (CSD), when neuronal activity shuts down for a short time Blackout does not sound like a good description. CSD travels across the cortex in a \"wave\", where the part currently affected by the wave is depolarized. Nothing is shutting down. There are a lot of people who get migraine headaches without aura, and there are a lot of people who get migraine auras without headaches. The article doesn't seem to take that into account at all, and I can't seem to find a link to the research? The DOI link just points at the same short article? Edit: I don't mean to sound like a debbie downer. I'm happy with any and all migraine research, and this seems to look at things from a different perspective, which might lead to new discoveries. reply Buttons840 11 hours agoparent> a cycle of vomiting and sleeping for a period of 72 hours I've been in such cycles, though mine usually last for about 6 to 12 hours only. That's long enough. I always figured the stress of my migraine caused my body to say, \"okay, get all the food out, we shouldn't be digesting anything at a time like this\". I've vomited 20 to 30 times in a night before. And the weird part is, I actually have good memories of these times. The exhaustion and sleep that follow are amazing, and I feel especially good the next day. I've read sometimes migraines have a serotonin effect, maybe there's some chemical reason my brain decides in retrospect that it was a good experience. I recently saw a Veritasium video about how the ending of an experience shapes our memory of it [0], maybe the relief of the migraine finally subsiding is a good ending and shapes my memory of it. [0]: https://www.youtube.com/watch?v=v4r71kEdYME reply SloopJon 17 hours agoprevReading this from the perspective of cluster headache, which gets some hand-me-down treatments from migraine, I just spent half an hour reading about CGRP blockers that have been approved in the last five or six years, including Aimovig and Ajovy for migraine, and Emgality for clusters. It's been several years since my last cycle (my longest break yet), but it's heartening to know that research continues, leading to more treatment options. reply zoky 5 hours agoparentHello, fellow clusterhead! I trust you’ve tried the usual treatments—Verapamil as a preventative, and 100% oxygen 10-15 L/min combined with injected Sumatriptan as an an abortive. I’ve gone from reducing my cluster periods from once a year like clockwork to no more than once every 2-3 years, and the periods have been shorter as well. Hope you are well. reply rndz 7 hours agoprevHave migraines from childhood(semi regularly, few per month). There have been a period where it stopped completely for several months. Had medication prescribed, but did not like side effects much (getting high, blood pressure rising, etc), and since ibuprofen usually helps to mitigate it, I stuck with it(I only get a migraine like once a year that is unbearable, to the point of crying). Few months ago, went on again to \"researching\" (listening any podcast episodes I could find on migraines) this topic a bit, and what I heard that I missed before - there is a considerable percentage of people who get migraines from fermented foods(or at least a connection they have made). So eliminating such food lowered my migraine count quite a bit (but I still do get them - and I think a handful of them might be stress related). reply pictureofabear 18 hours agoprevI've had migraines twice in my life, both preceded by chronic lack of sleep (thesis writing!). Auras preceded painful headaches in both cases. I cannot imagine having chronic migraines. Perhaps research like this will prompt us not search for the cure to migraines per se but the underlying cause. reply neom 16 hours agoparentrando anecdotal point: I've never had a migraine in my life, I think I can count my headaches not caused by alcohol on one hand. I also have DEC2 mutation so i'm a \"short sleeper\". I suppose one could infer if someone doesn't suffer from little sleep in all aspects generally, it would track they would never have had a migraine if it was related to sleep? reply DenisM 4 hours agorootparentHow did you find out about DEC2? Is there a test? reply neom 2 hours agorootparentYes but I had to work quite hard to get it. You have to find a researcher interested in it. I did it because people wouldn't shut up about being concerned about my health due to my sleep, so I googled, found out it was a thing, and then started hunting down a researcher interested. I'm afraid who I worked with is no longer looking at it, so I can't pass you to them, sorry about that. :\\ reply jellyfishbeaver 6 hours agoprevI've only had probably 3 or so migraines in my life, I sympathize with people who get them often. I can't imagine that level of pain regularly. One strange thing about migraines for me is that it usually gets worse and worse until eventually I throw up. Immediately after barfing, the migraine loosens its grip and I feel normal again. Always found it super strange. reply strictnein 15 hours agoprevI get migraines from two situations: 1. Stress. Significant stress can lead to migraines and I can feel them slowly start to build in the upper front of my brain above my left eye (or at least that's what it feels like). This can be stress from work or from kids or whatever. 2. Blips. I don't know how to describe these really. It's like my brain doesn't quite know how to process what's going on. An example: I gave my kids some ice cream for a snack one day after asking my wife if I should and her saying yes. 5 minutes later, she comes into the kitchen and says \"Do we have any ice cream for the kids?\". I respond with \"Yes, I just gave them some, like we discussed.\" Her response: \"We should give the kids some ice cream.\" Now, that's a little bit of a weird conversation, and I think she was distracted by someone messaging her on her phone, but my brain just kind of \"blipped\" and didn't quite know how to handle it and it was instant blurred vision. reply ajkjk 14 hours agoparentI'm convinced from some anecdotes in my life that a non-trivial amount of neurological behavior is like your blips: caused by actual qualities of the data that the brain is processing, rather than anything lower-level like chemistry and diet. Here's an example. After some weird arguments a few years ago I began to notice how people's brains respond when they're overwhelmed by someone talking to them with an abstraction they can't easily follow. Some people kinda 'blank out': they'll start saying 'yeah' or 'okay' in a loop, I think sort of buying time until more data comes in to see if it will make sense. Others strain themselves to follow, but become notably tired or irritable in the process. Others 'fend off' the abstraction by saying things like: 'I don't want to talk about that' or 'that sounds too complicated' . Others seem to not even register it, like they'll instantly switch the subject on a dime and seemingly not notice they did it. Others just pretend to follow even though they don't, or even just don't follow and don't pretend and just say nonsense. And I wouldn't be surprised at all if getting a migraine is another possible response that I just haven't observed/been told about yet. It's really interesting, once you're watching out for it. Also I need to stop saying complicated semi-incoherent shit. It seems to be bad for other people's brains. reply thinkingemote 11 hours agorootparentI have also noticed this and have searched in vain trying to find a definition and/or psychological discussion about this. When I was noticing it in others I would notice myself doing it also! The closest I got was in street hypnotists. They seek to overwhelm the target to put them in that state and then do their suggestions. The target retains the suggestions but with no awareness of hearing. They may even talk to you. The intentional manufacture of the state I think is called \"induction\". But I think it mostly occurs normally between many of us at a lower level (but the hypnotists dont talk about that). The bit I mostly interested in was what myself and others would say and why they would say it when in that non manufactured state. Imagine two people in a blip talking to each other! The other psychological definition related to the talking bit would be a type of \"Freudian slip\". reply jks 14 hours agoparentprevThe blip could be a focal seizure, you might want to talk with a neurologist? https://en.wikipedia.org/wiki/Focal_seizure reply dontwearitout 14 hours agoparentprevYour \"blip\" anecdote is super interesting. I know that exact sensation of disorientation (but without the migraines thankfully), and I'm curious if this has been studied in depth anywhere. reply Conasg 10 hours agoprevI've had crippling migraines for a few years now. Often 20 or so per month. I rarely get headaches, and usually it's more of a feeling of exhaustion and malaise, following aura that includes facial numbness. The worst part is that it affects my work pretty badly by causing brain fog that lasts for days. I'm on Ajovy injections which has reduced how many I get, but the only thing that really helps is taking 800mg of ibuprofen. That gives me a few hours of comfort. reply zmmmmm 9 hours agoprevIt seems like an interesting mechanism, that the brain is somehow signalling by modifying CSF which in turn then then activates nerves. The question I have is mechanistically, how is the brain actually doing that. If CSF sits on the other side of the blood-brain barrier, it doesn't contact brain cells. So you'd have to introduce some other layer of signalling that does communicate between them. reply webnrrd2k 10 hours agoprevJust a data point I'd like to throw in... I'll get some horrible headaches if I don't get enough sleep and eat too much salt. It took me a while to figure out the trigger because it take two things, lack of sleep and eating something salty, and then there is a delay of 12 hours or so. reply lrasinen 11 hours agoprevDoes anyone else get nasal aural symptoms? My go-to signs were a tingling sensation in the nasal cavities, followed by things smelling horrible. I've had the classic visual sawtooth pattern exactly once, but the nasal stuff was there maybe 75% of the time. What seems to have helped was blood pressure medication. reply elric 6 hours agoparentMy olfactory sense gets a major boost when I'm about to have a migraine. This is usually unpleasant, but sometimes it's nice... reply DougN7 15 hours agoprevIn case it helps anyone else, my wife gets migraines immediately from eating anything with milk, onions, corn, green beans, bananas and a few others. However, she can eat all of the above if it is organic. We’re from the US and visited Europe (Germany, Switzerland, The Netherlands) and she could eat anything there, organic or not. Hope it helps someone. reply bloqs 9 hours agoprevGetting up at the same time every day of the week without fail, and reducing overall caffiene intake helped mine reply RobertRoberts 18 hours agoprevI wonder if there are multiple different causes. Because I have had migraines regularly since I was a little kid and sleep was the only cure. (Hours of crying until I'd pass out) But when I got to a teen I tried every pain killer I could find and finally I found that none of them helped unless I took it very early when the pain started. (Within maybe 30-40 min) And even then only ibuprofen worked. (And I have to take quite a bit) But even then the pain doesn't go away for some time. But at least it doesn't get any worse. So I keep ibuprofen in every car, bag, and a few places around the house. reply loloquwowndueo 18 hours agoparentOTC medication is not typically strong enough to fully fend off a migraine (no, excedrin migraine doesn’t count). Go see a doctor, explain your symptoms etc and get prescribed something like rizatriptan. reply redwall_hp 17 hours agorootparentSeconding Rizatriptan. It's a game changer. It also seems to work better earlier, before the pounding really sets in, but in the worst case it still turns it down a lot. In addition to triptans, there's another class of migraine medication that's relatively new, but given the cost and back and forth your doctor will have to do, they prefer to start with triptans: *gepant drugs like Nurtec and Ubrelvy. reply monetus 17 hours agorootparentJust wanted to third this recommendation, as it does defeat some of my migraines. I had brain surgery, get em every week. Rizatriptan is a life saver. reply sveiss 14 hours agorootparentprevEmphasis on the \"something like\": there are several different drugs in this class (triptans), and it might take a couple of tries to get one that works for you. Personally, sumatriptan doesn't work reliably, rizatriptan makes me feel super woozy, but eletriptan works well and without noticeable side effects. reply loloquwowndueo 3 hours agorootparentHence seeing a doctor to get a prescription and follow up :) We tried with diclofenac first, didn’t work at all. reply RobertRoberts 16 hours agorootparentprevDid you have any side effects from rizatriptan? reply wheels 12 hours agoparentprevFor all of the other folks here saying ibuprofen isn't strong enough: I have the same, and only with ibuprofen (vs. other over-the-counter drugs like aspirin or paracetemol): if I catch it early enough, sometimes it staves off the full blown migraine development. I've had migraines since childhood, fortunately only every couple months for 2-4 hours. Weirdly, I'm so used to the pain at this point that I kind of take it in stride. The downside is that ibuprofen also often upsets my stomach, and in the bad ones, I barf from the pain, so there's a gamble as to which route to go down. reply gergo_barany 10 hours agoparentprevThis is similar to my symptoms. So far I haven't found any over the counter painkiller that helps me reliably. At least not in the doses you're supposed to take. A few years ago I read something about a theory that migraines have to do with the body's heat regulation, and it was suggested to try a hot shower. That does seem to help me. So nowadays when I get a migraine (thankfully only about five times per year or so) I run a hot bath and stay in the tub for two to three hours. It's not fun, and for the first few minutes the headache gets worse and really starts pounding (blood pressure changes due to the heat maybe? I don't know). But then the heat does seem to take a lot of the edge off, and after a while the pain actually stops. Which it doesn't reliably do with painkillers only. Much better than lying around in bed, unable to sleep. Not medical advice, your mileage will vary. But might be something others would want to try. If you're going to suffer and wait for it to stop, you might as well consider doing that in the bath. reply greenice 17 hours agoparentprevSame experience (sleep helped as a kid but now not anymore). A neurologist prescribed me Metagelan. reply RobertRoberts 16 hours agorootparentI asked this about the other recommended drug above to... Any side effects you got from Metagelan? Any idea how/why it works for you? reply marsovo 15 hours agoprev [–] So as to not bury the lede: I'm not giving medical advice, but I wonder how many people's migraines are worsened by the Valsalva maneuver? How many would be prevented with a daily low-dose aspirin? I had my first migraine (with aura) as a teenager. What a scary thing for you to lose proper vision, get a truly nightmarish headache, then feel nauseous and throw up! On average I'd say I got 1-5 a year for many years. I managed them with Migranal. Triptans didn't help. Migranal was an expensive pain to get, and is a poster child for our crazy pharmaceutical situation, but I digress. I had a concussion a few years ago after which I could hardly go two weeks without getting at least one scotoma. In contrast to before, sometimes it would be just the aura without headache. For insurance reasons I moved over to Nurtec which wasn't bad, and a lot less unpleasant a taste. But 30+ incidents a year was crippling. My neurologist was convinced my troubles were due to a PFO (patent foramen ovale): in the womb, we obviously do not breathe. A hole between the left and right sides of the heart allows blood to flow through from the mother. At birth a flap is supposed to cover this hole, but in many people it doesn't close all the way and there's a residual \"shunt\" which allows the \"dirty\" blood to flow straight through without going through the lungs (which also filter out junk, like microclots for example). When you strain, such as when bearing down/lifting something heavy/Valsalva maneuver, it really pushes this stuff through. Indeed I seemed to notice migraines came more frequently when I was better about exercising. I had some other sporadic things over the years. My neurologist said \"the brain is a big network. Throw a bunch of crap in there, and you get nonlinear outcomes.\" He was convinced the PFO was to blame for all of this. I was referred to an interventional cardiologist, whose research had concluded, basically: \"not all migraines are resolved by closing the PFO. But, if blood thinners make the migraines go away, in those cases closure does make them go away.\" Bayes at work. I was put on blood thinners and indeed they virtually stopped. (Sure enough, one of the two I did have in the year I was on Plavix--again, down from 20-30--was when I was doing the Valsava to pop my ears while a flight was landing). After a year I did have the PFO closed. I am still on low-dose Aspirin (baby/EC). The neurologist had prescribed it years earlier when he first tested me for the PFO, but I resisted having to take medication daily. My original cardiologist who examined me further for the PFO said it was tiny and thought it was clinically insignificant, and didn't think I needed the aspirin. I've had the PFO closed for a year. I had one migraine during the procedure itself (not unexpected), and one more after about six months when I went off the aspirin. So I went back on the aspirin, and will presumably continue to be on it for life. There are still unanswered questions: none of this explains the concussion, does it? The theory I've pieced together is that migraine is a probabilistic phenomenon. Things can make it more or less likely, presumably by putting the brain in a more or less resilient state. Bad sleeping habits? Inconsistent timing of food (low blood sugar)? Dehydrated? This is playing with dry kindling, waiting for the spark: perhaps a microembolism crossing through the PFO, or some other vasoactive compound (I've had them from the sun reflecting off of something and catching my eye). It was around puberty time when it all began, so a lot of stuff is going on. My thought is that perhaps the concussion \"permanently\" lowered my threshold. My mother had always suffered from migraines. For other reasons (old with heart issues), she went on blood thinners temporarily. Then was off the blood thinners. Then she was back on blood thinners. Sure enough, after my experience we looked at her migraine log, and can you guess what pattern we found? At present, closure is only indicated after you've had a stroke (I had at least one event that adequately qualified). I'm lucky: a few years before my adventure, my coworker had a stroke (while lifting weights) with permanent consequences, after which his PFO was closed. We already know that migraineurs are frequently at higher risk of stroke. I would bet that PFO closure will become a more routine procedure in the coming decades. reply ilikepi 12 hours agoparent> After a year I did have the PFO closed... If it's not too much to ask, how did you find that procedure and its recovery? I have a moderate PFO and have occasional migraines with aura as well, though in my case the pain component is minimal. My vascular specialist has suggested I consider having the PFO closed, but I have not looked into what is involved. I had a procedure to \"deal with\" a dural AVF a few years ago, and for some reason I have more anxiety about the PFO...but maybe that's just because I haven't read up on it yet... reply marsovo 4 hours agorootparentNot at all. Honestly the procedure was fairly trivial, as surgeries go. The procedure itself took about 15 minutes; I was awake and even watched the progress. They go in through a catheter in the thigh/groin area, snake up and deploy the device, which is roughly a double-sided umbrella/clamshell, one half on each side of the hole. Mine was a GORE Cardioform. It's safe for MRI within some parameters: I have a card I have to show the technologist. In total, of course, it was an all-day affair. For me the worst part was having to lie perfectly still for about 4 hours in recovery, to not pop a stitch. Even so, that night at home I did have some bleeding and ended up going to the hospital. They simply applied pressure and a stronger bandage. If I remember correctly, I basically had to avoid strenuous activity for a couple weeks, but aside from that initial bleeding recovery was not a big deal. For me it seemed like a relatively low risk / low impact procedure (modulo the US healthcare cost lottery). I enjoy scuba diving as well, and I know that PFO also correlates with increased risk of decompression sickness. In addition to the migraine aura, I'd had, let's say a small handful of other minor visual glitches over the years (even predating the concussion), so from my perspective, my most valuable resource, my mind, was getting attacked by microemboli, and for me, correcting the issue mechanically was preferable to living on pharmaceuticals (especially since that only solves part of the equation anyway). reply elric 13 hours agoparentprevClosing a PFO is the only procedure I know of that can cure migraine. Sadly it seems like not everyone's migraines are caused by a PFO. reply hilbert42 13 hours agoparentprev [–] \"I managed them with Migranal. Triptans didn't help.\" Similarly, I found that triptans (sumatriptan) wasn't as good or as effective as ergotamine-based formulations. Whilst sumatriptan worked it didn't suppress migraine pain as effectively as did ergotamine, nor was its effective duration anywhere near as long. A single 1mg dose of ergotamine would last 24 hours whereas 100mg (the stronger dose, 50mg being the other) of sumatriptan would last only 6 or so hours and its action not as effective. The other problem is that until recently sumatriptan† was much more expensive than ergotamine. I say 'recently' because where I am in Australia the price of ergotamine has gone up eightfold in recent months. Now, it seems the only drugs availabe are the triptans—and that's not the only problem, here triptans are only available in packs of two (or four on script for half strength tablets), which is a ridiculously small quantity—one runs out of the drug when it's most needed. (Clearly, the bastards who set the regulations don't get migraines.) On the other hand I could get 100 capsules of ergotamine on a single script. As far as I'm concerned, I'm back in the dark ages before my migraines were diagnosed. No doubt, medicos and pharmacologists who read this will say that triptans are better drugs in that they are more selective, that is their vasoconstrictive effecs are more specific and thus less harmful than ergotamine and no doubt there's some truth to that. That said, what they and drug manufacturers don't emphasize is that the triptans also have bad side effects such as serotonin syndrome which can be brought on by interactions with other drugs such as the TCAs/tricyclics, MAOIs/monoamine oxidase inhibitors just to mention a few (of the side effects). Also, they quickly become ineffective upon repeated dosing — hence the restricted quantities per script. Yes, ergotamine also has some nasty side effects but not those I've mentioned for the triptans. One of the greatest benefits of ergotamine is that it's just as effective after taking it for decades as when one first starts (I know from experience). It seems to me the benefits of the triptans have been oversold and that medicos et al actually believe the mantra. It's great for Big Pharma to have everyone believe that given that ergotamine is a readily-available natural product (from ergot fungi), and that any competent lab could refine it for medical use. It's great for Pharma to have everyone believe ergotamine is obsolete. In the meantime the switch from ergotamine to triptans has caused me nothing but trouble. ___ † Before the recent huge price hike a capsule of ergotamine was about $1 versus $8 for the half strength 50mg sumatriptan tablet (where I am in Australia sumatriptan is available OTC in packs of two for ~$16, the full strength 100mg being script only and even then only in packs of two). Now do the sums: in my case full-strength (100mg) sumatriptan only lasts 6 hours versus 24 for the 1mg ergotamine. Thus the 24-hour cost for ergotamine is [was] $1 whereas sumatriptan is 24/6 x $8 which makes it 32 times as expensive as the ergotamine! Moreover, in my case, I break open a capsule of ergotamine and take only half (0.5mg) as that's usually all I'll need. So that makes sumatriptan 64 times more expensive. Right, Big Pharma has ripped us off yet again, big-time! It's damned time governments did something about this. reply marsovo 4 hours agorootparent [–] This is very interesting for me to read -- my experience was the opposite: dihiydroergotamine was always preposterously expensive (hundreds of dollars for one dose); the history of it was like one of those fascinating novels with a disastrous chain of unintended consequences. It's basically a fungus, used forever, it should be trivial. It predated the FDA, and therein lay the bizarre economic and regulatory paradox: it was grandfathered in, in a way, but the delivery mechanism (a nasal spray) was a later invention that went through the full clinical research gamut. There's no money in the drug to do clinical trials, so it fell into this weird gray area. I wish I could find the article from when I looked into it, presumably some plaintive search at the time, \"why is Migranal so expensive?\" reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "A study in mice, published in Science on 4 July 2024, reveals new insights into migraine causes, linking brief brain 'blackouts' to changes in cerebrospinal fluid that activate pain receptors in the skull.",
      "The research suggests migraines might serve as a protective mechanism signaling the need for rest and identifies a previously unknown gap in the protective layers around the trigeminal ganglion.",
      "This discovery could lead to the development of new treatments for migraines by targeting the newly identified mechanisms."
    ],
    "commentSummary": [
      "A study suggests that cortical spreading depression (CSD), a \"brain blackout,\" might be involved in migraines.",
      "People reported various triggers and treatments, including dietary changes, medications like Rizatriptan, and lifestyle adjustments such as improved sleep and hydration.",
      "The discussion underscores the complexity of migraines, with different triggers and treatments being effective for different individuals."
    ],
    "points": 84,
    "commentCount": 72,
    "retryCount": 0,
    "time": 1720303961
  }
]
