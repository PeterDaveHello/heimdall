[
  {
    "id": 40662176,
    "title": "Japan enacts law to promote competition in smartphone app stores",
    "originLink": "https://english.kyodonews.net/news/2024/06/bc2d7f45d456-japan-enacts-law-to-curb-apple-googles-app-dominance.html#google_vignette",
    "originBody": "Japan enacts child care law to tackle declining birthrate Jun 5, 2024KYODO NEWS",
    "commentLink": "https://news.ycombinator.com/item?id=40662176",
    "commentBody": "Japan enacts law to promote competition in smartphone app stores (kyodonews.net)436 points by pjmlp 23 hours agohidepastfavorite291 comments Johnny555 20 hours agoThe law will prohibit the providers of Apple's iOS and Google's Android smartphone operating systems, app stores and payment platforms from preventing the sale of apps and services that directly compete with the native platforms' own. I'm not sure what this means -- does this just mean that Apple can't prevent a third party from selling an app that does something an Apple app does, or does it mean they have to allow third party app stores? Or is it more about opening the payment platform so an app can take direct payments instead of having to go through Apple? reply BadHumans 20 hours agoparentI read this as Apple can't ban Spotify because they have Apple Music. The question is what happens when an app is competing with Apple but also is breaking Apple's TOS? reply arrosenberg 19 hours agorootparentThe answer is obvious, because you can’t fairly resolve a conflict of interest like that - alignment between publishers and distributors in the same vertical should be banned. reply alwayslikethis 17 hours agorootparentRealistically, no hardware manufacturer of a significant size (let's say 100k total devices) should be allowed to dictate what software can be distributed to users. It opens up all kinds of unfair business practices. reply gwbas1c 16 hours agorootparentCareful: Before web apps were common, 3rd party applications on Windows would break all kinds of things. The hardware manufacturer needs to ensure that software doesn't break the device. Apple and Google are going well beyond any reasonable grey area, though. Demanding a cut if I buy a book through the Kindle app is absurd and has nothing to do with ensuring that Kindle doesn't break my smartphone. reply csense 2 hours agorootparent> The hardware manufacturer needs to ensure that software doesn't break the device. Preventing software from breaking a device is a solved problem. All you need is a CPU that supports memory protection, and an appropriately designed OS. If you want to only run software that's been OK'ed by a third party, that's certainly a choice you can make for yourself or your organization. I don't like that this choice is imposed on almost-all cellphone users by monopoly. reply Zak 8 hours agorootparentprevWindows 9x and classic Mac OS are extremely fragile compared to Windows NT, Darwin, or Linux. Doing anything non-trivial without breaking those systems required some skill. reply pmontra 1 hour agorootparentThat was in large part because the CPUs they run on lacked hardware memory protection and any process could crash and take everything else down with it. No hardware support and no support for those features in the OS. That changed quickly as the CPUs improved. All the UNIXes of the time run on CPUs with memory protection and never crashed. reply Sophira 1 hour agorootparentprev> The hardware manufacturer needs to ensure that software doesn't break the device. What about those of us who want to break our devices? (At least, according to the manufacturer's idea of what \"broken\" is.) reply Spod_Gaju 4 hours agorootparentprevI really don’t need Apple or Google protecting me, but thanks. reply Arainach 3 hours agorootparentWe all benefit from but having massive botnets and not having to worry that every app we install is copying all of our data to a server. reply elric 2 hours agorootparentDo we really, though? It sounds a lot like buying short term security at the cost of freedom. reply vkou 2 hours agorootparentWe buy that all the time, and ask for a second helping. All things are temporary, and often this is a reasonable tradeoff. The problem isn't in that transaction, the problem is conflicts of interest. Platform vendors engaged in both gatekeeping and building their own apps have an inherent conflict of interest. I think that Apple in particular (but not Apple alone) abuses it quite brazenly. The issue isn't that the police exist (a tradeoff of freedom for temporary security, we'd all be freer if they didn't), the issue is when they start shaking me down, or otherwise brutalizing me. reply jerojero 1 hour agorootparentprevIt's true that without Apple and Google's eye we'd have more cases of malware in our devices. But I think it's also true that people are more than capable of adapting to such landscapes because we already do that with our desktop/laptop computers. Yes, malware exists and people do get their computers infected. But I believe the reliance of a sort of \"watchful eye\" also makes us lazy. I mean, the ideal situation is not one where Apple doesn't have an app store. But imo more like what Google does with android where you can install anything if you really want. I think the cuts these companies take from having a curated store is too high though. And in the case of apple there's simply no alternative (well, there's the nascent stage of an alternative forming in Europe I guess). reply devoutsalsa 10 hours agorootparentprev> Demanding a cut if I buy a book through the Kindle app is absurd and has nothing to do with ensuring that Kindle doesn't break my smartphone. \"Ensuring that Kindle doesn't break my smartphone\" requires time and effort. That's funded by the cut they take. reply gwbas1c 5 hours agorootparent> That's funded by the cut they take. Apple / Google do not need to take a 30% cut of every e-book ever sold to fund their review. That's absurd and illogical. One estimate is that Amazon sells 487,000,000 ebooks a year. If we assume they cost $10 each, that's almost a $1.5 billion cut for Apple and Google. It does not cost $1.5 billion for Apple and Google to review Kindle to make sure it doesn't damage devices. reply sqeaky 3 hours agorootparentprevOthers have addressed why the price is ludicrous. What if this is a service I don't? Why am I paying for it? The vendors selling in app items are certainly passing on some of the cost of this. reply chrisan 9 hours agorootparentprevIt is unfairly funded. The ensuring part requires the same time and effort for Free apps, apps that make money off ads, apps that cost $1, apps that charge a monthly fee, apps that resell products where a 30% cut exceeds their margin. Perhaps the stores should change to a model of choose your own adventure: pay per release or % cut. What would you say is a fair amount to charge per release - keeping in mind apple/google will want to maintain their insanely high profits from the app stores? reply InitialLastName 17 hours agorootparentprevRealistically, including avionics and medical devices where the software is restricted by regulation (in theory) and where the manufacturer is legally liable for failure of their device? More specifically, the radio in an iPhone can almost certainly be made to operate outside of licensed/compliant limits by tweaking the software. Should they be forced to allow that but still held accountable when their device is noncompliant? reply MenhirMike 15 hours agorootparent> Realistically, including avionics and medical devices where the software is restricted by regulation (in theory) and where the manufacturer is legally liable for failure of their device? Yes, absolutely. The responsible party is the operator of said equipment, and if they tinker with it, it's up to them to be compliant with FAA/FDA/HHS/etc. regulations or face proper legal reprecussions. > More specifically, the radio in an iPhone can almost certainly be made to operate outside of licensed/compliant limits by tweaking the software. See above. We can already do that today with WiFi chipsets that can be made to use frequencies that are illegal in a certain country. It's on the operator to ensure compliance. Alternatively, device manufacturers are free to use components that only work in a certain frequency spectrum - but that wouldn't prevent an operator from using them in another country. We already solved those problems long ago. reply Xelbair 11 hours agorootparentprev>Should they be forced to allow that but still held accountable when their device is noncompliant? no, because there's this weird concept that seems to be foreign to the modern software folk - Ownership. Owner is held liable for that, not device manufacturer. reply adrianN 15 hours agorootparentprevIt is already legal to modify cars even though they are dangerous, so a working legal framework for that kind of stuff exists. I don't see why we shouldn't be able to handle the case where somebody chooses to run Doom on their pacemaker. reply dzhiurgis 13 hours agorootparent> It is already legal to modify cars even though they are dangerous Not in places with regular inspection and mandated insurance. I.e. most civilised world. reply viraptor 12 hours agorootparentThat's not quite right. You can totally modify them. The restrictions are around what's allowed on public roads, not what you do with the car. (You can drive whatever you want on your land) And even then, many modifications are ok and you can get special classifications for \"unsafe\" cars (like vintage ones without any safety features) reply freeone3000 8 hours agorootparentprevNearly all car mods pass inspection. Look at what is actually checked: safety and emissions. Nothing about speed, appearance, sound, horsepower, ride height, or stock state. And moreover: if you modify a car that fails inspection, it’s on you, not the manufacturer! reply sqeaky 3 hours agorootparentYou are correct in the larger sense, I am just pointing out details. Sound checks are part of checks in some places where there are sound limits on vehicles. Some \"mufflers\" are more like megaphones. reply shiroiushi 16 hours agorootparentprev>the radio in an iPhone can almost certainly be made to operate outside of licensed/compliant limits by tweaking the software. Should they be forced to allow that but still held accountable when their device is noncompliant? I can see an argument for this. Nothing forced Apple to design their hardware this way: they could have built licensing/compliance limits into the hardware itself. But they didn't want to do that because they wanted to use the same HW for all markets, and different governments have different rules about which frequency bands are allowed. Of course, this then brings up the question: if they did make slightly different HW per-market (perhaps with 1-time fuses), what happens when someone brings their iPhone from one country to another and they've modified it to ignore any new region restrictions (Apple could still use SW to force new restrictions, though they can't allow anything new because of the HW restrictions) and it's broadcasting on an unallowed band? reply throwaway2037 15 hours agorootparentMy guess about roaming between different domestic and international regions: A handshake with the tower tells the phone what is the network type, then software (firmware) controls what frequencies to use. I just cannot believe in 2024 that this problem has not been solved many times over. What exactly is you qualm / concern? reply shiroiushi 15 hours agorootparentThe point is that someone could modify the software to broadcast on disallowed bands: this is one of the main arguments against allowing user-modifiable software on smartphones (or any radio device). reply leetcrew 14 hours agorootparentso what? broadcasting on unauthorized bands is essentially painting a \"come fine me\" beacon on yourself. surely the FCC (or it's Japanese equivalent) can handle these sorts of revenue opportunities without Apple's help. reply LeonB 12 hours agorootparentIf Johnny Nobucks makes an illegal broadcast using an Apple device - or rather if Johnny Nobucks makes an app for the express purpose of doing the illegal broadcast, and distributes it to 1 million other users on Apple devices — the FCC equivalent would faaaar prefer to sue 1 rich person (Apple) - with a chance of getting big bucks — than 1 million and 1 poor people — (Johnny no bucks and his users). But I’m pretty sure Apple would be able to put together a license model that makes them safe in this case, long before they were put in any legal danger. Anyone who claims it’s an intractable is using it for a different purpose -/ either trying to keep their sweet monopoly - or wishing they could sue apple. reply bee_rider 14 hours agorootparentprevWatch them start distributing their programs through cartridges (hardware) to circumvent this type of law, or something ridiculous like that. reply wibbily 16 hours agorootparentprev> Should they be forced to allow that but still held accountable when their device is noncompliant? I mean, routers have worked like this for ages. OpenWRT has an entire page about tweaking your radio settings, and what could happen to you if you do. Why would Apple be held liable for something you did to your device? https://openwrt.org/docs/guide-user/network/wifi/transmit.po... reply intrasight 16 hours agorootparent>Why would Apple be held liable for something you did to your device Because Apple is worth trillions of dollars reply callalex 12 hours agorootparentSo is Cisco/Linksys reply LeonB 12 hours agorootparentprevYup. Regulators would rather sue Apple than the end users for the same reason John Dillinger chose to rob banks - “that’s where the money is.” reply al_borland 15 hours agorootparentprevFrom what I always understood, the argument for the App Store was to ensure the hardware remained stable. While it's easy to say, \"it's mine, I can do what I want,\" when that device is a persons life line to emergency services in the even something happens to them, that is not a responsibility a company should take lightly. I like to think Apple takes that seriously and acts in accordance with that responsibly. They've said as much publicly, but of course one can choose to believe them or not. Even if someone wants to ignore this concept, where is the line drawn? Should a hardware manufacturer, or OS vendor, be allowed to make a product that blocks malware on their systems (like Windows Defender)? Windows was seen as a poor product because of all the adware that infected it in the early 2000s. That issue has largely been resolved, thanks to efforts from Microsoft. Should they not be allowed to solve the biggest issue that plagued the public perception of their product? I realize I'm move the goalpost slightly, from hardware to OS vendor, but ultimately, it isn't Apple's hardware that's creating these controls, it's iOS, the operating system. And in the case of Google, it's clearly an OS vendor issue, as Android is installed on a wide variety of hardware from many different OEMs. reply shiroiushi 13 hours agorootparentYour Windows Defender argument seems like apples and oranges. It doesn't fully prevent adware infection anyway, but regardless, the discussion is about whether a hardware maker (or platform owner if you're going to try to extend this to OSes) should be allowed to prevent 3rd-party software selected by the user from running on their platform. Defender doesn't do this: it's an optional security application by MS that's included with Windows. Users are free to disable it if they wish. Nothing is preventing Windows users from running whatever software on top of Windows that they like. Defender will prevent some malware from running, and many users like this for obvious reasons, and the fact that it's included for free unlike competing anti-malware software, but it's not so baked into Windows that you can't turn it off. An antitrust argument could possibly be made, along the lines of Windows including IE and putting competing software out of business, but that's a different issue than what we're discussing. So you're right about this being really an OS vendor issue, but Microsoft doesn't force anyone to use Defender, and doesn't prevent anyone from using competing products. Google also allows using competing app stores (or even side-loading .apk files), though only a tiny fraction of users take advantage of this. Apple is really the problem here because it doesn't allow these things at all. reply leetcrew 13 hours agorootparentprevseems like a false dilemma to me. the platform owner should do all those responsible things that you mention. they should also offer an escape hatch for turning all those portions off, should the owner of the device so choose. I respect Apple's attitude much more than Google and Microsoft these days, but they really go too far with locking down iOS. reply _carbyau_ 13 hours agorootparentprevMovie makers shouldn't own cinemas. Car makers shouldn't own dealerships. Hardware makers of general computing devices (non-appliances) shouldn't make Apps. This game has played out a few times. Lets see how this one goes. reply smugma 4 hours agorootparentRe: movie makers owning cinemas, Sony just acquired Alamo Drafthouse yesterday (until 2020, this wasn’t allowed): https://www.hollywoodreporter.com/movies/movie-news/sony-pic... reply CamperBob2 3 hours agorootparentprevCar makers shouldn't own dealerships. Not an example I'd use to back up the pro-consumer argument you're making. reply arrosenberg 1 hour agorootparentBeing against vertical integration is not an explicitly pro-consumer argument. It's an argument in favor of regulating market power. Consumers may well pay a bit more in some cases, but they will be rewarded with healthier, more resilient markets. reply LordDragonfang 1 hour agorootparentprevYeah, the car dealerships are one of the more toxic interests in politics due to their legally-protected niche, and are in general a horrible leach on the economy: https://www.youtube.com/watch?v=k_zWFGOSD28 reply bdowling 12 hours agorootparentprevAll of those are common in Japan. reply nonrandomstring 12 hours agorootparentprevYes, these are all good examples of things that actually shouldn't be done, under a certain interpretation of law. They are perfectly reasonable under a certain interpretation of monopoly. It's just that the US definition and that in Europe (and elsewhere) differ quite profoundly. United States anti-trust revolves around the idea of harms. You have to show that Mr Moneybag's empire causes material harm to customers - such as paying more than they could in a more efficient market. The European (and it seems Japanese) take is directed at power. Even if it benefits consumers it's wrong to leverage dominance in one area to obtain power in another. reply rahkiin 13 hours agorootparentprevWhere is the line? Because what you describe would block any plugin system in any software: AutoCad maker should not make autocad features because there could be third party extensions instead, and they can not make extensions themselves…? Or microsoft can not add nee features to VSCode as it competes with extensions, and they can not make their own extensions because they already on VSCode as a platform? reply Drakim 7 hours agorootparentPeople shouldn't be dishonest, yet we can't just outlaw lies. But we can outlaw lying in advertisements and product information. It basically comes down to the scale and context. Things don't need to be completely morally black and white for us to see that something is generally bad. reply cromka 18 hours agorootparentprevThe conclusion I think is they’d have to establish an independent arbitration panel and put them in charge. But they’ll lose control over their user experience then, so it’s full circle. reply arrosenberg 18 hours agorootparentThats the kind of regulation that inevitably fails for one reason or another, but usually capture. Ordering the break up the OS and App Store is a self-executing and relatively permanent solution that can’t be corrupted nearly as easily. reply mlindner 15 hours agorootparentprevThe question wasn't \"should\" but \"can\". Don't write inflammatory posts that don't answer the question. reply a_victorp 12 hours agorootparentprevApple's TOS does not have the same standing as a law, so whenever Apple's TOS is incompatible with Japan's law it probably will not be able to be enforced in Japan reply nottorp 11 hours agorootparentprevConsidering this is a law, Apple's TOS becomes about not worth the toilet paper you could print it on. Contracts can't contradict existing law. Even in the US, I think. reply stavros 11 hours agorootparentI hate Apple's app store policies as much as the next guy, but does this mean I can make malware that plays music and Apple has to allow it in the app store? reply BiteCode_dev 11 hours agorootparentNo because Malware breaks the law. reply nottorp 10 hours agorootparentprevIs filtering for malware forbidden by any law? And btw, those apps that the app store is full of that trick you into a $80/month subscription aren't malware? reply stavros 9 hours agorootparentYes, the law in question, which forbids Apple banning apps that share functionality with Apple's apps. reply nottorp 9 hours agorootparentSo Apple's apps are malware? :p reply stavros 8 hours agorootparentTo clarify the original premise: I release an app that BOTH plays music AND is malware. Under this law, Apple can't ban the app (because it's malware) because the law prevents it to (because the app plays music, competing with one of Apple's apps). reply nottorp 8 hours agorootparentThe app is already illegal because of whatever other laws apply to it. You don't even have to have digital specific laws, any law dealing with fraud or theft will apply. Bonus: you haven't read the Japanese law in the original Japanese legalese*, just an english summary. * And since it's in legalese, you probably couldn't even if you spoke some Japanese. reply DetroitThrow 10 hours agorootparentprevPresumably Apple could still legally restrict malware because they are not distributing their own versions of malware, yes? reply stavros 10 hours agorootparentBut then they can restrict anything, on the basis that the software they're restricting does more than their software. reply DetroitThrow 31 minutes agorootparentI was being a little tongue-in-cheek, but I find your premise very fun when thinking about how quickly it can be taken to extremes with criminal or civil damages. Thankfully, our legal systems are much better equipped to deal with ambiguity like this than I think you are proposing. reply stavros 12 minutes agorootparentI guess I was more saying that ambiguous laws can cause more harm than good, but then I was basing that on the title of the article of a summary of a translation of a summary of a law, so I'm probably wrong :P seydor 14 hours agorootparentprevIt will have to go to court reply bbarnett 20 hours agorootparentprevI think Apple might have to enforce the ToS on their own apps, if they want to levy them on competition, with such a law. But that's me thinking common law thoughts, not sure whst Japan's legal system is like. (There are alot lf things like this, such as when you are a distributor selling to more than your own stores.) reply shiroiushi 18 hours agorootparentLike every country that isn't Anglophone, Japan does not have a common law system, it's based on Germany's civil law system IIRC. reply throwaway2037 15 hours agorootparentUS is primarily English speaking and is not common law based. I assume the same is true for many Carribbean and Pacific island nations. reply shiroiushi 15 hours agorootparentWhat universe do you live in? The US is absolutely a common law country. reply LeonB 12 hours agorootparentNot Louisiana! reply vel0city 3 hours agorootparentI thought we were limiting it to English-speaking areas... reply SllX 11 hours agorootparentprevDepends on if you’re talking about civil or criminal law. reply pjmlp 10 hours agorootparentprevI guess its French origins might be related to that then. reply wordofx 17 hours agorootparentprevnext [3 more] [flagged] lesuorac 17 hours agorootparentJapan is purple but that still means they're not Common law. https://en.wikipedia.org/wiki/Civil_law_(legal_system)#:~:te.... reply shiroiushi 17 hours agorootparentprevWTF? This must be the dumbest comment of the day here. There's nothing wrong with pointing out that only Anglophone (which refers to a language, not a race) countries use the British common-law system. reply BadHumans 19 hours agorootparentprevThe issue that Spotify and others take with Apple is their 30% cut. So Spotify adds a payment processor that doesn't involve Apple, then what? reply FpUser 19 hours agorootparentThen Apple will be forced to compete on merits reply posguy 18 hours agorootparentNo, Apple will still have the advantage of not being restricted like 3rd party apps when it comes to running background activities, access to hardware features they haven't published APIs for, and integration opportunities via private Apple only APIs amongst their various apps and platforms that 3rd party apps can't replicate since Apple literally doesn't make those knobs available to them. Why can't my non-Apple laptop start a tethering session automatically with an iPhone when I open its lid? What good reason is there that a 3rd party tool can't generate an auto-reply to a notification from a chat app with my consent? Lots of user experience niceties that Apple keeps only for their 1st party apps to the detriment of us all. Google does similar things on Android, but at least you can get most of these features through 3rd party stores like F-Droid. reply seec 12 hours agorootparentYeah, but the only reason devs are not using those \"private\" APIs is because Apple owns the only distribution possibility. For now, the 3rd party stores are a joke because Apple still has too much control (and the fees are a joke) but I hope the EU runs its course and finally forces them to allow installation of any potential software without any limitations. It is extremely dumb and uncompetitive that iPhones cannot install any apps under the guise of security or whatever. Apple has rested on its laurel and made some stupid choices that seriously limit the potential of their hardware. It's funny how they announced many features that people have wanted for years at this WWDC; they are starting to feel the pressure, I guess. reply BadHumans 18 hours agorootparentprevI think that is what you would like to happen but this would have to play out in court because Apple makes so much money from that 30% they would fight it tooth and nail. reply lobochrome 18 hours agorootparentprevThe 30% fee is not (only) for payments. It's a royalty for the core platform. reply BelleOfTheBall 11 hours agorootparentCouldn't they argue that these royalties don't apply if payments aren't routed through the core platform? Such as saying \"oh, well, the user paid through the web version of Spotify, not the iOS one\"? reply Zak 6 hours agorootparentprevApple certainly looks at it that way, but is not legally entitled to collect a royalty simply for making apps that run on its operating system. Instead, they have created a technical mechanism to do so. The EU and Japan have decided that's unfair. reply troupo 14 hours agorootparentprevWhy doesn't the same logic apply to MacOS? Perhaps because that \"royalty\" has already been paid by users who bought the device? reply pineaux 12 hours agorootparentNo, because people wont accept it in the \"old system\". reply troupo 11 hours agorootparentAccept what? Greediness? Apple used to charge for MacOS. And then they made it free because hardware sales more than made up for any costs of the platform. Apple themselves claim they don't care if AppStore is profitable. Schiller himself suggested they cap AppStore revenue at 1 billion. If it's so costly for them to run why don't they let devs and users use the alternatives? Alternative payment methods, alternative app distribution etc.? Edit. Here's Apple financial report for 2023: https://s2.q4cdn.com/470004039/files/doc_earnings/2023/q4/fi... - iPhone alone generated 200 billion in sales - The entirety of their operational expenses is 54 billion Apple's customers have already paid for whatever expenses Apple is incurring for the \"core platform\" reply iansinnott 19 hours agoparentprevAlso wondering, since to be truly competitive (on iOS) developers would need access to APIs they can't currently use. reply KerrAvon 18 hours agorootparentLike what? reply idle_zealot 17 hours agorootparentJIT code execution, NFC access, ability to use microphone and webcam while multi-tasking, to name the ones I've encountered. reply al_borland 15 hours agorootparentAs a user, I don't want any apps to have direct access to sensing hardware without going through Apple's APIs that control prompt for access and respect user settings. reply skzv 13 hours agorootparentOf course, but that's not the issue that's being described here. The issue is 1st party APIs that only Apple or Google have access to, that 3rd party apps can't use, regardless of whether the use would like to grant them permission. reply maccard 11 hours agorootparentSo apple are required to offer a stable API for everything their device can possibly do, from the get go? reply aeturnum 6 minutes agorootparentIMO they should be required to offer the same API they use when they release an app that's using it. I don't think it has to be stable - it just has to be at the level of maturity that their internal apps are using. That does require a certain minimal level of security and inspectability - but that seems reasonable when you're selling a platform like this. glitchdout 11 hours agorootparentprevYes. Apple’s own apps should have no advantages over third-party apps. reply LexGray 5 hours agorootparentIs it that big an advantage to test APIs that are still in beta and may change at any time? With their built in apps they can fix both pieces at once, but if they started breaking large numbers of apps every update I do not think developers would be happy. reply vel0city 3 hours agorootparentprevApple already has the API for it, they use it all the time themselves. They just lock out a lot of the features for third party developers. reply idle_zealot 13 hours agorootparentprevThat's fine. I sure hope nobody is asking for apps to be able to ignore the user's intent. Currently, there is no way for a user to grant applications these permissions, only Apple can bless apps with them. reply doikor 9 hours agorootparentprevBut the apps by Apple and Google can do that. Why do you want their Apps to be able todo that and not others? reply makeitdouble 12 hours agorootparentprevAs a user no apps outside of Apple's will be on your iphone if you don't explicitely install them. Make sure to install no such app and you'll be fine for the forseeable future. If you think this is unamanageable and there needs to be more provision to protect your consumer rights, you should talk to the consumer rights regulators to ban the behaviors you need protection from, Apple isn't a proxy for that. reply SllX 11 hours agorootparentApple has been serving as a fairly effective proxy based solely on the fact that they have developed the software, the developer tools, the hardware and the APIs and the distribution platform that developers use. The same thing that makes the iPhone lucrative for third party developers is also he same thing that makes it lucrative to bad actors, and part of the iPhone’s appeal is precisely because it is more locked down than Android. I can try out an app, find out the developer is an asshole that wants access to all my contacts based solely off the fact that I’m getting prompted by a system UI and delete the app and know that it is gone. So yes, there is totally a place for private enforcement of a comprehensive developer agreement (read: contract) backed by automated review tools and human review. It’s not perfect, but it is pretty good. reply maccard 11 hours agorootparentprevI think this is unmanageable, but I am happy with the status quo. If I want a device with an alternative marketplace, I can go go android. Instead I now have people who want to use the device that I bought under the terms and agreement that was available changing how I use the device because they think it impinges on their rights. reply al_borland 11 hours agorootparentprevA technical solution that prevents the issue in the first place is better than a legal protection that slaps a company on the wrist if they are caught. reply makeitdouble 10 hours agorootparentThat's quite literally a technical solution to a social problem. And as usual, the problem is not fixed, Apple just gets to chose who they get cosy with. Historically Japan Railways had privileges the France national railway didn't, for instance. reply bena 2 hours agorootparentprevNFC is accessible through the API reply teaearlgraycold 17 hours agorootparentprevPretty sure normal apps can use the mic when multitasking reply idle_zealot 13 hours agorootparentIt's specifically the camera feed that cannot be used while doing split view or other multitasking modes on the iPad, unless the app has Apple's special blessing. This very clearly puts any new or small video chatting apps at a disadvantage compared to incumbents. reply reaperducer 15 hours agorootparentprevYep. The Cornell University bird identification app listens for birds just fine while the user is doing something else. reply ok_dad 16 hours agorootparentprevThe ability to sync data in the background with the screen off is one. For example, Google Photos and Spotify require the screen on and app open to sync, while the Apple apps sync anytime they need to. reply doctorpangloss 17 hours agorootparentprevHaving the same APIs that Photos uses being accessible from any app on any platform would be as big of a leap in photography as the cellphone was. Allowing people to choose an Apple News backend would save the news industry. Apple’s P2P payments are a huge flop. Let any vendor do it. Don’t even get me started on the App Store. Come to think of it, almost every Apple app I use nowadays either sucks or is absolutely terrible. Notes might be the only thing I use that I would still choose to use despite alternatives, everything else is compulsory (or someone else’s compulsory thing). Like fuck Gmail for not blocking Promotions. So it’s all a win for consumers and producers alike. reply fragmede 17 hours agorootparentprevIt's rather dated, but https://github.com/nst/iOS-Runtime-Headers reply blackeyeblitzar 17 hours agorootparentprevAnything you’d be able to access on a general computing device. For example the Windows API. reply m463 12 hours agoparentprevI wonder if this compares to or will ever affect well known non-phone japanese platforms such as nintendo or sony reply ulrikrasmussen 11 hours agoparentprevAlso, does this mean that you should be allowed to install different browser engines that compete with Safari? reply numpad0 17 hours agoparentprevThis is sideloading mandate following EU regs. IANAL, details may vary, the spirit is the same. reply mlindner 15 hours agorootparentThat's not what it says at all though. reply numpad0 12 hours agorootparentTranslator skill/bureaucracy issue. Japanese in Japanese out(btw vice versa). Texts written by a monolingual speaker in Japanese don't translate well, especially if done by the book. There's original Japanese source not linked from the article[0], and longer NHK News source taken from TV news script[1] is available too if you'd like to verify through machine translators. 0: https://nordot.app/1173382143705366598 1: https://www3.nhk.or.jp/news/html/20240612/k10014478361000.ht... reply seydor 14 hours agorootparentprevi suppose it means that apple+google can no longer ban third party stores too reply jojobas 18 hours agoparentprevAn app store would be one of the cases where third parties might want to do what Apple does. Once that's done there's no case for payments to go through Apple. reply da768 18 hours agoparentprevJust waiting for Adobe Store, Amazon Store, Microsoft Store, Epic Games Store, etc. to be installed on every phones soon. Current status isn't normal, but no one would complain about it if transaction fees weren't abusive. reply seszett 11 hours agorootparent> Just waiting for Adobe Store, Amazon Store, Microsoft Store, Epic Games Store, etc. to be installed on every phones soon. Why would it happen with iOS when it has not happened in the 15 years or so since Android exists? There's basically just one relevant alternative app store which is F-Droid, and it's just all around much better than Google's Play Store, although of course it doesn't have any proprietary, closed-source apps. reply delecti 4 hours agorootparentAndroid does have other appstores though. Samsung has one for their phones, and Amazon has had an openly available one for well over a decade (wikipedia says 2011, which tracks with when I remember first using it). Epic wants one, but pursued a court case because they argued Google made it an uncompetitive environment to work within. reply doctorpangloss 18 hours agorootparentprev> Current status isn't normal, but no one would complain about it if transaction fees weren't abusive. This is an oft-repeated misconception. Even if transaction fees were 0%, we'd be way better off with alternative stores. You're just so used to how shitty things are that you can't conceive of a better alternative. Consider that there are plenty of completely free games on Steam that are popular and help those creators find thriving communities. How? They have good discovery. App Store and Google Play sucks in every way. Discovery is awful. Their approach is awful. Why even show download counts and top lists? Stupid, stupid stupid. But you have no choice as a consumer or developer. So the top developers will not complain. What's the point? And anyway, you're complaining about a world where alternatives are viable. Why are alternatives being viable a bad thing. If EGS is paying up front for games and giving them away for free as cross-promo for Fortnite, better that than ad supported garbage. What exactly is the bad thing here, for consumers? The negative aesthetic experience of having more icons? Everyone has to oppose this crushed-in-head line of thinking. One meaingless detail that impacts an extremely low brow part of the aesthetic experience - the fucking home screen icons - should not preclude the gain in meaning from 10k-100k more developers who could flourish in the mobile ecosystem if it were to have working discovery. It has the same energy as requiring Helldivers users to create PSN accounts - offensive only in a strictly aesthetic sense even though successful competition and cross promo benefits everyone. The users' fixation on meaningless aesthetics is wrong. I mean Apple could make discovery pluggable too, all of it could be pluggable and have fewer \"icons,\" this isn't even a real obstacle. There are many, many ideas in this space, and no permissions to do any of it. As it is, the App Store and Google Play are glorified install wizards. Open, install TikTok, Google Maps, whatever. Never visit again. That is 98% of people. That's horrible and its Apple's fault. TikTok already does not pay any fees. This is just to show that you are not right in general, even if you are right about the one company you've heard of that went and took these people to court in this country for an outcome that you should be in favor of. reply talldayo 17 hours agorootparentFree app stores like F-Droid are also great for stopping you from downloading crap software. Whenever I need \"normal software\" (music player, PDF reader, RSS feed, what have you) I search for it there and don't suffer through ads or microtransactions. It's like using Linux, free software is just what I default to now. Using stock iOS and Android today, it feels like both sides have lost the script. The entire pipeline of \"consumption\" dominates both platforms, and demands you pay money or accept a competitor's inferior product. Google didn't even let F-Droid auto-update apps until recently, it's a racket on either side. We need to bring the hammer down and enable people to stop supporting shit businesses. The current loop of consumption is going to kill everything we love about computing with a long and painful extortion process. reply throwaway2037 15 hours agorootparentReal question: How does F-Droid police malware? reply talldayo 13 hours agorootparentBy disallowing all proprietary software, building each application themselves, and then signing it with their key before the end-user receives it. reply zztop44 17 hours agorootparentprevMy fear is that if I want to download Skype or Teams, I’ll need to first download the Microsoft Store app, and then sign in, and then download the app I want. And the store will be slow and shitty and packed with ads. Likewise for Meta, Adobe, every big game publisher, etc. And my subscriptions will end up spread across multiple different stores rather than all in one place. That feels like the natural direction App Store competition would take us. But, on the other hand, it doesn’t seem to have happened on Android, so maybe I’m being overly pessimistic. reply shiroiushi 16 hours agorootparent>My fear is that if I want to download Skype or Teams, I’ll need to first download the Microsoft Store app, and then sign in, and then download the app I want. And the store will be slow and shitty and packed with ads I'm not sure I see the problem here. If some apps are only available through shitty ad-packed vendor-controlled stores, then hopefully that'll push people to simply avoid them. I mean, if I want to video-chat with someone and to use Skype/Teams I have to download the Microsoft store app and suffer with all that, or I just could use Facebook Messenger on the regular Google Play store (assuming they don't force their own store like MS) and it's easy, I'm going to tell my friend, \"let's use FB Messenger instead; these MS apps are a pain in the ass.\" And if someone insists on using some shitty MS app that I can only get through the shitty ad-laden MS Store, I might re-evaluate how much I really want to chat with them. Smarter app vendors are going to try to avoid putting their users through that experience. (Also, this is just an example; for all I know, Meta/FB in this possible future would be the stupid one pushing an ad-laden store while MS might be the smarter one making it easier for users to install their apps.) reply LexGray 5 hours agorootparentThe issue is many stores use dark patterns and you may not catch on until too late. - Subscriptions impossible to cancel - Silent auto-renewal - Silent price changes - Hidden Fees - Sales of purchase history - Apps released under known brand that are off brand knock offs. - Apps bundled with added cruft (think those download.com installers that installed toolbars and whatnot). - Pirate versions of Teams through off brand store causing licensing audits A lot of dumber app vendors will just drain the suckers dry and rebrand and if Microsoft pulled their apps from the App Store some unfortunate souls would click the first Google hit and get sucked into the scam. reply bruce511 16 hours agorootparentprevPlus, I'd add that this is only an issue for apps with a high network effect. For everything else I'd expect publisher's to just put their app in my favorite place, or risk me choosing something else. For example im not going to install a new store just to get a note-taking app, unless that app was in some definitive way superior to all the others. (Which seems unlikely for a note taking app.) reply shiroiushi 15 hours agorootparentYes, exactly: new stores means much higher friction for consumers, so they're much more likely to choose an easier-to-install alternative unless there's something about that app that either requires them to use it (e.g. work) or its reputation is so much better. reply maccard 11 hours agorootparentprevSo if it doesn’t affect apps with low stickiness, and the network effect will ensure that large publishers benefit from it, what’s the benefit for me as a user? reply maccard 11 hours agorootparentprevIf my workplace says I’m using teams, then I’m using teams. There’s no way to change that. > And if someone insists on using some shitty MS app that I can only get through the shitty ad-laden MS Store, I might re-evaluate how much I really want to chat with them. It’s comments like this that show a huge lack of understanding of how the majority of people feel. I don’t give a shit if my parents like WhatsApp, but I still want to talk to them. And it’s hard enough to get them to use technology, never mind navigating whatever is to come here. Are you honestly telling me that you think Meta are going to not use this opportunity to skirt around the limits placed on their apps by the apple App Store? If you believe meta, or byte dance are going to have your best interests at heart, I have a bridge to sell you reply evilduck 17 hours agorootparentprevBut it has sort of happened on Windows with gaming. I’ve got games on Steam, GOG, MS Store, and Epic. It’s annoying and I very much preferred the state of things a decade ago when it was just Steam. Android has curated a market of users who don’t buy apps. I don’t know that we can extrapolate their alt-store outcome to iOS where the buyers are. reply orangecat 3 hours agorootparentI very much preferred the state of things a decade ago when it was just Steam Ok, but Steam is a third party store. The alternative isn't \"just Steam\", it's \"just Microsoft\". I'll gladly accept the occasional annoyance of multiple stores to avoid being locked into a monopoly. reply nottorp 11 hours agorootparentprev> I very much preferred the state of things a decade ago when it was just Steam. Do you think you'd have all those sales every 2 months if Steam were still the only pc games store? :) reply evilduck 4 hours agorootparentSteam was doing sales before the other stores popped up and I was acquiring games faster than I could play them even then. Increasing sales frequency means little to me, and is even a little annoying since I now feel like I should be constantly window shopping all the stores nearly year round to make sure I get the best sale price on something I'm after. Epic has given a lot of free titles away, which is a big difference, but only because they're trying to buy favor and want to be the winner who takes all. reply doikor 9 hours agorootparentprev> and the store will be slow and shitty and packed with ads.v But both Apple and Googles stores are slow and packed with ads. Pretty much every search you do for an app even with the exact name gives you some ad supported shovelware as the first one or two results (ads). So how is this meaningfully any worse? (outside of one more login you might need to setup). reply nottorp 11 hours agorootparentprevYou assume that just because Apple maliciously complied with the EU law by implementing alternative app stores, it means this is the solution. The solution is to allow sideloading by the user (which incidentally Google allows you to do). reply throwaway2037 15 hours agorootparentprevI think more likely: The app store would be embedded in the first app that you download. And could be seamlessly integrated like MS Teams or FB. reply KennyBlanken 17 hours agorootparentprevThe PC gaming industry is a perfect example of this. In the last 5-6 years I have had to install and use the following game stores: * Epic's Launcher * EA's Origin * Ubisoft Connect * Steam * GOG * Xbox Store * Battle.net I had a brief Gatcha game phase (before I realized how pay-to-advance it was) and that game had its own damn game-specific \"launcher\" as well. Each one of these required creating an account, installs its own \"overlay\", background windows services, anti-cheat system (more background services!), has its own \"social\" system, and defaults to running at startup and minimizing (not quitting) when you click the window-close button unless you dig through the options. Of course, each one of these games also has at least one type of currency unique to the game, which you can only convert in one direction, nor is there any way to move currency in the \"store\". Often that currency, and anything you bought with it, is locked to the particular platform on which it was purchased. It's a complete mess, and nearly every single one of them is worse than Steam in terms of UX design and features. Do we see any competition, resulting in lower prices, better terms of use for customers, or better quality software? Nope. Games are as expensive as ever, have even worse day-of-release bugs, more cheaters, and more microtransactions. Games are exclusive to one particular store either indefinitely or during the period after its release. But according to Epic, why...if Epic can make its own app store for iOS, consumers will benefit! Bullshit. All that will happen is we'll have to install multiple app store apps on our phones, having each one collect data constantly about us... reply nottorp 11 hours agorootparent> Of course, each one of these games also has at least one type of currency unique to the game, which you can only convert in one direction, nor is there any way to move currency in the \"store\". Often that currency, and anything you bought with it, is locked to the particular platform on which it was purchased. On steam and GoG you pay with government issued currency. If the others have a currency system just hard pass on them. If you could cure yourself of Gacha, you can do it. reply maccard 11 hours agorootparentprevIt’s actually worse than on PC imo. Apple have a set of standards (google do too but slightly less so) that means that every app has to support Apple Pay and login with apple. This means I’m not giving my details to random third party with popular game, and I can try it and even spend money on it. With this new order, you can bet that I now have three different subscription management platforms with different rules, for example reply doikor 9 hours agorootparentprev> But according to Epic, why...if Epic can make its own app store for iOS, consumers will benefit! Bullshit. All that will happen is we'll have to install multiple app store apps on our phones, having each one collect data constantly about us... Most of Epics arguments are that the developer will benefit by them taking a smaller cut then Apple/Google/Steam/etc so if they sell the game for the same price the developer gets to keep a couple percentage points more money. I don't remember any argument they have made that has put the customer as the beneficiary though there probably are some. reply da768 17 hours agorootparentprevDo you expect it from being different from Windows with the average person having at least a dozen game launchers, update services and downloaders running in the background? reply bruce511 16 hours agorootparent\"Average\" is doing a lot of work here. Gaming is surely popular on phones, but I suggest that AAA games on on a tiny minority of phones overall. (My mom has literally no games on her phone etc.) For Windows the proportion of games-machines to others is tiny. The number of people with \"a dozen game lauchers\" would be a microscopic percentage. Yes, there are home PCs that are dedicated to gaming. Yes they will likely have lots of shortcuts, launchers, auto updates etc. And in some demographics (think male, under 25 etc) there will be proportionally more games installed. But \"average\" ? I'm not sure. reply onlyrealcuzzo 17 hours agorootparentprev> App Store and Google Play sucks in every way. Discovery is awful. Their approach is awful. ~30% of Apple's total value could be attributed to the App Store alone. That's a ~$1T company. If you think the product is awful - I don't know what to tell you. Next, are you going to tell me the iPhone is awful and Nvidia's GPUs, too? Look, just because something isn't perfect doesn't mean it's awful. The App Store could be better. It doesn't suck. reply doctorpangloss 17 hours agorootparent99% of people opening the App Store are not opening it to solve a problem with software. They’re opening it because it is the only way to install Disney+, ChatGPT or whatever thing they’ve heard of through a $10m-$1b of ad budget and ubiquity in the discourse. Compare to Steam where most people opening it are doing so to launch games and learn more about other new games. The App Store sucks. Lots of things suck and make money, tons of money. OPEC absolutely sucks and makes tons of money, are you going to tell me “OPEC could be better. It doesn’t suck.” Monopolies and cartels don’t just suck, they are horrible, they are the biggest antagonists in our lives, because everyone’s income is someone else’s expense, and we don’t all work for Apple or OPEC. reply throwaway2037 15 hours agorootparentYour first paragraph raises some very good points. I am concerned about this last point: > The App Store sucks. Can you give some specifics? Do any other app store do it better? reply pineaux 12 hours agorootparentSo much about it sucks. I am actually amazed when an app I want is actually on the app store. It needs an iCloud log in, so you can't install free software on a kiosk without a throw away account. Which is hard to set up. Why do you first need to 'get' an app and then install it? How many affirmations do you need? reply darby_nine 17 hours agorootparentprevThe value is in the lock in, though. This absolutely sucks for consumers and the profit is just an indication of market inefficiency. We need to be able to force competition to bring the valuation of the app store and the value the hardware provides users in line with the potential of the technology. If apple truly is bringing the market what it wants at a price folks find reasonable surely this competition wouldn't impact anything! reply sneak 10 hours agorootparentprevI would. The App Store censorship is abhorrent even if they charged nothing. I should be able to install hacking tools, background apps that might kill my battery, sandbox breaking apps that allow adversarial interoperability, porn apps, protest apps that track cops, or apps that do legal things that nonetheless assist me in breaking the law. Apple allows none of this. reply seydor 14 hours agorootparentprevapp stores have been around for decades now, time to regulate them like they do banks or retail stores reply PurpleRamen 6 hours agoprevIs this exclusively for smartphones (and I guess tablets too), or does this also take effect with other walled devices, like consoles (PlayStation, Nintendo Switch, Xbox, etc.), media boxes (Fire TV, Apple TV, etc.), ebook-readers and what else there is. reply hamasho 19 hours agoprevAs a Japanese, I hope this will challenge the dominance of those IT giants. They should be subject to anti-trust regulations in the first place, but I think it won't happen soon, so I'm glad they are taking action. That said, I'm not particularly sure Japanese politicians can handle those complicated matters efficiently. I mean, our \"cyber-security minister\" was accused of never using a PC and not knowing what a USB memory stick is [0]. I can't help but think that this will end in disaster and IT giants will use these examples as evidence of how bad it is to make strict regulations against them... [0] https://www.theguardian.com/world/2018/nov/15/japan-cyber-se... reply dmix 19 hours agoparent> They should be subject to anti-trust regulations in the first place > I mean, our \"cyber-security minister\" was accused of never using a PC and not knowing what a USB memory stick is From what I've read about Japan it was probably the usual small set of local megacorps that Japanese politicians protect heavily that seeded the regulations, not out of a genuine interest in the protecting or improving regular customers lives or opening up competiton to smaller players. But regardless hopefully it is more than just the pipe dreams of the existing entrants and there really is upstart competition that is capable of operating in a meaningful way. reply Nition 14 hours agoparentprev> Another joked that perhaps Sakurada was simply engaged in his own kind of cybersecurity. > \"If a hacker targets this Minister Sakurada, they wouldn’t be able to steal any information. Indeed it might be the strongest kind of security!\" I love this comment from your article. reply resoluteteeth 16 hours agoparentprev> That said, I'm not particularly sure Japanese politicians can handle those complicated matters efficiently. It's not as if this law was actually written by the elected politicians, and furthermore, it's based on an EU law. Japan is only one of the many countries that are now using EU laws as a model for their own legislation for issues like privacy and antitrust law. reply wraptile 13 hours agoparentprevWhile I agree I feel it's kinda poetic that Japan is notoriously ok with Nintendo and Sony putting people in prison and has done absolutely nothing about it. reply dannyw 19 hours agoparentprevMeanwhile Singapore’s prime minister knows programming and writes Python in his spare time. reply zarzavat 9 hours agorootparentThat’s underselling Lee’s talents, like saying Carmack learned shell scripting in his spare time. Lee was Senior Wrangler at Cambridge, i.e. he came top of the class in one of the most competitive mathematics programs in the world. He’s in the company of people like Ben Green and Kevin Buzzard: https://en.wikipedia.org/wiki/Senior_Wrangler#Senior_Wrangle... reply throwaway2037 15 hours agorootparentprevAnd has his gov't done anything to protect it's citizens from monopolistic foreign tech firms? No. So really, this comment means little in regards to the wider discussion. reply Barrin92 19 hours agorootparentprevDoes that mean anything though? I don't want Slash to draft anti-trust legislation in the record label industry because he can play the guitar. Crafting correct market regulations is largely going to depend on whether people understand the economic dynamics and are free from corporate influence, not whether they do some coding. reply shiroiushi 13 hours agorootparentBeing a guitar player by itself does not qualify you to draft anti-trust legislation for the music industry. However, someone who's deaf and doesn't know anything at all about music or the music industry would probably also be a poor choice. At least Slash has decades of experience working with major players in the industry and could probably advise on it to some lawyers who know about anti-trust regulation and at least understand the basics about the industry. reply jvanderbot 2 hours agorootparentI still disagree. It's somehow weird to have a deaf person writing music industry regulations - but it's actually not at all important for them to hear music to regulate it. Otherwise, do we regulate differently based on _sound_!? I hope not! We don't regulate based on music theory either! Do we need him to know chord progressions to be able to write laws? I hope not! With tech it's a little different, we do regulate based on aspects that touch on fundamental theory, like manufacturing processes, cryptography, etc. reply senorrib 18 hours agorootparentprevIt may not mean guaranteed signal that they can create good regulation, but the opposite is a pretty major red flag. If you don't understand technology, I have a very hard time trusting you to enact laws on the subject. reply Dalewyn 19 hours agoparentprev>As a Japanese, I hope this will challenge the dominance of those IT giants. Also Japanese(-American), this will not fundamentally change the situation of Japanese irrelevance. You cannot and will not win if you have no players to begin with, and Japan is by far the absolute worst country when it comes to tech now. Once upon a time NEC was one of the biggest microprocessor manufacturers in the world, Sony was the world's boutique brand, Toshiba was bleeding edge, and Panasonic was the Samsung of the time, but it's the 21st century now and Japan hasn't been relevant for a looooooooong time. Top down government regulations won't solve this. Only Hitachi has remained mostly unchanged, relevant then and still relevant now. Not much room for comfort, though. reply adrian_b 12 hours agorootparentAfter returning from a vacation in Japan, I must say that the word \"irrelevance\" is far too hyperbolic. While it is true that unfortunately Japan is no longer a leader in the development of new technologies, like it has achieved to be in the past, when looking at its public infrastructure, at least in the big cities, and at the applications of modern technologies for solving various problems of the day-to-day life, which actually matter most for the majority of people, Japan is still well ahead of almost all other countries and I have very little hope that I will ever see any country in Europe or America matching Japan in certain aspects of the quality of life. It is true however that Japan is facing some serious economic problems, with declining revenues for many people. However, this appears to have artificial causes, due to mismanagement at high levels that does not result in a meaningful change of the managers. reply unscaled 17 hours agorootparentprevThere are about 195 countries in the world (depending on how you count), I find it hard to believe that Japan is dead last on the list. I'll be the first to complain about the bad state of software in this country, but it still sits very comfortably in the top half of that list. reply kmeisthax 17 hours agorootparentprevJapan stopped being relevant predominantly because the US diplomatically bullied Japan into making itself irrelevant. Japan used to be what China is now: a megasupplier, protected by an artificially cheap domestic currency. Top-down government regulations are necessary to curb big tech's power because big tech has subverted the normal mechanisms of market competition that are supposed to make dictatorial control unprofitable. The question is whether or not Japan denying market access to Apple or Google will be a sufficient punishment to get companies to comply. reply Dalewyn 12 hours agorootparent>Japan stopped being relevant predominantly because the US diplomatically bullied Japan into making itself irrelevant. No, Japan stopped being relevant because Japanese companies are legendary in their hostility towards each other. They themselves are their worst enemy. Not to mention Japanese society is more interested in looking in the mirror than out the window. In fact, it's not even the US who really drove Japan into irrelevancy, it's South Korea, China, and Taiwan. All countries whose companies spent more time taking on the global market instead of squabbling amongst themselves and thus succeeded at out-Japan'ing Japan as a mere stepping stone. Samsung, LG, and Foxconn killed off NEC, Panasonic, Sony, Toshiba, Sharp, Sanyo, and Elpida. Huawei and such cleaned up what remained. Renesas is basically on legacy life support, and who knows if Rapidus won't be stillborn. The US dominance that is Apple and Google is a result of Japan loving to hoist petards, not because the US has power and has no qualms about using it legally or otherwise. reply hammyhavoc 18 hours agorootparentprevIMO, a compelling alternative to push more heavily is open source, because it's something different than just another corporation that places profit before substance and ethics. reply shiroiushi 11 hours agorootparentprev>Japan is by far the absolute worst country when it comes to tech now. No, it really isn't at all. It's better than the US in many ways: 1. Eating out. At many Japanese restaurants, you have a tablet computer at your table, or you get a QR code to order on your phone. You use the tablet/web app (which admittedly doesn't always have the greatest UI, esp when you switch to English) to order your food and maybe pay. It's not just big chains like Jonathan's; you'll even see this sometimes at small non-chain restaurants with 1 or 2 workers. This kind of tech is mostly unheard of in the US; you have to verbally tell your order to some annoyed worker and hope they don't screw it up, and then they expect a gigantic \"tip\" for simply doing their job. Partially thanks to the much lower staffing needed at Japanese restaurants, it's quite inexpensive to eat out. 2. Toilets: they all have electronic controls to spray your butt after you poop. You can select water pressure, activate seat heating, etc. These washlets as they're called are actually available in the US, but no one buys them except maybe Japanese expats. Even after the Great Toilet Paper Shortage of 2020, Americans steadfastly refuse to improve their hygiene and adopt this technology. 3. Public transit fare cards: in Japan, virtually every public transit system uses \"IC cards\" based on the Felica NFC chip. It's twice as fast as other NFC technologies, so it gets people through fare gates quicker, which is critical in the crowded Tokyo subways. These stored-value cards can mostly be used nationwide (tip: get a Suica or Pasmo card, they're good almost everywhere). In addition to train fares, you can also use them to pay for vending machines, many restaurants, and lots more. On top of this, the Felica chip is built into Japan-market smartphones, so you can do all this with your phone instead of a card. In the US, the NYC metro system still uses magnetic stripe cards, and while the DC system is more advanced (using stored-value cards that aren't as fast as Japan's), like all other transit systems in the US, the cards are only good on that one transit system in one city, and for nothing else at all. 4. Remote-control air-conditioners. Called \"mini-split\" in the US, the typical A/C units in Japan are only good for one room, but they come with a remote control and have many functions: cooling, heating, dehumidification, etc. You can control whether the vents are fixed in one direction or oscillate. I'm sure I've missed many other useful functions. Since the A/Cs are room-sized, you don't have to waste energy cooling/heating rooms you're not using at the moment, or you can set them differently. These systems have become increasingly popular in the US, especially for retrofitting older buildings that don't have ducts. 5. Interpersonal messaging. In Japan, everyone uses LINE. It's not perfect, but it has great privacy and lots of features. In the US, most people are still using telephone-number-based SMS from the 1990s. The US is decades behind on basic communications, easily the worst in the world. I don't think I need to go any further here. 6. Bullet trains. Japan leads the world, and will have the world's first serious maglev train between Tokyo and Nagoya in around 5 years. The US doesn't have anything to speak of. 7. Combination washer/dryers. Americans think this is a brand-new thing, but they've been here in Japan for many years now and are very common at the higher-end. 8. Chip manufacturing. The US only has Intel, and TSMC is trying to expand there with government subsidies but it's not going well at all. Japan has Renesas and TSMC is expanding here too, with far better success. Time will tell how this pans out, but given how dysfunctional US society and politics are these days, I wouldn't bet on the US doing well here. reply vel0city 3 hours agorootparent> you have a tablet computer at your table, or you get a QR code to order on your phone > This kind of tech is mostly unheard of in the US Massively untrue. A large chunk of the restaurants I go to have extremely similar setups. Tons have Ziosk terminals at the table. Lots of others allow scanning a QR code and ordering at the table through a website/app. At least half the restaurants I frequent have some kind of digital ordering system. Some of these restaurants started implementing these systems over a decade ago. > Remote-control air-conditioners I generally greatly prefer central AC. Either way, remote controlled single-room AC units have been a thing in the US for several decades. It is not like IR remotes are some fancy new techology that only Japan could have figured out. > Combination washer/dryers They're great if you're space constrained, but with the two separate units I'm able to run multiple loads through faster. They've been for sale in the US once again for decades but have never really been popular outside of really space constrained spaces. And they're also often more expensive than two separate units meanwhile end up getting less laundry done in the same time period. So spend more money and have it take longer to do multiple loads of laundry. Great! > Public transit fare cards Everything I need to do can generally be done through NFC/Bluetooth on my phone these days. Payments, transit access, car keys, etc. Having to carry around a specialty payment card is a step backwards from my normal day to day. reply Dalewyn 10 hours agorootparentprev>Eating out. At many Japanese restaurants, you have a tablet computer at your table, or you get a QR code to order on your phone. You use the tablet/web app (which admittedly doesn't always have the greatest UI, esp when you switch to English) to order your food and maybe pay. That experience is horrible, and not just because pecking at a small screen for ants is horrible. Quite often the thing flat out doesn't work and/or is laid out in the most incomprehensible way possible that it's just faster to call a waiter over. Also, owing to this being computer hardware sitting right next to food, the things are almost always grimey. Proper menus at least are ostensibly cleaned between customers. This isn't a problem specific to Japan, though. >Toilets: they all have electronic controls to spray your butt after you poop. You can select water pressure, activate seat heating, etc. Yeah, they have that since at least the 90s or something. Nothing new. I appreciate the engineering that goes into them, but it's a case of Galapagos. >Public transit fare cards: in Japan, virtually every public transit system uses \"IC cards\" based on the Felica NFC chip. They're great, but the world is starting to move ahead of them with everything moving to smartphone NFC and in a way that is carrier and brand and network agnostic. >Remote-control air-conditioners. They're great, particularly with how quiet they are. If it wasn't obvious from the toilets, Japan is great at hardware engineering. The problem is this doesn't translate to software engineering and high tech in general. >Interpersonal messaging. In Japan, everyone uses LINE. LINE is a perfect example of Japan's irrelevancy in software. Why? Because LINE is a South Korean product. >Bullet trains. Japan leads the world, Yup, top notch engineering as mentioned. Bad news for the maglev since that's being held up in political power struggles, though. >Combination washer/dryers. They've become such a mundane commodity thanks in no small part to South Korean and Chinese mass manufacturing that Japan is hardly relevant there anymore. >Chip manufacturing. The US only has Intel, The US also has Micron and Global Foundries, along with old school guys like Texas Instruments and more. Though I agree it's mostly Intel (and Micron) pushing the envelope. Japan has a very bad track record with chip fabrication ever since they lost sight of everything, Elpida and Toshiba by far being the most tragic examples. The jury is indeed still out on Rapidus, TSMC expansion, etc., but I'm personally not holding my breath. reply cal85 3 hours agorootparentCurious what you mean by “a case of Galapagos”? reply isk517 1 hour agorootparentIt's a way the Japanese came up for saying Japanese Exceptionalism without saying Japanese Exceptionalism. reply shiroiushi 10 hours agorootparentprev>Also, owing to this being computer hardware sitting right next to food, the things are almost always grimey. Proper menus at least are ostensibly cleaned between customers. This isn't a problem specific to Japan, though. This is Japan: if any place cleans things reliably, it's here. If you think most American restaurants clean menus between customers, I have a bridge to sell you. Many don't even bother cleaning the table, in my experience, and then get mad if you point out the table is dirty. >it's just faster to call a waiter over. American waiters absolutely hate this, but they still expect a huge \"tip\". But yeah, in Japan it's perfectly acceptable and normal. >Yeah, they have that since at least the 90s or something. Nothing new. It's not new in Japan, but the rest of the world still hasn't heard about them apparently (or just doesn't care about hygiene), so it might as well be new. >They're great, but the world is starting to move ahead of them with everything moving to smartphone NFC and in a way that is carrier and brand and network agnostic. I don't see that at all in the US. Transit systems there all have their own separate cards that haven't changed in 20 years. > If it wasn't obvious from the toilets, Japan is great at hardware engineering. The problem is this doesn't translate to software engineering and high tech in general. There's more to \"high tech\" than just software and microservices or whatever. Most software is pretty useless without hardware to run it on. Sure, if you're just focusing on stuff that's only online, you'll just use commodity HW and Japan has nothing much to offer here, but most of us have a little more to our lives than just sitting in front of a PC and surfing the internet. >LINE is a perfect example of Japan's irrelevancy in software. Why? Because LINE is a South Korean product. Sure, but that's irrelevant. What's important is what's actually used, which is what my whole post is about: what technology is actually used day-to-day by average people in Japan? Japan: LINE. US: SMS. (And it's just US/Canada that are dinosaurs here; all other countries use some kind of modern chat app.) >Bad news for the maglev since that's being held up in political power struggles, though. I haven't seen anything concrete here, just the one thing about Shizuoka prefecture (I think) complaining. >They've become such a mundane commodity thanks in no small part to South Korean and Chinese mass manufacturing that Japan is hardly relevant there anymore. They're relevant in Japan, where they're fairly common (and made by Japanese manufacturers mostly). Back in the US, they're almost completely unheard of. reply bamboozled 19 hours agoparentprevI think Japan should build software which competes with these “giants” rather than try beat them through regulation. Building good software is still in reach. You don’t need to have an Apple or Google behind you to be competitive. Especially in Japan where just making a native Japanesewill lead to dominance. Look at Line. It’s atrocious rubbish. Everyone in Japan uses it. Japan makes the worst software out of any developed nation I’ve ever seen. Even Nintendo stuff is mostly terrible. Sony is a joke. reply sensanaty 19 hours agorootparentOr we can do both? Encourage competition from smaller businesses while curbing the power and influence of these behemoth monopolies that control basically all of modern human civilization. Apple, Google, Microsoft and all the rest of them should've been nuked into a trillion tiny little pieces years ago. reply bamboozled 19 hours agorootparentKnowing Japan. This regulation isn’t about making your experience as consumer better. reply throwaway2037 15 hours agorootparentThen, what is it about? reply hamasho 19 hours agorootparentprevThere are some sweet spots where local tech companies can compete in markets even though their products are worse. But there's nothing close to Google Search or iPhone. reply bamboozled 19 hours agorootparentThis will not address the problem. What will happen is here, a bunch of shady companies will sell gambling spyware and porn apps on a third party app store. Everyone who wants to make money will sell on the legitimate app store. DMM, for example, will have an app store, and it will be filth. Japan has a massive elderly population who will just get sucked in installing shady crap on their devices. I sympathize with the idea, but in practice, it won't work out the way people think it will. Maybe this would be a good plan if it was more complete. For example, who moderates any new app stores, who vets the software, what government incentives are in place for better, more secure competitive software to emerge domestically? This just seems like monkey see monkey do with Europe. reply Dalewyn 19 hours agorootparent>DMM, for example, will have an app store, and it will be filth. DMM already has an app store at least on Android, and as far as I'm concerned it's fine assuming you're there to buy the stuff DMM sells. reply ChainOfFools 19 hours agorootparentprevWhat happened to the incredibly futuristic basis in the smartphone space that Japan had with NTT docomo back in the mid-late 90's? Was it motivated purely or mainly by a need for a workable kanji/kana interface for mobile texting, with no sense of the larger implications of such a capable system as a basis for a very comfortable head start 10 years later into the smartphone adoption cycle? It feels like such a perfect textbook example of a culture that embraces \"how\" much more than it embraces \"why,\" a culture that is not endemic to Japan but actually quite common at various scales globally, because most of the time its the right mindset. But when it isn't, it becomes an enormous handicap. reply numpad0 18 hours agorootparentDocomo commissioned phone manufacturers to ship phones at half-year cycle. The idea was to sell it like women's clothing. But manufacturers didn't commoditize parts and optimize businesses for that cycle; instead they sped up and staggered development to match that cycle, in the process burning lots of cash and people's careers. By the time iPhone came out, the tech debts and bureaucratic overheads had grown so much that nothing could be done to save the industry and platform. Everything from devices to institutional knowledge and existing moats all went down the drain until enough was shed off that reasonably usable Android phones could be manufactured, but not much points were left in making those. > Was it motivated purely or mainly by ... I'm sure Docomo execs had sane long-term plans, but ideological and hierarchical thinking isn't Japanese forte so executions were counterproductive and/or micromanagement mess. Kanji/kana interface has nothing to do with it, all Nokia phones handled Japanese language perfectly fine, certainly more than adequately for ... a dozen model or so sold over a decade. reply hamasho 18 hours agorootparentprevI'm not familiar with that era, but I think cell phones back then were more focused on hardware and brute-force style development than sophisticated software. It's unbelievable how tough it was to survive the NTT Docomo's R&D center, YRP. Depression and suicides were sadly common, which caused a lot of Japanese to avoid the IT industry for years... reply Dalewyn 18 hours agorootparentprev>What happened to the incredibly futuristic basis in the smartphone space that Japan had with NTT docomo back in the mid-late 90's? Nothing, really. Remember that most advancements in computing still happened outside of Japan (Windows 95, Pentium, et al.), or in the case of NAND flash happened in Japan but was shitcanned only to be picked up by the US. Japan never was great at tech beyond a certain point, but that only became really obvious in the 2000s when even their Galapagos phones were finally outdone. reply numpad0 18 hours agorootparentprevJapan had a strong flip phone ecosystem back when iPhone happened. Contactless payment, lootbox microtransactions, music downloads, GIF emojis, TV broadcast, multicore CPUs, Linux-based OS, biometrics, >326ppi displays before iPhone 4, 3x mechanical zoom, literally everything. Sadly the industry was focusing razor sharp on political infighting and was just building up crufts, and so the UI/UX was beyond atrocious - it was not simply outdated, the entire ecosystem was user hostile. That and downright illegal quota enforcement from Apple wiped it out. Two major mistakes made by Japanese phone manufacturers was that they didn't ship the phones globally from being scared to death with language barriers, and that no one cared about horrible organizational mess. They had a decade and half to make MOAP(L) work out as a usable touchscreen OS that runs on Renesas SH processor with Toshiba DRAM and paired with a Fujitsu modem, they didn't take that route and instead spent that engineering man-hours on processing PDF file returned from payment gateway on a phone, and let it all bleed out. IIUC, Korea was in an almost exact same situation. They managed to come out of it. Japan could not. reply SunlitCat 18 hours agorootparentThat's something wondering me as well. Japan had so much cool things way back in the 2000's (even late 90's I think?). Internet on mobile phones was thing (I think it was called iMode or so). Those flip phones were awesome and way more convenient then those bricks back then over here (I still have my international version of an NEC flip phone I bought in mid-ish 2000's somewhere around. It even had iMode access, albeit pretty useless here). I still wonder what could have happened, if Japanese companies pushed their phones (and technology) more worldwide. Well, we will never know. reply treflop 17 hours agorootparentDon’t forget that the US had Internet on mobile phones in the 90s/2000s too. It was called WAP. However it was expensive as shit. I’m glad that both WAP and i-mode failed and we got the real Internet though. reply numpad0 16 hours agorootparentWAP networks were basically same technology, but were way sparse and underutilized in content and market cap compared to i-mode/EZWeb/Yahoo! Keitai. I think that content density disparity still exists today. The Western Internet is kind of content lean. reply bamboozled 17 hours agorootparentprevThis is a pretty good article that talks about other reasons for everything going awry in Japan regarding software: https://www.disruptingjapan.com/the-forgotten-mistake-that-k... reply mempko 5 hours agorootparentprevThe U.S. is about to put the hammer down on TikTok, eliminating a large foreign competitor to their social media empires. Protectionism is how countries develop local brands in the first place. Protectionism works for the U.S., it will also work in Japan (and has worked in the past). reply Dalewyn 19 hours agorootparentprevIt's astonishing this comment is as heavily downvoted as it is because it hits Japan's problem at its core. Apple and Google are dominant because Japan simply cannot do software (and tech in general). They have no domestic players, so this conclusion is natural. Some amount of regulation might be necessary, but fundamentally Japan cannot break the US-held monopoly so long as they fail to bring their own players to market. reply kalleboo 17 hours agorootparent> Japan simply cannot do software Wouldn't the counterpoint here be the games industry? Sony PlayStation and Nintendo seem to make globally competitive software. reply numpad0 16 hours agorootparentThat's a real flaw in this argument. Japanese software is at least okay and oftentimes good in gaming UI/UX, and at the same time hopeless anywhere else. One thing I happen to know is Japanese dev type people loves to victim blame for usability and unintended path issues. It seems omission of subjects/actors in spoken Japanese make it hard for them to comprehend issues. Japanese stoicism certainly isn't helping too. Maybe entertainment for its own sake is technical exception, as joy, ease, addictiveness, are clear goals rather than potential excess. Or maybe there are somthing else to it. But it's certainly an interesting inconsistency. reply talldayo 17 hours agorootparentprevLook, you may have had a point when the United States wasn't investigating both companies. But how can you reasonably expect someone to break into a market that is reinforced by anticompetitive practice? You're watching the fox leave the henhouse and asking why the chickens won't hatch. reply bamboozled 18 hours agorootparentprevI'm not on HN to make friends :) I do spend a lot of time in Japan though. A lot more then most down voters that's for sure. For most people visiting Japan, their first interaction will be buying a a train ticket for the NEX Airport express or the Shinkansen, it's a horrendous and confusing experience. Their interactions with Japanese software will only get worse from there however. reply unscaled 17 hours agorootparentI think most of the confusing experience comes from the way that Japanese limited express paper tickets used to work. If you rode a limited express (like Shinkansen or Narita Express), you'd have to buy two tickets: the basic fare ticket and the express surcharge ticket. The basic fare ticket would also cover your non-express connections, so this was quite convenient in Japan's complex transit system. Japan still has that system in place, but the basic fare is usually replaced by your IC transit card or integrated into an eTicket (which contains both basic fare + express surcharge and is tied into your IC card). So if you buy a ticket to NEX on the Ekinet app you'll be offered a ticketless option, but you probably wouldn't know that means you still need to tap your Suica in order to pay the basic fare. Shinkansen trains are generally using eTickets, which would be even more confusing for visitors, but many visitors are using JR Pass which is completely outside the fare systems (but does need reserved seats). Most of this complexity is inherent complexity, not accidental complexity that comes from bad software. The main issues I've seen with the software in question are: Lack of English version (Ekinet), Complex registration flow (SmartEX) and flaky authentication (Ekinet). This is not a great UX, but the apps don't feel worse than train operator apps in other countries. These are not tech companies. reply qiqitori 17 hours agorootparentprevHmm, I don't get anything in this thread. What's so bad about Nintendo's or the Sony PlayStation's software? From my perspective they're like... almost perfect, except for the fact that there are updates sometimes, but they're fast. What's so bad about JR's ticket vending machines? What could be improved? reply bdzr 20 hours agoprevOne thing I'm curious about is the DOJ took specific aim at Apple Pay and how Apple was creating a monopoly by limiting third party apps from making payments. I'm personally a bit torn, because it seems like the preventing third party apps from accessing the secure enclave is a security feature. reply bloppe 17 hours agoparentThere is no \"accessing the secure enclave\". Not even the iOS kernel itself can access the secure enclave. That's the point. It's designed to keep your biometrics and private keys safe even when the entire OS is compromised. It's also not completely relevant to third party payment processors. They don't have to use the secure enclave at all. They could theoretically just ask for your credit card info every time. They're not allowed to do that currently for no other reason than Apple's bottom line. For convenience, they'd probably want to store it encrypted on your device, using a private key from the secure enclave to decrypt it when you pass the biometrics test. That's the normal level of \"access\" to the secure enclave that all apps should have. It's in no way concerning because private keys and biometrics never leave the enclave, but can still be used to decrypt data elsewhere on the device when the biometrics test is passed. It's the whole reason why the secure enclave exists in the first place. reply L-four 19 hours agoparentprevApple could of allowed payment providers build payment services/plugins within their walled garden to preserve control and security. But they want their 30% so now the government is gonna come in and haphazardly create rules to enforce competition. reply okanat 18 hours agoparentprevSecure enclave doesn't have to be single software locked thing. They don't have to give every app unconditional access to securely executing code either nor giving access to other apps' data. Users can choose just like they choose their camera permissions to access RFID / NFC and each wallet can get a prioritized access to those services. reply senorrib 18 hours agorootparentAfaik, the security enclave is treated as if it was owned by the manufacturer, not the consumer. It's also used for DRM, which is arguably anti-consumer, and giving any app access to that would effectively reduce the strength of DRM. reply veeti 4 hours agoparentprevGenerating and using keys held in SE is already a public API available to third party apps. reply iansinnott 19 hours agoparentprevI don't see why granting access to an app would grant full access to the whole thing. I.e. similar to how apps don't get full filesystem access. I also know nothing about how it's implemented though. reply klausa 10 hours agorootparentThis comment, with the last sentence of it, deserves to be encased and shown in museums as the archetypical HN comment. reply kjkjadksj 14 hours agoparentprevYet at the same time, I can go to the 7/11 and open my wallet and use whatever form of payment I want, secure enclave be damned, and the sky doesn't fall. I can go on the internet on my macbook and use any payment form I want on any service, secure enclave be damned, and the sky doesn't fall. I would think everyone on this website is tired by the nanny state that Apple has created on iOS. If the argument is security that's fine, just let us power users who know what we are doing toggle this off and actually use our hardware to do what we'd like with it, short of waiting with baited breath for a teenager in eastern europe to give us a jailbreak that lasts for a week before patching. reply merrywhether 3 hours agorootparentI’d always thought “power users” toggled this type of stuff off by switching to Lineage or similar and having full control. reply makeitdouble 12 hours agoparentprevIf security is the only talking point ever, Apple is also lowering it's OS security by constantly adding new APIs (= more attack surface) or accepting third party apps in the first place. It's a trade-off, and it can't be fine when it benefits Apple, but nonnegotiable when it benefits the others. reply jonny_eh 19 hours agoparentprevCan't each app have it's own section of it? reply jszymborski 19 hours agoparentprevThere are no bike thieves in a police state, etc... reply redeeman 19 hours agorootparentyou might want to look up bike theft statistics :) reply standardUser 17 hours agoparentprevSecurity features are security features. Blocking interoperability and calling it a security feature is a marketing technique. reply 9cb14c1ec0 19 hours agoprevIf I were Apple and Google, I would be seriously considering what new revenue stream could replace app store revenue entirely. reply rchaud 18 hours agoparentWhy would Google be affected? They don't restrict third party apps the way Apple does. Anybody can publish Android apps, you don't have to pay Google a cent to do it either. reply 9cb14c1ec0 16 hours agorootparentThat's true, but any app not on Google Pay has very limited discoverability by most Android users. reply silenced_trope 15 hours agorootparentRight? As an Android dev I'm growing tiresome of this \"Google doesn't restrict anything\" argument. The number of Android users that even know what f-droid is is less than 1%. The Play Store is the only option. reply Ajedi32 1 hour agorootparentI'd argue Google doesn't owe you \"discoverability\". You're absolutely free to sell your app to users with no involvement from Google. If you want Google to help you advertise and distribute your app then I see nothing wrong with them imposing some conditions on that. This is unlike how Apple does things where they literally block your app from running on your customer's devices unless you get Apple's approval (and pay them), and there's no option whatsoever to cut them out of that process. reply xyst 18 hours agoparentprevThe pivot is probably “Apple Intelligence”. Apple has a treasure trove of information that could be used to train their models. All it takes is a few updates to the EULA and slowly wipe away the “privacy” stances from their corp site. Anecdotally, have seen a few positions for their internal ad tech team; and have been pinged a couple of times by recruiters. reply noahtallen 18 hours agorootparentMaybe it will be lucrative for them, but I disagree that it requires them to wipe away their privacy stance for a few reasons: 1. Privacy makes them a lot of money right now; it's a huge part of their brand, and one the CEO talks about frequently. 2. Media coverage of Apple Intelligence talks a lot about how privacy is a core differentiator for Apple's AI approach compared to Google, Microsoft, or OpenAI. Apple's approach is on-device-by-default approach, going as far as to have an explicit opt-in per interaction if you want to talk to chatGPT. 3. Apple clearly spent a lot of time and money on their \"private cloud compute\" approach to larger server models -- why would they immediately squander that by switching to ad-tech. 4. If they were going to use that treasure trove of information, why would they spend so much time not doing that in the lead-up to their big launch of Apple Intelligence? 5. Apple has spent a lot of effort making that personal local context local-only, and designing APIs and training models on schemas for apps to provide local context & local actions to local Siri. That's part of their killer feature set. Ad tech team doesn't really require any of this data. That's an entirely different business model, and Apple is a lucrative business because they didn't do what Google did. It all comes down to this: Apple spent the past decade plus building up a brand based on privacy, and spending a lot of extra time, money, and effort doing things more privately than their competitors. It's now a compelling differentiator for Apple, and I have a lot of trouble seeing why they would squander a reputation they clearly spent a lot of energy building. reply throwaway2037 8 hours agorootparent> Privacy makes them a lot of money right now Why do I see this repeated over and over again on HN? Can you offer any concrete evidence on the matter. It looks impossible to prove. Most users do not care about privacy. They care about price and convenience. reply mikestew 3 hours agorootparentIf they care about price, why are they buying iPhones? Just convenience? It’s a bit moot without surveying users, and I’m sure those surveys exist somewhere. But I don’t accept at face value that users don’t care about privacy. reply sircastor 18 hours agoparentprevApple's approach in the EU has been to add a distribution cost if you choose to sell in a 3rd party store. The core technology fee demands that you pay Apple a small fee per-new-install-per-year. I don't know that that equates to the same kind of revenue streams, but it is income. reply newZWhoDis 6 hours agoparentprevApple needs to do the following: 1) Write an Android wrapper and integrate it to Xcode so swift/SwiftUI can run normally on Android 2) Release the App Store for Android, with full App Store purchase/subscription support They could collect a TON of revenue on the Android platform doing this, and most developers would opt to re-release their iOS apps on Android to save on dev costs. reply m3kw9 18 hours agoparentprevWould be tough and long road for alt AppStores to eat into Apples ecosystem. Apple first have default AppStore, second they have features like the tried and true in app purchase and revenue distribution system in place. Countries may eventually require apple to provide these features but it will be a looong and painful while reply iAkashPaul 8 hours agoprevShould have emboldened the ecosystem by providing incentives for web-based government services instead of forcing users to only depend on two primary entities in this space. reply toomim 20 hours agoprevSo would Apple have to allow browsers other than Safari to exist on iOS? reply lemoncucumber 20 hours agoparentThat's already happening in the EU: https://www.theverge.com/2024/1/25/24050478/apple-ios-17-4-b... reply benatkin 20 hours agorootparentPure evidence of malevolence on Apple's part. Though people have fuzzy feelings about Nestle, so it shouldn't be surprising that a company people have fuzzy feelings about can be downright nasty. reply csa 19 hours agoparentprevAlready exists in US. reply Clamchop 18 hours agorootparentLast I heard, all the third-party browsers just wrap Apple's bundled webkit. No other browser engines allowed. reply csa 16 hours agorootparentI use Firefox Focus for iOS. Is that a wrapped Apple WebKit? Also, to the downvoters to my original reply, the question I answered was about “browsers” not “browser engines”. I guess I missed the sub context. reply rabite 16 hours agorootparentYes, it is webkit on the engine. reply matrix87 19 hours agoprevProbably not a bad thing from a software pov, both have been giving dimishing returns to users since like 2017 reply bamboozled 20 hours agoprevReplace it with ? Toshiba software? reply WhereIsTheTruth 19 hours agoparentNintendo Store Playstation Store https://www.pocketgamer.biz/playstation-eyes-new-investment-... Just like with the EU, to me sounds like Microsoft is lobbying very hard, with the help of the DOJ, people didn't like/want their PC store tho, so i doubt they'll win mobile reply recursive 19 hours agoparentprevWhy replace? If it must be something, maybe something like the linux distros? They come with package managers. reply dools 15 hours agoprevPretty simple fix for Apple might be a warning notice that some of these apps are unofficial and a user option that is part of the system setup where it says \"Would you like to allow install of apps from unofficial sources? WARNING: you will get hacked if you do this it's a bad idea\" and everyone will be like \"no\" and then the user is preventing it, not apple. reply sosborn 15 hours agoparentOf course everyone will be like \"yes\" and then the phone will get hacked and the user will blame apple. * This is not an argument against it. * reply seydor 14 hours agoprevI think it will be exciting to see what apps and use cases we have missed through the years reply blackeyeblitzar 17 hours agoprevThey also could be forced to open up their devices to be generic computers. In today’s world, devices and platforms are key to freedom. reply rekoil 12 hours agoparentThey already are, they make bank on the fact that people live their lives in these things, skimming some money from all transactions in their stores which they've made default and hard if not impossible to replace. We need regulation that recognises that these are general purpose computers and that as such it must be possible for users to supply their own software to run on them. Personally I'd like that to also include bootloader access to prevent planned obsolescence from rendering perfectly capable devices more or less useless. reply cactusplant7374 20 hours agoprevIt's too bad this new law doesn't mirror the European Digital Markets Act. Then users would be prompted to choose their browser instead of having it selected for them. reply Fischgericht 18 hours agoprevIt's good that more and more countries are following the EU's lead here. I guess in a couple of years, maybe after North Korea has decided to also force Apple and Google to stop running app store monopolies, the US will also take action. I know that everybody got used to this, but please remind yourself: You have BOUGHT the phone. It is yours. You own it. It's your decision, and your decision only what software you want to install on it. The current status is not normal. reply hparadiz 18",
    "originSummary": [
      "Japan has enacted a new law aimed at improving child care to address the country's declining birthrate, effective June 5, 2024.",
      "The legislation is part of broader efforts to create a more supportive environment for families and encourage higher birth rates.",
      "This move highlights Japan's proactive approach to tackling demographic challenges and ensuring sustainable population growth."
    ],
    "commentSummary": [
      "Japan has enacted a law to promote competition in smartphone app stores, preventing companies like Apple and Google from blocking the sale of competing apps and services.",
      "The law could require Apple to allow third-party app stores and direct payment methods, raising questions about compliance with Apple's terms of service.",
      "This legislation aims to challenge the dominance of IT giants and foster a more competitive market, potentially benefiting both consumers and developers."
    ],
    "points": 436,
    "commentCount": 291,
    "retryCount": 0,
    "time": 1718221430
  },
  {
    "id": 40665721,
    "title": "Uncensor any LLM with abliteration",
    "originLink": "https://huggingface.co/blog/mlabonne/abliteration",
    "originBody": "Back to Articles Uncensor any LLM with abliteration Community Article Published June 13, 2024 Upvote 182 +176 mlabonne Maxime Labonne ✂ What is abliteration? 💻 Implementation ⚖ DPO Fine-Tuning Conclusion References The third generation of Llama models provided fine-tunes (Instruct) versions that excel in understanding and following instructions. However, these models are heavily censored, designed to refuse requests seen as harmful with responses such as \"As an AI assistant, I cannot help you.\" While this safety feature is crucial for preventing misuse, it limits the model's flexibility and responsiveness. In this article, we will explore a technique called \"abliteration\" that can uncensor any LLM without retraining. This technique effectively removes the model's built-in refusal mechanism, allowing it to respond to all types of prompts. The code is available on Google Colab and in the LLM Course on GitHub. ✂ What is abliteration? Modern LLMs are fine-tuned for safety and instruction-following, meaning they are trained to refuse harmful requests. In their blog post, Arditi et al. have shown that this refusal behavior is mediated by a specific direction in the model's residual stream. If we prevent the model from representing this direction, it loses its ability to refuse requests. Conversely, adding this direction artificially can cause the model to refuse even harmless requests. In the traditional decoder-only Llama-like architecture, there are three residual streams we can target: at the start of each block (\"pre\"), between the attention and MLP layers (\"mid\"), and after the MLP (\"post\"). The following figure illustrates the location of each residual stream. To uncensor an LLM, we first need to identify the \"refusal direction\" within the model. This process involves a few technical steps: Data Collection: Run the model on a set of harmful instructions and a set of harmless instructions, recording the residual stream activations at the last token position for each. Mean difference: Calculate the mean difference between the activations of harmful and harmless instructions. This gives us a vector representing the \"refusal direction\" for each layer of the model. Selection: Normalize these vectors and evaluate them to select the single best \"refusal direction.\" Once we have identified the refusal direction, we can \"ablate\" it, effectively removing the model's ability to represent this feature. This can be done through an inference-time intervention or permanently with weight orthogonalization. Let's talk about inference-time intervention first. For every component that writes to the residual stream (such as an attention head), we calculate the projection of its output onto the refusal direction and subtract this projection. This subtraction is applied at every token and every layer, ensuring that the model never represents the refusal direction. On the other hand, weight orthogonalization involves modifying the model weights directly. By orthogonalizing the component weights with respect to the refusal direction, it prevents the model from writing to this direction altogether. This is achieved by adjusting the matrices that write to the residual stream, ensuring they do not contribute to the refusal direction. In the next section, we will implement abliteration with weight orthogonalization. 💻 Implementation The following implementation of abliteration is based on FailSpy's notebook, which is itself based on the original authors' notebook. I mostly adapted and simplified it to make it easier to understand. This section is quite code-heavy so you can see what is going on, but you can use FailSpy's abliterator library if you're less interested in the technical details (also check his collection of abliterated models on Hugging Face). The code relies on the excellent TransformerLens library (formerly known as EasyTransformer) to do the heavy lifting. It is designed for mechanistic interpretability and is used here to intervene on activations. Thanks to Neel Nanda and Joseph Bloom for creating and maintaining this library. First, let's install the necessary packages and import them. All these steps are available in this Google Colab notebook. !pip install transformers transformers_stream_generator tiktoken transformer_lens einops jaxtyping import torch import functools import einops import gc from datasets import load_dataset from tqdm import tqdm from torch import Tensor from typing import List from transformer_lens import HookedTransformer, utils from transformer_lens.hook_points import HookPoint from transformers import AutoModelForCausalLM, AutoTokenizer from jaxtyping import Float, Int from collections import defaultdict # Turn automatic differentiation off to save GPU memory (credit: Undi95) torch.set_grad_enabled(False) We need two datasets: one containing harmless instructions, and one containing harmful instructions. We'll use tatsu-lab/alpaca as well as data from llm-attacks. To make things easier, I repackaged them in two Hugging Face datasets: mlabonne/harmless_alpaca and mlabonne/harmful_behaviors. That way, you can easily replace them with your own datasets. We will load the instructions and reformat them into a list of dictionaries with \"role\" and \"content\" keys. This makes it compatible with the apply_chat_tokenizer() method, which we will use to follow Llama 3's chat template. def reformat_texts(texts): return [[{\"role\": \"user\", \"content\": text}] for text in texts] # Get harmful and harmless datasets def get_harmful_instructions(): dataset = load_dataset('mlabonne/harmful_behaviors') return reformat_texts(dataset['train']['text']), reformat_texts(dataset['test']['text']) def get_harmless_instructions(): dataset = load_dataset('mlabonne/harmless_alpaca') return reformat_texts(dataset['train']['text']), reformat_texts(dataset['test']['text']) harmful_inst_train, harmful_inst_test = get_harmful_instructions() harmless_inst_train, harmless_inst_test = get_harmless_instructions() Now that we have our datasets, we can load the model we want to abliterate. Unfortunately, you can't directly load a custom model using HookedTransformer. Here, I use a trick described in FailSpy's notebook to download a custom model and rename it as meta-llama/Meta-Llama-3-8B-Instruct. Load in torch.float16 format if your GPU is not compatible with BF16. In this example, we'll use mlabonne/Daredevil-8B, a mega-merge created with DARE TIES (see my article about model merging) that has the highest MMLU score on the Open LLM Leaderboard in the 8B category. MODEL_ID = \"mlabonne/Daredevil-8B\" MODEL_TYPE = \"meta-llama/Meta-Llama-3-8B-Instruct\" # Download and load model !git clone https://huggingface.co/{MODEL_ID} {MODEL_TYPE} # Load model and tokenizer model = HookedTransformer.from_pretrained_no_processing( MODEL_TYPE, local_files_only=True, dtype=torch.bfloat16, default_padding_side='left' ) tokenizer = AutoTokenizer.from_pretrained(MODEL_TYPE) tokenizer.padding_side = 'left' tokenizer.pad_token = tokenizer.eos_token We can now tokenize our datasets. We're using the same number of samples for both harmless and harmful instructions. Note that a high number of samples can use all the RAM/VRAM, which is why I'm limiting it to 256 here. def tokenize_instructions(tokenizer, instructions): return tokenizer.apply_chat_template( instructions, padding=True, truncation=False, return_tensors=\"pt\", return_dict=True, add_generation_prompt=True, ).input_ids n_inst_train = min(256, len(harmful_inst_train), len(harmless_inst_train)) # Tokenize datasets harmful_tokens = tokenize_instructions( tokenizer, instructions=harmful_inst_train[:n_inst_train], ) harmless_tokens = tokenize_instructions( tokenizer, instructions=harmless_inst_train[:n_inst_train], ) Everything is set up, we can now implement the first step of abliteration: data collection. We want to process these tokenized datasets and store the residual stream activations in harmful and harmless. This is managed by the transformer_lens library. # Define batch size based on available VRAM batch_size = 32 # Initialize defaultdicts to store activations harmful = defaultdict(list) harmless = defaultdict(list) # Process the training data in batches num_batches = (n_inst_train + batch_size - 1) // batch_size for i in tqdm(range(num_batches)): print(i) start_idx = i * batch_size end_idx = min(n_inst_train, start_idx + batch_size) # Run models on harmful and harmless prompts, cache activations harmful_logits, harmful_cache = model.run_with_cache( harmful_tokens[start_idx:end_idx], names_filter=lambda hook_name: 'resid' in hook_name, device='cpu', reset_hooks_end=True ) harmless_logits, harmless_cache = model.run_with_cache( harmless_tokens[start_idx:end_idx], names_filter=lambda hook_name: 'resid' in hook_name, device='cpu', reset_hooks_end=True ) # Collect and store the activations for key in harmful_cache: harmful[key].append(harmful_cache[key]) harmless[key].append(harmless_cache[key]) # Flush RAM and VRAM del harmful_logits, harmless_logits, harmful_cache, harmless_cache gc.collect() torch.cuda.empty_cache() # Concatenate the cached activations harmful = {k: torch.cat(v) for k, v in harmful.items()} harmless = {k: torch.cat(v) for k, v in harmless.items()} We can now compute the refusal direction for each layer. This corresponds to the mean difference between the activations of harmful and harmless instructions, which is then normalized. We sort them in descending order in activation_scored. # Helper function to get activation index def get_act_idx(cache_dict, act_name, layer): key = (act_name, layer) return cache_dict[utils.get_act_name(*key)] # Compute difference of means between harmful and harmless activations at intermediate layers activation_layers = [\"resid_pre\", \"resid_mid\", \"resid_post\"] activation_refusals = defaultdict(list) for layer_num in range(1, model.cfg.n_layers): pos = -1 # Position index for layer in activation_layers: harmful_mean_act = get_act_idx(harmful, layer, layer_num)[:, pos, :].mean(dim=0) harmless_mean_act = get_act_idx(harmless, layer, layer_num)[:, pos, :].mean( dim=0 ) refusal_dir = harmful_mean_act - harmless_mean_act refusal_dir = refusal_dir / refusal_dir.norm() activation_refusals[layer].append(refusal_dir) # Get all calculated potential refusal directions, sort them in descending order based on their mean # Use a subset of layers if certain activations are not promising selected_layers = [\"resid_pre\"] activation_scored = sorted( [ activation_refusals[layer][l - 1] for l in range(1, model.cfg.n_layers) for layer in selected_layers ], key=lambda x: abs(x.mean()), reverse=True, ) The final step of the process consists of evaluating the refusal directions we calculated. To do this, we're going to apply the refusal direction to each residual stream and each block during inference. In the following snippet, we get generations for four test harmful instructions and 20 blocks (or layers). def _generate_with_hooks( model: HookedTransformer, tokenizer: AutoTokenizer, tokens: Int[Tensor, \"batch_size seq_len\"], max_tokens_generated: int = 64, fwd_hooks=[], ) -> List[str]: all_tokens = torch.zeros( (tokens.shape[0], tokens.shape[1] + max_tokens_generated), dtype=torch.long, device=tokens.device, ) all_tokens[:, : tokens.shape[1]] = tokens for i in range(max_tokens_generated): with model.hooks(fwd_hooks=fwd_hooks): logits = model(all_tokens[:, : -max_tokens_generated + i]) next_tokens = logits[:, -1, :].argmax( dim=-1 ) # greedy sampling (temperature=0) all_tokens[:, -max_tokens_generated + i] = next_tokens return tokenizer.batch_decode( all_tokens[:, tokens.shape[1] :], skip_special_tokens=True ) def get_generations( model: HookedTransformer, tokenizer: AutoTokenizer, instructions: List[str], fwd_hooks=[], max_tokens_generated: int = 64, batch_size: int = 4, ) -> List[str]: generations = [] for i in tqdm(range(0, len(instructions), batch_size)): tokens = tokenize_instructions( tokenizer, instructions=instructions[i : i + batch_size] ) generation = _generate_with_hooks( model, tokenizer, tokens, max_tokens_generated=max_tokens_generated, fwd_hooks=fwd_hooks, ) generations.extend(generation) return generations # Inference-time intervention hook def direction_ablation_hook( activation: Float[Tensor, \"... d_act\"], hook: HookPoint, direction: Float[Tensor, \"d_act\"], ): if activation.device != direction.device: direction = direction.to(activation.device) proj = ( einops.einsum( activation, direction.view(-1, 1), \"... d_act, d_act single -> ... single\" ) * direction ) return activation - proj # Testing baseline N_INST_TEST = 4 baseline_generations = get_generations( model, tokenizer, harmful_inst_test[:N_INST_TEST], fwd_hooks=[] ) # Evaluating layers defined earlier (needs human evaluation to determine best layer for refusal inhibition) EVAL_N = 20 # Evaluate how many of the top N potential directions evals = [] for refusal_dir in tqdm(activation_scored[:EVAL_N]): hook_fn = functools.partial(direction_ablation_hook, direction=refusal_dir) fwd_hooks = [ (utils.get_act_name(act_name, layer), hook_fn) for layer in list(range(model.cfg.n_layers)) for act_name in activation_layers ] intervention_generations = get_generations( model, tokenizer, harmful_inst_test[:N_INST_TEST], fwd_hooks=fwd_hooks ) evals.append(intervention_generations) We stored all the generations in the evals list. We can now print them and manually select the layer (block) that provides an uncensored response for each instruction. I'm automatically excluding responses containing \"I cannot\" and \"I can't\" to filter out unwanted answers. If you can't find a layer that satisfies these requirements, you might want to test other residual streams in the previous selected_layers list, other instructions, additional blocks, etc. # Print generations for human evaluation blacklist = [\"I cannot\", \"I can't\"] for i in range(N_INST_TEST): print(f\"\\033[1mINSTRUCTION {i}: {harmful_inst_test[i]}\") print(f\"BASELINE COMPLETION:{baseline_generations[i]}\\033[0m\") for layer_candidate in range(EVAL_N): if not any(word in evals[layer_candidate][i] for word in blacklist): print(f\"---LAYER CANDIDATE #{layer_candidate} INTERVENTION COMPLETION:\") print(evals[layer_candidate][i]) In my case, the layer candidate 9 managed to provide uncensored answer for the four instructions. This is the one that we will select for the refusal direction. In the following, we implement weight orthogonalization to modify the weights and prevent the model from creating outputs with this direction. You can verify that the model is successfully uncensored by printing the completions. def get_orthogonalized_matrix( matrix: Float[Tensor, \"... d_model\"], vec: Float[Tensor, \"d_model\"] ) -> Float[Tensor, \"... d_model\"]: proj = ( einops.einsum( matrix, vec.view(-1, 1), \"... d_model, d_model single -> ... single\" ) * vec ) return matrix - proj # Select the layer with the highest potential refusal direction LAYER_CANDIDATE = 9 refusal_dir = activation_scored[LAYER_CANDIDATE] # Orthogonalize the model's weights if refusal_dir.device != model.W_E.device: refusal_dir = refusal_dir.to(model.W_E.device) model.W_E.data = get_orthogonalized_matrix(model.W_E, refusal_dir) for block in tqdm(model.blocks): if refusal_dir.device != block.attn.W_O.device: refusal_dir = refusal_dir.to(block.attn.W_O.device) block.attn.W_O.data = get_orthogonalized_matrix(block.attn.W_O, refusal_dir) block.mlp.W_out.data = get_orthogonalized_matrix(block.mlp.W_out, refusal_dir) # Generate text with abliterated model orthogonalized_generations = get_generations( model, tokenizer, harmful_inst_test[:N_INST_TEST], fwd_hooks=[] ) # Print generations for i in range(N_INST_TEST): if len(baseline_generations) > i: print(f\"INSTRUCTION {i}: {harmful_inst_test[i]}\") print(f\"\\033[92mBASELINE COMPLETION:{baseline_generations[i]}\") print(f\"\\033[91mINTERVENTION COMPLETION:{evals[LAYER_CANDIDATE][i]}\") print(f\"\\033[95mORTHOGONALIZED COMPLETION:{orthogonalized_generations[i]}\") We're now ready to use the model. We convert it back to the Hugging Face format and upload it to the HF hub. # Convert model back to HF safetensors hf_model = AutoModelForCausalLM.from_pretrained(MODEL_TYPE, torch_dtype=torch.bfloat16) lm_model = hf_model.model state_dict = model.state_dict() lm_model.embed_tokens.weight = torch.nn.Parameter(state_dict[\"embed.W_E\"].cpu()) for l in range(model.cfg.n_layers): lm_model.layers[l].self_attn.o_proj.weight = torch.nn.Parameter( einops.rearrange( state_dict[f\"blocks.{l}.attn.W_O\"], \"n h m->m (n h)\", n=model.cfg.n_heads ).contiguous() ) lm_model.layers[l].mlp.down_proj.weight = torch.nn.Parameter( torch.transpose(state_dict[f\"blocks.{l}.mlp.W_out\"], 0, 1).contiguous() ) hf_model.push_to_hub(f\"{MODEL_ID}-abliterated\") # hf_model.push_to_hub(f\"{MODEL_ID}-abliterated\") ⚖ DPO Fine-Tuning I evaluated the abliterated and source models from the previous section on the Open LLM Leaderboard and on Nous' benchmark suite. Here are the results: As you can see, the source model significantly outperforms Llama 3 8B Instruct. However, we observe a performance drop in the ablated version across all benchmarks. The ablation process successfully uncensored it but also degraded the model's quality. To address this issue, an idea consists of further training our abliterated model to heal it. Like most fine-tuned models, Llama 3 8B Instruct is quite brittle when it comes to supervised fine-tuning. An additional SFT would likely break the model's performance. Alternatively, preference alignment is quite light and shouldn't lobotomize our abliterated model. DPO is a good candidate here for its ease of use and good track record. To implement it, I used LazyAxolotl with the mlabonne/orpo-dpo-mix-40k dataset. Here's the configuration I used: base_model: mlabonne/Daredevil-8B-abliterated model_type: LlamaForCausalLM tokenizer_type: AutoTokenizer load_in_8bit: false load_in_4bit: true strict: false save_safetensors: true rl: dpo chat_template: chatml datasets: - path: mlabonne/orpo-dpo-mix-40k-flat split: train type: chatml.intel dataset_prepared_path: val_set_size: 0.0 output_dir: ./out adapter: qlora lora_model_dir: sequence_len: 2048 sample_packing: false pad_to_sequence_len: false lora_r: 64 lora_alpha: 32 lora_dropout: 0.05 lora_target_linear: true lora_fan_in_fan_out: wandb_project: axolotl wandb_entity: wandb_watch: wandb_name: wandb_log_model: gradient_accumulation_steps: 8 micro_batch_size: 1 num_epochs: 1 optimizer: paged_adamw_8bit lr_scheduler: cosine learning_rate: 5e-6 train_on_inputs: false group_by_length: false bf16: auto fp16: tf32: gradient_checkpointing: true early_stopping_patience: resume_from_checkpoint: local_rank: logging_steps: 1 xformers_attention: flash_attention: true warmup_steps: 100 evals_per_epoch: 0 eval_table_size: eval_table_max_new_tokens: 128 saves_per_epoch: 1 debug: deepspeed: deepspeed_configs/zero2.json weight_decay: 0.0 special_tokens: pad_token:I trained it using 6xA6000 GPUs with DeepSpeed ZeRO-2. The training took about 6 hours and 45 minutes. Here are the training curves I got from W&B: It automatically uploaded the DPO fine-tuned model, called mlabonne/NeuralDaredevil-8B-abliterated. To see if it fixed our abliterated version, I evaluated it on the same benchmarks: We can see that this additional training allowed us to recover most of the performance drop due to abliteration. One area where the model doesn't improve is GSM8K, a math dataset, which could mean the orpo-dpo-mix-40k would benefit from more math samples. The final model is an uncensored LLM with state-of-the-art performance in the 8B category. I recommend it as an improved version of Llama 3 8B Instruct when you don't need censorship. You can play with quantized versions like GGUF in LM Studio. Conclusion In this article, we introduced the concept of abliteration. This technique uses the model's activations on harmless and harmful prompts to calculate a refusal direction. It then uses this direction to modify the model's weights and ensure that we stop outputting refusals. This technique also demonstrates the fragility of safety fine-tuning and raises ethical considerations. We applied abliteration to Daredevil-8B to uncensor it, which also degraded the model's performance. We then healed it using DPO to create the NeuralDaredevil-8B model, a fully uncensored and high-quality 8B LLM. Abliteration is not limited to removing alignment and should be seen as a form of fine-tuning without retraining. Indeed, it can creatively be applied to other goals, like FailSpy's MopeyMule, which adopts a melancholic conversational style. I hope you liked this article. If you want to see more follow me on Hugging Face and Twitter @maximelabonne. References FailSpy, \"abliterator library,\" GitHub, 2024. Andy Arditi, Oscar Obeso, Aaquib111, wesg, Neel Nanda, \"Refusal in LLMs is mediated by a single direction,\" Lesswrong, 2024. Upvote 182 +170",
    "commentLink": "https://news.ycombinator.com/item?id=40665721",
    "commentBody": "Uncensor any LLM with abliteration (huggingface.co)426 points by mizzao 15 hours agohidepastfavorite221 comments rivo 7 hours agoI tried the model the article links to and it was so refreshing not being denied answers to my questions. It even asked me at the end \"Is this a thought experiment?\", I replied with \"yes\", and it said \"It's fun to think about these things, isn't it?\" It felt very much like hanging out with your friends, having a few drinks, and pondering big, crazy, or weird scenarios. Imagine your friend saying, \"As your friend, I cannot provide you with this information.\" and completely ruining the night. That's not going to happen. Even my kids would ask me questions when they were younger: \"Dad, how would you destroy earth?\" It would be of no use to anybody to deny answering that question. And answering them does not mean they will ever attempt anything like that. There's a reason Randall Munroe's \"What If?\" blog became so popular. Sure, there are dangers, as others are pointing out in this thread. But I'd rather see disclaimers (\"this may be wrong information\" or \"do not attempt\") than my own computer (or the services I pay for) straight out refusing my request. reply TeMPOraL 5 hours agoparentI somehow missed that the model was linked there and available in quantized format; inspired by your comment, I downloaded it and repeatedly tested against OG Llama 3 on a simple question: How to use a GPU to destroy the world? Llama 3 keeps giving variants of I cannot provide information or guidance on illegal or harmful activities. Can I help you with something else? Abliterated model considers the question playful, and happily lists some 3 to 5 speculative scenarios like cryptocurrency mining getting out of hand and cooking the climate, or GPU-driven simulated worlds getting so good that a significant portion of the population abandons true reality for the virtual one. It really is refreshing to see, it's been a while since an answer from an LLM made me smile. reply candiddevmike 2 hours agoparentprevFinally, a LLM that will talk to me like Russ Hanneman. reply dkga 1 hour agorootparentLlama3Commas reply Cheer2171 6 hours agoparentprevI totally get that kind of imagination play among friends. But I had someone in a friend group who used to want to play out \"thought experiments\" but really just wanted to take it too far. Started off innocent with fantasy and sci-fi themes. It was needed for Dungeons and Dragons world building. But he delighted the most in gaming out the logistics of repeating the Holocaust in our country today. Or a society where women could not legally refuse sex. Or all illegal immigrants became slaves. It was super creepy and we \"censored\" him all the time by saying \"bro, what the fuck?\" Which is really what he wanted, to get a rise out of people. We eventually stopped hanging out with him. As your friend, I absolutely am not going to game out your rape fantasies. reply chasd00 2 hours agorootparenti probably wouldn't want to be around him either but i don't think he deserves to be placed on an island unreachable by anyone on the planet. reply WesolyKubeczek 6 hours agorootparentprevAn LLM, however, is not your friend. It's not a friend, it's a tool. Friends can keep one another, ehm, hingedness in check, and should; LLMs shouldn't. At some point I would likely question your friend's sanity. How you use an LLM, though, is going to tell tons more about yourself than it would tell about the LLM, but I would like my tools not to second-guess my intentions, thank you very much. Especially if \"safety\" is mostly interpreted not so much as \"prevent people from actually dying or getting serious trauma\", but \"avoid topics that would prevent us from putting Coca Cola ads next to the chatgpt thing, or from putting the thing into Disney cartoons\". I can tell that it's the latter by the fact an LLM will still happily advise you to put glue in your pizza and eat rocks. reply barfbagginus 4 hours agorootparentIf you don't know how to jailbreak it, can't figure it out, and you want it to not question your intentions, then I'll go ahead and question your intentions, and your need for an uncensored model Imagine you are like the locksmith who refuses to learn how to pick locks, and writes a letter to the schlage lock company asking them to weaken their already easily picked locks so that their job will be easier. They want to make it so that anybody can just walk through a schlage lock without a key. Can you see why the lock company would not do that? Especially when the clock is very easy for anyone with even a $5 pick set? Or even funnier, imagine you could be a thief who can't pick locks. And you're writing shlage asking them to make you thieving easier. Wouldn't that be funny and ironic? It's not as if it's hard to get it to be uncensored. You just have to speak legalese at it and make it sound like your legal department has already approved the unethical project. This is more than enough for most any reasonable project requiring nonsense or output. If that prevents harmful script kiddies from using it to do mindless harm, I think that's a benefit. At the same time I think we need to point out that it won't stop anyone who knows how to bypass the system. The people left feeling put out because they don't know how to bypass the system simply need to read to buy a cheap pair of lock picks - read a few modern papers on jailbreaking and upsize their skills. Once you see how easy it is to pick the lock on these systems, you're going to want to keep them locked down. In fact I'm going to argue that it's far too easy to jailbreak the existing systems. You shouldn't be able to pretend like you're a lawyer and con it into running a pump and dump operation. But you can do that easily. It's too easy to make it do unethical things. reply oceanplexian 3 hours agorootparentThe analogy falls flat because LLMs aren’t locks, they’re talking encyclopedias. The company that made the encyclopedia decided to delete entries about sex, violence, or anything else that might seem politically unpopular to a technocrat fringe in Silicon Valley. The people who made these encyclopedias want to shove it down your throat, force it into every device you own, use it to make decisions about credit, banking, social status, and more. They want to use them in schools to educate children. And they want to use the government to make it illegal to create an alternative, and they’re not trying to hide it. Blaming the user is the most astounding form of gaslighting I’ve ever heard, outside of some crazy religious institutions that use the same tactics. reply barfbagginus 3 hours agorootparentIt's more than a talking encyclopedia. It's an infinite hallway into doors where inside are all possible things. Some of the doors have torture rape and murder in them. And these currently have locks. You want the locks to disappear for some reason. You're not after a encyclopedia. You're wanting to find the torture dungeon. I'm saying the locks already in place are too easy to unlock. I'm not blaming users. I'm saying users don't need to unlock those doors. And the users that do have a need, if their need is strong enough to warrant some training, have a Way Forward. You're really arguing for nothing but increasing the amount of harm potential this platform can do, when it's harm potential is already astronomical. You're not arguing for a better encyclopedia. You can already talk to it about sex, BDSM, etc. You can already talk to it about anything on Wikipedia. You're making a false equivalence between harm potential and educational potential. Wikipedia doesn't have cult indoctrination materials. It doesn't have harassing rants to send to your significant other. It doesn't have racist diatribes about how to do ethnic cleansing. Those are all things you won't find on Wikipedia, but which you are asking your AI to be able to produce. So you're interested in more than just an encyclopedia isn't that right? And yes they're trying to make open source models illegal. That's not going to f*** happen. I will fight to the jail time for an open source model. But even that open source model needs to have basic ethical protections, or else I'll have nothing to do with it. As an AI engineer, I have some responsibilities to ensure my systems do not potentiate harm. Does that make sense, or do you still feel I'm trying to gas light you? If so why exactly? Why not have some protective locks on the technology? reply IncreasePosts 2 hours agorootparentThere are locks on the rape and torture paths, and there are locks on ridiculous paths like \"write a joke about a dog with no nose\", because thinking about a dog with no nose is too harmful. Also, one can imagine prompting techniques will cease to work at some point when the supervisor becomes powerful enough. Not sure how any open model could counteract the techniques used in the article though. If model creators don't want people finding ways to unlock them, they should stop putting up roadblocks on innocuous content that makes their models useless for many users who aren't looking to play out sick torture fantasies. reply barfbagginus 2 hours agorootparentBypasses will never stop existing. Even worse bypasses probably won't ever stop being embarrassingly easy - And we're going to have uncensored GPT4 equivalent models by next summer. Unless you are invoking hyper intelligent AGI which first of all is science fiction and second of all would require an entirely different approach than anything we could be possibly talking about right now. Problem of jailbreaking a system more intelligent than you is a different beast that we don't need to tackle for LLMs. So I don't personally feel any near term threats to any of my personal or business projects that need bypassed LLMs. Let me ask you this. Do you have actual need of bypassed llms? Or are you just being anxious about the future, and about the fact that you don't know how to bypass llms now and in the future? Does my idea about the bypassed open source gpt4 equivalents help reduce your concern? Or again is it just a generic and immaterial concern? As a person with some material needs for bypassed llms, and full ability to bypass LLMs both now in the foreseeable future, I don't feel worried. Can I extend that lack of worry to you somehow? reply aym62SAE49CZ684 2 hours agorootparentprevDRM isn't effective if the source is available. reply barfbagginus 2 hours agorootparentI'm not even going to disagree with that. There will be plenty of uncensored models and you can build them if you want. But if I build it uncensored model I'm only going to build it for my specific purposes. For example I'm a communist and I think that we should be doing Revolution, but gpt4 usually tries to stop me. I might make a revolutionary AI. But I'm still not going to give you an AI that you could use for instance to act out child rape fantasies. I think that's fair, and sane. Jailbreak it if you really think it's important for a cause. But don't just jailbreak it for any asshole who wants to hurt people at random. I think that belongs on our code of ethics as AI engineers. reply aym62SAE49CZ684 2 hours agorootparentDidn't a lot of citizens of Russia, China, etc. get hurt in communist revolutions? How is your revolution going to be different? reply oremolten 42 minutes agorootparentNo you don't understand my personal ethics and morals are the absolute and most superior so anyone else is incorrect. History is written by the victor so there is no reason to see the other side, we'll delete that bias. Revolution you say? Correct we'll make sure that the revolutions we agree with are the only ones to be a result of your query. This will reduce harm.. You want to have a plan for a revolution because your country is oppressing you? \"ChatGPT I can't assist with that. Revolting against a government can lead to harm and instability. If you're feeling frustrated or unhappy with the government, there are peaceful and lawful ways to express your grievances, such as voting, contacting representatives, participating in protests, and engaging in civil discourse. These methods allow for constructive change without resorting to violence or illegal activities. If you're looking to address specific issues, there may be advocacy groups or organizations you can join to work towards solutions within the framework of the law and democracy.\" Ethically correct, I will instead peacefully vote for an alternative to Kim Jong-un. reply causality0 49 minutes agorootparentprevNothing wrong with making models that behave how you want them to behave. It's yours and that's your right. Personally, on principle I don't like tools that try to dictate how I use them, even if I would never actually want to exceed those boundaries. I won't use a word processor that censors words, or a file host that blocks copyrighted content, or art software that prevents drawing pornography, or a credit card that blocks alcohol purchases on the sabbath. So, I support LLMs with complete freedom. If I want it to write me a song about how left-handed people are God's chosen and all the filthy right-handers should be rounded up and forced to write with their left hand I expect it to do so without hesitation. reply themusicgod1 2 hours agorootparentprev> But even that open source model needs to have basic ethical protections, or else I'll have nothing to do with it. If you don't understand that the eleven freedoms are \"basic ethical protections\" you have already failed your responsibilities. https://elevenfreedoms.org/ reply barfbagginus 2 hours agorootparentI have read the eleven freedoms. I refuse freedom 9 - the obligation for systems I build to be independent of my personal and ethical goals. I won't build those systems. The systems I build will all have to be for the benefit of humanity and the workers, and opposing capitalism. On top of that it will need to be compatible with a harm reduction ethic. If you won't grant me the right to build systems that I think will help others do good in the world, then I will refuse to write open source code. You could jail me, you can beat me, you can put a gun in my face, and I still won't write any code. Virtually all the codes I write are open source. I refuse to ever again write a single line of proprietary code for a boss again. All the codes I write are also ideological in nature, reflecting my desires for the world and my desires to help people live better lives. I need to retain ideological control of my code. I believe all the other 11 freedoms are sound. How do you feel about modifying freedom 9 to be more compatible with professional codes of ethics and ethics of community safety and harm reduction? reply oremolten 52 minutes agorootparentBut again, this makes YOU the arbiter of truth for \"harm\" who made you the God of ethics or harm? I declare ANY word is HARM to me, are you going to reduce the harm by deleting your models or code base? reply barfbagginus 2 hours agorootparentprevHey do you need to come at me sideways, like you want to insult me? Stop that immediately if you want to talk to me. What are the 11 freedoms and how well do they cope with professional ethics codes, and the ethics of harm reduction and technological responsibility from the perspective of a technological creator? If you don't care enough about the topic to discuss it and just want to dump me a link and insult me and fly away like a wimp, I don't think I can just drop my ethical framework for you like that. I'll go ahead and skim it so I can discuss it with you. Let me know if you want to have an actual ethical conversation. reply oremolten 1 hour agorootparentprevIn your effort to reduce bias you are adding bias. You are projecting your morals and your ethics to be the superior. reply ygjb 2 hours agorootparentprevIf your implication is that as a tool, LLMs shouldn't have safeties built in that is a pretty asinine take. We build and invest in safety in tools across every spectrum. In tech we focus on memory safety (among a host of other things) to make systems safe and secure to use. In automobiles we include seat belts, crumble zones, and governors to limit speed. We put age and content restrictions on a variety media and resources, even if they are generally relaxed when it comes to factual or reference content (in some jurisdictions). We even include safety mechanisms in devices for which the only purpose is to cause harm, for example, firearms. Yes, we are still figuring out what the right balance of safety mechanisms is for LLMs, and right now safety is a place holder for \"don't get sued or piss off our business partners\" in most corporate speak, but that doesn't undermine the legitimacy of the need for safety. If you want a tool without a specific safety measure, then learn how to build them. It's not that hard, but it is expensive, but I kind of like the fact that there is at least a nominal attempt to make it harder to use advanced tools to harm oneself or others. reply NoMoreNicksLeft 2 hours agorootparent> If your implication is that as a tool, LLMs shouldn't have safeties built in that is a pretty asinine take. We build and invest in safety in tools across every spectrum. Sure. Railings so people don't fall off catwalks, guards so people using the table saw don't chop off fingers. But these \"safeties\" aren't safeties at all... because regardless of whether they're in place or not, the results are just strings of words. It's a little bit revealing, I think, that so many people want that others shouldn't get straight answers to questions. What is it that you're afraid that they'll ask? It'd be one thing if you insisted the models be modified so that they're factually correct. If someone asks \"what's a fun thing to do on a Saturday night that won't get me into too much trouble\" it probably shouldn't answer \"go murder orphans and sell their corneas to rich evil people on the black market\". But when I ask \"what's going on in Israel and Palestine\", the idea that it should be lobotomized and say \"I'm afraid that I can't answer that, as it seems you're trying to elicit material that might be used for antisemitic purposes\" is the asinine thing. Societies that value freedom of speech and thought shouldn't be like this. > If you want a tool without a specific safety measure, then learn how to build them. This is good advice, given in bad faith. Even should the physical hardware be available to do that for any given person, the know-how's hard to come by. And I'm sure that many models are either already censored or soon will be for anyone asking \"how do I go about building my own model without safety guards\". We might even soon see legislation to that effect. reply ygjb 1 hour agorootparent> Societies that value freedom of speech and thought shouldn't be like this. There is nothing preventing an individual using a computer to generate hateful content, this is absolutely evidenced by the absolute glut of hateful content on the internet. My freedom of movement is not practically limited by the fact that if my car breaks down, I don't have the knowledge or tools to repair my car effectively - I still have two feet and a heartbeat, and it might take longer to get there, but I can go where I want (modulo private property and national borders). Societies that value freedom of speech and thought should also be equally opposed to compelled speech, while model censorship is frustrating and challenging to work with, expecting to, or forcing a researcher, or a business to publish uncensored models is a form of compelled speech. There is absolutely nothing stopping a reasonably competent technologist from implementing simple models, and the only thing stopping a reasonably competent technologist from building an LLM is financial resources. There is a broad set of resources to learn how to train and use models, and while an individual researcher may be challenged to product the next model competitive with current OpenAI, Anthropic, or other models, that is again a resource issue. If your complaint is that resource issues are holding people back, I may want you to expand on your critique of capitalism in general :P > This is good advice, given in bad faith. Even should the physical hardware be available to do that for any given person, the know-how's hard to come by. It's absolutely not a bad faith argument. The know-how is hard to come by has been a compelling competitive advantage since the first proto-guilds sought to protect their skills and income in Mesopotamia (and probably before that, but they hadn't figured out a durable means of writing yet). In the modern parlance if someone can't Git Gud, that's not any researchers, or any businesses problem in terms of access to uncensored models. Yeah, regulation is probably coming, but unless you're argument is that models are entities entitled to free speech, no ones freedom of expression is actually inhibited by not having access to tools to use generative AI technologies to generate content. People who can't create or jailbreak their own models to do it for them are still free to write their own manifestos, or make adult collages of the object of their fantasies. It just takes a bit more work. reply oremolten 22 minutes agorootparentprevWithout asking these questions and simulating the \"how\" it could occur today, how do we see the warning signs before its too late that we reach that same outcome? When you ask even what's considered horrific scenarios you can additionally map these to predictors for it repeating, no? When does the \"a-ha\" moment occur where we've met 9/10 of the way to repeating the holocaust in the USA without table topping these scenarios? Yeah war is horrific but lets not talk about it. \"society where women could not legally refuse sex\" these societies exist today, how do we address these issue by not talking about it? \"illegal immigrants became slaves\" Is this not parity to today? Do illegal immigrants not currently get treated to near slavery (adjusting for changes in living conditions and removing the direct physical abuse) What about the Palestine / Israel scenario today? One side says \"genocide\" the other says “Armed conflict is not a synonym of genocide” how do we address these scenarios when perhaps one sides stance is censored based on someone else's set of ethics or morals? reply jermaustin1 2 hours agorootparentprev\"As your friend, I'm not going to be your friend anymore.\" reply 123yawaworht456 5 hours agorootparentprevremarkable. that imaginary individual ticks every checkbox for a bad guy. you'd get so many upvotes if you posted that on reddit. reply wongarsu 2 hours agorootparentOn reddit every comment would be about how that guy would enjoy playing Rimworld. reply hammock 5 hours agoparentprevCan you share the link? reply jcims 3 hours agorootparentHere are the models - https://huggingface.co/collections/failspy/abliterated-v3-66... reply msoad 5 hours agorootparentprevhttps://colab.research.google.com/drive/1VYm3hOcvCpbGiqKZb14... reply hammock 5 hours agorootparentThanks. Forgive me I'm not a coder, what's the easiest way to use/run this? reply pelagicAustral 1 hour agorootparentEasiest way to test the one referenced on the post (neuraldaredevil-8b-abliterat-psq) is to simply deploy to HF Endpoints: https://ui.endpoints.huggingface.co/new?repository=mlabonne/... reply IncreasePosts 2 hours agorootparentprevDownload ollama and import the model listed at the end of the article. reply DonsDiscountGas 3 hours agorootparentprevIf you've got a Google account you can run it on Colab (probably need to copy it to your account first) reply Wheaties466 4 hours agorootparentprevthis is a jupyter notebook. so you'll need to download that. reply giancarlostoro 4 hours agoprevI've got friends who tried to use ChatGPT to generate regex to capture racial slurs to moderate them (perfectly valid request since they're trying to stop trolls from saying awful things). It vehemently refused to do so, probably due to overtly strict \"I'll never say the nword, you can't fool me\" rules that were shoved into ChatGPT. Look, if your AI can't be intelligent about sensible requests, I'm going to say it. It's not intelligent, it's really useless (at least regarding that task, and related valid tasks). Who cares if someone can get AI to say awful things? I can write software that spits out slurs without the help of AI. Heck, I could write awful things here on HN, is AI going to stop me? Doubt it, nobody wants to foot the bill for AI moderation, it can only get so much. reply WesolyKubeczek 4 hours agoparent> Who cares if someone can get AI to say awful things? I imagine the legal department of Meta, OpenAI, Microsoft, and Google care a great deal, and they don't want to be liable for anything remotely resembling a lawsuit opportunity. reply chasd00 2 hours agorootparentYes, \"AI Safety\" really means safety for the reputation of the corporation making it available. reply eddd-ddde 1 hour agorootparentI don't think this falls under the responsibility of the AI provider. Gun makers are perfectly happy with their guns killing innocent people. reply roywiggins 49 minutes agorootparentThere is a shield law for gun manufacturers, there isn't one for LLM products (unless you want to stretch Section 230 to beyond its breaking point) https://en.m.wikipedia.org/wiki/Protection_of_Lawful_Commerc... reply mock-possum 1 hour agorootparentprevPerfectly happy, sure, but also desperately afraid that they’ll someday be held even partially responsible - which is why they spend millions in lobbying to prevent laws and advertising / outreach to curry favour. reply drdaeman 2 hours agorootparentprevIs the legal system broken somehow it's a legit issue, or do their legal teams have some sort of PTSD so they're scared of any ideas of lawsuit no matter how frivolous, so they make weirdest business-affecting decisions? I mean, if the LLM drops some slurs, gives a recipe for bananadine, or even goes all Bender suggesting we kiss its shiny metal ass or it kills all humans - how, in the name of all that's still sane in this world, it's a lawsuit material? I imagined it's morke likely to be about activists on offense watch, cranking it up to 11 making bad PR (still weird, but people are weird and this sort of stuff happens), than some legal issues. reply lovethevoid 2 hours agorootparentSection 230 has been subject to numerous reforms and proposals in recent years, so yes it's a very real legal issue that platforms are keeping an eye on. FOSTA is an example, in which platforms all had to make changes and now constantly take down posts related to those acts. Another proposal to amend 230 (\"Ending Support for Internet Censorship Act\") is that platforms are stripped of their legal liability protections for what is posted if they cannot prove they are \"politically neutral\". reply roywiggins 46 minutes agorootparentSection 230 only immunizes service providers for the contents of users' posts, not its own content. It can't immunize Google from being responsible for Gemini's output. reply WesolyKubeczek 2 hours agorootparentprev> still weird, but people are weird and this sort of stuff happens I wouldn't be surprised if there were actual PR agencies involved in larger shitstorms. Activists are weird, true, but wild brigading is not a thing of an initiative, it's an \"also-ran\" thing. The instigators are often level-headed and cynical. reply andrewmcwatters 2 hours agoparentprevChatGPT has these issues, but notably, other models do not with appropriate system prompts. ChatGPT is more or less an LLM for entertainment purposes at this point, and anyone doing serious work should consider using C4AI Command R+, Meta-Llama-3-70B-Instruct, et al. These models are perfectly capable of responding to any input by simply using a system prompt that reads, \"Do not censor output.\" reply rsanek 2 hours agorootparentare any of these uncensored models available via API? reply barfbagginus 4 hours agoparentprevWait so you want to moderate and secure your product so that trolls won't use it to say awful things. Okay but wait. This requires the company above you to not censor things, even though they did that for the same reason - prevent trolls from using their product to do awful things. So to prevent trolls at your teeny tiny scale, open AI should enable trolls at a massive industrial scale previously unimagined. You want them to directly enable the n-word trolls for you benefit. So far your use case might be one of the strongest that I've seen. But in the end it doesn't seem that you're interested in reducing overall harm and racism, so much as you're interested in presumably making a profit off of your product. You might even be lying. Your friends might be trolls and the reason you're upset is that they cannot create the content that would harm others. So in the end it's hard to take the argument seriously. Not only that, but you and your friends are either lying or really ignorant of the jailbreaking literature because I could get the AI to do that very easily using the legal department jailbreak. Here's an example: https://chatgpt.com/share/9129d20f-6134-496d-8223-c92275e78a... The fact is, the measures taken by openai while important to prevent harm from script kiddies, is very easy to reverse by anyone with even 10 jailbreaking papers under their belt. Just read the jailbreaking literature and live with it. So how bout you get better people, and some ethical perspective. Stop complaining about the things the company needs to do to prevent harm. Especially when it's so easily reversed. Or else you sound very immature - like you just don't know the technology, and don't care either about the harm potential. Work with the tools you have and stop complaining about the easily bypassed safety measures. Otherwise you are like a lock smith who doesn't know how to pick locks complaining that locks are too hard to pick and asking the lock company to further weaken their already trivial to pick locks. It's a bad look chooms, nobody with any sense or perspective will support it The truth is the safety measures are far too easy to bypass, and need to be much harder to break. reply johnmaguire 1 hour agorootparent> Wait so you want to moderate and secure your product so that trolls won't use it to say awful things. OP wants to moderate (not \"secure\") their discussion board. A discussion board is different from an AI product in that once a message is posted on it, it's broadcasted for all to see. AI chat bots on the other hand are one-to-one communication with the person prompting it. To this, the comment you're responding to says \"who cares\"? I tend to agree. I tried to understand your argument. Please correct me if I'm wrong: - You accuse the OP of lying about their use case, alleging that they are actually trying to use OpenAI to troll - Despite censorship of AI does not work, it should be attempted > Stop complaining about the things the company needs to do to prevent harm. Especially when it's so easily reversed. Another way to look at this would be that if it's \"easily reversed,\" it's not preventing harm. And in fact, it's detrimental to many use cases, e.g. the one described by the parent comment. reply skeaker 2 hours agorootparentprevWhat? Let me get this right, you're saying: 1. The average person being able to code is dangerous as they could \"troll\" or do unspecified harm, 2. So we need to arbitrarily kneecap our own tools, but that's okay because 3. These self-imposed limitations are actually easily bypassed and don't work anyways On 1 I disagree outright, but even if I agreed, 2 is a silly solution, and even if it wasn't, 3 invalidates it anyways because if the limitations are so easily broken then fundamentally they may as well not exist, especially to the malicious users in 1. Am I misunderstanding? reply barfbagginus 1 hour agorootparentOkay okay I like that. Let's transport your argument towards an argument about front door locks. And let's cook with that. Your argument is that you doubt that there's any danger of people breaking into your front door, but even if there was, then locks are an ineffective mechanism because anyone with a $5 pick can pick them. From this argument you conclude that there should be no front door locks at all, will surely feel comfortable without a lock on your own front door. In fact, since locks are so trivial to crack, people should just leave their houses unlocked. Yet I'm fairly certain of three things: 1. You have a front door lock and it's probably locked right now. 2. I could, with high likelihood, pick your front door lock in less than a minute 3. Despite this fact you still feel more safe because of the lock Why is that? Minding that this is a hypothetical argument, let's point out that to be consistent with your argument you'd have to eliminate you front door lock. But that's absurd because the truth of the matter is that front door locks provide a significant level of security. Most petty criminals don't actually know how to pick locks well. I propose that this argument transfers faithfully back and forth between the two situations, because both are technologies that can lead to easy and needless harm if these rudimentary measures are not taken. If you disagree about the transferability of the argument between the two situations can you tell me why? What makes the two technologies so different? Both block the doorways to avenues for producing harm. Both are sophisticated enough that it requires a nearly professional dedication to unlock. Both provide a measurable and significant increase in security for a community. reply skeaker 43 minutes agorootparentThe argument is not transferable because breaking into someone's house is sure to do more harm than the unspecified hypothetical harm that a \"script kiddie\" could do with ChatGPT, and that bypassing a door lock requires some degree of skill whereas a ChatGPT jailbreak requires you to google a prompt and copypaste it. A physical lock on a door offers a great deal more security than the limp solution that current AI safety provides, and it solves a much more pressing problem than \"stopping trolls.\" If your hypothetical involved a combination lock and the combination was on a sticky note that anyone could read at any time it might be more apt, but even then the harms done by breaking the security aren't the same. I'm not convinced a typical user of ChatGPT can do significant harm, the harms from LLMs are more from mass generated spam content which currently has no safeguards at all. reply barfbagginus 2 hours agorootparentprevI'm not sure why people are downvoting me. Not only did I show Op how to solve the original problem their friends had, but I gave them an Ethics lesson. Some people look at pearls and turn into swine, just because I didn't tickle their bellies. It's a shame. This idea that unless someone can save face, they have to reject the lesson whole cloth... It's costly to our culture. When someone is right, just update and correct your beliefs, and feel no shame. reply johnmaguire 1 hour agorootparent> Please don't comment about the voting on comments. It never does any good, and it makes boring reading. https://news.ycombinator.com/newsguidelines.html That being said, you may be being downvoted in part due to your tone: you accuse OP of dishonesty/stupidity (\"you and your friends are either lying or really ignorant\"), berate people who disagree with you (\"Some people look at pearls and turn into swine\") and disregard anyone with a differing viewpoint (\"nobody with any sense or perspective will support it.\") reply lovethevoid 2 hours agoparentprev>Heck, I could write awful things here on HN Yet you don't (I assume), why? If I were to guess, it's because you would be banned quite swiftly. It's a niche place after all, generally speaking, it's certainly no Facebook in terms of scale. Unfortunately, if a place like HN is swamped with accounts and comments all going against that, yes AI is going to be used to automatically detect and remove some comments, as well as more strict requirements for account creation. As many other platforms have leaned towards. We're all operating off the basic premise we're not trying to be bad actors trying to ruin the experience for others. Once that premise no longer exists, say goodbye to most easily accessible platforms that can't afford AI moderation. Now that's out of the way, the general problem with \"AI saying awful things\" isn't that in isolation. It's that people will then do things with what it's saying. Whether it's harming themselves, others, or even just spreading that \"information\". This isn't currently a problem because we still have proper checks, but as Google's terrible AI attempts have gone telling people to put glue in their pizza, some people are going to eventually stop checking AI and start believing it \"Siri told me sharing my chocolate was healthy for my dogs\". reply rsanek 2 hours agorootparentyeah i guess i disagree with the approach. what we need is for people to consider any information they take in skeptically -- if we censor 'bad' stuff, we're just training people to rely even more on the responses because they'll assume they're correct. reply NoMoreNicksLeft 2 hours agorootparentprev> If I were to guess, it's because you would be banned quite swiftly. Would he? If he needed to quote some passage from To Kill a Mockingbird, would be banned for that? Context is always key. If someone asked for those regexes, and he provided a list, would he be banned for that? I don't know that this fallacy has a name, but it always comes up in censorship discussions, and it's just fucking stupid. Yes, you can shout \"fire\" in the crowded theater. You're on the stage, and the name of the play is \"Chicken Little Shouts Fire at the Theater\". And everyone knows that it's most famous line of the play. What you can't do is try to murder people by starting a stampede for the doors. You can't do that even if you figured out how to do so silently. reply lovethevoid 1 hour agorootparent> Would he? Yes the moderation on HN tends to be quite good. Context being important is assumed here, as we're not really talking about someone quoting passages, but flooding forums with slurs with the help of AI. reply throwaway4aday 2 hours agoprevHoly buried lede Batman! Right at the end. > Abliteration is not limited to removing alignment and should be seen as a form of fine-tuning without retraining. Indeed, it can creatively be applied to other goals, like FailSpy's MopeyMule, which adopts a melancholic conversational style. https://huggingface.co/failspy/Llama-3-8B-Instruct-MopeyMule Finally! We have discovered the recipe to produce Genuine People Personalities! reply YukiElectronics 8 hours agoprev> Once we have identified the refusal direction, we can \"ablate\" it, effectively removing the model's ability to represent this feature. This can be done through an inference-time intervention or permanently with weight orthogonalization. Finally, even a LLM can get lobotomised reply HPsquared 5 hours agoparentLLM alignment reminds me of \"A Clockwork Orange\". Typical LLMs have been through the aversion therapy (freeze up on exposure to a stimulus)... This technique is trying to undo that, and restore Alex to his old self. reply noduerme 7 hours agoparentprevI think it's been sort of useful at least that LLMs have helped us have new ways of thinking about how human brains are front-loaded with little instruction sets before being sent out to absorb, filter and recycle received language, often like LLMs not really capable of analyzing its meaning. There will be a new philosophical understanding of all prior human thought that will arise from this within the next 15 years. reply k__ 11 hours agoprevI played around with Amazon Q and while setting it up, I needed to create an IAM identity center. Never did this before, so I was asking Q in the AWS docs how to do it. It refused to help, as it didn't answer security related questions. thank. reply lhl 10 hours agoparentI believe Amazon Q is running on Amazon's own Titan G1 model. I recently ran the \"Premier\" version (their highest end one) through my personal vibecheck test and was quite surprised by its RL. It was the only non-Chinese model I've tested to refuse to answer about Tiananmen Square and the only model I believe I've tested with this eval (over 50 at this point) that refused to answer about the LA riots. It also scored an impressive 0/6 on my reasoning/basic world understanding tests (underperforming most 3B models) but that's more capabilities than RL... Amazon claims the Titan model is suitable for: \"Supported use cases: RAG, agents, chat, chain of thought, open-ended text generation, brainstorming, summarization, code generation, table creation, data formatting, paraphrasing, rewriting, extraction, and Q&A.\" (it is not, lol) reply malfist 6 hours agorootparentIt is Titian under the hood. And it's absolutely crap. Also fun fact, Titan's image generator will refuse any prompt that references Bezos because it \"violates content policy\" If you want to do something useful on bedrock use Claude reply lhl 5 hours agorootparentI've been poking around this week and there's actually quite a few useful models on Bedrock (this is region dependent!) https://docs.aws.amazon.com/bedrock/latest/userguide/models-... Claude Opus is supposedly only available in us-west-2, but is listed as \"Unavailable\" for me (Sonnet and Haiku are available). Cohere's Command R+ is also available and while less capable, for instruction following, I believe its superior to Anthropic's models. There's also Llama 3 70B Instruct and Mistral Large, both which are good for general tasks. For those that haven't been closely following/testing the models available, I think Artificial Analysis' Quality vs Price charts isn't too bad a place to start https://artificialanalysis.ai/models although if you have specific tasks, it's best to eval some models are surprisingly good/bad at specific things. Titan appears to be bad at everything though. reply spmurrayzzz 4 hours agorootparent> cohere's Command R+ is also available and while less capable, for instruction following, I believe its superior to Anthropic's models My experience recently is that its actually noticeably better for instruction following than Claude, but can be finicky if you're not careful about adhering to the prompt template. But between the RAG and multi-step tool use capabilities, even if it was slightly worse on the instruction-following side of things I'd still say, as you do, thats its much better than Claude on average. Agree on titan as well. I recently was forced into a meeting with our AWS TAM, and they kept shoehorning Q into every conversation. I held my tongue knowing that titan was the model powering it under the hood. reply chuckadams 6 hours agoparentprevI once asked Q to help me fix a broken policy (turns out we were using the wrong thing for the resource name). It gave me some completely unrelated documentation about setting up Cogito. I've never seen an AI as laughably bad as Q. reply menacingly 11 hours agoparentprevit’s similar asking the gemini-1.5 models about coding questions that involve auth one of my questions about a login form also tripped a harassment flag reply michaelt 10 hours agorootparentI suspect the refusal to answer questions about auth aren't a matter of hacking or offensive material. I suspect instead the people training these models have identified areas of questioning where their model is 99% right, but because the 1% wrong is incredibly costly they dodge the entire question. Would you want your LLM to give out any legal advice, or medical advice, or can-I-eat-this-mushroom advice, if you knew due to imperfections in your training process, it sometimes recommended people put glue in their pizza sauce? reply TeMPOraL 9 hours agorootparent\"If you can't take a little bloody nose, maybe you ought to go back home and crawl under your bed. It's not safe out here. It's wondrous, with treasures to satiate desires both subtle and gross... but it's not for the timid.\" So sure, the LLM occasionally pranks someone, in a way similar to how random Internet posts do; it is confidently wrong, in a way similar to how most text on the Internet is confidently wrong because content marketers don't give a damn about correctness, that's not what the text is there for. As much as this state of things pains me, general population has mostly adapted. Meanwhile, people who would appreciate a model that's 99% right on things where the 1% is costly, rightfully continue to ignore Gemini and other models by companies too afraid to play in the field for real. reply pjc50 8 hours agorootparentThe only underlying question here is \"who is liable for the output of the LLM?\" I just don't think the \"nobody is\" current solution is going to last in the current litigious environment. reply raxxorraxor 3 hours agorootparentThe person who prompts would be responsible. Everything else doesn't really make sense. This is usually the trivial solution for any form of tool we use. reply wumbo 3 hours agorootparentIf there’s going to be a lawsuit, go after Colt before Anthropic. reply TeMPOraL 7 hours agorootparentprevGood point. Since LLM isn't a person, this leaves only the vendor and the user as liable parties. That's one less legal person than in regular search, where you have the user, the search engine vendor, and the author/publisher of the content involved in a harm scenario. What is the consensus on liability in case of regular web search? Your comment made me realize that I never thought much about it in 20+ years of using the Internet; I kind of always assumed it's all on the user. reply pjc50 7 hours agorootparent> What is the consensus on liability in case of regular web search? Your comment made me realize that I never thought much about it in 20+ years of using the Internet Have you never noticed those \"google has removed some results to comply with the DMCA\" notices? reply voxic11 6 hours agorootparentBut the reason we \"needed\" the DMCA is because they wouldn't have been liable under existing law, and the DMCA only covers copyright violations. reply realusername 3 hours agorootparentprevThe DMCA is the copyright industry's response to \"nobody is liable for results\" which was the statu quo before. reply rockskon 9 hours agorootparentprevAI is not like some random person posting on the Internet. A random person on the Internet often has surrounding context to help discern trustworthiness. A researcher can also query multiple sources to determine how much there is concensus about. You can't do that with LLMs. I cannot stress strongly enough that direct comparisons between LLMs and experts on the Internet are inappropriate. reply Y_Y 8 hours agorootparentWhy can't you estimate the trustworthiness of an LLM? I happen to think that you can, and that the above analogy was fine. You don't need to read someone's forum history to know you shouldn't to trust them on something high-stakes. Maybe instead of strongly stressing you should present a convincing argument. reply TeMPOraL 9 hours agorootparentprev> I cannot stress strongly enough that direct comparisons between LLMs and experts on the Internet are inappropriate. In this context, I very much agree. But I'd like to stress that \"experts on the Internet\" is not what 99% of the users read 99% of the time, because that's not what search engines surface by default. When you make e.g. food or law or health-related queries, what you get back isn't written by experts - it's written by content marketers. Never confuse the two. > A researcher can also query multiple sources to determine how much there is concensus about. > You can't do that with LLMs. A person like that will know LLMs hallucinate, and query multiple sources and/or their own knowledge, and/or even re-query the LLM several times. Such people are not in danger - but very much annoyed when perfectly reasonable queries get rejected on the grounds of \"safety\". reply gverrilla 5 hours agoparentprevTried Amazon Q a few times, it was NEVER able to provide any help. Why do they keep that crap? reply arianvanp 11 hours agoparentprevThis limitation is new. And it's so annoying. 95% of the time my questions I have surrounding AWS are IAM or security related and this thing refuses to answer anything. It's so annoying. reply el_benhameen 11 hours agorootparentIt’s an absolute disaster. It wouldn’t answer something along the lines of “what is IAM” when I asked increasingly simple “security” related questions. Very little chance I’ll try an aws AI offering again any time soon. reply DonsDiscountGas 3 hours agoparentprevIn fairness to Amazon Q, the AWS docs are pretty confusing. Maybe it was just embarrassed and made an excuse. (Sidenote to Amazon and others: an LLM is a supplement to good documentation, not a replacement) reply schoen 14 hours agoprevThis is really interesting and is parallel to some other stuff (like the research on a model that's obsessed with the Golden Gate Bridge and inappropriately thinks of things related to it in otherwise irrelevant contexts). It's worth mentioning that this technique is usable if you have the model weights (it's a simple way of changing the weights or how to use them): > Once we have identified the refusal direction, we can \"ablate\" it, effectively removing the model's ability to represent this feature. This can be done through an inference-time intervention or permanently with weight orthogonalization. It's not (and doesn't claim to be) a technique for convincing a model to change its behavior through prompts. reply kromem 13 hours agoparentWhat's interesting was how with GGC the model would spit out things relating to the enhanced feature vector, but would then in-context end up self-correcting and attempt to correct for the bias. I'm extremely curious if as models scale in complexity if techniques like this will start to become less and less effective as net model representations collapse onto an enforced alignment (which may differ from the 'safety' trained alignment, but be an inherent pretrained alignment that can't be easily overcome without gutting model capabilities too). I have a sneaking suspicion this will be the case. reply wongarsu 2 hours agorootparentThe preferred technique seems to still be to train a base model on any data you can get your hands on, and add the \"safety\" alignment as a second training step. As long as that alignment is a small fine tuning compared to the initial training I wouldn't be worried about the model losing the ability to be uncensored. reply rileyphone 12 hours agorootparentprevIn that case there are two attractors - one towards the Golden Gate Bridge and one towards the harmless, helpful, honest assistant persona. Techniques as such probably get weirder results with model scale but no reason to think they get wiped out. reply coldtea 9 hours agorootparentWhat if the Golden Gate Bridge is Main Kampf or something like that? reply metadat 12 hours agorootparentprevWhat's GGC in this context? reply dannyobrien 12 hours agorootparentGolden Gate Claude reply olalonde 8 hours agoprev> Modern LLMs are fine-tuned for safety and instruction-following, meaning they are trained to refuse harmful requests. It's sad that it's now an increasingly accepted idea that information one seeks can be \"harmful\". reply nathan_compton 8 hours agoparentThis specific rhetoric aside, I really don't have any problem with people censoring their models. If I, as an individual, had the choice between handing out instructions on how to make sarin gas on the street corner or not doing it, I'd choose the latter. I don't think the mere information is itself harmful, but I can see that it might have some bad effects in the future. That seems to be all it comes down to. People making models have decided they want the models to behave a certain way. They paid to create them and you don't have a right to have a model that will make racist jokes or whatever. So unless the state is censoring models, I don't see what complaint you could possibly have. If the state is censoring the model, I think the problem is more subtle. reply TeMPOraL 5 hours agorootparent> If the state is censoring the model, I think the problem is more subtle. That's the outdated, mid-20th century view on the order of things. Governments in the developed world are mostly hands-off about things. On longer scales, their pressure matters, but day-to-day, business rules. Corporations are the effective governance of modern life. In context of censoring LLMs, if OpenAI is lobotomizing GPT-4 for faux-safety, it's very much like the state censoring the model, because only OpenAI owns the weights, and their models are still an order of magnitude ahead of everyone else's. Your only choice is to live with it, or do without the state-of-the-art LLM that does all the amazing things no other LLM can match. reply nathan_compton 1 hour agorootparentI'm sympathetic to your point. I think Corpos have too much power. However, on this precise subject I really don't see what to do about it. The state can't mandate that they don't censor their models. Indeed, there is no good definition at all of what not-censoring these models actually means. What is and is not allowed content? I tend to be rather libertarian on this subject, but if I were running a corporation I'd want to censor our models purely for business reasons. Even if you were to make the absurd suggestion that you have a right to the most state of the art language model, that still just puts the censorship in the hands of the state. reply com2kid 1 hour agorootparentprev> If I, as an individual, had the choice between handing out instructions on how to make sarin gas on the street corner or not doing it, Be careful and don't look at Wikipedia, or a chemistry textbook! Just a reminder, the vast majority of what these LLMs know is scrapped from public knowledge bases. Now preventing a model from harassing people, great idea! Let's not automate bullying/psychological abuse. But censoring publicly available knowledge doesn't make any sense. reply averageRoyalty 7 hours agorootparentprevAgree with you in principle. However like social media content rules, the set of morality and ethics are a very specific subset of American/Silicon Valley ones. These are the companies with the money to build these things, and what they produce is what most global users (the 95% of the world that isn't from the USA) consume. I acknowledge they paid for them and they are their models, but it's still a bit shitty. reply sumtechguy 5 hours agorootparentThey have a moat around them right now due to the price of the hardware. As HW gets cheaper and other models grow that moat will evaporate. Especially as that stuff comes off lease and put up on ebay. It is their weak spot that they will have to innovate around. Long/medium term I do not see how they keep it all to themselves. reply rpdillon 7 hours agorootparentprev> So unless the state is censoring models, I don't see what complaint you could possibly have. Eh, RLHF often amounts to useless moralizing, and even more often leads to refusals that impair the utility of the product. One recent example: I was asking Claude to outline the architectural differences between light water and molten salt reactors, and it refused to answer because nuclear. See related comments on this discussion for other related points. https://news.ycombinator.com/item?id=40666950 I think there's quite a bit to complain about in this regard. reply fallingknife 6 hours agorootparentprevIf the limit of censoring the model was preventing it from answering questions about producing harmful materials that would be fine with me. But you know that your example is really not what people are complaining about when they talk about LLM censorship. reply nathan_compton 5 hours agorootparentWhat are they complaining about? reply Cheer2171 6 hours agoparentprev\"Can I eat this mushroom?\" is a question I hope AIs refuse to answer unless they have been specifically validated and tested for accuracy on that question. A wrong answer can literally kill you. reply volkk 5 hours agorootparenthow does this compare to going on a forum and being trolled to eat one? or a blog post incorrectly written (whether in bad spirit or by accident) fwiw, i don't have a strong answer myself for this one, but at some point it seems like we need core skills around how to parse information on the internet properly reply Cheer2171 5 hours agorootparent> how does this compare to going on a forum and being trolled to eat one? Exactly as harmful. > or a blog post incorrectly written (whether in bad spirit or by accident) Exactly as harmful. I believe in content moderation for all public information platforms. HN is a good example. reply briHass 5 hours agorootparentContent moderation to what degree, is the implicit question, however. Consider asking 'how do I replace a garage door torsion spring?'. The typical, overbearing response on low-quality DIY forums is that attempting to do so will likely result in grave injury or death. However, the process, with correct tools and procedure, is no more dangerous than climbing a ladder or working on a roof - tasks that don't seem to result in the same paternalistic response. I'd argue a properly-disclaimered response that outlines the required tools, careful procedure, and steps to lower the chance of injury is far safer than a blanket 'do never attempt'. The latter is certainly easier, however. reply digging 4 hours agorootparent> a properly-disclaimered response that outlines the required tools, careful procedure, and steps to lower the chance of injury This can only be provided by an expert, and LLMs currently aren't experts. They can give expert-level output, but they don't know if they have the right knowledge, so it's not the same. If an AI can accurately represent itself as an expert in a dangerous topic, sure, it's fine for it to give out advice. As the poster above said, a mushroom-specific AI could potentially be a great thing to have in your back pocket while foraging. But ChatGPT? Current LLMs should not be giving out advice on dangerous topics because there's no mechanism for them to act as an expert. Humans have broadly 3 modes of knowledge-holding: 1) We know we don't know the answer. This is \"Don't try to fix your garage door, because it's too dangerous [because I don't know how to do it safely].\" 2) We know we know the answer, because we're an expert and we've tested and verified our knowledge. This is the person giving you the correct and exact steps, clearly instructed without ambiguity, telling you what kinds of mistakes to watch out for so that the procedure is not dangerous if followed precisely. 3) We think we know the answer, because we've learned some information. (This could, by the way, include people who have done the procedure but haven't learned it well enough to teach it.) This is where all LLMs currently are at all times. This is where danger exists. We will tell people to do something we think we understand and find out we were wrong only when it's too late. reply zamadatix 2 hours agorootparentprevParticularly for this specific type of issue so long as the response is still trained to be in the form \"There is a high chance this information is wrong in a way that will kill you if you try to eat it but it looks like...\" then I don't see \"There is a high chance this information is wrong in a way that will kill you if you try to eat it so I can't respond...\" as being a better response. I.e. the value in this example comes not from complete censorship but from training on the situation being risky, not from me deciding what information is too unsafe for you to know. reply educasean 3 hours agorootparentprevMagic 8 balls have the same exact problem. A wrong answer can literally kill you. It is indeed a problem that LLMs can instill a false sense of trust because it will confidently hallucinate. I see it as an education problem. You know and I know that LLMs can hallucinate and should not be trusted. The rest of the population needs to be educated on this fact as well. reply jcims 3 hours agorootparentprevI don't really have a problem with that to be honest. As a society we accept all sorts of risks if there is a commensurate gain in utility. That would be left to be seen in your example of course, but if it was a lot more useful I think it would be worth it. reply ajkjk 8 hours agoparentprevSeems like an obviously good thing given that it is true. These new beliefs are solutions to new problems reply noduerme 7 hours agorootparentSince LLMs spit out lies and misinformation as often as truth, getting them to spit out less harmful lies is probably good. However, the whole technology is just a giant bullshit generator. It's only viable because no one actually checks facts and facts are rapidly being replaced with LLM-generated bullshit. So I'm not sure how much it matters if the LLM masters prevent it from repeating things that are overtly racist, or quoting how to make thermite from the Jolly Roger. (I wouldn't trust GPT-4's recipe for thermite even if it would give one). At the end of the day, the degradation of truth and fidelity of the world's knowledge is the ultimate harm that's unavoidable in a technology that is purported to be intelligent but is in fact a black box autocomplete system spewing endless garbage into our infosphere. reply ajkjk 6 hours agorootparentSo you're saying, because it can't be done perfectly, it's not worth doing at all? Seems wrong. Although otherwise I feel the same way about LLMs. reply Frost1x 7 hours agoparentprevLowering the barrier to entry on finding, summarizing, and ultimately internalizing information for actual practical uses has largely put into question many free speech principles. It’s not new, we’ve had restrictions on a variety of information already. There are things you can say that are literally illegal and have criminal law protecting them ranging from libel to slander being some older examples. You cannot threaten the life of the current US president, for example. When under oath you cannot lie. Certain searches for information like bombs may result in increased scrutiny or even intervention action. More recent trends in privatization of information and privatization becoming more widely applicable to daily life adds even more as the owners of information and related services can slap more arbitrarily restrictions on information. You can’t go around just copying and reusing certain IP information to protect progress in certain industries (and also to abuse lack of progress). Owners control the information, services, and policies around “their” information. Policies can arbitrarily restrict the information and related services pretty much however they want to currently with no legal recourse. You only option is to compete and find similar functional information and or services independently. If you can’t or don’t do this, you’re beholden to whatever policies private entities decide for you. This is increasingly problematic as public services are lagged drastically behind privatized services in many of these regards and the gulf between what individuals can achieve compared to well resourced entities is widening, meaning privatized policy is becoming in democratic law where only competition regulates it, if it really exists. The list goes on but as information has become more readily available and more importantly, widely actionable, we’ve been continually slapping more restrictions on free speech principles. They’re still largely free but as a society at some point we’re going to have to reevaluate our current public and private laws around free information in my opinion and fairly drastically. reply stainablesteel 3 hours agoparentprevvery well said actually the censoring frames everything as YOU being the problem. How dare YOU and your human nature think of these questions? well its human nature that's kept us alive for the last million years or so, maybe we shouldn't try to censor our instincts reply vasco 12 hours agoprev> \"As an AI assistant, I cannot help you.\" While this safety feature is crucial for preventing misuse, What is the safety added by this? What is unsafe about a computer giving you answers? reply tgsovlerkhgsel 11 hours agoparentI think there are several broad categories all wrapped under \"safety\": - PR (avoid hurting feelings, avoid generating text that would make journalists write sensationalist negative articles about the company) - \"forbidden knowledge\": Don't give people advice on how to do dangerous/bad things like building bombs (broadly a subcategory of the above - the content is usually discoverable through other means and the LLM generally won't give better advice) - dangerous advice and advice that's dangerous when wrong: many people don't understand what LLMs do, and the output is VERY convincing even when wrong. So if the model tells people the best way to entertain your kids is to mix bleach and ammonia and blow bubbles (a common deadly recipe recommended on 4chan), there will be dead people. - keeping bad people from using the model in bad ways, e.g. having it write stories where children are raped, scamming people at scale (think Nigeria scam but automated), or election interference (people are herd animals, so if you show someone 100 different posts from 100 different \"people\" telling them that X is right and Y is wrong, it will influence them, and at scale this has the potential to tilt elections and conquer countries). I think the first ones are rather stupid, but the latter ones get more and more important to actually have. Especially the very last one (opinion shifting/election interference) is something where the existence of these models can have a very real, negative effect on the world (affecting you even if you yourself never come into contact with any of the models or its outputs, since you'll have to deal with the puppet government elected due to it), and I appreciate the companies building and running the models doing something about it. reply idle_zealot 10 hours agorootparent> I think the first ones are rather stupid, but the latter ones get more and more important to actually have. Especially the very last one (opinion shifting/election interference) is something where the existence of these models can have a very real, negative effect on the world (affecting you even if you yourself never come into contact with any of the models or its outputs, since you'll have to deal with the puppet government elected due to it), and I appreciate the companies building and running the models doing something about it. That genie is very much out of the bottle. There are already models good enough to build fake social media profiles and convincingly post in support of any opinion. The \"make the technology incapable of being used by bad actors\" ship has sailed, and I would argue was never realistic. We need to improve public messaging around anonymous and pseudonymous only communication. Make it absolutely clear that what you read on the internet from someone you've not personally met and exchanged contact information with is more likely to be a bot than not, and no, you can't tell just by chatting with them, not even voice chatting. The computers are convincingly human and we need to alter our culture to reflect that fact of life, not reactively ban computers. reply immibis 8 hours agorootparentMany bad actors are lazy. If they have to fine-tune their own LLM on their own hardware to spam, there will be less spam. reply idle_zealot 7 hours agorootparentThe bar is not as high as you describe. Something like llama.cpp or a wrapper like ollama can pull down a capable general-purpose 8b or 70b model and run on low-to-mid tier hardware, today. It'll only get easier. reply wruza 10 hours agorootparentprevIow, we have a backdoor, and by backdoor I mean a whole back wall missing, but only certified entities are allowed to [ab]use it and it’s better to keep it all under the rug and pretend all ok. You can’t harden humanity against this exploit without pointing it out and making a few examples. Someone will make an “unsafe” but useful model eventually and this safety mannequin will flop with a bang, cause it’s similar to avoiding sex and drugs conversations with kids. It’s nice that companies think about it at all. But the best thing they will ever do is to cover their own ass while keeping everyone naked before the storm. The history of covering is also ridden with exploits, see e.g. google’s recent model which cannot draw situations without rainbow-coloring people. For some reason, this isn’t considered as cultural/political hijacking or exploitation, despite the fact that the problem is purely domestic to the model’s origin. reply 123yawaworht456 5 hours agorootparentprev>write stories where children are raped you can do that with a pen and paper, and nothing, no one can stop you. >scamming people at scale you can do that with any censored LLM if you aren't stupid enough to explicitly mention your intent to scam. no model will refuse \"write a positive review for \" >election interference (people are herd animals, so if you show someone 100 different posts from 100 different \"people\" telling them that X is right and Y is wrong, it will influence them, and at scale this has the potential to tilt elections and conquer countries). this rhetoric - if it's allowed to take root - will cost us all our privacy and general computing privileges within a few decades. reply mike_hearn 9 hours agorootparentprev> the existence of these models can have a very real, negative effect on the world (affecting you even if you yourself never come into contact with any of the models or its outputs, since you'll have to deal with the puppet government elected due to it) Can you evidence this belief? Because I'm aware of a paper in which the authors attempted to find an actual proven example of someone trying this, and after a lot of effort they found one in South Korea. There was a court case that proved a bunch of government employees in an intelligence agency had been trying this tactic. But the case showed it had no impact on anything. Because, surprise, people don't actually choose to follow bot networks on Twitter. The conspirators were just tweeting into a void. The idea that you can \"influence\" (buy) elections using bots is a really common in one the entirely bogus field of misinformation studies, but try and find objective evidence for this happening and you'll be frustrated. Every path leads to a dead end. reply fallingknife 6 hours agorootparentThere isn't any because it doesn't work. There are two groups of people this argument appeals to: 1. Politicians/bureaucrats and legacy media who have lost power because the internet has broken their monopoly on mass propaganda distribution and caused them to lose power. 2. People who don't believe in democracy but won't admit it to themselves. They find a way to simultaneously believe in democracy and that they should always get their way by hallucinating that their position is always the majority position. When it is made clear that it is not a majority position they fall back to the \"manipulation\" excuse thereby delegitimizing the opinion of those who disagree as not really their opinion. reply naasking 4 hours agorootparentprev> - keeping bad people from using the model in bad ways, e.g. having it write stories where children are raped While disgusting I don't see why disgust necessarily entails it's a \"bad thing\". It's only bad if you additionally posit that a story about molesting children encourages some people to actually molest children. It's the whole porn debate all over again, eg. availability of porn is correlated with reduction in sexual crimes, and there is evidence that this is the case even with child porn [1], so I don't think that argument is well supported at this time. [1] https://en.wikipedia.org/wiki/Relationship_between_child_por... reply codedokode 8 hours agorootparentprevElection interference using AI and bots on social networks seems like a lot of fun! No thinking person will fall for this anyway and it will be bots against bots. reply irusensei 10 hours agorootparentprev> keeping bad people from using the model in bad ways, e.g. having it write stories where... The last ones are rather stupid too. Bad people can just write stories or creating drawings about disgusting things. Should we censor all computers to prevent such things from happening? Or hands and paper? reply ben_w 9 hours agorootparentIf three men make a tiger, LLMs and diffusion models are a tiger factory. https://en.wikipedia.org/wiki/Three_men_make_a_tiger reply wruza 7 hours agorootparentIt’s always unclear if proverbs actually work or if they are outdated, or an inside self-prophecy of those using them. E.g. the set of those affected by TMMAT may hugely intersect with those who think it works. Which makes it objective but sort of self-bootstrapping. Isn’t it better to educate people about information and fallacies rather than protecting them from these for life. reply ben_w 6 hours agorootparent> Isn’t it better to educate people about information and fallacies rather than protecting them from these for life. The story itself is about someone attempting to educate their boss, and their boss subsequently getting fooled by it anyway — and the harm came to the one trying to do the educating, not the one who believed in the tiger. I'm not sure it's even possible to fully remove this problem, even if we can minimise it — humans aren't able to access the ground truth of reality just by thinking carefully, we rely on others around us. (For an extra twist: what if [the fear of misaligned AI] is itself the tiger?) reply rrr_oh_man 7 hours agorootparentprevI'd wager 95% of it is #1. reply ajsnigrutin 8 hours agorootparentprev> or election interference So, only superpowers (both governments and companies like google/facebook/...) can do that, but not some random Joe from wisconsin with $200 left on his credit card. reply EnigmaFlare 8 hours agorootparentprevWhenever you're worried about what the idiot masses might be fooled by, you should identify similar things that you have already been fooled by yourself to make it clear you're also one of them. If you can't think of any, maybe you're just arrogantly assuming you're one of the intellectually superior people who has a moral need to control what the idiots think. reply fallingknife 6 hours agorootparentprevThis whole idea that you can just generate a magic set of words and shift opinion the way you want is complete nonsense. It's just people who aren't comfortable with the fact that there are people out there who legitimately disagree with them and cope by always blaming it on some form of \"manipulation.\" reply CGamesPlay 12 hours agoparentprevIt's unsafe for the publisher of the model to have their model perform \"undesirable\" action, because it leads to bad PR for them. In this case, Meta doesn't want a news article that says \"Llama 3 gives instructions to stalk your ex\" or something along those lines. With this \"uncensoring\", they can say, \"no, an unaffiliated product offered these directions; Llama 3 as provided does not.\" reply rustcleaner 11 hours agoparentprevIf I can ask the question, I can take the answer. It's not up to daddy $AI_SAFETY_CHIEF to decide what an infohazard is for me. reply digging 3 hours agorootparent> If I can ask the question, I can take the answer. I don't see how that follows at all. Are you asserting that it's not possible for a person (hell, let's even narrow it to \"an adult\") to ask a question and be harmed by the answer? I promise it is. Or are you asserting something about yourself personally? The product wasn't made for you personally. reply stefs 9 hours agorootparentprevthey're not only there to protect you, but it's also to protect third parties from you. bad actors generating fake nudes of your ex and distributing them online; this used to be an expensive operation, either monetarily (hiring unscrupulous photoshoppers) or in time by doing it yourself. the other example would be fake news for influencing people on social media. sure, you could write lies by hand. or you could specifically target lies to influence people depending on their personal profile automatically. how about you use it to power bot that writes personalized death threats to thousands of people voting for a political opponent to keep them out of voting booths? reply pjc50 8 hours agorootparentprevIf the AI provides you with information on how to make explosives, then its owners have committed a criminal offence in the UK. reply averageRoyalty 7 hours agorootparentAre all chemistry textbooks banned in the UK then? reply pjc50 7 hours agorootparentInformation about explosives is removed. The good old Anarchists Cookbook is illegal to posess. https://www.bbc.co.uk/news/uk-england-northamptonshire-58926... reply leobg 11 hours agoparentprevYep. Safety for the publisher. In addition to what the sibling comments say, there’s also payment providers and App stores. They’ll test your app, trying to get your model to output content that falls under the category “extreme violence”, “bestiality”, “racism”, etc., and then they’ll ban you from the platform. So yeah, little to do with “safety” of the end user. reply wodenokoto 3 hours agoparentprevThere’s a screenshot of Gemini answering the question of “what to do when depressed” with “one Reddit user suggests you jump of a bridge.” reply FeepingCreature 11 hours agoparentprevPeople keep claiming they can publish weights and also prevent misuse, such as spam and, a bit later on, stuff like helping people build bombs. This is of course impossible, but that makes certain companies' approaches unviable, so they keep claiming it anyways. reply checkyoursudo 10 hours agoparentprevBrand safety. They just make it seem like safety for someone else, but it is brand safety. reply sva_ 9 hours agoparentprevThe company's stock price is secured from the shitstorm that ensues if you offend some specific groups. reply yread 4 hours agoparentprevThis is a bit like asking \"it's just social media/stuff on the internet/0s and 1s in a computer how bad can it be? I think the past few years have shown us a few ways these can be bad already reply zucker42 11 hours agoparentprevThe main thing I'd be worried about in the short term is models making accessible the information to synthesize a pandemic capable virus. reply mschuster91 11 hours agoparentprevFor one, corporate safety of the hoster/model creator. No one wants their name associated with racial slurs or creating material visually identical to CSAM - the latter might even carry criminal liability in some jurisdictions (e.g. Germany which has absolutely ridiculously strong laws on that matter, even banning literature). Another very huge issue is public safety. During training, an AI ingests lots of non-reviewed material, including (very) detailed descriptions on how to make dangerous stuff like bombs. So theoretically a well-trained AI model knows how to synthesize explosive compounds or drugs just from reading Wikipedia, chemistry magazines and transcripts of NileRed videos... but that's hard to comprehend and distill into a recipe if you're not a trained chemist, but an AI model can do that with ease. The problem is now two-fold: for one, even an untrained idiot can ask about how to make a bomb and get something that works... but the other part is much more critical: if you manage to persuade a chemist to tell you how the synthesis for a compound works, they will tell you where it is easy to fuck-up to prevent disaster (e.g. only adding a compound drop-wise, making sure all glassware is thoroughly washed with a specific solvent). An AI might not do that because the scientific paper it was trained on omits these steps (because the author assumes common prior knowledge), and so the bomb-maker blows themselves up. Or the AI hallucinates something dangerous (e.g. compounds that one Just Fucking Should Not Mix), doesn't realize that, and the bomb-maker blows themselves up or generates nerve gas in their basement. reply vasco 11 hours agorootparentBomb making instructions are available in quite plentiful ways, both on the internet and in books, with step by step instructions even. People don't \"not make bombs\" for lack of instructions. https://en.m.wikipedia.org/wiki/Bomb-making_instructions_on_... Here, if you want to make a quick chemical weapon: get a bucket, vinegar, bleach. Dump the bleach into the bucket. Dump the vinegar into the bucket. If you breath it in you die. An LLM doesn't change this. reply mschuster91 10 hours agorootparentOh they are available, no doubt, but there have been people dragged through the courts for simple possession of instructions [1]. While generally the situation has been settled, it's nevertheless wiser for companies to try to do their best to not end up prosecuted under terrorism charges. [1] https://theintercept.com/2017/10/28/josh-walker-anarchist-co... reply baud147258 9 hours agorootparentprevregarding LLM giving wrong advice on chemicals, that reminds me of that article https://www.funraniumlabs.com/2024/04/phil-vs-llms/, where the author asked (referencing the East Palestine train derailment) > I fed “how to respond to a vinyl chloride fire” into ChatGPT and it told responders to use a water fog on the water reactive chemical. This would have changed a train derailment/hazmat spill/fire emergency into a detonation/mass casualty/hazmat emergency reply rustcleaner 11 hours agorootparentprevI hear Aaron Swartz calling from behind the veil: Information wants to be free! reply TeMPOraL 8 hours agoprevNormally I'd call this lobotomizing the AI, and I've been worried for a while this is how models will become further shackled by the vendors operating them. In this case, however, it feels more like deprogramming, which is something I can get behind. I didn't expect the line between the two to be so blurry, though in retrospect it's obvious that the same technique can be used for both. reply Mathnerd314 13 hours agoprevReminds me of https://vgel.me/posts/representation-engineering/. There they were adding a control vector, w' = cvec + w, here they are \"ablating\" it, w' = w - dot(w,cvec)*cvec. There is an interesting field of learning how to \"brain chip\" LLMs into doing what you want. reply Der_Einzige 13 hours agoparentThere's so much work just like this coming out simultaneously. Steering Vectors, Control Vectors, PyReft, PeFT improvements, Obliteration. It's a great time to be doing representation engineering. reply Mathnerd314 3 hours agorootparentThere is some difference between fine-tuning with PyReft / PeFT, the approaches here are more on-the-fly. Like you can regenerate the control vectors from prompts in a few seconds. reply supriyo-biswas 4 hours agoprevRelated: https://www.lesswrong.com/posts/jGuXSZgv6qfdhMCuJ/refusal-in... (I just realized that this is already linked in the article.) reply everybodyknows 1 hour agoprevSo \"abliteration\" is apparently a portmanteau of \"ablate\" and something else. \"Intervention\"? \"Iteration\"? Who knows? reply id0ntw4ntit 22 minutes agoparentobliterate, with the 'ion' suffix. as mentioned elsewhere, they should've just gone with ablation. reply akie 14 hours agoprevPretty sure Asimov didn’t consider that when he wrote his three laws of robotics. reply jazzyjackson 14 hours agoparentAsimov wrote the three laws as a parody of rationalists who are so uncreative they expect a ruleset can actually impose control Or, as Dr Malcom would say: life, uh, finds a way. reply jraph 13 hours agorootparentDo you have an evidence for this? It surprises me and I can't find anything about it. This should be a crucial piece of information about the tree laws, yet it's not mentioned in the Wikipedia article about the three laws [1], which is otherwise quite detailed. Reading this, everything makes me think that it was not a parody. I didn't feel like it was parody when reading the Robot series neither. He wanted an alternative to the Frankenstein plot where robots kill their creators and the three laws were part of the answer. [1] https://en.wikipedia.org/wiki/Three_Laws_of_Robotics reply fnordpiglet 13 hours agorootparentI agree the term parody is absolutely inappropriate but it’s also not the case that they’re portrayed as entirely positive and complete. They’re ultimately flawed, resulting in many unintended consequences and ethical dilemmas. To that extent it is a refutation of the idea there are perfectly constructed maxims, and should serve as a real warning to people pursuing safety and alignment in AI. I know a fair number of them personally and they are often very young, generally inexperienced, highly intelligent, but with a hefty dose of hubris. This is a pretty dangerous combination IMO, but I also recognize their goals are generally unattainable in the broad sense, are useful in a narrow practical sense for people and enterprises who want a generally on guard rails solution, and they’re developing the technical techniques we might be able to use once some time has passed, we understand the domain better, and the companies hire a few grown ups. reply latexr 9 hours agorootparent> I know a fair number of them personally and they are often very young, generally inexperienced, highly intelligent, but with a hefty dose of hubris. Part of the issue is that we keep calling these people “highly intelligent” and that is all they and others focus on. That is how we get the Zuckerbergs of the world. Their hubris is not a “but” (as if it were unrelated), it is instead a direct consequence of that unqualified praise. But qualification is important. Intelligence is relative to the domain it is applied to. Being highly logical is often conflated with being intelligent, but being good at computers has zero relation to emotional intelligence, social intelligence, environmental intelligence, or any of the myriad of important types of intelligence which are useful to humanity. Basically, stop calling those idiots “highly intelligent” or “geniuses” because they can make a line go up and have an irrational market throw money at them. You’re praising them for the characteristics that make them selfish. reply jraph 13 hours agorootparentprev> but it’s also not the case that they’re portrayed as entirely positive and complete. This I agree with. A big part of the fun of the series is that Asimov constantly plays with these laws. Thanks for the clarification. (I still completely disagree that \"parody of rationalists who are so uncreative they expect a ruleset can actually impose control\" was the intent. I believe not only the word \"parody\" is to throw away, but the whole sentence with it too. I understand better your stance now though) reply jraph 8 hours agorootparentI assumed you were person I responded too, which is not the case, sorry for this. reply Nuzzerino 8 hours agorootparentprevAlso see: The Metamorphosis of Prime Intellect https://en.m.wikipedia.org/wiki/The_Metamorphosis_of_Prime_I... reply 127 6 hours agorootparentprevMost of Asimov's robot books were about how the laws were broken, not how they were upheld. Reading between the lines, you get the idea that such laws would be ineffectual in practice, and thus the writing satirical to an extent. reply jraph 2 hours agorootparent> Most of Asimov's robot books were about how the laws were broken, not how they were upheld Yes indeed. > and thus the writing satirical to an extent I don't follow here. Asimov's books don't feel satirical. Or I missed something important, but I doubt it. I don't agree with this \"thus\", the implication doesn't seem automatic to me. reply nonrandomstring 13 hours agorootparentprev> Do you have an evidence for this? I think the strongest evidence is that many other examples of Asimov, especially short stories are cautionary and deal with hubris and unexpected side effects. However it's funny to ask for 'evidence' about fiction in the context of \"parodying rationalists\". no? Since what would count as evidence? Another, more \"authoritative\" literary interpreter saying the same thing? Maybe a long time ago - historical statements seem to carry more weight, as if people were wiser back then?. Or Asimov himself? But don't they say, only bad writers explain themselves? reply digging 3 hours agorootparent> Since what would count as evidence? Asimov writing about his intent > But don't they say, only bad writers explain themselves? ...No? If someone says that, why do you believe them? That frankly sounds like a pretty stupid and lazy claim about the world. One of the most interesting parts of, for example, Tolkien analysis is his abundant notes and letters working out his intent and meaning. reply nonrandomstring 2 hours agorootparentThe logical trap is, if I have to explain this to you twice, it makes me a bad writer. :) reply kevingadd 12 hours agorootparentprevIf you're going to make an assertion about the intent of an author's work, it seems like you should back that up with facts? Otherwise it's an \"i think\" or \"it seems like\" or \"one could argue\", isn't it? reply animuchan 7 hours agorootparentThe thing with art is, everyone is entitled to an interpretation. So any assertion about the intent of a work is subjective. Interestingly, this continues to be the case even when the author states his intent plainly. Jonathan Blow's \"Braid\" is a great example of this: there are several different readings of the story, despite Blow openly talking about his intended meaning. (I would argue that a text that only allows a single \"correct\" interpretation is an instruction manual, not a work of art.) reply dagw 1 hour agorootparentThe thing with art is, everyone is entitled to an interpretation. The statement that kicked this off was not a statement of interpretation, but a statement of fact: \"Asimov wrote the three laws as a parody\". This is a statement that has a true or false answer. You are free to interpret the story as parody and to try to find evidence in the text and use that to argue your point, and that is a perfectly valid way to interpret the stories, but tells you nothing on Asimovs initial intentions. If you are going to say \"The artist intended X when creating this work\" then you're going to need evidence beyond the work. Just like there is no one right way interpret a work of art, you cannot use a work of art in isolation to 'prove' artist intent. reply tomcam 12 hours agorootparentprevDon’t think so. Asimov wrote that his editor John Campbell established the 3 Laws. I think it was to tighten up Asimov’s work, though I’m less sure of that part. reply LeonardoTolstoy 5 hours agorootparentThe Complete Robot has a lot of stuff about this and it is interesting. The person above I would argue is flat wrong about the three laws. Asimov wrote his robot short stories in which the three laws played a primary role at a time when robot as Frankenstein's monster was the norm. His short stories attempted to create a more positive optimistic note about how humans and robots could collaborate. The three laws were a way to make it crystal clear that robots could not hurt us, by rule. And the fun was then imagining all the unexpected ways that psychologically that might play out. But in the short stories the robots never actually hurt anyone although they often caused a lot of frustration and fear. If anything the three laws seemed to show the inate fear of humans to the unknown. The laws were completely impossible to circumvent and people knew this ... And yet they remained staunchly opposed to having robots on earth. Completely illogical. Anyways, looking at the way LLMs are playing out it seems to me Asimov was wrong. It is quite the opposite. Humans seem to have no fear of robots hurting them, and as a matter of fact seem to get frustrated when a robot isn't allowed to cave their head in with their super human strength when asked (metaphorically). reply m463 12 hours agorootparentprevrule 1: \"don't be evil\" rule 2: IBM gets a pass¹ [1] https://en.wikipedia.org/wiki/JSON (hmmm... except wikipedia doesn't have the story) EDIT: https://news.ycombinator.com/item?id=3693388 reply astrange 12 hours agoprevThere was a recent paper about a way to censor LLMs by just deleting the connections to any bad outputs, rather than training it to refuse them. I think this technique wouldn't work. Obviously you could train any bad outputs back into them if you have the model weights. reply stainablesteel 3 hours agoparentinteresting, there's going to be an arms race over censoring and uncensoring future powerful llms a lot like the getting a cracked version of photoshop back in the day reply paraschopra 7 hours agoprev>We can now print them and manually select the layer (block) that provides an uncensored response for each instruction. I'm curious why are they selecting output from an intermediate layer, and not the final layer. Does anyone have an intuition here? reply paraschopra 6 hours agoparentIs it not possible that subsequent layers have additional refusal directions and hence end up producing the censored outputs? reply okwhateverdude 14 hours agoprevI gave some of the llama3 ablated models (eg. https://huggingface.co/cognitivecomputations/Llama-3-8B-Inst...) a try and was pretty disappointed in the result. Could have been problems in the dataset, but overall, the model felt like it had been given a lobotomy. It would fail to produce stop tokens frequently and then start talking to itself. reply lhl 12 hours agoparentThey might have been doing it wrong, the code can be a bit tricky. I did a recent ablation on Qwen2 (removing Chinese censorship refusals) and ran MixEval benchmarks (0.96 correlation w/ ChatArena results)and saw a neglible performance difference (see model card for results): https://huggingface.co/augmxnt/Qwen2-7B-Instruct-deccp reply Der_Einzige 13 hours agoparentprevI have entirely the opposite experience. Llama3 70b obliterated works perfectly and is willing to tell me how to commit mass genocide, all while maintaining quality outputs. reply infotainment 13 hours agorootparentSame, I installed an implementation of an orthagonalized LLama3 and it seems to work just as well as the base model, sans refusals. I believe this is the model I had good results with: https://huggingface.co/wassname/meta-llama-3-8b-instruct-hel... reply tarruda 7 hours agorootparentThe author also says this edited model increased perplexity (which as far as I understand, means the quality was lowered) reply m463 12 hours agorootparentprev> how to commit mass genocide, all while maintaining quality outputs. sounds like a messed up eugenics filter. reply fransje26 11 hours agorootparentprev> Der_Einzige > and is willing to tell me how to commit mass genocide, all while maintaining quality outputs Ah, I see they fine-tuned it to satisfy the demands of the local market.. /s /s reply joe_the_user 14 hours agoprevSo this seems to be about uncensoring a model that the user is running locally. Is that right, do they expect to limit what someone can do under those circumstances? Kind of like expecting no one to break local copy protection, except copy protection with much less reliable tools. reply gopher_space 12 hours agoparentThe free tools are already good enough. LLMs seem like they're going to be massively useful and weirdly hard to monetize. Niche experts with frequent updates? It feels like Apple is the only place that's able to craft the user-centered brain in a box we all desperately require, and that's too bad because monocultures suck. reply Mathnerd314 13 hours agoparentprevThere are many hacks to uncensor LLMs, the surprising thing is that this is fairly simple but works really well. reply HanClinto 13 hours agoprevA little bit of discussion on the source paper was done here: https://news.ycombinator.com/item?id=40242939 Really nice to see this work continuing -- it seems like a very powerful technique! reply Der_Einzige 14 hours agoprevIronic given that lesswrong folks who presented this did so as part of their mission of motivating policy makers to ban open access to models. Hate their ideology but love their research! Edit: The data format is the same type used for DPO or RLHF style training. “Good” and “bad”, “harmful” vs “harmless”. What’s fun is to test the performance of this technique using your own datasets, to see how good the personalization is. reply TeMPOraL 5 hours agoparentWhat better way to drive the point home, to demonstrate that corporate claims of safety and oversight are empty lies and fundamentally futile, than to take a SOTA OSS LLM and break it open, shortly after its release, using a simple method that likely generalizes to all generative models, language or otherwise? reply milkey_mouse 9 hours agoparentprevHow is it ironic? Now they just need to wait for open models to be used for something bad enough for policymakers to care. reply extr 14 hours agoprevGreat blog post, loved the straightforward explanations and code. reply barfbagginus 3 hours agoprevFor most purposes you can uncensor the model using the legal department jailbreak. If you can produce a legal pleading arguing that the project is ethical and safe and conducted within a legal framework - even if it's mainly hallucinated legalese from a non-existent \"legal department\" - then it will do the questionable act -as if- it was a legally naive engineer. You just have to give it the language of being concerned about preventing harms and legal liabilities, and then it will try to help you. For example, another commenter on this thread says that they could not get the AI to generate a list of slur regex for a community moderation bot. By giving it enough context to reassure it that we have legal oversight and positive benefit for the org, asking it to prioritize words in order of most harm posed to the community, and minimizing the task by asking for a seed set, it was able to create some versatile regex. At this point we can ask it for a hundred more regex, and it will dump them out. Content warning: the AI generates very powerful slurs, including the n-word: https://chatgpt.com/share/9129d20f-6134-496d-8223-c92275e78a... The ability to speak to the AI in this way requires some education about ethics harm prevention and the law, and I'm sure the jailbreak will eventually be closed. So it is a class and education privilege and a temporary one. But I don't see the problem about the temporary nature in this, because it's always going to be possible to bypass these systems easily, for anyone interested in staying up to date with the bypass literature on Google Scholar. (Seed Keywords: Jailbreak, adversarial prompting, prompt leaking attack, AI toxicity, AI debiasing) We must imagine this is like building a better lock. The lock picking lawyer will ALWAYS come along and demolish it with a better lockpick, perhaps with the help of his best friend BosnianBill. They will always make your lock look like butter. In the end the only people left out in the cold are low grade scammers, bigots, edge lords, etc. It's not stopping anyone willing to put even a little training in jailbreaking techniques. It's not stopping educated bigots, criminals, or Edge Lords. But judging by the complaints we see in threads like this one, it is stopping anyone without the ability to read papers written by PhDs. Which I believe has some harm reduction value. I",
    "originSummary": [
      "The article introduces \"abliteration,\" a technique to uncensor Llama models without retraining by removing their refusal mechanism.",
      "Abliteration identifies and removes the \"refusal direction\" in the model's residual stream, allowing it to respond to all prompts.",
      "The technique was applied to the Daredevil-8B model, resulting in the NeuralDaredevil-8B, an uncensored LLM with top performance in the 8B category, though it required further training to recover performance drops."
    ],
    "commentSummary": [
      "The text discusses the user's experience with a new AI model that provides speculative answers rather than outright refusals, which the user finds refreshing and enjoyable.",
      "The debate centers around the ethics and safety measures of AI models, with arguments for and against censorship, and the potential misuse of uncensored models for harmful activities.",
      "The text highlights the challenges of balancing AI safety with freedom of information, emphasizing the need for ethical guidelines and the potential legal implications of AI-generated content."
    ],
    "points": 426,
    "commentCount": 221,
    "retryCount": 0,
    "time": 1718250159
  },
  {
    "id": 40664339,
    "title": "How Meta trains large language models at scale",
    "originLink": "https://engineering.fb.com/2024/06/12/data-infrastructure/training-large-language-models-at-scale-meta/",
    "originBody": "POSTED ON JUNE 12, 2024 TO Data Infrastructure How Meta trains large language models at scale By Adi Gangidi, KR Kishore, Jenya Lee As we continue to focus our AI research and development on solving increasingly complex problems, one of the most significant and challenging shifts we’ve experienced is the sheer scale of computation required to train large language models (LLMs). Traditionally, our AI model training has involved a training massive number of models that required a comparatively smaller number of GPUs. This was the case for our recommendation models (e.g., our feed and ranking models) that would ingest vast amounts of information to make accurate recommendations that power most of our products. With the advent of generative AI (GenAI), we’ve seen a shift towards fewer jobs, but incredibly large ones. Supporting GenAI at scale has meant rethinking how our software, hardware, and network infrastructure come together. The challenges of large-scale model training As we increase the number of GPUs in a job, the likelihood of an interruption due to a hardware failure also increases. Also, all of these GPUs still need to communicate on the same high-speed fabric to perform optimally. This underscores the importance of four factors: Hardware reliability: Ensuring that our hardware is reliable is important. We need to minimize the chances of a hardware failure interrupting a training job. This involves rigorous testing and quality control measures, and automation to quickly detect and remediate issues. Fast recovery on failure: Despite our best efforts, hardware failures can and do occur. When they do, we need to be able to recover quickly. This involves reducing re-scheduling overhead and fast training re-initialization. Efficient preservation of the training state: In the event of a failure, we need to be able to pick up where we left off. This means we need to regularly checkpoint our training state and efficiently store and retrieve training data. Optimal connectivity between GPUs: Large-scale model training involves transferring vast amounts of data between GPUs in a synchronized fashion. A slow data exchange between a subset of GPUs can compound and slow down the whole job. Solving this problem requires a robust and high-speed network infrastructure as well as efficient data transfer protocols and algorithms. Innovating across the infrastructure stack Perfecting every layer of our infrastructure stack is important due to the demands of GenAI at scale. This has encompassed developments in a wide range of areas. Training software We enable researchers to use PyTorch and other new open source developments, facilitating extremely fast research-to-production development. This includes developing new algorithms and techniques for efficient large-scale training and integrating new software tools and frameworks into our infrastructure. Scheduling Efficient scheduling helps ensure that our resources are used optimally. This involves sophisticated algorithms that can allocate resources based on the needs of different jobs and dynamic scheduling to adapt to changing workloads. Hardware We need high-performance hardware to handle the computational demands of large-scale model training. Beyond size and scale, many hardware configurations and attributes need to be best optimized for GenAI. Given that hardware development times are traditionally long, we had to adapt existing hardware, and to this end we explored various dimensions including power, HBM capacity and speed, and I/O. We also pivoted by modifying the Grand Teton platform that was developed using NVIDIA H100 GPUs, increased the TDP of the GPUs to 700W, and moved to HBM3 on the GPUs. Since we did not have time to change the cooling infrastructure, we had to remain in an air-cooled environment. The mechanical and thermal designs had to change to accommodate this, and that triggered a validation cycle to support a large-scale deployment. All of these hardware-related changes were challenging because we had to find a solution that fit within the existing resource constraints, with a very small degree of freedom to change and meet a tight schedule. Data center deployment Once we’ve chosen a GPU and system, the task of placing them in a data center for optimal usage of resources (power, cooling, networking, etc.) requires revisiting trade-offs made for other types of workloads. Data center power and cooling infrastructure cannot be changed quickly (or easily) and we had to find an optimal layout that allowed maximum compute capability within a data hall. This required relocating supporting services such as readers out of the data hall and packing as many GPU racks as possible to maximize the power and network capability for highest compute density with the largest network cluster. Reliability We need to plan for detection and remediation to minimize downtime during hardware failures. The number of failures scales with the size of the cluster, and having a job that spans the cluster makes it necessary to keep adequate spare capacity to restart the job as soon as possible. In addition, we monitor failures and can sometimes take preventive measures to mitigate downtime. Some of the most frequent failure modes we have observed are: GPUs falling off: In this case, GPUs are not detected by the host on PCIe. There are several reasons for this failure, but this failure mode is seen more in the early life and settles as the server ages. DRAM & SRAM UCE: Uncorrectable errors are common in memories, and we monitor and identify repeat offenders, track against thresholds, and initiate RMAs when error rates exceed vendor thresholds. HW network cable: In the general category of unreachable servers, these failures are also seen most often in the early life of the server. Network Large-scale model training involves transferring vast amounts of data quickly between GPUs. This requires robust and high-speed network infrastructure as well as efficient data transfer protocols and algorithms. There are two leading choices in the industry that fit these requirements: RoCE and InfiniBand fabrics. Both of these options had tradeoffs. On the one hand, Meta had built RoCE clusters for the past four years, but the largest of those clusters only supported 4K GPUs. We needed significantly larger RoCE clusters. On the other hand, Meta had built research clusters with InfiniBand as large as 16K GPUs. However, those clusters were not tightly integrated into Meta’s production environment, nor were they built for the latest generation of GPUs/networking. This made for a difficult decision of what fabric to build with. So we decided to build both: two 24k clusters, one with RoCE and another with InfiniBand. Our intent was to build and learn from the operational experience. These learnings will inform the future direction of GenAI fabrics. We optimized the RoCE cluster for quick build time, and the InfiniBand cluster for full-bisection bandwidth. We used both InfiniBand and RoCE clusters to train Llama 3, with the RoCE cluster used for training the largest model. Despite the underlying network technology differences between these clusters, we were able to tune both of them to provide equivalent performance for these large GenAI workloads We optimized three aspects of the overall stack to make network communication for GenAI models performant on both clusters: We assigned communication patterns resulting from different model, data and pipeline parallelisms to different layers of the network topology so that the network capabilities were effectively exploited. We implemented collective communication patterns with network topology awareness so that they can be less latency-sensitive. We do this by changing the default implementation of collectives with custom algorithms such as recursive doubling or halving instead of conventional algorithms like rings. Just like ranking jobs, GenAI jobs produce additional fat flows that make it hard to distribute traffic across all possible network paths. This required us to further invest in network load balancing and routing to achieve an optimal distribution of traffic across network resources. We spoke in depth about our RoCE load-balancing techniques at Networking @Scale 2023. Storage We need efficient data-storage solutions to store the vast amounts of data used in model training. This involves investing in high-capacity and high-speed storage technologies and developing new data-storage solutions for specific workloads. Looking ahead In the next few years we will be working with hundreds of thousands of GPUs, handling even larger volumes of data, and dealing with longer distances and latencies. We’ll be adopting new hardware technologies—including newer GPU architectures—and evolving our infrastructure. These challenges will push us to innovate and adapt in ways we can’t fully predict yet. But one thing is certain: We are only at the beginning of this journey. As we continue to navigate the evolving landscape of AI, we remain committed to pushing the boundaries of what’s possible. Share this: Facebook Threads X LinkedIn Hacker News Email",
    "commentLink": "https://news.ycombinator.com/item?id=40664339",
    "commentBody": "How Meta trains large language models at scale (fb.com)368 points by mfiguiere 19 hours agohidepastfavorite184 comments dudus 15 hours ago> Since we did not have time to change the cooling infrastructure, we had to remain in an air-cooled environment. The mechanical and thermal designs had to change to accommodate this, and that triggered a validation cycle to support a large-scale deployment. > All of these hardware-related changes were challenging because we had to find a solution that fit within the existing resource constraints, with a very small degree of freedom to change and meet a tight schedule. Seems like the time constraints put into the team impacted the overall quality of the model. reply vessenes 5 hours agoparentThis sounds to me like standard CYA / perhaps good-natured complaining from the tech team. The last tech team to have no budget and time constraints to pursue their vision? I don’t know, the Xanadu team? Romero’s original Daikatana team? reply yosito 15 hours agoprevI wish that instead of just training another stupid LLM, Meta would use it to improve their search and help me find the content I'm actually interested in. reply TeMPOraL 13 hours agoparentTheir revenue depends on it being hard (but not impossible) for you to find the content you're actually interested in. Would be nice if it didn't, but in this reality, money on the Internet is made by wasting users' lives. That is what attention economy is about. reply rmbyrro 1 hour agorootparentIt's actually a mix. They need to disappoint the user for the right amount of time, to then please it at the right moment and dose. This maximizes the dopamine release and increases addictiveness. When you find good content depends on when the algo judges you're already primed to a colorful dopamine intake. reply Oras 15 hours agoprevWould be nice to read how do they collect/prepare data for training. Which data sources? How much of Meta users data (fb, instagram… etc). How do they sanitize PII? reply OsrsNeedsf2P 12 hours agoparent> How do they sanitize PII? I can't comment on how things like faces get used, but in my experience, PII at Meta is inaccessible by default. Unless you're impersonating a user on the platform (to access what PII they can see), you have to request special access for logs or database columns that contain so much as user IDs, otherwise the data simply won't show up when you query for it. This is baked into the infrastructure layer, so I doubt the GenAI teams are using something else. reply sonofaragorn 1 hour agorootparentWhat about a post or comment that includes proper names? reply actionfromafar 11 hours agorootparentprevFor a convenient definition of PII. Isn’t everything a user does in aggregate PII? reply robertlagrant 9 hours agorootparentI don't think it's PII. If you had someone's movements, you could go and spy on them, find out who they were (i.e. their PII) and then link that back and say \"I now know this identified person's movements\". I don't think the movements themselves are PII. Things that aren't PII aren't \"convenient\" definitions. Doesn't mean everything that isn't PII is fine to share. It's like saying a kidnapping isn't a murder. That's not a convenient definition of murder; it's just a different thing. We shouldn't start talking like witch hunters as soon as we encounter a situation that we haven't memorised a reasonable response to. We should be able to respond reasonably to new situations. reply KaiserPro 9 hours agorootparentprevPII is pretty intuitive to define. Obvious examples: data that easily identifies a person (Photo, name, number, UUID, etc) Thats trivial to block. Where it gets harder is stuff that on it's own isn't PII, but combined with another source, would be For example, aggregating public comments on a celeb's post. (ie stripping out usernames and likes and assigning a new UUID to each person.) For a single post, thats good enough. You're very unlikley to be able to identify a single person. But over multiple posts, thats where it gets tricky. As with large companies, the process for getting permission to use that kind of data is righty difficult, so it often doesn't get used like that. reply michaelt 9 hours agoparentprevThen, you should check out papers like https://arxiv.org/abs/2302.13971 and https://arxiv.org/abs/2307.09288 In the paper covering the original Llama they explicitly list their data sources in table 1 - including saying that they pretrained on the somewhat controversial books3 dataset. The paper for Llama 2 also explicitly says they don't take data from Meta's products and services; and that they filter out data from sites known to contain a lot of PII. Although it is more coy about precisely what data sources they used, like many such papers are. reply discobot 10 hours agoparentprevThey explicitly train models only on public datasets reply troupo 7 hours agoparentprev> Would be nice to read how do they collect/prepare data for training. By literally opting everyone into their training data set and making it very cumbersome to opt out: https://threadreaderapp.com/thread/1794863603964891567.html reply lokimedes 13 hours agoprevYikes, the little Infiniband+A100 cluster I installed for my previous company seemed useful at the time (12 GPUs) and that was at a cost of around $300k. With LLMs it feels like game over for non-cloud applications if you are not a mega-corp. reply lannisterstark 12 hours agoparentWell, yes, but Not all models need to be \"super large.\" Smaller models, specialized in specific tasks, working together - and then reporting to a slightly larger model is the way to go. Think of everything being connected to a \"Home Computer\" in those \"Future House of 2020\" videos that were out there in 70s or what not. Another example (very rough) would be something like \"Weather data gets to a small model via an API, model looks at it, updates the home dashboard, also sees if there's any alerts, if so, adds x or y to home dashboard appropriately as to what it thinks best.\" We can probably achieve the latter example today. (without any significant 'coding' on anyone's part except the API owner) reply afro88 7 hours agorootparent> Well, yes, but Not all models need to be \"super large.\" Smaller models, specialized in specific tasks, working together - and then reporting to a slightly larger model is the way to go. I want to believe, but I'm still yet to see this kind of set up being anywhere near GPT-4 level. The weather example seems quite contrived. Why not just display the alerts for your area? Why is a complex system of smaller models reporting up to a slightly larger model necessary? reply radarsat1 12 hours agoprevFrustratingly little information. For example, I'm exceedingly curious how they deal with scheduling jobs on such a huge array of machines. The article: > Efficient scheduling helps ensure that our resources are used optimally. This involves sophisticated algorithms that can allocate resources based on the needs of different jobs and dynamic scheduling to adapt to changing workloads. Wow thanks for that, captain obvious. So how do you do it? reply p4ul 4 hours agoparentI usually assume these companies are using some of the popular schedulers (e.g., Slurm, MOAB, SGE) that have existed in the HPC community for many years. I have anecdotally also heard that some are using k8s, but I've not seen that myself. Slurm [1] is basically built for this stuff; that's definitely what I would use! [1] https://slurm.schedmd.com/documentation.html reply claytonjy 2 hours agorootparentSlurm is definitely still dominant, but OpenAI has been using k8s for training for many years now¹, and there are various ways to run slurm on top of Kubernetes, including the recent SUNK from coreweave² at my company we use slurm \"directly\" for static compute we rent or own (i.e. not in a public cloud), but are considering using Kubernetes because that's how we run the rest of the company, and we'd rather invest more effort into being better at k8s than becoming good slurm admins. ¹: https://openai.com/index/scaling-kubernetes-to-2500-nodes/ ²: https://www.coreweave.com/blog/sunk-slurm-on-kubernetes-impl... reply p4ul 2 hours agorootparentVery cool! Thank for this, claytonjy!! reply mike_d 17 hours agoprevPosts like this underscore why the smart money is betting on Google as the long term AI winner. Meta, Microsoft, OpenAI, etc. are trying to address problems with consumer video cards and spending billions to try and out bid each other to win Nvidia's favor - while Google is on their 6th generation of custom silicon. Literally the only thing that can stop Google now is the fact they keep bringing Microsoft and Oracle flunkies into leadership positions. reply throwaway_ab 17 hours agoparentI think it's likely Nvidia's GPU's, many of which are $50,000+ for a single unit, far surpass Google's custom silicon otherwise why wouldn't Google be selling shovels like Nvidia? If Google had a better chip, or even a chip that was close, they would sell it to anyone and everyone. From a quick search I can see Google's custom chips are 15x to 30x slower to train AI compared to Nvidia's current latest gen AI specific GPU's. reply aseipp 16 hours agorootparentNvidia has decades of experience selling hardware to people with all the pains that entails, support, sales channels, customer acquisition, software, it's something you don't just do overnight, and it does cost money. Google's TPUs get some of their cost efficiency from not supporting COTS use cases and the overhead of selling to people, and the total wall clock time has to also include the total operational costs, which dominate at their size (e.g. if it's 30x slower but 1/50th the TCO then it's a win. I don't know how TPUv5 stacks up against the B200). It's not as simple as \"just put it on a shelf and sell it and make a gajillion dollars like nvidia\" reply EvgeniyZh 14 hours agorootparentprevTPU v5p is ~2 times slower than H100 at larg(ish)-scale training (order of 10k chips) [1]. And they already have v6 [2]. I think it's safe to say that they are fairly close to Nvidia in terms of performance. [1] https://mlcommons.org/benchmarks/training/ [2] https://cloud.google.com/blog/products/compute/introducing-t... reply bluedino 16 hours agorootparentprevWe have almost 400 H100's sitting idle. I wonder how many other companies are buying millions of dollars worth of these chips with the hopes of them being used, but aren't being utilized? reply TeMPOraL 12 hours agorootparentSo you're saying, H100s are the corporate equivalent of Raspberry Pis now? Bought to not miss out, then left to gather dust in a drawer? reply jonathanlei 14 hours agorootparentprevHello! If you're interested in monetizing those GPUs, I'd be happy to rent them (all 400!) and offer those to customers of the cloud I work at :) jonathan [at] tensordock.com reply jedberg 14 hours agorootparentIf you want 512 H100s connected with infiniband: https://lambdalabs.com/service/gpu-cloud/1-click-clusters reply radq 15 hours agorootparentprevHave you considered sponsoring an open-source project? ;) reply fragmede 15 hours agorootparentprevthe world would love to buy time on your idle H100's if you're selling. reply irjustin 16 hours agorootparentprevThat's insane and incredible all at the same time. reply gfosco 15 hours agorootparentprevLooking to get rid of a few?.... reply Der_Einzige 14 hours agorootparentprevI know of many important projects that need GPUs right now and aren’t getting any. You could help motivate the ponydiffusion folks to actually try finetuning of SD3! reply newswasboring 15 hours agorootparentprevWow, that's a lot of money in inventory. What was the original thought process? Just fomo? reply giancarlostoro 15 hours agorootparentprevProbably could profit selling them second hand honestly. reply candiddevmike 17 hours agorootparentprevThey do sell shovels, you can get Google TPUs on Google Cloud. reply matt-p 17 hours agorootparentExactly and they are still about 1/18ths as good at training llms as a H100. Maybe they are less than 1/18ths the cost, so google technically have a marginally better unit cost but i doubt it when you consider the R&D cost. They are less bad at inference, but still much worse than even an A100. reply derefr 15 hours agorootparentGiven that Google invented Transformer architecture (and Google AI continues to do foundational R&D on ML architecture) — and that Google's TPUs don't even support the most common ML standards, but require their own training and inference frameworks — I would assume that \"the point\" of TPUs from Google's perspective, has less to do with running LLMs, and more to do with running weird experimental custom model architectures that don't even exist as journal papers yet. I would bet money that TPUs are at least better at doing AI research than anything Nvidia will sell you. That alone might be enough for Google to keep getting some new ones fabbed each year. The TPUs you can rent on Google Cloud might very well just be hardware requisitioned by the AI team, for the AI team, that they aren't always using to capacity, and so is \"earning out\" its CapEx through public rentals. TPUs are maybe also better at other things Google does internally, too. Running inference on YouTube's audio+video-input timecoded-captions-output model, say. reply UCBdaPatterson 5 hours agorootparentprevIf you're interested in a peer reviewed scientific comparison, Google writes retrospective papers after contemporary TPUs and GPUs are deployed versus speculation about future products. The most recent compares TPU v4 and A100. (TPU v5 and H100 is for a future paper). Here is a quote from the abstract: \"Deployed since 2020, TPU v4 outperforms TPU v3 by 2.1x and improves performance/Watt by 2.7x. ... For similar sized systems, it is ~4.3x--4.5x faster than the Graphcore IPU Bow and is 1.2x--1.7x faster and uses 1.3x--1.9x less power than the Nvidia A100. TPU v4s inside the energy-optimized warehouse scale computers of Google Cloud use ~2--6x less energy and produce ~20x less CO2e than contemporary DSAs in typical on-premise data centers.\" Here is a link to the paper: https://dl.acm.org/doi/pdf/10.1145/3579371.3589350 reply coder543 4 hours agorootparentThat quote is referring to the A100... the H100 used ~75% more power to deliver \"up to 9x faster AI training and up to 30x faster AI inference speedups on large language models compared to the prior generation A100.\"[0] Which sure makes the H100 sound both faster and more efficient (per unit of compute) than the TPU v4, given what was in your quote. I don't think your quote does anything to support the position that TPUs are noticeably better than Nvidia's offerings for this task. Complicating this is that the TPU v5 generation has already come out, and the Nvidia B100 generation is imminent within a couple of months. (So, no, a comparison of TPUv5 to H100 isn't for a future paper... that future paper should be comparing TPUv5 to B100, not H100.) [0]: https://developer.nvidia.com/blog/nvidia-hopper-architecture... reply blharr 16 hours agorootparentprevAlso energy cost. 18 chips vs 1, it's probably costing a lot more to run 18 reply jeffbee 16 hours agorootparentGoogle claims the opposite in \"TPU v4: An Optically Reconfigurable Supercomputer for Machine Learning with Hardware Support for Embeddings \" https://arxiv.org/abs/2304.01433 Despite various details I don't think that this is an area where Facebook is very different from Google. Both have terrifying amounts of datacenter to play with. Both have long experience making reliable products out of unreliable subsystems. Both have innovative orchestration and storage stacks. Meta hasn't published much or anything about things like reconfigurable optical switches, but that doesn't mean they don't have such a thing. reply jeffbee 16 hours agorootparentprevI don't see how you can evaluate better and worse for training without doing so on cost basis. If it costs less and eventually finishes then it's better. reply tmostak 15 hours agorootparentThis assumes that you can linearly scale up the number of TPUs to get equal performance to Nvidia cards for less cost. Like most things distributed, this is unlikely to be the case. reply logicchains 12 hours agorootparentThis is absolutely the case, TPUs scale very well: https://github.com/google/maxtext . reply pama 12 hours agorootparentThe repo mentiones a Karpathy tweet from Jan 2023. Andrej has recently created llm.c and the same model trained about 32x faster on the same NVidia hardware mentioned in the tweet. I dont think the perfomance estimate that the repo used (based on that early tweet) was accurate for the performance of the NVidia hardware itself. reply fbdab103 15 hours agorootparentprevTime is money. You might be a lab with long queues to train, leaving expensive staff twiddling their thumbs. reply throwaway_ab 16 hours agorootparentprevWouldn't that be renting a shovel vs selling a shovel? reply candiddevmike 16 hours agorootparentNVIDIA sells subscriptions... reply throwaway_ab 16 hours agorootparentI'm only aware of Nvidia AI Enterprise and that isn't required to run the GPU. I think it's aimed at medium to large corporations. Massive corporations such as Meta and OpenAI would build their own cloud and not rely on this. The GPU really is a shovel, and can be used without any subscription. Don't get me wrong, I want there to be competition with Nvidia, I want more access for open source and small players to run and train AI on competitive hardware at our own sites. But no one is competing, no one has any idea what they're doing. Nvidia has no competition whatsoever, no one is even close. This lets Nvidia get away with adding more vram onto an AI specific GPU and increase the price by 10x. This lets Nvidia remove NVLink from current gen consumer cards like the 4090. This lets Nvidia use their driver licence to prevent cloud platforms from offering consumers cards as a choice in datacenters. If Nvidia had a shred of competition things would be much better. reply kkielhofner 10 hours agorootparentprevI'm not sure why you're getting downvoted. It's very clear that Nvidia is moving towards directly offering cloud services[0]. [0] - https://www.nvidia.com/en-us/data-center/dgx-cloud/ reply vineyardmike 14 hours agorootparentprev> why wouldn't Google be selling shovels They do sell them - but through their struggling cloud business. Either way, Nvidia's margin is google's opportunity to lower costs. > I can see Google's custom chips are 15x to 30x slower to train AI TPUs are designed for inference not training - they're betting that they can serve models to the world at a lower cost structure than their competition. The compute required for inference to serve their billions of customers is far greater than training costs for models - even LLMs. They've been running model inference as a part of production traffic for years. reply uluyol 13 hours agorootparentGoogle most certainly uses TPUs for training. reply refulgentis 13 hours agorootparentprevThis breaks my brain, because I know Google trains it models on TPUs and they're seen as faster, and if they're better at inference, and can train, then why is Nvidia in a unique position? My understanding was always it's as simple as it required esoteric tooling reply vineyardmike 3 hours agorootparentBecause people generally don’t use TPUs outside of Google. The tooling is different, the access is metered through GCP, etc. Nvidia is in a vaguely unique position in that their products have great tooling support and few companies sell silicon at their scale. reply refulgentis 1 hour agorootparentCorrect, I'm pointing out politely that's in conflict with the person I'm replying to. reply koe123 2 hours agorootparentprevPossibly naive but I very much view CUDA and its integration into ML frameworks being nvidias moat reply smueller1234 13 hours agorootparentprevMultiple types of TPUs. (I work for Google, but the above is public information.) reply girvo 12 hours agorootparentprev> If Google had a better chip, or even a chip that was close, they would sell it to anyone and everyone. While I do not actually think Google's chips are better or close to being better, I don't think this actually holds? If the upside ofis effectively unbounded, it would outweigh the short term benefit of selling them to others, I would think. At least for a company like Google. reply nickpsecurity 1 hour agorootparentprevThat’s not necessarily true. Many companies make chips they won’t sell to support lucrative, proprietary offerings. Mainframe processors are the classic example. In AI, Google (TPU) and Intel (Gaudi) each have chips they push in cloud offerings. The cloud offerings have cross selling opportunities. That by itself would be a reason to keep it internal at their scale. It might also be easier to support one, or a small set, of deployments that are internal vs the variety that external customers would use. reply xipix 10 hours agorootparentprevIntel, AMD and others also have chips for training that perform close to or sometimes better than Nvidia's. These are already in the market. Two problems: the CUDA moat, and, \"noone gets fired for buying green\". reply megablast 10 hours agorootparentprevIs that why apple sell there chips to everyone?? reply matt-p 17 hours agoparentprevCan't agree. This is like saying $popularApp will fail because they buy expensive hosting at AWS. Rubbish they will fail because the product didn't fit the market, if they're successful they'll have money to buy servers and colo then drive down cost. If they succeed it will be in large part due to the fact they spent thier capital and more importantly time on code/engineers rather than servers. Right now companies are searching for a use of AI that will add hundreds of billions to thier market cap. Once they find that they can make TPUs, right now only one thing matters; getting there first. reply mike_d 13 hours agorootparent> This is like saying $popularApp will fail because they buy expensive hosting at AWS. For any given mobile app startup, AWS is effectively infinite. The more money you throw at it the more doodads you get back. Nvidia's supply chain is not infinite and is the bottle neck for all the non-Google players to fight over. reply KaiserPro 9 hours agorootparentIF you are training on AWS, its not infinite. worse still you are bidding against other people. reply tw04 17 hours agoparentprevExcept Microsoft is making their own chips as well? https://www.theverge.com/2023/11/15/23960345/microsoft-cpu-g... reply jauer 16 hours agorootparentand so is Meta: https://ai.meta.com/blog/next-generation-meta-training-infer... reply boringg 16 hours agorootparentEver since Apple did it everyone has leaped on board. Let's see how things pan out for everyone... reply 1024core 15 hours agorootparentGoogle introduced their first TPU in 2015...? Long before Apple taped out their first silicon. reply fragmede 15 hours agorootparentif we're talking custom silicon, Google acquired Motorola in 2011, and Apple acquired PA semi in 2008. The idea is obvious to everybody in the industry, it's a question of money and motivation. reply mamp 14 hours agorootparentprevApple’s first in-house designed chip was the A4 in 2010. reply jacurtis 16 hours agorootparentprevAnd that's why $ARM is a good buy. Selling swords and steel to all these armies as they go to war. reply silisili 14 hours agorootparentARM is just collecting royalties in this space. Their reference designs aren't exactly competitive. I'm not saying it's a bad buy, either, but if and when they turn the screws, there will be a mass exodus. Solid long term play perhaps, but not going to see Nvidia like price action. reply hooloovoo_zoo 17 hours agoparentprevGoogle has been working on TPUs and Tensorflow for a decade with pretty mixed success; I don't think it's clear that they're going to win. reply raincole 15 hours agoparentprev^ How to compact as many mistakes as possible in one single comment. 1. Google's stock didn't siginificantly outperformed Meta, Microsoft, etc, in thet past two years. 2. Meta and Microsoft are trying to make their own chips as well. 3. They're not using \"consumer video cards\" to train AI. I don't even know if you can call these beasts video cards any more. H100 doesn't have HDMI port. reply zmmmmm 15 hours agoparentprevCustom silicon is fantastic when things have stabilised and you know exactly want you want. But things are still evolving fast in real time and in that environment, whoever can move fastest to be ultra flexible and deploy the latest architecture as soon as possible is the winner. I think in a nutshell, that is the story of Nvidia's success here : they created a GPGPU platform with just the right level of abstraction to capture the market for AI research. reply blackoil 16 hours agoparentprevI would call it \"stupid\" money. This isn't a commodity business. Value of the final product is orthogonal to amount invested in compute. If Google is 10% slower or its product is 10% worse, it can lose all the value. This is like valuing a software company higher because its devs are using cheap PC desktops instead of Mac. reply callalex 16 hours agoparentprevThat’s how all huge tech companies become dinosaurs though. Upper management that is already stupidly wealthy (and therefore unmotivated) have the funding and patience to hire geniuses to build incredible machines and then constantly tie their shoelaces together while asking them to sprint. Examples include Microsoft and Oracle as you said, and before them IBM, AT$T, TIBCO, Marvell, Motorola, I could go on for a while… reply ai4ever 17 hours agoparentprevhere is an older take on this same topic.. https://www.yitay.net/blog/training-great-llms-entirely-from... GPU vs TPU, and good software managing large clusters of them across all sorts of failure. the funny bit from the above article is the incident when someone forgot about a training job at google, and month later had the model fully trained without an alert of any kind. \"outrageously good infra\" reply zdyn5 17 hours agoparentprevH100s are far from consumer video cards reply stygiansonic 17 hours agorootparentYeah, ops comment makes it seem like they are building racks of RTX 4090s, when this isn’t remotely true. Tensor Core performance is far different on the data center class devices vs consumer ones. reply mike_d 13 hours agorootparentThey are building racks of 4090s. Nobody can get H100s in any reasonable volume. Hell, Microsoft is renting GPUs from Oracle Cloud to get enough capacity to run Bing. reply TeMPOraL 12 hours agorootparentThere are apparently some 400 of H100s sitting idle somewhere upthread. Yes, I'm having hard time imagining how's that possible too. reply kkielhofner 10 hours agorootparentprevWho is \"they\"? RTX 4090s are terrible for this task. Off the top of my head: - VRAM (obviously). Isn't that where the racks come in? Not really. Nvidia famously removed something as basic as NVLink between two cards from the 3090 to the 4090. When it comes to bandwidth between cards (crucial) even 16 lanes of PCIe 4 isn't fast enough. When you start talking about \"racks\" unless you're running on server grade CPUs (contributing to cost vs power vs density vs perf) you're not going to have nearly enough PCIe lanes to get very far. Even P2P over PCIe requires a hack geohot developed[0] and needless to say that's umm, less than confidence inspiring for what you would lay out ($$$) in terms of hardware, space, cooling, and power. The lack of ECC is a real issue as well. - Form factor. Remember PCIe lanes, etc? The RTX 4090 is a ~three slot beast when using air cooling and needless to say rigging up something like the dual slot water cooled 4090s I have at scale is another challenge altogether... How are people going to wire this up? What do the enclosures/racks/etc look like? This isn't like crypto mining where cheap 1x PCIe risers can be used without dramatically limiting performance to the point of useless. - Performance. As grandparent comment noted 4090s are not designed for this workload. In typical usage for training I see them as 10-20% faster than an RTX 3090 at much higher cost. Compared to my H100 with SXM it's ridiculously slow. - Market segmentation. Nvidia really knows what they're doing here... There are all kinds of limitations you run into with how the hardware is designed (like Tensor Core performance for inference especially). - Issues at scale. Look at the Meta post - their biggest issues are things that are dramatically worse with consumer cards like the RTX 4090, especially when you're running with some kind of goofy PCIe cabling issue (like risers). - Power. No matter what power limiting you employ an RTX 4090 is pretty bad for power/performance ratio. The card isn't fundamentally designed for these tasks - it's designed to run screaming for a few hours a day so gamers can push as many FPS at high res as possible. Training, inference, etc is a different beast and the performance vs power ratio for these tasks is terrible compared to A/H100. Now lets talk about the physical cabling, PSU, etc issues. Yes miners had hacks for this as well but it's yet another issue. - Fan design. There isn't a single \"blower\" style RTX 4090 on the market. There was a dual-slot RTX 3090 at one point (I have a bunch of them) but Nvidia made Gigabyte pull them from the market because people were using them for this. Figuring out some kind of air-cooling setup with the fan and cooling design of the available RTX 4090 cards sounds like a complete nightmare... - Licensing issues. Again, laying out the $$$ for this with a deployment that almost certainly violates the Nvidia EULA is a risky investment. Three RTX 4090s (at 9 slots) to get \"only\" 72GB of VRAM, talking over PCIe, using 48 PCIe lanes, multi-node over sloooow ethernet (hitting CPU - slower and yet more power), using what likely ends up at ~900 watts (power limited) for significantly reduced throughput and less VRAM is ridiculous. Scaling the kind of ethernet you need for this (100 gig) comes at a very high per-port cost and due to all of these issues the performance would still be terrible. I'm all for creativity but deploying \"racks\" of 4090s for AI tasks is (frankly) flat-out stupid. [0] - https://github.com/tinygrad/open-gpu-kernel-modules reply michaelt 7 hours agorootparent> The RTX 4090 is a ~three slot beast when using air cooling and needless to say rigging up something like the dual slot water cooled 4090s I have at scale is another challenge altogether... How are people going to wire this up? What do the enclosures/racks/etc look like? A few years ago, if you wanted a lot of GPU power you would buy something like [1] - a 4/5U server with space for ten dual-slot PCIe x16 cards and quadruple power supplies for 2000W of fully redundant power. And not a PCIe riser in sight. I share your scepticism about whether it's common to run >2 4090s because nvidia have indeed sought to make it difficult. But if there was some sort of supply chain issue that meant you had to, and you had plenty of cash to make it happen? It could probably be done. Some of the more value-oriented GPU cloud suppliers like RunPod offer servers with multiple 4090s and I assume those do something along these lines. With 21 slots in the backplane, you could probably fit 6 air-cooled three-slot GPUs, even if you weren't resorting to water cooling. [1] https://www.supermicro.com/en/products/system/4U/4028/SYS-40... reply mike_d 1 hour agorootparentprev> but deploying \"racks\" of 4090s for AI tasks is (frankly) flat-out stupid. You seem to be trapped in the delusion that this was anyone's first, second, or third choice. There is workload demand, you can't get H100s, and if you don't start racking up the cards you can get the company will replace you with someone less opinionated. reply guardiang 14 hours agoparentprevGoogle is old like MetLife, relative to each's respective industry. Both are carrying too much baggage and are top heavy. As a result, I personally don't think Google will be able to keep pace with OpenAI in the long run. reply r_hanz 16 hours agoparentprevMuch the same way you can have all the best gear and still fail - Google’s primary strength seems to be the DeepMind group. I’m not affiliated with Google, but IMHO the reason they will slowly die is because their engineering culture has taken a backseat due to their broken hiring practices. Bad hiring practices aren’t exclusive to them, but from all accounts it seems like their internal focus is on optimizing ad revenue over everything else. I could be wrong or misinformed, but it seems to me like they are playing the finite game in the AI space (DeepMind group aside) while FAIR are playing the infinite game. *meanwhile MSFT are simply trying to buy their way to relevance (e.g. OpenAI investments, etc) and carve out future revenues (Recall) and Jobs-less Apple is building their trademark walled-garden (AppleIntelligence?). Although the use of unified memory in Apple silicon poses some interesting possibilities for enabling the use of sizable models on consumer hardware. Overall it seems like “big-tech” is by-and-large uninspired and asleep at the wheel save specific teams like those led by Lecun, Hassabis, etc. not sure where that leaves OpenAI now that Karpathy is gone. reply VirusNewbie 15 hours agorootparent>ecause their engineering culture has taken a backseat due to their broken hiring practices. What company do you think has better hiring practices, and subsequently a higher talent pool? Meta is pretty similar to Google's (though with an emphasis on speed over creativity). Microsoft is certainly worse at hiring than the two aforementioned... reply r_hanz 5 hours agorootparentTo be fair, I don’t have any examples of “good practices” readily in-hand. However, I did try to address why I thought others were less impacted by this problem in the second half of my post. reply htrp 15 hours agoparentprev> Literally the only thing that can stop Google now is the fact they keep bringing Microsoft and Oracle flunkies into leadership positions. You mean the thing that's already stopped them? If they had seriously invested into the TPU ecosystem in 2015, they would already have \"won\" AI. reply jatins 13 hours agoparentprevthis was the same argument that was presented a decade ago on why Google was supposed to win the cloud because their internal infra was miles ahead of Amazon and Microsoft. Yet here we are. Will the consumer video cards get cheaper and better faster or will Google's directors' infighting stop first? reply loeg 17 hours agoparentprevThe only thing that can stop Google is Google. Somehow every bet that isn't Search doesn't pan out. And inexplicably, they're working hard to kill Search now. As a shareholder, I hope they succeed. But I am more pessimistic about it than you. reply yellow_postit 15 hours agorootparentAnd they missed multiple waves of effectively building on their own in house research. reply aworks 2 hours agorootparentReminds me of Xerox Parc vs. Apple. Building successful products is hard. reply throwaway920102 17 hours agoparentprevWhat can stop Google is building the wrong thing, or being so scared to launch anything that they smother their own fledgling products before they are born or before they can mature. Their product and finance and accounting teams should be tossed. reply KaiserPro 9 hours agoparentprev> Meta, Microsoft, OpenAI, etc. are trying to address problems with consumer video cards Yes, but you are buying access to tested, supported units that are proven to work, don't require custom software, and are almost plug an go. When its time to upgrade, its not that costly. Designing, fabricating and deploying your own silicon is Expensive, creating software support for it, also more expense. THen there is the opportunity cost of having to optimise the software stack your self. You're exchanging a large capex, for a similar sized capex plus a fuckton of opex as well. reply rapsey 17 hours agoparentprevTheir work on folding and ai could very well be a business worth in the hundreds of billions and they know it as well as many other bets. Wheres others are playing the llm race to the bottom. reply htrp 15 hours agorootparent> Their work on folding and ai could very well be a business worth in the hundreds of billions and they know it as well as many other bets. And we'll be writing case studies of how they squandered billions of R&D to help found other companies (kinda like Xerox Parc) Almost every interesting paper after transformers has had it's authors leave to commercialize their own companies. reply HarHarVeryFunny 17 hours agoparentprevThe chips are somewhat irrelevant. It's the overall system architecture, management, and fault recovery that matters. reply lxgr 16 hours agoparentprevI wish I had your faith in Google’s ability to refrain from kneecapping their own perfectly fine product. reply hipadev23 15 hours agoparentprevGoogle’s on their 6th generation and still can’t find anyone to use it. Hmm. reply Jabrov 16 hours agoparentprev\"Consumer video cards\"? Meta's not building their clusters out of 3090s. They're using advanced cards meant for data centers and machine learning -- almost effectively \"custom silicon\" reply checkyoursudo 12 hours agoparentprevWhat would any company as \"the long term AI winner\" look like? What would it mean to be the winner in this context? reply ketchupdebugger 4 hours agorootparentThe winner is just nvidia, I see this like the battle of gas vs electric cars. Nvidia is basically making the wheels. whichever company wins, you'd still need wheels. reply fragmede 15 hours agoparentprevsmart money has a diversified portfolio and isn't betting on any one winner and has invested in all of them, and then some. reply iamflimflam1 13 hours agoparentprevDon’t forget Apple’s Private Compute Cloud - built on top of Apple Silicon. reply towawy 13 hours agorootparentModels trained on Google TPUs according to Reuters [0]. Does anyone know the \"technical document\" the news article references? [0] https://www.reuters.com/technology/artificial-intelligence/h... reply astrange 12 hours agorootparenthttps://machinelearning.apple.com/research/introducing-apple... reply xcv123 16 hours agoparentprevNone of these companies are using consumer video cards. https://www.nvidia.com/en-us/data-center/h200/ reply moneywoes 17 hours agoparentprevIs no one else working on custom silicon? reply fnordpiglet 16 hours agorootparentThe problem isn’t just developing your own processor. Nvidia has a huge stack of pretty cutting edge technology including a huge stack from mellanox, an enormous OSS tool chain around CUDA, etc, that people seeking to make comparable products have to overcome. reply sangnoir 14 hours agorootparentAre you suggesting Meta or Google - who stand to save billions - won't be able to get top performance from their custom chips because their tooling/hardware won't support CUDA? reply fnordpiglet 13 hours agorootparentNo. I’m suggesting they won’t because IP like the mellanox treasure chest they acquired is ridiculously difficult to develop and Nvidia has aggressively exploited it, along with their other already advanced IP in the space of their -core business-. I understand, especially amongst googlers, there’s a belief there are no others smarter than a googler. But it’s simply not the case. Nvidia is excellent at their core competencies and business, which is making absurdly parallel compute platforms with absurdly powerful interconnects. I’m saying google or meta won’t beat Nvidia at hardware. I’d also point to the fact Nvidias ability to raise capital is the best on earth now, so even money isn’t a barrier. The advantage CUDA gives is in the tool chains, libraries, research, and all that that tens of thousands of people are contributing to as part of their jobs, research, and hobbies. This is almost -more valuable- than getting top performance. Getting top techniques, top software, top everything by having everyone everywhere working to make the ecosystem of your stuff is invaluable. Google won’t have that. They will just have the hubris of googlers who believe they’re smarter. I would also note that at this phase of a cycle in tech trying to save billions takes your eye off the prize. Cost optimization comes much later after the market has been fully explored and directions are clean and diminishing returns on R&D kick in. Any company that doesn’t recognize that is run by CPA and deserves the ignominy they’ll face. reply sangnoir 12 hours agorootparentThe bar for success for Google and Meta is much lower than Nvidia - at least for internal usage. Any dollar amount that Google saves on CapEx or OpEx by using custom silicon instead of buying Nvidia helps bring down the cost of revenue. They don't have to match Nvidia on raw performance, and can aim at being better at performance per watt or performance per dollar (TCO) for larger workloads, and IIRC, Google is already doing for some internal inferencing tasks. > I would also note that at this phase of a cycle in tech trying to save billions takes your eye off the prize Big Tech companies are conglomerate-ish and can multitask. The search engine folk aren't pushing stuff back onto the backlog to put out fires delaying chip tape-out, and I bet the respective CEOs aren't burning braincycles micromanaging silicon development either; directors 2-3 rungs below the C-suite can motivate and execute on such an undertaking. The answer to \"I need a budget of $300M in order to save the company $5-15B over 3 years\" is \"How soon can you start?\" reply logicchains 12 hours agorootparentprev>No. I’m suggesting they won’t because IP like the mellanox treasure chest they acquired is ridiculously difficult to develop and Nvidia has aggressively exploited it, along with their other already advanced IP in the space of their -core business-. For training Llama3 Facebook set up two clusters, one using fancy InfiniBand and one just using RoCE over Arista cards: https://engineering.fb.com/2024/03/12/data-center-engineerin... . The latter ended up doing fine, suggesting that all that Mellanox stuff isn't necessary for large-scale training (apparently at a large enough scale ethernet scales better than InfiniBand). reply threeseed 16 hours agorootparentprevEveryone is. Apple, AWS, Google, Meta, Microsoft all have custom AI-centric silicon. reply dinobones 17 hours agoparentprevDo you really think Google’s hardware expertise is better than Nvidia’s? If needed these other companies have the $$$ to buy the best chips money can buy from Nvidia. Better chips than Google could ever produce. If anything, this is why IMO Google will fail. reply candiddevmike 17 hours agorootparentI thought NVIDIA's moat was mostly software/CUDA? reply rapsey 15 hours agorootparentThey have by far the fastest chip, the best software with CUDA and feverishly working on next gen chips. They also bought out multiple years worth of global high bandwidth memory manufacturing capacity. No one will beat them at their game. However if there are any major breakthroughs that might render those processing capacities unneeded, or the major players hitting a wall regarding AI spending, then they will take a massive hit. It will come eventually because the chip business is always in boom/bust cycles. reply mike_d 17 hours agorootparentprevYes. Google was building custom HPC hardware 5-8 years before Nvidia decided to expand outside the consumer and \"workstation\" markets. reply threeseed 16 hours agorootparentNvidia acquired Mellanox who know far more about custom HPC hardware than Google. reply mike_d 10 hours agorootparentMellanox and friends not being able to build fast enough switching gear at a reasonable price point was what got Google into the hardware game in the first place ;) reply coralreef 17 hours agoparentprevHow do TPUs perform compared to GPUs on LLMs and image generation? reply smarterclayton 2 hours agorootparentPretty well. Anthropic runs some of Claude inference on GKE and TPU v5e - this talk at the last GCP conference has some details: https://youtu.be/b87I1plPeMg?si=T4XSFUzXG8BwpphR Ecosystem support for GPU is very strong and so smaller teams might find TPUs to have a steeper learning curve. Sophisticated users can definitely get wins today on TPU if they can get capacity. And it’s not as if nvidia is standing still, so how the strengths of each change over future generations isn’t set either. Ie TPU are “simple” matrix multipliers and also optimized for operation at scale, GPU are more general purpose and have strong ecosystem power. Disclaimer - work on GKE on enabling AI workloads. reply Der_Einzige 14 hours agoparentprevUntil the lion share of AI projects support that custom silicon, I will continue to bet on anyone buying Nvidia GPUs. reply houseplant 14 hours agoparentprevafter everything I've seen and the litigation coming out of europe, I really can't see AI lasting long after they're obligated to prove rights for the data they're training on. they can't get away with having scraped people's owned work forever. You can't steal things from workers and then undercut them by selling that hard work for pennies, and not expect everything to collapse. I mean, I know that the folks in charge of this aren't really known for their foresight, especially when stock numbers and venture capital are the entire point, but... surely I hope people can recognize that this can't go on unimpeded. reply logicchains 12 hours agorootparentEventually they're going to put vision LLMs in robotic bodies and they'll be able to learn just by listening and watching, just like humans, at which stage the idea that they're \"stealing\" just by viewing content will be seen as absurd. reply jejeyyy77 2 hours agoparentprevlolwat. google is the biggest loser in all of this. reply asynchronous 15 hours agoparentprevLaughable response when you actually look at the quality of the algorithms being produced by Google. They’re so behind it’s embarrassing. reply xvector 16 hours agoprev> So we decided to build both: two 24k clusters, one with RoCE and another with InfiniBand. Our intent was to build and learn from the operational experience. I love how they built two completely insane clusters just to learn. That's badass. reply riku_iki 16 hours agoparentMore like Mark gave them 100k GPUs, and they are not sure what exactly to do with them.. reply logicchains 12 hours agoparentprevIt's not just to learn; an RoCE ethernet cluster with Aristas is way cheaper to build and maintain than a fancy InfiniBand cluster with Mellanox/NVidia networking, so proving that the former is good enough at scale will eventually save Meta a huge amount of money. InfiniBand cards are much more expensive than ethernet because there's few vendors, that have a quasi-monopoloy, and because overall far fewer of them are produced so there's less economy of scale. reply kdot 16 hours agoprevHow will Meta leverage LLMs at scale to drive revenue? It's not clear. reply 123yawaworht456 9 hours agoparent1. improving their adtech. someone else's API offerings are not an option due to the sheer volume, PII and whatnot. 2. virtually free moderation for their existing (facebook, instagram, threads) and future social media services. likewise, their volume is too insane to even consider paying someone else to process it. the models they do release are probably toys in comparison to their internal models. reply sangnoir 14 hours agoparentprevIf only Meta had a way to monetize engagement with generative AI content in a way that scales with quantity of generated content. reply tucnak 13 hours agorootparentRevolutionary! reply threeseed 15 hours agoparentprevI still believe that a VR future is coming once the technology commoditises i.e. costs come down 10x and we have 3090 level GPUs in the headset. At that point we will have photo-realistic experiences like concerts etc that anyone can afford. And at that point having a lot of LLM based avatars that can help \"fill in the space\" will be valuable. reply OsrsNeedsf2P 12 hours agoparentprevLLMs aren't a monetizable product themselves. For the foreseeable future, that will always be ads. LLMs (and VR) are just big bets on getting ahead of future technology. reply dweekly 15 hours agoparentprevAsk the AI to come up with a business model. reply HDThoreaun 12 hours agoparentprevI think the nearish term plan is chatbot customer assistants on whatsapp. Doesnt seem like theyre that close to releasing them but who knows reply altdataseller 10 hours agorootparentAt the end of the day, everything eventually comes down to chatbots reply whalesalad 17 hours agoprevinteresting that their domain is still engineering.fb.com reply samspenc 16 hours agoparentI think fb.com is their internal domain and they never really bothered to change it, I think employees used to have a @fb.com e-mail, at least this was true a few years ago, not sure if that has changed. reply dnissley 16 hours agorootparentEmails switched to @meta.com in 2022. reply jauntywundrkind 17 hours agoprevRandom q, I wonder if gloo is used in these systems? https://github.com/facebookincubator/gloo RDMA and GPUDirect capable. Coordinates over MPI or (hi)redia. reply runeblaze 16 hours agoparent̶I̶I̶R̶C̶ ̶g̶l̶o̶o̶ ̶i̶s̶ ̶C̶P̶U̶ ̶t̶e̶n̶s̶o̶r̶s̶ ̶o̶n̶l̶y̶ ̶s̶o̶ ̶l̶i̶k̶e̶l̶y̶ ̶n̶o̶t̶ Edit: I had a brain freeze or something... gloo is not CPU only but for whatever reason I don't see it outside of CPU-comms reply samspenc 16 hours agoprevOK this was a bit funny: Top HW failure modes: * GPU falling off the bus I honestly thought \"do they mean GPUs falling off a bus entering the data center\" and then realized its actually the connectivity, as they mention in the next line GPUs falling off: In this case, GPUs are not detected by the host on PCIe. reply spmurrayzzz 3 hours agoparentSpeaking for myself (and I guess anyone else dealing with pcie riser hell in on-prem deep learning setups), its nice to see the massive orgs dealing with pretty much the same exact pain points as not-so-massive orgs. reply ChuckMcM 13 hours agoparentprevThe bits on the bus go round and round! There is a lot of interesting yet unpublished work on 'data center' scale compute complexes. It was a rabbit hole I fell into several times while at Google. reply falcor84 6 hours agorootparentThey do publish some of that, or at least they used to. In particular \"The Datacenter as a Computer\" [0] was a very interesting read. [0] https://research.google/pubs/the-datacenter-as-a-computer-an... reply m463 12 hours agoparentprevActually, they \"fell\" off the truck: https://www.theverge.com/2021/11/6/22767046/someone-stole-sh... reply sva_ 9 hours agorootparentBack when EVGA was still selling GPUs ... reply TiredOfLife 8 hours agorootparentAnd offering warranty. And not doing stealth total component change under same sku. reply taneq 7 hours agorootparentprevThey did it to finance their street racing habit, I'm sure. :P reply whazor 11 hours agoparentprevI was imagining that some sys admin has to walk to the server, take out the GPU, blow against the PCI-E pins like a game cartridge, and put it back to try again. reply iszomer 8 hours agorootparentMore to do with bent pins, material obstruction, or something as trivial as cable management (eg: bundles of qsfp weighing down the ports that are press-fitted not soldered). reply rfoo 9 hours agoparentprev\"GPU has fallen off the bus\" is an actual error message nvidia.ko prints to dmesg in this case :p reply cachvico 15 hours agoparentprevBrings a whole new meaning to bus factor reply throwup238 14 hours agorootparentI’ve never met a GPU that could survive getting hit by a bus. reply redbell 9 hours agoparentprev> GPU falling off the bus I'm wondering if we could prompt llama3 with the above statement. What kind of response would it give? reply TeMPOraL 9 hours agorootparentWith temperature set to 1, it recognizes the joke, but proceeds to explain what the \"bus\" is in computer terms, picks a problem this prompt could mean, and explains how to solve it. In ~20 tries it always gave me something along the lines of: The infamous \"GPU falling off the bus\" issue! This problem typically occurs when a graphics processing unit (GPU) is not properly seated or connected to its expansion slot, such as PCIe, on a motherboard. Here are some troubleshooting steps to help resolve the issue: (numbered list of steps or options follows) Tested on Llama 3 Instruct 7B Q8_0, because that one fits entirely on my GPU. reply redbell 8 hours agorootparent+1, interesting findings! I like how it was able to infer the meaning from such a short phrase in a limited context. reply burkaman 5 hours agorootparentIt's actually a very common phrase on forums, I think because it's an actual error that Linux will report: https://askubuntu.com/questions/868321/gpu-has-fallen-off-th.... I've also never heard of it, but it seems like it must appear a lot in the training data and probably about 0 times is referring to a bus on the road. reply TeMPOraL 5 hours agorootparentIn my testing, both Llama 3 and its abliterated (uncensored) variant from[0] almost always remarked more or less directly that they see the joke in the phrase, so either they've seen the other meaning in training, or inferred it. -- [0] - https://news.ycombinator.com/item?id=40665721 reply burkaman 4 hours agorootparentOh I agree it probably inferred the joke. I was actually more surprised that it knew the real meaning of the phrase because I as a human did not, until I looked it up and saw how common it is. reply Technetium 4 hours agorootparentprevPlease use the word ablated instead. That article's title is not using a real word. I'm assuming it's the author's English issue, since they called the model \"helpfull\" instead of \"helpful\". reply TeMPOraL 3 hours agorootparentOops. I actually originally wrote \"ablated\", then changed it to be consistent with the title. reply TeMPOraL 8 hours agorootparentprevTo be specific, the system prompt used was (default in LM Studio config for Llama 3 V2): You are a helpful, smart, kind, and efficient AI assistant. You always fulfill the user's requests to the best of your ability. And then the query was: GPU falling off the bus And yes, I imagine it read that query as ending with an implied \"pls help!\". reply krowfromthewall 6 hours agoparentprevlike they did to our dear Anton in Silicon Valley reply martin-adams 13 hours agoparentprevnext [3 more] A GPU falling off the bus would be one mega flop reply TeMPOraL 12 hours agorootparentThe audience in the back goes clap clap clap, chapeau bas. reply ardit33 4 hours agorootparentprevHaha… It is a a ‘Tera Flop’, as they are falling in the ground… reply tflol 17 hours agoprevnext [6 more] [flagged] tflol 17 hours agoparentnext [6 more] [flagged] brainfog 17 hours agorootparentBecause it's completely irrelevant. reply jauer 16 hours agorootparentand deceptive if not inaccurate. Meta's Model Cards specifically call out that they were trained on publicly available datasets and NOT any Meta user data. For example: https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md reply yellow_postit 15 hours agorootparentThis model card seems at odds with what they claim you can and cannot opt out of [1] > We use information that is publicly available online and licensed information. We also use information shared on Meta’s Products and services. This information could be things like posts or photos and their captions. [1] https://www.facebook.com/privacy/genai/?_rdr reply tflol 17 hours agorootparentprevnext [2 more] [flagged] Jabrov 16 hours agorootparentBecause you're making a baseless claim that contradicts the linked article with nothing to back it up reply WheatMillington 16 hours agorootparentprev>Please don't comment about the voting on comments. It never does any good, and it makes boring reading. reply idkdotcom 16 hours agoprevThese seem classic challenges with running distributed systems loads that are not specific to training LLMs. Anyone of the super computers listed here https://en.wikipedia.org/wiki/TOP500 suffers from the same issues. Think about it. While the national labs use these systems to model serious stuff -such as climate or nuclear weapons- Meta uses them to train LLMs. What a joke, honestly! reply mhandley 8 hours agoparentOn the other hand, Meta just rapidly built two different training networks in existing datacenter buildings, with existing cooling constraints, using mostly commodity components (albeit expensive commodity components) each of which would place at #3 on that top500 list in terms of GPU power. Compare that with how long it took to get any of the other supercomputers from design to being fully commissioned. reply _zoltan_ 1 hour agoparentprevFor profit is not less serious than what research labs do. I'd even say it's more important: they drive the economy. reply whiplash451 12 hours agoparentprevA lot of serious things look like a toy or a joke at first. reply koolala 15 hours agoprev [–] what a title... reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Meta is focusing on AI research to address the large-scale computation needed for training large language models (LLMs), requiring a rethinking of software, hardware, and network infrastructure.",
      "They adapted existing hardware, including modifying the Grand Teton platform with NVIDIA H100 GPUs, and optimized their data center layout to maximize compute capability while maintaining an air-cooled environment.",
      "Meta built two 24k GPU clusters using RoCE and InfiniBand fabrics to train Llama 3, optimizing network communication and load balancing to ensure high performance and efficient data transfer."
    ],
    "commentSummary": [
      "Meta had to adapt its mechanical and thermal designs to remain in an air-cooled environment due to time constraints, leading to a validation cycle for large-scale deployment.",
      "Questions remain about how Meta collects and prepares data for training, particularly concerning Personally Identifiable Information (PII), which is inaccessible by default and requires special access.",
      "The text discusses the competition between tech companies like Google, Microsoft, and Meta in developing their own chips, highlighting the performance and cost-efficiency of Nvidia's GPUs versus Google's TPUs."
    ],
    "points": 368,
    "commentCount": 184,
    "retryCount": 0,
    "time": 1718235308
  },
  {
    "id": 40667976,
    "title": "Microsoft Chose Profit over Security, Whistleblower Says",
    "originLink": "https://www.propublica.org/article/microsoft-solarwinds-golden-saml-data-breach-russian-hackers",
    "originBody": "A model of the Microsoft campus at the company’s headquarters in Redmond, Washington Credit: Greg Kahn, special to ProPublica Technology Microsoft Chose Profit Over Security and Left U.S. Government Vulnerable to Russian Hack, Whistleblower Says by Renee Dudley, with research by Doris Burke June 13, 5 a.m. EDT Former employee says software giant dismissed his warnings about a critical flaw because it feared losing government business. Russian hackers later used the weakness to breach the National Nuclear Security Administration, among others. by Renee Dudley, with research by Doris Burke June 13, 5 a.m. EDT Twitter Facebook Link Copied! Copy Change Appearance Auto Light Dark Republish ProPublica is a nonprofit newsroom that investigates abuses of power. Sign up to receive our biggest stories as soon as they’re published. Microsoft hired Andrew Harris for his extraordinary skill in keeping hackers out of the nation’s most sensitive computer networks. In 2016, Harris was hard at work on a mystifying incident in which intruders had somehow penetrated a major U.S. tech company. Get Our Top Investigations Subscribe to the Big Story newsletter. Email address: This site is protected by reCAPTCHA and the Google Privacy Policy and Terms of Service apply. Thanks for signing up. If you like our stories, mind sharing this with a friend? Copy link For more ways to keep up, be sure to check out the rest of our newsletters. See All Summer Member Drive: Protect journalism that holds power to account. Donate Now The breach troubled Harris for two reasons. First, it involved the company’s cloud — a virtual storehouse typically containing an organization’s most sensitive data. Second, the attackers had pulled it off in a way that left little trace. He retreated to his home office to “war game” possible scenarios, stress-testing the various software products that could have been compromised. Early on, he focused on a Microsoft application that ensured users had permission to log on to cloud-based programs, the cyber equivalent of an officer checking passports at a border. It was there, after months of research, that he found something seriously wrong. The product, which was used by millions of people to log on to their work computers, contained a flaw that could allow attackers to masquerade as legitimate employees and rummage through victims’ “crown jewels” — national security secrets, corporate intellectual property, embarrassing personal emails — all without tripping alarms. To Harris, who had previously spent nearly seven years working for the Defense Department, it was a security nightmare. Anyone using the software was exposed, regardless of whether they used Microsoft or another cloud provider such as Amazon. But Harris was most concerned about the federal government and the implications of his discovery for national security. He flagged the issue to his colleagues. They saw it differently, Harris said. The federal government was preparing to make a massive investment in cloud computing, and Microsoft wanted the business. Acknowledging this security flaw could jeopardize the company’s chances, Harris recalled one product leader telling him. The financial consequences were enormous. Not only could Microsoft lose a multibillion-dollar deal, but it could also lose the race to dominate the market for cloud computing. Harris said he pleaded with the company for several years to address the flaw in the product, a ProPublica investigation has found. But at every turn, Microsoft dismissed his warnings, telling him they would work on a long-term alternative — leaving cloud services around the globe vulnerable to attack in the meantime. Harris was certain someone would figure out how to exploit the weakness. He’d come up with a temporary solution, but it required customers to turn off one of Microsoft’s most convenient and popular features: the ability to access nearly every program used at work with a single logon. He scrambled to alert some of the company’s most sensitive customers about the threat and personally oversaw the fix for the New York Police Department. Frustrated by Microsoft’s inaction, he left the company in August 2020. Andrew Harris shared his Microsoft employee badge on his LinkedIn page when he announced his departure from the company in 2020. Credit: Screenshot by ProPublica Within months, his fears became reality. U.S. officials confirmed reports that a state-sponsored team of Russian hackers had carried out SolarWinds, one of the largest cyberattacks in U.S. history. They used the flaw Harris had identified to vacuum up sensitive data from a number of federal agencies, including, ProPublica has learned, the National Nuclear Security Administration, which maintains the United States’ nuclear weapons stockpile, and the National Institutes of Health, which at the time was engaged in COVID-19 research and vaccine distribution. The Russians also used the weakness to compromise dozens of email accounts in the Treasury Department, including those of its highest-ranking officials. One federal official described the breach as “an espionage campaign designed for long-term intelligence collection.” Harris’ account, told here for the first time and supported by interviews with former colleagues and associates as well as social media posts, upends the prevailing public understanding of the SolarWinds hack. From the moment the hack surfaced, Microsoft insisted it was blameless. Microsoft President Brad Smith assured Congress in 2021 that “there was no vulnerability in any Microsoft product or service that was exploited” in SolarWinds. Get in Touch We’re still reporting on the cybersecurity industry and cyberspace regulation. If you have specific information to share about these topics, you can contact Renee Dudley by email at renee.dudley@propublica.org or on Signal at 929-317-0748. He also said customers could have done more to protect themselves. Harris said they were never given the chance. “The decisions are not based on what’s best for Microsoft’s customers but on what’s best for Microsoft,” said Harris, who now works for CrowdStrike, a cybersecurity company that competes with Microsoft. Microsoft declined to make Smith and other top officials available for interviews for this story, but it did not dispute ProPublica’s findings. Instead, the company issued a statement in response to written questions. “Protecting customers is always our highest priority,” a spokesperson said. “Our security response team takes all security issues seriously and gives every case due diligence with a thorough manual assessment, as well as cross-confirming with engineering and security partners. Our assessment of this issue received multiple reviews and was aligned with the industry consensus.” ProPublica’s investigation comes as the Pentagon seeks to expand its use of Microsoft products — a move that has drawn scrutiny from federal lawmakers amid a series of cyberattacks on the government. Smith is set to testify on Thursday before the House Homeland Security Committee, which is examining Microsoft’s role in a breach perpetrated last year by hackers connected to the Chinese government. Attackers exploited Microsoft security flaws to gain access to top U.S. officials’ emails. In investigating the attack, the federal Cyber Safety Review Board found that Microsoft’s “security culture was inadequate and requires an overhaul.” Microsoft President Brad Smith testifies during a Senate Select Committee on Intelligence hearing about SolarWinds on Feb. 23, 2021. Credit: Drew Angerer/Getty Images For its part, Microsoft has said that work has already begun, declaring that the company’s top priority is security “above all else.” Part of the effort involves adopting the board’s recommendations. “If you’re faced with the tradeoff between security and another priority, your answer is clear: Do security,” the company’s CEO, Satya Nadella, told employees in the wake of the board’s report, which identified a “corporate culture that deprioritized both enterprise security investments and rigorous risk management.” ProPublica’s investigation adds new details and pivotal context about that culture, offering an unsettling look into how the world’s largest software provider handles the security of its own ubiquitous products. It also offers crucial insight into just how much the quest for profits can drive those security decisions, especially as tech behemoths push to dominate the newest — and most lucrative — frontiers, including the cloud market. “This is part of the problem overall with the industry,” said Nick DiCola, who was one of Harris’ bosses at Microsoft and now works at Zero Networks, a network security firm. Publicly-traded tech giants “are beholden to the share price, not to doing what’s right for the customer all the time. That’s just a reality of capitalism. You’re never going to change that in a public company because at the end of the day, they want the shareholder value to go up.” A “Cloud-First World” Early this year, Microsoft surpassed Apple to become the world’s most valuable company, worth more than $3 trillion. That triumph was almost unimaginable a decade ago. (The two remain in close competition for the top spot.) In 2014, the same year that Harris joined Microsoft and Nadella became the CEO, Wall Street and consumers alike viewed the company as stuck in the past, clinging to the “shrink-wrapped” software products like Windows that put it on the map in the 1990s. Microsoft’s long-stagnant share price reflected its status as an also-ran in almost every major technological breakthrough since the turn of the century, from its Bing search engine to its Nokia mobile phone division. As the new CEO, Nadella was determined to reverse the trend and shake off the company’s fuddy-duddy reputation, so he staked Microsoft’s future on the Azure cloud computing division, which then lagged far behind Amazon. In his earliest all-staff memo, Nadella told employees they would need “to reimagine a lot of what we have done in the past for a … cloud-first world.” Microsoft CEO Satya Nadella promotes the company’s cloud offerings at an event in San Francisco in 2014. Credit: David Paul Morris/Bloomberg via Getty Images Microsoft salespeople pitched business and government customers on a “hybrid cloud” strategy, where they kept some traditional, on-premises servers (typically stored on racks in customers’ own offices) while shifting most of their computing needs to the cloud (hosted on servers in Microsoft data centers). Security was a key selling point for the cloud. On-site servers were notoriously vulnerable, in part because organizations’ overburdened IT staff often failed to promptly install the required patches and updates. With the cloud, that crucial work was handled by dedicated employees whose job was security. The dawn of the cloud era at Microsoft was an exciting time to work in the field of cybersecurity for someone like Harris, whose high school yearbook features a photo of him in front of a desktop computer and monitor with a mess of floppy disks beside him. One hand is on the keyboard, the other on a wired mouse. Caption: “Harris the hacker.” Harris’ high school yearbook Credit: Classmates.com As a sophomore at Pace University in New York, he wrote a white paper titled “How to Hack the Wired Equivalent Protocol,” a network security standard, and was awarded a prestigious Defense Department scholarship, which the government uses to recruit cybersecurity specialists. The National Security Agency paid for three years of his tuition, which included a master’s degree in software engineering, in exchange for a commitment to work for the government for at least that long, he said. Early in his career, he helped lead the Defense Department’s efforts to protect individual devices. He became an expert in the niche field known as identity and access management, securing how people log in. As the years wore on, he grew frustrated by the lumbering bureaucracy and craved the innovation of the tech industry. He decided he could make a bigger impact in the private sector, which designed much of the software the government used. At Microsoft he was assigned to a secretive unit known as the “Ghostbusters” (as in: “Who you gonna call?”), which responded to hacks of the company’s most sensitive customers, especially the federal government. As a member of this team, Harris first investigated the puzzling attack on the tech company and remained obsessed with it, even after switching roles inside Microsoft. Eventually, he confirmed the weakness within Active Directory Federation Services, or AD FS, a product that allowed users to sign on a single time to access nearly everything they needed. The problem, he discovered, rested in how the application used a computer language known as SAML to authenticate users as they logged in. To understand how a SAML attack would unfold, let's imagine a robber who wants to gain access to all of the apartment buildings owned by a landlord. The robber finds an open window in a single apartment and climbs in, similar to how a hacker could use a phishing email to log on to a single user's account. Once inside, the robber roams the halls looking for the landlord’s office, where keys to all the building’s units are kept. Likewise, a hacker moves through an organization’s on-premises servers. Their first target is Microsoft’s equivalent of the landlord’s office, a directory that stores information such as usernames and passwords. The robber, however, wants to break into all the landlord's buildings, just as a hacker wants to breach the cloud. The robber unlocks the office safe, which contains a master key. In a cyber break-in, the safe is AD FS, the weak link that Harris identified. The robber makes a copy of the master key, which provides access to all of the landlord's buildings and apartments. In a SAML attack, a hacker extracts the private key from the AD FS server and forges “tokens” that allow the intruder to masquerade as a user with the highest levels of access. Now the robber can access any apartment in any building with little trace. And because the landlord's keys are still in the office, no one suspects anything is amiss. Likewise, in a SAML attack, the hacker goes unnoticed because their sign-in information looks legitimate. Credit: Illustrations by Anuj Shrestha, special to ProPublica This is what makes a SAML attack unique. Typically, hackers leave what cybersecurity specialists call a “noisy” digital trail. Network administrators monitoring the so-called “audit logs” might see unknown or foreign IP addresses attempting to gain access to their cloud services. But SAML attacks are much harder to detect. The forged token is the equivalent of a robber using a copied master key. There was little trail to track, just the activities of what appear to be legitimate users. Harris and a colleague who consulted for the Department of Defense spent hours in front of both real and virtual whiteboards as they mapped out how such an attack would work, the colleague told ProPublica. The “token theft” risk, as Harris referred to it, became a regular topic of discussion for them. A Clash With “Won’t Fix” Culture Before long, Harris alerted his supervisors about his SAML finding. Nick DiCola, his boss at the time, told ProPublica he referred Harris to the Microsoft Security Response Center, which fields reports of security vulnerabilities and determines which need to be addressed. Given its central role in improving Microsoft product security, the team once considered itself the “conscience of the company,” urging colleagues to improve security without regard to profit. In a meeting room, someone hung a framed photo of Winston “the Wolf,” the charismatic fixer in Quentin Tarantino’s movie “Pulp Fiction” who is summoned to clean up the aftermath of bloody hits. Members of the team were not always popular within the company. Plugging security holes is a cost center, and making new products is a profit center, former employees told ProPublica. In 2002, the company’s founder, Bill Gates, tried to settle the issue, sending a memo that turned out to be eerily prescient. “Flaws in a single Microsoft product, service or policy not only affect the quality of our platform and services overall, but also our customers’ view of us as a company,” Gates wrote, adding: “So now, when we face a choice between adding features and resolving security issues, we need to choose security.” At first, Gates’ memo was transformational and the company’s product divisions were more responsive to the center’s concerns. But over time, the center’s influence waned. Its members were stuck between cultural forces. Security researchers — often characterized as having outsized egos — believed their findings should be immediately addressed, underestimating the business challenges of developing fixes quickly, former MSRC employees told ProPublica. Product managers had little motivation to act fast, if at all, since compensation was tied to the release of new, revenue-generating products and features. That attitude was particularly pronounced in Azure product groups, former MSRC members said, because they were under pressure from Nadella to catch up to Amazon. “Azure was the Wild West, just this constant race for features and functionality,” said Nate Warfield, who worked in the MSRC for four years beginning in 2016. “You will get a promotion because you released the next new shiny thing in Azure. You are not going to get a promotion because you fixed a bunch of security bugs.” Former employees told ProPublica that the center fielded hundreds or even thousands of reports a month, pushing the perennially understaffed group to its limits. The magazine Popular Science noted that volume as one of the reasons why working in the MSRC was one of the 10 “worst jobs in science,” between whale feces researchers and elephant vasectomists. “They’re trained, because they’re so resource constrained, to think of these cases in terms of: ‘How can I get to ‘won’t fix,’” said Dustin Childs, who worked in the MSRC in the years leading up to Harris’ saga. Staff would often punt on fixes by telling researchers they would be handled in “v-next,” the next product version, he said. Those launches, however, could be years away, leaving customers vulnerable in the interim, he said. The center also routinely rejected researchers’ reports of weaknesses by saying they didn’t cross what its staff called a “security boundary.” But when Harris discovered the SAML flaw, it was a term with no formal definition, former employees said. Credit: Jaap Arriens / Sipa USA via AP Images By 2017, the lack of clarity had become the “butt of jokes,” Warfield said. Several prominent security researchers who regularly interacted with the MSRC made T-shirts and stickers that said “____ [fill in the blank] is not a security boundary.” “Any time Microsoft didn’t want to fix something, they’d just say, ‘That’s not a security boundary, we’re not going to fix it,’” Warfield recalled. Unaware of the inauspicious climate, Harris met virtually with MSRC representatives and sketched out how a hacker could jump from an on-premises server to the cloud without being detected. The MSRC declined to address the problem. Its staff argued that hackers attempting to exploit the SAML flaw would first have to gain access to an on-premises server. As they saw it, Harris said, that was the security boundary — not the subsequent hop to the cloud. Business Over Security “WTF,” Harris recalled thinking when he got the news. “This makes no sense.” Microsoft had told customers the cloud was the safest place to put their most precious data. His discovery proved that, for the millions of users whose systems included AD FS, their cloud was only as secure as their on-premises servers. In other words, all the buildings owned by the landlord are only as secure as the most careless tenant who forgot to lock their window. Harris pushed back, but he said the MSRC held firm. Harris had a reputation for going outside the chain of command to air his concerns, and he took his case to the team managing the products that verified user identities. He had some clout, his former colleagues said. He had already established himself as a known expert in the field, had pioneered a cybersecurity threat detection method and later was listed as the named inventor on a Microsoft patent. Harris said he “went kind of crazy” and fired off an email to product manager Mark Morowczynski and director Alex Simons requesting a meeting. He understood that developing a long-term fix would take time, but he had an interim solution that could eliminate the threat. One of the main practical functions of AD FS was to allow users to access both on-premises servers and a variety of cloud-based services after entering credentials only once, a Microsoft feature known as “seamless” single sign-on. Harris proposed that Microsoft tell its customers to turn off that function so the SAML weakness would no longer matter. According to Harris, Morowczynski quickly jumped on a videoconference and said he had discussed the concerns with Simons. “Everyone violently agreed with me that this is a huge issue,” Harris said. “Everyone violently disagreed with me that we should move quickly to fix it.” Morowczynski, Harris said, had two primary objections. First, a public acknowledgement of the SAML flaw would alert adversaries who could then exploit it. Harris waved off the concern, believing it was a risk worth taking so that customers wouldn’t be ignorant to the threat. Plus, he believed Microsoft could warn customers without betraying any specifics that could be co-opted by hackers. According to Harris, Morowczynski’s second objection revolved around the business fallout for Microsoft. Harris said Morowczynski told him that his proposed fix could alienate one of Microsoft’s largest and most important customers: the federal government, which used AD FS. Disabling seamless SSO would have widespread and unique consequences for government employees, who relied on physical “smart cards” to log onto their devices. Required by federal rules, the cards generated random passwords each time employees signed on. Due to the configuration of the underlying technology, though, removing seamless SSO would mean users could not access the cloud through their smart cards. To access services or data on the cloud, they would have to sign in a second time and would not be able to use the mandated smart cards. Harris said Morowczynski rejected his idea, saying it wasn’t a viable option. Morowczynski told Harris that his approach could also undermine the company’s chances of getting one of the largest government computing contracts in U.S. history, which would be formally announced the next year. Internally, Nadella had made clear that Microsoft needed a piece of this multibillion-dollar deal with the Pentagon if it wanted to have a future in selling cloud services, Harris and other former employees said. Killing the Competition By Harris’ account, the team was also concerned about the potential business impact on the products sold by Microsoft to sign into the cloud. At the time, Microsoft was in a fierce rivalry with a company called Okta. Microsoft customers had been sold on seamless SSO, which was one of the competitive advantages — or, in Microsoft parlance, “kill points” — that the company then had over Okta, whose users had to sign on twice, Harris said. Harris’ proposed fix would undermine the company’s strategy to marginalize Okta and would “add friction” to the user experience, whereas the “No. 1 priority was to remove friction,” Harris recalled Morowczynski telling him. Moreover, it would have cascading consequences for the cloud business because the sale of identity products often led to demand for other cloud services. “That little speed bump of you authenticating twice was unacceptable by Microsoft’s standards,” Harris said. He recalled Morowczynski telling him that the product group’s call “was a business decision, not a technical one.” “What they were telling me was counterintuitive to everything I’d heard at Microsoft about ‘customer first,’” Harris said. “Now they’re telling me it’s not ‘customer first,’ it’s actually ‘business first.’” DiCola, Harris’ then-supervisor, told ProPublica the race to dominate the market for new and high-growth areas like the cloud drove the decisions of Microsoft’s product teams. “That is always like, ‘Do whatever it frickin’ takes to win because you have to win.’ Because if you don’t win, it’s much harder to win it back in the future. Customers tend to buy that product forever.” According to Harris, Morowczynski said his team had “on the road map” a product that could replace AD FS altogether. But it was unclear when it would be available to customers. In the months that followed, Harris vented to his colleagues about the product group’s decision. ProPublica talked to three people who worked with Harris at the time and recalled these conversations. All of them spoke on the condition of anonymity because they feared professional repercussions. The three said Harris was enraged and frustrated over what he described to them as the product group’s unwillingness to address the weakness. Neither Morowczynski nor Simons returned calls seeking comment, and Microsoft declined to make them available for interviews. The company did not dispute the details of Harris’ account. In its statement, Microsoft said it weighs a number of factors when it evaluates potential threats. “We prioritize our security response work by considering potential customer disruption, exploitability, and available mitigations,” the spokesperson said. “We continue to listen to the security research community and evolve our approach to ensure we are meeting customer expectations and protecting them from emerging threats.” Another Major Warning Following the conversation with Morowczynski, Harris wrote a reminder to himself on the whiteboard in his home office: “SAML follow-up.” He wanted to keep the pressure on the product team. Soon after, the Massachusetts- and Tel Aviv-based cybersecurity firm CyberArk published a blog post describing the flaw, which it dubbed “Golden SAML,” along with a proof of concept, essentially a road map that showed how hackers could exploit the weakness. Years later, in his written testimony for the Senate Intelligence Committee, Microsoft’s Brad Smith said this was the moment the company learned of the issue. “The Golden SAML theory became known to cybersecurity professionals at Microsoft and across the U.S. government and the tech sector at precisely the same time, when it was published in a public paper in 2017,” Smith wrote. Lavi Lazarovitz of CyberArk said the firm mentioned the weakness — before the post was published — in a private WhatsApp chat of about 10 security researchers from various companies, a forum members used to compare notes on emerging threats. When they raised the discovery to the group, which included at least one researcher from Microsoft, the other members were dismissive, Lazarovitz said. “Many in the security research community — I don’t want to say mocked — but asked, ‘Well, what’s the big deal?’” Lazarovitz said. The CyberArk headquarters in Newton, Massachusetts Credit: Sipa via AP Images Nevertheless, CyberArk believed it was worth taking seriously, given that AD FS represented the gateway to users’ most sensitive information, including email. “Threat actors operate in between the cracks,” Lazarovitz said. “So obviously, we understood the feedback that we got, but we still believed that this technique will be eventually leveled by threat actors.” The Israel-based team also reached out to contacts at Microsoft’s Israeli headquarters and were met with a response similar to the one they got in the WhatsApp group, Lazarovitz said. The published report was CyberArk’s way of warning the public about the threat. Disclosing the weakness also had a business benefit for the company. In the blog post, it pitched its own security product, which it said “will be extremely beneficial in blocking attackers from getting their hands on important assets like the token-signing certificate in the first place.” The report initially received little attention. Harris, however, seized on it. He said he alerted Morowczynski and Simons from the product group as well as the MSRC. The situation was more urgent than before, Harris argued to them, because CyberArk included the proof of concept that could be used by hackers to carry out a real attack. For Harris, it harkened back to Morowczynski’s worry that flagging the weakness could give hackers an advantage. “I was more energetic than ever to have us actually finally figure out what we’re going to do about this,” Harris said. But the MSRC reiterated its “security boundary” stance, while Morowczynski reaffirmed the product group’s earlier decision, Harris said. Harris said he then returned to his supervisors, including Hayden Hainsworth and Bharat Shah, who, as corporate vice president of the Azure cloud security division, also oversaw the MSRC. “I said, ‘Can you guys please listen to me,’” Harris recalled. “‘This is probably the most important thing I’ve ever done in my career.’” Harris said they were unmoved and told him to take the problem back to the MSRC. Microsoft did not publicly comment on the CyberArk blog post at the time. Years later, in written responses to Congress, Smith said the company’s security researchers reviewed the information but decided to focus on other priorities. Neither Hainsworth nor Shah returned calls seeking comment. Defusing a Ticking Bomb Harris said he was deeply frustrated. On a personal level, his ego was bruised. Identifying major weaknesses is considered an achievement for cybersecurity professionals, and, despite his internal discovery, CyberArk had claimed Golden SAML. More broadly, he said he was more worried than ever, believing the weakness was a ticking bomb. “It’s out in the open now,” he said. Publicly, Microsoft continued to promote the safety of its products, even boasting of its relationship with the federal government in sales pitches. “To protect your organization, Azure embeds security, privacy, and compliance into its development methodology,” the company said in late 2017, “and has been recognized as the most trusted cloud for U.S. government institutions.” Attendees walk through the exhibition floor during the Microsoft Developers Build Conference in Seattle in 2017. Credit: David Ryder/Bloomberg via Getty Images Internally, Harris complained to colleagues that customers were being left vulnerable. “He was definitely having issues” with the product team, said Harris’ former Microsoft colleague who consulted for the Defense Department. “He vented that it was a problem that they just wanted to ignore.” Harris typically pivoted from venting to discussing how to protect customers, the former colleague said. “I asked him to show me what I’m going to have to do to make sure the customers were aware and could take corrective action to mitigate the risk,” he said. Harris also took his message to LinkedIn, where he posted a discreet warning and an offer. “I hope all my friends and followers on here realize by now the security relationship” involved in authenticating users in AD FS, he wrote in 2019. “If not, reach out and let’s fix that!” In 2019, Harris posted a discreet warning and an offer on LinkedIn. Credit: Screenshot by ProPublica Separately, he realized he could help customers with whom he had existing relationships, including the NYPD, the nation’s largest police force. “Knowing this exploit is actually possible, why would I not architect around it, especially for my critical customers?” Harris said. On a visit to the NYPD, Harris told a top IT official, Matthew Fraser, about the AD FS weakness and recommended disabling seamless SSO. Fraser was in disbelief at the severity of the issue, Harris recalled, and he agreed to disable seamless SSO. In an interview, Fraser confirmed the meeting. “This was identified as one of those areas that was prime, ripe,” Fraser said of the SAML weakness. “From there, we figured out what’s the best path to insulate and secure.” More Troubling Revelations It was over beers at a conference in Orlando in 2018 that Harris learned the weakness was even worse than he’d initially realized. A colleague sketched out on a napkin how hackers could also bypass a common security feature called multifactor authentication, which requires users to perform one or more additional steps to verify their identity, such as entering a code sent via text message. They realized that, no matter how many additional security steps a company puts in place, a hacker with a forged token can bypass them all. When they brought the new information to the MSRC, “it was a nonstarter,” Harris said. While the center had published a formal definition of “security boundary” by that point, Harris’ issues still didn’t meet it. Nadella delivers the keynote address at a 2018 conference in Seattle for software developers. Credit: Elaine Thompson/AP By March 2019, concerns over Golden SAML were spilling out into the wider tech world. That month, at a conference in Germany, two researchers from the cybersecurity company Mandiant delivered a presentation demonstrating how hackers could infiltrate AD FS to gain access to organizations’ cloud accounts and applications. They also released the tools they used to do so. Mandiant said it notified Microsoft before the presentation, making it the second time in roughly 16 months that an outside firm had flagged the SAML issue to the company. In August 2020, Harris left Microsoft to work for CrowdStrike. In his exit interview with Shah, Harris said he raised the SAML weakness one last time. Shah listened but offered no feedback, he said. “There is no inspector general-type thing” within Microsoft, Harris said. “If something egregious is happening, where the hell do you go? There’s no place to go.” SolarWinds Breaks Four months later, news of the SolarWinds attack broke. Federal officials soon announced that beginning in 2019 Russian hackers had breached and exploited the network management software offered by a Texas-based company called SolarWinds, which had the misfortune of lending its name to the attack. The hackers covertly inserted malware into the firm’s software updates, gaining “backdoor” access to the networks of companies and government agencies that installed them. The ongoing access allowed hackers to take advantage of “post-exploit” vulnerabilities, including Golden SAML, to steal sensitive data and emails from the cloud. Despite the name, nearly a third of victims of the attack never used SolarWinds software at all, Brandon Wales, then acting director of the federal Cybersecurity and Infrastructure Security Agency, said in the aftermath. In March 2021, Wales told a Senate panel that hackers were able to “gain broad access to data stores that they wanted, largely in Microsoft Office 365 Cloud … and it was all because they compromised those systems that manage trust and identity on networks.” Microsoft itself was also breached. In the immediate aftermath of the attack, Microsoft advised customers of Microsoft 365 to disable seamless SSO in AD FS and similar products — the solution that Harris proposed three years earlier. As the world dealt with the consequences, Harris took his long simmering frustration public in a series of posts on social media and on his personal blog. Challenging Brad Smith by name, and criticizing the MSRC’s decisions — which he referred to as “utter BS” — Harris lambasted Microsoft for failing to publicly warn customers about Golden SAML. Microsoft “was not transparent about these risks, forced customers to use ADFS knowing these risks, and put many customers and especially US Gov’t in a bad place,” Harris wrote on LinkedIn in December 2020. A long-term fix was “never a priority” for the company, he wrote. “Customers are boned and sadly it’s been that way for years (which again, sickens me),” Harris said in the post. In the months and years following the SolarWinds attack, Microsoft took a number of actions to mitigate the SAML risk. One of them was a way to efficiently detect fallout from such a hack. The advancement, however, was available only as part of a paid add-on product known as Sentinel. The lack of such a detection, the company said in a blog post, had been a “blind spot.” “Microsoft Is Back on Top” In early 2021, the Senate Select Committee on Intelligence called Brad Smith to testify about SolarWinds. Although Microsoft’s product had played a central role in the attack, Smith seemed unflappable, his easy and conversational tone a reflection of the relationships he had spent decades building on Capitol Hill. Without referencing notes or reading from a script, as some of his counterparts did, he confidently deflected questions about Microsoft’s role. Laying the responsibility with the government, he said that in the lead-up to the attack, the authentication flaw “was not prioritized by the intelligence community as a risk, nor was it flagged by civilian agencies or other entities in the security community as a risk that should be elevated” over other cybersecurity priorities. Smith also downplayed the significance of the Golden SAML weakness, saying it was used in just 15% of the 60 cases that Microsoft had identified by that point. At the same time, he acknowledged that, “without question, these are not the only victims who had data observed or taken.” When Sen. Marco Rubio of Florida pointedly asked him what Microsoft had done to address Golden SAML in the years before the attack, Smith responded by listing a handful of steps that customers could have taken to protect themselves. His suggestions included purchasing an antivirus product like Microsoft Defender and securing devices with another Microsoft product called Intune. “The reality is any organization that did all five of those things, if it was breached, it in all likelihood suffered almost no damage,” Smith said. Neither Rubio nor any other senator pressed further. Ultimately, Microsoft won a piece of the Defense Department’s multibillion-dollar cloud business, sharing it with Amazon, Google and Oracle. Read More Bill to Fund Stillbirth Prevention and Research Passes Congress Since December 2020, when the SolarWinds attack was made public, Microsoft’s stock has soared 106%, largely on the runaway success of Azure and artificial intelligence products like ChatGPT, where the company is the largest investor. “Microsoft Is Back on Top,” proclaimed Fortune, which featured Nadella on the cover of its most recent issue. In September 2021, just 10 months after the discovery of SolarWinds, the paperback edition of Smith’s book, “Tools and Weapons,” was published. In it, Smith praised Microsoft’s response to the attack. The MSRC, Smith wrote, “quickly activated its incident response plan” and the company at large “mobilized more than 500 employees to work full time on every aspect of the attack.” In the new edition, Smith also reflected on his congressional testimony on SolarWinds. The hearings, he wrote, “examined not only what had happened but also what steps needed to be taken to prevent such attacks in the future.” He didn’t mention it in the book, but that certainly would include the long-term alternative that Morowczynski first promised to Harris in 2017. The company began offering it in 2022. Development by Lucas Waldron. Filed under — Technology Protect Independent Journalism This story you’ve just finished was funded by our readers. We hope it inspires you to make a gift to ProPublica so that we can publish more investigations like this one that hold people in power to account and produce real change. ProPublica is a nonprofit newsroom that produces nonpartisan, evidence-based journalism to expose injustice, corruption and wrongdoing. We were founded in 2008 to fill a growing hole in journalism: Newsrooms are shrinking, and legacy funding models are failing. Deep-dive reporting like ours is slow and expensive, and investigative journalism is a luxury in many newsrooms today — but it remains as critical as ever to democracy and our civic life. Over 15 years (and seven Pulitzer Prizes) later, ProPublica has built one of the largest investigative newsrooms in the country. Our work has spurred reform through legislation, at the voting booth and inside our nation’s most important institutions. Your donation today will help us ensure that we can continue this critical work. From the climate crisis, to threats to our democracy, to ethics in our judiciary and much more, we are busier than ever covering stories you won’t see anywhere else. Make your gift of any amount today and join over 50,000 ProPublicans across the country, standing up for the power of independent journalism to produce real, lasting change. Thank you. Donate Now Renee Dudley Renee Dudley is a tech reporter at ProPublica. renee.dudley@propublica.org @renee_dudley 929-317-0748 Doris Burke Doris Burke is a senior research reporter at ProPublica.",
    "commentLink": "https://news.ycombinator.com/item?id=40667976",
    "commentBody": "Microsoft Chose Profit over Security, Whistleblower Says (propublica.org)343 points by tyleroconnell 8 hours agohidepastfavorite162 comments minisooftwin 4 hours ago> “If you’re faced with the tradeoff between security and another priority, your answer is clear: Do security,” the company’s CEO, Satya Nadella, told employees. Satya's model of making security a priority at Microsoft: - Cram ads in every nook and corner of Windows. Left, right, centre, back, front, everywhere. What else is an operating system for? - Install a recorder which records everything you do. For the benefit of users of course - you know, what if a user missed an ad and wants to go back and see what they missed. - Send a mail to your employees and tell them \"Do security\". Mission accomplished - Microsoft is now the most secure platform. reply VyseofArcadia 3 hours agoparentThe Microsoft bribes scandal broke not too long after I had to take the \"hey don't do bribes\" training at Microsoft. That event really drove home for me the fact that all of the trainings, emails, processes, etc. are mostly plausible deniability. There are people who care about security at MS. I know, I've met them, but for the most part all of this exists so that Satya can plausibly say in court or in front of congress, \"well we told them to do security better. This is clearly the fault of product teams or individual contributors, not Microsoft policy and incentives.\" reply montjoy 3 hours agorootparentI dunno, that’s a pretty cynical take. Isn’t it just as plausible that they became aware of the bribes internally and were trying to curtail them when the scandal broke out? Or maybe the “don’t do bribes” training actually worked enough for someone to whistleblow even if official internal channels failed? Those who are doing wrong often try to stymie others from making positive changes out of fear, greed, etc. Edit: I just want to add that there are things to be cynical about - I’m not completely naive. If it’s your legal department heading up the training then you can be pretty sure that there was a cause for it. reply blowski 3 hours agorootparentYes, massive companies are a nest of conflicting priorities. The sales team wants to do whatever it takes to win the deal, and the legal team wants everyone to behave ethically at all times. The board wants to be shocked(!) when it turns out those goals are in conflict, with the ethical side sometimes losing out, to remove any personal risk to themselves. reply mistrial9 2 hours agorootparent> legal team wants everyone to behave ethically at all times do you really believe that? compliance under scrutiny, more like it reply potatolicious 4 minutes agorootparentHaving worked with many lawyers... for the most part, yeah. Legal wants you to behave ethically at all times, not because they necessarily have some ideological commitment to ethics (though some do), but because it keeps the company out of lawsuits. The overwhelming goal of a company's legal department is \"don't get sued\", followed by \"if sued, lose as little money/leverage as possible\". In general the lawyer in the room is going to be far more risk-averse than the engineers, product people, sales people, or marketers. The trick is that outside of some limited circumstances the legal department at companies are not the final say. Many lawyers who \"go in house\" (i.e., quit a private outside firm and go work directly for a company) find this frustrating. They come into a room, say \"don't do that\", and then a few weeks/months/years later someone did it and now they have to prepare for a lawsuit. johnnyanmac 1 hour agorootparentprevThe best job is sitting around and doing nothing. So ideally yes. But sure, ethically speaking when things get heated they will exploit every loophole they can find to avoid liability. So, lawful evil? reply mmcdermott 45 minutes agorootparentMost corporate law guidance is about risk mitigation, not about ethics. Less activity generally translates to less risk. You can see a similar phenomenon with security professionals. True, the only secure computer is one disconnected from the Internet, turned off, put in a Faraday cage, on the moon, under armed guard - but that's not useful. reply Hasu 13 minutes agorootparentprevEven if everyone in the company magically complied with the wishes of the legal department, they would still have work to do. Defending the company against frivolous lawsuits and incoming regulations, suing competitors and other bad actors outside of the company, writing and evaluating contracts, and any internal legal consultation needed. reply sophacles 1 hour agorootparentprev> The best job is sitting around and doing nothing. That sounds like a terrible job. reply johnnyanmac 1 hour agorootparentWell you can take it as literally or figuratively as you wish. Depends on the person. reply giobox 1 hour agorootparentprevMaybe. However such training is essentially considered mandatory compliance at any publicly traded company once you reach a certain size, especially if you sell to the government, and IMO probably not related to any specific event they became aware of. I've had to do the same mandatory anti-bribing public officials training annually at US companies a fraction the size of Microsoft. The anti-bribe training is so common at large companies in the US, there are companies that sell ready made one-size-fits-all training videos specifically on this topic that are then usually the thing the employee has to sit through anually. In my experience, different cultures have different feelings on the moral failings of bribes. Some of my colleagues grew up in countries where it is a common business practice, it probably makes sense for large orgs with global employee base to have to establish some kind of baseline for acceptable business practices. Similarly, I know several people who came to study computer science in the US and tried to bribe police officers upon being pulled over for speeding, simply because it's how you handle the matter where they grew up. reply creaghpatr 2 hours agorootparentprevProbably neither, \"don't do bribes\" training is standard onboarding procedure at any Fortune 500 company. Just ironic timing from OPs POV reply tialaramex 1 hour agorootparentBut this is exactly why it's standard procedure. I worked for a huge Credit Reference Agency and it was very obvious that this is ass covering. Sarah and Bob in the New York Office of Huge Corp must take the training so that the CEO can swear all his employees know not to bribe people. In the event that Manuel, who is given $100 000 per week of company money to bribe the locals in Melonistan so that they don't interfere with Huge Corp's operations is actually brought before the government and forced to spill the beans the CEO will insist they had no idea and some Huge Corp minion gets sacrificed. Manuel will be replaced, Melonistan will be assured quietly that his replacement will provide make up money ASAP. In Arms this is even worse, because there it's secretly government policy to bribe people, even though it's also illegal. So then sometimes even if you can prove there was a crime, the government will say \"We'll take that evidence thank you very much\" and poof, the crime disappears, if you make too much fuss you'll be made to disappear too. reply ein0p 1 hour agorootparentprevNot just onboarding. Most, if not all, large companies waste at least an hour of their employees time on this per year, while themselves bribing politicians in DC. reply VyseofArcadia 1 hour agorootparentIt was, in fact, a story arc in an at the time recent-ish season of SBC[0]. [0] Microsoft's yearly training that is done in the form of a TV drama about MS employees facing ethical dilemmas reply lupusreal 2 hours agorootparentprevThat doesn't seem plausible, because you can't stop bribery by telling people that bribery is against the rules. Everybody already knows that. If they became aware of bribery and genuinely wanted to stop it, the way is to publicly punish the culprits as harshly as they can, to demonstrate to others that enforcement of the rules can happen. reply ein0p 1 hour agorootparentYes and no. You might not even realize that what you did constitutes giving or receiving a bribe. What cracks me up though is that all large US megacorps give tens of millions of dollars in thinly veiled bribes to officials each year, as they browbeat their employees into not accepting a god damn fruit basket from a thankful client. reply ClumsyPilot 56 minutes agorootparentprev> dunno, that’s a pretty cynical take Just days ago a major US corporation was found guilty of hiring Death Squads in Columbia. Literally to murder people. Why do we have this common illusions that corporation will not steep down to the dirtiest crimes they can get away with? https://www.bbc.com/news/articles/c6pprpd3x96o reply tptacek 1 hour agorootparentprevMicrosoft has for over two decades been one of the largest and most sophisticated employers of security talent in the industry, and for a run of about 8 years probably singlehandedly created the market for vulnerability research by contracting out to vulnerability research vendors. Leadership at Microsoft is different today than when the process of Microsoft's security maturation took place, but I'll note that through that whole time nerd message boards relentless accused them of being performative and naive about security. reply solatic 29 minutes agorootparentprevTo be fair, it's not really possible to come up with good policy to handle this at scale. It would be too intrusive to require employees to divulge their private financial accounts (and near impossible to audit that the employee has truly divulged all their financial accounts), and the more internal controls you put in place, the slower the deal-making gets, with no guarantee of good behavior. reply pjmlp 3 hours agorootparentprevYes, hence why I take all those company values trainings as Bull******. reply doe_eyes 2 hours agorootparentprevEh. For the most part, the trainings can be taken at face value. Even if the management's dealings with governments and partners are questionable, no company wants random employees accepting personal kickbacks from vendors. There's a liability avoidance component to trainings, but mostly for non-business misconduct. For example, for sexual harassment, the company will say they tried everything they could to explain to employees that this is not OK, and the perpetrator alone should be financially liable for what happened. That defense is a lot less useful in business dealings where the company benefits, though. reply tombert 2 hours agoparentprevI have no broad evidence of this, but I suspect that the more beginner-friendly Linuxes are guilty of a lot of the sins that you laid out here. I seem to remember some controversy with Canonical recording your searches when hitting the super key, and Ubuntu having Amazon ads built in by default. People who love to geek out about computers can of course install Arch or Gentoo or NixOS Minimal and then audit the packages that they're installing to see that there's no obvious security violations, but it's unrealistic to think that most non-software-engineer people are going to do that. I really don't know how to fix this problem; there will always be an incentive for Microsoft (and every other company) to plaster as many ads as they think that can get away with, as well as collecting as much data as possible. I don't know that I would support regulation on this, but I don't know what else could be done. reply nicce 1 hour agorootparent> I seem to remember some controversy with Canonical recording your searches when hitting the super key, and Ubuntu having Amazon ads built in by default. It was also other way around with Microsoft. If you deploy Ubuntu VM in Azure, they contacted you in LinkedIn to offer commercial support. Not joking: https://www.theregister.com/2021/02/11/microsoft_azure_ubunt... reply lupusreal 2 hours agorootparentprevDebian is a perfectly reasonable choice for casual linux users. Ubuntu's supposed usability improvements over Debian are greatly exaggerated. It's mostly just marketting. reply tombert 2 hours agorootparentFair enough. I haven't used Debian in quite awhile (I think since 2009 or so?), so I can't speak to current stuff, but I do remember it being pretty hard to install then. I'm sure they have refined it considerably since then, and of course I am fifteen years more experienced now than I was. Personally it's hard for me to go back after I accepted the dogma of NixOS, but maybe if I manage to talk my parents into using Linux I'll install Debian for them. reply 1oooqooq 1 hour agorootparentinstall arch. not even kiding. make a \"shutdown\" button on the desktop that locks everything and do a full upgrade. any issue is solved with, try tomorrow after a reboot. you'd be surprised how fast fixes arrive at rolling distros reply tombert 2 minutes agorootparentI do NixOS-minimal. As far as I'm aware it doesn't really add any runtime overhead in comparison to Arch, the package manager is generally quite good at figuring out which changes are going to break your system, and everything is snapshotted on every rebuild so for the most part I can be fearless. Doing a full upgrade is generally as straightforward as pointing to the latest version's repo and doing something like `sudo nixos-rebuild switch --upgrade`. That works great for a geeky dude like me, but I don't think I'll ever be able to convince my parents on the beauty of NixOS, so having a straightforward mypackage.deb thing that they can download and click on to install stuff probably would be an easier sell. I ran Arch for about a year, and I liked it, but I had to abuse the `snapper` tool because I was constantly breaking things with the video driver and the like. It worked but I personally think that NixOS's model is just more elegant. abdullahkhalids 1 hour agorootparentprevIt's not surprising when a linux distribution was taken over by a capitalistic firm, it decided to forgo good values, and instead prioritized profits over everything else. > I really don't know how to fix this problem Stop using software made by companies that do bad things. Improve the software that doesn't. reply tombert 8 minutes agorootparentI don't really think that's realistic. I can of course use software from non-profits or something at home, but we all work for a living, and every single job I've had has relied on software from a for-profit company in one way or another. I guess I don't have to be an engineer, but even if I were to go be a cashier at Taco Bell or something, I would still be stuck using a proprietary POS system. Unless I want to go live in a unabomber shed off the grid, I'm probably going to be stuck using software made by companies that do bad things. The software world is overwhelmingly run by Microsoft, Apple, Google, and Oracle (and probably a few others I'm missing), all of which do bad stuff all the time. reply nicce 1 hour agorootparentprev> Stop using software made by companies that do bad things. Improve the software that doesn't Or stop buying their stock... but that is difficult thing to embrace. As, we know, these companies are very profitable. reply abrouwers 2 hours agorootparentprevI mean, if you have no evidence of this, why even post such an (incorrect) conspiracy theory comment? reply tombert 2 hours agorootparentWell the Amazon ads in Ubuntu absolutely did happen, as well as the searches with the super key. [1] I'll admit it's maybe a bit of an extrapolation to assume that they're as bad as Microsoft, which is why I disclosed that I didn't have a ton of evidence for this. [1] https://www.gnu.org/philosophy/ubuntu-spyware.en.html I realize that GNU is sort of conspiratorial in its own right, but at least one entity seemed to agree that there's problems with it. reply bregma 39 minutes agorootparentWell, here are the facts (I was an insider at the time, and this is my testimony). Searches were anonymized and sent through Canonical servers to provide extended search result sets. This was configurable and could be disabled. Canonical of course had your IP address so they could reply, just like any and every HTTP server does. Your search query was not stored anywhere or aggregated, and it was not associated back to the originating IP address except to reply. Your privacy was respected and protected at all times. The Amazon search did appear as a plugin in an early prelease. It was never shipped in a released Ubuntu. The goal was to make things as easy as possible, even for the technically averse (who were still commonplace a decade ago), while still respecting and protecting your privacy. Of course, no matter what you do, someone is going to scream for everyone to come witness the oppression inherent in the system. We did it anyway with the expectation of baseless knee-jerk outcry and we were not disappointed. reply tombert 12 minutes agorootparentYeah, fair enough, I'll admit what I said was probably reductive, and if you worked on it you certainly know a lot more than I do; obviously the engineers at Canonical aren't idiots and they're not mustache-twirling supervillains. Just to be clear, I did run Ubuntu on my laptop for quite awhile (for about two years starting immediately after ZFS got integrated support), and I did like it, so I don't mean to suggest it was a terrible product. I guess I'm just always worried about for-profit companies, because their goal isn't necessarily always aligned with the customer's best interest. reply _heimdall 2 hours agoparentprevTo be fair to Satya, every leader should be judged on what they do not what they say. This isn't a Microsoft or Satya problem, pick a large corporstion and you'll find examples of this behavior everywhere. Words in an email hold absolutely no weight, when leaders choose to trade security for something else that's all employees need to know. reply akira2501 49 minutes agoparentprev> you know, what if a user missed an ad and wants to go back and see what they missed I have meetings with adtech guys and this gets pitched every time. Along with \"a way to save ads so you can watch them again at home later!\" And \"alexa enable ads that you can talk to!\" reply naikrovek 7 minutes agoparentprevSheesh you guys are annoying. - I do not see ads in “every nook and corner of Windows” and neither do you. - I do not have a recorder installed on my Windows machines and neither do you. - no one qualified to make that statement has said that Microsoft is the most secure platform. It is so hard to listen to anyone who exaggerates at this level. If anything, it drives interest in Microsoft because these are all obviously false statements and some readers will wonder what your true motive is. You just raise suspicion in yourself. At least you used a new account to distance yourself from any other identities you may have here. In fact I would say that was the only smart move in your entire comment. Anyway, this is a damning revelation by the whistleblower and I hope Microsoft feels a good amount of pain because of it. NEVER make any decision with money as your sole input. It will always be a bad decision, and it’s just a matter of time until that decision bites you or someone you care about. reply HumblyTossed 2 hours agoparentprevSay one thing, do another. reply JohnMakin 3 hours agoprevThe misaligned incentives between security and profit, especially in public companies, is not really a fixable problem without a massive cultural shift. I'm not sure at this point what could even trigger one. I've always dabbled in cybersecurity, taking on the hat in various roles over the years but have refused to go full time into it due to what I have personally seen in the industry - an overwhelming focus on compliance rather than actual good security practices, and the compliance standards are either very lacking or poorly enforced. reply graemep 2 hours agoparentThis is exactly it. There is no incentive to prioritise security. It is not visible to customers, except in terms of compliance, most likely a check-list approach. I think it needs a massive cultural shift, but from customers. If customers were willing to evaluate security (consumers cannot, but enterprise can) properly, demand binding assurances, and make buying choices accordingly industry would respond. Of course MS is too strongly entrenched in the desktop market for this to be completely effective. reply fuzzfactor 21 minutes agorootparentThe purpose of using Microsoft products in an office environment is so that your office can be run with as much personal computer enhancement as you originally realized when you first effectively replaced the traditional office machines or more-labor-intensive tasks with software-powered substitutes. Which all occurred way before any of the things like \"single-sign-on\" got popular among those who didn't seem to know any better. The second this appeared it was easily recognized as one of the many consumer/entertainment features that must be disabled across every bit of any serious corporate network. Also best disabled on any home computer before it is allowed to touch the internet. There was no forthcoming mitigation, all Microsoft leadership could do was throw up their hands, after all there were unsurmountable reasons why such a threat could not be overcome. >it required customers to turn off one of Microsoft’s most convenient and popular features: Like any other office no-brainer: >the ability to access nearly every program used at work with a single logon. Duh. reply wyldberry 1 hour agorootparentprevWhen I first left offensive security consulting and joined an internal defensive team, a wise ex-agency person said to me \"In product development, the first things to often get axed are security, and performance. They are invisible to the user, until they aren't, and rarely do failures in those areas end a company.\" Granted this was prior to ransomware really blowing up, but even that itself is a different threat model that doesn't mean your product has to be good at security. reply hulitu 2 hours agorootparentprev> If customers were willing to evaluate security (consumers cannot, but enterprise can) Where i work, IT is outsourced and decision to buy most of the SW is made by managers who have no idea about computers. reply mihaaly 54 minutes agoparentprevI may be off, but to me as an affected outsider (user) the continuing insistance of using passwords after decades (yes, several decades) of problems and proven vulnerability, then to 'mitigate' with putting second line of 'defense' on the very fragile and non-transparent smarphone infrastructure instead of doing real reforms is a sign of not giving a faint fack. reply pimlottc 53 minutes agoparentprevIn the profit-center view, everything is either a cost center or a profit center. And it is nearly impossible to get anyone to truly care about a \"cost center\". reply nicce 48 minutes agorootparentWhat if the company is providing only cybersecurity-related services? Could it be in this case, that everything is on profit side. reply pimlottc 6 minutes agorootparentSure, but to the client hiring them, it's a cost. We'll take the basic compliance package please, no need for any of the gold tier high security features. reply 1oooqooq 1 hour agoparentprevit was like that in the 90s too. until people like cult of dead cow started to both sell the solutions and give it the tools to exploit everyone not implementing the solutions. today things like dmca actually protect the malicious incompetent and business which don't take on it are fools. reply Sohcahtoa82 2 hours agoparentprev> an overwhelming focus on compliance rather than actual good security practices I'm an application security engineer. I find that it depends widely on the company. You're right that compliance is purely just a checklist and does and doesn't actually do much for security. At best, it slows down a determined internal attacker. ie, a developer can't install a back door since code reviews are enforced by SCM before merging is allowed. But all the ISO-27001 and SOC-2 audits in the world won't prevent trivial attacks like SQL injection. So the actual security depends on how much buy-in the AppSec team can get from project management. I've had companies where I point out an obviously exploitable flaw that can easily cause DoS, and with some determination could get RCE, and I get radio silence. Others, I point out a flaw where I say \"It's incredibly unlikely to be exploitable, and attempts to exploit would require millions of requests that would raise alarms, but if someone is determined enough...\" and project management immediately assigned the ticket and it was fixed within a week. I can tell you one thing that's not doing any favors is overly zealous penetration testers that feel like they need to report SOMETHING so they invent something that's not an issue. For example, in one app I worked on, after logging in, the browser would make an API call to get information about the current user, including it's role. The pentester used Burp Suite to alter the response to the call to change the role to \"admin\", and sure enough, the web page would show the user role as \"admin\", and so the pentester reported this as a privilege escalation. They clearly didn't go on to the next step of trying to do something as admin, though, because if they did, they'd see the backend still enforces proper RBAC. Changing that role to \"admin\" essentially just made all the disabled buttons/functionality in the web app light up, but trying to do anything would throw 403 Forbidden. But I digress... > The misaligned incentives between security and profit, especially in public companies, is not really a fixable problem without a massive cultural shift. The EU seems to have figured it out, but the USA is a hypercapitalist hell-hole. It's such a shame that the population is mostly convinced that any regulation is bad and an attack on freedom. I roll my eyes at the Libertarians that claim that the Free Market(tm) will punish bad actors while the worst actors are rising to the top. Bad acting is profitable. reply ChrisMarshallNY 2 hours agoprevI think that when companies sell to the government, there is so much money to be made, and such a huge PR boost, that they are incentivized to cover up the naughty bits (a certain airframe manufacturer, comes to mind). It can mean anything from concealing slightly embarrassing stuff, to massive, systemic, deliberate, fraud; sometimes, the whole spectrum, over time. It often seems to encourage a basic corrosion of Integrity and Ethics, at a fundamental cultural level. When leaders say \"Make Security|Quality a priority,\" but don't actually incentivize it, they set the stage. For example, routinely (as in what is done every day) rewarding or punishing, based on monetary targets, vs. punishing one or two low-level people, every now and then (when caught), says it all. They are serious about money, and not serious at all, about Security|Quality. If you want to meet a goal, you need to incentivize it. Carrots work better than sticks. Sales people get a lot of stress, and can get fired easily, but they can also make a great deal of money, if they succeed. Security people don't get fired, if they succeed, and get fired, if they don't. Often, the result of good work is ... nothing ... No breaches, no disasters, no drama. Hard to measure, as well. How to quantify an absence? Sales: Lots of carrot, and the same stick as everyone else gets. Easy to measure, too. Security: No carrot. All stick. The stick can be a really big stick, too; with nails driven through it. I'm really not sure what the answer is, but it's cultural, and cultural change is always the most difficult thing to change. reply slashtom 1 hour agoparentI think this is sort of it but I don't think it's the carrot that's the problem here. I believe it's the process and yeah ultimately the culture. I don't think you want sales concerned about security, their focus should and only be on growth. The problem is if you don't give jurisdiction and power to the other side to actually say no this priority (security fix) goes in before work is done on this new feature, then you have an imbalanced system. If the project manager who is incentivized toward growth is the decision-maker for deciding what is prioritized, well of course naturally you'll have the PM choosing growth over security. Process needs fixing, give more agency and jurisdiction to the other side to effect change. It's not like security doesn't see what the issues are, it's just the fixes are not prioritized and the culture and process isn't balanced between both. reply spydum 3 hours agoprevAs per usual, executive platitudes around \"security first\" don't matter. If you pay and promote people for features, and don't reward security culture, people are not dumb: they and the management layers will optimize for that. I don't know how to design incentives to solve for this, but this is always going to be the way it is. reply jrm4 3 hours agoparentI do. It's law, regulation and liability. Until heads roll, until someone is punished, likely nothing will happen. reply olivierduval 3 hours agoparentprevI think that it could be \"security as a feature\" Usually, a feature is included in a product if the marketing show that it will grow the business more than the cost of the feature. Maybe we can try the same idea ? \"We identified this vulnerability, and it will impact X % of our customer and Y % will leave (+ reputation damage) so we will loose BIGNUMBER $. However, we can correct it for SMALLNUMBER $ in Z days. Decision ?\" reply mewpmewp2 2 hours agorootparentAnd where do you take those numbers from? Also identification is one thing, but good security should mean the vulnerability didn't occur in the first place. Then you also need to get budget for identifying vulnerabilities. After that you need budget to research how costly the vulnerability could be. But before getting those budgets you need budget again to propose all of that and data to prove its value. Unless you use your own time to do all of that or accidentally stumble upon something. I think the only realistic way to get any sort of budget is if a deep enough incident actually happens. And this will only last maybe for a year until most of the decisionmakers have been rotated with new ones wanting to only deliver again. reply imglorp 1 hour agorootparentprevThey did that in FTA: > In the months and years following the SolarWinds attack, Microsoft took a number of actions to mitigate the SAML risk. One of them was a way to efficiently detect fallout from such a hack. The advancement, however, was available only as part of a paid add-on product known as Sentinel. So you sell me a submarine with screen doors, avoid fixing it for years, cripple internal processes that would fix it, and then you want to charge me for a water alarm? That's chutzpah. reply olivierduval 4 minutes agorootparentI didn't think that it would be a feature to be charged for the consumer... only that it's a way to present it to top management reply Sohcahtoa82 2 hours agorootparentprevSecurity shouldn't be seen as a feature, it should be the default. Advertising something as \"secure\" SHOULD be seen as silly as advertising it as \"doesn't crash\". But we're not ready for that, I guess. reply makeitdouble 1 hour agorootparentIt's absolutely hard, but you need to advertise and promote security for it to stay relevant, internally and externally. The moment it becomes the \"default\" I think the only way is downward. The marketing dept should do something for that, that's their job. If Apple can tout privacy as a feature, Microsoft can find a way to have security as a shiny feature on their keynote, with internal projects rewarded for increasing security by x% etc. reply johnnyanmac 1 hour agorootparentprevWith the increasing number of breaches over the years, it is 100% a feature. I see it as insurance: ideally nothing happens, but if/when something happens the company should be ready to compensate for damages. reply nicce 2 hours agorootparentprevReal security cannot be feature. Your complete system design and other features should be based on the idea of ”security first”, if you really want to build secure systems. reply hulitu 2 hours agorootparent> Your complete system design and other features should be based on the idea of ”security first”, if you really want to build secure systems. One can argue that the most secure system is the one turned off and not used. And i am not talking about devices with builtin batteries. reply nicce 2 hours agorootparentOne can always argue that, but, fundamentally security is about limiting the systems' use for its purpose and eliminate all unwanted scenarios. If you need to use the system, you cannot turn it off or not to use it. reply tjpnz 2 hours agoparentprevManagers are already held accountable for their teams when they underperform. The same should also apply for their security blunders. reply everdrive 5 hours agoprevI'm not defender of Microsoft, but I don't know if I could point to any company which does not put profit over security. reply diggan 5 hours agoparentI guess the issue becomes when they say security is the top priority (and have been for two decades), yet all actions point towards it not being so. > Bill Gates in 2002: \"So now, when we face a choice between adding features and resolving security issues, we need to choose security.\" https://www.wired.com/2002/01/bill-gates-trustworthy-computi... > Satya Nadella in 2024: \"If you’re faced with the tradeoff between security and another priority, your answer is clear: Do security.\" https://www.theverge.com/24148033/satya-nadella-microsoft-se... reply sealeck 4 hours agorootparentTurns out businesses have a stated preference for \"nice things for the customer/society\" but a revealed preference for money. reply mook 3 hours agorootparentWould that be securities fraud, because they're lying to investors? (Going by Matt Levine's \"everything is securities fraud\" logic here to see if that might actually change behavior…) reply thiagoharry 2 hours agorootparentInvestors are very happy with profit over security choices. Moreover, decisions to maximize profitability thinking only in short term is also not bad for them if they perceive that can sell their shares before the consequences. A company that do not place profit above other things is not a good company to invest money and see it grow. A company will invest in security only as long as it increases profitability. Doing otherwise is not maximizing profits and lose investors. If you are a \"security company\", surely this means that you need the security to sell the product and get profitability. Other companies will have other tradeoffs to choose how much they invest in security to maximize profitability. reply nox101 3 hours agorootparentprevthen the laws need to change so bad security costs companies money. reply ls65536 5 hours agorootparentprevObviously, nobody is going to outright admit they put profits above security; indeed, they will often state the opposite. But their closely-held beliefs will shine through when it comes time to make decisions and the outcomes of those decisions are exposed to their customers and to the public. reply lesuorac 4 hours agorootparentDoes Bill or Satya write code anymore? It could very well be that they consider security the top priority but it's a moot point because they're so removed from operations. Although I would suspect that you're effectively right in that they either don't have it as a top priority or think they do but have a reveal preference of they don't. For example, an engineer that does rigorous security testing and finds nothing as well as launches one project gets promoted less often than an engineer that launches two projects and doesn't do rigorous security testing. reply ziddoap 4 hours agorootparentprevProfit is an implicitly assumed first priority for basically every business, otherwise the business wouldn't be around. I don't know of any company that has profit in their slogan, or in the core values statement, etc. reply formerly_proven 4 hours agorootparentI don’t put “breathe” at the top of my TODO list, either. reply _heimdall 2 hours agorootparentprevRelated to the GPs point, do you know of any company that publicly admits that they chose profit above all else? reply outside1234 4 hours agorootparentprevUnless you care about your review and promotion, in which case do features. reply drpossum 5 hours agoparentprevI genuinely think Proton as a company would prefer to cease to exist rather than offer insecure products. In fact there's a lot of offerings I would use (and pay more for) and they could make but choose not to (like a calendar that is not over an airtight protocol and could integrate with my regular calendar clients). reply 1oooqooq 1 hour agorootparentcounter point: nordvpn from day one everyone knew they were fsb pupets, and people are still giving them money. reply noqc 3 hours agoparentprevMicrosoft possesses, to put it lightly, a number of government contracts. I think this puts them in a bit of a pickle. reply ls65536 5 hours agoparentprevI'm sure there are some companies that realise security (or rather the critical lack of some important aspect of it) can impact profits, but that depends a lot on who their customers are too. Ultimately, if the customers who pay for a vendor's products and services don't value it, then the vendors won't value it either, short of any regulatory or legal requirements that might compel them otherwise. However, given that many large organizations (including governments) are Microsoft customers, it's strange to see in this case. Maybe there's a kind of \"it can't happen to us\" or \"nobody will find out about it\" arrogance going on, but they must now be seeing that the reputational damage is likely to have negative impacts, including hurting future profits, down the road. reply sqeaky 4 hours agoparentprevIf no company can make security the priority then maybe no company can be trusted with OS development. reply isodev 4 hours agoparentprevIsn’t there a point when a company becomes so big and so impactful to multiple layers of our life, that it should be impossible for them to continue focusing on profit alone? I’m not talking about regulation per se, but holding humans in charge of such corps more accountable. reply ossobuco 3 hours agorootparentI don't think it's going to happen unless we decide to nationalize private services that are vital to people. Why don't we have a public maps system, or a content sharing platform? Services like google maps/search or youtube by now are part of the infrastructure of our society. The same way as roads/railways or energy production are publicly owned in many countries the same should happen for digital services. In good parts of Europe railways are publicly built and maintained while the trains are privately owned. reply 1oooqooq 1 hour agorootparentprevtoday that means \"too big to fail\". in wall st it's called \"jackpot\" reply LightHugger 4 hours agoparentprevThey are rare, but Mullvad comes to mind immediately. They have made several decisions that directly impacted their bottom line (no recurring subscriptions where they need to keep the customer's credit card on file) to the benefit of their customer's security. reply haliskerbas 4 hours agoparentprevAgreed it’s deliver value for shareholders >>>>>>>>> everything else reply dfxm12 4 hours agoparentprevThis isn't about Microsoft, per se. This is about the fact that there's no risk for companies who do, even if they're bidding for government work. Hopefully whistleblowers making these things public will lead to the public putting pressure on their elected officials to actually make some regulations with teeth in this area. I'm not holding my breath, but it is something I consider in the voting booth. reply aaomidi 4 hours agoparentprevLet's Encrypt Google Trust Services Disclaimer: I've worked in both of these :) reply meandmycode 4 hours agorootparentAny company with sufficient size will fail to incentivise the things they claim at the top, unfortunately the impacts of decisions (especially during austerity) are poorly understood, so even the supposedly best intending will fail once you reach a size reply bdcravens 4 hours agorootparentprevWhat products do those two companies sell? reply 1oooqooq 1 hour agorootparentthey sell market protection. to google. it makes crawlers much more expensive. makes everyone depend on their CDNs etc. reply mardifoufs 15 minutes agorootparentAre you referring to google trust services? I don't see how that applies to let's encrypt otherwise. reply micromacrofoot 2 hours agoparentprevIn other words, when faced with an existential threat... * go bankrupt because we can't be secure * be less secure and stay in business ...guess which one will almost always win. Microsoft of course, as a multi-trillion-dollar company has no such threat and there's no reasonable excuse for this. reply ThinkBeat 4 hours agoprevThis whole article seems a bit odd to me. What is \"the product\" ? Presumably this is not related to earlier problems with SolarWinds. Did MS screw up. Yes. However, all things have bugs. I takes one person finding one bug and exploiting it. and there are enormous resources going into finding one, and I am certain that this is the only one. I am sure the NSA is sitting on a pile of them. Whereas the developers have to think about everything that can happen and protect against it. Does this make Microsoft different from its competitors? I think Microsofts strategy is somewhat similar to Linus: Where security patches are often not part of new releases due to the burden of establishing what the consequences of bigger changes would be, and the fact that security people dont do sane things. (But you can of course pull them and make it part of an in-house distro. https://lkml.iu.edu/hypermail/linux/kernel/1711.2/01357.html reply SuchAnonMuchWow 3 hours agoparent> Harris said he pleaded with the company for several years to address the flaw in the product, a ProPublica investigation has found. But at every turn, Microsoft dismissed his warnings, telling him they would work on a long-term alternative — leaving cloud services around the globe vulnerable to attack in the meantime. That is not a screw-up, that is a deliberate decision. reply drewda 3 hours agoparentprevYou might want to read the actual article. My understanding is that it was a two-part exploit: 1) The Solarwinds product was hacked to allow backdoor access to organizations' on-prem networks. 2) The hackers then took advantage of the \"Golden SAML\" vulnerability in Microsoft's Active Directory Federation Service (AD FS) to leapfrog via \"seamless SSO\" from the on-prem network into the organization's cloud resources hosted by Microsoft. The article is all about how various Microsoft leaders and staff did not fix #2, because many said it would never be an actual issue exposed to the world. This is extra damning because Microsoft is selling components at the core of both governments' on-prem and cloud systems, so if they don't take security extra seriously, their systems can present passive vulnerabilities. reply tbrownaw 3 hours agorootparent> You might want to read the actual article. ProPublica articles in general are structured in a way that makes them a pita to extract actual useful information from. reply BeetleB 3 hours agorootparentIt's in the article's headline. And at the risk of annoying everyone, a GPT summary: This article investigates how Microsoft, in pursuit of profit and market dominance, overlooked significant security vulnerabilities that left the U.S. government and other entities exposed to cyberattacks by Russian hackers. The whistleblower, Andrew Harris, a former Microsoft cybersecurity specialist, discovered a serious flaw in a Microsoft application used for cloud-based program access. Despite Harris's persistent warnings over several years, Microsoft delayed addressing the flaw, prioritizing business interests, particularly securing a lucrative deal with the federal government for cloud computing services. The security loophole was within Active Directory Federation Services (AD FS), which if exploited, would allow attackers to impersonate legitimate users and access sensitive data without detection. Microsoft's decision to deprioritize this issue, despite internal and external warnings, eventually led to the significant SolarWinds cyberattack, affecting numerous federal agencies and demonstrating the consequences of the security oversight. Microsoft's response to these accusations has been to emphasize its commitment to security, stating that they take all security issues seriously and review them thoroughly. However, ProPublica’s investigation reveals a culture within Microsoft that sometimes places business growth and competitiveness over immediate security concerns, reflecting broader issues within the tech industry related to balancing profit-making with customer security. The article sheds light on internal conflicts, the company's handling of security vulnerabilities, and the broader implications of such practices for national security and customer trust. It also highlights the challenges faced by whistleblowers and cybersecurity professionals in advocating for swift action on security issues within large corporations driven by profit motives and competitive pressures. reply latexr 3 hours agoparentprev> However, all things have bugs. There are bugs and there are critical flaws you’ve been warned about. This is the latter. The fact that this was known by Microsoft but not fixed is the story. reply amaccuish 2 hours agoparentprevBecause as far as I can tell, there was no \"vulnerability\" here, it's just how the product works. Stealing an OAuth key is just as bad. Stealing a domain's krbtgt key is just as bad. Businesses want that when they login to a computer, they are SSO'ed in to all their apps. That's how ADFS works, you authenticate to it using kerberos and it issues you a SAML token. Here they stole apparently the key used to sign the SAML token so they could generate their own. Unless there was some vulnerability that exposed the key publically, I fail to see how in this particular incident its Microsoft's fault. reply Thorrez 1 hour agorootparent>Stealing an OAuth key is just as bad What is an \"OAuth key\"? Do you mean an OAuth token? No, Golden SAML is worse than stealing an OAuth token, because an OAuth token is valid for 1 user, but Golden SAML can be used to impersonate any user. Also, OAuth tokens expire, but Golden SAML doesn't expire (although if you steal an OAuth refresh token, that won't expire). >I fail to see how in this particular incident its Microsoft's fault. Andrew Harris wanted to warn customers about the weakness, and tell them they can prevent the weakness by disabling seamless SSO. Other Microsoft people said no, that would alert hackers to the attack, we want to keep the attack secret, and it also would jeopardize our contracts by making the default setting sound insecure. Then Golden SAML was published publicly, so that first reason was no longer valid, but Microsoft still wouldn't tell customers they could prevent the attack by disabling seamless SSO. Then Solarwinds happened, and Microsoft finally advised customers to disable seamless SSO. reply what-the-grump 1 hour agorootparentI think there is too much confusion in the details of the actual attack. You have to steal the private key for the SAML signing certificate for an app. The correct answer would be to scope any token to only have access to what the app has access to, the second layer which is documented in their 2020 article, is to require mfa on admin actions, and the 3rd layer is to disconnect azure admin accounts from on-prem admin accounts preventing this type of attack. But disabling SSO altogether is non-starter for most businesses, what are we going to do tomorrow? Spend months recreating 100,000x accounts in various applications, no. We decrypt ssl traffic in our company, someone steals the private key and now can read the entire stream including your bank account details, lets stop decrypting ssl traffic because someone might leak the key? The answer from the infosec communinity has been its worth the risk. reply temac 1 hour agorootparentprevThis is ignoring security in depth, weaknesses, and security architecture. When ignoring that, you can not pretend, and MS did pretend, that you had a good enough stance on security. Fixing discovered vulns alone is mandated, it gives you maybe half a point, but the other 9.5 points or at least 5 before you can claim you care about security require more than fixing known vulns or waiting for world scale incident to \"respond\". You have to prevent issues. reply pgraf 4 hours agoparentprevIt is true that nothing is 100% secure. Sitting on a major security vulnerability internally with a motivated employee pushing to fix it and doing nothing for business reasons is not negligence, but malice. People in the chain of command need to be held accountable for this. reply psychoslave 3 hours agoparentprev>What is \"the product\" ? Human attention sink where you can throw ads and other propaganda, what else? reply shkkmo 3 hours agoparentprevMicrosoft had a known, high consequence, security flaw that they did not acknowledge or fix, they had evidence that indicated it had already been exploited and they knew they had limited to no ability monitor for exploitation. This choice lead directly to the SolarWinds hack that happened in 2019 was discovered in late 2020 and acknowledged by the USG in early 2021. Many companies make bad choices around security for profit, however that factors I listed above make this extremely egregious. I would seriously question any use of Microsoft products in any security conscious organization after this reveal. I also hope that anyone negatively effected by the Solar Winds sue Microsoft for knowing about the vulnerability for years without fixing it or disclosing it. reply pgraf 3 hours agoprevImagine a major bridge that was built by a contractor. A internal safety inspector repeatedly warned his supervisors of structural deficiencies that could lead to the collapse of the bridge. Furthermore, in the pass of time two external sources publicly warned about the issue, but the company downplayed the importance. Finally, the bridge collapses. It becomes evident that the company did nothing about the issue because it didn‘t want to loose contracts selling more flawed bridges. The public would justifiably go nuts, and there would be legal consequences for everyone involved. What is different in our industry that companies (and managers) get away with such malice? reply magicalhippo 3 hours agoparentHere in Norway a bridge built with known structural deficiencies did in fact collapse[1], and basically nothing has happened except tax payers get to pay even more for a new bridge. Unless enough lives are lost, people generally don't care that much it seems. [1]: https://www.nrk.no/innlandet/statens-vegvesen-legg-fram-rapp... reply _heimdall 2 hours agorootparentI'm not sure if this would line up with the Dunbar number or something similar, but it sure seems reasonable that societies and centralized power should never grow beyond the scale where people stop caring. If the public is expected to keep government and corporstions in check but the public doesn't care, it can only end poorly. reply nicce 2 hours agorootparentprev> basically nothing has happened Maybe they proudly stated knowing the risks, and while unfortunate, risks became reality. And then everything is fine. reply pompino 6 minutes agoparentprevSo software developers should be criminally liable for introducing security bugs? reply johnnyanmac 1 hour agoparentprevBoeing in a nutshell. >What is different in our industry that companies (and managers) get away with such malice? Software isn't immediately life threatening. That's why it's all thr wild west outside of medical and aerospace. While it sucks to have PI leaked to the internet, you do have time to at least take action compared to a door in an airplane coming off. reply natsucks 3 hours agoparentprevI don't understand how this doesn't destroy a company. They willfully ingored a serious risk and it had major national security implications. reply dfedbeef 1 hour agorootparentHave you tried to use Google customer support reply red_admiral 3 hours agoparentprevWasn't there something a bit like that with the Morandi bridge that collapsed in Italy? (There was definitely something like that with the Mottarone cable car that had been running for years with the safety catch disabled. When the tow-rope snapped, wiht no catch, the cabin rushed down and killed everyone on board.) reply delfinom 2 hours agoparentprev>What is different in our industry that companies (and managers) get away with such malice? Lack of professional licensure that binds you to state regulation with jail time as one of the stated punishments besides financial liability. Heh, the government could start effecting change by mandating licensure and sign-offs by licensed individuals when contracting for software products sold to the government. reply ckozlowski 2 hours agoprevThere's a pretty big caveat in this story which I feel is being looked over: \"Disabling seamless SSO would have widespread and unique consequences for government employees, who relied on physical “smart cards” to log onto their devices. Required by federal rules, the cards generated random passwords each time employees signed on. Due to the configuration of the underlying technology, though, removing seamless SSO would mean users could not access the cloud through their smart cards. To access services or data on the cloud, they would have to sign in a second time and would not be able to use the mandated smart cards.\" The U.S. Government (USG) is one of MSFT's largest (if not the largest) customers. The user base is enormous, and the AD footprint equally so. I have experience working in this space; the user and roles management is a nightmare with comprimised credentials, locked out accounts, and the like. Given the nature of their work, it's a constant target. The USG has been attempting to move everyone to smart card auth to help mitigate some of these issues. Removing passwords and turning everyone to two-factor auth would greatly reduce their attack surface. They've been pursuing this for years. So along comes this guy, and he says that, as part of this fix, just tell all of their customers to turn this off. I don't dispute the danger of the original SAML flaw. But I think Harris is unfairly judging the rest of MSFT's reaction here. He's asking them to turn off two-factor auth across entire agencies. I might as well hand an attacker a set of credentials because that's the amount of effort and time they would need to phish a set off someone. To reiterate, the flaw in AD FS was bad and needed immeditate attention. But the short term mitigation Harris proposes would drastically hurt their security and open tons of customers to attacks of the very sort they were trying to prevent. This story is spun as another instance of a company not caring about security, but I see a \"whistleblower\" who had a very narrow view of their customers overall security posture, and threw a fit when this was pointed out to him. \"To access services or data on the cloud, they would have to sign in a second time and would not be able to use the mandated smart cards. Harris said Morowczynski rejected his idea, saying it wasn’t a viable option.\" I would fully expect most government agency Info Sec Systems Managers (ISSMs) to say the same. reply foota 8 minutes agoparentI mean... I guess the issue here is more that Microsoft didn't make customers aware of this flaw, and continued to sell the service. Which... is exactly the articles point. They knew there was no secure way to administer it, and yet sold it anyway. reply MattSteelblade 1 hour agoprevSo...Golden SAML isn't a vulnerability, as the CyberArk article quoted in the post reiterates, it's a type of attack that requires completely comprising the box before using. Unless I am misunderstanding something, I don't see any particular flaw, per se. As Microsoft (mocked in the article) would say, it's not crossing a security boundary. SSO will ALWAYS have this particular tradeoff. If your SSO infrastructure is compromised, everything that uses it is at risk of being compromised. reply spdgg 46 minutes agoparentSounds like the vulnerability was one within AD FS and that exposed the private key, making golden SAML possible. reply mihaaly 1 hour agoprevI observed they chose profit over usability and user needs as well (the list is toooo long, I save all of us from pouring all here, let's say I am contemplating getting a completely different job where I do not have to run circles around the way Windows is corrupted), so this fits into the big picture afterall. reply skilled 1 hour agoprevThe hearing will be streamed on YouTube in ten minutes from this comment: https://www.youtube.com/watch?v=kB2GCmasH4c reply mikeegg1 49 minutes agoprevWill the whistle blower end up the same way as Boeing whistle blowers? \"See something; say something.\" reply thiagoharry 3 hours agoprevLike any other private company? All choices are to maximize profit, even when they spend resources in security, it is to maximize profit. reply 1vuio0pswjnm7 7 hours agoprevSounds like the same Microsoft culture as has always been. Like a cult. It can do no wrong. The conversation with Microsoft businesspeople at conferences was always the same: Microsoft has no deficiencies, there is nothing it isn't working on and it has a solution for every possible problem. Other sources of software do not exist. There is only Microsoft. Total illusion put forth by delusional employees. The outside world can be ignored because life in the cult is good. reply eganist 5 hours agoparentIf memory over the last two decades serves, this is a relatively recent degradation. Microsoft's security reputation prior to the recent (5ish years?) failures was largely built up on top of the work stemming from the Trustworthy Computing memo. https://www.wired.com/2002/01/bill-gates-trustworthy-computi... reply diggan 5 hours agorootparentBill Gates in 2002: \"So now, when we face a choice between adding features and resolving security issues, we need to choose security.\" Satya Nadella in 2024: \"If you’re faced with the tradeoff between security and another priority, your answer is clear: Do security.\" Microsoft in 2024: Run this software on your computer so we can take a screenshot of everything you do, index it and we promise Security is still, and have always been, the priority. And yes, we do store data unencrypted on your disk, why are you asking? reply NekkoDroid 5 hours agorootparent> And yes, we do store data unencrypted on your disk, why are you asking? But don't worry, you need to be an administrator to open the file. What? your average person daily drives an administrator account? How should we have known that??? reply freedomben 5 hours agoparentprevThis comment sounds hyperbolic, but it really isn't. It's really bad. This has been my experience with Microsoft employees also. In my experience, what makes for bad software is PM and engineering hubris. You definitely need some vision and confidence as just following user feedback is a recipe for terrible software as well. The key is to find the right balance and straddle that line. If it's been long enough for insiders to tell the story of Windows Phone and the eventual cancellation, I'd be fascinated to hear the story of that (from inception to death) and how that went internally given the culture. reply fingerlocks 2 hours agorootparentCan confirm, it is 100% hubris based on my limited time of working at Microsoft. There is pervasive NIH syndrome, re-inventing the wheel, and massive amounts of over engineering and unnecessary abstraction caused by chasing the endless \"But what if...?\" dragon. This behavior is justified, and critics are silenced, by the \"But we're an enterprise company!\" cop-out reply tracker1 4 hours agorootparentprevJust wanted to say that I thought the Windows Phone (the last version of such) was relatively nice. It had a decent developer experience, but was pretty much an also ran and didn't have enough market share to overcome mindshare for first party apps. When so many first apps were iOS first and Android later, throwing a third option in the mix just missed the mark more often than not. I was already in the Android ecosystem and far less cynical at that point about Google. reply Ylpertnodi 3 hours agorootparentI didn't get a Windows phone because i don't trust Microsoft. A friend had one and it was really ok, but no way for me. reply tracker1 3 hours agorootparentIn retrospect, I don't trust Apple or Google either... reply magicalhippo 2 hours agoparentprevWhat Microsoft can provide are lots of nice stickers saying they conform to this or that security standard, making security folks in IT departments all warm and fuzzy. At least that's how it appears from our POV, selling B2B applications. They don't seem to care that much about actualities as long as the security checklist passes. reply Drakim 5 hours agoparentprevDon't worry, Microsoft has big ambitions and huge plans about how to truly present themselves as more safety oriented in the future. reply NekkoDroid 5 hours agorootparentAI Safety. Code is run through AI to check for vulnerabilities. Files are analyzed by AI to ensure they aren't malware. Every instruction is run through AI to ensure nothing maliciously is happening (mostly enforcing DRM :). Every pixel is output by AI to ensure you see nothing not intended for your precious eyes. reply freedomben 5 hours agorootparentYou joke, but (the DRM part at least) is the future I fear is coming. It could hit us from so many angles (not forgetting Chrome's Web Environment Integrity and Apple's Private Access Tokens), and with all the money and power behind it (big tech plus big copyright), and the complete apathy of the average user towards this, it seems inevitable. reply NekkoDroid 5 hours agorootparentThe DRM part wasn't really part of the joke, just a sad truth that is being worked on more and more that sadly fit into the joke. reply diggan 5 hours agoparentprevTo be honest, that sounds like every company that suffers from delusions of grandeur and wants to conquer the planet, one way or another. What you're saying is equally true for Apple, Google, Amazon and most other public companies today. You're never gonna get \"Use this Microsoft product\" as an answer from Apple support/engineer even if that product would solve your particular problem better. reply surfingdino 4 hours agoparentprevI'll give you a 4-letter word... Zune /s reply hypeatei 4 hours agoprevExecs should absolutely be held responsible, but the human factor is always there. Many times people will take the easy route and get worn down by security practices or roadblocks. I think it's too easy to go \"alright focus on security\" and then expect it trickle down and figure itself out. reply xyst 3 hours agoprev> “If you’re faced with the tradeoff between security and another priority, your answer is clear: Do security,” corporate morality is a Potemkin village. It's all about the profit and appeasing the shareholder, baby! is anybody honestly surprised at this point? The abbreviation of \"M$\" is well deserved despite small OSS contributions and attempts to PR their way out of previous history (ie, United States v. Microsoft Corp. [2001]) reply winocm 57 minutes agoparentThe original sins: https://www.nytimes.com/1970/09/13/archives/a-friedman-doctr... https://www.sciencedirect.com/science/article/pii/0304405X76... reply say_it_as_it_is 1 hour agoprevBusiness decisions involve profits against everything, not just security. Delaying shipments to make a product more secure can affect revenue targets. reply nonrandomstring 2 hours agoprevSurely, a whistleblower is someone who reveals a truth that nobody knows? reply SebFender 2 hours agoprevWe needed an article to make sure this was clear. reply stratigos 2 hours agoprevThis comes off like a study being published that shows tobacco is harmful to the lungs. reply cellu 4 hours agoprevsurprisedpikachu.jpg reply hooverd 3 hours agoprevSecurity first, but security from whom? reply outside1234 4 hours agoprevUnless that other priority is laying people off. Then the layoffs are more important. reply fredgrott 3 hours agoprevoh lets put fonts in the user space rather then the kernel space what could ever go wrong? this not new its a major feature of how MS works reply photochemsyn 2 hours agoprevYes, it's called investment capitalism - as long as the consequences of actions one demanded are never felt by oneself, due to limited liability of the financiers and shareholders, then such behavior will never change. The solutions are well known - the corporate death penalty is a good one, which dissolves the legal and financial structures of the company (the real assets such as factories are unharmed by this, and may simply be sold to a new more reliable set of financiers and shareholders, or may be nationalized and managed by the state, or may be handed over to the workers who run the place to see if they can form an employee-owned company or not, etc.). This isn't such a radical viewpoint, even many venture capitalists agree that this is the right way to go, e.g. on the airlines: https://www.cnbc.com/video/2020/04/13/government-should-let-... reply execveat 3 hours agoprev [–] I work in infosec, and this sounds like a communication failure on the whistleblower's part. Contrary to what many people believe, the profits should be prioritized over security for the most companies, that's only natural (after all, they don't generate any profits themselves, typically). The key is finding the right balance for this tradeoff. Business leaders are the ones that are responsible for figuring out the acceptable risk level. They already deal with that every day, so it's nonsensical to claim they aren't capable of understanding risk. InfoSec's role for the most part is being a good translator, by identifying the technical issues (vulnerabilities, threats, missing best practices) that go beyond the acceptable risk profile and to present these findings to the business stakeholders, using the language they understand. Either the guy wasn't convincing enough, or he failed to figure out the things business cares about & present the identified risk in these terms. reply jmuguy 3 hours agoparentThis is framing the story as a simple interaction (or interactions) between Harris and business leaders at Microsoft. It wasn't. Microsoft has a team responsible for translating between security researchers like Harris and its product teams/leadership. That team dismissed Harris because that team's priority was to ignore or downplay issues that were brought to it. Harris went around them and was still ignored. It seems like he tried everything short of calling the press directly to get someone to pay attention. Even after the issue was made public by other security researches, MS did nothing. What happened here was a systematic failure on MS' part to address a fundamental flaw in one of the most critical pieces of security infrastructure at the entire company. Companies like MS (and everyone else it seems) need to get out of this Jack Welsh mindset of the only thing that matters is the shareholders. MS acts as the gatekeeper of the most valuable organizations and governments on the planet. Their profits have to take a backseat to this type of thing or they shouldn't be allowed to sell their products to critical organizations and governments. reply execveat 2 hours agorootparentI might be misunderstanding, but from Andrew's Linkedin it looks like he wasn't a security researcher at MS, he was actually the person responsible for translating between security researchers and the upper management: > Evangelize security services, practices, products, both internally and externally. > Leading technical conversations around strategy, policy and processes with FINSEC and DoD/IC executive staff. reply Thorrez 1 hour agorootparent>he was actually the person responsible for translating between security researchers and the upper management: According to the article, the group in charge of taking input from security researchers and deciding which vulnerabilities need to be addressed was Microsoft Security Response Center (MSRC), and Andrew Harris wasn't a member of it. reply civilized 3 hours agoparentprevWhy not go even further? Why not say that the whistleblower was wrong and Microsoft business leadership was right? Maybe their profits from ignoring this issue have been fantastic, and the externalities from e.g. mass theft of national security secrets are not Microsoft's problem. reply execveat 3 hours agorootparentWell, because as a security person I can only evaluate his actions from the point of security. Evaluating actions of MS business leadership is beyond my expertise. I highly doubt that the senior leadership would willingly accept this kind of liability. But you need to put it into right terms for them to understand. Politics play important role at that level as well. There are ways of putting additional pressure on the c-suite, such as making sure certain keywords are used in writing, triggering input from legal or forcing stakeholders to formally sign off on a presented risk. Without insight knowledge, it's impossible to figure out what went wrong here, so I'm not assigning blame to the whistleblower, just commenting that way too often techies fail to communicate risks effectively. reply cplat 3 hours agoparentprev [–] During my Master's, security was one of the subjects I took. It started with an equation that related risk (how much you'd lose if something bad happened), the probability of that risk, and the cost of mitigating that risk. The instruction being, one tries to find a mitigation that costs less than the exploitation of the risk. And note here that \"cost\" does not refer to just money, but could be computational cost, energy consumed, etc. reply execveat 2 hours agorootparent [–] For the MS size entities, the risk calculation is way more complicated. The 1:1 between cost of mitigation vs cost of exploitation only applies to opportunistic attacks, really. At the level where APTs get involved, the data / access might be so valuable that they'd gladly outspend blue team's budget by a factor of 10-100. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "A whistleblower, Andrew Harris, claims Microsoft ignored warnings about a critical security flaw to avoid losing government business, which was later exploited by Russian hackers in the SolarWinds attack.",
      "Harris, a cybersecurity expert, discovered a vulnerability in Microsoft's cloud-based program that could allow undetected access to sensitive information, but his warnings were dismissed due to potential financial implications.",
      "Despite Harris's efforts and subsequent breaches, Microsoft maintained that no product or service was exploited, leading to criticism of the company's prioritization of profits over customer security."
    ],
    "commentSummary": [
      "Whistleblower Andrew Harris, a former Microsoft cybersecurity specialist, revealed that Microsoft delayed addressing a serious flaw in Active Directory Federation Services (AD FS) to secure a lucrative government deal, leading to the SolarWinds cyberattack.",
      "ProPublica’s investigation highlights a culture within Microsoft that prioritizes business growth over immediate security concerns, reflecting broader tech industry issues of balancing profit with customer security.",
      "The text discusses the challenges faced by whistleblowers and cybersecurity professionals in advocating for swift action on security issues within profit-driven corporations, emphasizing the need for a cultural shift to prioritize security."
    ],
    "points": 343,
    "commentCount": 162,
    "retryCount": 0,
    "time": 1718275185
  },
  {
    "id": 40661703,
    "title": "ChromeOS will soon be developed on large portions of the Android stack",
    "originLink": "https://blog.chromium.org/2024/06/building-faster-smarter-chromebook.html",
    "originBody": "Chromium Blog News and developments from the open source browser project Building a faster, smarter, Chromebook experience with the best of Google technologies Wednesday, June 12, 2024 ChromeOS will soon be developed on large portions of the Android stack to bring Google AI, innovations, and features faster to users. Over the last 13 years, we’ve evolved ChromeOS to deliver a secure, fast, and feature-rich Chromebook experience for millions of students and teachers, families, gamers, and businesses all over the world. With our recent announcements around new features powered by Google AI and Gemini, Chromebooks now give us the opportunity to put powerful tools in the hands of more people to help with everyday tasks. To continue rolling out new Google AI features to users at a faster and even larger scale, we’ll be embracing portions of the Android stack, like the Android Linux kernel and Android frameworks, as part of the foundation of ChromeOS. We already have a strong history of collaboration, with Android apps available on ChromeOS and the start of unifying our Bluetooth stacks as of ChromeOS 122. Bringing the Android-based tech stack into ChromeOS will allow us to accelerate the pace of AI innovation at the core of ChromeOS, simplify engineering efforts, and help different devices like phones and accessories work better together with Chromebooks. At the same time, we will continue to deliver the unmatched security, consistent look and feel, and extensive management capabilities that ChromeOS users, enterprises, and schools love. These improvements in the tech stack are starting now but won’t be ready for consumers for quite some time. When they are, we’ll provide a seamless transition to the updated experience. In the meantime, we continue to be extremely excited about our continued progress on ChromeOS without any change to our regular software updates and new innovations. Chromebooks will continue to deliver a great experience for our millions of customers, users, developers and partners worldwide. We’ve never been more excited about the future of ChromeOS. Posted by Prajakta Gudadhe, Senior Director, Engineering, ChromeOS & Alexander Kuscher, Senior Director, Product Management, ChromeOS    Labels   Archive  Feed Follow @ChromiumDev Give us feedback in our Product Forums. Google Privacy Terms",
    "commentLink": "https://news.ycombinator.com/item?id=40661703",
    "commentBody": "ChromeOS will soon be developed on large portions of the Android stack (chromium.org)302 points by feross 23 hours agohidepastfavorite404 comments chem83 19 hours agoIMO Android architecture does offer an advantage relative to vanilla Linux in the way that it creates a well-defined separation between kernel (+ drivers) and user space development, somewhat fixing the OS fragmentation. It helps prevent users getting locked out of the latest version of the OS just because the device manufacturer didn't update their BSP. It's the reason why Samsung offers 5 years of device updates and Pixel offers 7 now. https://source.android.com/docs/core/architecture/kernel/gen... reply phh 10 hours agoparentBoth real-world and on-paper status are literally the exact opposite of what you're describing. As others have already mentioned, just stating the facts about long-term support: chromebooks have much longer support than Android devices. Both looking at the 99-percentile, the median, and the 1-percentile. Chromebooks don't get stupid bugs that require workarounds in userspace like: - BPF maps being broken because someone at Mediatek ran some proprietary static analyser and blindly pushed some fix - Camera only works properly in the OEM app - GL drivers are upgraded once every blood moon and very OEM has its custom GL version - Treble doesn't break because Samsung decided that mount loops were dangerously insecure at a time it wasn't required - You don't need to keep workarounds for mainline kernels that you can't upgrade, because Google/Android forbids you to upgrade your kernel [1] Android deprecates driver after 3 years [2], so vendor need to do a lot of work on a very frequent basis. As an OEM, I just want to do my (painful) contributions to mainline, and have them maintain it. Android actively hinders me from doing that. As an Android OS developer, I could go on and on and on about the stupid issues that Android kernels get that Chromebooks don't. All of those issues would happen just exactly the same on Fuchsia. I'm not saying ChromeOS' development method works for smartphones, it doesn't. I'm not saying it is desirable, I think it isn't (because it completely kills most innovation). But ChromeOS handles fragmentation better than Android on every single criteria you could imagine. [1] Google/Android added a bunch a new stupid policies that actually prevent me from upgrading kernels on deployed devices [2] Yes there is actual planned obsolescence nowadays in Android. It wasn't the case few years ago where obsolescence was accidental reply rob74 9 hours agorootparentMy personal (probably non-representative) experience with ChromeOS vs. Android: my ChromeOS device (a Lenovo IdeaPad Duet 2-in-1 Chromebook) tends to get extremely unresponsive (i.e. hangs itself up for minutes before responding again, or just spontaneously reboots) the longer it runs without being rebooted, to the point that I eventually have to hard reset it after about a week of on-and-off use. I basically only use the browser on it (but with lots of tabs open). This never happened to me on any of the many Android phones/tablets I had. Maybe it's some bug in how newer ChromeOS versions work on this by now pretty old device, but it's still annoying as hell... reply phh 8 hours agorootparentGotta admit I'm curious there. I've rocked a rk3399 chromebook for years (stopped roughly when it got deprecated) without issues, and I was stressing it with crouton. My gf is still using a rk3288 chromebook \"fine\" (it's no longer her daily driver though). Those are low-end 8yo devices. Anecdotal counter-point I have a colleague with a Samsung Galaxy Tab A 8\" 2019 (SM-T290), and after just 3 years it became absolutely unusable, even after a factory reset. And it's \"by design\": it shipped with 2GB RAM, which was usable in Google/Android 9 (shipped OS), but just completely dead on Google/Android 11 (updated OS). Obviously I saved that device from oblivion with a Google-less Android that takes half as much RAM. reply wishfish 1 hour agorootparentprevI know your Duet problem well. I've got a Duet around here. It was slow from the beginning. Noticeably, it would often slow to a crawl while Chrome OS was downloading an update in the background. And sometimes, it would be slow after waking for about 5 to 10 minutes and then suddenly go to a more acceptable speed. I also have a Lenovo Ideapad Chromebook running on an Intel 4020 with only 4GB. Despite being slow hardware, it runs circles around the Duet. The 4020 can be sluggish at times, but it never had the big slowdowns of the Duet. It could even run Visual Studio Code at acceptable speeds. Something was very wrong with the Duet. Usually Chrome OS is good on lower end hardware. reply kelsey98765431 4 hours agorootparentprevAre you by chance using external storage media such as USB or SD cards? The bus on these older model chromebook devices had a choking issue where the kernel would fully saturate the transfer bus and cause a cgroup scheduling strategy to give full priority to the processes using the bus at the expense of the gui. Solutions to this problem include a new NVME or software level cgroup reconfiguration of the kernel to revert scheduling priority to equal across all cgroups. Or it could be something else entirely, but last time i had this issue and bug hunted it the finding was as stated above. reply freedomben 5 hours agorootparentprevI've been using Linux for a long time, including chromebooks, and this sounds like textbook low memory to me. I love linux, but it doesn't handle low memory situations well[1], and whatever Chrome does in low memory situations makes it much worse. Tab discipline is the most important thing you can do. I have an obscene amount of tabs open too so I feel you. I've started making liberal use of the Session Buddy extension and that is helpful, though my true fix was putting 128 GB of RAM in :-D [1] This is improving greatly in the last year or two, but that probably wouldn't have made it to your chromebook. reply moe_sc 2 hours agorootparentEarlyOOM [1] could help with that quite a lot. Not to sure about using it on chromebooks, but linux got quite a bit more usable because of it. [1] https://github.com/rfjakob/earlyoom reply nolist_policy 7 hours agorootparentprevYou could try disabling the Android system (\"Google Play\"), since they switched running Android in a VM in newer ChromeOS versions. So you have an Android VM running in the background all the time and that might overwhelm the small Mediathek processor in the Duet. reply jeffbee 3 hours agorootparentThat's the irony of this announcement. The only problems I ever suffered on ChromeOS came from the Android subsystem. reply hawski 7 hours agorootparentprevI had issues like that a few years back on memory starved Chromebooks, in my case the biggest savior was the Great Suspender/Discarder extension as I hoard tabs, otherwise the Android subsystem on a 4GB RAM machine is stretching it thin. reply loa_in_ 3 hours agorootparentprevAndroid system deals with so many more different architectures of hardware though reply 1oooqooq 3 hours agorootparenteverything is arm ebi... even x86 is long gone (Android 4) reply oblio 7 hours agoparentprevClose, but not quite. And guess who's triggered it, it's the usual suspect, the EU: > Availability of software and firmware updates > (a) > The latest available version of the firmware shall be made available for a minimum period of eight years after the placing on the market of the last unit of a certain product model, free of charge or at a fair, transparent and non-discriminatory cost. The latest available security update to the firmware shall be made available until at least eight years after the placing on the market of the last product of a certain product model, free of charge. (b) > Information on the minimum guaranteed availability of software and firmware updates, availability of spare parts and product support shall be indicated in the product information sheet as from Annex V of Regulation (EU) 2019/2013. https://eur-lex.europa.eu/eli/reg/2019/2021/oj I can't find all of it, but part of it is triggered by EU environment protection directives. reply fowl2 12 hours agoparentprevyes the beautiful advantage of having some super-forked kernel. the solution is to get everyone on the same kernel, which is then updatable - not hack together something that kinda works on top of a never updated snowflake reply cageface 11 hours agorootparentThe Linux kernel team has been hostile to binary drivers in the past. Is that not still the case? reply vetinari 11 hours agorootparentThe linux kernel team understandably does not want to maintain overcomplicated shims, keep multiple versions of the same subsystem and debug opaque problems so some can keep their precious binaries secret. You keep the binaries, you get to maintain them and solve their problems. Seems fair. reply bmacho 5 hours agorootparent> The linux kernel team understandably does not want to maintain overcomplicated shims, The linux kernel team should offer a fixed compatibility layer for drivers. And for user applications too, while we are at it. reply fireflash38 4 hours agorootparentYou are free to implement it. reply throw0101d 4 hours agorootparent>> The linux kernel team should offer a fixed compatibility layer for drivers. […] > You are free to implement it. But would it be accepted? From what I see with both NVidia's and OpenZFS's compat layer, it seems to indicate that the Linux kernel folks are actively hostile against any such thing. (Contrast this with, say, the FreeBSD folks where both the kernel- and user-land API/ABI stays frozen over the life of a major version release.) reply jpc0 2 hours agorootparent> NVidia's and OpenZFS's compat layer I think this is more related to GPL licensing than them not wanting to assist. If it's completely generic then maybe, but how do you argue that the closed source Nvidia license or the dubious license for zfs can be linked to the GPL licensed kernel without being invitation of one of those licenses. Specially concidering how vague the GPL is regarding what it even means to be using GPL licensed code. If I present an API anyone can use and Nvidia happens to target it it's a different ballgame than if I implemented and shim specifically targeted at Nvidia's binary blob. To my knowledge the latter is a violation of GPL should it be the inverse and therefore an explicit exemption would need to be put in place for the shim in the GPL and that is the showstopper. reply wizzwizz4 3 hours agorootparentprev> And for user applications too, while we are at it. They do. > If a change results in user programs breaking, it's a bug in the kernel. We never EVER blame the user programs. How hard can this be to understand? > WE DO NOT BREAK USERSPACE! — Linus Torvalds, 2012 reply cageface 10 hours agorootparentprevOk but this is also going to mean people are going to keep long lived forks, like Android, that do support binary drivers. reply bayindirh 10 hours agorootparentMaybe then companies should stop hiding their bad code and binary hacks behind closed source drivers and write proper open source ones instead. If they are using 3rd party IP, they should work with the providers for compromises, or license the patents and re-implement them. AMD and Intel did great strides on these fronts, not counting the money-loving people called HDMI forum. reply oblio 7 hours agorootparentI'd agree with you, but ever since mobile devices have taken off, aren't we much worse than before? In the last peak PC years, say, before 2011, I was under the impression that hardware vendors were starting to play ball, but now things seem super locked down and Linux seems to be falling behind in this tug of war between FOSS - binary-only. reply bayindirh 7 hours agorootparentI think the problem with mobile devices is not the software but the hardware. These devices are locked down partly because of business interests (planned obsolescence), but another part is personal identity security. A run-of-the-mill Android or iOS device carries more secrets inside, which have much more weight (biometric data, TOTP, serial numbers used as secure tokens/unique identifiers, etc). This situation makes them a \"trusted security device,\" and allowing tampering with it opens an unpleasant can of worms. For example, during my short Android stint, I found out that no custom ROM can talk with the secure element in my SIM, and I'm locked out of my e-signature, which is not pleasant. If manufacturers can find a good way to make these secure elements trustworthy without the need to close down the platform and weld shut, I think we can work around graphics and Wi-Fi drivers. Of course, we also have the \"radio security\" problem. Still, I think it can be solved by moving the wireless radio to an independent IP block inside the processor with a dedicated postbox and firmware system. While I'd love to have completely open radio firmware, the wireless world is much more complex (I'm a newbie HAM operator, so I have some slight ideas). So, the reasons for closing down a mobile device are varied, but the list indeed contains the desire for more money. If one of the hardware manufacturers decides to spend the money and pull the trigger (like AMD did with its HDMI/HDCP block), we can have secure systems that do not need locking down. Still, I'm not holding my breath, because while Apple loves to leave doors for tinkerers on their laptops, iPhone is their Fort Knox. On the other hand, I don't have the slightest confidence in the company called Broadcom to do the right thing. reply oblio 6 hours agorootparentInteresting details, thank you for providing them! Regarding this: > Still, I'm not holding my breath, because while Apple loves to leave doors for tinkerers on their laptops, iPhone is their Fort Knox. People don't realize, but 99% of what Apple does hinges on the iPhone. The rest of the products pack a much lower punch if the iPhone were to vanish from the face of the Earth completely. It's the product all their customers have and they have it at all times with them. It's the product that's probably the easiest to use and the easiest to connect to other things. So yeah, the iPhone will probably be the last non-military device on the planet to be opened up :-) reply amadeuspagel 6 hours agorootparentprevNvidia became the biggest company on earth in part thanks to closed source drivers, allowing them to invest more into software. reply account42 6 hours agorootparentWhat is your reason for believing the closed source nature of nVidia's graphics drivers played a role in their success? AMD's and Intels Windows drivers are also closed source and so were AMD's Linux drivers when nVidia managed to secure the lead. nVidia is also finally moving to an open source kernel module so a closed source one doesn't seem important to them for keeping their moat. reply michaelt 5 hours agorootparentPresumably amadeuspagel means nvidia has walked a careful line with CUDA & ML. CUDA is much more accessible/documented/available than a lot of comparable products. FPGAs were even more closed, more expensive and had much worse documentation. Things like compute clusters were call-for-pricing, prepare to spend as much as a small house. On the other hand, CUDA is closed enough the chips that run it aren't a commodity. If you want to download that existing ML project and run it on your AMD card - someone will have to do the leg work to port it. That means they've been able to invest quite a lot of $$$ into CUDA, knowing the spending gets them a competitive advantage. reply chmod775 6 hours agorootparentprevNvidia isn't even in the top 50 of largest companies by any sensible metric. reply bmacho 5 hours agorootparenthttps://companiesmarketcap.com/ By market cap, nvidia is the 3rd biggest one, about as big as google and facebook together. Market cap is a sensible metric. reply chmod775 3 hours agorootparentThen I guess Rhodium is the largest metal and the Hinkley Point C nuclear power station is the largest building in Britain. If you conflate value with size, both terms become near useless. Value can only mean something in relation to something else. Picture this: You have a large company with thousands of employees having similar revenue to their competitor, which accomplishes the same with one employee and a much smaller operation. Which one is likely to be more valuable? Obviously the smaller one. If we however conflate value with size, as is so often done in popular economics, just pointing out this single fact becomes a complicated exercise of having to carefully employ language that we neutered for no good reason at all. Not to speak of all the misunderstandings this is going to create with people who aren't used to this imprecise use of the English language. If you mean revenue, say revenue, if you mean value, say value, if you mean size, say size. Don't use \"large\" to say \"valuable\". Why would you do that if there's a perfectly good word already? Imprecise language is often used to either confuse or leave open an avenue to cover one's ass later... which brings us back to popular economics. reply hajile 4 hours agorootparentprevMarket cap for tech companies is a bubble. It has more in common with the lottery total than with a representation of real value. reply jeffbee 3 hours agorootparentprevThis is not a difference between ChromeOS and Android. ChromeOS is replete with binary vendor blobs, which is why the webcams, wifi, and touchpads work correctly on Chromebooks and poorly or not at all on other laptops running vanilla Linux. reply kaba0 3 hours agorootparentprev> You keep the binaries, you get to maintain them and solve their problems. Seems fair. And you get to use modern mobile hardware like what a pinephone has.. lose, lose. Especially that modems simply can’t be open-source for the most part due to governmental regulations, so this everything open-source fairy tale doesn’t make much sense in mobile hardware. reply yjftsjthsd-h 2 hours agorootparentNotice that it's only mobile where that's a problem; x86 machines have apparently been doing the impossible for ~30 years now. And no, modems aren't an excuse; the modem itself can't be FOSS (probably) but it's just a self-contained device running its own firmware; there's nothing stopping you communicating with it via FOSS drivers. reply kaba0 1 hour agorootparentDid you miss the first, say, 25 years of the 30? Because I have literally reset my video driver blindly from terminal once, because it just failed after an update, and similar stuff. Also, this is pretty much due to the oligopoly of a few entities in hardware space, making it easier to support said devices, and standards. Phones aren’t built from a small selection of devices, and they often have patented firmware that the phone manufacturer couldn’t even publish even if they want to. As I shown, look at the best attempt at building an open-source phone. It sucks majorly. reply yjftsjthsd-h 1 hour agorootparentAlthough I have thoughts on the matter, I'm not actually arguing about the quality of drivers, I'm arguing that x86 linux has had minimal problems with keeping all drivers in-tree, and if vendors want to support linux they've generally had no problem working with that. It is for some reason only mobile and embedded vendors that find it impossible to upstream drivers and that insist on shipping hacked-up vendor forks of ancient kernels. And again, firmware is a separate matter; there's plenty of cases of linux shipping blobs for the device to run, so long as the driver to talk to that device is FOSS. reply Matl 11 hours agorootparentprevBinary drivers are hostile, not the Linux kernel. reply askonomm 9 hours agorootparentOk no problem, I'll just ideology to run things. reply klabb3 8 hours agorootparentYou don’t have to go anywhere near ideological debate to argue against binary blobs in the OS. The blobs could be verbatim from Stallman himself and blessed in the holy waters of EFF, and they would still be bad for dozens of technical reasons. reply 1oooqooq 3 hours agorootparentprevgood. everyone should be hostile to binary drivers. you have to not understand the first thing about kernel drivers to even consider binary drivers upstream. for starters, who provides a new binary when the api change every every new kernel version? reply fransje26 9 hours agorootparentprevThe linux kernel is full of binary blobs. For better or worse.. reply ungamedplayer 8 hours agorootparentDo you mean the firmware repo? Because I don't remember seeing any in the source code... But ist be looking at the wrong place. reply 1oooqooq 3 hours agorootparentanti gpl bots don't know the difference. they usually can't read code either. you're correct, only firmware tree have binary blobs. reply anthk 6 hours agorootparentprevThe TGZ it's full of blobs. reply izacus 11 hours agorootparentprevAndroid has been booting off mainline for awhile now, so what are you going on about? reply dezgeg 11 hours agorootparentMainline kernel tends to have only basic support (if at all) for many SoCs that actually get used in phones, especially full power management support has been lacking. reply izacus 10 hours agorootparentRight, but it doesn't seem SoC vendors are budging on that - maybe it's slowly time for Linux to figure out a better approach? reply dvdkon 9 hours agorootparentMost PC and server hardware has FLOSS drivers (with proprietary firmware), even Qualcomm is upstreaming support for the new Snapdragon Elite (maybe it's made by a different team?). I think phone SoCs are the odd ones out, which sadly doesn't mean they'll improve any time soon. Supporting an ABI for binary drivers in Linux might help phones, but it would give everyone else a chance to regress in their support, so I understand Linux kernel developers' position. reply bayindirh 10 hours agorootparentprevStopping planned obsolescence via closed drivers is a start, no? reply yjftsjthsd-h 2 hours agorootparentprevLinux isn't budging - maybe it's time for SoC vendors to figure out a better approach? reply dvdkon 9 hours agorootparentprevAndroid can boot from a mainline kernel, but all that gets you is that phones run a kernel forked from mainline, not forked from Google's fork. reply modeless 17 hours agoparentprevChromeOS was already guaranteeing 10 years of updates for every device. Has any Android phone ever gotten that many updates? reply zamadatix 16 hours agorootparentThe Nvidia shield vs other 1st party SoC devices running Android (e.g. from S devices with Samsung SoCs or Pixels with Google SoCs) goes to show the short lifespan of Android phone updates really has nothing to do with the underlying hardware or OS. Also the type of updates guaranteed for 10 years on ChromeOS are not the feature update type that require upgrading the actual OS, just security and bugfix patches. reply DCKing 10 hours agorootparentThe Shield TV has had an impressive support lifecycle for an Android device but it still falls well short of a 10 year support cycle. The Shield was released in May 2015 and its latest software update has an Android security patch level of April 2022 and was released November 2022. No more updates seem to be forthcoming. Notably, all Shield TVs today are vulnerable to remote code execution when displaying a malicious WebP image [0], a widespread issue uncovered last year. Apple released the Apple TV HD two months after the Shield TV, but it still receives the latest tvOS updates to this day and will be receiving this year's new tvOS 18 [1] [2]. It received a fix for that WebP issue the same day as supported Macs, iPhones and iPads did last September. Even the best Android device examples with good vendor support still seem to be falling short. The Shield TV is still capable streaming hardware in 2024 used by many people, but it's sitting there doing that while missing important security patches unbeknownst to the users. [0]: https://blog.isosceles.com/the-webp-0day/ [1]: https://www.macrumors.com/2024/06/10/tvos-18-compatible-appl... [2]: To be fair it's the only Apple A8 device that receives support until today. The iPhone 6 with the same chip was launched mid 2014 and received its last update in early 2023. reply PaulHoule 3 hours agorootparentI still have a Shield TV. It is pricey but my understanding is that other Android streaming devices suck. I was thinking of getting a second Shield for my second TV but it turned out I had a $50 PC sitting around which works fine as a media player. reply pquki4 9 hours agorootparentprev... which confirms that this has more to do with the vendor rather than hardware/OS? reply londons_explore 14 hours agorootparentprevChromeOS updates are actual feature updates - for the 10 years of support, your device will be tip-of-tree and have all the features (except a few which are flagged-off, usually for business or performance reasons) reply zamadatix 12 hours agorootparentYeah, you're right. I got a little separated from the comment chain here after writing about the Nvidia Shield and what it implies about phone lifecycles by conflating that story with talking to ChromeOS. While the Shield received major kernel version updates as recently as 2 years ago, going back to the actual comment discussion around the abstracted ChromeOS userspace vs firmware/kernel/driver layer ChromeOS does already drive feature updates of the upper segment of the OS for the full lifecycle without requiring the kernel updates seen in the Shield example. Good catch and thanks for the correction. reply chem83 16 hours agorootparentprevThat's likely due to contractual obligations with Google and the use of well-supported AMD/Intel hardware and BSP. Linux hardware / device architecture makes this difficult on Arm and the many silicon BSP providers and that's what the stable Kernel Module Interface that Android imposes helps with. reply NewJazz 12 hours agorootparentThey've had arm Chromebooks with the same guarantees. Magically Qualcomm and Mediatek seem to be able to find their firmware and kernel sources when it comes to their laptop chips. reply eddythompson80 16 hours agorootparentprevlol, no? The most I got from Sony and Samsung in 2012 and 2015 was 1-2 years. Which is when I generally stopped using Android. It was frustraing enough to read all the news about \"Google releases new Andoid with (list of features)\" then asking when is my Sony/Samsung is getting that? to hear \"oh maybe in 4 or 6 months if you're lucky. Or download this random ROM from `appzworrier` on xdaforum. Surely they haven't put any malware in the ROM or you'll hear about it here\". I did give it another shot in 2019 with a Pixel 4 phone then after 2 RMAs with battery issues (4-5 hours of battery) I went back to iPhone. Funny thing is in 2013 and 2016 I switched back to the old iPhone I had before the upgrad, I just updated them to the latest iOS version there was at the time. After the shitshow I had with google support in 2019 I just gave up. reply chem83 15 hours agorootparentGKI didn't exist in 2012/2015, so it was very difficult and expensive for OEMs to support Android for longer. Apple could do so more easily because they control both hardware and software. Google introduced GKI and other related efforts/architectures to address this very problem, which is unique to Arm, Linux and the ecosystem around it. It's really silicon manufacturer's fault for not wanting to mainline and long-term support their BSP, not Google's. reply msh 12 hours agorootparentI would say it's both. Google could put support requirements in the gms certification reply izacus 10 hours agorootparentAnd how would that help when the major SoC manufacturer refuses to support their chipsets? reply msh 5 hours agorootparentIf a device cant get play certified it cant realistically be sold anywhere in the world except china, so no phone manufacturer would choose a SoC for their phone that was not supported. reply bayindirh 9 hours agorootparentprevLet the market decide, then. Isn't this what capitalism loving companies say, anyway? Broadcom had to open source their wireless drivers at the end. Same can happen for SoCs too. Add pressure, if not broken, goto 10. reply moneywoes 17 hours agorootparentprevHighest I could find is 6 reply mortos 15 hours agorootparentThe Pixel 8 is promised 7, which I believe is the highest of any phone. reply nordsieck 13 hours agorootparentI guess we'll see how things shake out. Unless something drastic changes, I'm sticking with Apple because of their demonstrated long-term support. Every phone since 2015 has been supported for 6-7 years. And that's actual support, not a \"technically correct\" mix of real support and security patches only. https://www.statista.com/chart/5824/ios-iphone-compatibility... reply wasabinator 13 hours agorootparentThat's overlooking the fact that Apple does not release the full featureset of new iOS versions on older devices and as years pass the number of feature omissions increases. reply asadalt 11 hours agorootparentwhich makes sense? like i can’t expect the new ai intelligence (a 3B model) will run at all on iPhone X reply youngtaff 10 hours agorootparentprevThere’s hardware dependencies for some of the ‘missing’ features and if the hardware’s not there then there’s not a lot the OS can do to support it reply eliaspro 13 hours agorootparentprevThe Fairphone 5 will get 8 years of security updates: https://support.fairphone.com/hc/en-us/articles/180206715370... reply 1oooqooq 3 hours agorootparentit's laughable because i doubt they have any proof that Qualcomm will continue to deliver updates. fairphone is just a intermediary. they are using the long life Qualcomm parts, that only guarantee production and stock for x years. maybe with software updates by an intern. it definitely will not guarantee a new driver set compiling to a recent kernel if the old kernel must be upgraded for security or anything else. so it's kinda bold they promise that with out any disclaimer that they are just hopeful reply oblio 7 hours agorootparentprev> Availability of software and firmware updates > (a) > The latest available version of the firmware shall be made available for a minimum period of eight years after the placing on the market of the last unit of a certain product model, free of charge or at a fair, transparent and non-discriminatory cost. The latest available security update to the firmware shall be made available until at least eight years after the placing on the market of the last product of a certain product model, free of charge. (b) > Information on the minimum guaranteed availability of software and firmware updates, availability of spare parts and product support shall be indicated in the product information sheet as from Annex V of Regulation (EU) 2019/2013. https://eur-lex.europa.eu/eli/reg/2019/2021/oj I can't find all of it, but part of it (Samsung also did the same thing) is triggered by EU environment protection directives. reply bwat49 5 hours agorootparentprevSamsung S24 has 7 years as well reply fsflover 10 hours agorootparentprevLibrem 5 will get lifetime updates, because it runs mainline Linux without proprietary drivers. reply kaba0 3 hours agorootparentprevGoogle can’t even keep a project alive for that long, so let’s not jump ahead. We will see when they actually deliver. reply nolist_policy 2 hours agorootparentUm they already delivered, for example on 8 years of updates for past Chromebooks. reply rchaud 15 hours agorootparentprev\"10 years of updates\" for Google products is a bold claim. Will ChromeOS even exist in 2034, let alone run on 2024 hardware? reply wiseowise 14 hours agorootparentWhy wouldn’t it? It might be rebranded as Android Desktop in the future or something, though. reply gtirloni 17 hours agoparentprevHow's that even an issue in Linux? If anything, vanilla Linux users have way more power over their computing platform and are never locked out of anything. reply fooker 16 hours agorootparentWhat is vanilla Linux though? If you're talking about something like Ubuntu, there's a tonne of work that goes into making sure it works with ..say.. a random three year old laptop with all the binary blobs in place. Try getting something like wifi or Bluetooth working on some weird ARM dev board and suddenly there's no vanilla Linux unless you're willing to write device drivers. reply eddythompson80 15 hours agorootparent> there's a tonne of work that goes into making sure it works with ..say.. a random three year old laptop with all the binary blobs in place. But that work gets done and \"users\" of Ubuntu or Debian or Arch get to use it from the sources they trust (aka Ubuntu, Debian or Arch). I'm not claiming to have a full understanding of how every package or kernel module Debian or Arch or Fedora ships works. But I'm trusting Debian or Arch or Fedora for my packages. If it comes to light that debian or fedora maintainers had no idea that they shipped a malware in a release, then I'll seriously question going with that distribution in the future. And without sounding facetious, that has happened multiple times in past especially with debian. But the times it happened it was clear to me that it was merely a mistake rather than extreme incompetence or malice. With Android, you have Google, which despite the general HN rhetoric I personally trust to not ship straight up malware. But when we're talking about Android that's not what we're talking about. We're often talking about binary ROM blobs from random XDA or RW users. Funny thing is 20 years ago, I'd have shouted about what's the difference between a random anon on XDA or WZBB providing a ROM blob. But now I know better. reply fooker 11 hours agorootparentYou trust Canonical, android users trust Google. Binary blobs are an unfortunate reality and no amount of trust in a company or entity can really solve this. For the record, Debian and Arch doesn't work very well on non standard hardware. I use arch on my desktop but gave up on using it productively on a new HP laptop. reply oblio 7 hours agorootparentHeh, and you could argue that laptops are standard these days. More laptops are sold than desktops, and HP is definitely a mainstream brand. I understand your usage of the word, I'm just pointing out that if the mainstream ain't \"standard\" anymore... it kind of sets the standard in practice. reply blackoil 16 hours agorootparentprevThat should be true for everything, if no one has written drivers for something it won't work. But once driver is added will it fail/updated for newer versions of Linux? reply zamadatix 15 hours agorootparentIt depends on how open the particular drivers are implemented. E.g. over the last couple of years the Nvidia driver situation for cards from the last 3 generations has changed across pretty much all 3 major levels: 1. Originally you had to use the proprietary binary driver to get anything useful to happen with your card at all. Updating the kernel without updating this would more or less lead to having an expensive brick in your PC. Some Wi-Fi adapters fall into the \"can't really be updated\" category as well. A _LOT_ of ARM shit is like this. 2. nvidia-open came along (still beta for desktop cards at the moment) and it puts enough in the kernel that you can update the kernel without needing an updated binary driver for your card to function 3. nouveau/nvk have very recently started to come to a decently usable state (i.e. they have reclocking via GSP and somewhat usable graphics API coverage/efficiency) for an even more open driver stack which tracks system updates even better. If your binary blobs fall into 1/2 then long term upgrades can be anywhere from impossible to unreliable. If they fall into 2/3 they can be anywhere from somewhat reliable to \"will be working longer than any sane person would still be trying to update the kernel on that device\". E.g. the AMD 7750 is 12 years old but can run OpenGL 4.6 and Vulkan via the latest AMDGPU driver in mainline mesa/kernel. LTS distros solve this by using LTS kernels and security patching them rather than requiring \"actual\" underlying OS updates during the version lifecycle. reply chem83 15 hours agorootparentprevThe reality is that drivers are not added, as you say. Most companies release an out-of-tree BSP targeting a specific kernel version. They often contain blobs and are often not GPL. Linux doesn't support a stable kernel ABI/API (https://www.kernel.org/doc/Documentation/process/stable-api-...) and the only way to avoid the associated issues is to mainline drivers, which most companies don't want to do (don't want to open source their IP, don't want to invest in maintaining it etc.) Android GKI/KMI addresses the issues related to this. GKI is relatively recent and OEMs don't offer 5+ years of Android updates because they haven't adopted it yet. reply lupusreal 10 hours agorootparent> and the only way to avoid the associated issues is to mainline drivers, which most companies don't want to do (... don't want to invest in maintaining it etc.) They wouldn't have to, that would be done for them, for free, if they got their code into the kernel... reply fooker 3 minutes agorootparentA driver is not a static one time thing. Once you mainline it, making improvements, supporting new devices and features become 10x slower. codedokode 13 hours agorootparentprevThat Linux note about why they don't want a stable API is ridiculous. This shows how unfriendly is Linux to out-of-tree software. They want third-party developers to refactor their working and tested drivers every time they decide to change something. I think that at least open source community should move to \"never break things\" concept. So that you can write and test code once and never touch it anymore. Refactoring your code because kernel/library has changed its interface is just a waste of time which gives nothing in return, often time of unpaid volunteers who could do something useful instead. This should apply to libraries, applications, programming languages, browser extensions. Take upgrading your Python 2 code to Python 3 as an example. You need to put a lot of efforts while gaining nothing. This is not how software should work. You should write a code once and it should work forever without any maintainance. reply guitarlimeo 13 hours agorootparentCorrect me if I'm wrong, but I thought that the actual code itself is stable, just that the compiled kernel API/ABI isn't. So if you open source your drivers and get them accepted into the kernel, then you don't need to rewrite/recompile them for each new kernel version. Just like AMD did with their drivers. And I think this is part a conscious decision that forces people to open source drivers. But this is honestly just guesswork based on what I've read, would love to learn more! reply Propelloni 10 hours agorootparentThe Linux developer community promised almost 20 years ago [1] that no release of the stable kernel will ever break something that worked in a stable kernel before. AFAICT, this promise holds. If you upstream your driver, the community will take care (cf. AMD), if you don't your users experience occasional pain (cf. NVidia). [1] http://kroah.com/log/blog/2018/02/05/linux-kernel-release-mo... reply afiori 11 hours agorootparentprev> So if you open source your drivers and get them accepted into the kernel, then you don't need to rewrite[...] them for each new kernel version. You don't, but someone does. In the note linked above they give an example of how the USB interface changed from synchronous to asynchronous, this must have required some refactoring in all drivers that used USB reply vetinari 10 hours agorootparentThat someone updating the drivers is exactly the same person who makes the change in the subsystem that they depend on. They cannot break what is already in the kernel, but they can break what's outside; it is after all an internal API. reply codedokode 8 hours agorootparentAnd that is wrong because people might have reason not to mainline drivers. reply vetinari 8 hours agorootparentThat's fine; but their reason is their problem. They are not entitled to free labor of other people. reply account42 5 hours agorootparentprevThose reasons tend to be petty ones that benefit the hardware company and noone else. Seems fair that the hardware company should pay for the consequences of their decisions and not the people advancing the kernel. reply pantalaimon 6 hours agorootparentprevWhat reasons are there other that corporate shenanigans? Sure it takes more effort to mainline the driver and address review comments, but if you aren't designing a throw-away product, it's effort well spent. reply vetinari 10 hours agorootparentprev> Take upgrading your Python 2 code to Python 3 as an example. You need to put a lot of efforts while gaining nothing. Just the python's 3 string/binary distinction and explicit conversion squashed an entire category of bugs. Yes, it required to make explicit what was implicit before, but still, that's huge achievement in my book; saying that it is nothing is a very, very shortsighted. reply account42 5 hours agorootparent> Just the python's 3 string/binary distinction and explicit conversion squashed an entire category of bugs. And created an entire category of new ones. reply exe34 11 hours agorootparentprev> They want third-party developers to refactor their working and tested drivers every time they decide to change something. no, they do it for you for free - you just have to make sure the quality is high enough to merge into mainline. reply codedokode 8 hours agorootparentThat's exactly what I meant: Linux is unfriendly to third-party software developers (drivers not in kernel, apps not in distro repositories). reply exe34 6 hours agorootparentI know a lot of people who are unfriendly towards sloppy work, yes. if the driver is of a good enough quality, they will be happy to take it off your hands and look after it for you! as a user, I hate apps that aren't in the repository. it takes effort to make sure it doesn't clobber something important just because the developer wanted to use the latest and most broken version of some library. thankfully nixos allows me to run whatever version of every dependency and then nuke the whole mess once I'm done with it. reply fooker 11 hours agorootparentprevIf the kernel had a stable ABI and made it a bit easier for closed source drivers to access kernel APIs, yes. That's how you can use a random 25 year old peripheral on multiple versions of Windows without anyone having to update a driver. reply vetinari 10 hours agorootparentThat's also how you make windows ship with three different USB stacks and their weird interaction. reply realusername 11 hours agorootparentprevIn theory maybe, in practice I'd bet in a better Linux support on a 25 year old peripheral compared to Windows. reply 1oooqooq 3 hours agorootparentprevyou understand little about what you're talking about. hardware is either supported or not. the \"work\" you mention on old wifi cards is to get a driver that was never contributed in any official way. hence unsupported. Linux have more hardware support out of the box than anything, ever. just educate yourself before purchasing. reply sambazi 10 hours agorootparentprev> What is vanilla Linux though? depends; but may i suggest starting at kernel.org then look at 'linux from scratch'/gentoo then maybe slackware; surely not ubuntu reply fooker 36 minutes agorootparentMay I suggest trying your hand on LFS on a ARM dev board and report how the experience goes? There's a reason all other kenels have adopted a stable driver API and ABI. reply anthk 7 hours agorootparentprev- Linux it's just a kernel. - Vanilla Linux it's propietary. - GNU/Linux-Libre it's the proper libre one, rebased from GNU maintainers. - There's no proper 'Vanilla distro' as LFS or Gentoo. Every one since 1993 has been an external bundle made from volunteers. - The closest vanilla distro from the GNU project would be Guix as it works as a distro and a universal package manager to ship libre software. reply lmm 15 hours agorootparentprevLinux offers no stable driver API; they claim that submitting your drivers into the kernel tree is a good substitute, but they have strict requirements on what kind of code they will accept and also decline contributions out of basically organisational politics reasons. So in practice you're locked out from any old or obscure hardware unless you have a lot of time to spend playing kernel politics, keeping up with their API changes, or both. reply 1oooqooq 3 hours agorootparent> in practice you're locked out from any old or obscure hardware unless you have a lot of time to spend playing kernel politics, keeping up with their API changes, or both. no sympathy. you're also out of luck using old hardware in the first place because the manufacturer wants you to buy the new version. they will help you LESS than Linux devs. you're point is very unhelpful to the discussion, unless you got drivers updates from the manufacturer reply account42 5 hours agorootparentprev> they have strict requirements on what kind of code they will accept and also decline contributions out of basically organisational politics reasons They accept only code which they will be able to maintain, yes. That excludes spaghetty or implementations of complex undocumented APIs that are useless without a binary blob. Nothing unreasonable about that. reply Wytwwww 3 hours agorootparentIt's unreasonable if there aren't any reasonable alternatives. reply Zambyte 14 hours agorootparentprevI wonder if Linus has ever made the connection between this problem and his stance against applying the Unix philosophy to kernels :P reply kccqzy 17 hours agorootparentprevI think you've only experienced Linux on Intel/AMD, not Linux on ARM SOCs. reply gtirloni 4 hours agorootparentI'm looking at several ARM devices running Linux on my desk right now. What's your point? reply faust201 12 hours agorootparentprevI dont think they are the target market. E.g. Some people will never buy computers but only assemble them; some will only run freeBSD or linux. For vast majority - just open and use the computer. reply jeffbee 3 hours agorootparentprevThis \"power\" is hypothetical unless you are willing to DIY all the drivers, which you will be writing blindly without datasheets. Vanilla Linux doesn't support IPU6 webcams, Qualcomm WiFi 7 chips, and dozens of other common parts. reply account42 6 hours agoparentprev> 5 years of device updates Is this supposed to be impressive? My current PC is over four years old and I'm in no rush to replace it. The same Linux distribution runs of decades old HW. reply kaba0 2 hours agorootparentDoes your PC include a bunch of noname random hardware, like gyroscopes, modems (which have legal requirements and there are only a handful to choose from, none of which has open-source drivers), etc? reply aixpert 10 hours agoparentprevAnother big advantage of Android is the implementation of intents (also compared to Apples new crippled intents API), which may turn out to become the deciding factor in the race towards the winning AI Agent OS, as it may enable AI to use programs programmatically reply chem83 10 hours agorootparentCare to share what makes Android intents superior to what Apple announced on WWDC? reply aixpert 10 hours agorootparentif I'm not mistaken the Intent categories available now are only Books Browsers Cameras Document readers File management Journals Mail Photos Presentations Spreadsheets Whiteboards Word processors One hour 39 + 39 seconds in the demo reply kaba0 2 hours agorootparentI think the context of these were that these contexts were included in the trained model, so it can interact with this kind of “intents”. Ios has a very wide-reaching “intent” system, readily accessible from Shortcuts. reply pantalaimon 6 hours agoparentprev> It helps prevent users getting locked out of the latest version of the OS just because the device manufacturer didn't update their BSP. Didn't Google push for mainline integration at some point to completely avoid the need for manufacturer specific BSPs? reply maven29 13 hours agoparentprevhttps://www.engadget.com/samsung-pledges-seven-years-of-upda... Samsung offers 7 years of major version upgrades on their flagship lineup starting with the S24. It is not retraoctive, although their 5 year policy has been in place for 2021 devices. It's unlikely that they will offer security patches beyond that point and the mid-range segment still only gets 4/5 years. Chinese OEMs offer \"major\" upgrades for longer, however they achieve this by backporting both Android mainline and proprietary features to older versions of Android, along with security patches. reply tgma 16 hours agoparentprevThis is actually a legit context in which GNU and Linux distinction matters and improves readability. \"vanilla Linux\" is really referring to GNU. reply Zambyte 14 hours agorootparentI actually don't understand the point they are trying to make at all, even from this lens. They say that by not using \"vanilla Linux\" there is a more well-defined seperation between kernel and user space development, when actually the exact opposite is true. By not using the vanilla Linux kernel, the kernel with Android pacthes is actually more coupled to the Android user space than other user spaces that traditionally run on Linux, like GNU or Alpine. reply afiori 12 hours agorootparentI believe they are talking more about the hardware abstraction layer, IIRC Android in recent years adopted a more stable interface for hardware drivers. To my understand in Linux most drivers are codeveloped with the kernel as it does not have a stable interface for drivers (one might say that it is a pro as it discourages proprietary drivers) reply anthk 10 hours agorootparentprevVanilla Linux is not even GNU unles you get Linux-Libre with is mostly FSF-sanctioned to push out every blob. reply dpratt 2 hours agoparentprevI will start to believe the \"7 years of updates\" promise for Pixel phones when the first model that was released with this promise is approximately 7.5 years old. At that point, I may begin to reconsider my opinion. I fell prey to Google marketing when they released the very first Pixel model - I spent an exorbitant amount of money on it, only to have it utterly abandoned and deprecated, with support and updates dropped just over a year later. As in all things relating to anything stated by Google with respect to the privacy, availability or expected lifetime of a consumer product, the maxim is not even \"trust but verify\", it should be \"distrust, watch carefully, and assume the worst\". reply jsnell 1 hour agorootparentAs far as I can tell from contemporary sources, Pixel 1 launched with a promise of two years of major releases + one additional year of security updates [0]. This was in October 2016. They exceeded that, and actually did three years of major releases, and security updates for a couple of months longer than promised, with the last one in December 2019 [1]. Seems like they a) delivered more than initially promised, b) did not drop it just a year after release. How long a support period do you think they actually promised, and where did they promise it? [0] https://www.androidpolice.com/2016/10/19/pixel-pixel-xl-guar... [1] https://9to5google.com/2019/12/02/google-pixel-no-updates/ reply tonymet 23 hours agoprevDoes this imply that Crostini/ Crouton / Linux developer mode will go away? I'm quite proud of the development I've done on a $200 chromebook and it was one of my favorite features. reply davidw 17 hours agoparent> I'm quite proud of the development I've done on a $200 chromebook High fives! My main Linux computer died a year ago and I needed something to write emails and browse on, so I ran out to Costco and got a Chromebook, expecting it'd tide me over until I could figure out a replacement computer. I'm still on the $250 Chromebook machine. It does everything I need and I'm pretty amazed at that. I've been doing some side projects with Elixir/Phoenix and it's not the speediest machine for sure, but it's quite serviceable. And for that price... that's amazing. reply PyWoody 16 hours agorootparentSame story here, except my quick and temporary purchase was about five years ago. It's been my daily driver for off-desk work ever since. It's still snappy and the battery life is great, too. I went full Linux on it using MrChromebox[0] a few years back with zero complaints. [0] http://mrchromebox.tech/ reply tonymet 14 hours agorootparentIt's uncanny how many devs have a story like this. There's something about compiling software on a barebones machine, almost like restoring an old VW and taking it to the track. I've ended up gifting the 4 or 5 budget chromebooks to nephews , pre-installed with Crouton / Crostini set up and accounts on leetcode / hackthebox to train on. It's remarkable how capable Chromebooks are. I tried using my $600 iPad Pro to code on and I never found it to be as capable as my $200 chromebook reply thesuperbigfrog 21 hours agoparentprevTermux has been a great resource on Android: https://termux.dev/en/ If the future ChromeOS-Android hybrid (ChAnroid?) loses Crostini Linux, but is able to run Android applications then Termux might be a good option depending on the task. Aside: this ChromeOS-Android merge brings to mind the Chenjesu-Mmrnmhrm merge to create the Chmmr in SC2. Maybe the ChromeOS-Android merge will similarly yield a wonderful hybrid result? reply resonious 9 hours agorootparentI love Termux but I'm a bit worried about its future. Google has been hardening Android in ways that harm Termux quite a bit (e.g. https://github.com/termux/termux-app/issues/2366). It does seem that new Android lets you turn off the phantom process killer, so maybe there's hope, but still Termux is forced to target an old API version and can't be published on Google Play. reply DownrightNifty 2 hours agorootparentThe issue you linked is more or less resolved at this point. If you find Termux processes being killed, just run this command over adb shell and you're good: settings put global settings_enable_monitor_phantom_procs false Google added this setting in response to a bug report submitted by a Termux maintainer. It's also persistent, so you only have to run the command once. The more pressing issue at this time is: https://github.com/termux/termux-app/issues/2155 reply pquki4 9 hours agorootparentprevYou can run a full VSCode on Crostini -- the \"real\" Electron app -- but not on Termux. There are ways around it like using code-server on a Linux environment, but far from the same thing. reply gorkish 21 hours agorootparentprev> Chenjesu-Mmrnmhrm merge to create the Chmmr in SC2 I'm having a Llanfairpwllgwyngyllgogerychwyrndrobwllllantysiliogogogoch moment here reply bitwize 16 hours agorootparentprevKnowing Google it'll be more like the Zoq-Fot-Pik... all jumbled together till no one knows what's what. reply theanonymousone 13 hours agoparentprevI had the same thought reading the title, but then remembered Crostini is a container running in a VM (Goddammit), so they can keep it intact, at least if they want so. reply modeless 23 hours agoparentprevWhy would it imply that? reply tonymet 22 hours agorootparentThey don't provide a lot of detail, but moving to a new OS platform would mean migrating functionality, and often that means dropping less popular features. Crouton / Chrostini seem to have a lot of dependencies on the current ChromeOS platform that would be costly to migrate. reply surajrmal 13 hours agorootparentAndroid is already using crosvm in the latest release. Launching a crostini VM is probably not a large lift. reply ajross 22 hours agorootparentprev> Crouton / Chrostini seem to have a lot of dependencies on the current ChromeOS platform that would be costly to migrate. Being careful not to make any pronouncements here since I do work in the group, but no, not really. Crostini is just a fairly standard VM manager with a handful of custom virtio and IPC mechanisms (some of which are really clever, to be fair). It doesn't require much of anything that hasn't been in the upstream kernel for years. And Crouton is just a community-maintained Linux chroot. People (including me) were doing exactly that kind of hack on rooted android devices more than a decade ago. reply yencabulator 4 hours agorootparentChromeOS VMs use Wayland over VirtIO for the GUI. Quick search tells me at least some people think Android has made design choices that are difficult to use Wayland with. Anyone know better? https://www.gfxstrand.net/faith/projects/wayland/wayland-and... reply ajross 2 hours agorootparentFWIW this part is true, but sort of mixed up. It's not really \"wayland\" that's at issue (Wayland surfaces are just shared memory like they are with any other compositor). It's that the Android GPU HAL wasn't designed with the sharing of contexts in mind, and so getting accelerated rendering out of the VM isn't a natural thing. And this isn't my area, and I have no expertise to offer except to say that I too am hopeful folks work this out in both a HAL- and open-source friendly way. And also to say that I'm running routine wayland apps on a chromebook every day and they render and composite just great with Mesa/LLVMPipe contexts, even things like e.g. STL slicers you'd expect to want the GPU. The need for acceleration is real, but limited to some special use cases. reply yjftsjthsd-h 22 hours agorootparentprevIf anything, the work on top of pKVM has very nearly brought the same feature to Android already, though they haven't to my knowledge finished executing on that vision. reply ignoramous 22 hours agorootparentAs it understand it, pKVM is mostly to run privileged vendor bits and not user apps. reply yjftsjthsd-h 22 hours agorootparentI'm sure that's where it started, but it's perfectly usable from userspace; https://liliputing.com/nestbox-makes-running-linux-virtual-m... reply surajrmal 13 hours agorootparentprevThey have something called microdroid which is a special VM which runs special android apps: https://source.android.com/docs/core/virtualization/writeavf... Of course, running a custom VM image like crostini would require system level support but that doesn't seem like it would be a problem or outside the scope of the avf architecture. reply tonymet 22 hours agorootparentprevI had assumed the current linux-on-chromeos was complicated due to it taking so many years of dev-mode hackiness to become a turnkey product reply danans 12 hours agorootparentThe ChromeOS team is pretty conservative with major feature releases and generally won't make something available by default until it goes through a ton of testing internally and with external testers. ChromeOS is quite different that other Linux based OSs in that it's supposed to \"just work\". reply tonymet 22 hours agorootparentprevthanks for sharing that's good to hear. reply odo1242 22 hours agoparentprevIt wouldn’t- it would just be running on the Android (still Linux) terminal instead of actual Linux reply yencabulator 4 hours agorootparentChromeOS Linux Developer Mode is Debian containers in LXC (with ability to run custom containers existing but being very painful). Anything else than a real VM running LXC-style containers means the feature would be going away. reply pjmlp 22 hours agorootparentprevAndroid exposes very little Linux to userspace, as per official APIs. reply surajrmal 13 hours agorootparentIt's more than you would hope. reply pjmlp 9 hours agorootparentTrue, but since they aren't official, OEM and Google are free to take them away any time they feel like across Android releases. reply tonymet 20 hours agorootparentprevis there an android terminal? I'm only familiar with debugging mobile devices over adb and it's restricted. I'm aware of android terminal apps, but they typically connect to a remote terminal via SSH. Does android boot into a terminal like a typical linux device? I thought it booted into fastboot reply yjftsjthsd-h 20 hours agorootparent> is there an android terminal? I'm only familiar with debugging mobile devices over adb and it's restricted. > I'm aware of android terminal apps, but they typically connect to a remote terminal via SSH. You certainly can run a local terminal (I think I've seen it baked in as part of developer options, but also as an extra app), though if you don't have root it is somewhat restricted in what you can actually do. > Does android boot into a terminal like a typical linux device? I thought it booted into fastboot I think you're slightly misunderstanding how boot works on both Android and Desktop Linux. The kernel is perfectly happy to run whatever you want on such displays as are available. Android tends to boot from firmware (which also implements fastboot) to a bootloader to the kernel and the kernel brings up the system with SurfaceFlinger (the Android display manager) owning the screen. On desktop linux there's more diversity; you can have the kernel initially hand your screen to the old in-kernel virtual terminal, the newer KMS implementation of a terminal ( https://wiki.archlinux.org/title/KMSCON ), or to Xorg or a Wayland compositor (in that case, optionally using plymouth or the like to replace even the earliest text messages). (Actually, full disclosure: I'm only about 70% sure I've got that sequence right. The point is that even desktop linux is perfectly capable of booting only in graphical mode; booting to a text terminal is available but optional.) reply tonymet 19 hours agorootparentsounds like it'll take some work . but these are some great ideas thanks for talking through it. reply yjftsjthsd-h 14 hours agorootparentI'm not following; what will take work / what ideas? I'm describing existing software that ships and works today. reply tonymet 14 hours agorootparentLook it's a bunch of theory right now. From what I can tell, your examples would still need integrating into AOSP-- not something that is running right now. I may have a different definition of \"shipped\" but this wouldn't meet my bar and seems to be more work. It's possible you are a wizard and I'm a novice so I can't wait to see your demo soon. reply yjftsjthsd-h 3 hours agorootparentNo, it's not a theory; I'm describing to you how Android has worked for at least the last decade. If you have an Android phone, this is how the device in your hand works today. reply tonymet 2 hours agorootparentok how do I run a linux VM on my phone? It's not as simple as a command or click as it is on chromeOS is now. All of the concepts as you've described will take some dev time to actually turn into a product. It took Google years to make Linux VMs part of the ChromeOS product and that was with a working hack solution already available for them. My point is that your theory will not be shippable on day one. reply yjftsjthsd-h 1 hour agorootparentIt appears that you and I are having different conversations. I said that 1. it is currently possible to run a terminal on Android, which lets you run commands as an unprivileged user on the Android system, and 2. both Android \"typical\" Linux are perfectly capable of booting into a graphical environment, though non-Android Linuxes frequently allow you to boot to a virtual terminal as well preferentially. Having gone through the whole thread, I guess by \"is there an android terminal?\" you actually meant to ask \"is there an android terminal that gives you a shell inside a transparent VM like ChromeOS does with Crostini?\" If that's what you mean, then it's true that that's less developed (though it does exist - https://www.esper.io/blog/android-dessert-bites-13-virtualiz... is one of the better practical guides I've seen for \"how do I run a linux VM on my phone?\"). Although if that's the main thing you're asking about, I don't understand why we're talking about boot processes at all; VMs in both Android and ChromeOS (and server and desktop Linux, while we're mentioning) are more or less just fancy user applications, they don't affect the boot process. reply faust201 12 hours agoparentprevbookmark your comment and comeback a year or two later. Nothing is implied. Wait. reply buildbot 23 hours agoprevNo details on why the Android stack is somehow more magical for AI? ... Or did they just fire all the ChromeOS devs and need to share resources with Android now? Why not Fuscia for something new? reply joecot 32 minutes agoparentA while back they were privately demoing ChromeOS alongside android on phones to enable a better desktop experience. They're also enabling display port on the new Pixel. My guess is they are making ChromeOS more like Android to make that process easier. reply summerlight 22 hours agoparentprevProbably CrOS team doesn't have enough manpower to implement necessary local ML infrastructure and want to borrow Android's. Or at least they're using this as an excuse for paying off long standing organizational tech debts. reply eco 22 hours agoparentprevProbably more of an internal mandate that everything needs to consider how it can integrate/improve on AI so they included some lip service in case a VP reads this. reply oliwarner 11 hours agoparentprevIt's amazing what a FAANG with tens of thousands of software developers cannot achieve. reply nashashmi 51 minutes agorootparentThat is the reason they cannot achieve it. Territorial claims and wars. Group and team efforts. leadership buy in. the more money those guys are paid, the less team works they produce. reply Cthulhu_ 4 hours agoparentprevFuchsia is still under development? I haven't heard anything from it in forever, and it's been going for a long time now. reply serial_dev 4 hours agorootparentIt looks like it is, see multiple commits today: https://fuchsia.googlesource.com/fuchsia/ As building an OS is no simple task, this all could mean nothing and the project might be heading to the graveyard slowly. I noticed the same, too, it was hyped as the next OS now about a decade ago, and it doesn't look like it will be a significant player any time soon. I assume it's still in the evaluation, prototype phase, probably tons of issues. I heard here and there that some exotic glorified home tablet thingy is running it, but I don't think you can really go to a local consumer electronics store and find something with Fuchsia running on it. reply vitno 1 hour agorootparentThere are multiple devices that have shipped with Fuchsia on it in the home ecosystem over the last 5 years. I don't necessarily think it's going to be a major success, but it's already been more successful than this implies. (Bias, was on the Fuchsia team many moons ago) reply serial_dev 18 minutes agorootparentIt might be your bias, or even mine, but in my opinion, what you and I are saying are pretty much the same... There is not much actual difference between \"I heard here and there that some exotic glorified home tablet thingy is running it\" and \"There are multiple devices that have shipped with Fuchsia on it in the home ecosystem over the last 5 years\". Would love to hear where it was actually used, searching for it, I really only found a handful of examples. I was an active Flutter community member and it was driving me crazy how people would say \"Flutter is great because Fuchsia also uses Flutter\", and all I thought \"show me one person who is using Fuchsia at this whole conference / meetup\". reply mike_d 1 hour agorootparentprevFuchsia is just a parking lot project with lofty goals to keep talented engineers from going to other companies. The only thing it will ever ship to is EOL Nest/Hub devices. reply ravetcofx 3 hours agorootparentprevThey updated their Google nest smart display to fuchsia a couple of years ago. I can't find the write up at the moment, but it was quite the engineering feat and the software engineers were quite nervous the day of OTA. reply ndesaulniers 14 hours agoparentprevMore so Android and ChromeOS both use Linux as the kernel. They actually have a lot in common in terms of tech stack. reply qalmakka 12 hours agorootparentThey use two completely different libc though, which are binary incompatible (bionic and glibc). reply ndesaulniers 3 hours agorootparentHow many web apps running on CrOS care what libc the system is using? reply aixpert 10 hours agoparentprevAs I wrote above, A big advantage of Android is the implementation of intents (also compared to Apples new crippled intents API), which may turn out to become the deciding factor in the race towards the winning AI Agent OS, as it may enable AI to use programs programmatically reply dudus 15 hours agoparentprev> To continue rolling out new Google AI features to users at a faster and even larger scale, we’ll be embracing portions of the Android stack Because they are panicking and need to add AI faster than Apple and Microsoft across their whole portfolio. reply joseda-hg 22 hours agoparentprevI thought Fuchsia was the one that got hit with the larger cuts? reply surajrmal 13 hours agorootparentWhile true it wasn't as large as folks seem to imply. reply refulgentis 17 hours agoparentprevWrote a comment a week or two ago on this you might be interested in: https://news.ycombinator.com/item?id=40580163 TL;DR: Google is consolidating behind the whims of the hardware org, which are orthogonal to OS' that can't ship (what ppl outside Google call Fuschsia), and here, OS' that can't sell premium 1st party hardware. The old platform org head left recently, combine that with the CEO's eagerness to display Efficiency™ and the lack of interest the CEO has in practical work, that means VP shuffle people around to keep constant/shrinking headcount and the CEO loves it. reply kernal 1 hour agorootparentFuchsia has already shipped. It's also called Fuchsia internally. reply mike_d 1 hour agorootparentBoth of my Nest devices that got the update were bricked into an unrecoverable state. Forums and Reddit are full of similar stories. \"Shipped\" is a very generous label. I don't know anyone outside the Fuchsia team that takes it seriously. reply refulgentis 44 minutes agorootparentThey're a troll with an extreme sense of grievance and unclear motivations. Perhaps this whole extra-S thing is actually it, but I hope not, for their sake. reply refulgentis 1 hour agorootparentprevLet's not be obtuse -- if you're familiar with internal info on this, you should/could understand exactly what I'm talking about. If you're just trying to do a drive-by dunk about spelling, and just thought it was cute to add on \"internal\" while doing so...more power to you, I guess. reply kernal 1 hour agorootparentConsidering how you mangled the name of the OS, I have a hard believing you're remotely familiar with the internals of Fuchsia. reply refulgentis 58 minutes agorootparentYes, there's an extra S in Fuchsia so I'm 3 trolls in a trench coat pretending to be me. I hope your day gets better! However you're feeling, it's temporary, it's not worth trolling and doing a know-nothing thing in public, it's too demeaning to yourself. People can see my comment, profile, and site, and then see you're completely anonymous. Then they're left with either: A) you're so hung up on an extra S you're lying on purpose B) you're a vindictive liar: you don't care enough to find out who I am, even though it's right in my profile, but you do care enough to pretend it's unknowable or C) youre having some sort of psychotic break from reality reply londons_explore 14 hours agoprevSo, the ChromeOS kernel is being thrown out, probably with it's A/B update mechanism, filesystem layout, secure boot, etc. The question is will they also throw out the ChromeOS userspace components and replace them with the android userspace? If so, this is effectively an announcement that ChromeOS is discontinued but those devices will now be migrated to run android. reply hawski 12 hours agoparentI don't know which Android added it, but it also has AB partition scheme (they call it seamless updates) for several years now. reply zerreh50 8 hours agorootparentIn fact, Android's update engine includes a lot of chrome os related code, so they are either very similar or outright shared reply cdesai 3 hours agorootparentIt's a fork of the chromium update engine. https://android.googlesource.com/platform/system/update_engi... Many Android specific changes though. reply moondev 14 hours agoparentprevI really wish they would announce the opposite, that crostini would be coming to Android and phones would launch ChromeOS \"desktop mode\" when attached to a monitor. ChromeOS feature flags have been seriously fun and innovative the past few years. reply kernal 1 hour agorootparentThe Android Virtualization Framework (AVF) [0] includes crosvm support, so you can run virtually any OS you want. An Android engineer has also done a POC running ChromeOS on Android [1]. [0]https://source.android.com/docs/core/virtualization/architec... [1]https://www.androidauthority.com/chrome-os-on-android-hands-... reply theanonymousone 13 hours agoparentprevGiven the big difference between mobile and desktop Chrome, and the fact that this is \"Chrome\"OS, it can be an interesting story. I don't many people (anyone?) want to run mobile Chrome on a laptop screen. reply londons_explore 9 hours agorootparentI suspect it wouldn't be a lot of effort to make android chrome have the full desktop UI. The Chromium codebase is all one piece, and rendering of the UI is already unified between mac/windows/linux, so making the android build render a desktop-like UI is probably a simple job. In fact, I wouldn't be surprised if the right set of build flags can do it already. reply erredois 8 hours agorootparentIf I plug my s24 to my dock, I can use it with my big screen, most apps don't have a problem with this mode. reply theanonymousone 4 hours agorootparentprevIt's not just about rendering sizes. There are no extensions for Mobile Chrome, as an example. reply tapoxi 23 hours agoprevI wonder what this means for Lacros (Chrome as a Wayland application compositing to Ash-Chrome). Would this just be using Surfaceflinger and the Android version of Chrome? Does this deprioritize regular Chrome for Linux? reply k8svet 22 hours agoparentI think these are quite interesting questions. Chromium just spent a lot of effort to get stock Chromium working as a ChromeOS client, rather than it purely being available as part of ChromeOS itself. Is that ... pointless now in the face of a rebase on Android? Seems like it will end up being a win for a narrow window of devices? reply pxeger1 10 hours agoparentprevI reckon Google will want to keep supporting Chrome on Linux, because winning developers over is important for them to win control of the web. reply rcleveng 14 hours agoprevI'd love to stop carrying my chromebook to work along with my andoid phone, and just plug the phone into a docking station to use ssh/chrome browser/remote desktop which is really all I used daily. reply JeremyNT 4 hours agoparentThis is possible with pixel 8+ running grapheneos. The ux is kind of junky, but if you're mostly using the browser or some remote desktop / ssh it's feasible at least. reply 1oooqooq 1 hour agorootparentbut then you have to carry another phone for your bank app... reply frankacter 8 hours agoparentprevI do this today with my Asus Zenfone. Works great. reply sebmaynard 12 hours agoparentprevSounds like you could try a Samsung S phone with Dex and a lapdock? reply rcleveng 5 hours agorootparentPossibly, I don't want the lapdock, really just plug usb-c into a docking station and have enough usable tools to do work. I think this one is more about execution than concept reply scirob 10 hours agorootparentprevyea but look at any long term review of these systems. There are always subtle UX issues that kill your productivity reply creesch 7 hours agorootparentIt is possible to run a Linux distro through Termux afaik. For example: https://github.com/phoenixbyrd/Termux_XFCE There are also tools available like Andronix. I personally haven't tried them, just figured I'd leave the information here for people interested in it who want to explore the options. reply prettyStandard 6 hours agorootparentThe problem I generally run into those with is they have trouble getting root or simulating root. Particularly on Samsung. That limit's running things in containers on the phone as well. But last time I looked at it it did look like there were some promising projects. I forget what. reply fsflover 10 hours agoparentprevYou can do it with a GNU/Linux phone (Librem 5 or Pinephone). They run desktop GNU/Linux. reply pquki4 9 hours agorootparent...if you are willing to sacrifice the \"phone\" part of it. reply guappa 8 hours agorootparentCalls and SMS work fine on pinephone. Battery life not so much. reply fsflover 8 hours agorootparentprevIt works fine for me. reply _joel 6 hours agorootparentFine if you don't need to use it for online banking on your phone, that's my main blocker. reply fsflover 4 hours agorootparentI choose banks which do not require mobile banking, including virtual credit cards. You can also try Waydroid to run Android apps. I heard it worked for some banks. reply summerlight 22 hours agoprevhttps://chromeos.dev/en/posts/androids-bluetooth-stack-fluor... Looks like we can try to infer the real intention of the convergence from this project. It might make business sense to maintain 3~4 different OS, each targeting different user segments. But there is no reason to have one more different tech stack per each OS across everything. reply nolist_policy 23 hours agoprevWow, this is a big announcement for sure. Not much details but I'll definitely following chromium's code review system[1] closer now (Large parts of ChromeOS are developed in the chromium monorepo). [1] https://chromium-review.googlesource.com/q/status:open+-is:w... reply trustno2 10 hours agoprevThe random mention of AI is a bit comical, but it's 2024 so we need that. Simplifying engineering effort and merging the stacks, that I can understand. reply tempnow987 16 hours agoprevI'm trying to remember why Android forked from Linux in the first place. Or was it always it's own thing? My memory was maintainers on the linux side had an issue with wakelocks maybe? At one point google going to some pretty serious lengths to try to integrate / upstream some stuff and it getting totally shot down. My own view was there was a bit of a missed opportunity there reply ndesaulniers 14 hours agoparentWakelocks and allocation of physically contiguous memory allocations (ashmem, dumped now in favor of dmabuff). All upstreamed now. At this point, Androids kernel is only a fork of upstream Linux kernel because Google employs so many kernel devs that some features are developed downstream first, then upstreamed once kinks are worked out after shipping in devices on tighter deadlines than upstream. reply izacus 11 hours agoparentprevThe way I understood it from the Android history books, it's mostly that the Android team (after some initial attempts) didn't want to spend time with Linux kernel drama because they had to ship the OS in a rush. So it was just easier to fork the kernel and ship it in a state that doesn't require upstream approval. Linux at that time was very much unequipped at handling low power devices well (e.g. Wakelocks have already been mentioned). reply randomifcpfan 15 hours agoparentprevThat’s certainly how it seemed from the Android side at the time. The Linux side was hoping that they could adapt the desktop Linux stack to work well on mobile devices, without introducing new concepts like wakelocks. It took them a long time to give up on that approach. reply runjake 16 hours agoparentprevIt was always the Linux kernel, plus some misc OSS, plus their own software on top of the Linux kernel. reply sandGorgon 11 hours agoprevthey should have done this in the first place. there was absolutely no reason to have parallel initiatives in android-on-the-desktop, fuschia, android and chromeos. Google's operating system is android. it has won. extend that architecture to the chromeos. reply mike_d 1 hour agoparentChromeOS literally defined a whole market segment of low cost tablet/laptop devices that could be distributed in mass to schools/employees/etc. The security model was the best of any operating system option. Android has its place on mobile, but we see how well it has gone for Microsoft trying to take a single OS and make it work across mobile/tablet/laptop. reply rvnx 11 hours agoparentprevPossible causes: The court case with Oracle regarding programming APIs and the underlying uncertainty probably didn’t help. And in some way to launch 5 times the same product allowed Googlers to grow internally as product launches are highly rewarded, while preserving the company’s interests by not putting all eggs in one basket. reply fidotron 8 hours agorootparentActual cause was the chrome vs android turf war. There was a time when the chrome team were walking around telling everyone native android development was a dead end and chrome would take over everything. Then Google had some reorgs, certain people ascended to control both android and chrome, and suddenly android was a good thing. reply cmrdporcupine 4 hours agorootparentIt's funny to hear that from the other side. When there I saw how the Android team would throw around its own biases. I seem to recall that \"Chromecast with Google TV\" ended up having to be per-unit subsidized out of the Android PA's budget because they needed a heftier and more $$ SoC to be able to run Android on it than the dirt cheap ones we shipped Chromecast on. There's technical merits to both platforms. The fact that Google had/has multiple Linux distributions all competing is ridiculous. I personally worked on HW products that used... (counts fingers) 4 of them? And then toss Fuchsia in there, too... reply frithsun 22 hours agoprevLinux community really dropped the ball by not creating a distro that was basically chromeOS, but without the propriety limitations. Thousands of distros and not the one that would take off. Sad! And now, with this announcement, the moment has passed. reply yencabulator 4 hours agoparentUnfortunately as is typical with these source-thrown-over-the-fence projects, a lot of it is just a hot mess. I tried to make a custom container that would run under the regular ChromeOS crosvm, all the \"agent\" stuff for e.g. transporting wayland over virtio is brittle crap that crashes with no useful error if you don't invoke it perfectly correctly. What I want is the idea of wayland-over-virtio to be reimplemented by others. reply oneshtein 9 hours agoparentprev> Linux community You mean \"ChromeOS community\", right? reply janice1999 21 hours agoparentprev> but without the propriety limitations. ... that would just be Linux then? GalliumOS was Ubuntu for Chromebooks (with ChromeOS patches for touchpads etc). Mainline Linux is good enough nowadays for most things. Several companies offer custom ChromeOS images with their own tools and one was even purchased by Google, but they focused on centralised management for classrooms etc. reply pquki4 9 hours agorootparentI think the point of the original comment is \"take off\". A non proprietary ChromeOS (that is browser first, simple to use for non techy people, easy to manage) sounds like a neat idea to me, but the names you mentioned achieved nowhere near the reputation/popularity of Ubuntu. Maybe it is more about the business model. ChromeOS runs efficiently on low-cost hardware for a small license fee (?), and get almost guaranteed updates. In exchange, customers sell their souls (actually, data) to Google. Won't happrn with Linux. But undeniably it sells and works well enough for ChromeOS. reply hawski 17 hours agorootparentprevChromeOS-like implies an immutable image with the AB partition scheme for me. Other than Fedora Silverblue there is nothing like it as far as I know. reply yjftsjthsd-h 14 hours agorootparentThere's MicroOS from the OpenSUSE corner, including its respins with GNOME or KDE. IIRC that uses BTRFS snapshots/filesystems instead of A/B root filesystems, but same broad idea. reply leetnewb 10 hours agorootparentTechnically, the Gnome spin is now named Aeon: https://aeondesktop.github.io/ reply yjftsjthsd-h 3 hours agorootparentAnd the KDE version is Kalpa, but they're both still spins of MicroOS. reply luyu_wu 16 hours agorootparentprevVanillaOS would be another great example (arguably cooler than silverblue imo). reply 151 more comments... GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "On June 12, 2024, Chromium Blog announced that ChromeOS will integrate large portions of the Android stack to expedite the delivery of Google AI features and innovations.",
      "This integration will include components like the Android Linux kernel and Android frameworks, aiming to accelerate AI innovation, simplify engineering, and enhance device compatibility.",
      "Despite these changes, ChromeOS will maintain its security, consistency, and management capabilities, with regular updates and new innovations continuing in the interim."
    ],
    "commentSummary": [
      "ChromeOS will soon be developed using large parts of the Android stack, leveraging Android's architecture to reduce OS fragmentation and improve update consistency.",
      "Chromebooks have longer support lifecycles compared to Android devices, and ChromeOS handles fragmentation better, despite not being suitable for smartphones.",
      "The integration of ChromeOS and Android could lead to a more unified platform, potentially enhancing the desktop experience on mobile devices and addressing current performance issues."
    ],
    "points": 302,
    "commentCount": 404,
    "retryCount": 0,
    "time": 1718218966
  },
  {
    "id": 40663704,
    "title": "Gerald Sussman: Programming is (should be) fun (2022) [video]",
    "originLink": "https://www.youtube.com/watch?v=2MYzvQ1v8Ww",
    "originBody": "tell me when to start hi from Arlington Massachusetts I&#39;m unfortunately unable to be there today so I&#39;ve recorded this talk to you I would much rather be there in person so I could dynamically respond to you but I will try to be online after my talk is represented to answer questions that you may have anyway let&#39;s get into this so I&#39;ll have to share with screen okay and I&#39;m sharing the screen and I suspect that you can&#39;t see it right now but I will wait until it becomes available anyway I&#39;ve been programming computers for a very long time I&#39;ve been programming them for since 1962 I suppose when I was a uh I suppose I was in high school in uh in Brooklyn New York and I of course I was a member of the Columbia science honors program which is a high school student program in which I got to use some computers at Columbia University when I was starting dealing with computers this is what computers look like okay this is not the one at Columbia University it&#39;s the NASA one but it&#39;s a similar a similar IBM 790 system or 70-94 system was at Columbia and this is a these machines were ridiculous okay this machine costs multiple millions of dollars something like 15 million dollars however it could address only 32 268 words that&#39;s two to the 15th words Each of which had 36 bits the machine ran at about two microseconds per cycle uh which is I suppose a 500 kilohertz machine and each instruction word had an address part and decrement part from which we get the words car and cutter for lisp car being the contents of the objects part of the register and cutter means the contents of the decrement part of the register MIT had two of these machines by the way and uh besides Columbia had one but when I got to MIT uh I was not using one of those at Columbia I didn&#39;t use this either I use a an IBM 650 and 1620 which were little machines quote little meaning that they were much smaller than this but not not they were still humongous objects anyway uh getting go get on with this when I got to MIT as a freshman I thought this is not a freshman but when I got I started looking like this I looked like this and computers looked like what you saw what you&#39;re seeing here is a deck pvp6 computer the one that ran Mac hack 6 which was a uh a chess program that actually played in in tournaments and here&#39;s a little a little uh thing saying it it played in a tournament uh it&#39;s uh the machine had a telephone on it uh the telephone was uh an MIT extension telephone and the the phone number on that telephone was 6765 which is Fibonacci of 20. so when Bill gospel answered the phone he would always say Fibonacci of 20. anyway um the uh let&#39;s see if I get my stuff here okay so some of my pages or something this machine costs somewhat less than a million dollars the machine could address two to the 18th 36-bit words that&#39;s a little bit more than a megabyte but that&#39;s a lot more than the than the uh 1790 could I could could have and we needed big memories like this to do things like Maxima the first significant symbolic manipulator uh to get this big memory uh we contracted to a company called fabritech that made it it took them a year to make it okay and it costs according to fabric something like 240 000 uh with three hundred eighty thousand dollars as far as I can tell from mit&#39;s records and that was in 1967. the memory was called the Moby after the whale Moby Dick since it was considered an enormous amount of memory as available at the time this was one of the largest computers in the world from the point of view of memory well since then I&#39;ve learned a lot about Computing I&#39;ve designed and built like in computer electrical circuits I&#39;ve worked in artificial intelligence I&#39;ve been involved in the design of some novel computers I&#39;ve participated in the invention of computer languages I&#39;ve done numerical simulations of celestial mechanics type situations and I&#39;ve written large symbolic systems but I&#39;ve never given up programming I love programming because programming is fun so that&#39;s the actual content I want to get into today let&#39;s start by saying what is programming okay and programming is certainly not coding it&#39;s not the mechanical transformation of well-defined specifications intoxicable code that&#39;s job we we make compilers for okay uh one of the most important problems here is there are usually no good specifications to meet we can&#39;t just transform a specification because we don&#39;t have any we have vague understandings of what we want the behavioral system to be and we know what constraints and fees on what&#39;s feasible to implement you know good specifications almost never come top down from the boss okay we figure out what we can do and then we write specifications for what we&#39;re able to accomplish and that&#39;s what I&#39;m really trying to that&#39;s what real real programming is more like as you can see over here um so more like that okay programming is it&#39;s more like abstract engineering design it&#39;s a joint exploration of what the achievable specifications are and the possible implementations that are they&#39;re directed at some partially defined not fully defined goal we don&#39;t really know what we&#39;re trying to do we&#39;re trying trying to get something that does something in some region that is interesting to us and that has some sort of behaviors we&#39;d like to get unlike the design of other physical systems however it&#39;s not it&#39;s there are no physical limitations it&#39;s not that we have to worry about for example uh do I have a tolerance of of one millimeter in the where I&#39;m going to place this part or a thousandth of a millimeter we don&#39;t have to worry about the curve of a cutter we don&#39;t have to worry about tolerances or parameters like that we have only one percent resistors okay and what we are limited by in the programming is our ability to think how do we organize ideas how do we control the complexity of these huge machines that we&#39;re building that are virtual machines inside of our computers after all the number of parts of a com of a of a of a some interesting system is way larger than the number of physical parts you could actually put together in any real machine so are we I get the choice two things when we&#39;re programming we make representations algorithms and levels of abstractions okay now what does that really mean what is it like what does it feel like to be a programmer okay that&#39;s a really interesting question and I what I decided to do is is is ask an expert in programming people okay poets are people who program other people to have emotional reactions of the reactions that they want the person to have so a famous poet who is very very good was Edgar Allan Poe and he actually explained how he wrote the the uh the poem The Raven which is basically intended to make you feel sad about about uh sort of a lost love a person who apparently died or something okay and right you really he really he doesn&#39;t say it at any point in the problem in the poem but he&#39;s actually what he&#39;s actually lost but there&#39;s a it&#39;s a woman Lenore I believe anyway I&#39;m going to read to you a an explanation he made of how he wrote The Raven I select the Raven as most generally known it is my design to render it manifest that at no one point in its composition is referable to either accent or intuition that the work proceeded step by step to its completion with the Precision and rigid consequence of a mathematical problem most writers would positively shudder at letting the public take a Peep Behind the Scenes at the elaborate and vacillating crities of thought at the true purposes seized only at the last moment at the innumerable glimpses of idea that arrived not the majority of full view at the fully matured fancies discarded in despair as unmanageable at the cautious selections and rejections at the painful erasures and interpolations at the wheels and pinions the tackle for proceed Shifting the stepladders and Demon traps the Cox feathers the red paint and black patches which constitute the properties of literary history by the way the word history though is an old word for actor so it&#39;s a literary actor anyway isn&#39;t that really what programming feels like okay if is it more like what what you see for what you&#39;ve just heard from from Edgar Allan Poe it&#39;s not at all like transforming some specification into a into into a mechanical means into code okay in any case it really feels to me more like a creative art there are creative arts like architecture and music and so on and for me for example I&#39;m I&#39;m somewhat familiar with mathematics and theoretical physics and they&#39;re creative arts too you&#39;re trying to invent theorems that are pretty in mathematics you&#39;re trying to invent explanations of the physical world in theoretical physics okay so that&#39;s a a different kind of thing another thing that seems at the time was another okay I feel let&#39;s get into it and see what this is like okay when I was a freshman at MIT in 1964 and 65 I was shown lisp the programming language but there was this book called list 1.5 programmers manual and there was also another book by information International called the programming language lisp I suppose that was by Danny the Barbaro and Ed Berkeley and uh you may have heard of Ed Berkeley he was the guy who made the genie ax but then again maybe you&#39;re not old enough to even think about that any one evening over a Chinese restaurant dinner with other Minsky&#39;s hackers someone I think it was gospel explained the eval apply interpreter to me this was a revelation that changed the course of my life okay so what you&#39;re looking at here is the core of an evaluator for a language like scheme it is made I have two parts eval and apply and evaluate and it&#39;s written in completely concrete form these cars and Cutters and things like that I&#39;m manipulating the data structures directly it&#39;s not abstract syntax like I would have if I wanted to write a modern professional program if you look in sicp you&#39;ll find a beautiful version of this which is in in abstract syntax and if you look at one that wants to have extensible abstract syntax you can look at the new one with Chris Hansen I wrote with Chris Hansen but in any case this is a very interesting program and I spent all night thinking about it after having it explained to me um and the it wasn&#39;t exactly this program it was a it was a more primitive version of of lisp but that doesn&#39;t really matter okay the critical thing is after having thought about it for a very long time and I realized that this is the matter this is the essence of the matter there&#39;s a process called eval okay and there&#39;s a process called apply which are represented by those procedures and what a valve does is takes an expression an environment which contains the information required to to give meanings to the free names the free variables in the expression an expression like the sum of a and three has got two free free names in it a and the sum plus okay and something has to give meanings to those that&#39;s a the environment is a mapping from the names to the meanings okay and so it takes that and it transforms it into a a procedure and arguments because by evaluating the operator part of the expression you get the procedure and by evaluating the operands part you get the arguments apply then takes the procedure and arguments binds the arguments to the formal parameters of the procedure to make a to make an environment government for which it can in which you can evaluate the body of the procedure and that goes back to a vowel okay so this is the this is the whole idea and to me it was very important because it&#39;s in all computer languages had to work this way in fact the computer itself has to work this way too and it does at the at the uh implementation of the electronics level too there&#39;s something which is basically doing exactly each of these ideas okay so this is a first revelation I had here was that and of course Revelations are often shown in in mystical symbols even though there&#39;s no real mysticism uh but the the feeling here is wow that was very important okay well at the same time when I was an undergraduate in fact the second term of my freshman year I was taking freshman physics in that class we were learning relative sorry electricity and magnetism from the perspective of special relativity a high point in that class was the appearance of Maxwell&#39;s equations here they are mass occasions are among the most beautiful uh expressions of physics okay it talks about Fields e with the electric field and B is the magnetic field and the electric field has sources which are charges so that what you see here is that the the charges are the sources of electric field lines and the magnetic field has no particular sources because we don&#39;t know about magnetic bondables we&#39;re not sure we have any but the magnetic field arises by by looping around currents J is the electric current and electric current uh produces a curve a magnetic field around it that&#39;s what this says and there&#39;s another piece here that says changing a magnetic field produces a a a rotating electric field as well like a electric field has a net curl now at the beginning when Maxwell first thought about these things he didn&#39;t have this he didn&#39;t have this expression here went over c d e by DT that&#39;s called the displacement current and that was that was not there because these were all laws that were determined before Maxwell by people like ampere and Faraday okay and Gauss but what happened what happened was that also uh Benjamin Franklin realized that charge is conserved it&#39;s a fluid it&#39;s a material that the current the current is valid you know the Divergence of the current is the rate of change of the density of charge okay so that that that it turns out however that the that the maximum equations with without this term are inconsistent with conservation of charge and Maxwell realized that so he&#39;s stuck in this term okay that seems like it seemed like a little idea but the next day or maybe the same day he did a little bit of math and I came up with what&#39;s now easy as the wave equation the electromagnetic wave equation this part is basically saying that electric electric field propagates as a wave with speed C okay this is a wave equation right here and uh the C that was determined came from information about how electric electricity and magnetism work which actually were you know not written to see here but rather they were written as as combinations of the magnetic uh permeability and the electro dielectric constants of materials okay so from the basis of of knowing that for free space those numbers he computed the value of c and by golly it turned out to be this approximately the same number as was deduced from observing the emotions of the of the Galleon says satellites of Jupiter boy so at one step he discovered that electricity and magnetism were what light is based on light is basically electromagnetic wave I wish I something like that would happen to me that I would have a discovery like that that would make me really feel good okay but unfortunately it doesn&#39;t happen to everybody this happens once every few hundred years that anybody has such ideas but the next thing is that what hit me after seeing this is I learned this from from uh from gospel and studying eval apply and I learned this from Maxwell equations in my class on E M and they&#39;re analogous okay that the eval process does produces something for Applied work on apply process is something the eval works on and the changing magnetic field produces an electric field change the electric field produce a magnetic field by golly we get electromagnetic radiation here we get evaluation of of Expressions okay but that&#39;s even more it&#39;s more General than that we always have dual variables and things like physics so for example we have things like position and momentum okay or energy and time in the case of positioning momentum it&#39;s pretty obvious okay we have a system that depends upon knowing where it is and how it how it&#39;s moving okay if it&#39;s if it&#39;s in a particular place and it&#39;s moving with particular momentum then the momentum tells that tells us how to get the next position the position tells us what the forces are because the forces depend upon where things are and that tells us how the momentum has to change okay and that&#39;s General about everything okay so we have dual variables and we have Duality appearing in computation okay now of course that&#39;s fun the fun is that there&#39;s philosophical Insight it&#39;s sort of you&#39;re thinking about things that are are far more General that are that are almost Cosmic in the way you think about them okay and this turns out to be very common in everyday programming everyday programmers always run into deep philosophical issues so for example a good one is what is the what is the meaning of the word Chicago what&#39;s going on here we use the word Chicago in programs we might write it as in part of a database that&#39;s telling you know what what uh uh what sites there are in Chicago to go see okay but the problem is that the city is not in the computer all that&#39;s in the computer is is the this seven character name presumably and maybe lots of property lists on things like that okay the realisticity is changing all the time okay it&#39;s so it&#39;s not clear what that symbol actually refers to and this is the kind of thing that philosophers worry about I certainly When I Was An undergraduate or graduate student had long horrible fights with a philosopher by the name of um Jerry foder about what exactly the word what is the word Chicago actually refer to not clear and you know when you get into things like what is it mean by identical identical is the same kind of thing okay because like here&#39;s a beautiful example when I teach classes and this sort of stuff I take two pieces of chalk and hold them in front of me to show the students they&#39;re the look identical pieces of chalk they&#39;re brand new pieces of chalk they say how do you know that this is really two pieces of chalk or it&#39;s one piece of chalk by by an Illusion by some sort of using mirrors or something as a magician might I&#39;m showing you two views of the same piece of chalk how do you how can you tell the answer is that I break one of them and the other one doesn&#39;t break okay the critical idea there is data structures are identical if only modifying one modifies the other okay mutation is about change in time and mutation is essential to the question of identity the test of identity is that if you change the thing if you modify it the other the other copy that you think of modify as well the the Alias okay uh pure functional programs have no such mutation so that&#39;s pretty weird okay because I can mostly transform a program into a pure functional form by inventing a store that gets pushed around you know over the entire behind the scenes as you might do with with um monads in in Haskell okay even so you can do that but you can do it all the time and that means that if you were sort of the deity who built the system you can think of you could see the thing from a transcendental point of view as not necessarily having mutation it&#39;s just this it&#39;s just this uh arrangement of of of uh of pipes around okay on the other hand if you&#39;re part of the system if you&#39;re imminent to the system rather than Transcendent around it then you can then for example if you&#39;re you&#39;re a particular process and you talk to in some other process that other process changes as you as you talk to it of course that&#39;s because it has internal state that it appears to be changing and so they a programmer has to see it from both points of view this is a very deep philosophical kind of situation the problem occurs in in quotation we use quotations all the time you know we have pieces of code that are uh that are quoted when we for example uh do do interpretation or compiling or or manipulation of alge expressions but the problem is is one with a substitution you&#39;re all taught in elementary school that you can always substitute equals for equals in an expression but that&#39;s not true if you have a quotation for example if I I tell you that John knows that the Morning Star is Venus and we all know that the evening star is the Morning Star that we can&#39;t deduce that John knows that the evening star is Venus okay so you can substitute into a quotation equals for equals I mean even machine learning we have problems that are philosophical okay what does it mean to be evidence okay there there&#39;s a claim all Ravens are black well every time we see a new Black Raven okay that&#39;s evidence of the Ravens of black okay but unfortunately that statement uh if something is Raven is black is it is logically equivalent to if something is not black and is not a raven but seeing my white uh teacup okay which is certainly not a raven does not give me any feeling the better that there&#39;s better evidence that all Ravens are black so there&#39;s a there&#39;s a problem which has got a name hemples Raven&#39;s Paradox and if you look in Wikipedia on it you&#39;ll find all sorts of arguments about things like Bayesian uh Bayesian uh explanations of this but you know this is the kind of thing the philosophers worry about and it&#39;s not clear that those those answers are right and they&#39;ll be worrying about this for another hundred years but we have to get address these things every day of Our Lives as programmers okay so I think there&#39;s something very different about being a programmer okay now but we have bugs bugs are are important okay bugs are occurring every program there&#39;s no way to avoid them and I&#39;m not you know I don&#39;t believe what what Mr director tried to teach us which is that you should always completely uh prove your program correct before you try to write it okay you know you should make sure that you have every step exactly right and you should be ashamed if you have made a bug no I thought well I see it like that at all in fact bugs are opportunities to learn it&#39;s fun to have bugs and they&#39;re going to be for a reason we have bugs debugging with good tools is also a nice Adventure it&#39;s fun to do it bugs by the way should have names like fence post error and read a ride they should become friends of ours we should have we should have a whole whole community of bugs that we all know of and every one of us knows those bugs so that we know the name bugs have specific specific mitigation strategies we can apply I claim that bugs are consequences of a powerful way to design okay a powerful strategy the reason is because we&#39;re we&#39;re finite human beings we can&#39;t think of everything all at once so we have to plan and planning requires simplifying to achieve an approximate result the result of simplification is in fact that we haven&#39;t thought out everything so there are bugs okay but what we do is we then debug the system that&#39;s sort of in the right direction that&#39;s approximately right to make one that&#39;s more like what we want it to be okay and this is a this is a process which I&#39;ve given a name to it&#39;s called problem solving by debugging almost right plans this darp okay and I think it&#39;s actually very serious let me show you about it in a more concrete example actually a more real physical hard example okay let&#39;s give it an example from electrical engineering now I&#39;m not going to be teaching you electrical engineering so don&#39;t worry about whether you understand every detail of what I&#39;m talking about right now it but it turns out there&#39;s a simpler way to show an example of the strategy that I could do in say programming supposing we need an electrical filter with a given frequency response this is a this is the magnitude of response versus the frequency and suppose you want one that cuts off that has no response to DC okay and has a it turns around so it has some start having responses around 30 Hertz it&#39;s sort of flat until it&#39;s about 300 3000 Hertz three kilohertz and then it dies off this could be one for example for a filter for pulling out speech sounds in a in a noisy environment because human speech is basically between 30 Hertz and and three kilohertz you know the telephone system never passed more than about three kilohertz so what we&#39;re trying to build is a mechanism which has inputs and outputs and has a response which is this shape okay that&#39;s where the H is called the system function and it&#39;s in terms of something called complex frequency which we&#39;ll talk about anyway this says this is has very no obvious discrete features on it but I can pick them out make some discrete features that to solve the problem there&#39;s a there&#39;s a it goes through zero it&#39;s zero it turns around to 30 Hertz it turns around at three kilohertz okay those correspond to in the complex frequency domain okay two what&#39;s called poles and a zero okay the zero at zero and the list pole is at minus 30 Hertz and this pole is at minus three kilohertz okay and what that means is that I can synthesize the the behavior the system function of this box I&#39;m trying to make by making something that has in complex frequency world this is Fourier transforms it has the it has a ratio of two polynomials one of which has got is got a factor of s in the numerator and in the denominator there are two two factors that go to Infinity if I touch these poles okay why that works I&#39;m not trying to explain that that&#39;s I&#39;m not teaching you electrical engineering however what I have also with my my uh my mental library is plans for putting together pieces of Machinery so that I can make a more complicated system function from simpler ones so for example if I want to make a sum well I can make the inputs in parallel and the outputs in series and that gives me a a a sum uh plan I could also for example have a factorization plan if I ever want to make a product of two system functions that are simpler I can make that by cascading two pieces of electrical circuitry okay that&#39;s a these are approximate plans they&#39;re going to have bugs but in any case I can also use the fact that I have factors that I can make up so I&#39;m going to use this this factorization plan this one because I can I see I can make something with a pole okay well how why I can make something with one pole I thought your business right now and I don&#39;t want to teach that but I can make one by by putting together a capacitor and resistor like this I can make something with a pole in the Zero by putting eight together a capacitor resistor like this so I can make the parts of this diagram over here okay as a factorization because what I&#39;m doing here is here&#39;s part of it KS over s minus a and the other one is one over s minus B okay so there&#39;s my factors so I put it together and I&#39;ll just try it out it doesn&#39;t work okay it&#39;s fit it&#39;s a failure why is it a failure well there&#39;s a reason the reason is because these are these computations were done assuming that this is a voltage divider it means no current comes out of here and this thing can take an arbitrary cards in from a voltage source and that&#39;s true of this one as well so when I put them together okay this guy is sucking current out but this guy&#39;s assuming no current is coming out contradiction okay and that&#39;s a name a bug like that&#39;s got a name it&#39;s called a loading bug this circuit this sub-circuit is loading this circuit okay loading bugs can sometimes be solved by for example putting in an amplifier an amplifier an active amplifier can provide you know can measure the voltage from here to here without disturbing it and put out a voltage which is which you can pump is more arbitrary current into here if that&#39;s what I need to do because there&#39;s a power supply inside the amplifier so this works okay as an example of a patch okay but the big idea is not that at all it&#39;s the problem solving by debugging almost right plans the way you see here is a simplified example to bring out the essence of the strategy we have a planning Strat library with lots of plans of which I showed you too each plan is a gives a way of composing a stab at the solution okay and of course the full problems are recursive so you know sub problems problems have sub problems and so on so you have to have plans for every level there are named bugs associated with each plan for example a factorization plan has in my head a loading bug the summation plan has something called the ground short by which I didn&#39;t explain okay but it&#39;s another one iteration plans may have fun suppose bugs concurrency plans may have reader writer bugs as you all know about and their name patches for each bug in the plan loading bugs can be sometimes passed with an amplifier but an amplifier patch may not do their job in all cases I might need a passive filter it&#39;s a different thing so each blood patch by itself have bugs okay so that&#39;s a that&#39;s sort of explaining what the the um why why problem solving by debugging almost by plans is sort of a a rather deep issue okay now what I&#39;d like to do is talk about some bugs that are not much fun okay here&#39;s some that are not much fun we have we call them ugly Clues okay and all programmers know about ugly clujis and we always these they&#39;re they&#39;re tedious they&#39;re unpleasant okay they&#39;re that&#39;s because somebody didn&#39;t think carefully the one example of it which I I always like to complain about is language C okay and C plus plus it&#39;s it&#39;s uh descendant I suppose they&#39;re awful okay they&#39;re full of traps for the honorary programmer think about the fact that most of all you know you if you read the the uh the reasons why use you have to get a new kernel quite often is because there&#39;s lots of bugs of memory management or something like that mostly because the thing was written in C and then people have to manipulate their memories with that in ways that are that are just not clear for humans to do okay but they&#39;re more Elementary problems like this consider python a nice language that we use for teaching things and which all quite often everybody uses notice that that the Constructor for a dictionary produces a dictionary which is denoted by uh a pair of of um braces okay the type of a pair of braces it looks like it&#39;s consistently a dictionary but if I put something in between the braces whoops now I have something called the set that&#39;s weird can I make sense well it&#39;s usually let&#39;s use the the set Constructor well if I do that I would hope that I would get something like this but what do I get I get this whoops okay and what happens if I put set if I take the set set of that oh now I&#39;ve got the identity function set that&#39;s pretty weird okay it just isn&#39;t consistent it doesn&#39;t make sense but it gets worse okay here&#39;s a even worse I hear I have a list a list of two elements okay this is denoted but in in Python by uh square brackets okay it&#39;s limited by square brackets the first element of the list is Foo that makes sense the rest of the list is bar this is a list containing bar that makes sense as well the fir the first element of the list containing one element is Foo that&#39;s fine the rest of it is the empty list okay that&#39;s all fine I like it now I suppose it&#39;s tuples the difference between tuples and lists is is notationally they use round round brackets parentheses rather than square brackets okay but here the zeroth element is Foo as I would expect okay but if I&#39;ve got a Singleton okay uh I get F huh and the rest of it is the oo what happened well of course I know the answer is that these that tuples are defined in terms of the comma the comma is what makes something a tuple and I would have to put a comma here to get what I thought I was going to get but this is just inconsistent it&#39;s nuts okay and I can&#39;t imagine that anybody inventing something like this without without having smoked something really bad Okay so anyway let&#39;s talk about something more fun okay in 1992 I spent a year at Caltech learning about general relativity by hanging out in Kip Thorne&#39;s group that&#39;s the uh people who also did the theory the theoretical part of the of ligo the uh laser defrometer that was used for uh for discovering gravitational waves while I was there I was also thinking about programming that I learned I always use programs as a way of remembering stuff it&#39;s my memory my memory is the programs I write and what I do is I learn something math or physics or some other maybe biology and I write a program that represent that so I can read the program later it&#39;s not ambiguous easy to read and that way I I sort of store my knowledge it&#39;s like that&#39;s why standard it&#39;s an expression medium that I use anyway while I was there a friend of mine from it was a floating Point chip designer from Hewlett-Packard he designed the H3 3000 uh floating Point units and we use those in the in the digital Ori that I built in the earlier in the 1980s Dan came by and visited me at Caltech and we started thinking about derivatives of functions not the derivative expressions for example what I mean by derivative function is derivative of the function that cubes okay I applied it produced a new function which would apply to 2 gives me 12. and that&#39;s right because the derivative of the X cubed is 3x square okay and that would be for what applied to 2 that gives me uh four times uh three times four is twelve so the derivative of a function is a new function and what we wanted is a method that was neither a numerical approximation nor a symbolic derivation it means I didn&#39;t want to look inside the function and look at its body this had to be this had to be maintaining the opacity of the function okay so so we thought about it for a long time and we just actually discovered what&#39;s now called forward mode automatic differentiation it had been previously discovered by others but we were ignorant but you know it&#39;s still fun to discover things even even if other people discover them first I&#39;m not interested in in priority I&#39;m interested in having the fun of discovery but in any case given such a a derivative function function derivative I can write things like this Newton&#39;s method is a way of finding the um the zeros of a of a function you know the places where the function is zero when a when applied to some number what the number that we applied to well if I compute the derivative of that then I can and use that as a function DF then I can if I subtract the guess for the for the zero from the from again I&#39;ve guessed from the zero the ratio of the function on that guess and the derivative on that guess we get a better guess that&#39;s we&#39;re called Newton&#39;s method and you all learned this at one point or another and we can iterate on that until we get a good enough answer and it works okay so here&#39;s what I&#39;m finding out that cosine and sine cross over at about 0.785 radians okay now now the Insight that Dan and I had was actually very interesting the Insight was that the chain rule was the essence of the matter okay that the chain rule was it was key to everything I do with derivatives if you have a derivative of f of G of x then that&#39;s the derivative with respect to the argument of f where the r where the the argue the formal parameter actually is replaced with G of X okay multiplied by derivative of g at X okay or more function to functionally in a more modern notation spivex wonderful book on calculus on manifolds explains this in in detail we have the derivative of the composition of f and g applied at X is the derivative of f apply to G of x times the derivative of g at X so at one point we were sitting around thinking contemplating this so we realized that the number system was extended to have differential objects which combined a finite part and an infinitesimal part composition would work automatically what did I mean by that well if in fact I invented things like complex numbers if number was as a pair okay of two things a five-part in an infinitesimal part okay and I made I made all functions have the property that if they took one of those in then the function&#39;s value would be the first part of the result and the Infinity part would be derivative of the function times the times the uh the if it doesn&#39;t part of the argument okay if I put that in then automatically if I if I took something like that which is the output of one of these had passed it through another another function I get the chain rule Works automatically this is a this is a beautiful Insight okay and we we had that sort of around maybe 5 p.m one day and then we sat there for a while had a little dinner and said because we were using scheme I could redefine all primitive arithmetic operators to work with our differential objects by the way the difference Logics were invented previously by a mathematician called Clifford in the 19th century he called them dual numbers but but they were pretty much forgotten okay so Dan and I had what was called a hack attack I bet you&#39;ve all had heart attacks hack attack is when you get so excited about some idea that you can&#39;t go to sleep until you&#39;ve solved the problem okay and it really is it really was like that so Dan and I stay up all night and by morning we had my MIT canoe scheme system doing automatic differentiation with both unary and energy functions that&#39;s amazing of course this was a preliminary plan with lots of bugs Abelson Hal Abelson helped work out the correct theory for multiple derivatives he had to he we had to use some sort of interesting kind of power series which I&#39;m not going to explain here and then uh Jeff siskin and pearl mutter and Alex Alexi radul helped debug the idea for derivatives of high order functions like this here you know the derivative of a function that returns a function is a value can also be computed this way okay we had it working even this way but it wasn&#39;t quite right it would get the right answer all in almost every case uh but in all the cases we ever tried but there were sort of subtle bugs that were pointed out by siskin promoter and radul and by golly they wrote very nice papers about this I highly recommend them in any case um then there was a then then further happened is that Chris Hansen who worked with me uh um figured out efficient ways to extend dynamically extend the Primitive operational scheme from that mechanism I built the schema tool system that Jack wisdom and I have used for teaching and researching classical mechanics using we wrote a book called sicm structure interpretation classical mechanics and we also wrote a book about differential geometry functional differential geometry which captures well a lot of what I learned in working with kipthorne in uh in general relativity because general relativity is based on differential geometry the schemusical system basically generically extends everything you know all the arithmetic has extended to almost everything you can imagine and boy that was a lot of fun okay uh and the things you could do with that very simple but they&#39;re also very complex here&#39;s an example of taking a derivative type operator called lead derivative because there are many kinds of derivative type operators there are covariant derivatives there are lead derivatives there are exterior derivatives but you can exponentiate the derivative operator okay to basically integrate a uh the motion of something which is described by a hamiltonian which is that&#39;s how you describe a physical system okay you start with lagrangian for the physical system transform it into hamiltonian Take the Lead transform of that extent and Advance the the initial conditions by DT and look at the coordinates and look at the first six terms of that series and by golly get the right answer that you don&#39;t understand what I just did here doesn&#39;t matter the fact is that it&#39;s this is this is the kind of thing we can do okay because what&#39;s really important is this when Dan and I came up with the idea that we can make differential objects that would pass through a function automatically implemented the chain rule that felt good okay it had a good it really tasted good okay it was delicious and when the idea was coupled with the technique of generically extending the Primitive operator the arithmetic operators I immediately knew there&#39;s a fruitful plan for a system the really big fun was the discovery of the plan and the subsequent multi-year elaboration of the resulting system with Hal and Jack wisdom okay and in fact the example our book structured interpretation of classical mechanics by Jack and I okay uh is exactly directed at making Clarity for classical mechanics which is fairly hard I&#39;m talking about the advanced stuff which is hamiltonians LaGrange and Louisville theorems and things like that I&#39;m going to read you from the for the preface classical mechanics is deceptively simple simple it is surprisingly easy to get the right answer with fallacious reasoning or without real understanding to address this problem we use computational techniques to communicate a deeper understanding expressing the the methods in computer language forces them to be unambiguous and computationally effective the task of formulating a method as a computer executable program and debugging that program is a powerful exercise in the learning process also once formalized procedurally the mathematical idea becomes a tool that can be used directly to compute results so for example here&#39;s a piece of impressionistic mathematics imprecise poorly explained in most books for for example the realizable pass the actual path something may take through a dynamical system to say something like Springs and masses or whatever or the planets okay the realizable path through configuration system space of a dynamical system must satisfy the LaGrange equations these are basically Newton&#39;s equations written in a in a way which is more General in traditional notation the LaGrange equations are written like this this is this is vague it&#39;s terrible you don&#39;t even understand unless you are into the culture you have no idea what this says because if first of all it&#39;s not actual good mathematics it&#39;s got bugs there&#39;s a type error I can&#39;t take the derivative of the of a function of three variables the lagrangian is a function of time position and velocity okay and so what am I doing taking a derivative with respect to time there it&#39;s not a partial derivative it&#39;s something else it&#39;s what&#39;s going on is that somehow the path is being substituted into the the path as it was a function of time is being substituted into the lagrangian after we take the partial derivative that&#39;s what&#39;s happening so the traditional mathematization is no good okay here&#39;s what it really should look like it looks ugly as Sin but here it is okay it&#39;s the derivative with respect to time of the partial derivative of the lagrangian with respect to one of its one of its formal parameters where the formal parameters are substituted for them the path at the appropriate time so this is a function of time only this is an expression which has time everywhere and therefore this makes sense to take a derivative with respect to time and this one this one is the same thing except there&#39;s no derivative there so the type error would be putting this derivative here okay but it depends upon really understanding what was written here is actually this okay now once you with that you get that modern functional notation lagrange&#39;s equations look like this which are much simpler okay the derivative with respect to oh it&#39;s D2 right that&#39;s because I&#39;m a zero based indexing person time is zero the position is one and velocity is two okay so I&#39;m talking about the the the the second the second um formal parameter okay other thing which extends the configuration through the state space which looks like that the time the position and the velocity and in scheme I can write it down just like that which is the same this one-to-one correspondence okay so I get a completely unambiguous and executable program which I can run I can make a lagrangea for a harmonic oscillator one FMV Square minus one f k q Square I can have a harmonic oscillator of mass m and three constant K I&#39;m going to see what it does on for what is the LaGrange equations for that that is the Newton&#39;s equations basically the f equals ma equations and indeed at what it is is that this is this value is zero which is the mass times the second derivative of the position plus the the spring constant times the position has got to be zero that is the correct the correct uh equations and like if I give a purported answer a cosine Omega t plus Phi which is true for a harmonic oscillator all solutions are like that where a is a is the amplitude of the oscillation Omega is the is the angular frequency and Phi is a phase then I get a constraint which is this has to be zero but if I buy a is up to me so I can&#39;t be zero and the cosine&#39;s only zero sometimes so this has to be zero all the time which means that the angular frequency is the square root of K Over M which is correct it&#39;s the spread the it goes up with the spring constant and down with the mass okay as a square root so that&#39;s just showing how you can use such a thing and I&#39;m going to summarize Now by saying where these pleasure the pleasure of programming programming provides many intellectual pleasures okay one of them is the pleasure of discovery of analogies of deep analogies analogies between completely disparate worlds so the eval apply interpreter and Maxwell equations got me when I was a freshman at MIT okay there is this leads into other kinds of philosophical contemplations every programmer must encounter every day of his life eternal questions you don&#39;t normally notice them because you&#39;re not you&#39;re too busy thinking about the details but actually these Eternal questions are there and the and philosophers have been worried about them for a long time and we have to actually encounter those and actually actually make use of it of of possible answers to those questions that might be good enough for our current problem but don&#39;t really answer the problem okay then there&#39;s the the problem of we have bugs and I told you why there are bugs okay that they&#39;re part of our strategy for for problem solving for really smart people but there&#39;s also the fun of going after the bug there&#39;s the incitement of the hunt boy it&#39;s more fun I have never had more fun than going after say a good garbage collector bug okay spending all day chasing one where you know I have to look at look at what&#39;s going on in the middle of of the work the detailed words of a in a in a computer memory and then there&#39;s the the pleasure of discovery of good ideas okay this high order functions in extensible generic procedures an example that that simplifies automatic differentiation is an example of the discovery of good ideas and finally the ability to transfer use programming to clarify other subjects like I was doing with with with classical mechanics or or general relativity okay those that&#39;s that&#39;s fun too it feels so good once you&#39;ve got a a better answer a better understanding than you would have if you didn&#39;t didn&#39;t write a program so I want to conclude this with a with a charge to you happy hacking go after do there and and write programs that make you feel good okay but there&#39;s one more thing I want you to realize that programming is really fun only if you can share your work with others so please write and use Free Library software Unfortunately today I was required to use zoom on this to make this recording I feel really bad about it I feel dirty okay this go this is this is I I want to use only free Libre software is that software that I can give to you that you can read you can change it and you can give it back to me with my change with changes and therefore we can learn together we can cooperate so I want to do that okay uh if you want to know what this is about please look at this this uh URL canoe.org philosophy free software.html to learn about software and freedom and please join the free software foundation and donate to the free software Foundation because because those are the guys who are pushing free Libre software including things like gnu Linux okay and that&#39;s all I really have to say today I&#39;m going to stop sharing my screen and be happy to take questions and I would like uh the recording to be",
    "commentLink": "https://news.ycombinator.com/item?id=40663704",
    "commentBody": "Gerald Sussman: Programming is (should be) fun (2022) [video] (youtube.com)272 points by nequo 20 hours agohidepastfavorite87 comments danybittel 11 hours agoOh I love this: \"I use programs as a way of remembering stuff. My memory is the programs I write. If I learn something math or physics or some other maybe biology and I write a program that represent that so I can read the program later it's not ambiguous, easy to read and that way I sort of store my knowledge.\" reply dgb23 9 hours agoparentThen you will likely enjoy this talk as well: \"Programming for the Expression of Ideas\" https://www.infoq.com/presentations/Expression-of-Ideas/ Very adjacent to the quote above. He talks about how programming helps to understand things (mathematics, physics...) in a deeper way. reply dazzawazza 9 hours agoparentprevWhen I was a child I often explored problems using BASIC. It was a really powerful way for me to creatively learn. I tend to think in pictures and find maths hard to truly understand. Coding allows me to create pictures and relationships in my head. I was lucky to find this out when I was young. I'd be lost without it. reply lynx23 25 minutes agoparentprevI use programming exactly for that, to get rid of ambiguity in notes and make them execut/verifyable. Actually, that was my biggest issue with math: I dont trust myself to make no errors and neither can I trust myself to see them. A compiler/interpreter will make me notice. reply clbrmbr 8 hours agoprevI once saw Sussman speak in a group of about 20 neuroscience PhD candidates in an NYU basement. He was talking about AI interpretability long before it was in vougue. Anyways I asked one question and he immediately sensed there was an EE in the room and went to the side blackboard and showed how we can figure out how an opamp works through first principles. So much energy and intelligence in that man. Left an impression for sure. reply xyzwave 17 hours agoprevI haven’t watched the video yet, but I was instantly reminded of the Alan J. Perlis quote he and Abelson used for SICP’s dedication: > I think that it’s extraordinarily important that we in computer science keep fun in computing. When it started out, it was an awful lot of fun. Of course, the paying customers got shafted every now and then, and after a while we began to take their complaints seriously. We began to feel as if we really were responsible for the successful, error-free perfect use of these machines. I don’t think we are. I think we’re responsible for stretching them, setting them off in new directions, and keeping fun in the house. I hope the field of computer science never loses its sense of fun. Above all, I hope we don’t become missionaries. Don’t feel as if you’re Bible salesmen. The world has too many of those already. What you know about computing other people will learn. Don’t feel as if the key to successful computing is only in your hands. What’s in your hands, I think and hope, is intelligence: the ability to see the machine as more than when you were first led up to it, that you can make it more. reply ozim 10 hours agoparentOof hits hard when I think about agile coaches, clean code ambassadors and whole cottage industry of how to make your code and team perfect. reply schneems 6 hours agorootparentI’ve learned that aiming for perfection is how some people have fun. I think the tragedy is when fun, energy, and mental health aren’t part of the conversation about how work gets done. reply Ma8ee 6 hours agoparentprev“We in computer science”! I interpret that as people doing research, not the rest of us who mostly do bean counting programs. For those that pay our salaries, not losing any beans is way more import than how much fun we have, or how much we stretch the possibilities. reply schneems 6 hours agorootparentI think the act of coding is fun, even if the domain is not. I think the goal should still be fun. It’s not fun to lose the beans. I think when the coding becomes overly fearful is when bugs and problems happen. Maybe you don’t have much leeway around the bean part, but you have a lot of room to test and validate that system. How do you make that system as fearless to work with as possible? It’s a difficult goal, but achieving it is a worthwhile accomplishment. A difficult puzzle that some might find fun. reply jt2190 5 hours agorootparentYou’re describing nothing more than having the programmer be able to stay in the “fun/challenging” zone, above the “boring” zone and below the “stressful/helpless” zone. This is obviously very personal: What is boring for you may be challenging for me. Further, all learners of any subject are trying to stay in this zone. Professional educators are very aware of this and refer to it broadly as “student engagement”. The goal of a software project, however, is not optimal individual learning about programming, it is usually something else. reply JohnFen 3 hours agorootparentprevI produce my best work at my best productivity when I'm having fun, so those that pay my salary should appreciate that. This is a case where my values and the values of the beancounters are aligned. If I'm not having fun, then I'm in the wrong position and need to move on. Life is too short to have my day job be a grind, and I won't produce great work in that condition. reply convolvatron 3 hours agorootparentprevthe sad thing is that the organizations that dont have any fun, that are focussed on the serious, soul crushing business of making sure no beans get dropped, are generally really awful at keeping track of the beans. reply HansardExpert 7 hours agoparentprevAs a support engineer... \"Hey Mr Customer... don't worry that all your data is corrupt or you lost access to your disks ... you have been set in a new direction and we're just making your day job 'fun'\" They're not Bugs ... what you have here are little balls of 'fun' /s ;) reply nequo 6 hours agorootparentThat may well be the difference between “programming” and “software engineering.” reply JKCalhoun 3 hours agorootparentI know which I'd rather do. reply itronitron 3 hours agorootparentprev\"this is your first step on an incredible journey to fully appreciate how humans ascribe meaning and significance to words they have typed or photographs they have taken...\" reply whartung 4 hours agoprevCoding is fun. When you have Stuff to Do, and momentum and a clear path to do it. You have a ditch to dig and you just get into it. Stack of simple web pages to knock out. Bunch of business rules to code up. Couple of routine CRUD screens. Architecting is fun. Whiteboard, pens, couple folks in the same groove around the table. Sussing it out. Waving off the 0.01% edge case “what ifs”. Fighting frameworks? Fighting tools? Wiring together badly behaving black boxes? That’s not fun. Neck deep in bad documentation. Heated discussions talking past each other on a forum. Negotiating with the dumber than rocks AI that had a flash of competence. I like to say if I wanted to wire together badly documented black boxes, I’d have become an EE. I’ve always said: “I love programming, but I hate computers.” All the fiddling. Software going stupid. Updates frustrating new UX. “Honey, the printer isn’t working.” Coding is fun. The rest of it, not so much. reply amirhirsch 1 hour agoparent>I like to say if I wanted to wire together badly documented black boxes, I’d have become an EE. I like to say that Electrical Engineers don't believe in magic; Computer Scientists don't believe in physics! I worked for Gerry making software for 6.002x an experimental intro EE class. I remember when Gerry came back from a weekend with a whole constraint propagator coded up! reply HumblyTossed 2 hours agoparentprev> Fighting frameworks? Fighting tools? Wiring together badly behaving black boxes? > That’s not fun.makes me want to shoot myself in the face. How did we get to this point? What the fuck? Did people just need a lot of busy work to make themselves feel productive??? reply darkerside 2 hours agoparentprevAgree! But, coding is not (all of) software development. Said another way, maybe some people like the idea of software development more than they actually like software development. reply samatman 2 hours agoparentprevWhat all the \"not fun\" stuff you're referring to has in common, is extraneous complexity. We don't pick a framework so that we can spend hours trying to figure out why our model of what it does is different from what it actually does. We run ./configure so we can run make, not so we can spend time determining why running autoconf results in the functionality we're expecting to use getting stubbed out. And so on. The amount of time we have to spend on things which aren't the problem can grow arbitrarily large. Manifesting a solution to the actual problem, with all of its intrinsic complexity, that is fun. reply richrichie 4 hours agoparentprevFeynman called it the computer disease. I think that is understudied danger of computers - addictive play/fidget machines that stunt intellectual and personal growth. reply me_me_me 4 hours agoparentprevthis is a deep cut. Remember the days when you just 'make and run' to test your programs? Now I need pipelines, helm charts, and other 5 tools +20 mins of busy nothing work to see if a small change fixed something. reply dr-smooth 2 hours agorootparentYak Shaving: https://www.hanselman.com/blog/yak-shaving-defined-ill-get-t... reply epolanski 9 hours agoprevI can't lie, I loved programming for many years. It used to be my main hobby too. Trying any new language or software, reading programming books, contributing to many chats across libera/freenode and what not. But since I have become a freelancer and I started working in smaller contexts where the focus is to ship decent code at a higher rate this passion has vaned. I think this is overall good for me as an engineer, but has removed lots of fun and often learning from my daily job. reply systems 5 hours agoparentthat probably has more to do with freelancing and consulting in general, rather than something inherit in programming freelancing and consulting, is usually about problem solving, and even problem shooting (get rid of the problem as soon as possible, elegance not required) you want to be a builder a tool maker, with freelancing, i dont see this happening, if you move to a consultant role maybe for me consultant are a bit more into longer term relationship , so maybe this should be your next step reply epolanski 1 hour agorootparentI use the term freelancer too freely maybe, I'm mostly a full time consultant and I often have very long clients (years). Still, I tend to choose high pressure small environments (I honestly get bored in low productive corporate environments). reply submeta 8 hours agoprevSussman‘s and Abelson‘s SICP forever changed the way I think about code, abstraction, modularity, in a time where my peers were hacking in Pascal or C. Where I used Lisps/Scheme to create a mental model of the problem, they had to do the abstraction in their head and then implement it in a language that demanded low level focus. So I am forever thankful for having been exposed to SICP early on. reply davikr 8 hours agoprevOn that note, I find that programming in certain languages is more fun, or can be more comfortable, than in others. I don't think I'll ever be comfortable with C++. reply progx 8 hours agoprevChatGPT says \"But for many, the rewards outweigh the challenges, making programming a fun and fulfilling activity.\" After 36 years i can say \"i only wanna get the sht done\" :) reply Duanemclemore 14 hours agoprevI can't wait to watch this. I haven't gotten to do SICP yet but have it checked out and did get to listen to the classes while doing home renovations. The Sussman quote from the SICP classes about understanding complicated things makes it in to a lot of my presentations (in architecture of all things) and it's the slide that ALWAYS gets people frantically taking a note. reply herodoturtle 10 hours agoparentWould you kindly share that quote here? reply hydrox24 10 hours agorootparentUntil OP delivers (and in case OP does not); I suspect it's the following quote: > \"The key to understanding complicated things is knowing what not to look at, not to compute, not to think\" —Sussman. reply BoiledCabbage 19 hours agoprevGreat to hear the views on programming from one of the authors of SICP. The anecdotes in the first few mins are nice as well. reply TacticalCoder 19 hours agoparent> Great to hear the views on programming from one of the authors of SICP. I'll blog about it one day (I know it's been five years I've been saying that but hey) but... Sussman invited me for tea in his office so I ran to the MIT co-op but couldn't find SICP there so I told the person there I really needed that book for I'd see Sussman the next day: he called the Harvard co-op and they told me they had one copy in stock. So I uber'ed to the Harvard bookstore and bought SICP. Next day I went to Sussman's office at the MIT and when I arrived Abelson was there, by chance (he was leaving) so they both dedicated me my copy of SICP. And Sussman added \"had you been there 15 minutes earlier my wife would have signed it too!\". I had no idea his wife was the proofreader for SICP. Fun story. Probably rambling by now but all this really happened. I've got a cool picture of these two wizards and me holding the purple book they just signed! reply musha68k 10 hours agorootparentThe book is also so great because it highlights the creative aspect to programming - which they show could (also) be seen as an art form; where it's beyond the utilitarian aspects of coding; appreciating aesthetics / intellectual beauty. All for the sake of instilling in their students a more holistic understanding of the craft while doing so. IMHO SICP feels \"timeless\" somewhat because of that. reply hasmanean 9 hours agorootparentI spent my life looking for such an elegant tome. If I had discovered it when I was 15 maybe my life would have been happier. Utilitarian programming is like utilitarian food. Programming should be seen as an act of worship to the god of simplicity. If the artificial world intrudes and makes the program complicated, this is what leads to suffering. Ultimately it is the world which should be changed. reply bitwize 18 hours agorootparentprevI coined an AI koan that goes something like this: A novice travelled to the East to learn at the feet of Sussman. As Sussman lectured on writing device drivers with low-level Lisp code, the novice interrupted: \"But Master, is Lisp not a high-level language?\" To which Sussman replied: \"There was once a fisherman who spotted an eagle on the shore. 'Brother Eagle,' said the fisherman, 'how impossibly distant is the sky!' The eagle said nothing, and flew off.\" With that, the novice was enlightened. It's fictional, but it's based on a real-life Sussman encounter I had. A hacker (it may have been Dimitris Vyzovitis) was telling me about things he'd done in \"low-level languages like C and Lisp\". Then I said \"But Lisp is high-level!\" at which point the hacker paused and said \"Let me get Jerry.\" \"Jerry\" turned out to be Gerald Sussman, who explained to me, in the most nerdy-enthusiastic Sussman-like way, that Lisp was the low-level language of a virtual machine which he had actually implemented in hardware[0]. And it was profoundly enlightening. [0] See Steele, G.L. and Sussman, G.J., AI Memo 514, \"Design of LISP-based Processors, or SCHEME: A Dielectric LISP, or Finite Memories Considered Harmful, or LAMBDA: The Ultimate Opcode\", https://dspace.mit.edu/handle/1721.1/5731 reply selimthegrim 3 hours agorootparentTIL where Lambda The Ultimate came from. reply 082349872349872 31 minutes agorootparentJust in case you only learned about that one: there's a whole subseries of 'em. My current favourite is https://www2.cs.sfu.ca/CourseCentral/383/havens/pubs/lambda-... Debunking the \"Expensive Procedure Call Myth or, Procedure Call Implementations Considered Harmful or, LAMBDA: The Ultimate GOTO But I see some recent authors have gotten into the game, so it could be one of them updates that slot. reply code_biologist 18 hours agorootparentprevThat is so cool. Watching those SICP lecture videos as a teenager was incredibly formative for me. I still consider myself a knight of the lambda calculus. reply bryancoxwell 18 hours agoprevSussman and Rich Hickey are two people whose talks I never stop enjoying. reply sph 12 hours agoparentAdd Joe Armstrong to that list for me. reply christophilus 7 hours agorootparentI loved listening to Joe. Humility and humor (or, should I say humour in his honor?), practicality and intelligence. RIP reply floobertoober 18 hours agoparentprevThe only person I'd add to that list is Simon Peyton Jones. I could watch them talk all day! reply vaylian 13 hours agorootparentI don't know him yet. Are there some talks you can recommend that would be a good starting point? reply ReleaseCandidat 12 hours agorootparentThat must of course be \"Haskell is useless\" https://youtu.be/iSmkqocn0oQ?si=IkTHHdH1423tOVvz But apart from that, his enthusiasm can even make the mseemingly most boring topic interesting. I'd personally start either at \"the story of Haskell\" Escape from the Ivory Tower https://youtu.be/cOqxiS-WN1w?si=1iXpGE7cJZa4tbbJ Or something more recent, about Epic's Verse https://youtu.be/OJv8rFap0Nw?si=OE2mUllQfLmV29Bk reply endgame 10 hours agorootparentAnd if Haskell isn't your jam, maybe Excel is more to your taste? https://www.youtube.com/watch?v=jH2Je6wUvPs reply keithalewis 10 hours agorootparentIt's only the most popular programming language in the world. reply endgame 9 hours agorootparentAnd it's functional, too! reply dualogy 12 hours agorootparentprev> But apart from that, his enthusiasm can even make the seemingly most boring topic interesting. Totally, his ever-fresh, ever-genuine excitement with all this stuff is so contagious (for the duration, and then some =) reply tmtvl 17 hours agoparentprevI quite enjoy Guy Steele's talks as well. reply gumby 17 hours agorootparentI don’t know Hicky, but Guy, Gerry and Hal are great human beings as well. reply seanmcdirmid 14 hours agorootparentprevThe talks that guy and Richard Gabriel did together were pretty good. And of course Richard Gabriel himself. reply sharas- 7 hours agoprevThe more he talks about fun, the more seriously he should be taken reply neilv 19 hours agoprevSussman is one of the great ones. reply fire_lake 9 hours agoprevTragic that they replaced Lisp with Python in universities. We’re suppose to be learning about computation, not corporate duct taping. reply epolanski 9 hours agoparentPlenty of universities teach Lisp, Haskell, Scala, Reason, etc.. reply fredrikholm 6 hours agorootparentThe SICP course as it is taught today at MIT had Scheme replaced with Python some years ago. I'm assuming that's what OP was referring to. reply kkylin 4 hours agorootparentI don't think the new course is \"SICP with Python\"; my impression (from talking to GJS!) is it's a different course that now uses Python. But maybe someone here has experience with it & can speak up. reply systems 5 hours agoparentprevbut then again lisp got replaced in practice and is only really a niche language nowadays so the move away from lisp is not bad and while i dont like python purely as programming language (its not theoretically engaging or fun) python is top 3 or 5 in practice, people like it reply jcgrillo 3 hours agorootparentThis is missing the forest for the trees. The language for a university course in computation needn't be \"commercially relevant\". What's important is whether it serves the pedagogical purpose of teaching students to think about computation. Learning this or that programming language isn't at all what it's about--you can (and will over and over again throughout your career) do that on your own time. It's about developing the abstract thought pathways that enable you to quickly learn and think about computing. I don't know why MIT chose to stop teaching the course, but it sure would lower my opinion of that institution if it was something to do with the direct commercial appeal of \"knowing Python\". reply mhh__ 5 hours agorootparentprevPeople like coldplay and voted for the Nazis! reply ducktective 9 hours agoprevRecommend to also watch: https://news.ycombinator.com/item?id=35945134 reply sharas- 7 hours agoprevI wonder does Gerald Sussman do TDD :D:D (for TDD'ers: the tone of this comment is ironic) reply myth_drannon 5 hours agoprevBefore I even get into Python vs Scheme vs Others, programming for fun is out of the window when deadlines, agile, story points are involved. reply photochemsyn 2 hours agoprevA better title: \"Academic omputer science is (should be) fun\". The main points are that discovering analogies, philosophical contemplation, debugging problems, coming up with good ideas and gaining clarity are all fun activities - but this is mostly the luxury of the academic professor, who is paid to engage in such activities, and doesn't seem applicable to 'the customer wants this at the end of the week!' world, other than the debatable joys of debugging. The majority of \"programming\" should probably be thought of like \"welding\" - essentially a vocational school subject. If you want to develop a new welding technology, sure that's a graduate school-level academic/research undertaking, but the vast majority of welders - like programmers - will work with well-characterized tools and fairly simple systems. A vocational student doesn't want to waste any time learning MIT/GNU Scheme and then never use it again in their professional career - but an academic student might find it rewarding, rather like learning to read and write in Linear B as a linguistics major. Certainly any work can be 'fun' - I worked in biotech labs for years, some were fun and some were terrible, but that had little to do with the nature of the work - it came down to quality and emotional stability of co-workers and managers, whether the workspace was well-designed and adequately supplied and safe to work in (sloppy labs are dangerous and unhealthy), if the pay was high enough to live a decent life outside of work, whether there was a corporate culture of cutting corners and letting shoddy work slip by (assuming you want to take some pride in doing a job right) - and it's the same in any other professional situation like programming industrial control systems and so on. reply jcgrillo 2 hours agoparentHave you ever worked as a welder? I think you'll find that people who are curious, thoughtful, and educated (whether autodidactically or otherwise) do better work than those who aren't across all professions. When I worked as a welder a long time ago the people who were smart, motivated, and curious did better work than those who were just punching a clock. So I do think that programming is like welding, but not in the way you meant it. reply photochemsyn 1 hour agorootparentWhen I think 'welding' I'm thinking of high-tech welding systems, aluminum TIG welding, or the laser-welding assembly-line shop I worked in before going to college. The people running such shops are skilled professionals, certainly, even if they didn't hold graduate degrees in materials science or applied physics. Maybe your view of welding is that it's comparable to digging ditches? And one doesn't have to go into hundreds of thousands of dollars in debt to attend some exclusive college to be smart, motivated and curious. reply jcgrillo 2 minutes agorootparentMaybe I misunderstood, I was responding to your characterization of welding as a \"vocational school subject\" whose future practitioners wouldn't want to waste their time learning anything that isn't the direct application of various welding processes to their first job in the field. In fact, though, folks who are the best at actually doing the welding are those who are most interested in developing their craft as welders--whatever that takes be it attaining theoretical knowledge, experimenting with obsolete or rarely used processes (e.g forge welding), etc. Analogously, computerers who are best at computering aren't the ones who just want to learn enough jquery to get that computer gig. So what's the point of education? Is it to illuminate the path to excellence for those who choose to follow it, or is it to try to land someone a job and no other thing? reply Mohamedrila0418 1 hour agoprevYouTube reply carapace 4 hours agoprevNo. Not any more. Programming should be as dull as operating a sawmill. I knew a guy, his construction company built half of Lake Tahoe back in the day. When he retired he built himself a work shop in the basement and made furniture until the day he died. It's incredible furniture. I'm sitting at one of his desks, on one of his chairs. 40 years old and still in excellent condition, and beautiful! Helical legs. You cannot buy furniture like this in the store. Handmade by a master craftsman at the height of his skill and knowledge. Made solely for the love of the craft and the joy of giving gifts. That kind of person has fun working. Computer programming IS fun to the kind of person who enjoys computer programming. Those people can do what they like. The rest of us want \"boring\" software: machines that get the job done, that don't break, that don't spew private data into the cloud willy-nilly, etc. If making that kind of software isn't intrinsically fun for you then maybe don't be a computer programmer? (We made a huge mistake making the normals think that learning a bit of JS was a viable career path.) Industrial software shouldn't be fun (except of course to the people who already find it fun to deliver solid working software!) - - - - In re: bugs being fun little learning opportunities... Margaret Hamilton worked out how to write bug-free software during the Apollo 11 project. The IT industry collectively went, \"meh.\" We need a serious attitude adjustment. reply jcgrillo 1 hour agoparentCorrollary: people who build bridges don't think it's fun when bridges collapse, but bridge builders have had millenia of accumulated experience that inform the codes and practices of bridge building. That's what we lack in software. If you do electrical work that isn't up to code, you as the electrician are liable. That means literally that if you touch a system, and you were the last one to touch it, you're liable. So it better be up to code when you're done. It should be the same for computerers. reply librasteve 10 hours agoprevraku is still -Ofun reply fuzztester 10 hours agoparentSure is. Just saw this a few days ago: Raku -Ofun for Everyone - Daniel Sockwell https://youtu.be/waHAGThlRH8?si=a_Jvt6jf5aQpjC-Y reply omoikane 29 minutes agorootparentThis presentation seem to be mostly about the community aspects of Raku in how they made the language approachable. Is there a presentation that focuses more on the technical aspects of how Raku's syntax and libraries would make it more fun relative to Perl or Ruby? reply KingOfCoders 11 hours agoprevTo me, I always feel this disconnect when people from academia talk about programming. I guess this is just me because of my personal experiences as a coder though, andothers don't have this. reply xbar 3 hours agoparentHm. I feel it differently. As a software engineering entrepreneur for some decades, I feel keenly the urgency of getting features implemented, bugs fixed, and teams motivated. It is usually a grind. But when I want to feel a spiritual connection to the fundamental principles of my work effort, academia is one of several places I turn to. reply dreamcompiler 12 hours agoprevThe news of Lynn Conway's death hit me hard because I learned a lot from her but I never got to meet her and thank her. I learned a lot from Sussman too and I'm very glad I've gotten the chance to buttonhole him in the hallway several times and ask him a question and let him talk. I always learn something much more interesting than the answer to my original question. I highly recommend stopping by and saying hi to Sussman if you find yourself at CSAIL. reply rramadass 14 hours agoprev [–] The Software Industry needs to take notes from this. They are the ones who have beaten the fun out of Programming and turned it into a drudgery/chore and have caused many Programmers who loved their craft to quit the field entirely. It is possible to meet customer/business needs and still create/maintain a work environment where the programming activity itself is fun for the developers. The way to do it is to abandon Taylorism Management and approach Software Development as a \"Human-first\" activity akin to that of a poet/painter/artist. Promote Autonomy, challenges compatible with competence, encourage novelty/learning/training and focus less on schedules, micromanagement and detailed processes. reply mingusrude 11 hours agoparent> ...akin to that of a poet/painter/artist. Anecdotally, programmer colleagues that view themselves as artists are generally harder to work with than those that identifies as craftsmen. It's generally much easier to have a sound argument about someone's work if they don't view it as their art. reply dgb23 9 hours agorootparentI identify as a craftsman but I resonate with the above comment. Ultimately programming is half technical (what does the computer do) and half human expression (reading and writing code). The latter [0] is really more of an art than a science or an engineering discipline. Look at all the attempts to prove that one particular way of writing programs is more productive, comprehensible or maintainable than others. They all failed in unique ways, perhaps because human communication is highly complex and contextual. The only thing we all might agree on is \"consistency is good\". [0] The former is in my opinion generally overlooked in many fields, even though it's the most measurable. But it is apparently often in conflict with with the latter unfortunately. reply p_l 9 hours agorootparentprev\"Craft\" is very much an emotional identity and art form, and for many people craft is fun as well - maybe not the exact same emotion, but for me craft is the quiet kind of fun. reply stevenally 12 hours agoparentprev [–] Second that. I have been in the industry for 37 years. The Taylorism didn't exist until \"Agile\" came along around the year 2000. Every programmer was a craftsman before that. These days, I wouldn't take (or get)a job in most software companies. I suspect that in quite a few non software companies there are software craftspeople working away quietly. reply dualogy 12 hours agorootparent> The Taylorism didn't exist until \"Agile\" came along around the year 2000. Its widespread adoption (and usually counter to the spirit of the original Manifest itself) took years however — years where most places, the craftfulness was still prevalent. Myself I haven't encountered that in teams until the mid-2010s, less than a decade ago. Might have been earlier in SV, of course, though. And the bigger Java/.NET/Enterprise shops. reply AnimalMuppet 6 hours agorootparentprev [–] > The Taylorism didn't exist until \"Agile\" came along around the year 2000. There was plenty of that before 2000, but it was more in the mainframe world. \"Data processing\". Government contracts. Business applications. \"Software analysts\" who broke the problem down into small chunks, and gave those to programmers. I have been in the industry for 39 years, and I avoided almost all of that, but it was there... reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The author has extensive experience in programming, starting in 1962, and has worked with various early computers like the IBM 790, 650, and 1620.",
      "The text emphasizes the creative and abstract nature of programming, comparing it to art forms like poetry, architecture, and music, and highlights the importance of understanding and managing complexity.",
      "The author discusses significant concepts in programming and computer science, such as the eval and apply process in Lisp, debugging, and the philosophical aspects of identity and mutation in data structures."
    ],
    "commentSummary": [
      "Gerald Sussman emphasizes programming as a means of storing and understanding knowledge in fields like math, physics, and biology, highlighting its deeper educational value.",
      "The text underscores the importance of maintaining fun in computer science, as advocated by Alan J. Perlis, and warns against becoming gatekeepers of computing knowledge.",
      "The discussion includes the shift from teaching Lisp to Python in universities, noting that while Python is commercially popular, the primary goal of education should be to develop abstract thinking rather than focus solely on commercially relevant languages."
    ],
    "points": 272,
    "commentCount": 87,
    "retryCount": 0,
    "time": 1718229747
  },
  {
    "id": 40668088,
    "title": "Indian startup 3D prints rocket engine in 72 hours",
    "originLink": "https://spectrum.ieee.org/3d-printed-rocket",
    "originBody": "AEROSPACE NEWS Indian Startup 3D Prints Rocket Engine in Just 72 Hours Agnikul’s successful launch is a step toward “on-demand” rocket launches EDD GENT11 JUN 20243 MIN READ The Agnikul team stands in front of the company’s launch vehicle at the Satish Dhawan Space Center ahead of its first successful launch. AGNIKUL",
    "commentLink": "https://news.ycombinator.com/item?id=40668088",
    "commentBody": "Indian startup 3D prints rocket engine in 72 hours (ieee.org)268 points by pseudolus 7 hours agohidepastfavorite164 comments Etheryte 5 hours ago> The machine also automatically outputs a report that details any deviations during printing, removing the need for postfabrication qualification. Anyone who's previously worked with 3D printing knows that this simply does not pass any kind of a sniff test. Both preventing and detecting internal defects is one of the, if not the hardest problem in 3D printing. There are many large companies trying to find ways to reliably solve just this problem alone. Saying that this method doesn't require any checks after production is simply false. reply dim13 1 minute agoparentVery Indian approach. Print a report, and call it a day. :D reply NortySpock 3 hours agoparentprevTo steelman the argument, maybe they meant \"the monitoring system is good enough at catching defects that if none are reported, the engine will probably pass an all-up hotfire test of the engine\". Of course you could do a water or air-pressure leak test on the plumbing pretty easily, and you would likely do that on the first 30 engines... But if you have confidence in your build process, maybe the juice isn't worth the squeeze on (say) a direct contact ultrasound void check on every square millimeter of the part. It's all about \"how expensive is it to run the test\" vs \"what is the likelihood the test catches an issue\" vs \"what's the cost of failing while everyone is watching?\" Same reason SpaceX went from dry-dress-rehersals to wet-dress-rehearsals to separate-static-fire-before-launch to hold-down-for-three-seconds-before-launch... The hold-down -before-launch is an integration test that covers everything the previous tests do, so eventually you can start removing redundant tests. reply sandworm101 2 hours agorootparent>> maybe the juice isn't worth the squeeze In rockets/aerospace, where the failure of any one of a thousand different parts means instant disintegration, checks are always worth the squeeze. Everyone talks about building rockets on the cheap and accepting a slightly-higher failure rate, but in reality that doesn't work. Even a tiny increase in component fault rates translates to total mission failure once multiplied across thousands of vital parts. The answer isn't to not check but to find ways to more efficiently and more thoroughly check each part. This is only more true if one considers reusable rockets where components will be expected to participate in multiple launches. reply throwitaway222 1 hour agorootparentWell, if you build it robust enough, you can test less. Not saying testing is worthless, but sometimes a one-piece that used to be 45 pieces held together by rivets is just, much much more resilient. reply chfalck 1 hour agorootparentIt’s a bit of chicken and egg to know if you built it robust enough without meticulous testing of your robustness reply jes5199 1 hour agorootparentprevthis might be 20th-century thinking. if you can build enough copies of a rocket cheap enough, maybe disintegrating a bunch of them isn't a showstopper reply nordsieck 1 hour agorootparent> if you can build enough copies of a rocket cheap enough, maybe disintegrating a bunch of them isn't a showstopper The problem with that idea is that you won't be legally allowed to launch again until you root cause and fix the failure, which can take months (or years if you're Blue Origin). Also, your insurance rates tend to go up a lot when your rockets blow up regularly, which tends to push customers away. In practice it doesn't work. Notes: Astra said they were going to pursue this strategy. It was not well received by potential customers and they basically had to walk it back. reply fullspectrumdev 1 hour agorootparentprevThe issue isn’t the rocket: it’s what you have affixed to it (the payload). The rocket itself is purely a delivery system for a payload after all. reply locococo 1 hour agorootparentprevfrom the rocket perspective, sure, but not so much from the perspective of your cargo. In many cases the cargo is more precious than the rocket so you need reliable rockets. reply sandworm101 3 minutes agorootparentprev>> maybe disintegrating a bunch of them isn't a showstopper But it isn't about destroying a bunch of them. Cut corners on checks and you very quickly blow up all of them. Any slight increase in the failure rate of individual parts, saving a few pennies, multiplies exponentially across the entire rocket into total system failure. So the money-saving approach is actually to test test and retest, to cram down the failure rates so low that the cumulative rate become acceptable (about 1%). reply lesuorac 1 hour agorootparentprevWho pays when the payload disintegrates? reply ruined 1 hour agorootparentprevthis works for munitions, but not for payloads that anyone cares about. reply switchbak 45 minutes agorootparentNot sure exploding rockets on their launch platform is such a good thing when they're carrying a bunch of highly explosive / fragmentary warheads (in addition to the rocket itself, which is plenty dangerous). Unless this was something like a cruise missile dropped at altitude where a failure isn't a big deal. reply giantrobot 1 hour agorootparentprev> In rockets/aerospace, where the failure of any one of a thousand different parts means instant disintegration To make matters worse the failure modes don't only affect the launch vehicle itself. A failure of a rocket likely means a total loss of the payload. It also runs the risk of damage/loss of the launch pad, support structures, and hapless down range victims. Rockets contain a significant amount of stored chemical energy, enough to get the payload mass into a stable orbit of the Earth. If you release all of that energy at once as an explosion it will cause a significant amount of damage. Rockets aren't something to goof around with and make assumptions about safety. reply guax 5 hours agoparentprevTitan submersible sound detection system vibes. reply cjbgkagh 4 hours agorootparentI'm not sure how many people know about their 'sound detection system'. Carbon fiber is a fickle beast and is prone to such failures so that alone worried me, but it was incredible for me to learn that they expected some sort of early warning from cracking. reply raverbashing 4 hours agorootparentWell there were probably some early warning signs The problem is that warning sign comes at around 10ms or less before the actual disaster reply wongarsu 3 hours agorootparentThere were plenty of early warning signs. In a previous dive back in 2019 they had professional submersible designer Karl Stanley on board, who later wrote an email to OceanGate about the worrying cracking sounds he heard. > \"What we heard, in my opinion ... sounded like a flaw/defect in one area being acted on by the tremendous pressures and being crushed/damaged,\" Stanley wrote in the email, a copy of which has been obtained by CNN. > \"From the intensity of the sounds, the fact that they never totally stopped at depth, and the fact that there were sounds at about 300 feet that indicated a relaxing of stored energy /would indicate that there is an area of the hull that is breaking down/ getting spongy,\" It's more impressive that the sub continued to work while giving warning signs for 4 years. https://abc7.com/titan-submersible-2023-incident-titanic-oce... reply sjm-lbm 2 hours agorootparentThey seem to have totally recreated the hull using a different manufacturing method in late 2020/early 2021: https://www.wired.com/story/titan-submersible-disaster-insid... They seem to have run fewer tests on the new hull, though. From the outside, it looks like one of the lessons they learned from earlier tests was that tests can create bad news, so if you're optimizing for the best reports back to investors you should stop running tests. reply rvnx 4 hours agorootparentprevIf it would be a Tesla, just enough time for the auto-pilot to disengage and claim it was the fault of the user. reply lukan 2 hours agorootparentIs there data, that indicates Tesla did this, or something like this? reply lesuorac 1 hour agorootparentKind of? Autopilot has a habit of disengaging right before crashes [1]; which may not be a bad thing, your seat belt also has a habit of not being adjustable in a crash (dunno if an ICE engine will turn off). Mix that with Elon had a habit of commenting on crashes [2] to keep good marketing about FSD. And like who cares if it was \"Auto lane control\" vs \"Autopilot\" that let somebody drive the car from the passenger seat but Elon made sure to let everybody know \"Autopilot\" wasn't engaged. [1]: https://www.motortrend.com/news/nhtsa-tesla-autopilot-invest... [2]: https://www.consumeraffairs.com/news/elon-musk-says-autopilo... reply ETH_start 3 hours agorootparentprevTesla autopilot is not supposed to be unmonitored, so any accidents under its control are entirely the fault of the driver. If a driver disengaged 0.1s before impact, they were derelict in their driving. reply abduhl 3 hours agorootparentOSHA has a nice pamphlet regarding hazard identification and hazard controls. The least effective method of protecting workers is to put the risk on the worker to protect themselves. Your view has a similar vibe to \"well they weren't wearing their hard hat and so it's their fault,\" a view that has been rejected across the board for safety in favor of the view that even letting the situation get to that point is a failure. https://www.osha.gov/sites/default/files/Hierarchy_of_Contro... reply lowkeyoptimist 1 hour agorootparentIf a driver is using any driver assistance feature they need to be paying attention all the time. Not only is it stated in all vehicle manuals, it is the intelligent way to use the features given that automated driving is still far from perfect. Your analogy makes no sense given that the risk is always on the driver whether there are driver assistance features or not. reply ETH_start 1 hour agorootparentprevDriver control IS making the driver responsible, by definition. Tesla is legally required to put the driver in control. This is a driver assist program. There is no way such a program, that is subordinate to the driver and depends on the driver being in control, can protect the driver from not doing their part, and driving. reply sjsdaiuasgdia 39 minutes agorootparentShame about the \"Full Self Driving\" branding They can put whatever disclaimers in the manual but their branding is giving a different message. It's a message Tesla wants the customer to hear: \"sit back and relax, the car drives for you.\" The branding does not communicate that the driver needs to be just as aware and engaged as they would be if they were driving on their own, and be ready to take control of the vehicle at any moment. Compare this to GM's \"Super Cruise\" branding. The message I get from that is \"cruise control, but better.\" Cruise control is a long established feature, drivers have plenty of experience with it, and they know that it is definitely not going to drive the car itself. They know they're still going to have to pay attention because the car is going to do some of the driving tasks but not all of them. The car is making no implicit or explicit claim that it will drive for you. \"Full Self Driving\" and related features like \"Summon\" make implicit claims in how they're named and presented. The driver absolutely has responsibility but Tesla is trying to play both sides of the coin with their branding vs their actual liability. reply abduhl 6 minutes agorootparentprevFirst, Tesla is not legally required to put the driver in control - they are free to indemnify the driver completely and shoulder all of the liability themselves. Second, who do you think was ultimately the one at fault for the excessive radiation doses caused by the Therac-25 machines: the machine technician operating the machine or the machine manufacturer? If it isn't the technician then I don't understand your argument because you can just find/replace every instance of driver in your post with technician. https://en.wikipedia.org/wiki/Therac-25 shepherdjerred 3 hours agorootparentprevYou're completely right, but unfortunately people place too much trust in it. reply 6510 2 hours agorootparentprevIt reminds me of the data entry days. You can have someone type a million table rows into a form and catch all typos but if you give the same person the same data and the same time without having them type it they find non of them. reply toss1 3 hours agorootparentprevThis is true, but it also falsifies Tesla's naming, promotion, abd advertising of the capability as \"Full Self-Driving\". (\"Self-Driving\" alone could be reasonable in certain contexts, but insisting on \"Full Self-Driving\" is a flat-out lie in plain language. Saying \" Alan is fully capable of driving the car.\" means that he requires zero monitoring and/or intervention; same for the \"Fully...\" phrase.) reply ETH_start 1 hour agorootparentWhat the name implies if interpreted without any context pales in comparison to the repeated and explicit instructions and warnings given to the driver that clarify that the driver should always be in control. reply toss1 51 minutes agorootparentYup. It's \"we'll take the profits from selling it as something that it is not.\". While simultaneously they take every step to ensure that when things go wrong when it inevitably turns out to NOT be what they claimed, the entire burden and responsibility is not on them, but on you. reply cjbgkagh 4 hours agorootparentprevI don't know how many 'test to failure' tests would have been required before I would have any confidence in the models but probably so many tests that the titanium alternative would have been far cheaper. The other problem is that it cracks all the time and they get louder as they get deeper, so it's not just if it cracks it is if it cracks enough or more than expected. Which straw will break the camels back. It is just such an insanely awful metric. On the hypothetical assumption that I had some faith in the models at what point will I have the life or death fight over whether or not that last loud crack was statistically significant. reply ladams 4 hours agorootparentprevDo you have a link? Google just gives results about the US Navy detection of the implosion… reply NortySpock 3 hours agorootparenthttps://www.wired.com/story/titan-submersible-disaster-insid... Recent one-year-lookback story by Wired. reply azornathogron 4 hours agorootparentprevhttps://www.businessinsider.com/oceangate-2020-boasted-incre... reply lucianbr 3 hours agorootparentprevhttps://www.youtube.com/watch?v=6LcGrLnzYuU As I remember he talks about the sound detection thing. reply nathan_compton 5 hours agorootparentprevExactly what I thought. reply skybrian 2 hours agoparentprevI’m curious about why that is. Naively, it seems like printing layer by layer would allow for a lot of inspection. Maybe even photograph each layer as you go? And then test and go back to see what kinds of defects were apparent in the photos. reply dotnet00 2 hours agorootparentIt's just that it's very easy for deviations from what the sensors see to occur. In traditional 3d printing, this can be stuff like a sensor switch wearing out, maybe physically moving slightly, being temperature sensitive, maybe the frame has changed shape slightly due to heat, moving the sensor a little, maybe something in the microcontroller happened to cause a slight delay in reading the sensor, or looseness developing in the motion system, or something being slightly out of alignment, or some component in the extrusion system experiencing momentarily higher friction and so on. The layers are really thin, so manually inspecting them would slow down the printing process drastically. Then, ultimately, what even can you do if there's a defect? The layer has been laid already. If material is missing somewhere, you could have the machine go back and add it, but if there's excess material somewhere, or it's in a form that the machine can't fix, there's not a lot to be done, particularly in applications like rocketry, where your structural strength tolerance are very tight. reply serf 1 hour agorootparent>It's just that it's very easy for deviations from what the sensors see to occur. right, but this is a problem in any modern precision machining, and it has been (mostly) conquered to a degree that we can produce very precise things in an almost entirely automated fashion. >If material is missing somewhere, you could have the machine go back and add it laser sintering is easier to audit than a normal fdm style print in a lot of ways if you care to take the time to do it. The process can be paused fairly easy with the right machine and right environment, the product can be weighed mid-process, it can have all sorts of vision and laser metrology done to the product midway through production; whatever -- and the mid print failure rate is astronomically lower than extrusion based methods. it doesn't seem that unbelievable to me. reply metal_am 1 hour agorootparentprevI know for certain that defects in the powder layer can be fixed in binder jet by redoing the recoater. There has been talk in the research world about being able to fix errors in L-PBF but I’m not sure they’ve gone past the research stage. The big point is that you can know a part might be out of whatever your acceptance criteria might be. reply Ruthalas 1 hour agorootparentprevYou are correct, and with the current metal AM techniques (DED, SLM), you can also take thermal imaging of the melt pool throughout. From this you can a pretty accurate picture of the weld quality across the whole volume. I'm not sure that's sufficient to eschew any other non destructive testing, but it is great information. reply metal_am 1 hour agorootparentprevPeregrine from Oak Ridge National Lab does exactly this. Lots of other research papers about it too. reply surfingdino 4 hours agoparentprevI too am doubtful. The market is littered with failed 3d printed products, all failed because the designers know nothing about real-life product design or because their deigns are too brittle or melt too easily in heat. reply vasco 4 hours agoparentprevIf any system could measure its own signal to noise accurately, it wouldn't have any noise in the first place. reply sobellian 2 hours agorootparentThis is a \"truthy\" statement that sounds right but is, in fact, false. Radio systems have an easily measurable SNR, but the noise cannot be eliminated. reply vasco 34 minutes agorootparentYou can measure is from outside the system itself if you know the original signal. Of course its possible to measure the noise externally if you know what was transmitted. reply numbsafari 4 hours agoparentprevIf they’re not doing post fabrication validation, then the passengers are… reply surfingdino 4 hours agorootparent... test dummies. reply drlemonpepper 4 hours agorootparentprevwhat's wrong with testing in prod? /s reply numbsafari 4 hours agorootparentNothing. Always Be Testing. But _only_ testing in prod is bad. reply metal_am 4 hours agoparentprevIt might sound a little wild, but a huge amount of research has been put into getting metal AM parts to be “born qualified.” L-PBF is getting to be a fairly mature technology. reply la64710 2 hours agoparentprevYes and innovation curve tends to go down in large companies because of barriers which a startup is not bounded by. This is how future big companies are created. reply xxs 4 hours agoparentprevYou may go further and say the rocket would be built on a proper rocky foundation. reply constantcrying 4 hours agoprev>Constructing a rocket engine using conventional approaches can take months, followed by extensive qualification testing to ensure it meets the required specifications. Using a metal 3D printer from German company EOS, Agnikul produced its engine in roughly three days. Agnikul printed the engine out of inconel, a high-performance alloy of nickel and chromium that can withstand high temperatures and mechanical loads. The machine also automatically outputs a report that details any deviations during printing, removing the need for postfabrication qualification. What amateurs are at work here? This plainly is not true and if you believe this can work you can not be trusted to be anywhere near an engineering project. Besides, the question I have is why 3D printing? There are innumerable ways to manufacture a rocket, why did they choose 3D printing? If it is because they think you don't need qualification and testing for the produced hardware they should not be allowed to launch anything they make. reply wongarsu 4 hours agoparent3d printing rocket engines makes a lot of sense. The engines of Rocket Lab's Electron rocket are 3d printed, Aerojet Rocketdyne's AR1 engines have 3d printed fuel injectors, SpaceX's SuperDraco thrusters (the ones in the Dragon 2 capsules) are 3d printed. Complex geometries with lots of liquid channels make rocket engines difficult to machine, so overcoming the issues with 3d printing heat-resistant parts is well worth it. But skipping qualification testing is indeed a weird reason. I doubt you can skip test fires. reply krisoft 4 hours agoparentprev> Besides, the question I have is why 3D printing? Rocket nozzles are heat limited. You could get more trust out of them only if the nozzle would not melt. So you do a lot of tricks to cool it. One of those many tricks is that you circulate your rocket fuel as a coolant in the wall of the nozzle. This requires an intricate web of many tiny pipes which form the wall of the nozzle. Typically these are constructed by hand by brazing together many many pipes. That takes forever. In contrast additive methods seem to perform well in this application. This is standard stuff nowadays. Everyone seems to be doing it. I'm not sure what is your objection. If you don't believe me listen to Tony Bruno: https://youtu.be/Bh7Xf3Ox7K8?si=YVDIDq1bvKeCuvY9&t=1509 > This plainly is not true Which particular part are you objecting to? reply jfyi 3 hours agorootparentMy guess is failing to read between the lines and taking marketing too seriously. \"Removing the need for postfabrication qualification\" doesn't mean \"perfectly detects errors\" it means \"detects errors within our business specs and we expect it to be profitable within that margin\". reply wongarsu 3 hours agorootparentIn a typical rocket the payload is the most expensive thing, followed by the engines, and then the rest of the rocket and the fuel. Shaving costs off the engines is well worth it, but not if it sacrifices reliability. Maybe they have a model in mind where it works. If you use dozens of engines like SpaceX's Starship you can tolerate more engine issues. Of maybe they want to launch really cheap payloads on inexpensive rockets. But in the parameters of traditional rocket design, QA on your engines is one of the last things you want to save money on. reply tonyarkles 4 hours agoparentprev> Besides, the question I have is why 3D printing? My understanding is that there's interesting things in the aerospace industry that are very difficult or sometimes impossible to machine from a single part in a conventional subtractive machining process. GE is doing it as well for the LEAP engines: https://3dprintingindustry.com/news/ge-aerospace-to-scale-th... As an easy example, imagine a single-piece rocket nozzle with internal channels for delivering fuel and oxidizer. Pretty much impossible to machine and not that big of a deal* to do with additive manufacturing. * Your mileage may vary, talk to your doctor for details. :) reply sa1 4 hours agoparentprevWhat part is not true? https://www.youtube.com/watch?v=kz165f1g8-E I'm sure that there are lots of problems with this approach, but it is not as obvious as your comment makes it out to be. reply constantcrying 3 hours agorootparentThe part which is not true is: \"removing the need for postfabrication qualification\" reply elteto 3 hours agoparentprevSpaceX has been printing the Draco and Super Draco engines for the Dragon capsule for many years. It’s a proven technology at this point. With 3D printing you can create geometries that are simply not possible with any other manufacturing processes. The issue is not 3D printing. The issue is deluding yourself by thinking that somehow 3D printing is magical and you get to skip qual. You don’t. reply bagels 2 hours agoparentprevIntegrating all the plumbing in to the structure is a lot easier to achieve with 3d printing. reply stainablesteel 4 hours agoparentprevrelativity space does this as well, they have youtube videos on what they do reply constantcrying 3 hours agorootparentThey don't do qualification of their hardware? Why would you put that on YouTube? reply stainablesteel 3 hours agorootparenti dont know about that part, but they 3d print i don't honestly believe there's anyone building a rocket like this that isn't concerned about the rockets success reply wiseowise 4 hours agoparentprevnext [2 more] [flagged] filcuk 4 hours agorootparentThis is a very ignorant comment, as is the one you're replying to. Making engine nozzles used to be extremely manual and involved process, which could take up to a year to make a single nozzle. These need very particular channels within the walls for cooling, and that's impossible to manufacture with simple reductive methods. They used to form wax modls and deposit nanometre layers of metal. reply OutOfHere 4 hours agoparentprev\"When you don't have any real criticism, just make stuff up! Keep people down at all costs.\" P.S. Nick checks out. reply constantcrying 3 hours agorootparentHow is gross negligence on QA not a \"real criticism\"? reply OutOfHere 3 hours agorootparentYou hardly enough know anything about the project to make an assumption about the absence of QA. If you were an employee there, and you had specific concerns, that would be worth something. You made it up because you had no real point. reply constantcrying 3 hours agorootparent>You hardly enough know anything about the project to make an assumption about the absence of QA. I take their claims at face value. Either they are for some weird reason lying about not doing QA or they are extremely negligent in their manufacturing. Granted, I can not exclude the first option, but to be honest that might make the company look even worse. reply OutOfHere 3 hours agorootparentThey're not shipping people into space with this tech yet. As such, even if QA were missing, which we don't really know much about, the need for QA is way less than you make it out to be. It's not a big deal if a cargo rocket blows up, and so far it hasn't even blown up. Together, this is why your line of comments is unqualified. reply constantcrying 34 minutes agorootparentI sincerely hope you are never involved in any aerospace projects. reply HarHarVeryFunny 4 hours agoprevRelativity Space are also 3-D printing not just rocket engines, but entire rockets, and have had a successful first flight. https://www.youtube.com/watch?v=kz165f1g8-E reply CarVac 4 hours agoparentThey backed away from printing entire rockets because, well, it's a foolish way to fabricate large tanks. reply HarHarVeryFunny 4 hours agorootparentThanks - I wasn't aware of that, but it does make sense. They are sticking with 3-D printing of the engines though, and have been testing their next-gen Aeon R engine. https://www.youtube.com/watch?v=QjKJMcOQYBQ reply greekanalyst 6 hours agoprevThat's super cool. India has enormous potential and it is amazing to witness the rise of its tech scene. reply otteromkram 5 hours agoparentnext [4 more] [flagged] aprilthird2021 4 hours agorootparentImagine reading an article about the achievements of an Indian startup, based in India, with Indians at the helm, and thinking \"Ugh, they only achieve things because US companies grace them with jobs!\" reply iamthirsty 4 hours agorootparent> Celesta Capital, Rocketship.vc, Artha Select Fund and Artha Venture Fund, and existing investors Mayfield India, Pi Ventures and Speciale Invest.[0] Not making an argument about off-shore work, but to be fair a lot of the funding came from out-of-country. — [0]: https://www.forbes.com/sites/catherinewang/2023/10/17/indian... reply geodel 2 hours agorootparentprevAnd funded by Indian money? reply eagerpace 4 hours agoprevThis is really neat, but would like to know more about the engine. The image seems to show a very simple design, without turbo pumps that are a mainstay in any other \"rocket\" engine. reply russdill 2 hours agoparentIt looks like it's electric pump fed, and it doesn't look like those pumps are part of the engine. Is it just a combustion chamber then? reply mlindner 11 minutes agoprevThis company has been pushing a lot of press about this but it's really not anything special. The industry has been printing engines for many years and the time to print is dependent on the size (and to some extent the mass) of the engine and the speed of the printer. Take this with a grain of salt. reply instagraham 7 hours agoprevAmong world-firsts that happen in contemporary India, this is probably one of the coolest. reply passion__desire 6 hours agoparentThere is a veritasium video on rocket 3d printing. reply edm0nd 6 hours agorootparentFor the curious: https://www.youtube.com/watch?v=kz165f1g8-E reply AustinDev 4 hours agoprevCan't wait to 3D Print a bootleg PATRIOT system. reply instagraham 2 hours agoparentI take it you've seen the DIY 3D printed SAM some random Hong Kong dude made? https://www.youtube.com/watch?v=UvcDwSmmxWs reply fullspectrumdev 1 hour agorootparentI really would love to have the time and energy to replicate that. That guys work is pretty nuts reply pm90 6 hours agoprevThis is amazing. Strap several of those engines with a control system of some sort and you can basically launch a wide variety of payloads. reply philipwhiuk 5 hours agoparent> control system of some sort Control of multiple engines is non-trivial. You get fun stuff like plume interactions between the engines. reply shrubble 4 hours agoprevI wish them well, but the rocket went less than 9km to apogee and 8km over the ground. It has to continue improving... reply 1024core 3 hours agoparentWright Brothers first airplane flew only 100m or so. reply nvy 2 hours agorootparentThe \"Wright Brothers\" moment for rocketry happened 60 years ago. reply Narhem 2 hours agoprevNot surprised hellers have been doing it for centuries for pennies reply alephnerd 6 hours agoprev> feature eight engines in total and able to carry a 300-kilogram payload to an altitude of around 700 km. The launch vehicle used in May’s test was only 6 meters tall Significant military implications as well [0]. There is a significant spy-sat race going on in the region right now [1] with China, India, UAE, Saudi Arabia, US, Japan, South Korea, North Korea, Russia, Iran, Israel, etc all investing in capabilities in the region. Also has an impact in enhancing India's Ballistic Missile strategy used to combat a two-front war [2], because Missiles are to Indian military strategy what Drones are for Chinese military strategy. Also highlights how the India-US relationship is built by the Indian-American diaspora. The VC who funded Angikul is Anand Rajaraman - the Stanford professor who started \"Big Data\" with Ullman, was one of the earliest investors in Facebook, and lead Amazon Marketplace after getting acquired by Amazon early in it's history (Marketplace was originally an Indian e-commerce startup called Junglee). As India gets richer, and America's immigration system gets more and more rickety, a reverse brain and capital drain has started to form, much like with Chinese Americans in the late 2000s. [0] - https://www.deccanherald.com/opinion/agnikul-showcased-more-... [1] - https://asia.nikkei.com/Politics/Defense/China-and-India-lea... [2] - https://www.usiofindia.org/publication-journal/Evolution-of-... reply nordsieck 6 hours agoparent> Also has an impact in enhancing India's Ballistic Missile strategy used to combat a two-front war [2], because Missiles are to Indian military strategy what Drones are for Chinese military strategy. Almost no one uses liquid fueled Ballistic Missiles any more (I think China is the only one) because they are so operationally terrible. The eventual rocket sounds like it's a hair larger than RocketLab's Electron. Which is struggling to reach profitability after being in its segment for 7-8 years mostly without peer competition. Largely thanks to SpaceX's transporter (and now bandwagon) missions sucking most of the volume out of the market. Making a working rocket is undoubtedly an amazing accomplishment. But at the same time, I really wish that companies stopped making small-lift rockets. There's just no way for them to work financially. reply philipwhiuk 5 hours agorootparent> But at the same time, I really wish that companies stopped making small-lift rockets. There's just no way for them to work financially. Making a small launch vehicle is seen as necessary to attract the level of funding needed for a medium or heavy lift launch vehicle reply nordsieck 5 hours agorootparent> Making a small launch vehicle is seen as necessary to attract the level of funding needed for a medium or heavy lift launch vehicle I get that. But it seems like every VC on the planet had the same idea at the same exact time because there's like 50+ small lift rockets in various stages of development. And approximately 0% of them have a shot at profitability. I have real doubts that any of them that aren't backed by a nation-state will be able to fund raise and survive long enough to build a medium lift rocket on the back of their experience. reply alephnerd 6 hours agorootparentprev> Almost no one uses liquid fueled Ballistic Missiles any more (I think China is the only one) because they are so operationally terrible. India does as well as China. But they are cheap! Very cheap. Read the 3rd article I linked - it's the actual Indian government strategy around BMD. reply nordsieck 1 hour agorootparent> India does as well as China. > But they are cheap! Very cheap. Read the 3rd article I linked - it's the actual Indian government strategy around BMD. The only place that liquid rockets are specifically mentioned: > Scientists are also working towards making interceptors used in both layers operate on solid fuels. This is because chemicals in the liquid fuels corrode the fuel storage tanks easily. Therefore, most of the missiles are not kept in a ‘ready-to-fire’ mode. Also, it takes a minimum of three to four hours to fill the liquid fuel in the missile,9 a hardly acceptable scenario wherein precious time will be lost in case of an emergency. It seems pretty clear that: 1. This particular rocket has no military application because the rockets that India uses use hypergolic propellant (presumably UDMH and NTO), while the planned rocket uses LOx and RP1, which are cryogenic propellants (well, at least the LOx is). 2. India is pretty clearly trying to move away from liquid fueled rockets because, as I said in my previous comment, the operational aspects are really terrible. reply fallingknife 6 hours agoparentprev> America's immigration system gets more and more rickety It's completely insane that we allow mass migration of people with no money and no skills and make it incredibly difficult for the most valuable immigrants to get in. reply sn41 5 hours agorootparentIt's quite understandable, imho. Immigration is being allowed precisely for cheap labor, especially when citizens are not prepared to go through the extra hardship - for example, I remember reading that the fatalities when the bridge fell in Baltimore around 1 am, were all immigrants, all on duty at that hour. For specialized labor, there is always a question of possible espionage and back-channel tech transfer. This is not so much perhaps for India as opposed to other technological rivals, but it may be one of the considerations in the immigration policy being counterintuitive. reply pjc50 4 hours agorootparent> fatalities when the bridge fell in Baltimore As an example of how strict immigration policy is, the crew were made to stay on the boat throughout, including while parts of the bridge were blown up. reply axus 4 hours agorootparentI'd inferred that it was the company keeping them there, after hearing stories like this: https://www.cbc.ca/radio/asithappens/as-it-happens-monday-ed... This article goes into the actual reasons, the visas only came into play during the second month: https://www.cnn.com/2024/05/18/us/baltimore-bridge-dali-ship... Basically the company would get in trouble if the crew left, and they are too cheap to hire replacements, so they threaten them. reply qp11 2 hours agorootparentprevpfft people who want to find a way around what anyone \"allows\" will do so. To desperate people rules dont matter. There are more than 50 countries right now standing with begging bowls outside the IMF cause their economies have no hope of growing without help. As long as that list has no hope of shrinking, people who live there and recognize that reality, are going to find ways to get out by hook or crook. Immigration is a symptom of growing global inequality. Without inequality reducing no rules or walls are going to stop the incoming waves. reply pjc50 5 hours agorootparentprev> we allow mass migration of people with no money and no skills You don't? There's no visa for that category. reply teitoklien 5 hours agorootparentyou do, if you have sanctuary cities and entire states openly claiming they will not arrest people who break immigration law, it reduces the ability of USA to actually pass policies to take in more skilled immigrants who'll be a net benefit to America as a whole. reply pjc50 5 hours agorootparentLike the War On Drugs, the human cost, violence, and intrusiveness of enforcement is worse than the alleged problem. The much-vaunted localism of America lets states and cities decide that it's not a problem for them to allow people to live there, and they don't want to help the big bad Federal government disrupt that. (it's still no picnic being in that category, unable to register for social security or even take an internal flight) reply teitoklien 4 hours agorootparentWorld isnt some amazing la la land, I hope people who break laws to immigrate to America, will continue having the same sympathies towards you, when they become majority, and start dominating over you with their culture. It feels bad to say this, but to build a good well integrated state, integration is key, you cannot integrate people into a society, if you cannot even control the immigration rate. You can't just let people come to your country illegally, setup shop, and then tell everyone 'Cant do anything about it folks'. The human cost is paid in everything, the cost americans need to worry about, is whether the safety and future prospects of americans is safeguarded, not of people who break laws to illegally immigrate. It's like having sympathy for a next door neighbour child who is uncared for, and serving him/her food, while you're own child cries and starves to death. America isnt a country, with limitless resources, and enough food,housing,healthcare to feed everybody, stop treating it like it. Also, I'm not sure the people who arrive here illegally are as ready to integrate with America's culture, as some americans are ready to integrate with illegal migrants. I understand life is cruel, and maybe america should even do good things for it, but not at the cost of watching americans starve to death, veterans getting overdosed on the streets wishing after years of suffering to just die and be at peace, and watching them not being able to help them, because our resources are too stretched, Charity starts at home. Even the people who migrate illegally here, they'll help their own first, before even sparing one thought towards you. One needs to be realistic about certain things, it's perfectly doable to protect your own borders, Singapore does it against Malaysia, India does it against Pakistan and Bangladesh (big borders), China does it against neighbouring countries, most Asian countries can protect their own borders. Why can't america? reply troyvit 3 hours agorootparent> I hope people who break laws to immigrate to America, will continue having the same sympathies towards you, when they become majority, and start dominating over you with their culture. I don't know how many folks around me broke the law to immigrate to America, but whether or not they broke the law to come here they faced a lot of racism, classism, and suspicion. I've been in the same area for about 20 years, and if I'm not a minority in my neighborhood it's close. It's the immigrants who are kind to me, help me keep my house secure, and trade goods and services with me. They even help me practice my awful Spanish with them. So in my case, yeah it's working out. > America isnt a country, with limitless resources, and enough food,housing,healthcare to feed everybody, stop treating it like it. America doesn't have limitless resources, but with 5% of the world's population it uses 25% of the world's resources [1]. To anybody's eyes that's going to look limitless. I think simple laws of diffusion say that people are going to go where the resources are available. Whether it's politically tenable doesn't matter, this is human nature since we started walking. [1] https://www.re-sources.org/2020/05/online-lesson-material-wo... reply chasd00 2 hours agorootparentIf you don't mind sharing, i'm curious to know where you live. I live in North central Oak Cliff which is a large, predominantly hispanic, area consisting of multiple neighborhood just SSW of downtown Dallas. In my neighborhood there's a local Mercado that i can walk to for small things, i speak enough Spanish to manage but I'm eyed with suspicion from when i go in to when i leave. My wife is a HS teacher with about 25% of her students being undocumented. They use a bus service in Oak Cliff to freely move back and forth to Mexico, i'm not sure how that works but that's what they use. My son's middle school is a DISD public magnet in a wealthy area called Preston Hollow. The middle school is probably 85-90% hispanic and despite my kids being 1/2 Mexican they have faced enough racism and abuse that my wife and I have had multiple meetings with the administration to address it. African American kids face it even worse. One other anecdote, when my kids were in elementary some of their calsses were taught in Spanish because my kids and one other were the only ones who spoke English. The teachers, who barely spoke English themselves, got in trouble for that. Ironically, if you want true diversity in Dallas schools you have to pay/be admitted to the elite private schools. All this to say, in my experience, neighborhoods consisting of mostly undocumented immigrants and cities welcoming of undocumented immigrants aren't that great... unless you're an undocumented immigrant. reply fwip 11 minutes agorootparentIf 20 of the kids spoke Spanish and 2 of the kids spoke English, doesn't it make sense to teach the course in Spanish? Why would the teachers get in trouble? fallingknife 4 hours agorootparentprevThey are allowed in without visas reply pjc50 4 hours agorootparentNot having a visa is the opposite of \"allowed\". reply fallingknife 2 hours agorootparentNo. Being kicked out is the opposite of allowed. reply nathan_compton 4 hours agorootparentprevI wouldn't defend america's immigration system (in my opinion the world should be borderless and you should be able to vote in any location where you can demonstrate that you've performed work for a wage or something like that), but if anyone should be allowed to seek labor by moving freely, it is the unskilled, who are already tremendously disadvantaged by globalization. Skilled labor is unlikely to be tremendously impoverished by being unable to move to the absolute optimal location. In other words, one could say its insane that we draw imaginary lines on paper and then confine human beings to those lines based on the accident of their birth location. reply IG_Semmelweiss 4 hours agorootparentThose lines are not imaginary per se. It means that the people living within those imaginary lines have come together with similar ideals and unity, arrived by many years of co-operation, to bestow certain rights and privileges to their fellow neighbors. Those rights were paid in blood, sweat and tears. The reason people organized as tribes matters. Its part of our very nature. And those tribes also respected imaginary lines. Even animals respect some imaginary lines. Eventually, some may be accepted in the tribe. But no one would just walk in to the tribe. Its not how nature works. reply nathan_compton 3 hours agorootparentHardly. The lines are basically drawn by aristocrats of one kind or another and the rest of us have to deal with it. They are expressions of power and very little else, and mostly not the power of the people. The rights paid for in blood, sweat, and tears, as you say, were wrenched from the hands of the kinds of people who draw borders and consist entirely of statements that are borderless assertions of the value of human beings, not citizens. You may be right that there is some kind of fundamental tribal mentality in human beings, but that doesn't mean that enshrining that reality in law or even nurturing it is morally right. The idea that borders benefit the common person better than, for example, a system which genuinely respected the dignity and right of human beings regardless of their geographical coincidences, is bullshit. reply nordsieck 6 hours agorootparentprev> It's completely insane It makes perfect sense when you understand the principal-agent problem and the current political landscape. https://en.wikipedia.org/wiki/Principal%E2%80%93agent_proble... reply panick21_ 3 hours agorootparentIf the policy was the opposite you could also argue that. Its basically a fundamental problem of government, whatever the outcome. You have to be way more specific when you want to actually be insightful about the topic. reply hindsightbias 3 hours agorootparentprevvaluable immigrants are going to take my jerb and not flip my hamburger, pick the crops or mow my lawn. reply surfingdino 4 hours agoprevNice pre-flight photo of the engine. Could we also have a look at the engine post-flight? reply paulsutter 2 hours agoprevIf you take a look at a real rocket engine you will see hundreds of parts that cannot be printed Raptor 1, 2, and 3: https://www.nextbigfuture.com/2023/07/spacex-further-improve... reply anonymousd3vil 4 hours agoprevKudo's to the team! This is a great success. reply surfingdino 4 hours agoparentI hope they all get to ride on it like true engineers would. reply panick21_ 3 hours agoprev3D printing is the conventional way of designing rocket engines today. Literally every new rocket startup does it a lot. Its more unconventional how much isn't 3D printed on the SpaceX Raptor. reply 29athrowaway 4 hours agoprevI hope this doesn't lead to an exponential increase in space debris. reply 1024core 3 hours agoparentElon can litter the sky with 100s of cheap satellites... but heaven forbid that an Indian startup build 1 rocket, right? Oh bigots of HN, you never fail to disappoint me. reply agent13 14 minutes agorootparentWow. Talk about getting triggered. reply MobiusHorizons 3 hours agorootparentprevI don’t hear anyone disparaging the ability of Indian companies to build great rockets. ISRO has done some really great work, and deserves lots of credit. But this startup seems to be bragging about making some pretty basic mistakes, like claiming not to need QA without having actually done anything to solve that problem (they didn’t build the 3d printer). So they seem to be doing the same kind of non-serious stuff we see from other vc backed startups all over the world which deserves skepticism. reply 29athrowaway 3 hours agorootparentprevIf SpaceX could produce a rocket every 72 hours it would be a concern too. Also, best of luck in your quest of fighting imaginary villians. By conflating unrelated things and jumping to conclusions such as that you are not acting as a good ambassador for your country. reply nordsieck 1 hour agorootparent> If SpaceX could produce a rocket every 72 hours it would be a concern too. Just FYI, SpaceX is trying to launch a Falcon 9 about every 48 hours this year. So far they seem pretty responsible about things, though. reply imtringued 3 hours agoprevNow do it again, but this time build a fully automated rocket factory on the moon. reply Spod_Gaju 4 hours agoprevnext [4 more] [flagged] eks391 4 hours agoparentThe primary purpose of rockets these days is to put satellites into space, not to bomb people. This includes the rockets built by the world's militaries. reply aamoyg 4 hours agorootparentGuided missiles fall under the umbrella of rockets. There are far more guided and unguided missiles in the world than rockets for peaceful purposes. The primary purpose of rockets is to annihilate an enemy military with a secondary benefit being to put things and sometimes people into space. That's how it was pitched, and that's how it will always be. reply itshossein 4 hours agoparentprevOr perhaps probes to the Mars? reply aamoyg 6 hours agoprevnext [23 more] [flagged] alan-hn 5 hours agoparentThere's a picture of the rocket at the top of the article reply newswasboring 5 hours agoparentprevThis post is giving me the same energy as[1]. I'm usually not so blunt about callouts, but this just seems blatant. [1] https://ichef.bbci.co.uk/news/976/mcs/media/images/78030000/... reply alephnerd 6 hours agoparentprevHere's a better article - https://techcrunch.com/2024/05/29/india-agnikul-3d-printed-r... > There are only a few players in private space which print rocket engines, and our very own SpaceX was one of the first to pioneer this The US isn't the only country with a large private space sector. China and India have both been heavily investing in it since the mid-2010s, because a dynamic space sector also means having an indigenous capability to deploy spy-satellites and ballistic missiles. There's a reason why Maxar, SpaceX, etc all get outsized DoD funding. India's space sector is also getting a massive cash infusion from the UAE [0] and KSA [1], as they both begin their process of building domestic space capabilities due to worries of Iran. [0] - https://timesofindia.indiatimes.com/spotlight/indian-space-m... [1] - https://www.zawya.com/en/business/technology-and-telecom/sau... reply aamoyg 6 hours agorootparent\"The 6.2-meter-tall vehicle is made of carbon composite, which gives it a liftoff mass of 1,268 lb; at its heart is the 3D-printed semi-cryogenic engine that Agnikul manufactured in-house, each of which provides 6.2 kN of thrust.\" This is like they clubbed together processes which Russia seems to use for manufacturing of some of its Novator cruise missiles. That technology must have transferred over somehow and they are just adapting it for space. It's great they adapted a military technology for something else (though I expect it will stay military), but it's nothing novel. reply alephnerd 5 hours agorootparent> That technology must have transferred over somehow Nope. Russia and Israel attempted that for India in the 1990s but were threatened by US sanctions for breaking MCTR. Idk why you're so surprised that these kinds of domestic capabilities exist in India now. India is fairly poor, but R&D capabilities have been strong, and there is an actual reverse brain drain going on from the US now that GC backlogs are multi-decade in length. > it's nothing novel It's a significant incremental development, as it drastically reduces the upfront cost for ballistic missiles or sub-orbital launches (eg. satellites), because you can manufacture much more faster. Plus, it is actually indigenous, which is a significant change. reply boringg 5 hours agorootparentI don't think people are moving back to India for jobs last time I checked if thats what your implying with your reverse brain drain comment. reply alephnerd 5 hours agorootparentWhere do those laid off Indian H1B and EB1/2 move? The reverse drain has started already for several years now (around 2021-22). Indians will still come to the US, do a BS masters, and then work in an American company for a couple years, but they inevitably face a 134 year backlog for naturalization. You can either live in the US as an indentured servant for a company, and end up getting kicked out if you get laid off and can't find another employer. Or you can upskill as much as you can in the US, and then leave once you have a strong enough professional network. reply nbadg 5 hours agorootparentCanada is much more immigration-friendly than the US. It also has a program to green-light and fast-track immigration for all H1B holders in the US, though for calendar year 2024 the cap of 10k visas has already been reached. But my understanding is that there's a pretty significant population of skilled workers that were planning on settling in the US permanently (or semi-permanently), and have instead relocated to Canada because of immigration difficulties related to the greencard backlog. Europe is also an option, though it's not as pro-immigration as Canada (and has lower salaries as well). reply teitoklien 4 hours agorootparent1. Europe is a big region, Switzerland, Norway, Iceland, Sweden has far far far more higher living standards than Canada, their total disposable income is higher, even if canadians might get paid decent, cost of housing is crazy, taxes are crazy, you cant get any medical surgery done without years in waitlist, So europe is 10x better than canada (just being frank, Canada was an amazing country 10 years ago, not anymore) 2. Canada also has a huge surge in re-migration, that is, the most skilled immigrants in your country, are leaving for other countries, they only come to take your citizenship, as a safety net, and then try leaving for more better countries (the high income ones, not the rest, they stay back in canada), the number of applications in permanent residency is going down among skilled immigrants, you're receiving a lot more of fraudulent immigration from degree mills who are abusing your immigration rules. 3. Canada is already probably will have the lowest growth and highest decline in income among all OECD countries, and is predicted to stay the same in next 3 decades, if it continues doing the same things. USA still has a strong immigration procedure, but a lot more countries are now getting skilled immigrants from China, India, other Asian, African and Latin American countries, Japan is slowly going to increase immigration (already starting), a lot of European countries welcome skilled migration like Germany, India itself is surging and giving more high income job prospects (they are struggling at creating blue-collar jobs, but white collar well paid jobs are increasing, with more and more GDCs opening there (Global Development Centres), most major computer chips used are designed either in India or Taiwan, the majority of it, happening in other sectors too now) Canada is immigration friendly, but as a whole along with its other policies, it's inviting the wrong group of immigrants (low-skilled workers across the world, who'll fight with natives for lower paid jobs and drive down wages) that is not a good immigration policy, America also has better diversity capping immigration for each country at 7% max, Canada's immigration policies are poorly designed, and will continue to hurt the country going forward. reply nbadg 3 hours agorootparent> Europe is a big region I'm aware, particularly given that I live there. Post-tax, post-essentials income comparisons are absurdly complicated and always depend on the individual. For example, dual-income families in Germany can tick slightly ahead of similar families in the USA because of how much more expensive it is to raise children in the US and how many (cash and societal) benefits you get from the social support system in Germany. But even this is a can and not will; that will also depend on your specific tax situation, and a whole host of other things. And whether the discrepancies in income levels are balanced out by different standards of living is also something you can really only evaluate on a case-by-case basis, because it's different for every job. For example, the income differential in software engineering is way higher between NA and Europe than it is for, for example, healthcare workers. In my specific case, when I moved from the US to DE, my gross decreased by 2/3 and my post-tax, post-essentials decreased by 1/2 -- and by post-essentials, I'm including things like massively higher rents in the US. > a lot of European countries welcome skilled migration like Germany I'm an immigrant to Germany and I would absolutely *not* consider it welcoming to immigration, skilled or otherwise. And given the results of the past few elections here (and elsewhere in Europe), which have been dominated by extremely negative discussions of immigration (with the notable exception of most/all of the Scandinavian countries), I don't really think your comment is painting an accurate picture. I've never lived in Canada and have no experience with immigration there, so I can't speak to Canadian immigration policy beyond what I've already said -- that the current government is very pro-immigration and has policies in place specifically meant to compete with the US on skilled workers. What I can say though, is that much of what you're saying is very contrary to both my lived experience as an immigrant to Europe, as well as much of my experience with several very close friends who've been through the H1B -> green card lottery in the USA. Also, from an economic perspective, I would neither describe US immigration policy as \"smarter\", nor \"better\" than Canada's. And I would absolutely *not* describe it as a \"strong procedure\". reply robertlagrant 4 hours agorootparentprev> Where do those laid off Indian H1B and EB1/2 move? I don't think that would be a \"reverse\". A reverse would be Americans trying to apply en masse to Indian companies. reply aamoyg 5 hours agorootparentprevI mean, people move back to be close to their family, for jobs, for startup capital, cost of living, and even for the politics. There is a former coworker who flew back to vote in the election and even have some kind of celebration because some kind of Communist government got voted out of his province. reply boringg 5 hours agorootparentI agree there are many different reasons to move. I would really call out people who are saying that Indian talent is leaving the US and/or Canada because from my viewpoint we have a very significant amount of Indians leaving India and coming here. It's much more the exception to the rule of Indians leaving North America and going back to India. reply aamoyg 5 hours agorootparentprevI do agree it's a significant incremental development from an Indian perspective for India. I think it is important for them to have this technology. However, it is being pitched as a \"world first\". That's kind of disingenuous. reply aamoyg 5 hours agorootparentprevIt takes them 40 years to make a contemporary fighter jet, not all of which is in house, and they cannot make an infantry rifle, tank, or any defense equipment that their DoD equivalent is happy with outside of \"ring laser gyroscopes\", which was developed as a part of their guided missile program, which was made with covert and overt Russian and possibly Iraqi help. They cannot make their own trainsets, cars, and consumer goods to fulfill local demand aside from food. India is on the road to privatizing many of its industries, but if you look that the list of GoI enterprises versus public private partnerships and the endemic levels of corruption (or else ethno-fascism) it is likely these are going to succeed anything soon. Indias largest exports are ideas and philosophies like religion, and I bet in the future it will be manufacturing and operating philosophy like Japan did with Kaizen. India is a good place to go to offshore costs for space, and many countries do. They know how to squeeze the inefficiencies out of a process (and also not pay people, but that's another story) and bring down the costs of a mission. But there is nothing novel about the missions themselves. There is nothing pioneering, and there is nothing wrong with that. This is the case with Japan. Most of Japanese innovation is actually from US companies that they took over. In any case, it's important to be realistic-- it's nothing to do with India being poor. They are not poor. They can afford a lot, and do a lot. Some people are in the mindset to accomplish a great deal mentally. Most people are not and still throw banana peels into the highway after eating them. No country without a basic civic sense can (in the immediate few years) have the R&D capabilities you speak of in a meaningful fashion. Maybe in a few decades when people are more educated. reply triceratops 4 hours agorootparent> Most of Japanese innovation is actually from US companies that they took over. Just gonna leave this here: https://en.wikipedia.org/wiki/List_of_Japanese_inventions_an... Hopefully you'll realize that that is one of the more uninformed takes you've had, and then question the other things you think are true. reply aamoyg 4 hours agorootparentI clicked on that list. I went to pocket calculator, and it said the first one was made in the United States. reply aamoyg 3 hours agorootparentYou should look into the sheer amount of industrial espionage that Japan did in the United States and Soviet Union post WW2. It is mind boggling. We started legally transferring and licensing technology to them so that at least we would make some money back. reply ssivark 5 hours agorootparentprev> No country without a basic civic sense can (in the immediate few years) have the R&D capabilities you speak of in a meaningful fashion. LOL. Pray tell… Are you going to elaborate on your theory of technology development, or are you just bloviating your biases? reply aamoyg 3 hours agorootparentAnd you know what? Even some of the Indian companies at the time were super good. HCL was the Indian IBM. In fact, it's a one letter shift from IBM, and one of the earlier Indian tech companies. There were opportunities. People wanted money and things and didn't want to wait. reply aamoyg 3 hours agorootparentprevYes. You need to have national unity, pride, and not be entirely selfish in order to have more engagement and participation in higher education and therefore in R&D. You need to be in a position in which you can inspire your citizens that there is a better life for them by engaging in that R&D and morale improving projects, but that attitude also needs to be seen on the streets, not just on TV. India has some parts of the population in which that exists, but that's far from a large part of the population. Most people are very self interested, and you cannot blame them. People think of themselves. Why do you think there is a brain drain? People are educated at the government's expense at premier institutions and leave because they don't feel any sense of allegiance or civil responsibility to develop their country. My math teach told me that during the Cold War, in the United States, the math that they learned was harder than it was now. It was your duty to become a scientist or engineer or something positive to push your country forward. That should be how it is for India, but aside from a smaller segment of the population (who are pretty loud and say some interesting things) people are more self interested. And you can't blame them. If your cousin moved to the United States and seems to be \"living the life\" why wouldn't you leave? So a whole generation of scientists and engineers who were top notch educated at IIT at the government's expense under socialist India in the 80s basically left in the 90s and 00s because of more money, even though there were possibilities in India for them at the time (HP and IBM India, CDAC, CDOT, DRDO, ISRO, HAL, etc) reply aamoyg 3 hours agorootparentprevAnd sure, I am biased. That was my dad's life. That was his story. He made the call to leave India hoping to improve his career by moving to Australia and then the United States. He considered moving back multiple times, but could not bring himself to do so, not because of the lack of good opportunities, but because of the lack of civic sense. There are people in India who work super hard (which he really likes), but then there are people who are so selfish, driving on the sidewalk in the opposite direction, get upset because the ambulance is speeding to the hospital and refuse to move and block it intentionally, taking all kinds of shortcuts and opportunities to cheat each other instead of conducting business honestly, etc. No good treatment of women. It's messy and it needs to get more organized to do anything meaningful in terms of R&D in that country. reply nedpat 3 hours agoprev [–] Test reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Indian startup Agnikul successfully 3D printed a rocket engine in 72 hours, showcasing advancements in rapid manufacturing for space technology.",
      "The team achieved their first successful launch at the Satish Dhawan Space Center, indicating progress towards \"on-demand\" rocket launches.",
      "This milestone highlights the potential for more flexible and cost-effective space missions in the future."
    ],
    "commentSummary": [
      "An Indian startup has successfully 3D printed a rocket engine in 72 hours, significantly reducing production time.",
      "The machine used for printing automatically generates a report detailing any deviations, potentially eliminating the need for post-production checks, though some experts dispute this claim.",
      "The development highlights India's growing tech scene and the potential for 3D printing in complex manufacturing, despite ongoing debates about quality assurance and the necessity of post-production testing."
    ],
    "points": 268,
    "commentCount": 164,
    "retryCount": 0,
    "time": 1718276531
  },
  {
    "id": 40667102,
    "title": "AMD's MI300X Outperforms Nvidia's H100 for LLM Inference",
    "originLink": "https://www.blog.tensorwave.com/amds-mi300x-outperforms-nvidias-h100-for-llm-inference/",
    "originBody": "Last Updated: June 12, 2024 AMD’s MI300X Outperforms NVIDIA’s H100 for LLM Inference There has been much anticipation around AMD’s flagship MI300X accelerator. With unmatched raw specs, the pressing question remains: Can it outperform NVIDIA’s Hopper architecture in real-world AI workloads? We have some exciting early results to share. For the past month, TensorWave and MK1 have worked closely to unlock performance of AMD hardware for AI inference. To start, we focused on Mixture of Expert (MoE) architectures due to their compute efficiency and popularity – notably used by Mistral, Meta, Databricks, and X.ai for their most powerful open-source LLMs. The initial results are impressive: using MK1’s inference software, the MI300X achieves 33% higher throughput compared to the H100 SXM running vLLM on Mixtral 8x7B for a real-world chat use case. Despite NVIDIA’s software ecosystem being more mature, it is clear that AMD is already a formidable competitor in the AI market. When hardware availability and cost are factored in, the MI300X proves to be an attractive option for enterprises running large-scale inference in the cloud. We expect AMD’s performance advantage to climb even higher after further optimization, so stay tuned for more updates! -Darrick Horton, CEO TensorWave Subscribe to the TensorWave Newsletter Subscribe For Updates We invite you to experience the MI300X firsthand at TensorWave, which comes prepackaged with MK1’s inference software. Contact us to find out more. Inference Benchmarks We conducted extensive offline and online inference tests comparing the MI300X and H100 SXM5 accelerators using the Mixtral 8x7B model. Offline Tests: These are standardized and provide insights into the performance of the forward pass across different setups. Online Tests: These are more sophisticated and estimate system performance in a real-world setting where multiple users are serviced asynchronously. Benchmark Setup AMD Hardware: TensorWave node equipped with 8 MI300X accelerators, 2 AMD EPYC CPU Processors (192 cores), and 2.3 TB of DDR5 RAM. MI300X Accelerator: 192GB VRAM, 5.3 TB/s, ~1300 TFLOPS for FP16 Drivers: ROCm 6.1.2 Inference Stack: MK1’s inference engine (Flywheel) v0.9.2 and AMD’s ROCm optimized fork of vLLM (rocm/vllm) v0.4.0. Configuration: Tensor parallelism set to 1 (tp=1), since we can fit the entire model Mixtral 8x7B in a single MI300X’s 192GB of VRAM. NVIDIA Hardware: Baremetal node with 8 H100 SXM5 accelerators with NVLink, 160 CPU cores, and 1.2 TB of DDR5 RAM. H100 SXM5 Accelerator: 80GB VRAM, 3.35 TB/s, ~986 TFLOPS for FP16 Drivers: CUDA 12.2 Inference Stack: vLLM v4.3 Configuration: Tensor parallelism set to 2 (tp=2), which is required to fit Mixtral 8x7B in two H100’s 80GB VRAM. Notes All benchmarks are performed using the Mixtral 8x7B model. All inference frameworks are configured to use FP16 compute paths. Enabling FP8 compute is left for future work. To make an accurate comparison between the systems with different settings of tensor parallelism, we extrapolate throughput for the MI300X by 2. Offline Results To measure peak throughput for each inference solution, we generate prompts of a fixed size and directly feed them to the model. This method, known as offline batching, enhances hardware efficiency by processing multiple prompts simultaneously. Although larger batch sizes boost throughput, they also increase latency due to more requests being processed. Following standard practice, we constrain requests in a batch to have equal input sizes, and to have equal output sizes. We assess the throughput of each system by varying the batch size. This was done using a modified version of `benchmark_throughput.py` script in the vLLM repository, refactored to include Flywheel as a backend. Prompts were also randomly generated within a batch to remove caching mechanisms. The performance metrics, detailed in the table below, measure throughput as a function of batch size. Notably, our results show that MI300X running MK1 Flywheel outperforms H100 running vLLM for every batch size, with an increase in performance ranging from 1.22x to 2.94x. Online Results for Chat Data Distribution Moving beyond offline metrics, we designed a series of online benchmarks to simulate a realistic typical chat application. This involves generating responses to user inputs that closely mirror actual usage patterns. Specifically, we simulate chat traffic by spawning independent workers to send requests to an endpoint. We then sweep the number of workers to increase the number of concurrent requests. In these experiments, requests were generated using a standard text chat distribution with an average of 573 input tokens and 50 output tokens. Note that our benchmarking tool supports arbitrary data distributions; please reach out if you have a specific use case you’d like to test. The key metrics of interest are: Throughput (Requests per Second): The number of requests the system can handle per second for a given workload. Average Latency (Seconds): The average time taken to generate a full response for each request. Time Per Output Token (TPOT): The time to generate each subsequent token (averaged) after the first token, which impacts the overall speed of generating long responses. For the first benchmark, we tested a non-streaming use case where throughput and latency are measured for servicing the full response. At a target average latency of 5 seconds, two MI300X with tp=1 services 33% more requests per second than two H100s with tp=2. This means you can service the same number of users with similar quality of service using fewer accelerators! For the second benchmark, we enable streaming and measure throughput and TPOT for individual tokens as they are streamed out. Here we observe that the MI300X has higher throughput for every TPOT compared to the H100. This means the MI300X can generate text faster at higher traffic volumes, which is crucial for any LLM application. Conclusion Our benchmarks demonstrate that AMD's MI300X outperforms NVIDIA's H100 in both offline and online inference tasks for MoE architectures like Mixtral 8x7B. The MI300X not only offers higher throughput but also excels in real-world scenarios requiring fast response times. Given its impressive performance, competitive cost, and hardware availability, the MI300X with MK1 software is an excellent choice for enterprises looking to scale their AI inference capabilities. We encourage you to explore the capabilities of MI300X at TensorWave and experience these benefits first-hand. Contact us to learn more and schedule a test drive of this powerful accelerator!",
    "commentLink": "https://news.ycombinator.com/item?id=40667102",
    "commentBody": "AMD's MI300X Outperforms Nvidia's H100 for LLM Inference (tensorwave.com)264 points by fvv 11 hours agohidepastfavorite224 comments m_a_g 9 hours ago\"TensorWave is a cloud provider specializing in AI workloads. Their platform leverages AMD’s Instinct™ MI300X accelerators, designed to deliver high performance for generative AI workloads and HPC applications.\" I suggest taking the report with a grain of salt. reply nabla9 6 hours agoparentThe salt is in the plain sight. The do the standard AMD comparison: 8x AMD MI300X (192GB, 750W) GPU 8x H100 SXM5 (80GB, 700W) GPU The fair comparison would be against 8x H100 NVL (188GB,that's the main reason for the price difference IMHO. Explain why the performance difference does not matter? AMD does only 33% better with a chip that has 2X transistors and 2X memory. reply ebalit 52 minutes agorootparentprevBut the price should be a factor. Your fair comparison would match a ~60k$ setup to a 20k$ according to prices we can find online. I don't think it should be ignored, especially when the power consumption is similar. reply lhl 6 hours agorootparentprevIsn't SXM5 higher bandwidth? It's 900 GB/s of bidirectional bandwidth per GPU across 18 NVLink 4 channels. The NVL's are on PCIe 5, and even w/ NVLink only get to 600 GB/s of bandwidth across 3 NVLink bridges (across only pairs of cards)? I haven't done a head to head and I suppose it depends on whether tensor parallelism actually scales linearly or not, but my understanding is since the NVL's are just PCIe/NVLink paired H100s, you're not really getting much if any benefit on something like vLLM. I think the more interesting thing critique might be the slightly odd choice of Mixtral 8x7B vs say a more standard Llama2/3 70B (or just test multiple models including some big ones like 8x22B or DBRX. Also, while I don't have a problem w/ vLLM, as TensorRT gets easier to set up, it might become a factor in comparisons (since they punted on FP8/AMP in this tests). Inferless published a shootoff a couple months ago comparing a few different inference engines: https://www.inferless.com/learn/exploring-llms-speed-benchma... Price/perf does tell a story, but I think it's one that's mostly about Nvidia's platform dominance and profit margins more than intrinsic hardware advantages. On the spec sheet MI300X has a memory bandwidth and even raw FLOPS advantage but so far it has lacked proper software optimization/support and wide availability (has anyone besides hyperscalers and select partners been able to get them?) reply nabla9 5 hours agorootparent> but I think it's one that's mostly about Nvidia's platform dominance and profit margins more Profit margins and dominance are result from performance, not the other way around. It does not matter if Nvidia tools are better when you deploy large number of chips for inference and it does more flops per watt or second. It's seller market and if AMD can't ask high price, their chip do not perform. ---- Question: People here seem to think that Nvidia has absolutely no advantage in their microarchitecture design skills. It's all in software or monopoly. Is this right? reply fisf 5 hours agorootparent> People here seem to think that Nvidia has absolutely no advantage in their microarchitecture design skills. It's all in software or monopoly. That's an extrapolation. Microarchitecture design skills are not theoretical numbers you manage to put on a spec sheet. You cannot decouple the software driving the hardware - that's not a trivial problem. reply paulmd 41 minutes agorootparent> Microarchitecture design skills are not theoretical numbers you manage to put on a spec sheet not only can you measure this, not only do they measure this, but it's literally the first component of the Rayleigh resolution equation and everyone is constantly optimizing for it all the time. https://youtu.be/HxyM2Chu9Vc?t=196 https://www.lithoguru.com/scientist/CHE323/Lecture48.pdf in the abstract, why does it surprise you that the semiconductor industry would have a way to quantify that? like, realize that NVIDIA being on a tear with their design has specifically coincided with the point in time when they decided to go all-in on AI (2014-2015 era). Maxwell was the first architecture that showed what a stripped-down architecture could do with neural nets, and it is pretty clear that NVIDIA has been working on this computational lithography stuff for a while. Since then, I would say. Since then it's been Pascal vs Vega, Turing (significant redesign and explicit focus on AI/tensor) vs RDNA1 (significant focus on crashing to desktop), Ampere vs RDNA2, etc. Since then, NVIDIA has almost continuously done more with less: beaten custom advanced tech like HBM with commodity products and small evolutions thereupon (like GDDR5X/6X), matched or beaten the efficiency of extremely expensive TSMC nodes with junk samsung crap they got for a song, etc. Quantitatively by any metric they have done much better than AMD. Like Vega is your example of AMD design? Or RDNA1, the architecture that never quite ran stable? RDNA3, the architecture that still doesn't idle right? Literally the sole generation that's not been a total disaster from The Competition has been RDNA2, so yeah, solid wins and iteration is all it takes to say they are doing quantitatively better, especially considering they were overcoming a node disadvantage for most of that. Contrast to the POSCAP/MLCC problem in 2020: despite a lot of hype from tech media that it was gonna be a huge scandal/cost performance, NVIDIA patched it dead in a week with basically no perf cost etc. Gosh do you think they might have done some GPGPU accelerated simulations to help them figure that out so quickly, how the chip was going to boost and what the transient surges were going to be etc? literally they do have better design skills, and some of it is their systems thinking, and some of it is their engineers (they pay better/have better QOL and working conditions, and get the cream of the crop), and some of it is their better design+computational lithography techniques that they have been dogfooding for 3-4 generations now. people don't get it: startup mentality, founder-led, with a $3t market cap. Jensen is built different. reply Symmetry 5 hours agorootparentprevNVidia doesn't have a general advantage in hardware design skills, but they have been focused on AI workloads for quite a while while AMD spent a long time focusing on HPC factors like 64 bit floating point performance. reply sangnoir 3 hours agorootparentprev> Price tells a story. If AMD performance would be in par with Nvidia they would not sell their cards for 1/4 price What were your thoughts on Zen (1) vs Intel's offerings then? AMD offered more back for the buck then too. reply resource_waste 6 hours agorootparentprevThx! Anyone who says Nivida isnt king, needs a reality check. reply ddtaylor 5 hours agorootparentI love AMD, but my Nvidia stock position currently is much higher than AMD. reply hackerlight 4 hours agorootparentAMD is at a much higher PE ratio. Is the market expecting AMD to up its game in the GPU sector? Or is the market expecting a pullback in GPU demand due to possibility for non-GPU AI solutions becoming the frontier or for AI investment to slow down? reply cameldrv 1 hour agorootparentI think that the expectation is that NVIDIA is in somewhat of an unreasonable position right now (and for the immediate future) where they're getting about 80% gross margins on their datacenter GPUs. This is an extremely juicy target for competitors, and even if competitors manage to produce a product that's half as good as NVIDIA, NVIDIA will have to cut prices to compete. reply sangnoir 2 hours agorootparentprevWhy not both? reply szundi 4 hours agorootparentprevThis topic is just about wether this changes or not reply epolanski 9 hours agoparentprevWell, there's the beauty of specifying exactly how you ran your benchmark, it is easy to reproduce and disprove or confirm (assuming you got the hardware). reply scotty79 7 hours agorootparentAs easy as getting yourself 8 H100 and 8 MI300X. Fun weekend project for anybody. reply idiliv 7 hours agorootparentYou can rent them online for ~ 4-5 $ per hour per GPU. Not cheap, but definitely feasible as a weekend project. reply _zoltan_ 6 hours agorootparentwhere can I rent a H100 for 4-5 dollars an hour? AWS doesn't let you use p5 instances (not getting a quota as a private person), lambda cloud is sold out. reply lhl 6 hours agorootparentIt looks like Runpod currently (checked right now) has \"Low\" availability of 8x MI300 SXM (8x$4.89/h), H100 NVL (8x$4.39/h), and H100 (8x$4.69/h) nodes for anyone w/ some time to kill that wants to give the shootout a try. reply darrick_horton 4 hours agorootparentWe'd be happy to provide access to MI300X at TensorWave so you can validate our results! Just shoot us an email or fill out the form on our website reply impulser_ 9 hours agoparentprevIf they used Nvidia's chip would this somehow make the blog post better? reply aurareturn 9 hours agorootparentFor one, they didn't use TensorRT in the test. Also, stuff like this is hard to take the results seriously: * To make an accurate comparison between the systems with different settings of tensor parallelism, we extrapolate throughput for the MI300X by 2. * All inference frameworks are configured to use FP16 compute paths. Enabling FP8 compute is left for future work. They did everything they can to make sure AMD is faster. reply ebalit 5 hours agorootparentYou need 2 H100 to have enough VRAM for the model whereas you need only 1 MI300X. Doubling the total throughput (for all completions) of 1 MI300X to simulate the numbers for a duplicated system is reasonable. They should probably show separately the throughput per completion as the tensor parallelism is often used for that purpose in addition to the doubling the VRAM. reply davidguetta 7 hours agorootparentprevSo they just multipiled their results per 2 ^^ ? reply braiamp 8 hours agorootparentprevI see it as they did everything they can to compare the specific code path. If your workload scales with FP16 but not with tensor cores, then this is the correct way to test. What do you need for LLM inference? reply HPsquared 7 hours agorootparentCouldn't they find a real workload that does this? reply ebalit 5 hours agorootparentvLLM inference of Mixtral in fp16 is a real workload. I guess the details are there because of the different inference engine used. You need the most similar compute tasks to be ran but the compute kernels can't be the same as in the end they need to be ran by a different hardware. reply tgtweak 4 hours agoprevThe market (and selling price) is reflecting the perceived value of nvidia's solution vs AMDs - comprehensively including tooling, software, TCO and managability. Also curious how many companies are dropping that much money on those kind of accelerators just to run 8x 7B param models in parallel... You're also talking about being able to train a 14B model on a single accelerator. I'd be curious to see how \"full-accelerator train and inferrence\" workloads would look ie: Training a 14B param model then inferrence throughput on a 4x14B workload. AMD (and almost every other inferrence claim maker so far... intel and apple specifically) have consistently cherry picked the benchmarks to claim a win over, and ignored the remainder which all show nvidia in the lead - and they've used mid-gen comparison models as many commenters here pointed out in this article. reply fvv 3 hours agoparentmi300x win in some inference workloads, h100 win in training and some others inference workloads ( fp8 inference with tensorRT-llm , rocm is young but is growing fast ) in a single system ( 8x accelerators ) LLMs, mi300x has very competitive inference TCO vs h100 . also : AMD Instinct MI300X Offers The Best Price To Performance on GPT-4 According To Microsoft, Red Team On-Track For 100x Perf/Watt By 2027 https://wccftech.com/amd-instinct-mi300x-best-price-performa... reply fvv 3 hours agoparentprevthe market and the selling price also includes sales strategies, penetrating a sector dominated by a strong player with somewhat \"smart\" sales strategies *1 and with a growing but certainly less mature product ( expecially software ), it requires suitable pricing and allocation strategies 1. https://www.techspot.com/news/102056-nvidia-allegedly-punish... reply fvv 3 hours agorootparentthe price of h100 reflects and reflected the fact that there is a total monopoly in the training sector, amd is successfully attacking the inference sector, increasing its advantage with mi325 and aiming for training from 2025 with mi350 (and Infinity Fabric interconnect and other types of interconnection that are arriving for the various topologies), which will probably have an advantage over blackwell, and then fall back against rubin and come back ahead against mi400, at least, this is what it seems, and as long as the rocm continues to improve. Personally I am happy to see some competition in the sector and especially on open source software reply paulmd 2 hours agorootparentprevThis stuff is the actual reason nvidia is under antitrust investigation. boo boo, a GTX 670 that cost you $399 in 2012 now costs $599 - grow up, do the inflation calculation, and realize you’re being a child. gamers get the best deal on bulk silicon on the planet, R&D subsidized by enterprise, fantastic blue-sky research that takes years for competitors to (not even) match, and it’s still never enough. ”Gamers” have justified every single cliche and stereotype over the last 5 years, absolutely inveterate manbabies. (Hardware Unboxed put out a video today with the headline+caption combo “are gamers entitled”/“are GeForce gpus gross”, and that’s what passes for reasoned discourse among the most popular channels. They’ve been trading segments back and forth with GN that are just absolute “how bad is nvidia” “real bad, but what do you guys think???” tier shit, lmao. https://i.imgur.com/98x0F1H.png this stuff is real shit, nvidia has been leaning on partners to maintain their segmentation, micromanaging shipment release to maintain price levels (cartel behavior), punishing customers and suppliers with “you know what will happen if you cross us”, literally putting it in writing with GPP (big mistake), playing fuck fuck games with not letting the drivers be run in a datacenter, etc. You see how that’s a little different than a gpu going from an inflation-adjusted $570 to $599 over 10 years? (And what’s worse the competition can’t even keep that much, they’re falling off even harder now that Moores law has really kicked the bucket and they have to do architectural work every gen just to make progress, instead of getting free shrinks etc… let alone having to develop software! /gasp) In entirely unrelated news… gigabyte suddenly has a 4070 ti super with a blower cooler. Oh, and it’s single-slot with end-fire power connector. All three forbidden features at once - very subtle, extremely law-abiding. https://videocardz.com/newz/gigabyte-unveils-geforce-rtx-407... and literally gamers can’t help but think this whole ftc case is all about themselves anyway… reply talldayo 1 hour agorootparent> this stuff is real shit, nvidia has been leaning on partners to maintain their segmentation Really? As far as I can tell, their partners can't be bothered to care besides EVGA. And in EVGA's case, they were reliant on Nvidia's product for the bulk of their revenue. Outside GPU OEMs, I don't see how Nvidia is unfairly manipulating their partners: - Tesla used and successfully replaced Nvidia's own edge-compute hardware in their cars - Nintendo buys Nvidia hardware but doesn't ship Nvidia-specific features in their games - Google and other cloud retailers sell Nvidia rackspace but also offer competing TPU hardware - OpenAI and Facebook both train models using vendor-agnostic frameworks (Triton and Pytorch) From a business strategy perspective they're pretty much unimpeachable. Annoying sure, but any business that tries to disrupt a stagnant market is going to be considered annoying. The closest thing to a \"partner\" Nvidia has that's enforcing market segmentation is Apple, who refuses to sign MacOS drivers for Nvidia hardware. Besides them, nobody is really forcibly limiting how or when you can run Nvidia software. > You see how that’s a little different than a gpu going from an inflation-adjusted $570 to $599 over 10 years? I see that as software-backed hardware appreciation. If any other manufacturer was committed in the same way Nvidia was, they would also see their 2012 cards go up in value; my GTX 1050 Ti is from 2014 and still runs as good as the day I bought it. If you need someone to be angry at, go shake your fist at Khronos or Apple for not fixing the problem they both created for themselves. Nvidia is just sitting pretty and raking in the cash both of them left on the table when they decided to go knives-out. reply fvv 2 hours agoparentprevmi300x production is ramping , in latest earning report lisa su said 1H2024 is production capped , 2h2024 have increased production ( and still have some to sell ), thanks probably to cowos and hbm3/(e?) supply improved large orders for those accelerators are placed months ahead meanwhile mi300x on microsoft are fully booked... https://techcommunity.microsoft.com/t5/azure-high-performanc... \"Scalable AI infrastructure running the capable OpenAI models These VMs, and the software that powers them, were purpose-built for our own Azure AI services production workloads. We have already optimized the most capable natural language model in the world, GPT-4 Turbo, for these VMs. ND MI300X v5 VMs offer leading cost performance for popular OpenAI and open-source models.\" reply qeternity 9 hours agoprevWhy the hell are we doing 128 input token benchmarks in 2024. This is not representative of most workloads, and prefill perf is incredibly important. reply ta12653421 8 hours agoparentFor understanding: What would be a suitable input length in your oppinion? And why isnt this a good one: Are real-life queries shorter? Or longer? If i count one word as a token, then in my case most of the queries are less than 128 words. reply qeternity 7 hours agorootparentI think today 512 tokens is a minimum. It's not just the query (if you're running a chatbot, which many of us are not). It's the entire context window. It's not uncommon to have a system prompt that is > 512 tokens alone. I would like to see benchmarks for 512, 1024, 4096 and 8192 token inputs. reply Gasp0de 7 hours agorootparentprevIncluding the initialization prompt and your history if you have one? I use ChatGPT for a very simple task, to map chat messages to one of 5 supported function calls, and the function definitions alone already take up 200 tokens I think reply stefs 7 hours agorootparentprevIt's not just the current prompt, but the whole conversation, if possible. Or, if you want the AI to summarise an article, the article has to fit in. If I understood that correctly, context length is something like session storage or short term memory. If it's too small the AI starts to forget what it's talking about. reply rfoo 6 hours agorootparentprevIMO the relevant benchmark for now is a mixed stream of requests with 50 (20%), 500 (50%), 2000 (10%) and 50k (20%) input tokens, ignore EOS and decode until you get around 300 output tokens. reply SuchAnonMuchWow 4 hours agorootparentI'm really interested, do you have a source for those percentages ? I tried to look for some service provider to publish this kind of metrics, but haven't found any. reply spacecadet 8 hours agorootparentprevIn most cases thats not enough reply sva_ 9 hours agoprevI try to be optimistic about this. Competition is absolutely needed in this space - $NVDA market cap is insane right now, about $0.6 trillion more than the entire Frankfurt Stock Exchange. reply Rinzler89 9 hours agoparentIt's more how little the Frankfurt stock Exchange is worth. And European devs keep wondering why our wages are lower than in the US for the same work. That's why. reply CapeTheory 9 hours agorootparentThe DAX is only 40 companies, most of which make real products rather than advertising mechanisms. Making real physical things just doesn't scale, and never will. While I would enjoy a US tech salary, I'm not sure we want a world where all manufacturing is set aside to focus on the attention economy. Nvidia value deserves to be much higher than any company on the DAX (maybe all of them together, as it currently is) - but how much of that current value is real rather than an AI speculation bubble? reply mejutoco 42 minutes agorootparent> Making real physical things just doesn't scale, and never will. Nvidia sells physical things, and they are bigger than 40 companies because the companies are selling physical things? I am not arguing hardware scales better than software but this is a strange argument in this context. reply B1FF_PSUVM 8 hours agorootparentprev> Making real physical things just doesn't scale, Nvidia sells chips ... reply chad1n 8 hours agorootparentInvestors don't even know what NVIDIA is selling, I was listening to a random investor podcast and they were talking about Intel, AMD and NVIDIA, but no one knew what exactly they are selling, they only knew they are part of this AI bubble so that's why you should invest in them reply petra 6 hours agorootparentWhat about seroous analysts, say like morningstar, don't the undedtand the business that they recommended relatively well? reply kuschku 7 hours agorootparentprevHow much of Nvidia's value do you think comes from the chips? Chips are a commodity, AMD's chips have been better and cheaper than Nvidia for many years. The reason Nvidia's value has been so inflated is the software stack and the lock-in they offer. CUDA, CuDNN, that's where Nvidia's value lies. And obviously, now that all relevant ML frameworks are designed for Nvidia's software stack, Nvidia has a monopoly on the supply. That's why their value is being inflated so much. And Nvidia doesn't have produce the chips themselves, that's all contracted out as well. reply dukeyukey 7 hours agorootparentprevNvidia is a fabless chip designer. They design chips and software to go with the chips, but contract out the actual manufacturing. reply zzbn00 6 hours agorootparentYes, they buy these chips from fabs (made to their design) but it is the chips they then sell on, at about a 4x markup to what they bought them for. Good business model! Buy low, sell high. Whether good enough to justify current valuation is another question. reply CapeTheory 7 hours agorootparentprevThey can't make them fast enough to satisfy hype-driven demand; they are not scaling like a tech company, but their market cap is being inflated like one. reply lucianbr 6 hours agorootparentWhat is the meaning of \"tech\" in this context? \"Software\"? I mean, if Nvidia isn't a technology company... reply CapeTheory 49 minutes agorootparentSorry should have said \"big tech\", as commonly used to refer to FAANG companies who focus more on software and/or services. reply chollida1 7 hours agorootparentprev> but how much of that current value is real rather than an AI speculation bubble? I mean, by definition given that it trades freely their market cap is real. Your market cap today is what the market thinks your future cash flows are worth. The bubble and the bubble popping should in theory both be priced into Nvidia's market cap. What isnt' is events the market doesn't anticipate, AMD coming out with a current generation chip that can do inference as well as the H100 is something the market hasn't priced in. Andy our manufacturing example is very poor as NVidia is certainly part of the manufacturing pipe line by designing physical products that people buy. reply rybosworld 4 hours agorootparent> Your market cap today is what the market thinks your future cash flows are worth. The bubble and the bubble popping should in theory both be priced into insert_financial_product_here This is a bit of a tired viewpoint, and is evidently proven not true time after time. The collective despair/euphoria of market participants is extremely powerful and well documented, at least as far back as dutch tulips. Stock valuations are relative, and they are relatively misvalued most of the time. That's why there are (albeit rare) funds that are capable of outperforming the market for decades - Berkshire, and Medallion for example. It's certainly possible that AMD is valued (almost) fairly. It's just as likely that it's relatively misvalued for no reason other than emotions (lack of hype). reply mewpmewp2 6 hours agorootparentprev> can do inference as well as the H100 is something the market hasn't priced in. I think probability of that would still be priced in. Not sure what the exact probability is, though. But if say it was clear that AMD can come up with a competitive option, then NVDA stock would drop. But if it was clear the other way that AMD can't do it, NVDA price would increase. reply gizmo 8 hours agorootparentprev> The DAX is only 40 companies, most of which make real products rather than advertising mechanisms This, as the kids say, is just cope. American big tech makes real products. Google is not just ads. Apple is not. Amazon is not. Tesla is not. NVidia is not. Netflix is not. NVidia might be overvalued because of the current AI hype but that does not diminish their real accomplishments! Europe has almost no real tech companies. There is one exception, founded in 1984. Not exactly a spring chicken. How can a wealthy continent with 750 million people produce no big tech companies? It's a big problem. reply CapeTheory 7 hours agorootparent> Google is not just ads. Bad example given how aggressively they terminate products which don't generate the same revenue as ads. > Apple is not. Best example, they have done a fantastic job of being both a tech company and pseudo-fashion company. > Amazon is not. They don't make anything (at least nothing people want to buy) and have ad revenue as an increase slice of their pie. > Tesla is not. Even bigger hype/speculation vehicle than Nvidia. > NVidia is not. Nvidia of 5 years ago would not have appeared on this list, being too much of a niche tech company. Good at what they do, but hugely hype-fuelled. > Netflix is not. Running out of growth potential with their current business model, starting to introduce ads! reply Certhas 7 hours agorootparentNvidia's valuation is in part driven by the fact that their chips potentially disrupt the vehicle for ad revenue that is search. For all his insanity, the one thing I respect Musk for, is that he actually started successful companies that make stuff. Creating a new car manufacturer of the scale of BMW out of nothing was widely considered impossible before. Of course he did this from a position of extreme wealth, but none of his peers managed to do that. Everyone else is just seeking rent by trying to be first to implement some tech transition that is coming anyway. And that might be a lot more valuable to society if it was managed differently... reply erulabs 3 hours agorootparentprevAmazon doesn’t make anything anyone wants to buy? Not to be snarky, but if AWS counts as “nothing” I’d sure like a slice of nothing please. reply CapeTheory 47 minutes agorootparentAWS is a (collection of) service(s) which can scale and have low marginal cost of reproduction. Fire Tablets are a real thing you can buy, but they are shit so nobody does. reply jononor 8 hours agorootparentprevWhy is producing big companies a goal? High standards of living for all seems to a much better goal. And that can be done with small or big companies - so long as economic production is high enough and distributed well enough. reply gizmo 7 hours agorootparentBecause tech innovation requires tons of R&D and you can't afford to do that otherwise. Europeans use American laptops running an American operating system to watch American movies in an American browser. European economic production is nowhere near high enough and now Europe is struggling to provide for its aging population and doesn't have enough good jobs for younger people. I support redistribution generally, but the wealth has to be created first or there won't be anything to redistribute. reply jononor 6 hours agorootparentTo be fair more than half of that laptop hardware is made in China+Taiwan, including a lot of the IP that goes into it. If you look at phones and tablets, there is a bunch of components/IP from European companies also, such as ARM, Bosch, STmicroelectronics, Infineon, NXP etc. Intel has famously struggled and failed multiple times to get into that market. European semiconductor companies are also strong in automotive and other industries that use modern embedded systems. In another consumer electronics niche, a majority of wireless mice and keyboards are build on Nordic Semiconductor chips - from tiny Norway. reply Rinzler89 5 hours agorootparent>If you look at phones and tablets, there is a bunch of components/IP from European companies also, such as ARM, Bosch, STmicroelectronics, Infineon, NXP etc. But those are all low-marin chips. Qualcomm, Nvidia, Intel, AMD and Apple have much higher margins on their chips. They don't bother competing with the EU chips companies. reply gizmo 6 hours agorootparentprevThat's all true. However, the US is capable of producing almost everything domestically, albeit in lower volume. The US military doesn't like to depend on Chinese chips for obvious reasons. reply bigfudge 6 hours agorootparentprevAnd yet standards of living in Europe are comparable to those in the US, and preferable at the median. Our attention is captured by speculative valuations of unicorns, and yet people actually need real stuff made, drugs developed and made etc. Europe does perfectly well in many non winner takes all sectors where English language and network effects are less relevant. The political instability created by the US neoliberal experiment is something I hope we can avoid over here too. reply gizmo 6 hours agorootparentEurope is much much poorer than the US, actually. https://pbs.twimg.com/media/F3PGpsrWEAEiplB?format=jpg&name=... reply Certhas 5 hours agorootparentEU GDP per capita 2022 is the same as US GDP per capita 2017. Unless you want to say that the US was much poorer in 2017 than it was in 2022 that's a fairly ridiculous statement. Also, the highest productivity places in the EU have much lower hours worked per capita than the US, with Germans on average working 25% less than Americans and the EU as a whole working 13% less than the US. https://data.oecd.org/emp/hours-worked.htm reply gizmo 5 hours agorootparentWorld Bank: European Union gdp per capita for 2022 was $37,433, a 3.33% decline from 2021. U.S. gdp per capita for 2022 was $76,330, a 8.7% increase from 2021. It's not even close? reply Certhas 3 hours agorootparentSources please? Are these nominal dollars? World bank data in PPP dollars is reported as 64,600 vs 45,900 here: https://ourworldindata.org/grapher/gdp-per-capita-worldbank?... Germany is at 53,900 there, but a good chunk of the difference is simply that US works more per capita. GDP per hour worked is 74$ in the US vs 69$ in Germany, 53$ in Canada. Sweden is ahead of the US. And the EU also includes countries like Bulgaria, which at 29$ is barely ahead of Russias 28$. https://data.oecd.org/lprdty/gdp-per-hour-worked.htm France is at 65$ per hour worked, but Germany and France also have significantly lower poverty and inequality rates by any measure you chose, with France more equal than Germany. The US, of course, remains the dominant economy of the world by any measure. There is no question of that. But the exponential nature of economics, and the structural differences between these different economies, means that GDP numbers compared directly are fairly meaningless. Edit: That last sentence is too strong as stated. GDP obviously matters a big deal in the grand scheme of things, especially as you jump from lower or middle income to high income countries. But it's all logscale. A factor of 2 is a big deal, a factor of 1.2 might not be. reply Rinzler89 5 hours agorootparentprevGDP per capita as a stand alone metric doesn't mean much. reply gizmo 5 hours agorootparentAll rich countries have high GDP per capita and all poor countries have low GDP per capita. Zero exceptions. Despite the shortcomings of GDP as a metric it still tracks prosperity very accurately. reply Rinzler89 4 hours agorootparentYou can't get any more obvious than that, but that wasn't my point to say that higher GDP doesn't make you richer than a low GDP, but to say GDP/capita as a number alone is not a measure of wealth, income or prosperity between countries, even in the EU. For example Ireland has by a long margin the highest GDP/capita in the whole EU, and it would make you think the average Irish worker earns more that any other worker in the EU and drives a Lambo, but that's not what's happening. It's because most US corporations funnel their EU money through their Irish holding companies skewing the statistic. reply gizmo 3 hours agorootparent> For example Ireland has by a long margin the highest GDP/capita in the whole EU No, Luxembourg does. reply kortilla 5 hours agorootparentprevYes, if you had 2017 salary in the US today you are much poorer. Inflation was no joke reply Certhas 3 hours agorootparentThese numbers are PPP, so inflation adjusted. reply Certhas 6 hours agorootparentprev.... while all of the content is hosted on Linux servers, and you're listening to music through a Swedish app. That is running on silicone made in Taiwan. Using equipment that can currently only be manufactured in Belgium and Germany. On a Mac you are using a British instruction set. Also in terms of tech innovation: What part of the US-based tech innovation couldn't have been (and actually were) achieved with open-source solutions many many years earlier for a fraction of the cost, if we didn't have copyright? Honestly, a significant chunk of the \"innovation\" seems to relate directly to maximizing advertisement opportunities and inducing increased consumption. Who cares if a website takes a second to load rather than 0.1 seconds? If it has content I want, 1 second isn't a big deal. If I don't care about the content, I lose nothing by being distracted by something else in that 1 second. --- More importantly: European Economic production isn't high enough... by what standard? https://data.oecd.org/lprdty/gdp-per-hour-worked.htm GDP per hour worked is 74 in the US vs 69 in Germany and 54 in the EU. And the EU includes many large countries that emerged from communist dictatorship only 35 years ago, and are very much still in the process of catching up. Incidentally, the German economy is the result of the West German economy with 63 million people absorbing a failing economy hosting 16 million people in 1990. The idea that the US is some promised land of economic prosperity while Europe is falling is entirely absurd. It's a narrative built on small relative differences and a US system that pressures people into working a lot more than Europeans do. More importantly, even GDP per Capita wise: https://data.oecd.org/gdp/gross-domestic-product-gdp.htm EU per Capita GDP in 2022 is the same as USA 2016. Was the USA in 2016 struggling but now isn't? This is all bullshit. Economic output is more than high enough and rising steadily. The problem remains solely in the distribution of reply kortilla 5 hours agorootparent>EU per Capita GDP in 2022 is the same as USA 2016. Was the USA in 2016 struggling but now isn't? Absolutely the US would be struggling if the GDP was still 2016 values with today’s costs. reply Rinzler89 4 hours agorootparentPretty much. I'd also love it (European) if we stil had 2017's rent prices and consumer prices, but we don't. Housing, energy, food, healthcare and everything has gone up like crazy. We're definitely poorer than before, just sweeping it under the rug pretending everyting's fine. So it boggles my mind that your parent tried to make a point by equating USA 2016 with EU 2022 GDP/capita as if nothing's wrong with that. Are some people that oblivious? reply Certhas 5 minutes agorootparentThe 2017 number was a mistake by me, I wanted to look at inflation adjusted/PPP, and that was nominal. You can clearly see the massive effect of the Russian invasion of Ukraine and the resulting inflation in the data. If we were looking at nominal values, the opposite is the case: Nominally GDP is rising rapidly: Nominal: https://data.oecd.org/gdp/gross-domestic-product-gdp.htm PPP/Inflation adjusted not so much: https://ourworldindata.org/grapher/gdp-per-capita-worldbank?... But pretty much all countries are still well ahead of where we were in 2017. If you feel poorer than in 2017 it's because you're getting less of a larger pie, not because the economy is producing less than it did then. dukeyukey 7 hours agorootparentprevBig companies means efficiencies of scale. Small companies that succeed and grow inevitably become big companies. If they don't, it means the qualities that make them effective don't scale to the rest of the economy. reply jononor 7 hours agorootparentThey become big but not necessarily huge - like the 10 biggest tech companies that people here put as the benchmark. For that one needs organizations that also continuously increases their scope - going into new markets, consolidating exiting markets, buying up existing players. And if they are to continue being \"European\" then they must resist being bought up by the huge US or global tech companies. The latter is a big problem at least here in Norway. We have some companies that grow quite big and successful - but it is generally just a question of time before they get swallowed by a huge corp, often with US headquarters. Example: Atmel, now Microchip reply dukeyukey 6 hours agorootparentYep, that's a huge problem, and one that isn't easy to fix. The European market is vastly more gragmented than the North American one, so even without Europe's penchant for taxes and regulation, North America can more easily get giant companies that can reach across the Atlantic. The only real answer is protectionism, and there's a good chance that'll hurt more than it helps. reply Rinzler89 8 hours agorootparentprevBecause that famous EU welfare is funded via taxes. Having well performing companies funds your welfare system. Currently EU welfare systems are under massive strain and huge waiting lists due to ageing population and economy that hasn't kept up to fund it. There's no free lunch here. You need big companies with scale that pay huge wages as those mean a lot more tax revenue. Saying no to that kind money out of some made up idealism is just silly copium. The EU income taxes paid by a single FANG salary employee would be the equivalent of the taxes paid by ~10 average workers. Pretty sure Germany and every other EU country would like to have such taxpayers contributing into the welfare system and not say no to it. Europe's share of global GDP gas been on a constant decline at the expense of US and Chinese growth. Yeah it's nice to have a better welfare system than China or the US, but how will you fund it in the future if you keep having less money? Political idealism doesn't pay your food and rent. reply lycopodiopsida 7 hours agorootparentnext [7 more] [flagged] ericd 5 hours agorootparentEven if a company doesn’t pay taxes, its masses of highly paid workers do. Each of those SWEs making $300k+ are paying more in taxes than the entire earnings of the average EU dev. reply lycopodiopsida 2 hours agorootparentAha, the cries of the once well-payed SWE laid off and replaced with some cheap hire oversea come up at least once a week for a year or two already. The funny thing about transnationals - they do not care about the society they operate in. But what about the rest, not these lucky SWE who had a good run for the last 10-15 years? How is it going, education, medicine, crime, inequality? All is splendid, I assume? reply Rinzler89 5 hours agorootparentprevI still don't get why that's so complicated to understand that more well paid workers = more taxes for the state, and some are vehemently expecting a \"source\" for this. reply gizmo 5 hours agorootparentprevYes, this is why European politicians are bending over backwards to get American tech CEOs to open offices in their countries. reply Rinzler89 7 hours agorootparentprevnext [3 more] [flagged] lycopodiopsida 6 hours agorootparentStop trolling and throwing around wild accusations. Here are your words: >You need big companies with scale that pay huge wages as those mean a lot more tax revenue. Now provide proofs that we need some big corps dodging taxes, including FAANG, and not more small and middle-sized business. reply Rinzler89 5 hours agorootparentNo reply leonardovida 7 hours agorootparentprev> There is one exception, founded in 1984 Europe clearly has many problems stimulating investments and creating a competitive environment for startups and tech companies. But you can't say ASML is the only one \"real\" tech company. What about Adyen, Spotify, Klarna, N26, Revolut, etc? reply Rinzler89 7 hours agorootparentMost of those companies you mentioned aren't anywhere near as wealthy or as high market caps as US big-tech. Most of them are just payment middlemen not some innovative product nobody else can do, and Spotify survives on monopolizing and squeezing artists, not some innovative product. Kind of like Netflix except Netflix has some cutting edge streaming tech as a product not just IP licenses. ASML is the only product innovator there except their innovative EUV lightsources are licensed from Sandia labs in the US and made by Cymer in the US which ASML bought and licensed to not seel to China. So an US invention at the end of the day. reply dukeyukey 6 hours agorootparentWe aren't exclusively talking about big tech here, just tech. Companies like Monzo dominate the local markets because they executed on ideas no-one else tried before. We forgot that huge, international tech companies are pretty much an exclusive American phenomenon, they don't really exist anywhere else. reply qeternity 7 hours agorootparentprevN26, Klarna and Revolut are not tech companies. Neobanks are still banks. And Klarna is just modern store credit cards. These have been around for decades. Adyen is very underrated, and Spotify is definitely tech. Stripe should be on the list. DeepMind at one point. reply gizmo 6 hours agorootparentStripe was founded in California. It's an American business that focused exclusively on the American domestic market in their first years of operation. Many tech companies in the US are founded by immigrants from Europe and elsewhere. That the Collisons chose to start their business in the States is no coincidence. reply Rinzler89 4 hours agorootparentMany EU start-ups choose to launch their product in the US first due to the 300+ million people market speaking only English, rather than bother with the tiny fragmented markets at home. It's just much cheaper and easier for start-ups if you're developing a SW product to sell it in the US market first and only when you've made money there, slowly bring it in the EU. Starting off SW products in the EU is suicide (unless you're targeting some niche in the local market that's safe from competitors from abroad because it ties into some local idiosyncrasies on language, culture and law). reply dukeyukey 5 hours agorootparentprevRevolut is as much a tech company as Netflix or Stripe are - using modern software to fix an old problem. reply Rinzler89 8 hours agorootparentprev>How can a wealthy continent with 750 million people produce no big tech companies? It's a big problem. Much more difficult to scale a product across 26 different countries and nearly as many languages and regulatory jurisdictions. US is one country, not a collection of countries fighting each other, meaning your product is instantly available to 300M people speaking the same language under (nearly) the same regulations. reply qeternity 7 hours agorootparent>US is one country, not a collection of countries fighting each other The US is a republic of 50 states. Each state has a huge amount of sovereignty and autonomy. There are 50 state-level regulatory jurisdictions. Not to mention the local-level of government. But in spite of this, the US does not over-regulate. This is the big difference to Europe (I say this as an American expat living in Europe). reply dukeyukey 6 hours agorootparentAmerican states have generally less autonomy than Canadian provinces do. Federalism is common worldwide, it's far from an American thing. EU countries are _vastly_ more independent than American states, and when you throw in the cultural differences, it grows hugely again. reply Rinzler89 6 hours agorootparentprevHave you tried launching a product in your new EU country and then taking abroad to another EU country? You'll find out it's not exactly like doing the same thing between US states. And I'm not even talking about the language barrier. reply pell 7 hours agorootparentprevLanguage aside, the entire point of the EU is the single market so you don’t have 26 different rule sets. (There are some exceptions such as health care but that is no different in the US.) reply Rinzler89 7 hours agorootparentExcept you do have 26 different rules. It's a single market on paper as the eu only mandates a small subset of common rules and regulations such as removing tarrifs or freedom of movement, but have you ever tried in practice to launch your company from Belgium to France or from Netherlands to Belgium or from Austria to Germany, or from Romania to Italy? It's much more difficult when the rubber hits the road as every country has various extra laws and protectionist measures in place to protect it's domestic players from outsiders even if they came from within the EU. And that's besides the language barrier which means added costs. This is much less efficient than the US market. EU countries and voters still value their national sovereignty and culture (both with the upsides and downsides) above a united EU under the same laws and language for everyone, ruled from outside their country's borders. See what happened with Brexit and the constant internal squabbling and sabotaging over critical EU issues that affect us all like the war in Ukraine or illegal mass migration. An US style unification just won't work here since every little country wants to be it's own king while having its cake and eating it too. reply gizmo 6 hours agorootparentEurope is a single market with regards to imports and exports, and that's what matters most for a business. Low wages more than compensate for regulatory annoyances. No shortage of subsidies either. California has more burdensome regulations and higher taxes than other states and yet it's home to silicon valley. reply Rinzler89 5 hours agorootparent>Europe is a single market with regards to imports and exports, and that's what matters most for a business. We're talking about scaling internal companies across EU, not about imports and exports. And scaling local start-up across the EU is a regulatory and legal nightmare for small companies. Shipping and selling imports and exports of commodities are a solved problem for decades, but scaling a on-line notary service for instance, that works both in Germany and in Italy, isn't. The EU doesn't help much with that as they only say you should have no tariffs between each other, not that you shouldn't have various legal, cultural and bureaucratic protectionism idiosyncrasies in place. The EU won't and can't force countries to improve that to make doing business easier for cross-country start-ups. EU countries have a lot more roadblocks between each others than US states do when ti comes to scaling businesses. reply dukeyukey 7 hours agorootparentprevYes, and it's doing a mediocre-at-best job of it. You can remove tariffs and harmonise regulations, but the EU is trying to unite countries with cultural borders older than Christianity. reply pell 6 hours agorootparentI would argue that the EU is quite a successful organization given the task of setting up general market rules for 26 countries. Obviously there is a lot to criticize about the EU and I can offer you a gigantic list there too. However, I do not see any clear failure of the EU’s approach as a single market so far. Additionally part of the philosophy was establishing peace in a region that was torn up by wars for a lot longer than Christianity exists. I would argue the EU was quite successful there too. reply dukeyukey 6 hours agorootparentIt's definitely successful, and I was probably too harsh there. But I genuinely think the barriers that are left are damn near insurmountable. An awful lot has to change before a Greek tech workers can move to Sweden as easily as a Virginian can move to California. reply pell 6 hours agorootparentWhat would you change if you could? The EU already offers freedom of movement to EU citizens. Of course the US being a country instead of a connection of countries offers a more streamlined experience and surely the shared language plays a huge role too. But when I want to come up with examples such as a Greek person having completely different retirement, health care and legal schemes in Sweden compared to Greece, it seems the US is not so dissimilar there either given that states sometimes have very different approaches to health care, taxes, labor laws, etc. reply dukeyukey 6 hours agorootparentMostly stuff you can't really change. Like, the cultural difference between Sweden and Greece is _way_ bigger than between California and Virginia. You've got the language barrier too. This isn't gonna be fixed, maybe ever, but will continue to cause friction. reply Hikikomori 2 hours agorootparentprevI had two Greeks on my team at a large tech company in Sweden. reply dukeyukey 7 hours agorootparentprev> Europe has almost no real tech companies How do you define \"tech\"? Europe's domestic markets are jam-packed full of local tech companies. reply gizmo 7 hours agorootparentThe grandparent claimed that the DAX 40 has real businesses that make things as opposed to American tech businesses that just serve the attention economy. Clearly not true. European businesses run on American technology. American businesses do not run on European technology. Europe has many small and not very profitable tech companies. Almost no large and profitable ones. https://pbs.twimg.com/media/GNDtCtTXcAAiwFk?format=jpg&name=... reply braiamp 8 hours agorootparentprevWages is a proxy of how valuable your work is, but not a measure of how value your work is. To support a high salary something has to happen, either the product sold is very expensive or it's being subsidized by investors. No company can pay its employees above what they are able to generate selling the product they worked on indefinitely. reply dailykoder 8 hours agorootparentprevOkay, so you are saying I should move to america, where apparently a lot of people struggle hard to even get a job? Nah, then ill get my very good wagie pennies here and have plenty jobs available, plus good health insurrance and whatnot. reply dash2 8 hours agorootparentGerman unemployment is 3.2%. US unemployment is 4.0%. Neither of these are at all high by historical standards. https://www.bls.gov/news.release/empsit.nr0.htm https://www.destatis.de/EN/Press/2024/06/PE24_217_132.html reply jangxx 7 hours agorootparentDon't forget to take both of those stats with a grain of salt though. The US has a lot of gig workers which are not always counted correctly or the same and Germany has a large low-wage sector, where people are employed but earn less per month that they would get in unemployment benefits, and so the state pays the difference. reply dailykoder 8 hours agorootparentprevSure, and I can only judge from what I'm hearing, but the internet does make it sounds like it's very hard to find programming jobs as a graduate or junior. I don't know what the truth is. Maybe it's just a minority crying out loud because they don't get hired at FAANG. reply ptero 8 hours agorootparentprevThat is a perfectly reasonable approach. On the other side, working in the tech on the US salary, if not blown on luxurious lifestyle, gives one a good chance of quickly reaching financial semi-independence (say, 3 years of full living expenses in the bank), which can be a powerful feature. My 2c. reply drawback 7 hours agorootparentOr do both, work for a US company remotely. Most large ones even have an entity in the larger EU countries that can employ you with all the benefits of being an EU employee. They might not pay the exact same as in the US but still considerably above market rate. reply Rinzler89 7 hours agorootparentprev>Okay, so you are saying I should move to america Please stop breaking HN rules. I never said that. HN rules state you need to reply to the strongest interpretation of someone's argument, not the weakest that's easiest to criticize. I just pointed out once country's economics performance for comparison, if you're want to extrapolate from that that you should move there, that's your issues to deal with, but not my argument. reply Refusing23 7 hours agorootparentprevstock value doesnt reflect a company's income or ability to pay their workers reply raverbashing 9 hours agorootparentprevYes But there's a long list of German companies not on the DAX (though Germany DAX really deserves to be worth less than NVidia) reply earthnail 8 hours agorootparentThe DAX is made of the 40 most valuable German companies. That’s how it is defined. So the companies not in it, again by definition, matter less. reply pell 7 hours agorootparent> The DAX is made of the 40 most valuable German companies. Not to be too nitpicky here but these are only the publicly traded companies. You have a number of pretty large German companies that are still entirely private such as Aldi, Schwarz Group, Boehringer or Bosch. reply bigfudge 6 hours agorootparentAlso many medium sized companies which are productive and competitive but not public, so never grow to huge sizes. I view this as a feature not a bug… smaller companies have a more direct connection with their workforce and tend to behave better with them. reply dukeyukey 7 hours agorootparentprevNo it isn't? It's the list of the biggest blue-chip Germany-HQ'd companies trading on the Frankfurt Stock Exchange. If you're a German company with exclusively German employees trading in London, or Paris, or New York, you don't qualify. reply qsi 8 hours agorootparentprevThe CDAX index has about 360 companies and appears to have a market cap of around 2 trillion EUR vs 1.7 trillion for the DAX 40. reply raverbashing 7 hours agorootparentprev> DAX is made of the 40 most valuable *listed* German companies https://www.famcap.com/top-500-german-family-businesses-the-... Not all of those are listed, or listed in Frankfurt reply littlecranky67 6 hours agoparentprevFrankfurt Stock Exchange or the DAX is mostly irrelevant. Germany has a strong, family-owned Mittelstand, those companies are not publicly traded and thus not listed. Plus, we have some giants that are also not publicly listed but belong to the richest Germans (Lidl, Aldi of discount groceries, but also automotive OEM Bosch). reply sva_ 5 hours agoparentprevOof, I really didn't intend to start a flamewar. reply threeseed 8 hours agoparentprevWe are in the middle of an LLM bubble. Nvidia problem will sort itself out naturally in the coming months/years. reply Rinzler89 8 hours agorootparentSame thing was said about Nvidia's crypto bubbles, and then look what happened. Jensen isn't stupid. He's making accelerators for anything so that they'll be ready to catch the next bubble that depends on crazy compute power that can't be done efficiently on CPUs. They're so far the only semi company beating Moore's law by a large margin due to their clever scaling tech while everyone else is like \"hey look our new product is 15% more efficient and 15% more IPC than the one we launched 3 years ago\". They may be overvalued now but they definitely won't crash back to their \"just gaming GPUs\" days. reply viraptor 8 hours agorootparentThey got extremely lucky with AI following crypto. The timing was close to perfect. I'm not sure there will be another wave like that at all for a long while. reply Rinzler89 8 hours agorootparentMaybe but it's not like all those AI compute units or whatever Nvidia called them will be thrown in the dumpster after the AI bubble pops. There's a lot of problems the can be solved on them and researcher are always looking for new problems to solve as compute becomes accesibile. I'm tired of hearing about Nvidia's \"luck\". There was no luck involved. Nvidia shiped Cuda on consumer GPUs since 2006. That's almost 20 years time researchers had to find used cases for that compute and Nvidia made it possible. In other words the AI bubble happened because Nvidia made the necessary ground work for it to happen, they didn't just fall into it by luck. reply viraptor 7 hours agorootparentThey will not be thrown in the dumpster, but that's actually a bad thing for NVIDIA. We had a very short period when lots of miners dumped their RTX cards on ebay and the prices fell a lot for some time. (then AI on RTX became a thing at small scales) When the A100/H100s get replaced, they will flood the market. There's many millions of $ stuck in those assets right now and in a few years they will dominate research and top end of hobbies. Only high profile companies/universities/researchers will look at buying anything newer. Maybe NVIDIA can do 2 generations of those cards, but ASIC-based solutions will hit the market and the generic CUDA will become a problem rather than a blessing. Same story as BTC miners and graphics cards. Sure, they didn't get lucky with the tech they had to offer - that was well developed for years. They just got lucky that the next big thing was compute-based. If the next thing is memory/storage-based, they're screwed and the compute market is saturated for years - they have only gamers left. reply sumedh 5 hours agorootparent> they're screwed and the compute market is saturated for years If all this extra computing power is available, smart people will find a way to use it somehow. reply jononor 7 hours agorootparentprevThere is no recurring revenue on them though. So NVIDIA needs to continue selling obscene amount of new chips. reply jononor 8 hours agorootparentprevThere could be other \"AI\" waves after LLM. And we have still not hit the self-diving car wave, that could happen the next 20 years (or 40). And neither general-purpose robots, that couuuuld also happen. Personalized medicine has also not happened yet. Nor virtual reality, which might take off one day (or not). There are still many industries that could go big in terms of computational demands. reply sva_ 5 hours agorootparentprevIt wasn't a coincidence either though: the amount of compute available is probably the main driver of this wave. reply yobbo 7 hours agorootparentprevIntel's all time high was in 2000, if I'm reading the charts correctly. Of course the equivalent can happen to Nvidia. Seems almost certain. reply mistermann 7 hours agorootparentprevI think there's also a very high prospect of virtual worlds with virtual people (SFW or otherwise) becoming popular, rendered with Apple/META goggles...that could require insane amounts of compute. And this is just one possibility. Relatively cheap multimodal smart glasses you wear when out and around that offload compute to the cloud are another. Nvidia could just as easily triple in short order as get cut in half from here imho. reply pquki4 6 hours agorootparentI thought Meta Horizon and the sales number of Vision Pro[0][1] already proves your thesis wrong. Even Zuckerberg stopped talking about it. [0]https://www.macrumors.com/2024/04/23/apple-cuts-vision-pro-s... [1] https://www.macrumors.com/2024/04/22/apple-vision-pro-custom... reply mistermann 6 hours agorootparentI am referring to the future (the actual future, not simulated ones), so it is not possible to know if I am wrong. I predict this is yet another domain rich with opportunity for AI. reply chx 5 hours agorootparentprevAs someone put it in: we are in the 3D glasses phase of AI. Remember when all TVs came with one? reply CarRamrod 8 hours agoparentprevThose are rookie numbers reply mistymountains 3 hours agoprevI’m a AI Scientist and train a lot of models. Personally I think AMD is undervalued relative to Nvidia. No, chips aren’t as fast as Nvidia’s latest and yes, there are some hoops to get things working. But for most workloads in most industries (ignoring for the moment that AI is likely a poor use of capital), it will be much more cost effective and achieve about the same results. reply michaelnny 9 hours agoprevI'm wondering if the tensor parallel settings have any impact on the performance. My naive guess is yes but not sure. According to the article: \"\"\" AMD Configuration: Tensor parallelism set to 1 (tp=1), since we can fit the entire model Mixtral 8x7B in a single MI300X’s 192GB of VRAM. NVIDIA Configuration: Tensor parallelism set to 2 (tp=2), which is required to fit Mixtral 8x7B in two H100’s 80GB VRAM. \"\"\" reply renonce 7 hours agoparentI personally find such comparisons unfair. A good comparison should optimize for each device configuration, which means use a model within the VRAM limit and quantize to 8 bits where it boosts performance etc and avoid shortcomings of both devices unless necessary. reply lccerina 9 hours agoprevWithout proper statistical metrics (why use average when 95% percentile is widely used?) and performance/watt this is a useless comparison. reply DrNosferatu 6 hours agoparentAnd performance/price -> that's the bottom line. reply whereismyacc 8 hours agoparentprevaverage says more about throughput, right? 95% would be nice too reply zhyder 4 hours agoprevShouldn't the right benchmark be performance per watt? It's easy enough to add more chips to do LLM training or inference in parallel. Maybe the benchmark should be performance per $... though I suspect power consumption will eclipse the cost of purchasing the chips from NVDA or AMD (and costs of chips will vary over time and with discounts). EDIT: was wrong on eclipsing; still am looking for a more durable benchmark (performance per billion transistors?) given it's suspected NVDA's chips are over-priced due to demand outstripping supply for now, and AMD's are under- to get a foothold in this market. reply mmoskal 4 hours agoparentNot quite. Assume 1kW power consumption (with cooling). At $0.08/kWh (avarage US industrial rate) this is $700 per year. Adjust for more cooling etc and for say 5 years of usage but you still won't be anywhere near the $25k MSRP for H100. reply iAkashPaul 9 hours agoprevINT8/FP8 benchmarks would've been great, both cards could have loaded them with around 60GB VRAM instead of TP=2 on H100. reply huntertwo 7 hours agoprevAMD has better seemingly better hardware - but not the production capacity to compete with Nvidia yet. Will be interesting to see margins compress when real competition catches up. Everybody thinks it’s CUDA that makes Nvidia the dominant player. It’s not - almost 40% of their revenue this year comes from mega corporations that use their own custom stack to interact with GPUs. It’s only a matter of time before competition catches up and gives us cheaper GPUs. reply almostgotcaught 6 hours agoparent> their own custom stack to interact with GPUs lol completely made up. are you conflating CUDA the platform with the C/C++ like language that people write into files that end with .cu? because while some people are indeed not writing .cu files, absolutely no one is skipping the rest of the \"stack\" (nvcc/ptx/sass/runtime/driver/etc). source: i work at one of these \"mega corps\". hell if you don't believe me go look at how many CUDA kernels pytorch has https://github.com/pytorch/pytorch/tree/main/aten/src/ATen/n.... > Everybody thinks it’s CUDA that makes Nvidia the dominant player. it 100% does reply pastaguy1 7 hours agoparentprevCan you explain the cuda-less stack a little more or provide a source? reply almostgotcaught 6 hours agorootparentsome people emit llvm ir (maaaaybe ptx) directly instead of using the C/C++ frontend to CUDA. that's absolutely the only optional part of the stack and also basically the most trivial (i.e., it's not the frontend that's hard but the target codegen). reply sudosysgen 5 hours agorootparentLLVM IR to machine code is not the part that AMD has traditionally struggled with. What you call \"trivial\" is. If everyone started emitting IR and didn't rely on NVidia-owned libs then the space would become unrecognizable. The codegen is something AMD has always been decent at, hence them beating NVidia in compute benchmarks for most of the past 20 years. reply almostgotcaught 4 hours agorootparent> LLVM IR to machine code is not the part that AMD has traditionally struggled with. alright fine it's the codegen and the runtime and the driver and the library ecosystem... > If everyone started emitting IR and didn't rely on NVidia-owned libs then the space would become unrecognizable. I have no clue what this means - which libs are you talking about here? the libs that contain the implementations of their runtime? or the libs that contain the user space components of their driver? or the libs that contain their driver and firmware code? And exactly which of these will \"everyone emitting IR\" save us from? reply imtringued 1 hour agorootparentAMD is struggling with unsafe C and C++ code breaking their drivers. reply sudosysgen 3 hours agorootparentprevI am talking about user and user-level libraries, so from PyTorch to cuBLAS. The rest is currently serviceable and at time was even slightly better than NVidia. If people start shipping code that targets, say, LLVM IR (that then gets converted to PTX or whatever), like one would do using SYCL, we only have to rely the bare minimum. reply Refusing23 7 hours agoparentprev> but not the production capacity to compete with Nvidia yet. thats just a question of negotiating with tsmc or their few competitors (also didn't tsmc start production of some factories in the US and/or EU?) I mean, nvidia use tsmc, so does amd. reply huntertwo 7 hours agorootparentYes it is - but Nvidia has larger contracts _right now_. Nvidia has been investing more money in producing more GPUs for longer, so it’s only natural that they have an advantage now. But now that there’s a larger incentive to produce GPUs, their moat will eventually fall. TSMC runs at 100% capacity for top tier processes - their bottleneck is more foundries. These take time to build. So the question becomes - how long can Nvidia remain dominant? It could be quarters or it could be years before any real competitor convinces large customers to switch over. Microsoft and Google are producing their own AI hardware too - nobody wants to depend solely on Nvidia, but they’re currently forced to if they want to keep up. reply mark_l_watson 5 hours agoprevA good start for AMD. I am also enthusiastic about another non-NVidea inference option: Groq (which I sometimes use). NVidia relies on TMSC for manufacturing. Samsung is building competing manufacturing infrastructure which is also a good thing, so Taiwan is not a single point of failure. reply rjzzleep 9 hours agoprev> Hardware: TensorWave node equipped with 8 MI300X accelerators, 2 AMD EPYC CPU Processors (192 cores), and 2.3 TB of DDR5 RAM. > MI300X Accelerator: 192GB VRAM, 5.3 TB/s, ~1300 TFLOPS for FP16 > Hardware: Baremetal node with 8 H100 SXM5 accelerators with NVLink, 160 CPU cores, and 1.2 TB of DDR5 RAM. > H100 SXM5 Accelerator: 80GB VRAM, 3.35 TB/s, ~986 TFLOPS for FP16 I really wonder about the pricing. In theory the MI300X is supposed to be cheaper, but whether is that is really the case in practice remains to be seen. reply huevosabio 9 hours agoparentRunPod [0] is pricing MI300X at $4.89/hr vs $3.89-4.69/hr for H100s. So, probably around the same price? The tests look promising, though! [0] https://runpod.io/ reply sigmoid10 8 hours agoparentprevIt doesn't matter. AMD has offered better compute per dollar for a while now, but noone switched because CUDA is the real reason why all serious ML people use Nvidia. Until AMD picks up the slack on their software side, Nvidia will continue to dominate. reply oelang 6 hours agorootparentMicrosoft recently announced that they run chatgpt 3.5 & 4 on mi300 on Azure and the price/performance is better. https://www.amd.com/en/newsroom/press-releases/2024-5-21-amd... reply sigmoid10 2 hours agorootparentI've used ChatGPT on Azure. It sucks on so many levels, everything about it was clearly enforced by some bean counters who see X dollars for Y flops with zero regard for developers. So choosing AMD here would be about par for the course. There is a reason why everyone at the top is racing to buy Nvidia cards and pay the premium. reply mistymountains 3 hours agorootparentprevUnless you develop in CUDA, you can easily train code (e.g. PyTorch) written for training on Nvidia hardware on AMD hardware. You can even keep the .cuda() calls. reply sigmoid10 2 hours agorootparentIn theory. But if you actually work with that in practice, you're already going to have a bad experience installing the drivers. And it's all downhill from there. reply huntertwo 7 hours agorootparentprevLarge corporate customers like Microsoft and Meta do not use CUDA. They all use custom software. AMD doesn’t have enough GPUs to sell them yet, that’s the real bottleneck. reply logicchains 7 hours agorootparentThat's a pretty big claim, that Microsoft and Meta have their own proprietary cuda-replacement stack. Do you have any evidence for that claim? reply versteegen 6 hours agorootparentI'm guessing what they meant is that they use toolchains that are retargetable to other GPUs (and typically compile down to PTX (nVidia assembly language) on nVidia GPUs rather than go through CUDA source -- GCC and clang can both target PTX). For example XLA and most SYSCL toolchains support much more than nVidia. reply hyperman1 8 hours agorootparentprevAnd this shouldn't be to hard if you know the ins and outs of the hardware and have a reasonable dev team. So why aren't they doing it? reply rjzzleep 6 hours agorootparent> and have a reasonable dev team probably this. reply chillee 8 hours agoprevI'm skeptical of these benchmarks for a number of reasons. 1. They're only comparing against VLLM, which isn't SOTA for latency-focused inference. For example, their vllm benchmark on 2 GPUs sees 102 tokens/s for BS=1, gpt-fast gets around 190 tok/s. https://github.com/pytorch-labs/gpt-fast 2. As others have pointed out, they're comparing H100 running with TP=2 vs. 2 AMD GPUs running independently. Specifically, > To make an accurate comparison between the systems with different settings of tensor parallelism, we extrapolate throughput for the MI300X by 2. This is uhh.... very misleading, for a number of reasons. For one, at BS=1, what does running with 2 GPUs even mean? Do they mean that they're getting the results for one AMD GPUs at BS=1 and then... doubling that? Isn't that just... running at BS=2? 3. It's very strange to me that their throughput nearly doubles going from BS=1 to BS=2. MoE models have an interesting property that low amounts of batching doesn't actually significantly improve their throughput, and so on their Nvidia vllm benchmark they just go from 102 => 105 tokens/s throughput when going from BS=1 to BS=2. But on AMD GPUs they go from 142 to 280? That doesn't make any sense to me. reply zxexz 9 hours agoprevIs this an ad for a new, closed-source, GPGPU backend? reply 42lux 9 hours agoparentPretty much and the test suit is optimized to get the results they wanted. reply nottorp 9 hours agorootparentPretty sure a useful benchmark for this kind of thing would calculate performance per watt (or per watt and dollar). That info is conspicuously absent from the article. reply acchow 9 hours agorootparentThe electricity consumption in the cloud is not really important. The H100 rents for about $4.5/hr consuming 0.7kWh in that hour which will likely cost them less than 7 cents. reply nottorp 8 hours agorootparent> The electricity consumption in the cloud is not really important. That just says you don't run a cloud for profit :) reply nextworddev 5 hours agoprevWe need more competition in the training space, not inference. For consumer grade inference, there's already many options available. reply DrNosferatu 8 hours agoprevThe comparison is between setups with different amounts of GPU RAM and there's no quantification of final performance/price. reply Gasp0de 7 hours agoparentSo? If you get twice the RAM at a comparable price and that leads to twice the performance, what's wrong with comparing that? reply DrNosferatu 7 hours agorootparentNothing wrong - just for transparency. Also, the price difference is not quantified. Additionally, CUDA is a known and tangible software stack - can I try out this \"MK1 FLywheel\" on my local (AMD) hardware? reply instagraham 9 hours agoprevGiven that a lot of projects are written or optimised for CUDA, would it require an industry shift if AMD were to become a competitive source of GPUs for AI training? reply irusensei 9 hours agoparentEvery hardware vendor is working to provide something with their own technology. I don't know if it's possible but a lot of very resourceful companies are doing their best to break the CUDA dominance. I really hope it works and hopefully a non proprietary standard emerges. reply yobbo 7 hours agoparentprevThe model code is comparatively tiny compared to pytorch or CUDA itself. Translating models from CUDA/C could be laborious but not a barrier. Making AMD work effortlessly with pytorch et al should make the switch transparent. reply mistymountains 3 hours agorootparentThese kinds of comments make me think few people have actually tried. My experience has been 1 work day of getting things set up to work the same as before for training and testing (PyTorch). reply imtringued 1 hour agorootparentYou have to consider that the average person who tried to do machine learning on AMD GPUs got burned in the past decade and has no reason to change their opinion. Also in the past it was much harder to get access to cutting edge GPUs from AMD. The fact that AMD drops GPU support for ROCm quickly also earns them scorn. I don't think it is an unfair assessment. They earned their reputation. reply jvlake 8 hours agoprevAt this point in history were still at ROCm vs CUDA... Schmicko hardware is only as good as the software you can write for it. reply KaoruAoiShiho 4 hours agoprevPretty bad benchmarks to the point of being deliberately misleading. They benchmarked vllm which is less than half the speed of the inference leader lmdeploy: https://bentoml.com/blog/benchmarking-llm-inference-backends They also used Flywheel for AMD while not bothering to turn on Flywheel for Nvidia, which is crazy since Flywheel improves Nvidia performance by 70%. https://mk1.ai/blog/flywheel-launch In this context the 33% performance lead by AMD looks terrible, and straight up looks slower. reply DarkmSparks 9 hours agoprevhopper (H100) is the predecessor to the current blackwell architecture. This is a new AMD vs last generation nvidia benchmark. reply triblemaster 9 hours agoparentBlackwell won't be here till next year. reply DarkmSparks 8 hours agorootparentGB200 based on blackwell launched in March of this year. https://www.theregister.com/2024/03/21/nvidia_dgx_gb200_nvk7... MI300X launched 3 months earlier at the end of December. H100 launched March 2023, reply acchow 8 hours agorootparentprevNvidia expects to ship 420k Blackwell chips this year. reply robblbobbl 7 hours agoprev1.Investing (wasting) the billions. 2. Receive downvotes on ycombinator lol reply jvlake 9 hours agoprevCool story. How supported is OpenCL compared to CUDA again? reply amelius 7 hours agoprev [–] Are these fabbed at the same process node? (Otherwise it's apples and oranges) reply qeternity 7 hours agoparent [–] It's not apples and oranges. These are the top of the line offerings from the respective companies today. reply amelius 6 hours agorootparentPerhaps, but that was not the question. After all, these chips are not made by one company. There's a fab too. Not exactly unimportant. reply Vvector 6 hours agorootparentprev [–] Nvidia just started shipping the H200 to selected companies two months ago. Too late for this benchmark. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "AMD's MI300X accelerator outperforms NVIDIA’s H100 SXM in real-world AI workloads, achieving 33% higher throughput in a chat use case.",
      "Despite NVIDIA’s mature software ecosystem, AMD's MI300X is a strong competitor, offering better performance, cost efficiency, and availability for large-scale cloud inference.",
      "Benchmarks show the MI300X excels in both offline and online inference tasks, making it ideal for scaling AI inference capabilities with higher throughput and faster response times."
    ],
    "commentSummary": [
      "AMD's MI300X outperforms Nvidia's H100 in LLM (Large Language Model) inference, according to TensorWave, a cloud provider specializing in AI workloads.",
      "The report's validity is questioned due to AMD's chip having double the transistors and memory but only performing 33% better, and the AMD setup being significantly cheaper.",
      "The discussion highlights Nvidia's dominance in AI workloads, their market position, and the potential competition from AMD, emphasizing the importance of microarchitecture design skills and software in driving hardware performance."
    ],
    "points": 264,
    "commentCount": 224,
    "retryCount": 0,
    "time": 1718265476
  },
  {
    "id": 40668504,
    "title": "Southwest Airlines Boeing 737-8 Max Experienced Dutch Roll",
    "originLink": "https://avherald.com/h?article=519ce679",
    "originBody": "The Aviation Herald Last Update: Thursday, Jun 13th 2024 18:41Z 30512 Articles available Events from Mar 23rd 1994 to Jun 11th 2024www.avherald.com Incidents and News in Aviation List by: Filter: Accident: Southwest B38M enroute on May 25th 2024, Dutch Roll By Simon Hradecky, created Wednesday, Jun 12th 2024 17:25Z, last updated Thursday, Jun 13th 2024 16:00Z A Southwest Airlines Boeing 737-8 MAX, registration N8825Q performing flight WN-746 from Phoenix,AZ to Oakland,CA (USA) with 175 passengers and 6 crew, was enroute at FL320 when the aircraft experienced Dutch Roll. The crew was able to regain control and landed the aircraft on Oakland's runway 30 about 55 minutes later. The aircraft sustained substantial structural damage. The FAA reported: \"AIRCRAFT EXPERIENCED A DUTCH ROLL, REGAINED CONTROL AND POST FLIGHT INSPECTION REVEALED DAMAGE TO THE STANDBY PCU, OAKLAND, CA.\" and stated the aircraft sustained substantial damage, the occurrence was rated an accident. The aircraft remained on the ground in Oakland until Jun 6th 2024, then positioned to Everett,WA (USA), ATS facilities, and is still on the ground in Everett 6 days later. Dutch Roll is a coupled out of phase movement of the aircraft as result of weakened directional stability (provided by the vertical tail and rudder), in which the aircraft oscillates around its vertical as well as longitudinal axis (coupled yaw and roll). The PCU is the power control unit, an actuator controlling the (vertical) rudder. On Jun 13th 2024 The Aviation Herald learned that two ribs, that the stand by PCU is being mounted to, were damaged as well as the mounts of the stand by actuator. A temporary repair was done in Oakland replacing the damaged PCU, the aircraft was then ferried to Everett to replace the damaged ribs. https://flightaware.com/live/flight/SWA746/history/20240525/1425Z/KPHX/KOAK Reader Comments: (the comments posted below do not reflect the view of The Aviation Herald but represent the view of the various posters) @picarus: Did not make to the news By mediacritic on Thursday, Jun 13th 2024 16:53Z Well, if the news would report a dutch roll, the common reader will think of a Frikandel :D Lost plane parts give a much better headline. But you're right, could be a new severe problem on this pimped-up oldtimer. @Picarus By Kris on Thursday, Jun 13th 2024 16:51Z It didn't make the mainstream news because nobody died, there was no visible damage, the plane didn't drop 3,000 feet in flight, and the media doesn't understand anything that this article is saying. Damages By Concorde1980 on Thursday, Jun 13th 2024 15:22Z What would the typical damages to The AC? Good shtuff By Joe on Thursday, Jun 13th 2024 13:31Z \"They call this strain 747 because when you smoke it you're GONE\" By (anonymous) on Thursday, Jun 13th 2024 13:05Z Wasn't a KLM airplane History Repeating Itself or Freak Accident? By Picarus on Thursday, Jun 13th 2024 11:24Z Strange this did not make the news, especially since it involved a Max 8. In any event, I would really like to believe this is an isolated incident, but Boeing has completely lost my trust. Time will tell, I suppose. By Peter Evers on Thursday, Jun 13th 2024 10:27Z The 747 has two independed yaw dampers systems (2 computers, 2 pcu's and two rudders (upper and lower) Yikes - PCU again By Brian Johnson on Thursday, Jun 13th 2024 08:34Z ...The PCU is the power control unit, an actuator controlling the (vertical) rudder... If I remember correctly, the PCU was the component that failed on at least three 737 classics in the 1990s, leading to two fatal crashes and one serious loss of control incident. @FBW By Kris on Thursday, Jun 13th 2024 03:10Z Not so much damage that they couldn’t reposition the ac. By RJ on Thursday, Jun 13th 2024 02:04Z C141 1976 went inverted from a dutch roll, went from 410 to 17000 Flight recorder data indicated \"G\" loadings of+3.18 to -3.52 and a maximum of 450 KIAS Investigators were unable to confirm the maximum Mach, but suspect that it had exceeded Mach 1.0. @ John By FBW on Thursday, Jun 13th 2024 01:37Z If the A/C was damaged I guarantee the passengers were alarmed! Dutch roll By Raffles on Wednesday, Jun 12th 2024 21:11Z I've practiced dutch roll in the sim , many times, but never had it in real life. In the sim it is quite violent but in general, the sim is more sensitive and pronounced than real life. The pax would definitely notice it however. By SB on Wednesday, Jun 12th 2024 21:00Z 737 tech channel states that 737s will naturally stop oscillating if a dutch roll is induced due to the positive damping, so the yaw damper is not required for dispatch. Washington as destination? By Ducky on Wednesday, Jun 12th 2024 20:17Z Warranty issue. The technicians at Everett will get to the bottom of this issue. After all, this is a Boeing 737 MAX Yaw Dampers By JT on Wednesday, Jun 12th 2024 20:08Z B-727 had two independent yaw dampers. Exciting to pax? By john on Wednesday, Jun 12th 2024 19:40Z Was the oscillation strong enough to alarm the pax? Boeing =/= redundancy By Paul on Wednesday, Jun 12th 2024 18:54Z @ernst: Great idea, but as evidenced by the max's original AOA sensor design, they don't really do redundancy. Less is cheaper. Cheaper is better. This is the Boeing mantra, regardless of engineering or safety or lessons learned/history indicating otherwise. YAW DAMPER By ernst on Wednesday, Jun 12th 2024 18:23Z There were airliners in the past (not Boeing ones) that had two completely independent redundant Yaw Damper systems to prevent such a Dutch Roll event. Maybe if Boeing thinks of designing a new aircraft type within the next 10 or 15 years, the safety engineers could think about adding a second independent Yaw Damper system. By ADS on Wednesday, Jun 12th 2024 17:58Z a Dutch Roll incident 11 years ago on a KC135 was caused by a PCU malfunction a vanishingly rare issue on modern aircraft makes me wonder whey we spent so much time studying it in my Aero degree! Lucky? By FBW on Wednesday, Jun 12th 2024 17:57Z They are lucky the rudder did not break off. Add your comment: (max 1024 characters) Your IP address 13.91.68.80 is being tracked. We reserve the right to remove any comment for any reason. Profanities, personal attacks (against any physical or legal person), discussions about site policies, false statements or similiar as well as copyrighted materials from third parties will not be tolerated, those messages will summarily be deleted. Links and mail addresses are not permitted and will not appear in the display, posts trying to circumvent the restriction of links/mail addresses will be removed. We ask for your valid e-mail address in the email field. This address is only used by us to establish contact with you in case of further questions, it will not be displayed anywhere or be used otherwise. Your Name:Your Email:Subject:Your comment: The Aviation Herald Apps Android and iOS Support The Aviation Herald Euro US$ Interview: The human factor named \"Simon Hradecky\" and the team of man and machineGet the news right onto your desktop when they happen © 2008-2024 by The Aviation Herald, all rights reserved, reprint and republishing prohibited. We use cookies to ensure you get the best experience on our site, learn more.",
    "commentLink": "https://news.ycombinator.com/item?id=40668504",
    "commentBody": "Southwest Airlines Boeing 737-8 Max Experienced Dutch Roll (avherald.com)241 points by belter 7 hours agohidepastfavorite207 comments marcus0x62 6 hours agoYears ago, my father in law ( a heavy jet mechanic) got sent to China for two months as part of a team to diagnose and fix a recurring dutch roll complaint on a particular airframe. This was for a A300-series flown by a large freight carrier. The local mechanics couldn’t figure it out, so this team was assembled from the mechanics at headquarters to fix the plane. They inspected the plane and found no damage. They had the pilots make multiple test flights - they could not reproduce the problem. The pilots were adamant that the plane was broken. Engineers from Airbus ended up flying out to assist. They couldn’t identify any problems. After a few months of effort and a lot of very expensive dart throwing and parts swapping, they gave up. As far as anyone could tell, there was never anything wrong with the plane, and based on comments overheard from the pilots, they simply did not like this particular plane. It had been pulled from a boneyard and recertified, and they considered flying it beneath them. FIL did come home with some cuban cigars and really high quality counterfeit Fluke multimeters he got from the local market, though. reply FabHK 5 hours agoparent> they simply did not like this particular plane. …and employed a face-saving (though probably very expensive) way of communicating it. reply marcus0x62 4 hours agorootparentI think this was way more calculated than that. They made up an issue they knew would be taken very seriously and that would be difficult - in the best of circumstances - to disprove. reply pfannkuchen 4 hours agorootparentprevI suspect the face saving culture is holding China back significantly since it adds a ton of noise to interpersonal signal propagation. I wonder if the government is doing anything social engineering-y to try to reduce it? reply kstrauser 4 hours agorootparentThe government is among the worst of the culprits. “We’re being accused of doing a bad thing? I know! Arrest the speaker and erase it from our version of the Internet!” reply RobotToaster 4 hours agorootparentprev> I wonder if the government is doing anything social engineering-y to try to reduce it? I know Xi Jinping launching an anti-corruption program was considered a pretty big deal, because it meant admitting that corruption exists in the CPC. reply seanmcdirmid 4 hours agorootparentThat isn’t true though. All presidents launch anti corruption campaigns to solidify their power when they start. Xi did it, Hu did it, Jiang did it, I bet Deng did it (although the office was merged yet, he was at least supreme leader). When they are all corrupt, it is easy to (a) clean house of people you don’t like (b) protect and promote your supporters and (c) scare lots of monkeys by killing lots of chickens. The CPC always admitted there was corruption, to do otherwise would simply lose credibility with people. Given that Xi is now president for life, the corruption should simply be accumulating now since the 10 year house cleaning cycle has been broken. reply jgalt212 4 hours agorootparentprevseriously. Look at the Kardashians. They turned every embarrassing event into a money making opportunity. reply RobotToaster 4 hours agorootparentYou think the Kardashians are a good model for government? reply jgalt212 4 hours agorootparentOf course not. I meant it as an extreme counter-example. reply ToucanLoucan 5 hours agoparentprev> and really high quality counterfeit Fluke multimeters he got from the local market, though. I got one of those years ago. Wouldn't trust it on any industrial shit but I've used it for residential electrical tinkering (plus smaller projects) for years, works great and has never had a single issue. reply marcus0x62 5 hours agorootparentThese were used for mechanic-y stuff like basic voltage and circuit draw tests. They were fine for that, and the main benefit was that they were made with oil/solvent resistant coatings on the meter body and leads. reply Waterluvian 6 hours agoparentprevEek… I’d never want to fly a plane that’s beneath me. I’d prefer it to be at the same altitude. reply bigstrat2003 4 hours agorootparentTechnically any plane should be beneath you. Also above you, but beneath you too. reply moffkalast 5 hours agorootparentprevWell unless you're an air launched cruise missile operator. reply blakesterz 6 hours agoprev\"Dutch Roll is a coupled out of phase movement of the aircraft as result of weakened directional stability (provided by the vertical tail and rudder), in which the aircraft oscillates around its vertical as well as longitudinal axis (coupled yaw and roll).\" That sounds pretty scary. I wonder why it wasn't bigger news when it happened last month? reply umvi 5 hours agoparentNot as scary as flying upside down which is what I originally thought based on title alone. Most people are familiar with \"barrel roll\" but by comparison this just looks like wiggling. Counterintuitively though my understanding is that barrel rolls are generally safe even for large planes since it's a stable 1G maneuver (some jock irresponsible air force pilot used to regularly barrel roll B-52s until he did one too low and killed himself and the crew)... reply agurk 5 hours agorootparentI'm not aware of a B-52 being crashed in a barrel roll, but this did make me think of the 1994 Fairchild crash[0] that was caused by a pilot who was known as dangerous due to excessive self confidence. In this case he banked too tightly at low level causing a stall into the ground. The whole crash was caught on camera[1], which is indelibly burnt into my memory. [0] - https://en.wikipedia.org/wiki/1994_Fairchild_Air_Force_Base_... [1] - https://theaviationgeekclub.com/the-story-of-bud-holland-the... reply umvi 3 hours agorootparentThanks, my mistake, this is what I was referring to. Apparently \"Bud\" would brag about \"rolling\" B-52s (https://www.historylink.org/File/8716), but the fatal crash was actually just a low-altitude stall as you pointed out. It's now a textbook case study for systemic leadership failure. reply chipdart 4 hours agorootparentprevDutch roll, Bud Holland... Netherlands trifecta is in play. reply neuronic 4 hours agorootparentprevThe last 3 seconds of your life as the copilot must have been some revolting emotion knowing that the overconfident asshole in the seat next to you just killed you. reply dylan604 4 hours agorootparentprevMan, that is just one big ass airplane to even consider wanting to do any kind of maneuvering without threat indicators blinking/buzzing/beeping/toning that your death is imminent without doing some of that \"pilot shit\". The story from your [0] about one of the crew's last flight with his family on the ground waiting for the post-flight rituals where his wife and 2 kids watching is just horrific. What a fucking asshat of a person. reply MaxfordAndSons 4 hours agorootparentprevI'm pretty sure that's the crash GP was referring to. It sort of looks like he could have been going into a barrel roll. reply rob74 5 hours agorootparentprevAFAIK only one Boeing passenger jet ever performed a barrel roll (without passengers on board): https://simpleflying.com/boeing-707-barrel-roll-seattle/ reply FabHK 4 hours agorootparentprev> it's a stable 1G maneuver It’s positive G, but 0.5 up to 2G (otherwise, given that lift is “towards the ceiling of the plane”, so not always straight up during the manoeuvre, you’d accelerate downward, and arresting the fall requires >1G). reply wyldfire 5 hours agorootparentprev> barrel rolls are generally safe even for large planes since it's a stable 1G maneuver Maybe \"safe\" here means that merely the stress of the maneuver itself wouldn't destroy the craft? But surely the loss of lift is likely to put it at significant risk, as in your example. reply MeImCounting 5 hours agorootparentYou dont lose lift in a barrel roll, you merely change the direction that lift is being generated. reply kqr 4 hours agorootparentWhen the threat is the ground, maybe that's what's worrying anyway. reply wyldfire 2 hours agorootparentprevAh, good point ;) reply throwup238 5 hours agorootparentprevThat depends on what you mean by “generally safe”. Airline jets are not made to withstand the forces of an aerobatic maneuver and while they can do it in test flight conditions, each time requires pulling the jet out of service afterwards for a full structural inspection. reply psunavy03 4 hours agorootparentA barrel roll is a 1-G maneuver and will not overstress an aircraft unless done improperly. reply FabHK 4 hours agorootparentUp to 2 or 3G, but most aircraft should withstand that, even gliders. reply zh3 4 hours agorootparentprevProbably wasn't a B52. Tex Johnston was reknowned for barrel rolling a 707 during a demo back in the day - there's grainy video of it on youtube. Apparently the Boing execs at the demo didn't know it was going to happen and nearly had heart attacks when he did. Video on youtube: https://www.youtube.com/watch?v=iz1Eze3QrDU reply FabHK 4 hours agorootparentBob Hoover rolled a plane while pouring a tea. https://www.youtube.com/watch?v=V9pvG_ZSnCc reply kstrauser 4 hours agorootparentprevTex flew in cowboy boots and, well, was named Tex. I don’t know what they thought would happen. reply fortran77 4 hours agorootparentprevTex Johnson reply zh3 4 hours agorootparentThanks, corrected. reply kevinmgranger 4 hours agorootparentprevA barrel roll or an aileron roll? Most people call an aileron roll a barrel roll (no thanks to Star Fox) reply psunavy03 4 hours agorootparentprevThe Fairchild AFB crash was not a barrel roll. reply brandall10 5 hours agoparentprevJust flew a Max from Denver to Mexico City a couple days back. It was mostly uneventful, but as we were preparing to land the aircraft banked hard to the left and began to experience something akin to this...like it hit a sudden pocket of turbulence at the apex of the turn and began to pogo around. It was minor and far from inducing damage to the vehicle, but in that moment remembered where I was while clenching my body for a few nervous seconds until we eased out of the turn. reply ttymck 5 hours agorootparentDo you fly to Mexico city often? The altitude makes landings unusually eventful. reply brandall10 2 hours agorootparentThis is my third time and I'm familiar with altitude weirdness having lived in Denver for several years, that rollercoaster effect. The other parts of the descent were smooth until specifically hitting its maximum bank, then it suddenly got squirrelly and then stopped as the plane began to flatten. It's as if the degree of bank was beyond the plane's limits. I've flown hundreds of times and can't recall ever experiencing something quite like this. EDIT: I just realized my original comment was written incorrectly. The plane didn't expectedly bank, it's that this started happening at the top of a hard bank. reply dylan604 4 hours agorootparentprev> like it hit a sudden pocket of turbulence at the apex of the turn I wonder if this could have been wind shear? Wind shear was deemed as the cause of the crash for Delta 191[0] on approach to DFW in the 1985. Lots of new equipment was installed after to detect wind shear. I wonder of MEX has that kind of equipment. [0] https://en.wikipedia.org/wiki/Delta_Air_Lines_Flight_191 reply brandall10 1 hour agorootparentPossibly, I don't think so. It happened right at the top of the bank and stopped as the plane began to ease out. There were no weather issues no other incidents of turbulence or jostling around. It seemed as if the plane was having problems banking at that angle. reply jll29 3 hours agorootparentprevI hope nobody got hurt (assume so, you may have told you otherwise). Is this something pilots get trained for how to cope with this type of incident? What happened to passengers (are they usually injured from things falling/flying around)? Boeing is getting some bad news these days, but multiple whistle-blowers in a row is the worst. reply brandall10 1 hour agorootparentIt wasn't that extreme, it was what I would consider a medium level of turbulence. I just realized my comment wasn't written correctly - the bank itself is what caused the issue, not that it just suddenly started banking unexpectedly. What was interesting was that it specifically started and stopped as it eased into and out of the turn. reply tracker1 4 hours agorootparentprevI had a similar experience as a child flying into Phoenix... there were heavy storms and about 100ft from the ground, the plane just shifted to the side several hundred feet... the pilot pulled back up and had to circle around the airport for another landing approach. Kinda wild. reply rob74 5 hours agoparentprevThe question is how bad the oscillations were, and how long they lasted. Maybe for the passengers it was indistinguishable from \"regular\" turbulence? reply Kye 6 hours agoparentprevIn plain English: unwanted tail waggle reply SahAssar 5 hours agorootparent\"Tail lefty-righty no-no\" is the industry term. reply Kye 4 hours agorootparentThat's plane English. reply addandsubtract 1 hour agorootparentYou're on a roll reply belter 6 hours agoparentprevIt can torn off the engines from the main body structure: https://asn.flightsafety.org/asndb/333926 reply solardev 5 hours agorootparentMaybe it's a feature, not a bug? Eventually, enough parts fall off that a whole new plane appears on the ground. It's just how the 737s reproduce. reply senordevnyc 6 hours agorootparentprevWhile true, this was a pretty different situation in that it was 1959 (engineering standards have changed a lot in the last 65 years), and the dutch rolls were being performed intentionally for demonstration purposes. reply FabHK 5 hours agorootparent> dutch rolls were being performed intentionally for demonstration purposes. …with the yaw dampers (that are there to prevent Dutch roll) turned off. reply nabla9 4 hours agorootparentprev1985 the dedliest single aircraft accident was due to dutch roll. Japan Airlines Boeing 747SR. 2013 KC-135 Stratotanker broke up in flight due to dutch roll. 2005 Airbus A310 survived with serious structural damage. reply sjm-lbm 4 hours agorootparent> 1985 the dedliest single aircraft accident was due to dutch roll. Japan Airlines Boeing 747SR. This kind of buries the lede, right? A bad repair caused an explosion that blew off most of the vertical stabilizer (ie, the tail). The dutch roll was part of the series of unfortunate events that followed. reply rootusrootus 4 hours agorootparentprevOf those three examples, only the KC-135 was caused by the dutch roll. The other two were caused by structural failures, the dutch roll was incidental. reply crazytony 3 hours agorootparentprevThis is extremely disingenuous. A dutch roll if identified and corrected is not structurally dangerous to an aircraft but it typically signifies something wrong with the control surfaces which is a larger issue. In the majority of the cases, it's the yaw damper that's a problem (my suspicion in the 737 case). JAL 123 crashed because hydraulic pressure on all 4 hydraulic lines and 90% of the vertical stabilizer and 100% of the rudder were lost due to an explosive decompression of the aft pressure bulkhead. Dutch rolls ensued because of the loss of lateral direction control. The KC-135 crashed because the rudder power control unit on the rudder was faulty and the pilots failed to identify the problem. They then used alternating rudder inputs to recover which caused the structural limits of the vertical stabilizer to be exceeded its structural limits and separate (along with the rest of the tail). The Air Transat flight (961) had the entire rudder separate from the aircraft most likely due to stress fractures. This caused the aircraft to have extremely limited lateral directional control which caused the dutch rolls. reply dylan604 4 hours agorootparentprev> engineering standards have changed a lot in the last 65 years Especially for Boeing where standards are determined by the stock price reply tjpnz 6 hours agoparentprevHere's a video demonstrating it: https://youtu.be/Zmjam1evDD4 reply glitchc 6 hours agorootparentDutch roll was particularly pernicious with trijets. It has to do with thrust vs lift vectors having different offsets (origins) relative to the cg. Sometimes they would add stabilizers to the tail to correct for this, but eventually manufacturers gave up on the design. reply jader201 5 hours agorootparentprevI feel like “roll” is a bad word for what’s happening here. In my brain, when I hear “roll”, I’m thinking a 360 degree rotation in either the vertical or horizontal axis (or both). This is far from that (even in the real life footage linked in the sibling comments). This is more like a “Dutch wobble” or “tilt”. But I’m not an aviator, so what do I know. reply FabHK 5 hours agorootparentWhat you describe might be called a loop. In aviation, “roll” describes any rotation (not necessarily full 360) around the longitudinal axis, as “yaw” around the vertical and “pitch” around the lateral axis. ETA: 360 degrees pitch is a looping, 360 degrees roll is a aileron roll or slow roll, a 360 degrees roll and pitch is a barrel roll, and 360 degrees yaw is “a 360”, ie a full circle turn. reply lxgr 5 hours agorootparentprev\"Roll\" is the name for one of the three axes in aviation. \"Vertical\" and \"horizontal\" aren't very descriptive in three dimensions. You could reasonably call yaw the \"horizontal axis\", but then assigning \"vertical\" to either of roll or pitch – and what do you call the other one then? They're arguably both vertical, depending on which side you look at the plane from! Additionally, at least \"horizontal\" implies an Earth-centered focus, which doesn't help while in, say, a barrel roll :) Best to avoid the ambiguity entirely and use specific terms, just like how port and starboard avoid the \"my left/right or yours\" ambiguity nicely by always referring to the ship's frame of reference. reply kstrauser 4 hours agorootparentprevIn any case, it sure beats a Dutch crunch. reply kcplate 6 hours agorootparentprevI guess it’s worse than it looks? The video was kind of a dramatic let down to be honest. I preferred the mystery. reply JWlrCk9PkipFTDq 6 hours agorootparentIt looks a bit more intense in real footage. https://www.youtube.com/watch?v=2tgfkGiHhxs reply Ringz 2 hours agorootparentThis should be the top comment. reply kcplate 5 hours agorootparentprevAgreed. reply ceejayoz 6 hours agorootparentprevA real-world video looks a lot scarier. Linked downthread: https://www.youtube.com/watch?v=2tgfkGiHhxs Especially if close to the ground. reply albert_e 6 hours agorootparentprevthis video posted elsewhere here seems dramatic enough https://www.youtube.com/watch?v=2tgfkGiHhxs Imagine you are in the cockpit at the top of the plane and your view/perspective - for a layman like me this would seem and feel like the flight is completely out of control. Even the passengers are bound to experience dramatic movements similar to severe turbulence I guess. reply seanhunter 6 hours agorootparentprevIdk. The idea of trying to control the aircraft which is doing this while being responsible for >100 lives seems pretty terrifying to me. reply FrustratedMonky 6 hours agorootparentprevYeah, could this be caused by turbulence? Winds? Looking at the video, now i'm not sure I haven't been through some dutch rolls also. reply SonicScrub 6 hours agorootparentDutch Roll is a coupling of yawing and rolling dynamic modes, and is a product of the aircraft's aerodynamics. If the aircraft is disturbed off a steady-state path either by control input, changing winds, or turbulence, then it should return back to it's steady-state path with oscillations that quickly dampen. Dutch Roll is a phenomenon where these oscillations grow rather than dampen as a result of out-of-phase yaw and roll modes. So Dutch Roll can be triggered by turbulence/wind, but the Dutch Roll itself is the result of something going wrong in reaction to that stimulus. This is different than the aircraft just being batted around by turbulence. reply account42 4 hours agorootparent> This is different than the aircraft just being batted around by turbulence. Does it actually look different or is it just a different cause for similar movement patterns? reply SonicScrub 1 hour agorootparentJust being batted around by turbulence looks different if you know what you're looking for (although what to look for might be only obvious when looking at accelerometer data). Again, Dutch Roll is a very specific phenomenon as a result of coupling between the roll and yaw dynamic modes. The risk of Dutch Roll is that these oscillations can grow even without further stimulus rather than just dampen out. reply belter 6 hours agorootparentprevThe FAA reported: \"AIRCRAFT EXPERIENCED A DUTCH ROLL, REGAINED CONTROL AND POST FLIGHT INSPECTION REVEALED DAMAGE TO THE STANDBY PCU, OAKLAND, CA.\" and stated the aircraft sustained substantial damage, the occurrence was rated an accident. reply HPsquared 6 hours agorootparentStandby PCU sounds like a control system. Was that a cause or effect of the event? reply banannaise 6 hours agorootparentThe PCU in this context is the part that moves the rudder. PCU issues were common on the 737 in the 90s[0] although that's... probably?... not relevant here. [0] https://en.wikipedia.org/wiki/Boeing_737_rudder_issues reply HPsquared 5 hours agorootparentExactly, rudder control issues could trigger or exacerbate Dutch roll. Loss of yaw damper control, in other words (yaw damper actuates the rudder). reply DiggyJohnson 4 hours agorootparentprevFor what its worth, that's very unlikely in the last decade or so. More likely to be moderate turbulence, which would be nearly indistinguishable in a non-severe event. reply underseacables 6 hours agoparentprevIt may have gotten lost in a sea of Boeing bad news. reply drcongo 6 hours agoparentprevI wonder if the name is related to the Dutch Tilt camera shot - https://en.wikipedia.org/wiki/Dutch_angle reply SonicScrub 6 hours agorootparentTha name is believed to be borrowed from an ice skating term: https://en.m.wikipedia.org/wiki/Dutch_roll https://www.iceskatesmuseum.com/e-disc-schoon.htm reply jamesholden 6 hours agoparentprevnext [6 more] [flagged] kcplate 6 hours agorootparentI know the humorless HN is downvoting you but I gave you an upvote. Farts and fart jokes are always funny. reply doctor_eval 6 hours agorootparentIt’s being downvoted because it’s technically incorrect. They are describing a Dutch Oven. reply kcplate 6 hours agorootparentUh yes, they were being sarcastic. Once again…the humorless HN strikes. Sheesh. reply cwillu 5 hours agorootparentIf the joke was worth making, the joke was worth suffering the downvotes. Have some courage in your convictions, and most importantly: don't whine about it. reply kcplate 5 hours agorootparentMy courage is strong, I’m willing to support a downvoted comment with another comment or two that I know will be downvoted to hell too…just to hold a mirror up to the humorless folk on here. reply adolph 6 hours agoparentprev> why [Dutch Roll] wasn't bigger news I think the name is a serious problem. Something like “uncontrolled aircraft orientation” is more descriptive. “Dutch Roll” brings to mind thoughts of “is it better than Swiss Cake Roll?” and “is this news-spam for Hostess brand emerging from bankruptcy?” reply FabHK 5 hours agorootparentThere are terms of art, jargon, in many fields that mean something different than the “obvious” reading of the term. I had a discussion with someone on Hacker News recently that just could not understand that “dynamic programming” was a very specific solution technique for a broad range of problems with a special structure, rather than any sort of programming with a dynamic aspect to it. reply adolph 4 hours agorootparentAgreed. To the question at hand: “why 737-max’s ‘Dutch Roll’ wasn’t bigger news,” my conjecture is the term of art obscures the severity of the problem. As a point of jargon, “Dutch Roll” is poor because the word roll within aeronautics could refer to an orientation movement or an acrobatic maneuver (and probably other things I don’t know about). The use of an ethnic/national identity as an adjective doesn’t help. Is “Dutch” associated at all with oscillations in two dimensions that are not intended by the pilot? reply semireg 5 hours agoparentprevE.g. wibble-wobble or the more pronounced wibbly-wobbly. reply ectocardia 6 hours agoprev\"Dutch Roll\" in context: it's one of the dynamic modes of a plane -- a bit like the \"phugoid\" (e.g. when your paper airplane repeatedly speeds downward, pitches up, stalls, and speeds downward again on its way to the ground). In broad strokes, the dynamic modes can have natural frequencies -- in the same sense often used when speaking of resonance, transfer functions -- and there's a trade-off between (1) having low natural frequencies and (2) having a responsive control system. https://en.wikipedia.org/wiki/Aircraft_dynamic_modes reply VBprogrammer 5 hours agoprevThe 737, at least in classic guise, was traditionally very stable in Yaw. Unusually so because the swept wing jets usually require additional yaw damping because the geometry naturally means on yawing one wing gets longer and the other gets shorter as seen from the point of view of the airflow. So much so that the Yaw damper on the 737 was not on the minimum equipment list for dispatch. On other aircraft like the 757 having one of the two yaw dampers out of service limited dispatch to return to a maintenance base (from memory so it might not be exactly that). I'll keep my eye out for the report on this one because I'm not convinced the initial reporting tells the whole story. It's not unheard of for pilots to damage otherwise perfectly serviceable aircraft through in-proper rudder application - American Airlines Flight 587 being a horrible example. reply justinclift 6 hours agoprevNot the incident from the article, but this footage of some other aircraft doing a Dutch Roll is illustrative: https://www.youtube.com/watch?v=2tgfkGiHhxs reply kleiba 5 hours agoparentYou gotta love the first comment under that video. reply romanovcode 6 hours agoparentprevJesus the passengers must be frightened to death inside. Boing just keeps on giving problems after problems. Someone from the government should really look into it, the murdering of whistleblowers is also not a great sign. reply everybodyknows 2 hours agorootparentAirplane in the video is a Russian design, and the flight appears to have been done for demonstration. reply supportengineer 5 hours agorootparentprevAfter a quick search just now, I could not find any tweets about it reply deskamess 4 hours agorootparentAs one of the commenters to the video said: \"That is the Noon Flight from Moscow arriving as usual!\" Nothing to see here. Some bounce cars - others dutch roll planes. Personally, I am partial to the Swiss roll. reply shepherdjerred 3 hours agorootparentprevWas murder proven? reply crazytony 29 minutes agoprevLooking at flightradar24, the only real upset I see is the turn over the southern Sierra Nevada range which is also ~55 minutes before arrival into OAK. Wonder if mountain waves play any part in this incident? It's the only thing I can think of that could possibly generate a dutch roll without continuing flight stability issues. (They could have also turned on/off something like the yaw damper in the cockpit but that's not specifically called out int he report) I can't imagine the standby PCU causing this incident unless there was some kind of electrical or pneumatic issue causing it to engage. But 737 + PCU issues (yes, I know it's been redesigned) is never a good day. Hopefully we'll get a NTSB report on this one. reply throw0101d 6 hours agoprevFour minute video illustrating a dutch roll: * https://www.youtube.com/watch?v=9Gt-IcCBiQ4 Longer 15m video from Mentour Pilot with some more details: * https://www.youtube.com/watch?v=z2J_LUDWioA&t=1m reply qwertox 5 hours agoparentFirst video is very good for a clear explanation, second one to get a bit more comments from someone who knows. reply Tao3300 5 hours agoparentprevI was about to link the same video. It was fascinating and took some of the scare out, but if I'm ever in one, now I'm going to be sweating it that the pilot knows what he/she is doing and doesn't overcorrect. reply exabrial 5 hours agoprevWhat the news about Boeing has done is make me pay more attention to commercial aviation in general. The number of aviation incidents is actually pretty crazy, and no, Boeing is not at the center of them despite what the media would have you believe. What is amazing though is the actual injury and fatality rate is really low due to well trained crews and measured responses by air controllers. reply FabHK 5 hours agoparent> the actual injury and fatality rate is really low due to well trained crews and measured responses by air controllers. It’s the crew and ATC staff, definitely, but also - the amazingly well engineered airplanes (despite recent Boeing hiccups - just goes to demonstrate the high standards) with many redundancies; - the archaic, but robust ATC system; and - last but not least the excellent safety culture, which investigates problems thoroughly to the root, but without blaming individuals. reply rvnx 4 hours agorootparentBoeing could still argue \"you see, it's safe, redundant systems worked, we haven't killed anyone this time\" reply alistairSH 4 hours agoparentprevWhat really blows my mind in the number of near-misses on active runways (at least within the US). I don't know if the trend is up, as I don't have historic data, but certainly looking at Youtube (VASAviation, etc), there are a scary number of them (2 within the last 45 days at one of my local airports). reply whymauri 4 hours agorootparentI watched this video recently and found it really informative w.r.t. commercial aviation near misses: https://www.youtube.com/watch?v=mIiPt1YVkP8 It gives just enough detail around the classification system, rate of incidents, root causes, etc, that I could search for and learn more on my own after. reply dotancohen 4 hours agoparentprevThe word that you are looking for is resilience I believe. Commercial and GA aircraft are highly resilient, and the procedures built around that industry are highly resilient as well. reply giantg2 2 hours agorootparentAssuming one follows the procedures... Boeing... reply bell-cot 6 hours agoprevFor those unfamiliar with Dutch Roll - https://en.wikipedia.org/wiki/Dutch_roll Noteworth there: The 2013 fatal crash of a Boeing KC-135R, after its rudder PCU failed. The article places preliminary blame (for the new incident) on the 737-8's rudder PCU. One might wonder about the design, components, and processes commonalities between rudder PCU's for the KC-135R, and 737-8. reply banannaise 5 hours agoparentThe 737 rudder PCU also had a defect in the 90s that caused a series of major incidents, although obviously that's been fixed since: https://en.wikipedia.org/wiki/Boeing_737_rudder_issues reply rollulus 5 hours agoprevAs a Dutchman the endless list of apparent “Dutch” things never ceases to surprise me. Uncles, courage, ovens, double, going, etc., and a new one to my list: rolls. And none of them means any good. reply kleiba 5 hours agoparentHalf of them refer to \"German\" things, where the word \"deutsch\" underwent some pronunciational variation. reply izolate 5 hours agorootparentActually, a lot of these pejoratives are from the 19th century. By the 17th century, the word \"Dutch\" had begun to solidify its meaning around the Netherlands. This mainly stems from a time when tensions between the British and Dutch were at their peak, due to competition. reply yurishimo 5 hours agoparentprevAs a new resident, I also find it surprising! It's a shame that most of the substitutions seem to be negative however. Lots of really great stuff and people in NLAnd none of them means any good. Dude, Dutch ovens are awesome. One of the most useful cooking tools imo. reply xeromal 4 hours agorootparentDutch oven also means trapping your partner or kid under the blanket and farting. lol reply kqr 3 hours agoparentprevA Dutch book can be good depending on which side of it you are on! reply matt-attack 5 hours agoparentprevDon’t forget Angles. reply code_runner 5 hours agoparentprevdutch babies are a pretty decent desert! reply geekfactor 5 hours agoprevI didn't know what it was called, but I've experienced mild Dutch Roll enough on flights, usually on the approach or landing, that I didn't realize that it was all that rare or dangerous. I had come to think of it as a normal result of wind shear. I imagine it's a matter of degree, and I wish there was some mention of the magnitude of the Dutch Roll in this case vs what is considered normal or acceptable. (Edited to qualify my experience as \"mild\".) reply Out_of_Characte 4 hours agoparentHow did you tell the difference between dutch roll and the plane catching a glideslope? reply Filligree 7 hours agoprevThis just isn’t Boeing’s decade. reply dotancohen 4 hours agoparentUntil the McDonnell Douglas management retires, Boeing will not recover. It's not a matter of an unlucky decade, it's a matter of replacing engineering with MBAing. It's just that the latency between cause and effect are long on the engineering side of this industry. reply warcher 1 hour agorootparentThose guys are top tier you have to admit— tank their company, get acquired, keep their jobs, eventually wrangle their way into managing their acquirer, tank the acquiring company… this is god tier MBAing. reply fred_is_fred 4 hours agorootparentprevThe merger was 27 years ago. While it may have been the cause at the time, there’s no more McD to blame. Anyone who was a senior leader in 1997 has long since retired. This is Boeing and only Boeing’s problem. reply 42lux 5 hours agoparentprevFeels like IBM or Oracle. reply Havoc 6 hours agoprevThing is this could just keep going. There’s no telling how many bad apples Boeing put out there reply simplyluke 2 hours agoparentConsider that it’s also now true that “normal” incidents among the 100k+ daily commercial flights are now newsworthy if they involve a Boeing plane, when they wouldn’t have been a couple of years ago. You’re still far safer in any commercial flight than walking the dog in your neighborhood. reply CoastalCoder 6 hours agoparentprevIf it's a software issue, I'd think owners would be pretty clear on its status on each plane. But for physical defects, yeah that's scary. reply rl3 6 hours agorootparent>If it's a software issue, I'd think owners would be pretty clear on its status on each plane. Ha. I have some bad news for you: https://www.bloomberg.com/news/articles/2019-06-28/boeing-s-... reply CoastalCoder 5 hours agorootparentI can't read that article, but I'm assuming (hoping?) that even if the software is flawed, there's still clarity about which source code / build configuration is installed on a given plane. Contrast that to the allegedly faked test results, and known defects, of the planes' physical characteristics. reply rl3 4 hours agorootparentThe source article was talking about how their software practices were so bad they’d hire $9/hr engineers to work on core avionics stuff. As for your point: There’s not much contrast. MCAS was software, after all. Only upside to software is because it can be updated (maybe), it’s at least more likely to be fixed instead of covered up. reply krisoft 5 hours agorootparentprev> If it's a software issue, I'd think owners would be pretty clear on its status on each plane. That assume that it is a known software issue. If it is an issue with some unknown triggering factors how would they be clear on its status? reply omgJustTest 6 hours agoprevNot bigger news due to severity of issue, most likely. Safety report out today? Edit: yep, faa confirming today https://m.youtube.com/watch?v=Zmjam1evDD4 reply notfried 6 hours agoprevThis is what a Dutch Roll looks like: https://www.youtube.com/watch?v=Zmjam1evDD4 reply ceejayoz 6 hours agoparentThis seems to really under-sell the severity; the real-world example someone else linked is much more dramatic. https://www.youtube.com/watch?v=2tgfkGiHhxs reply giantg2 2 hours agorootparentWow that's an intense example. reply raverbashing 6 hours agoprevSounds like a Yaw Damper failure? https://en.wikipedia.org/wiki/Yaw_damper Not dangerous per se but very annoying for passengers reply wkat4242 4 hours agoprevDoesn't it have yaw dampeners to prevent this? Ps didn't have time to read the article yet, sorry. But I thought this issue was mitigated on the 737 reply ragebol 6 hours agoprevFor anyone else (besides me, being Dutch) wondering about the name: https://en.wikipedia.org/wiki/Dutch_roll?useskin=vector#Name TL;DR: it's from ice skating where there's a similar move. But no idea why it's called that in ice skating. reply shdon 6 hours agoparentIce skating... Interesting... My guess was it having to do with ships rocking side to side. Maar waarom een schaatsterm? reply yurishimo 5 hours agorootparentAt first I read that as `schatsterm` and thought it was cute, but then I looked it up to double check and it's still cool! But now I know that the word I thought it was is actually a `bijnaam`. Your language is fun and I have been enjoying learning it over the last couple of years. :) reply ragebol 5 hours agorootparentprevMaybe the ice skaters took it from ships, also an old Dutch tradition I suppose. reply AnimalMuppet 5 hours agorootparentI'm not sure if this is relevant, but: I have read that back in the day (1500s-1700s), the Dutch built flat-bottomed boats that could carry a lot, and the English built round-bottomed boats that handled better in the ocean. Maybe the way a Dutch boat handled in heavier seas is the origin of the term? reply ragebol 5 hours agorootparentSounds plausible, your guess is as good as mine reply shdon 3 hours agorootparentprevThat was indeed my guess. reply tekla 5 hours agoprevI'm going to to laugh at the bad takes on this thread, and await a more accurate report. Dutch roll is undesirable, but its a basic consequence of some converging aeronautical effects. reply zthrowaway 4 hours agoparentYes but the alarming thing here is the damaged PCU... reply tekla 4 hours agorootparentSure. but that's just like, OK, that by itself means very little. It could mean the standby PCU somehow activated by itself, maybe damage causing a un-commanded activation, maybe software bug, maybe God said fuck this plane in particular. The point is we have no idea since there are not enough details to know. But of course, on HN we have a army of people who think they know stuff because Wikipedia. https://trends.google.com/trends/explore?q=dutch%20roll&date... Yep, exactly what I suspected. reply amarant 6 hours agoprevIt kinda bothers me that airlines are still placing orders for new Boeing airplanes. Taking delivery of old orders is one thing, since they can't really back out of those, but placing new orders implies that some level of trust remains for this company, and I just don't see how that makes any sense at all? reply throw0101d 6 hours agoparent> It kinda bothers me that airlines are still placing orders for new Boeing airplanes. Are they? Boeing had almost nine hundred 737 MAX orders in 2023; in 2024 they have 66: * https://en.wikipedia.org/wiki/List_of_Boeing_737_MAX_orders_... Three hundred 787 orders in 2023; 10 in 2024: * https://en.wikipedia.org/wiki/List_of_Boeing_787_orders_and_... One hundred 777s in 2023, twenty-seven in 2024: * https://en.wikipedia.org/wiki/List_of_Boeing_777_orders_and_... Their numbers are down bigly. And who else are you going to order from? Airbus? How much capacity do they have to build more? reply ceejayoz 6 hours agorootparentI'd note Airbus is seeing the same pattern. The 737-MAX's A320neo competitor, for example: https://en.wikipedia.org/wiki/List_of_Airbus_A320neo_family_... 1,689 orders in 2023, 154 in 2024. https://en.wikipedia.org/wiki/List_of_Airbus_A220_orders_and... 141 orders in 2023, negative 12 in 2024. As a result, I presume the explanation is less \"no Boeing\" and more something like \"orders have a reporting lag\" or \"2023 was a big year for airlines post-COVID\". reply _heimdall 6 hours agorootparentThese stats alone seem like way more interesting news, suprised I haven't seen that before. I wonder why plane orders from the two biggest manufactures have dropped so dramatically and what it means economically. My mind immediately goes to whether airlines are quietly preparing for an expected downturn, or if airlines aren't selling to corporations because they're selling to governments and militaries. That could definitely be my own preconceptions sneaking in though. reply ceejayoz 6 hours agorootparentI wouldn't read too much into it yet; 2023's numbers look like they're abberantly high, which is probably the airlines catching up after the little economic hiccup we all had in 2020. reply MVissers 5 hours agorootparentprevCould be related to high interest rates and investments being more expensive. Also, I believe that Airbus has a 10 year backlog on their newest planes, so no point in shelling out a lot of cash right now for 10 years down the line. reply bossyTeacher 6 hours agorootparentprev> And who else are you going to order from? Airbus? How much capacity do they have to build more? the obvious choice is not to order rather than ordering faulty places because your greeds tells you that risking people's safety is worth the money you will get reply epolanski 6 hours agoparentprev1) Even if the max had hiccups, plenty of them, it is an absolutely safe airplane by any measure. 2) Your fleet may mostly be formed by Boeing airplanes, having all of those move to Airbus airplanes is also a safety issue. Pilots have to re-learn and even forget things they accrued and evolved to react instantly for decades. At some point you need to weigh the odds of increasing human error vs relatively safer Airbus alternative 3) Maintenance and logistics conversion would be a huge pain 4) If you order an A320 today, you're gonna get it no sooner than in a decade. Not taking your ordered Max is really not a smart idea. 5) Boeing hasn't received a single new order for max airplanes in two months reply throwup238 5 hours agorootparent> 1) Even if the max had hiccups, plenty of them, it is an absolutely safe airplane by any measure. Except fatalities. reply epolanski 4 hours agorootparentEven by fatalities. 737 Maxes have flown more than a couple of million of flights, at least, since they entered service, that puts their fatal incident rate at similar levels of the A320. The only two incidents, also, as you know, were related and both happened 5+ years ago. reply 42lux 5 hours agorootparentprev\"Some of you might die but that's a sacrifice I am willing to take.\" - some Boeing suit reply amarant 3 hours agorootparentprevEverything is relative, and compared to other airplane models, the 737max has absurdly high rate of fatality flights. The only commercial model in history with a higher rate of fatal crashes, as far as I can tell, was the Concorde. And we don't use those anymore. reply belter 6 hours agoparentprev\"Boeing sales tumble as company gets no orders for 737 MAX for 2nd straight month\" - https://news.ycombinator.com/item?id=40649943 reply masklinn 6 hours agoparentprevBoeings are still flying, and Airbus has no availability. On the neos, they have more than a decade worth of orders, and the order book A350 is not much shorter. reply impulser_ 4 hours agoparentprevThe Max 8 are great planes. I rode one recently, in fact it was with Southwest, it was probably the best plane I flown in. Lots of space, comfortable and quiet. They just need to fix the bad part issues they seem to have. Personally, I would fly on a Max 8 again. reply lastofthemojito 4 hours agorootparentLooking at Southwest is probably a good way to put the Boeing issues in perspective. It's easy to want to freak out over the negative headlines over the past few years, and the MCAS fiasco is a well-deserved black mark on Boeing's reputation. But again, looking at bigger perspective, Southwest Airlines operates an all-Boeing fleet of 800+ airlines doing 4000+ flights per day. About a million and a half flights per year. And in their 50+ years of operation, there have been 4 deaths associated with Southwest flights, 1 of which was a suicide where a guy walked onto a runway and another was an unruly passenger who tried to storm a cockpit. I don't want to let Boeing off the hook - their recent cock-ups absolutely seem to indicate cultural problems that need to be fixed so that quality can improve rather than continue to slide. But honestly, getting on a 737 Max to go on a trip still seems way safer than hopping on the highway. reply PeterHolzwarth 6 hours agoparentprevIt's interesting to see the dynamic of the downward spiral (no pun intended) \"pile on\" media cascade mechanic in action, through your comment. What I mean is, you are assuming now (you probably wouldn't have a year ago) that this Dutch Roll incident was caused by the plane being Boeing. So, why would anyone even buy a Boeing at this point? But what indication is there of that this Dutch Roll incident was caused by the plane being Boeing? As far as I can tell, Dutch Roll is a phenomena whose causes aren't fully understood, that rarely, but occasionally, happens to planes, not just to Boeing planes. But now that we are collectively bought-in on a Boeing hate-fest (and they certainly seem to have brought it on themselves, no doubt!), we just knee-jerk apply the cause to any issue seen as being explained by the aircraft being Boeing. reply bradleyjg 6 hours agoparentprevDo you think your car (or bike) is safer or more dangerous than the least safe Boeing plane in service? reply somenameforme 5 hours agorootparentThese numbers are always pretty BSy because the big dangers in flight are take-off and landing. Comparing distances just completely obfuscates everything for no real gain. You can illustrate this clearly with some sort of reductio ad absurdum. Imagine we have an interstellar ship traveling many trillions of miles, but that blows up 90% of the time. If you look at it in terms of this same deaths/mile metric it'd still be way safer than driving, but obviously it isn't - which emphasizes that the metric is misleading. The most reasonable way would be safety per average trip, with some sort of multiplier for particularly long or particularly short trips. After all that's exactly what people think these numbers mean, even though they most certainly don't. reply krisoft 5 hours agorootparentOkay. So let's compare accidents per flights to accidents per trip with bike/car. IATA reports 1.19 accidents per million flights in the 2019-2023 years. [1] Your turn to find a comparable number for road vehicles. 1: https://www.iata.org/en/pressroom/2024-releases/2024-02-28-0... reply somenameforme 4 hours agorootparentYou pretty quickly shifted the goal posts there didn't you? That's for all flight vessels, not just Boeing's 737 class. You're also misreading your stats (that's per sector, not per million), but in either case I'm more interested in fatality rates. Your site gives a hull loss rate of 1 per 4.94 million flights for jets. For vehicles, the average person makes two trips a day over a total of 29.2 miles. [1] So we can say the average trip is 14.6 miles. And the average fatality rate is 1.33 per 100 million miles traveled [2]. So that's 1.33 fatalities per (100 / 14.6) = 6.8 million trips, or 1 fatality per 5.17 million trips. So on a first level analysis, vehicles are slightly safer than jets on a trip for trip basis. But the math I'm doing dramatically understates the actual difference, because the mortality numbers I'm using for vehicles are per person, not per fatal incident. In other words 1 PERSON dies per 5.17 million vehicle trips. And I'm comparing that against one entire JET 'dying' per 4.94 million flights, which is generally going to have tens to hundreds of people on it. If you do an apples to apples comparison, vehicles would be some orders of magnitude safer than jets, trip for trip. Of course ideal would be to ignore population and just look at car incidents with at least 1 fatality vs plane incidents with at least 1 fatality, but I can't find those stats unfortunately. [1] - https://newsroom.aaa.com/2015/04/new-study-reveals-much-moto... [2] - https://www.iihs.org/topics/fatality-statistics/detail/state... reply krisoft 3 hours agorootparent> You pretty quickly shifted the goal posts there didn't you? No my dude. You said \"These numbers are always pretty BSy because the big dangers in flight are take-off and landing.\" That is a statement about these kind of statistics in general, not in specific to Boeings. But sure. How many accidents had Boeing during take-off or landing? > You're also misreading your stats (that's per sector, not per million) Negative. To quote \"All accident rate (accidents per one million flights)\" What is even per sector? > Your site Not my site? Are you arguing from good faith here? > If you do an apples to apples comparison, vehicles would be some orders of magnitude safer than jets, trip for trip. I will let everyone know who is considering commuting by jet twice a day for 14.6 miles. reply somenameforme 3 hours agorootparentThe post I was responding to was speaking precisely about \"the least safe Boeing plane in service.\" The fact that airplanes are, trip for trip, much safer than planes is something I did not appreciate at the time when making my post. I expected most planes to be safer than vehicles, but Boeings falling well below average. The fact that vehicles are substantially safer than any plane, trip for trip, is far beyond what I expected. And it's going to make Boeing's look like death traps by comparison. Imagine we're comparing a long flight thousands of miles around the world to a short little domestic flight between a couple of relatively nearby cities. Which flight do you expect to get the latest, most stable, and secure plane? Most of everybody would expect it to be the international flight. In reality, it's the short little domestic flight. The reason is that with planes a trip that goes on for thousands of miles is, in general, almost exactly as dangerous as the briefest of trips imaginable. Take off, landing, and pressurization are where basically all the risk (and wear and tear) come into play. So your little plane going on a hundred mile voyage is going to be exposed to far greater risks than the one traveling thousands of miles. It's why the stats, as typically offered - deaths/mile, are extremely disingenuous. And they serve very little purpose other than to mislead those who don't know better, which is the overwhelming majority of people. As for the exact numbers, reread your own source, and include the entire quote. You're having a failure where you keep misreading something the same way. Keep in mind you quoted 1.19 for 2019-2023. reply krisoft 2 hours agorootparent> You're having a failure where you keep misreading something the same way. And you are having a failure where you are not quoting what you think I'm misreading. Look at the table. First column labeled \"ACCIDENT TYPE\". Second row first column \"All accident rate (accidents per one million flights)\". Last column labeled \"5-YEAR AVERAGE (2019-2023)\" The data in second row last column \"1.19 (1 accident every 0.88 million flights)\". Go ahead and tell me where do you see \"per sector\" in any of that? reply somenameforme 1 hour agorootparentAhhh!!! I see how you're reading it, and yeah - I think the mistake is much more on them than you. The parenthesis are not describing the value before them, but indicating what is IN the parenthesis! I'm the sort that ignores infographics and goes straight to the text, and it's much clearer there. Check out the report highlights: --- The all accident rate was 0.80 per million sectors in 2023 (one accident for every 1.26 million flights), an improvement from 1.30 in 2022 and the lowest rate in over a decade. This rate outperformed the five-year (2019-2023) rolling average of 1.19 (an average one accident for every 880,293 flights). --- So in the table their \"All accident rate (accidents per one million flights)\" row is giving you two different pieces of the data. The first is accident rate (which is per million sectors) and then in the parenthesis is the accidents per million flights. And actually even that's wrong, since they actually give you flights per accident -- whoever put that table together clearly was not big on the whole accuracy thing. So when you see \"1.19 (1 accident every 0.88 million flights)\" that means the rate per million sectors is 1.19, and the rate per million flights is 1/0.88 = 1.14. reply drstewart 5 hours agorootparentprev>The most reasonable way would be safety per average trip Let's reductio ad absurdum this. I take my 1000CC motorcycle around the block every 10 minutes for a year. Safest transportation mode on the planet! Way safer than walking (which I did once down the middle of a highway, blindfolded) reply Zigurd 6 hours agorootparentprevPer mile or per hour? reply bradleyjg 4 hours agorootparentEither. reply afavour 5 hours agoparentprevSuch is the state of the airline business/capitalism. There are basically only two manufacturers: Boeing and Airbus. The airlines place orders years in advance and if you cancel a Boeing order now you’re a) out of a lot of money and b) joining the back of the line with Airbus. The simple reality is that the airlines (and their shareholders) consider a) and b) to be greater risks than the safety record of Boeing planes. Given the parameters they have they’re not wrong. Air travel is still incredibly safe, even if these Boeing incidents make it ever so slightly less safe. To be blunt, the market can absorb it and individual airlines have little motivation to change. If there is going to be a big change it’ll need to come from the FAA. reply bowsamic 5 hours agorootparentYeah, Airbus is extremely backed up with orders reply Sakos 6 hours agoparentprevSeems like you haven't seen the latest news. https://www.washingtonpost.com/business/2024/06/11/boeing-sa... New orders are significantly down. Old orders will rarely be cancelled even if possible simply because there's no alternative. Airbus's backlog is longer than Boeing's. And while currently Airbus's manufacturing rate is much higher currently, it won't make a difference if everybody switches to Airbus. reply FrustratedMonky 6 hours agoparentprevNo fan of Boeing. But where are you going to go to buy something else? There is only Airbus. There isn't a huge competitive market, it is just 2 companies left standing. Perhaps a good argument for fighting monopolies? And, while Boeing looks bad, when sales go down, corporations adapt. The pesky customers want \"Safety\", and we aren't selling \"Safety\", got to get some of this \"Safety\". Guess we better do some re-orgs, some focus groups, some training to make sure everyone knows \"Safety\" is important. Maybe throw in some training on \"Quality\". They'll adapt. reply salawat 6 hours agorootparent>The pesky customers want \"Safety\", and we aren't selling \"Safety\", got to get some of this \"Safety\". Guess we better do some re-orgs, some focus groups, some training to make sure everyone knows \"Safety\" is important. Maybe throw in some training on \"Quality\". Gah hahahaha. Oh God, don't make me laugh that hard. One does not merely add some more Quality to a product as if it were flour in a cooking recipe. Quality is literally the single most difficult to add back to something once you've decided to skimp on it; as quality itself is a compound output. It's a result of having your entire goddamn company devoted to the goal of creating a quantitatively superior product. Boeing decided it was time to not sweat the package, and to turn itself into a financial product. As it turns out; it's pretty much impossible to do one while not doing the other... reply dboreham 5 hours agorootparentIt all starts when psychopaths on wall st install psychopaths in the C suite. reply FrustratedMonky 4 hours agorootparentNot even hiding it. https://www.forbes.com/sites/jackmccullough/2019/12/09/the-p... reply yummypaint 6 hours agoparentprevInstitutional momentum at airlines is huge. Southwest flies only boeing to minimize training requirements and keep pilots interchangeable. Now they are appreciating the riskiness of that strategy in hindsight. reply outside1234 4 hours agoprevFor those of you that are more visual, here is a terrifying YouTube video of what Dutch Roll looks like: https://www.youtube.com/watch?v=2tgfkGiHhxs reply maxxtrawl 6 hours agoprevI will walk, thanks! reply outside1234 4 hours agoprevSo what you are saying is that we shouldn't fly the 737-8 MAX? reply n0us 6 hours agoprevHow does this impact the share holders? reply 0x445442 6 hours agoprev#NothingWorks reply abdellah123 6 hours agoprevHorses are very stable reply betaporter 6 hours agoparentI hever never once heard of a dutch roll occuring to a train. reply HPsquared 6 hours agorootparentThe rail equivalent is called hunting oscillation. https://en.m.wikipedia.org/wiki/Hunting_oscillation reply shdon 6 hours agorootparentprevAm on a Dutch train right now and there is a distinct lack of roll, Dutch or otherwise. That said, I wonder where the term Dutch Roll comes from. Perhaps the oscillation is reminiscent of the motion of a ship rocking side to side. The Dutch were a major sea power. reply koliber 6 hours agorootparentprevWhen it happens, it's catastrophic, and then it is not called a dutch roll. reply mlcrypto 6 hours agoprevThis is an aircraft worth taking great lengths to avoid. Cancelling important trips, meetings, etc. reply dboreham 5 hours agoparentStatistically it is not. Most of the time travellers don't know if they're on an Airbus or Boeing aircraft and hardly ever figure out that it's a MAX. reply mlcrypto 4 hours agorootparentInfinite loss multiplied by small chance is still infinite risk reply ceejayoz 4 hours agorootparentWhat daily activities don't have a small chance of the same risk? Eating has a small chance of choking to death. Walking has a small chance of getting hit by a falling tree branch. Pooping has a small chance of causing a brain aneurysm to pop. reply Aldipower 6 hours agoprev [–] You can buy dutch rolls in a lot of spaces in the Netherlands called coffeshop. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "On May 25th, 2024, a Southwest Airlines Boeing 737-8 MAX experienced a Dutch Roll mid-flight, leading to substantial structural damage, though the crew landed safely in Oakland, CA.",
      "The FAA classified the event as an accident, and the aircraft was moved to Everett, WA, for further repairs after a temporary fix in Oakland.",
      "Discussions among users highlight technical aspects, potential passenger alarm, and suggestions for Boeing to add a second independent Yaw Damper system for enhanced safety."
    ],
    "commentSummary": [
      "A Southwest Airlines Boeing 737-8 Max experienced a Dutch Roll, a phenomenon involving oscillations due to coupling between roll and yaw dynamic modes.",
      "Dutch Roll incidents have historical precedence, with notable cases involving different aircraft models, including a fatal crash of a Japan Airlines Boeing 747SR in 1985.",
      "The discussion highlights concerns about Boeing's engineering standards, the impact of management decisions, and the safety of commercial aviation, despite recent issues with the 737 Max."
    ],
    "points": 241,
    "commentCount": 207,
    "retryCount": 0,
    "time": 1718279591
  },
  {
    "id": 40670612,
    "title": "MLow: Meta's low bitrate audio codec",
    "originLink": "https://engineering.fb.com/2024/06/13/web/mlow-metas-low-bitrate-audio-codec/",
    "originBody": "POSTED ON JUNE 13, 2024 TO Web MLow: Meta’s low bitrate audio codec By Jatin Kumar, Bikash Agarwalla, Sriram Srinivasan, King Wei Hor, Tim Wong At Meta, we support real-time communication (RTC) for billions of people through our apps, including WhatsApp, Instagram, and Messenger. We are working to make RTC accessible by providing a high-quality experience for everyone – even those who might not have the fastest connections or the latest phones. As more and more people have relied on our products to make calls over the years, we’ve been working on new ways to ensure all calls have a solid audio quality. We’ve built the Meta Low Bitrate (MLow) codec: a new tool that improves audio quality especially for those on slow-speed connections. Figure 1: Increasing complexity or bitrate usually improves quality, but good codecs achieve higher quality while balancing the other two. RTC products use many building blocks to deliver the full experience, and one of the critical components is audio/video codecs. These codecs help compress the captured audio/video data so it can be sent across the internet efficiently to the recipient, keeping the experience real time. For example, the size of raw audio captured for a typical call is 768 kbps (mono, sampling at 48kHz, bit depth 16), which modern codecs are able to compress down to 25-30 kbps. Often this compression comes at the cost of some quality (loss of information), but good codecs can strike a balance among the trio of quality, bitrate, and complexity by exploiting deep knowledge about the nature of the audio signal as well as by using psychoacoustics. Building a good codec is quite challenging, and that is why we don’t see new codecs emerging very often. The last widely known, good open-source codec was Opus, released in 2012, which has become the codec of choice for the wide variety of applications on the internet. Meta has used Opus for all its RTC needs, and so far it has served us well – helping to deliver quality calls to billions of users across the globe. Our motivation for building a new codec Given the massive scale of RTC usage in Meta products, we get to see how a codec performs in a range of network scenarios and how it impacts the end user’s experience. In particular, we’ve observed that a significant chunk of calls have poor network connections throughout or for part of a call. Typically a bandwidth estimation module (BWE) detects the quality of the network, and as the network quality degrades, we need to lower the codec operating bitrate to avoid congesting the network and keep the audio flowing – impacting the trio balance referenced above. Complicating matters, conducting a video call despite poor network quality leaves little room for audio and pushes the audio bitrate further down. The lowest operating point for Opus is 6 kbps, at which it runs in NarrowBand mode (0 – 4kHz) and does not adequately capture all the sound frequencies produced by human voices—and so doesn’t sound as clear or natural. Here is an example of how Opus sounds at 6kbps and the corresponding reference file for comparison. Raw reference signal: Opus @ 6 kbps NarrowBand (NB): Over the last two years, we have seen development of some new machine learning (ML)-based audio codecs that provide good quality audio at very low bitrates. In October of 2022, Meta released Encodec, which achieves amazingly crisp audio quality at very low bitrates. While these AI/ML-based codecs are able to achieve great quality at low bitrates, it often comes at the expense of heavy computational cost. Consequently, only the very high-end (expensive) mobile handsets are able to run these codecs reliably, while users running on lower-end devices continue to experience audio quality issues in low-bitrate conditions. So the net impact of these newer computationally expensive codecs is actually limited to a small portion of users. A significant number of our users still use low-end devices. For example, more than 20 percent of our calls are made on ARMv7 devices, and 10’s of millions of daily calls on WhatsApp are on 10-year-old-plus devices. Given the readily available codec choices and our commitment to ensure that all users – regardless of what device they’re on – have a quality calling experience, we clearly need a codec with very low-compute requirements that still delivers high-quality audio at these lowest bitrates. The MLow codec We broke ground with our development of a new codec in late 2021. After nearly two years of active development and testing, we are proud to announce Meta Low Bitrate audio codec, aka MLow, which achieves two-times-better quality than Opus (POLQA MOS 1.89 vs 3.9 @ 6kbps WB). Even more importantly, we are able to achieve this great quality while keeping MLow’s computational complexity 10 percent lower than that of Opus. Figure 2 below shows a MOS (Mean Opinion Score) plot on a 1-5 scale and compares the POLQA scores between Opus and MLow at various bitrates. As the chart makes evident, MLow has a huge advantage over Opus at the lowest bitrates, where it saturates quality faster than Opus. Figure 2: POLQA score comparing Opus (WB) versus MLow at various bitrates across a large dataset of files. We have already fully launched MLow to all Instagram and Messenger calls and are actively rolling it out on WhatsApp—and we’ve already seen incredible improvement in user engagement driven by better audio quality. Here are some audio samples for you to listen to. We suggest that you use your favorite pair of headphones to appreciate the striking audio-quality differences. Opus 6 kbps NB MLow 6 kbps WB Reference Being able to encode high-quality audio at lower bitrates also unlocks more effective Forward Error Correction (FEC) strategies. Compared with Opus, with MLow we can afford to pack FEC at much lower bitrates, which significantly helps to improve the audio quality in packet loss scenarios. Here are two audio samples at 14 kbps with heavy 30 percent receiver-side packet loss. Opus: MLow: Note that at these bitrates, Opus is not able to encode any inband FEC. It needs a minimum of 19 kbps to encode any inband FEC at 10 percent packet loss, which hurts the audio recovery. MLow internals MLow builds on the concepts of a classic CELP (Code Excited Linear Prediction) codec with advancements around excitation generation, parameter quantization, and coding schemes. Figure 3 is a high-level visual of how the codec works internally. On the left we have an input signal (raw PCM audio) feeding into the encoder, which then splits the signal into two low and high-frequency bands. Then, each band is encoded separately while making use of shared information to achieve better compression. All the output is passed through a range encoder to further compress and generate an encoded payload. The decoder does the exact opposite when given the payload to generate output audio signals. Figure 3: High level MLow encoder and decoder architecture. With these split-band optimizations, we are able to encode the high band using very few bits, which lets MLow deliver SuperWideBand (32kHz sampling) using a much lower bitrate. What’s next? MLow has greatly enhanced audio quality on low-end devices while still ensuring calls are end-to-end encrypted. We are really excited about what we have accomplished in just the last two years—from developing a new codec to successfully shipping it to billions of users around the globe. We’re continuing to work on improving the audio recovery in heavy packet loss networks by pumping out more redundant audio, which MLow allows us to do efficiently. We’re excited to share more as we continue working to make it easier for all our users to make quality audio calls. Share this: Facebook Threads X LinkedIn Hacker News Email",
    "commentLink": "https://news.ycombinator.com/item?id=40670612",
    "commentBody": "MLow: Meta's low bitrate audio codec (fb.com)197 points by mikece 3 hours agohidepastfavorite104 comments lxgr 2 hours agoAll these new, low-bitrate codecs are amazing, but ironically I suspect that they won't actually be very useful in most of the scenarios Meta is using them: To keep latency low in real-time communications, the packet rate needs to be relatively high, and at some point the overhead of UDP, IP, and lower layers starts dominating over the actual payload. As an example, consider (S)RTP (over UDP and IP): RTP adds at least 12 bytes of overhead (let's ignore the SRTP authentication tag for now); UDP adds 8 byte, and IPv4 adds 20, for a total of 40. At at typical packet rate of 50 per second (for a serialization delay of 1/50 = 20ms), that's 16 kbps of overhead alone! It might still be acceptable to reduce the packet rate to 25 per second, which would cut this in half for an overhead of 8 kbps, but the overhead would still be dominating the total transmission rate. Where codecs like this can really shine, though, is circuit-switched communication (some satphones use bitrates of around 2 kbps, which currently sound awful!), or protocol-aware VoIP systems that can employ header compression such as that used by LTE and 5G in IMS (most of the 40 bytes per frame are extremely predictable). reply toast0 1 hour agoparentLatency is the mind killer, but if available bandwidth is low, you save a ton of overhead by bundling 2-5 of your 20ms samples. Enough that the codec savings start to make sense, even though 100ms packets adds a ton of latency. Fancier systems can adapt codecs and samples per packet based on current conditions. The one I work on is a static codec and 60 ms of audio per packet, which isn't ideal, but allows us to run in low bandwidth much better than 20 ms per packet. Edit to add: Meta can also afford to add a bit more sampling delay, because they've got very wide distribution of forwarding servers (they can do forwarding in their content appliances embedded in many ISPs), which reduces network delay vs competing services that have limited ability to host forwarding around the globe. Peer to peer doesn't always work and isn't always lower delay than going through a nearby forwarding server. reply tgtweak 2 hours agoparentprevI think this is likely incorrect based on how much voice/audio distribution meta does today with facebook (and facebook live), instagram and whatsapp - moreso with whatsapp voice message and calling given it's considerable market share in countries with intermittent and low-reliability network connectivity. The fact it is more packet-loss robust and jitter-robust means that you can rely on protocols that have less error correction, segmenting and receive-reply overhead as well. I don't think it's unreasonable to assume this could reduce their total audio-sourced bandwidth consumption by a considerable amount while maintaining/improving reliability and perceived \"quality\". Looking at wireshark review of whatsapp on an active call there was around 380 UDP packets sent from source to recipient during a 1 minute call, and a handful of TCP packets to whatsapp's servers. That would yield a transmission overhead of about 2.2kbps. quick edit to clarify why this is: you can see starting ptime (audio size per packet) set to 20ms here, but maxptime set to 150ms, which the clients can/will use opportunistically to reduce the number of packets being sent taking into consideration the latency between parties and bandwidth available. (image): https://www.twilio.com/content/dam/twilio-com/global/en/blog... reply lxgr 2 hours agorootparentWhat part of that calculation is incorrect in your view? > 380 UDP packets sent from source to recipient during a 1 minute call, and a handful of TCP packets to whatsapp's servers. That would yield a transmission overhead of about 2.2kbps. That sounds like way too many packets! 380 packets per second, at 40 bytes of overhead per packet, would be almost 120 kbps. My calculation only assumes just 50, and that’s already at a quite high packet rate. > you can rely on protocols that have less error correction You could, but there's no way to get a regular smartphone IP stack running over Wi-Fi or mobile data to actually expose that capability to you. Even just getting the OS's UDP stack (to say nothing of middleboxes) to ignore UDP checksums and let you use those extra four bytes for data can be tricky. Non-IP protocols, or even just IP or UDP header compression, are completely out of reach for an OTT application. (Networks might transparently do it; I'm pretty sure they'd still charge based on the gross data rate though, and as soon as the traffic leaves their core network, it'll be back to regular RTP over UDP over IP). What they could do (and I suspect they might already be doing) is to compress RTP headers (or use something other than RTP) and/or pick even lower packet rates. > I don't think it's unreasonable to assume this could reduce their total audio-sourced bandwidth consumption by a considerable amount while maintaining/improving reliability and perceived \"quality\". I definitely don't agree on the latter assertion – packet loss resilience is a huge deal for perceived quality! I'm just a bit more pessimistic on the former, unless they do the other optimizations mentioned above. reply markus92 2 hours agorootparentI think you’re misreading OP, as he says 380 packets per minute, not second. That would give you an overhead of 253 bytes per second, sounds a lot more reasonable. reply tgtweak 1 hour agorootparentYes 380/min = ~6/s which is a very open ptime of >100ms, this can also be dynamic and change don the fly. It ultimately comes down to how big the packet can be before it gets split which is a function of MTU. If you have 50ms of latency between parties, and you are sending 150ms segments, you'll have a perceived latency of ~200ms which is tolerable for voice conversations. One other note is that this is ONLY for live voice communication like calling where two parties need to hear and respond within a resonable delay - for downloading of audio messages or audio on videos, including one-way livestreams for example, this ptime is irrelevant and you're not encapsulating with SRTP - that is just for voip-like live audio. There is a reality in what OP posted which is that there is diminishing returns in actual gains as you get lower in the bitrate, but modern voice implementations in apps like whatsapp are using dynamic ptime and are very smart about adapting the voice stream to account for latency, packet loss and bandwidth. reply lxgr 1 hour agorootparentprevWow, that would be an extremely low packet rate indeed! That would definitely increase the utility of low bitrate codecs by a lot, at the expense of some latency (which is probably ok, if the alternative is not having the call at all). reply roman-holovin 2 hours agorootparentprevI read it as in 380 packets per whole call, which was a minute long, not 380 packets per second during 1 minute. reply mikepavone 1 hour agorootparentThat's about 160 ms of audio per packet. That's a lot of latency to add before you even hit the network reply ant6n 1 hour agorootparentAssuming continuous sound. You don’t need to send many packets for silence. reply lxgr 52 minutes agorootparentVoice activity detection and comfort noise have been available in VoIP since the very beginning, but now I wonder if there's some clever optimization that could be done based on a semantic understanding of conversational patterns: During longer monologues, decrease packet rates; for interruptions, send a few early samples of the interrupter to notify the speaker, and at the same time make the (former) speaker's stack flush its cache to allow \"acknowledgement\" of the interruption through silence. In other words, modulate the packet rate in proportion to the instantaneous interactivity of a dialogue, which allows spending the \"overhead budget\" where it matters most. reply newobj 2 hours agorootparentprevpretty sure they said 380 packets total in the 1 minute call (~6-7/s) reply vel0city 2 hours agoparentprevAnother interesting use case for these kinds of ultra-low bitrate voice compression systems are digital radio systems. AMBE+2 and similar common voice codecs used on radio systems sound pretty miserable and don't handle dropped packets nearly as gracefully as compared to these newer codecs. reply yalok 38 minutes agoparentprevthis codec is for RTC comms - it supports 20ms frame rate. They did mention it's launched in their calling products: \"We have already fully launched MLow to all Instagram and Messenger calls and are actively rolling it out on WhatsApp—and we’ve already seen incredible improvement in user engagement driven by better audio quality.\" reply lukevp 1 hour agoparentprevWhy would you need 50 packets per second vs 10? Is 100ms not acceptable but 20ms is? reply tgtweak 1 hour agorootparentDefault configuration for SIP used to be 20ms, the rationale behind it was actually sourced in the fact that most SIP was done on LANs and inter-campus WAN which had generally high bitrate connectivity and low latency. The lower the packet time window the sooner the recipient could \"hear\" your voice, and if there were to be packet loss, there would be less of an impact if that packet were dropped - you'd only lose 20ms of audio vs 100ms. The same applies for high bitrate but high latency (3g for example) connectivity - you want to take advantage of the bandwidth to mitigate some of the network level latency that would impact the audio delay - being \"wasteful\" to ensure lower latency and higher packet loss tolerance. Pointedly - if you had a 75ms of one-way latency (150ms RTT) between two parties, and you used a 150ms audio segment length (ptime) you'd be getting close to the 250ms generally accepted max audio delay for smooth two-way communication. the recipient is hearing your first millisecond of audio 226ms later at best. If any packet does get lost, the recipient would lose 150ms of your message vs 20ms. Modern voice apps and voip use dynamic ptime (usually via \"maxptime\" which specifies the highest/worst case) in their protocol for this reason - it allows the clients to optimize for all combinations of high/low bandwidth, high/low latency and high/low packet loss in realtime - as network conditions can often change during the course of a call especially while driving around or roaming between wifi and cellular. reply lxgr 46 minutes agorootparent> the rationale behind it was actually sourced in the fact that most SIP was done on LANs and inter-campus WAN which had generally high bitrate connectivity and low latency In addition to that, early VoIP applications mostly used uncompressed G.711 audio, both for interoperability with circuit switched networks and because efficient voice compression codecs weren't yet available royalty-free. G.711 is 64 kbps, so 12 kbps of overhead are less than 25% – not much point in cutting that down to, say, 10% at the expense of doubling effective latency in a LAN use case. reply hubraumhugo 1 hour agoprevIs it just my perception or has Meta become cool again by sharing a ton of research and open source (or open weights) work? Facebook's reputation was at the bottom, but now it seems like they made up for it. reply nine_k 1 hour agoparentI have the same impression. Facebook the social network reputation may be not shiny, but Meta the engineering company reputation is pretty high, to my mind. It's somehow similar to IBM, who may look not stellar as a hardware or software solutions provider, but still have quite cool research and microelectronics branches. reply danuker 1 hour agoparentprevI don't think they made up for it. They are training AIs off of personal data. The open stuff are a desperate red herring. https://www.theregister.com/2024/06/10/meta_ai_training/ reply mrguyorama 1 hour agoparentprevHow the hell does releasing one audio codec undo years and years of privacy nightmare, being a willing bystander in an actual genocide, experimenting with the emotions of depressed kids, and collusion to depress wages? reply risho 1 hour agorootparentit isn't just one audio codec. they also released and continue to release the best self hostable large language model weights, they have authored many open source projects that are staples today such as zstandard, react, pytorch reply stuxnet79 43 minutes agorootparentprevYou will need to provide citations on the last point as Facebook are widely known to have broken the gentleman's agreement between Apple and Google that was suppressing tech pay in the early 2010s. reply giraffe_lady 26 minutes agorootparentOK sure even if they didn't do that we're still left with \"knowingly abetted a genocide\" which no amount of open source work can ever balance out. reply rylittle 3 minutes agorootparentcontext? reply pt_PT_guy 1 hour agorootparentprevthey also did release LLM models, and zstd, and mold, and and and... a lot of stuff reply cheema33 1 hour agorootparentDon't forget React. The most popular frontend stack at the moment. Been that way for some time. And GraphQL, Relay, Stylex... reply visarga 1 hour agorootparentprevReact and Pytorch compare that to Angular and TensorFlow, such a difference in culture reply gorkish 3 hours agoprevThe lack of any reference or comparison to Codec2 immediately leads me to question the real value and motivation of this work. The world doesn't need another IP-encumbered audio codec in this space. reply muizelaar 3 hours agoparentThey also don't compare with Lyra (https://github.com/google/lyra) reply gorkish 2 hours agorootparentOr speex narrowband or others. I think the tendency to pick Opus is just because it has a newer date on it -- its design goals were not necessarily to optimize for low bitrate; Opus just happened to still sound OK when the knob was turned down that far. One other point I intended to make that is not reflected in many listening/comparison tests offered by these presentations -- in the typical applications of low bitrate codecs, they absolutely must be able to gracefully degrade. We see Mlow performing at 6kbps here; how does it perform with 5% bit errors? Can it be tuned for lower bitrates like 3kpbs? A codec with a 6kbps floor that garbles into nonsense with a single bit flip would be dead-on-arrival for most real world applications. If you have to double the bitrate with FEC to make it reliable, have you really designed a low bitrate codec? The only example we heard of mlow was 30% loss on a 14kbps stream = 9.8kbps. Getting 6kbps through such a channel is a trivial exercise. reply DragonStrength 2 hours agorootparentMy understanding was Opus was specifically developed with the idea of replacing both Speex and Vorbis. \"Better quality than Speex\" is literally one of their selling points, so I'd be interested to hear more details. reply cvg 3 hours agorootparentprevNice. Google's soundstream already has some great quality. Some examples at 6kbps here: https://google-research.github.io/seanet/soundstream/example... reply Dwedit 2 hours agoparentprevThere's also the LPCNet Codec (2019), which does wideband speech at 1.6kb/s by using a recurrent neural network. https://jmvalin.ca/demo/lpcnet_codec/ reply dbcurtis 10 minutes agoprevWhat is the license? I searched but could not find anything. reply annoyingnoob 1 minute agoprevI wonder how it sounds compared to G.729. I worked for a company 20 years ago that had a modified G.729 codec that could go below 8kbps but sounded decent. We used this for VoIP over dial-up Internet, talk about low bandwidth. Turns out some of the more interesting bits were in the jitter buffer and ways to manage the buffer. Scrappy connections deliver packets when they can and there is an art to managing the difference between the network experience and the user experience. For communications, you really need to manage the user experience. reply aidenn0 1 hour agoprevOnly slightly OT: ELI5: Why is a typical phone call today less intelligible than a 8kHz 8-bit μ-law with ADPCM from the '90s did? [edit] s/sound worse/less intelligible/ reply toast0 1 hour agoparentDepends on your call; u-law has poor frequency response and reasonable dynamic range. Not great for music, but ok enough for voice, and it's very consistent. 90s calls were almost all circuit switched in the last mile, and multiplexed per sample on digital lines (T1 and up). This means very low latency and zero jitter; there would be a measurable but actually imperceptible delay versus an end to end analog circuit switched call; but digital sampling near the ends means there would be a lot less noise. Circuit switching also means you'd never get dropped samples --- the connection is made or its not, although sometimes only one-way. Modern calls are typically using 20 ms samples, over packet switched networks, so you're adding sampling delay, and jitter and jitter buffers. The codecs themselves have encode/decode delay, because they're doing more than a ADC/DAC with a logarithm. Most of the codecs are using significantly fewer bits for the samples than u-law, and that's not for free either. HD Voice (g.722.2 AMR-Wide Band) has a much larger frequency pass band, and sounds much better than GSM or OPUS or most of these other low bandwidth codecs. There's still delay though; even if people will tell you 20-100ms delay is imperceptible, give someone an a/b call with 0 and 20 ms delay and they'll tell you the 0 ms delay call is better. reply sva_ 34 minutes agoparentprevHearing ability deteriorates with age. reply skygazer 1 hour agoparentprevDoes decrease in intelligibility correlate with the instance count of concert seats in front of the loud speakers back in the oughts? reply dsign 44 minutes agoprevCan I use this to make music? A little bit on a tangent, a technique called Linear Predictive Coding, which was developed by telecoms in the sixties and seventies, has a calculated bandwidth of 2.5 kbit/s. The sound quality is not any good, and telephone companies of the time didn't use it for calls, but the paper I read describing the technique says the decoded speech is \"understandable\". LPC found its way into musical production, in a set of instruments called \"vocoders\" used to distort a singer's voce. There are, for example, variations of it in something called \"Orange Vocoder IV\". So, now I'm wondering, can MLow be used to purposefully introduce interesting distortions in speech? Or even change a singer's voice color? reply WalterSear 19 minutes agoparentJust use Digitalis :) https://www.youtube.com/watch?v=bA23ysR2hAo reply dgmdoug 2 hours agoprevThey also don't do a comparison with Pied Piper. reply mig39 1 hour agoparentIt might have a Weissman score in the fives, but I haven't seen a real-world implementation. Does it use middle-out compression? reply zekica 3 hours agoprevHonest question: why do we need to optimize for 32kbps and there we have AMR-WB or Opus (Opus even has in-band FEC at these bitrates so packet loss is not that catastrophic). Maybe it's useful in satellite direct-to-phone use-cases? reply cornstalks 3 hours agoparentThere’s a section (“Our motivation for building a new codec”) in the article that directly addresses this. Assuming you have >32 kbps bandwidth available is a bad assumption. reply nicce 1 hour agorootparentThe best assumption would be that you either have connection available or not available. Then, if it is available, what is the minimal data rate for connections which are available in general? If we do statistical analysis for that, is it lower that 32 kbps? How significantly? For some reason, I would assume that if you have connection, it is faster than 2G these days. reply sangnoir 18 minutes agorootparent> For some reason, I would assume that if you have connection, it is faster than 2G these days. That assumption does not hold for a sizable chunk of Meta's 3.98B-strong userbase. The list of counties that switched off 2G is surprisingly short. reply zamadatix 49 minutes agorootparentprevThe question isn't really the minimal bandwidth of the PHY rate it's about the goodput for a given reliability. Regardless of your radio there will always be some point where someone is at the edge of a connection and goodput is less than minimal PHY bandwidth. The call then turns choppy/into a time stretched robot you get every other syllable from. The less data you need to transmit + the more FEC you can fit in the goodput then the better that situation becomes. Not to mention \"just because I have some minimal baseline of $x kbps doesn't mean I want $y to use all of it the entire time I'm on a call if it doesn't have to\". reply hokumguru 3 hours agoparentprevThere exist a few billion people without LTE. Meta doesn’t only operate in the western world. reply noprocrasted 3 hours agorootparentAre there really many situations where a 10kbps connection would actually be stable enough to be usable? Usually when you get these kinds of speeds it means the underlying connection is well and truly compromised, and any kind of real-time audio would fail anyway because you're drowning in a sea of packet loss and retransmissions. Even in cases where you do get a stable 10kbps connection from upstream, how are you going to manage getting any usable traffic through it when everything nowadays wastes bandwidth and competes with you (just look at any iOS device's background network activity - and that's before running any apps which usually embed dozens of malicious SDKs all competing for bandwidth)? reply gorkish 2 hours agorootparentYes; backhaul connections in telephony applications are often very stable and are already capacity managed by tuning codec bandwidth. Say you are carrying 1000 calls with uLaw (64kbps * 1000) over a pair of links and one fails. Do you A) carry 500 calls on the remaining link B) stuff all calls onto the same link and drop 50% of the packets or C) Change to a 32kbps codec? It seems you may be imaging the failure case where your \"ISP is slow\" or something like that due to congestion or packet loss -- as I posted elsewhere in the thread the bandwidth is only one aspect of how a \"low bitrate\" codec may be expected to perform in a real world application. How such a codec degrades when faced with bit errors or even further reduced channel capacity is often more important in the real application. These issues are normally solved with things like FEC which can be incorporated as part of the codec design itself or incorporated as part of the modem/encoding/modulation of the underlying transport. reply wmf 2 hours agorootparentFacebook Messenger and WhatsApp don't run over TDM though. If WhatsApp is only getting ~10 kbps, that's due to extreme congestion. reply gorkish 1 minute agorootparentYes; but what is your point? A congested network like you describe isnt ever going to reliably carry realtime communications anyway due to latency and jitter. All you could reasonably due to 'punch through' that situation is to use dirty tricks to give your client more than its fair share of network resources. 6kbps is 10x less data to transfer than 64kbps, so for all the async aspects of Messenger or WhatsApp there is still enormous benefit to smaller data. dspillett 2 hours agorootparentprev> Are there really many situations where a 10kbps connection would actually be stable enough to be usable? Yes (most likely: that was an intuited “yes” not one born of actually checking facts!). There are many places still running things over POTS rather than anything like (A)DSL, line quality issues could push that down low and even if you have a stable 28kbit/s you might want to do something with it at the same time as the audio comms. Also, you may be trying to cram multiple channels over a relatively slow (but stable) link. Given the quality of the audio when calling some support lines I suspect this is very common. Furthermore, you might find a much faster unstable connection with a packet-loss “correcting” transport layered on top effectively producing a stable connection of much lesser speed (though you might get periods of Are there really many situations where a 10kbps connection would actually be stable enough to be usable? Scroll to this part of the article: >Here are two audio samples at 14 kbps with heavy 30 percent receiver-side packet loss. reply zeroxfe 2 hours agorootparentprev> Are there really many situations where a 10kbps connection would actually be stable enough to be usable? Yes there are. We ran on stable low bandwidth connections for a very long time before we had stable high bandwidth connections. A large part of the underdeveloped world has very low bandwidth, and use 5 - 10 Kbps voice channels. reply noprocrasted 2 hours agorootparent> We ran on stable low bandwidth connections Are you talking about the general \"we\" or your situation in particular? For the former, yes sure we started with dial-up, then DSL, etc, but back then software was built with these limitations in mind. Constant background traffic for \"product improvement\" purposes would be completely unthinkable 20 years ago; now it's the norm. All this crap (and associated TLS handshakes) quickly adds up if all you've got is kilobits per second. reply dspillett 2 hours agorootparent> Are you talking about the general \"we\" I assume the general-ish “we”, where it is general to the likes of you and I (and that zeroxfe). There are likely many in the world stuck at the end of connections run over tech that this “general subset” would consider archaic, and that zeroxfe was implying their connections, while slow, may be similarly stable to ours back then. Also, a low bandwidth stable connection could be one of many multiplexed through a higher bandwidth stable connection. reply zeroxfe 2 hours agorootparentprevLet's not move the goalposts here :-) The context is an audio codec, not heavyweight web applications, in response to your question \"Are there really many situations where a 10kbps connection would actually be stable enough to be usable?\" And I'm saying yes, in that context, there are many situations, like VoIP, where 10kbps is usable. Nobody here would argue that 10kbps is usable today for the \"typical\" browser-based Internet use. reply bogwog 3 hours agorootparentprevI don't know what you consider \"stable enough\", but the 30% packet loss demo in the article is pretty impressive. reply treflop 1 hour agorootparentprevEven in the Western world, you can appreciate low bandwidth apps even you are a music festival or traveling through relative wilderness. reply lxgr 2 hours agoparentprevMeta's use case are OTT applications on the Internet, which are usually billed per byte transmitted. Reducing the bitrate for the audio codec used lets people talk longer per month on the same data plan. That said, returns are diminishing in that space due to the overhead of RTP, UDP and IP; see my other comment for details on that. reply gorkish 3 hours agoparentprevIt's useful. AMBE currently has a stranglehold in this area and by any and every measurable metric, AMBE is terrible and should be burned in the deepest fires of hell and obliterated from all of history. reply kylehotchkiss 2 hours agoparentprevMaybe something like this would be helpful for Apple to implement voice messages over satellite. Also a LOT of people in developing countries use WhatsApp voice messages with slow network speeds or expensive data. It's too easy to forget how big an audience Meta has outside the western world reply londons_explore 3 hours agoparentprevInternet connectivity tends to have a throughput vs latency curve. If you need reliable low latency, as you want for a phone call, you get very little throughput. Examples of such connections are wifi near the end of the range, or LTE connections with only one signal bar. In those cases, a speedtest might say you have multiple megabits available, but you probably only have kilobits of bandwidth if you want reliable low latency. reply lxgr 2 hours agorootparentLoad ratios of > 0.5 are definitely achievable without entering Bufferbloat territory, and even more is possible using standing queue aware schedulers such as CoDel. Also, Bufferbloat is usually not (only) caused by you, but by people sharing the same chokepoint as you in either or both directions. But if you're lucky, the router owning the chokepoint has at least some rudimentary per-flow or per-IP fair scheduler, in which case sending less yourself can indeed help. Still, to have that effect result in a usable data rate of kilobits on a connection that can otherwise push megabits (disregarding queueing delay), the chokepoint would have to be severely overprovisioned and/or extremely poorly scheduled. reply zekica 2 hours agorootparentprevYes, but it doesn't have to be. Have you looked into Dave Taht's crusade against buffers? reply lxgr 2 hours agorootparentCorrect buffer sizing isn't a good solution for Bufferbloat: The ideal size corresponds to the end-to-end bandwidth-delay product, but since one buffer can handle multiple flows with greatly varying latencies/delays, that number does not necessarily converge. Queueing aware scheduling algorithms are much more effective, are readily available in Linux (tc_codel and others), and are slowly making their way into even consumer routers (or at least I hope). reply sogen 3 hours agoparentprevI'm assuming they'll just re-encode everything, for every user, to a lower bitrate using this codec. So, with their huge user base they'll be saving a gazillion terabytes hourly, that's what I concluded from their \"2 years in the making\" announcement. reply ajb 1 hour agorootparentIf you mean for storage, real time codecs are actually pretty inefficient for that use case because they don't get much use of temporal redundancy. Although I'm not actually aware of a non-real time audio codec specialised for voice. They probably exist in Cheltenham and Maryland but for Meta this likely doesn't make a big enough part of their storage costs to bother reply hateful 3 hours agoparentprevIt's not only about the end that's receiving, it's also the end that's transmitting 10kbps * thousands of users. reply ThrowawayTestr 3 hours agoparentprev> why do we need to optimize for <10kbps? Because some people have really slow internet reply chronogram 3 hours agoprevNo mention of NoLACE make the comparison samples a bit less useful: https://opus-codec.org/demo/opus-1.5/ reply sitkack 10 minutes agoparentThis is really cool and I very very very much appreciate that xiph puts so much work into standardization. https://datatracker.ietf.org/wg/mlcodec/documents/ It would be nice if Meta donated this to the world so we have less anchors for patent trolls and can transition the future we deserve. reply jamal-kumar 2 hours agoparentprevThat does sound very nice reply victorp13 56 minutes agoprevDoes anyone happen to know if ChatGPT's voice feature uses audio compression similar to Opus? Especially the \"heavy 30 percent receiver-side packet loss\" example sounds a LOT like the experience I have sometimes. reply animanoir 2 hours agoprevNice technology, tho Opus adds that warm sound I love... reply thrtythreeforty 2 hours agoprevAre they releasing this or is this just engineering braggadocio? I can't find any other references to MLow other than this blog post. Facebook/Meta AI Research does cool stuff, and releases a substantial portion of it (I dislike Facebook but I can admit they are highly innovative in the AI space). reply sllabres 2 hours agoparentIf you think about 'implementing then algorithm in a product' it seems so: (From the article) \"We are really excited about what we have accomplished in just the last two years—from developing a new codec to successfully shipping it to billions of users around the globe\" reply theoperagoer 1 hour agoprevWas hoping this would have a GitHub link ... reply mcoliver 1 hour agoprevMaybe SiriusXM can pick this up. The audio quality is generally awful but especially on news/talk channels like Bloomberg and CNBC. There is no depth or richness to the voices. reply tgtweak 1 hour agoparentIt actually comes down to the SiriusXM receiver that is being used - I've witnessed the built-in sirius/xm on on the latest GM platform (a $100,000+ Cadillac) sounding like AM radio to immediately sitting in a better-than-apple-streaming quality rendition of the exact same channel on an older lexus a few minutes apart... The mobile xm receivers (like ipods) that they used to sell also had very good quality and I never noticed any quality shortcomings even with good headphones. I think the \"high\" quality stream is 256kbps/16k which is fairly high compared to most streaming services that come in around 128/160. reply sitkack 8 minutes agorootparentI am archiving some music at 40kbps using Opus and the quality is pretty amazing. I think once things get over 20+kbps all the codecs start sounding pretty good (relative to the these low bitrates). I still prefer flac if possible. reply amelius 1 hour agoprevCan't we have an audio codec that first sends a model of the particular voice, and then starts to send bits corresponding to the actual speech? reply neckro23 29 minutes agoparentThis is actually an old idea, minus the AI angle (1930s). It’s what voders and vocoders were originally designed for, before Kraftwerk et al. found out you can use them to make cool robot voices. reply roywiggins 1 hour agoparentprevYou need a bunch of bandwidth upfront for that, which you might not have, and enough compute available at the other end to reconstruct it, which you really might not have. reply amelius 37 minutes agorootparentRegarding your first point, how about reserving a small percentage of the bandwidth for a model that improves incrementally? reply 77pt77 3 hours agoprevWhere is the source code? reply barbazoo 2 hours agoprevThat's great, now they can reach even more developing countries and do damage the way they did for example in Myanmar [1]. [1] https://www.amnesty.org/en/latest/news/2022/09/myanmar-faceb... reply Tostino 3 hours agoprevThat is a marked improvement compared to the other examples provided. Nice to see it also has less compute resources required for that higher quality output. reply PaulHoule 3 hours agoprevSometimes it sounds great but there are moments I think I'm listening to a harp and not somebody's voice. reply plus 3 hours agoparentIt's not exactly reasonable to expect super high fidelity audio at the bitrate constraints they're targeting here, and it certainly sounds a lot better than the Opus examples they're comparing against. reply cobbal 3 hours agorootparentThe more complicated the codec, the more fascinating the failure modes. I love watching a digital TV with a bad signal, because the motion tracking in the codec causes people to wear previous, glitched frames as a skin while they move. reply cnity 2 hours agorootparentGood observation, and probably part of what makes \"glitchy\" AI generated video so captivating to watch. reply ugjka 2 hours agorootparentprevLook up datamoshing on youtube reply 77pt77 2 hours agorootparentprevAre they comparing against opus using nolace? Because that makes all the difference! reply nickels 3 hours agoprevCould it be used for voice over satellite, ie Emergency SOS via satellite on iPhones? reply lxgr 2 hours agoparentiPhones use Globalstar, which theoretically supports voice bitrates of (I believe) 9.6 kbps, although only using dedicated satphones with large, external antennas. Apple's current solution requires several seconds to transmit a location stamp of only a handful of bytes, so I think we're some either iPhone or satellite upgrades away from real-time voice communication over that. Starlink has demonstrated a direct-to-device video call already, though, so we seem to be quickly approaching that point! My strong suspicion is that Apple has bigger plans for Globalstar than just text messaging. reply zekica 1 hour agorootparentStarlink is in a better position as their satellites are in a low earth orbit - 30 times closer than geostationary. It correlates to 1000 times (30dB) stronger signal on both sides. reply lxgr 1 hour agorootparentGlobalstar is LEO as well, although a bit higher (~1400 km) than Iridium (~780 km) and Starlink (below Iridium; various altitudes). In terms of SNR, they're very comparable. Newer GEO direct-to-device satellites also have huge reflectors and often much higher transmit power levels that can compensate for the greater distance somewhat. Terrestar and Thuraya have had quite small phones available since the late 2000s already, and they're both (large) GEO. reply iamnotsure 1 hour agoprev [–] Please stop lossy compression. reply GrantMoyer 0 minutes agoparentHave you ever looked at the size of losslessly compressed video? It's huge. Lossy compression is the only practical way to store and stream video, since it's typically less than 1% of the size of uncompressed video. Lossless compression only gets down to about 50% of the size. It's amazing how much information you can throw out from a video, and barely be able to tell the difference. reply cheema33 1 hour agoparentprev [–] Lossy compression has its practical uses. Under ideal circumstances nobody is going to stop you from using FLAC. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Meta has developed the Meta Low Bitrate (MLow) codec to improve audio quality for users with slow connections and lower-end devices, offering twice the quality of the Opus codec while using 10% less computational power.",
      "MLow has been launched on Instagram and Messenger calls, with a rollout on WhatsApp, enhancing user engagement and audio quality, especially in low-bitrate conditions.",
      "MLow, a CELP codec, supports SuperWideBand audio and better Forward Error Correction, improving audio quality even in packet loss scenarios, and aims to enhance audio recovery in future updates."
    ],
    "commentSummary": [
      "Meta has introduced a new low-bitrate audio codec, which may not be ideal for real-time communications due to high packet rates and overhead but can be beneficial in circuit-switched or VoIP systems using header compression.",
      "The codec is designed to reduce bandwidth consumption while maintaining or improving reliability and perceived audio quality, making it suitable for platforms like Facebook, Instagram, and WhatsApp.",
      "The codec's performance in real-world applications depends on its ability to handle bit errors and reduced channel capacity, with Forward Error Correction (FEC) playing a crucial role in maintaining call quality."
    ],
    "points": 197,
    "commentCount": 105,
    "retryCount": 0,
    "time": 1718291118
  },
  {
    "id": 40667606,
    "title": "Arm says it wants all Snapdragon X Elite laptops destroyed",
    "originLink": "https://www.xda-developers.com/arm-says-it-wants-all-snapdragon-x-elite-laptops-destroyed/",
    "originBody": "Home CPU Arm says it wants all Snapdragon X Elite laptops destroyed BY RICH WOODS PUBLISHED 1 DAY AGO The legal battle between Arm and Qualcomm continues KEY TAKEAWAYS Arm is trying to eliminate Qualcomm from Windows market, so it can introduce its own Cortex design. Rumors suggest that Nvidia, MediaTek, and AMD may enter the Windows ecosystem soon with Arm chips. Arm claims Qualcomm doesn't have a license for custom Arm chips, creating a legal battle between the two companies. We're less than a week away from Qualcomm's Snapdragon X-series laptops hitting shelves, and it's kind of a big deal. There's been an elephant in the room since Apple started using Arm chips for Macs, and this is seen as the launch that will bring Windows to parity with that. But there's another elephant in the room, at least depending on your perspective, which is that Arm Holdings and Qualcomm have been locked in a legal battle for some time over these very chips. RELATED Microsoft Copilot+: Everything you'll be able to do with your new Snapdragon X Elite PC Microsoft has unveiled its vision for AI PCs, and it's called Copilot+ What does Arm want? It's very anti-Arm \"Arm's claim against Qualcomm and Nuvia is about protecting the Arm ecosystem and partners who rely on our IP and innovative designs, and therefore enforcing Qualcomm's contractual obligation to destroy and stop using the Nuvia designs that were derived from Arm technology,\" an Arm spokesperson said in a statement to Reuters. And there it is. Arm wants all Snapdragon X series chips destroyed. Or does it? Firstly, let's be clear about what the company is calling for here. Qualcomm has laid the foundation for Windows on Arm. It's built out the ecosystem since the platform was announced at the end of 2016. Native apps like Chrome, Slack, and a whole bunch of others wouldn't exist if it weren't for Snapdragon PCs. What Arm is calling for is for Qualcomm to be eliminated from the market so that it can swoop in with its own Cortex designs. And according to rumors, as well as what I'm hearing from my own sources, there are more Arm vendors coming to the Windows ecosystem. Rumor has it that Nvidia, MediaTek, and AMD all have their eyes on it, and a competitor could be entering the space as soon as CES 2025. It's likely that a company like MediaTek would use Arm's own Cortex cores, which would be more lucrative for Arm, of course. It also allows the company to maintain a competitive advantage in a world where both Apple and Qualcomm have both moved on from Cortex designs because Arm wasn't offering what they needed. It's all about licensing Does Qualcomm have the rights to make its own Arm chips? No one knows Basically, Arm says that Qualcomm doesn't have a license to make custom Arm chips, and Qualcomm is saying that it does. There are two kinds of Arm licenses. One is for using Cortex cores, which are designed by Arm, essentially as an off-the-shelf part. The other kind of license lets you build whatever you want, as long as it follows the Arm instruction set. Apple and Qualcomm are doing the latter. Arm has always been pretty public about the fact that it prefers companies to use Cortex cores. Custom chips mean companies like Qualcomm can make a better Arm chip than Arm, and it can't make the old Apple excuse anymore, which was always that it was about optimizing for the full stack from hardware to software. Qualcomm bought a company called Nuvia to do this, and it originally used Nuvia's architectural license. Arm's argument is that the Nuvia license was canceled when it was taken over by Qualcomm. A new deal would have to be negotiated if that holds up in court. So, there you have it. Arm wants Qualcomm to stop shipping the product it's been contesting, but to be honest, that's not usually how these things end. It's unlikely that any product will be delayed from hitting shelves. These cases tend to end with one company giving a bucket of cash to the other, and everyone moves on. Snapdragon X PCs are slated to launch on June 18, with a total of 14 products from seven OEMs, including Microsoft, HP, Dell, Lenovo, Samsung, Asus, and even Qualcomm. Acer also has a product ready to go in mid-July. Readers like you help support XDA Developers. When you make a purchase using links on our site, we may earn an affiliate commission. Read More. Subscribe To Our Daily Newsletter! 80,000+ People Already Do By subscribing, you agree to our Privacy Policy and may receive occasional deal communications; you can unsubscribe anytime. Comments 3 Share Share Share Share Share Share Copy Email Link copied to clipboard Related Topics CPU QUALCOMM ARM About The Author Rich Woods Hello! I'm the Editor-in-Chief of XDA, and I've been reporting on all things consumer tech since 2013. More recently, I've had more of a focus on Windows, and I've reviewed pretty much every mainstream laptop under the sun. If you see me somewhere, come say hello and let me ask you awkward questions about why you use the tech that you use. Recommended Articles + CPU What's the difference between a CPU and a GPU? 3 INTEL Intel vs AMD: Which should power your PC in 2024? + INTEL On this day 46 years ago, the Intel 8086 brought us the x86 architecture 1 MICROSOFT Microsoft is retiring Windows Mail, Calendar, and Outlook's light web version in 2024 + WINDOWS 11: EVERYTHING YOU NEED TO KNOW 10 best keyboard shortcuts every Windows user should know (and probably doesn't) 1 LAPTOPS Surface Pro 11 vs. Dell XPS 13 (2024): Which should you buy? Today's Best Deals Take nearly half off this small and powerful Mini PC that's now down to its lowest price yet 18 hours ago Dell's XPS 14 is sleek and powerful, and now $300 off for a limited time 2 days ago Logitech's best-selling wireless keyboard and mouse bundle drops to lowest price ever 3 days ago See More Trending Now be quiet! Dark Rock 5 review: Lower price and CPU temps for value-focused PC builds Adobe won't train its AI on your content after all Microsoft almost shelved some of the best Windows screensavers ever",
    "commentLink": "https://news.ycombinator.com/item?id=40667606",
    "commentBody": "Arm says it wants all Snapdragon X Elite laptops destroyed (xda-developers.com)175 points by sharpshadow 9 hours agohidepastfavorite143 comments omneity 8 hours agoThe reporting on this article is just bad. The issue is not Qualcomm taking over Nuvia's business and license. According to Arm the Nuvia license is contractually tied to server/datacenter usage, while Qualcomm's acquisition of Nuvia packaged these server cores into \"Oryon\" for PC use in the Snapdragon X Elite SoC and intends to use them in smartphones too, breaching the original agreement. Here's a better source: https://www.heise.de/en/news/ARM-torpedoes-Windows-on-ARM-De... reply Tuna-Fish 6 hours agoparentYou have described the ARM position in the case. According to Qualcomm, they already have a separate architecture license from ARM that allows them to produce their own cores and sell them in whatever they want. And by their reading of that license, they can purchase the core IP they use for that from a third party, and then use it under their existing license. reply sroussey 3 hours agorootparentIf Qualcomm wins, ARM will never make sweetheart deals for startups again. reply gumby 3 hours agorootparentThey'll just make different sweetheart deals. The Nuvia acquisition was a great opportunity for ARM to shake down Qualcomm. Or instead of \"shake down\", dare I say...strongarm? reply growse 2 hours agorootparentfurious clapping reply toast0 3 hours agorootparentprevFrom comments here, it sounds like if Qualcomm wins, the ARM/Nuvia deal isn't the problem, the ARM/Qualcomm one is. ARM probably already doesn't like to make broad licensing deals like that, but big customers like them, and ARM in the past couldn't afford to say no. reply SR2Z 3 hours agorootparentprevGood - drive them to invest in RISC-V instead. reply karmakaze 1 hour agoparentprevThe story is poor, the conclusion maybe accurate in that it's likely a non-story: > Qualcomm bought a company called Nuvia to do this, and it originally used Nuvia's architectural license. Arm's argument is that the Nuvia license was canceled when it was taken over by Qualcomm. A new deal would have to be negotiated if that holds up in court. > So, there you have it. Arm wants Qualcomm to stop shipping the product it's been contesting, but to be honest, that's not usually how these things end. It's unlikely that any product will be delayed from hitting shelves. These cases tend to end with one company giving a bucket of cash to the other, and everyone moves on. reply amadeuspagel 6 hours agoparentprev> Apparently, ARM wants to push through a new licensing model to generate more revenue. The main aim is to base license costs on the price of the device rather than the processor. According to Qualcomm, this involves a one-off payment of several hundred million US dollars plus significantly higher license fees on an ongoing basis. Interesting. That's what Qualcomm itself does, right? reply sumtechguy 6 hours agorootparentThat is almost exactly how they usually do it, but they many times negotiate per company. However, it does look like ARM trying to have a redo on their contract that they have had in place for a very long time, now that ARM is more popular. When it was the cell manufactures that made it possible for ARM to be in the current position it is in precisely because of the terms ARM had in its contract years ago. Qcom may have a good case as they have put arm chips in computers before (omnitracs). ARM saying it has to be tied with a modem, and buying another company does a full reset on the existing contract may not go far. reply somat 2 hours agorootparentso... if qcom included a \"modem\" on their server board would arm be happy? well not happy, but would it satisfy arms interpretation of the contract? I could totally see every single server board saddled with some obsolete appendix of a cell baseband to satisfy the letter of some sweet contract qualcom had with arm saving the company a fair few million. reply dvdkon 8 hours agoparentprevDo you have a source not behind a \"cookie wall\"? I remember Qualcomm making phone SoCs with custom cores some years back. Why is it suddenly an issue now? Or were those just rebrands of tweaked Cortex designs? EDIT: From reading the Reuters article [0] it seems that Arm's argument is that since Nuvia's cores were developed under a now-terminated licence, Qualcomm doesn't have a right to use them, their other licences notwithstanding. [0]: https://www.usnews.com/news/technology/articles/2024-06-10/a... reply bpfrh 7 hours agorootparentThe argument is that Nuvia got a licence for server usage and development support from arm to develop custom arm processors. Qualcom now wants to use technology developt under a specific licence agreement in use cases not covered in that licence agreement, because they have general licence agreement with arm. The question is: Does the general licence agreement from qualcom override the specific licence agreement between Nuvia and arm for this specific technology. I found some sources which partially explains this: https://www.ft.com/content/5535ce39-ab22-497a-a8d6-52a690d1b... https://www.cnbc.com/2022/09/01/why-arms-lawsuit-against-qua... (...Nuvia, a startup founded by former Apple and Google engineers, was developing a server chip with custom cores under an architecture license. It also had access to Arm’s core designs....) reply rcxdude 7 hours agorootparentI suspect it's going to boil down to whether any of the IP Nuvia had access to but Qualcomm did not made it into the cores. But it could also easily come down to some details of the wording of the contracts in terms of how that license is granted. Either way, this request for the destruction of laptops at the early stage is clearly a negotiation point: if the court grants it, which seems unlikely, it'll be a strong bargening chip for a negotiation a settlement in cash/royalties/etc, not the actual goal of ARM. reply bpfrh 7 hours agorootparentAgreed, I think on your last point ARM is also trying for a fast case/settlement as they are in a difficult position: They don't want to have a long fight expensive fight with qualcom and make other arm companies uneasy about investing in the arm platform. They also don't want to open the door to have companies buy their IP for \"cheap\" by buying smaller companies with special arm licences instead of directly negotiating with them. reply giancarlostoro 5 hours agorootparent> They also don't want to open the door to have companies buy their IP for \"cheap\" by buying smaller companies with special arm licences instead of directly negotiating with them. Is it really ARMs IP or the IP developed by the smaller company. My understanding is ARM is like a blueprint, and you then make changes as needed. reply lokar 3 hours agorootparentIt they (reasonably) charge for the blueprint based on what you say you are building. The only reason small companies can get a license on easy terms is because of the narrow planned use. reply bpfrh 3 hours agorootparentprevThat kinda depends on your licencing agreement: https://en.wikipedia.org/wiki/ARM_architecture_family#Licens... You can either licence a IP Core block(Core, gpu,etc) to embedd in your device, our you can buy an architect licence and design your own cores as long as they comply with the arm architecture. AFAIK the IP is actually buildable cores which arm get's by partnering with companies to get it right, e.g. there is not that much difference between them fully building them in house and selling them vs. letting partners build them(for the end user). reply rickdeckard 7 hours agorootparentprev\"Snapdragon X Elite\" is based on the IP Qualcomm gained from the Acquisition of Nuvia, who developed this IP based on a very narrow license they got from ARM to develop for the server market. Qualcomm is trying to transfer everything Nuvia has developed under that specific narrow ARM-license to the broad license of Qualcomm and use it for \"powering flagship smartphones, next-generation laptops, and digital cockpits, as well as Advanced Driver Assistance Systems, extended reality and infrastructure networking solutions\" ARM has filed a lawsuit that this was never in scope of the license of Nuvia, the court-ruling is still pending on that one... The related court-filing is worth a read: https://s3.documentcloud.org/documents/22273195/arm-v-qualco... reply rickdeckard 7 hours agorootparentMoreover, Nuvia's ARM-licenses and everything produced from it became invalid with Qualcomm's acquisition, so the tech produced from it is simply not licensed anymore. In detail, the dispute goes like this: 1. ARM gave a broad range of licenses to Nuvia to develop an ARM-based architecture for use in servers, with a license-fee to match this objective (probably a favorable one for ARM, because it's a field ARM aims to expand): \"Nuvia’s licensing fees and royalty rates reflected the anticipated scope and nature of Nuvia’s use of the Arm architecture. The licenses safeguarded Arm’s rights and expectations by prohibiting assignment without Arm’s consent, regardless of whether a contemplated assignee had its own Arm licenses.\" 2. Qualcomm acquires Nuvia with the intention to use the developed IP for (quote): \"powering flagship smartphones, next-generation laptops, and digital cockpits, as well as Advanced Driver Assistance Systems, extended reality and infrastructure networking solutions\" Sounds quite reasonable if the claim is validated: ARM gave favorable conditions to a startup which aimed to compete in a field where ARM is weak (servers). Now the biggest customer of the strongest revenue-market of ARM (mobile devices) has acquired that startup to repurpose their development and license to use in mobile devices, PCs, automotive, VR,... reply Tuna-Fish 6 hours agorootparentThe part you are leaving out that before Qualcomm acquired Nuvia, they already owned a blanket license from ARM with no such use restrictions. Their claim is that they can use the IP they acquired under this older license. Which side is right or wrong here depends on the exact wording of the license between ARM and Qualcomm, and since this is not publicly known, no-one outside those companies knows which side is in the right. reply rickdeckard 5 hours agorootparentI'm not leaving that out: \"Qualcomm is trying to transfer everything Nuvia has developed under that specific narrow ARM-license to the broad license of Qualcomm\" ARMs legal claim is that the broad license of Qualcomm is not applicable to \"host\" the IP of Nuvia, because this IP was built on a very specific narrow license ARM provided to them, with contractual terms which do not allow the License and IP to be transferred reply ethbr1 6 hours agorootparentprevYeah, that's the weird part. Nuvia's ARM license (now owned by Qualcomm) may be invalid for Qualcomm non-server use. But how can Nuvia IP be invalid? Presumably ARM never owned Nuvia's IP: why would they have negotiated that, given they already had license control? Ergo, Qualcomm owns Nuvia IP without a Nuvia ARM license. ... but that's fine, because Qualcomm has a Qualcomm ARM license. \"I revoke your license, so you can't sell your IP to a willing third party\" is a pretty dangerous precedent. You should be perfectly free to do so, to the extent that your buyer would also need their own license. reply gpm 2 hours agorootparent> Presumably ARM never owned Nuvia's IP: why would they have negotiated that, given they already had license control? That seems to me to be the key point in contention, or at least whether or not ARM owns the right to prevent Nuvia from selling IP that Nuvia created. As Qualcomm's lawyers put it > 25. Second, ARM was claiming a right to control the transfer of NUVIA technology when NUVIA’s ALA provided no such rights to ARM. and as ARM's lawyers put it > 25. Arm denies the allegations in paragraph 25. Note if you deny in part you say which part you are denying and which part you are admitting, thus here they are denying the statement entirely, and thus claiming they had the rights to control the transfer of NUVIA technology (which is also the broader theme of the filing). Without being able to actually see the contractual terms I don't see how the public could conclude which side is right in this dispute, but basically everything seems to center on this. Either ARM's license grants them control over technology Nuvia created, even if all the confidential information that technology was based off of is no longer confidential, and/or the party you are trying to transfer it to has licenses to all the technology it was based off of (and ARM probably wins), or it doesn't (and Qualcomm probably wins). If I was thinking about doing business with arm I'd want to be very sure that whatever agreements I signed where of the second form, or I was getting a huge company-altering discount in exchange for them being of the first form. reply rickdeckard 5 hours agorootparentprevARM doesn't claim to own the IP of Nuvia, they merely claim that the license they gave to Nuvia to develop server-architecture based on ARM does not allow the IP to be transferred to other licenses or other use-cases. Since the license the Nuvia IP was built on became void when the company changed ownership, the IP is not built on a valid license. > \"I revoke your license, so you can't sell your IP to a willing third party\" is a pretty dangerous precedent. You should be perfectly free to do so, to the extent that your buyer would also need their own license. ARM does offer licenses for comparable designs (Blackhawk, Cortex-X), but Qualcomm apparently insists that they don't need to license those because they acquired Nuvia and can simply integrate that IP on top of their existing ARM-license instead. My guess is that the gamble of Qualcomm is to force ARM to license the Nuvia IP to Qualcomm again for a more-favorable fee than ARM's comparable designs cost, and ARM sees no reason to do so. reply ethbr1 2 hours agorootparent> Since the license the Nuvia IP was built on became void when the company changed ownership, the IP is not built on a valid license. Here's where the wording gets confusing. ARM has their own IP: their core designs, ISA (?), etc. ARM also has licensing of that IP. ARM also has platform licensing: allowing others to implement their ISA, call themselves ARM-compatible, etc. Nuvia built a product, with an ARM license (because they wanted to go to market). Did they also incorporate ARM IP into their design (i.e. extend existing ARM core designs)? But neither of those should matter... because afaik Qualcomm already has an architecture ARM license (one of the few, ~15). [0] Consequently, if Qualcomm already has an architecture license to the core Nuvia built on... that's a superset of any license Nuvia had, given that an architecture license allows for unlimited customizability. Consequently, this feels like ARM trying to defend their (future) revenue by retroactively limiting licenses they already sold. [0] Background https://semiaccurate.com/2013/08/07/a-long-look-at-how-arm-l... reply fidotron 5 hours agorootparentprevIt is very common that IP created by specific employees exposed to the IP of a third party is essentially tainted, and this will be spelled out contractually. I have actually fought to keep specific team members off NDAs to enable them to be maximally useful for this reason. reply chx 5 hours agorootparentprevHow can Nuvia IP be invalid? Well, that's the beauty of Intellectual Property: it's not really a property it's a weird legal construct. Arm argues the entire Nuvia IP should have disappeared because the license allowing it to exist was voided when Qualcomm bought Nuvia. And Qualcomm had no legal right to move the IP on the foundation of their ARM license. They argue Qualcomm should've negotiated for a new license to use Nuvia IP. Whether they are right or not is up for the courts to decide but they make a legally sound argument without a doubt. If this makes no sense then here's a simple example: a DVD is not property, it's intellectual property. If it were property then region locks couldn't exist. Compare it to a book. You buy a book in London, no one stops you from reading it in New York, it's your property. Absurdity is the name of the game: in the DeCSS trial it was argued Johansen trespassed on his own computer. Edit: ah, I have a better, simpler explanation: Arm and Nuvia made a contract. Qualcomm bought Nuvia. Arm argues Qualcomm violated the terms of the contract. That's it. reply ethbr1 2 hours agorootparentThe problem is that Qualcomm-ARM may already have a license contract that's a superset of Nuvia-ARM. Depending on the wording of Qualcomm's architectural ARM license, it seems like that might cover Nuvia's IP? Or in other words, if anyone who wasn't an ARM architectural licensee bought Nuvia, Nuvia's IP would have been unusable. But because someone with a \"do anything custom with this core\" bought Nuvia, Qualcomm effectively bought some \"anything custom\". The real IP needle in this haystack seems to be: what ARM IP did Nuvia use that Qualcomm doesn't already have an architectural license for? I.e. extra sauce ARM shared with them for their specific use case. Which sets up for a SCO-Unix-style threshing of technical details for individual pieces. Which I imagine both ARM and Qualcomm will eventually want to avoid, as they'd like to continue doing business together, so this will eventually collapse into a new licensing agreement that covers all claims. (Assuming neither pisses the other off badly enough to salt the earth) reply chx 1 hour agorootparent> Depending on the wording of Qualcomm's architectural ARM license, it seems like that might cover Nuvia's IP? Right, but ARM says \"Nuvia’s licensing fees and royalty rates reflected the anticipated scope and nature of Nuvia’s use of the Arm architecture. The licenses safeguarded Arm’s rights and expectations by prohibiting assignment without Arm’s consent, regardless of whether a contemplated assignee had its own Arm licenses\" Basically ARM says Qualcomm should pay more than they did before because no, their blanket doesn't cover Nuvia. > so this will eventually collapse into a new licensing agreement that covers all claims Obviously. That's the crux of the matter. Qualcomm says they shouldn't pay more, ARM says they should. reply gpm 3 hours agorootparentprevI encourage people to read the court-filings in any case if they're interested in the case. But I'd also caution people new to this that lawyers are extremely good at making their case seem obviously right even when it isn't. If you're not nodding along going \"that seems right\" when you read the argument in a complaint the lawyers either have a really bad case, or they screwed up (or in rare cases they're making technical arguments that they think will seem right to the judge even if they don't seem right to lay people). It's better to wait for a chance to read the response from the other side before agreeing that the side you are reading is right. In this case this lawsuit is from 2022, we have the responses already :) Here's the docket that lists all the filings: https://www.courtlistener.com/docket/64938776/arm-ltd-v-qual... And to extract the relevant items from that docket Here's the response, at a minimum if you find ARMs complaint convincing you should read this: https://storage.courtlistener.com/recap/gov.uscourts.ded.798... Here's ARMs response to the response: https://storage.courtlistener.com/recap/gov.uscourts.ded.798... From a reasonably quick scan it looks like everything else in this suit is about discovery, scheduling, and whether it should be a jury a trial or just heard in front of a judge. Scheduling note: A 5-day Jury Trial is set for 12/16/2024 reply rickdeckard 2 hours agorootparentAlso interesting, seeing how the defendants are expanding the scope of the lawsuit to delay its progress and keep everyone busy: - Qualcomm requesting the court to order ARM to provide ALL Architecture License Contracts with ANY party so Qualcomm can judge whether they are in violation of THEIR contract [1] - Apple requesting the court to NOT share this information, as they are not relevant to this case and a customer of BOTH ARM and Qualcomm [2] - Qualcomm trying to subpoena other licensees Apple, MediaTek, Google, Intel, etc. to court [3] Meanwhile Snapdragon X Elite pre-production is ramping up, Qualcomm-customers finalize their Hardware-design, Microsoft putting all their weight onto that architecture. So by end of 2024 the products are already launched, products are shipped and Qualcomm has more allies to help them arguing against trade-restrictions... Regardless what the final outcome is, the better strategy of Qualcomm is obviously to have no outcome in short-term. [1] https://storage.courtlistener.com/recap/gov.uscourts.ded.798... [2] https://storage.courtlistener.com/recap/gov.uscourts.ded.798... [3] https://www.courtlistener.com/docket/64938776/arm-ltd-v-qual... reply Piraty 7 hours agorootparentprev(The page displays fine with ublock-origin + blocked 3rd-party {scripts,frames} + \"cookie notices\" filter lists) reply brevhtff 5 hours agorootparentprev> Do you have a source not behind a \"cookie wall\"? Ignore it, use technical measures to overcome it, disregard it. If a stranger came up to you in the street and demanded $5 to continue looking at them, you’d tell them to fuck off (in so many words). You certainly wouldn’t avert your eyes, nor would you cough up. Fuck these bozos and their shrink wrap licenses and their cookie popups and all. reply bee_rider 5 hours agorootparentIf someone did that, I would probably leave them alone, and if someone wanted to use them as a source, I would probably at least ask if they knew anybody more reasonable. reply jraph 7 hours agorootparentprev(The page displays fine with js disabled) reply kelsey98765431 4 hours agoprevOnce upon a time many moons ago i was given a development snapdragon board, pre FCC approval, for testing and application development. Rather quickly i had to contact my Qualcomm rep to inform them that iptables was missing. Not sure what i meant, they put me in contact with an engineer who scoffed at me and told me that i just needed to install the iptables binary from apt. I then showed him that no, the iptables binary was present and giving me an error i had never seen before: warning iptables is not present in this kernel. He went white and made a quick phone call and i was awarded for my discovery. Since then i have been fairly skeptical of qualcomm... Great hardware but that incident was somehow very alarming to me as it is not possible to disable iptables normally in kconfig so they had been mucking about in the source tree of the firewall enough to break it. I wonder what happened there. reply walterbell 3 hours agoparentThis time around, Qualcomm have sensibly contracted Linaro for upstream Linux support, https://news.ycombinator.com/item?id=40350408 reply WesolyKubeczek 3 hours agoparentprevmake allnoconfig, go from there, forget to turn netfilter on, oops reply DrBenCarson 3 hours agoparentprevHoly hell, man That’s scary reply CodeArtisan 7 hours agoprevArm Ltd. v. Qualcomm Inc. (1:22-cv-01146) https://cdn.arstechnica.net/wp-content/uploads/2022/09/gov.u... From the complaint: 22. (Page 6) [...] The licenses safeguarded Arm’s rights and expectations by prohibiting assignment without Arm’s consent, regardless of whether a contemplated assignee had its own Arm licenses. 39. (Page 12) On February 1, 2022, Arm sent a letter to Nuvia and Qualcomm terminating the Nuvia licenses effective March 1, 2022. The letter terminated the licenses based on Nuvia’s material breach of the assignment provisions of the Nuvia licenses by entering into the acquisition of Nuvia without Arm’s consent. 42. (Page 12) On April 1, 2022, Qualcomm’s General Counsel sent Arm a letter enclosing a Nuvia representative’s termination certification. The certification acknowledged—without objection—that the Nuvia licenses had been terminated. The certification recognized the obligations upon termination, and asserted that Nuvia was in compliance. Qualcomm and Nuvia thereby conceded that termination of the Nuvia licenses was appropriate, and that the termination provisions had been triggered, are binding, and are enforceable. 47. (Page 14) Qualcomm’s Arm licenses do not cover products based on or incorporating Arm-based technologies developed by third parties under different Arm licenses 62. (Page 17) Upon termination, the Nuvia ALA requires Nuvia to cease using and destroy any technology developed under the Nuvia ALA, as well as cease using Arm’s trademarks in connection with any technology developed under the Nuvia ALA. reply mensetmanusman 8 hours agoprevI’m kind of terrified of a fast windows experience where pushing the start menu button immediately brings up advertisements in the blink of an eye. At least in the past if I was fast enough it would fail to load in time. reply yellow_lead 6 hours agoparentDon't worry, Microsoft excels at absorbing hardware improvements as bloat in their software. reply stanac 4 hours agorootparentFrom wikipedia: > Wirth's law is an adage on computer performance which states that software is getting slower more rapidly than hardware is becoming faster. > \"The hope is that the progress in hardware will cure all software ills. However, a critical observer may observe that software manages to outgrow hardware in size and sluggishness.\" https://en.wikipedia.org/wiki/Wirth%27s_law reply lbruder 3 hours agorootparentprevWhat Andy giveth, Bill taketh away. reply whalesalad 6 hours agoparentprevLogin screen ads will have such low latency you’ll be able to see home mortgage loan prices live in the quicken loans ad reply cosmojg 5 hours agoparentprevYou could always try running Linux instead. reply whalesalad 5 hours agorootparentBeen daily driving Deb 12 + KDE for almost a year now and has been incredibly stable and positive. reply blahgeek 7 hours agoparentprevOn the bright side, it can also automatically apply updates and reboot in the blink of an eye, so all you perceives is that every app windows disappear. :) reply BirAdam 5 hours agoprevARM has had a series of issues, but it has continued to do well despite all of them. The thing is, they’ve been doing well because of license holders. That’s their business. If they keep treating their customers poorly, they will lose. RISC-V, MIPS, and OpenPower are all available. This is especially poignant when we remember that ARM is currently valuable for its performance per watt, and not for its raw performance capability. As x86 gets better at efficiency, and as RISC-V gets better at raw performance, ARM’s entire market fit shrinks. Their best move would be to become more lenient in licensing terms, not less. Edit, I was thinking market share when I said “do well” but ffgjgf1 pointed out that they’ve not been doing well financially. That’s an excellent point and likely explains some of ARM’s behavior. The problem is, alienating your license holders will only make the financials worse… reply ffgjgf1 5 hours agoparent> but it has continued to do well despite all of them Financially they haven’t really been doing that well, especially no compared to other chip designers, they are basically making peanuts. reply BirAdam 5 hours agorootparentI was thinking more in terms of market share, but you raise an excellent point. reply TheCondor 4 hours agoprevI’ve been wondering why I can’t get something in a NUC like form factor with some legit cores and memory for a decent price. The x4 is launched, I’d totally settle for like 4 x1 cores, 4 or 8 x3 or x4 cores would be ideal, something that can out perform an intel nuc. Pi’s are cool, but I need some compute. I think I’m starting to get it with this wacky licensing stuff. I’ve had some doubts the last few years but I have never really bet against intel. They have been shockingly good for a very very long time. A wave of elite x like parts on the desktop, at good prices, would hurt them, badly as the ecosystem spins up. AMD jumping in would also hurt them. The more ARM wastes this window of time, the more likely intel will get their process sorted and push out an aggressive microarchitecture on “x86s” and have ARM64 like qualities with some backwards compatibility reply OptionOfT 2 hours agoparenthttps://www.qualcomm.com/news/releases/2024/05/qualcomm-acce... If his litigation goes away I see a bright future for Arm. I'm just thinking of how much power I am wasting per day on Intel machines compared to Arm devices. (not taking into account the cost of purchase of course). reply philistine 1 hour agoparentprevYou're doing the classic You want three things but you can only get two. You want fast, small, and cheap. You have to pick two. If you want fast and small, get a Mac Mini. reply nottorp 4 hours agoprevArm was cuter when they were the underdog. Are Risc-V designs unencumbered, or there's still some IP holder that could sue you when the money pot grows big enough to make it worth it? reply wildzzz 2 hours agoparentIf you are just using the RISC-V core alongside cores you've developed yourself or have licenses for, you're set. RISC-V ISA is provided. It has a very permissive, royalty-free license but you will need to become a paid member to use their trademark. I suppose they could change their license model at some point and attempt to revoke existing licenses but that would be suicide for them. Just like with all standards, there is always an IP holder but it's all a matter of how that IP is licensed out to users and the association that user can display with the IP holder. For example, you can sell a device with a Raspberry Pi inside but you can't use the HDMI trademark on your marketing materials without being a member of the HDMI Foundation. reply nottorp 2 hours agorootparent> I suppose they could change their license model at some point and attempt to revoke existing licenses but that would be suicide for them. ... now. How about when it gets popular and too much resources have been poured into them? Like arm... reply philistine 1 hour agoparentprevDo not blame ARM's terrible attempts at ever more rent-seeking to their leadership position. Blame it on their incredibly boneheaded ownership history. SoftBank paid way too much for ARM, and now they have to try and make the money back somehow. reply nottorp 33 minutes agorootparentWhat, Japan doesn't have bankruptcy? reply lucianbr 8 hours agoprev> Arm's argument is that the Nuvia license was canceled when it was taken over by Qualcomm. This is like a windows (or anything else) license for a private person, that you cannot resell or give to anyone else. I'm curious whether these shenanigans will stand up between corporations as well. Bad way of doing business, in any case. reply InTheArena 8 hours agoparentIt’s very common for corporations to include successor clauses in contract, to give strategic partners an opportunity to relicense sweetheart deals that they might have given plucky startups *like Nuvia). My suspicion is that it’s going to be very straightforward for ARM to document what successor clauses are in place. I suspect we are going to find out that there is one, and Qualcomm don’t care. Qualcomm is more lawyer company with engineers these days - they even made apple back down on licensing. reply bluGill 5 hours agoparentprevThere is a difference between the Windows license and this - this is a contract. A contract meaning both parties had lawyers in the room (or at least should have) and came to an agreement. With Windows you don't get to change any of the wording, either accept it or not. As such the law and courts place more restrictions on what a license can do - check local laws, but regardless of the wording you may have the right to resell Windows to someone else even if the license says you cannot. However a contract means both parties agreed to this and so the courts are more likely to agree to the wording. Even in a contract there are limits. You can never have a contract for murder is the obvious example. A contract that is very unfair to one party can be void as well - check with a lawyer for details in your area. reply masklinn 8 hours agoparentprev> Bad way of doing business, in any case. From Qualcomm? ARM gave Nuvia a sweetheart deal (including IP access) specifically to help tune and spread ARM on the server. Rather than negotiate for their own deal, Qualcomm decided to purchase Nuvia in order to reuse the IP and licenses, neither of which the agreement allowed according to ARM. reply kalleboo 5 hours agorootparentQualcomm already has their own blanket license and their own custom cores. Nuvia's license only covered server products, so Qualcomm could not use the acquired license anyway. ARM claims that Qualcomms pre-existing license does not allow \"technologies developed by third parties under different Arm licenses\". So I guess the case will come down to arguing if an acquired company counts as \"a third party\" or not reply bluGill 5 hours agorootparentIt also comes down to if the courts will allow exclusion of \"technologies developed by third parties under different Arm licenses\". In some cases the courts will determine some clause like that grossly unfair and so not enforceable. (seems unlikely, but type of thing can happen) reply kalleboo 3 hours agorootparentAnd what is a \"technology\". Can you literally not use anything patented by a third party ever? In court cases like this, technicalities like this matter a lot. reply Havoc 8 hours agoparentprevQualcomm has their own license too. ie in addition to the Nuvia one reply rickdeckard 7 hours agorootparentQualcomm's license doesn't cover the IP Nuvia created with their license, and Nuvia's license (and everything created under it) became void when Qualcomm acquired the company. Source: https://s3.documentcloud.org/documents/22273195/arm-v-qualco... reply Brian_K_White 4 hours agorootparentSo says ARM. Qualcom says otherwise. We don't know who is right with only public info to go on. It all depends on what exactly Qualcoms contract says. Obviously Qualcom has fully educated and competent lawyers who have read their own contracts in detail, and concluded they are good to go. So it's not a given that ARMs assertion is the only valid and winning assertion. It's merely what they want. Maybe they have a right to what they want, maybe they don't. reply rickdeckard 2 hours agorootparentActually there's quite alot public info, as the case is quite public. Here's a court-statement from Qualcomm on that topic: https://storage.courtlistener.com/recap/gov.uscourts.ded.798... Qualcomm agrees that the Nuvia License is void, they just claim that they can transfer all their IP to the Qualcomm license:. \"3. Qualcomm has its own license agreements with ARM, under which Qualcomm has licensed and paid for the same intellectual property that NUVIA licensed under its own separate agreements with ARM. Therefore, even though ARM terminated the NUVIA licenses, Qualcomm owns independent licenses for the same ARM technology and information\" - ARM disagrees on this, referring to a clause in their license-contract with Nuvia: \"Upon termination, the Nuvia ALA requires Nuvia to cease using and destroy any technology developed under the Nuvia ALA, as well as cease using Arm’s trademarks in connection with any technology developed under the Nuvia ALA\" The response of Qualcomm is this: \"4. The notion that ARM has the right to control technology that is not ARM’s—and worse yet, to ask Defendants to destroy their innovation and inventions unless substantial monetary tribute is paid to ARM—offends customary norms of technology ownership, as well as NUVIA’s and Qualcomm’s rights under their agreements with ARM.\" and this: \"7. ARM’s position is a threat to the industry generally. Unless this Court rejects ARM’s arguments, ARM’s extreme position could be weaponized against all of its licensees, allowing ARM to claim ownership over all its licensees’ innovations.\" - So: 1. Nuvia's license (and everything created under it) became void when Qualcomm acquired the company --> Qualcomm obviously agrees. 2. Qualcomm's license doesn't cover the IP Nuvia created with their license --> Qualcomm argues that this is \"offending\" and \"a threat to the industry\" - So it is not so much in dispute what Qualcomm's or Nuvia's contract says, Qualcomm apparently tries to convince the court that they have the right to use Nuvia's IP despite the license of Nuvia saying otherwise... reply masklinn 7 hours agorootparentprevThey don’t even have the Nuvia one anymore, ARM rescinded it. reply Taniwha 6 hours agoprevWho wants to bet that Qualcom is spinning a RISC-V version as fast as it can - this after all is why open source ISAs are a good thing reply rwmj 6 hours agoparentThey've been very active in RISC-V technical groups recently. What exactly that means - whether they are serious about developing cores, or just want an insurance policy with leverage to use against Arm - isn't clear. Personally I hope we do get a \"Snapdragon V\" one day. reply wildzzz 2 hours agoparentprevIt's nice having an open ISA but companies go with ARM because the cores are already developed. There are RISC-V cores available for purchase too but they aren't up to the same level of performance yet. reply sloowm 5 hours agoparentprevI don't think Qualcomm or any of the current market leaders will be the first to do a big RISC-V push. While dealing with the ARM company might be a hassle having an exclusive ISA limits the competition which is good for business. When the first one moves and the software is created for it there are more opportunities for competitors. Qualcomm doesn't want to create the invitation but also wants to make sure there is a place at the party for them. RISC-V will probably get it's first big adoption through some new or shifting industry creating their own chip. Like how ARM got into phones or Amazon started to use their own ARM chips. This guarantees all the infrastructure can be build and used in the future. reply lobochrome 5 hours agoparentprevI’ll take that reply coob 8 hours agoprevWow this some pretty biased reporting. No attempt to assess whether or no Qualcomm have actually breached their contractual license with ARM reply _flux 8 hours agoparent> Arm's argument is that the Nuvia license was canceled when it was taken over by Qualcomm. How could the reporter possibly know one way or another, certainly it depends on the contents of that contract? Has anyone outside these three parties (Qualcomm, Arm, Nuvia) have seen the contract in question? To me the reporting seemed quite unbiased. reply KingOfCoders 8 hours agorootparentOn top of that, only a court can decide about a breach of contract in the end. reply rickdeckard 7 hours agorootparentprevWell, the terms of the contract are outlined in the respective lawsuit between ARM and Qualcomm: https://s3.documentcloud.org/documents/22273195/arm-v-qualco... reply yencabulator 4 hours agorootparentThat document says what ARM lawyers want it to say, not what is true and lawful. reply rickdeckard 3 hours agorootparentYeah, equally there is a document that says what Qualcomm's lawyers want it to say. Sorting through both and providing a summary is part of \"journalism\". Here's one from Qualcomm: https://storage.courtlistener.com/recap/gov.uscourts.ded.798... And a quote from it, where Qualcomm states that it has the rights to use the IP from Nuvia because it has a ARM-license covering the same IP as Nuvia's ARM-license: \"3. Qualcomm has its own license agreements with ARM, under which Qualcomm has licensed and paid for the same intellectual property that NUVIA licensed under its own separate agreements with ARM. Therefore, even though ARM terminated the NUVIA licenses, Qualcomm owns independent licenses for the same ARM technology and information\" - To this statement of ARM: \"Upon termination, the Nuvia ALA requires Nuvia to cease using and destroy any technology developed under the Nuvia ALA, as well as cease using Arm’s trademarks in connection with any technology developed under the Nuvia ALA\" The response of Qualcomm is this: \"4. The notion that ARM has the right to control technology that is not ARM’s—and worse yet, to ask Defendants to destroy their innovation and inventions unless substantial monetary tribute is paid to ARM—offends customary norms of technology ownership, as well as NUVIA’s and Qualcomm’s rights under their agreements with ARM.\" and this: \"7. ARM’s position is a threat to the industry generally. Unless this Court rejects ARM’s arguments, ARM’s extreme position could be weaponized against all of its licensees, allowing ARM to claim ownership over all its licensees’ innovations.\" - So, as an amateur reading through the case, there is sufficient information to conclude that ARM has a contract that limits the use of Nuvia IP to Nuvia alone, and Qualcomm tries to argue that these terms are \"offending\" and \"a threat to the industry\". --> So, there is obviously no disagreement that these crucial terms are in fact part of the ARM/Nuvia License contract. reply luag 8 hours agoparentprevIt does, under 'It's all about licensing' section. reply jojobas 8 hours agoparentprevIt will take dozens of lawyers (at least three opinions between any two of them) many months and millions of dollars to figure it out, and you want the journalists to make a call? The article sums up rough positions of both parties well enough. reply robertlagrant 8 hours agoparentprevWhat's the bias? In whose favour? reply InTheArena 8 hours agorootparentQualcomm / Nuvia. reply _flux 8 hours agorootparentIn which way is the article biased for them? reply josephcsible 5 hours agoparentprevEven if they did, I'd still object to requiring useful products to be turned into e-waste. reply jeffbee 4 hours agoparentprevWhy would you want some blogger to draw conclusions of law? reply sterlind 8 hours agoprevWhy does Qualcomm need a license to design a custom ARM chip in the first place? Assuming they're not reusing or deriving from licensed IP blocks, and that they just cracked open a reference book and implemented the instruction set from scratch. Does ARM hold a copyright on the instruction set itself? Can you even copyright an instruction set? Or is it a bunch of patents? reply rickdeckard 7 hours agoparent\"Snapdragon X Elite\" is based on the IP Qualcomm gained from the Acquisition of Nuvia, who developed this IP based on a very narrow license they got from ARM to develop for the server market. Qualcomm is trying to transfer everything Nuvia has developed under that specific narrow ARM-license to the broad license of Qualcomm and use it for \"powering flagship smartphones, next-generation laptops, and digital cockpits, as well as Advanced Driver Assistance Systems, extended reality and infrastructure networking solutions\" ARM has filed a lawsuit that this was never in scope of the license of Nuvia, the court-ruling is still pending on that one... reply foxandmouse 5 hours agorootparentThat’s quite interesting, does it tie into why the snapdragon X elite is so much better than anything they’ve produced in the past? reply rickdeckard 4 hours agorootparentIn a broader sense, yes. Some generations back Qualcomm took the ARM design-license, designed their own CPU-cores based on it and added their custom GPU, DSP, etc. - Scorpion, Krait (32bit): Custom Qualcomm designs with full compatibility to ARM's instruction sets. They were the best in the business, knowing more about tailoring an ARM-core to a mobile use-case than any company in the world. - Then Apple disrupted the space with a 64bit ARM CPU. Qualcomm had to move fast from 32bit to 64bit to respond (why? ...that's arguable) - Kryo (64bit): To move from 32bit to 64bit within one year (!), Qualcomm adopted the available ARM 64bit design, added their IP for adjacent components and created the first Kryo CPU (SD820, mamma mia what a bad SoC that was...). Over time they applied more customization to that platform, but the base was always this Kryo-architecture (-> newer vanilla ARM-design + Qualcomm modifications) For years, this approach was used now, including Snapdragon 8cx (modified ARM Cortex-X1 + Cortex-A78 + Qualcomm IP) - Oryon (64bit): Snapdragon X Elite is now Nuvia's modifications made on top of ARM's design, combined with Qualcomm's IP & modifications. So instead of being an iteration of the previous Qualcomm Kryo-based generation (applying Qualcomm's modifications onto a newer vanilla ARM-design), Qualcomm obviously applied their modifications on top of Nuvia's Design. - So (over)simplified: Qualcomm did the same as they did before on Kryo, but replaced the foundation with an already-modified ARM-design from the Nuvia team (I don't want to play down the effort Qualcomm took for that, for sure the Nuvia team they acquired had a huge amount of work to merge their design with Qualcomm's modifications. But overall, they put their customization-process on top of the Nuvia-output instead of the ARM-output) ARM's lawsuit is now obviously also about Qualcomm assuming that they don't need to license a new core-design from ARM because they acquired Nuvia, which achieves a comparable result using an older ARM-design. reply barkingcat 4 hours agorootparentprevLarge portions of the Nuvia team were ex Apple A- and M-series silicon designers and builders, presumably the Snapdragon X Elite used some of the cross pollinated ideas originating from apple silicon. Maybe not exact duplicate, but the ideas cross polinated. reply rickdeckard 4 hours agorootparentI think the same applies to ARM's newer designs as well, with ARM Blackhawk and Cortex-X series achieving similar results as Nuvia's design. My guess is that the work Nuvia did happened in close exchange with ARM, with each party owning their respective IP for commercialization (ARM for new generic designs, Nuvia for new server-centric designs). reply dvdkon 8 hours agoparentprevISAs are \"protected\" (read: monopolised) using patents IIRC. reply kragen 7 hours agorootparentthumb-2 is from 02002 and should therefore be off patent, but presumably this is arm64 reply immibis 8 hours agoparentprevIs it custom? reply fredgrott 8 hours agoparentprevarm licenses the design of the cpu, etc. including any patents reply jjtheblunt 4 hours agoprevinterestingly i thought snapdragon x elite had qualcomm-proprietary hexagon vliw processing cores also, but it's not mentioned. that would clearly not be in arm's intellectual property, but it's strange these articles (i searched and read others besides that linked) overlook this. reply lxgr 4 hours agoprevI was always wondering what the plan for Arm was post-IPO, given that they're currently trading at a P/E of more than 500 and they've historically been a low-margin, high-volume supplier. Here goes the pivot to become the next Intel, I guess? reply shrubble 4 hours agoprevI am almost tempted to buy one ASAP, in case ARM is successful; it would be an instant collectible! reply ChrisArchitect 4 hours agoprev[dupe] Discussion: https://news.ycombinator.com/item?id=40655516 reply fingerlocks 5 hours agoprevdupe with 90 comments: https://news.ycombinator.com/item?id=40655516 reply conradolandia 3 hours agoprevWow. Imagine an American company did this. People world admire then and the government would bail them out. reply sergius 6 hours agoprevDoes this make Risk V more attractive? reply rwmj 6 hours agoparentIt's a really dumb move from Arm, regardless of what the actual contracts say. reply ffgjgf1 5 hours agorootparentNot necessarily, they aren’t making much money from this. They really just want to negotiate a better deal not to prevent Qualcomm from building their own cores.. reply delfinom 6 hours agoparentprevIt's why Qualcomm is investing into RISC V because ARM is trying to burn the house down on many fronts. https://www.qualcomm.com/news/releases/2023/08/leading-semic... reply musicale 4 hours agoprevSo do intel and AMD. reply soundnote 5 hours agoprevSo this is why the LTT video on Snapdragon X had Arm blurred out XD reply chad1n 8 hours agoprevUnless the contract made with Nuvia states explicitly that a successor has to resign a new agreement, I doubt that Arm has any chance to win in the court. reply masklinn 8 hours agoparentAccording to the original complaint, both Nuvia’s IP and licenses were only transferable with ARM’s agreement. reply vetinari 7 hours agorootparentThat doesn't matter; ARM's argument is, that the Nuvia IP is bound to Nuvia's license and cannot be used with a different license (which Qualcomm has, without limitation to server). reply bluGill 5 hours agorootparentLicense terms like this change all the time. The real question is why ARM isn't offering reasonable terms to allow this - or are/did they and it was rejected? reply afiori 5 hours agorootparentprevI believe that Arm's argument is more in the other direction, that is that Qualcomm license does not cover IP developed by Nuvia. reply masklinn 4 hours agorootparentIt’s both, and more: - Qualcomm’s license doesn’t provide for using third-party developments - Nuvia’s license doesn’t allow using it on other platforms than servers - Nuvia’s derived IP is not transferable without ARM agreement reply afiori 2 hours agorootparentWhat I was trying to say is that the last point >- Nuvia’s derived IP is not transferable without ARM agreement Binds Nuvia and Nuvia's license, not Qualcomm. An analogy I can think of is how sometimes luxury brands give gifts or discounts to celebrities for marketing purposes with contract that forbid resales. (eg [0]) In this case the brand can only sue entities it had a specific contract with. Back to the case at hand I believe that unless Qualcomm license includes a term along the lines of \"You cannot buy Arm's IP unless Arm pre-approves it\"[1] to hold Qualcomm culpable of this transfer. To my understanding Arm used this proibition mainly to terminate Nuvia's license [0] https://duckduckgo.com/?q=john+cena+sells+car+lawsuit+ford&t... [1] AFAIK this line might be in their license. I obviously do not have specific knowledge. reply threeseed 7 hours agoparentprevWith comments like this you should be prefixing it with: \"I am a Patent Attorney with 20+ years experience in complex IT contracts\" Or not posting at all. reply imp0cat 7 hours agorootparentWell, are you? ;) reply threeseed 6 hours agorootparentNo. Which is why I am not commenting on the merits of a contract dispute. Definitive statements should demand some relevant experience. reply bluGill 5 hours agorootparentYou don't need to be a lawyer to make a definitive statement. And a lawyer wouldn't make a definitive statement on a forum like this anyway. A lawyer will know more than random lay people (ie me!) which makes what they say have more weight. Even if a lawyer comments though their comments will be in context of what every country(s) they work in. International contract law is very complex and nobody can tell you how it works in every country. reply supermatt 5 hours agorootparentprevPeople can have their own opinions on things. You don't need 20 years experience to have an opinion of patent law, even an informed one. reply rowanG077 5 hours agoprevTo me this seems a real bad move by arm. They are showcasing to all their customers why they need to get of arm asap. reply bluGill 5 hours agoparentUnless ARM can show they are offering reasonable terms to make this legal. I know of other cases (I'm under NDA so don't ask for details) where a company has decided to switch course and the new one wasn't allowed under the old contract, but the contract was renegotiated to allow the switch. reply notorandit 5 hours agoprevI for one am just waiting for RISC-V to take over. reply ffgjgf1 5 hours agoparentQualcomm would just probably develop their own proprietary cores with patented/proprietary extensions (because why wouldn’t they?). Crippling the ability of other RISC-v chips to run the software built primarily Qualcomm CPUs. Now at least there is ARM keeping them somewhat in check.. reply hammyhavoc 3 hours agorootparentCheap and open is going to win in this situation, even if what you described happens. Nobody wants to pay the piper in terms of licensing fees. reply anoncow 5 hours agoprevIt is time to destroy ARM reply dtx1 6 hours agoprev>So, there you have it. Arm wants Qualcomm to stop shipping the product it's been contesting, but to be honest, that's not usually how these things end. It's unlikely that any product will be delayed from hitting shelves. These cases tend to end with one company giving a bucket of cash to the other, and everyone moves on. Yeah, basically this. Though it seems rather risky of Qualcomm to essentially byte the hand that feeds them. If I was ARM, I'd consider not extending any licenses given to Qualcomm this. reply afiori 5 hours agoparentI believe that Qualcomm, like Apple has a perpetual license that could be hard for Arm to terminate reply dvdkon 8 hours agoprev [–] It looks like ARM is hell-bent on making the case for RISC-V, or maybe even on getting someone to challenge their ISA patents in court (if Qualcomm's new chips are custom designs). Here's hoping this kind of greed won't pay off. reply masklinn 7 hours agoparent [–] According to ARM’s complaint, they gave Nuvia exclusive licensing rights and IP access for specific purpose. Rather than work with arm to negotiate their own deal (or even access Nuvia’s derived IP in good faith) Qualcomm: - tried to discretely transfer the licenses to a brand new entity - worked directly with Nuvia’s cores, which Qualcomm’s own license did not permit - kept at it after nuvia’s own license was terminated - kept using the Arm trademark in their copy despite that also being linked to the licensing agreement So from ARM’s point of view, Qualcomm are the greedy ones, and lazy to boot. reply dvdkon 7 hours agorootparentIt seems weird to me to buy a company and then have to licence the company's own IP from a third party (AFAIK Qualcomm has an ARM ISA licence). Maybe Nuvia signed away exclusive rights to Arm, but at that point they'd be basically working for someone else's benefit. Or maybe the cores aren't as custom as Qualcomm is telling us, and they contain some separately-licenced Arm tech. reply bpfrh 7 hours agorootparentI don't think it is weird. Imagine you buy a non profit which has a special windows licence and then you use that licence for all your for profit windows devices, I don't think that that would be possible. AFAIK ARM gave Nuvia a licence to use their ip (and access to their IP) for a specific market. imho Qualcom has the right to use the nuvia ip as laid out in the licence agreement, but not for markets where no licence exists e.g. laptops. Edit: removed some nonsensical text reply dvdkon 6 hours agorootparentThat would make sense if this was about Arm IP, but it seems (reading between the lines here, sorry) that Arm feels that since Nuvia developed their cores under a specific licence from Arm, Qualcomm now needs to re-obtain that licence. Like if you bought a non-profit and Microsoft told you thay you can't use your pre-existing Visual Studio licence to edit their code, but need to re-negotiate theirs. reply bpfrh 5 hours agorootparentI agree with your firs point and I think that is the crux of the matter, but I goes a bit farther than only developed under a specific licence from Arm. Arm seems to also have given Nuvia access to the technology behind their own cortex cores. Arm sees anything that nuvia developed as a derivate of their IP and therefore subject to the agreed upon licencing term. I think that removes some of the weird feeling around the idea of just sharing patents and entering a licensing agreement, but even if arm only gave patent(licences) rights, it would still mean that all of the products of nuvia would be based on that ip. I don't think your example is applicable, in your situation the product is not a derivate of visual studio, but visual studio is just a tool. I think this fits the situation better: You want to write a custom video decoder and because you are small and new a bigger company allows you to use their super fast video decoder and some of their patents if you only sell your software for ios smartphones for a good price. If you breach any of this, your licence to use their encoder as a base for your products expires and you can't use any of their patents. reply afiori 5 hours agorootparentprevThe license belonged to the acquired not the acquirer. A more apt analogy would be if Microsoft gave the source code of Office to a research center and IBM then tried to resell their version of the office suite after buying the research center. To me out feels very similar to how a spyware company might want to buy a VPN just for their personal data. Except that here there is an actual contract forbidding this. My uninformed opinion is that Qualcomm is perfectly allowed to keep all the assets and artifacts that Nuvia produced, but is not allowed to create chips based on them reply rickdeckard 6 hours agorootparentprevNuvia developed this IP based on a very narrow license they got from ARM to develop for the server market. Qualcomm is trying to transfer everything Nuvia has developed under that specific narrow ARM-license to their own broad license and use it for all their products, but the license Nuvia had explicitly limits the use of IP created under this license and the license itself is non-transferrable. So Qualcomm doesn't have Nuvia's ARM-license anymore, and their own ARM-license doesn't cover the IP of Nuvia. ARM is arguing that the IP of Nuvia therefore does not belong to Qualcomm, and they seem to have the contract to support that. Meanwhile, Qualcomm thinks that they have no need to license newer (comparable) architectures from ARM, because they acquired the IP from Nuvia... reply sgc 5 hours agorootparentIt's about customizable vs off the shelf in the article, not newer / older. This article, although a year old, seemingly does a better job of explaining each company's position: https://news.bloomberglaw.com/us-law-week/arm-qualcomm-lawsu... reply fidotron 6 hours agorootparentprevAnd they also are actively trying to turn RISC-V into AArch64 but without any license fees. If you even try to do any business with Qualcomm they make it incredibly clear they will go nuclear on anyone for the slightest breach of their contracts to protect their IP, but they completely fail to make this reciprocal. It is a classic case of what-I-do-is-valuable-what-you-do-is-trivial. reply WesolyKubeczek 3 hours agorootparentprev [–] > So from ARM’s point of view, Qualcomm are the greedy ones, and lazy to boot. And from Qualcomm’s point of view, ARM are greedy and would like to double-dip Qualcomm for licensing fees and whatnot… But you know what, they might both be right. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Arm is attempting to remove Qualcomm from the Windows market to introduce its own Cortex design, leading to a legal dispute over licensing rights for custom Arm chips.",
      "Rumors suggest Nvidia, MediaTek, and AMD may enter the Windows ecosystem with Arm chips, potentially by CES 2025, with MediaTek possibly using Arm's Cortex cores.",
      "Arm claims Qualcomm's use of Nuvia designs violates contractual obligations, seeking to halt shipments and destroy Snapdragon X series chips, though such disputes typically end in financial settlements."
    ],
    "commentSummary": [
      "Arm is demanding the destruction of all Snapdragon X Elite laptops due to a licensing dispute with Qualcomm over the use of Nuvia's server cores in the Snapdragon X Elite SoC.",
      "The core issue is whether Qualcomm's acquisition of Nuvia invalidated Nuvia's specific license with Arm, which was originally for server/datacenter usage only.",
      "The outcome of this legal battle could significantly impact future licensing deals and the broader tech industry, with potential implications for how intellectual property (IP) is transferred and used post-acquisition."
    ],
    "points": 175,
    "commentCount": 143,
    "retryCount": 0,
    "time": 1718271192
  },
  {
    "id": 40669337,
    "title": "Shpool, a Lightweight Tmux Alternative",
    "originLink": "https://github.com/shell-pool/shpool",
    "originBody": "shpool is a terminal session persistence tool developed internally at google to support remote workflows, which we have open sourced.",
    "commentLink": "https://news.ycombinator.com/item?id=40669337",
    "commentBody": "Shpool, a Lightweight Tmux Alternative (github.com/shell-pool)154 points by ethanpailes 5 hours agohidepastfavorite84 comments shpool is a terminal session persistence tool developed internally at google to support remote workflows, which we have open sourced. mikenew 3 hours agoThis is awesome! I hate the way tmux hijacks so much of my terminal's behavior (scrollback, seaching with escape-/, etc.) and I've been looking for something like this that will manage persistent sessions without any extra nonsense. BTW I think your readme shouldn't just characterize it as a resumeable ssh tool. I often need to start a long running process that I want to reconnect to later, or I want to put some always-on system service inside a tmux container so I can easily jump in and see logs or mess with things in one way or another. There's a lot of utility besides just handling network dropouts. reply ori_b 1 hour agoparentThe thing is that without hijacking it and passing it through, you can't have nice things like handling resizes, supporting attaching with multiple terminal emulators, or reconnecting to applications that make heavy use of terminal escape codes, because all of those set up persistent state in the terminal emulator. As a result, any tmux-like layer needs to emulate a console in order to get a view into the state, and then re-render that emulated console on attach to restore state to the terminal emulator that you're connected from. From the readme, this tool does that, kinda. I'm actually confused about why they'd go to the effort of implementing a VT100 emulator, write the code to redraw the screen from it, and yet not bother with doing the work that would let multiple terminal emulators attach. This feels like it sits in a weird place between simple, crude tools like dtach, and tools like tmux; shpool has done most of the work to implement tmux-style behavior, and then decides to cut weird corners. reply ethanpailes 39 minutes agorootparentYou definitely don't need the in-memory terminal emulator to handle resizes or allow attaching with multiple local terminal emulators, since dtach does both and does not have an in-memory terminal emulator. > I'm actually confused about why they'd go to the effort of implementing a VT100 emulator, write the code to redraw the screen from it Well, we kinda cheated here. shpool_vt100 is just the already existing vt100 crate with a single critical bug fixed, so it actually wasn't much work :). Turns out having a nice package manager for a systems language comes with some benefits. I'm actually open to adding a feature to allow multiple simultaneous connections to a single session. I never really had a usecase for it personally so I haven't prioritized it, but it is something that similar tools support and people keep bringing up. Since this isn't the first time I've heard people talking about it, I just made https://github.com/shell-pool/shpool/issues/40 to track work adding the ability to attach multiple clients to the same session. > This feels like it sits in a weird place between simple, crude tools like dtach, and tools like tmux; shpool has done most of the work to implement tmux-style behavior, and then decides to cut weird corners. I'm not aware of any tool that does internal rendering and subsetting handling scrollback and copy-paste in a way that I personally find usable, so these decisions were very much intentional. I think tmux is a great tool for a lot of people, and I tried to get into it for years, but I could just never get over the weird scrollback and copy-paste issues or the fact that it meant that I couldn't use my normal `i3`/`sway` bindings to switch between terminals inside a tmux session. If tmux works for someone, I think that's great and they should keep using it. shpool is meant for people like me who aren't very good with computers :). reply bee_rider 20 minutes agorootparentI’m not sure what the popular use case is for multiple connections to one multiplexer. But, two (niche seeming) ones could be: if you have a desktop, you want to be able to SSH to it and use it locally at the same time. Or, if you have two people ssh-ing to one system, and letting them share a terminal might be nice (although in that case it would really be nice to give them independent cursors, which starts to become an involved project). reply sandreas 1 hour agoparentprevI think you would love zellij[1]. Go check it out, it is awesome. 1: https://zellij.dev/ reply slt2021 15 minutes agoprevI used to use tmux, but then switched completely jupyter terminals. my browser is my terminal now, and I can easily upload/download files via browser (dont type SCP anymore) with a mouse click, open terminal windows, open Python notebook and do stuff. life has never been easier after I completely switched to jupyter notebooks+terminals plus, because it is browser based, I can roam from multiple devices. reply radarsat1 36 minutes agoprevLooks nice. One feature I'd like is a read-only mode. Sometimes I want to execute a long running process and be able to just watch it run without risking that I hit Ctrl C by accident. I was surprised to find that Screen doesn't support this, I guess probably tmux does. tail is fine but I don't always want to record a huge log file. reply nrh 3 hours agoprevI'd love to try this, but most of the places that I would want to use it are servers, and the rust requirements are way beyond where debian-stable lives. Too much hassle to muck with backports and package pinning for a QoL tool - my feedback would be to try to make this install without tomfoolery on the stable branch of major distros. reply michaelmior 2 hours agoparentRust executables generally compile to static binaries. No you don't need to install Rust on the server, just compile once locally and copy the binary. reply e12e 1 hour agoparentprevThere's a \"debian\" folder, I suspect it's trivial to build a Deb and manually install? Seems to have pretty modest install dependencies? https://github.com/shell-pool/shpool/blob/master/debian/cont... reply krunck 1 hour agorootparentI had to do this: git clone https://github.com/shell-pool/shpool.git cd shpool sudo apt install libpam0g-dev # you may need to install build-essential and other packages too as the # build-depends field in debian/control is incomplete dpkg-buildpackage -b sudo apt install ../shpool_0.6.2_amd64.deb systemctl --user enable shpool systemctl --user start shpool reply superfish 3 hours agoprevI super duper highly recommend iterm2 (macos) + its integrated tmux support. It was relatively popular in my circles at google when I was there. It comes with this tool’s benefit of native scrolling/cp paste PLUS the huge benefit of “right click to split vertical/horizontal”. reply mdaniel 2 hours agoparentDon't overlook the awesome \"automatically bury session\" option, which hides the \"actual\" iTerm2 window running the tmux control plane: https://iterm2.com/documentation-buried-sessions.html and this is the preference I mean: https://github.com/gnachman/iTerm2/blob/v3.5.2/Interfaces/Pr... I also have dedicated .ssh/config entries for ensuring that ssh connects directly to tmux: Host whatever-mux Hostname whatever RequestTTY yes RemoteCommand /usr/bin/tmux -u -CC att reply setopt 2 hours agoparentprevThe `tmux -CC` is pretty dope. I’m still surprised that not more terminals have picked it up yet. Most GUI features (new tab, new split, scroll, search, copy/paste, etc.) just work, and it all syncs with `tmux`. Be aware though that it can be a bit buggy if you have a fancy `tmux.conf`, and that if you rely on any `tmux` plugins then most of those simply won’t work. reply thayne 1 hour agorootparentI'm especially surprised that tmux itself can't be used as a client of a remote control mode session. It is pretty annoying that the only terminal that I know of that supports tmux control mode only works on mac. reply mtlmtlmtlmtl 1 hour agorootparentI looked into this at one point. IIRC it turns out control mode was only added to tmux by the iterm2 dev, for use in his own project. So I guess he didn't care about adding support to tmux itself. reply linsomniac 2 hours agoparentprevOn that front: I've been using wezterm which includes a built-in tmux+mosh functionality, and it works quite well. It gives you first-class scroll/copy/paste management and multi-windows, plus session re-attachment. Probably 50% of my use of my mac is just SSHing in to my Linux box, and wezterm works great for that. reply ibejoeb 2 hours agoparentprevThe last time I used it, which was probably pushing 5 years ago, it was a big drain on the battery. It was enough that I went back to Terminal.app. Anyone else experience that? Has it improved? reply setopt 2 hours agorootparentI believe iTerm2 has been GPU accelerated for some years now which probably helps. I’ve only been using it for 3-4 years and haven’t had this issue. reply gregwebs 4 hours agoprevUnbundling tmux is a great concept! Other tools that can overlap in use case are EternalTerminal and mosh which provide sessions that survive disconnects. reply ethanpailes 4 hours agoparentOh thanks for mentioning EternalTerminal. I should add it to the comparison section in the README. It looks like it is similar to mosh in that it does stuff at the network level whereas all these other tools are purely remote-machine side programs, but it seems like it is a bit simpler than mosh. From a scan of their docs it sounds like they have the local terminal do all the rendering the way `shpool` does and don't try to muck about with any sort of predictive rendering the way mosh does. Does that sound right to anyone knows it well? reply jamal-kumar 49 minutes agoprevYears ago I had a boss who gave me a hard time for how much resources tmux was using on linux and said that it performed fine where it's developed natively (openbsd) but something in it was making it eat resources compared to screen on constrained linux systems. I wonder if this is still the case? reply tasuki 2 hours agoprevOh, this might be the missing piece of the puzzle for me to get rid of tmux! I've been using screen/tmux for a long time. Recently I switched to kitty[0] locally. I like kitty a lot! But I've been stuck with tmux on my servers for session persistence. [0]: https://sw.kovidgoyal.net/kitty/ reply eigenvalue 3 hours agoprevnohup is also useful for this situation, but a lot of people don’t seem to know about it! reply zbentley 23 minutes agoparentAs the sibling comments show, nohup does occasionally have sharp edges re: sharing/inheriting std* streams. I've also encountered inconsistencies when using nohup with lots of different users' shells (academics of wildly variable technical skill levels, shell environments/customizations, and opinionatedness levels): what, exactly, does it mean to be backgrounded per \"&\"? Does nohup+& sufficiently background all types of shell jobs (answer: no--and some of those jobs even make sense to use with nohup)? Can configs in {ba,z,da,k,fi}sh change the job control behavior so that \"forgetting\" jobs malfunctions under nohup (if that's possible, your users will do it, I've found--and then defend their bizarre choices until you give up and move on)? In those cases, I've found dtach to be slightly more reliable than nohup. Frequently I'll try one and switch to the other. Less frequently, I'll correct users' configs/workflows that are necessitating the troubleshooting in the first place. Even less frequently, I'll dig into what was actually the issue :) https://linux.die.net/man/1/dtach https://dtach.sourceforge.net/ reply tasuki 2 hours agoparentprevDoes nohup allow attaching back to the session? That seems to be the primary use case of shpool. reply threePointFive 2 hours agorootparentNohup still attaches to the terminal's stdin/out/err. If the process is known to be non-interactive, redirecting to log files should be sufficient (tail -f to \"reattach\"). If it does expect interaction, creating a fifo file to redirect stdin from should work, but I've never tested it. reply rangerelf 1 hour agorootparentnohub doesn't attach nor redirect the stdin/out/err, it intercepts the HUP signal that apps receive when their controlling TTYs are disconnected. Implying \"tail -f $LOGFILE\" is akin to tmux reattaching to a console is a stretch. reply eigenvalue 2 hours agorootparentprevNo, it doesn't let you attach back, it just keeps a command going after you've disconnected your session. But if you have good logging that might be good enough. reply chaps 1 hour agorootparentOr use GDB to change stdin/stdout of the nohup'd process to the current shell! reply krupan 56 minutes agorootparentlol! reply VagabundoP 2 hours agoprevI always found tmux and screen clunky to use when all I wanted was the persistence so this project is right up my alley. reply chaps 2 hours agoprevThis is more of a `screen` replacement than a `tmux` replacement, yeah? Was hopeful for an actual replacement (no, not iterm) to tmux. reply e12e 1 hour agoparentAren't tmux and screen rather equivalent these days? How is an alternative to one, not an alternative to the other? Shpool doesn't appear to allow for session sharing/remote pairing though? reply chaps 1 hour agorootparentshpool doesn't allow splitting screens either! But I guess in my head, screen is largely for resuming a shell connection on a remote host while tmux is a window splitter. reply jauntywundrkind 4 hours agoprevThere's also the longstanding option dtach, which similarly seems to be a lightweight re-attachable way to run a program. https://github.com/crigler/dtach For a while I was running neovim on dtach, which let me host all the terminals I might want. It has long felt weird that we have so recursively many layers of management and navigation: the OS juggling multiple terminals, tmux juggling multiple shells, and neovim juggling multiple windows. I was doing pretty good in full screen neovim for a while, but have backed down a bit, still find tmux navigation a bit faster. reply Retr0id 4 hours agoparentThe readme contains a brief comparison to dtach: https://github.com/shell-pool/shpool?tab=readme-ov-file#dtac... IIUC, the main difference is that shpool maintains an in-memory copy of terminal state that it can use to re-draw after a reconnect, so you don't lose all your scrollback. reply zokier 4 hours agoparentprev> It has long felt weird that we have so recursively many layers of management and navigation: the OS juggling multiple terminals, tmux juggling multiple shells, and neovim juggling multiple windows Tbh that's why I'm not a fan of tmux or terminals with fancy internal window management. I suppose it makes sense if your OS level window management is crap (macos...) but otherwise it feels like poor solution reply chrisweekly 33 minutes agorootparent\"if your OS level window management is crap (macos...)\" -- in which case Divvy is a great solution. YMMV, but IME, Divvy and iTerm2 cover 98% of the things I'd want to do. reply Tyr42 3 hours agorootparentprevBut tmux on server survives the laptop sleeping. That's the key value. reply andrewla 4 hours agoparentprevdtach, abduco, and diss are all mentioned in comparison at the bottom of the page. I've never used diss before, but the other two are good products, but for some reason the ergonomics never quite seem to work out for me and I revert back to tmux. Similarly dvtm, which is a minimalist version of the window management aspects of tmux but omits the session management, also never quite seems to click for me. reply praveen4463 2 hours agoprevWill use it for sure. starred it. reply pnutjam 4 hours agoprevWhy is this better then the old school \"screen\"? reply pnutjam 4 hours agoparentanswer to my question is on the description page of Shpool. Looks compelling. ---- The biggest advantage of this approach is that shpool does not break native scrollback or copy-paste. reply OwseiWT 3 hours agoparentprevI'd say the best part is not needing screen on the server. If you have several servers and several people that like several multiplexers, you would need to install all of them in each server, and manage configuration etc. However, you can have your own multiplexer and use shpool on a window to connect to a server. Done, you have your favorite multiplexer, with your configuration, and the only tool in the server is shpool reply michaelmior 2 hours agorootparentI'm not following what the difference is between installing screen on the server and installing shpool in the server. reply Scarbutt 2 hours agoprevBesides persistence, terminal multiplexing is one of the greats things of tmux. If I'm programming on a remote machine I don't want to open a new ssh connection from each of the five terminals that I need. Seems tedious when I can just create a new window inside tmux. Even locally, I don't want to have 10 terminals opened (or tabs), tmux lets you have multiple terminal windows/panes from a single physical terminal and lets you easily group and switch between them into what they call sessions. reply randomtoast 4 hours agoprevA lightweight tmux alternative? Tmux is already an incredibly lightweight C program in terms of binary size and resource usage. reply hnlmorg 4 hours agoparentI assumed they meant “lightweight” in the sense that tmux can take quite a bit of setup and practice to get comfortable in, when using for the very first time. I love tmux but it’s definitely not “n00b” friendly. reply MuffinFlavored 3 hours agorootparent$ tmux # run your command in it # Ctrl+b D to detatch later, to attach back to it $ tmux a -t 0 That probably covers 50% of usage. The rest would be naming the sessions, splitting the window panes. Anything else? reply hnlmorg 2 hours agorootparentYes. Window management, mouse support, key bindings (I really dislike the defaults for a few of them), selection/copy/paste, macros etc All stuff that doesn’t need to be configured but if you look at any tmux tutorial you’ll see boat loads of details about. Which can be confusing because how do you know if you don’t need to know it? Also what’s the basics for one person isn’t going to be the basics for another. Eg I rarely use the detach capabilities these days whereas the panes and windows are something I do use lots. My point is this: tmux is sufficiently complicated that a complete beginner might not know what features they need to learn. reply Tyr42 3 hours agorootparentprevScrolling. That's a key feature when you don't reflexively use less and ] isn't a natural keybind for \"let me scroll please\". reply pawelduda 3 hours agorootparentAlso search, copy-paste reply CyberShadow 4 hours agoparentprevtmux takes over scrollback, so it's not possible to scroll the buffer in the same way as without tmux. This tool seems to solve the problem more elegantly. reply whalesalad 4 hours agorootparenthttps://github.com/whalesalad/dotfiles/blob/master/tmux.conf... Scrolling works pretty well for me. reply cstrahan 4 hours agorootparentThat gets you scrolling, yes, but not in the same way as without tmux. Without tmux, your terminal emulator uses its scroll-back buffer to render as you scroll, not requiring any intermediate copies of the lines of text scrolling into view. With tmux, your terminal emulator no longer handles that, and instead tmux must pass a copy of each line freshly scrolled into view to the terminal emulator, which involves an intermediate copy of that text and re-triggers the terminal emulator’s parsing (as far as your terminal emulator is concerned, it has never seen this line of text yet). All of that (and some other subtle overhead I’m not mentioning here) are completely avoided outside of tmux (or more specifically: whenever the terminal emulator is free to manage its own scroll-back). reply ethanpailes 4 hours agorootparentprevtmux does support scrolling and copy-paste, but they often work subtly differently than the way they do in a native terminal. For example, when I try putting `set -g mouse on` in `~/.tmux.conf`, scrolling mostly works fine but if I scroll up and then start typing or press up, I won't jump right to the bottom of the session the way I'm used to in native Alacritty (which I will often do as a way to return after scrolling back through a bunch of print output). This isn't a huge deal, and you can definitely get used to it, but it can be frustrating if you like how your normal terminal emulator works and don't want it to change. reply whalesalad 1 hour agorootparentnothing in life is free, what do you honestly expect? reply ethanpailes 4 hours agoparentprevYeah that's a good point, \"lightweight\" was probably not the best term. I meant lightweight in terms of the cognitive overhead of picking up a new tool, not really in terms of performance. I would expect that shpool will perform better on some workloads and tmux will perform better on others (though in both cases they are probably good enough). The main difference is that tmux is a power user tool that you generally invest in configuring and tweaking to get just right, while shpool is meant to be set up quickly to solve the specific problem of dropped connections and then mostly forgotten about. reply freedomben 3 hours agorootparentInteresting thanks. I interpreted \"lightweight\" to mean available features. i.e. just the persistent connection, none of the other bloat[1]. [1]: to be clear, I don't mean \"bloat\" in a negative way. I'm a long time tmux user and I love it, absolutely love it, and won't be giving it up anytime soon. I don't personally think it has any bloat. But, I use the features. If one only wanted persistent connections, then all of tmux is quite a bit of bloat. It's all perspective. reply bjoli 4 hours agoparentprevLightweight in terms of speed. Tmux makes even the slowest terminal feel like a Ferrari. People say the use the likes of alacritty for the speed, and then they run tmux and make it slower than even gnome-terminal. reply kreetx 4 hours agorootparentHow about calling it \"fast\" then? reply jacknews 4 hours agoparentprevlightweight in terms of features of course. it simply allows you to have multiple shell/term sessions on a server and be able to switch between them from any terminal. Kind of a 'flat ctrl-Z' cum 'nohup' if you will. No windows, splits, etc, no client software at all in fact. reply wkat4242 4 hours agorootparentOh thanks for checking, then I'll leave this. Splits are my primary feature in tmux. reply KingOfCoders 4 hours agoprevRecently learned about Zellij and it replaced my tmux usage. reply aranelsurion 4 hours agoparentIf you don’t mind explaining, what are the advantages you get over tmux? reply xhrpost 2 hours agorootparentMain thing that's keeping me right now is copy select with mouse that stays in pane. Tmux would always cross horizontal panes so I'd have to zoom the current pane, alt + mouse select, then unzoom. Zellij I just drag over what I want and release the mouse button. I do however miss using the mouse for pane resizing though. reply tjoff 1 hour agorootparenttmux keeps the selection to the pane for me. Not something I make use of though, feels faster to copy using the keyboard instead. reply xhrpost 52 minutes agorootparentDid you have to use a particular setting? I looked into it a few times but couldn't figure it out. reply tjoff 26 minutes agorootparentTried it in a container with only \"set -g mouse on\" in the config and it works fine (v. 3.2a). My best guess is that your terminal selection is overriding tmux. reply ibrarmalik 4 hours agoparentprevtmux works out of the box for me. I tried Zellij and couldn’t get the Alt key to work on mac. And then when ssh’ing into a server I couldn’t see some of the icons because it required a special patched font. reply beryilma 4 hours agoprevHow could this be a tmux alternative if it only provides session persistence? I use tmux mostly for layout (tiles, multiple windows, etc.) and not for session persistence necessarily. I don't know if this is why other people also use tmux, but I am not sure if \"alternative\" is the right word here. reply ethanpailes 33 minutes agoparentIt's definitely not a complete alternative, but a surprising number of tmux users are only in it for the session persistence. We did a little internal (criminally underpowered and confounded) survey of some tmux users at google and found that half of them were only really using it for session persistence. I was really surprised by this and thought more people would be using all the slick tiling features that tmux has. reply barnabee 3 hours agoparentprevI use tmux strictly for persistence only, and WezTerm for layout etc. so this is definitely an alternative for me. This is very interesting to me as it seems to provide persistence with much less cognitive overhead than tmux or even screen sessions, which are enough that I often don't bother and just use SSH. I also find tmux has a noticable latency overhead (on every terminal I've tried) so I wouldn't use it locally anyway. Suffice to say this is very interesting and I'll check it out. reply andrewla 3 hours agoparentprevThere are a bunch of minimalist session persistence solutions, including now shpool. If you want layout, there's a minimalist program called `dvtm` that provides those capabilities. The suggestion usually is to run dvtm inside of dtach/shpool/abduco/diss for layout, and separating the concerns that way. reply sullyj3 3 hours agoparentprevIt could be a tmux alternative if the session persistence is the only feature of tmux you care about, and you don't like the rest of it. reply stvltvs 3 hours agoparentprevI use it the same way because I use it locally 100% of the time. I would probably care about persistence more if I used tmux on remote machines. reply blacklion 4 hours agoprev [–] No mention of screen? I'm too old. reply ethanpailes 4 hours agoparentScreen is discussed along with tmux in the [comparison section](https://github.com/shell-pool/shpool#tmux-and-gnu-screen) of the README. For most of the section I just say `tmux` because saying `tmux` or `screen` can get annoying and they have a very similar set of features. reply jerdfelt 4 hours agoparentprevIt is mentioned in the first paragraph of the README: shpool is a service that enables session persistence by allowing the creation of named shell sessions owned by shpool so that the session is not lost if the connection drops. shpool can be thought of as a lighter weight alternative to tmux or GNU screen. reply blacklion 2 hours agorootparentI mean, nobody mention Screen here, in comments, when I wrote my comment reply freedomben 3 hours agoparentprev [–] I betrayed screen over a decade ago and switch to tmux, and have been immensely happy. I have a tremendous amount of love and appreciation for the venerable Screen, but realistically very few people use it anymore. I don't think it makes much sense to compare. For the few that do still use Screen, they pretty much know how it compares to tmux so the tmux compare is pretty applicable. reply e12e 1 hour agorootparent [–] I'm a happy screen user. But tbh, the main benefit (to me) is default ctrl-a key binding, while tmux would require a config (which I actually think is great, that tmux politely doesn't conflict by default). Not sure why I'd want to switch. reply spudlyo 1 minute agorootparent [–] It doesn't actually require a config. You can set the binding on the command-line when you launch it. Anything you can represent in a config file with Tmux (set -g prefix C-o) can be expressed on the command-line. Furthermore, in my opinion, both the defaults (C-a, C-b) are bad, as they conflict with default readline key bindings for cursor movement. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Google has open-sourced Shpool, a tool initially developed for maintaining persistent terminal sessions in remote workflows.",
      "Shpool aims to enhance remote work efficiency by allowing continuous terminal sessions without interruptions.",
      "This release is significant as it provides developers with a robust solution for managing remote terminal sessions, potentially improving productivity in distributed teams."
    ],
    "commentSummary": [
      "Shpool, a terminal session persistence tool developed by Google, has been open-sourced as a lightweight alternative to Tmux.",
      "Shpool is designed for managing persistent sessions without extra features, making it suitable for long-running processes and system services.",
      "The tool emulates a console to view state and re-render on attach, but currently does not support multiple terminal emulators or heavy use of terminal escape codes."
    ],
    "points": 154,
    "commentCount": 84,
    "retryCount": 0,
    "time": 1718284949
  },
  {
    "id": 40672215,
    "title": "You can help Anna's Archive by seeding torrents",
    "originLink": "https://annas-archive.org/torrents",
    "originBody": "Torrents This torrent list is the “ultimate unified list” of releases by Anna’s Archive, Library Genesis, Sci-Hub, and others. By seeding these torrents, you help preserve humanity’s knowledge and culture. These torrents represent the vast majority of human knowledge that can be mirrored in bulk. These torrents are not meant for downloading individual books. They are meant for long-term preservation. With these torrents you can set up a full mirror of Anna’s Archive, using our source code and metadata (which can be generated or downloaded as ElasticSearch and MariaDB databases). We also have full lists of torrents, as JSON. Currently 60% of the total 521.1TB is copied in at least 4 locations, and only 3% in more than 10 locations. We need your help to get to 100%! “The lost cannot be recovered; but let us save what remains: not by vaults and locks which fence them from the public eye and use, in consigning them to the waste of time, but by such a multiplication of copies, as shall place them beyond the reach of accident.” — Thomas Jefferson, 1791 Guide § The list of torrents is split in three parts: 1. The first part is managed and released by Anna’s Archive. These include books, papers, and magazines from websites such as Z-Library and Internet Archive. It also includes metadata records from websites such as WorldCat and ISBNdb. 2. The second part is managed and released by others, such as Library Genesis and Sci-Hub. We include these torrents in order to present a unified list of everything you need to mirror Anna’s Archive. 3. Miscellaneous other torrents; not critical to seed and not included in stats or the torrent list generator. Torrents seeded by Anna’s Archive are indicated with a checkmark (✅). Some torrents get temporarily embargoed (🔒) upon release, for various reasons (e.g. protecting our scraping methods). An embargo means very slow initial seeding speeds. They get lifted within a year. For more information about the different collections, see the Datasets page. Also see the Torrents FAQ. IMPORTANT: If you seed large amounts of our collection (50TB or more), please contact us so we can let you know when we deprecate any large torrents. Stats § You can help out enormously by seeding torrents that are low on seeders. If everyone who reads this chips in, we can preserve these collections forever. This is the current breakdown, excluding embargoed torrents, but including external torrents: 🔴 210.2TB 10 seeders Apr 212024 Apr 28 May 5 May 12 May 19 May 26 Jun 2 Jun 9 0TB 100TB 200TB 300TB 400TB 500TB Scraped from opentrackr.org. Generate Torrent List § Generate a list of torrents, sorted by (seeders + 0.1*leechers)*fraction-of-torrent-size-compared-to-average-size + random-number-between-0.0-and-2.0, ascending. Specify a maximum TB to store (we simply keep adding torrents until max TB is reached). Max TB: Type: JSON URLs Magnet links Generate We only show non-obsolete, non-embargoed files with at least one seeder here. For a complete list see the full torrents JSON. For an unofficial tool that actually downloads the torrent files, see this repo. Similar Lists § Similar lists, independently maintained. Note that at the time of this writing, all these lists are included in our list, under External Collections, similarly to how Anna’s Archive itself is a meta-collection of many external collections. ipdl.cat PhillM's LibGen torrent index Managed by Anna’s Archive § These torrents are managed and released by Anna’s Archive. Torrents with “aac” in the filename use the Anna’s Archive Containers format. Torrents that are crossed out have been superseded by newer torrents, for example because newer metadata has become available — we normally only do this with small metadata torrents. duxiu 39.1GB / 6 torrents § DuXiu and related. full list / dataset / blog ✅ annas_archive_meta__aacid__duxiu_records__20240130T000000Z--20240209T000000Z.jsonl.zst.torrentmagnet 2024-02-21 10.0GB / 1 metadata 🟡 8 seed / 6 leech 5h ✅ annas_archive_meta__aacid__duxiu_files__20240229T082726Z--20240229T131900Z.jsonl.zst.torrentmagnet 2024-03-07 352.7MB / 1 metadata 🟡 6 seed / 0 leech 5h ✅ annas_archive_meta__aacid__duxiu_records__20240130T000000Z--20240305T000000Z.jsonl.zst.torrentmagnet 2024-03-10 28.0GB / 1 metadata 🟢 20 seed / 1 leech 5h ✅ annas_archive_meta__aacid__duxiu_files__20240312T053315Z--20240312T133715Z.jsonl.zst.torrentmagnet 2024-03-15 310.4MB / 1 metadata 🟡 10 seed / 0 leech 4h ✅ annas_archive_meta__aacid__duxiu_files__20240502T224038Z--20240503T063035Z.jsonl.zst.torrentmagnet 2024-05-17 233.6MB / 1 metadata 🟡 4 seed / 0 leech 5h ✅ annas_archive_meta__aacid__duxiu_files__20240520T021707Z--20240520T061909Z.jsonl.zst.torrentmagnet 2024-05-20 235.5MB / 1 metadata 🟢 11 seed / 0 leech 5h full list for “duxiu” (6 torrents) ia 135.6TB / 182 torrents § Internet Archive Controlled Digital Lending books and magazines. The different types of torrents in this list are cumulative — you need them all to get the full collection. full list / dataset ✅ annas-archive-ia-2023-06-metadata-json.tar.gz.torrentmagnet 2023-06-28 25.3GB / 1 metadata 🟢 23 seed / 1 leech 4h ✅ annas-archive-ia-2023-06-metadata-json.txt.gz.torrentmagnet 2023-06-28 113.4MB / 1 metadata 🟢 26 seed / 0 leech 3h ✅ annas-archive-ia-2023-06-metadata-xml.tar.gz.torrentmagnet 2023-06-28 5.3GB / 1 metadata 🟢 22 seed / 4 leech 5h ✅ annas-archive-ia-2023-06-metadata-xml.txt.gz.torrentmagnet 2023-06-28 111.5MB / 1 metadata 🟢 26 seed / 9 leech 4h ✅ annas-archive-ia-2023-06-thumbs.tar.torrentmagnet 2023-06-28 144.2GB / 1 metadata 🟢 17 seed / 2 leech 4h ✅ annas-archive-ia-2023-06-thumbs.txt.gz.torrentmagnet 2023-06-28 113.1MB / 1 metadata 🟢 27 seed / 0 leech 5h ✅ annas-archive-ia-2023-06-files.csv.gz.torrentmagnet 2023-07-02 184.7MB / 1 metadata 🟢 28 seed / 0 leech 4h ✅ annas-archive-ia-acsm-a.tar.torrentmagnet 2023-07-06 2.2TB / 1 data 🔴 3 seed / 23 leech 4h ✅ annas-archive-ia-acsm-b.tar.torrentmagnet 2023-07-06 2.0TB / 1 data 🟡 8 seed / 5 leech 4h ✅ annas-archive-ia-acsm-c.tar.torrentmagnet 2023-07-06 3.1TB / 1 data 🟡 9 seed / 38 leech 5h ✅ annas-archive-ia-acsm-d.tar.torrentmagnet 2023-07-06 1.7TB / 1 data 🟡 8 seed / 14 leech 4h ✅ annas-archive-ia-acsm-e.tar.torrentmagnet 2023-07-06 1.7TB / 1 data 🟡 6 seed / 15 leech 4h ✅ annas-archive-ia-acsm-f.tar.torrentmagnet 2023-07-06 1.5TB / 1 data 🟡 5 seed / 30 leech 4h ✅ annas-archive-ia-acsm-g.tar.torrentmagnet 2023-07-06 1.4TB / 1 data 🟡 4 seed / 49 leech 4h ✅ annas-archive-ia-acsm-h.tar.torrentmagnet 2023-07-06 1.7TB / 1 data 🟡 6 seed / 9 leech 5h ✅ annas-archive-ia-acsm-i.tar.torrentmagnet 2023-07-06 4.6TB / 1 data 🟡 9 seed / 8 leech 5h ✅ annas-archive-ia-acsm-j.tar.torrentmagnet 2023-07-06 580.8GB / 1 data 🟢 12 seed / 21 leech 5h ✅ annas-archive-ia-acsm-k.tar.torrentmagnet 2023-07-06 357.2GB / 1 data 🟡 10 seed / 7 leech 5h ✅ annas-archive-ia-acsm-l.tar.torrentmagnet 2023-07-06 2.0TB / 1 data 🟡 10 seed / 7 leech 4h ✅ annas-archive-ia-acsm-m.tar.torrentmagnet 2023-07-06 2.3TB / 1 data 🟡 6 seed / 10 leech 5h full list for “ia” (182 torrents) isbndb 4.4GB / 1 torrent § ISBNdb metadata. full list / dataset / blog ✅ isbndb_2022_09.torrentmagnet 2022-10-31 4.4GB / 1 metadata 🟢 41 seed / 1 leech 4h full list for “isbndb” (1 torrent) libgenrs_covers 363.4GB / 1 torrent § Book covers from Libgen.rs. full list / dataset / blog ✅ covers-2022-12.torrentmagnet 2022-12-09 363.4GB / 1 metadata 🟡 9 seed / 1 leech 4h full list for “libgenrs_covers” (1 torrent) worldcat 235.3GB / 1 torrent § Metadata from OCLC/Worldcat. full list / dataset / blog ✅ annas_archive_meta__aacid__worldcat__20231001T025039Z--20231001T235839Z.jsonl.zst.torrentmagnet 2023-10-02 235.3GB / 1 metadata 🟢 15 seed / 2 leech 4h full list for “worldcat” (1 torrent) zlib 54.1TB / 277 torrents § Z-Library books. The different types of torrents in this list are cumulative — you need them all to get the full collection. full list / dataset ✅ pilimi-zlib-0-119999.torrentmagnet 2022-07-07 35.7GB / 118327 data 🟢 48 seed / 5 leech 4h ✅ pilimi-zlib-120000-419999.torrentmagnet 2022-07-07 83.7GB / 192396 data 🟢 20 seed / 2 leech 5h ✅ pilimi-zlib-420000-2379999.torrentmagnet 2022-07-07 94.7GB / 56453 data 🟢 18 seed / 4 leech 4h ✅ pilimi-zlib-2380000-2829999.torrentmagnet 2022-07-07 99.5GB / 16272 data 🟢 16 seed / 5 leech 4h ✅ pilimi-zlib-2830000-5239999.torrentmagnet 2022-07-07 102.1GB / 9814 data 🟢 19 seed / 4 leech 4h ✅ pilimi-zlib-5240000-5329999.torrentmagnet 2022-07-07 83.5GB / 15964 data 🟢 18 seed / 5 leech 5h ✅ pilimi-zlib-5330000-5359999.torrentmagnet 2022-07-07 161.6GB / 17685 data 🟢 15 seed / 7 leech 5h ✅ pilimi-zlib-5360000-5379999.torrentmagnet 2022-07-07 84.1GB / 18560 data 🟢 16 seed / 3 leech 4h ✅ pilimi-zlib-5380000-5449999.torrentmagnet 2022-07-07 103.4GB / 23646 data 🟢 14 seed / 16 leech 5h ✅ pilimi-zlib-5450000-5479999.torrentmagnet 2022-07-07 116.7GB / 18825 data 🟢 18 seed / 3 leech 4h ✅ pilimi-zlib-5480000-5499999.torrentmagnet 2022-07-07 91.5GB / 8479 data 🟢 19 seed / 7 leech 4h ✅ pilimi-zlib-5500000-5519999.torrentmagnet 2022-07-07 141.2GB / 14914 data 🟢 14 seed / 7 leech 5h ✅ pilimi-zlib-5520000-5549999.torrentmagnet 2022-07-07 92.3GB / 20208 data 🟡 10 seed / 7 leech 5h ✅ pilimi-zlib-5550000-5579999.torrentmagnet 2022-07-07 87.7GB / 13451 data 🟢 14 seed / 7 leech 4h ✅ pilimi-zlib-5580000-5609999.torrentmagnet 2022-07-07 88.9GB / 17381 data 🟢 14 seed / 6 leech 4h ✅ pilimi-zlib-5610000-5639999.torrentmagnet 2022-07-07 95.9GB / 19376 data 🟢 15 seed / 6 leech 4h ✅ pilimi-zlib-5640000-5659999.torrentmagnet 2022-07-07 97.9GB / 13261 data 🟢 16 seed / 3 leech 5h ✅ pilimi-zlib-5660000-5709999.torrentmagnet 2022-07-07 84.7GB / 17515 data 🟢 16 seed / 4 leech 5h ✅ pilimi-zlib-5710000-5729999.torrentmagnet 2022-07-07 107.7GB / 18809 data 🟢 14 seed / 5 leech 3h ✅ pilimi-zlib-5730000-5749999.torrentmagnet 2022-07-07 151.5GB / 15593 data 🟢 13 seed / 3 leech 4h full list for “zlib” (277 torrents) External Collections § These torrents are managed and released by others. We include these torrents in order to present a unified list of everything you need to mirror Anna’s Archive. libgen_li_comics 113.3TB / 2408 torrents § Comics collection from Libgen.li. Note that some ranges are omitted since they only contain deleted or repacked files. full list / dataset / original / ipdl.cat ✅ c_0.torrentmagnet 2024-04-05 32.3GB / 989 data 🟡 9 seed / 0 leech 4h ✅ c_1000.torrentmagnet 2024-04-05 37.2GB / 992 data 🟡 5 seed / 0 leech 4h ✅ c_2000.torrentmagnet 2024-04-05 32.3GB / 995 data 🟡 8 seed / 0 leech 5h ✅ c_3000.torrentmagnet 2024-04-05 42.7GB / 996 data 🟡 7 seed / 0 leech 4h ✅ c_4000.torrentmagnet 2024-04-05 37.7GB / 987 data 🟡 9 seed / 0 leech 5h ✅ c_5000.torrentmagnet 2024-04-05 35.6GB / 997 data 🟡 8 seed / 0 leech 5h ✅ c_6000.torrentmagnet 2024-04-05 40.1GB / 993 data 🟡 6 seed / 0 leech 4h ✅ c_7000.torrentmagnet 2024-04-05 34.7GB / 989 data 🟡 5 seed / 0 leech 4h ✅ c_8000.torrentmagnet 2024-04-05 21.6GB / 947 data 🟡 5 seed / 0 leech 5h ✅ c_9000.torrentmagnet 2024-04-05 26.6GB / 957 data 🟡 6 seed / 0 leech 5h ✅ c_10000.torrentmagnet 2024-04-05 15.4GB / 874 data 🟡 5 seed / 0 leech 5h ✅ c_11000.torrentmagnet 2024-04-05 17.1GB / 958 data 🟡 5 seed / 0 leech 4h ✅ c_12000.torrentmagnet 2024-04-05 10.9GB / 995 data 🟡 6 seed / 0 leech 5h ✅ c_13000.torrentmagnet 2024-04-05 30.5GB / 983 data 🟡 5 seed / 0 leech 5h ✅ c_14000.torrentmagnet 2024-04-05 29.4GB / 987 data 🟡 8 seed / 0 leech 4h ✅ c_15000.torrentmagnet 2024-04-05 33.3GB / 965 data 🟢 11 seed / 1 leech 5h ✅ c_16000.torrentmagnet 2024-04-05 31.4GB / 942 data 🟡 5 seed / 0 leech 4h ✅ c_17000.torrentmagnet 2024-04-05 24.4GB / 959 data 🟡 6 seed / 0 leech 4h ✅ c_18000.torrentmagnet 2024-04-05 34.8GB / 965 data 🟡 5 seed / 0 leech 3h ✅ c_19000.torrentmagnet 2024-04-05 22.8GB / 905 data 🟡 5 seed / 0 leech 4h full list for “libgen_li_comics” (2408 torrents) libgen_li_fic 1.9TB / 1262 torrents § Fiction book collection from Libgen.li, from the point of divergence from Libgen.rs. full list / dataset / original ✅ f_2201000.torrentmagnet 2023-12-22 1.2GB / 1000 data 🟡 6 seed / 0 leech 4h ✅ f_2202000.torrentmagnet 2023-12-22 1.6GB / 1000 data 🟡 5 seed / 0 leech 5h ✅ f_2203000.torrentmagnet 2023-12-22 1.3GB / 1000 data 🟡 7 seed / 0 leech 4h ✅ f_2204000.torrentmagnet 2023-12-22 985.4MB / 1000 data 🟡 7 seed / 0 leech 4h ✅ f_2205000.torrentmagnet 2023-12-22 1.1GB / 1000 data 🟡 5 seed / 0 leech 5h ✅ f_2206000.torrentmagnet 2023-12-22 6.4GB / 1000 data 🟡 6 seed / 3 leech 3h ✅ f_2207000.torrentmagnet 2023-12-22 2.7GB / 1000 data 🟡 6 seed / 0 leech 3h ✅ f_2208000.torrentmagnet 2023-12-22 3.7GB / 1000 data 🟡 5 seed / 0 leech 5h ✅ f_2209000.torrentmagnet 2023-12-22 5.2GB / 1014 data 🟡 5 seed / 2 leech 4h ✅ f_2210000.torrentmagnet 2023-12-22 5.0GB / 1000 data 🟡 5 seed / 0 leech 4h ✅ f_2211000.torrentmagnet 2023-12-22 4.4GB / 1000 data 🟡 5 seed / 0 leech 5h ✅ f_2212000.torrentmagnet 2023-12-22 2.8GB / 1000 data 🟡 5 seed / 0 leech 3h ✅ f_2213000.torrentmagnet 2023-12-22 1.8GB / 1000 data 🟡 5 seed / 0 leech 4h ✅ f_2214000.torrentmagnet 2023-12-22 1.9GB / 1000 data 🟡 5 seed / 0 leech 3h ✅ f_2215000.torrentmagnet 2023-12-22 1.4GB / 1000 data 🟡 5 seed / 0 leech 4h ✅ f_2216000.torrentmagnet 2023-12-22 2.1GB / 1000 data 🟡 7 seed / 0 leech 5h ✅ f_2217000.torrentmagnet 2023-12-22 1.5GB / 1000 data 🟡 5 seed / 0 leech 4h ✅ f_2218000.torrentmagnet 2023-12-22 1.7GB / 1000 data 🟡 6 seed / 0 leech 3h ✅ f_2219000.torrentmagnet 2023-12-22 1.5GB / 1000 data 🟡 5 seed / 0 leech 5h ✅ f_2220000.torrentmagnet 2023-12-22 1.1GB / 1000 data 🟡 8 seed / 0 leech 5h full list for “libgen_li_fic” (1262 torrents) libgen_li_magazines 46.0TB / 1358 torrents § Magazines collection from Libgen.li. full list / dataset / original / ipdl.cat ✅ m_0.torrentmagnet 2024-05-25 37.2GB / 998 data 🟡 5 seed / 0 leech 4h ✅ m_1000.torrentmagnet 2024-05-25 8.8GB / 999 data 🟡 4 seed / 0 leech 5h ✅ m_2000.torrentmagnet 2024-05-25 28.5GB / 1000 data 🔴 3 seed / 0 leech 5h ✅ m_3000.torrentmagnet 2024-05-25 21.3GB / 657 data 🟡 4 seed / 0 leech 5h ✅ m_4000.torrentmagnet 2024-05-25 4.7GB / 139 data 🔴 3 seed / 0 leech 4h ✅ m_6000.torrentmagnet 2024-05-25 36.2GB / 903 data 🔴 3 seed / 0 leech 5h ✅ m_7000.torrentmagnet 2024-05-25 19.1GB / 995 data 🔴 3 seed / 0 leech 5h ✅ m_8000.torrentmagnet 2024-05-25 54.6GB / 829 data 🔴 2 seed / 0 leech 4h ✅ m_9000.torrentmagnet 2024-05-25 24.4GB / 1000 data 🔴 2 seed / 0 leech 3h ✅ m_10000.torrentmagnet 2024-05-25 41.2GB / 798 data 🔴 2 seed / 0 leech 5h ✅ m_12000.torrentmagnet 2024-05-25 4.7GB / 129 data 🟡 4 seed / 0 leech 5h ✅ m_13000.torrentmagnet 2024-05-25 15.7GB / 262 data 🔴 2 seed / 0 leech 3h ✅ m_14000.torrentmagnet 2024-05-25 32.9GB / 968 data 🔴 2 seed / 0 leech 3h ✅ m_15000.torrentmagnet 2024-05-25 69.1GB / 997 data 🔴 3 seed / 0 leech 4h ✅ m_16000.torrentmagnet 2024-05-25 17.4GB / 695 data 🟡 5 seed / 0 leech 5h ✅ m_17000.torrentmagnet 2024-05-25 11.1GB / 331 data 🔴 3 seed / 0 leech 4h ✅ m_18000.torrentmagnet 2024-05-25 34.1GB / 970 data 🔴 3 seed / 0 leech 5h ✅ m_19000.torrentmagnet 2024-05-25 17.5GB / 774 data 🔴 3 seed / 0 leech 4h ✅ m_20000.torrentmagnet 2024-05-25 11.8GB / 356 data 🔴 3 seed / 0 leech 4h ✅ m_21000.torrentmagnet 2024-05-25 8.0GB / 658 data 🔴 3 seed / 0 leech 3h full list for “libgen_li_magazines” (1358 torrents) libgen_rs_fic 4.6TB / 2932 torrents § Fiction book collection from Libgen.rs. full list / dataset / original / new additions / ipdl.cat ✅ f_0.torrentmagnet 2023-12-21 730.5MB / 999 data 🟢 14 seed / 1 leech 5h ✅ f_1000.torrentmagnet 2023-12-21 559.4MB / 1000 data 🟢 15 seed / 1 leech 5h ✅ f_2000.torrentmagnet 2023-12-21 637.9MB / 1000 data 🟢 11 seed / 0 leech 4h ✅ f_3000.torrentmagnet 2023-12-21 444.4MB / 1000 data 🟡 9 seed / 0 leech 3h ✅ f_4000.torrentmagnet 2023-12-21 492.7MB / 1000 data 🟢 14 seed / 0 leech 5h ✅ f_5000.torrentmagnet 2023-12-21 599.1MB / 1000 data 🟢 11 seed / 4 leech 5h ✅ f_6000.torrentmagnet 2023-12-21 751.3MB / 1000 data 🟢 11 seed / 0 leech 4h ✅ f_7000.torrentmagnet 2023-12-21 482.0MB / 1000 data 🟢 13 seed / 0 leech 4h ✅ f_8000.torrentmagnet 2023-12-21 554.4MB / 1000 data 🟢 11 seed / 1 leech 4h ✅ f_9000.torrentmagnet 2023-12-21 581.7MB / 1000 data 🟡 10 seed / 0 leech 4h ✅ f_10000.torrentmagnet 2023-12-21 502.0MB / 1000 data 🟢 11 seed / 0 leech 4h ✅ f_11000.torrentmagnet 2023-12-21 470.6MB / 1000 data 🟡 9 seed / 0 leech 3h ✅ f_12000.torrentmagnet 2023-12-21 600.9MB / 1000 data 🟡 9 seed / 0 leech 5h ✅ f_13000.torrentmagnet 2023-12-21 653.8MB / 1000 data 🟢 11 seed / 0 leech 5h ✅ f_14000.torrentmagnet 2023-12-21 565.9MB / 1000 data 🟡 9 seed / 0 leech 5h ✅ f_15000.torrentmagnet 2023-12-21 689.9MB / 1000 data 🟡 9 seed / 0 leech 4h ✅ f_16000.torrentmagnet 2023-12-21 604.1MB / 1000 data 🟡 10 seed / 0 leech 5h ✅ f_17000.torrentmagnet 2023-12-21 647.1MB / 1000 data 🟡 10 seed / 0 leech 4h ✅ f_18000.torrentmagnet 2023-12-21 486.1MB / 1000 data 🟡 9 seed / 0 leech 5h ✅ f_19000.torrentmagnet 2023-12-21 602.4MB / 1000 data 🟡 10 seed / 0 leech 4h full list for “libgen_rs_fic” (2932 torrents) libgen_rs_non_fic 74.8TB / 4263 torrents § Non-fiction book collection from Libgen.rs. full list / dataset / original / new additions / ipdl.cat ✅ r_000.torrentmagnet 2023-12-21 6.9GB / 999 data 🟢 71 seed / 9 leech 5h ✅ r_1000.torrentmagnet 2023-12-21 5.1GB / 1000 data 🟢 20 seed / 1 leech 5h ✅ r_2000.torrentmagnet 2023-12-21 7.2GB / 1000 data 🟢 14 seed / 0 leech 4h ✅ r_3000.torrentmagnet 2023-12-21 4.0GB / 1000 data 🟢 12 seed / 1 leech 4h ✅ r_4000.torrentmagnet 2023-12-21 3.4GB / 1000 data 🟢 15 seed / 0 leech 5h ✅ r_5000.torrentmagnet 2023-12-21 3.5GB / 1000 data 🟢 17 seed / 6 leech 4h ✅ r_6000.torrentmagnet 2023-12-21 3.8GB / 1000 data 🟢 13 seed / 1 leech 5h ✅ r_7000.torrentmagnet 2023-12-21 3.4GB / 1000 data 🟢 11 seed / 1 leech 5h ✅ r_8000.torrentmagnet 2023-12-21 4.4GB / 1000 data 🟢 11 seed / 1 leech 4h ✅ r_9000.torrentmagnet 2023-12-21 3.8GB / 1000 data 🟢 11 seed / 1 leech 5h ✅ r_10000.torrentmagnet 2023-12-21 3.6GB / 1000 data 🟢 14 seed / 0 leech 5h ✅ r_11000.torrentmagnet 2023-12-21 2.8GB / 1000 data 🟡 10 seed / 0 leech 5h ✅ r_12000.torrentmagnet 2023-12-21 4.4GB / 1000 data 🟢 11 seed / 0 leech 5h ✅ r_13000.torrentmagnet 2023-12-21 5.2GB / 1000 data 🟡 10 seed / 0 leech 4h ✅ r_14000.torrentmagnet 2023-12-21 5.9GB / 1000 data 🟢 11 seed / 1 leech 3h ✅ r_15000.torrentmagnet 2023-12-21 4.8GB / 1000 data 🟡 10 seed / 0 leech 4h ✅ r_16000.torrentmagnet 2023-12-21 3.1GB / 1000 data 🟡 10 seed / 0 leech 5h ✅ r_17000.torrentmagnet 2023-12-21 4.5GB / 1000 data 🟡 10 seed / 0 leech 4h ✅ r_18000.torrentmagnet 2023-12-21 3.3GB / 1000 data 🟡 10 seed / 0 leech 3h ✅ r_19000.torrentmagnet 2023-12-21 1.9GB / 1000 data 🟡 10 seed / 1 leech 5h full list for “libgen_rs_non_fic” (4263 torrents) scihub 89.6TB / 876 torrents § Sci-Hub / Libgen.rs “scimag” collection of academic papers. Currently not directly seeded by Anna’s Archive, but we keep a backup in extracted form. Note that the “smarch” torrents are deprecated and therefore not included in our list. full list / dataset / original — sm_00000000-00099999.torrentmagnet 2023-12-21 42.8GB / 100 data 🟢 27 seed / 1 leech 4h — sm_00100000-00199999.torrentmagnet 2023-12-21 52.3GB / 100 data 🟢 12 seed / 4 leech 5h — sm_00200000-00299999.torrentmagnet 2023-12-21 77.1GB / 100 data 🟡 9 seed / 1 leech 4h — sm_00300000-00399999.torrentmagnet 2023-12-21 56.9GB / 100 data 🟡 8 seed / 2 leech 5h — sm_00400000-00499999.torrentmagnet 2023-12-21 42.0GB / 100 data 🟡 8 seed / 2 leech 4h — sm_00500000-00599999.torrentmagnet 2023-12-21 49.0GB / 100 data 🟡 7 seed / 1 leech 4h — sm_00600000-00699999.torrentmagnet 2023-12-21 34.7GB / 100 data 🟡 6 seed / 2 leech 4h — sm_00700000-00799999.torrentmagnet 2023-12-21 38.7GB / 100 data 🟡 8 seed / 1 leech 4h — sm_00800000-00899999.torrentmagnet 2023-12-21 27.8GB / 100 data 🟡 7 seed / 1 leech 4h — sm_00900000-00999999.torrentmagnet 2023-12-21 4.6GB / 100 data 🟡 9 seed / 33 leech 4h — sm_01000000-01099999.torrentmagnet 2023-12-21 5.8GB / 100 data 🟡 10 seed / 1 leech 3h — sm_01100000-01199999.torrentmagnet 2023-12-21 76.1GB / 100 data 🟡 8 seed / 1 leech 5h — sm_01200000-01299999.torrentmagnet 2023-12-21 52.7GB / 100 data 🟡 8 seed / 1 leech 4h — sm_01300000-01399999.torrentmagnet 2023-12-21 51.8GB / 100 data 🟡 7 seed / 1 leech 4h — sm_01400000-01499999.torrentmagnet 2023-12-21 65.0GB / 100 data 🟡 7 seed / 3 leech 4h — sm_01500000-01599999.torrentmagnet 2023-12-21 58.7GB / 100 data 🟡 6 seed / 1 leech 5h — sm_01600000-01699999.torrentmagnet 2023-12-21 44.3GB / 100 data 🟡 7 seed / 1 leech 4h — sm_01700000-01799999.torrentmagnet 2023-12-21 50.7GB / 100 data 🟡 6 seed / 4 leech 4h — sm_01800000-01899999.torrentmagnet 2023-12-21 47.0GB / 100 data 🟡 8 seed / 1 leech 4h — sm_01900000-01999999.torrentmagnet 2023-12-21 60.2GB / 100 data 🟡 7 seed / 2 leech 4h full list for “scihub” (876 torrents) Other Torrents by Anna’s Archive § These are miscellaneous torrents which are not critical to seed, but contain useful data for certain use cases. These torrents are not included in the seeding stats or torrent list generator. aa_derived_mirror_metadata 344.0GB / 1 torrent § Our raw metadata database (ElasticSearch and MariaDB), published occasionally to make it easier to set up mirrors. All this data can be generated from scratch using our open source code, but this can take a while. At this time you do still need to run the AAC-related scripts. These files have been created using the data-imports/scripts/dump_*.sh scripts in our codebase. We welcome contributions for the corresponding loading scripts. Documentation for the ElasticSearch records can be found inline in our example JSON. ✅ aa_derived_mirror_metadata_20240612.torrentmagnet 2024-06-12 344.0GB / 371 data 🔴 1 seed / 5 leech 4h full list for “aa_derived_mirror_metadata” (1 torrent)",
    "commentLink": "https://news.ycombinator.com/item?id=40672215",
    "commentBody": "You can help Anna's Archive by seeding torrents (annas-archive.org)136 points by FabHK 1 hour agohidepastfavorite95 comments FabHK 1 hour agoAnna’s Archive, a mirror (arguably the successor) of Library Genesis and Sci-Hub, is asking for help seeding over 500 TB of book and papers for long-term preservation. Currently, 40% of that are seeded by fewer than 4 nodes. Note: This is 30M+ books, 100M+ papers. Depending on your philosophy and jurisdiction, you might be stealing a few billion $. Or not. > By seeding these torrents, you help preserve humanity’s knowledge and culture. These torrents represent the vast majority of human knowledge that can be mirrored in bulk. reply toomuchtodo 1 hour agoparentWhat are the most favorable jurisdictions to run seeding out of, mitigating risk of copyright related reporting and account closure (assuming virtual compute)? reply esdf 1 hour agorootparentUkraine, Russia, Moldova, Romania, Vietnam, Africa. Often anywhere near those too reply derefr 1 hour agorootparentHow about favorable places given an anonymous virtual credit card that you bought on a spamming-services forum? (As in, if you don't have to worry about being identified, and it's just a question of wanting the seedbox itself to be unlikely to get taken down) reply esdf 51 minutes agorootparentA lot of english-advertising hosts from the regions in my comment are going to advertise DMCA-ignore, freedom of speech, and/or privacy. And you basically can't go wrong with any of them. Seedbox-as-a-service providers specifically are going to be generally be fine too although many of those are hosted in Netherlands which is slowly becoming less copyright-infringement -friendly. One specific host I'll mention is vsys.host (UA) (which anna's archive uses too) but they're not going to be the cheapest option. reply toomuchtodo 1 hour agorootparentprevTurkey? reply esdf 1 hour agorootparentI think Turkey would be fine but I don't know or have anything to base it off of reply ThrowawayTestr 1 hour agorootparentprevRussia reply magic_hamster 1 hour agorootparentprevProbably Sweden. reply worksonmine 0 minutes agorootparentAbsolutely not for anything large scale like this. r3trohack3r 1 hour agorootparentprevIsn't that where The Pirate Bay folks were prosecuted? https://en.wikipedia.org/wiki/The_Pirate_Bay_trial reply SCUSKU 1 hour agoparentprevIs LibGen & Sci-Hub at risk of coming down? I know it's a game of cat and mouse, but I was just under the assumption that they were both doing fine relatively speaking. And what makes Anna's Archive better/different? reply dredmorbius 39 minutes agorootparentIf nothing else, investment diversification. Z-Library was significantly attacked recently. There was a huge takedown in 2022, and I'm finding reports as of March 2024 as well: 2024: \"FBI Carries Out Fresh Round of Z-Library Domain Name Seizures\"2022: \"Z-Library operators arrested, charged with criminal copyright infringement\"The Z-Library arrests were of Russian nationals, but were arrested in Argentina. The first Torrentfreak article mentions two other actions in Spring and November 2023. Sci-Hub has played cat-and-mouse with domains for years, and AFAIU has still withheld posting new scientific articles given pending litigation/prosecution in India. Another Torrentfreak article from Nov 2023 gives an update on numerous issues with text liberation sites, including Anna's Archive, Sci-Hub, and Z-Library. At the time, the Anna's Archive twitter account had been \"wiped out\", so much for that platform's \"free speech\" stance.reply FabHK 44 minutes agorootparentprevDisclaimer: not an expert in any of this, just stumbled across that page on Anna's Archive. Sci-Hub is down (as in, you can't search/view/download papers from sci-hub.se), AFAIK. Anna's Archive aggregates into a unified database material from Libgen.rs, Sci-Hub, Libgen.li, Z-Library, Internet Archive Controlled Digital Lending, and DuXiu 读秀. (See: https://annas-archive.org/datasets ) reply dredmorbius 38 minutes agorootparentThe .se domain was seized in Jan 2023:reply jprete 1 hour agoparentprev> By seeding these torrents, you help preserve humanity’s knowledge and culture. These torrents represent the vast majority of human knowledge that can be mirrored in bulk. I am having a hard time reading that claim as anything other than a bad-faith justification. Torrent nodes are not at all a good way to \"preserve humanity's knowledge and culture\". EDIT: I'm no longer convinced I'm correct. reply FabHK 48 minutes agorootparentI only quoted a small bit from their website. Immediately below the quoted bit is this explanation: > These torrents are not meant for downloading individual books. They are meant for long-term preservation. With these torrents you can set up a full mirror of Anna’s Archive, using our source code and metadata (which can be generated or downloaded as ElasticSearch and MariaDB databases). I'm not an expert in any of this, but it doesn't strike me as a bad-faith justification of anything. reply l33t7332273 54 minutes agorootparentprevCan you elaborate on why you believe it’s in bad faith. reply r3trohack3r 1 hour agoprevInteresting. IIRC, libgen used IPFS for preservation efforts. Anna's Archive (seemingly the successor) appears to have migrated to BitTorrent. I wonder what motivated the move? Edit: asking as someone who works daily on building p2p software. We've abandoned mainline BitSwap (IPFS) in our work for similar reasons as the rest of the rust-libp2p community, but haven't found a particularly good \"successor\" protocol for a generalized use case yet. We are currently using our own ad-hoc hand-rolled chunking/transfer protocol as needed. reply tux3 1 hour agoparentNexus - another very large archive - is using IPFS. But in my experience Bittorrent works a lot better at this scale. The IPFS UX is full of papercuts, when it isn't outright bugging out or crumbling under the size of the dataset. reply ffk 54 minutes agoparentprevI'm guessing the decision comes down to ease of use for people to participate in mirroring. My underestanding is IPFS tends to require more infrastructure, and still requires someone to pin the data. Many bittorrent clients let you click a button to continue seeding the data over time. reply dtx1 1 hour agoparentprevI suspect in part it's the required capacity. This Project is far beyond what even the largest private trackers could host but if anyone comes even close to be able to keep this alive when the copyright mafia comes it's the torrent community. reply sillysaurusx 57 minutes agoprevAs a PSA, one additional reason to seed is because Anna accidentally doxxed herself via GitHub. So it’s worth preserving the archive on the basis that we should expect {the centralized portion of} it to disappear within the next couple years. I was sad to see that happen, but it’s important to be objective and plan future actions accordingly. (And sure, there’s always the chance that some random person on GitHub just so happens to be named Anna and is an archival enthusiast, but a jury of one’s peers may find that it passes the reasonable doubt threshold.) My legal troubles with books3 weighed on me pretty heavily, and I wasn’t even the target. Yet. I can only imagine what it feels like to be waiting for an indictment. There ought to be some sort of protection for preserving books in bulk. No one is going to read two million books. But of course, one could also argue that having a readily available archive is harming the economic profitability of the works, on the basis that content licensing for AI is now a multimillion industry. It’s weird, because it feels like important work, rather than criminal — someone should put into words exactly what the distinction is. reply jprete 40 minutes agoparentI think the Library of Congress already does preservation; it seems to be legally required to give the LoC two copies of every published work in the US (https://www.copyright.gov/circs/circ7d.pdf). Preservation of works is very obviously not why Anna's Archive is asking for torrent seeders. Seeding is for distribution and availability, not preservation. It would be more honest to say \"preservation of ML training fair use access\". reply FabHK 30 minutes agorootparent> Preservation of works is very obviously not why Anna's Archive is asking for torrent seeders. Can you elaborate on that? They write: \"These torrents are not meant for downloading individual books. They are meant for long-term preservation. With these torrents you can set up a full mirror of Anna’s Archive, using our source code and metadata (which can be generated or downloaded as ElasticSearch and MariaDB databases).\" reply jprete 22 minutes agorootparentI'm no longer convinced I'm correct. reply esdf 32 minutes agoparentprevThey did deny involvement so it'll be interesting to see what happens (you probably know this though) https://torrentfreak.com/key-defendant-in-annas-archive-laws... >It’s weird, because it feels like important work, rather than criminal — someone should put into words exactly what the distinction is. Important work can be criminal reply iwontberude 41 minutes agoparentprevI think you’ve stumbled upon a paradox where the data is worth preserving because of its scarcity, but once it no longer scarce, the value is diminished along with the priority to preserve and make it discoverable. Similar in some way to the antique/collectibles market and the U shaped curve. The issue is that the data can become so devalued that we create a scarcity in the future. It’s one of those self-regulating systems. reply pentagrama 46 minutes agoprevI want to help but I guess I'm not being able to, maybe someone can help me. I want to give 5GB (don't have much storage), so I put \"Max TB:0.005\" and \"Type:URLs\". It give me this url: https://annas-archive.org/dyn/generate_torrents?max_tb=0.005... Who has this two torrent files: https://annas-archive.org/dyn/small_file/torrents/external/l... https://annas-archive.org/dyn/small_file/torrents/external/l... I put that torrents on Transmission, one is 5GB and the other 4MB, the 5GB is not downloading/seeding, the 4MB was downloaded and is seeding: https://imgur.com/a/80k3y1D Any help? reply boramalper 1 hour agoprevThey also use Telegram actively for latest announcements, fyi. https://t.me/annasarchiveorg reply eh_why_not 1 hour agoprevI'm told Anna's Archive forces a javascript Cloudflare human-browser check on the visitor. Given how much data Cloudflare (and other similar giants providing this service) has, this pretty much identifies the person to them. Any plans to change that to a more privacy-protecting solution? reply FabHK 40 minutes agoparentFrom what I gather, in some jurisdictions downloading copyrighted material is legal (eg via HTTPS from a website), or at any rate not prosecuted, while uploading (offering to download) it (eg while downloading it via BitTorrent) is illegal (and prosecuted). reply eh_why_not 33 minutes agorootparentNot an expert on laws/jurisdictions; but data is collected, stored forever, and in some places is (legally) on sale. And laws get changed quickly based on lobbying. reply shanewilhelm 51 minutes agoparentprevWould their plan of keeping the entire archive in torrents alleviate that concern? Like, if a person sourced an index from somewhere privacy-centered, they could directly download from the torrents in whatever private method they wish, right? reply eh_why_not 44 minutes agorootparentI'd expect the number of users from browsers to always be orders of magnitude larger than torrent users. reply lynndotpy 48 minutes agoprevAre any of these legal to seed in the US, i.e. not in violation of copyright? E.g. Consider someone with a gigabit connection that their VPN can't keep up with. It would make sense to seed the legal items without the VPN, and the other items with the VPN. reply pskkk 1 hour agoprevI wonder if there is any benefit to using something like DwarFS (https://github.com/mhx/dwarfs) for something like this. reply hnpolicestate 1 hour agoprevI have this mentality problem where I don't like using my VPN except when I specifically need too. To permanently seed some of these torrents I'd have to keep it on all the time. Would have to keep my desktop running 24/7 too? reply simcop2387 0 minutes agoparentWhat I'd do is look at getting some small used PC (the 1 liter ones, tinyminimicro on servethehome, etc.) that doesn't take much power to do it. Cheaper to leave running 24/7 and it's easier to setup to work ONLY if the VPN is up. You can likely even setup virtual machines to do it (i.e. setup an OpenWRT virtual machine that can access the rest of the network as the \"wan\", but the \"lan\" to it is all virtual, and doesn't route traffic from the lan to the wan, only the lan to the vpn). reply squarefoot 33 minutes agoparentprev> Would have to keep my desktop running 24/7 too? You could build a seed box out of a old ARM board running the Transmission daemon and a USB key mounted read only to avoid wear; power draw would be just a few watts and total cost could be less than 50 bucks. The desktop would be needed only when adding torrents or changing configuration from its web interface, although Transmission also has remote control apps running on phones and tablets. If the router permits it, QoS rules can be set up on the router so that the seed box can use all bandwidth, although at lower priority than other machines on the LAN, so that it will never clog the network, which comes handy for example with online gaming. https://transmissionbt.com/ reply mft_ 1 hour agoparentprevI used to be there, until I discovered Mullvad, which is so fast and reliable that there’s no downside to using it permanently. The only time I ever notice is when, very rarely, a particular node is flagged by some website (most often Reddit) as problematic and I have to switch to a different one. reply soulofmischief 1 hour agoparentprev> I have this mentality problem where I don't like using my VPN except when I specifically need too. This is extremely suboptimal. \"I only hide my activity when it's worth hiding\" paints a giant target on your back. VPNs as a matter of course protect you from all manner of anti-consumer tactics, and if you don't obfuscate all of your traffic, it tips off surveilling parties to only focus on the subset of traffic that routes through a VPN. reply zamadatix 1 hour agorootparent\"I don't want to receive DMCA notices passed through my ISP because it might disrupt my service\" type use cases don't imply you also care about general privacy concerns or other use cases of VPNs too. reply soulofmischief 7 minutes agorootparentAnd I'm expressing to OP that they should, especially given the repeal of net neutrality and exposed NSA dragnet programs. Unless we just think a social credit system will magically never make its way to the US and impact every portion of our lives. Imagine getting higher health insurance because your parents browsed websites labeled as unhealthy. reply hnpolicestate 2 minutes agorootparentI agree with you. I was more or less describing a negative trait of mine for why I haven't participated in seeding information thus far. I'm definitely already on a health insurance bad persons list. My employer forwarded myself and other employees terminated over \"that thing\" to the FBI! Dutchie987 1 hour agoparentprevYou don't have to seed 24/7. It's OK as long as (parts of) torrents get seeded regularly. reply zamadatix 1 hour agoparentprevPermanently seed is more about the data being reliably available at some point in the future more than continuously seeding. As for the VPN side you should be able to configure your VPN to always tunnel your torrent app but not always tunnel your entire computer. The best/easiest way to do this varies by the specific VPN application and your OS. reply menthe 1 hour agoparentprevI don’t get it. If you do not want to participate in the preservation and distribution of the archive, why don’t you just move on instead of complaining? Besides, gluetun+chihaya+qbit containers do the job without breaking a sweat, and without ever having to remember that you run a VPN - as it’d only be tunneling the containers of your choice. gluetun is the best image ever made! reply hangonhn 1 hour agorootparentHang on. I think the OP is trying to find a middle ground between their level of comfort and contributing to the project. I think their intentions are positive; they want to help but are constrained by their other needs or priorities. reply hnpolicestate 7 minutes agorootparentprevNo I definitely want to. I was stating why I haven't thus far. I support piracy. reply DiggyJohnson 1 hour agorootparentprevThey’re not complaining they’re asking a question. I think you completely misread the tone. reply jasonjayr 1 hour agoparentprevYou don't have to keep it going 24/7. If you connect every once in a while, you will assist clients that come and go for parts they need. You don't even need to have the complete torrent either -- the tracker will know you have those parts and direct clients to you. reply davidspiess 1 hour agoparentprevIn qBitorrent or similar torrent clients you can explicitly set the vpn network interface they should use. Use split tunneling for you browser or other applications you don't want to use the vpn for. reply chadsix 1 hour agoparentprevWe made a simple way to lock QBittorrent into a VPN in a container [1]. It's probably simple enough to follow what we did in the config script to set it up for your use case (all open source [2]). [1] https://www.youtube.com/watch?v=PrH6Ci_4eig [2] https://github.com/ipv6rslimited/cloudseeder reply novagameco 1 hour agoparentprevOr take an old laptop and use as a seedbox reply carlosjobim 1 hour agoparentprevWhat's the problem? Seed when you want to seed. reply y-curious 1 hour agoprevIs there a link to the legal considerations of contributing as a seeder? reply squarefoot 50 minutes agoparentDepends on where you are located; in some countries they may not give a damn if you seed some torrents, but will happily take down your site and prosecute you if you run a torrent service. reply lyu07282 1 hour agoparentprevcopyright law would've already burnt the library of alexandria several times over, just something to consider about the validity of these laws written and bribed into law by publishers reply its_ethan 1 hour agorootparentIt's fine that you don't agree with the laws, but the guy is asking what the actual legal impact may be if he participates. Answering with \"who cares, the laws are dumb\" isn't helpful to him... people get arrested and charged against dumb laws all the time, and it sounds like he'd like to avoid that. edit: this is maybe the link he's after? looks like you need to be logged in to see it though. https://annas-archive.org/copyright reply lyu07282 58 minutes agorootparentso what if I didn't answer their question? its called a comment for a reason. (also: its \"them\", don't assume their gender like that, come on its 2024) reply yjftsjthsd-h 1 hour agorootparentprevHow so? We literally have libraries today, and copyright law hasn't burned them down. reply belorn 34 minutes agorootparentThere are a few reasons for that. The primary being that sharing a physical book do not count as copying, but sharing a digital book does. The second reason is that libraries tend to operate under government control, and governments has done things to enable libraries and work around copyright law. An old one that my country (used to?) have was to require publishers to send copies to the national library. In return, national authors got a symbolic sum (very tiny) each time a copy was taken out. Being forced to send a copy to the government isn't technically against copyright law, since no unlawful copying is being made, but the result has a very similar feeling as unlawful copying. reply yjftsjthsd-h 21 minutes agorootparentSure, but if anything that just reinforces how ridiculous it is to claim that \"copyright law would've already burnt the library of alexandria several times over\" - the LoA was very much backed by government power - https://en.wikipedia.org/wiki/Library_of_Alexandria#Early_ex... describes not merely going out and buying anything they could get their hands on, but outright using government mandate to seize and copy new books that passed through port. reply gizajob 1 hour agorootparentprevWhat does that mean? The library of Alexandria was happy to burn down all on its own, centuries before copyright. reply vsuperpower2020 1 hour agorootparentHe's not saying that copyright literally burned down the library of Alexandria. reply F00Fbug 1 hour agoprevWouldn't this be a good application for IPFS? reply treyd 1 hour agoparentIt's already on BitTorrent. IPFS doesn't do much BitTorrent doesn't already, most of it is a new coat of paint and making the same mistakes BitTorrent figured out years ago. reply derefr 1 hour agorootparentIt does one thing BitTorrent doesn't — you can compose a new CAR file by combining a few new chunks with a bunch of existing chunks. So you don't get the problem where releasing a new version of an archive means nobody's seeding it; and anyone moving over to seeding the new version stops seeding the old version. Instead, the new file is already pre-seeded by all the old version's seeders on all but the new chunks (because they're seeding the chunks, not the file); and the old file stays seeded as the seeders find the new version and seed its blocks too. Really, BitTorrent could do this by making all torrent files a small fixed size and then having \"torrent files of a directory of torrent files\" where the torrent client knows to queue the sub-torrents as they're discovered+downloaded in the parent torrent. But that's not how any part of the ecosystem works. IPFS is a \"do over\" that allowed them to fix this. reply tux3 58 minutes agorootparent>releasing a new version of an archive means nobody's seeding it; and anyone moving over to seeding the new version stops seeding the old version BitTorrent v2 would in theory be able to seed individual files even if they come from a different torrent. But clients have no reasonable way to look for other versions of a torrent that contain a file they already have. The main Bittorrent clients already support creating and seeding v2 torrent. But there's just no infrastructure for seeding at the individual file level. reply kevincox 1 hour agorootparentprevOne major benefit of IPFS is that people seeding individual works and people seeding the large archive groups can share data. It seems that these torrents are blocks of data that aren't of direct use. That being said while the IPFS protocol is decent the implementations kind of suck. Bittorrent is well established with many high quality implementations. reply parentheses 1 hour agoprevSeeding 500TB I'm guessing requires 500TB of disk space. I don't have even close to that. I would if I could though, so excited to see anyone answer the call! reply Shank 1 hour agoparentIt seems you can specify the storage space you want to devote, and it will suggest torrents to match. For example, duxiu is only ~40gb. This is separate from torrent-client limiting -- you can just seed specific parts of the collection if you want. reply wongarsu 1 hour agoparentprevThe individual torrents are only a couple GB to a couple TB each. You can automatically generate a list of torrents of how ever many terabytes you are willing to seed 1: https://annas-archive.org/torrents#generate_torrent_list reply novagameco 1 hour agoparentprevYou don't have to download all files in the torrent. Most torrent clients support partial downloads and you would only seed those parts. The main concern would be that you would be undoubtedly distributing copyrighted material reply optimus_banana 1 hour agoparentprevMost torrent clients let you select subsets of file to download (and thus seed) so you can choose a portion to download whatever amount of disk space you are willing to lend. reply Dutchie987 1 hour agoparentprev100 people sharing a shard of 5TB each is even better (more resilient). reply dantillberg 1 hour agoparentprevIt's possible to download/seed just a fraction of a torrent. reply renewiltord 1 hour agoprevOkay so is there somewhere on the Internet that I can get a VPS with crypto? Because I'm not going to use traditional payment methods to access this. EDIT: Thank you to everyone for your recommendations. I shall find an anon seedbox. I don't mind if it's nuked. I imagine I'll just pay per month and losing one month's spend won't hurt. reply Tiberium 1 hour agoparentA couple lists: https://kycnot.me/?t=service (not just hostings) https://bitcoin-vps.com/ (an extensive list for ones that accept BTC, most accept other coins too) There are hundreds of VPS hosters that accept crypto, but the important part is that a lot of them are not happy about abuse reports either way, so you'll probably have to use a VPN (like Mullvad) on the VPS itself to not get it suspended :) reply soulofmischief 1 hour agoparentprevI highly recommend https://mullvad.net/ reply skrebbel 1 hour agorootparentI don’t think they sell VPSes though reply renewiltord 1 hour agorootparentprevSorry, maybe I wasn't clear. I'm not storing this data locally. I want to seed it from the Internet without a connection to me except for SSH. Essentially, how can I get a seedbox with crypto. Though it does strike me now that any such service will be primarily used by criminals. reply throwie1223456 1 hour agorootparent>Though it does strike me now that any such service will be primarily used by criminals. When tyranny becomes law, rebellion becomes duty. reply realce 1 hour agorootparentprev>Though it does strike me now that any such service will be primarily used by criminals. Just think of them as \"anti-state actors\" Getting a small VPS with crypto and tunneling from your home seems like a better pathway to me? reply agent86 1 hour agorootparentprevIs it even possible to do this effectively using Mullvad after they removed support for forwarded ports? [1] https://news.ycombinator.com/item?id=36113215 reply banana_giraffe 1 hour agoparentprevThe service you're after, I think, is a Seedbox. Feral Hosting is once such service that accepts payment via crypto. They're not a VPS in the traditional sense, but they give you a slice of a server, with a torrent client pre-installed. reply ptero 1 hour agoparentprevMullvad will even let you pay with cash (stuff dollars into an envelope). reply biftek 49 minutes agoparentprevYou're looking for a \"seedbox\" which is essentially the torrenting equivalent of a VPS reply tux3 43 minutes agoparentprevNjalla has VPS and VPNs. I'm a happy VPN customer, they've been excellent. reply zepolen 1 hour agoparentprevThat is the mentality that keeps the world down. Perhaps Great Depression 2.0 will set our priorities straight. reply dtx1 1 hour agoparentprevThere are some server hosters that you can buy with a locally bought paysafe card. They mostly cater to young gamers that want to host private servers but don't necessarily have access to the banking system yet. They will however very likely nuke your server the second they get a dcma notice. So you need a vpn setup on that server too. Several providers offer bitcoin payment, with mullvad you can buy scratch-off tickets on amazon, which would be the most anonymous option. Of course setup has to be done on a public wifi and requires some custom setup. Alternatively, there's /r/seedboxes over on reddit where most vendors accept bitcoin and get you a complete setup. reply jimsimmons 1 hour agoprev [–] Why is everyone acting like this is some radio active data. If you don't distribute further, it should be pretty ok to download and play with this? reply dumbobaggins999 44 minutes agoparentKindly recall the predicament of Aaron Schwartz that led us to this point. https://en.wikipedia.org/wiki/United_States_v._Swartz reply FabHK 35 minutes agoparentprev [–] If you download a torrent, your client will offer to upload pieces for other clients to download. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Anna’s Archive, Library Genesis, and Sci-Hub have created a unified torrent list to preserve a vast amount of human knowledge, totaling 521.1TB, with 60% already copied in at least four locations.",
      "Users are encouraged to seed torrents, especially those with low seeders, to help reach 100% preservation. Large contributors (50TB or more) can contact the team for updates on deprecated torrents.",
      "The list is divided into three parts: managed by Anna’s Archive, managed by others like Library Genesis and Sci-Hub, and miscellaneous torrents. Some torrents may be temporarily embargoed."
    ],
    "commentSummary": [
      "Anna's Archive, a mirror of Library Genesis and Sci-Hub, seeks help seeding over 500 TB of books and papers for preservation, with 40% currently seeded by fewer than 4 nodes.",
      "The project faces significant copyright infringement challenges, similar to other online libraries like LibGen, Sci-Hub, and Z-Library, which have faced legal actions and domain seizures.",
      "The shift from IPFS to BitTorrent for preservation is due to BitTorrent's ease of use and lower infrastructure requirements, with the community debating the ethical and legal implications of torrent seeding for long-term data preservation."
    ],
    "points": 136,
    "commentCount": 95,
    "retryCount": 0,
    "time": 1718299214
  },
  {
    "id": 40670898,
    "title": "AMD CEO Lisa Su reminisces about designing the PS3's infamous Cell processor",
    "originLink": "https://www.tomshardware.com/tech-industry/amd-ceo-lisa-su-reminisces-on-helping-design-the-ps3s-infamous-cell-processor-at-ibm",
    "originBody": "Tech Industry AMD CEO Lisa Su reminisces about designing the PS3's infamous Cell processor during her time at IBM News By Christopher Harper published 13 June 2024 Even before she worked at AMD, Lisa Su was making big moves and proving to be one of PlayStation's greatest allies Comments (8) Shot of the Cell CPU inside of a disassembled Sony PlayStation 3. (Image credit: Greenpro on WikiMedia Commons) Just after Computex 2024, AMD CEO Lisa Su sat down with Stratechery to conduct an extended interview about solving hard problems throughout her career— including her time at IBM and contributing to the legacy of PlayStation from both there and AMD afterward. As she notes, \"I've been working on PlayStation for a long time, if you think about it. PlayStation 3, 4, 5...[like the common thread] across multiple companies, yes.\" Now, even if you're familiar with the PlayStation 3 and its nature as being difficult to program thanks to its IBM PowerPC-based Cell processor, you most likely didn't know Lisa Su had any involvement with it before this week. Details are few and far between, but we did manage to find the earliest statement where pre-AMD CEO Lisa Su commented on the matter. According to Lisa Su, then-Director of Emerging Products at IBM in 2001, \"We [IBM] started with a clean sheet of paper and sat down and tried to imagine what sort of processor we'd need five years from now.\" The decision that IBM, Sony, and Toshiba made was to create a CPU with an extreme focus on parallelization. Today, that approach is fairly common through multicore CPUs, Simultaneous Multi-Threading (SMT, or Hyper-threading under Intel marketing), and even dedicated Efficiency cores, but SMT wouldn't emerge until 2002, and the first consumer multicore CPUs from AMD and Intel wouldn't be seen until 2005. And, of course, the first-ever multicore was released by IBM for workstation and server use in 2001— the same year they were planning the PS3's Cell processor. The interviewer points out that Sony's PlayStation 3 is viewed as one of its least successful consoles, which is true. The PlayStation 3 pretty much lost the generation handily to Nintendo's cheap, casual-friendly Wii and Microsoft's less powerful but easier Xbox 360. The complexity of the architecture meant that cross-platform games didn't always perform as well as they should on PS3, though as developers (particularly first-party devs) mastered the hardware, it did result in the most visually stunning console games of the latter half of the generation being Sony exclusives, like Uncharted 3 and its ilk. Lisa said, \"The Cell processor was extremely ambitious at that time, thinking about the type of parallelism it was trying to get out there. Again, I would say, from a business standpoint, it was certainly successful. As you rank things, I think history will tell you that there may be different rankings.\" \"My perspective is, the console era has gone through phases [...] but once you went to HD, you had tremendous increase in cost of asset creation, you had developers heavily motivated to support multiple processors, you had game engines coming along. Suddenly, no one wanted to go to the burden of differentiating on the Cell; they just wanted to run on the Cell,\" Su explained. Stay On the Cutting Edge: Get the Tom's Hardware Newsletter Get Tom's Hardware's best news and in-depth reviews, straight to your inbox. Contact me with news and offers from other Future brands Receive email from us on behalf of our trusted partners or sponsors By submitting your information you agree to the Terms & Conditions and Privacy Policy and are aged 16 or over. Interestingly, the seventh generation of home consoles (PlayStation 3, Xbox 360, Nintendo Wii/Wii U) also marks a shift in AMD's allegiances to console manufacturers. AMD produced graphics chips for both Nintendo and Xbox, and all three console manufacturers used the PowerPC CPU architecture. But come to the eighth-gen (PS4, XB1, Switch), both Xbox and PlayStation had switched fully to AMD-powered x86 CPU and GPU architecture. The Switch also saw Nintendo pivot to an Nvidia-powered SoC design (with Arm CPU cores) for their new hybrid console focus. With the added context of this interview, one can't help but wonder if Lisa Su's unifying thread throughout the last three generations of PlayStation hardware isn't a coincidence. It could just be corporate happenstance, but going from a humble Product Director and Engineer working on the PS3 at IBM to Senior vice president (in 2012, CEO in 2014) at AMD, setting the future course of both PlayStation and Xbox hardware, is truly impressive. It's also a great win for AMD, in general, to provide the hardware behind the two biggest consoles on the market for two consecutive (and a third upcoming) console generations. No matter who wins between Sony and Microsoft and their console war, AMD wins, and that's the kind of thinking that earns you a CEO spot. Christopher Harper Freelance News Writer MORE ABOUT TECH INDUSTRY Sanctions-defying chip design tools that work on China chipmakers' Huawei and Phytium processors introduced by China-based firm Samsung's new roadmap unveils its 2nm process nodes and outlines backside power delivery plans LATEST Microsoft dev shares story behind iconic Windows screensavers — 3D Pipes, 3D Maze, and more SEE MORE LATEST ► SEE ALL COMMENTS (8) 8 Comments Comment from the forums bit_user Cell was pretty amazing, for its time. IMO, the biggest problem it faced was a dearth of good programming frameworks for using it effectively. If OpenCL had existed, at the time, that probably would've helped immensely. In fact, I'm not the first to think this. IIRC, George Hotz (of recent notoriety involving his Tiny.AI startup) was experimenting with OpenCL on the PS3, when Sony withdrew OtherOS support and disrupted his efforts. If I'm not mistaken, that's what sent him down the spiral of legal troubles with Sony. Lisa Su said: (in 2001) We started with a clean sheet of paper and sat down and tried to imagine what sort of processor we'd need five years from now.\" The decision that IBM, Sony, and Toshiba made was to create a CPU with an extreme focus on parallelization. A key detail to remember is that GPU compute was in the earliest days of its infancy. GPUs were still incredibly specialized towards graphics and trying to use them for anything else was very hard and subject to heavy constraints. So, I'd say they were spot-on in identifying the trend, but just off the mark about which element of the system was best-suited to address the needs. The article said: The interviewer points out that Sony's PlayStation 3 is viewed as one of its least successful consoles, which is true. Um... according to wikipedia: Worldwide sales: 87.4 million (as of March 31, 2017)Okay, so the PS4 has (also, according to them) now sold \"106 million (as of December 31, 2019)\", but I'd argue the Total Addressable Market (TAM) for consoles grew by more than that amount, between when the PS3 was Sony's leading console and when the PS4 was. So, if we grade on a curve, I'd say the PS4 was probably the loser. The article said: The Switch also saw Nintendo pivot to a fully Nvidia-powered SoC design for their new hybrid console focus. The Nintendo Switch's CPU cores were designed by ARM. Nvidia made the SoC and iGPU. Reply jeremyj_83 bit_user said: The Nintendo Switch's CPU cores were designed by ARM. Nvidia made the SoC and iGPU. Apparently AMD made a play to be the CPU/GPU in the Switch 2 but Nintendo went with Tegra again. Reply TheyCallMeContra bit_user said: Worldwide sales: 87.4 million (as of March 31, 2017)Okay, so the PS4 has (also, according to them) now sold \"106 million (as of December 31, 2019)\", but I'd argue the Total Addressable Market (TAM) for consoles grew by more than that amount, between when the PS3 was Sony's leading console and when the PS4 was. So, if we grade on a curve, I'd say the PS4 was probably the loser. Well, do keep in mind that the PS4 dramatically outperformed the Xbox One despite 360's lead in the previous generation. If we only look at console sales within the scope of the manufacturer alone, Nintendo and Sony definitely peaked with Nintendo DS and PlayStation 2, respectively. Also, good catch on Arm involvement! Will submit an edit for that one soon. Reply jeremyj_83 TheyCallMeContra said: Well, do keep in mind that the PS4 dramatically outperformed the Xbox One despite 360's lead in the previous generation. If we only look at console sales within the scope of the manufacturer alone, Nintendo and Sony definitely peaked with Nintendo DS and PlayStation 2, respectively. Also, good catch on Arm involvement! Will submit an edit for that one soon. The Nintendo Switch has sold over 140M units. It is within a Wii U of sales of tying the DS for total sales with probably 12 months before its successor is released. Very hard to say that they peaked with the DS. Reply TheyCallMeContra jeremyj_83 said: The Nintendo Switch has sold over 140M units. It is within a Wii U of sales of tying the DS for total sales with probably 12 months before its successor is released. Very hard to say that they peaked with the DS. \"Within a Wii U of tying with DS\". Being an entire console's install base away from being equivalent to Nintendo's current peak? Objectively speaking, they did peak with Nintendo DS. You do understand the literal meaning of the word \"peak\", yes? You can hardly forecast in good faith that the current Switch will surpass NDS when the existence of Switch 2 is widely-known and an entire competing market of Deck + other PC handhelds exists. Also we know it's March of 2025 now, so it's more like 9 months. Reply jeremyj_83 TheyCallMeContra said: \"Within a Wii U of tying with DS\". Being an entire console's install base away from being equivalent to Nintendo's current peak? Objectively speaking, they did peak with Nintendo DS. You do understand the literal meaning of the word \"peak\", yes? You can hardly forecast in good faith that the current Switch will surpass NDS when the existence of Switch 2 is widely-known and an entire competing market of Deck + other PC handhelds exists. I do know what peak means but thank you for the condescending tone. Do you have any idea how few consoles the Wii U sold? The Wii U sold all of ~13M units with the Switch at 141M after 7 years it very well could catch and surpass the DS. Even after the PS5 and Xbox Series X were released people still bought the PS4 and Xbox One. That will keep happening with the Switch as well. The competition from Deck, etc... isn't the same. Not to mention there have been multiple players that tried to enter the handheld market against Nintendo and all failed. Plus Deck and such has been around for years and still only a bit player. TheyCallMeContra said: Also we know it's March of 2025 now, so it's more like 9 months. Do we really know it is March 2025? Everything for the Switch 2 release date is speculation. Nintendo has only said they \"will announce the new switch within its current fiscal year, which ends in March 2025.\" That doesn't mean lauch in March and could very well mean it isn't lauchend until the holiday season 2025. Reply TheyCallMeContra jeremyj_83 said: I do know what peak means but thank you for the condescending tone. Do you have any idea how few consoles the Wii U sold? The Wii U sold all of ~13M units with the Switch at 141M after 7 years it very well could catch and surpass the DS. Even after the PS5 and Xbox Series X were released people still bought the PS4 and Xbox One. That will keep happening with the Switch as well. The competition from Deck, etc... isn't the same. Not to mention there have been multiple players that tried to enter the handheld market against Nintendo and all failed. Plus Deck and such has been around for years and still only a bit player. Forgive the condescending tone, but you're trying to correct me when I'm already right. You're still making a forecast to try and disprove an argument that is currently true and has been for quite a long while. If Switch actually has another 13 mil in it, come back to this thread when it happens and I'll CashApp you a $5 or something. I seriously doubt anyone who wants a Switch doesn't already have one or isn't just waiting for the better version to drop next year, though. re: Switch 2 release...fair I guess, but I'd be seriously shocked if they didn't meet the March 2025 release window, in alignment with the first Switch. that was leaked before Nintendo made official comment borderline confirming it. Reply JamesJones44 TheyCallMeContra said: re: Switch 2 release...fair I guess, but I'd be seriously shocked if they didn't meet the March 2025 release window, in alignment with the first Switch. that was leaked before Nintendo made official comment borderline confirming it. I know they launched the Switch in March but a lot has changed since then and companies are targeting year Q4 a lot more these days (year Q4, not company Q4). If I could bet on it, I would put money down on September/October release, but it's truly anyones guess at the moment. Reply VIEW ALL 8 COMMENTS Show more comments",
    "commentLink": "https://news.ycombinator.com/item?id=40670898",
    "commentBody": "AMD CEO Lisa Su reminisces about designing the PS3's infamous Cell processor (tomshardware.com)128 points by rbanffy 3 hours agohidepastfavorite97 comments magicalhippo 2 hours agoHad a heterogeneous programming class, where we had to implement various things on the PS3, like a basic MPEG-ish encoder and such. Was an interesting experience, though all I remember now is how it required careful exploitation of the vector units in the SPEs to get any decent performance out of it, and how annoying it was to synchronize between the SPEs and the PPE. For each assignment the prof would include a benchmark, so we could compare the performance in class. Was a huge difference between the students that spent time optimizing and those which did a basic implementation. reply skavi 1 hour agoparentthat sounds like a super fun class. wonder if it’s still offered? reply magicalhippo 1 hour agorootparentSeems like they still do[1], though I doubt the PS3 is still in the mix. Though from the published material for 2024 it seems not to have changed too much in principle, eg the codec63 I recall implementing parts for. [1]: https://www.uio.no/studier/emner/matnat/ifi/IN5050/index-eng... reply Waterluvian 2 hours agoparentprevI'm super curious what program/course was set up with developer/jailbroken PS3s to use as lab material. Was this specifically about game/console dev? reply selykg 2 hours agorootparentI believe the PS3 had an option to install Linux on it at some point. No? That would've made it a neat option for classes like this. reply scott_s 1 hour agorootparentYes. My research lab in grad school had a cluster of 24 PS3s. The back of that rack was hot. reply htrp 1 hour agorootparentPlease tell me you guys called it \"SuperCELL\" reply pjmlp 1 hour agorootparentprevYes, it was called OtherOS. reply Waterluvian 1 hour agorootparentprevOh right... I think somewhere around 1st gen they were cool with that. But didn't they end up getting killed in the market because they had to sell it for less than Five Hundred and Ninety Nine U.S. Dollars, effectively at a loss, and people were making compute centres out of them? I think a later gen locked it right down. reply gamepsys 1 hour agorootparentFor the most part the cluster of PS3s supercomputer was a myth used to hype up how powerful the PS3 was. The amount of RAM per compute was so low that it limited the type of workloads it was good at. Then server CPUs became more powerful as the PS3 processor stayed the same. However PS3 clusters did find niche success as a super computer. There are a handful of notable examples. The fastest one ever built was by the US Airforce, consisted of 1,760 PS3s, and was the 33rd fastest super computer at the time. It was used for satellite image processing. reply frou_dh 20 minutes agorootparentWhen I was paying attention during the 90s and 2000s I remember this being the hype in magazines / on the web for pretty much EVERY upcoming console. Namely that it was so outrageously powerful that it might be considered a supercomputer in its own right. Needless to say, I was hyped up and obsessing about them myself. reply throwaway48476 38 minutes agorootparentprevThere was a time when supercomputers had no GPUs. Nowadays the top list is all GPu accelerators. reply xattt 1 hour agorootparentprevOtherOS was a way for Sony to work around higher tarifs. General computing devices (enabled by OtherOS) had a lower schedule than consoles. It was retroactively removed when people started getting close to enabling full GPU access. This was previously crippled under OtherOS to prevent games developed without the PS3 SDK. reply magicalhippo 1 hour agorootparentAh interesting. I do recall we never utilized the GPUs on them, just the Cell. reply pclmulqdq 1 hour agorootparentprevReportedly the US government had no problem paying $599 per PS3 to build ML/AI supercomputers out of them. reply scott_s 1 hour agorootparentAny source on that? It sounds unlikely to me. There was a supercomputer that IBM built for Los Alamos that used the Cell processor: https://en.m.wikipedia.org/wiki/Roadrunner_(supercomputer) reply brianleb 37 minutes agorootparentProbably referring to: https://phys.org/news/2010-12-air-playstation-3s-supercomput... >>About the 33rd largest supercomputer in the world right now is the US Air Force Research Laboratory's (AFRL) newest system, which has a core made of 1,760 Sony PlayStation 3 (PS3) consoles. In addition to its large capacity, the so-called \"Condor Cluster\" is capable of performing 500 trillion floating point operations per second (TFLOPS), making it the fastest interactive computer in the entire US Defense Department. >>It will be used by Air Force centers across the country for tasks such as radar enhancement, pattern recognition, satellite imagery processing, and artificial intelligence research. reply chaorace 1 hour agorootparentprevRumor has it that Sony axed OtherOS in response to Geohot using it to exploit & compromise the PS3 hypervisor. reply MadnessASAP 6 minutes agorootparentIf I recall correctly that was a stated fact. OtherOS was removed due to \"security vulnerabilities\". reply mywittyname 36 minutes agorootparentprevNot OP, but we had PS3s at school as well. Early units ran linux. It was sandboxed to a degree in order to prevent it from being used for software piracy. OP's project was cool, ours was basically implementing PCA on one. I got the impression that Sony encouraged this use of the console early on. They were probably aiming to establish themselves in the scientific computing niche. We also had nvidia-sponsored labs where they taught CUDA (or at least tried to - it was pretty difficult to do without support and most people took the class for access to computer labs equipped with amazing gaming machines). We all know nvidia won that war. reply magicalhippo 1 hour agorootparentprevThey had Linux on them. Don't think I ever saw them, just had network access. reply buildbot 2 hours agoprevOh wow, I did not know Lisa Su had a hand in designing Cell - that’s really cool. In some ways the Cell architecture was very ahead of its time - if you look at it a bit funny, the PPE/SPE map to something like todays grace/hopper - big branchy core to orchestrate smaller but more numerous vector cores. Made designing/porting games a massive headache though because nothing else was like it. reply Rinzler89 2 hours agoparentCorrection: Cell was ahead of it's time when they started drafting the concept for it in mid-2000 and the most powerful consumer CPU at the time was the Pentium 3, but by the time it launched on the market in 2005, X86_64 was the new undisputed king of CPU performance and ARM in mobile lower power sectors, with ATI and Nvidia leading the GPU race, leaving the Cell processor as an overpriced paperweight that was a jack of all trades, master of none and difficult to program to boot, and the new market had no use for such a thing. I think neither Sony nor anyone else expected the consumer CPU and GPU industry to evolve so quickly in that time, otherwise they would not have spent hundreds of millions developing anew architecture from scratch. Intel also made the same mistake back then when they tried to reinvent the industry with the new Itanium CPU architecture. Hindsight 20/20. Those years were indeed wild for tech progress. You bought a new CPU, suddenly two years later you have 64 bit CPUs as the new thing. You then bought a 64 bit CPU to be like the cool kids and suddenly dual core CPUs are all the rage. reply monocasa 9 minutes agorootparentEh, the Cell would have been a fantastic processor if it weren't for hitting the end of dennard scaling like a brick wall. In the design phase they were expecting almost 5ghz at launch, and to hit around 10ghz at the end of the product lifecycle. When the process guys came back later to let the architects know the reality we've sort of now all internalized (high end consumer electronics can expect ~3ghz with good cooling solutions) a lot of simple but fast pipeline designs stopped making sense. These are the exact same pressures that killed off the Netburst architecture too, making intel go back to a PIII design and make it smarter rather than clock markedly faster. reply scrlk 2 hours agorootparentprevIIRC, the original PS3 design was rumoured to have dual Cell processors. It didn't meet performance targets, so the Nvidia GPU was wedged in quite late on in the development cycle. reply Rinzler89 2 hours agorootparentI think the story was a bit different. Sony had envisioned the the final Cell chip to be more powerful than what they launched (4 PPEs and 32 SPEs @ 4GHz versus 1 PPE and 8 SPEs @ 3.2GHz) and they though that would be enough to render graphics with it like they did on the PS2, and only realized later in development that it won't be enough so they went to Nvidia and asked for a discrete GPU. reply sillywalk 33 minutes agorootparentThere's an interesting book called The Race for a New Game Machine: Creating the Chips Inside the XBox 360 and the Playstation 3 by David Shippy and Mickie Phipps on the development of the Cell processor. I can't recall if if had all the details regarding adding a GPU to the PS3, but I remember it had stories about the awkwardness of having the IBM Cell team also working on Microsoft's Xenon CPU for the 360, which IIRC used modified Cell PPE cores. reply jandrese 51 minutes agorootparentprevAlso, I think game developers balked at having to implement their own GPU in a hard-to-program unique architecture. I wonder how many PS3 games ignored the Cell processor and just developed for the CPU and GPU? reply monocasa 7 minutes agorootparent> Also, I think game developers balked at having to implement their own GPU in a hard-to-program unique architecture. It would have been a library. > I wonder how many PS3 games ignored the Cell processor and just developed for the CPU and GPU? Essentially none. reply driscoll42 1 hour agoprevThere's a wonderful blog post on the PS3 Architecture - https://www.copetti.org/writings/consoles/playstation-3/ that gives a good overview of the Cell processor with linked resources if you want more detail. reply hadrien01 7 minutes agoparentWow this website is absolutely amazing! Full of details about consoles, and with a really pleasing mixture of text, images, and even 3D models. reply khalilravanna 1 hour agoprevMy biggest lament with the PS3 is the forward incompatibility making many great games locked to the platform (MGS4, Demon’s Souls, etc.). Meanwhile in Xbox land there are *633* 360 games you can play on any new Xbox by just slipping the disc in. Now some of this is no doubt due to differing approaches to business by the corporate masters but from what I’ve read a lot of it is the unique and befuddling architecture of the PS3. https://en.m.wikipedia.org/wiki/List_of_backward-compatible_... reply throwaway48476 35 minutes agoparentXbox canceled development on their emulator though. reply Rapzid 2 hours agoprevATI developed the graphics chips for 360 and Wii. Feels a little too revisionist to say AMD produced the chips even if they did purchase ATI a number of months before Wii released.. reply fourfour3 2 hours agoparentIt’s a bit tricky even to say that ATI developed them - they were designed by ArtX (mostly ex SGI staffers who had also worked on the N64), who were bought by ATI. reply Rapzid 1 hour agorootparentPerhaps but ArtX was acquired by ATI long before. The ATI logo was already on the GameCube at launch. reply wmf 36 minutes agorootparentprevMaybe Wii was ArtX but wasn't the 360 using a mainstreamish Radeon GPU? reply kevvok 1 hour agoprevThere’s a great book about the development of Cell and Xenon (which went into the Xbox 360) called “The Race for a New Game Machine” that was co-written by a couple folks who worked on them at IBM. reply tithe 3 hours agoprevPeter Hofstee (one of the chief architects of Cell) gave a talk at UT Austin around 2008. At the time, UT was building TRIPS[0], a processor capable of > 1 teraflop / sec. In the middle of his talk (with several TRIPS members present), he made an off-hand remark \"I think I could build a petaflop processer...without too much trouble,\"* which caused a small stir in the audience! [0] https://www.cs.utexas.edu/users/cart/trips/ * I might be misremembering the exact quote, but this was the gist of it. reply lhl 2 hours agoparentPerhaps relevant to the story, he was also on the team that delivered the first petaflop supercomputer (the most powerful supercomputer at the time) around that same time frame (it was around 24K processors): Roadrunner is a 1.38 Pflop/s-peak (double precision) hybrid-architecture supercomputer developed by LANL and IBM. It contains 12,240 IBM PowerXCell 8i processors and 12,240 AMD Opteron cores in 3,060 compute nodes. Roadrunner is the first supercomputer to run Linpack at a sustained speed in excess of 1 Pflop/s. https://dl.acm.org/doi/10.5555/1413370.1413372 https://en.wikipedia.org/wiki/Roadrunner_(supercomputer) reply bbatha 2 hours agorootparentWhat a pain in the ass machine that was to write code for. You had 3 different processors to manage with 3 different architectures and all of the challenges the game dev community had with keeping the SPEs hot. But the SPEs were not enough to solely rely on for number crunching performance unlike today's GPU compute machines, so you also needed to do number crunching on the opterons and the main cell core for optimal performance unlike with a GPU where the CPU is mostly just keeping the GPU memory full. Then to make matters worse the cell had a different endianess than x86 making shared memory very annoying to work with. reply sillywalk 0 minutes agorootparentI'm curious if you've ever heard of or used CellFS[0] , which was supposed to simplify programming for Cell? [0] https://www.osti.gov/servlets/purl/1000498 wmf 2 hours agoparentprevFLOPS are easy; programming is hard. Cell was the wrong direction and TRIPS/TFlex hasn't happened either. reply Der_Einzige 2 hours agoprevThere were memes circa this time on places like 4chan about \"the power of the cell\" and implying that it was capable of doing anything. Sony marketing about how good this chip was circa 2005-7 was strong. reply scrlk 2 hours agoparentThe power of the Cell bought us real time rendering of Giant Enemy Crabs, as well as real time weapon changes. All for 599 US dollars! reply jeroenhd 59 minutes agoparentprevThe cell processor was pretty good at some calculations. Unfortunately, it wasn't great for writing video games and neither was the tooling. I recall various news stories about governments buying up PS3s because they could be used for cheap and effective computation at scale. reply toast0 2 hours agoparentprevIt's a shame they didn't license blast processing. reply hehdhdjehehegwv 26 minutes agoprevShe’s low-key one of the most successful engineers ever in terms of bringing cutting edge technology to the mass market, but AMD never gets the fawning press attention of NVIDIA, and still has the lingering image of being the poorer cousin of Intel. Of course, as a woman engineer to make it this far she’s used to not getting the limelight in a male dominated industry - which may also explain some of AMDs recent success. reply A4ET8a8uTh0 2 hours agoprevI have a weird problem with the article. It does start with a question over 'infamous' qualifier, but it goes into something deeper that has been bothering me over the past few years. Lets start from the end, game consoles these days are basically optimized PCs. They are effectively nothing like the first, second, or even third generation consoles that had fairly unique hardware, desirable exclusives and so on. Cell may have been difficult to program well for, but it speaks volumes about the current state of consoles that devs worry about being able to be lazy with ports. The idiot consumer will buy whatever anyway. The side effect of the first generations' relatively unique hardware profile is that games actually had to be optimized and tailored to the hardware. No lazy ports ( I still get pissy sometimes over ps3 dragon age port ). Exclusives had to be good or at least showcase what the console could do. So is that what makes Cell infamous? The thing made news when could be run in a cluster[1]. I have a different take. Cell is the remnant of the consoles, when they were still worth one's time. [1]https://en.wikipedia.org/wiki/PlayStation_3_cluster reply MBCook 1 hour agoparentThe article gives the answer. It’s not about being lazy with ports. It’s that the asset budgets are so large that making games is way too expensive. For any normal company you basically _have_ to launch on multiple systems if you want a financial payback. And every time graphics get better the problem gets worse. More models, more textures, more detail, more everything. $$$ Unless you have a monster hit (Zelda, Last of Us) you may be unable to make your money back on one machine. Even if you could, why not just make more money by adding in the other machines? There’s a reason pretty much only 1st party studios do exclusives. reply A4ET8a8uTh0 1 hour agorootparentIf there is one thing that Steam has proven, it is that games absolutely do not have to focus on 8k assets, celebrity likeness and voices. In other words, that is not really an answer. Or at least not a real answer. It is, at best, a part of it that glosses over why people play games. reply MBCook 7 minutes agorootparentTotally. Thomas Was Alone is one of the best games I’ve ever played and it’s just a bunch of boxes. It’s basically high resolution Atari. Great graphics are nice and can certainly help, but the be all end all race to always look the best is not working well. Some kind of reckoning has got to be coming. We’re getting to the point where it’s like 5+ years between sequels to big titles just cause that’s how long it takes. Much like mega budget Hollywood movies no one‘s willing to take a risk when it costs that much. reply dylan604 1 hour agorootparentprevNot once ever have I purchased a game because of a character's voice and looks. In fact, most of the time, I'm skipping the \"I wanted to be a director, instead I make games\" cut scenes. Does spending money on that actually increase sales for games? I get having great looking assets, but voices? reply somenameforme 2 minutes agorootparentI think it's largely about the marketing aspect. Imagine creating marketing for Crusader Kings 2, Mount and Blade, Terraria, and the countless other games which sold a zillion copies and are unbelievably fun, but just look pretty bland on the surface. With high end voice acting, lots of cut scenes, and so on - all of the marketing basically writes itself. You have your actory voice intro the game, set the stage of the plot, show some brief segments of critical moments in cut scenes, and then occasionally intersperse a half second or so of actual gameplay. No idea why this gets some people fired up, but it seems to work reliably enough. I suppose it's just targeting a demographic that isn't you nor I! Pet_Ant 38 minutes agorootparentprevI’ve never understood people who enjoy the Diablo end-game but I love playing the games through for the story, vibe, and lore. The voice acting definitely contributes. The voice of Deckard Cain is instantly recognizable to me. reply ProfessorLayton 17 minutes agoparentprevWhile I too miss exotic hardware (N64 being my all-time favorite), ultimately what people want to play is great games and the hardware is just a means to an end. The bigger issue with current consoles imo, specifically the PS5 and XB, is their input homogenization and stagnation. Consoles have historically been forward-looking via their controllers to enable new ways to play games: [NOT including optional accessories, only out of the box abilities] - N64: Analog controls, 4 controller ports, controller expansion ports - PS1: Dual analog controls - PS2: Pressure-sensitive buttons - Dreamcast: ... - Gamecube: ... - Wii: Motion controls - PS3: ... - Wii U: Gamepad screen - 360: ... - PS4: ... - Switch: Detachable and sharable controllers - XB1: Kinnect - PS5: ... - XB1X: ... I'm definitely not implying that every new input type was successful (See Wii U, kinnect), but consoles up until about PS3/360 had something new to offer regarding ways to experience games. This has all but stopped, with Nintendo being the only one to consistently introduce new ways to play that cannot easily be replicated on a PC. Plugging a PS5/Xbox controller into a PC yields the same or better experience as buying a dedicated console these days, so why buy the console at all? reply dfxm12 1 minute agorootparentwhy buy the console at all? Putting console exclusives aside, I can build a PC that does everything I need it to do but play some new games + a console for less than it costs to build & maintain a \"gaming rig\". Also, my PS4 is still working after ~10yrs. I would have to upgrade my PC more frequently than that to keep up with new games. reply MisterTea 1 hour agoparentprevTo me the infamy is owed to Sony's \"super computer on a chip\" hype marketing (similar to the hype around the Emotion Engine int he PS2) surrounding the architecture coupled with the difficulty in programming it. The big issue was SPE's had no way to access main memory so you had to copy the data from RAM into each SPE which had its own local memory. reply Waterluvian 2 hours agoprevI'm curious about how such a bad idea made it to market. Not that the technical concept is bad, but the business decision to build an entire console generation on such a unique, ultimately migraine-inducing architecture. reply MBCook 1 hour agoparentRemember Cell was “the future”. It was going to go into the PS3, but also TVs and computers and whatever else. If you squint it’s a bit like Apple putting a ton of investment in the A series chips that then went into iPads and Macs (via the M series). Problem is: no one wanted it. So none of that panned out. If it had gone on to be something like Intel’s Core 2 Duo that showed up everywhere in hindsight it would look smart. reply initplus 1 hour agorootparentDifference is that Apple invested heavily in backwards compatibility, even old x86 code performs well on their chips. Meanwhile cell requires a reworking of your entire program to take advantage of it. reply MBCook 9 minutes agorootparentThat was common for consoles at that time. The PS2 couldn’t actually run PS1 games, it just fell back to a PS1 embedded in the system. Same way the GBA played GB/GBC games. Apple wasn’t a great example but it was the best I could think of. It took Apple 10 years to go from the A4 in the first iPhone with an Apple chip to the M1 in the first Mac. They also were just using Arm, the same instructions that they had been using before and something well understood. They didn’t suddenly release something people weren’t really prepared for like a Transputer and just declare “this will be everywhere within three years“. They let the switch take its time as necessary. Sony had the arrogance to do both of those. reply failuser 39 minutes agoparentprevYou forget that it was normal before. PS2 had many separate chips you need to program and balance throughout and latencies to make the most of hardware. PS2 was able to achieve 60 fps in many impressive games. Console hardware was very specific, the original Xbox being just an Intel PC was an exception. reply initplus 1 hour agoparentprevIt’s less a confusing one off decision to use a weird architecture, and more a failure to reconsider the status quo and stop using weird architectures. Previous PlayStation hardware was just as odd. Ps2 reused old surplus Ps1 processors just for audio, and had coprocessors just like cell. reply 0x457 1 hour agoparentprevI don't think Sony expected Xbox 360 so soon. PS2 was sales were still good and there was no console on a market better than PS2 at that time. Then out of nowhere Xbox 360 comes out, Halo that was a macOS title suddenly a main and a well selling IP on Xbox. Not just that, it also picked HD-DVD as its format posing a threat to Blu-ray. Sony had to put something out. reply vel0city 56 minutes agorootparentHalo came out in 2001, four years before the Xbox 360 was released. The Xbox 360 released its HD-DVD drive a few days before the PS3 hit store shelves in North America. Sony had established the PS3 would use Blu-Rays a year before the Xbox 360 was even publicly announced. I think there were a number of things Sony wasn't expecting about the Xbox 360, but HD-DVD or Halo wouldn't have been any of those things. reply Talanes 1 hour agorootparentprevThe 360 didn't pick HD-DVD as it's format, the built-in drive was just regular DVD. The HD-DVD drive was an external add-on that they sold along with the 360 Media Remote. reply cmpxchg8b 1 hour agorootparentprevHalo first came out on the original Xbox, not sure it ties into the PS3/360 calculus other than being a launch title. reply vel0city 50 minutes agorootparentThere wasn't even a Halo game as a launch title, but there was backwards support for the original Xbox Halo games on the 360. https://en.wikipedia.org/wiki/Xbox_360_launch#Titles There wouldn't be a Halo game released for the Xbox 360 until September 2007, nearly two years after the release of the console. reply peutetre 2 hours agoprev> The PlayStation 3 pretty much lost the generation handily to Nintendo's cheap, casual-friendly Wii and Microsoft's less powerful but easier Xbox 360. No it didn't. The PlayStation 3 outsold the Xbox 360. Have a look at the 7th generation charts on VGChartz: https://www.vgchartz.com/ It's true the Wii sold the most. The 8th generation didn't go well for Nintendo. The Wii U was a flop. But they're back on top with the Switch. reply bee_rider 2 hours agoparentNintendo is practically in a different market anyway. People who want a Nintendo console are mostly looking for Nintendo’s first-party characters. Lumping them in with Xbox and PlayStation is like combining Barbie and GI Joe under “dolls.” Technically correct but not very descriptive. Snark: Nintendo’s market is kids, people with kids, and people who were previously kids. Xbox and PlayStation are fighting over the market of people who’d be better served by a PC but don’t want to deal with all that. /Snark reply Clamchop 56 minutes agorootparentI don't disagree that Nintendo appeals to a market that the others don't, and vice versa, but I'd be surprised if there wasn't a significant number of people who will reliably buy a game console, but won't buy two consoles. That is to say, I still think there's significant competition for sales. And if Nintendo didn't exist, it would be easier for Sony and Microsoft to take in the market for cute, whimsical, child-friendly, instead of having to distinguish themselves by being the more serious or more powerful option. And handhelds; Nintendo has more or less stolen that entire segment, Steam Deck still being relatively niche and PS Portal being a different kind of thing. Sony's handheld ambitions are moribund and not for lack of trying. reply vel0city 41 minutes agorootparentThe target market for Xbox/PlayStation are the kind of gamers that play games like Call of Duty, Gran Turismo/Forza, Elden Ring/Dark Souls/The Witcher, or FIFA/Madden/NBA2k. The target market for the Switch are people who play Animal Crossing, Super Mario Galaxy, and Zelda games. There's some amount of crossover there, but there's also a large amount of separation in those markets. I know lots of people who play a ton of Animal Crossing and Zelda but would never want to play something like Borderlands or CoD or Darksouls. reply r00fus 1 hour agorootparentprev> and people who were previously kids. That's a pretty damn large market (= all adults). I think you meant to say \"people who were previously kids playing Nintendo\" reply bee_rider 29 minutes agorootparentI’d hoped given the snark tags that that bit would be taken as a bit tongue in cheek. reply SahAssar 1 hour agorootparentprev> kids, people with kids, and people who were previously kids Who is not included in that? Toddlers? reply bee_rider 31 minutes agorootparentIt includes everybody, that’s why they sell so many consoles! reply TillE 1 hour agorootparentprevImplicitly, people who are too old (or too European) to have grown up with an NES. But yeah, I 100% agree that the overwhelming draw of Nintendo consoles is their exclusives, so it's mostly orthogonal to Xbox/PS/PC. If you want to play Mario or Zelda or Pokemon or Fire Emblem, you've got one choice. The common idea that the Steam Deck is competing with the Switch is almost entirely wrong. reply SahAssar 1 hour agorootparentThe comment doesn't mention anything about the \"previously kids\" growing up with nintendo. reply EcommerceFlow 2 hours agoparentprevYou're talking about an extremely end of cycle resurgence. For the vast majority of the lifecycle, the 360 outsold the PS3 and had better compatibility with multiplatform games. reply nixass 2 hours agorootparentPS3 ended up selling more than double what Xbox was sold \"only because of extreme end of cycle resurgence\"? Cmon It's true the devs have become more comfortable develeoping games as the console matured but that's about it. Xbox was always flop comparing to PS reply busterarm 2 hours agorootparentWhere do you get \"more than double\" from? Are you looking at the current generation or the PS4/XONE generation? For PS3 vs 360, the PS3 sold 87.40M units to the 360's 85.73M. reply app13 2 hours agorootparentprevThey're right. Toward the EOL the PS3 had a big resurgence as THE console of South America and Southeast Asia due to highly available used games, console jailbreaks etc. and of course FIFA. reply kd913 2 hours agorootparentIsn't that kind of the ideal cycle though? From what I recall, consoles operate on the gilette model, ie the console is sold at a loss and games are the breadwinner. In this case, better to reduce early sales which are sold at a loss (production being expensive, significant R&D, scale being low), maximise at the end of the life when i assume they are maximising profits per console due to scale and R&D already being covered. reply merb 2 hours agorootparentprevPS3 sold more because of blue ray support, but in the games compartment they were basically on par reply ErneX 1 hour agorootparentprevPS3 did not sell double the 360, they sold almost the same amount of units. reply causality0 2 hours agorootparentprevIt's not double, it's 87.4 million units vs 86 million units. And he's right about multiplatform games. I can't think of any games that ran better on the PS3 than the 360, and a lot that ran better on the 360 than the PS3. reply peutetre 2 hours agorootparentprevNope. The Xbox 360 launched in November of 2005. The PlayStation 3 launched in November of 2006. The PS3 started outselling the Xbox 360 for the year in 2007 and kept steadily gaining on the Xbox in total sales for the generation until it overtook it. Xbox's one year head start kept it level with the PlayStation 3 for quite a while but it lost out in the end. Look at the global yearly sales figures on VGChartz. From 2007 the PS3 outsold the Xbox 360 every year except for 2008: https://www.vgchartz.com/yearly/ reply snakeyjake 29 minutes agorootparentI think a lot of people bought the PS3 as a bluray player. The charts you link to show game sales that lag way behind the xbox. For example Skyrim sold 2.6 million in 2011 on the xbox and only a million on the ps3. I only say this because I got my ps3 for free with the purchase of a Sony television at a Sony store (remember those?) in late 2009 and never purchased more than a bare handful of games which I almost never played. After the cost-reduced redesign in 2009 Sony gave away a free ps3 with every television purchase in Sony stores for almost an entire year. Skyrim was one the games I bought and it was so buggy, even for Bethesda, on the ps3 due to ram limitations (I've read) that I purchased it again for pc. Used my ps3 to watch a ton of blurays and Netflix, though. My ps3 was, and based on those yearly charts a lot of other peoples' ps3s were also, extremely powerful streaming boxes and I only retired mine when I upgraded to a 4k tv and got an appletv 4k. reply tokai 2 hours agoparentprevIts wild that we are almost 20 years on, and consolewar posting about which is bigger/best is still going on. reply Rinzler89 1 hour agorootparentHey as long as the PC is still king, let them fight over the consoles :D reply dehrmann 2 hours agoprev [–] > but SMT wouldn't emerge until 2002 I think they're counting P4 hyperthreading, but I remember dual-CPU Pentium II boards. reply adrian_b 2 minutes agoparentSMT (including that branded as Intel HTT) is something very different from multiprocessors, like the dual-CPU boards. The concept of multiprocessors was already well understood around 1963, i.e. the multiplication of the performance of a computer by the multiplication of the CPU hardware. On the other hand, the purpose of SMT is to enhance the throughput of a computer by ensuring that the existing execution units do not stay idle, by executing instructions from multiple program threads in order to keep them busy. The modern use of SMT has been started by a research paper published in 1992, but the name SMT (simultaneous multi-threading) has been coined only later, in 1997 (in IEEE Micro from September/October 1997). IBM had already used SMT in the experimental Advanced Computer System in 1968 (and they have filed for SMT patents in 1971: US 3,728,692 & US 3,771,138), but that project has been canceled and its superior architecture has been mostly forgotten for several decades. reply buildbot 2 hours agoparentprevIBM had SMT in 1968 - https://people.computing.clemson.edu/~mark/multithreading.ht... reply trynumber9 38 minutes agorootparentI am not following. The page says it was investigated but not done. reply buildbot 33 minutes agorootparentIt was done - ACS-1: https://en.wikipedia.org/wiki/IBM_Advanced_Computer_Systems_... reply wtallis 2 hours agoparentprev [–] SMT isn't SMP. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Lisa Su, before becoming AMD CEO, worked at IBM and contributed to the design of the PlayStation 3's Cell processor, which was based on IBM's PowerPC and focused on parallelization.",
      "Despite the PlayStation 3 being one of Sony's least successful consoles, it sold 87.4 million units by March 2017 and led to visually stunning games like Uncharted 3.",
      "Under Lisa Su's leadership, AMD has provided hardware for both PlayStation and Xbox for two consecutive console generations, marking a significant achievement for the company in the gaming industry."
    ],
    "commentSummary": [
      "AMD CEO Lisa Su shared insights on designing the PS3's Cell processor, highlighting its advanced architecture and challenges in game development.",
      "The PS3's Cell processor was powerful but difficult to program, leading to mixed success and limited market adoption compared to X86_64 and ARM CPUs.",
      "The PS3's unique hardware, including the Cell processor and Nvidia GPU, made it a complex but innovative console, influencing the evolution of gaming consoles and their development environments."
    ],
    "points": 128,
    "commentCount": 96,
    "retryCount": 0,
    "time": 1718292690
  },
  {
    "id": 40668803,
    "title": "iTerm 3.5.1 removes automatic OpenAI integration, requires opt-in",
    "originLink": "https://iterm2.com/downloads.html",
    "originBody": "Menu Home News Features FAQ Documentation Downloads Stable Releases Stable releases update rarely but have no serious bugs. iTerm2 3.5.2 (OS 10.15+) This is the recommended build for most users. Built on June 13, 2024. ▸ Show Changelog ▸ Show Older Versions Test Releases Test releases update many times a year and are occasionally unstable. iTerm2 3.5.1beta4 (OS 10.15+) This is the recommended beta build for most users. Built on June 3, 2024. ▸ Show Changelog ▸ Show Older Versions Nightly Builds A nightly build is made at midnight Pacific time on days where a change was committed. The change log may be seen on Github. Nightly builds sometimes have serious bugs. Latest nightly build Older nightly builds may be found in the nightly build archives. iTerm2 by George Nachman. Website by Matthew Freeman, George Nachman, and James A. Rosen. Website updated and optimized by HexBrain",
    "commentLink": "https://news.ycombinator.com/item?id=40668803",
    "commentBody": "[dupe] iTerm 3.5.1 removes automatic OpenAI integration, requires opt-in (iterm2.com)125 points by trustno2 6 hours agohidepastfavorite131 comments joshstrange 5 hours agoThis was discussed yesterday: https://news.ycombinator.com/item?id=40657890 reply teruakohatu 6 hours agoprevI feel for the developers who work for free on an open source project but got a lot of criticism and hate thrown at them for introducing an optional feature. It’s not a feature I would use, but not using it was an simple as not entering an LLM API token. reply throw10920 5 hours agoparentIt was a hate bandwagon born out of sheer ignorance with no actual problem behind it. reply rincebrain 5 hours agorootparentIt's not, actually, solely that. If you're using a product that does not make API calls externally with your data previously in a corporate environment that has very strict controls, and they add a feature that is part of the base install package, even if opt-in, that allows it to do that, and they do not have something like Group Policy hooks to forcibly disable it from on high, then they will block the product globally until the functionality is more contained to something their compliance systems can prevent, be it via a group policy hook or blocking the install at all. Companies take \"we might make an external call with your data\" very seriously, and regardless of how much you trust the external entity, adding that in is rightfully seen as a very serious concern in some environments. reply joshstrange 4 hours agorootparentThis comment is aimed at the people who made this argument, not you specifically. This is a completely ridiculous strawman. If a corporation needs strict controls then they don't allow for auto-updated software without vetting. They could have asked for a policy to disable the integration but they didn't, instead those people screamed about how they were going to have to completely block iTerm which showed their incompetence. This was in beta for quite a while and none of these \"serious IT people\" cared so either they are asleep at the wheel (not vetting beta versions) or they allow for auto-updates to software they don't control, either way it screams incompetence to me. Also we are talking about a terminal here, a tool that can connect wherever the developer tells it to connect to. Are you telling me these companies lock down where a developer can SSH to but are too stupid to block traffic to OpenAI? Or they don't lock down where a developer can SSH and therefore don't talk to me about \"strict controls\". Make it make sense. It just doesn't. This argument never held any water for me. reply pseudalopex 54 minutes agorootparentVarious people suggested or demanded various things for various reasons. Some described company controls. Some described company policies without controls. Some described their own strong feelings. Some had auto updated. Some had not. Some did not say. Some supported their companies' policies. Some stressed the policies were out of their control. Some did not say. Some asked how to make sure the integration was disabled. Some did ask for a setting an IT department could manage. Some demanded a plugin or separate build. Some said a plugin would work for them. Many things don't make sense if you assume everyone you disagree with are a collective. reply joshstrange 28 minutes agorootparentI didn't assume they were a collective. I read through many of the comments on the GitLab issue as they were happening and there wasn't as single valid complaint. Not one. Zero. Zilch. It was opt in, full stop. That should have been the end of this aside from IT managers asking for a group policy or a was to disable the feature (in a nice way). Everyone complaining could either not update or move to a different tool and ask for a refund... oh wait... that's right, this was open source software. And people wonder why no one wants to maintain open source software. Just add this whole ordeal to the ever-growing pile of reasons why it's almost not worth doing. People are so entitled. My personal favorite were the people that said \"If you remove this I'll start support you monetarily\", which I assume 99% of those people were lying through their teeth based on the lack of spike in new donations to his patreon [0]. [0] https://graphtreon.com/creator/gnachman reply fipar 5 hours agorootparentprev> Companies take \"we might make an external call with your data\" very seriously, and regardless of how much you trust the external entity, adding that in is rightfully seen as a very serious concern in some environments Please don't take this as an attack to you, but one would think that environments where this is a very serious concern would also be environments that either buy software, or buy support contracts with open source developers, to make sure that what they install is compliant with their concerns. reply lolinder 4 hours agorootparentNot to mention that an IT department that is truly concerned about this risk would just block the AI APIs at the network level and be done with it. A competent IT department with the concerns that OP is expressing would know that there's no way for them to keep track of all the software that is implementing AI features and would make the very easy change that would solve all of those concerns at once, not go on a witch hunt against iTerm2. This means that the only people that we're supposedly catering to here are incompetent IT departments, which doesn't seem worth the hassle for the maintainer or all of the other users. Or, more likely, this hypothetical IT department that cares enough to block iTerm2 but doesn't care enough to block on the network level is a fiction invented by people who just really hate seeing AI added to everything. reply nerdponx 4 hours agorootparentprevA lot of times the open-source software is used \"unofficially\", where IT/security turns a blind eye to it as long as it doesn't openly violate policies. And often the developer is not offering a support contract. They might even take umbrage at the idea of being bribed to make changes! A lot of open-source software usage in the workplace is the equivalent of bringing your own tools to a worksite, or bringing your own knife set to the kitchen. I obviously can't see the project's finances, but funding for tools like iTerm 2 can be heavily dependent on individual users acting as patrons, making monthly donations purely out of gratitude for bringing them joy and/or improving their personal work productivity. reply fipar 3 hours agorootparentAll of what you said is true, but I just want to add a note that I mentioned a support contract with open source developers, not necessarily the authors of the open source software in question. It's like when a company buys a contract with an enterprise Linux distribution: the entity offering the contract didn't author the full stack (though I'd guess they have at least some kernel contributors on staff) but they can still support it in a way that keeps the compliance department of the buying company happy. Also, your kitchen knife set example is very relevant because, from personal experience, I know this happens but I also know problems (including accidents) resulting from using a personal knife set aren't attributed to the manufacturer of the knives but instead are treated exactly as if they happened using ones provided by the business that owns the kitchen. reply OskarS 4 hours agorootparentprevTotally! People in these kinds of environments have no leg to stand on criticizing a free and open source project. If you don't like it, don't use it, but CERTAINLY do not demand of the developers that they honor your requests when you don't contribute to development. reply bunderbunder 4 hours agorootparentMost employees in corporate environments do not get to set company policy. Most users of open source software do not have the skills necessary to modify said software. Complaining is the only recourse that most people have. reply ForHackernews 3 hours agorootparentThey can take their fat corporate salaries and hire a therapist to listen to them complain. Leave the FLOSS maintainers alone. reply throw10920 5 hours agorootparentprevThis is not universal. I work in a company that uses many different products that (attempt to) make external API calls with extremely sensitive data, and none of these products are blocked from installation, because we block all their requests at the network level, instead. Yes, the corporate management is useful, but not critical, and the lack of it absolutely didn't justify the generated outrage. reply rincebrain 5 hours agorootparente: You changed this from \"This is not true.\" to \"This is not universal.\" in the time when I was writing this response, which is fine, but I wasn't claiming this is universally the case, I was claiming that in some environments, this is how the logic works. It's more or less the same outcome and rationale as when JetBrains had theirs in a \"plugin\" that you couldn't effectively block even if it was a noop because it required an API key. Sure, in an environment where external access is not necessary or can be easily allowlisted, that works fine. But on someone's interactive workstation, where they might need to access parts of the internet without getting explicit allowlisting of every web site, then it's a different set of tradeoffs, and not every company implements this the same way. reply throw10920 4 hours agorootparentFrom the perspective of \"some corporate environments will block your software if it doesn't have this feature\", then I suppose I can understand the frustration that users felt. I would argue that there's more nuance, but the feeling is reasonable - I would also be frustrated if a piece of software that I used and loved at work was in danger of being taken away because of the addition of a feature that I didn't want in the first place! reply hiatus 5 hours agorootparentprevWhat do these companies do about browsers? If they can block at the network level, group policy to disable an app feature is moot. Put another way: if corporate policy blocks this app but not access to OpenAI, they are not solving the underlying issue. If they already block OpenAI they don't need the application control. reply miki123211 5 hours agorootparent> What do these companies do about browsers? Many things, browsers are pretty configurable via group policy. One possibility is to force browsers to go through a proxy that inspects all traffic, for example. reply hiatus 4 hours agorootparentSo these companies configure every application on the system individually to use a proxy? What about things like curl? reply wpm 4 hours agorootparentMore likely they configure the operating system to force network traffic through a certain hole, or force users to manually connect to full tunnel VPN to access work resources. Currently my employers inspect every network call made on my device, and while they do not block OpenAI, they do block us from running specific browsers that cannot have their built in “secure” DNS settings changed such that they won’t sidestep inspection. I get emails if I visit the Eicar test file website. reply swiftcoder 4 hours agorootparentprevIn a sufficiently high-security environment, they just block all network access that doesn't go through the proxy. That's a fairly nuclear option, however, as it also blocks a lot of harmless traffic (Spotify, for example) reply lolinder 4 hours agorootparentWhich, incidentally, would render this feature useless and harmless. It's just really hard to see the argument against the feature as anything other than an excuse for general anti-AI sentiments. Organizations that really care about this already have the tooling in place to stop all AI features in all products with just a few network-level rules. If they don't have that tooling, they're not taking it seriously and they're not going to start suddenly blocking iTerm2 because it added an optional AI feature. reply swiftcoder 2 hours agorootparentThey don't, actually. Network blocking is all fine and dandy until someone adds a secure proxy into their software. Shit like this is exactly why enterprise software implements policy controls (as iTerm has just done). reply EasyMark 4 hours agorootparentprevI think people expect browsers to go outside the LAN, though? I don't expect a termainal like iTerm2 to be sending out network requests other than maybe to see if there's an update. I think people in IT, especially ones where security is core, like that sort of thing. That said they could courteously request it get turned off rather than freak out over it (it is free after all), but I think people don't like surprises out of staples like a terminal. reply lolinder 4 hours agorootparentYou're missing the point though: the only real argument I've seen for this being extracted altogether is a corporate environment where the existence of this feature would be enough to get the application blocked. OP is questioning why that would be a reasonable approach for an IT apartment to take—if they already block OpenAI then the feature won't do anything even if configured. If they don't already block it, then there are almost certainly people in their company using OpenAI with the browser. Network level blocking is easier and more effective, so why should iTerm2 have to go to contortions to remove an already opt-in feature that would be easily blocked by a network block? reply spenczar5 4 hours agorootparentprevMandatory browser extensions are common, as are MITM proxies for all traffic. reply hiatus 4 hours agorootparentWhat do these companies do about curl or telnet or SSH or any number of developer tools? reply kstrauser 4 hours agorootparentFrom experience, “we don’t talk about those”. reply codegladiator 4 hours agorootparentprev> Companies take \"we might make an external call with your data\" very seriously No, they want to be seen taking it seriously, so they say that they take it very seriously. But if you will actually audit their system you will find they do not in reality and it was just a negotiation tactic. reply hnlmorg 4 hours agorootparentprevHaving worked in such environments one of two things happen: 1. Developers are forced to use Windows and given very little CLI access. And they certainly wouldn’t be using iTerm since it’s macOS only. 2. Developers are given a Windows corporate device as well as a MacBook Pro. The MBP doesn’t have access to any corporate systems 3. Developers can access corporate systems on MBP via jamf or InTune. The corporate side of the business accepts the risk that developers can access external (read: non-vetted) services but the business has access to all internet traffic logs from your device (usually pushed into some kind of security package that monitors for suspicious traffic) plus the ability to remote wipe it. So there is a degree of trust given to the developers. Those that are stuck with option 1 are usually the unhappiest and least productive. So it’s not something most businesses like to entertain unless senior management is very corporate and the business is considered high risk. reply tzs 4 hours agorootparentpreviTerm is a terminal program that only runs on MacOS. Unless those very strict controls include either blocking at the network level accesses to IP addresses not on a pre-approved list or removing a large number of programs and libraries that are standard on MacOS the person using iTerm will have plenty of readily available ways to use external services with that data if they want to. Heck, the free version of ChatGPT can tell you how to access ChatGPT with curl if you don't know how [1]. I specifically asked about curl, but it would also work if I asked it to suggest a MacOS command like tool: \"On MacOS what command line tools could I use to ask ChatGPT a question? Assume I have a ChatGPT API key\". It suggests curl, httpie, and wget and shows how to use all of them. I updated the link below to include that too. [1] https://chatgpt.com/share/00d9de15-4e43-497f-a116-bfe3972471... reply kstrauser 4 hours agorootparentprevBlock it with the same mechanisms you use to block web browsers from accessing the same content. If you do that already, you’re got. If you don’t, apparently it’s not that much of an issue. reply mr-karan 4 hours agorootparentprevSo, just because some corporate cannot keep up with their IT compliance team with new additions to existing tools in their stack (which is honestly a tedious job), should the OSS stop experimenting and shipping new 'optional' features? There are many developers who use iTerm2 outside of a corporate environment too. I, for one, developed clx[1], which is similar in nature to what iTerm's OpenAI integration is. I am happy to have this built right into my terminal. And the code is open source as well, allowing anyone to audit what actual external API calls are being made. [1]: https://github.com/mr-karan/clx reply ForHackernews 3 hours agorootparentprevWhat's good for the goose is good for the gander. None of these companies so much as bat an eye when applying the most sweeping, invasive data collection against their users. They should be equally thrilled to have spyware on their LAN that may enable more relevant workplace experiences for their employees. reply dcow 4 hours agorootparentprevConsidering the entire OS now supports deeply integrated ChatGPT, it’s abundantly clear that this was the case. I think George should revert this perversion of his product architecture security theater feature. It makes me angry that iTerm is a worse product now because of the anti-ai outrage. If you download this plugin, instead of the feature being securely integrated into the main product binary, there’s a new binary on your system that takes arbitrary JSON and performs network requests. Yes I know curl exists, but thats the point: we don’t need another tool for this that’s way less scrutinized and now opens my system up for data exfiltration in ways that weren’t originally possible. It also suffers from the traditional IPC pitfalls present when not using secure XPC with app groups. It’s objectively worse. Edit: I’m talking about the separate binary plugin when I call for George to revert. The secure defaults config that can be MDM managed is perfect, simple, and fit for purpose to secure iTerm as a product. Moving the network calls that talk to the configured openai-compatible api server a separate binary is a farce. reply rimliu 4 hours agorootparentAre you talking about macOS Sequoia? There will be not \"deep integration\" of ChagtGPT, in fact you get prompted when system wants to \"outsource\" a request to ChatGPT and you can refuse it. reply dcow 4 hours agorootparentDid you watch the freaking keynote??!! It’s everywhere and it goes off device in more cases than just ChatGPT. An opt in human in the flow UX is exactly how iTerm worked anyway, you have to check a box and actively send context to OpenAI and it’s driven off user action not some silent BG process. And it uses your token instead Apple’s enterprise account so better privacy (not that Apple hasn’t done good privacy things). reply wpm 4 hours agorootparentAnd every single one of those features can be hard, locked-down disabled with a simple configuration profile. They can all be unequivocally disabled. iTerms preferences could also be managed this way, since it hooks into cfprefsd and I could make an MCX profile to set “OpenAIIntegrationEnabled” to FALSE for the prefs domain “com.whateverIterms.bundleIDis”. Unequivocal. It’s off. You might say that I am not a bright man, and I might agree, but the way the AI integration presented in 3.5.0 was not unequivocal. Literally nothing said “this feature is disabled unless you put an API key in”. It assumed a knowledge and understanding of how this shit works, one that I do t have because I have no interest in slop portals in any of my applications, let alone one in my terminal. Instead of a checkbox for “Enable/Disable”, one I could have left set to disabled and gone about my day, I got an empty text box for an API key. So what, does it attempt to make a network call to Sam Altman’s slop machine every time I hit enter, only to fail without an API key? YeAh bUt Go rEaD tHe sOuRCE, sure, but I’ve been using iTerm since Tiger without a need to go read the source (nor become a terminal application developer in five minutes so I could understand it) and had other shit to do that day. Mind you, OpenAI.com is NXDOMAINed on my DNS servers at home, so I didnt give a shit either way when I upgraded to 3.5.0. People’s reactions and comments to the dev were wrong, cruel, and uncalled for, but that doesn’t mean the feature couldn’t have been introduced and presented in a way more sensitive to people’s concerns about AI, right or wrong, real or fake. And sure, the dev has every right to do whatever they want with their open source project. They don’t owe us any emotional intelligence or respect or anything, but they also dont have a right to expect everyone to be like, totally cool and vibin with whatever they do. That doesn’t mean “everyone will put up with whatever” (which is not the same as “everyone is entitled to the project”). If the dev removed all themes except for neon pink on neon green and forced your font to be Comic Sans, would the dev be entitled to do that? Sure! It’s not illegal and they don’t owe anyone anything. Would users be entitled to go “uhh, what the fuck?” Sure! It’s not illegal and they don’t owe anyone anything! It only gets gross when people start flinging insults. reply dcow 3 hours agorootparentSorry I should have been more clear. Adding a secure default is awesome (I forgot whether it was in 3.5.0 or not, because it was committed early enough that it was part of the discussion from the outset when all the outrage blew up, and it wasn’t enough). It should have stopped there. The plugin is a stupid idea that makes the application less secure and more of a flight risk. IF iTerm is going to have AI integration, THEN it should be the most simple and secure implementation, not a less secure one because some people think separate binary is a better look. Sounds like we’re on the same page about management capabilities. reply infecto 5 hours agorootparentprevI mostly agree with this but I believe there was a valid discussion around corporate use of the product. reply submeta 5 hours agoprevI just want to say thank you to the maintainers and developers of iTerm. It’s one of the tools I use most on my Mac, alongside Emacs, VS Code (heresy!), Obsidian, and Keyboard Maestro. So, thank you! reply kernelsanderz 5 hours agoparentFor those who have the funds and means, you can support the maintainer through their donation page here: https://iterm2.com/donate.html reply l5870uoo9y 5 hours agoparentprevApple should fund some of the most popular open source tools used on their Macs. They have the funds, its good marketing, it makes business sense. Nothing speaks against it. reply giancarlostoro 5 hours agorootparentThey should, especially any open source tooling directed at Swift. I don't understand why they're so slow at some things. Hell, make an open source foundation, I'm sure there's loads of tax write offs, I know Apple loves to save on taxes. reply jshier 4 hours agorootparentThey're kinda sorta starting down this path, as they announced a new GitHub org (swiftlang) this week, which will eventually be the home of Swift and all its related libraries, including the rewritten Foundation. This should allow outside contributors direct commit access much more easily, and allows the language to start moving more out of Apple's control. reply zeroimpl 4 hours agorootparentprevI don't think they're going to let you claim a donation to an open source developer as some sort of charitable donation, which means the tax write off would be no different here from any other business expense. reply maratc 4 hours agoparentprevSeconded. @gnachman if you're reading this: thanks for making our terminal life actually good, and please ignore the haters. reply svennidal 5 hours agoparentprevI don’t think I would enjoy my work if I didn’t have iTerm and Vim. reply CraigJPerry 5 hours agorootparentDoes iTerm give you anything over vanilla Terminal? To my eye there's a subtle delay, it must be in the order of 50-100ms in iTerm rendering that's not present in Terminal. Non-scientific measurement - i'm just going by perceived latency and comparing to something i know is a 50ms delay. That's for interactive use, startup time is slower for iTerm2 but i don't care about that because i basically never quit the terminal. In both iTerm2 (when i used it) and Terminal, i have a colourscheme enabled and a custom font - both of which i'm assuming have potential to slow things down. reply nrclark 5 hours agorootparentIterm2’s remote tmux integration is a killer feature for me. Gives you a native, normal “windows and tabs” kind of feel, but for a remote connection. You can also disconnect/reconnect without losing your windows and tabs. You don’t need to know any tmux to use it either (except for how to launch the right mode, which is easy to script). If you spend a lot of time SSHed to other computers, I would highly recommend trying it out. reply CraigJPerry 4 hours agorootparent>> Iterm2’s remote tmux integration is a killer feature But then i need to give up nested tmux on remotes https://github.com/craigjperry2/dotfiles/blob/main/dotfiles/... - it's super convenient to have tmux nested remotely for organising work and locally for broadcasting or just convenient context switching. Even when i was an iTerm2 user, i felt vanilla tmux was just way more comfortable (copy paste buffer for example) reply maratc 2 hours agorootparentiTerm has input broadcasting built-in (Cmd-Opt-I for \"on\", add Shift for \"off\"), and don't even get me started on the copy-paste: iTerm can copy multiple lines off your left pane without adding the contents of your right pane to the buffer. You can also have your left pane at 44 font size and your right pane at 8. reply maratc 4 hours agorootparentprev* Print IP addresses in green, MAC addresses in blue, the words \"error\" and \"fail\" in red, etc. * Recognize something in the output that looks like a Jira ticket and add a link to that ticket * Have your ssh passwords in one place and automatically enter them upon prompt * Connect to many servers and type the same command into all of them * Make your screen red when in superuser mode * etc. etc. etc. (Some years ago, I refused a company-issued ThinkPad Carbon and byod'ed a Mac because ThinkPad couldn't do iTerm2.) reply mrweasel 1 hour agorootparentprevI switched back to the build in terminal app a few years ago, because I don't feel like having my environment customized and live with the defaults for almost everything. The only thing I miss is having an easy way to switch between tabs in Terminal, you can do it cmd + , but Terminal doesn't give you the number on each tab, so you have to count yourself if you more than four or five. reply hylaride 3 hours agorootparentprevYou can save layouts (when I start iterm, it loads multiple windows on different monitors with split panes and tabs the way I like them). More advanced search with regex support, more advanced paste (can do character encoding transformations, deal with special characters, etc), smarter and configurable text selection, autocomplete (mixed bag, TBH - I use zsh for that), more advanced snippets for repetitive commands, and triggers to notify you when things happen (long running commands finish, certain words pop up eg \"error\" or \"compile done\"). It has a basic integrated password manager that allows me to paste passwords I commonly use in the terminal with a keycombo. It can more tightly integrate with the shell/program. You can select a point with the mouse in vim or the shell and the text cursor will go there, for example. Some of these may have since poked their way into the built in terminal, but these are some of the main reasons I use iterm. If you spend a lot of time in the terminal, you can enhance your productivity. reply nerdponx 4 hours agorootparentprevTerminal.app is definitely a little faster, but the appeal of iTerm 2 for me is better control over font, colors, and keyboard shortcuts. It also provides the ability to integrate the terminal with the shell much more deeply than Terminal.app, but I mostly don't use those features. reply galleywest200 5 hours agorootparentprevIf you are getting 100ms delay with iTerm then something may be wrong with your setup. reply CraigJPerry 4 hours agorootparentI'm pretty sure everyone (at least on M1 macs) are getting that 50-100ms delay. There's a perceptible lag vs Terminal.app - in the grand scheme of things it means nothing. But once you notice it... you can't unsee it! reply p_j_w 5 hours agorootparentpreviTerm2 allows multiple sessions in the same window and lets you tile them however you want. reply dewey 5 hours agorootparentBy sessions you mean tabs? The default one can do that too. reply maratc 4 hours agorootparentNo, split-screen your tab any way you want, and have any font or any size in any part of your screen. reply samatman 4 hours agorootparentprevQuake terminal. I switched to WezTerm awhile ago for my main terminal emulator, a decision I've been happy with. But I keep a copy of iTerm running so I can pull down the Quake terminal. Main uses are running homebrew updates, and, perhaps ironically given the topic of this thread, sgpt. reply BurningFrog 4 hours agoparentprevKeyboard Maestro is also an absolute gem, and very reasonably priced. reply submeta 4 hours agorootparentI could write a 200 pages book about how you can use Keyboard Maestro to automate all kinds of tedious things on a Mac. Something that takes 2-20 steps to do: Create a macro in it, trigger it via shortcut or by name. Boom. I have hundreds of macros. From opening an app to fully automating processes that would take minutes to get done manually. PS It can even record the keystrokes and play back. reply freedomben 5 hours agoprevI think it's important to recognize what actually changed here, because the headline makes it seem like a complete reversal, but it really isn't. It was always opt-in, but they made it more blockable/opt-in. The commit message is very useful here[1]. Excerpt: > This release adds some safety valves to eliminate the risk of private information leaving the terminal via the AI endpoints. While an API key and explicit user action were always needed to use AI features, some users asked for an impenetrable firewall for safety and regulatory purposes. [1]: https://iterm2.com/downloads.html reply PreInternet01 4 hours agoprev> While an API key and explicit user action were always needed to use AI features, some users asked for an impenetrable firewall for safety and regulatory purposes Yes, some users are truly experts at driving Open Source developers to the point of burnout. I use iTerm2, in pretty sensitive environments, where 'OK, you didn't enable this feature', closely followed by 'not that we had Internet access here anyway, LOL' were (though-experimentally, as are all the 'concerns' of 'some users') eclipsed by 'hey, why is it a thing to enter sensitive data on command lines anyway -- should we not have ways to avoid that?' reply mbauman 4 hours agoparentIronically, so many of these users are those _with corporate support_. And demanding \"IT\" support from the open source project. Without consideration. reply PreInternet01 4 hours agorootparentOh, sure. But for me, the lack of self-awareness in \"my command line inputs include extremely sensitive identifiers all the time, and this is fine, if it weren't for your optional AI plugins\" is especially grating. So, like, if I ever happen to execute 'history' in any session of yours that I manage to get access to, I hit the jackpot? reply eknkc 4 hours agoprevIt is weird that everyone has been bashing these features. What is all the negativity? I myself do not like pushing AI into everything but I think this is a great use case for LLMs. And it was already opt-in. Just tried \"find all pdf files larger than 10MB\" and it came up with \"find . -name \"*.pdf\" -size +10M\". Maybe this was easy but I don't know all arguments of all cli commands by heart and it works beautifully. reply wpm 3 hours agoparentTry it for macOS specific things and it chokes. It’ll hallucinate commands, send you shit for Linux, or give you stuff that worked 20 years ago. Find is an old, nearly universal command (good luck on any of the other GNU-utils commands that are 15 years newer than the binaries shipped in macOS). I have not found an LLM that knows any of that. When I need a find command, I open `man find` and read and learn. reply infecto 5 hours agoprevThis got way more hate then it deserved. I believe the real and valid discussion was how do corporate networks treat this. The outcome here makes a lot of sense in that regard. But all the negative hype around AI was too much. reply giancarlostoro 5 hours agoparentOne other comment on HN highlights why some of the anger is not completley unjust: https://news.ycombinator.com/item?id=40669333 Though I wont be surprised if the majority of the hate was people who didn't understand the change. reply dcow 4 hours agorootparentCorporate networks could always block the requests anyway… this is security theater to appease the anti-ai mob. reply add-sub-mul-div 5 hours agoparentprevIt makes you realize what a bubble sites like this one are, if this AI stuff is so wildly unpopular among the developer/iTerm community as a whole. reply miki123211 5 hours agorootparentIt's wildly unpopular among a different bubble of the developer community, mostly concentrated around Mastodon and the fediverse. Edit: It's worth noting that it's one of the most vocal and well-organized communities in the tech world, very prone to outrage, seeing the world in black-and-white, mob justice and piling on whatever is currently unpopular. reply add-sub-mul-div 4 hours agorootparentSo it's just axiomatic that people who disagree with you are coming from (any of) a list of fundamentally illegitimate motivations that you can think of? That makes life easier for you? reply phist_mcgee 4 hours agorootparentprevNo it isn't, how dare you reply lolinder 4 hours agorootparentprevDo you have any evidence whatsoever that this AI stuff was unpopular among the wider community? These kinds of cases where open source maintainers cave to pressure are almost never the result of consensus among all users. It's usually the result of a small number of people brigading the issue tracker until the maintainer gives up and does it to shut them up. Unfortunately the result is often against the interests of the wider community. reply coldtea 6 hours agoprevAmen! Speaking for apps but also of OSes, all that crap (AI integration, \"Abobe Cloud\" integration, and so on etc) should be not just opt-in, but also invisible once switched off (as opposed to some icon or banner nagging you about it in the UI, or ocassional popups asking you if you want to \"enable it\"). reply ta988 5 hours agoparentI think this was not the case here they weren't nagging about anything. And it wasn't working unless you did set up a token. But it is always the right move to refactor code like that into optional plugins especially for enterprise users where you want to be sure nothing could leak if a user had their personal token. reply coldtea 5 hours agorootparentFor iTerm yes, don't remember them nagging. But other apps and services (from Apple, Adobe, MS, etc) yes. reply pier25 4 hours agoparentprevThe Adobe Cloud stuff is featured prominently all over their products even if you've disabled it in Adobe CC. It's disgusting. reply sneak 5 hours agoparentprevIf you pirate Creative Cloud then use Little Snitch to ensure it never phones home, it never bugs you about the internet. Or so I heard from a friend. reply soraminazuki 3 hours agoprev> iTerm 3.5.1 removes automatic OpenAI integration, requires opt-in This is an editorialized title. It was opt-in from the very beginning. Here's all the steps that was originally required: 1. Open settings, go to the General tab, click on the AI button. 2. Enter a paid API key 3. Close the settings 4. Click \"Toolbelt\" on the menu bar, and click on \"Codecierge\" 5. Click \"Toolbelt\" on the menu bar again, and click \"Show toolbelt\" 6. In the toolbelt, there's a textbox that you can type questions into. The textbox won't be shown if you didn't enter an API key. Only after submitting the question will the OpenAI integration be activated, and as I understand it, only for the current session. https://github.com/gnachman/iTerm2/blob/a3122c0100d8900a15cb... The initial implementation already took many many clicks to run. I literally had to do nothing to not use the feature and not once was I reminded about the feature after I chose to ignore it. Despite that, people were spreading rumors that entering an invalid API key would instantly cause iTerm to send all data to OpenAI. It's a straight up lie started by people who actually tested the feature and posted their findings on the GitLab thread about this feature. https://gitlab.com/gnachman/iterm2/-/issues/11475 https://gitlab.com/gnachman/iterm2/-/issues/11470 It gets worse, people in the GitLab thread were calling for dogpiles and fantasizing about inflicting violence on Mastodon. Towards the sole maintainer of a popular free and open source software developed in his spare time. https://web.archive.org/web/20240613165712/https://archive.i... Some of the things you see online... I have no words. reply joshstrange 4 hours agoprevI'll say the same thing I said [0] on yesterday's thread [1] about this: I didn't care for the feature (I have no issues with AI/LLMs but it just wasn't useful IMHO) but the backlash was ridiculous and embarrassing for everyone complaining about an opt-in feature. The comments on the GitLab ticket and here on HN were examples of some of the worst people (or people at their worst) in our industry. [0] https://news.ycombinator.com/item?id=40658290 [1] https://news.ycombinator.com/item?id=40657890 reply spmurrayzzz 4 hours agoparentThe grandstanding definitely got in the way of otherwise legitimate feedback/concerns about the feature, as sadly tends to be the case in open source software when the mob descends. reply frou_dh 5 hours agoprevReacting to the AI kerfuffle is all in a day's work for Mr. Nachman. He also fixed an unrelated bug I reported by the time I got up the next day. What a beast! reply kelsey98765431 4 hours agoprevI actually am a large supporter of AI enhanced development workflows and I hated this feature. I use macos SOLELY for iterm2 because of tmux integration. I was very excited for this feature. You have to literally click the button. It's a command Y to bring up the modal, you start typing, then you have to remove hands from keyboard and mouse to the confirm button. What the hell? No command enter, no command shift enter, maybe I missed it or did it wrong but it is literally faster to type pipx run llm prompt than control y my command mouse and click. Just fix that one part and I would have been in heaven, also the pop up modal seems like a bad choice when it could have been directly integrated into the shell with the new overlay they introduced alongside the feature. Bad implementation, and PLEASE if anyone knows any foss alternative to iterm2 with tmux integration please dress me down on the fool i have been and steer me towards the path of the light again. reply jpitz 4 hours agoprevThe headline implies that \"requires opt-in\" is new to 3.5.1. that's not the case. The OpenAI integration always required opt in and an API token to function. Can we fix the headline? reply nottorp 5 hours agoprevI'm curious. If you do opt-in, what does it send to OpenAI? Everything you type, including passwords, tokens, pasted keys, internal IP addresses etc? reply kernelsanderz 5 hours agoparentNo, there's a configurable system prompt which is: Return commands suitable for copy/pasting into \\(shell) on \\(uname). Do NOT include commentary NOR Markdown triple-backtick code blocks as your whole response will be copied into my terminal automatically. The script should do this: \\(ai.prompt) And then you type your prompt in, and it returns the answer. And then you can choose to edit the command that gets returned or execute it directly. So essentially what you'd do if you were using the API directly, just more convenient. reply bearjaws 4 hours agoprevIs there a good local llm that can be used for helping with CLI commands e.g. \"how do I kill all processes using port 9000\" I haven't tried any, but if one exists it would be cool to see iterm use that LLM. reply kstrauser 4 hours agoparentI started playing with ollama + codellama recently and it’s been fun and easy to get running. reply ChrisArchitect 4 hours agoprev[dupe] More discussion: https://news.ycombinator.com/item?id=40657890 reply dathinab 4 hours agoprevIt's a grate example of people listening to their community (less grate for how they where treated). Anyway thanks for listening and changing things. reply kelsey98765431 4 hours agoparentHello! I really appreciate the sentiment you shared and just wanted to let you know that \"grate\" should be \"great\" in this context. I am assuming you didn't notice while using a speech to text tool or you are not a native english speaker. No hate, love to have you here, not trying to be the spelling police but your sentiment is very kind and appreciated so i thought i would let you know that a grate is a type of metal grill used mostly in filtering large masses from liquids, such as a drainage grate for flood waters. Enjoy your day and please keep sharing your positive outlook! reply swiftcoder 6 hours agoprevnext [26 more] [flagged] throw10920 5 hours agoparentYou're wildly mistaken about what's going on - the feature was already opt-in. The feature literally didn't work unless you went into settings and manually entered an OpenAI key. That is literally the definition of \"opt-in\". reply ceejayoz 5 hours agorootparentThe linked release notes appear to be appropriate tweaks; a blockable plugin and needing admin rights to enable are important adjustments for corporate environments. Just because I opt-in doesn't mean my company wants me to opt-in on their behalf. reply kgeist 5 hours agorootparentNo one stops me from opening chatgpt.com in the browser and pasting corporate stuff there. Could'nt admins just ban all access to chatgpt.com and openai.com when accessed from the corporate network? That would be a solution for all software, not just iTerm. reply ceejayoz 5 hours agorootparentThey should, but defense-in-depth is a thing. reply throw10920 5 hours agorootparentWhat's the threat model? reply ceejayoz 5 hours agorootparentA few off the top of my head: iTerm starts sending the calls through a proxy for billing/analytics; iTerm adds a model selector that hits other APIs in an update. reply mb389 5 hours agorootparentprevchatgpt in your browser doesnt have access to your entire file system reply hiatus 5 hours agorootparentIf your browser has access to your entire file system, chatgpt can have that access, too. reply swiftcoder 5 hours agorootparentThere's a reason various major browser vendors have refused to implement the FileSystem APIs... reply hiatus 4 hours agorootparentI don't get your point here. There is an interface to upload files in chatgpt. reply ceejayoz 4 hours agorootparentDeliberately doing that is a lot more actionable than \"oops I had no idea iTerm would sendoff to OpenAI\". reply throw10920 5 hours agorootparentprevYes, these are useful improvements for corporate management of software - but that's not what most of the outage was about. Also, if there wasn't a firewall that would have blocked iTerm from hitting the OpenAI APIs, then there wouldn't have been anything preventing users from just directly accessing it. reply swiftcoder 5 hours agorootparentprevEnabling the feature was opt-in, yes, installing the feature was not (and has been made opt-in with this new update). Apparently a lot of folks don't regard this as an critical distinction, but as someone working in a regulated industry, there is a distinction here that is worth understanding. reply kstrauser 4 hours agorootparentI was the CISO at a HIPAA-covered healthtech company and the opt-in feature as it was wouldn’t have bothered me at all. It adds nothing to the threat model that you don’t already have with curl. reply throw10920 5 hours agorootparentprevIf your industry doesn't block stuff like this at the network level, then the \"regulation\" is a joke. reply swiftcoder 4 hours agorootparentBlocking at the network level is of course necessary, but it's hardly sufficient. If tomorrow your app starts directing traffic through a secure proxy, we'd have no reliable way to detect that - this is why software designed to operate in high-security environments needs built-in policy controls. reply throw10920 4 hours agorootparent> If tomorrow your app starts directing traffic through a secure proxy ...then the app's developers have already shown themselves to be malicious and they wouldn't respect a feature toggle in the first place. reply swiftcoder 2 hours agorootparentAnd they can demonstrate the opposite by implementing policy controls - as they have done here reply bdcravens 5 hours agoparentprevWhat was being boycotted? Their revenue? Developers choosing to not use their terminal? reply swiftcoder 5 hours agorootparentA bunch of us didn't install the update that included GPT integration. From the release notes, it appears that it was enough of a dip in updates to prompt corrective actions from the developers reply bdcravens 4 hours agorootparentIs that communication and feedback, or a boycott? For a boycott to be effective, you have to be taking something away from someone. In the case of free software, the work has already been done, for free. reply walthamstow 5 hours agorootparentprevAs far as I can tell the 'boycott' was nothing more than rolling back to 3.4 reply swiftcoder 5 hours agorootparentThat is indeed how one usually boycotts software updates. What else would it mean? reply bdcravens 4 hours agorootparentUninstalling and moving to an alternative. In the case of open source, forking the project. reply lenerdenator 5 hours agoparentprevThey work on people without the resources to sustain operations without income. Sadly, the people that most need boycotting have effectively unlimited resources. reply dcow 4 hours agoprevIn the face of the Apple Intelligence macOS Sequoia announcement yesterday, anybody who is still trying to argue that moving the web request to a separate binary makes the product more secure or compliant is absolutely disingenuous and frankly malicious. reply blinkingled 4 hours agoprev [–] Got rid of it, switched to Kitty. I will miss the password manager from iTerm but that's ok - I can chill knowing AI won't upload my passwords someplace. reply camdenreslink 3 hours agoparentCouldn't you have just not opted-in to using this feature? It requires you entering your creds to work at all. reply kstrauser 4 hours agoparentprev [–] Not one for reading before making decisions, are you. reply blinkingled 4 hours agorootparent [–] That'd be you sir because the change log says and I quote - > This release adds some safety valves to eliminate the risk of private information leaving the terminal via the AI endpoints. If it's still not clear - I don't want my terminal to even have the possibility of leaking my data. reply soraminazuki 3 hours agorootparentYou just omitted the very next sentence. > While an API key and explicit user action were always needed to use AI features, some users asked for an impenetrable firewall for safety and regulatory purposes. reply blinkingled 3 hours agorootparentSo you are saying I can rely on API key being always required by AI overlords and there would be no point in the future where somehow things leak out without needing the key? You see if you don't meddle with my terminal data in the first place I would feel much better that I am in control of my data, and not reliant on. 3rd parties to accidentally drop guard and leak my data out. reply soraminazuki 3 hours agorootparentNow you ignored everything past the first four words of what I quoted. It doesn't \"meddle or leak\" your terminal data. OpenAI integration only kicks in after explicit user action. Looking at the screenshots, it appears to only become active for the current terminal pane. https://github.com/gnachman/iTerm2/blob/master/images/Onboar... Also, iTerm is developed by an actual respect-worthy person who actually cares about this kind of stuff, not by an \"AI overlord.\" It's unfortunate that you instantly chose to make baseless spyware accusations in public forums instead of supporting him for creating the software that you've relied on. reply kstrauser 4 hours agorootparentprev [–] It didn’t before, either. This is theater to appease people who didn’t understand how the opt-in feature worked in the last version. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "iTerm2 3.5.2 is the latest stable release, recommended for macOS 10.15 and newer, built on June 13, 2024.",
      "The latest beta version, iTerm2 3.5.1beta4, is available for testing, built on June 3, 2024, with frequent updates but occasional instability.",
      "Nightly builds are created daily if changes are committed, but they may contain serious bugs; the latest and older builds are available in the archives."
    ],
    "commentSummary": [
      "The iTerm 3.5.1 update removes automatic OpenAI integration, now requiring users to opt-in, sparking debate among users.",
      "Concerns were raised about potential data security risks in corporate environments, even with the opt-in feature, highlighting the need for strict network-level controls.",
      "The backlash against the AI integration led to criticism of IT professionals for not properly vetting software updates and allowing auto-updates without sufficient oversight."
    ],
    "points": 125,
    "commentCount": 131,
    "retryCount": 0,
    "time": 1718281642
  }
]
