[
  {
    "id": 40190542,
    "title": "R. Bradley Lathe: A Symbol of Human Ingenuity",
    "originLink": "http://www.lathes.co.uk/bradley-pow-lathe/",
    "originBody": "email: tony@lathes.co.uk Home Machine Tool Archive Machine-tools Sale & Wanted Machine Tool Manuals Catalogues Belts Books Accessories The R. Bradley Lathe - Home-made in a Prisoner of War Camp - If you think making a lathe at home is difficult, this might just encourage you to have a go. This is a true testament to human ingenuity. Next page missingemail: tony@lathes.co.uk Home Machine Tool Archive Machine-tools Sale & Wanted Machine Tool Manuals Catalogues Belts Books Accessories The R. Bradley Lathe - Home-made in a Prisoner of War Camp -",
    "commentLink": "https://news.ycombinator.com/item?id=40190542",
    "commentBody": "A small lathe built in a Japanese prison camp (1949) (lathes.co.uk)439 points by CommieBobDole 15 hours agohidepastfavorite116 comments navane 15 hours agoI'm only part into the story, but I already love it. The prisoners-of-war were tired hiding their lathe every time they might be searched, that they hung up a sign \"workshop\" above one of their huts, and timed it so that the new round of guards thought it had always been there. reply paulgerhardt 13 hours agoparentFor an extended read, you may try \"King Rat\" by James Clavell - same author of the current hit show \"Shogun\". The book is set in the same Changi PoW camp where the author was held during WWII. The character Lieutenant-Colonel Larkin is based off Lieutenant-Colonel E. E. \"Weary\" Dunlop who was the real life surgeon using these tools for creating artificial limbs amongst other things and whom Clavell knew and presumably Bradley too. It's not exactly a cheery read but very much an inspiring one in terms of survival, ingenuity, and moral complexity. reply navane 13 hours agorootparentThanks, I've read it, don't remember the lathe in it. Awesome book. So much better than shogun or any of his other works from his \"trader series\". I recommend King Rat to people who need to get back in to reading books. reply wddkcs 10 hours agorootparentHow dare you slander The Noble House. I love most of his works, but Straung is peak Clavell. reply sifar 9 hours agorootparentJoss ! reply satiric 9 hours agorootparentprevIt's not specifically about Changi, but you might also be interested in the book \"Rats of Rangoon\" by Lionel Hudson which is all about the author's experiences during WWII in the Rangoon prison camps (also run by the Japanese of course) reply BlueUmarell 14 hours agoparentprevAs I understood, one of the japanese officer's hut - which is even bolder! Hence why they tricked the translator to have the japanese character for \"workshop\". A lot of smart and bold moves all along. Especially as the japanese were known for their \"harsh\" treatments (humiliations, beatings, torture, slow death, brutal death etc etc) toward prisoners, anything that would lead to a cue that they were doing something hidden would have had a radical and definitive answer... reply dan-robertson 10 hours agorootparentI assumed it was the hut used by the PoW officers. Japan did not treat PoWs particularly well (though British PoWs were treated much better than Chinese) and did not follow the 1929 Geneva Convention on the treatment of PoWs. Under that convention, officers are to be treated with due regard to their rank and part of that means being quartered separately from the other ranks (as well as not being made to work). I think officers would also be separated in PoW camps in Japan. reply darkwater 3 hours agorootparentI don't fully understand why a convention that allegedly tries to protect human rights makes a so bold difference just based on job positions (aka ranks). I mean, I do understand where it comes from, and especially when it applies to prisoners of war, so military forces. They basically can only think in hierarchies so designing a system that works with hierarchies will have more chances to be actually followed. But still, I find it weird. reply rsynnott 0 minutes agorootparentBecause it's from 1929. The past is a foreign country, and all that. Beyond the 'militaries like hierarchy' angle, there was a huge _class_ aspect there. avar 1 hour agorootparentprevBecause it was written and enacted primarily by recently warring western nation states, not a group of detached philosophical monks living on a mountaintop. The elites in those societies had a clear interest in carving out special privileges for themselves, which is why officers receive preferential treatment. The 1929 Geneva convention has other things you may find objectionable, such as Section II, article 9 (paragraph 3)[1]: Belligerents shall, so far as possible, avoid assembling in a single camp prisoners of different races or nationalities. Which is there for obvious reasons. Can you imagine the horror of being housed in a racially unhomogenous camp? People in 1929 sure could. 1. https://avalon.law.yale.edu/20th_century/geneva02.asp#art9 reply teshigahara 13 hours agoparentprevI find it hard to believe that they copied the word for workshop (presumably 工房) convincingly enough that it wasn't obviously written by someone without any understanding of how to actually write the language. It's extremely obvious when someone tries to copy Chinese characters without any understanding of stroke order, stroke pressure, etc. The way that someone would show how a character looks to someone without any knowledge (ie textbook form) and how they would naturally write such a sign is also different. You would be able to tell instantly that a non-native prisoner wrote it. Actually, signs were also written right-to-left horizontally during that period but it's likely that someone showing them how to write on a piece of paper would have written vertically, so they would probably not even have the knowledge to know the correct order of the text. reply OJFord 45 minutes agorootparent> obviously written by someone without any understanding of how to actually write the language. It's extremely obvious when someone tries to copy Chinese characters without any understanding of stroke order, stroke pressure, etc. Or someone who is not a professional wooden sign carver, perhaps? I'm natively familiar with English writing, but if I carved 'workshop' I'm not sure it would look any better than someone imitating me, nor obviously like I'd used correct 'stroke' order. > You would be able to tell instantly that a non-native prisoner wrote it. And that might not be suspicious at all anyway? reply iaseiadit 13 hours agorootparentprevIf all signs in the prison camp were written right-to-left instead of vertically, they probably would have noted that before creating the sign. Especially considering their lives depended on it. reply teshigahara 9 hours agorootparentIf you can't read the signs how would you know it's right-to-left? You are only seeing two unknown characters, you don't know which comes first. It's not about vertical vs horizontal. It's that someone who speaks English would assume that all of these signs they can't read are written left-to-right, and write the vertical characters they are copying in the wrong order. reply fenomas 4 hours agorootparentThis is such a facially bizarre thing to contest. They got a Japanese NCO to write the text, and presumably copied it as he wrote it. Why imagine that the NCO wrote it vertically and the soldiers horizontally? Likewise the article doesn't suggest that anyone thought the sign was written by a native speaker; why even imagine that's a requirement? I mean, consider this in the abstract - the objections you're making here rest on the implicit assumption that you know more about the realities of life in a Japanese POW camp than TFA's author. (After all, if he fabricated the story about the sign he'd obviously fabricate it so as to be consistent with his experiences in the camp.) Do you really think that premise is more likely than the alternative - that TFA's extremely brief telling of the story simply doesn't include whatever details would answer your objections? reply kybernetikos 1 hour agorootparentMost likely the circumstance they got the Japanese NCO to write the text in was a conversation about learning Japanese and how to write it too. Nobody is deliberately trying to stop them from learning to write, they are most likely in favour of it, the trickiness was just around avoiding the Japanese running the camp from noticing their interest in workshops specifically. If stroke order is important in this context then I expect the Japanese NCO showing them the characters would have told them and explained the proper order. reply fenomas 33 minutes agorootparentI don't think the writing matters a whit. The only premise the story depends on is: that the camp guards saw the workshop and took it for granted that it was approved by somebody, since it was orderly and operating openly. If you accept that, it doesn't matter if the sign was amateurish or even upside-down - it would just look like something the workers had been told to make, or had made themselves to test the tools or to pass the time. A bunch of posters here seem to be imagining that the sign was the lone keystone of the ruse, and that for some reason it needed to look like it was written by a native speaker or else the whole plan toppled. But nothing in TFA suggests that, it just says the sign was one of several things the POWs did to make the whole setup look like it had approval to be there. reply j16sdiz 8 hours agorootparentprevYou knew it is rtl when you see a paragraph is aligned to right reply teshigahara 8 hours agorootparentNo, this doesn't make sense in the context of Japanese. One of these signs is written right to left: https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQdiwLY... https://auctions.afimg.jp/h1085974646/ya/image/h1085974646.2... Which one is it? There is no way to tell unless you already know the characters. Unless someone could read the existing signs they would almost certainly assume they were left-to-right and make any new sign like that if they only had the characters to copy. reply freeopinion 6 hours agorootparentI appreciate what you are pointing out here. I agree with you that getting it just right would be a challenge. Did you happen to see the lathe? I ask you, which would be more difficult to get right in the smallest detail? While most Allied soldiers would not be literate in Japanese, that doesn't mean they would all be completely ignorant, either. It just takes one to know enough to ask about character order. While I agree that it was high risk, I'm willing to believe the people who were present when they say they pulled it off. Sometimes we dodge bullets without even knowing they were fired. reply livueta 5 hours agorootparentAs someone who both a) does precision fab work as a hobby and b) made the somewhat unfortunate decision to memorize many thousands of kanji without caring about stroke order: it's harder. Sorry. 100% agree with the parent: even though I can read Japanese at a fairly advanced level, having not properly learned stroke order is a massive bitch. I can't handwrite for shit, and that's obvious to me and anyone else who can read Japanese of any degree of complexity. It is so many orders of difficulty above \"ask[ing] about character order\" that I can't even begin to verbalize what a category difference of difficulty it is. Handwriting Japanese that looks correct to a native reader assumes years of naturalization. reply freeopinion 4 hours agorootparentI already agreed that writing kanji without years of practice would be very hard to make look native. But they said they did it and it worked. Maybe it was obviously not native and it didn't matter. I don't know. But I'm not going to say they lied about making the sign. Can we agree that it seems improbable that they fooled anybody about who drew the sign and also agree that they got to keep their workshop and their tools and have an amazing (and true) story? reply sidewndr46 11 hours agorootparentprevIt's a prisoner of war camp. All the signs are made by prisoners. reply teshigahara 9 hours agorootparentIt would be much easier for a random Japanese soldier who actually knows the language to just write on the few necessary signs than trying to direct a prisoner to do so, who will probably end up making mistakes and make it almost illegible. This just sounds like a nice explanation but it's unlikely to be the case. reply marshray 5 hours agorootparentI'd be kinda surprised if they let outsiders do their calligraphy for them. reply eszed 11 hours agorootparentprevAnd yet it worked.... Your knowledge of Japanese orthography gives you an interesting perspective. I'd be fascinated to know, given the obstacles you note, how exactly the prisoners overcame them. Did they have someone in the camp with basic knowledge of Chinese orthography? Did someone know enough to note carefully the way in which the characters were written? Did they keep the paper with the characters on it, and then hand-reproduce the precise structure? Were the guards generally illiterate, and therefore not notice the errors? All of those would be spurs to further research, which your reflexive dismissal of the premise would preclude. An open-minded approach to historical texts usually generates more-interesting questions and answers than a closed one. reply icambron 11 hours agorootparentAn alternative possibility is that many other the signs around the camp were made by prisoners over the normal course of their labor and thus this one did not need to hide its authorship. The deception is in acting like it was always there and was supposed to be, not in pretending its was physically written by an official. reply nxicvyvy 11 hours agorootparentprevOr do the guards just not want to speak out of line or question their superiors. Or do the guards all know but don't care because things are being fixed up around the place. Or are all the signs in the camp created by prisoners? So much is unknown about the situation to make the claims made above. reply riehwvfbk 7 hours agorootparentOr another (and I think the most likely possibility, given what we know about human nature): one of the guards ran a profitable little side business selling basic machined parts in town made with free labor. In exchange, the prisoners got to make stuff they needed also. Only the high-ranking prisoners were in on the scheme. The rest were told the story about \"deception\", which is what we see relayed here. reply bobthepanda 9 hours agorootparentprevThe camp in this article is located in Changi, in Singapore. Singapore has always had a large Chinese population (it actually was originally in Malaysia upon that country's independence but got kicked out for being too Chinese). It would be surprising if not a single one was familiar with some Chinese writing. reply CrazyCatDog 8 hours agorootparentThey did, the translator communicates to the prisoners in English, and they pass along in Japanese to the guards. The article says they asked the translator. reply bobthepanda 8 hours agorootparentThey didn’t know the word which is different from not having any knowledge of how Chinese characters are written. Chinese is not 1:1 with Japanese so that’s not surprising. reply brazzy 10 hours agorootparentprevDid they have someone in the camp with basic knowledge of Chinese orthography? This is definitely a possibility, but even then... > Did someone know enough to note carefully the way in which the characters were written? Did they keep the paper with the characters on it, and then hand-reproduce the precise structure? This would be unlikely to work, because the characters would be written on paper using a pen or pencil, which produces quite different strokes that a brush, which is what you would have to use for a sign. Even if you know how brush strokes should look like, I can't really say how difficult it would be to produce brushwork that credibly looks like what someone would produce who has been doing it all their adult life, if you lack the experience. reply simonh 12 hours agorootparentprevFor handwriting sure, but print characters as for a sign would be easier. reply brazzy 11 hours agorootparentHow would they print a sign? reply krisoft 9 hours agorootparent“Print characters” is a style of the characters. You can still hand paint them, they don’t have to be printed to be in that style. Look at this image for example: https://www.ideastream.org/community/2022-09-01/making-it-ol... That is clearly a hand painted letter but is using a printed style (as opposed to a cursive one) Japanese writing has the same distinction. These letters on the sign are in printed style: https://www.flickr.com/photos/japanesepod101/3706680254/ The shapes are simplified and regularised. Compared with these caligraphy style letters: https://www.flickr.com/photos/12567713@N00/70734240/in/photo... reply teshigahara 8 hours agorootparent\"Print characters\" aren't hand painted on signs and you will rarely see it written in any context outside of extremely old books. There's no such thing as \"print characters\", anyway. Presumably you are referring to 明朝体 (although that sign you linked is actually 丸ゴシック, which is much more recent). Besides, even if it was written down like that it will still be incredibly obvious. It would be like if you had your child try to copy Times New Roman and pass it off as the real thing. It's actually harder than writing normally unless you have a stencil. reply krisoft 1 hour agorootparent> It would be like if you had your child try to copy Times New Roman and pass it off as the real thing. Copying a shape exactly is absolutely possible. (Any shape.) Yes most people doesn’t have the skills to pull it off, but then again they are also not making lathes in captivity (or at all). > It's actually harder than writing normally unless you have a stencil. Then you make a stencil. > There's no such thing as \"print characters\", anyway. It will be hard to convince me about that when my eyes can see it. You can choose to not understand what I’m saying. > Presumably you are referring to 明朝体 I assure you it is not called that in English. reply brazzy 10 hours agorootparentprevI had the same thought, but as the other responses note, there are many possible explanations. Yet another one: maybe some of the prisoners actually knew basic Japanese? It would be a very useful skill in their situation, and learning the basics of how to write kanji is not that hard. It wouldn't be calligraphy, but it just needs to look good enough that it might just be sloppy writing. reply MeteorMarc 14 hours agoparentprevYeah, social engineering for establishing a workshop! reply gwern 11 hours agoprevMade a PDF with the missing page: https://gwern.net/doc/cs/security/1949-bradley.pdf reply iaseiadit 13 hours agoprevSome photos from the camp here: https://www.rnz.co.nz/national/programmes/eyewitness/galleri... Photo of men with artificial limbs built in the camp: https://www.awm.gov.au/collection/C4416 A wireless set hidden in the sole of a prisoner's sandals: https://www.awm.gov.au/collection/C14187 reply mhh__ 6 hours agoparent> Please note: > This website contains names, images and voices of deceased Aboriginal and Torres Strait Islander peoples. > This website contains war-related material, including images which some people may find confronting and disturbing. how trite reply oliyoung 4 hours agorootparentThe indigenous warning isn't \"trite\", it's a culturally sensitive practice that most Australians are used to seeing. reply OJFord 40 minutes agorootparentI have no horse in this, but that sounds more like an argument that it is trite? reply kragen 6 hours agorootparentprevtraditional aboriginal people might not want to be exposed to that and close the web page rather than continuing. it's a taboo similar to certain kinds of photographs in cultures you may be more familiar with reply EnigmaFlare 4 hours agorootparentIt's not really out of concern for people's cultures. Lots of cultures have all sorts of taboos about types of images or information. Muslim fundamentalists for instance don't like pictures of any people, of any ethnicity, alive or dead. This is just a nonsense fashion for the Australian government. All their websites have something like the one at the bottom of the page: \"The Australian War Memorial acknowledges the traditional custodians of country throughout Australia. We recognise their continuing connection to land, sea and waters. We pay our respects to elders past and present.\" reply defrost 4 hours agorootparentIt makes a change from chaining aboriginals neck to neck which they kept up after I was born, along with taking babies, and having exclusion zones \"boundary roads\" in cities. Is that the era you want to return to? reply EnigmaFlare 4 hours agorootparentI'd prefer the happy middle ground between glorifying a race and chaining them neck to neck. reply flir 1 hour agorootparentLack of a warning is \"glorifying\" (odd word choice, I'd maybe go with \"foregrounding\"?) the culture that doesn't need the warning. reply defrost 4 hours agorootparentprev> how trite How gammon. reply concordDance 53 minutes agorootparentI don't quite understand how you think a racist/colorist slur is acceptable here. reply defrost 46 minutes agorootparentIn context here \"trite\" would be the term diminishing recognition of particular race issues, and \"gammon\" is the term disparging a particular attitude within the UK and doesn't target all with a particular genetic background or skin colour. As an Australian I don't find the \"trite\" comment that diminishes recognition of indigenous contributions to the commonwealth war effort to be acceptable here and labelled that comment as gammon. Curiously, in addition to the UK term which derives from a ham cut, there's an Australian indigenous term common in NSW and the NT with an unrelated etymology that also works here to a degree. reply class3shock 15 hours agoprevWow, as someone that loves vintage machine tools and owns a Monarch 10EE (http://www.lathes.co.uk/monarch/index.html) it's funny to see this little corner of the internet find its way here. This is one of those sites that has information likely not found anywhere else online and few places offline. reply varjag 14 hours agoparentNice one! I liked the carriage rollers 10EE has so much that I made similar ones on my Chinese lathe instead of finicky gibs it came with. reply class3shock 13 hours agorootparentSounds like a fun project. How do you find the lathe? I always preferred old American iron to something new from China/Taiwan but having to haul around two tons of lathe when you move is not a trivial task. reply varjag 11 hours agorootparentHeh right nobody buys the mini lathe over a Monarch as a matter of preference! I got it when I wanted to learn some machining and didn't have a ton of space. Over the years had a few mods including the usual (tapered bearings on the spindle, 4\" 4 jaw chuck…) and some funny ones. It's an okay machine but the design isn't rigid enough, it really needs the double amount cast iron for the bed given its swing. Still I did a lot of precision work on this that came really handy professionally later, when interacting with MEs and toolmakers. It looks really banged up now, and I had to change the leadscrew that became visibly worn. Funny enough the much derided Derlin change gears look pristine! reply jdietrich 13 hours agorootparentprevTaiwanese lathes are generally good, with some being truly excellent. The common Chinese benchtop lathes are best regarded as a kit of parts - they're crude, but can be made into a satisfactory machine with some fitting and fettling. reply hi-v-rocknroll 8 hours agoparentpreva. Unmarried, b. lots of room, or c. very understanding wife unit permits a man cave? reply class3shock 8 hours agorootparentHaha, I am unattached but have no space to speak of sadly (a large home shop is a future goal, just need the home first along with the very understanding wife). It lived first in a friends garage, then a machine shop of another friend, and now in yet another friends garage (in another state). If you are trying to imagine all of that moving about I encourage you to do it with the Benny Hill theme tune playing, that should properly set the tone. reply flir 1 hour agorootparentlooks at 230kg wood lathe mutters darkly My wife practically forced me to get the damn thing. She's not the one that has to get it into the back of the van. reply hi-v-rocknroll 7 hours agorootparentprev:D Nice. I'm also working towards that goal of a man cave with neon and gold Schlitz sign and a pool table next to the milling machine, table saw, drill press, and central vac. 70's-90's rock playlist sets just about the right mood for making stuff. reply class3shock 6 hours agorootparentThat's the dream. I have access to a nice makerspace to tide me over for now. I don't know if you follow the youtube machinist community at all but you may find some enjoyment in This Old Tony, Ave, Tom Lipton, or many of the others in it if the home shop bug has you. reply burcs 14 hours agoprevThis is fascinating really interesting to see how these are built first hand. My father-in-law is one of the only companies still building speed lathes and it's basically the same lathe they have built since 1937. I'm pretty sure their customers range from SpaceX to Pharma co's and they are just a small shop in midwest PA. reply avhon1 13 hours agoparentThese lathes? https://www.crozierspeedlathes.com/ https://www.youtube.com/watch?v=NzaOsPcaoi4 It looks to me like they're for hand-finishing (deburring or polishing?) parts. I wonder in what kinds of work that comes up often enough to justify dedicated machines? reply class3shock 13 hours agorootparentAerospace, semiconductor, etc., basically anywhere you have precision round parts. You can debur alot by hand but not so much external diameters which are often done on these machines for low volume / prototyping / reworking / etc. reply burcs 9 hours agorootparentprevHa you found them, yeah that's his company. reply pjc50 1 hour agoprevPreviously on HN: https://news.ycombinator.com/item?id=29645825 - a whole radio built in a POW camp, from scratch. reply acyou 3 hours agoprevLovely article. But I have my doubts. Were these prisoners really outwitting their guards, or might it have been the other way around? Is not instructing their captors in engineering subjects, navigation and astronomy, which all have substantial military applications, clearly disloyalty to the Allies at the time? Is this not just a story about how the Japanese were able to gain military intelligence on Western manufacturing and machine shop techniques from these brilliant craftsmen with their \"secret\" lathe? We have all seen the movie The Great Escape and various other prisoner/POW movies, but that's not how things were. Assuming that the bunkhouses were separate from the workshop facilities, and fully under Japanese control, I assume that the Japanese were able to inspect each and any of these marvelous technical works in progress at their leisure. The most cynical take is that it appears no effort was spared in sharing decades or even centuries of technological progress and advancement in various technologies with the Japanese, who at the time were at war with the Allies. In other words, you can be pretty smart, but pretty stupid. Or, as a POW you just need to do what you can to survive, and you're also smart enough to rescue yourself from court martial at the end. reply dpe82 2 hours agoparent> Is this not just a story about how the Japanese were able to gain military intelligence on Western manufacturing and machine shop techniques from these brilliant craftsmen with their \"secret\" lathe? Seems pretty unlikely; simple machine tools as described in this article were already an old and widely known technology at the time. reply Daub 6 hours agoprevMy father showed me how to use a lathe when I was around 14. He warned me to never use it unless he was present. Of course I ignored him. The very first time I used it, my shirt sleeves caught in it and my shirt was ripped from my body in a second. I Never touched it again. Moral of the story: respect lathes. reply ehnto 6 hours agoparentRotating assemblies in general, the amount of energy at play is often very deceptive. Ask anyone who has caught a finger in a spinning bicycle wheel. reply yboris 6 hours agoparentprevI wonder if he'd be more persuasive if he explained the reasons for not using it without supervision with graphic examples of danger. reply Daub 5 hours agorootparentNow the internet is full of examples of such accidents. One glance of those would have been enough. reply flir 59 minutes agoprevWhat I really want to know: where is it now? reply tvb12 5 hours agoprevMachining metal parts is pretty loud. How was any of this done in secret? reply defrost 5 hours agoparentCamp cooperation - time it with choir practice and|or any other loud activities, cheering during staged boxing matches, use lookouts to warn when guards get close, work under beds with blankets, sheets thrown over to drown noise, etc. reply Doxin 3 hours agoparentprevUsing a lathe isn't super loud unless you're doing it poorly or have some horrendous interrupted cut. reply miohtama 13 hours agoprevUnrelated to excellent article content, do we have today an AI solution to enhance the quality and readability of scanned PDFs? reply gield 13 hours agoparentOCR has been around for a few decades by now: https://nl.wikipedia.org/wiki/Optical_character_recognition reply dan-robertson 9 hours agoparentprevI think the solution here is not so much AI as finding some library which has the magazine and making new scans. That would also help with the hard-to-solve-with-ai problem of the missing page. reply iancmceachern 13 hours agoparentprevIt doesn't necessarily need to be AI, it could be a simple filter, etc reply notjulianjaynes 11 hours agoparentprevI'd also be curious about this as I am presently dealing with some PDFs with both handwriting and poorly scanned type. I can read and understand these documents but the text is not recognized using tesseract OCR. reply flir 1 hour agorootparentMy use-case was easier than yours (OCRing scans of microfiche of typewritten pages) but I found this https://github.com/Stirling-Tools/Stirling-PDF incredibly handy. I still had to write a bit of Python, mostly for levelling and centering the text blocks, but it really is a PDF swiss army knife. reply pbhjpbhj 10 hours agorootparentprevI've been looking at handwriting recognition offerings and signed up for ChatGPT [1] to trial access for this task against Irish genealogical records, which it does quite well on. However, ChatGPT itself recommended that I use Transkribus [2]. That looks to be quite an esoteric interface, but my initial test suggests it's pretty good -- I've not made a proper comparison yet as I'm wanting to have a workflow of downloading images, transcribing them and maybe compiling them into a PDF with manual corrections. [1] an 'expert' like this but it wasn't this one I used, can't locate it right now as am on a different computer, https://chat.openai.com/g/g-LCtICWzCt-historical-handwriting... [2] https://www.transkribus.org/ reply blackeyeblitzar 14 hours agoprevI wonder if there’s any resource that shows how to build all these tools from scratch. What would it take to bootstrap manufacturing? reply qup 13 hours agoparentYou're looking for the Gingery Lathe. He builds it and bootstraps a home metalworking shop, building new tools with the old ones. It's a book series. reply ajot 9 hours agoparentprevYou could take a look at the Multimachine, an open source combined lathe, mill and drill press from the early 00s web forums. I first saw it mentioned in the Open Source Ecology project ~10 years ago. https://opensourcemachine.org/ reply _whiteCaps_ 11 hours agoparentprevDepending on how far you want to go, you might like to read The Knowledge by Dartnell. It covers smelting, etc. reply RecycledEle 8 hours agoparentprev> I wonder if there’s any resource that shows how to build all these tools from scratch. https://gingerybookstore.com/ reply jojobas 10 hours agoparentprevThat's likely impossible as all easily accessible coal and iron ore are long gone. Maybe if you have access to scrap metal. reply WillAdams 8 hours agorootparentPretty easy to make charcoal from wood. The first book in the Gingery series is how to set up an aluminum foundry for casting. reply jojobas 7 hours agorootparentAluminium casting (assuming already metallic aluminium) is vastly simpler than getting iron/steel from ore. Sure you could remelt scrap metals, but in \"deep future\" where all metals have corroded away you seemingly can't restart industries on Earth, our only option is not to let ourselves collapse. reply kragen 6 hours agorootparentcorroded metal is ore, copper takes geological time to corrode, and getting iron from ore is hard work but not rocket science. reply jojobas 5 hours agorootparentI guess major equipment items weighing many tons could be used as ore deposits, but most iron is not that. Your nails/screws/whatever are pretty much unrecoverable once corroded. In other words, humans are not an exception to the global entropy growth. reply defrost 4 hours agorootparentNews today, two killed in Texas when 158 tonnes of steel rolls off the back of four low loaders - a single massive heavy guage pipe segment with in line pressure vessels and bleed off tee pieces likely destined for a oil|gas processing plant. Point being, there are massive metal deposits in industrial ares and mineralenergy processing plants - raw girders, fleets of 100 tonne+ trucks, crushers, screen sections, etc. Of some interest, one of many papers* on adaptive metal work by \"stone age\" people https://www.asha.org.au/pdf/australasian_historical_archaeol... turning horseshoes into spear points, etc. Might even be the same paper that describes people making their own forges from scratch to work recovered iron. Yes, things corrode, but not all things, some get isolated from oxygen and found again. * Not the one I was thinking of, apologies - there's a good one I'll have to work to find that goes into more detail and range on \"outsider\" metal working. reply flir 1 hour agorootparentThere's use of meteoric iron from pre-iron-age societies, too (most famously https://en.wikipedia.org/wiki/Tutankhamun%27s_meteoric_iron_... and see \"Bronze Age\" on this page https://en.wikipedia.org/wiki/Meteoric_iron) reply greenavocado 10 hours agorootparentprevCoal is easily accessible on eBay https://www.ebay.com/itm/121215404611 reply jojobas 9 hours agorootparent\"The early Mediterranean iron smelting started around 12th or 11th century BC. It is assumed the pioneering metallurgists sourced their coal off eBay.\" reply dylan604 9 hours agorootparentBut that came to a grinding halt when the seller's PayPal account was closed for suspicious activity. This is also believed to be the reason the Roman formula for concrete was lost. reply jojobas 9 hours agorootparentThe breakdown of ancient metalworking supply chains was caused by what later became known as Sea Peoples, but was actually a misheard reference to the CEO of Paypal. reply hi-v-rocknroll 8 hours agorootparentprevYes. This is also how gods gave humans fire, don't you know? There are many trade-offs between purity of original work, processed state of original inputs, effort, availability, and cost. Perhaps folks in small towns with just an Ace Hardware or Homeless Despot lack access to suppliers in say Richmond, Virginia. reply jojobas 8 hours agorootparentRight, Prometheus got caught shipping dangerous goods and was blacklisted by eagle de-livery. reply wizzwizz4 15 hours agoprevOne of the pages is missing. I wonder whether anyone has a copy of it? reply avhon1 14 hours agoparentIndeed, someone does! TFA is a scan of a reprint of the original article. The reprint is \"The Machinist's Bedside Reader\" by Guy Lautard. The missing page is pg. 159. The Internet Archive has a copy which can be checked out and read for free here: https://archive.org/details/machinistsbedsid0000laut edit: the book is also currently in print, and can be purchased new on Amazon for $50 https://www.amazon.com/dp/1953439063 reply card_zero 15 hours agoparentprevYes, it's here: https://archive.org/details/machinistsbedsid0000laut/page/15... reply peteradio 14 hours agoprev [10 more] [flagged] Rinzler89 14 hours agoparentInstead, they were trying to get lathed. I'll see myself out. reply 082349872349872 14 hours agorootparentWhen your bed is true and all is aligned, you can hope to get a little tailstock? reply class3shock 14 hours agorootparentIf nothing else it'll help you tap a hole properly. reply tejtm 12 hours agorootparentBORING reply peteradio 11 hours agorootparentBOROI OI OI OING reply brcmthrowaway 11 hours agoparentprevChrist what is going on here reply huytersd 14 hours agoparentprev [–] That just supports my argument that men shouldn’t have to try to get laid. It should be provided for free by the state. “If you can’t afford your own wife, one will be provided to you by the state”. reply 082349872349872 13 hours agorootparent [–] When Diogenes was reproached for public indecency, he replied \"if I could get rid of hunger by rubbing my belly, I'd do that too\" reply Rinzler89 13 hours agorootparent [–] giga_chad.jpg reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The website lathes.co.uk offers a variety of machine tools for sale, including manuals, catalogues, belts, books, and accessories.",
      "A notable highlight is the R. Bradley Lathe, which was crafted in a Prisoner of War camp, demonstrating human creativity and resourcefulness.",
      "For further inquiries, contact tony@lathes.co.uk for more information."
    ],
    "commentSummary": [
      "The discussion delves into a small lathe constructed by prisoners of war in a Japanese prison camp in 1949, referencing the Geneva Convention of 1929, Chinese characters' authenticity on signs, and the role of lathes in metalworking.",
      "It covers metalworking tools, lathing techniques, the treatment of prisoners of war, and the difficulties encountered in clandestine metal parts production.",
      "Participants exchange personal anecdotes, metalworking resources, and engage in debates on interconnected topics, including state-provided sexual partners for men."
    ],
    "points": 439,
    "commentCount": 116,
    "retryCount": 0,
    "time": 1714328186
  },
  {
    "id": 40187656,
    "title": "Reinventing Zilog's Z80 CPU: A Modern Open-Source Silicon Clone",
    "originLink": "https://github.com/rejunity/z80-open-silicon",
    "originBody": "Announcement On April 15 of 2024 Zilog has announced End-of-Life for Z80, one of the most famous 8-bit CPUs of all time. It is a time for open-source and hardware preservation community to step in with a Free and Open Source Silicon (FOSS) replacement for Zilog Z80. GOAL: To develop a drop-in Z80 replacement in 8-bit home computers such as ZX Spectrum. The first fabrication of FOSS Z80 is scheduled for June of 2024! Zilog Z80 modern free and open source silicon clone On the path to become a silicon proven, pin compatible, open-source replacement for classic Zilog Z80. FOSS Z80 leverages OpenROAD flow and FOSS 130 nm Skywater PDK to synthesize production ready silicon. Tiny Tapeout infrastructure is used to test and pool design with many others to reduce the cost of physical chip fabrication at Skywater Foundries. The first iteration of FOSS Z80 silicon The first iteration is developed with Tiny Tapeout 07 using 130 nm process and fits on a 0.064 mm2 die area. The first fabrication is scheduled for June of 2024 as a part of CI 2406 Shuttle. The implementation is based around Guy Hutchison's TV80 Verilog core. Read documentation for Tiny Tapeout 07 version Below is the image of GDSII integrated circuit layout for FOSS Z80. It is the result of automatic place-and-route flow in OpenROAD using 130 nm \"gates\" logic elements. Further Plan / ToDo Add thorough tests of the Z80 instruction set (including 'illegal' instructions) ZEXALL to testbench Add thorough tests for timing of input/output signals Compare different implementations: Verilog core A-Z80, Netlist based Z80Explorer Tapeout with ChipIgnite in QFN64 package, create a PCB adapter from QFN64 to DIP40 Tapeout with DIP40 package Create gate-level layouts that would resemble the original Z80 layout, see the original chip dies below. Zilog designed Z80 by manually placing each transistor by hand. Z80 Pinout ,---------.__,---------.--> CLK |6 35| A5 -->D4 |7 34| A4 -->D3 |8 33| A3 -->D5 |9 32| A2 -->D6 |10 31| A1 --> VCC |11 30| A0 -->D2 |12 29| GNDD7 |13 28| /RFSH -->D0 |14 27| /M1 -->D1 |15 26| /RESET/INT |16 25| /BUSRQ/NMI |17 24| /WAIT`--------------------- Documentation Z80 Users Manual Z80 Users Manual from Mostek Zilog Data Book All the information about Z80 Undocumented instructions Opcode table and timing Oral History of the Development of the Z80 Oral History Panel on the Founding of the Company and the Development of the Z80 Microprocessor M. Shima on Demystifying Microprocessor Design Z80 Patents (expired) Patent US4605980 -- input voltage spike protection (expired) Patent US4332008A -- ??? (expired) Patent US4486827A -- reset circuitry Z80 Die shots How to \"read\" die shots nMOS variant Z8400 with 'Zilog 75' marking and Zilog Z8400 with 'DC' letter marking CMOS variants Zilog Z84C00 and its 8MHz version Nintendo Z80 variant from Super Game Boy SGB-CPU 01 produced in 1994 Sean Riddle's image of Mostek MK3880 clone metal layer removed Pauli Rautakorpi's images of Z80 clones: National Semiconductor NSC800, Mostek MK3880, MME9201 with 'U880/5' markings Zeptobar’s images of Zilog Z0840004PSC from 1990, Soviet KR1858VM3 with an uncommon layout, MME Z80A a clone on a 5um technology larger than the original Zilog chip, Soviet KR1858VM1 a clone of U880/6 which is in turn a clone of Z80, Soviet T34VM1 based on U880/5 Z80 Reverse Engineering Z80 Instruction Register deciphered Z80 Tri-stated Data & Address bus gates Z80 (un)documented behavior The instruction decode PLA in the Z80 microprocessor Why the Z-80's data pins are scrambled How the Z80's registers are implemented The Z-80's 16-bit increment/decrement circuit reverse engineered The Z-80 has a 4-bit ALU XOR, the silicon for two interesting gates explained WZ aka MEMPTR, esoteric register of the Z80 Existing Z80 implementations TV80 in Verilog https://github.com/hutch31/tv80 TV80 in Verilog https://github.com/Obijuan/Z80-FPGA A-Z80 in Verilog https://github.com/gdevic/A-Z80 its overview and details Z80 net-list level emulator https://github.com/gdevic/Z80Explorer and its overview and Users Guide What is Tiny Tapeout? Tiny Tapeout is an educational project that aims to make it easier and cheaper than ever to get your digital designs manufactured on a real chip. To learn more and get started, visit https://tinytapeout.com. Resources FAQ Digital design lessons Learn how semiconductors work Join the community Build your design locally",
    "commentLink": "https://news.ycombinator.com/item?id=40187656",
    "commentBody": "Zilog Z80 CPU – Modern, free and open source silicon clone (github.com/rejunity)342 points by jnord 22 hours agohidepastfavorite64 comments Renaud 22 hours agoWhat Tiny Tapeout is doing is amazing. Who would have thought that makers and students could have their own chip design made real for so little money? The tools look amazing as well. You'll won't design the next Intel CPU on that 130nm process but to think that the Z80 will fit on 0.064 mm2 is just amazing. It's great that there will still be an alternative to the official chip now that it won't be manufactured any more. Now I want that gorgeous mauve ceramic package with a gold-plated cover over the chip... https://twitter.com/l_vanek/status/1783557817133039738/photo... https://tinytapeout.com/ reply ashleyn 19 hours agoparent130nm process puts them at roughly Pentium III era. Not bad! reply skywal_l 16 hours agorootparentActually more of a Pentium IV. Pentium III: 250 nm to 130 nm [0] Pentium VI: 180 nm to 65 nm [1] Which is indeed amazing. [0] https://en.wikipedia.org/wiki/Pentium_III [1] https://en.wikipedia.org/wiki/Pentium_4 reply rvense 17 hours agorootparentprevThat's wild. A Pentium III would still be useful in a pinch. How big was a P3 die, though? reply Someone 17 hours agorootparenthttps://en.wikipedia.org/wiki/Pentium_III#Katmai: “The Katmai contains 9.5 million transistors, not including the 512 Kbytes L2 cache (which adds 25 million transistors), and has dimensions of 12.3 mm by 10.4 mm (128 mm²). It is fabricated in Intel's P856.5 process, a 250 nm complementary metal–oxide–semiconductor (CMOS) process with five levels of aluminum interconnect” That’s 2,000 times the area of this 0.064 mm² Z80. https://en.wikipedia.org/wiki/Pentium_III#Tualatin: “The third revision, Tualatin (80530), was a trial for Intel's new 130 nm process” I can’t easily find the die size if that. reply unnah 16 hours agorootparentOn that kind of process, you could make a 1024-core Z80 machine, leaving half the area for memory, interconnect and I/O. With suitably smart programming and an embarrassingly parallel problem, it might even be able to beat a Pentium III in performance... although it looks like the single-core Pentium III can run 128-bit SSE instructions at 2 cycles per instruction. reply Someone 10 hours agorootparentSuitably smart programming and a problem that suits the hardware. I doubt there are many of the latter. A Z80 has a 4-bit ALU (https://en.wikipedia.org/wiki/Zilog_Z80#Microarchitecture), making even integer addition take quite a few cycles (15 for 16-bit addition, reading http://www.z80.info/z80time.txt) And then there’s the clock speed difference. The first Pentium III ran at 450MHz, the fastest Z80 at 50MHz (https://en.wikipedia.org/wiki/Zilog_eZ80) I think those two combined already will cost you a factor of around 100 in speed versus that pipelined Z80, much more versus a Z80 proper. Things get worse if you want to add or subtract 32- or 64-bit integers (another factor of 2 or 4, ballpark) If you want to do integer multiplication and division of any size and all floating point operations you will have to do those in software, and likely lose whatever speed advantage you might still have. O, and each core will be limited to 64kB of memory. Those interconnects better be fast and use DMA, so you can keep computing while you shuffle data around. reply unnah 1 hour agorootparentYes, there are definitely challenges, and there must be a reason no one is seriously selling 1024-core versions of old 8-bit processors... but perhaps on an Intel 130 nm process you could make a faster Z80 than just 50 MHz. Quick googling didn't reveal what process Zilog is currently using for eZ80. reply chx 21 hours agoparentprevTo save a click > 160 x 100 um tile + ASIC + demonstration board: The standard price is $300 plus shipping. > However, Efabless is sponsoring a special early bird offer of $150 (plus shipping), limited to one order per person. > Each extra tile is $50, and extra analog pins start from $40 per pin. Unless I am badly mistaken 160 x 100 um is .16 x .1 mm which means the tile is 0.016 mm2 meaning a 0.064 mm2 die takes four slots? reply mk_stjames 17 hours agorootparentYes it is taking up a 2x2 tile on the tinytapeout. https://app.tinytapeout.com/projects/668 reply rowanG077 16 hours agorootparentprevI could not find the pin capabilities. Is it possible to build an sdram controller or even drive ddr? reply robxorb 20 hours agoprevFor those wondering, the 6502 and various derivatives are still being manufactured, by one of its original creators [0] - so I'd guess an equivalent development in the world of the Z80's nemesis is unlikely anytime soon. [0] https://www.westerndesigncenter.com/wdc/chips.php reply monocasa 18 hours agoparentInterestingly the classic z80 was end of lifed just two weeks ago. https://hackaday.com/2024/04/19/end-of-life-for-z80-cpu-and-... reply Fatnino 14 hours agorootparentWhat will the ti-83/4 calculator use now? reply UncleSlacky 13 hours agorootparentThe eZ80, which is still being produced: https://en.wikipedia.org/wiki/Zilog_eZ80#Use_in_commercial_p... reply ksherlock 20 hours agoparentprev65*C*02, which might fail if your 6502 code depends on illegal opcodes or incidental memory access or BCD math cycle timing or BCD math flags or perhaps decimal mode being set in an interrupt routine. reply grumpyprole 18 hours agorootparentThe 65C02 came out in 1983, so I think we've had plenty of time to document and workaround these issues! reply giantrobot 18 hours agorootparentHey, don't touch my space heater![0] [0] https://xkcd.com/1172/ reply polpo 17 hours agoparentprevIt could happen at any time to the 65C02. The Z80 was only EOLed a few weeks ago because they couldn’t get wafers from their fab any more. Any chip on an old process is at risk of this. reply sitkack 17 hours agoparentprevThe PDIP version is being discontinued, but the eZ80 is still being manufactured. https://arstechnica.com/gadgets/2024/04/after-48-years-zilog... https://en.wikipedia.org/wiki/Zilog_eZ80 http://www.zilog.com/docs/um0077.pdf https://www.zilog.com/docs/ez80acclaim/ps0153.pdf reply RetroTechie 18 hours agoparentprevIt would be interesting to know Zilog's sale volumes for discrete Z80s (say, over the past decade). What uses they were purchased for, and DIP/PLCC/flatpack ratios. There must be millions floating out there. But with distributors like Mouser or Farnell gone, for anyone looking to buy some, it's eBay & co which tends to be a crapshoot. reply lelanthran 15 hours agorootparent> It would be interesting to know Zilog's sale volumes for discrete Z80s (say, over the past decade). Not the past decade, but two decades ago (2005) the z80 was still popular. At work, I was working on a product based on, IIRC, a Rabbit Semiconductor product, which was a module with on-chip ethernet. It was a Z80 running at 40Mhz. Personally, I also had a little siemens organiser thing, that also was z80 based (not sure of the actual specs). I recall trying to write programs for it and failing (may not have been open; no way to reprogram or download new code to it, maybe). [EDIT: The organiser was a siemens IC35] reply Fatnino 14 hours agorootparentAren't ti-83 calculators still sold today using z80? reply ndiddy 11 hours agorootparentAny Z80 based (not eZ80) TI calculators on the market today have the Z80 core built into an ASIC instead of a discrete chip, meaning that they wouldn’t be impacted by parts availability. reply zczc 19 hours agoparentprevAnd eZ80 (binary compatible, but not pin-compatible with Z80) is still being manufactured by Zilog. reply belter 21 hours agoprevZ80 was the CPU of the ZX Spectrum. Oh the memories... https://en.wikipedia.org/wiki/ZX_Spectrum reply LeFantome 7 hours agoparentIt was also the CPU in my first computer--the Coleco ADAM: https://en.wikipedia.org/wiki/Coleco_Adam I still have my copy of Programming the Z80 that I got as a kid: https://en.wikipedia.org/wiki/Programming_the_Z80 reply ztetranz 11 hours agoparentprevAnd of course the TRS-80 and clones such as the Dick Smith System-80 that we had in Australia and New Zealand. Lots of good memories programing with EDTASM. I only had a cassette drive so if my code went wrong I usually had to hit reset and reload EDTASM and my code again from tape. reply Andrex 8 hours agoparentprevI thought it was used for the Game Boy, but apparently while there are many similarities they're basically incompatible[0]. 0. https://forums.nesdev.org/viewtopic.php?t=18335 reply userbinator 13 hours agoparentprevAlso the many unbranded MP3/\"MP4\" players that very widespread in the mid to late 2000s: https://en.wikipedia.org/wiki/S1_MP3_player reply mattl 20 hours agoparentprevSo many good machines: the Amstrad CPC range, a whole slew of Sega consoles, the early MSX stuff and of course the Tatung Einstein. 3 inch disk machines of the world unite! reply flohofwoe 18 hours agorootparentAll 8-bit computers manufactured in East Germany too (via the reverse enginered Z80 clone U880). For instance the KC85/2..4: https://floooh.github.io/virtualkc/p010_kc85.html An \"Adrian's Digital Basement\" episode about the KC85/3: https://www.youtube.com/watch?v=At9UNYFHuaE reply mchannon 18 hours agorootparentprevYou forgot about Donkey Kong (and Junior, and 3, and Mario Bros.) Punch-Out!! But most noteworthy is Galaga: Ran on 3, count 'em, 3, Z80's. reply flohofwoe 17 hours agorootparent...also Pacman btw (the original arcade machine), and other 80's arcade machines like Pengo or Bomb Jack (notably Bomb Jack was two Z80 computers duct-taped together, the sound was handled by a separate Z80 board which controlled three AY-3-8910 sound chips). reply rwmj 19 hours agorootparentprevNot to mention the world of CP/M business machines which was surprisingly large in the UK well into the late 80s, again thanks to Amstrad: https://en.wikipedia.org/wiki/Amstrad_PCW reply yincrash 14 hours agorootparentprevMy mobile computer of choice in school, the TI-83 series calculators. reply Fatnino 14 hours agorootparentWhich still goes for full price today despite using an EOL chip. reply rsynnott 13 hours agoprevWow. Just looked it up; the Z80 is now _50 years old_. reply userbinator 13 hours agoprevI wonder how compatible it is with the original Z80, which had many undocumented instructions as well as the infamous \"trap gates\" (look at the \"Oral History Panel on the Founding of the Company and the Development of the Z80 Microprocessor\" documented linked on that page) that might've had an effect on certain obscure instruction sequences and designed to identify the difference between it and clones. reply Koshkin 20 hours agoprevI couldn’t help but notice that the circuit layout looks like a uniform gate array rather something resembling a custom layout you usually see in die photos. reply flohofwoe 17 hours agoparentBecause it's a Verilog implementation which is much closer to a software CPU emulator than the real thing (e.g. it has nothing to do with the original Z80 \"transistor layout\"). For instance here's the LD A,(DE) \"instruction payload\": https://github.com/rejunity/z80-open-silicon/blob/974c7711b2... And here's the same machine cycle in my software emulator: https://github.com/floooh/chips/blob/bd1ecff58337574bb46eba5... Both set the address bus to the content of the DE register (and at the same time the MREQ|RD pins need to be set somewhere to indicate a memory read to the outside world, in my emulator this happens in the _mread macro), and in the next clock cycle load the data bus into the A register. What's interesting though is that the Verilog implementation doesn't seem to update the internal WZ register with DE+1, which makes me wonder if undocumented behaviour is correctly implemented, but maybe updating WZ is handled elsewhere (there are references to the WZ register in other places). In the end, if it looks and feels like a Z80 from the outside (e.g. the right pins are active at the right time) the internal implementation doesn't matter. reply Vogtinator 32 minutes agorootparentIt only looks like that if you treat it as a C-like programming language, which it is not. Unless specified otherwise, all the statements are \"executed\" in parallel by the synthesized logic. There is no emulation. reply rowanG077 14 hours agorootparentprevJust because this is done in verilog doesn't make it emulation. It's probably just machine placed and routed. Almost everything is these days in digital design. reply flohofwoe 14 hours agorootparentBut look here, it's even doing a switch-case over the opcode, which is very typical for a software CPU emulator: https://github.com/rejunity/z80-open-silicon/blob/974c7711b2... Instruction decoding on a real Z80 CPU doesn't work at all like that :) A non-emulator-approach would probably use the reverse engineered Z80 netlist from visual6502.org to base the design on, no idea if this is even doable with modern chip design tooling(?) If anything, the netlist is useful to verify the Verilog implementation (as is mentioned here in the readme: https://github.com/rejunity/z80-open-silicon?tab=readme-ov-f...) reply flaghacker 9 hours agorootparentThat switch-case gets optimized and compiled down to logic gates by the synthesis tools. It'll be a different set of gates from the original netlist (which might also have used a more regular grid structure for this), but it won't be _that_ different. It's not somehow running this switch-case in software emulation on a different CPU instantiated in this design. reply codebje 9 hours agorootparentprevInstruction decoding on a real Z80 CPU works pretty much like that, as it happens. There's a big PLA table that takes the IR inputs and a handful of other control signals (like \"is 0xED prefixed\") and lights up output control signals to say what the instruction to be executed is. See https://static.righto.com/files/z80-pla-table.html for this table laid out nicely. Verilog isn't imperative code, to be executed one line after another in sequence. It's a description of combinatorial logic to be wired up to inputs and outputs, gated by a clock edge. Everything in the Verilog module \"runs\" at the same time, there's no jumping to a branch, there is instead logic to wire up one \"casez\" block or another to the relevant output signals. All the blocks are lit up, only one has its output selected to connect to the output wires. The PLA block is more convenient to a hardware engineer laying out a CPU by hand. You can see everything together and trace execution easily. Downstream consequences of decode are done elsewhere. Upstream decode of control signals are done elsewhere. The Verilog is more convenient to a hardware engineer relying on tools to route logic: the Verilog does more than the PLA - it does the additional control signal inputs, and it does the downstream consequences like determining which register(s) are used on which register bus. It's laid out more like a software decode of the instruction bits because it's easier to think about groups of opcodes than individual ones. In execution, though, they wind up doing very similar things. reply rowanG077 13 hours agorootparentprevSo what? A re-implementation of a CPU doesn't require the netlist to be equal. That would mean just moving to a new process node or tooling suddenly means your new brand new CPU is \"software emulating\" the old one just because it might do somethings slightly differently. A frankly ridiculous proposition. reply tcbawo 19 hours agoprevI had heard about Z80’s 4-bit ALU (2x for 8-bit math). Is this considered a major bottleneck? Were there later extensions that added higher bit integer math? I’m curious whether an open source version of the chip will enable new features and variants. reply flohofwoe 19 hours agoparent> Is this considered a major bottleneck? No, because an ALU instructions with a register as source is already running as fast as possible (at 4 clock cycles, which is the duration of an opcode fetch 'machine cycle'). Or from a different perspective: an 8-bit ALU wouldn't have made math instructions faster, but would have cost twice as many transistors. The 4-bit ALU is just an internal implementation detail that isn't visible to the outside (except maybe through the existence of the half-carry flag which indicated a carry from the lower into the higher nibble). And if you want a CPU replacement that plugs directly into old home computers, the CPU needs to have the original instruction timing, otherwise software that depends on 'cycle counting' won't work (probably less of an issue on the ZX Spectrum though because the Speccy didn't have a programmable video hardware like for instance the Amstrad CPC). The eZ80 is a modernised and more efficient design, with (among other things) a wider ALU: https://en.wikipedia.org/wiki/Zilog_eZ80. Not an option for keeping old home computers alive though, for this you'd want an exact Z80 clone with the original timings and undocumented behaviour. reply becurious 15 hours agorootparentCycle counting was key on the Spectrum - for obvious things like the tape load routines but also for advanced techniques like the ‘Rainbow processor’ - updating the attribute bytes (those responsible for the infamous color clash) as each scan line progressed you could get different colors on each scan line. reply RetroTechie 11 hours agorootparentOnce made a tape-loading like pattern, and tried to get it as stable (not moving up or down on screen) as possible. Managed to produce a program where with key presses, you could change delay in the loop in +/- 1 clockcycle increments. Mind you: fastest Z80 opcodes take 4 cycles. How then? Well, there's also opcodes that take 5 cycles. Or 6. Or 7. And 8=2*4, 9=4+5, etc. Program just automated the insertion/removal of those in the inner loop. Of course I had to pick instructions that didn't mess with some Z80 registers. Great fun (& educational) figuring out stuff like that. Fun times... reply flohofwoe 14 hours agorootparentprev> ...updating the attribute bytes (those responsible for the infamous color clash) as each scan line progressed... Ah clever! Didn't think of that. Probably the closest thing to \"racing the beam\" since the Atari 2600 :) reply userbinator 13 hours agoparentprevThe Netburst P4s also used a half-wide (16-bit) ALU running at 2x the clock frequency (actually by clocking on both edges, like DDR RAM), which meant ALU operations with a carry/borrow between the two halves took an extra cycle: https://www.realworldtech.com/isscc-2001/7/ reply Retr0id 21 hours agoprevDoes anyone know what clock speeds we might be able to expect from this? reply drmpeg 21 hours agoparentThis page says 50 MHz. https://github.com/rejunity/z80-open-silicon/blob/main/docs/... reply swetland 16 hours agorootparentThat's the expected clock rate for the TT07 run... but Tiny Tapeout designs only have 8 in, 8 out, and 8 bidirectional IOs (plus a reset and clock input) available, so they're using a multiplexing strategy where the Z80 clock runs at 1/4 of the base clock rate and alternates between control signals, A0-A7, control signals, and A8-A15 on the OUT pins: https://github.com/rejunity/z80-open-silicon/blob/68438f0019... So you'd get an effective 12.5MHz Z80 clock and need a bit of external logic to demultiplex the full IO interface. Still not too shabby! The goal (per the project README) appears to be to prototype with TT07 and then look into taping out standalone with ChipIgnite in QFN44 and DIP40 packages (which would be able to have the full traditional Z80 bus interface and run at the full clock rate). reply tyingq 17 hours agorootparentprevInteresting. Saw this on the Wishbone Z80 project notes: \"Guy Hutchison (see TV80 project) has synthesized an early version of the core in a 130nm TSMC process. He determined the design to contain about 20k gates and run at about 240 Mhz. While the speed is somewhat less than \"target\", optimizations of the logic should increase this somewhat.\" Guy Hutchison's TV80 is also mentioned on this project's page. reply Dwedit 14 hours agoparentprevIf you were designing a new compatible processor for older systems, the limiting factor would be the memory bus. A cache would be necessary to get high speeds. The cache would need to know about all bank-switching performed by the system, and understand how the memory banks are mapped into the memory space. Could have: * Plain read-only memory (you cache this) * Plain RAM not shared with other devices (you cache this) * Memory-mapped IO (you don't cache this) * RAM shared with other devices where the other device does not write there, such as video memory (write-through cache, full read cache) * RAM shared with other devices where the other device can write there (don't cache this) reply Retr0id 11 hours agorootparentIIRC this is what the people making swap-in accelerators for 6502 chess computers are doing reply Someone 3 hours agorootparentBecause it’s easy to ‘cache’ the entire memory of the host system, it’s better described as a new computer that only slows down to access the memory locations that affect I/O (video, audio, keyboard, I/O ports, etc.) https://www.e-basteln.de/computing/65f02/65f02/: “The idea is to use this as a “universal” accelerator for 6502 and 65C02-based host computers – just plug it into the CPU socket. The only thing the FPGA board needs to know about its host is the memory map: Where does the host have memory-mapped I/O? Up to 16 different memory maps can be stored in the FPGA, and selected via a mini DIP switch. Upon power-on, the 65F02 grabs the complete RAM and ROM content from the host and copies it into the on-chip RAM, except for the I/O area. Then the CPU gets going, using the internal memory at 100 MHz for all bus accesses except for any I/O addresses – for these, the internal CPU pauses, and an external bus cycle is started at whatever the external clock speed is.” reply kumarski 16 hours agoprevLooks dope. (I was on early efabless.com team) open source EDA. reply phendrenad2 17 hours agoprevI wonder if this will ever be priced competitively with the massive number of used or NOS Z80 chips out in the wild. reply ein0p 16 hours agoparenteBay says a Chinese Z80 clone is less than $4 with free shipping. This isn’t even going to be competitive with lower end FPGAs. It’s more of a fun “why not” type of project. reply mindcrime 13 hours agoprev [–] So, um... you're saying I should not have dumped my life's savings into ordering Z80 chips as part of the \"last time buy\"? :-( reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Zilog has declared the End-of-Life for the popular 8-bit CPU Z80, spurring the open-source community to work on a Free and Open Source Silicon alternative for it.",
      "The aim is to develop a replacement compatible with 8-bit home computers like the ZX Spectrum, with the first production of FOSS Z80 expected in June 2024, utilizing OpenROAD flow and a 130 nm Skywater Process Design Kit.",
      "Efforts include comprehensive testing of the Z80 instruction set, exploring various implementations, and generating gate-level designs resembling the original Z80 layout, offering additional resources and documentation for potential project contributors."
    ],
    "commentSummary": [
      "The discussion delves into the availability and design of a contemporary open-source Z80 CPU clone, along with exploring the feasibility of developing a low-cost chip utilizing Tiny Tapeout's tools.",
      "It compares the Z80 processor to modern processors, highlights its limitations, and addresses challenges faced when utilizing a 1024-core Z80 machine.",
      "The significance of the Z80 processor in historical devices/systems, Verilog implementation of a Z80 CPU emulator, emphasis on cycle counting in Z80 programming, and considerations for developing new processors compatible with older systems are covered, with a mention of regret over prior investment in Z80 chips and a request for guidance on future steps."
    ],
    "points": 342,
    "commentCount": 64,
    "retryCount": 0,
    "time": 1714302225
  },
  {
    "id": 40189682,
    "title": "Have Modern Tools Compromised Software Efficiency?",
    "originLink": "https://rufatmammadli.medium.com/did-we-lose-our-way-in-making-efficient-software-30-mb-doc-file-vs-browser-fed12dd866a4",
    "originBody": "Apollo computer DSKY user interface unit (Wikipedia) Did we lose our way in making efficient software? — ~30 MB doc file vs browser Rufat MAMMADLI · Follow 2 min read · 17 hours ago -- 1 Yesterday, my father told me he needed to install Microsoft Word on his laptop to work on his doc file. He also wanted to transfer his doc file to his work computer to continue to work on it when he is at the office. I thought a web app might be good for him. I showed him the Google Docs option because he already has a Google Account, and it is easier to use, cloud-based, and syncs automatically. I set up his doc file in Google Docs, and… When I type anything in the doc file, it takes seconds to see it on screen. The doc file size was ~30 MB, and contains some images, and simple tables but mostly text. Unfortunately, Chrome and/or Google Docs couldn’t handle it. He didn't want to pay for Microsoft Office. So, I installed LibreOffice for him. And I tested the doc file on it. It was quicker than The Flash. After this, I kept thinking about today’s software standards. Are we going backward about performance-wise software development? Are the newest, cool, modern tools, frameworks, and languages taking us backward efficiency-wise? It looks like the hardware specs are increased to handle these types of web apps, browsers were unnecessary If we had pure native apps only. Like, why the mobile phones need to have 8 or 16 GB RAM? The web needs native rendering instead of some UI rendering engine wrappers. You can’t open a ~30 MB Word file in Google Docs on a well-spec laptop because the browser needs more memory and CPU usage. I think we lost our way of developing optimized, efficient, and performance-wise applications. We need to solve this issue. Even the 2K of RAM Apollo computer (1966) took humanity to the moon, but you can’t work on a ~30 MB doc file in a browser in 2024. I’m focused on the web because today everyone in the industry is also concentrating on PWA applications for the future. Thank you for reading!",
    "commentLink": "https://news.ycombinator.com/item?id=40189682",
    "commentBody": "Did we lose our way in making efficient software? (rufatmammadli.medium.com)227 points by rumad 17 hours agohidepastfavorite410 comments s1k3s 17 hours agoI want to make native apps but Apple and Microsoft seem to be trying really hard to stop me. I have to buy developer accounts, buy certificates for signing binaries, share 30% of my revenue with them for barely any reason and so on. Not to mention the mess they've introduced in their APIs - especially Microsoft. So of course we choose the much simpler, much cheaper way of the web. reply Aaron2222 11 hours agoparent> I have to buy developer accounts The Apple Developer Program is only needed for macOS if you want to do sign your binaries or distribute through the Mac App Store. And you only have to pay Microsoft if you want to publish to the Microsoft Store (or use Visual Studio if you're a company that has more than 5 Visual Studio users, more than 250 computers, or more than $1 Million USD in annual revenue). > buy certificates for signing binaries Fair (though both Windows and macOS will run apps that haven't been signed, with more warnings of course). > share 30% of my revenue with them for barely any reason Only if you use their stores (Mac App Store or Microsoft Store), and it looks like the Microsoft Store won't take any cut if you do your own payments and it's not a game. reply fragmede 9 hours agorootparentIf you don't sign your binaries on macOS, the friction for the user to run your app is prohibitive, outside of developer-focused communities. reply Aaron2222 6 hours agorootparentYes, it definitely adds quite a bit of friction. Though my other points about not needing to pay for the Apple Developer Program unless you want to codesign (at a much lower price than what you pay for a codesigning certificate suitable for signing Windows programs) and not having to pay Apple 30% (or 15%, or anything) on macOS still stand. reply stale2002 4 hours agorootparentYour point doesn't really stand, no. Your solution solves a made up problem that nobody cares about, and doesn't solve the one that actually matters, which is to successfully make and distribute good software to users. Someone shouldn't have to add the the fine print line \"Assume I am talking about things that matter, instead of things that don't\" to every statement or opinion that they have. reply mr_toad 9 hours agorootparentprevIt didn’t take my kids long to learn how to run unsigned binaries, and neither of them are developers. reply palijer 8 hours agorootparentI've walked a couple hundred customers (American small business owners) through installing an unsigned MacOS application.There was plenty of friction for enough of them to cause us onboarding problems and for us to invest in doing it the Apple way. A lot of it introduced from 2017 onwards and I think now it says something akin to \"this application will hack your computer and is a virus\" and you need to click the smaller hidden \"ignore\"s a few times to do what you want. reply Pesthuf 9 hours agorootparentprevAn actual customer won’t like it when you tell them they have to turn off or bypass a security feature to run your software. Not when other software doesn’t need it. reply jjnoakes 8 hours agorootparentOnce I get a project to \"actual customers\" I don't mind paying and signing my binaries. reply ryandrake 7 hours agorootparentHow about \"actual users\" rather than \"actual customers?\" We should not normalize this because it eats away at free software. It is totally unreasonable to have to pay the operating system's manufacturer in order for person A to simply distribute software to person B, outside of manufacturer's infrastructure. The manufacturer has nothing to do with that distribution, and has no business \"warning\" the user about this software. reply rblatz 7 hours agorootparentYou don’t have to pay to do that on MacOS, they can bypass the warning saying it’s unsigned and that the developer can’t be positively identified. reply Aaron2222 6 hours agorootparentApple should really provide free codesigning for free/open source software. reply stavros 2 hours agorootparentWhat does software being signed signify? Does it mean it's vetted? Can a malware author pay the $X and have their malware signed? reply almostnormal 3 hours agorootparentprev(almost) everyone has an SSL certificate for the web. An OS could check if software is signed with one. And maybe display a warning for only domain validation. reply samplatt 7 hours agorootparentprevThis is something that definitely chafes. Even in a large-company enterprise environment, so many worthy & legitimate projects never end up shipping due to financial or office-politics reasons. Putting up paywalls between devs and their work that they to spend both time and money on is bloody stupid. reply fragmede 8 hours agorootparentprevyet. everyone knows kids are good with getting around restrictions on computers, whether put there by their parents or otherwise. reply sydbarrett74 8 hours agorootparentprevYes, but your kids have a technical parent, so chances are they both have significantly above-average intelligence. reply otteromkram 6 hours agorootparentI technically have two parents. How far up the smarts pole am I? reply wredue 6 hours agorootparentprevI’m not sure what is “prohibitive” about pressing literally one button. reply fragmede 6 hours agorootparentThere are a bunch of words around the button. If you read them, they make you not want to press the button. reply alpaca128 3 hours agorootparentprevIt's often not just one button. It's a button, then opening the settings, manually navigating to the right section, clicking Open Anyway and then entering your password. reply dimask 2 hours agorootparentprevIf it is a company laptop, it can be impossible (unless you sort of hack it to circumvent the security settings they put). reply bitwize 8 hours agorootparentprevAnd if you don't sign your binaries on Windows, Windows Defender will assume they're malware and silently delete them. reply Repulsion9513 21 minutes agorootparentNah, much more common that \"SmartScreen\" will assume they're malware and throw up a big warning prompt (which the user will say \"can't be bypassed\" because they didn't click \"More info\"). reply AnonymousPlanet 6 hours agorootparentprevThat statement is just not true. We don't sign our software and we never had that happen with any customer. It neither happened to any unsigned software on any of my own machines, in spite of running Defender on them. reply thombat 5 hours agorootparentprevNope. Or at least, never happened to me. This comment section is starting to read like a \"Bad Times\" virus warning https://web.archive.org/web/20060925013545/http://www.making... reply thombat 5 hours agorootparentAnd having re-read \"Bad Times\" for the first time in years, the \"screw-up your VHS tracking\" is a testament to its age. reply bitwize 1 hour agorootparent\"...translate your documents into Swahili, make your TV record Gigli, neuter your pets and give your laundry static cling.\" https://www.youtube.com/watch?v=zvfD5rnkTws Seriously, though, I've had the Windows Defender thing happen to freshly compiled binaries I made. The only way to prevent it from happening is to sign your binaries, or submit them individually to Microsoft using your Microsoft account for malware analysis. It flagged the binary as being some sort of trojan (which name I looked up and found that it was a Windows Defender designation for \"I don't know the provenance of this binary so I'm going to assume it's bad\") and quarantined it. reply jgord 7 hours agoparentprevOne of the reasons I moved to Javascript web development after many years as C/C++ dev, and after the hell of making iphone apps for Apples appstore - you dont have to get a licence, get approval or make an installer, if you ship a web 'app'. reply sumnole 6 hours agorootparentWith the added bonus of being available to users across platforms. reply wredue 6 hours agorootparentprevYou don’t have to do any of that for native apps either. What on earth is happening in this comment section? reply wsc981 6 hours agorootparentI think on macOS it's kinda a requirement, even if you ship outside of the AppStore, to be trusted by consumers. Because I think the app needs to be signed by Apple, in order to start the app without a warning and I think in order for Apple to perform the signing, you'd need a developer account. I might be wrong here as I have been focused pretty much only on mobile, so feel free to correct me. reply fingerlocks 6 hours agorootparentprevYeah, the only actual hurdle from Apple is the measly 8 bucks a month for a developer account. I would happily pay ten times that amount just to avoid the node_modules dumpster fire reply worthless-trash 5 hours agorootparentAnd the hardware around it, that needs to be updated and managed. reply hoseja 3 hours agorootparentprevPeople aren't ignoring Apple hard enough. Why americans bother with it is way beyond me. reply Nevermark 1 hour agorootparentI find it inexplicable when people respond to a particular problem with a suggestion on which large platform/ecosystem someone should use instead, or avoid. Switching ecosystems is nowhere near that trivial. Ecosystem choices are dependent on content and tool investments, other devices owned, product groups, integrated technologies, network effects between people, between companies, customer relationships, existing phone payments, existing ecosystem familiarity and skills, on and on. As for developers, they often need to be on the top 2-3 platforms to be a serious choice for customers. Nothing wrong with highlighting different pros and cons of different ecosystems. But a suggestion to switch ecosystems, without a very deep understanding of someone's particular situation, just isn't helpful advice. reply StressedDev 5 hours agoparentprevTo be blunt, you do not have to create a developer account, sign binaries, or share 30% of your revenue with Microsoft. MS's API are not a mess in my opinion. You do have several options (traditional Win32, .NET, UWP, etc.). These options all work fairly well and are very flexible. As for, Apple, I do not know but I suspect you can make Mac applications without a developer account. You need a developer account for iPhone. It's $99 a year the last time I looked. This is not a lot of money if you are serious about making an application. reply sobellian 4 hours agorootparentIf you don't sign your Windows installer, then the first N users to use it will get a scary pop-up message saying that the AV \"protected your PC.\" I think you might also need do code-signing if you distribute through the MS store. Compare with the web where LetsEncrypt just works without demanding a king's ransom. As for the APIs, it is very easy to get into dependency hell between all the different UI technologies, .NET implementations, and target systems. Want to develop a brand new plain-old GUI app? Probably simple (although I've never tried, the web is right there). Need to develop a plugin for an existing application, or a new app for something like Hololens? Have fun. reply krageon 18 minutes agorootparentprev> This is not a lot of money It is a lot of money when you consider it should be free and serves exactly no purpose. reply xvilka 8 hours agoparentprevApple certificates are cheap compared to Microsoft. To get rid of UAC on Windows you have to buy certificate for thousands of dollars. reply pquki4 7 hours agorootparentI don't think you \"get rid of\" UAC, you just put the author's name on the screen instead of unknown publisher. (And why do you need elevated privilege? Most applications don't) unless you are referring to \"smart screen\" which is a very different thing, although quite similar from a user's perspective. reply cosmotic 10 hours agoparentprevDevil's advocate: you now get a lot of tooling for 'free', which used to cost hundreds or thousands of dollars. reply _aavaa_ 8 hours agorootparentCounter offer: the tooling is ultimately for their benefit. They need the apps to make their platform what it is. reply eviks 4 hours agorootparentThey ultimately need money, not apps or platforms, so this is exactly how they achieve that ultimate benefit, no top-level logic will just justify free here reply fauigerzigerk 2 hours agorootparent>no top-level logic will just justify free here Not true. The technical term for \"ultimately need money\" is discounted future cash flow. It is impossible to know for sure what price you have to charge for any particular item at any given time in order to optimise for this metric. Realistically, the answer depends on the state of competition between platforms. We all know what that state is. reply eviks 1 hour agorootparent> It is impossible to know so it is true, you can't provide any top-level logic to justify 0, you need some facts reply fauigerzigerk 33 minutes agorootparentIf \"top level logic\" is supposed to mean \"analytic statements\" then you are right. The optimal price cannot be derived analytically. As this is such a pointlessly contrived interpretation of the term \"logic\" in this context, I chose to use a different one: Is there a set of empirical circumstances under which an optimisation algorithm could conclude that the optimal price is zero? The answer to that is clearly yes. reply ChuckMcM 7 hours agorootparentprevYeah that doesn't quite work. I agree that the cost of tooling has gone to nearly zero in most cases, but not giving it away will limit the people willing to even try to develop code for your platform. reply varispeed 9 hours agorootparentprevOne off few hundred or few thousands is nothing in comparison to 30% tax. That said, I don't know about Mac, but you can build apps using free tools - maybe in not as convenient way, but certainly you can. I remember, because I was someone who couldn't afford Visual Studio licence and had to make do with GNU tools. The greed of these companies put me off from developing anything. reply dhosek 8 hours agorootparentIn the early 90s, you could expect to pay anywhere from $200–1000 for a good C/C++ compiler. Now it’s free. The 30% tax, as many people have already pointed out, is only if you want to sell through the store. Back in the 90s, if you were selling software, downloading off the net wasn’t really a thing yet and you could easily expect to end up giving up 40–60% of the retail sales price and out of what was left you were paying for manufacturing of product so you’d maybe get 20–40% of the retail sales price. Which leaves the certificate thing and while it’s an annoyance, it’s also nice as a software user to know that a program I’m running is the program it claims to be without much friction on my part, and the cost can’t be that prohibitive since I don’t remember the last time I ended up with an unsigned binary on my Mac, even for free software like TeX and friends or Aquamacs. reply saagarjha 3 hours agorootparentIt’s free because of free software, not because of platforms that used to sell these tools. reply eviks 4 hours agorootparentprev> and the cost can’t be that prohibitive since I don’t remember the last time I ended up with an unsigned binary on my Mac, even for free software like TeX and friends or Aquamacs. Ok, so your app tastes aren't that varied then (or maybe it's the memory), plenty of devs of various little utilities don't bother paying reply astrange 7 hours agorootparentprevIt's 15% for almost all developers, not 30%. reply sojournerc 7 hours agorootparentA distinction without a difference reply astrange 5 hours agorootparentIt's a 50% difference. reply dcow 3 hours agorootparentIt’s a 100% difference (; reply samplatt 7 hours agorootparentprevICT departments in many large companies often force dev teams to use certain tools, because it's what's on their list of 'approved tools for devs'. Getting new tools on this list is often stonewalled for usually office-politics reasons. Sometimes devs are locked into the tools they use. This situation is shit, but not uncommon. reply modeless 6 hours agorootparentprevThe problem with this argument is that the tools for proprietary platforms are inferior to the cross-platform ones in many cases. VSCode is better than XCode or Android Studio. GCC and Clang are better than MSVC. We don't need platform lock-in to subsidize good tools because the best tools are unencumbered. I'd happily build iOS apps without XCode or any of Apple's frameworks to save the 30% fee. Heck, I'd do it even if I still had to pay the 30%, I hate being forced to use XCode. reply rTX5CMRXIfFG 5 hours agorootparent> VSCode is better than XCode or Android Studio That’s just your opinion though isn’t it reply scotty79 2 hours agorootparentThat's just an opinion. Not only his. It's shared by many. reply HPsquared 1 hour agorootparent\"This is my opinion. There are many like it, but this one is mine. My opinion is my best friend. It is my life. I must master it as I must master my life. Without me, my opinion is useless. Without my opinion, I am useless.\" reply fingerlocks 5 hours agorootparentprevI don’t use Xcode, I develop in vim. You can run the deploy/signing step on the cli without launching Xcode. reply nox101 3 hours agorootparentprevOn which platform? On Apple's there cost is part of the premium you pay for the device. Cheapest Mac is $599. Cheapest windows machine is $199? $99. So arguably some of tbat $400-500 is for the extra software. Or would compare against Linux where you could also get a machine for $25 reply threeseed 3 hours agorootparentYou don't have to buy a brand new Mac. Older M1 devices which are still very fast are available for much cheaper. reply TheCapeGreek 3 hours agorootparent\"Much cheaper\" is still very relative. I got a second hand 8GB M1 MBP last year for $900, as is the standard price in my region. The cheapest M2 Air brand new retails for ~$1200. Meanwhile I've just ordered a new non-mac machine with up to 5GHz boost and 32GB RAM for a whopping... $1000, including extended warranty and delivery fee. reply jcelerier 16 hours agoparentprevwhenever I do native (native as in, compiled without going through some bytecode / VM / interpreter ...) apps for mac / windows / linux I don't have to do any of this, I just use Qt reply 01HNNWZ0MV43FF 16 hours agorootparentBut then you have to use c++ or Python, and figure out a good way to ship 10 dlls reply jcelerier 16 hours agorootparentI ship apps that statically link against Qt, but even if I didn't it's not like \"shipping DLLs\" wasn't a solved problem two decades ago reply BearOso 16 hours agorootparentprevYou can static-link in all of Qt. Just build Qt yourself. It can strip out all the things you don't need, even symbols from the libraries you do use, so your binary isn't going to be that big. reply delduca 15 hours agorootparentI do not think it is possible to have a commercial application, you have to pay Qt’s license. reply Maxatar 11 hours agorootparentYou can statically link Qt in compliance with the LGPL. The LGPL only requires that users are able to substitute the LGPL'd portion of an application with an alternative and compatible implementation. Using a shared object/DLL is the traditional way of doing so, but you can also accomplish this by providing the object files for your application to allow users to link their own substitutions statically. The FSF explicitly permits this as documented here: https://www.gnu.org/licenses/gpl-faq.en.html#LGPLStaticVsDyn... reply Arnt 14 hours agorootparentprevNah. But do it. You just have to open your source, that part which depends on Qt. It's not a real problem. But get a commercial license anyway, the cost is small compared to the other costs of developing your program, and you want to be friends with them. (There's someone on HN who lives on a single-line modification of an open source program. Trust me, source availability of the source code of your client app won't really make a difference.) reply ale42 12 hours agorootparent> There's someone on HN who lives on a single-line modification of an open source program Now I want to know more about this :-) reply Arnt 12 hours agorootparentHe's a nice guy. If you want your company to buy his product, you send your boss a link to the product's home page (which doesn't say \"open source\") and tell your boss that this product is great. Your boss looks at the pricing and description, and says ok. reply turrini 14 hours agorootparentprevI do as well. I program everything in C++ with Qt 6 (commercial license), compile statically where convenient, and use a single code base for all platforms (mobile, desktop, web). I handle the responsiveness of interfaces, DPI, and other micro-adjustments directly in a simple QML template. reply bobajeff 11 hours agorootparentprevWhat about code signing. Won't people that run your programs want to do so without the OS claiming it will harm their computer? reply zer00eyz 16 hours agoparentprevOk Set up CC processing on the web: How much are you going to pay stripe? 2.9% + 30¢ ... that means you have to charge 10 bucks to get down to a 6% transaction fee. Quite the price floor and an interesting cap on your pricing model! What does managing chargebacks cost you? The moment your taking money your going to hire in customer service, or spend time dealing with CS. What happens when you get a chargeback, or do a refund? Most of the time you loose money (processing fees etc) If your under a million bucks a year apple is 15%. If you're building a low price app or a value add app, odds are that apple is going to be a far better deal for you than doing it on your own. reply tomhallett 16 hours agorootparent$10 = 6% fee; $5 = 8% fee. Both of which are far better than apple’s fees, so that point is a bit confusing. Chargebacks = customer support. I agree with that, but if you have a B2C business which has any non-trivial revenue (OP is talking about word doc apps, so we’re obviously not talking about indie $2 side project apps), then you would already have CS anyway. I fully understand there is an opportunity cost with any service and where those costs get realized, but your examples don’t seem like a slam dunk in apple’s favor. reply zer00eyz 15 hours agorootparent>> then you would already have CS anyway Would you? Because I would argue that CC processing is the point where you NEED near real time CS. Before that handling customer issues can be done better through forums, and you're going to get a lot of self service support from those. >> (OP is talking about word doc apps, so we’re obviously not talking about indie $2 side project apps) Your competing with free, libra office, Zoho writer (shockingly popular)... I would not know how to price the product to compete... 2 bucks a month as a trial? Would I pay 10 bucks a year if you were great? IF you got said productivity app past 100k users, getting to a million isnt a stretch (you have velocity and popularity). Unless your doing something really slimy, your going to be able to get a better rate out of apple if you ask your rep. reply jokethrowaway 13 hours agorootparent5% of my support has to do with payments and it's all about refunds. Everybody pays for stuff online reply SiVal 9 hours agorootparentprevDoes Apple charge 15% for each dollar up to a million plus 30% for each dollar above a million, or when you cross a million (in a year), do they suddenly jump to 30% of everything? IOW, if I have earned $999,999 so far this year, I have to pay Apple about $150,000. If I then make another $1 sale, do I owe a few cents more or another $150,000? And once your rate goes to 30%, does it stay there the following year, or does the whole system reset to zero each year? reply ceejayoz 9 hours agorootparenthttps://developer.apple.com/app-store/small-business-program... You’d owe the few extra cents. You stay at 30% if annual proceeds continue to hit $1M/year. If not, you requalify for 15%. reply zer00eyz 9 hours agorootparentprev15 percent on the first million in a year 30% for everything after. Subscriptions are 15% for renewals (and maybe for all subs). If your pulling in more than a few million a year from apple, and your not \"gaming\" or gaming the system I hear they are fairly open to negotiate. YMMV reply palijer 8 hours agorootparentprevDealing with Apple is a tax as well though. How do you calculate a price for not being able to release your main product? Usually without clear indications of what exact interpretation of a rule you are breaking... We've had delays of a week because of things like we mentioned \"Android\" in an integration setting that had been there for years. reply littlestymaar 15 hours agorootparentprevEven when using Stripe (which is a premium payment service that's more expensive than most options) you'd be better off than the 15% from Apple as long as you sell for more than $2.5. And that's not even counting the up from cost that come with Apple (subscription + the need to buy a Mac). How is chargeback being managed on Apple? I doubt they are swallowing the cost on their side, so I don't really see the difference between what'd get with a bank: you're losing the money anyway. reply zer00eyz 14 hours agorootparentAt 5 bucks a customer, you need 200k new ones a year to break a million bucks. TO break even with apple you have about 80k a year all in cost to deal with all your refunds and charge backs.... after taxes, insurance and overhead that's 40-60k take home for a CS agent. What is the charge back rate on digital goods? Im going to tell you that if your a small player it will be WAY higher than apple. Apple will cut a consumer off if they have a high refund rate, your CS agent will have no such insight. %5-10 of your charges will just turn into refunds. Is that a process where you're killing license keys? Oh did you forget you now have infrastructure to run to issue and maintain said key? What is that going to cost you? Dont want to run like that... well ok then expect your return rate to go even higher. That discount CC processor is going to look at your refund and charge back rate and jack your fees up sky high (because that's the name of the game). Once you get past a million bucks the open question is \"do I do enough business to negotiate with apple\". IN the case of a dry business oriented app, that has enough popularity to make that much, you might see apple willing to negotiate with you much sooner than a game dev who has sneaky buy options and huge charge back rates. reply jokethrowaway 13 hours agorootparentChargebacks are a pain but are not that frequent. You need to make a way to refund your product easily discoverable because customers go unpunished. You can use chargeback protection on stripe or use a different payment provider which absorb the 15$ fee for chargebacks reply 6510 7 hours agorootparentprevThe cheapest in my country is 7 cent per transaction, the most popular is 25 cents. We also don't do claw-backs. reply paulddraper 16 hours agorootparentprev> apple is going to be a far better deal ? Your math seems to show the exact opposite. reply exe34 16 hours agorootparentprevDo any of these problems go away when you sell in the walled garden? reply tppiotrowski 10 hours agoparentprev> share 30% of my revenue with them for barely any reason Does the App Store collect sales tax and remit on your behalf? If it does then I think it's worth it or face registering both in the EU and UK ($0 tax threshold) as well as 50 US states (once you hit the allowed limit) will take you a long time. reply d357r0y3r 10 hours agorootparent30% cut for handling taxes? That's wild. reply threeseed 3 hours agorootparentIt's 15% for most developers. And it's for a lot more than just handling taxes. reply tppiotrowski 5 hours agorootparentprevYou'd think so until you look into doing it yourself. It's more work than building a simple app. reply eviks 4 hours agorootparentAnd you'd thinking would reverse again to the common sense baseline when you realize that alternative providers outside of locked systems don't charge 30% reply pc86 10 hours agorootparentprevIf you live in the US the only entity you need to collect sales tax for is the state you live in. Despite what they may say you are under no legal obligation to collect sales tax for the other 49 states, nor the EU or UK. reply bdw5204 10 hours agorootparentI'm pretty sure that was changed by South Dakota v. Wayfair[0]. Most states seem to only require you collect the tax if you have 200 shipments into the state or $100k in revenue because going after a small time out of state e-commerce business over a few dollars of tax probably wouldn't be worth it but a large firm in Delaware refusing to collect tax on shipments into California would probably be hearing from California's government. If you're shipping overseas, you can probably ignore foreign taxes if you don't have a business nexus there. Especially if you have no desire to ever visit those countries. Basically just leave it up to your customers to pay whatever tax they owe. [0]: https://en.wikipedia.org/wiki/South_Dakota_v._Wayfair,_Inc. reply tppiotrowski 5 hours agorootparent> Every company selling goods and services to European customers needs to collect value-added tax (VAT), even if their business is not established in Europe. https://stripe.com/guides/introduction-to-eu-vat-and-vat-oss reply bdw5204 4 hours agorootparent> Enforcement of judgments issued by foreign courts in the United States is governed by the laws of the states. Enforcement cannot be accomplished by means of letters rogatory in the United States. Under U.S. law, an individual seeking to enforce a foreign judgment, decree or order in this country must file suit before a competent court. The court will determine whether to recognize and enforce the foreign judgment. Obviously, its not a good idea to bet your business on the courts not enforcing an EU fine when you can just add the VAT and cost of the handling hassle to the price for EU customers. https://travel.state.gov/content/travel/en/legal/travel-lega... reply TheCapeGreek 3 hours agorootparentThe operating idea from governments is that in the digital age, when you sell something to a customer abroad, you're selling to them on their turf and not yours. That's why you're considered liable for sales tax in the first place. Doesn't matter that your own country of residence may or may not care. For all intents and purposes it's as if you physically flew to the country and hand-delivered your software/product to your customer. It's clearly an awful \"patch\" to outdated concepts on how commerce works compared to pre-internet, but it's what we have right now. reply raincole 2 hours agoparentprevIt's either me having reading comprehension issue, or it's surprisingly unclear which certificate I need to buy to publish an app on Microsoft Store and what the minimum cost is. Considering the whole point to have Windows is to use apps I'd expect they made the process super smooth. reply silverquiet 16 hours agoparentprevI work at a place that ships an app to both Apple and Microsoft Desktops (we could even do Linux is there was ever any demand for it). We use this old thing called Java which still seems to work. I don't develop it though so I guess I don't have to worry about too much of my resume getting caught up with unfashionable languages (let's face the facts about what most tech these days is trying to advance - promotions - not the state of the art). reply Maxatar 10 hours agorootparentJava apps are not native on either macOS or Windows. reply foresto 10 hours agorootparentNor Linux. The only Java desktop app I've ever used (on any platform) without frustration was Slay the Spire, and it only passes because it's a game and doesn't require desktop integration of any kind. reply astrange 7 hours agorootparentI hear Minecraft is popular. I use JDownloader sometimes, it's totally fine. Weka is bad, but not worse than other academic apps. reply dexwiz 9 hours agorootparentprevSlay the spire is built using libGDX which provides a lot of cross platform support on top of Java. For platforms like Switch without JVM support it probably ships a compiled version without JIT. reply patrick451 9 hours agorootparentprevMatlab is a java app. I used it on both windows and linux without any complaints. reply colecut 16 hours agorootparentprevOP is obviously talking about mobile apps. reply Sardtok 15 hours agorootparentYeah, that Microsoft App Store on mobile is a b**h. reply matheusmoreira 3 hours agoparentprevJust stop making apps for Apple, Microsoft, Google platforms. Truth is everything except Linux is just somebody else's digital fiefdom where we developers are but serfs and the users are even lower. It's either Linux or the web. reply rini17 16 hours agoparentprevYou can't use Qt? reply injuly 16 hours agorootparentQt licensing is its own mess. For commercial software, the pricing is 350-500$ per developer, per month. Seriously [1]. The company that now owns the framework doesn't seem to acknowledge the gap between big enterprises and solo developers/smaller teams. [1] Yes, one can use Qt for commercial software without buying a license (as long as it is dynamically linked), but their marketing does everything it can to hide that fact. Also, the newer additions to Qt do not fall in this category – for those, you have to pay. reply turrini 14 hours agorootparentMess? Here are the most commonly used options: - Go LGPL. Sure, you will need to ship binaries and libs, but there are tools within the SDK that do this automatically for you (windeployqt, macdeployqt, etc.). And as others have stated, it is a problem that was solved years ago. - Go Commercial to link statically. If you are a single developer, there is an annual license available for $499 (up to $100k yearly revenue). reply dexwiz 9 hours agorootparentIt always shocks me developers complain so much about QT licensing. For any other business, an expense that small for so much value seems trivial. Without a decent UI software is a terrible for experience for most users. reply pjmlp 16 hours agorootparentprevImagine that, having to pay for the tools one has to use for their work, what an abuse. reply Maxatar 10 hours agorootparentHaving to pay a monthly fee in perpetuity in order to distribute an application is absolutely egregious. reply readyman 6 hours agorootparentThe fee is for selling someone else's software. I personally despise capitalism, but your complaint about it is among the least convincing ones I have ever heard. reply Veserv 11 hours agorootparentprevThat is 4,200-6,000 $/yr. In the US, a junior developer in a software company costs (all-inclusive, not just salary) around 150,000-200,000 $/yr. That is 2-4% of yearly cost on tooling. That is not very much. It might not be worth the price, but that is hardly ridiculous. It is quite believable to get a 4% productivity improvement from appropriate tooling. You need to do a cost-benefit analysis to determine the answer to that question. reply guappa 4 hours agorootparentNo they'd rather spend weeks to reimplement scrolling. reply mardifoufs 3 hours agorootparentLol scrolling on qt is worse than on the web. I mean, you can use normal scrolling super easily on both (you don't have to do anything, and it just works). But truly custom scrolling is much harder on qt than web. In a way that's a good thing, but again, the default is just as easy on the web as it is on QT. Plus you don't have to deal with the qtquick/qtwidgets/etc thing and the non open source parts of qt reply guappa 2 hours agorootparentI have to use for work a software that is implemented in electron. I think less than 1% of the users use it on mobile, but it's designed as a mobile interface. To scroll you need to click and drag, or you need to click 5px buttons. Regular mouse scroll doesn't work. reply rini17 2 hours agorootparentprevBecause subverting users' expectations about scrolling is the step 0 of efficient software. /s reply a1o 16 hours agorootparentprevYou will still be in binary sign hell and Windows Defender may wake up one day and decide your app is a virus \"when it does X\", which is exactly it's business case. Complaining to MS will do nothing since their online thing will check and not find anything. Boom, entire software business gone for reason out of control. Doesn't care about your signed certificate too. reply xet7 8 hours agorootparentThat's why the only way to develop software is to provide URL for login. MS desktops are usually too locked down to install anything. reply hsbauauvhabzb 12 hours agorootparentprevI’ve always been curious if this counts as decimation, espionage or antitrust? reply timeon 1 hour agoparentprevHow do you host your apps? reply blackeyeblitzar 4 hours agoparentprevWe need freedom and privacy oriented general computing hardware and software again. Not these locked down operating systems tied to one walled garden. reply j45 10 hours agoparentprevChoosing the web is great. Choosing overly complex web frameworks is still a guilty pleasure of too many projects. reply mattl 16 hours agoparentprevYou don’t have to do any of that for a native Mac app. Signing it is a good idea but not required and you can distribute it from your own website or even from GitHub/Lab where you can tell people it’s not notarized and they’ll need to command click and open it the first time. reply cmiles74 16 hours agorootparentIn my opinion, this will become harder and harder to do with every release of Windows and MacOS. I wouldn't count on the average customer of these vendors being willing to shop outside of their plaatform's app stores forever. reply rblatz 6 hours agorootparentDoes a sizable portion of people shop for apps in the Microsoft or MacOS App Store? I was under the impression that neither are very popular. reply fragmede 9 hours agorootparentprev> tell people it’s not notarized and they’ll need to command click and open it the first time. That's not realistic for Apple users who are used to ergonomic software. It's not technically required to notarize, but practically speaking, it is. reply mattl 8 hours agorootparentIt’s really only practical for dev tools or niche open source desktop apps. reply EGreg 16 hours agorootparentprevThe reason that Apple and Microsoft require all this is also that native apps have a lot more access to the system. reply Nextgrid 15 hours agorootparentThis doesn't matter. Notarization doesn't do anything against a dedicated attacker willing to commit illegal acts. Notarization is supposed to deter malware by a combination of static/dynamic analysis and attaching some real-world legal entity to any signed binary so law enforcement can follow up on if malicious activity is happening. Analysis is not bulletproof and can be worked around. The legal entity requirement is also trivial to nullify. At least in the UK, the company registration authority charges a nominal fee (payable by credit card - stolen if necessary) and puts you on the company register. Dun & Bradstreet scrapes that and that's how you get the DUNS number necessary to register for an Apple dev account. All of this is trivial to get through if you don't mind breaking the law and making up a few fake documents and providing a stolen CC (and assuming you're already planning to break the law by distributing malware, this is not a problem). Finally, even if the \"legal entity\" bit was bulletproof, law enforcement just doesn't give a shit about the vast majority of online crime anyway. All of these requirements are just a way to lock down access to the walled garden and put as many roadblocks to laymen trying to make their own software (in favor of big corps) masquerading as security theatre. reply mike_hearn 15 hours agorootparentNotarization does do things against attackers, yes. Firstly, stolen CCs tend to get reported especially if you make a big purchase. If you use a stolen CC to buy a developer certificate then it's going to get revoked the moment the real owner notices, and then your apps will be killed remotely by Apple before they've even been detected as malicious. Still, the big win of notarization is that Apple can track down variants of your malware once it's identified and take them all out simultaneously. They keep copies of every program running on a Mac, so they can do clustering analysis server side. On Windows there's no equivalent of notarization, but the same task is necessary because otherwise malware authors can just spin endless minor variants that escape hash based detection, so virus scanners have to try and heuristically identify variants client side. This is not only a horrific resource burn but also requires the signatures to be pushed out to the clients where malware authors can observe them and immediately figure out how they're being spotted. Notarization is a far more effective approach. It's like the shift from Thunderbird doing spam filtering all on its own using hard-coded rules, to Gmail style server side spam filtering. > All of these requirements are just a way to lock down access to the walled garden I've been hearing this for over a decade now. In the beginning I believed it, but it's been a long time and Apple have never made macOS a walled garden like iOS is. There's no sign they're going to do it either. After all, at least some people have to be able to write new apps! reply worthless-trash 5 hours agorootparent> They keep copies of every program running on a Mac, so they can do clustering > analysis server side. Are you sure about this ? I did not give apple permission to keep a copy of my software that I am writing. reply mike_hearn 1 hour agorootparentYes you did, if you have notarized your app: https://developer.apple.com/support/terms/apple-developer-pr... Section 5.3: \"By uploading Your Application to Apple for this digital notary service, You agree that Apple may perform such security checks on Your Application for purposes of detecting malware or other harmful or suspicious code or components, and You agree that Apple may retain and use Your Application for subsequent security checks for the same purposes.\" reply saagarjha 2 hours agorootparentprevIsn’t that something you agree to when you notarize? reply api 16 hours agoparentprevThe native world also refuses to create a standard UI API, making everyone use either Qt or Electron because sorry writing it over again for each platform is a hard “no.” Not even big companies do that anymore. reply ryandrake 7 hours agorootparentYes. Not only are they refusing (and have been for decades) to create a standard UI API, they are 1. actively making their own UI APIs as different as possible from one another, even down to requiring different programming languages to use them, and 2. killing things that they once supported, which ease cross-platform code (both major platforms walking away from OpenGL in favor of their incompatible native APIs). reply AnonymousPlanet 5 hours agorootparentprevNot only that. There are people who go to great lengths to make sure that native apps don't work properly across desktop environments even on the same OS. They also call out anyone who dares to complain about it. reply jwells89 11 hours agorootparentprevWhy would platform maintainers want to encourage the lowest common denominator apps that such an API would undoubtedly result in (as a standardized UI API by definition cannot leverage platform strengths)? Apps like that get made anyway but as it stands at least there’s a healthy crop of smaller/indie native alternatives which often best the behemoths in UI/UX. That would likely disappear with the addition of a standardized UI API, as it would probably also come with the abandonment of the old specialized APIs. reply Klonoar 7 hours agorootparentPlatform maintainers (Apple, Microsoft, etc) already do this by being on web standards panels. ;P reply api 11 hours agorootparentprevThey are already doing that. Everyone uses Electron now. A good common API would lead to much better results. reply vb6sp6 12 hours agoparentprevI'm not sharing any revenue with Microsoft for my desktop apps :) reply Rochus 13 hours agoparentprevnext [16 more] [flagged] hn_version_0023 12 hours agorootparentThe alternative that exists today that I can buy and all the apps I need for work will actually exist and function correctly is called… reply Rochus 11 hours agorootparent> all the apps I need for work The whole thing is like an intentional vicious circle. People buy the systems because certain applications are available on them (or rather because that's what everyone does), and the application manufacturers support the systems where the most customers are expected. But if one takes an impartial look at which applications or functions are really needed for a company, there are certainly alternatives. Unfortunately, the open source community sabotages itself, e.g. by constantly changing the ABI of essential functions and thus undermining the portability of non-open source applications (see e.g. https://news.ycombinator.com/item?id=32471624). reply Rochus 9 minutes agorootparentI find it very regrettable that now also on HN the flagging function is being misused more and more often to suppress other, but completely legitimate views. It is obvious that the majority of people are unaware of this problem or marginalize it, but that does not make it any less critical. My statement was: Apparently, people prefer to buy expensive devices that eavesdrop and patronize them. As long as this continues and people don't run away from these manufacturers, they will continue with the trend and patronize people even more. reply antiframe 11 hours agorootparentprevGrapheneOS for mobile. Any Linux distro on desktop. reply LAC-Tech 11 hours agorootparentprevI run desktop linux and I've had no issue with joining zoom or teams calls. reply worksonmine 2 hours agorootparentprevThe hardware is difficult but people are working on it. If you really want all firmware to be open old Thinkpads are popular but I've never tried it myself. And Linux/*BSD should be your OS. I've been using Linux for over a decade and don't miss anything. If your work mandates something you can't solve with Linux the issue is with your work and you should push to change that. reply bojan 12 hours agorootparentprevFairphone 4. reply toastal 3 hours agorootparentSustainable brands don’t remove their headphone jacks then start selling wireless headphones that require more batteries & rely on firmware updates reply bobim 3 hours agorootparentprevWith which OS to have both privacy and Banking apps running in a jailed-signed container? (Genuine question) reply dan-robertson 9 hours agorootparentprevI think your comment would be better if it would at least humour the idea that people might have legitimate reasons for their preferences, even if they don’t match yours. reply guappa 4 hours agorootparentprevIn several countries you can't file your taxes or access your bank without a google/apple smartphone. People need to live too. reply Rochus 16 minutes agorootparentSuch a state would not only be unsocial, but would also have to accept the question of why it is so interested in forcing such a device on every citizen. reply threeseed 3 hours agorootparentprevWhat country only lets you file your taxes and do banking via a smartphone. It's always been an app in addition to a website. reply guappa 2 hours agorootparentIn sweden you can't login to A LOT of stuff without a smartphone. Including taxes, getting a covid certificate, applying to rent an apartment. reply KittenInABox 12 hours agorootparentprevUnfortunately, yes, having one's personal information accessible to large, private companies really doesn't matter to most people. The only people I know who really care about this stuff are tech people, stalking victims, and victims of domestic abuse. [Admittedly this is becoming more aware for women trying to get abortions, but they're also a minority shamed to silence most of the time.] This isn't going to change until there are real, public, personal stakes for the majority of people. reply greenthrow 16 hours agoparentprev\"Barely any reason\"... except they created and maintain the entire plarform and tooling that you're building on. And in Apple's case they give it away for free with any mac. I'm old enough to remember when buying development tooling for DOS or Windows was $$$$$$ reply cmiles74 16 hours agorootparentApple started giving away the development environment because they had such an anemic software ecosystem. They had a handful of OpenSTEP developers and a larger crowd of die-hard Mac people, the successful ones mostly moving away from the platform. Today Apple is taking percentage of every dollar made from application developers who participate in their App store and they are making it increasingly difficult to avoid this with every release. IMHO, they are making far more dollars today than they ever did selling development hardware and SDK licenses. reply cma 16 hours agorootparentprevThey had a $100 yearly dev fee for ios. reply mattl 16 hours agorootparentOnly if you want to distribute via the App Store. There’s also TestFlight and distribution of source code I believe if you want to avoid that. reply internetter 16 hours agorootparentBoth of these are completely false. Testflight distribution without a developer license is impossible. Asking users to compile the app themself is infeasible, as the XCode toolchain is upwards of 18gb and they will be required to compile it once every week to keep it on their device. The developer fee is unavoidable — even with EU intervention reply Nextgrid 15 hours agorootparentEven signing for your own device (if you manage to get your users to do this) requires an Apple ID in good standing. reply cma 16 hours agorootparentprevSounds much less generous than OP. reply glass-z13 16 hours agorootparentprevCan i borrow your compiler for a few days? reply claytonwramsey 16 hours agoprevThere is perhaps some irony in the fact that this blog was posted to Medium, which serves 10.88 MB for a 265-word article. reply apantel 13 hours agoparentThe ads are the real content from Medium’s perspective. The article is actually the medium by which the real content is delivered, like a train carrying dark passengers. The article is not what Medium cares about delivering to your browser, but the ads. And delivering the ads requires a lot of complexity. reply Animats 10 hours agorootparentThe article is an ad: \"*** provides uptime monitoring and flow-based monitoring for APIs.\" This is an important subject, thus it's one for which clickbait is generated. Size is a problem. I look at my Rust compiles scroll by, and wonder \"why is that in there?\". I managed to get tokio out, which took some effort. The whole \"zbus\" system was pulled in because the program asks if the user is in \"dark mode\". That brought in the \"event-listener\" system. Lately, \"bash\" in a Linux console has become much slower about echoing characters. Did someone stick in spell check, or a LLM for autocomplete, or something? reply boustrophedon 5 hours agorootparentI'm not sure if it's related, but I have the git branch in my PS1 and I've noticed that it's much slower to show a new prompt when inside very large repositories now, and I don't think that was the case previously. reply fragmede 9 hours agorootparentprevI'd check your .bashrc because that shouldn't be happening without your say so. reply Animats 7 hours agorootparent.bashrc hasn't change since 2021. But Ubuntu pushed a new /usr/bin/bash in mid-March. reply Aloisius 10 hours agoparentprevFirefox about:process reports the article taking 239 MB of memory and 0.06-0.2% of my CPU ten minutes after it finished loading - 45% of the CPU time seems to be spent in Google's reCAPTCHA. I wish Mozilla or Google or someone aggregated statistics for cpu/memory/energy usage by domain to shame devs who clearly don't otherwise care. reply zer00eyz 16 hours agoparentprevAnd browsers are larger that some operating systems. And talk about a closed off ecosystem ... WASM is still crippled and JS/HTML/CSS is your only real viable option for web development. The web feels like 2005 again. Only thing is, this time the popups are embedded in the page... reply vnuge 15 hours agorootparentI think I would prefer 2005 web again. I'd probably be able to see more of the internet. I use heavy DNS filtering, no javascript on untrusted sites, no cookies, no fonts, VPN and so on. With cloudflare blocking me I basically can't see the majority of websites. reply snoman 10 hours agorootparentOh don’t worry. Once dns-over-https becomes standard, you won’t be able to do any dns filtering anymore. reply prmoustache 3 hours agorootparentI don't know about other browsers but on firefox I can decide which DNS server is used for DNS over https. reply vnuge 9 hours agorootparentprevWhy not? I can still mitm DOH now? I try to use DOH for everything I have. I did recently switch to self hosted recursive resolution. reply anthk 1 hour agoparentprevFor that I fire up a Gemini browser against gemini://gemi.dev/bin/waffle.cgi and paste the URL. For non Gemini network users, just change medium.com to scribe.rip at the URL. reply kmstout 7 hours agoparentprevIt's fine in a text-mode browser. reply stephc_int13 16 hours agoprevMy opinion about this is that yes, we lost our way, and the reason is very simple, it is because we could. It was the path of least resistance, so we took it. Software has been freeriding on hardware improvements for a few decades, especially on web and desktop apps. Moore's law has been a blessing and a curse. The software you use today was written by people who learned their craft while this free-ride was still fully ongoing. reply alerighi 16 hours agoparentThe thing that makes me crazy is that the thing that we do on computers are basically the same each year, yet software are more and more heavy. For example just in 2010 a Linux distribution with a DE just started did consume 100Mb of RAM, an optimized version 60Mb of RAM. I remember it perfectly. I had 2Gb of RAM and did not have even a swap partition. Now just a decade later, a computer with less than 8Gb of RAM is unusable. A computer with 8Gb of RAM is barely usable. Each new software uses Electron and consumes roughly 1Gb of RAM minimum! Browsers consume a ton of RAM, basically everything consumes an absurd amount of memory. Not talking about Windows, I don't even know how people can use it. Every time I help my mother with the computer is so slow, and we talk about a recent PC with an i5 and 8Gb of RAM. It takes ages to startup, software takes ages to launch, it takes 1 hour if you need to do updates. How can people use these system and not complain? I would throw my computer out of the window if it takes more than a minute to boot up, even Windows 98 was faster! reply flenserboy 10 hours agorootparentThink also about all the finished stand-alone applications which have been discarded because of replacement APIs, or because they were written in assembly. We had near-perfect (limited feature-wise from a 3-decade view, of course) word processors, spreadsheets, and single-user databases in the late 80s/early 90s which were, except for many specific use-case additions, complete & only in need of regular maintenance & quality-of-life updates were there a way to keep them current. They were in many cases far better quality & documented than almost any similar applications you can get your hands on today; so many work-years done in parallel, repeated, & lost. If there wouldn't be software sourcing & document interchange issues, it would be tempting to do all my actual office-style work on a virtual mid-90s system & move things over to the host system when printing or sending data. Addition: consider also how few resources these applications used, & how they, if they were able to run natively on contemporary systems, would have minuscule system demands compared to their present equivalents with only somewhat less capability. reply anonzzzies 4 hours agorootparent> limited feature-wise from a 3-decade view Outside gaming, ai and big data, aka things for instance my parents don’t use at all, what limited feature wise? Browsers, sure, however my father prefers Teletext and newsgroups and Viditel (doesn’t exist anymore but he mentions it quite a lot) over ad infested slow as pudding websites. Email didn’t change since the 90s, word processors changed but not with stuff most people use (I still miss WP; it was just better imho; I went over to Latex because I find Word a complete horror show and that didn’t change), spreadsheets are used by pros and amateurs alike as a database mostly for making lists; nothing new there. You can go on and on; put an average user behind a 80s/90s pc (arguably after win95 release; DOS was an issue for many and 3.1 was horrible; or Mac OS) and they will barely notice the difference. Except for the above list of ai, big data, gaming and most importantly, browsers. Ai is mostly an api so that can be fixed (I saw a c64 openai chat somewhere) , big data is a very small % of humanity using that and gaming, well, depends what you like. I personally hate 3d games; I like 80s shmups and most people who game are on mobile playing cwazy diamonds or whatnot which I can implement on an msx 8 bit machine from the early 80s. Of course the massive multiplayer open world 3d stuff doesn’t work. Anyway; as I said before here responding to what software/hardware to use for their parents; whenever someone asks me to revive their computer, I install Debian with i3 wm and dillo and ff as browser, Libreoffice and thunderbird. It takes a few hours to get used to but people (who are not in IT or any other computer fahig job) are flabbergasted by the speed, low latency and battery life. I did an x220 (with 9 cell) install last week; from win xp to the above setup; battery life jumped from 3 to 12 hours and everything is fast. I install about 50 of those for people in my town throughout the year; people think they depend on certain software, but they really usually don’t. If they do, most things people ask for now work quite well under Wine. I have a simple script which starts an easy ‘Home Screen’ on i3 with massive buttons of their favourite apps which open on another screen (1 full screen per screen); people keep asking why Microsoft doesn’t do that instead of those annoying windows… reply sydbarrett74 8 hours agorootparentprevYour sentiment is probably shared by many dusting off old systems and going back to first principles. SerenityOS is one example. reply hobs 7 hours agorootparentprevIt's because a lot of it is fashion, doesn't matter if you have an old working shirt, need new shirt. reply VelesDude 4 hours agorootparentprevMy daily runner is a T400 Laptop with 4GB RAM on a fairly slim Linux distro. But in the last 6-12 months it is starting to feel a little tight when it comes to anything web browsing. Even things like Thunderbird are getting very bulky in keeping up with web rendering standards. I pulled down an Audiobook player the other day, once all dependencies were meet, it need 1.3GB to function! At least VLC is still slim. reply prmoustache 3 hours agorootparentI think there is a thing about starting to boycott overly heavy websites. There are some useful resources: https://greycoder.com/a-list-of-text-only-new-sites/ There are also some tricks to have a lighter web browsing by default: - try websites with netsurf, links or w3m first - using a local web to gemini proxy to browse many websites with a lightweight gemini browser. And you can go a long way by using an adblocker and/or disabling javascript by default using an extension with a toggle. reply paulryanrogers 15 hours agorootparentprevWindows 98 was often running on fragmented disks. I recall it taking minutes before I could do useful work. And having multiple apps open at once was more rare. While possible it often ended in crashes or unusable slowness. reply jimmaswell 6 hours agorootparentprevI have a Macintosh Plus, SE, 7200, and iMac G3 (System 6, 6, 7, 9) I've been using for fun lately after fixing many of them up. Even with real SCSI harddrives in the SE, 7200, and iMac, they're such a joy to use compared to a modern OS. Often much more responsive, UI is always more consistent, not to mention better aesthetics. They really don't make software like they used to. A web browser or OS should not be slow on any modern hardware but here we are. reply wvenable 11 hours agorootparentprev> The thing that makes me crazy is that the thing that we do on computers are basically the same each year I think that is some kind of fallacy. We are doing the same things but the quality of those things is vastly different. I collect vintage computers and I think you'd be surprised how limited we were while doing the same things. I wouldn't want to go back. Although I will say your experience with Windows is different than mine. On all my machines, regardless of specs, start up is fast so the point where I don't even think about it. reply tomsmeding 16 hours agorootparentprevNot discounting your lament about memory use, this caught my eye: > I would throw my computer out of the window if it takes more than a minute to boot up, even Windows 98 was faster! Sure, Windows has grown a lot in size (as have other OSes). But startup is typically bounded by disk random access, not compute power or memory (granted, I don't use Windows, if 8GB is not enough to boot the OS then things are much worse than I thought). Have you tried putting an SSD in that thing? (And yes, I realise the irony of saying \"just buy more expensive hardware\". But SSDs are actually really cheap these days.) reply ponector 15 hours agorootparentBut that is true. My laptop with windows, i7, nvme, 32gb ram now feels the same as my old laptop with i7, SSD and 16gb ram did 7 years ago. Bloat ware everywhere, especially browsers. reply fuzzfactor 3 hours agorootparentA brand new mid-range business PC is not as snappy as they were brand new 20 years ago with XP. And that was on an IDE HDD, with memory speed, processor speed and cores a fraction of today, and 512MB of graphics memory or less. reply noahtallen 2 hours agorootparentprevThis whole thread needs a huge amount of salt and some empirical examples. I think if you compared side-by-side it’d be different. I remember my upgrade from 2019 MacBook to M1, when every single task felt about 50% faster. Or from swapping a window laptop’s HDD with an SSD. (Absolutely massive performance improvement!) Waiting forever for older windows computers to boot, update, index or search files, install software, launch programs, etc. Waiting ages for an older iMac to render an iMovie timeline. Others in the thread talking about the heyday of older spreadsheet and document programs that were just as fast. So? I bet you could write a book on the new features and more advanced tools that MS Excel offers today compared to 1995. We went from things taking minutes to taking seconds. So you could improve things by 50% and that could be VERY noticeable. (1min to 30s, for example.) If your app already launches in 500ms, 250ms is not going to make your laptop feel 2x faster even if it is. On top of that, since speed has been good enough for general computing for several years now, new laptops focus more on energy efficiency. I bet that new laptop has meaningfully better battery and thermal performance! reply FeepingCreature 2 hours agorootparentIf you keep your software up to date, every hardware upgrade will feel like a significant improvement. But you're comparing the end of one hardware cycle to the beginning of the next. You regain by upgrading what you previously lost to gradual bloat. reply anonymoushn 11 hours agorootparentprevI think Windows taking 1 minute on SSDs is typical, and it takes like 40 if you want to use a spinning magnet reply xboxnolifes 8 hours agorootparentMost of my windows PC's boot time happens before my computer even starts loading the OS. If I enabled fast boot in my bios, I'm pretty sure my PC would boot in around 15 seconds. reply test6554 5 hours agorootparentprevBack in my day websites didn't have \"dark mode\" and we liked it. We didn't trust the compiler to do our optimizations in the snow (both ways). etc. reply timeon 1 hour agorootparentBack in my day there was only \"dark mode\" and we liked it. reply sydbarrett74 8 hours agorootparentprevBlame surveillance capitalism for a lot of this. All those hundreds (thousands?) of trackers running simultaneously add up. reply vnuge 15 hours agoparentprev> It was the path of least resistance, so we took it. Well said. I believe many of the \"hard\" issues in software were not \"solved\" but worked around. IMO containers are a perfect example. Polyglot application distribution was not solved, it was bypassed with container engines. There are tools to work AROUND this issue, I ship build scrips that install compilers and tools on user's machines if they want but that can't be tested well, so containers it is. Redbean and Cosmopolitan libc are the closest I have seen to \"solving\" this issue It's also a matter of competition, if I want users to deploy my apps easily and reliably, container it is. Then boom there goes 100mb+ of disk space plus the container engine. reply mike_hearn 15 hours agorootparentIt's very platform specific. MacOS has had \"containers\" since switching to NeXTStep with OS X in 2001. An .app bundle is essentially a container from the software distribution PoV. Windows was late to the party but they have it now with the MSIX system. It's really only Linux where you have to ship a complete copy of the OS (sans kernel) to even reliably boot up a web server. A lot of that is due to coordination problems. Linux is UNIX with extra bits, and UNIX wasn't really designed with software distribution in mind, so it's never moved beyond that legacy. A Docker-style container is a natural approach in such an environment. reply skydhash 10 hours agorootparentIs it? I'm using LXC containers, but that mostly because I don't want to run VMs on my devices (not enough cores). I've noted down the steps to configure them if I ever have to redo it so I can write a shell script. I don't see the coordination problem if you choose one distro as your base and then provision them with shell scripts or ansible. Shipping a container instead of a build is the same as building desktop apps instead of electrons, optimizing for developer time instead of user resources. reply mike_hearn 2 hours agorootparent> if you choose one distro as your base Yes obviously if you control the whole stack then you don't really need containers. If you're distributing software that is intended to run on Linux and not RHEL/Ubuntu/whatever then you can't rely on the userspace or packaging formats, so that's when people go to containers. And of course if part of your infrastructure is on containers, then there's value in consistency, so people go all the way. It introduces a lot of other problems but you can see why it happens. Back in around 2005 I wasted a few years of my youth trying to get the Linux community on-board with multi-distro thinking and unified software installation formats. It was called autopackage and developers liked it. It wasn't the same as Docker, it did focus on trying to reuse dependencies from the base system because static linking was badly supported and the kernel didn't have the necessary features to do containers properly back then. Distro makers hated it though, and back then the Linux community was way more ideological than it is today. Most desktops ran Windows, MacOS was a weird upstart thing with a nice GUI that nobody used and nobody was going to use, most servers ran big iron UNIX still. The community was mostly made up of true believers who had convinced themselves (wrongly) that the way the Linux distro landscape had evolved was a competitive advantage and would lead to inevitable victory for GNU style freedom. I tried to convince them that nobody wanted to target Debian or Red Hat, they wanted to target Linux, but people just told me static linking was evil, Linux was just a kernel and I was an idiot. Yeah, well, funny how that worked out. Now most software ships upstream, targets Linux-the-kernel and just ships a whole \"statically linked\" app-specific distro with itself. And nobody really cares anymore. The community became dominated by people who don't care about Linux, it's just a substrate and they just want their stuff to work, so they standardized on Docker. The fight went out of the true believers who pushed against such trends. This is a common pattern when people complain about egregious waste in computing. Look closely and you'll find the waste often has a sort of ideological basis to it. Some powerful group of people became subsidized so they could remain committed to a set of technical ideas regardless of the needs of the user base. Eventually people find a way to hack around them, but in an uncoordinated, undesigned and mostly unfunded fashion. The result is a very MVP set of technologies. reply titzer 11 hours agorootparentprev> A lot of that is due to coordination problems. The dumpster fire at the bottom of that is libc and the C ABI. Practically everything is built around the assumption that software will be distributed as source code and configured and recompiled on the target machine because ABI compatibility and laying out the filesystem so that .so's could even be found in the right spot was too hard. reply fch42 7 hours agorootparentTo quote Wolfgang Pauli, this is not just not right, it's not even wrong ... The \"C ABI\" and libc are a rather stable part of Linux. Changing the behaviour of system calls ? Linus himself will be after you. And libc interfaces, to the largest part, \"are\" UNIX - it's what IEEE1003.1 defines. While Linux' glibc extends that, it doesn't break it. That's not the least what symbol revisions are for, and glibc is a huge user of those. So that ... things don't break. Now \"all else on top\" ... how ELF works (to some definition of \"works\"), the fact stuff like Gnome/Gtk love to make each rev incompatible to the prev, that \"higher\" Linux standards (LSB) don't care that much about backwards compat, true. That, though, isn't the fault of either the \"C ABI\" or libc. reply mike_hearn 1 hour agorootparentThings do break sadly, all the time, because the GNU symbol versioning scheme is badly designed, badly documented and has extremely poor usability. I've been doing this stuff for over 20 years now [1] [2], and over that time period have had to help people resolve mysterious errors caused by this stuff over and over and over again. Good platforms allow you to build on newer versions whilst targeting older versions. Developers often run newer platform releases than their users, because they want to develop software that optionally uses newer features, because they're power users who like to upgrade, they need toolchain fixes or security patches or many other reasons. So devs need a \"--release 12\" type flag that lets them say, compile my software so it can run on platform release 12 and verify it will run. On any platform designed by people who know what they're doing (literally all of the others) this is possible and easy. On Linux it is nearly impossible because the entire user land just does not care about supporting this feature. You can, technically, force the GNU ld to pick a symbol version that isn't the latest, but: • How to do this is documented only in the middle of a dusty ld manual nobody has ever read. • It has to be done on a per symbol basis. You can't just say \"target glibc 2.25\" • What versions exist for each symbol isn't documented. You have to discover that using nm. • What changes happened between each symbol isn't documented, not even in the glibc source code. The header, for example, may in theory no longer match older versions of the symbols (although in practice they usually do). • What versions of glibc are used by each version of each distribution, isn't documented. • Weak linking barely works on Linux, it can only be done at the level of whole libraries whereas what you need is symbol level weak linking. Note that Darwin gets this right. And then it used to be that the problems would repeat at higher levels of the stack, e.g. compiling against the headers for newer versions of GTK2 would helpfully give your binary silent dependencies on new versions of the library, even if you thought you didn't use any features from it. Of course everyone gave up on desktop Linux long ago so that hardly matters now. The only parts of the Linux userland that still matter are the C library and a few other low level libs like OpenSSL (sometimes, depending on your language). Even those are going away. A lot of apps now are being statically linked against muslc. Go apps make syscalls directly. Increasingly the only API that matters is the Linux syscall API: it's stable in practice and not only in theory, and it's designed to let you fail gracefully if you try to use new features on an old kernel. The result is this kind of disconnect: people say \"the user land is unstable, I can't make it work\" and then people who have presumably never tried to distribute software to Linux users themselves step in to say, well technically it does work. No, it has never worked, not well enough for people to trust it. [1] Here's a guide to writing shared libraries for Linux that I wrote in 2004: https://plan99.net/~mike/writing-shared-libraries.html which apparently some people still use! [2] Here's a script that used to help people compile binaries that worked on older GNU userspaces: https://github.com/DeaDBeeF-Player/apbuild reply titzer 4 hours agorootparentprevglibc is not stable on Linux. Syscalls are. reply fch42 2 hours agorootparenthttps://cdn.kernel.org/pub/software/libs/glibc/hjl/compat/ It's providing backwards compatibility (by symbol versioning). And that way allows for behaviour to evolve while retaining it for those who need that. I would agree it's possibly messy. Especially if you're not willing or able to change your code providing builds for newer distros. That said though... ship the old builds. If they need it only libc, they'll be fine. (the \"dumpster fire\" is really higher up the chain) reply saagarjha 2 hours agorootparentprevglibc is ABI-compatible in the forward direction. reply vnuge 11 hours agorootparentprev> Practically everything is built around the assumption that software will be distributed as source code Yup, and I vendor a good number dependencies and distribute source for this reason. That and because distributing libs via package managers kinda stinks too, it's a lot of work. Id rather my users just download a tarball from my website and build everything local. reply skydhash 10 hours agorootparentI don't think that users expect developers to maintain packages for every distro. I had to compile ffmpeg lately for a debian installation and it went without an hitch. Yes, the average user is far away from compiling packages, but they're also far away from random distributions. reply metalspoon 10 hours agorootparentprevI think flatpak is closer to .app bundles. So, the argument is a little unfair. reply guestbest 7 hours agoparentprevI disagree. It’s all the frameworks and security features like telemetry of the operating systems and those framework libraries. There are programs written in Lazarus (free pascal) that run blazing fast on windows, even the modern ones like Windows 11. Keeping the software written for a specific purpose for the desktop is the best bet for quickness and stability. Every modernization (hardware and framework) in software is a tax on the underlying software in its functional entirety reply asp_hornet 8 hours agoparentprev> path of least resistance Great take. It feels like the path of least resistance peppered with obscene amounts of resume driven development. Complexity in all the wrong places. reply fuzzfactor 2 hours agorootparent>Did we lose our way It wasn't supposed to be like this but it looks like most people never have found the way by now. So, misguided efforts, wasted resources, and technical debt piles up like never before, and at an even faster rate than efficiency of the software itself declines on the surface. reply eternityforest 12 hours agoparentprevMoore's law is still going, but we stopped making software slower. We use JITs and GPU acceleration and stuff in our mega frameworks, and maybe more importantly, we kind of maxed out the amount of crazy JS powered animations and features people actually want. Well, except backdrop filter. That still slows everything down insanely whenever it feels like it. reply EGreg 16 hours agoparentprevNow imagine same but with AI killer bot swarms. Slaughterbots. Because we could ! As long as we have COMPETITION as the main principle for all tech development — between countries or corporations etc. — we will not be able to rein in global crises such as climate change, destruction of ecosystems, or killer AI. We need “collaboration” and “cooperation” at the highest levels as an organizing principle, instead. Competition causes many huge negative externalities to the rest of the planet. reply HappMacDonald 16 hours agorootparentWhat we really need is some way to force competition to be sportsmanlike. EG: cooperating to compete, just like well adjusted competitors in a friendly tournament who actually care about refining their own skills and facing a challenge from others who feel the same way instead of cutting corners and throats to get ahead. Cooperation with no competition subtracts all urgency because one must prioritize not rocking the boat and one never knows what negative consequences any decision one makes might prove to have. You need both forces to be present, but cooperation must also be the background/default touchstone with adversarial competition employed as a tool within that framework. reply EGreg 15 hours agorootparentI don’t see any urgency in depleting ecosystems or building AI quickly or any other innovations besides ones to safeguard the environment, including animals. Human society has developed far slower throughout all history and prehistory, and that was OK. We’ve solved child mortality and we are doing just fine. But 1/3 of arable farmland is now desertified, insect populations are plummeting etc. Urgency is needed the other way — in increasing cooperation. As we did ONE TIME with the Montreal Protocol and almost eliminated CFCs worldwide to repair the hole in the ozone layer reply constantcrying 12 hours agoprevAgain and again people complain about this. But it remains a fact that essentially nobody actually wants this. Developers certainly like to have their completely integrated, connected and universal computing platform (the web). And users do not seem to particularly care about performance as long as it is good enough. And that is exactly the standard that is set, software is allowed to be so bad that it doesn't really annoy the user too much. Management doesn't care either, certainly creating good software isn't important when good enough software has already been developed. Sure, I would like things to be different, but until one group decides that a drastic departure is necessary, nothing will change. There are also no real incentives for change, from any perspective. reply austinjp 2 hours agoparentI take your point, but people certainly do complain about performance and download sizes, they just do it indirectly by describing the side effects. Only recently my partner asked me why her laptop got so hot. An iPhone user I know said they hate it when their phone got \"screen freeze\", in other words when it became unresponsive. They didn't describe it in terms of performance or app size, even though those are the underlying problems. Anyone trying to download a large app on a phone with poor signal will get frustrated, and people who live in areas with unreliable internet experience third daily. People on low incomes or living in developing nations can often only use older devices which get clogged up with large, slow apps and become frustrating to use. If you feel that people don't care about performance and download size, you may be asking the wrong people the wrong questions. reply II2II 8 hours agoparentprevI suspect a few things are going on here. This is not exactly a new phenomena. People have been complaining about software bloat since at least the mid-1990's. I suspect someone older than myself would gleefully explain that the complaint's went back to the mid-1980's, mid-1970's, etc.. Eventually it gets to the point where only outliers will complain. Everyone else will simply upgrade, put up with the bloat, or stick with old software. Then there is the question of whether the bloat is worth the benefits. If Docs was a simply a clone of Word, few people would have adopted it. Some people use it because it is free, others since they want to work on or access their documents from various devices, yet others want to collaborate on documents seamlessly. If you're getting something out of the bloat, you're less likely to think of it as bloat. We also have to consider that some bloat isn't really bloat. It's easy to point to AppleWorks on the Apple II then bemoan how modern word processes require about five orders of magnitude more resources, while ignoring how resource intensive the niceties are. Want to use proportional fonts that look nice at any size and have them rendered properly on the screen? That's about three orders of magnitude more video memory, additional CPU and use for rendering the text, etc.. I'm using that example since it is something people can actually see. Now consider the things they cannot see (such as working on documents larger than the computer's main memory, the memory required for Unicode fonts, the ability to switch between the working document and research notes, memory protection to prevent an ill behaved application from wiping out all of your work). Yes, bloat exists. On the other hand, a lot of the increased resource use is actually quality of life improvements. reply VelesDude 2 hours agoparentprevThe area I feel this difference the most is when you use software that didn't fall into this trap. Things like MYOB EXO/CRM, SAP ERP systems have code bases stretching back decades that innovate at a glacial pace. As such it is basically 2000's tech that we are forced to still use and that has turned into its big advantage. It is always fun to put up the task managed on these to see them using 20-30MB of RAM with a large part of that being the current data base loaded in. VLC & Blender are other examples of this. reply impossiblefork 11 hours agoparentprevDo we really though? Web developers do of course, but I've hardly touched web development myself. Web interfaces etc., are a choice, but I think it's driven by commercial needs-- a desire for subscription revenue instead of one-time sales, etc. Much of the modern cloud-based or half-online world is quite unnatural from a user perspective, and where there is no need for monetisation-- for example with OpenOffice, the software can expected to remain a desktop application. reply edanm 4 hours agorootparent> Web developers do of course, but I've hardly touched web development myself. Web interfaces etc., are a choice, but I think it's driven by commercial needs-- a desire for subscription revenue instead of one-time sales, etc. This is certainly a big part of it, but it's not the whole story. For one thing, there were ways to achieve those business models with native software too - \"web-based=subscription\" isn't actually a requirement, or the only way to go. But users in the early 2000s, many of them more technical than users today, rejected this idea. It felt like native apps had to cost money one time, and doing otherwise would be wrong. But with Web, since users understood that it was hosted elsewhere, it \"made sense\" for it to be a subscription, so users went with it. This also affected the technical things as well. Auto-updating was incredibly frowned upon in the 2000s - you bought it, you got to keep it as-is. So companies had to work very hard to keep multiple versions working all the time. Most of these biases by technical users have gone away. We now have auto-updating subscription native apps, e.g. Photoshop works that way today. But these technical biases drove the usage of the web, because it was so much technically easier for businesses, and allowed much better business models. (And, and this isn't even getting into the whole \"installing software was really hard for users\" thing!) reply constantcrying 11 hours agorootparentprevCertainly there has been almost no pushback. I don't think most users really care for native applications, what they like to see is clicking on something and having it work instantly, web apps deliver that. reply jwells89 11 hours agorootparentI think an overwhelming majority of users aren’t technical enough or well enough versed in UI/UX to be able to put a finger on the frustrations they experience with software, and this is something that’s important to remember when considering complaints coming from a more technically-inclined minority — even if only a small number of techy folks are unhappy, these frustrations likely exist in the larger userbase too even if most users are unable to articulate them. In addition, some percentage undoubtedly perceive these issues but are just too busy to bother with sending in feedback. With all that considered, I believe the extent of pushback that is possible is quite limited as long as the app technically works, but this is far from an accurate indicator of user happiness. reply skydhash 10 hours agorootparent> I think an overwhelming majority of users aren’t technical enough or well enough versed in UI/UX to be able to put a finger on the frustrations they experience with software They're ok with it because they don't know it could be better. A spinner every 2 minutes? 12 minutes to open slack? They accept it as fact of life until a better software comes in, and now they're wondering why they haven't come across something like this sooner. reply ryandrake 7 hours agorootparentSadly, I think people are starting to accept, as a fact of life, that software gets worse every time a developer touches it. People dread \"upgrades\" because it's going to get slower, buggier, the UI is going to change unnecessarily, and there's nothing they can do about it besides try to stay on the previous version, which is often impossible with web software. reply SkyPuncher 11 hours agoparentprevYep. The most successful startup I worked at had a SPA that downloaded 5MB bundle and preloaded a bunch of data. Took nearly 10 seconds to startup. Nobody complained about that. In fact, few people complained about a few portions of the app that had abysmal performance. It often wasn’t until 60 second load times that customers started complaining. They still raved and raved about that software because it solved an extremely valuable problem for them. A job that took literally a week could now be done in minutes. As the space heated up, we needed to improve some things, but most people literally did not care. It would always be stack ranked last in the list. reply trgn 10 hours agorootparentStartup time of an SPA is meaningless when it's for the sort of app you open once in the morning and then use during the rest of the day. It's a single startup-hit, and the user suffers it in between closing the tabs from the previous day and fiddling with some emails. Doesn't matter it is 10-20 seconds. The problem with the long startup is that it tends to cloud any discussion on performance. Code loading and parsing is basically the biggest bar in the app-perf breakdown of your profiles, and thus spins this narrative that this is the thing to optimize for, because it's the biggest bang. Rather than say, responding to user selections, reducing jitter and sluggishness while scrolling, etc... I'm starting to believe that for a large class of apps, developers should look at it as if they are writing video games: the user will tolerate the spinner before the level, but then it needs to be silky smooth after. And the _smooth after_ requires a whole class of other optimizations; it's striving for a flat memory profile, it's small ad-hoc data transfer, it's formatting data into usable layout at lower levels in the stack, it's lazy loading of content in the background, etc... Those are the areas where web-devs should be looking a (again, this only applies for that sort of SPA; e.g read-only content, blogs and such, should display _fast_). reply skydhash 10 hours agorootparentMost of my current applications have been opened since the computer booted (11 days ago). Where I'm drawing the line is wasting resources and time while I'm using them. As you said, mostly about user interactions and scrolling because UI is bound to IO for some reason. I remember all the big software taking time to startup (Adobe's, Autodesk's, even Microsoft Office's), but once they do, it was pretty smooth unless you launch that computer/gpu intensive operation. But now, things like Slack causes the computer fan to scream. reply yen223 9 hours agorootparentprev\"They still raved and raved about that software because it solved an extremely valuable problem for them. A job that took literally a week could now be done in minutes.\" This is a big point isn't it. We seem to think that customers are choosing \"slow\" over \"fast\", when a lot of times they are really choosing between \"slow\" vs \"manual\" (i.e. very very slow) reply skydhash 7 hours agorootparentYou’d always take a bike instead of walking if you can’t get a car. No one is looking to waste time when they need to get something done. If a tool is the only thing in town, they’ll praise it. Until your competitor came with something better in the way that matter. reply yen223 7 hours agorootparentI do not doubt that if customers had a choice between \"slow\" and \"fast\", all things being equal customers will pick \"fast\". Customers aren't stupid. But in a surprising number of cases, either customers don't have that choice (because the market hasn't provided a \"fast\" solution yet), or all things are not equal (say, the fast solution is fast because it's missing features that are crucial). And this is why it always looks like customers are content with poorly-performing solutions. reply metalspoon 10 hours agoparentprevThe blog is flawed. 33MB is likely no problem for the web. It's just that Google Docs devs haven't cared about that size probably because hardly anybody creates a huge doc on their platform. Or maybe his dad created a complicated doc file that G Doc failed to parse. That's different from us devs losing efficiency from our deployment platform. reply somenameforme 7 hours agoparentprevYou're saying two very different things. Nobody actually wants this is not the same as nobody is actually moving forward to achieve this. Performance is something like a tragedy of the commons. Because everybody is just doing what's in their own short term interests, but the long-term consequence is where we are today: you'd need what would literally have been a supercomputer, not that long ago, to run a word processor. It's kind of funny to imagine this parallel world where you send a PC of today back to the 70s. Whichever government got their hands on it would be keeping it ultra classified and hiding it away, like it was some device, too dangerous for the public, that could computationally solve any problem imaginable, create anything imaginable. reply ab8 9 hours agoprevIt is interesting to see most people lay the blame at the feet of developers. The reality is that these are all business decisions: 1) Move to the cloud because the business likes the steady payout of subscriptions. Business customers love not having to hire IT teams and demand six 9s of uptime because it is someone else’s responsibility. But performance needs to just be acceptable to end users. 2) Customers refusing to upgrade on-premises software, that led to long maintenance cycles and endless patches 3) Developing once for the web vs. Multiple times for different platforms – each needing its own developers and testers. No amount of expertise on the part of developers is going to address these fundamental forces. reply teeray 8 hours agoparent> Customers refusing to upgrade on-premises software After a certain period of time, that software worked just fine for those customers. Photoshop is a great example. Sure, you won’t get the flashiest features, but CS4 will still work for you on a Win7 machine without any additional fees paid. reply ryandrake 7 hours agorootparentOnce I commit to buying a version of Software X, I'm happy with it. As a user I expect Software X to work as-installed for decades to come. I don't want new features. I don't want the UX to change on me all of a sudden. I don't want it to get slower. Bugfixes and security fixes are fine, as long as everything else remains the same. I wish more developers understood and respected this. reply mike_hearn 1 hour agorootparent> Bugfixes and security fixes are fine, as long as everything else remains the same. Devs absolutely do not enjoy backporting bug fixes to 5 different LTS versions of their software and then getting user complaints because there's inevitably an important customer who is six versions back. It's inefficient with expensive dev time and it's better for the business to use that time to create new features. edanm is correct, a lot of this is historical caused by very loud and angry tech users around the turn of the millennium. Want to know why Chrome won? When telling that story people tend to focus on performance or security, but that's not really it. Chrome won because Larry Page overrode all the internal screaming about silent web-style auto update for desktop apps. Oh boy, a whole lot of people really hated that idea, in fact Google had to develop their own software update engine from scratch to make it happen. Page didn't care. He understood that the ability to release a new version of web apps every week without the user noticing was a huge competitive advantage for the web, IE also updated in the background as part of the OS, and he wanted Google's desktop apps to have that same advantage. Meanwhile Firefox stuck with the old model of rare releases and letting users choose whether to upgrade or not. It was a disaster. Old Firefoxes constantly annoyed web devs by preventing them from using new features. Security patches got reverse engineered and exploited. Still, Firefox's passionate fanbase loudly rejected the Chrome approach because they felt it took away their control. Eventually the Mozilla guys accepted that they were wrong, their fans were wrong and Larry Page was correct. But it took years and in that time Chrome had built up a huge reputational advantage. reply edanm 4 hours agorootparentprevThis attitude is why the web won, IMO. When it comes to native apps, in the 2000s, this was the common attitude of users. But it's much harder to implement from a business perspective! Both in terms of business models, and in terms of dev time - having a bunch of possibly-incompatible versions lying around is a lot of overhead. On the web, where most technical users understood this is technically impossible, they were willing to allow businesses to act differently, keep the software always-updated, and charge per usage. And since that's much easier and more lucrative for companies, they all switched to that. (Now everyone kind of accepts that model, which is why today's Photoshop works via subscription, but the \"damage\" was done and the web won.) reply ehnto 6 hours agorootparentprevThis is the real reason I use linux and open source, I want stability and flexibility not inevitably enshittifying SaaS. I am not an OSS or FOSS die hard, and I even advocate for a return to selling software as a deliverable so making small applications is a viable small business. But SaaS is the only viable business model it seems. reply wpm 5 hours agorootparentprevDevelopers probably do. Businesses don't. Gotta find a way to sell the same shit every year. reply worksonmine 2 hours agoparentprevYou can have efficient web apps running on the cloud, it's just a server after all. The issue lies with developers developing on machines their users can't afford and not caring about performance and efficient code. reply 160 more comments... GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The writer shares their struggle working on a 30 MB document in Google Docs due to performance issues, questioning if software development prioritizes modern tools over efficiency.",
      "They advocate for developing more performance-oriented and efficient applications, highlighting the importance of optimizing software development practices."
    ],
    "commentSummary": [
      "The discussion explores challenges developers encounter when creating native apps on platforms like Apple and Microsoft, focusing on platform requirements' impact and the trade-off between compliance and user satisfaction.",
      "Key points include security settings on company laptops, developer account costs, software licensing, cross-platform development, and the rise of web development versus native apps.",
      "Additional topics encompass certificate needs, software distribution, privacy worries, and the ongoing evolution and updates in software technology."
    ],
    "points": 227,
    "commentCount": 410,
    "retryCount": 0,
    "time": 1714321376
  },
  {
    "id": 40190446,
    "title": "Williams-Sonoma Fined $3.18M for False 'Made in USA' Labels",
    "originLink": "https://www.scrippsnews.com/business/company-news/williams-sonoma-fined-3-18-million-dollars-for-falsely-labeling-products-as-made-in-usa",
    "originBody": "BusinessCompany News Actions Facebook Tweet Email Williams-Sonoma fined $3.18 million for falsely labeling products as 'Made in USA' The Federal Trade Commission said the retailer violated a 2020 order it settled that made the same allegations. Mark Lennihan / AP Williams Sonoma. By: Alex Arger Posted at 8:42 PM, Apr 24, 2024 Williams-Sonoma could be paying a hefty fine for claiming a small chunk of its products were \"Made in USA\" when they weren't. In a federal court filing on Monday, the Federal Trade Commission asked a judge to sign an order that would fine the luxury home goods company $3.18 million for violating a 2020 order regarding the same false label claims. Williams-Sonoma settled those charges and was required to pay $1 million to the FTC, and the following year, it submitted a report describing how it had complied with every provision in the order. However, the FTC's new claims state the company has violated the order with multiple deceptive U.S.-origin claims in the years since — including on three products in July 2021, when it filed the compliance order. One such claim, which the FTC says Williams-Sonoma made between April 2022 and August 2023, involved certain PBTeen mattress pads that were advertised as \"crafted in America from domestic and imported materials.\" The federal body said in numerous instances, those products were actually \"wholly imported\" from China. The FTC said this finding led them to discover six other products on the retailer's websites that were advertised as American-made despite either containing foreign components or being processed internationally. It states Williams-Sonoma couldn't demonstrate the validity of its own claims, and the retailer admitted to Reuters that the allegations made in the complaint were true. Williams-Sonoma ranked No. 1,469 on Forbes' largest companies in the world list last year. With brands under its own moniker, Pottery Barn, West Elm and more, the company raked in nearly $8.7 billion in sales last year. It's interesting to question, however, if U.S.-made labels have at all contributed to the company's success. One survey last year found two-thirds of its 1,000 respondents said they regularly sought out \"Made in America\" products, and 50% said they'd be willing to pay more for them. Another survey from the Reshoring Institutein 2020 found over 69% of respondents preferred American-made products, and more than 80% were willing to pay 10% to 20% more for them. Copyright 2024 Scripps Media, Inc. All rights reserved. This material may not be published, broadcast, rewritten, or redistributed. Most Recent Student protests across US put pressure on the White House to act on Israel Scripps News Staff 8:30 PM, Apr 28, 2024 In Real Life: A Hidden War Sebastian Walker 8:07 PM, Apr 28, 2024 What does it mean when your dog puts their paw on you? Jennifer Graham Kizer 6:12 PM, Apr 28, 2024 Business Chipotle nixes new concepts for now after closing health food spinoff Justin Boggs 10:40 AM, Apr 26, 2024 These are the countries where TikTok is already banned AP via Scripps News 8:23 AM, Apr 26, 2024 Tesla driver in crash that killed motorcyclist said he was using Autopilot AP via Scripps News 10:41 AM, Apr 25, 2024 Where Main Street Meets Wall Street",
    "commentLink": "https://news.ycombinator.com/item?id=40190446",
    "commentBody": "Williams-Sonoma fined $3.18M for falsely labeling products 'Made in USA' (scrippsnews.com)194 points by randycupertino 15 hours agohidepastfavorite85 comments randycupertino 15 hours agoIt's not the first time- The Federal Trade Commission said the retailer violated a 2020 order it settled that made the same allegations. Here is the 2020 order it got in trouble over originally: https://www.ftc.gov/news-events/news/press-releases/2020/07/... reply heyoni 15 hours agoparentFunny, I just reported them to the FTC for not allowing me to unsubscribe from their mailing lists despite never opting in. reply blackeyeblitzar 14 hours agorootparentThis is one of the problems with current legislation (CAN SPAM). My understanding is that individuals don’t have standing to sue, but instead all they can do is report issues to the government and hope they do something about it. reply lldb 13 hours agorootparentYep. And even worse states aren’t allowed to give individuals standing to sue either under the preemption clause (expect for in cases of deceptive advertising via email). reply hinkley 15 hours agoparentprevHopefully they multiply the fines by pi every time they repeat the violation. “You want to make it 10M?” in angry dad voice. reply sahila 12 hours agorootparentWhat’s special about pi? That’s just a 3x multiplier, seems more appropriate it should be an exponential fine increase to strongly deter repeat fines. reply valleyer 10 hours agorootparentMultiplying by pi (or any number greater than one) repeatedly is exponential increase. reply hinkley 5 hours agorootparentprevIf you read the article this is their second offense and they settled the first for 1 million. It's just around pi, and the square root of ten is just a hair above pi. \"pi\" is funnier to say than \"3.1 something\" reply geor9e 14 hours agoprevCompany With $8.7B in Yearly Sales Pays $3.18M Marketing Fee Covering Several Products for The Last Four Years, Will Report As \"Misc Expenses\" In Quarterly Report reply jayyhu 13 hours agoparentI think in a more fair world, they should be forced to recall all falsely labeled \"Made In USA\" products and offer to replace them with real products made in the USA. reply buildsjets 10 hours agoparentprevSee, that's the secret, because if you call them \"Legal Expenses\" you will definitely wind up on trial in the Supreme Court of the State of New York. Misc can cover a lot of things. reply hk1337 15 hours agoprevRule of Acquisition #239 - Never be afraid to mislabel a product. reply fullshark 14 hours agoparentMy god it's actually a real one, kudos https://memory-alpha.fandom.com/wiki/Rules_of_Acquisition reply PontifexMinimus 15 hours agoparentprevEspecially when any fine will be merely a rap on the knuckles. From the article: > the company raked in nearly $8.7 billion in sales last year. reply _heimdall 15 hours agorootparent> It's interesting to question, however, if U.S.-made labels have at all contributed to the company's success. The article did follow up the total sales number with this caveat though. It seems reasonable that the products could have accounted for more than $3.18M in sales, though we really don't know total sales of the falsely labeled products or what percent of those sales only happened due to the Made in the USA label. reply TheCleric 13 hours agorootparentI feel like that was the weakest part of the article. I don’t particularly find that to be an interesting question in this context because it’s a bit irrelevant if it works. What matters is whether they think it works, and whether it’s a worthwhile exchange for them to lie to increase sales with the hope/knowledge that it will be profitable in the long term. reply thaumasiotes 14 hours agorootparentprev> we really don't know total sales of the falsely labeled products or what percent of those sales only happened due to the Made in the USA label. We can speculate about that second question, though. Have you ever made a purchase decision based on a country-of-origin label? reply mostlysimilar 13 hours agorootparentPersonally I will spend significantly more to buy made in America. reply vundercind 11 hours agorootparentprevMIUSA is a good marker of quality for many products—not because US workers or processes are exceptional in ways that those in poorer countries can’t be, but because US manufacturing has high labor costs, so it doesn’t make sense to cut corners that it might in other countries, because you’re already not able to chase the very-price-sensitive market. For example, it’s rare to see shoes made in the US (or other rich countries, for that matter) that are made with anything worse than mid-tier full grain leather, because what’s the point? If you use US manufacturing to make crappy bonded or corrected leather shoes, you’re just going to have the most-expensive crappy shoes on the market, so why bother? There are exceptions, but it’s a decent signal. reply _heimdall 12 hours agorootparentprevFrom William-Sonoma specifically? No, I haven't. In general, absolutely though in those instances I end up always buying from a smaller company as well that actually produces the product. I'll buy USA-made jeans from Round House for example, but it wouldn't make a difference to me if I saw a sticker in a big chain store selling jeans. reply wccrawford 14 hours agorootparentprevI'm not sure why you're being downvoted for that. It was part of Walmart's early success that they sold products made in America, and there was a long time that people griped about it no longer being true. Clearly people care. reply Tagbert 13 hours agorootparentThe question may have come off as implying that people do not made buying decisions based on the country of origin label. I imagine that many people’s own buying habits show them that the label does have an impact. reply wccrawford 11 hours agorootparentI took it the opposite way (that it implied they do make decisions based on that info) but I guess it could be interpreted either way. reply hi-v-rocknroll 7 hours agorootparentprevFines should be proportional to income sheet and balance sheet, or they fail to have a deterrence effect or may violate the 8th a. reply thaumasiotes 14 hours agoparentprevI'm always amused by the meme image that fused \"low quality copper ingots\" with \"virgin vs chad\". https://i.kym-cdn.com/photos/images/original/002/042/283/f27... \"Sorry, no refunds.\" reply tombert 14 hours agoprevSince this is the second time this has happened, I kind of feel like subsequent offenses need to grow exponentially, like 3x for each new fine. That would make the next fine $9M, than $27M, then $91M etc. I think that would avoid the fine just becoming an extra tax. reply blackeyeblitzar 13 hours agoparentI think even the first offense should be all associated revenues plus a punitive fine on top of that. This current fine is nothing. reply imagetic 14 hours agoprevIt sounds like they made a good investment on that order of MADE IN USA labels. reply hi-v-rocknroll 7 hours agoparentThe Sapian MADE IN USA scam was even worse as it both evaded taxes and misrepresented the country of origin by shipping stuff through the Northern Mariana Islands. https://www.bmwe.org/journal/1999/09SEP/b04.htm https://en.wikipedia.org/wiki/Jack_Abramoff_CNMI_scandal reply gmd63 14 hours agoprevAny lawyers here able to explain why the fine is negligible? Feels like a complete joke, encouraging dishonesty as the winning strategy in business. Are judges scared they'll be assassinated or something? Do we need to anonymize them? reply vl 14 hours agoparentTo know if fine is disproportionately small you need to know the volume of sales of the mislabeled goods. If they sold only $500k of goods, then fine is unreasonably large, for example. reply rrrrrrrrrrrryan 3 hours agorootparentAlso, the damage to the brand is almost certainly greater than the fine. reply 1123581321 13 hours agorootparentprevI don't know the sales volume, but this fine is for, if I read correctly, seven products. WS has roughly 15,000 products across its brands. So the fine may be in the ballpark of one year's sales, which would be about 5.5x their profit on those sales. But that assumes these products' revenue and margins are proportionate to the rest of them. reply tmaly 9 hours agoprevReminds me of the case a farmer is bringing against USDA on the USDA organic label. They are allowing foreign farms to label things organic without any controls or checks. reply boppo1 2 hours agoparentCan you recall the case name? I'd like to read it! reply xyst 7 hours agoprevYou would think a California company would be smart and brand their products as “designed in USA” (like their Silicon Valley counterparts) and have “made in foreign country” in small print either on the box or hidden in the instruction manual reply hi-v-rocknroll 7 hours agoparentLaser engraved in 0.5 pt font on white plastic. reply Simulacra 15 hours agoprevThis always bugged me about Harley Davidson, because only around 70 to 80% of the parts are made in America. Most of Harley-Davidson's sold across the globe are made outside of America. Yet.. it's \"Made in America\" when in fact it's just assembled here. Always felt that was dishonest but IIRC they use a special loophole reply kbenson 15 hours agoparentAlmost no vehicle manufacturers make all their own parts, and vetting the source of every part to make sure also were all made in the states would just mean that some parts are unavailable. Also, its important to think of the other side of the made in the U.S.A. label which was to bring foreign manufacturers to the states for our market. Think Honda and Toyota. Getting the majority of the parts made here and the assembly here was a major benefit to those areas it affected compared to having that all happen in Japan. If you required 100% or close to it, would that have even happened? There are both positives and negatives to the current system, but in a global market I'm not sure it's sane to expect complex products with many parts to have a single national origin. reply mindslight 15 hours agorootparentTo do this right you'd have to set up something like a VAT style system where each vendor in the tree specifies how much of the value of their product was produced in the US. Otherwise the obvious incentive is to import the largest subassemblies you can, with perfunctory final assembly in the US. Then again this would give foreign made products a running start (Amazon Chineseum that's 3x the cost of Aliexpress - 70% made in the US!), and it would probably be only that last 20-30% that actually resulted in domestic blue collar jobs which are the ostensible desire of such labeling. reply londons_explore 14 hours agorootparentThis is done, but it's pretty easy to game by pricing parts 'unusually'. Especially when it is a subsidary of your own company producing the parts overseas, you can decide if ECU control circuit boards are worth $1 or $1000. And you would be able to produce paperwork and find experts who would value it at either, or anywhere in the middle. (since there is no market price for highly specialist circuit boards which can't be sold to anyone else). reply gamblor956 6 hours agorootparentNo, you can't. This is governed by transfer pricing laws. Tax authorities can and will assess penalties for behavior like that. reply mindslight 14 hours agorootparentprevI'd say there's a hard floor for the cost of board components plus the labor/management of assembling them. But yes I can imagine fraud being generally rampant. It does make you think though, now that we've got such computation and communication capabilities - if creating and publishing such kinds of reports became mandatory, such that individual consumers could benefit from a lot more analysis on what they're buying rather than the mere final price. reply KennyBlanken 15 hours agorootparentprevNobody is \"expecting complex products with many parts to have a single origin.\" And normally I'd agree that Made In the USA is diet xenophobia, but Harley specifically, and heavily, lean into being \"made in the USA\" - as do their owners, and certain politicians who introduced tariffs (and caused retaliatory tariffs in the EU. Which Harley sidestepped by, drumroll please...opening assembly plants in southeast Asia!) Some engine components, some drivetrain components, all the electricals, all the suspension, and wheels are all made outside the US. That leaves the frame/body panels and some engine/drivetrain components as the only stuff sourced in the US - and yes, the engine/bike assembled in the US. reply kbenson 7 hours agorootparent> And normally I'd agree that Made In the USA is diet xenophobia, but Harley specifically, and heavily, lean into being \"made in the USA\" - as do their owners And so do Toyota and Honda for the vehicles that are made here, but even those are only a certain percentage, which allows them to say so. It's the same playing field, so I don't really think it matters. And honestly, I think most people if pressed care that it's assembled here by people here and that at least most the parts are from here, especially major parts. Beyond that it's just corporate branding and PR, and as long as they're all playing by the same rules and people generally understand and agree with it (which I think they do, and if they don't I think if they were pressed to think about it they would come to understand fairly quickly and think it's mostly okay if presented all the facts), so I have a hard time getting thinking it's all that big of a deal, as long as the majority of U.S. sold bikes are made in the U.S. and bikes sold elsewhere are made wherever. I do think it's all a bit stupid and a big PITA, but they all do it so they're all stupid and weird in the same way. Note: I'm not really sure why people decided to downvote you instead of engage, it's not like I think your POV on HD is invalid, just different than how I see it, so I gave you an upvote to counteract it a little. Maybe someone else can do the same. Seemed a shame to have a valid position not given any discussion. reply karaterobot 15 hours agoparentprevIf 4 out of 5 parts were made in the country, and the bike was assembled in the country, it does not bother me to claim the bike is made in the USA. For a complex assembly with hundreds or thousand of pieces, there's some wiggle room, and 70-80% is close enough that I wouldn't feel lied to. On the other hand, if I bought a wooden spoon, or even a saucepan, and it said \"Made in USA\" on it, and I found out that it wasn't 100% made in the USA from materials produced here, I'd feel differently because those are relatively simple products (compared to a motorcycle) where every piece much more crucially defines the whole. I don't know what the cutoff is, though: a product doesn't have to be 100% USA-made components, but 51% would be too little. Ballpark, 70%+ would be the over-under where I'd start to feel misled. reply rootsudo 15 hours agorootparentDoes it count as made in USA if said HD motorcycle has software, made, in, India? How about designers not in USA? reply karaterobot 11 hours agorootparentOn a motorcycle? Not to me. Maybe to others. I would interpret the \"made\" in \"made in USA\" as referring specifically to the manufacturing of the physical bike. reply jjtheblunt 13 hours agorootparentprevThat reminds me of Apple's \"Designed in California\" on some products. (My 2016 Mac Pro also said Made in the USA, as it was in Texas.) reply geor9e 14 hours agoparentprevAre any brands better? Looks like the only car squeaking over 80% is Tesla on this big table https://kogod.american.edu/autoindex/2023 It's very difficult being a Mechanical Engineer trying to source American parts. America just doesn't have the infrastructure for a lot of things you need. And even when it does, often you find China can do it better, faster, and cheaper. I can get a box of CNC machined parts for $10 each via 2 day DHL for a $200 shipping fee. An America shop would take just as long and charge me a couple hundred bucks for just one. And try finding some niche bearing on mcmaster-carr if you filter by USA, nearly the entire catalog disappears. A global supply chain is just the reality of mass producing machines these days. And imports can only have one Country of Origin so we can't be too detailed on the labels. reply damontal 15 hours agoparentprevI guess it depends on what you mean by “made” reply jrexilius 14 hours agoparentprevI've run into this problem trying to build hardware in the US[1]. There are certain components where it's just not possible to find them in the US. But that is as a start-up.. as a company the size of Harley, they certainly could design and commision them here if they wanted. [1] https://www.anomie.tech/articles/making-hardware-in-the-US/ reply bugbuddy 15 hours agoparentprevThe easy fix for this is to use a nutrition style labeling for origin. Just list the origin of each part. Problem solved. reply readyman 15 hours agorootparentEasy? Maybe it seems that way if you know nothing whatsoever about American politics. reply coolhand2120 14 hours agoparentprevWhat part do you consider to be critical enough in the act of making you call it “made here”? Virtually nothing is wholly made in any one place. Even poetry relies on the paper and pencil industry. reply bequanna 15 hours agoprevCool, so to summarize the fine: - The size is so small it is essentially meaningless to the company. - Customers who were tricked receive nothing. - The FTC has more play money to continue bureaucratic empire building and create more do-nothing jobs. reply whoknowsidont 14 hours agoparentI love how instead of being mad at example #4012 about American companies profiting from fraudulent activity, you spend effort blaming the organization punishing them. This is a _civil_ penalty and its the largest one to date for violating a previous order and agreement about Williams-Sonoma stopping fraudulent use of \"Made in USA.\" More severe penalties would have ultimately required the matter to be handled directly by other government entities. In fact the DOJ themselves could have handed out harsher penalties or pursued company executives for perjury. The FTC did what they were able to do and instead of saying \"it'd be better if the FTC had more authority and discretion in what punishes to give.\" you just throw snark at them and not at the political party that continually has made every government institution's teeth dull :^). reply nathanaldensr 14 hours agorootparentThey're both to blame, which I'm sure GP would've clarified if you had asked them. Stop seeing in false dichotomies. reply rootusrootus 14 hours agorootparentOnly about half of everyone believes both sides are bad. This puts the average somewhere in the correct direction, at least. reply bequanna 14 hours agorootparentprevIf the FTC is this impotent and toothless, why do they even exist? reply pylua 15 hours agoparentprevAs a lay person, and not a lawyer, I'm not sure how this is not considered large scale fraudulent misrepresentation. It seems like the fines are extremely light, and there is no real accountability. In the very least, they should be required to offer refunds to all people who bought those products. reply CivBase 15 hours agoparentprevDo FTC fines go to the FTC budget? reply akira2501 14 hours agoparentprevPure FUD. https://www.ftc.gov/enforcement/recent-ftc-cases-resulting-r... reply bequanna 14 hours agorootparent28 cases resulted in payments in 2022. This is a joke. You may as well round that down to zero. reply akira2501 8 hours agorootparent28 cases resulting in $392 million returned, to 1.9 million people, with $10.4 million going to the treasury, and $6.9 million for costs. \"More than 90% of the $392 million that the FTC returned to consumers came from cases resolved before the Supreme Court’s 2021 ruling in AMG Capital Management, LLC v. FTC, which stripped the FTC of its ability to recover redress for consumers pursuant to Section 13(b) of the FTC Act. By comparison, in the four years preceding AMG, the FTC returned more than $11 billion to consumers using its Section 13(b) authority.\" Source: https://www.ftc.gov/news-events/news/press-releases/2023/06/... What always baffles me, is, everyone here is a consumer. You may imagine you'll one day be a business that has to deal with regulatory burden created by the FTC, but you'd still be a consumer. You can argue they're not efficient, or effective, or politically motivated, but complaining that they exist and do their congressionally mandated job is amazing to me. reply deadbabe 15 hours agoparentprevWhat exactly have customers lost by buying something that wasn’t actually made in America? reply TaylorAlexander 15 hours agorootparentWell it’s certainly an abstract loss, so saying “what exactly” is slightly unfair as the abstract loss cannot be exactly quantified. But probably the simplest answer is that the customers may have paid less if they knew it was not made in the USA, and so they have lost money. So perhaps a class action lawsuit will be filed. I guess loss of money is concrete enough. More abstractly the people may have hoped to support the work of their fellow countrymen, and would instead have been undercutting them with overseas labor. That loss is not financial, but in the way the US works perhaps some monetary value could be conjured for this as well. Or perhaps that loss is considered equal to the amount they overpaid for the product. In any case I think I’ve convinced myself that the customers should receive some financial compensation and the company should be required to clearly notify people of what they have done, to scrub any goodwill they earned that could have led to an increase in future purchases at the store. reply thaumasiotes 14 hours agorootparenthttps://satwcomic.com/my-little-pony reply TaylorAlexander 14 hours agorootparentI’ve read the comic but I’m very curious to learn more about how you feel it relates here. I guess the biggest thing I’m taking away from your comic is that maybe the people from Denmark don’t care much about some religious diet restrictions, and I’m not seeing how that relates to this story. reply thaumasiotes 4 hours agorootparentLearning that what you thought was beef was in fact pork and horse is the same injury as learning that what you thought was Made in the U.S.A. wasn't. Judging by the actual response, it's not something that rises to the level of requiring compensation, because the injury is imaginary. reply _heimdall 15 hours agorootparentprevSome customers do actually make purchasing decisions to support products made in their home country. Buying a product falsely labeled as Made in the USA tricks the customer into supporting a product or business that they would otherwise have wanted to avoid supporting. Said differently, the customer lost the freedom to support a product actually made in the US while simultaneously supporting something they didn't mean to. reply nytesky 14 hours agorootparentprevThe article lists a teen marketed mattress as one of the “made in USA” products actually made in china. After the last decade of lead contamination and other toxic products from China, I would imagine some parents are wary of a product they spend close proximity to for 10 hours every night for years. https://www.thestreet.com/opinion/china-has-a-history-of-sel... reply sosodev 15 hours agorootparentprevWhat do companies gain by pretending that their product is made in America? reply glimshe 14 hours agorootparentMany people in the US will pay more for a product made in the US. I'm one of these people. reply rabuse 15 hours agorootparentprevIt's called fraud. That word used to mean something. reply bpfrh 14 hours agorootparentprevI would argue that if they bought a more expensive product because of the label \"made in usa\" then they lost the difference between what they would have paid for the cheaper product and what they paid for the product labelled \"made in usa\". reply zitterbewegung 14 hours agorootparentprevWe prosecute companies for false advertisement so why would this be any different? People sometimes buy products for a indicator that says made in the USA reply coolhand2120 15 hours agorootparentprevFaith in the system reply fragmede 15 hours agorootparentprevjobs reply CivBase 15 hours agorootparentprevThe funds and impetus to buy something that was made in America. I don't know how you'd quantify that as a function of the product's price, but they figured out a way to put a price tag on emotional damage so I'm sure someone can figute it out. reply calderknight 15 hours agorootparentprevA USA made product reply suyash 14 hours agoprev [–] So company supposedly cheated customers by mislabelling and government is collecting a huge fine, will the people who got cheated see their money back or it just goes to Uncle Sam's pocket? reply azinman2 8 hours agoparentIt goes back to customers: https://www.ftc.gov/enforcement/recent-ftc-cases-resulting-r... reply VS1999 14 hours agoparentprevWhat do you mean supposedly? They absolutely cheated customers. reply rcstank 14 hours agoparentprev [–] We need to legislate that any fines like this go back to customers instead of government /s reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Williams-Sonoma fined $3.18 million for falsely labeling products as 'Made in USA', breaching a 2020 settlement order.",
      "The Federal Trade Commission discovered deceptive U.S.-origin claims on items imported from China, despite the company's sales success.",
      "Legal consequences follow the company's misleading labeling practices, highlighting the importance of accurate product origin claims."
    ],
    "commentSummary": [
      "Williams-Sonoma faced a $3.18 million fine for inaccurately labeling products as \"Made in USA,\" going against a Federal Trade Commission regulation.",
      "The incident sparked discussions on how U.S.-made labels influence consumer purchases, the importance of clear labeling, and the challenges of intricate international supply chains.",
      "Concerns arose over the FTC's approach to imposing fines and reimbursing consumers affected by purchasing falsely labeled items, highlighting the potential financial and health hazards consumers faced due to misleading labeling practices."
    ],
    "points": 194,
    "commentCount": 85,
    "retryCount": 0,
    "time": 1714327439
  },
  {
    "id": 40187971,
    "title": "Zed Decoded: Enhancing Zed Editor with Ropes and SumTree",
    "originLink": "https://zed.dev/blog/zed-decoded-rope-sumtree",
    "originBody": "Zed Decoded: Rope & SumTree Thorsten Ball Nathan Sobo Antonio Scandurra Max Brunsfeld April 23rd, 2024 For this second post in Zed Decoded, our blog & video series in which we're taking a closer look at how Zed is built, I've talked to Zed's three co-founders — Nathan, Max, Antonio — about the data structure at the heart of Zed: the rope. Companion Video: Rope & SumTree This post comes with a 1hr companion video, in which Thorsten, Nathan, Antonio, and Max use Zed to look at how Zed uses the Rope and SumTree types. It's a loose conversation in which we write and read a lot of code to understand how the rope in Zed works and is implemented. Watch the video here: https://youtu.be/uUu9eFNNbjg Going in I knew that data structures to represent text are a favorite topic in text editor circles. I had also used the Rope type in the Zed codebase and knew a little bit about ropes. What I didn't really understand was how Zed's rope is implemented and what that implementation enables. So I asked Nathan, Max, and Antonio about it. They've been writing code on top, below and inside the rope for years now. And, as it turns out, Zed's rope isn't really a rope, at least not in the classical sense. It's very impressive, but first: what's a rope? Why not a string? One of the most important things a text editor has to do is to represent text in memory. When you open a file in a text editor, you expect to see its contents and want to navigate through it, and — hey, that's where the name comes from — you also want to edit the text. To do that, a text editor has to load the file contents into memory — you can't just let the user stare at the raw bytes on disk. You also want to keep it in memory, because not every change should immediately be saved to disk. So the question is: how do you represent the text in memory? My naive first choice would be to use a string. Good old string. Our best friend from when we started programming. Why not use a string? It's how we represent text in memory all the time. It's not immediately obvious that that would be a bad choice, right? Hey, I bet you could go a long way with a string, but there are some problems with strings that prevent them from being the best choice, especially once you start dealing with large files and want your program to still be efficient and responsive. Problems with strings Strings are usually allocated as a continuous block of memory. That can make edits inefficient. Say you have a file with 20k lines in a string and you want to insert a word right in the middle of that string. In order to do that, you have to make room for the new word in the middle of the string, so that you still end up with a string that's a continuous block of memory. And to make room, you have to move all of the text that would come after your newly inserted word. And moving here really means making allocations. In the worst case you have to move everything — all 20k lines — to make room for your new word. Or say you want to delete a word: you can't just poke a hole in a string, because that would mean it isn't a string — a continuous block of memory — anymore. Instead you have to move all the characters except the ones you deleted, so that you end up with a single, continuous block of memory again, this time without the deleted word. When dealing with small files and small strings, these aren't problems. We all do similar string operations all day every day, right? Yes, but most of the time we're talking about relatively small strings. When you're dealing with large files and thus large strings or a lot of edits (maybe even at the same time — hello multiple cursors!) these things — allocations, moving strings in memory — become problems. And that's not even touching on all the other requirements a text editor might have of its text representation. Navigation, for example. What if the user wants to jump to line 523? With a string and without any other data, you'd have to go through the string, character by character, and count the line breaks in it to find out where in the string line 523 is. And then what if the user presses the down-arrow ten times to go down ten lines and wants to end up in the same column? You'll again have to start counting line breaks in your string and then find the right offset after the last line break. Or, say you want to draw a horizontal scrollbar at the bottom of your editor. To know how big the scroll thumb has to be, you have to know how long the longest line in the file is. Same thing again: you have to go through the string and count the lines and this time keeping track of the length of each line too. Or what if the text file you want to load into your editor is larger than 1GB and the language you used to implement your text editor can only represent strings up to 1GB? You might say \"1GB of string should be enough for everybody\" but that only tells me you haven't talked to enough users of text editors yet. Kidding aside, I think we've established that strings probably aren't the best solution to represent text in a text editor. So what else can we use? What's better than a string? If you really love strings, you might now be thinking \"better than a string? easy: multiple strings.\" And you wouldn't be that far off! Some editors do represent text as an array-of-lines with each line being a string. VS Code's Monaco editor worked that way for quite a while, but an array of strings can still be plagued by the same problems as a single string. Excessive memory consumption and performance issues made the VS Code team look for something better. Luckily, there are better things than strings. Builders of text editors have long ago realized that strings aren't the best tool for the job and have come up with other data structures to represent text. The most popular ones, as far as I can tell, are gap buffers, piece tables, and ropes. They each have their pros and cons and I'm not here to compare them in detail. It's enough to know that they are all significantly better than strings and different editors made different decisions in face of different trade-offs and ended up with different data structures. To give you a taste: Emacs uses gap buffers, VS Code uses a twist on piece tables, Vim has its own tree data structure, and Helix uses a rope. Zed, too, uses a rope. So let's take a look at the rope and see what advantages that has over a string. Ropes Here's how Wikipedia explains what a rope is: A rope is a type of binary tree where each leaf (end node) holds a string and a length (also known as a \"weight\"), and each node further up the tree holds the sum of the lengths of all the leaves in its left subtree. Instead of a continuous block of memory, a rope is a tree and its leaves are the characters of the text it represents. Here's what the text \"This is a rope\" would look like in a rope: A rope representing \"This is a rope\" You might now be thinking that this is a lot more complex than a string and you'd be right — it is. But here's the crucial bit that makes this a rope triumph over strings in many cases: the leaves - \"This\", \" is \", \"a \", \"rope\" — are essentially immutable. Instead of modifying strings, you modify the tree. Instead of poking holes in strings and moving parts of it around it memory, you modify the tree to get a new string. And by now, we as programmers have figured out how to efficiently work with trees. Let's use the example from above again: deleting a word at a certain position in the text. With a string, you'd have to reallocate all of the text that comes after the word, possibly the whole string. With a rope, you find the start and end positions of the word you want to delete, then split the tree at these two positions so you have four trees, you throw away the middle two trees (that only contain the deleted word), concatenate the other two, then rebalance the tree. Yes, it does sound like a lot and it does require some algorithmic finesse under the hood, but the memory and performance improvements over strings are very real: instead of moving things around in memory, you only have to update a few pointers. That might look silly for a text as short as \"This is a rope\", but it pays off big time when you have very large texts. I understand that this is very abstract, so let me show you. Let's take a look at Zed's rope implementation. Zed's rope implementation Zed has its own rope implementation in its own crate: rope. (One reason for why Zed has its own implementation instead of using a library is that a lot of libraries didn't exist when the Zed founders laid the groundwork for Zed in 2017.) The main type in the rope crate is Rope. Here's how you'd use it: let mut rope = Rope::new(); rope.push(\"Hello World! This is your captain speaking.\"); So far, so similar to String. Now let's say we have two ropes: let mut rope1 = Rope::new(); rope1.push(\"Hello World!\"); let mut rope2 = Rope::new(); rope2.push(\"This is your captain speaking.\"); If we want to concatenate them, all we have to do is this: rope1.append(rope2); assert_eq!( rope1.text(), \"Hello World! This is your captain speaking.\".to_string() ); The call to rope1.append connects the two trees — rope1 and rope2 — by building a new tree that contains both. That's barely more than updating a few pointers. Compare that to strings: if you concatenate two strings, you'll have to move at least one of them in memory so that they end up next to each, forming a continuous block. Often you have to move both of them, because there's not enough space after the first string. Again: the text in this example is laughably short, but what if someone wants to have ten copies of the 25k line SQLite amalgamation in a single file? What about replacing a word? // Construct a rope let mut rope = Rope::new(); rope.push(\"One coffee, please. Black, yes.\"); // Replace characters 4 to 10 (0-indexed) with \"guinness\". rope.replace(4..10, \"guinness\"); assert_eq!(rope.text(), \"One guinness, please. Black, yes.\"); What happens under the hood: replace() creates a new rope that contains all of the nodes of the original rope, up until the 5th character (c) the new text, guinness, is appended to the new rope the rest of the original rope, everything after character 11, is appended to the new rope Deleting a word? Just replace it with \"\": let mut rope = Rope::new(); rope.push(\"One coffee, please. Black, yes.\"); rope.replace(4..10, \"\"); These operations are very quick even when dealing with large amounts of text, because then most of the nodes in the tree can be reused and only have to be rewired. But what happens with the word that was deleted, \"coffee\"? The leaf nodes that contain these characters will get automatically cleaned up as soon as no other node references them anymore. That's what immutable leaf nodes in ropes enable: when a rope is mutated, or a new rope is constructed from an old one, or two ropes are merged into a new one, essentially all that's changing are references to leaf nodes. And those references are counted: as soon as there's no reference to a node anymore, the node gets cleaned up, deallocated. To be precise and get technical: the leaf nodes, the ones containing the actual text, aren't fully immutable in Zed's rope implementation. These leaf nodes have a maximum length and if, say, text gets appended to a rope and the new text is short enough to fit into the last leaf node without exceeding its maximum length, then that leaf node will be mutated and the text appended to it. On a conceptual level, though, you can think of the rope as a persistent data structure and its nodes as reference-counted immutable nodes in a tree. That's what makes it a better choice than the string and brings us back to the question we skipped above: why did Zed chose a rope instead of one of the other data structures? Why use a rope in Zed? Zed's goal is to be a high-performance code editor. Strings, as we saw, won't get you to high-performance. So what do you use instead? Gap buffers, ropes, piece tables? There isn't a single, obvious best choice here. It all comes down to specific requirements and trade-offs you're willing to make to meet those requirements. Maybe you've heard that gap buffers can be faster than ropes, or that they're easier to understand, or that piece tables are more elegant. That may be true, yes, but that still doesn't mean they're an obvious choice over, for example, a rope. Here's what the author of ropey, a popular rope implementation in Rust, wrote about the performance trade-offs between ropes and gap buffers: Ropes make a different performance trade-off, being a sort of \"jack of all trades\". They're not amazing at anything, but they're always solidly good with O(log N) performance. They're not the best choice for an editor that only supports local editing patterns, since they leave a lot of performance on the table compared to gap buffers in that case (again, even for huge documents). But for an editor that encourages non-localized edits, or just wants flexibility in that regard, they're a great choice because they always have good performance, whereas gap buffers degrade poorly with unfavorable editing patterns. Ropes are \"not amazing at anything, but they're always solidly good.\" It depends on what you want to do, or what you want your editor to be able to do. So what if you really want to make use of all the cores in your CPU? In \"Text showdown: Gap Buffers vs Ropes\" concurrency is mentioned in a paragraph at the end: Ropes have other benefits besides good performance. Both Crop and Ropey [note: both are rope implementations in Rust] support concurrent access from multiple threads. This lets you take snapshots to do asynchronous saves, backups, or multi-user edits. This isn't something you could easily do with a gap buffer. In the companion video you can hear what Max said about this paragraph: \"Yeah, it matters more than any of that other stuff.\" Nathan added that \"we use that all over the place\", with \"that\" being concurrent access, snapshots, multi-user edits, asynchronous operations. In other words: concurrent access to the text in a buffer was a hard requirement for Zed and that's why the rope ended up being the top choice. Here's an example of how deeply ingrained concurrent access to text is into Zed: when you edit a buffer in Zed, with syntax highlighting enabled, a snapshot of the buffer's text content is sent to a background thread in which it's re-parsed using Tree-sitter. That happens on every edit and it's very, very fast and efficient, since the snapshots don't require a full copy of the text. All that's needed is to bump a reference count, because the reference-counting for the nodes in Zed's rope is implemented with Arc, Rust's \"thread-safe reference-counting pointer\". That brings us to the most important bit: how Zed's rope is implemented. Because it isn't implemented like the classic rope you see on Wikipedia and its implementation gives Zed's rope certain properties that other rope implementations might not have and that implementation is actually what put the rope ahead of other data structures. It's not a rope, it's a SumTree Zed's rope is not a classic binary-tree rope, it's a SumTree. If you open up the definition of Zed's Rope, you'll see that it's nothing more than a SumTree of Chunks: struct Rope { chunks: SumTree, } struct Chunk(ArrayString); A Chunk is an ArrayString, which comes from the arrayvec crate and allows storing strings inline and not on the heap somewhere else. Meaning: a Chunk is a collection of characters. Chunks are the leafs in the SumTree and contain at most 2 * CHUNK_BASE characters. In release builds of Zed, CHUNK_BASE is 64. So then what is a SumTree? Ask Nathan and he'll say that the SumTree is \"the soul of Zed\". But a slightly more technical description of a SumTree is this: A SumTree is a B+ tree in which each leaf node contains multiple items of type T and a Summary for each Item. Internal nodes contain a Summary of the items in its subtree. And here are the type definitions to match, which you can find in the sum_tree crate: struct SumTree(pub Arc>); enum Node { Internal { height: u8, summary: T::Summary, child_summaries: ArrayVec, child_trees: ArrayVec, { 2 * TREE_BASE }>, }, Leaf { summary: T::Summary, items: ArrayVec, item_summaries: ArrayVec, }, } trait Item: Clone { type Summary: Summary; fn summary(&self) -> Self::Summary; } So what's a Summary? Anything you want! The only requirement is that you need to be able to add multiple summaries together, to create a sum of summaries: trait Summary: Default + Clone + fmt::Debug { type Context; fn add_summary(&mut self, summary: &Self, cx: &Self::Context); } But I know you just rolled your eyes at that, so let's make it more concrete. Since the Rope is a SumTree and each item in the SumTree has to have a summary, here's the Summary that's associated with each node in Zed's Rope: struct TextSummary { /// Length in UTF-8 len: usize, /// Length in UTF-16 code units len_utf16: OffsetUtf16, /// A point representing the number of lines and the length of the last line lines: Point, /// How many `char`s are in the first line first_line_chars: u32, /// How many `char`s are in the last line last_line_chars: u32, /// How many UTF-16 code units are in the last line last_line_len_utf16: u32, /// The row idx of the longest row longest_row: u32, /// How many `char`s are in the longest row longest_row_chars: u32, } All nodes in the SumTree — internal and leaf nodes — have such a summary, containing information about its subtree. The leaf nodes have a summary of their Chunks and the internal nodes have a summary that is the sum of the summaries of its child nodes, recursively down the tree. Let's say we have the following text: Hello World! This is your captain speaking. Are you ready for take-off? 5 lines of text. If this is pushed into a Zed Rope, the SumTree beneath the Rope would look like this, simplified: A SumTree representing \"Hello World!This isyour captain speaking.Are youready for take-off?\" with some summary fields left out (I left out some of the fields of TextSummary to keep the diagram small-ish and also adjusted the maximum size of the chunks and maximum number of children per node. In a release-build of Zed, all five lines of the text would fit in a single node.) Even with only three summary fields — len, lines, longest_row_chars — we can see that the summaries of the internal nodes are the sum of their child nodes summaries. The root node's summary tells us about the complete text, the complete Rope: 72 characters, 5 lines, and the longest line has 22 characters (your captain speaking.). The internal nodes tell is about parts of the text. The left internal node here tells us, for example, that it's 38 characters from \"Hell\" to \"spea\" (including newline characters) and that there are two line breaks in that part of the text. Okay, you might be thinking, a B+ tree with summarized summaries — what does that buy us? Traversing a SumTree The SumTree is a concurrency-friendly B-tree that not only gives us a persistent, copy-on-write data structure to represent text, but through its summaries it also indexes the data in the tree and allows us to traverse the tree along dimensions of the summaries in O(log n) time. In Max's words, the SumTree is \"not conceptually a map. It's more like a Vec that has these special indexing features where you can store any sequence of items you want. You decide the order and it just provides these capabilities to seek and slice.\" Don't think of it as a tree that allows you to lookup values associated with keys (although it can do that), but think of it as a tree that allows you lookup items based on the summaries of each item and all the items that come before it in the tree. Or, in other words: the items in a SumTree are ordered. Their summaries are also ordered. The SumTree allows you find any item in the tree in O(log N) time by traversing the tree from root to leaf node and deciding which node to visit based on the node's summary. Say we have a Rope with three lines of text in it: let mut rope = Rope::new(); rope.push(\"Line one.\"); rope.push(\"This is the second line.\"); rope.push(\"This is three.\"); Once the Rope is constructed, it looks like this: A SumTree representing \"Line one.This is the second line.This is three.\" with some summary fields left out Like we said above: each leaf node will hold multiple Chunks and each leaf node's summary will contain information about the text in its Chunks. The type of that summary is TextSummary from above. That means each node's summary can tell us about the len of the text in its chunks, the lines & rows in them, the longest line, and all the other fields of TextSummary. The internal nodes in the SumTree then contain summaries of the summaries. And since the items in the tree — internal nodes, leaf nodes, chunks — are ordered we can traverse that tree very efficiently, because the SumTree allows us to traverse the tree based on the values in the summaries. It allows us to seek along a single dimension, a single field for example, of a given summary. Say we want to find out what the line and column in the rope is at the absolute offset 26. Meaning: what's at character 26? In order to find out, we can traverse this three-line rope along the len field of the TextSummary. Because the len field, when added up from left to right, is an absolute offset. So in order to find what's at absolute offset 26, we traverse down the tree, taking left or right turns, depending on the len value in the summaries of the internal nodes, until we end up at the leaf node we want: let point = rope.offset_to_point(26); assert_eq!(point.row, 1); assert_eq!(point.column, 16); On the surface, the call to rope.offset_to_point(26) converts an absolute 0-based offset (26) into a row and column (1 and 16). What happens inside of offset_to_point is that a cursor traverses the SumTree until it finds the items where the aggregated len of the TextSummary is >= 26. Once it found the first leaf node where that's true, it finds the exact chunk where the offset matches and parks the cursor there. On the way there, it not only kept counting the len field of the summaries it came across, but it also aggregated the lines field, which contains row and column. Neat, right? Here's what I just described but in actual code, here's the offset_to_point method on the Rope: fn offset_to_point(&self, offset: usize) -> Point { if offset >= self.summary().len { return self.summary().lines; } let mut cursor = self.chunks.cursor::(); cursor.seek(&offset, Bias::Left, &()); let overshoot = offset - cursor.start().0; cursor.start().1 + cursor .item() .map_or(Point::zero(), |chunk| chunk.offset_to_point(overshoot)) } What it does is the following: sanity checks that the offset isn't past the end of the whole Rope creates a cursor that seeks along the usize (offset) and Point (a Point is a struct with two fields: row and column) dimensions, aggregating both while doing that once it found an item at offset, it calculates the overshoot: the offset we're looking for might be in the middle of a single chunk and cursor.start().0 is the usize (offset) at the start of a given chunk take the lines up to the start of the current chunk (cursor.start().1) — which is the summary of TextSummary.len of the complete tree to the left of the current item! add them to the lines at the offset in the, possibly, middle of the chunk (chunk.offset_to_point(overshoot)) The most interesting bit here is, of course, the cursor that's constructed with self.chunks.cursor::(). This particular cursor has two dimensions and allows you to seek to a value on one dimension (usize) in the given SumTree and in the same operation also get the sum of the second dimension, the Point, at a particular cursor position. The beauty of that is that it can do that in O(log N) time, because each internal node contains a summary of summaries (meaning: the total len of all items in its subtree in this case) and it can skip over all of those where lenusize { if point >= self.summary().lines { return self.summary().len; } let mut cursor = self.chunks.cursor::(); cursor.seek(&point, Bias::Left, &()); let overshoot = point - cursor.start().0; cursor.start().1 + cursor .item() .map_or(0, |chunk| chunk.point_to_offset(overshoot)) } This seeking along one dimension and aggregating (summarizing!) along multiple dimensions is made possible by some very clever Rust code that I won't explain in detail, but the short version is this. Given a TextSummary like the following (which we already saw in full above) and a ChunkSummary that wraps it... struct TextSummary { len: usize, lines: Point, // [...] } struct ChunkSummary { text: TextSummary, } ... we can define the len and lines fields as sum_tree::Dimensions: impl sum_tree::Dimension for usize { fn add_summary(&mut self, summary: &'a ChunkSummary, _: &()) { *self += summary.text.len; } } impl sum_tree::Dimension for Point { fn add_summary(&mut self, summary: &'a ChunkSummary, _: &()) { *self += summary.text.lines; } } With that we can then construct cursors for a given sum_tree::Dimension or a tuple of dimensions (which is what we did above with (usize, Point)). Once constructed, the cursor can then seek along any Dimension we have defined and aggregate either the complete TextSummary or just a single Dimension of a given summary type. Or we can also seek along a single dimension and then get the whole summary once the cursor has found the target: let mut rope = Rope::new(); rope.push(\"This is the first line.\"); rope.push(\"This is the second line.\"); rope.push(\"This is the third line.\"); // Construct cursor: let mut cursor = rope.cursor(0); // Seek it to offset 55 and get the `TextSummary` at that offset: let summary = cursor.summary::(55); assert_eq!(summary.len, 55); assert_eq!(summary.lines, Point::new(2, 6)); assert_eq!(summary.longest_row_chars, 24); The cursor stopped in the middle of line 2, at column 6, and up until that location the longest_row_chars in the summaries was 24. This is very, very powerful stuff. To scratch the surface: it allows us to easily convert between UTF8 rows/columns and UTF16 rows/columns, which is sometimes required when working with the language server protocol (LSP). Just seek to a UTF8 Point and aggregate the UTF16 Points: fn point_to_point_utf16(&self, point: Point) -> PointUtf16 { if point >= self.summary().lines { return self.summary().lines_utf16(); } let mut cursor = self.chunks.cursor::(); cursor.seek(&point, Bias::Left, &()); // ... you know the rest ... } But there's more and I could go on and on and on and show more examples or use big words like monoid homomorphism, but I'll stop here. You get the idea — the SumTree, a thread-safe, snapshot-friendly, copy-on-write B+ tree is very powerful and can be used for more than \"just\" text, which is why it's everywhere in Zed. Yes, literally. Everything's a SumTree Currently there are over 20 uses of the SumTree in Zed. The SumTree is not only used as the basis for the Rope, but in many different places. The list of files in a project is a SumTree. The information returned by git blame is stored in a SumTree. Messages in the chat channel: SumTree. Diagnostics: SumTree. At the very core of Zed sits a data structured called DisplayMap that contains all the information about how a given buffer of text should be displayed — where the folds go, which lines wrap, where the inlay hints are displayed, ... — and it looks like this: struct DisplayMap { /// The buffer that we are displaying. buffer: Model, /// Decides where the [`Inlay`]s should be displayed. inlay_map: InlayMap, /// Decides where the fold indicators should be and tracks parts of a source file that are currently folded. fold_map: FoldMap, /// Keeps track of hard tabs in a buffer. tab_map: TabMap, /// Handles soft wrapping. wrap_map: Model, /// Tracks custom blocks such as diagnostics that should be displayed within buffer. block_map: BlockMap, /// Regions of text that should be highlighted. text_highlights: TextHighlights, /// Regions of inlays that should be highlighted. inlay_highlights: InlayHighlights, // [...] } Guess what? All of these use a SumTree under the hood and in a future post we will explore them, but the point I want to make now is this: The Zed co-founders didn't so much decide to use a rope over a gap buffer or over a piece table. They started with the SumTree, recognized how powerful it is, how it fits Zed's requirements, and then built the rope on top of it. The rope may be at the heart of Zed, but the SumTree is, to quote Nathan again, \"the soul of Zed\". Looking for a better editor? You can try Zed today on macOS. Download now! We are hiring! If you're passionate about the topics we cover on our blog, please consider joining our team to help us ship the future of software development.",
    "commentLink": "https://news.ycombinator.com/item?id=40187971",
    "commentBody": "Zed Decoded: Rope and SumTree (zed.dev)178 points by avinassh 22 hours agohidepastfavorite32 comments BaculumMeumEst 16 hours agoI’ve tried Zed and it’s really nice but there is a very high bar for switching to a new editor - it has to be sufficiently better than alternatives and also have widespread adoption. There are so many man hours that went into things like undo-tree and magit. And thanks to the sheer popularity of mainstream editors, I can get language support for languages that aren’t even publicly available like Jai. I want to use it, but I don’t think I am going to be able to justify investing in new editors at this point unless there is something extremely compelling about it. reply cedws 14 hours agoparentI tried Zed a few weeks ago and was extremely impressed. I was expecting it to be another half baked 'Rewrite it in Rust' hobby project, but it's actually very much usable for the kind of development I'm doing. Since then I've completely swapped it out with VSCode and loving it so far. There's only one or two small issues which I'm hoping will be ironed out soon. Zed is also really really fast. I can feel the input latency is significantly lower than VSCode and faster feedback helps me write code more fluently. reply bioneuralnet 12 hours agorootparentDitto! The only thing that occasionally bumps me is a few missing features in vim-mode (marks & recording). But it feels so much faster than VS Code, I'm willing to put up with it so far. reply lsllc 10 hours agoparentprevI tried Zed a while ago, not long after they first launched. It was pretty interesting (gpu+rust+tree-sitter+lsp), but not enough to draw me from neovim. After they went open source (and launched extensions!) I looked at it again and actually I'm really impressed with the progress (incl. vim keybindings ftw!). I'm at the point of using it for some new projects and continuing with neovim for my existing ones -- there's a lot of community input e.g. nvim-surround functionality was just added. I think at this point it does everything that I have in my (somewhat minimal) nvim config (and much of that is tree-sitter/LSP). The one big feature I'm interested in is dap support so I can finally give up on gdb cli. reply MatthiasPortzel 17 hours agoprevThere’s a good series of posts called “Rope Science” by the author of the now-unmaintained Xi editor. These posts were cited as an inspiration by the Lapce author. => https://xi-editor.io/docs/rope_science_00.html reply ayewo 14 hours agoparentI first learned of the rope data structure and piece table [1] from the Tree Sitter docs: https://tree-sitter.github.io/tree-sitter/using-parsers#prov... 1: https://en.wikipedia.org/wiki/Piece_table reply cbarrick 8 hours agoparentprevIn particular, the \"summary\" from the SumTree in this Zed article reminded me of the \"metric\" abstraction described in Raph's Rope Science. reply alberth 17 hours agoprevI really want to love Zed. And truly believe they are onto something big with multiplayer coding, chat, collab. But I’ve run into so many bugs I’ve had to stop using Zed. The amount of time I’ve spent filing bugs, troubleshooting, documenting errors etc - is counter productive. It makes me realize that I’m old enough now to just want things to work. I’ll give it another go once it hits 1.0 Until then, wishing them the best and for them to make the developer world better for us all. reply icholy 14 hours agoparentThis is how I've been feeling about neovim recently. reply _virtu 12 hours agorootparentI'm curious as to why. I finally went all in on neovim about two years ago and have been having an amazingly stable experience. reply pkos98 13 hours agorootparentprevare you referring to the actual editor or to the plugins you used? the editor itself has been very stable for me (daily usage) and the plugin ecosystem definitely offers so many choices that most of the time I can find stable/well-working ones for my use case. reply icholy 8 hours agorootparentNeovim core has lots of bugs. reply Terretta 17 hours agoparentprev> The amount of time I've spent troubling shoot... Conveys a lovely element of gardening: new bits of code appear like shoots in the garden, often troubling if they aren't what one planted, taking time to weed giving nutritious shoots room to grow. reply verticalscaler 15 hours agorootparentZed? More like Zen. reply josephg 11 hours agoprevI’ve seen the SumTree pattern show up a bunch recently. As I understand it, VSCode has something similar for syntax highlighting. And I’ve both seen and written several implementations of this pattern in the various text CRDTs I’ve interacted with. - though in each case the structure is given a different name. (Eg RangeTree, ContentTree, G-Tree, etc). It’s nice to see the idea is catching on. It’s a good one. reply mightyham 17 hours agoprevSimilar to a SumTree, if all you need is incrementally computed values without data persistence, a heap makes for a more efficient data structure. I remember learning about this from the following blog: https://timvieira.github.io/blog/post/2016/11/21/heaps-for-i... reply iamdamian 10 hours agoprevI've been trying Zed today and am thoroughly impressed. Everything feels incredibly well considered, even for edge cases. A niche example I tried on a whim that just worked: You can multi-select lines (say, the start of every other line, or arbitrary points in the file) and then use Vim commands to apply changes only at those locations. I'm kind of blown away. reply eddd-ddde 10 hours agoparentBtw this is how kakoune works all around. Multiple selections are first class citizens Also since there is no extra visual mode it is very easy to select something and then operate on it. reply Syzygies 17 hours agoprevRopes remind me of Haskell's type ShowS = String -> String `Text` is generally preferred to `String` in Haskell, but I've happily written parsers that depend on `ShowS` to defer concatenation to linear time final output. `Text` to me always felt more C-like, and better suited to C-like applications. I've survived in math partly by attempting to catalog how others think. I sense a divide in Haskell, between people who prefer to view the compiler as a hermetically sealed abstraction, and those of us who try exploit what we think the compiler is doing. To view `ShowS` as a rope one needs to consider how the compiler handles lazy evaluation. reply lupire 16 hours agoparentYou don't need to anything about the compiler internals to know that a String is a linked list, and that concatenating a linked list is expensive, and that ShowS is a function interface that enables optimized contatenation. Rope is a data structure, not a function. https://hackage.haskell.org/package/base-4.19.1.0/docs/GHC-S... https://hackage.haskell.org/package/base-4.19.1.0/docs/Data-... reply Syzygies 15 hours agorootparentData structures are useless without functions, and functions are useless without data structures. In use, ropes are like `ShowS`. It feels to me like you're making a grammatical distinction, of a kind I try to avoid so I can think more abstractly. Our categories are historical accident. reply hiccuphippo 4 hours agoprevSince it's for a code editor and using trees, I wonder if directly using an AST would be a good idea. You don't need to store strings for keywords like \"function\" or \"while\", and you get syntax highlight and formatting basically for free. reply rochak 17 hours agoprevGreat explanation. If anyone has good references for learning this by making a minimal editor like sed from scratch, please share. Wait, now that I think about it, does the name “Zed” draw inspiration from “sed”? If yes, that’s rad. reply bartekpacia 13 hours agoparentIt draws from the original UNIX text editor called \"ed\". https://zed.dev/faq#why-zed reply nickzelei 16 hours agoparentprevI assumed it was the alternative pronunciation of Zero. reply humzashahid98 16 hours agoparentprevI have a simple (well, simple by my standards because I've spent a long time with it) implementation of a rope in Standard ML [0], OCaml [1] and F# [2]. (See tiny_rope.sml, brolib.fs or lib/tiny_rope.ml for implementation if you can read any of those languages; the F# implementation is probably easiest to read.) [0] https://github.com/hummy123/brolib-sml [1] https://github.com/hummy123/brolib [2] https://github.com/hummy123/brolib-fs The essence of a data structure is a binary tree where the internal nodes (the N2 case) contains a pointer to the left subtree, an integer containing the total length of the strings in the left subtree and a pointer to the right subtree. Then there are leaf nodes (the N0 case) which contain simply strings. (There are some other cases in the type like N1, N3 and L2, but those are solely for balancing because I built my ropes on top of 1-2 Brother Trees as described by Ralf Hinze, and those aren't essential to the rope data structure.) When indexing (which is necessary for the insertion and deletion operations), you have a simple recursive algorithm which can be best seen in the recursive \"ins\" function. In the internal N2 nodes, the algorithm is to compare the index (given as an argument) with the left metadata. If the index argument is less than the left metadata, recurse to the left subtree passing the same index; otherwise, recurse to the right subtree, subtracting the index argument with the left metadata. By the end, when you eventually reach the leaf case, the index argument is equal to the position you want to insert into in the current node. (I haven't tried to understand the maths behind this but it's how the data structure works.) At that point, all you do is insert into the leaf node's string (this is the same as inserting at an arbitrary index in any normal string) if you can without exceeding the maximum limit, or else you can insert another node. Then you unroll the recursion. Unrolling the recursion involves updating the left subtree metadata when you reach the parent, and it also involves balancing. (I'm using 1-2 Brother Trees for balancing but ropes don't really care which balancing you use or if you use one at all.) That's pretty much all there is to ropes. The deletion and substring algorithms just require minor modifications (the user might specify a range that includes more than one subtree, so you might need to recurse on both subtrees). You can extend the idea behind ropes to hold more metadata too. For example, rope.sml (Standard ML) also tracks line metadata to allow indexing by line number. The changes required for this are: store an array at the leaf nodes containing indices of line breaks in the string also at this node, and at internal N2 nodes you should also store an additional integer indicating the number of lines in the left subtree. There is an idea I haven't found too useful which is, if two leaf nodes have strings that can be joined without reaching the maximum limit, then join them. I haven't found this idea to improve performance much although it theoretically should. I want to give a shout out to the MLton compiler for Standard ML here - the two rope implementations compiled with it handily beat the fastest ropes in Rust which is surprising. (My code performs well with F# and OCaml, but MLton takes it a level beyond that.) reply fallat 17 hours agoprevThere are so many high performance editors that are just lists of strings. It's evidently false they are not high performance. reply simonw 16 hours agoparentFrom the article: If you really love strings, you might now be thinking \"better than a string? easy: multiple strings.\" And you wouldn't be that far off! Some editors do represent text as an array-of-lines with each line being a string. VS Code's Monaco editor worked that way for quite a while, but an array of strings can still be plagued by the same problems as a single string. Excessive memory consumption and performance issues made the VS Code team look for something better. reply MatthiasPortzel 17 hours agoparentprevLike what? Vim, Emacs, Helix, VSCode, Zed, Xi, Lapce, and Monaco all do not. reply sp33der89 14 hours agorootparentKakoune does, and I found it (without extensions) to perform excellent even with multicursor. Not saying that array of lines is more performant than ropes, but just adding a datapoint here. reply aktau 13 hours agorootparentprevWhat is Vim's internal data structure nowadays? I thought it was essentially a list (array) of strings. reply jgalt212 12 hours agoprev [–] pylint yells at me when my modules are more than 1500 lines. maybe it assumes the module is stored as string and not a rope. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Zed Decoded delves into using ropes, a data structure central to the Zed text editor, offering enhanced editing and memory management over typical strings.",
      "The blog and video illustrate the drawbacks of strings and the benefits of utilizing ropes, particularly SumTree, for managing extensive text efficiently.",
      "SumTree facilitates effective indexing and text manipulation in Zed, enhancing key features such as text display, inlays, and highlights, resulting in better performance and concurrency support in this high-performance code editor."
    ],
    "commentSummary": [
      "Users discuss the pros and cons of Zed, Neovim, and Kakoune code editors, highlighting Zed's speed and functionality.",
      "The conversation covers the use of data structures like ropes and the advantages of abstract syntax trees in code editing, referencing the \"Rope Science\" series and praising MLton compiler's performance with ropes.",
      "Concerns about memory usage and performance issues in editors storing text as arrays of strings are addressed in the discussion."
    ],
    "points": 178,
    "commentCount": 32,
    "retryCount": 0,
    "time": 1714305644
  },
  {
    "id": 40188511,
    "title": "LoRA+: Enhanced Large Model Adaptation",
    "originLink": "https://arxiv.org/abs/2402.12354",
    "originBody": "Computer Science > Machine Learning arXiv:2402.12354 (cs) [Submitted on 19 Feb 2024] Title:LoRA+: Efficient Low Rank Adaptation of Large Models Authors:Soufiane Hayou, Nikhil Ghosh, Bin Yu View PDF Abstract:In this paper, we show that Low Rank Adaptation (LoRA) as originally introduced in Hu et al. (2021) leads to suboptimal finetuning of models with large width (embedding dimension). This is due to the fact that adapter matrices A and B in LoRA are updated with the same learning rate. Using scaling arguments for large width networks, we demonstrate that using the same learning rate for A and B does not allow efficient feature learning. We then show that this suboptimality of LoRA can be corrected simply by setting different learning rates for the LoRA adapter matrices A and B with a well-chosen ratio. We call this proposed algorithm LoRA$+$. In our extensive experiments, LoRA$+$ improves performance (1-2 $\\%$ improvements) and finetuning speed (up to $\\sim$ 2X SpeedUp), at the same computational cost as LoRA. Comments: 27 pages Subjects: Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (stat.ML) Cite as: arXiv:2402.12354 [cs.LG](or arXiv:2402.12354v1 [cs.LG] for this version)https://doi.org/10.48550/arXiv.2402.12354 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Soufiane Hayou [view email] [v1] Mon, 19 Feb 2024 18:33:49 UTC (548 KB) Full-text links: Access Paper: View PDF TeX Source Other Formats view license Current browse context: cs.LGnewrecent2402 Change to browse by: cs cs.AI cs.CL stat stat.ML References & Citations NASA ADS Google Scholar Semantic Scholar 1 blog link (what is this?) export BibTeX citation Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer (What is the Explorer?) Litmaps Toggle Litmaps (What is Litmaps?) scite.ai Toggle scite Smart Citations (What are Smart Citations?) Code, Data, Media Code, Data and Media Associated with this Article Links to Code Toggle CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub Toggle DagsHub (What is DagsHub?) GotitPub Toggle Gotit.pub (What is GotitPub?) Links to Code Toggle Papers with Code (What is Papers with Code?) ScienceCast Toggle ScienceCast (What is ScienceCast?) Demos Demos Replicate Toggle Replicate (What is Replicate?) Spaces Toggle Hugging Face Spaces (What is Spaces?) Spaces Toggle TXYZ.AI (What is TXYZ.AI?) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower (What are Influence Flowers?) Connected Papers Toggle Connected Papers (What is Connected Papers?) Core recommender toggle CORE Recommender (What is CORE?) IArxiv recommender toggle IArxiv Recommender (What is IArxiv?) About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs. Which authors of this paper are endorsers?Disable MathJax (What is MathJax?)",
    "commentLink": "https://news.ycombinator.com/item?id=40188511",
    "commentBody": "LoRA+: Efficient Low Rank Adaptation of Large Models (arxiv.org)174 points by veryluckyxyz 20 hours agohidepastfavorite44 comments batterseapower 19 hours agoThe other recent improvement suggested for LoRA is DoRA: https://magazine.sebastianraschka.com/p/lora-and-dora-from-s.... It really does seem to strongly outperform LoRA - see also https://www.answer.ai/posts/2024-04-26-fsdp-qdora-llama3.htm... reply josalhor 18 hours agoparentI just skimmed over LoRA+ and DoRA and I see no reason why these improvements could not go hand in hand. Actually, LoRA+ seems to be about efficient training while DoRA seems about improving the ability to actually learn, making it significantly more robust. Although I still have my questions on how the improvements of LoRA+ would be applied to the magnitude vector. reply WithinReason 18 hours agoparentprevThe two methods seem to be independent, wonder if you can combine them for even better performance. Interestingly both seem to indirectly modify the optimisation process, in my opinion effectively trying to fix a bad optimiser. Seems like we still have a long way to go after Adam... reply neodypsis 15 hours agorootparent> Seems like we still have a long way to go after Adam... A preprint in arxiv suggests that Adam works better than SGD for training LLMs due to the issue of class-imbalance [0]. It appears that scaling the gradient step helps with the training, for example, see another approach suggested in [1]. 0. https://arxiv.org/pdf/2402.19449 1. https://arxiv.org/pdf/2402.02347 reply Ger_Onimo 17 hours agoparentprevI've just started playing with DoRAs for fine-tuning TTS models towards particular styles of speech, and they're working extremely well! reply allpaca 16 hours agorootparentCan you tell us more about it? Have you reported the results of your experiments in a post? reply mysfi 14 hours agorootparentCount me interested here as well, specially if it is about the style of speech. I had a fun project in mind that involved the style of speech. reply cooljoseph 11 hours agoparentprevThose blog posts are pretty bad. Just read the original paper, https://arxiv.org/pdf/2402.09353. The key section is 4.1. reply cuuupid 19 hours agoprevI’m struggling to understand from this paper whether the approach is better in the general sense (all cases, with wider models seeing greater benefits) or purely for wider models (with narrower models seeing detriment)? If it’s the former this could effectively halve finetuning cost overnight which would go a significant way towards enabling a wider array of use cases for LoRA. reply ironbound 17 hours agoprevI've had sucess with GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection https://arxiv.org/abs/2403.03507 reply Scipio_Afri 4 hours agoparentThis uses less memory so you can do fine tuning or hardware with less vram but at a cost of taking longer on training - there is a throughput penalty, the paper detailing the technique shows something like a 15% decrease in throughput. reply youssefabdelm 17 hours agoprevA better name would've probably been FastLoRA or something reply mobilemidget 13 hours agoparentI was expecting to read about LOng RAng radio communication. https://en.wikipedia.org/wiki/LoRa reply throwaway2562 17 hours agoparentprevfLORA reply yau8edq12i 19 hours agoprevnext [26 more] What an unfortunate name... I initially thought this was about wireless communication. https://en.wikipedia.org/wiki/LoRa reply sorenjan 19 hours agoparentThis gets mentioned here everytime an article about LoRA is posted. Sometimes acronyms means multiple things, they're not in the same field so the risk of confusion beyond short headlines is negligible. It's a bit like if someone reading a bicycling article and getting annoyed that FTP means Functional Threshold Power instead of File Transfer Protocol, or reading about machine learning and getting confused that MLP doesn't mean My Little Pony. reply rakoo 18 hours agorootparent\"computer science\" and \"bicycles\" aren't the same domain, it's fine to have the same acronym. \"computer science\" and \"tv shows\" aren't the same domain, it's fine to have the same acronym. \"computer science\" and \"computer science\" are the same domain, it's not a good idea to use the same acronym. reply dragonwriter 18 hours agorootparent> \"computer science\" and \"computer science\" are the same domain, it's not a good idea to use the same acronym. But “radio communication\" is not “computer science”, even though people sometimes plug radio transceivers into computers, just like “tv shows” aren't “computer science” just because people sometimes view or store their shows on a computer, and “bicycles” aren’t “computer science” because sometimes people mount computers on their bikes. reply WithinReason 18 hours agorootparentprevLarge Models is in the title so it's obviously not about radio reply GuB-42 18 hours agorootparentThe acronym is also spelled out in the title: LoRA = Low Rank Adaptation. reply rakoo 14 hours agorootparentprevLarge models is not spelled in full, and doesn't explicitely says it's not about the communication protocol. reply squigz 18 hours agorootparentprevContext is hard! reply rakoo 14 hours agorootparentSo instead of LoRa and anything else, everyone now has to say LoRa (the communication protocol) or LoRa (the large model thing). Having to add context all the time makes everything so much simpler ! reply squigz 13 hours agorootparentOr potentially include the necessary context in the title of the post. reply WithinReason 14 hours agorootparentprevLow rank adaptation is abbreviated LoRA reply nostrademons 17 hours agorootparentprev\"Computer science\" isn't really one domain anymore - the field split into several subdomains in the 2010s. Just try to get a job as a \"computer scientist\" now - the recruiter would be like \"No, are you a web developer? Mobile developer? Backend developer? Data scientist? Data engineer? Cloud engineer? AI engineer? Machine learning developer?\" reply the__alchemist 18 hours agorootparentprevI think the reason this keeps coming up is encoded in your second sentence, in conjunction with the HN medium: LoRa and LoRA are both, unfortunately, things that the target audience are likely to be interested in and/or knowledgeable with, but a general audience is not. Also, both use a non-standard case mix. reply 1024core 17 hours agorootparentprev> Sometimes acronyms means multiple things Exactly. Like WiFi: from ancient times it has meant \"Wife's Fidelity\". reply teaearlgraycold 17 hours agorootparentTell my WiFi love her reply EGreg 17 hours agorootparentprevFinally! Some people have been screaming to change the acronym since 2001. But these tech bros group didn’t listen. Such hubris! https://en.m.wikipedia.org/wiki/Crypto_naming_controversy reply mattlondon 17 hours agorootparentprevBut these are clearly both in the same field as everyone keeps saying mentioning it here! So clearly there is confusion. It certainly tricked me on first reading - \"ah cool - efficient lora+ that sounds cool... Ah wait no it's just some machine learning spam\" reply yau8edq12i 16 hours agorootparentprev> This gets mentioned here everytime an article about LoRA is posted. I wonder why! reply IshKebab 17 hours agorootparentprevYes but radio protocols and AI methods are a lot closer than most overlapping acronyms. This is obvious from the fact that it gets mentioned every time an article about LoRA is posted. reply kcorbitt 18 hours agoparentprevThis specific variant \"LoRA+\" described in this paper is even harder to search for. I was doing some research on this technique recently and it turns out that \"Lora+\" matches with \"Lora\" in Discord search, which is quite unhelpful. :) reply SquareWheel 18 hours agorootparentDiscord search is one of the worst I've ever used. They remap words like \"localization\" to \"local\", which makes it impossible to search for more specific terms. reply rytill 18 hours agoparentprevThat’s LoRa. This is LoRA. reply bee_rider 19 hours agoparentprevThe idea of low rank approximations is not new, truncated SVDs for example have been used for many decades. reply yau8edq12i 19 hours agorootparentThe acronym LoRA used in the context of deep learning (2021) is about seven years younger than the radio communication protocol called LoRa (2014). Type \"lora\" in a search engine and see what you get. reply kleiba 19 hours agorootparentIn the first 10 search results, I now get a mix of results for either of the two technologies when searching with Google. reply blamestross 19 hours agoparentprevYeah I would prefer they didn't offusicate the actually useful search term reply allpaca 16 hours agoprevThis is old, having been released in February... Why do you talk about it now? reply axpy906 18 hours agoprev [–] In 2024 are folks still swapping out LoRA adapters? Is this still relevant? reply ac2u 18 hours agoparentCan’t tell if your tone is inquisitive or incredulous :) If the later please point out the alternatives. reply bckr 18 hours agoparentprev [–] Why would it not be? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The paper presents a new algorithm, LoRA+, enhancing the Low Rank Adaptation (LoRA) method for fine-tuning models with large width in Machine Learning and Artificial Intelligence fields.",
      "LoRA+ boosts performance and fine-tuning speed by utilizing distinct learning rates for adapter matrices A and B, showcasing up to a 2X SpeedUp in experiments.",
      "The algorithm's efficacy is validated through comprehensive experiments, illustrating its advancement in optimizing model performance."
    ],
    "commentSummary": [
      "The debate focuses on the efficiency of two enhancements to the LoRA model: LoRA+ and DoRA, with suggestions that combining them may boost performance.",
      "Concerns are raised about using the same acronym for distinct technologies, prompting discussions on comparisons with other methods and potential effects on fine-tuning expenses.",
      "This ongoing debate underscores the continual progress and hurdles in refining large models for training and adaptation in the realm of deep learning."
    ],
    "points": 174,
    "commentCount": 44,
    "retryCount": 0,
    "time": 1714311674
  },
  {
    "id": 40191510,
    "title": "Introducing Dotenv: Simplifying File Loading with Unix-like Functionality",
    "originLink": "https://github.com/gyf304/dotenv",
    "originBody": "I like the idea of using dotenv files, but I dislike having to use different language-specific libraries to read them.To solve this, I created a small utility that lets you prefix any command with \"dotenv\" to load the \".env\" file.This is how I imagine dotenv would work if it had started as a UNIX utility rather than a Node.js library.",
    "commentLink": "https://news.ycombinator.com/item?id=40191510",
    "commentBody": "Dotenv, if it is a Unix utility (github.com/gyf304)163 points by gyf304 13 hours agohidepastfavorite81 comments I like the idea of using dotenv files, but I dislike having to use different language-specific libraries to read them. To solve this, I created a small utility that lets you prefix any command with \"dotenv\" to load the \".env\" file. This is how I imagine dotenv would work if it had started as a UNIX utility rather than a Node.js library. tuyiown 4 hours agoPlease just see this `env -S \"$(cat .env)\" ` Believe it or not that’s all you need. > S, --split-string=S process and split S into separate arguments; used to pass multiple arguments on shebang lines edit: forgot the quotes around shell substitution reply kazinator 2 hours agoparentAlso, if we are going to involve the shell, we could also just make .env a shell fragment and do this: sh -c '. .env; ' There is a way to pass commands to it which are reliably executed, like thisL sh -c '. .env; \"$@\"' -- command arg1 arg2 arg3. The non-option arguments passed to the shell are available as `\"$@\"`. A command consisting of nothing but `\"$@\"` basically executes the arguments. We can use `exec`, speaking of which: sh -c '. .env; exec \"$@\"' -- command arg1 arg2 arg3. What I'm getting at is that this form is fairly easily exec-able; execl(\"/bin/sh\", \"/bin/sh\", \". .env; exec \\\"$@\\\"\", \"--\", \"command\", \"arg1\", \"arg2\", \"arg3\", (char *) 0); The command and arguments can be arbitrary strings, not subject to any shell mangling. reply ibotty 2 hours agoparentprevThis will fail with comments. Of course you can script around that as well (I have done so), but it's not bulletproof. It makes sense to have a dedicated tool for the job. reply kazinator 2 hours agoparentprevThis is now syntax that requires processing by the shell. The nice thing about utilities like env and dotenv is that they can be easily exec-ed: execl(\"/usr/bin/dotenv\", \"/usr/bin/dotenv\", \"command\", \"arg\", (char *) 0); -S is a fairly recently added option to the GNU Coreutils env (possibly inspired by BSD?). I have a window to an Ubuntu 18 VM where it's not available. You want $(cat .env) quoted, as in \"$(cat .env)\" so that the content of the file is reliably passed as one argument. -S will split on whitespace; but it respects quoting, so spaces can be protected. Basically .env has to be prepared with the features of -S in mind. Of which that thing has quite a few: escape sequences like , commenting, environment variable substitution. reply userb 1 hour agoparentprevWhat is the difference with or without \"-S\". 'env $(cat .env) ' still work? reply tuyiown 1 hour agoparentprevedit 2: seems that there are expectation around a complex .env unspecified file format I was totally not aware of, I was just merely trying to share the simplest way I've ever found to store and reuse env vars reply petepete 12 hours agoprevI think direnv already does a good job in this space, and it's already available in your package manager. https://direnv.net/ reply reissbaker 10 hours agoparentI don't think direnv and dotenv are really the same — dotenv manages environment variables for a program, whereas direnv manages environment variables for an interactive shell. As an example of the difference, dotenv is useful for running programs inside Docker containers — which do not inherit your interactive shell's environment variables — whereas direnv isn't particularly useful there. Ditto for programs run via init systems like systemd or even classic SysV init. On the other hand, direnv is convenient for end-user env var config, since it's aware of your shell's working directory and updates the env vars based on it without needing to run extra commands. reply kelnos 12 hours agoparentprevI've used direnv, but I think a nice property of OP's dotenv is that it's explicit: if I want to pass env vars, I run my program under it. If I don't, then I don't. There's no \"hidden behavior\" for me to forget about and then get surprised by. reply TheRoque 11 hours agorootparentAs far as I'm aware of, Direnv's behavior is not hidden at all. Whenever you cd into the directory, you get a message listing all the new en var activated. And when you change the .envrc, you get another message saying that direnv has been deactivated. I never had happen to me \"oh shoot !! I forgot this env var was activated because I'm in this dir\". reply macintux 7 hours agorootparentI don't think \"hidden\" and \"explicit\" are true antonyms. From your description, direnv is implicit and noisy, whereas dotenv seems to be (unless you embed it in a script) explicit and quiet. reply cortesoft 5 hours agorootparentprevWhat happens when, 30 commands later, you execute a command in your shell and didn't remember that message from a day or so ago? reply kreetx 5 hours agorootparentOf you're in the directory, don't you want the env to be loaded? For me, being able to forget it is one of the features. reply steezeburger 3 hours agorootparentNot always. I can think of a scenario where I have an env I need to load for some infrastructure stuff, and I may call some commands in a directory that has another .env that's part of what I'm trying to deploy. Generally this scenario is short lived as I'm quickly moving infra commands to automated ci/cd, but I've definitely been in this scenario more than once. reply kreetx 2 hours agorootparentGot it! I think nested envs become confusing really quickly, so there it is indeed better to do it explicitly. My assumption overall in this is that most people have just one .env per project (or perhaps in sub-folders per environment, e.g prod, staging, local), but these don't nest. With nested .env files, the mental overhead they bring remove (IMO) most of the benefits, if not more. reply malicka 5 hours agorootparentprevIt’s the same as forgetting you put something in your .profile or .bashrc, no? In any case, both forgetting the env config and using the same shell for days in a row seem like two things that probably don’t coincide too often. reply jeffhuys 3 hours agorootparentI never close my shell, I never reboot my laptop unless necessary - an uptime of 6+ months is normal. So my experience may be different. reply 3836293648 10 hours agorootparentprevOh yeah, that's the default. Everyone I know im ediately disabled that and I even forgot about it till now reply toastal 3 hours agoparentprev…Just don’t commit your .envrc to a repository & add it to the .(git|hg)*ignore. Provide an example if you want, but don’t expect everyone to want to use your exact settings. This is for your personal environment. reply hk1337 7 hours agoparentprevI know ohmyzsh and zsh has this covered for auto loading the .env when you enter the directory reply grounder 11 hours agoprevCompare with dotenvx - https://github.com/dotenvx/dotenvx This is my current tool of choice. reply tester457 12 hours agoprevdotenv started as a ruby library actually. The first implementation inspired the others such as the golang version of the library. reply wodenokoto 1 hour agoprevSince loading dotenv files happens together with executing code I I have decided to trust my .env files just like I trust the rest of my code not to delete my entire system and therefore I source them. reply miohtama 12 hours agoprevThere is also shdotenv that allows you to load different .env file formats and convert between them, e.g. for UNIX shell. https://github.com/ko1nksm/shdotenv reply sirwitti 3 hours agoparentI recently discovered shdotenv and I like it a lot! reply gyf304 9 hours agoparentprevThis is a very neat project that seems to accomplish the same goal and have some extra features. reply mongol 12 hours agoprevWould not sh -c '. .env; echo $MY_VAR' do the same thing? (I am not in front of a shell at the moment.) reply emmanueloga_ 8 hours agoparentThere are like a couple dozen different ways to do this... I have this on my .bashrc: alias loadenv='export $(xargsPrograms should simply bundle their dependencies. awww. i don't think it's OK in any way to download libc6/msvcrt as many times as I download __any__ software. even more, is there a strong difference between dependency and runtime environment? if sensible people does not bundle the whole python distribution to a \"stuff.py\" then why bundle libopenssl.so to a webserver application? IMO, a saner approach would be just not to confuse dependencies: appX depends on libY 1.9; appZ depends on libY 2.0; people are quick to declare that appX and appZ are incompatibe as they can not run on the same system due to \"conflicting dependencies\". but who said you have to seach libY in /usr/lib*/libY.so? if you need different versions of a lib, just install them in separate dirs and make your apps find the right one (eg. by setting RPATH or versioned .so filenames). reply imtringued 3 hours agorootparentprevDocker is \"programs bundling their dependencies\". reply forrestthewoods 3 hours agorootparentIn an extremely heavyweight and needlessly convoluted way. reply bitwize 4 hours agoprevC? Y u no Rust? reply masklinn 3 hours agoparentThe same thing already exists in Rust, it’s both a library for in-process loading and a binary, I use it daily and only for the binary: https://github.com/allan2/dotenvy reply bitwize 1 hour agorootparentOnce I actually wrote a version in Emacs Lisp, for purposes of being able to run stacks that depended on .env configuration in Emacs buffers. reply spullara 10 hours agoprev [–] Claude 3 Opus & GPT-4 have some comments on a few things that you might want to check to make sure they aren't hallucinating. reply VWWHFSfQ 10 hours agoparent [–] what are the suggestions reply spullara 10 hours agorootparent [–] GPT-4: The code provided has a few potential issues, including security vulnerabilities: Buffer Overflow and Memory Allocation Errors: The malloc function in read_file does not check if the memory allocation fails (it checks if buffer is NULL instead of buffer). This can lead to a null pointer dereference if malloc fails and returns NULL. There's a possibility of buffer overflow or improper handling if the file size read by ftell is exactly MAX_FILE_SIZE, because an additional byte is added (buffer = malloc(file_size + 1);). Improper Input Validation: The skip_until_charset and skip_charset functions do not check if the end of the buffer is reached within their loops. If the charset does not appear in the buffer, this could lead to reading beyond the buffer's limit, causing undefined behavior. Use of Uninitialized Variables: In the main function, the value_end pointer is set conditionally inside the if (cur == '\"' || cur == '\\'') block but used unconditionally outside it. This can lead to the use of an uninitialized pointer if the if block is not executed. Command Line Argument Handling: The check if (argc < 1) is unnecessary because argc will always be at least 1 in any C program, as it includes the name of the program itself. Insufficient validation of command arguments, specifically not handling scenarios where argv[1] might not be provided correctly or safely. Potential Command Injection: The execvp call uses user-supplied input (argv[1] and argv[2...]) without sanitization. If the environment or the .env file can be manipulated, it could lead to command injection vulnerabilities. Error Handling: The program's error messages (e.g., \"Missing equal after key\") are clear but the handling of such errors (e.g., continuing to parse or halting operations) may need to be revisited based on desired robustness. Claude: Here are a few potential security vulnerabilities and bugs I noticed in the code: The read_file function does not properly handle the case where malloc fails and returns NULL. It should check if buffer is NULL after the malloc call and handle the error appropriately, likely by returning an error code. Currently it would dereference a NULL pointer. The code trusts the contents of the .env file. If an attacker can modify the .env file, they could potentially inject malicious values into environment variables that might be used in an unsafe way by the child process. There are no checks on the size of keys or values. A malicious .env file could contain extremely long keys or values that could potentially cause issues. The code uses strchr to look for meta characters like '=', '#', etc. If the .env file contains non-ASCII characters, this could potentially lead to unexpected behavior. The code uses execvp to execute the provided command with the parsed environment. If the command is provided by an untrusted source, this is a command injection vulnerability. The code should validate the command is on an allowlist before executing. The parsing allows for escape sequences in values, but doesn't have bounds checks. A maliciously crafted escape sequence could read or write out of bounds. The code will modify the process environment even if executing the final command fails. It may be better to parse into a separate data structure and only update the environment if the final execvp call will succeed. It's a good practice to set a proper process umask before writing files or creating directories accessible to other users on the system. While the code looks generally well written, for a utility like this that parses untrusted input and executes commands, it requires very careful security review and hardening. I'd recommend a thorough security audit before using it on untrusted .env files or with untrusted commands. Adding additional input validation, using allowlists, and limiting the scope of what env vars and commands are allowed would help mitigate some risks. reply brenns10 10 hours agorootparentI think if you're not willing to manually verify the output of these generative machine learned algorithms, then you probably shouldn't present them to somebody as if you've done them the service of a free code review. reply gertop 9 hours agorootparentI don't understand people who chime in just to regurgitate GPT slop that they haven't verified (and sometimes not even read!) I don't it hard to believe that they truly believe they're doing us a favor. Surely they're doing it just to feel smart or included? reply spullara 9 hours agorootparentprevI suggested he look at it since he is the expert on his code lol. reply gyf304 9 hours agorootparentprev* Memory allocation NULL check: this is a bona-fide bug introduced by my refactoring * MAX_FILE_SIZE: I don't think this is true. * skip_until_charset, skip_charset bound check bug: I don't think this is true * Uninitialized value_end: I don't think this is true - and if true should be caught by -Wall -Werror flags. * argc < 1 check not being necessary: This is not true, you can make argc == 0 by using the exec family of libc functions. * Error Handling: Currently all parsing errors should cause the program to exit, which I think is the desired behavior. * Unsanitized input for .env: Intended behavior. * Unsanitized input for execvp: Intended behavior. reply jmprspret 10 hours agorootparentprev [–] GPT-4 is not a static code analyser reply dotancohen 5 hours agorootparent [–] These LLMs are not a lot of things that people think they can use them for, apparently. Therapist, search engine, cheap coding labour, the list goes on. These LLMs produce syntacticly valid language, no more. And information contained within is a by-product and not necessarily factual nor correct. reply jeffhuys 3 hours agorootparent [–] Still incredibly helpful. Therapy? Helped me a couple of times on some dark days. It’s not a replacement for it, but it helped me. Search engine? Why not? As long as you verify claims, you’re good, and especially nowadays the answers are preferable to whatever Google thinks should be on the first 10 pages (hint: it’s all crap). Cheap coding labour? Again, as long as you verify. A senior programmer can get a lot more done in a day this way. It’s not necessarily factual or correct, no. But it’s incredibly helpful and useful. As always, the real answer is somewhere in the middle. Just don’t ask it about politics. It’s so blatantly obviously biased. reply dotancohen 1 hour agorootparent [–] If you don't mind, I'm really interested in how you used an LLM for therapy. My Gmail username is the same as my HN username if you feel comfortable talking about it and helping others. Thank you! reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The author developed a utility enabling users to load dotenv files effortlessly using a prefixed command, removing the dependency on language-specific libraries.",
      "This utility mimics the functionality of dotenv as a UNIX utility rather than a Node.js library, simplifying the process of loading dotenv files."
    ],
    "commentSummary": [
      "The discussion explores managing environment variables, dealing with conflicting dependencies, enhancing code security, and the drawbacks of using Large Language Models (LLMs) for various tasks.",
      "Various methods and tools are proposed for managing environment settings, resolving conflicts, and bolstering code security.",
      "There is a debate on optimal practices for managing environment variables and the efficiency of LLMs in diverse functions."
    ],
    "points": 163,
    "commentCount": 81,
    "retryCount": 0,
    "time": 1714335946
  },
  {
    "id": 40189346,
    "title": "Uncomfortable Traditional Life Jackets Replaced with One-Use Balloons",
    "originLink": "https://fearoflanding.com/accidents/accident-reports/the-unstallable-plane-that-stalled/",
    "originBody": "On 26 Apr 24, Gene says: 26 April 2024 at 13:31 I didn’t see life jackets while flying being in the slightest bit comfortable, but then I googled aircraft life jackets, and they were very different than what I was familiar with. Instead of being canvas bags of kapok or styrene or whatever, they’re thin flat plastic one-use balloons. Yeah, I don’t see them being daily use items. Also, the 3 paragraphs after the first picture are repeated after the second one. Reply to this comment",
    "commentLink": "https://news.ycombinator.com/item?id=40189346",
    "commentBody": "The unstallable plane that stalled (fearoflanding.com)163 points by sni 18 hours agohidepastfavorite159 comments jcalvinowens 17 hours agoIt is not necessary to empirically determine a specific airplane's stall speed in order to operate it safely. It's not required in the US, we just use the number the manufacturer publishes. It's normal for airplanes of the same model to fly differently: I fly a little fleet of six Citabrias, and their stall characteristics are radically different. You'd expect more uniformity from a modern aluminum airplane, but still: nobody should be flying an airplane like this so close to the edge the exact stall speed needs to be known numerically within one knot. The 40lbs of gas I burn flying for an hour decreases the stall speed by more than 1mph on those Citabrias I fly. EDIT: I was mistaken, this isn't a requirement in Europe either. reply fransje26 29 minutes agoparent> It is not necessary to empirically determine a specific airplane's stall speed in order to operate it safely. It's not required in the US, we just use the number the manufacturer publishes. It is not necessary as in \"there is no legal obligation\", true. But if you want to live, it is absolutely necessary. The numbers provided by the manufacturer will tell you absolutely nothing about the stalling characteristics of the airplane as soon as you start modifying it. The non-linearity of the phenomena involved in stalling also mean that \"intuition\" and \"small changes\" will be of absolutely not help to determine by how much you changed the characteristics of your airplane. reply WalterBright 15 hours agoparentprev> nobody should be flying an airplane like this so close to the edge the exact stall speed needs to be known numerically within one knot. An experienced pilot can feel the stall coming on with a bit of a \"burble\" in the stick. My dad (fighter pilot) told me that knowing exactly where the stall point is is life and death. When you're in a dogfight, the winner often is the one that can turn inside the other. Turning as tight as you can requires getting exactly on that edge of the burble. It's the same thing as in automobile and motorcycle racing. How close you can get to a slide without sliding is the difference between victory and ignominy. reply fransje26 35 minutes agorootparent> An experienced pilot can feel the stall coming on with a bit of a \"burble\" in the stick. Without any disrespect to the flying abilities of your Dad, as an aerospace engineer, the short answer to that is \"no\". You will have the luck to feel a \"burble\" if the airplane has been been built to be give you that warning. You have absolutely no guarantee that once the airplane has been modified beyond its original configuration, you will get any form of warning. This extreme behavior is actually partly present in the current case, with the stalling point jumping from the wing tip immediately to the wing root. That's no good. It can be even worse if you're flying a plane that is licensed under the \"experimental\" category by the FAA. One of the preferred airfoil for some time was the NACA 5-series, as it can have very little lift-induced pitching moment. It has also criminal stalling characteristics with absolutely no warning. Something beautifully illustrated by the following lift polar: https://media.cheggcdn.com/media/76b/76b33250-7f26-41f6-8f2c... reply jcalvinowens 13 hours agorootparentprevSure, that's why I said \"airplane like this\": in car terms, the plane we're talking about here is a minivan. reply JumpCrisscross 4 hours agorootparentprev> An experienced pilot can feel the stall coming on with a bit of a \"burble\" in the stick Stall recovery has also been an essential part of my flight training. reply wkat4242 2 hours agorootparentThe most dangerous situations are when you're flying low and slow like before a landing though, when there's often no time for recovery (especially when you end up in a spin due to asymmetric stall). When I was flying I always worried about this too the point of approaching too fast. But I never finished my PPL anyway reply datascienced 2 hours agorootparentprevAlthough can you recover at a height 15m above terrain? reply peteradio 15 hours agorootparentprevImagine dogfighting in a Cessna! I imagine the arms to be pilot wielded colt 45s and first officer to be a jug of whiskey. reply buildsjets 12 hours agorootparentOne of the last aircraft shot down in the European theater of WWII was a Piper Cub that shot down a Fieseler Storch with a .45 pistol. They landed next to the crashed Storch, captured the pilot, and treated his injuries. Then handed him over to the Russians, who I am sure also treated him in accordance with the provisions of the Geneva convention. https://theaviationgeekclub.com/that-time-a-usaaf-piper-l-4h... reply wizzwizz4 11 hours agorootparentPerhaps in accordance with the spirit, but the Geneva convention was in 1949, so those provisions didn't exist yet. reply dabluecaboose 10 hours agorootparentThere were several Geneva Conventions prior to the 1949 one; the \"Convention relative to the Treatment of Prisoners of War\" was signed in 1929. https://ihl-databases.icrc.org/en/ihl-treaties/gc-pow-1929 reply resolutebat 6 hours agorootparentprevThe current Geneva Convention was in 1949, but the original Geneva Convention dates back to 1864. Of course, the previous poster is noting that the USSR blithely disregarded the conventions for others' POWs as well as its own: most returning soldiers were considered traitors to the Motherland and summarily executed or sent to the Gulag to die more slowly. reply dekhn 11 hours agorootparentprevif you like that idea and science fiction, see the second book in the Greatwinter trilogy by Sean McMullen (read them in order, though) reply KennyBlanken 6 hours agorootparentprevhttps://en.wikipedia.org/wiki/Cessna_A-37_Dragonfly https://en.wikipedia.org/wiki/Cessna_O-2_Skymaster https://www.airforce-technology.com/projects/ac-208-combat-c... reply logical_proof 15 hours agorootparentprevLove this description lol reply macintux 16 hours agoparentprevI'll admit to complete and total ignorance, but: > ...we just use the number the manufacturer publishes. From the article it sounds like this plane was radically modified, to the point where the manufacturer's stall speed would be irrelevant. Why wouldn't you want to confirm for yourself where the speed is after so many changes? reply dramm 15 hours agorootparentRadical has all the wrong implications. It's a \"major alteration\" in a regulatory sense, done from an approved kit of parts, with a very well documented installation and post-installation operation and maintenance procedures. The aircraft was modified with a Robertson STOL kit. A common type of modification to make to a \"bush aircraft\". In the USA the modification is covered by an STC (Supplemental Type Certificate), the installation needs to be supervised and approved by an A&P technician with IA (Inspection Authority). The STC will modify the airspeed indicator markings, including the stall speed markings (bottom of green and white arcs), and modify the approved flight manual/pilot operating handbook and maintenance documentation for the aircraft. Since this is a major alteration (in a 14CFR regulatory sense) that modifies the flight characteristics of the aircraft it needs to be test flown after the work, and the STC will also separately requires this. I expect/hope the STC includes instructions for checking stall characteristics including airspeed. In European countries a similar level of regulation/documentation is followed based on the USA STC. Give the description of the pilot's sad lack of understanding of basic operation of the aircraft I am doubting they even read the pilot operating handbook. reply avianlyric 12 hours agorootparentAs noted in the article, the plane had been modified with far more than just the STOL kit. > A further issue was that his Cessna 185 had been extensively modified. The addition of floats, cargo pack, short take-off and landing kit and a three-blade propeller had never had their combined effect documented. That’s a lot of things that modify the flight characteristics of a plane, all interacting together in what seems to be a previously untested configuration. I can completely see how each individual modification might modify the planes flight characteristics in a well know manner. But I struggle to see how anyone could realistically predict the result of all the modifications without some basic empirical testing. reply dramm 10 hours agorootparentA three bladed prop modification along with floats is very common. Wipaire, the leading float manufacturer has its own STC for that. In combination with STOL kits is not uncommon either. I expect the Robinson STOL STC explicitly accommodates floats (Wipaire’ STOL STC sure do) but not sure since I don’t have the paperwork in front of me. You are over inflating issues here. Are you a pilot? The issue here is straight up incompetent operation of an aircraft. Operating a floatplane and separately operating with a STOL kits can increase complexity and risks especially when aircraft are mishandled/abused like here with an incompetent pilot who does not know what they are doing. You don’t need to dream up issues from combination of stuff here, the simple linear addition of issues was more than enough for an incompetent pilot to get into trouble, and seems they are lucky they did not get into more trouble before, and lucky they were not killed. The C185 is a beautiful well behaved workhorse used extensively in bush and floatplane flying, often with multiple STCs to upgrade these aircraft. reply jcalvinowens 16 hours agorootparentprevMy understanding is that in the US, part of the modification would be updating the plane's official operating limitations, and there could be a new stall number. Still a number from the manufacturer, not a number empirically determined by testing that specific airplane. For special one-off modifications, I don't know: I've always been told that's almost impossible with certified airplanes. One Citabria I've flown had vortex generators installed on the leading edge of the wings, but the club sold it over a year ago and I don't remember if it listed a modified stall speed. I do remember it said \"not airworthy if more than N of the VGs are broken off\", I think it was three? reply dghlsakjg 16 hours agorootparentprevStall speed depends on so many factors that it can change significantly in a single flight. Weight, altitude, density altitude, angle of attack etc. are all going to have an effect. In other words, sure, you might want to confirm it, but you should also give yourself some margin since you don’t ever really know what the stall speed is until you stall. reply KolmogorovComp 12 hours agorootparent> angle of attack etc This is wrong. The angle of attack is not a parameter of the stall speed, it is the cause of the stall (for a given configuration, assuming well below transonic speed). This is why for example, for precise handling you should use an angle-of-attack indicator (a large majority of fighter jets and more generally military aicraft have it). > you should also give yourself some margin since you don’t ever really know what the stall speed is until you stall The manufacter speed already take into account the afore-mentioned parameters into account, and the resulting speed is the worst case scenario, if not told otherwise (usually max weight, max forward CG). One should not fly with an arbitrary speed margin, but instead use well-known speedq (1.3Vs, 1.45Vs, where Vs=stall speed in the given config) depending on the flight phase, and remember well the bank angle limit associated with them. reply vdqtp3 6 hours agorootparent> The angle of attack is not a parameter of the stall speed, it is the cause of the stall You're right of course, based on the real definition of angle of attack which is based on relative wind. When people say angle of attack is a parameter of stall speed, they're separating speed from pitch. This isn't really useful but people seem to struggle with the concept of relative wind, so it's a kind of shorthand. reply dghlsakjg 6 hours agorootparentThis is what I was getting at. Thanks. Also there’s planes that have slats and other devices that effectively change the angle of attack of the wing. And then there’s planes that direct prop wash over the wing so that the power on stall speed is much lower than power off despite the angle of attack being different. reply SoftTalker 8 hours agorootparentprevAs Juan Browne (blancolirio) likes to say, you can stall a wing at any airspeed and any attiude but only one critical angle of attack. reply BruinsInSeven 15 hours agorootparentprevHow do you determine a margin without some form of a baseline? reply dghlsakjg 15 hours agorootparentYou read the operator’s handbook which will give you that information. In a certified aircraft the manufacturer has already done the test flying reply ryandrake 10 hours agorootparentYour airplane can also be in the \"experimental\" category, which generally means \"homebuilt\" so the stall speeds and characteristics can only be determined through test flight. I made the decision to empirically demonstrate stall speeds on the very first flight of my homebuilt, before attempting to land, so I knew for sure what they were. reply dghlsakjg 6 hours agorootparentI also have a homebuilt, and went and found out my stall speed. But most homebuilts will still have a stall speed for planes built to plan. In any case, this is an article about a Cessna. Where the stall speed is in the POH and also marked on the ASI. reply WalterBright 15 hours agorootparentprevDon't forget ice! reply greedo 14 hours agorootparentAnd temperature... reply hluska 16 hours agorootparentprevThe plane had a cargo pack and a Robertson STOL. Cargo packs are essentially for bush planes and as an example, the 1975 Skywagon’s owners manual even had one diagrammed. Robertson STOL’s are extremely common in Northern Canada to the point that even as a passenger I know about them. It’s nothing too radical. Edit - Here’s a copy of the 1975 owners manual: https://www.seaplanescenics.com/documents/1975-cessna-185f-p... reply rob74 2 hours agoparentprevI'm not a pilot or an aviation expert, but I think you're arguing against a strawman here. The investigation report said nothing about determining the precise stall speed, it just recommended \"that pilots be informed about the stall behaviour of the Robertson short take-off and landing kit on the Cessna 185 aircraft, both by listing the issue in the aircraft flight manual supplement and through education through the aviation authority\". The fact that the stall speeds were not listed on the test flight report was just further proof that the pilot didn't actually test the stall behaviour of the aircraft. What the investigation report also didn't mention is that pilots should actually practice stall recovery and not just think \"if I believe strongly enough that my plane is unstallable, I can ignore stall recovery\". But that probably goes without saying... reply t0mas88 16 hours agoparentprevEuropean pilot here :) it's also not a European thing. It works the same here as it does in the US, you use the published number in the flight manual. But this plane had significant modifications done. And if you do things that significantly change performance, you'll need to get updated performance data. Or the provider of the supplemental type certificate that allows the modification has to provide an updated flight manual with that data. reply sokoloff 14 hours agoparentprev> nobody should be flying an airplane like this so close to the edge the exact stall speed needs to be known numerically within one knot Citabria's are often flown in aerobatics (Citabria backwards is airbatic and for a while, they were the only aerobatic aircraft being commercially manufactured in the US) and a lot of aerobatic maneuvers involve stalling the wing. reply SR2Z 13 hours agorootparentYes, but anyone doing aerobatics so close to the ground that they can't recover from a stall is understood to be doing that at their own risk. Anyone doing the above with unfavorable wind has a death wish. reply jcalvinowens 12 hours agorootparentprevYou don't really look at the airspeed indicator for that, you feel the stall in the reversible controls. That feel is incredibly precise. reply taneq 3 hours agoparentprev> It's normal for airplanes of the same model to fly differently In my experience any equipment this large is effectively handmade, with all the variability implied by this. I'd hope aircraft are made to tighter tolerances than the stuff I work with but a couple of millimeters is often enough to have a noticeable effect on things like actuator travel. > nobody should be flying an airplane like this so close to the edge the exact stall speed needs to be known numerically within one knot. This is exactly my takeaway as well. Rather than trying to determine the characteristics of the machine to the n'th degree, assume a realistic degree of variation and design for it accordingly. reply hinkley 16 hours agoparentprevDecreases the stall speed? How does that work? reply ketralnis 16 hours agorootparentThings that can influence stall speed include weight, power, center of gravity, flaps/landing gear configuration, and more. Why? Well, stall speed isn't a real thing. There isn't a speed at which you stall, that's not how it works. It's a convenient short-hand that we use for the more complicated reality. The physical reality is that stalls happen at a particular angle of attack (AOA) into the apparent wind. That is, the angle of your wings relative to the airflow. Up to the critical angle a higher AOA means more lift to counteract gravity. As you slow down you generate less lift because there's less airflow over the wings. So as you slow down, in order to generate a similar amount of lift you have to increase your AOA. If you keep slowing down and adjusting your AOA to compensate, you'll reach a speed that's low enough and therefore AOA high enough that adding more AOA no longer adds more lift (the air no longer flows smoothly over the wing). That's the stall speed, the speed at which more AOA no longer generates more lift. But it's the AOA that's the problem, not the airspeed. In addition to lower speeds needing more AOA, you also need a higher AOA if you weigh more. A wrong but illustrative way to think about it might be that you need the engine's thrust pointed more towards the ground the more you weigh. That means that as you burn fuel (lose weight) the AOA that will stall you doesn't change, but the excess AOA available due to your weight-change does so in effect the air speed at which you would be near the critical AOA to stay airborn does change. Stall speed is still a useful concept especially while landing but it's misleading outside of landing and when anything else is remotely unusual like weight or modifications to the plane. For this reason the FAA has been trying to get AOA indicators installed in planes and to train pilots to look at those instead of thinking about stall speeds https://www.faa.gov/sites/faa.gov/files/2022-01/Angle%20of%2... reply sokoloff 14 hours agorootparentprevThe Robertson STOL mod droops the ailerons with flaps, changing the effective angle of incidence of the wing. A friend had a Robinson-equipped 182 and we could quite comfortably operate in/out of Marlboro Airport (1650' paved with trees at one end and a fence at the other). reply vdqtp3 6 hours agorootparentThat's not TERRIBLY impressive, considering a stock 182P has a 50' obstacle take off distance of 1350' of at max gross on an ISA day reply t0mas88 16 hours agorootparentprevThe published speed is at maximum takeoff weight, with the most unfavourable center of gravity (usually most forward) and idle power If the conditions are better (not at max weight, rear center of gravity, engine power adding more airflow over the wings) you can fly below the published stall speed number. reply hinkley 15 hours agorootparentYeah I was thinking backward. Lower stall speed is better, not worse behavior. reply matheusmoreira 15 hours agorootparentprevThis is a good resource: https://ciechanow.ski/airfoil/ Useful to think of the airplane as standing still while the engine accelerates the air around it. To fly, you need the air to move over the wings quickly and in the right direction. You can sort of trade how fast you need the air to go for how ideally the air is flowing over the wings. If you angle the wings just right against the air flow, and/or you bend them out of shape just right with flaps, you can slow down a lot while relying on the air itself to carry your plane. If you're flying against the airflow, you need to go faster. This is usually done during take off and landing. The pilot lowers the flaps when approaching to land and flares the aircraft before touchdown, all to make the air flow efficiently into the wings, thereby allowing the aircraft to slow down without falling straight down like a stone. That's why weather is so important for flights. Pilots need to be ready to call TOGA and go maximum thrust at a moment's notice just in case some crosswind or heat wave or something screws up the direction of the air flow just as they're about to land. Many an admiral cloudberg article has been written due to that sort of thing. You angle the plane just right, slow it down just to the edge of stalling, then some phenomenon happens and increases your stall speed past your current speed... reply the__alchemist 16 hours agoprevSoapbox: Stall speed is an approximation. It's baffling that GA aircraft don't have one of the most safety-critical measurements: AOA. Stall airspeed varies with a number of factors; this includes the mods described in the article, and weight change from burning fuel, passengers, payload etc. AOA is more invariant to that as a metric for choosing stall speed, speed down final etc. reply sumofproducts 16 hours agoparentI was very skeptical of this until I had the chance to fly one of those brand-new C172 models that come equipped with 'em. They're so convenient! Sure, ye olde haptic feedback + inner ear + stall horn/shaker combo has always worked for me—but if you are a new or overwhelmed or complacent or unlucky pilot, having a big angry indicator sitting atop the glare shield furiously (visually & audibly) informing you of the approaching cross-control stall that is about to bury you in your base-to-final grave makes danger IMPOSSIBLE to miss. The LEDs were bright enough to be clearly visible even under direct sun, but the Geiger-esque clicking and chattering increasing in urgency as I approached critical AoA made it for me. No need to put your head down or even alter your scan to include it: you can hear trouble coming! reply speedbird 45 minutes agorootparentThis +++. Most important parameter is AoA and we only have poor proxy for it in most GA. Nuts reply ExoticPearTree 55 minutes agoparentprev> Stall speed is an approximation. It's baffling that GA aircraft don't have one of the most safety-critical measurements: AOA. All aircraft equipped with a six pack, you have an artificial horizon indicator which can tell you what is your angle of attack. And Cessna's have them in all planes, even 50-60 years old ones. reply ultrarunner 16 hours agoparentprevSoapbox next to your soapbox: GA planes are old specifically because of the FAA and their overly restrictive regulations. The cost involved to create an AoA sensor & readout is minimal and at least one company has done it with an IMU only. The cost to certify, sell, and install an AoA sensor (in terms of both money and time waiting to get on the schedule of an FAA-blessed installer) is more than most people find it to be worth. Food for thought: this also applies to shoulder harnesses in many cases. Aviation could be cheaper, safer, and better in general if the FAA was not stuck in the 60s. reply jrockway 11 hours agorootparentI don't think that's completely true. There is a combination of market size and regulatory burden; not a lot of people are buying GA aircraft (compared to say, the number of people buying iPhones), so there isn't an enormous financial incentive to get people out of their C172 or Bonanza. I also think that these old airplanes are really ships of theseus. Maybe there are some original stickers and seats, but that's about it. Safety and avionics upgrades on these old airframes are definitely in the financial reach of many readers of this forum, and I'm sure many people are flying \"old\" airplanes that have AoA sensors and IFR-certified glass panels and backups. Day to day they probably feel a lot like airline pilots. reply vdqtp3 6 hours agorootparent> many people are flying \"old\" airplanes that have AoA sensors and IFR-certified glass panels and backups Yes and no. They're out there, but they aren't as common as you'd think or hope. AoA in particular is rare. There's an awful lot of planes that are still running GNS430 or GNS530s (probably more than any other single setup), and more than a few with the original nav/comm equipment like the KX170B. A real glass panel (something like the G1000 or G3X) is really rare in an old plane. Maybe 5-10%? reply sokoloff 14 hours agorootparentprevAoA indicators are able to be installed in certified aircraft as minor modifications, per the FAA policy from 2014. There are FAA regulations that are overly conservative IMO, but I think the FAA has a sensible stance on AoA indicators. reply almostnormal 11 hours agorootparentCS-SC251c in the EU. reply KennyBlanken 6 hours agorootparentprevThe FAA is stuck in the 60's because there is a massive industry supporting the ancient technologies in general aviation. Modern stuff, like you point out, is far more reliable, cheaper, lower power consumption, and more functional. That better reliability means less need for aircraft mechanics and avionics shops. Nobody would want their discreet component transponder overhauled if it could be replaced with a cheaper unit that uses modern wizardry like logic chips or even (gasp) a microcontroller. Ditto for leaded fuel air cooled piston engines with manual mixture controls that require teardowns all the time. That bullshit is only still around because Continental and Lycoming want it to. reply wkat4242 2 hours agorootparentYeah and because of that we're also still exposed to lead fumes and pollution :( It's really time for GA to get into this century. reply teeray 9 hours agoparentprevMy favorite anecdote I heard of from a flight instructor is that of a cargo plane taking off that had some heavy vehicle tied down in its hold. It broke free of its straps during takeoff and rolled to the back of the aircraft, which shifted the CG such that the plane entered an unrecoverable stall and crashed. reply aaronharnly 7 hours agorootparentI’m not sure if this is the one your instructor was referencing, but there was a fairly notorious such event in Afghanistan about a decade ago. https://en.wikipedia.org/wiki/National_Airlines_Flight_102 https://www.faa.gov/lessons_learned/transport_airplane/accid... reply wkat4242 2 hours agorootparentAlso the excellent (as usual) write up from Admiral Cloudberg https://admiralcloudberg.medium.com/strength-in-numbers-the-... reply wlonkly 7 hours agorootparentprevThere is a video which makes its way around the Internet regularly of this crash[1] which was attributed to exactly that, killing all seven on board. (Which makes it pretty heavy for an anecdote, admittedly.) [1] https://en.wikipedia.org/wiki/National_Airlines_Flight_102 reply analog31 5 hours agoparentprevI'm not a pilot, but now I'm curious. What do you actually do if the plane stalls, or is about to do so? Other than die? reply Toutouxc 5 hours agorootparentYou decrease the angle of attack, i.e. you let the nose drop a bit. When the stall has fully developed into a spin, you perform the spin recovery procedure for your aircraft. Hopefully you still have some altitude left. reply lovecg 10 hours agoparentprevI’ve been thinking that now that we have glass cockpits the airspeed indicator could automatically reflect the current stall speed under all conditions. I.e. green arc could shrink in a steep turn, etc. reply the__alchemist 9 hours agorootparentI concur that this would be an equivalently-useful mechanism to displaying AOA directly. reply andoando 15 hours agoparentprevBut we got a nice tea kettle whistle to tell us were about to die. reply buildsjets 12 hours agorootparentI've always thought it sounded like a kazoo. Or sometimes a harmonica. But I like the imagery of a a little tea kettle on a hob under the panel. reply gameshot911 14 hours agoparentprevGentle reminder that it's good practice to define your acronyms at least once, particularly for audiences they may not be SMES. reply stordoff 10 hours agorootparentSMES = Subject Matter Experts? (searching SMES/SME mostly brings up small and medium-sized enterprises) reply mango7283 6 hours agorootparentYes, subject matter expert is what it means in that context. Now I need an SME in linguistics to tell me if the usage as an acronym on that sentence is irony or not. reply talkingtab 16 hours agoprevNon sequitur from non-pilot: I was once in Duluth, MN in the bitter cold and watched a Cessna with skis (for landing on the frozen lakes of the Boundary Waters) land at the airport. It was the utterly bewildering to see how slowly it was going in the air. And how little distance it took to stop. Short Landing Kit I assume. I've seen ducks and geese come into land on lakes at higher speeds! reply int_19h 14 hours agoparentThere are quite a few planes specifically designed for that kind of thing, e.g. https://en.wikipedia.org/wiki/Antonov_An-2: \"According to the operating handbook, the An-2 has no stall speed. A note from the pilot's handbook reads: \"If the engine quits in instrument conditions or at night, the pilot should pull the control column full aft and keep the wings level. The leading-edge slats will snap out at about 64 km/h (40 mph) and when the airplane slows to a forward speed of about 40 km/h (25 mph), the airplane will sink at about a parachute descent rate until the aircraft hits the ground.\" As such, pilots of the An-2 have stated that they are capable of flying the aircraft in full control at 48 km/h (30 mph) ... This slow stall speed makes it possible for the aircraft to fly backwards relative to the ground: if the aircraft is pointed into a headwind of roughly 56 km/h (35 mph), it will travel backwards at 8 km/h (5 mph) whilst under full control.\" reply schoen 9 hours agorootparentI once got to ride in an An-2 on a tourist sightseeing flight. I've always remembered the sense that the pilot didn't even bother to line up with the runway ahead of time when landing, but simply took a gentle turn onto it upon reaching it, at roughly the speed of a car. I'm sure this is a slight exaggeration in my memory, but it really was able to fly incredibly slowly and take turns in an incredibly short distance. reply hakfoo 3 hours agorootparentYou'd think that sort of handling would make for some interesting use cases-- aerial photography for example, maybe some sorts of cropdusting where the slower speed offers more precision. I wonder if there's anything else built that fits that market today, since I suppose it's a hard-sell to convince Western pilots and businesses to buy Soviet old-stock. reply hinkley 16 hours agoparentprevSome of those Cessnas have a stall speed so low they can fly backward on a windy day. reply Wistar 15 hours agorootparentI have been a passenger in a C-170 with a STOL kit that flew backwards over a beach on the Washington coast. We landed well behind where we took off. The takeoff and landing were both nearly vertical. Had a steady wind from the ocean. reply schoen 9 hours agorootparentWas that intentional or unintentional? reply 1letterunixname 14 hours agorootparentprevYou forgot to unfurl the course sails and get out and push. reply 20after4 15 hours agorootparentprevI'm sure it's happened a few times with unsecured planes in a windstorm. It would be a neat trick to see a pilot pull that off intentionally and under control. reply 1letterunixname 14 hours agoparentprevA certain STOL-modified Piper Cub barely needs a runway longer than a driveway. https://youtu.be/hPakbghLe38 reply simonblack 13 hours agoprevSo many factors that could be blamed in this story. As in every other aircraft accident. But EVERY plane will stall. THERE IS NO SUCH THING AS AN 'UNSTALLABLE' PLANE. I once overloaded a plane inadvertently, by having a heavy load plus a full weight of fuel. At the point of lift-off, the stall-warning started screaming at me as I started to go into the climb. Training kicked in and I pushed the nose down to maintain flying speed. Practically no climb at all possible though. I did a very quick and low circuit, landed and offloaded. Funny how those little tense moments stay with you for ever. :) reply Dylan16807 10 hours agoparentEVERY? Is a plane with a thrust-to-weight ratio over 1 able to stall at full throttle? reply simonblack 5 hours agorootparentA stall is a loss of lift from an aerofoil due to a high Angle of Attack causing a separation of the airflow from the aerofoil. No laminar flow = no lift. There is such an animal as a high-speed stall, where the angle of attack is greater than 15 degrees. Can happen when a gung-ho pilot makes a high speed dive over his girl-friends house and leaves it too late and too low to pull out gently. ('Gently' being a pull back on the controls such that the Angle of Attack is less than 15 degrees, and curves enough to pull smoothly out of the dive, go into a climb and away to safety.) He pulls back too hard on the controls. The angle of attack goes over 15 degrees, there is no 'lift' to stop the dive, and the plane continues the trajectory straight into the girl-friend's house. Whether that has ever actually happened, I do not know. But that was the warning not to be a smart-arse as told to me by my CFI. reply vdqtp3 6 hours agorootparentprevYes reply H8crilA 17 hours agoprevJust so we're clear, there's practically no such thing as an \"unstallable plane\". If some pilot believes that then their license should be revoked. Even jet engines can experience stalls internally on the compressor blades, and even helicopters can experience stalls on their retreating blade. I would compare it to someone believing that their car cannot possibly lose traction. Exceptions, which of course must exist, include some fly-by-wire setups which limit the actuation of flight surfaces so that it should be theoretically impossible to put an aircraft in that situation, and rumored properties of some abnormal constructions like the An-2. Although even there you should repeatedly get comfortable with what happens in/around a stall, at least in simulators. The fact that air started to separate and the end of the wing, and not at the root, is scary. It means the pilot wouldn't get the normal warning in the form of airframe shaking. Bad modification. reply sfeng 17 hours agoparentCanard aircraft, for example, stall the canard first, resulting in the nose dropping, preventing the main wing from ever stalling. reply maximilianburke 17 hours agorootparentThe main wing can still be stalled in a canard; it’s not easy but it is possible and when it happens it’s almost unrecoverable because the canard will be stalled too and no flying surfaces will have sufficient lift to correct the condition. It’s a condition called “deep stall” reply Wistar 15 hours agorootparentUsually in a cross-controlled “slip” where the fuselage is held at a fairly dramatic angle relative to the slipstream (relative wind) and the fuselage “blanks out” one side of the main wing. reply brazzy 15 hours agorootparentprevIIRC non-canard aircraft can have this happen when the stall causes the plane to fall at an angle where the wake turbulence of the wings covers the elevators. reply renhanxue 10 hours agorootparentprevThis is far from universally true. The Saab 37 Viggen fighter jet (which was also the first series produced canard aircraft) is capable of departing from controlled flight into a stalled attitude in no less than five different ways, according to its flight manual: > If the angle of attack exceeds the permitted limits, some yaw disturbances appear around alpha 25-28°, and at alpha 28-30° there are weak pitch-up tendencies. If the stick is moved forward to counter the pitch-up, the aircraft returns to normal alpha, possibly after overshooting up to alpha ~50°. Note that the angle of attack instrument only shows the area -4° to +26°. > If the stick movement forward at the pitch-up is too small or is made too late, such that the angle of attack does not immediately decrease, the aircraft departs into superstall or spin. If the pitch-up occurs without aileron input, the departure usually results in superstall. If the pitch-up occurs with any aileron input active, the aircraft is affected by adverse yaw and the likelihood of a spin increases. In addition to the superstall, the aircraft has two spin modes, in the flight manual referred to as flat and oscillating. The difference is basically the rotation speed and if there are oscillations in pitch and/or roll or not. The recovery is pretty conventional: > In superstall or spin the pitch authority is good, which eases recovery. Aileron input results in adverse yaw, that is to say rolling right gives a yaw to the left and vice versa. Rudder authority is negligible. > Recovery from superstall and oscillating spin is accomplished by moving the stick to a position somewhat forward of the neutral pitch position, with ailerons and rudder neutral. To recover from a flat spin, the yawing rotation must be stopped first, which is accomplished with neutral pitch and full roll input in the direction of the rotation (\"stick into the spin\"). When the rotation has just about ceased, recovery is accomplished with neutral ailerons and the stick somewhat forward of neutral, just like when recovering from superstall and oscillating spin. In addition to regular stalled attitudes though, the aircraft also exhibits another stalled attitude with autorotation, the \"plunging spiral\" (sv. störtspiral) which can also be encountered in two variants. I'm honestly not sure how exactly it works aerodynamically. The flight manual says: > In certain adverse dynamic scenarios, the aircraft can enter an uncontrolled attitude of the autorotating type, here called plunging spiral . The plunging spiral, which can be either right side up or inverted, is considered to be the potentially most dangerous form of uncontrolled flight that has been discovered during the spin tests of aircraft 37. > The most common form of the plunging spiral is the inverted one. The following attitudes/maneuvers repeatably result in an inverted plunging spiral: 1) somersault into inverted position from oscillating spin (for example while attempting to recover from a spin with the stick fully forward), 2) stalling the tailfin through so-called \"knife edge flying\". The inverted plunging spiral is characterized by: 1) negative load factor (-1 to -3 G) 2), low nose, 3) very high rate of rotation in the roll axis (≥ 200°/s), 4) high sink rate (≥ 150 m/s). > Moving the stick back and/or aileron input to either side tends to increase the rate of the roll rotation. The rotation can be stopped by moving the stick fully forward with no aileron input. When the rotation has ceased, the stick is moved back to neutral pitch, and the aircraft recovers to controlled flight. > The aircraft only departed into a non-inverted plunging spiral on a few occasions during the spin tests. It has not been possible to define any repeatable attitude or maneuver that results in a non-inverted plunging spiral. During the spin tests the non-inverted plunging spiral only occurred on the following two occasions (not repeatable): 1) when recovering from an inverted superstall, 2) when recovering from an oscillating non-inverted spin. The non-inverted plunging spiral is characterized by: 1) positive load factor (+1 to +3 G), 2) low nose, 3) very high rate of rotation in the roll axis (≥ 200°/s), 4) high sink rate (≥ 150 m/s). > In a non-inverted plunging spiral, aileron inputs have no effect. Instead, the roll rotation must be stopped by pulling gently back on the stick until the rotation ceases. When the rotation has ceased, the stick is moved forward to the neutral pitch position and the aircraft recovers into controlled flight. reply t0mas88 15 hours agoparentprevVery bad modification. And wrong (but natural) response from the pilot trying to pick up the dropping wing with aileron input. That would have made the asymmetric stall worse. reply golergka 17 hours agoparentprevEdit: It seems that I completely misread what Wikipedia said, disregard this comment. reply cjbgkagh 17 hours agorootparentOver 18k An-2 were produced during the time of 1947-2001. It’s an unusual plane due how old it is and that many are still in operation so general stats should take that into account. It’s well known for being nearly impossible to stall with a stall speed of 30 knots - if it does stall it’ll sink at the rate of a parachute which is still faster than you’d want to hit the ground for a landing. It’s also easy to pick up that speed by dipping the nose. If someone crashes an an-2 by stalling it they had to really work hard to do it. Any pilot that did this would be considered unsafely inept to an almost unimaginable degree. This article is the first I'm hearing of a Cessna 185 being considered unstallable and I do wonder it that title was picked for engagement. Float planes are extra dangerous with more that can go wrong and less margins for safety. reply Tuna-Fish 16 hours agorootparentWasn't AN-2 the plane where the manual advises that if you need to land and cannot see your landing site, you should fly low and slow and intentionally stall the plane? reply cjbgkagh 15 hours agorootparentI don't know but that would make sense, landing at the speed of a parachute is better than crashing. Not sure on the exact numbers but a parachute sink rate of 5m/s is over the usual landing touch down sink rate of 1m/s. I would guess at that rate there might still be some damage to crew and airframe. If done skillfully I would image it would be possible to trade some forward speed for a slower sink rate right before touchdown to make for much softer landing. reply H8crilA 17 hours agorootparentprevhttps://aviation.stackexchange.com/questions/65718/what-make... There are many ways of totalling a plane beyond a stall. reply dramm 16 hours agoprevWhat a horrible click-bait title. There is nothing about a C185 or one modified with a STOL kit that is unstallable. A better title would be something like \"Clueless pilot stalls aircraft. Which unfortunately is not an uncommon thing. reply LeifCarrotson 16 hours agoparentI wonder if they're practicing modern journalism strategies that are worried about libel suits? Or it's so obviously satire to them that they don't need to clarify? When they write: > However, the investigation discovered that despite his experience, he had never practised stall recovery on the Cessna 185. The pilot had no knowledge of the aircraft’s stall behaviour at all. His opinion was that the Cessna 185 simply didn’t stall. In writing targeted at lay readers, I would expect this to be followed with something like \"This opinion, of course, is complete lunacy. All aircraft can stall. Practicing stall recovery should be a normal part of pilot training.\" reply ordu 15 hours agorootparentI believe this article doesn't need such clarifications. It says in unambiguous terms that Cessna had stalled, with an obvious logical implication that a pilot was dead wrong. The article even discusses differences of how the stall occurs in modified and unmodified versions of a plane. To not get the message a reader must be not a lay person, but an exceptionally dumb one. reply axus 15 hours agoparentprevIt was the pilot who believed this Cessna \"never stalled\". And so he did not recognize and had no idea what to do when it did. reply leobg 17 hours agoprevStick and Rudder (Wolfgang Langewiesche 1972): > There are situations in flying when he who \"ducks,\" he who flinches, is lost. The most important example is the re­covery from a stall at low altitude-getting that stick forward and pointing the nose at the ground; that does require courage, and no two ways about it. [...] It might seem that learning to fly the conventional airplane must necessarily be mostly a matter of drill, like animal training, like making a dog not eat when he wants to eat, making him jump through a flaming hoop when he does not want to jump. [...] But another view of the problem is also possible. It may be that our common sense, our natural reactions mislead us simply because they are working on the basis of wrong ideas in our minds concerning the wing and how it really flies, the controls and what they really do. [...] Perhaps what happens when the beginner reacts wrongly in an airplane is similar to what happened in the early days of the automobile, when a man trying to stop in an emergency would pull back on the wheel as if he had reins in his hands and would even yell \"Whoa.\" There was nothing re­ally wrong with his reactions, with his intentions; the only thing wrong was the image in his head that made him see the automobile as a sort of mech­anized horse, to be controlled as horses are controlled. Had he clearly seen in his mind's eye the mechanical arrangement we take for granted now-the clutch that can disconnect the motor, the brakes that can clamp down on the wheels; had he clearly appreciated that the thing was a machine and had no soul at all, not even a horse's soul, and that thus there was no use in speaking to it-he would then have done the right thing without diffi­culty. It may be that, if we could only understand the wing clearly enough, see its working vividly enough, it would no longer seem to behave contrary to common sense; we should then expect it to behave as it does behave. We could then simply follow our impulses and \"instincts.\" Flying is done largely with one's imagination! If one's images of the airplane are correct, one's behavior in the airplane will quite naturally and effortlessly also be correct. reply cromulent 16 hours agoprevThe report of the investigation: https://turvallisuustutkinta.fi/material/attachments/otkes/t... reply TomK32 15 hours agoparentIt's amazing how much work they've put into creating this quite unique configuration just to find out the stall behaviour. reply ordu 15 hours agorootparentI believe it is normal for flight incidents investigations. I read a lot of Kyra Dempsey writings[1] on accidents and there is a lot of examples of a detailed investigations probing all possible hypotheses even those which are not very probable. They need to know for sure. [1] https://admiralcloudberg.medium.com/ reply tomaskafka 16 hours agoprev\"As a result, the aircraft pitches up unless the pilot controls the flight path using the elevator. If this pitch up is not controlled, the aircraft is at risk of exceeding the stall angle of attack.\" This has been happening for me in MSFS, and I considered it a really weird behavior - why can't I take off in a same way as from the runway? Now I know. reply t0mas88 15 hours agoprev> It is common for flight operations in the wilderness to have the take-off weight close to the maximum. It is common for the majority of flights in general to have take-off weight close to the maximum, not just bush flying. The same goes for the remark that \"center or gravity was close to the forward limit\", that sounds like a risk but it is not. If it's exactly at the limit, that's fine and perfectly safe to fly. If it's over the limit, it's illegal to fly. reply buildsjets 15 hours agoparentIt is not uncommon for flight operations in the wilderness to be up to 15% heavier than the certified maximum limit, and still be perfectly legal. Relevant regulation: 14 CFR § 91.323 - https://www.ecfr.gov/current/title-14/chapter-I/subchapter-F... reply t0mas88 14 hours agorootparentInteresting, tnx. We don't have such a general exception in EASA. Only possible with a special permit and a lot of paperwork for a ferry flight or similar. reply buildsjets 14 hours agorootparentIt's magical how the airframe gets 15% stronger just by arriving in Alaska! reply buildsjets 14 hours agoprevI've been involved in general aviation since the late 1970s, currently commercial/twin/instrument rated, have a degree in aerospace engineering, and do what my username says for a living. So I know airplanes, and why they do the things they do. But an appreciation for what is physically occurring during a stall, how the resulting balance of forces influence the aircraft handling during a stall and recovery, and why the recovery must be handled in a specific sequence to avoid overstressing the airplane or losing directional control, are not intuitive and have never been a very strong point among pilots, or even instructors. As a result, you have poorly educated instructors passing along old folk tales to new pilots, who then take them as gospel. This has been exacerbated over the years by the segregation of the pilot population into two separate groups, who often receive training that stresses different objectives. Part 141 flight schools are the typical staring point for airline pilots. In these schools, even when flying little Pipers and Cessnas, stall avoidance is the primary method which is taught. Pilots are instructed to initiate a \"stall recovery\" at the \"first indication of stall\", which is taught to be stall warning horn indicator sound happening. The goal is to recover with no loss of altitude, and the technique is to add power to power out of the indicated stall immediately upon hearing the stall warning indicator, and use the elevator to keep the airplane at the same altitude. The problem is that the stall warning horn indicator typical starts sounding about 5 knots in advance of a fully developed stall when flying more or less straight and level. So these pilots never experience a fully developed stall, just an approach-to-stall, and often develop an extreme fear of entering an actual stall. This recovery technique also only works in an airliner in the case of an approach-to-stall or at most a very shallow stall. To recover an airliner from a fully developed stall, you must use the same techniques as you do in a Cessna 172, which is to drop the nose to lower the angle of attach and trade some altitude in for airspeed. This was vividly demonstrated by the five co-captains of Air France Flight 447, who tried to power their way out of a fully developed stall, that could have been easily recovered from by a typical student pilot using Cessna techniques. I learned to fly in a Part 61 school, which typically are things like flying clubs and independent flight instructors teaching people who mostly fly for fun, or for light commercial use like charters, and will be mostly flying light propeller aircraft. In this environment, the aircraft have a much lower power to weight ratio, you cannot power your way out of even an approach-to-stall. Instead of stall avoidance, stall entry, recovery, and exit is taught. Instead of the goal being to minimizing altitude loss, maximizing aircraft control is taught. The goal is to prevent a poorly-handled stall from developing into a spin, which will usually result in a fatal accident like this one did. So if you learned in a Part 61 school, instead of starting recovery as soon as you heard the stall horn, you kept going until a full stall, and the expectation is that the student would say \"STALL\" when they identified that the airplane had entered a fully developed stall, based on changes in the handling characteristics and sudden drop in altitude. The student would then initiate a recovery by pushing the nose down to reduce the angle of attack, smoothly adding power and right rudder at the same time to add airspeed and counteract for yaw, and then recover from the resulting shallow dive with no more than 200' loss of altitude, but keeping the airplane under perfect directional control. So in part 61 schools, instead of recovering as soon as you heard the horn blip, you spent a lot of time with it going off in your ear. In fact my instructor used to pull the circuit breaker for it after it went off, as we already knew we were going to be spending a lot of time in an incipient stall condition, and could tell more from the changes in sound of the air going past us than some buzzer. But even in this environment, there are some instructors who just don't like doing stalls, so spend the minimum required amount of time teaching and practicing them. If you are a pilot who learned in a Part 141 school, or in Part 61 but didn't spend a lot of time in deep stalls, one of the best investments you could make in yourself is to find an experienced instructor and ask to spend an hour doing \"Falling Leaf\" stalls, where you alternately keep the airplane in a deep stalled condition, and then recover while you really practice your rudder coordination. They're fun! You will loose your fear of stalls and develop skills/instincts that might save you in an situation like this pilot ended up in. You don't need a fancy aerobatic airplane, a Cessna 172 will work fine, but I prefer a 150 because it does not handle like a minivan. Here's a good video on YouTubes to show what it would typically look and sound like during a lesson: https://www.youtube.com/watch?v=Ocv2YDLk5t0 Note that this is not an example of a perfectly textbook executed falling leaf, it's a student pilot performing them with varying degrees of competence (some good, some not.. keep them ailerons neutral!) and an proficient instructor allowing them to make some mistakes and experiment within the safe bounds of aircraft controlability. So a typical lesson with a good instructor. reply howard941 12 hours agoparent> instead of recovering as soon as you heard the horn blip, you spent a lot of time with it going off in your ear. In fact my instructor used to pull the circuit breaker for it after it went off, as we already knew we were going to be spending a lot of time in an incipient stall condition, and could tell more from the changes in sound of the air going past us than some buzzer. This is sort of the way I was taught. In primary training we'd always wind up doing a lot of minimum controllable airspeed work and then move on to power on stalls/departures and low power stalls/approaches. Recognition was pretty easy with the buffeting followed by the nose dropping. Coordination and control were the goals. Later on we'd also do accelerated stalls. I hated doing low speed work so my instructor made us do it every time even after I started doing them without him in the airplane. reply jillesvangurp 3 hours agoparentprevHere's another video of an instructor teaching a student falling leave stalls: https://www.youtube.com/watch?v=CpBX-B5mQ18 Not a pilot but I've dabbled a lot with flight simulators over the years. This is one of those things simulators struggle with. Even X-plane will just spin irrecoverably and crash when you try stuff like this. reply tim333 1 hour agoparentprevYeah I got a PPL and there was no mention in my training of stuff like that which happened to this plane. That said I'm surprised it behaved like that - turning dramatically to one side on stalling. All planes I've flown, model aircraft that I've built and paper darts just lose lift when they stall rather than yawing to one side. It seems there was something off with the aerodynamics of that plane. reply speedbird 52 minutes agorootparentSTOL kit and possibly trim setting causes tip stall rather than root. reply speedbird 54 minutes agoparentprevI’ve got a few hundred hours in the usual pipers and light twin, but I’d say I never really learned about flying, and particularly “flying the wing” until I got into aerobatics and deliberately upsetting the aircraft. It’s priceless learning to go beyond the normal and being comfortable with it. reply ambicapter 16 hours agoprev> Cessna aircraft have a hinge line on the upper surface. As a result, turning the aileron down causes a sharp angle on the upper surface. The air is unable to flow around such a sharp edge and stays attached. The result is a sudden right wing tip flow separation. Should this say \"The air is unable to flow around such a sharp edge and does not stay attached\"? reply throwawayben 7 hours agoparentI think it's just the \"s\" that got added: \"the air is unable to [...] stay attached\" reply jiveturkey 16 hours agoparentprevI also caught this seeming error. It stood out especially since the rest of the article is so well written. I don't know anything about flying so I thought it was perhaps a lay misunderstanding on my part. reply rayiner 10 hours agorootparentI think it’s just a typo and the author meant to write “doesn’t stay attached.” In a stall the flow detaches: https://youtu.be/3Ahtc-uePY4?si=CPvBpZwx2YbPLFb3 reply rayiner 11 hours agoprevI love the thoroughness annd precision of aircraft accident investigations. Amazing watching these folks do their jobs. reply BXLE_1-1-BitIs1 15 hours agoprevStalling with a wing drop at 15m, you will be hitting the ground or water before recovering, even with perfect technique. It seems his takeoff technique was adequate for thousands of takeoffs until a gust hit at the wrong time and place and yanked the rug out from under him. reply Waterluvian 18 hours agoprevLife jackets seem like they’d be problematic in an enclosed cabin where their use is if you’ve crashed and are taking on water. That’s a bit different compared to a pleasure craft or other vessel that has an outside deck and likely more time to react. But I don’t really know things. Perhaps most float plane emergencies that require life jackets don’t suffer from my perception of the issue. reply BalinKing 17 hours agoparentIndeed, passengers inflating their life jackets too early directly caused many of the deaths on Ethiopian Airlines Flight 961 [0]. This is why the modern safety briefing includes the bit about waiting until you've exited the cabin to inflate your life jacket. I have no clue how this applies to floatplanes, though—I'm curious for more details about when the article says \"there are approved life jackets which could be used to deal with these circumstances\". [0] https://en.wikipedia.org/wiki/Ethiopian_Airlines_Flight_961 reply Someone 17 hours agorootparent> I'm curious for more details about when the article says \"there are approved life jackets which could be used to deal with these circumstances\" In context, that says: “Floating and automatically operating life jackets aren’t practical, specifically because of cases like this where the occupants have to dive out of the capsized aircraft in order to escape the cabin. However, there are approved life jackets which could be used to deal with these circumstances.” So, I guess there are approved life jackets that do not automatically inflate and are neutrally buoyant, thus minimally hindering attempts to leave a submerged plane while wearing one. reply BalinKing 15 hours agorootparentYeah, I guess my main confusion is whether that means a normal (uninflated) airliner life jacket that you're just required to put on pre-emptively, or something more specialized. reply _trampeltier 17 hours agorootparentprev\"Many of the passengers survived the initial crash, but they had disregarded, did not understand, or did not hear Leul's warning not to inflate their life jackets inside the aircraft, causing them to be pushed against the ceiling of the fuselage by the inflated life jackets when water flooded in. Unable to escape, they drowned.\" reply fallingknife 16 hours agorootparentThat seems strange to me. Why would they not simply take off the life jacket and swim out? reply Waterluvian 15 hours agorootparentThe inflatable ones are often designed to be impossible to remove… at least not easily. They inflate into a thing that’s kind of gently choking you. reply Waterluvian 16 hours agorootparentprevDefinitely far up the list on horrible ways to die. reply andrewaylett 16 hours agoparentprevIn my vernacular, I distinguish between \"life jackets\" and \"buoyancy aids\". Apparently most people don't. A buoyancy aid has a foam core, and always provides buoyancy. It's the sort of thing you'd wear while kayaking, but it's far too bulky to want to wear it unless you expect to go in the water. A life jacket is inflatable, and normally automatic. If you're at risk of falling in, and to do so would be dangerous, you should probably wear one of these -- if you go in the water, it'll inflate automatically. This isn't suitable if you might get wet without wanting the life jacket to inflate, though, and you can get equivalents with manual inflation. The ones you get on aircraft are cheaper than ones you're expected to re-use by wearing multiple times but in neither case will you inflate it multiple times. The downside of a manually-inflated life-jacket is that you need to be conscious to inflate it. The downside of an automatic life-jacket is that if you get wet, it'll inflate. The downside of the buoyancy aid is that it's always bulky, but on the other hand if you're wearing it, it'll always work. reply daedalus_f 16 hours agorootparentI’ve heard that distinction used in the UK. The other down side of buoyancy aids I was told about is that many (most?) will not turn you face up if you are unconscious. Gives useful extra mobility for sports but can be fatal if the wearer is unconscious. reply brazzy 15 hours agorootparentIt depends on the design of the specific device. A buoyancy aid with a foam collar will do it, but is less comfortable to wear. With an inflatable life jacket, to collar is not noticeable until it inflates. reply eesmith 16 hours agorootparentprevWhich vernacular is that? In boating a life jacket does not need to be inflatable. https://uscgboating.org/recreational-boaters/life-jacket-wea... says: > There are four basic design types: Inherent, Inflatable, Hybrid, and Special Purpose. > There are two main classes of PFDs. > * Those which provide face up in-water support to the user regardless of physical conditions (lifejackets). > * Those which require the user to make swimming and other postural movements to position the user with the face out of the water (buoyancy aid). It mentions both \"Foam filled lifejackets\" and \"Inflatable lifejackets\". reply andrewaylett 14 hours agorootparentSeems like it's maybe a UK thing, and my (fairly limited) water-sports experience is kayaking and small sailboats. Isn't the English language fun? reply eesmith 2 hours agorootparentInteresting! I checked out what the RNLI had to say at https://rnli.org/safety/lifejackets and that page only mentions lifejackets along with gas. However, if you \"Download our lifejackets and buoyancy aids guide as a PDF (3.48MB)\" at https://rnli.org/-/media/rnli/downloads/1983319_choose_it_we... you'll read \"Children’s lifejackets may rely on foam, air and foam, or CO2 only to provide buoyancy\" and \"Air and foam or CO2 lifejackets meet the requirements of a level 150 lifejacket and are suitable for offshore use. Normally, foam lifejackets provide level 100 buoyancy and are suitable for inshore use.\" A further search of the RNLI site finds https://rnli.org/magazine/magazine-featured-list/2018/june/t... with \"The foam-based Beaufort lifejacket [of the 1970s] upped the buoyancy level, allowing a crew member to also support the person being rescued.\" but by the 1990s \"The bulkier gear of all-weather lifeboat crews meant they needed a more compact lifejacket, which inflated automatically on hitting the water using a built-in gas canister.\" Since I learned my small watercraft skills in the warm waters of Florida, instead of chilly UK, I can see how that would make a difference. reply travisjungroth 17 hours agoparentprevIt sounds like the two surviving passengers would have died if they had life jackets on. I can’t imagine getting out of an inverted, flooded 185 cabin with a life jacket on. I think there was some sense to not requiring life jackets on seaplanes. They’re much more confined spaces than most pleasure boats, not to mention that you’re usually on a boat rather than in it. The flooding is also usually just about instant as the airplane rolls over. Seems common for reactive legislation to not actually fix the situation that’s being reacted to. Requiring shoulder harnesses during takeoff and landing (which is the case in the US) would have actually kept the deceased passenger conscious to escape, as said in the report. But they didn’t change that law. reply lettergram 17 hours agorootparentMy reflex is to never mandate safety procedures. To put it simply, why should the state use force to mandate something like safety. The implication being if someone refuses the force of the state is used on them… which is definitely not good or improving safety. Mandating the seatbelts exist, sure. Mandating people wear them? Idk about that. In the case of tractors for instance, wearing a seatbelt is downright dangerous. You cannot jump out then, and will be killed by a tractor if it flips. reply user_7832 15 hours agorootparent> In the case of tractors for instance, wearing a seatbelt is downright dangerous. You cannot jump out then, and will be killed by a tractor if it flips. The \"proper\" solution would be to have a rollcage so that even a flipped tractor does not crush its occupants. Not having a roll cage (presumably to save $) is a result of weaker/less mandated safety procedures already. Cars have a roof crush test. The solution isn't \"jump out when big machine starts tipping\", it's \"protect the humans in the machine\". [0] - https://www.consumerreports.org/cro/2012/02/rollover-101/ind... reply roywiggins 16 hours agorootparentprevEveryone ends up paying for that, in the form of insurance rates. reply Waterluvian 16 hours agorootparentNot only that but I think there’s also a meaningful quality to living in a society with excessive avoidable deaths. I personally think it contributes to a “shields up, guard up” culture that I’ve experienced and found exhausting. reply VS1999 16 hours agorootparentprevAnd healthcare insurance if you live in the US, and regardless of where you live it clogs up your entire healthcare system as Jimmy-no-seatbelt flies into the trauma center. reply roywiggins 12 hours agorootparentIn countries with universal healthcare you pay too, it's just called \"taxes\" instead. reply Jiro 16 hours agorootparentprevThat's an argument against a state-run healthcare system. It gives the state reason to classify arbitrary things as \"increases the cost of insurance\" and prohibit them. reply lettergram 16 hours agorootparentprevThey could just not cover injuries where a seatbelt isn’t warn. That said, we have evidence that seatbelt wearing didn’t impact insurance rates. Literally look at the rates over time, even after these laws were enacting, insurance rates rose fast as ever reply roywiggins 12 hours agorootparentThat's not how emergency room care works. It doesn't matter whether it's covered or not, you're going to get treated; quite likely the hospital ends up eating the bill if insurance doesn't pay. reply tzs 14 hours agorootparentprevIt's not just you and your passengers that are less safe when you drive without seatbelts. If you have to make a sudden sharp swerve when driving centrifugal forces try to move you from in front of the steering wheel, which can make it harder for you to remain in control. That increases the danger to nearby vehicles and pedestrians (and to nearby property that you might hit). Seat and shoulder belts help keep you in place in front of the steering wheel. reply artine 17 hours agorootparentprevMandating the wearing of seatbelts isn’t entirely about protecting the person wearing the seatbelt. An unbelted occupant becomes a projectile in a sufficiently violent collision, and that projectile can cause harm to people outside of the vehicle. reply macintux 16 hours agorootparentHeck, I recently saw a video (may be an old one) of a driver who fell out of his car while showing off his acceleration. Now the entire car is an uncontrolled projectile. reply Waterluvian 16 hours agorootparentInteresting that pleasure craft have dead-man switches you can optionally affix. They’re also designed to turn anti-clockwise forever if nobody is at the wheel. I guess because there aren’t seatbelts and these boats are usually open-top. reply foobar1962 7 hours agorootparent> pleasure craft... turn anti-clockwise forever if nobody is at the wheel. That's probably a result of the \"paddle wheel\" effect of the propellor rotating through the water: a bug not a feature. reply AnimalMuppet 17 hours agorootparentprevOr harm to other people inside the vehicle. reply ramesh31 17 hours agoprev>“I let it lift off by itself. It was well-trimmed and it lifted off normally by itself.” It sounds like the pilot wasn't fully prepared and engaged to compensate for propeller torque at the moment the aircraft left the surface of the water. At full takeoff power in a single engine aircraft this can be very intense and jarring, particularly with a high pitch ascent and full prop pitch. All it took was a momentary lapse in keeping the wings level to stall out at that speed. >The indirect causal factor was the pilot’s lack of experience with stalling the aircraft. He told the investigation that he had never stalled the aircraft, which meant that he was unable to recognise the stall during the take-off. It's this lack of stick and rudder skills at the root of the incident. reply travisjungroth 17 hours agoparent> The pilot had set the trim so that the aircraft would lift off from the step and begin to climb away. The rudder trim was set almost as far right as it could go. The pilot described the take-off as quick and easy. “I let it lift off by itself. It was well-trimmed and it lifted off normally by itself.” Further down. > The maintenance team discovered an incorrect right wing geometric twist, which was unrelated to the hangar roof collapse but probably happened during repairs done previously in the USA. As a result, the aircraft had a tendency to roll and had been uncomfortable to fly because of a lack of aileron trim. This might explain why the pilot had the aircraft trimmed full right rudder on take-off: to correct for this roll. He may have actually had too much rudder. They don’t say this explicitly, but correcting for roll with rudder means you’ll be cross controlled. He was dangerously near stall speed without realizing it. Some turbulence could cause a small partial stall. If the airplane was straight, it would have just dropped the nose a bit and corrected. But with a twist in one wing and 2/3 of rudder trim engaged, it’s more like it entered a snap roll. One wing was stalled, one was still making lift. The airplane felt fine to the pilot, but it was essentially modified to be a snap roll machine. I don’t think a stock 185 would have even been capable of what happened here. reply toast0 16 hours agorootparentNote that the twist had been repaired before the accident. > When they repaired the damage to the right wing, they also corrected the geometric twist, removing the aircraft’s tendency to roll. However, since the repairs were completed five days before the accident, the pilot may have set the rudder based on pre-repair experience with the plane. He may not have been informed of the change in twist, or may not have understood it. reply ramesh31 17 hours agorootparentprevKind of my point though. The pilot was disengaged from the controls, and relying on trim settings for takeoff. Regardless of the different roll characteristics, if he had been actively controlling the yoke at the time rather than needing a split second to react and correct, the accident probably would not have occurred. reply travisjungroth 11 hours agorootparentI really doubt that. He was still “actively controlling” the yoke. This is a back country 185, it’s not like he had the autopilot engaged. In my experience as a flight instructor, pilots having the airplane trimmed out properly generally only improved control. reply overspeed 17 hours agoprev> His opinion was that the Cessna 185 simply didn’t stall. There's your problem. Don't opine on operating characteristics of a production aircraft. Read the handbook. This incident was caused by poor airmanship. reply nickff 15 hours agoparentIt seems like he was asked a question which compelled him to opine. You seem to be assuming that he went ‘out on a limb’ of his own accord, without any basis for that assumption. reply 1letterunixname 14 hours agoprev [–] The mindset of that pilot inherently dangerous and complacent. A friend of mine was a casual GA pilot in college. He was constantly practicing failure modes and making contingency plans such as engine failure at different points of takeoff and practicing stall recovery at various speeds, altitudes, bank angles, and AoAs. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Traditional aircraft life jackets are now thin plastic one-use balloons, which Gene finds uncomfortable and unsuitable for daily use.",
      "Gene observed three paragraphs in his post were repeated, impacting the overall quality and coherence."
    ],
    "commentSummary": [
      "Understanding stall speeds in aircraft is crucial, especially in scenarios like modifications, aerobatics, and emergency situations.",
      "Key factors like angle of attack, weight, and power play a significant role, underscoring the necessity for proper training and pilot proficiency in stall recognition and recovery.",
      "Adhering to manufacturer guidelines, obtaining adequate training, and staying informed about aircraft modifications are pivotal for safe aircraft operation and flight safety."
    ],
    "points": 163,
    "commentCount": 159,
    "retryCount": 0,
    "time": 1714318346
  },
  {
    "id": 40194636,
    "title": "Exciting \"Thorium Transition\" in Atomic Nuclei with Lasers",
    "originLink": "https://www.tuwien.at/en/tu-wien/news/news-articles/news/lange-erhoffter-durchbruch-erstmals-atomkern-mit-laser-angeregt",
    "originBody": "Atomic Nucleus Excited with Laser: A Breakthrough after DecadesTU Wien window.site = {google_maps_api_key: 'AIzaSyDBqriGwD5zIP6gWTW5imxv8cjHpVxuzAA'}; document.createElement(\"picture\"); picture{display:block;overflow:hidden;position:relative}sidebarMenuAjaxUrl = '/en/tu-wien/news/news-articles?tx_typoscriptrendering%5Bcontext%5D=%7B%22record%22%3A%22pages_1502%22%2C%22path%22%3A%22tt_content.list.20.wpbootstrap_navigation%22%7D&tx_wpbootstrap_navigation%5Baction%5D=pages&tx_wpbootstrap_navigation%5Bcontroller%5D=Navigation&cHash=4a4c0dcdef3be9a49a7f3233c8fded4f&no_cache=1&tx_wpbootstrap_navigation[id]=' Skip to content(Accesskey: 1) Skip to navigation(Accesskey: 2) Skip to search(Accesskey: 7)Close page navigation LoginDE Open page navigation Search Start search TU WienOverview News fuTUre fit About TU Wien Organisation Sustainability Corona A university for all Working at TUW TUW Community Campus Contact StudiesOverviewNews Studies Admission Studying at TU Wien Student support services Teaching at TU Wien Best Teaching Awards 2024 Continuing Education Helping Ukraine International ResearchOverview News Events Profile Facilities Successes Networks TUW Doctoral School RTI support Funding opportunities Databases PartnershipsOverview Industry Relations Technology Offers Inventions, Patents, Commercialization Start-ups Center for Technology and Society, opens an external URL in a new windowUniversity Alliances EULIST TU Austria, opens an external URL in a new windowtuw.media, opens an external URL in a new windowFundraising ServicesOverview Newsletter Event Calendar Media Library Campus services Reporting system Digitalisation IT-Services TUW-Jobportal, opens an external URL in a new windowTISS, opens an external URL in a new windowTUW alumni club, opens an external URL in a new windowTUW Career Center, opens an external URL in a new windowContinuing EducationInternalOverview Portal (TISS, SAP, TYPO3,...), opens an external URL in a new windowNews articlesTU Wien&sol;News&sol;News articles&sol;29. April 2024 Atomic Nucleus Excited with Laser: A Breakthrough after Decades The \"thorium transition\", which physicists have been looking for for decades, has now been excited for the first time with lasers. This paves the way for revolutionary high precision technologies, including nuclear clocks. Enlarge image© Oliver Diekmann, TU Wien1/3 images1 of 3 images or videosA laser beam changes the state of thorium nucleui, which are embedded in a crystalA laser beam changes the state of thorium nucleui, which are embedded in a crystalEnlarge image© Foto Wilke1/3 images1 of 3 images or videosEnlarge image© PTB1/3 images1 of 3 images or videosPTB-researcher Johannes Tideau in the laser lab in BraunschweigPTB-researcher Johannes Tideau in the laser lab in BraunschweigPhysicists have been hoping for this moment for a long time: for many years, scientists all around the world have been searching for a very specific state of thorium atomic nuclei that promises revolutionary technological applications. It could be used, for example, to build an nuclear clock that could measure time more precisely than the best atomic clocks available today. It could also be used to answer completely new fundamental questions in physics - for example, the question of whether the constants of nature are actually constant or whether they change in space and time.Now this hope has come true: the long-sought thorium transition has been found, its energy is now known exactly. For the first time, it has been possible to use a laser to transfer an atomic nucleus into a state of higher energy and then precisely track its return to its original state. This makes it possible to combine two areas of physics that previously had little to do with each other: classical quantum physics and nuclear physics. A crucial prerequisite for this success was the development of special thorium-containing crystals. A research team led by Prof. Thorsten Schumm from TU Wien (Vienna) has now published this success together with a team from the National Metrology Institute Braunschweig (PTB) in the journal \"Physical Review Letters\".Switching quantum statesManipulating atoms or molecules with lasers is commonplace today: if the wavelength of the laser is chosen exactly right, atoms or molecules can be switched from one state to another. In this way, the energies of atoms or molecules can be measured very precisely. Many precision measurement techniques are based on this, such as today's atomic clocks, but also chemical analysis methods. Lasers are also often used in quantum computers to store information in atoms or molecules.For a long time, however, it seemed impossible to apply these techniques to atomic nuclei. \"Atomic nuclei can also switch between different quantum states. However, it usually takes much more energy to change an atomic nucleus from one state to another – at least a thousand times the energy of electrons in an atom or a molecule,\" says Thorsten Schumm. \"This is why normally atomic nuclei cannot be manipulated with lasers. The energy of the photons is simply not enough.\"This is unfortunate, because atomic nuclei are actually the perfect quantum objects for precision measurements: They are much smaller than atoms and molecules and are therefore much less susceptible to external disturbances, such as electromagnetic fields. In principle, they would therefore allow measurements with unprecedented accuracy.The needle in the haystackSince the 1970s, there has been speculation that there might be a special atomic nucleus which, unlike other nuclei, could perhaps be manipulated with a laser, namely thorium-229. This nucleus has two very closely adjacent energy states – so closely adjacent that a laser should in principle be sufficient to change the state of the atomic nucleus.For a long time, however, there was only indirect evidence of the existence of this transition. \"The problem is that you have to know the energy of the transition extremely precisely in order to be able to induce the transition with a laser beam,\" says Thorsten Schumm. \"Knowing the energy of this transition to within one electron volt is of little use, if you have to hit the right energy with a precision of one millionth of an electron volt in order to detect the transition.” It is like looking for a needle in a haystack – or trying to find a small treasure chest buried on a kilometer-long island.The thorium crystal trickSome research groups have tried to study thorium nuclei by holding them individually in place in electromagnetic traps. However, Thorsten Schumm and his team chose a completely different technique. \"We developed crystals in which large numbers of thorium atoms are incorporated,\" explains Fabian Schaden, who developed the crystals in Vienna and measured them together with the PTB team. \"Although this is technically quite complex, it has the advantage that we can not only study individual thorium nuclei in this way but can hit approximately ten to the power of seventeen thorium nuclei simultaneously with the laser – about a million times more than there are stars in our galaxy.\" The large number of thorium nuclei amplifies the effect, shortens the required measurement time and increases the probability of actually finding the energy transition.On November 21, 2023, the team was finally successful: the correct energy of the thorium transition was hit exactly, the thorium nuclei delivered a clear signal for the first time. The laser beam had actually switched their state. After careful examination and evaluation of the data, the result has now been published.\"For us, this is a dream coming true,\" says Thorsten Schumm. Since 2009, Schumm had focused his research entirely on the search for the thorium transition. His group as well as competing teams from all over the world have repeatedly achieved important partial successes in recent years. \"Of course we are delighted that we are now the ones who can present the crucial breakthrough: The first targeted laser excitation of an atomic nucleus,\" says Schumm.The dream of the atomic nucleus clockThis marks the start of a new exciting era of research: now that the team knows how to excite the thorium state, this technology can be used for precision measurements. \"From the very beginning, building an atomic clock was an important long-term goal,\" says Thorsten Schumm. \"Similar to how a pendulum clock uses the swinging of the pendulum as a timer, the oscillation of the light that excites the thorium transition could be used as a timer for a new type of clock that would be significantly more accurate than the best atomic clocks available today.\"But it is not just time that could be measured much more precisely in this way than before. For example, the Earth's gravitational field could be analyzed so precisely that it could provide indications of mineral resources or earthquakes. The measurement method could also be used to get to the bottom of fundamental mysteries of physics: Are the constants of nature really constant? Or can tiny changes perhaps be measured over time? \"Our measuring method is just the beginning,\" says Thorsten Schumm. \"We cannot yet predict what results we will achieve with it. It will certainly be very exciting.\" Original publicationLaser excitation of the Th-229 nucleus, Physical Review Letters:https://journals.aps.org/prl/accepted/2c07aYbeC981d47c171619f5604116053962ac79a, opens an external URL in a new windowFull paper (preprint): https://www.tuwien.at/fileadmin/Assets/tu-wien/News/2024/Thorium_Preprint.pdf, opens an external URL in a new windowPicture downloadDownload pictures provided by TU Wien and PTB, opens an external URL in a new windowFurther readingHistorical Background: The Long Search for the Thorium Transition, opens an external URL in a new windowApplications: Nuclear Clocks and the Fundamental Constants of Nature, opens an external URL in a new windowThe Crystal Trick: High-tech Gemstones for Nuclear Science, opens an external URL in a new window Download full info package (PDF), opens a file in a new windowContactProf. Thorsten Schumm Institute of Atomic and Subatomic Physics TU Wien +43 1 58801 141896thorsten.schumm@tuwien.ac.at Text: Florian Aigner Skip to footer TU WienNewsfuTUre fitAbout TU WienOrganisationSustainabilityCoronaA university for allWorking at TUWTUW CommunityCampusContactStudiesNewsStudiesAdmissionStudying at TU WienStudent support servicesTeaching at TU WienBest Teaching Awards 2024Continuing EducationHelping UkraineInternationalResearchNewsEventsProfileFacilitiesSuccessesNetworksTUW Doctoral SchoolRTI supportFunding opportunitiesDatabasesPartnershipsIndustry RelationsTechnology OffersInventions, Patents, CommercializationStart-upsCenter for Technology and Society, opens an external URL in a new windowUniversity AlliancesEULISTTU Austria, opens an external URL in a new windowtuw.media, opens an external URL in a new windowFundraisingServicesNewsletterEvent CalendarMediaLibraryCampus servicesReporting systemDigitalisationIT-ServicesTUW-Jobportal, opens an external URL in a new windowTISS, opens an external URL in a new windowTUW alumni club, opens an external URL in a new windowTUW Career Center, opens an external URL in a new windowContinuing EducationInternalPortal (TISS, SAP, TYPO3,...), opens an external URL in a new window© TU Wien # 1502 Legal noticeAccessibility DeclarationData Protection Declaration (PDF) Cookie settingsTop menu levelTU WienBack: list subpages of parent page \"News\" Back to: NewsNews articles Facebook X LinkedIn YouTube InstagramAbout Cookies and other techniques × Our website uses cookies and integrates content from third-party providers to ensure you get the best experience on our website, for analytical purposes, to provide social media features, and for targeted advertising. This it is necessary in order to pass information on to respective service providers. If you would like additional information about cookies and content from third-party providers on this website, please see our Data protection declaration.Settings MandatoryAllow mandatory cookies These cookies are required to help our website run smoothly.NamePurposeLifetimeTypeProviderCookieConsent Saves your settings for the use of cookies on this website. 1 year HTMLHomepage TU Wien SimpleSAML This is needed to distinguish between the sessions of the logged-in users.session HTTPLogin TU Wien SimpleSAMLAuthToken This is needed to distinguish between the sessions of the logged-in users.session HTTPLogin TU Wien fe_typo_user Is needed so that in case of a Typo3 frontend login the session ID is recognized to grant access to protected areas.session HTTPHomepage TU Wien staticfilecache Is needed to optimize the delivery time of the website.session HTTPHomepage TU Wien JESSIONSID Is needed so that in case of a LectureTube the session ID is recognized to grant access to protected areas.session HTTPLectureTube TU Wien _shibsession_lecturetube This is needed to distinguish between the sessions of the logged-in users.session HTTPLectureTube TU WienWeb statisticsAllow statistic cookies These cookies help us to continuously improve our services and adapt our website to your needs. We statistically evaluate the pseudonymized data collected from our website.NamePurposeLifetimeTypeProvider_pk_id Used to store a few details about the user such as the unique visitor ID. 13 months HTMLMatomo TU Wien _pk_ref Is used to store the information of the users home website. 6 months HTMLMatomo TU Wien _pk_ses Is needed to store temporary data of the visit. 30 minutes HTMLMatomo TU Wien nmstat Is used to record the behaviour on the website. It is used to collect statistics about website usage, such as when the visitor last visited the website. The cookie does not contain any personal data and is only used for website analysis. 1000 days HTMLSiteimprove siteimproveses Is used to track the sequence of pages that a visitor views during his/her visit to the website. The cookie does not contain any personal data and is used solely for website analysis.session HTTPSiteimprove AWSELB Always occurs in pairs with siteimproveses (for load balancing on the provider server)session HTTPSiteimproveMarketingAllow marketing cookies With the help of these cookies and third-party content we strive to improve our offer for our users. By means of anonymized data of website users we can optimize the user flow. This enables us to improve ads and website content.NamePurposeLifetimeTypeProvider_ga Is needed to distinguish the sessions of the users from each other.persistent HTTPGoogle Analytics _gali Is needed to determine which links are clicked on a page.expires immediately HTTPGoogle Analytics _gat This is a function-related cookie, whose tasks may differ. 2 years HTTPGoogle Analytics _gid Is needed to distinguish users and create statistics. 24 hours HTTPGoogle Analytics _gads Required to enable websites to display advertising from Google, including personalized advertising. 13 months HTTPGoogle Analytics _gac_ Required by advertisers to measure user activity and the performance of their advertising campaigns. 90 days HTTPGoogle Analytics _gcl_ Required by advertisers to determine how often users who click on their ads end up taking an action on their website. 90 days HTTPGoogle Analytics _gcl_au Contains a randomly generated user ID. 90 days HTTPGoogle _gcl_aw Is set when users click on a Google ad on the website and contains information about which ad was clicked. 90 days HTTPGoogle __utma Is used to record visits and visitors. 2 years HTTPGoogle Analytics __utmb Is used to detect new visits. 30 minutes HTTPGoogle Analytics __utmc Is used in connection with __utmb to determine whether it is a new (recent) visit.session HTTPGoogle Analytics __utmd Is used to store and track visitor journeys through the site and classifies them into groups (marketing/tracking). 1 second HTTPGoogle Analytics __utmt Is needed to limit the query rate on Google Analytics. 10 minutes HTTPGoogle Analytics __utmz Is needed to determine from which source/campaign visitors come. 6 months HTTPGoogle Analytics __utmvc Is needed to collect information about user behavior on multiple websites. This information is used to optimize the relevance of advertising on the website. 24 hours HTTPGoogle AdSense utm_source Is needed to tag URLs with parameters to identify the campaigns that forward traffic.expires immediately HTTPGoogle Analytics __utm.gif Is needed to save browser details.session HTTPGoogle Analytics gtag Is needed to perform remarketing. 30 days HTTPGoogle AdSense id Is needed to perform remarketing. 2 years HTTPGoogle AdWords 1P_JAR Is needed to optimize advertising, provide ads that are relevant to users, improve campaign performance reports, or prevent users from seeing the same ads more than once. 2 years HTTPGoogle AID Is needed to activate targeted advertising. 2 years HTTPGoogle Analytics ANID Is needed to display Google ads on non-Google websites. 2 years HTTPGoogle AdSense APISID Unknown functionality 2 years HTTPGoogle Ads Optimization AR Is needed to profile visitors&#039; interests and display relevant ads on other websites. This cookie works by uniquely identifying your browser and device. 2 years HTTPGoogle AdSense CONSENT Is needed to store the preferences of visitors and personalize advertising.persistent HTTPGoogle DSID Is needed by DoubleClick for advertising displayed in various places on the web and used to store the preferences of users. 2 years HTTPDoubleclick DV Is needed to store user preferences and other information. This includes, in particular, the preferred language, the number of search results to be displayed on the page, and the decision whether or not to activate the Google SafeSearch filter. 2 years HTTPGoogle HSID Contains the Google account ID and the last login time of the user. 2 years HTTPGoogle IDE Is needed by DoubleClick to record and report the actions of users on the website after viewing or clicking on one of the provider&#039;s ads, with the purpose of measuring the effectiveness of an advertisement and displaying targeted advertisements to users. 2 years HTTPDoubleclick LOGIN_INFO Is used to store the credentials of users of Google services. 2 years HTTPGoogle NID Is used to store information about user settings. 6 months HTTPGoogle OTZ Is needed to link activities of visitors with other devices that are previously logged in via the Google account. In this way, advertising is tailored to different devices. 1 month HTTPGoogle RUL Is needed by DoubleClick to determine whether advertising has been displayed correctly in order to make marketing activities more efficient. 1 year HTTPDoubleclick SAPISID Is needed by YouTube to store user settings and to calculate user bandwidth.persistent HTTPGoogle SEARCH_SAMESITE Enables servers to mitigate the risk of CSRF and information leakage attacks by specifying that a particular cookie may only be sent on requests originating from the same registerable domain. 6 months HTTPGoogle SID Contains the Google account ID and the last login time of the user. 2 years HTTPGoogle SIDCC Is needed to store information about user settings and information for Google Maps. 3 months HTTPGoogle SSID Is needed to collect visitor information for videos hosted by YouTube on Google Maps integrated maps.persistent HTTPGoogle __SECURE-1PAPISID Is needed for targeting purposes to create a profile of the interests of website visitors. 2 years HTTPGoogle __SECURE-1PSID Is needed for targeting purposes to create a profile of the interests of website visitors. 2 years HTTPGoogle __SECURE-3PAPISID Is needed for targeting purposes to create a profile of the interests of website visitors. 2 years HTTPGoogle __SECURE-3PSID Is needed for targeting purposes to create a profile of the interests of website visitors. 2 years HTTPGoogle __SECURE-3PSIDCC Is needed for targeting purposes to create a profile of the interests of website visitors. 2 years HTTPGoogle __SECURE-APISID Is needed to profile the interests of website visitors in order to display relevant and personalized advertising through retargeting. 8 months HTTPGoogle __SECURE-HSID Is needed to secure digitally signed and encrypted data from the unique Google ID and to store the last login time that Google uses to identify visitors, prevent fraudulent use of login data, and protect visitor data from unauthorized parties. This may also be used for targeting purposes to display relevant and personalized advertising content. 8 months HTTPGoogle __SECURE-SSID Is needed to store information about how visitors use the site and about the ads they may have seen before visiting the site. Also used to customize ads on Google domains. 8 months HTTPGoogle test_cookie Is set as a test to check whether the browser allows cookies to be set. Does not contain any identification features. 15 minutes HTTPGoogle VISITOR_INFO1_LIVE Is needed by YouTube to store user settings and to calculate user bandwidth. 6 months HTTPYoutube facebook Is used to Enable ad delivery or retargeting 90 days HTTPMeta (Facebook) __fb_chat_plugin Is needed to store and track interactions (marketing/tracking).persistent HTTPMeta (Facebook) _js_datr Is needed to save user settings. 2 years HTTPMeta (Facebook) _fbc Is needed to save the last visit (marketing/tracking). 2 years HTTPMeta (Facebook) fbm Is needed to store account data (marketing/tracking). 1 year HTTPMeta (Facebook) xs Is needed to store a unique session ID (marketing/tracking). 1 year HTTPMeta (Facebook) wd Is needed to log the screen resolution. 1 week HTTPMeta (Facebook) fr Is needed to serve ads and measure and improve their relevance. 3 months HTTPMeta (Facebook) act Is needed to store logged in users (marketing/tracking). 90 days HTTPMeta (Facebook) _fbp Is needed to store and track visits to various websites (marketing/tracking). 3 months HTTPMeta (Facebook) datr Is needed to identify the browser for security and website integrity purposes, including account recovery and identification of potentially compromised accounts. 2 years HTTPMeta (Facebook) dpr Is used for analysis purposes. Technical parameters are logged (e.g. aspect ratio and dimensions of the screen) so that Facebook apps can be displayed correctly. 1 week HTTPMeta (Facebook) sb Is needed to store browser details and security information of the Facebook account. 2 years HTTPMeta (Facebook) dbln Is needed to store browser details and security information of the Facebook account. 2 years HTTPMeta (Facebook) spin Is needed for promotional purposes and social campaign reporting.session HTTPMeta (Facebook) presence Contains the \"chat\" status of logged in users. 1 month HTTPMeta (Facebook) cppo Is needed for statistical purposes. 90 days HTTPMeta (Facebook) locale Is needed to save the language settings.session HTTPMeta (Facebook) pl Required for Facebook Pixel. 2 years HTTPMeta (Facebook) lu Required for Facebook Pixel. 2 years HTTPMeta (Facebook) c_user Required for Facebook Pixel. 3 months HTTPMeta (Facebook) bcookie Is needed to store browser data (marketing/tracking). 2 years HTTPLinkedIn li_oatml Is needed to identify LinkedIn members outside of LinkedIn for advertising and analytics purposes. 1 month HTTPLinkedIn BizographicsOptOut Is needed to save privacy settings. 10 years HTTPLinkedIn li_sugr Is needed to store browser data (marketing/tracking). 3 months HTTPLinkedIn UserMatchHistory Is needed to provide advertising or retargeting (marketing/tracking). 30 days HTTPLinkedIn linkedin_oauth_ Is needed to provide cross-page functionality.session HTTPLinkedIn lidc Is needed to store performed actions on the website (marketing/tracking). 1 day HTTPLinkedIn bscookie Is needed to store performed actions on the website (marketing/tracking). 2 years HTTPLinkedIn X-LI-IDC Is needed to provide cross-page functionality (marketing/tracking).session HTTPLinkedIn AnalyticsSyncHistory Stores the time when the user was synchronized with the \"lms_analytics\" cookie. 30 days HTTPLinkedIn lms_ads Is needed to identify LinkedIn members outside of LinkedIn. 30 days HTTPLinkedIn lms_analytics Is needed to identify LinkedIn members for analytics purposes. 30 days HTTPLinkedIn li_fat_id Required for indirect member identification used for conversion tracking, retargeting and analytics. 30 days HTTPLinkedIn U Is needed to identify the browser. 3 months HTTPLinkedIn _guid Is needed to identify a LinkedIn member for advertising via Google Ads. 90 days HTTPLinkedInSave Accept all cookies",
    "commentLink": "https://news.ycombinator.com/item?id=40194636",
    "commentBody": "Atomic nucleus excited with laser: A breakthrough after decades (tuwien.at)160 points by geox 5 hours agohidepastfavorite64 comments cantrevealname 2 hours ago> If the wavelength of the laser is chosen exactly right ... then maybe a special atomic nucleus could be manipulated with a laser, namely thorium-229. On November 21, 2023, the team was finally successful: the correct energy of the thorium transition was hit exactly, the thorium nuclei delivered a clear signal for the first time. So what's the wavelength? I felt like the article left me hanging. The answer is: 148.3821 nm Yes, I admit that it's meaningless to me. It's sort of like a big news story announcing that Malaysia Airlines MH-370 has been located somewhere in the world's oceans, but not saying where because a number like 148.3821 km SSE of the Cocos Islands is going to be meaningless to most people. reply infogulch 2 hours agoparentOh that's about 0.0000000014 football fields. More seriously, apparently it takes a photon with a wavelength of 92nm to eject an electron from a hydrogen atom. Maybe this is a reasonable reference/refresher: https://web.archive.org/web/20210413042937/https://www.nagwa... reply thebruce87m 27 minutes agorootparentAmerican football or European football? This is like the gallon thing all over again. reply whatshisface 1 hour agoparentprev148nm is on the lower end of UV-C. It's higher-energy than the furthest ultraviolet light that the sun produces (200nm). If it were produced artificially, it'd be heavily absorbed by the atmosphere to the point of near opacity. If the visible spectrum was an octave, where the \"tone\" of a color wrapped around from red back to blue the way G wraps to A, it'd be the blue one octave above visible blue. reply roenxi 39 minutes agoparentprevPhysics like this (really I'd call it materials science; it isn't but it has immediate practical applications on building things) is a bit of a sleeper in terms of importance. Small improvements in tolerances and materials drive huge changes in what is economically feasible at the other end of the science-engineering-machining pipeline. \"We've built a higher precision thing\" is usually huge news. Take semiconductors, where the entire industry is driving crazy value entirely from getting better at moving atoms around by a few nanometers. Missing out on the magic number does seem like a bit of a problem, but really the expectations on the audience are already quite low. That number could easily turn out to be worth more than a trillion dollars to humanity at large, but I'd bet most readers just think of it as a party factoid. reply M95D 1 hour agoparentprevThey could at least say it was a UV laser. reply colecut 2 hours agoparentprevI guess they could have said the laser frequency is about 2.02 petahertz reply fsh 3 hours agoprevThe measurement was already confirmed by a different group: https://arxiv.org/abs/2404.12311 This is important since impurities in the crystals used lead to all kinds of fluorescence that could be mistaken for a signal from the Thorium ions. Now two groups have seen exactly the same signal in different Thorium-doped crystals which is very covincing that they have found the actual nuclear transition. reply dguest 1 hour agoparentI went looking for an arXiv link for this paper and found the one you linked. Kind of weird that this new paper is only on the group's website [1] and not on the arXiv. [1]: https://www.tuwien.at/fileadmin/Assets/tu-wien/News/2024/Tho... reply tlb 3 hours agoprevFrom the paper, the light is UV-C at around 140nm or 8.4 eV. But it has to be very precisely the right energy to cause the transition, since nuclear states don’t have any place to dump excess energy to. reply est 1 hour agoprev> But it is not just time that could be measured much more precisely in this way than before. For example, the Earth's gravitational field could be analyzed so precisely that it could provide indications of mineral resources or earthquakes This has military applications as well, right? Replacing GPS for nuclear submarines. https://news.ycombinator.com/item?id=29213751 https://news.ycombinator.com/item?id=36222625 reply rbanffy 42 minutes agoprevProfessor Thorsten discovering a mystery of Thorium... How appropriate. Thor must be proud. reply lifeisstillgood 59 minutes agoprev>>> For example, the Earth's gravitational field could be analyzed so precisely that it could provide indications of mineral resources Hold on how does that work? I have had a sort of sci-fi idea that sufficiently sensitive gravitational field measurements coukd detect the passing of submarines (I am not sure on the maths tbh) - which would render a lot of nuclear strategy moot. Just need to get a grasp on the maths reply fodkodrasz 17 minutes agoparentActually the method of detecting mineral deposits by mapping gravitational field is already in use since a long time! The Eotvos pendulum (an instrument aka. Eotvos torsion balance) designed in 1888 started this kind of measurement. It was used commonly by the 1920s by geophysicist for mapping underground deposits by measuring the gradient of the gravitational field very precisely. This instrument was deprecated later by even better tools for surveying. The instrument was initially constructed for the experiment showing that inertial and gravitational mass are the same (well, linearly correlated) to a great precision: https://en.wikipedia.org/wiki/E%C3%B6tv%C3%B6s_experiment https://www.nature.com/articles/118406a0 (pretty useless link, but a famed periodical) Detecting submarines is way harder, practically impossible. as others have already pointed out. reply meindnoch 52 minutes agoparentprevhttps://apps.dtic.mil/sti/pdfs/AD1012150.pdf Gravitational Detection of Submarines, PM Moser 1989 reply perihelions 30 minutes agorootparentLook at what paper actually says: flat \"not achievable\" in the abstract; and the scaling laws on page 4 are third- and fourth- inverse powers of distance (!!!!); and on page 7 they're considering ranges of the same length scale as a submarine itself (few hundreds of meters), and even there it's hopeless. This one's never going to happen. Geologic mass concentrations are an entirely different story: you get a gravitational monopole, which is a more reasonable inverse square law. (No monopoles for a submarine, because by design they have a mean density equal to water—as the paper explains). reply moffkalast 47 minutes agorootparentprevThat then makes single SLBM drone swarms the new meta. Spread them over a large enough area and it'll just seem like tectonic activity. reply rpastuszak 33 minutes agoparentprevCheck out quantum navigation systems. They're not used to track submarines, but rather as an alternative to GPS for submarines (using tiny differences in the Earth's gravitational field to determine position). (IIRC) Royal Navy trialed it (officially) for the first time last year. reply Geenkaas 1 hour agoprevMy high school physics class flashes back to me, I don't think I understand a fraction of it but it seems very exciting (pun intended). I was reading up on this (now outdated) wiki page: https://en.wikipedia.org/wiki/Isotopes_of_thorium#Thorium-22... And it mentions the application as qubit for quantum computers. If the state change is relatively simple, cheap and stable, what could this do for quantum computing? I picture a crystalline processor holding Thorium nuclei as the brains of a new supercomputer? Would that be viable? reply bamboozled 1 hour agoprevFor example, the Earth's gravitational field could be analyzed so precisely that it could provide indications of mineral resources Resources companies are salivating reply worldsayshi 40 minutes agoparentIt wouldn't be precise enough to measure things like what type of rock you have underneath when you're thinking about digging a tunnel or to find land mines in dirt right? reply alexey-salmin 1 hour agoprevDid anyone understand how they hold a nucleus (not an atom) in a crystal? Nucleus is charged and seeks electrons, I thought you need an electromagnetic trap for that (which the article says they don't use). reply spuz 1 hour agoparentThey use Th4+ ions, not nuclei. The lack of 4 electrons in the Th cations is compensated for by the surrounding F- anions. reply yread 1 hour agoprevIs there some direct application? Like using the excitation states of different atoms for storing information? reply irjustin 1 hour agoparentThe article directly stated more precise atomic clocks. reply pzs 1 hour agorootparentNot a physicist, so I am asking out of curiosity and to learn: have the limitations to the precision of current atomic clocks posed any problems? reply sgt101 1 hour agorootparentsynchronization of compute across data centers is something I've used atomic clocks for, precision and cost are an issue. reply nullc 31 minutes agorootparentprevWe derrive most of our other units from time, so differences in time accuracy translate into metrology improvements more generally. Existing atomic clocks based on electrical interactions are extremely sensitive to the surrounding magnetic and electrical environment-- so for example accuracy is limited by collisions with other atoms, so state of the art atomic clocks have optically trapped clouds in high vacuums. Beyond limiting their accuracy generally makes the instruments very complex. One could imagine an optical-nuclear atomic clock in entirely solid state form on a single chip with minimal support equipment achieving superior stability to a room sized instrument. reply huytersd 1 hour agorootparentprevIf I remember correctly GPS is effected but the ultra precise version the gov uses can error correct pretty well. I would think greater GPS precision at a lower cost? reply raverbashing 1 hour agorootparentprevUsually these application, while they're good, they're just the initial idea people have given the current understanding The cool applications usually come later (or they're more esoteric). The researchers were more excited to determine the actual frequency than think about clocks reply danans 3 hours agoprev> This makes it possible to combine two areas of physics that previously had little to do with each other: classical quantum physics and nuclear physics. Is quantum physics now considered part of classical physics? If so then man, time flies! reply JanisErdmanis 2 hours agoparentWe currently don’t know how to calculate the nucleus’s bound state despite a thorough understanding of individual pieces that make it together, as explored in colliders like CERN and others. The problem is similar to telling at what temperature water is boiling, freezing, and its density from knowing the properties of a single water molecule. We understand quantum mechanics and Columb forces govern the properties; it is incredibly hard to renormalise the system from an energy scale of a gas to a liquid or solid. Similarly, it is for a quark-gluon plasma; thus, phenomenological models are used, like how the nuclear potential could look and the masses for different combinations of nuclei. reply emblaegh 2 hours agoparentprevClassical there is used in the sense of “non relativistic”. reply dxuh 3 hours agoparentprevThere is a big difference between \"classical\" quantum mechanics (about 100 years old now!) and quantum field theory (~50 years old). Maybe that's what they mean? reply snthpy 2 hours agorootparentWhen I was at university about 25 years ago, QM was about electron transition energies, with QED being a refinement of that for things like fine structure. In experimental HEP you had QCD and quark gluon plasma which informs things like the LHC experiments at CERN. IIRC nuclear physics was largely phenomenological with a lot of observations that had simple models fit to them without being able to reduce those to the particle physics models. This might be about establishing a link between the phenomenological nuclear models and the fundamental QM models. reply scotty79 3 hours agoparentprevI think they want to convey that nucleus is a really weird place so quantum physics of everything else is classical by comparison. reply sharpshadow 1 hour agoprevThat’s great news! As far as I understand constants are not an easy topic and being able to analyse more precisely is a big win. reply dtx1 2 hours agoprevWhat does \"exciting a nucleus\" mean? reply cshimmin 2 hours agoparentIt means getting the nucleus to absorb a certain energy above its ground state. Since it is a quantum object, it can only absorb/emit energy in very specific amounts at once (“quanta”). The details of how the nucleus manifests that extra energy are complicated, but you can imagine it as like, picking up a certain vibrational frequency. reply euroderf 1 hour agorootparentWith enough absorptions, can the nucleus tear itself apart (i.e. fission) ? reply the8472 41 minutes agorootparentYes, that's one possibility[0]. Or the energy can be sufficient to alter the decay rates of other nuclear reactions (alpha/beta decay, etc.) compared to the base isotope. A weird example: Excited tantalum-180[1] is more stable than its base state. [0] https://en.wikipedia.org/wiki/Photofission [1] https://en.wikipedia.org/wiki/Isotopes_of_tantalum#Tantalum-... reply huytersd 1 hour agorootparentprevBut then what happens? Does it expel an electron/release energy etc.? reply greenbit 58 minutes agorootparentProbably just emits another photon of the exact same wavelength a short time later. The time would be probabilistic, like 50% chance of emission in X amount of time. reply graycat 27 minutes agorootparentPhysics does not emphasize this, but the half life concept essentially assumes a Poisson process (Cinlar, Stochastic Processes) which has a Markov (past and future conditionally independent given the present, details from the Radon-Nikodym theorem, with a cute von Neumann polynomial proof, Rudin, Real and Complex Analysis) assumption. The half life concept seems to be standard over much of physics. That a Markov assumption could hold might suggest some new physics. reply atoav 2 hours agoparentprevNot a physicist but \"exciting\" a thing means to make it oscillate e.g. by adding energy into the system. A violin player is exciting the string of her instrument using a bow. Now in this case they use lasers. I suspect if you choose the right wavelenght (=frequency) of light there is some sort of resonance phenomenom. reply sebws 2 hours agoparentprevThe article mentions switching between \"energy states\": > This nucleus has two very closely adjacent energy states – so closely adjacent that a laser should in principle be sufficient to change the state of the atomic nucleus. > the correct energy of the thorium transition was hit exactly, the thorium nuclei delivered a clear signal for the first time. The laser beam had actually switched their state. I don't know enough to explain any further. reply guidedlight 2 hours agoparentprevThorium-229 has two energy states. A ground state, and an excited isometric state. The laser is used to transition the nucleus from the ground state to the excited isometric state. reply tinco 1 hour agorootparentNot a physicist, so this comment is more of a guess with the intention of someone correcting me, but I think the thing all the physicists leave out because it's probably very obvious is that when an excited nucleus returns to its ground state, it will emit radiation. So they hit their thorium with a laser, and then instead of the laser passing through, it gets absorbed, and then they get a flash of radiation back, letting them know the thorium was excited. The delay between the laser pulse and the flash of radiation is a property of the particular thorium nucleus, and is not affected by environmental circumstances like temperature or electric/magnetic fields, so can be relied on as a very precise measurement of time. reply phendrenad2 2 hours agorootparentprevIsometric? Like, is the nucleus gaining a virtual proton or something? reply nullc 50 minutes agorootparentNucleons occupy orbital energy states like electrons. The application of energy can shift the state of the nucleus, and some of these alternative states are relatively stable. https://en.wikipedia.org/wiki/Nuclear_shell_model reply bboygravity 2 hours agorootparentprevAnd then? reply topspin 1 hour agorootparentAnd then the nuclei return to the ground state. That process is probabilistic and measured in half-lives. The key point is that the decay back to ground state happens at a very precise rate that is not influenced by effectively anything, and can be measured accurately. Thus, a clock. reply dtx1 1 hour agorootparent> That process is probabilistic and measured in half-lives > The decay back to ground state happens at a very precise rate that is not influenced by effectively anything That sounds contradictory to me. reply topspin 1 hour agorootparentI suppose it could: the term \"probabilistic\" applies to the quantum probability of any one metastable isomer (excited nucleus) decaying to ground state. In application you measure large numbers of decays, and in great numbers the decay curve is extremely precise. reply gilgoomesh 2 hours agoparentprevApplying energy to lift its electrons across a band gap. In this case, applying 8.35574 electron volts. reply mypalmike 2 hours agorootparentI thought this was an excitation of the state of the nucleus rather than that of electrons. reply popol12 1 hour agorootparentYou’re right, parent read too fast reply Turneyboy 1 hour agorootparentprevWe know how to do this and have observed this tons of times at this point. This would not be novel in any way. This is about exciting the nucleus which is completely different. reply gwd 2 hours agoprev> It could be used, for example, to build an nuclear clock that could measure time more precisely than the best atomic clocks available today. Are today's atomic clocks really so imprecise? Without further explanation of this, it reminds me of this comic (which is alas showing its age both by mentioning flash, and by implying that 1024 is already a uselessly high number of cpus to support): https://xkcd.com/619/ reply fancy_pantser 2 hours agoparentIntel GPU joke in the alt text, truly timeless. reply nullc 2 hours agoprev [–] Now how the heck do you generate ~148.38nm light with a narrow linewidth? Their approach using four-wave mixing inherently results in short pulses. .. and given that it decays through gamma emission, does this mean we could now build an optically pumped gamma ray laser? reply karma_pharmer 1 hour agoparentThe last sentence of the paper seems to imply that this result will give people a reason to want to develop those: The development of dedicated VUV lasers with narrow linewidth will make it possible to access a new regime of resolution and accuracy in laser M¨ossbauer spectroscopy and to perform coherent control of a nuclear excitation\" Previously, if you wanted to manipulate nuclear states, you needed a synchrotron. Now, you need an infinitely less expensive instrument. I suppose the idea is that that will generate a lot of interest in improving the less-expensive instrument. reply M95D 1 hour agoparentprev [–] The gamma emission would have to re-excite other atoms in a cascade to create a laser. Since the exciting energy is UV, not gamma => no cascade amplification. A \"wavelength converter\" might be possible. PS: Are you sure it's gamma emission? That takes more energy than the exciting UV photon. reply karma_pharmer 45 minutes agorootparent [–] > PS: Are you sure it's gamma emission? That takes more energy than the exciting UV photon. Apparently it is neither: Decay of the 229Th isomeric state of the neutral thorium atom occurs predominantly by internal conversion (IC) with emission of an electron https://www.nature.com/articles/nature17669 https://en.wikipedia.org/wiki/Internal_conversion This is pretty weird. You shine UV light (with exactly the right wavelength) on 229Th, and it spits out electrons. But not like the photoelectric effect, where the electrons stop as soon as you turn off the light. No no. The Thorium keeps spitting out an exponentially-decaying stream of electrons for hours after you stop illuminating it. Almost like an exponentially-discharging solar-powered current source (for a very specific wavelength of \"solar\"). reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Physicists have made a significant advancement by stimulating the \"thorium transition\" in atomic nuclei with lasers, offering prospects for highly accurate technologies such as nuclear clocks.",
      "They identified a unique state in thorium atomic nuclei that laser beams can control, enabling precise measurements and future technological innovations.",
      "This discovery may lead to the development of advanced atomic clock technology leveraging light oscillation for enhanced precision."
    ],
    "commentSummary": [
      "Scientists manipulated thorium-229 nuclei with a laser, showcasing applications in materials science and industries like semiconductors.",
      "The precision and frequency of the laser were crucial for this success, opening doors for potential economic impacts.",
      "Potential applications include using thorium-229 for quantum computing qubits, gravitational detection for submarine tracking, and creating more accurate atomic clocks due to its radiation decay for precise time measurements."
    ],
    "points": 160,
    "commentCount": 64,
    "retryCount": 0,
    "time": 1714366893
  },
  {
    "id": 40191723,
    "title": "Ollama 0.1.33-rc5 Introduces Llama 3, Phi 3, and Qwen 110B",
    "originLink": "https://github.com/ollama/ollama/releases/tag/v0.1.33-rc5",
    "originBody": "ollama / ollama Public Notifications Fork 4.5k Star 62k Code Issues 739 Pull requests 182 Actions Security Insights Releases v0.1.33-rc5 v0.1.33 Pre-release Pre-release Compare github-actions released this · 16 commits to main since this release v0.1.33-rc5 ec1acbb Models: Llama 3: a new model by Meta, and the most capable openly available LLM to date Phi 3 Mini: a new 3.8B parameters, lightweight, state-of-the-art open model by Microsoft. Moondream moondream is a small vision language model designed to run efficiently on edge devices. Dolphin Llama 3: The uncensored Dolphin model, trained by Eric Hartford and based on Llama 3 with a variety of instruction, conversational, and coding skills. Qwen 110B: The first Qwen model over 100B parameters in size with outstanding performance in evaluations What's Changed Fixed issues where the model would not terminate, causing the API to hang. Fixed a series of out of memory errors on Apple Silicon Macs Fixed out of memory errors when running Mixtral architecture models Experimental concurrency features New concurrency features are coming soon to Ollama. They are available OLLAMA_NUM_PARALLEL: Handle multiple requests simultaneously for a single model OLLAMA_MAX_LOADED_MODELS: Load multiple models simultaneously To enable these features, set the environment variables for ollama serve. For more info see this guide: OLLAMA_NUM_PARALLEL=4 OLLAMA_MAX_LOADED_MODELS=4 ollama serve New Contributors @sidxt made their first contribution in #3705 @ChengenH made their first contribution in #3789 @secondtruth made their first contribution in #3503 @reid41 made their first contribution in #3612 @ericcurtin made their first contribution in #3626 @JT2M0L3Y made their first contribution in #3633 @datvodinh made their first contribution in #3655 @MapleEve made their first contribution in #3817 @swuecho made their first contribution in #3810 @brycereitano made their first contribution in #3895 @bsdnet made their first contribution in #3889 @fyxtro made their first contribution in #3855 @natalyjazzviolin made their first contribution in #3962 Full Changelog: v0.1.32...v0.1.33-rc5 Contributors secondtruth, swuecho, and 11 other contributors Assets 10 42 3 17 9 17 1 70 people reacted",
    "commentLink": "https://news.ycombinator.com/item?id=40191723",
    "commentBody": "Ollama v0.1.33 with Llama 3, Phi 3, and Qwen 110B (github.com/ollama)144 points by ashvardanian 13 hours agohidepastfavorite48 comments wiktor-k 4 hours agoOllama is simply great! I was quite surprised how easy it is to integrate through their API. A simple chat using Ollama + llama3 is less than 40 lines of TypeScript: https://github.com/wiktor-k/llama-chat reply oulipo 2 hours agoparentNice! Would there be a way to do that streaming, with streaming voice input too? reply jerrygenser 10 hours agoprevI wonder if Ollama will or plans to have other \"Supported backends\" than llama.cpp. It's listed on the very last line of their readme as if the llama.cpp dependency is just incidental and a very minor detail rather than Ollama as a deployment mechanism for llama.cpp and gguf based models. reply sh79 2 hours agoparentTheir behaviour around llama.cpp acknowledgement is very shady. Until the very recent, there was no mention of llama.cpp in their README at all and now it's tucked away all the way down. Compare that to the originally proposed PR for example: https://github.com/ollama/ollama/pull/3700 reply margorczynski 29 minutes agorootparentDo you know maybe what are these alternative engines they're talking about? Or is it just a way to evade the fact that at the end of the day it is just a wrapper around llama.cpp? reply jmorgan 8 hours agoparentprevYes, we are also looking at integrating MLX [1] which is optimized for Apple Silicon and built by an amazing team of individuals, a few of which were behind the original Torch [2] project. There's also TensorRT-LLM [3] by Nvidia optimized for their recent hardware. All of this of course acknowledging that llama.cpp is an incredible project with competitive performance and support for almost any platform. [1] https://github.com/ml-explore/mlx [2] https://en.wikipedia.org/wiki/Torch_(machine_learning) [3] https://github.com/NVIDIA/TensorRT-LLM reply smcleod 4 hours agorootparentMLX and TensorRT would be really nice! reply sdesol 9 hours agoparentprevI don't think they will move away from llama.cpp until they are forced to. The number of people contributing to llama.cpp is quite significant [1] and it wouldn't make sense to use another backend given how quickly llama.cpp is iterating and growing. [1] https://devboard.gitsense.com/ggerganov?r=ggerganov%2Fllama.... Full disclosure: This is my tool reply refulgentis 7 hours agorootparentghost of christmas future The chance onnx becomes significantly relevant here went from 1% to 15% this week. They're demo'ing ~2x faster inference with Phi-3. There's been fits and starts on LLMs in ONNX for a year, but, with Wintel's AI PC™ push, and all the constituent parts in place (4 bit quants! adaptive quants!), I'd put very good money on it. reply sdesol 6 hours agorootparentSo you are saying Ollama is a strong MS acquisition in the future if onnx works out. reply refulgentis 5 hours agorootparentno, ONNX is a Microsoft project, I don't know why people know what Ollama is and I don't think they will in a year reply sdesol 5 hours agorootparentI know it is a Microsoft Project. My reasoning is, if Ollama supports ONNX and if it can provide performance on par or better than llama.cpp, it would make sense for Microsoft to acquire Ollama for distribution reasons. reply Cheer2171 8 hours agoparentprevIt is open source, so if you want to see this in ollama, pull requests are welcome. :) reply anotherpaulg 9 hours agoprevI actually just benchmarked Llama3 70B coding with aider, and it did quite well. It scored similar to GPT 3.5. You can use Llama3 70B with aider via Ollama [0]. It's also available for free via Groq [1] (with rate limits). And OpenRouter has it available [2] for low cost on their paid api. [0] https://aider.chat/docs/llms.html#ollama [1] https://aider.chat/docs/llms.html#groq [2] https://aider.chat/docs/llms.html#openrouter reply typpo 6 hours agoparentPaul's benchmarks are excellent and they're the first thing I look for to get a sense of a new model performance :) For those looking to create their own benchmarks, promptfoo[0] is one way to do this locally: prompts: - \"Write this in Python 3: {{ask}}\" providers: - ollama:chat:llama3:8b - ollama:chat:phi3 - ollama:chat:qwen:7b tests: - vars: ask: a function to determine if a number is prime - vars: ask: a function to split a restaurant bill given individual contributions and shared items Jumping in because I'm a big believer in (1) local LLMs, and (2) evals specific to individual use cases. [0] https://github.com/typpo/promptfoo reply jkh1 4 hours agoparentprevAnother benchmark: https://www.biorxiv.org/content/10.1101/2024.04.19.590278v2.... reply stephen37 1 hour agoprevI love working with Ollama, I was really surprised at how easy it is to build a simple RAG system with it. For example: https://github.com/stephen37/ollama_local_rag reply addandsubtract 9 minutes agoparentNice, I've been looking out for something like this! What's Jina AI and how is it local if I need an API key for it? Also, this is the first time I'm hearing about poetry. Might be worth including in the prerequisites (unless I can just stick with pip?) reply yjftsjthsd-h 11 hours agoprev[Why] do models require a new version? It can already take arbitrary gguf; I assumed they just had a registry online reply ynniv 11 hours agoparentThey do, and I was using the \"new\" models before the update. Perhaps there is tuning or bug fixes for them? Or they just want to confirm that these are supported. There are some new models that do have different architectures, so sometimes an update is necessary. reply refulgentis 7 hours agorootparentPhi 3 has a unique architecture that needed some additions to llama.cpp's conversion script. Also Phi 3 is an absolute mess, there's no reliable way to latch on to when it's done writing a message and no one wants to admit it, people are patching around it instead. ex. I could condition on \"||||\", but it'd still be wrong. Pretty much everything Phi 3 feels like it needed to all come out within 48 hours a month too early. The ONNX genai library doesn't work on Mac, at all, the mobile SDKs don't support it...sigh reply FieryTransition 10 hours agoparentprevBecause the way they are quantized takes time to get bug-free when new architectures are released. If a model was quantized with a known bug in the quantizer, then it effectively makes those quantized versions buggy and they need to be requantized with a new version of llamacpp which has this fixed. reply thedatamonger 11 hours agoprev [–] this looks very awesome. can someone tell me why there is no chatter about this? is there something else out there that blows this out of the water in terms of ease of use and access to sample many LLM's ? reply brrrrrm 10 hours agoparentHN isnt really the best space for LLM news - r/LocalLlama and twitter are much better. I think HN has some cultural issues with “AI” news reply wkat4242 10 hours agorootparentHmm I don't think so. Most comments are pretty positive. I think the articles are just not really upvoted unless it's really big news, makes sense because HN is for more than just AI. But I don't think it's anti-AI like most people here would be pretty anti-cryptocurrency (and for good reason IMO) reply p1esk 9 hours agorootparentI didn’t upvote it because I don’t use Ollama. To experiment with LLMs I use Huggingface. Does Ollama provide something I cannot get with Huggingface? reply jkh1 4 hours agorootparentRunning locally is sometimes necessary, e.g. you don't want to send sensitive data to any random third party server. reply lolinder 5 hours agorootparentprevOllama provides a web server with API that just works out of the box, which is great when you want to integrate multiple applications (potentially distributed on smaller edge devices) with LLMs that run on a single beefy machine. In my home I have a large gaming rig that sometimes runs Ollama+Open WebUI, then I also have a bunch of other services running on a smaller server and a Raspberry Pi which reach out to Ollama for their LLM inference needs. reply p1esk 5 hours agorootparentSure, maybe it’s better for niche use cases like yours. HF is the biggest provider of llms, and I guess I haven’t run into it’s limitations yet. reply gertop 8 hours agorootparentprevHugging face is a model repository. Ollama allows you to run those models. Different things. reply p1esk 8 hours agorootparentI run models using HF just fine. I mean I’m using HF transformers repo, which gets models from HF hub. Or do you mean commercial deployment of models for inference? reply simonw 8 hours agorootparentAre you talking about the Hugging Face Python libraries, the Hugging Face hosted inference APIs, the Hugging Face web interfaces, the Hugging Face iPhone app, Hugging Face Spaces (hosted Docker environments with GPU access) or something else? reply p1esk 8 hours agorootparentI updated my comment above: I’m using HF transformers repo, which gets models from HF hub. reply simonw 8 hours agorootparentDo you have an NVIDIA GPU? I have not had much luck with the transformers library on a Mac. reply p1esk 8 hours agorootparentOf course. I thought Nvidia GPUs are pretty much a must have to play with DL models. reply wkat4242 8 hours agorootparentOllama supports many radeons now. And I guess llama.cpp does too, after all it's what ollama uses as backend. reply p1esk 7 hours agorootparentPyTorch (the underlying framework of HF) supports AMD as well, though I haven’t tried it. reply objektif 8 hours agorootparentprevWell being able to run these models on CPU was pretty much the revolutionary part of llama.cpp. reply p1esk 8 hours agorootparentI can run them on CPU - HF uses plain Pytorch code - fully supported on CPU. reply tmostak 6 hours agorootparentBut it's likely to be much slower than what you'd get with a backend like llama.cpp on CPU (particularly if you're running on a Mac, but I think on Linux as well), as well as not supporting features like CPU offloading. reply p1esk 5 hours agorootparentAre there benchmarks? 2x speed up would not be enough for me to return to c++ hell, but 5x might be, in some circumstances. reply SushiHippie 2 hours agorootparentI think the biggest selling point of ollama (llama.cpp) are quantizations, for a slight hit (with q8 or q4) in quality you can get a significant performance boost. reply chadsix 10 hours agoparentprevOllama is really organized - it relies on llama but the UX and organization it provides makes it legit. We recently made a one-click wizard to run Open WebUI and Ollama together, self hosted and remotely accessible but locally hosted [1] [1] https://github.com/ipv6rslimited/cloudseeder reply FieryTransition 10 hours agoparentprevI use a mix of using llamacpp directly via my own python bindings and using it via llamacpp-python for function calling and full control over parameters and loading, but otherwise ollama is just great for ease of use. There's really not a reason not to use it, if just want to load gguf models and don't have any intricate requirements. reply Cheer2171 8 hours agoparentprevWhy do you think there is no chatter about this? There have been hundreds of posts about ollama on HN. This is a point release of an already well known project. reply CharlesW 9 hours agoparentprevI can recommend LM Studio and Msty if you're looking for something with an integrated UX. reply gertop 8 hours agoparentprevLM Studio is a lot more user friendly, probably the easiest UI to use out there. No terminal nonsense, no manual to read. Just double click and chat. It even explains to you what the model names mean (eg diff between Q4_1 Q4_K Q4_K_M... For whatever reason all the other tools assume you know what it means). Built-in model recommendations are also handy. Very friendly tool! However it's not open-source. reply throw03172019 11 hours agoparentprev [–] Lola a has been brought up many times on HN. It’s a great tool! reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The Ollama Public Notifications Fork has launched version 0.1.33-rc5, bringing in new models such as Llama 3, Phi 3 Mini, Moondream, Dolphin Llama 3, and Qwen 110B.",
      "Updates include fixes addressing model termination problems and out of memory errors while introducing new concurrency capabilities for managing simultaneous requests and model loading.",
      "The release also highlights the addition of several new contributors making their initial contributions to the project, fostering collaboration and growth within the community."
    ],
    "commentSummary": [
      "Ollama version 0.1.33 is discussed along with its integration with technologies like Llama 3, Phi 3, and Qwen 110B, using llama.cpp as a backend.",
      "Speculation includes the potential of alternative engines MLX and TensorRT, the relevance of ONNX, and the chance of Microsoft acquiring Ollama.",
      "Discussions cover benchmarks, examples of utilizing Ollama for language models, challenges with Phi 3, and leveraging HF libraries and models for deep learning, recommending tools like LM Studio and Msty for a seamless user experience."
    ],
    "points": 144,
    "commentCount": 48,
    "retryCount": 0,
    "time": 1714337322
  },
  {
    "id": 40192204,
    "title": "Urgent Action Needed to Stop SB 1047: Threat to Open-Source AI",
    "originLink": "https://www.affuture.org/post/9-context/",
    "originBody": "COMMUNITY Call-To-Action on SB 1047 April 28, 2024 California legislators, under the influence of Effective Altruism activists, are trying to sneak through a disastrous bill for open-source AI and the technology industry generally. SB 1047 creates an unaccountable Frontier Model Division that will be staffed by EAs with police powers, and which can throw model developers in jail for the thoughtcrime of doing AI research. It’s being fast-tracked through the state Senate. Since many cloud and AI companies are headquartered in California, this will have worldwide impact. We need your help to stop this now. It only takes a few minutes, and three steps, but you need to do them soon, ideally this week. Non-California residents are very welcome to comment, and indeed many of the supporters are not from California. 1. Submit a position letter to the bill author, which ensures that your position shows up on all future bill analyses that state Senators read. Go to https://leginfo.legislature.ca.gov/faces/billNavClient.xhtml?bill_id=202320240SB1047, click on “Comments to the Author”, register an account and submit an opposition letter. Use your own words for the letter, and feel free to borrow liberally from or cite friend of AFTF Context Fund’s analysis of the bill (https://www.context.fund/policy/sb_1047_analysis.html). One of key arguments being advanced is that this is “pro-little guy”, so if you’re involved in the open source movement, are a startup founder, or an investor, heavily cite your experience and how the bill affects you personally. If it’s still before May 5th, also submit comments to the Senate Appropriations Committee: https://sapro.senate.ca.gov/position-letters. Unfortunately, you have to submit to both the committee and the author to make sure the position letter is fully considered at the hearing. 2. Also add your public comment to the Context Fund analysis document (https://docs.google.com/document/d/1rk8uRTs0tiQ8O4k6ZVkwnOQX5n2PSdFhh0sXeXoDNM4/edit). 3. Tell your friends about this and introduce it to the media, publications you are connected to, and your own social media followings, especially those serving the ML, tech and startup audiences. Most builders and investors who would be affected by this bill are not aware of it, but the special interests are. We need to change that quickly. Thank you for your help!",
    "commentLink": "https://news.ycombinator.com/item?id=40192204",
    "commentBody": "Call-to-Action on SB 1047 – Frontier Artificial Intelligence Models Act (affuture.org)135 points by jph00 12 hours agohidepastfavorite95 comments Animats 11 hours agoI just sent in some comments. It's too late to stop \"deep fakes\". That technology is already in Photoshop and even built into some cameras. Also, regulate that and Hollywood special effects shops may have to move out of state. As for LLMs making it easier to people to build destructive devices, Google can provide info about that. Or just read some \"prepper\" books and magazines. That ship sailed long ago. Real threats are mostly about how much decision power companies delegate to AIs. Systems terminating accounts with no appeal are already a serious problem. An EU-type requirement for appeals, a requirement for warning notices, and the right to take such disputes to court would help there. It's not the technology. reply andy99 11 hours agoparent> Systems terminating accounts with no appeal are already a serious problem. Right, there is no issue with how \"smart\" ML models will get or whatever ignorant framing about intelligence and existential risk gets made up by people who don't understand the technology. The real concern is dumb use of algorithmic decision making without recourse which is just as valid whether it's an if statement or a trillion parameter LLM. reply mquander 5 hours agoparentprevAccording to the bill, a model has \"hazardous capabilities\" if and only if the existence of the model makes it \"significantly\" easier to cause the damages the bill covers. If Google is equally good at telling you how to build a bomb and Photoshop is equally good at producing deepfakes, then the bill takes no issue with your LLM. reply mhuffman 8 hours agoparentprev>As for LLMs making it easier to people to build destructive devices, Google can provide info about that. Or just read some \"prepper\" books and magazines. That ship sailed long ago. However, those can and are tracked. The thing making them nervous is the ability to do that on your own with no possible way for someone to track or catch you. Same with deepfakes. They don't care if you are doing it with photoshop, because that can be reviewed. They care that you can do it and not be caught/stopped/punished for it. reply AnarchismIsCool 8 hours agorootparentOk this is insanity. The thing keeping people from making destructive devices is the difficulty in synthesizing white fuming nitric acid and similar required precursors. You can Google wherever shit you want if you use someone else's wifi and you should be able to at least Google whatever you want without the feds showing up. The danger of ai has nothing to do with what the average Joe might try to do, it has everything to do with what soulless corporations are doing to you right now and how it enables them to be even worse in the future. Right now your roof is being scanned by aircraft with cameras and AI is being used to determine how old it is and if there are tree branches nearby. They're also looking at and classifying objects in your back yard to determine safety risks. It's not horribly accurate but because of the scale it doesn't matter to the companies, you just get fucked. Accidentally bag something you didn't scan at the self checkout? They have AI for that too, there are multiple reports of people being hunted down and charged with theft for simple mistakes. Your chances of having your life ruined of degraded because of AI are massively higher than your chances of being hurt by a random individual using it to build destructive devices. reply robotnikman 8 hours agorootparentCouldn't have worded it any better myself. AI is already being used for horrible things by companies and state actors and no one bats an eye over it. reply mhuffman 8 hours agorootparentIndividuals are usually the targets of companies and the government, whereas the government is immune from any blowback for with it does and has, right now, very cozy relationships with large corporations. reply mhuffman 8 hours agorootparentprevAll good points, but then ask yourself why this [0] is being put under Homeland Security and not just a business lobbying group. [0]https://www.msn.com/en-us/news/us/us-homeland-security-names... reply AnarchismIsCool 7 hours agorootparentBecause this is how us politics work. We freak out about stuff and immediately assume terrorism instead of billionaire profiteering. reply mhuffman 7 hours agorootparentWell that is what we are told to support the profiteering part, right? reply fragmede 7 hours agorootparentprev> The danger of ai has nothing to do with what the average Joe might try to do Why not both? With the story of a high school principal being framed by a coworker who deep faked a racist anti-semitic rant that the principal didn't say, I'd say the danger of AI also has to do with what an average Joe that wants to cause you harm can do. That doesn't diminish the threat from corporations, but a jilted lover can now ruin your life in additional ways. https://www.washingtonpost.com/dc-md-va/2024/04/26/baltimore... reply AnarchismIsCool 7 hours agorootparentIn the case of the example, it didn't work, but every day people are being dropped from their homeowners/car/health insurance. Yes there are dangers there but they ultimately come down to evidentiary standards. We can't do the thing we always do where all risk is perceived based off of extremely rare incidents so we destroy everyone's privacy while the stuff actually harming people at scale is ignored. reply jkuli 8 hours agorootparentprevIs this true? reply mhuffman 8 hours agorootparentThis is a snippet from OP site: >SB 1047 creates an unaccountable Frontier Model Division that will be staffed by EAs with police powers, and which can throw model developers in jail for the thoughtcrime of doing AI research. It’s being fast-tracked through the state Senate. Since many cloud and AI companies are headquartered in California, this will have worldwide impact. Of course that is scare propaganda, but when you put it with what the Federal govt is doing here[0], it makes it pretty clear that the real worry is people have access to \"dangerous\" information with no oversight. I can imagine policing agencies at every level getting very nervous with lone-wolf or tiny militia types getting access to information without any triggers flipping and alerting them and with no way to get any evidence if they do want to arrest them for something. [0]https://www.msn.com/en-us/news/us/us-homeland-security-names... reply Animats 3 hours agorootparentIt's somewhat exaggerated, but the bill definitely creates a \"Frontier Model Division\" with a rather vague charter and some enforcement authority. reply jkuli 8 hours agorootparentprevThe evidence is the damage caused by their actions. No need to punish people for crimes that haven't occurred. reply mhuffman 8 hours agorootparentI am not making a judgement here, just stating the reasons why they would want to do it. reply pcthrowaway 12 hours agoprevThis bill sounds unbelievably stupid. If passed, it will just result in a migration of AI projects out of California, save a few which are already tied to the EA movement. I'm not under the impression that the EA movement is better suited to steward AI development than other groups, but even assuming they were, there is no chance for an initiative like this to work unless every country agreed to it and followed it. reply mquander 11 hours agoparentThe bill doesn't give special treatment to \"EA\" models, so what does it matter whether projects are tied to EA or whether EAs are good stewards? Either it's a good law or it isn't. At a glance it looks like it's not going to affect AI projects that are basically consumers of existing models, which is most projects. reply sangnoir 8 hours agorootparentI suspect the \"EA\" label is author having an ax to grind or throwing red meat at people who already hate EA. Sam Altman is on record lobbying for \"AI safety laws\" which would have a similar effect of raising the bar of entry incredibly high using legal peril, and he was reportedly ousted from the board by EA-aligned folk. reply ShamelessC 4 hours agorootparentWith all due respect, the people on HN are _weirdly_ in favor of a group whose unstated “zeroth” tenet is basically “be born into wealth, get extremely lucky, disregard common criticism of capitalism, reframe libertarianism if you need. Then and only then can you begin your righteous mission of donating your money “effectively”. Could you at least consider that the group’s entire premise seems like nothing more than the post hoc rationalization of a bunch of wealthy educated elites with low social and emotional intelligence? The levels of tone deaf I perceive as someone who doesn’t have, can’t have that much wealth to even begin my journey in their little club are enormous. There’s very little that is subtle about it and it’s frankly offensive and _clearly_ used as a justification for insecure Bay Area “liberals” who find themselves with lots of money and a political identity that makes them insecure about that fact. The answer? You’re actually saving mankind with your money! It’s just simple Bayesian logic! The same thing that got you here! (Spoiler alert: it was more to do with luck than skill). If you’re on board, I guess it’s not as offensive? But for me and others, it’s like elites trying to brag about how great they are while the rest of us fight for scraps. So when you see someone with an axe to grind, maybe consider that EA’s messaging is not as universally appealing as you think, and may even be outright tone deaf enough to cause one to reasonably find it disgusting. > and he was reportedly ousted from the board by EA-aligned folk. It seems naive to me to assume that members of EA in positions of power don’t secretly have their own motivations. Furthermore it’s not very “Bayesian” to assume that a implies b here with so many hidden variables at play. reply fragsworth 7 hours agorootparentprev> At a glance it looks like it's not going to affect AI projects that are basically consumers of existing models, which is most projects. If it affects the base projects (especially the open source ones like Llama) then it affects the consumers. And it certainly looks like it's planning to affect the base projects, in a lot of negative ways. If this bill passed in any way remotely similar to what it is now, Meta would have to entirely stop releasing open source Llama updates. Which is perhaps the intent of the legislation. reply pcthrowaway 7 hours agorootparentprevI mean I was just going off the second sentence of the article, my bad: > SB 1047 creates an unaccountable Frontier Model Division that will be staffed by EAs with police powers, and which can throw model developers in jail for the thoughtcrime of doing AI research If the bill says nothing about who will be staffing this agency, and there are indeed no ties to EA (which seems unlikely to me if EA is behind the bill), then the author of the article is doing us a disservice by misrepresenting it. reply echelon 12 hours agoparentprevHonestly it would be good for AI if it left California. California has too much regulatory burden and taxation. reply kbenson 11 hours agorootparentMy initial interpretation of this is along the lines of \"this very contentious thing that many people are afraid will cause lots of problems if not handled carefully with checks and balances should move out of the current place it's generally being done that cares a lot about and puts a lot of checks and balances into place, because they have too many.\" Am I jumping to conclusions and is there a different interpretation you think I should be coming away with? reply AnthonyMouse 11 hours agorootparentThat interpretation isn't necessarily unmeritorious. Suppose you have a place with Level 7 checks and balances and people are content to live under them. If you dial it up to 9, then they move to a place at Level 2, which otherwise wouldn't have been worth it because of other trade offs. So the new rules don't take you from Level 7 to Level 9, they take you from Level 7 to Level 2. But there is also another interpretation, which is that the new thing is going to happen in whatever place has the least stringent rules anyway, so more stringent rules don't improve safety, they just deprive your jurisdiction of any potential rewards from keeping the activity local, and provide people in other jurisdictions the benefit of the influx of people you're inducing to leave. reply kbenson 10 hours agorootparentSo, obviously there is also interpreting that statement in a vacuum, which I sort of did, and interpreting it as a criticism of this current proposal. I think it's slightly ambiguous what was meant, even given it's location, because it seemed like a general sentiment and not specific to this proposal. I'm not sure in the general context your first came is super applicable, given that there's a lot of exposure and worry about this topic. Laws and regulation can apply to use as well as development, and large markets can have outsized effects when they require things (such as how CA emissions laws and GDPR have), and I'm not sure worrying about chasing away business is a worthwhile concern when many people are very afraid of a societal consequences of the thing in question. For what it's worth I don't really follow the same stance when it comes to military technology, because that's meant to be used in a situation when local (which in that case can be national) laws have little or no sway, but I'm open to arguments about how viewing them differently isn't useful. reply coffeebeqn 9 hours agorootparentprevWhy did it happen in the Bay Area in the first place then? People love to hate on CA but it sure seems to keep producing interesting products that the rest of the world cannot. reply janalsncm 7 hours agorootparentPeople that complain about “regulatory burden” and “taxation” don’t have any specific complaints, just vague platitudes that may or may not be true depending on what you’re talking about. It doesn’t make any sense to talk about the number of regulations. What matters is what those regulations are. Likewise, it doesn’t make sense to talk about the amount of taxation without talking about who is being taxed. reply jph00 11 hours agoprevI've written a submission to the authors of this bill, and made it publicly available here: https://www.answer.ai/posts/2024-04-29-sb1047.html The EFF have also prepared a submission: https://www.context.fund/policy/2024-03-26SB1047EFFSIA.pdf A key issue with the bill is that it criminalises creating a model that someone else uses to cause harm. But of course, it's impossible to control what someone else does with your model -- regardless of how you train it, it can be fine-tuned, prompted, etc by users for their own purposes. Even then, you can't really know why a model is doing something -- for instance, AI security researchers Arvind Narayanan and Sayash Kapoor point out: > Consider the concern that LLMs can help hackers generate and send phishing emails to a large number of potential victims. It’s true — in our own small-scale tests, we’ve found that LLMs can generate persuasive phishing emails tailored to a particular individual based on publicly available information about them. But here’s the problem: phishing emails are just regular emails! There is nothing intrinsically malicious about them. A phishing email might tell the recipient that there is an urgent deadline for a project they are working on, and that they need to click on a link or open an attachment to complete some action. What is malicious is the content of the webpage or the attachment. But the model that’s being asked to generate the phishing email is not given access to the content that is potentially malicious. So the only way to make a model refuse to generate phishing emails is to make it refuse to generate emails. Nearly a year ago I warned that that bills of this kind could hurt, rather than help safety, and could actually tear down the foundations of the Enlightenment: https://www.fast.ai/posts/2023-11-07-dislightenment.html reply zer00eyz 10 hours agoparent> A key issue with the bill is that it criminalises creating a model that someone else uses to cause harm. Build a model that is trained on the corpus of gun designs. Should be an interesting court case and social experiment. reply Dalewyn 9 hours agorootparentI was about to say how quick a lot of people are to blame the gun (the tool) and the manufacturer, but when it comes to \"AI\" (the tool) suddenly they turn 540 degrees and blame the user. The reasonable take of course is that the tools are never to blame, they are just tools after all. Blame the bastard using the tools for nefarious ends, whether it's guns or \"AI\" or whatever else the case may be. reply zer00eyz 9 hours agorootparentWorld Trade Center bombing 1993 The Oklahoma City bombing 1995 Most people with a high school level of chemistry and a trip to the library can cause a lot of damage. David Hann https://en.wikipedia.org/wiki/David_Hahn the radioactive Boy Scout single handedly created a superfund site. This will quickly turn into a first amendment case and die in court I would think. reply janalsncm 7 hours agorootparentTypically it also requires precursor materials. I am against laws and systems of government that turn me into a suspect just for learning information. It is not ok that a secret investigation into our private lives is triggered simply by being curious. reply hackermatic 8 hours agoprevI encourage people to look for a variety of opinions on this bill -- and its various parts -- so you can better figure out which parts you actually want to keep, change, or remove, and give your legislators that specific feedback. Alliance for the Future is a lobby group of effective accelerationists who endorse some of Marc Andreesen and Peter Thiel's views in their manifesto, and based on that plus this article, they seem to oppose the bill entirely. A place to start for a breakdown of what's in the bill is the Context Fund analysis that AFTF links to. That analysis cites similar critiques from EFF, the Software & Information Industry Association, and others. All of these are from the perspective of voting against or substantially changing the bill. I haven't found \"pro bill\" opinions as easily, but I haven't been plugged into the conversations around this, so I'm missing anything that doesn't appear on the first few pages of Google or DDG. reply interroboink 11 hours agoprevI feel like the legal definition of \"AI Model\" is pretty slippery. From this document, they define: “Artificial intelligence model” means an engineered or machine-based system that, for explicit or implicit objectives, infers, from the input it receives, how to generate outputs that can influence physical or virtual environments and that may operate with varying levels of autonomy. That's pretty dang broad. Doesn't it cover basically all software? I'm not a lawyer, and I realize it's ultimately up to judges to interpret, but it seems almost limitless. Seems like it could cover a kitchen hand mixer too, as far as I can tell. reply _heimdall 12 hours agoprevAnyone have a link to a less biased explanation of the bill? I can't take this one too seriously when it baselessly claims people will be charged with thought crimes. reply s1k3s 12 hours agoparentWhy not read the bill itself? https://leginfo.legislature.ca.gov/faces/billNavClient.xhtml... It's not that big reply thorum 11 hours agorootparentThe bill only applies to new models which meet these criteria: (1) The artificial intelligence model was trained using a quantity of computing power greater than 10^26 integer or floating-point operations. (2) The artificial intelligence model was trained using a quantity of computing power sufficiently large that it could reasonably be expected to have similar or greater performance as an artificial intelligence model trained using a quantity of computing power greater than 10^26 integer or floating-point operations in 2024 as assessed using benchmarks commonly used to quantify the general performance of state-of-the-art foundation models. …and have the following: “Hazardous capability” means the capability of a covered model to be used to enable any of the following harms in a way that would be significantly more difficult to cause without access to a covered model: (A) The creation or use of a chemical, biological, radiological, or nuclear weapon in a manner that results in mass casualties. (B) At least five hundred million dollars ($500,000,000) of damage through cyberattacks on critical infrastructure via a single incident or multiple related incidents. (C) At least five hundred million dollars ($500,000,000) of damage by an artificial intelligence model that autonomously engages in conduct that would violate the Penal Code if undertaken by a human. (D) Other threats to public safety and security that are of comparable severity to the harms described in paragraphs (A) to (C), inclusive. …In which case the organization creating the model must apply for one of these: “Limited duty exemption” means an exemption, pursuant to subdivision (a) or (c) of Section 22603, with respect to a covered model that is not a derivative model that a developer can reasonably exclude the possibility that a covered model has a hazardous capability or may come close to possessing a hazardous capability when accounting for a reasonable margin for safety and the possibility of posttraining modifications. reply jph00 11 hours agorootparentPretty much all models, including today's models, already fall foul of the \"Hazardous capability\" clause. These models can be used to craft persuasive emails or blog posts, analyse code for security problems, and so forth. Whether such a thing is done as part of a process that leads to lots of damage depends on the context, not on the model. So in practice, only the flops criteria matters. Which means only giant companies with well-funded legal departments, or large states, can build these models, increasing centralization and control, and making full model access a scarce resource worth fighting over. reply andy99 10 hours agorootparentReally I feel the opposite way, that none of today's models or anything foreseeable meets the hazardous capability criteria. Some may be able to provide automation but I don't see any concrete examples where there's any actual step change in what's possible due to LLMs. The problem is it's all in the interpretation. I imagine some people will think that because a 7B model can give a bullet point list of how to make a bomb (step 1: research explosives) or write a phishing email that sounds like a person wrote it that it's \"dangerous\". In reality the bar should be a lot higher, like uniquely making something possible that wouldn't otherwise be, with concrete examples of it working or being reasonably likely to work, not just the spectre of targeted emails. I've been actually thinking there should be a bounty for a real hazardous use of AI identified. The problem would be defining hazardous (which would hopefully itself spur conversation). On one end I imagine trivial \"hazards\" like what we test models with today (like asking to build a bomb) and on the other it's easy to see there could be a shifting goalposts thing where we keep finding reasons something that technically meets the hazard criteria isn't reall hazardous. reply yellow_postit 11 hours agorootparentprevvery similar to what the Whitehouse put out [1] in terms of applicability being based on dual use & size. It is hard not to see this as a push for regulatory capture, specifically trying to chill open source development in favor of some well-funded industry closed-source groups which can adhere to these regulations. A harms-based approach, regardless of the model used, seems more able to be put into practice. [1] https://www.whitehouse.gov/briefing-room/presidential-action... reply janalsncm 7 hours agorootparentprevI don’t understand how a law can expect someone to foresee and quantify potential future damage. I understand the impetus to hold companies responsible, but that is simply impossible to know. reply purlane 11 hours agorootparentprevThis sounds entirely reasonable! reply 65a 9 hours agorootparent640kb should be enough for anyone! reply _heimdall 10 hours agorootparentprevWill do, thanks! I must have just missed it if the original page linked to it. reply polski-g 11 hours agorootparentprevLooks like it's trying to literally regulate speech. This would be struck down per Bernstein v DOJ. There is freedom of speech regardless if it's written in English or C. reply ickelbawd 8 hours agorootparentI dunno. This one seems trickier to me. Since it’s not really the code that’s the key part of the AI—it’s the trained numerical weights. Can you read and write in this language of floating point numbers? I doubt that. So perhaps yes you can write the code that could train a LLM and you could freely distribute that, but does freedom of speech allow you to train the model weights and distribute those? reply hellojesus 5 hours agorootparentYes. Hence freedom of speech. You're allowed to write them in a book and sell the book if you must. reply _heimdall 9 hours agorootparentprevWriting code absolutely does not fall under free speech. Neither does any product development. Ford isn't allowed to ignore seat belt requirements and claim the government is infringing on the designers' freedom of speech/expression. reply carbocation 8 hours agorootparenthttps://www.eff.org/deeplinks/2015/04/remembering-case-estab... reply _heimdall 7 hours agorootparentThat case was extremely specific and doesn't mean that all code written falls under free speech. It was also tried in a very different time. Given that we can't even allow free speech on digital platforms today, I'm not sure that many courts would allow for free speech claims to fall under the first amendment. reply jkuli 7 hours agorootparentprevHaha, confidently incorrect. You made that up didn't you. Absolutely times infinity. reply gedy 11 hours agoparentprevBriefly: - Developers must assess whether their AI models have hazardous capabilities before training them. They must also be capable of promptly shutting down the model if safety concerns arise. - Developers must annually certify compliance with safety requirements. They must report any AI safety incidents to a newly created Frontier Model Division within the Department of Technology. - Cluster Operation Regulation: OOpolicies to assess whether customers intend to use the cluster for deploying AI models. Violations may lead to civil penalties. - A new division within the Department of Technology will review developer certifications, release summarized findings, and may assess related fees. - The Department of Technology will establish a public cloud computing cluster named CalCompute, focusing on safe and secure deployment of large-scale AI models and promoting equitable innovation. reply _heimdall 9 hours agorootparentI don't work on ML directly so I'm definitely coming at it from a more general CS angle, but I wouldn't feel comfortable anywhere near the first two bullets. My outsider's understanding is that we really don't know specifically how the models learn what they learn or why they give specific answers. Is it possible that we could even know whether a model could present hazardous capabilities prior to training it? Or after it for that matter? reply gedy 8 hours agorootparentyeah, my problem with a lot of these people pushing for “regulating“ AI is that they seem to always assume the organizations they are dealing with are big, well-funded corporations. But this will affect individual or open source developers in California just as well. It’s a really dumb proposal, hopefully it doesn’t go through. reply Imnimo 11 hours agoprev>(2) “Hazardous capability” includes a capability described in paragraph (1) even if the hazardous capability would not manifest but for fine tuning and posttraining modifications performed by third-party experts intending to demonstrate those abilities. So if I hand-write instructions to make a chemical weapon, and aggressively \"fine-tune\" Llama 7B to output those instructions verbatim regardless of input, Meta is liable for releasing a model with hazardous capabilities? reply thorum 11 hours agoparentThe text says “in a way that would be significantly more difficult to cause without access to a covered model” and in another place mentions “damage by an artificial intelligence model that autonomously engages in conduct that would violate the Penal Code if undertaken by a human” so that probably doesn’t count. Though it might be open to future misinterpretation. reply Imnimo 11 hours agorootparentI don't agree with that reading. As long as my custom chemical weapon instructions are not publicly available otherwise, then it is surely more difficult to build the weapon without access to the instructions. The line about autonomous actions is only item C in the list of possible harms. It is separate from item A which covers chemical weapons and other similar acts. reply simonh 11 hours agorootparentIf someone is so keen on doing something in a way that’s illegal that they go to all that trouble specially to get in trouble with the law, maybe that’s up to them. reply Imnimo 10 hours agorootparentIt's not the person who does the fine-tuning I'm worried about, it's the person who releases the base model who the law also makes liable. The point is that, because fine-tuning can trivially induce behavior that satisfies the standard for \"hazardous capability\" in any model, the law effectively makes it illegal to release any covered model. reply jph00 11 hours agorootparentprevYou're missing the point. Liability here would also fall on the open source developer who created a general purpose model, which someone else then went on to fine-tune and prompt to do something harmful. reply elicksaur 10 hours agoprevI’ll happily support regulation of the space when the bill writers of these proposals stop using definitions of “artificial intelligence” that could reasonably be construed by a lawyer to cover literally any computer program. > (b) “Artificial intelligence model” means an engineered or machine-based system that, for explicit or implicit objectives, infers, from the input it receives, how to generate outputs that can influence physical or virtual environments and that may operate with varying levels of autonomy. reply protocolture 11 hours agoprevQuestion. What happens if I write a piece of software that is harmful that doesnt have the AI label. It seems dumb to have a separate classification for harms caused by trained AI models. The training aspect doesnt seem to limit liability at all. A judge might rule differently, but thats why the justice system is built such as it is, to make intelligent decisions based on the specific facts of a case. I am betting that software that causes some significant harm is already outlawed. So this whole thing is just a waste of time. reply carbocation 8 hours agoparentThe substance of my comment to Sen Wiener hinges on a similar point to what you raise here. It's incredibly difficult to imagine doing a good job of regulating model training, especially in a few years when the available flops are high enough that this limit is being hit often. It's much more straightforward to regulate actions: constructing WMD is illegal, synthesizing drugs is illegal, etc. If the state wants to tighten up its laws about various activities, go for it. That's the right place to act. Injecting itself into the model training process seems very unlikely to yield any substantive benefits and very likely to hinder progress. reply andy99 12 hours agoprevI agree with this (the call to action, not the act) and will try and respond and share it, but it's a lobby group right (\"Alliance for the Future\"), I'd like to know who is funding it and a bit more about it. reply stefan_ 12 hours agoparentIt seems to be run by.. a twitter troll? https://twitter.com/psychosort reply jph00 11 hours agorootparentWhat's with the ad-hominem? I can't see where you're getting that from at all. The folks involved in this lobby group are listed here: https://www.affuture.org/about/ reply andy99 10 hours agorootparentI'd be much more interested in where the money is from than who the people are. Not trying to attack anyone and at some level I don't care because I agree with the cause but I can still imagine benefactors that would make the whole thing look bad. reply artninja1988 11 hours agorootparentprevWhy do you say he's a troll? reply phkahler 9 hours agoprevIt would be really helpful if folks like Sam Altman and Elon would STFU about dangers and claims of AGI or better in the next months. If you're actually worried about AI we need to ban any generative AI that can replicate a specific person's voice or appearance. Beyond that I don't see any immediate danger. reply coffeebeqn 9 hours agoparentA cynical take is that they are just generating hype and investment in their companies. We’re getting close to AGI so better throw a few billion our way before you miss out - is a nice pitch. How long has Musk been promising full self driving in the next X months (while making people pay $10k for it)? Anyone taking his word as anything close to reality is a fool reply jkuli 7 hours agoprevI'm unable to register. This is GME stonks all over again. It takes less than 1 second to process an account. There are 18,000 seconds in five hours. There must be a lot of comments that they don't agree with. Maybe they shut it down to protect humanity from extinction? reply throwing_away 12 hours agoprevSlow down there, California. Florida is growing too fast as it is. reply carbocation 9 hours agoprevI think this advice is incomplete. For those of us who live in California, shouldn't we be contacting our representatives? reply nonplus 11 hours agoprevI guess I think we should hold models used for non-academic reasons to a higher standard, and there should be oversight. I don't know if all the language in this bill does what we need, but I'm against letting large corporations like a META or X live test whatever they want on their end users. Calling out derivative models are exempt sounds good; only new training sets have to be subjected to this. I think there should be an academic limited duty exemption, models that can't be commercialized likely don't need the rigor of this law. I guess I don't agree with affuture.org and think we need legislation like this in place. reply jph00 11 hours agoparentIt sounds like you do agree with affuture.org though. The proposed draft does not hold models used for non-academic reasons to a higher standard, and \"models that can't be commercialized\" are covered by it. It will be far harder to academics to work on large models under this draft. reply synapsomorphy 11 hours agoprevI don't think this bill would be that effective, but I do feel that if we as a species don't do something drastic soon, we won't be around for a whole lot longer. And I'm not sure if it's even possible to do something drastic enough at this point - regulating datacenters would just make companies move to other countries, just like this would probably just make companies move out of CA. reply squigz 11 hours agoparent> we won't be around for a whole lot longer. Why do you think that? > regulating datacenters would just make companies move to other countries To say nothing of the potential issues regarding free society going down this route will yield - and has arguably already yielded. reply synapsomorphy 5 hours agorootparentTo me, AI is clearly on a trajectory to be more intelligent than humans in a few decades or less, even if it isn't that smart now - the funding and talent going into it these days is just crazy and there is no fundamental difference between the logic of our brains and transistors. I personally don't believe humans can coexist with a superintelligence. I agree with your point and most other points about the negatives of regulating compute but like, if the other side of the scale is species-level genocide, does any of it matter? reply squigz 5 hours agorootparent> if the other side of the scale is species-level genocide, does any of it matter? Possibly not, but I have to disagree with your assessment of our future. It doesn't seem as clear to me that we couldn't co-exist with an AI superintelligence (putting aside any arguments about what that even means for now) reply xbar 11 hours agoparentprevThis bad law is not a substitute for your possibly-good law. Bad lawmakers commit this fallacy all the time. Write your good law idea down and send it to a lawmaker who will act. reply cscurmudgeon 11 hours agoprev> A developer of a covered model that provides commercial access to that covered model shall provide a transparent, uniform, publicly available price schedule for the purchase of access to that covered model Interesting, we don't have transparent, uniform, publicly available price schedule for healthcare and other basic needs (electricity, e.g. see PGE). Something is fishy here. reply johnea 12 hours agoprevnext [2 more] [flagged] choilive 12 hours agoparentI can't tell if this is satire. If so, good one. If not - you should think about what at least what the second order effects would be here reply s1k3s 12 hours agoprev [–] The article suggests that this act will effectively destroy any open source AI initiative in California. After reading the act, this seems to be the correct assumption. But, is Open Source AI even a thing at this point? By the way, this is how the EU does things and that's why we're always behind on anything tech :) reply AnthonyMouse 11 hours agoparent> is Open Source AI even a thing at this point? What do you mean? You can download llama.cpp or Stable Diffusion and run it on your ordinary PC right now. People make variants using LoRA adapters and things with relatively modest resources. Even creating small specialized models from scratch is not impossibly expensive and they often outperform larger generalized models in the domain they're specialized for. Creating a large model like llama or grok takes a lot of resources, but then it's entities with a lot of resources that create them. Both of those models have open weights. reply s1k3s 11 hours agorootparent> (j) (1) “Developer” means a person that creates, owns, or otherwise has responsibility for an artificial intelligence model. For as long as you don't distribute the model and you only use it for yourself, you don't fall under this definition (if I understand correctly). reply AnthonyMouse 11 hours agorootparentOpen source implies that you are distributing it. reply s1k3s 11 hours agorootparentYour comment implies you're not distributing it, you're using something that was distributed to you [in this case by META]. If you would distribute a mod of the model you would fall under the restrictions of this bill. Which is why I asked the original question: are people even doing this? reply AnthonyMouse 11 hours agorootparentBut then Meta is distributing it. And if you modify it in a way that others may find useful, you might also like to distribute your modifications. reply OKRainbowKid 12 hours agoparentprev [–] This is also why we have actually meaningful consumer protections in place. reply s1k3s 11 hours agorootparent [–] Can you give me an example of effective consumer protection? reply xbar 11 hours agorootparentThe food safety laws of 1906 and the safe cosmetics laws of 1935. Seriously, the death and destruction caused by lead, morphine and mercury in everyday things was not a joke. reply noodlesUK 11 hours agorootparentprev [–] Yes, consumer rights in Europe are great for buying various goods and services! For example, the EU regs for flight delays are a great example of consumer protection that is actually beneficial. You get paid cash compensation (which often exceeds the face value of the ticket) if you’re delayed more than a certain amount. reply karaterobot 11 hours agorootparent [–] FYI, you are entitled to a refund in the U.S. if your flight is delayed significantly. As of last week, it's 3 hours for domestic flights, or 6 hours for international. The refund is automatic. It's been true for a long time that people could get refunds for significant delays, but the definition of how long \"significant\" means was not defined. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "SB 1047, a bill in California, poses risks to open-source AI and tech industry by establishing a Frontier Model Division staffed by Effective Altruism activists with policing powers, potentially leading to the imprisonment of model developers.",
      "The bill is swiftly progressing through the state Senate and has global repercussions due to California's high concentration of cloud and AI companies, requiring prompt action such as submitting opposition letters, contributing to an analysis document, and raising awareness on social media.",
      "Urgent collective efforts are crucial to halt the bill's passage, emphasizing the need for immediate engagement and involving others in the advocacy against SB 1047."
    ],
    "commentSummary": [
      "The conversation centers on the risks and regulations related to artificial intelligence models, focusing on California's SB 1047 bill.",
      "Concerns encompass the misuse of AI, lack of accountability in algorithmic decision-making, privacy challenges, and surveillance issues.",
      "Debates include the bill's necessity, its potential impact, liability, regulating AI models, implications for developers and open-source projects, and the effects on innovation, economic growth, consumer protections, and AI advancement beyond human intelligence."
    ],
    "points": 135,
    "commentCount": 95,
    "retryCount": 0,
    "time": 1714340042
  },
  {
    "id": 40191098,
    "title": "Reckless Policies Fueling Rise of Large Vehicles in America",
    "originLink": "https://www.vox.com/future-perfect/24139147/suvs-trucks-popularity-federal-policy-pollution",
    "originBody": "Share this story Share this on Facebook Share this on Twitter Share this on Reddit Share All sharing options Share All sharing options for: The reckless policies that helped fill our streets with ridiculously large cars Reddit Pocket Flipboard Email David Zipper is a senior fellow at the MIT Mobility Initiative, where he examines the interplay between transportation policy and technology. His work has been published in the Atlantic, Slate, Bloomberg, the Washington Post, and elsewhere. This story is part of a group of stories called Finding the best ways to do good. Cars, you might have noticed, have grown enormous. Low-slung station wagons are all but extinct on American roads, and even sedans have become an endangered species. (Ford, producer of the iconic Model T a century ago, no longer sells any sedans in its home market.) Bulky SUVs and pickup trucks — which have themselves steadily added pounds and inches — now comprise more than four out of every five new cars sold in the US, up from just over half in 2013, even as national household size steadily declines. Inside this story Why bigger cars are bad for the environment — and people How tax loopholes and tariffs encourage larger car models How big cars are deadly to pedestrians Solutions to “car bloat” The expanding size of automobiles — a phenomenon I call car bloat — has deepened a slew of national problems. Take road safety: Unlike peer nations, the US has endured a steep rise in traffic deaths, with fatalities among pedestrians and cyclists, who are at elevated risk in a crash with a huge car, recently hitting 40-year highs. Vehicle occupants face danger as well. A 2019 study concluded that compared to a smaller vehicle, an SUV or a pickup colliding with a smaller car was 28 percent and 159 percent, respectively, more likely to kill that car’s driver. Car bloat also threatens the planet. Because heavier vehicles require more energy to move, they tend to gulp rather than sip the gasoline or electricity that powers them, increasing greenhouse gas emissions. Extra weight also accelerates the erosion of roadways and tires, straining highway maintenance budgets and releasing microplastics that damage ecosystems. SUVs and pickup trucks make up more than 80 percent of new car sales in the US. Their height and weight make them significantly more likely to injure pedestrians, cyclists, and other road users, and they also make it harder to see pedestrians crossing the street. Here, a pickup truck crashed into and seriously injured a pedestrian before smashing into a storefront in Los Angeles in 2014. Al Seib/Los Angeles Times via Getty Images What lies behind this shift? Some Americans prefer bigger cars, especially when gas prices are low, for their ample storage space, ability to see over other vehicles on the road, and perceived safety benefits (more on that later). But shifting consumer demands tell only part of the story. For half a century, a litany of federal policies has favored large SUVs and trucks, pushing automakers and American buyers toward larger models. Instead of counteracting car bloat through regulation, policymakers have subtly encouraged it. That has been a boon for car companies, but a disaster for everyone else. Here are some of the most egregious examples. Why we let bigger cars pollute more After the 1970s OPEC oil embargo triggered a spike in gas prices, the federal government adopted an array of policies intended to reduce energy demand. One of Congress’s most consequential moves was creating the Corporate Average Fuel Economy (CAFE) standards, which require that the average fuel economy (miles per gallon, or MPG) of a carmaker’s vehicles remain below a set threshold. Pressed by auto lobbyists, Congress made a fateful decision when it established CAFE. Instead of setting a single fuel economy standard that applies to all cars, CAFE has two of them: one for passenger cars, such as sedans and station wagons, and a separate, more lenient standard for “light trucks,” including pickups and SUVs. In 1982, for instance, the CAFE standard for passenger cars was 24 mpg and only 17.5 mpg for light trucks. That dual structure didn’t initially seem like a big deal, because in the 1970s SUVs and trucks together accounted for less than a quarter of new cars sold. But as gas prices fell in the 1980s, the “light truck loophole” encouraged automakers to shift away from sedans and churn out more pickups and SUVs (which were also more profitable). Car ads of the 1980s and 1990s frequently featured owners of SUVs and trucks taking family trips or going out with friends, activities that could also be done in a sedan or station wagon. The messaging seemed to resonate: By 2002, light trucks comprised more than half of new car sales. In the early 2000s, the federal government made these distortions even worse. During the George W. Bush administration, CAFE was revised to further loosen rules for the biggest cars by tying a car model’s efficiency standard to its physical footprint (which is basically the shadow cast by the vehicle when the sun is directly above it). President Obama then incorporated similar footprint rules into new greenhouse gas emissions standards that are overseen by the Environmental Protection Agency (EPA). Dan Becker, who led the Sierra Club’s global warming program from 1989 to 2007, told me that he and others warned federal lawmakers that adopting footprint-based standards was a mistake. “People like me were saying, ‘give carmakers another loophole and they’ll use it,’” he said. “But we lost.” Those concerns proved justified. The average vehicle footprint expanded 6 percent between 2008 and 2023, a “historic high,” according to an EPA report, which also found that some carmakers, such as General Motors, actually had lower average fuel economy and higher average carbon emissions in 2022 than in 2017. To its credit, the EPA recently announced revisions to its vehicle GHG rules that would narrow (but not close) the gaps between standards for large and small cars. But the shift toward electric vehicles may further entrench car bloat. The EPA’s rules assume that all EVs, regardless of their design, generate no emissions — a questionable assumption, because EVs create emissions indirectly through the production and transmission of power that flows into their batteries. A huge or inefficient battery requires more electricity, which can lead to significant pollution (especially in regions where fossil fuels dominate the energy mix). The EPA’s policy of treating all EVs equally makes a monstrously wasteful vehicle like the Hummer EV seem cleaner than it is, encouraging carmakers to manufacture more of them. To counteract EV bloat, Peter Huether, a senior research associate at the American Council for an Energy-Efficient Economy, would like to see the EPA revise its GHG rules to consider emissions from power generation and transmission: “If these standards look at upstream emissions, it could have a downstream effect on shape and size of EVs.” Blocking smaller cars from abroad What does a 60-year-old trade dispute have to do with car bloat? More than you might imagine. In the early 1960s, Europe raised the ire of American officials by slapping a 50 percent tariff on chicken exported from the United States. In retaliation, the US enacted a 25 percent tax on pickup trucks imported from abroad. The dispute is long forgotten, but the “Chicken Tax” lives on. Although the tariff was initially aimed at Germany’s immense auto industry (Volkswagen in particular), it also applies to pickups imported from newer automaking powers such as Japan and South Korea, where carmakers are often adept at building vehicles much smaller than those available to Americans. Toyota’s Hilux Double Cab pickup, for instance, weighs several hundred pounds less than a 2024 Ford F-150 Tremor or Lariat and is about half a foot shorter. But Americans who might want it are out of luck. Toyota does not sell the Hilux in the US (but does in countries like India and Britain); the 25 percent tariff would make it prohibitively expensive. “The Chicken Tax has prevented competitive Asian or European truck makers from entering the US market,” said Jason Torchinsky, a co-founder of the Autopian, a media outlet focused on the auto industry. “American manufacturers have really never had to compete.” John Krafcik, who previously led Hyundai, has called the Chicken Tax “one of the most important determinants of how the [auto] industry looks today and how it operates today in the US.” The tariff has been condemned by everyone from the Libertarian Cato Institute, the center-right American Enterprise Institute, and the left-leaning Tax Policy Center. “Tariffs in general hurt consumers, and the Chicken Tax is no exception,” wrote Robert McClelland of the Tax Policy Center. There are other protectionist rules blocking smaller vehicles from abroad: Carmakers from China, an emerging automaking behemoth, face a 25 percent tariff enacted by Donald Trump. As a result, Americans cannot buy small Chinese EV sedans like the BYD Seagull that cost around $10,000, barely a fifth the price of an average American car. The Seagull, a small, low-cost electric sedan from Chinese automaker BYD VCG/VCG via Getty Images Refrigerators are transported on a Japanese mini truck, also known as a kei truck. These often have bed lengths comparable to American-style pickup trucks but are much shorter in height, lighter, and safer for other road users — yet they’re exceedingly hard to obtain in the US. Nicolas Datiche/AFP via Getty Images And those hoping to import a kei truck, a miniature pickup common in Japan, must navigate a labyrinth of federal and state rules. (Even Afghanistan seems ahead of the US in minitruck offerings, as the Wichita Eagle’s Dion Lefler noted in a tongue-in-cheek 2023 column: “In the land of the free, why can’t we have mini-pickup trucks like the Taliban?”) These policies have established a regulatory moat protecting US automakers whose profits disproportionately come from pricey, hulking SUVs and trucks. The Hummer Tax Loophole In 1984, Congress stopped allowing small business owners to take a tax deduction for the purchase price of cars used for work. But the bill included a giant loophole: To protect those who need a heavy-duty vehicle (think farmers or construction workers), Congress made an exception, known as Section 179, for cars that weigh over 6,000 pounds when fully loaded with passengers and cargo. Today such behemoths are eligible for a tax deduction of up to $30,500, while business owners who opt for a smaller car can claim nothing at all. Few car models were heavy enough to qualify for the tax break 40 years ago, but that is no longer the case: A Hummer 1, for instance, weighs about 10,300 pounds (leading Section 179 to be dubbed the “Hummer Tax Loophole”). Other huge cars, such as a Chevrolet Suburban or an F-250 Ford Super Duty truck can qualify, too. “Few folks at EPA know about Section 179,” said Becker, the former Sierra Club executive. “But every auto dealer does.” Some car dealerships even offer handy Section 179 guides on their websites. The tax advantage of buying a behemoth may be powerful enough to tilt the vehicle purchase decisions of individuals like real estate agents, who use their vehicles for both professional and personal use. And as cars electrify, the added tonnage from batteries will allow more models to qualify for favorable tax treatment. If Section 179 sounds crazy, consider another federal loophole that has endured for decades. In 1978, Congress established the “Gas Guzzler Tax,” requiring automakers to pay between $1,000 and $7,700 for every car produced that gets less than 22.5 miles per gallon. But the tax only applies to passenger vehicles like sedans and station wagons. SUVs and pickups, which often have much worse gas mileage, are exempt. That omission makes no sense from a policy perspective, but it is good news for carmakers producing inefficient behemoths. Freezing the gas tax Every time a car owner fills her gas tank, a portion of the bill goes into the federal Highway Trust Fund, a central source of funding for roads and mass transit. That tax rate is set at $0.184 per gallon, a level that has been frozen since 1993, when Bill Clinton was less than a year into his presidency. Congressional proposals to increase the gas tax to close a yawning highway budget gap, or at least tie it to inflation, have gone nowhere. Over the last 31 years, consumer prices have risen 113 percent, making the real value of the gas tax less than half what it was in 1993. That decline has reduced the cost of powering a huge SUV or truck with abysmal gas mileage, like the 6,270-lb 2024 Cadillac Escalade that gets around 16 mpg. A 2018 OECD study found that the US had the lowest average gas tax (including both federal and state taxes) among rich nations, which averaged $2.24 per gallon — four times the typical US rate. “Why are European cars so small?” said McClelland, of the Tax Policy Center. “One reason has got to be the much higher gasoline tax.” Federal policy ignores crash risk for anyone outside a car A vehicle’s design affects not just the safety of its occupants, but also people walking, biking, or inside other cars. Although seemingly obvious, this basic truth has eluded federal regulators for decades. Car safety rules are laid out in the encyclopedic Federal Motor Vehicle Safety Standards (FMVSS), which touches on everything from power windows to seat belts. But the FMVSS revolves around protecting a vehicle’s occupants; nothing within its 562 pages limits a car’s physical design to protect someone who might come into contact with it in a collision. That omission invites an arms race of vehicle size — precisely what the US is experiencing. Nor does the National Highway Traffic Safety Administration (NHTSA) consider pedestrians, cyclists, or other car occupants when calculating its safety ratings from crash tests. Unlike safety ratings in Europe and elsewhere, the American crash ratings program also ignores the danger that vehicle designs pose to those walking and biking. NHTSA’s myopic focus on car occupants is a boon for the heaviest and tallest cars, which pose disproportionate risk to those outside of them. Weightier vehicles exert more force in a crash, and they require additional time to come to a halt when a driver slams on the brakes. A 2023 study by the Insurance Institute for Highway Safety (IIHS) found that vehicles with tall, flat front ends (common on big pickups and SUVs) are significantly more likely to kill pedestrians in crashes. An earlier IIHS study found that large cars also make it harder to see pedestrians at intersections. The US is in the midst of a car fatality crisis, exacerbated by the risks large cars pose to pedestrians. Here, a pickup truck driver in Santa Ana, California, quickly applies brakes as two pedestrians cross in front. One is not visible. Mindy Schauer/Digital First Media/Orange County Register via Getty Images With pedestrian and cyclist deaths now soaring, NHTSA last year took its first, tentative step toward protecting so-called vulnerable road users by proposing that its vehicle safety ratings be revised to include an evaluation of automatic pedestrian braking technology, which can force a vehicle to halt before striking someone on foot. But even if adopted, it would not affect NCAP’s 5-star safety rating, the hallmark of the program. And NHTSA’s focus on automatic pedestrian braking, an imperfect tech fix, ignores car bloat, a root cause of America’s traffic safety crisis. Earlier this year, a paper co-authored by former NHTSA executive Missy Cummings gave an ominous assessment of automatic braking systems, concluding that they did not work consistently. By contrast, the potential safety benefits of constraining vehicles’ weight and height have been well established. Why can’t we fix things? All of these policies have distorted the US car market, leading the 278 million vehicles plying American roads to become ever bigger, more dangerous, and more destructive. So why have they remained on the books after the growing societal costs of car bloat became impossible to miss? To find an answer, consider who benefits from oversized vehicles. American carmakers like Ford and GM (which are headquartered in Michigan, a crucial swing state) rely on juicy margins from big SUVs and pickups, which are more expensive and profitable than smaller models. They enjoy protection from foreign competition through tariffs like the Chicken Tax, as well as favorable policies like CAFE’s light-truck loophole. The regulatory status quo suits domestic automakers just fine — and they act as a roadblock to even modest attempts to change it. In 2022, for example, the largest auto industry association lobbied District of Columbia council members against a proposal to charge owners of the most egregiously oversized cars $500 per year, seven times more than a light sedan (the District adopted the policy anyway). SUVs and trucks now overwhelmingly dominate the offerings of US carmakers. Here, a Cadillac SUV is on display at the 2019 North American International Auto Show in Detroit. Daniel Acker/Bloomberg via Getty Images As American sales of big SUVs and trucks have surged, their owners are likely to resist policy moves they see as penalizing them. Many are likely to be unaware of the federal loopholes and policy oversights that have distorted their vehicle choices. The negative externalities of supersized cars — in emissions, crash deaths, and the erosion of tires and pavement — are what economists call a market failure, since their costs are borne by society writ large, not the people who buy big pickups and SUVs. Left unaddressed, those societal costs will grow as more people replace their modest-sized cars with big SUVs or trucks. After all, everyone else seems to be doing it — why not do the same, if only for self-preservation? Regulation can end such a cycle toward enormity. Countries including France and Norway have enacted weight-based taxes to counteract car bloat’s collective costs and avoid giving huge vehicles implicit subsidies. But American policymakers have done the exact opposite, and they rarely even acknowledge the problem. Asked explicitly about ways that the Department of Transportation could address car bloat, Secretary Pete Buttigieg ducked, calling merely for “further research.” With the feds refusing to lead, it has fallen on state and local leaders to try and address car bloat themselves. Colorado and California, for instance, have proposed weight-based vehicle registration fees, following the District of Columbia’s lead. But such moves are an imperfect solution to a national problem (vehicles can, after all, be driven across state lines). A true policy fix will require action from Congress, NHTSA, and the EPA. It need not begin with new regulations or taxes. Federal leaders could do a world of good if they simply unwind the ill-advised policies already on the books. Kendra Levine contributed research assistance. Will you support Vox today? We believe that everyone deserves to understand the world that they live in. That kind of knowledge helps create better citizens, neighbors, friends, parents, and stewards of this planet. Producing deeply researched, explanatory journalism takes resources. You can support this mission by making a financial gift to Vox today. Will you join us? One-Time Monthly Annual $5/month $10/month $25/month $50/month Other $ Yes, I'll give $5/month Yes, I'll give $5/month We accept credit card, Apple Pay, and Google Pay. You can also contribute via",
    "commentLink": "https://news.ycombinator.com/item?id=40191098",
    "commentBody": "The reckless policies that helped fill our streets with large cars (vox.com)134 points by NaOH 14 hours agohidepastfavorite112 comments nisa 13 hours agoNot only in the USA. Germany also has a growing number of big SUVs. What's interesting is that it was possible 25 years ago to build a 3 litre/100km car. The Audi A2. https://en.m.wikipedia.org/wiki/Audi_A2 Now a modern SUV has at least double the weight and a worse wind resistance value and uses 6l/100km of fuel. And while there are limits what physics will allow it seems it was possible to reduce fuel consumption and emissions by at least 50% 25 years ago. With modern computer simulations and technical progress over the last 25 years something like this should be at least possible? Yet it's difficult to buy a car like this today. Somewhere something went really wrong. reply speedgoose 13 hours agoparentYou could buy the hybrid fossil i3 for many years, which was kinda the successor of the Audi a2, but better. Or also the electric i3 which was arguably better than the hybrid version if you have a charging network where you live. I think it’s trivial to buy a better car than the Audi A2 nowadays. But not with an ICE. reply nisa 13 hours agorootparentSure but that's not the point. Reduced fuel consumption using hybrid technology works in the city. EVs are even better. It's just fascinating that the technology was there and wasn't improved upon in 25 years. We could drive non-hybrid ICE cars were a big chunk of cars is using 50% less fuel now if we would have avoided that SUV trend. reply pkolaczk 9 minutes agorootparent> Reduced fuel consumption using hybrid technology works in the city. Works on a highway as well, but the difference is smaller. reply Grimeton 12 hours agorootparentprevVW - GOLF I - Average weight: 800kg, 1.8l, 112bhp, average fuel consumption: 10 litres/100km VW - GOLF 8 - Average weight: 1450kg, 1.0l, 110bhp, average fuel consumption: 4.6l/100km So the car is nearly twice as heavy and uses less than half as much gas. And there were no optimizations? reply nisa 12 hours agorootparentMy point is that it could use 2l/100km if that would be the optimization target. Golf 8 is nowadays a few classes above what the Golf 1 was. It's also a rather big car compared to the Golf 1. reply Grimeton 12 hours agorootparent>My point is that it could use 2l/100km if that would be the optimization target. No it cannot. There are more things involved than just air drag when it comes to fuel consumption in a car. >Golf 8 is nowadays a few classes above what the Golf 1 was. It's also a rather big car compared to the Golf 1. Exactly. It's way heavier and uses less than half of what the Golf I used back in the day. This to me demonstrates that you don't understand the basics behind all this. Maybe you should read up on stuff. reply speedgoose 13 hours agorootparentprevMy point is that the technology has improved. We ditched the ICE for electric motors on the way. Those very lightweight fancy cars didn’t sell very well (~150k A2, ~250k i3) but that’s another problem. reply namdnay 13 hours agoparentprevThere was also dieselgate :) reply nisa 13 hours agorootparentDieselgate was about NOx emissions and avoiding using so much AdBlue. I'm not sure that a bigger tank for that would have a big impact. As it's happening in the exhaust system there shouldn't be an impact on fuel consumption. reply mytailorisrich 13 hours agoparentprevCarmakers build what sells. So what went \"wrong\" is that people prefer powerful cars with plenty of space, comfort, and bells and whistles rather than \"3l/100km\"... reply voisin 12 hours agorootparent> Carmakers build what sells. This is only true when carmakers are sufficiently experimenting and letting customers decide what sells, but when they all make extremely similar products in each category, it isn’t true. For examples look at the truck category - in North America at least, there is effectively no way to get a truck that is not loaded with electronics and silly features as the makers stopped competing on anything else. There is no “light weight, utilitarian build” truck that can be bought. And magically they all have super high hoods and massive interiors. The same is true in every category I have looked at. reply rcpt 12 hours agorootparentVans reply voisin 12 hours agorootparentMinivans? Work vans? Where’s the experimentation? They all seem to compete on the same narrow list of metrics leading to homogenization of the product category. So it isn’t “we build what sells” but rather “customers need to buy and we define what’s available quite narrowly” reply piva00 10 hours agorootparentWork vans in Europe have quite some variety to choose from, usually the segment is divided in 3 sizes: small ones based on compact cars chassis, a mid-size and larger ones with the chassis adaptable from a box van to a mini-truck, even with the possibility to exchange part of the rear for a bed truck. Some brands have an extended version for the mid/larger sizes, either to accommodate more passengers, have larger boxes or to fit a box truck enclosure on the back. reply voisin 8 hours agorootparent> in Europe That’s the difference. I am writing from a North American perspective and I am often jealous of the seemingly endless variety of options available elsewhere in the world. Just know that we North Americans aren’t necessarily choosing to buy the giant trucks - in many cases we would love smaller options but they don’t exist. reply mytailorisrich 5 hours agorootparentprevBecause that's what the market prefers. Sorry but people are not \"innocent victims\" and carmakers do not increase their own costs for the sake of it. They kept adding electronics and \"silly\" features because on average consumers went for it when given the choice. reply korhojoa 12 hours agorootparentprevConveniently forgetting that the SUV trend started due to a shitty workaround to fuel economy requirements. There are so many unnecessarily large cars around nowadays. reply aporetics 12 hours agorootparentprevAs though marketing never influences what people want. reply lionkor 13 hours agorootparentprevThis is the drug dealer argument; if it wasn't me, someone else would reply mbostleman 12 hours agorootparentI thought it was an observation of a market that is primarily free. reply roughly 8 hours agorootparentThe entire article this comment thread is attached to is a liturgy of all of the policies affecting this market, and all the distortions created by the various actors. reply mytailorisrich 4 hours agorootparentprevCorrect. While there are many rules and regulations that affect the auto markets, ultimately cars, including Europe, have grown in size because of market forces (I.e. what consumers prefer) reply Freedom2 8 hours agorootparentprevDespite tariffs and regulations that restrict how \"free\" the market is compared to other vehicle markets overseas? reply mytailorisrich 13 hours agorootparentprevWhatever you want to call it that's the reality. reply OtherShrezzing 12 hours agorootparentprevThat's not the case in Europe though. Lots of people in Europe have exactly two preferences when purchasing a vehicle: low price, and good fuel economy. All of Europe's best selling cars in 2022[1] fit either or both of those criteria. Europe has a strongly revealed culturally-ingrained preference towards economical cars. [1] https://www.statista.com/statistics/1127929/best-selling-car... reply blibble 12 hours agorootparentit is the case, the average european car is becoming a ridiculous SUV/pickup too the rate of increase in mass of european cars is now greater than that of the US reply mytailorisrich 5 hours agorootparentprevCars in Europe are becoming, and have already become, massive as well. Preference towards economical cars does not mean any of my previous comment isn't true. At some point cars are economical enough and other features become a better selling point in the market instead of absolute best possible fuel economy. reply mitthrowaway2 12 hours agorootparentprevAre you saying that the reasons in the article are incorrect, and what happened was a change in customer preferences? reply ethbr1 12 hours agorootparentprev> Carmakers build what sells. Eh, they build what makes them the most profit. That's a much more complicated can of worms than what sells. It includes tax policy, rebates, financing rates, mileage standards, engine technology costs, etc. reply mbostleman 12 hours agorootparentIf enough people wanted the same cars as the Vox author wants, they’d be made. reply Sabinus 10 hours agorootparentNot if you set up the market such that other cars are significantly cheaper. The market is only free to respond to demand when an intersection of demand and regulation makes it economical. reply eddd-ddde 10 hours agorootparentprevDrugs sell really good yet we have laws against that. Maybe some regulation on what a car company can sell would be good. reply Zpalmtree 12 hours agorootparentprevExactly. This article is all over the place. It's the tariffs making americans buy SUVs, european cars are too expensive! Then they complain that a tax increase on SUV owners in DC was opposed? They clearly don't want Americans to have cheaper cars - just force people to use the cars they think are 'correct'. Gotta save the environment, who cares about the consumer? reply cebert 13 hours agoprevThere are several great examples of factors that contribute to vehicle size bloat in this article. One factor I don’t see mentioned often is the increased rates of obesity in the US. We’re nearing 40% of the population being obese [1]. Perhaps there’s several Americans who are so large they need larger vehicles in order to comfortably fit inside? [1] https://www.cdc.gov/obesity/data/prevalence-maps.html reply crazygringo 12 hours agoparentLarger cars are predominantly taller and longer, not wider. Except in extreme cases, obesity is not playing a major factor here. It's more a question of being tall. I'd love to be able to drive sporty cars, but I generally just don't physically fit. I fit fine in an SUV. A regular sedan I can fit, but because of the length of my legs, I have to slide the seat back if I'm in the front, or ask the person in front to slide their seat forwards if I'm in the back. Obviously it doesn't work if someone else as tall as me is in the seat behind/ahead. Also sedans are terrible for my posture because I have to slouch so that my head isn't hitting the roof. My lower back and neck hurt so bad after a long drive because of the forced slouching. Fortunately I use public transportation for 99% of my needs and none of this is an issue usually. reply mitthrowaway2 12 hours agorootparentThe 1966 Ford Thunderbird was not a narrow car, at 1963 mm (210 mm wider than my Civic). But it's still narrow compared with the 2123 mm wide F-150 Raptor parked on my street, which sticks out so far into the road that makes it almost impossible for Amazon delivery trucks to navigate. Of course it does that while also being extremely tall, making it impossible for many pedestrians to look past its hood to spot oncoming traffic on the road, or for passing drivers to spot people stepping out behind it. https://www.carsized.com/en/cars/compare/ford-thunderbird-19... reply 0_____0 13 hours agoparentprevMore true for people above a certain height. There are many countries with a higher obesity rate than the U.S. but none who sell 80% trucks and SUVs. Edit: if the above hypothesis is true, we would expect average car size to correlate with obesity after controlling for factors like proximity to a dense urban area. Could be true. If you gots data let's go reply simonjgreen 13 hours agorootparentWith US ranking 10th, there really aren’t many https://data.worldobesity.org/rankings/ reply happypumpkin 12 hours agorootparentAnd all of the ones above the US are seemingly small island nations, with #1 being a US territory :| reply ronnier 13 hours agorootparentprevVery few and those that do, can they afford larger cars like those in the USA can? reply redwall_hp 13 hours agorootparentThe answer is a resounding no. The rankings speak for themselves. https://data.worldobesity.org/rankings/ reply rfrec0n 13 hours agorootparentprevCars of all sizes are usually much cheaper outside the US, so I don't think it's a price thing. Maybe just because they don't spend as much time in the car or something? reply Apreche 12 hours agoparentprevYou've got it backwards. The cars caused the obesity by creating suburbs where people drive everywhere. reply aimor 10 hours agoparentprevI think it's entirely possible that car interiors are getting more cramped. High center consoles between the front seats stop me from spreading my legs, deep glovebox stops me from stretching them, even the headrest juts forward now making it that much more difficult to recline. There about a hundred things like this getting in my way, add in a carseat or two and I'm not even comfortable driving. reply hollywood_court 13 hours agoparentprevI’m not obese but I’m a big guy at 6’3” ~290lbs. I daily drive a Land Cruiser and I don’t think I’ll ever drive anything much different. I also have a full size 2500 pick up and even that is a little small for me. Driving my wife’s cars has become a burden. Even the Land Cruiser is a bit tricky to get in and out of without hitting my head on the door. Edit: I’m not fat guys. I power lift 3 to 5 times per week and wear a size 36 in blue jeans. I’m no Tom Haviland (I can only dream) but I’m not some porky bastard either. Abs are still slightly visible although I don’t lift for aesthetics. I’m simply trying to hit 1750lbs combined on the big 3 before I turn 45. reply sniggers 12 hours agorootparentYou are medically obese, with a BMI of 36.2: https://www.nhlbi.nih.gov/health/educational/lose_wt/BMI/bmi... reply hollywood_court 12 hours agorootparentPerhaps according to that. But my body fat percentage is only 26 according to the latest DEXA scan I took in February. reply sniggers 12 hours agorootparentFair enough, BMI can be misleading for powerlifters! reply kccqzy 12 hours agorootparentprevYou are obese, but you don't want to admit it. Or more likely perhaps you are used to seeing so many other people being far more visibly obese than you that you honestly don't see yourself as obese. Regardless, it's a big problem in America. reply hollywood_court 12 hours agorootparentOr perhaps I lift weights? reply kccqzy 10 hours agorootparentThen congrats you are among the tiny minority for whom BMI is not an accurate measure of obesity. You are simply unlike 99% of the people for whom BMI obesity means they are actually obese. That however doesn't detract from the argument in this thread that Americans are being more obese and that affects their vehicle choice. reply hollywood_court 10 hours agorootparentBMI is actually rather inaccurate when measuring obesity in athletic individuals such as myself. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3445161/#:~:tex.... reply readthenotes1 13 hours agoparentprevObserved this, including both size of seat and difficulty in getting in and out of normal-height cars reply ZenThereWere0 13 hours agoprevInvert this point (\"SUV or a pickup colliding with a smaller car was 28 percent and 159 percent, respectively, more likely to kill that car’s driver.\") to get that one is less likely to die in a big vehicle. It is in your interest to buy and drive the biggest car you can afford, due to the presence of other large cars on the road. See [1] 12 black swans to avoid, YouTube video from ER physician Dr. McGuff. First thirty seconds. Force equals mass x acceleration. [1] https://youtu.be/xoPCIUwc4zY?si=wUvR7bhkBthiTsU_ reply AnthonyMouse 12 hours agoparent> Invert this point (\"SUV or a pickup colliding with a smaller car was 28 percent and 159 percent, respectively, more likely to kill that car’s driver.\") to get that one is less likely to die in a big vehicle. That's not how inversions work. If you're in a small car and get into a collision with a large truck, you're likely to die. But if you're in a truck and get into a collision with another large truck, you're still likely to die, because you still got hit by a truck. Meanwhile truck collisions are more likely given the same drivers because they have more surface area which provides less margin to avoid an impact. reply trgn 13 hours agoparentprevPerfect illustration of the tragedy of the commons. Until there is top down intervention, people choose to be either part of the problem, or choose to be part of the solution. reply mitthrowaway2 12 hours agoparentprevIt sounds like it's also in your interest to legislate safety standards that ensure other peoples' vehicles are less likely to kill you when they crash into your own. reply saxelsen 12 hours agoparentprevThis is the reason my wife insisted that we get an SUV: Concern that if she were to be in a crash with another SUV/truck, they'd obliterate her in a small car. reply jasonladuke0311 10 hours agoparentprevYep it’s very much a prisoner’s dilemma. If I choose a smaller car then my family is in greater danger. reply arbuge 13 hours agoparentprevReally interesting video that one. reply wepple 12 hours agoparentprevYeah, we had a very compact car. Drove my firstborn home from the hospital in it, surrounded by monster vehicles. Then traded it for a (smallish) SUV. I may wish it weren’t that way, but I’m playing the game. reply simonjgreen 13 hours agoprevAs someone who travels to the US a lot, I’ve wondered what drives this phenomenon for a long time now as it’s unlike anywhere else in the world for the vehicles you see. My assumption had been a snowballing macro edition of one-upmanship that was in the car makers favour so they leaned in with marketing, I had no idea it was legislatively beneficial to go bigger. 4 out of 5 vehicles sold being SUV or Truck is crazy though, no way does 4 out of 5 of the population need that surely. reply redwall_hp 13 hours agoparentMy recommendation to fix the problem is a federal highway tax based on the fourth power of axle weight. This also solves the problem of heavy EVs not paying gas tax. https://en.wikipedia.org/wiki/Fourth_power_law Maybe couple that with a requirement for a higher license tier with more liability to drive \"light trucks,\" based on the licensing and taxation systems other countries use. reply mitthrowaway2 12 hours agorootparentAbsolutely. Alberta implemented a $200 tax on EVs this year, in part \"because they're heavier\" and create more road wear; if they cared about that they could have simply taxed weight in proportion to the wear it creates. That puts the user fees and incentives all in the right place, and doesn't let heavy IC vehicles off the hook either. reply quantum_state 11 hours agorootparentprevWould like to second this … heavier vehicles tend to cause road damage more … reply toast0 12 hours agorootparentprevWell, we can just add more axles, I guess. reply redwall_hp 12 hours agorootparentGood thing tollways already price per axle. I think the going rate in New York is something like $10 to cross a bridge, $5 for every additional axle, and more for vehicles over 7000lb. I wouldn't be surprised if there were federal or state laws that would already account for that on noncommercial vehicles. reply travisb 13 hours agoparentprevThe road to hell is paved with good intentions. You are right that most trips don't need SUVs and pickups, but many families need large-ish, relatively powerful vehicles like the now dead station wagon. But it is uneconomical for car builders to make station wagons, so they don't. Instead everybody who can't fit into an economy car is pushed into an SUV or pickup by practical concerns. On the other side, large and powerful luxury cars are also not economic for car makers to build, so instead car makers add luxury to large utility models. From there it's a trickle-down of cachet so owning a sedan or hatchback is insufficient for those who need or want to fit in with a higher economic class. reply KevinMS 7 hours agoparentprevThe \"station wagon\" (estate in other countries) and smaller hatchbacks have gotten trashed by american popular culture. But americans still need them, so they replaced them with \"tall\" wagons, aka, the SUV/CUV, which can also be marketed as being \"rugged\" because \"freedom\" sells in america. It really is a tragedy since wagons and hatchbacks are the most practical cars. reply openrisk 13 hours agoprevThe fact that fuel prices is the US are like 2.5x lower than Europe may also have contributed to the oversizing trend. You would think twice about gratuitously moving around a multi-ton behemoth if it actually hurt you economically. reply Scaevolus 13 hours agoprevEVs aren't zero emission on the road, either; roughly half of the pollution of an ICE car is not from the exhaust, but from the tires breaking down and releasing tiny (likely carcinogenic) particles, and heavy electric vehicles wear down their tires faster than a lighter car. reply 0_____0 13 hours agoparentThe conversation around microplastics rarely includes this info - a significant fraction of microplastics in an urban environment are from car tires, potentially the single largest source IIRC. reply uoaei 13 hours agoparentprevAt a rate proportional to the fourth power of the weight per-axle[1]. So considering how much more EVs weigh compared to their ICE counterparts, we can only hope that compact and lightweight EVs become more prominent. [1] https://en.m.wikipedia.org/wiki/Fourth_power_law reply AnthonyMouse 13 hours agorootparentEVs don't weigh that much more than ICE cars, e.g. Tesla Model 3 weighs around the same as a Ford Taurus. Moreover, EV or not is the irrelevant part of the equation. Model Y is ~4300 lbs, Ford Explorer is ~4500, Ford Excursion is ~7000, so which one do you want your neighbors driving? \"Everyone should ride a 100 lb motorcycle\" is the sort of hypothetical alternative that the people currently driving a big SUV are just going to scoff at and vote out anyone who attempts to force it. You'd be much more likely to succeed by e.g. trying to find a different tire composition that reduces particulates. reply redwall_hp 12 hours agorootparentI drive a 3000lb Honda Fit. A 2002 Camry, which is a perfectly reasonable family sedan, tops out at about 3300lb. The 2024 is up to a beefy 3600. The answer is none of your options, emphatically. I want to see extreme taxation (to the tune of the fourth-power of axle weight) and increases in liability for operating a vehicle over 3999lb. reply AnthonyMouse 12 hours agorootparentThen all the cars will weigh exactly 3999 lbs, which is about what they do right now (average new car is within 100 lbs of that), and everything will cost more because your tax will hit all the small businesses that use light trucks for legitimate needs instead of grocery runs. Also, it's a de facto ban on long-range EVs (EV battery weight is proportional to range), which seems counterproductive if you care anything about climate change, because then the vehicles that see the highest number of miles continue to run on petroleum. Stop trying to regulate random indirect metrics and concentrate on the actual problems. Is it traffic and too many vehicle miles? Then build higher density housing and subways. Is it air quality? Then address particulate emissions using general rules rather than specific ones -- regulate rate at which car tires can expel particulates. If they have to reduce the weight of the car to reduce the cost, fine, but if they can find another way, that's good too. If you don't like the status quo, identify why and address that, don't just pick some isolated aspect of it and try to ban it without contemplating what incentives the rules are going to create. That's how we got into this mess. reply Filligree 12 hours agorootparentprevWould it be unreasonable to put a third axle on? reply IshKebab 12 hours agoparentprevParticulates are only a local air quality issue. They don't cause global warming. This barely-an-issue is only ever brought up by people trying to naysay EVs. I've never once heard it mentioned before EVs became popular, despite heavy ICE cars being extremely common. reply tomjakubowski 13 hours agoparentprev\"zero emission\" usually refers to greenhouse gases reply tmountain 13 hours agorootparentWhich is unfortunate because any metric in a vacuum can be misleading regarding contribution to the aggregate. reply cagenut 13 hours agoparentprevthats not what the word emissions means reply gregwebs 13 hours agoprevMany Americans seem to have been told they need to drive a tank-like car to be safe. I point out that the tank is safer in a crash, but a smaller car may be able to stop in half the distance and avoid the crash altogether. I am assuming that its safer to focus on avoiding a crash than surviving it, but maybe I am wrong. Certainly it would depend on the driver. If you want to text on your phone and not maintain distance with the car in front than the tank approach is probably safer for you (but dangerous for everyone else). reply 10000truths 13 hours agoparentIt also fails to take into consideration that once everyone else gets a larger vehicle, you're less safe because two large vehicles colliding does far more damage than two small vehicles colliding at the same speed. reply AnthonyMouse 12 hours agoparentprev> I point out that the tank is safer in a crash, but a smaller car may be able to stop in half the distance and avoid the crash altogether. The problem there is that they're not actually correlated. A BMW M5 has a shorter stopping distance than a Mazda Miata even though it weighs almost twice as much. reply gregwebs 12 hours agorootparentGood point, when buying a car, one can try to buy good stopping distance. Still, all things being equal bigger classes of cars take longer to stop: https://www.consumerreports.org/cars/car-safety/best-and-wor... I also wonder what happens in real world non-ideal scenarios (including rain and snow) rather than on the perfect test tracks they test stopping distance on. Perhaps the fundamental advantage of a smaller car is increased maneuverability. reply AnthonyMouse 12 hours agorootparentThere isn't really anything fundamental. A heavier car has more momentum but it can also be fitted with larger tires with more contact area and the force of gravity pressing it into the road provides increased traction in proportion to mass. Cars aren't built to meet the theoretical limits of physics, they're built with trade offs against cost and other factors, so in practice things like stopping distance are less related to weight and more related to how much you prioritize it over the competing trade offs. A heavier car with the same stopping distance might cost more, but then there are people willing to pay for that. reply FireBeyond 8 hours agorootparentprevThat's a little contrived, no? I don't compare my RS 5 with a Corolla, either. The Audi and BMW have brakes like that because they also are literally designed to spend some time on the track (mine has integrated laptimers and speed calculators for 0-60 times, etc., and shows me individual tire temperatures not just pressure, as well as component temperatures for drive train, transmission, differential). reply crazygringo 12 hours agoparentprevBut smaller cars tend to have weaker brakes, and less weight that means less friction against the road in braking. Brakes are \"sized\" for the car they're in. Bigger cars have more powerful brakes. It would be a major safety issue if heavier cars couldn't brake as quickly as safety required. So no, smaller cars don't have any general advantage in being able to stop earlier and therefore avoid accidents. reply peoplefromibiza 12 hours agorootparentThat's true but only in perfect conditions. A not perfect breaking system or imperfect road conditions in a lighter car will cause a much smaller mass to not break as quickly as safety requires. reply snapplebobapple 11 hours agoprevSo get rid of cafe standards and import duties/blocks and add a federal gas tax to price the increased pollution and fatalities of larger vehicles. This is a policy that would actually work and let us fire a lot of beaurocrats to save even more money reply j-bos 12 hours agoprevI drive a small car, and my family has had SUVs ever since they could afford them. Were it not for cost, I would also drive an SUV or a big truck. A small car is a pain to carry people, luggage, furniture, plants, tool boxes, or anything beyond common commute items. I suspect most people with SUVs would agree. Small cars are great, until the days when they're not. reply prmoustache 12 hours agoparentSUV do not carry more stuff than lighter passenger cars of similar interior size. Actually I found out there are use cases where an SUV is worse than a convertible or even a bicycle with a trailer. I once carried home a small tree from my kids bicycle trailer and was amused to see peopme struggling to put large plants inside their SUV without spilling dirt all over the trunk carpet. Those with a pickup truck fared a bit better yet still had a harder time than I did because their truck bed height. The only guy who was as comfortable as I was putting a large plant in his car was the guy who put the pot on the passenger seat of his convertible. reply mitthrowaway2 12 hours agoparentprevHave you considered a minivan? reply politelemon 13 hours agoprevAny insight as to what happened in the UK? We seem to have caught the same bug, despite our streets being ill equipped to handle them. reply ejensler 13 hours agoparentUK just seems to be cargo-culting bad US policies at this point. reply userbinator 13 hours agoprevMid-century cars were very large too, but they had much better visibility. reply newsclues 12 hours agoprevLocal parking laws are a great tool. Limit the amount of parking required, sizes, etc. invest in transit and bike infrastructure. Just make cars/drivers pay for the real costs on urban society for car storage. reply barronli 12 hours agoprevPursuit of luxury is an important factor like for bigger houses. reply throwaway22032 10 hours agoprevIf I look back at how the UK has evolved over my lifetime the main issue is essentially that we are just wealthier and therefore do more. Either that or some sort of Jevons Paradox e.g. people spend less on basic necessities and more on luxuries. There are far more cars on the roads now, and there are far more people who can simply afford to own a car that gets 20-30mpg instead of 50-70mpg. I don't really know how we can \"solve\" this. The main route being taken at the moment is basically to try to reduce living standards, but to code it (e.g. \"reduce car journeys\") in such a way that everyone doesn't revolt. reply Zpalmtree 12 hours agoprevWhat's wrong with letting people drive the cars they want to reply happypumpkin 12 hours agoparentI'm normally on board with variations of this argument for things that don't significantly affect other people, but cars very much affect other people. Larger cars are more likely to kill other drivers, and especially bikers and pedestrians. The damage they do to the road is also a LOT more [1] so people who drive smaller vehicles and/or walk/bike are effectively subsidizing people who drive larger vehicles. They also cause more emissions meaning more medical issues and accelerated climate change. [1]: https://en.wikipedia.org/wiki/Fourth_power_law reply blibble 12 hours agoparentprevbecause cars have gigantic externalities heavier cars are deadlier in accidents, they cause more damage to road surfaces, they require wider roads and parking spaces which consumes land, they require more resources to produce, they require more energy to operate, they produce more pollution, and so-on reply sniggers 12 hours agoparentprevThe concepts of freedom and being comfortable in your car do not compute for non-Americans (and city-slicker-Americans) reply prmoustache 12 hours agoparentprevThat might be ok if people were responsible and not poorly trained to the task. Also norms exist for a reason and resources on this very planet have to be shared with billions of other human beings. reply pimlottc 11 hours agoparentprevLiterally more road deaths reply asadotzler 11 hours agoparentprevBecause cars have an impact on all of us, not just those driving them. These impacts range from tens of thousands of auto deaths every year to trillions of dollars in environmental damage that we will need to clean up eventually. Your libertarian fantasies are a poor match for the actual world. reply User23 12 hours agoprev> Carmakers from China, an emerging automaking behemoth, face a 25 percent tariff enacted by Donald Trump. As a result, Americans cannot buy small Chinese EV sedans like the BYD Seagull that cost around $10,000, barely a fifth the price of an average American car. Uhhh? So pay the tariff and get a $12,500 electric car? Still sounds like a pretty good deal plus the government gets some revenue. Where is the problem? Where is this “cannot” coming from? reply kccqzy 7 hours agoparentIt's because BYD (rightly IMO) predicts that if they sell such a vehicle, the government will immediately react by increasing the tariff. (Also the $10,000 cost doesn't include shipping.) reply Freedom2 13 hours agoprev [–] One thing that isn't often mentioned is the diversity of the US landscape. For those who go hiking or even straying off the common road, having high ride height is desirable, which suits large SUVs perfectly. reply prmoustache 12 hours agoparentPoor excuse. Similar challenges and diversity exist in many countries. Also, only a fraction of the population have that use case, that doesn't account for 80% of the market. Mountains and dirt fireroads/doubletracks exist in every countries. You know which car was the most popular for years until the end of its production in Spain by farmers and people living at the very end of dirt roads in spanish mountains? The citroen C15! I let you google for an image. It was so light that even with only front wheel drive it was much more capable off the road than most SUV. reply FireBeyond 8 hours agoparentprev [–] I would estimate that the percentage of the US population who goes off road even once in their truck or SUV is in the single digits at most (by which I mean anything more serious than parking in a grass field at the County Fair). reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Larger vehicles are increasingly common on American roads, leading to road safety issues, higher emissions, and infrastructure strain.",
      "Consumer choices and federal policies promoting SUVs and trucks are driving this trend towards larger vehicles.",
      "Calls for regulations to tackle safety concerns for pedestrians and cyclists, alongside proposed regulatory changes at federal, state, and local levels, face pushback from the auto industry."
    ],
    "commentSummary": [
      "The article examines the rise of SUVs and larger cars, pointing out the lack of progress in enhancing fuel efficiency and cutting emissions.",
      "It delves into various factors such as car technology advancements, consumer preferences, safety issues, and environmental consequences.",
      "Highlighting the link between car size, obesity trends, and body composition, it stresses the importance of regulating vehicle weight and enforcing policies to tackle safety, pollution, and urban infrastructure challenges while promoting responsible resource use and awareness of broader societal and environmental consequences."
    ],
    "points": 134,
    "commentCount": 112,
    "retryCount": 0,
    "time": 1714332215
  },
  {
    "id": 40192359,
    "title": "Personal Computing's Impact on Personal Library Science",
    "originLink": "https://www.bramadams.dev/issue-55/",
    "originBody": "Issue 55: Personal Computing Paves the Way How the past of Personal Computing gives us a hint into the future of Personal Library Science < Previous Issue Dear Reader, Sometimes, a moment happens not instantaneously, but over many seasons. Looked back on with the blessing of hindsight, there exists a discrete before and after the moment, but for those who lived it, the moment may have had a very long \"during\" period. Those who live in the \"during\" find themselves impatient, looking to be on the other side of the event, anxiously waiting for \"now\" to be \"history\". They want to live to see the thing complete. They want it to be done. For when \"now\" becomes history, there is a sense of peace of mind, a complete loop, a chord that resolves into something. History cannot be told while it is happening, therefore, not only because the people involved are too busy or too confused to puzzle it out, but because what is happening can’t be made sense of until its implications have been resolved. -- Everything Is Obvious: *Once You Know the Answer The curse of those who live and breathe is the task of trying to make sense of what will be – while it is – before it was. The \"During\" In issue #54, I wrote about a moment that is very much in its \"during\" phase, the formalization of Personal Library Science. I can not know how the story of Personal Library Science will be told by those who have the convenience of history to aid them. I can only discuss the landscape as I see it, the work completed by those who came before me, and the work being done now by me. Someday, libraries will be fully mechanized. Then, without leaving one’s office, it will be possible to pick up the phone, dial in a code, and have the actual paper one is looking for almost instantly at hand. Something of the sort has got to happen, or our libraries will become buried in the mass of books and articles now being printed, and searching in the old way will become hopeless. -- Pieces of the Action (1970) The \"during\" is hard work, and very lonely work. There are no promises of success, and indeed, the path is one where you can't see more than three feet ahead of you and you exist on the cliff's edge of extinction by any silly mishap. The work of \"during\" is exhausting, and it constantly holds you taut and alert, afraid of the shadows that lurk beyond the campfire's edge. Fortunately, the work of the edge isn't without some form of solace. The work becomes manageable, perhaps even virtuous, if we allow ourselves to commiserate with those who struggled and strived in the past to make their dreams a reality. To understand their history is to write our story. In this issue, we'll discuss a topic that is firmly in our past. This history is not only a guide for Personal Library Science, but a key player in it's very fiber of being. We will be talking about the Personal in the portmanteau that is Personal Library Science, which comes from Personal Computing. Personal Computing In 1975, the Altair 8800 was released. Widely considered the first personal computer, the critical advancement of computing was driven by affordability and programmability. Easier, more natural programming languages like BASIC and lower costs for hardware components made computers not massive time sharing leviathans owned only by defense departments for missile ballistic calculations and academia interested in pushing engineering and acquiring grants, but machines people could bring into their own homes. The computer was no longer just a tool for major cost intensive purposes, but minor individualized purposes. Personal computing made computing became less about humanity, and more about humans. As the 1990s opened, the workstation technology of the previous decade was beginning to look distinctly threatened by newer, low-cost and high-performance personal computers based on the Intel 386 chip and its descendants. For the first time, individual hackers could afford to have home machines comparable in power and storage capacity to the minicomputers of ten years earlier—Unix engines capable of supporting a full development environment and talking to the Internet. -- The Cathedral & the Bazaar: Musings on Linux and Open Source by an Accidental Revolutionary Scientists paved the way for engineers, engineers paved the way for hackers, hackers paved the way for hobbyists, hobbyists paved the way for everyone else. Today we live in a world where every person's phone is slightly different than their neighbors, and this is a good thing. Idiosyncratic technology creates a natural world. Solve the problems that need solving, then get out the way. Companies establish their DNA very early on. It can make them tremendously successful, but it can also make it hard for them to escape when what served them well in the early days doesn't serve them so well any more. I remember being an intern at IBM Research in Yorktown Heights around 1982, seeing the culture still dominated by batch processing. Even when they were doing timesharing, they talked in terms of virtual card readers and virtual card punches. Everything was still 80-column records. With DEC, it was the timesharing mentality that they never escaped. And I suppose with Microsoft it's an open question whether they'll be able to move beyond the desktop-PC mentality. Seibel: And 20 years from now people will be talking about how Google can't get past how to sell ads on the Internet. author's note -- ironically this was posted a few weeks before the writing of this issue: The man Who Killed Google Search -- Coders at Work: Reflections on the Craft of Programming (2008) What does the legacy and lessons of the rise of Personal Computing at the end of the 20th century teach us about the possible path of Personal Library Science? Companionship In 2020, I made the claim that the advantage of GPT models was not its ability to scale, but its ability to solve local problems. This claim began to come true years later with the release of the GPT Store (discussed in issue #42). Suddenly, individual people – not at the organization level – found themselves being able to create their own solutions to their unique, idiosyncratic problems. Idiosyncratic technology creates a natural world. By writing detailed instruction sets and using tactics like semantic search and function calling, LLMs leverage the effort of the individual into emergent and complex processes. This leverage is the same leverage that made personal computing so ubiquitous, so quickly. The leverage created a new asset class, data. It gave us (you and me) superpowers of communication, and those of us who could communicate with the programs themselves (programmers) found themselves solving problems that gave them access to near limitless wealth. It’s cheaper to reuse existing software components than to write code from scratch, which also makes it possible for entrepreneurs to start software companies with fewer up-front costs. The entire software industry owes its financial success to leveraging this arbitrage. -- Working in Public: The Making and Maintenance of Open Source Software Personal Library Science Is Leverage For One's Library Personal Library Science is the leverage of LLM technology, applied to a personal library. A personal library differs from a impersonal library in the fact that a personal library is an interpretation of a source material. These interpretations include: photographs from different photographers at the same event, or favorite scenes from a movie, or favorite passages from books, parts of songs that bring you to tears, etc. Importantly, these interpretations create unique sets that go on to create unique problems which require unique, idiosyncratic solutions. Sound familiar? While a painting or a prose description can never be other than a narrowly selective interpretation, a photograph can be treated as a narrowly selective transparency. But despite the presumption of veracity that gives all photographs authority, interest, seductiveness, the work that photographers do is no generic exception to the usually shady commerce between art and truth. Even when photographers are most concerned with mirroring reality, they are still haunted by tacit imperatives of taste and conscience. The immensely gifted members of the Farm Security Administration photographic project of the late 1930s (among them Walker Evans, Dorothea Lange, Ben Shahn, Russell Lee) would take dozens of frontal pictures of one of their sharecropper subjects until satisfied that they had gotten just the right look on film—the precise expression on the subject’s face that supported their own notions about poverty, light, dignity, texture, exploitation, and geometry. In deciding how a picture should look, in preferring one exposure to another, photographers are always imposing standards on their subjects. Although there is a sense in which the camera does indeed capture reality, not just interpret it, photographs are as much an interpretation of the world as paintings and drawings are. Those occasions when the taking of photographs is relatively undiscriminating, promiscuous, or self-effacing do not lessen the didacticism of the whole enterprise. This very passivity—and ubiquity—of the photographic record is photography’s “message,” its aggression. -- On Photography In issue #54, I wrote: ...personal library science is focused on your relationship with your information. How do we store information so that it useful at a later date? How do we transform our information into new valuable assets in different creative domains? How do we do all of this while being flexible enough for the idiosyncrasies, proclivities, likes and dislikes of eight billion distinct individuals? How do we chronicle the information diet of a single person as they learn new things, interact with the world at different phases in their life? How do we make sure we can pass down our best knowledge to generations below? Many of these questions were asked and solved by the existence of personal computing. The existence of software has had a foundational impact on the types of solutions we can create to solve these large, incalculable problems. By solving them over and over in slightly different ways that make sense to an individual the problem eventually dissolves. In fact, I'd argue that the history of personal computing teaches us that all top down solutions are less complete – and therefore less useful – than bottom up emergent solutions. The issue is that we are now deluged with data, our interpreter antenna is going haywire trying to calculate, to store, to relate, to understand. LLMs have changed the math entirely in this endeavor, particularly thanks to the ability to store, reference, and transform data that we find to be important, not data that others tell us is important. Solutions such as Commonplace Bot and Quoordinates are proof points of the value of our personal libraries. This insight is critical to our work, and the frameworks and technological gains provided by personal computing and LLMs make our task of hoping to one day understand ourselves, to use our personal libraries to secure our intellectual legacies and create our art, a bit more manageable. Up Next Next week, we will dive into the history of the commonplace book. The commonplace book is the foundation for the commonbase (commonplace + database) and is an integral ingredient of Personal Library Science. Teaser ...Humanity often comes up with certain great ideas concurrently including calculus, natural selection, the use of language, etc. This concurrent creation is known as adjacent possible, and is driven by the available technology and discourse of an era. The invention of the commonplace book is no different and has lived many different lives as: the zibaldone, the miscellany, the zettelkasten, the... Ye Olde Newsstand - Weekly Updates Week of April 26, 2024 ft. Chris. P Bacon Bram Adams Bram Adams Canvas Apps, speedboat accidents, the First Hokage was just a dude Thanks for reading, and see you next Sunday! ars longa, vita brevis, Bram Sunday, 28 April 2024 Bram Adams newsletter Share on Facebook Share on LinkedIn Share on 𝕏 Copy link Comments",
    "commentLink": "https://news.ycombinator.com/item?id=40192359",
    "commentBody": "Personal computing paves the way for personal library science (bramadams.dev)121 points by _bramses 12 hours agohidepastfavorite21 comments A_D_E_P_T 1 hour agoI work in a very interdisciplinary, and somewhat niche, tech/engineering field. For the past 15 years, I've been saving every relevant PDF that I can find -- mostly studies of the sort published by Elsevier and Springer, but also books and presentations. I now have around 10k, which probably makes it the largest private library focused on this particular domain of expertise. It has been extremely useful, especially because it's text-searchable and the really important papers are properly categorized. A local LLM will make it 100x more useful. Also, it might not even need be \"local.\" If I make it available via the web, I can probably sell access to other scientists and engineers in my field. Recent advances really benefit data hoarders out there. I'd add that these days it totally makes sense to download libgen's entire archive, because (1) storage has never been cheaper, and (2) you can use it to train local LLMs. reply squigz 1 hour agoparentThe data hoarder community would encourage you to release that collection for free, not try to profit from it. reply hervature 1 hour agoparentprev> If I make it available via the web, I can probably sell access to other scientists and engineers in my field. Out of curiosity. Does this statement come from complete ignorance of or complete disregard to copyright of the author? reply throwaway11460 1 hour agorootparentIf it's research it very probably is at least partially publicly funded. Regardless of whatever the law says, I don't think it's immoral to take it and offer better services around it that will be useful enough that someone decides to pay. reply hervature 1 hour agorootparentDo you not see the hypocrisy in stating that someone should be able to take something partially publicly funded and profit from it while the creator of said work should not retain some rights over said profit? By extension of the transitive property of the nebulous \"partially\", the LLM wrapper should be provided for free with complete disregard to the wrapper's creator since it is a derivative of partially publicly funded work. reply throwaway11460 1 hour agorootparentYeah, the LLM maybe - though nobody paid for the training costs in that case and that feels weird. If the public paid for a work, it should be able to use it and not be required to give away their own derivative work for free since they already paid for it through taxes. While I rather like the idea of having to provide access to derivates of publicly funded works, I fear that people would rather not use it than invest money into innovative approaches of using it. Of course if the public pays for the training and development costs, then by all means it should be available. And the library itself and the computing resources to operate it cost money that someone needs to pay. Publishers didn't pay for the research and yet they can profit from it - why this guy shouldn't? reply walterbell 1 hour agorootparentprev> complete ignorance of or complete disregard to copyright A question that could be appended to many LLM discussions! reply teja_nemana 9 hours agoprev> The \"during\" is hard work, and very lonely work. There are no promises of success, and indeed, the path is one where you can't see more than three feet ahead of you and you exist on the cliff's edge of extinction by any silly mishap. The work of \"during\" is exhausting, and it constantly holds you taut and alert, afraid of the shadows that lurk beyond the campfire's edge. Well said. All anyone can do is to do the lonely work till you can't anymore or you find friends to not be lonely at that work anymore. reply openrisk 4 hours agoprevPersonal computing has stagnated for such a long time, it creates substantial uncertainty about what state it might evolve to if and when the next step actually happens. In this respect local LLM's are simply the tip of the iceberg, pointing out the vast amount of personal information processing that is available in principle but does not actually happen. reply walterbell 3 hours agoparentOne could argue that personal computing (desktop) software piracy lead to web-based SaaS subscription licensing. In theory, mobile app stores solved device software piracy, at the cost of high distribution fees, policy restrictions and telemetry. Thanks to Linux being used at scale in Android and WSL, it's now maintained and capable on the desktop, as a hypothetical foundation for personal computing innovation. But even there, native GUI toolkits took a backseat to web and CLI. Remember Chandler? http://www.osafoundation.org/ Investors poured small fortunes into cauldrons of smart devices, wearables and AR/VR, with little to show as nascent ecosystems failed to achieve escape velocity, due to closed hardware and software that forestalled the experimentation which birthed personal computing. Apple Silicon has reinvigorated walled laptops. Hopefully next month's derivative Qualcomm SoC from PC OEMs can offer good price/performance/watt for Apple-competitive-yet-open Arm laptops and tablets that can run any Linux distro, with retail SSDs and RAM, plus AI silicon roadmap. A modular Framework Arm laptop would be a good start to rebooting PC innovation. reply idle_zealot 2 hours agorootparentHow does slightly improved laptop hardware relate to re-invigorating desktop software? Surely desktop computing has stagnated because most users are primarily or exclusively mobile users. In Mac land Apple has been progressively dumbing down their interfaces, in Windows land Microsoft is more focused on extracting maximum value from their users than trying to meaningfully improve their platform. In Linux land there are some interesting things happening with Nix/Guix around declarative system configurations, and around Fedora with its layered images+Flatpak distros for making systems more reliable, and System76 may be doing something novel interface-wise with Cosmic marrying powerful tiling/tabbing window layouts with intuitive controls and the niceties of an all-in-one desktop environment. From my perspective desktop computing is definitely advancing, but only for hobbyists, not for mainstream desktop operating systems. reply walterbell 1 hour agorootparent> How does slightly improved laptop hardware relate to re-invigorating desktop software? If Arm SystemReady laptops with good performance/watt have an open security foundation (declarative, immutable OS at EL2) to support multiple competing \"app store\" equivalents on Linux, the resulting revenue and competitive market can reward innovative desktop software - open, closed or hybrid. Without an Apple tax on storage and memory, funds can be redirected to a competitive market of smaller ISVs. reply quest88 6 hours agoprevI've had related ideas lurking at the back of my mind for a while now. Essentially, I want to save more things locally and and interact with it. For example, I have a bunch of book notes stored in Bear. I'd like to be able to ask questions about those notes, and also show the pages of the book itself. reply walterbell 11 hours agoprev> Personal Library Science is the leverage of LLM technology, applied to a personal library. A personal library differs from a impersonal library in the fact that a personal library is an interpretation of a source material. These interpretations include: photographs from different photographers at the same event, or favorite scenes from a movie, or favorite passages from books, parts of songs that bring you to tears, etc. Importantly, these interpretations create unique sets that go on to create unique problems which require unique, idiosyncratic solutions. Would an LLM-driven \"Personal Library\" require manually annotated textual interpretation of each curated item, or could it derive personal interpretations from user history and the uniqueness of curated items/sets? For those who have been using local, offline LLMs with a manually curated text/image corpus, what have been the most valuable or surprising use cases? Author demo video (2023), https://youtube.com/watch?v=7TgqMRz2r3M & tooling comment (2024), https://news.ycombinator.com/item?id=39789712 > Inspired by the commonplace book format, I take highlights from Kindle and embed them in a DB. From there I build (multiple) downstream apps but the central one, Commonplace Bot is a bot that serves as a retrieval and transformer for said highlights. Related: https://en.wikipedia.org/wiki/Lifelog reply _bramses 5 hours agoparent> Would an LLM-driven \"Personal Library\" require manually annotated textual interpretation of each curated item, or could it derive personal interpretations from user history and the uniqueness of curated items/sets? I’ve personally found that tagging is less robust than LLM embeddings (mainly due to dimensionality), but human appended thoughts about a source — also embedded — serve even better as tags. Example: “this is a quote about dinosaurs…” (Old way of doing things) Tags: dinosaurs, jurassic, history Query: “dinosaurs” > results = 1… (New way of doing things) Embedded Quote: [0.182…] User Added Thought: “this dinosaur reminds me of a time i went to six flags with my cousins and…” Embedded User Added Thought: [0.284…] Query: “dinosaurs” > results = 2 (indexes = sources, thoughts) The \"thoughts\" index can do a second layer cosine similarity search and serve as a tag on its own to fetch similar concepts. Basically a tree search created by similarity from user input/feedback loops. reply dartos 10 hours agoparentprev> Would an LLM-driven \"Personal Library\" require manually annotated textual interpretation of each curated item No. In something like this you’d probably have the LLM annotate and curate your personal library for you. Potentially by creating and assigning tags or topics based on the content of your library. reply unshavedyak 10 hours agorootparentYea. I had this discussion not too long ago about this. I'd love to have a combination of a library (Personal Knowledge Management style), data ingestions, and a current world view/state. The PKM is the stored info to write to and query against (both for LLMs and humans). The data ingests are just a pipeline of digital inputs to the system, like chat logs, maybe (transcribed) webcam feeds, files i'm currently editing on desktop, browsing history, etc. The current world view is the interpretation of what i'm doing - to tie all the ingests together and give them context. Eg in isolation browsing some Rust crates might not be that useful. But if i'm also editing Project X on my computer then it's reasonable to assume the searching is related to X. However if it's been 8 hours since any Project X activity, it's less likely related. Same goes for context-less chat logs (as happens frequently in my house) where they are extensions of a voice conversation, etc. All of this stuff is of course insanely privacy invading, so i'd only implement this locally. I also wouldn't even store most of it for fear of data invasion, but using it to fuel a PKM automatically seems pretty sexy. Like browser history, but for your life. This is all just wishful thinking though, LLMs have been moving too fast for me to even bother toying with this. I should note though that i did not intend for LLMs to be \"smart\". Rather, in a RAG-like fashion (i think is the term), i want to just let LLMs do what they're good at - summarization & autocomplete, and let the world view / PKM store the real data. reply skybrian 3 hours agoparentprevI imagine an LLM could work well for doing autocomplete while saving and annotating documents. But it’s not personal unless you edit the result to say what you want to say. reply mistrial9 10 hours agoprev [–] there may be an important divergence implied by this essay .. people here ask about using an LLM.. but the essay refers to \"different photographs of the same scene from different photographers\" or other personal collection items that are related but subjective or not-authoritative There is a rush in public to condense and summarize many authoritative publications to find patterns, or to replace a human expert with automated results.. yet that is fundamentally different than taking multiple incomplete perspectives to add to a human library-owners knowledge and investigations. It is subtle to speak it but not subtle in its implications.. taking \"data as facts\" and condensing them or reordering them or rewriting an output based on them, using automation, is different than a human mind taking in many inputs for human mind knowledge and enabling new outputs from a human author. reply _bramses 5 hours agoparent [–] > There is a rush in public to condense and summarize many authoritative publications to find patterns, or to replace a human expert with automated results.. yet that is fundamentally different than taking multiple incomplete perspectives to add to a human library-owners knowledge and investigations. It is subtle to speak it but not subtle in its implications.. taking \"data as facts\" and condensing them or reordering them or rewriting an output based on them, using automation, is different than a human mind taking in many inputs for human mind knowledge and enabling new outputs from a human author. You nailed it! Thanks for noticing the divergence! reply walterbell 3 hours agorootparent [–] There's lots of interesting work that came out of BCL in 1960s, https://en.wikipedia.org/wiki/Biological_Computer_Laboratory > The focus of research at BCL was systems theory and specifically the area of self-organizing systems, bionics, and bio-inspired computing; that is, analyzing, formalizing, and implementing biological processes using computers. BCL was inspired by the ideas of Warren McCulloch and the Macy Conferences, as well as many other thinkers in the field of cybernetics. On cybernetics, https://www.pangaro.com/definition-cybernetics.html > Artificial Intelligence (AI) grew from a desire to make computers smart, whether smart like humans or just smart in some other way. Cybernetics grew from a desire to understand and build systems that can achieve goals.. it connects control (actions taken in hope of achieving goals) with communication (connection and information flow between the actor and the environment).. Later, Gordon Pask offered conversation as the core interaction of systems that have goals. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The newsletter delves into the link between the past of Personal Computing and the future of Personal Library Science, stressing the use of technology to address personalized issues.",
      "It draws comparisons between the evolution of personal computers and the possibilities of LLM (Personal Library Management) technology in shaping personal libraries.",
      "The newsletter underscores the significance of distinct viewpoints and approaches in personal libraries, underscoring the need to comprehend our interaction with information, promising an upcoming analysis of the commonplace book's history and its relevance to Personal Library Science."
    ],
    "commentSummary": [
      "The post discusses the benefits and challenges of building a personal PDF library in a specialized engineering field, addressing copyright concerns and expenses.",
      "It explores the latest developments in desktop computing to improve personal libraries, including Lifelog Models and Large Language Models for annotation and summarization.",
      "Additionally, it examines the impact of automation on information condensation and the historical significance of the Biological Computer Laboratory and cybernetics, with a focus on Artificial Intelligence and Cybernetics aiming for goal achievement through control and communication, highlighting conversation as crucial in goal-oriented systems."
    ],
    "points": 121,
    "commentCount": 21,
    "retryCount": 0,
    "time": 1714341420
  }
]
