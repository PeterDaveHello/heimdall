[
  {
    "id": 38773429,
    "title": "Japan Moves to Break Apple and Google App Store Monopolies",
    "originLink": "https://asia.nikkei.com/Business/Technology/Japan-to-crack-down-on-Apple-and-Google-app-store-monopolies",
    "originBody": "Your Account Account details Newsletters Group subscription Log out Log In Subscribe World China Japan India South Korea Indonesia Taiwan Thailand U.S. East Asia China Hong Kong Macao Taiwan Mongolia Japan South Korea North Korea Southeast Asia Indonesia Thailand Malaysia Singapore Philippines Vietnam Myanmar Cambodia Laos Brunei East Timor South Asia India Pakistan Afghanistan Bangladesh Sri Lanka Nepal Bhutan Maldives Central Asia Kazakhstan Uzbekistan Turkmenistan Tajikistan Kyrgyzstan Oceania Australia New Zealand Papua New Guinea Pacific Islands Rest of the World Middle East Russia & Caucasus North America Latin America Europe Africa Trending Israel-Hamas war COP28 Taiwan elections China debt crunch Inflation Supply Chain Ukraine war Taiwan tensions Explainer Business Business Semiconductors Automobiles Energy Transportation Retail Travel & Leisure Media & Entertainment Food & Beverage Finance Electronics Startups Markets Markets Market Spotlight Currencies Commodities Property IPO Bonds Wealth Management Tech Tech #techAsia China tech Startups 5G networks Cryptocurrencies DealStreetAsia 36Kr/KrASIA Politics Politics China Japan India South Korea Indonesia Taiwan Thailand U.S. East Asia China Hong Kong Macao Taiwan Mongolia Japan South Korea North Korea Southeast Asia Indonesia Thailand Malaysia Singapore Philippines Vietnam Myanmar Cambodia Laos Brunei East Timor South Asia India Pakistan Afghanistan Bangladesh Sri Lanka Nepal Bhutan Maldives Central Asia Kazakhstan Uzbekistan Turkmenistan Tajikistan Kyrgyzstan Oceania Australia New Zealand Papua New Guinea Pacific Islands Rest of the World Middle East Russia & Caucasus North America Latin America Europe Africa Economy Economy China Japan India South Korea Indonesia Taiwan Thailand U.S. East Asia China Hong Kong Macao Taiwan Mongolia Japan South Korea North Korea Southeast Asia Indonesia Thailand Malaysia Singapore Philippines Vietnam Myanmar Cambodia Laos Brunei East Timor South Asia India Pakistan Afghanistan Bangladesh Sri Lanka Nepal Bhutan Maldives Central Asia Kazakhstan Uzbekistan Turkmenistan Tajikistan Kyrgyzstan Oceania Australia New Zealand Papua New Guinea Pacific Islands Rest of the World Middle East Russia & Caucasus North America Latin America Europe Africa Features The Big Story Asia Insight Business Spotlight China up close Market Spotlight Datawatch Special Reports Infographics Opinion Opinion Editor-in-Chief's Picks The Nikkei View Life & Arts Life & Arts Life Arts Tea Leaves Obituaries Books Podcast Subscribe Account details Newsletters Group subscription Log out Log In Your Account Account details Newsletters Group subscription Log out Log In Subscribe Technology Japan to crack down on Apple and Google app store monopolies Antitrust curbs to require tech giants to allow third-party app platforms and billing Google and Apple both require mobile apps to generally use their billing systems. (Photo by Masaharu Ban) RYOHEI YASOSHIMA and RIHO NAGAO, Nikkei staff writersDecember 27, 2023 01:22 JSTJapan CopyCopied TOKYO -- Japan is preparing regulations that would require tech giants like Apple and Google to allow outside app stores and payments on their mobile operating systems, Nikkei has learned, in a bid to curb abuse of their dominant position in the Japanese market. Legislation slated to be sent to the parliament in 2024 would restrict moves by platform operators to keep users in the operators' own ecosystems and shut out rivals, focusing mainly on four areas: app stores and payments, search, browsers, and operating systems. Read Next Technology Apple and Google on hook for foreign app tax in Japan code proposal Economy Japan antitrust watchdog to probe Google over dominance in search Technology Apple and Google warned on app stores by Japan antitrust watchdog Startups GIC-backed Coda offers fresh alternative to Big Tech app stores Technology Apple rings up $46bn for developers via App Store in Japan Latest On Technology Technology Tencent shares regain some ground as Beijing moves to ease gaming fears Technology Samsung's next Galaxy phones to feature instant call translation Technology As cyberscams surge, Taiwan's Gogolook eyes growth in Asia Sponsored Content About Sponsored Content This content was commissioned by Nikkei's Global Business Bureau. Discover the all new Nikkei Asia app Get Insights on Asia in your inbox Register for our newsletters Connect With Us About us Contact us Sitemap Help Terms of use Copyright Privacy & cookie policy Information Transmission Advertising Nikkei Inc. No reproduction without permission. Nikkei Asian Review, now known as Nikkei Asia, will be the voice of the Asian Century. Celebrate our next chapter Free access for everyone - Sep. 30 Find out more",
    "commentLink": "https://news.ycombinator.com/item?id=38773429",
    "commentBody": "Japan to crack down on Apple and Google app store monopoliesHacker NewspastloginJapan to crack down on Apple and Google app store monopolies (nikkei.com) 526 points by mfiguiere 17 hours ago| hidepastfavorite275 comments freedomben 15 hours agoI think Google really made a strategic mistake by copying Apple on the payment restrictions. It kills me to see Apple and Google lumped together as \"monopolies\" when Google&#x27;s policy is IMHO 100x less monopolistic than Apple&#x27;s given you can sideload and use completely different app stores, and enable \"developer mode\" without paying a subscription fee just to run your own app on your own device. But the payment restrictions are definitely monopolistic-style abuse. Had they not have copied that, I don&#x27;t think they&#x27;d be under this microscope and losing lawsuits and what not.It does still blow my mind that Apple won their lawsuit from Epic, yet Google lost, when Google is far less restrictive. IANAL but from what I&#x27;ve understood it mainly came down to the fact that G execs put the stuff in writing whereas Apple did not, so with G there was some real damning evidence of the anti-competitive behavior. But ironically, the reason G execs were in the position of having to buy off people and make deals to stifle competition is because of their looser reins over the platform. If they&#x27;d been draconian and hyper-controlling from the start, refusing side-loading and similar like Apple does, they wouldn&#x27;t have had to pay people off and make deals to crush competition as that competition couldn&#x27;t have even gotten off the ground in the first place. reply udkl 12 hours agoparentstratechery has a reasonable explanation in one of the recent articles :\"That last point may seem odd in light of Apple’s victory, but again, Apple was offering an integrated product that it fully controlled and customers were fully aware of, and is thus, under U.S. antitrust law, free to set the price of entry however it chooses. Google, on the other hand, “entered into one or more agreements that unreasonably restrained trade” — that quote is from the jury instructions, and is taken directly from the Sherman Act — by which the jurors mean basically all of them: the Google Play Developer Distribution Agreement, investment agreements under the Games Velocity Program (i.e. Project Hug), and Android’s mobile application distribution agreement and revenue share agreements with OEMs, were all ruled illegal.This goes back to the point I made above: Google’s fundamental legal challenge with Android is that it sought to have its cake and eat it too: it wanted all of the shine of open source and all of the reach and network effects of being a horizontal operating system provider and all of the control and profits of Apple, but the only way to do that was to pretty clearly (in my opinion) violate antitrust law.\"The key is &#x27;unreasonably restrained trade&#x27; - Any OEM was eligible to use Android, but what google did was restrict competition by &#x27;entered into one or more agreements that unreasonably restrained trade&#x27;https:&#x2F;&#x2F;stratechery.com&#x2F;2023&#x2F;googles-true-moonshot&#x2F; reply robertlagrant 12 hours agorootparentI think even this is a little unfair. Almost no one buys Android because they are a horizontal operating systems provider, and OEMs don&#x27;t use it because of same, because the former don&#x27;t care and the latter already know the reality. People use it because it&#x27;s their best option, and not due to any monopolistic practices excluding alternatives. It&#x27;s just the best option. reply andyferris 9 hours agorootparentUmm, some people like Samsung phones and some people like Pixels and there are lots of other popular handset makers.A small number of purchasers want to side load and therefore exclude Apple as an option.I think it’s not right to say the horizontal ecosystem isn’t a reason for Android’s commercial success. In reality it is a driving reason why Android succeeded over (and completely obliterated) Nokia&#x2F;Microsoft&#x2F;RIM - the variety of OEMs innovating rapidly meant they just couldn’t keep up. In the early days people frequently upgraded their Android phone with a better one, often from another Android manufacturer.So, Google knowingly created an open market and now is (or at least appears to be) abusing it. reply echelon 4 hours agorootparent> A small number of purchasers want to side load and therefore exclude Apple as an option.An overwhelming majority of consumers don&#x27;t even know what side loading is. These decisions are not things lay consumers think about at all.The fact is that both Apple and Google set too many rules on a platform that is now essential for operating in modern society. These devices and platforms need to be opened up to competition. These companies are collecting tax on all innovation happening because of their gatekeeper positions. reply LargeTomato 11 hours agorootparentprevAgreed. If Google locked down their app store more aggressively they&#x27;d be legally in the clear. They played nice, just not nice enough, so they got anti-trusted. reply jonhohle 10 hours agorootparentThey didn’t play nice. They made back channel deals with privileged partners. Love or hate the Apple arrangement, but they seem consistent on giving everyone the same terms. That’s what makes one anti-competitive and the other not. reply mcny 6 hours agorootparent> Love or hate the Apple arrangement, but they seem consistent on giving everyone the same terms.Previously on hacker news: https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=36882467From the article:> Today, there are more than 2.4 million mini-programs that can be run within WeChat. If this trend continues, there will come a time when developers will concentrate on developing new useful mini-programs to run on WeChat instead of stand-alone apps which have to abide by certain rules before it becomes available on the App Store.> If Apple cracks down on WeChat’s practices, Chinese customers will have little reason to ever buy an iPhone.> So, if Apple wants the Chinese to continue to buy iPhones, they have to let WeChat – one of the most privacy-invasive and censorship driven apps on the planet, continue to ignore its App Store rules.back home [uber] skirts the boundaries of the law if not outright crosses it several times and it is still available on the app store. Not that any of this excuses anything Google does but just wanted to point out that Apple most definitely does NOT give everyone the same terms.[uber] https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=14178397 https:&#x2F;&#x2F;www.nytimes.com&#x2F;2017&#x2F;04&#x2F;23&#x2F;technology&#x2F;travis-kalanic... https:&#x2F;&#x2F;archive.ph&#x2F;uJ0vm reply amadeuspagel 10 hours agorootparentprev> Love or hate the Apple arrangement, but they seem consistent on giving everyone the same terms.They can do that, because there&#x27;s no other choice. If a company wants to offer their apps to iOS users, they have to go through the App Store, so they&#x27;re in no position to bargain for better terms. reply shwouchk 8 hours agorootparentIndeed. And if a company wants to deal with US consumers, they need to comply with US law. That’s reasonable. And if&#x2F;when some companies get referential treatment, you get (justified) uproar. reply echelon 4 hours agorootparentprevOne was a jury trial, the other was not.Apple&#x27;s overall behavior is worse and far more anti-competitive.There&#x27;s no reason Apple should tax dating, yet they do for over 50% of Americans who date online. They tax all communication and commerce by dictatorially controlling the most essential device of our century and being one of only two primary vendors.I&#x27;d say I&#x27;m glad Apple and Google don&#x27;t control the Internet, but they kind of de-facto do control it for most people. reply type0 11 hours agorootparentprev> product that it fully controlled and customers were fully aware ofNo my experience, there are plenty of these techy customers online but I have yet to meet such Apple user IRL reply Terretta 11 hours agorootparent>> product that it fully controlled and customers were fully aware of> No my experience, there are plenty of these techy customers online but I have yet to meet such Apple user IRLThe entire premise and point of the Apple experience is not having to be fully aware of the tech. At all. Ever.For most Apple users, the tech is just not the point. It&#x27;s that it is air. Don&#x27;t think about it unless it&#x27;s missing or stinks. reply bluSCALE4 11 hours agorootparentThis. I left Android because they were slowly becoming a more rigid company and everything I had loved and valued were gone and Apple slowly started adding them to their OS. I was fully aware what I was buying into and what I was losing but I no longer cared because it felt only a matter of time before Android is the same. Though I&#x27;ve gotten less use out of my Apple phone, I don&#x27;t really care because I&#x27;ve given up on the idea that a phone is an uncompromised computer. reply Drew_ 6 hours agorootparentprev> The entire premise and point of the Apple experience is not having to be fully aware of the tech. At all. Ever.This couldn&#x27;t be further from the truth. They change the tech every year. The Apple experience is being distinctly aware of all the new tech available every year because of Apple&#x27;s sheer force of marketing and FOMO. reply Klonoar 6 hours agorootparentThe average consumer isn’t paying attention to this stuff.Literally: as long as it’s faster, longer battery life, and better camera, they will buy it or upgrade to it. It’s a status symbol. reply downWidOutaFite 10 hours agorootparentprevThis answer is unsatisfying to me because \"not allowing trade with 3rd parties\" could be seen as a version of \"unreasonably restrained trade\".In general I think it&#x27;s incorrect to try to divine a consistent legal principle from these two cases. Trials have an element of randomness so either case could have gone the other way under different judges and lawyers. reply bobthepanda 9 hours agorootparentIIRC the Apple case went differently because* it has always been locked down, from the get go, so it has never misrepresented itself to consumers or the people participating in its ecosystem* it gained marketshare despite this very limitation, indicating that to some extent that is what consumers were expecting to purchase. Apple had zero marketshare in phones before iPhone. And many attempts have been made before and since, like Windows Phone, Fire Phone from Amazon, the Facebook phone, etc.* they hadn’t, at the time, told anyone else not to participate in other stores or on other platforms reply AnthonyMouse 13 hours agoparentprev> IANAL but from what I&#x27;ve understood it mainly came down to the fact that G execs put the stuff in writing whereas Apple did not, so with G there was some real damning evidence of the anti-competitive behavior.It mainly came down to the fact that they were in different courtrooms and the higher-level appellate courts haven&#x27;t yet decided how they&#x27;re going to reconcile the results (possibly by overturning one of them).It&#x27;s kind of an interesting case study in the arbitrariness of the law. The most important question in either case is if excluding competing app stores is permissible. It&#x27;s obviously anti-competitive, but doing anti-competitive things is sometimes allowed if you have a legitimate justification. Apple&#x27;s argument is presumably that they need to for security. This is, of course, BS, because a user who wanted Apple to vet all of their apps could still choose not to install any from outside of Apple&#x27;s store even if Apple didn&#x27;t prohibit them from doing so.Google could make the same claim -- they have to discourage these filthy competitors because some of them might not be selective enough in what they include, so suppressing them improves security -- and it would be equally BS. But then you uncover some emails that make them look unsympathetic, or admitting that the pretext is a farce, and now it&#x27;s less likely they get away with the charade.The root of the problem here is that the rule that you can do something anti-competitive if you have an excuse has the potential to swallow the entire law. \"Our competitors are smelly and vile and we have to protect our customers from interacting with them even if the customer explicitly wants to do that\" is a generic excuse that could be used to justify any anti-competitive behavior. That&#x27;s easier to see if you can read some emails conceding the underlying motive, but it&#x27;s true in either case. Hopefully the higher courts will be able to see that in both cases once they&#x27;ve seen it in one of them. reply fsckboy 12 hours agorootparent>higher-level appellate courts haven&#x27;t yet decided how they&#x27;re going to reconcile the resultsthere are many areas of the law that don&#x27;t set precedent and don&#x27;t need reconciliation reply AnthonyMouse 12 hours agorootparentBoth of the cases have been appealed. Appellate court decisions set precedent within their jurisdiction. If the Supreme Court takes the case (not unreasonable that the two could be heard together), they&#x27;ll issue an opinion and create a national precedent. reply GeekyBear 12 hours agoparentprev> It does still blow my mind that Apple won their lawsuit from Epic, yet Google lostWhen Google chose to open the Android OS, it created a marketplace for Android devices which it attempted to control by the use of anticompetitive contracts and actions.The parallels with Microsoft and Windows are obvious. Microsoft has been found guilty of anticompetitive actions in the Windows PC marketplace it created by opening up Windows.Yet Microsoft also has the XBox, which it did not open up to other hardware makers and which is just as much of a walled garden as iOS.There have been no legal ramifications of Microsoft choosing to be the sole maker it&#x27;s own product, nor of having it&#x27;s product be a walled garden.It&#x27;s not illegal to have a monopoly over your own product and it&#x27;s not illegal to have a walled garden. reply CydeWeys 11 hours agorootparent> It&#x27;s not illegal to have a monopoly over your own product and it&#x27;s not illegal to have a walled garden.The larger point being made is that once your product has a commanding share of the market, it should be illegal, as it&#x27;s clearly anticompetitive by that point. reply GeekyBear 11 hours agorootparentThen you will have to make it illegal for Microsoft to have products like Microsoft Office or XBox that it has not opened up. reply sensanaty 11 hours agorootparentI don&#x27;t think anyone would be against breaking up M$ a bit. Why wouldn&#x27;t you want some proper M$ Office alternatives?Xbox is a bit trickier presumably because of PC gaming and Sony&#x2F;Nintendo, though considering it&#x27;s M$ I say fuck it and force them to open it up anyways. reply nradov 9 hours agorootparentBreaking up Microsoft wouldn&#x27;t necessarily produce more proper Office alternatives. And we already have multiple alternatives from Apple, Google, and LibreOffice. Those don&#x27;t have all the same features but they&#x27;re good enough for basic tasks. reply GeekyBear 5 hours agorootparentIf you accept the argument that companies \"with a commanding share of the market\" should be forced to open up their products, then you can emulate the antitrust remedy the EU put into place for Microsoft in the Windows client&#x2F;server market (for example, products like Novell Netware or Samba).Microsoft was required to open up the algorithms and APIs required to become a Windows Server, Domain Controller, file server, print server, group policy object Server... everything.That&#x27;s how Samba on Linux became a completely compatible Windows domain controller. Microsoft was required to share all the information and APIs Samba required.Do the same thing with Office software. Don&#x27;t just claim the file formats are open, force the release of the algorithms needed to interpret the data in those files to allow opening modifying and saving changes in a completely compatible manner.> Microsoft has given up its nine-year fight against antitrust regulators in Europe, saying yesterday that it would not challenge a court judgment from last month and would share technical information with rivals on terms the software giant had long resisted.European regulators and some software groups in Europe hailed the deal as a breakthrough that should open the door to freer competition, especially in the market for the server software that powers corporate data centers and the Internet.https:&#x2F;&#x2F;www.nytimes.com&#x2F;2007&#x2F;10&#x2F;23&#x2F;technology&#x2F;23soft.html reply nradov 4 hours agorootparentWhat specifically do you think is missing in the file format documentation? Several competing products have already implemented it. The algorithms seem pretty simple, at least much simpler than emulating a domain controller.https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Office_Open_XML?wprov=sfla1 reply throwaway2037 8 hours agorootparentprevWhat alternatives for MS Office exist from Apple? Also, I will get lots of hate here: If you are a regular MS Office user, LibreOffice is just terrible. Try to use it for a few weeks... ugh. reply insane_dreamer 4 hours agorootparentI much prefer Apple Pages, Numbers, Keynote to MS Office -- even after years of first using MS Office and being highly fluent in it reply nradov 8 hours agorootparentprevApple gives away Pages, Numbers, and Keynote as free MacOS applications. As for LibreOffice being \"terrible\", that may be true for some users but it hardly implies that Microsoft has a monopoly on productivity software. reply bsder 7 hours agorootparentprev> Breaking up Microsoft wouldn&#x27;t necessarily produce more proper Office alternatives.Oh, it absolutely would.Break Microsoft into the \"Office Apps\" and \"Office EMail\" and \"Public Email\" pieces and prevent them from talking to one another by backdoor APIs. You will get all manner of competitors popping up once the APIs required to do what is needed are all public.The biggest problem in competing with Microsoft is the integration. If you force Microsoft, itself, to have to use the same public integration as everybody else, screaming will ensue. reply nradov 6 hours agorootparentWhich backdoor APIs are you referring to? I have done extensive Office scripting and everything you can do through the UI can also be done through the public COM API. I&#x27;ve never found anything missing. reply 0xFF0123 11 hours agorootparentprevI would understand the argument that the office doc file format should be public and understandable, but I don&#x27;t see an argument that the program itself should be open. reply GeekyBear 11 hours agorootparentI believe the argument that grandparent comment made was that \"once your product has a commanding share of the market, it should be illegal, as it&#x27;s clearly anticompetitive by that point\" reply thfuran 9 hours agorootparentMicrosoft Office so obviously is not like the only way to install software on a computer that I can&#x27;t help but think this isn&#x27;t entirely good faith. reply nradov 9 hours agorootparentprevThe Office document file format is public and understandable. Many other products have implemented the standard.https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Office_Open_XML?wprov=sfla1 reply jonhohle 10 hours agorootparentprevAt what level should that commanding share be? Microsoft wasn’t found guilty of anti-competitive behavior because Windows was on 98% of PCs sold, it was because they used that position to ensure that no other businesses could compete. A company owning their product end to end isn’t an issue. It’s using that position to manipulate pricing in a market (the definition of anti-competitive behavior). reply throwaway2037 8 hours agorootparent> At what level should that commanding share be?Whatever the courts in each jurisdiction decide. Also, actions by the corporations are considered during the judgement. reply madeofpalk 11 hours agoparentprevThe jury found that Google created a market of Android app distribution, and then they squashed competition in that market.No just market exists for Apple (it&#x27;s an entirely closed and self-contained ecosystem) so there was no need to &#x27;squash competition&#x27; - it just doesn&#x27;t exist!https:&#x2F;&#x2F;www.theverge.com&#x2F;24003500&#x2F;epic-v-google-loss-apple-w... reply jahewson 14 hours agoparentprev> enable \"developer mode\" without paying a subscription fee just to run your own app on your own deviceApple actually doesn’t charge a fee for this. You can build an app in Xcode and install it on your own device. You can’t distribute that app publicly though.> G execs were in the position of having to buy off peopleThere’s your answer - having a monopoly is not problem, abusing it is. reply AnthonyMouse 13 hours agorootparent> There’s your answer - having a monopoly is not problem, abusing it is.But tying (e.g. of an app store to a platform) is classic monopoly abuse. reply peyton 12 hours agorootparentTying means forcing people to buy an undesired good when they buy a desired good. Is anybody buying an iPhone who doesn’t want the App Store? reply AnthonyMouse 12 hours agorootparentOf course there are. There are people who like the hardware, or the OS, or don&#x27;t want a green bubble, but would be happy to have F-Droid or Steam on iOS, or get a discount on existing apps by buying them through someone who charges a lower fee. Or just have the competition to Apple&#x27;s store which could cause Apple to charge a lower fee themselves. reply askonomm 7 hours agorootparentAs if Apple was some government provided service that you are entitled to because you pay taxes. It&#x27;s not. It&#x27;s a for-profit company of which you are not a board member. If you want F-droid, use Android. If you want App Store, use iPhone. If you want both at the same time, well, buy both phones then. People who want a Ferrari and a minivan don&#x27;t ask Ferrari to make minivans, they buy both, because both together is ridiculous. reply summerlight 11 hours agorootparentprevhttps:&#x2F;&#x2F;www.justice.gov&#x2F;archives&#x2F;atr&#x2F;competition-and-monopol...Your interpretation on tying is not well aligned with usual legal interpretation. Unless the App Store is considered not to be a separate product (which had been Apple&#x27;s argument for a long time on App Store and Safari by designating them as \"system services of iOS\"), this is clearly tying. The question is if Apple&#x27;s current practice is illegal tying or not. Whether the customer wants it or not is not important here; you can give them freely to gain market dominance then reap profits later on whenever the competitors are all gone. reply TheCycoONE 6 hours agorootparentOn that note, how is Microsoft getting away with it with respect to the office apps which use to be available independently but now can only be purchased in bundles; adding Teams on top of that? reply ElectroNomad 14 hours agorootparentprevIt only works for around 10 days… reply jahewson 14 hours agorootparent1 year if you have a developer account, otherwise yeah 7 days? reply bpye 13 hours agorootparentThere are also entitlements you still can’t use and a 3 app limit with a free account, at the very least. reply jahewson 12 hours agorootparentThat’s true! reply mmanfrin 12 hours agorootparentprev> having a monopoly is not problem, abusing it is.Having a monopoly should be case enough. reply catach 8 hours agorootparentMaybe, but that&#x27;s not US law. reply gchamonlive 12 hours agoparentprevGoogle couldn&#x27;t have been restrictive from the start because of how Android came and solidified itself. Google tapped and profitted heavily on opensource, whereas apple had not only their OS but all the hardware developed in-house, without external collaboration for the most part.The scenario surrounding iOS history lends itself pretty well for solid walled gardens reply nicce 12 hours agoparentprev> If they&#x27;d been draconian and hyper-controlling from the start, refusing side-loading and similar like Apple does, they wouldn&#x27;t have had to pay people off and make deals to crush competition as that competition couldn&#x27;t have even gotten off the ground in the first place.Difference here is that Apple manufactures and controls all the devices but Google does not.When Google’s decisions impact other manufactures or they even are dependent on it, it becomes monopoly problem. But Apple does not impact anybody else. reply orenlindsey 11 hours agoparentprevI agree, Google is wayyyy less monopolistic than Apple. And it&#x27;s absolutely hilarious that Google lost while Apple won. reply dilawar 7 hours agoparentprevI read that apple case was heard by a judge and Google case by a jury. Apparently a jury tends to be less rigorous at interpretation of laws than a judge?! Not surprising (I saw the movie 8 (or 12) angry men). reply SllX 3 hours agoparentprevI mean you said it: Google sold Android has one thing but secretly stifled it from actually being that thing. Apple doesn&#x27;t even offer iOS as a product, it&#x27;s an integrated product component developed in-house just like their A-series chips. They don&#x27;t offer the App Store as a product, it&#x27;s an integrated product-component just like the Camera.Apple took the console approach and Google took the Wintel-ish approach, and that led to a different series of business decisions that created a different set of market conditions involving a different set of business partners that created a different set of facts and a different set of case records when Epic sued them both. It&#x27;s not like Apple didn&#x27;t put anything in writing, but the stuff that made them look bad only made them look bad in the PR sense, not a legal sense. Apple&#x27;s restrictions on the iPhone are technologically and contractually enforced through a standard agreement that every developer agrees to, whereas Android doesn&#x27;t have any technological restrictions, just Google&#x27;s lawyers going around paying off would-be competitors to not compete with Google Play which is a huge difference given that Android is supposedly open and that was one of its original selling points. Personally I still don&#x27;t think Google should have lost their case with Epic at the District level, maybe hammered a bit under State antitrust law enforcement for the payoffs to not compete with them, but not lost to Epic; but they did lose their case at the District level to Epic so that&#x27;s completely on them. reply shwouchk 8 hours agoparentprevThe difference is that apple is and always was, explicitly a closed platform. Take it or leave it.Google on the other hand, tried to market android as an open platform with eg many OEMs producing hardware for it, but the reality is that they are anything but. reply matheusmoreira 8 hours agoparentprev> enable \"developer mode\" without paying a subscription fee just to run your own app on your own deviceThat&#x27;s a great way to fail hardware attestation. It makes my bank&#x27;s app assume I&#x27;m a fraudster and refuse me service.Google absolutely deserves to be lumped in with the likes of Apple because of stuff like this. They sell people \"open\" systems and then they punish them when they \"tamper\" with the system.I can only hope some government out there will put an end to their little digital fiefdoms. reply jongjong 4 hours agoparentprevI don&#x27;t understand why anyone uses Apple. I feel like they&#x27;ve all been brainwashed. Ubuntu is way better, especially for tech-savvy users.Only times I used Apple was at work because I was forced to. I was forced to use Apple by 3 different companies. It&#x27;s essentially a cult. I hated using it and it slowed me down significantly.The tech industry is essentially a giant PsyOp and only brainwashed, highly suggestible people can participate. I think governments should treat these megacorps as what they are; foreign intelligence operations to gain power over citizens. reply toasted-subs 12 hours agoparentprevLet alone you have to pay a subscription fee for Apple. reply TekMol 16 hours agoprevI have a web application with a lot of users. My users are happy to use the web. But because of better monetization options, I sometimes dabble with the idea to build a native mobile app.Some years ago, I tried to build an Android app. It required an insane amount of tooling. Hundreds and hundreds of megabytes of stuff. GUI applications you have to use etc. I didn&#x27;t even try to build an iOS app because that probably means you have to own a mac.Is this still the same?Or are there some linux command line tools these days I can use to convert a web app into an app that I can put on the Android&#x2F;iOS app stores? reply whstl 16 hours agoparentThere are things like Cordova that make it a bit easier, but yes you need hundreds of MBs of stuff to compile and test. Debugging was a bit of a nightmare, though.It is also possible to deploy to Apple with things like Github Actions without personally owning a Mac (and it can publish to Google too, naturally), but then testing is not trivial.I know 90% of HN will disagree but there is a market opportunity here to make this better. reply stouset 14 hours agorootparentI’ll be honest, if you don’t own or regularly use a Mac or an iPhone, the odds that you are going to make compelling software for either of those platforms is effectively zero.The web and app stores are littered with the corpses of failed, poorly-ported iOS and macOS utilities written by developers who didn’t fully understand that those systems have their own design language, cultural norms, and feature sets. They chew through battery, perform poorly, confuse users, look like shit, and feel completely alien.Should that totally stop you from porting some useful tool? Maybe not. But the chance that it will see any sort of use outside of an extremely niche set of users is slim and it’s worth accepting that upfront if you’re going to spend your time and effort on it. reply vladvasiliu 14 hours agorootparentWhile I agree that the app will likely feel off, that doesn&#x27;t mean it won&#x27;t work, depending on what it does.I have an iPhone, and even though the number of non-default apps I use is quite small, I get the feeling that every other app is just a web view wrapper or whatever that just doesn&#x27;t care about iOS standards. The examples that come to mind are: Uber, Teams and Philips Hue. The latter is a laggy mess for some reason, that behaves the same on an iphone 14 pro as on a 7. Teams also is a shitshow, but who does that actually surprise? reply klausa 3 hours agorootparentThis is very funny&#x2F;sad because Uber has one of the biggest iOS teams in the world. reply bitzun 12 hours agorootparentprevI hear this refrain a lot, but the reasons I&#x27;ve gotten for native apps vs webapps were not very compelling to me. I&#x27;d be interested in a detailed comparison of some real examples of native and web (or react native-like or webview wrapper) apps and the practical differences (excluding obvious ones like hardware access.) reply vladvasiliu 1 hour agorootparentOh, I&#x27;m not saying it can&#x27;t be done, and if it could, I&#x27;d be happy.It of course depends on what somebody deems \"compelling\". What comes to mind is, mostly, behavior that feels off when any sort of animation is involved. Mostly the \"rubber-band effect\" when scrolling past the first &#x2F; last element of a list as well as the scrolling action itself. This is surprising to me, since I&#x27;d expect ios&#x27; web view, being safari, to behave the right way. But I wouldn&#x27;t be surprised for these sites to hijack scrolling, as many sites do on the desktop.Then there are the expected gestures which don&#x27;t always work. Take the uber app and the \"back\" gesture: they sometimes do, but sometimes they don&#x27;t, even when there&#x27;s a back arrow present. And when they do, they feel off. The current image doesn&#x27;t begin sliding smoothly from the edge, it waits a bit and then it \"jumps\" to some distance. Also, in Uber, for some reason, just scrolling up and down for example in my past trips stutters. Yeah, I know it loads those as it goes, but even after the list is populated, it keeps stuttering. After a while, if I stay on that page, it seems to be smooth, though.Then there&#x27;s the typography, which is often weird.Now I guess you could always have those issues even with full-on native apps, especially the last point. And I doubt it&#x27;s impossible to go out of your way to mess up scrolling, as some devs do that for the browser.But mostly, native apps behave a certain way which some people may come to expect from their devices. To me, it&#x27;s compelling enough that, given the choice, I&#x27;ll pick the \"native\" app over the weird one. I also don&#x27;t care the least bit about \"brand experience\" or whatever it&#x27;s called, so not having your weird font is actually a feature for me. I also only have an iphone, so don&#x27;t care about the app behaving the same on multiple platforms (though I can see the value in that for support).Also, I don&#x27;t care that the app is actually native, what I care about is its behavior. So, if an app is able to have the same behaviors while actually being a bunch of JS inside a web view, I don&#x27;t really have a problem with that. reply austinprete 13 hours agorootparentprevHuh, surprised by this because the Hue app runs great for me, don’t think I’ve ever seen it lag as a daily user for a couple years. Honestly kind of blown away but how well it (and the Hue platform as a whole) works coming from a company that I don’t usually associate with tech.Goes to show that experience doesn’t always generalize, and I’m curious what’s different in our cases to cause that (since I’m also using on a iPhone 14 Pro). reply vladvasiliu 13 hours agorootparentI&#x27;m generally very happy with how my Hue setup works, but for some reason the Hue app sometimes lags on the simpler screens, like the settings tab and configuring the remotes&#x27; buttons. The Home tab where you set the scenes &#x2F; brightness and such usually works fine. reply pests 12 hours agorootparent> sometimes lags on the simpler screens, like the settings tab and configuring the remotes&#x27; buttonsThat&#x27;s because there is no cloud storage or place to store those settings other than the devices (or the bridge) themselves.You are not waiting on the app. You are waiting on a request and response to the device over Zigbee to save the settings, which is not an instant action.Maybe they could just close the screen and finish the work in the background, but I like knowing it completed successfully. reply type0 11 hours agorootparentprev> Teams also is a shitshowI heard it works wonderful if you use Microsoft Surface Duo devices reply derefr 13 hours agorootparentprevWhat is \"compelling software\"?Say there exists a single-page, single-view form-based web utility (e.g. a tax calculator for some niche.) Does the job. Efficient UX. Solves a need rather than a want, so it doesn&#x27;t need to be pretty.Only problem is that it&#x27;s hosted on some old website, meaning:1. you have to bookmark it (and the bookmark probably doesn&#x27;t come with a nice icon, and you could lose track of it in the future because it&#x27;s not part of some \"store download history\" list);2. you can&#x27;t access the calculator when offline;3. the site&#x27;s server could go down tomorrow, and then \"your\" calculator wouldn&#x27;t be there any more.AFAICT, from the perspective of a mobile user, the only thing that could possibly be improved about this experience, would be the very fact of it being hosted on the web. As long as it&#x27;s made into a client-local installable package, registered in an app store with a name and icon, all the above problems are solved. And you get those key advantages, just by saving the webpage .html to a file, wrapping it in Cordova or whatever, and submitting that as your app.(You also get almost these advantages using Progressive Web Apps that use offline capabilities — all except for discoverability and esp. re-discoverability through a unified app-store UX. If the app stores allowed the submission of PWAs, such that they appeared in the store listings and download history right alongside native apps, I think a lot of use-cases for Cordova et al would be moot.) reply newaccount74 12 hours agorootparentIn theory I agree with you.In practice, poorly ported web apps often fail in various ways. Things like content covered by the keyboard, or the layout breaking when you rotate the device, or random other stuff that results from a lack of testing. It&#x27;s what happens when developers don&#x27;t use the device they&#x27;re targeting. reply stouset 1 hour agorootparentprevI&#x27;ll be more descriptive:Applications (not games) developed and published for macOS and iOS by lone developers with little to no experience on those platforms is overwhelmingly unlikely to be used by enough people to reasonably justify its development.There are exceptions to this for specific niche use-cases. There are situations where this has not ended up being true. There are times where one might want to write such an application to scratch their own itches with no real need or desire to gain a critical mass of users. None of these invalidates my general point. reply type0 11 hours agorootparentprev> What is \"compelling software\"?\"Compelling\" software in the one you are compelled to use, for example you go to a techy-utopia-restaurant and want to order order your food, but then are compelled to install their app because their system is automated &#x2F;s reply Jensson 14 hours agorootparentprevGames are the main appstore revenue driver, and I don&#x27;t think that game ports have much to do with iOs standards. It isn&#x27;t like Fortnite or Genshin Impact on iPhone is significantly different than on Android. reply mmebane 12 hours agorootparentGenshin Impact is (potentially) an interesting case - the iOS version has supported game controllers for almost 3 years now, but there&#x27;s been no hint of support coming for Android. There&#x27;s definitely some suspicion in the Genshin community that Apple has an understanding or agreement with Hoyoverse to keep iOS the premier mobile platform. reply throwaway2037 8 hours agorootparentWould it be legal under US law for Apple to make payments to Genshin to keep iOS as the premier mobile platform? Related: Similar question if Sony wanted to guarantee Final Fantasy was always better on their platforms. reply stouset 14 hours agorootparentprevNo, but it doesn’t sound like GP was talking about games. Yes, games are a completely different animal. reply whstl 14 hours agorootparentTrue, but there is also a business opportunity for dramatically simplifying game deployment, not only web apps.Unity has tools that can partially help with that but their execution also sucks. You still need some half-working script downloaded from Github or some random site to deploy to Apple&#x2F;Google if you don&#x27;t want the hassle of using the default tools.It&#x27;s the same for consoles. It&#x27;s an ungodly pain in the ass even with Unity. Godot makes some of its money by porting to consoles, for example. But I don&#x27;t know if there&#x27;s any money there for a scalable solution. reply lotsofpulp 14 hours agorootparentprevI assume \"games\" here means gambling? Or something that is not legally gambling, but effectively is. reply numpad0 11 hours agorootparentApple cracked down on traditional gaming on iOS, and as an unforseen side effect, yes, \"mobile gaming\" means suggestive gambling apps that are eating the world. reply HideousKojima 12 hours agorootparentprevIt means video games. And even amongst the most predatory mobile games, not all of them use gambling-adjacent \"gacha\" mechanics, plenty just have mountains of microtransactions. reply whstl 12 hours agorootparentprevI agree in principle, however IMO there is room for much simpler apps that don&#x27;t necessarily follow the UI conventions of the platform and still provide a magical experience. The keyword here being \"simple\".The app of my Bank (Commerzbank) looks the same in both platforms (despite being native, last time I asked), and IMO provides a much better experience than lots of native Apple or Google tools. And IMO a better UI&#x2F;UX too. Same with Monobank from Ukraine, astonishing app that doesn&#x27;t really care about the platform and gets things done in IMO a better way than native-inspired apps.I would prefer for most apps to move into the Commerzbank direction than in the... I don&#x27;t know, Pixelmator? Which is 100% native but still feels very unintuitive sometimes (\"Save as\" for example) despite being my daily pic editor for 15 years... reply Aleksdev 10 hours agorootparentprevSteve jobs once said that great software has 1:50 ratio. Meaning for every 50 poorly designed apps there is 1 very well designed app. He said that in the 90s, I think it’s still true to this day. reply Qwertious 4 hours agorootparentSounds like Sturgeon&#x27;s law. reply jwells89 9 hours agorootparentprevYes, ports where each platform is treated as a checkbox to be ticked are practically always poor, with little to no testing. The dev dumps a cross-compiled binary they may not even be capable of running themselves and that’s the end of it. Sometimes they happen to work passably but that’s not a given. reply staplers 13 hours agorootparentprevnext [–]confuse users, look like shit, and feel completely alien.Developers eyes glaze over when you start talking about users. reply beepbooptheory 11 hours agorootparentprevIsnt this what style guides and, like, profiling are for? Which we should be using anyway right?I don&#x27;t understand this fundamentally epistemological point. I thought in some part my skill as a programmer is being able make things not necessarily for me. Or at least, as a (maybe kind of niche) frontend dev, I don&#x27;t think I would have ever gotten a job without some semblance of that skill.I feel like if I relied on my own experience instead of agreed upon standards I would be much more a designer who makes bad code than a coder who maybe just needs a little design&#x2F;ux direction.ps: Is this not a sufficient resource?https:&#x2F;&#x2F;developer.apple.com&#x2F;design&#x2F;human-interface-guideline... reply stouset 11 hours agorootparentThat is absolutely a useful resource, maybe even a necessary one, but I personally doubt it’s sufficient.It’d be like trying to learn a foreign language entirely off something like Duolingo. At some point the only way for you to communicate like a native is going to be to embed yourself in the language and culture, otherwise you’ll never quite express things like a native. reply wouldbecouldbe 14 hours agorootparentprevI know you can&#x27;t port a web app to it easily. But Expo, wrapper around React Native, does do a great job at handling that. It also comes with a build in ci&#x2F;cd and over the air bug fixes (alternative to codepush)I&#x27;ve set up a custom flow with Fastlane with React Native. Works pretty well, but Major Version, OS and architecture update are a huge pain. reply whstl 13 hours agorootparentInteresting. I haven&#x27;t worked with React Native or Expo, but that sounds exactly like what I was suggesting. reply wouldbecouldbe 13 hours agorootparentIt&#x27;s YC funded: https:&#x2F;&#x2F;www.ycombinator.com&#x2F;companies&#x2F;expo reply a1o 14 hours agorootparentprev> Debugging was a bit of a nightmareDebug in Android Studio, connect phone on USB, enable USB debugging, hit play buttonDebug in Xcode, connect iphone (wireless), hit play button reply whstl 14 hours agorootparentExactly. You need Android Studio or Xcode. Fucking pain in the ass. reply noarchy 13 hours agorootparentYou don&#x27;t need them, but they do make things easier for a lot of people. With Android, at least, you can do plenty from the command line if that&#x27;s your jam. reply whstl 13 hours agorootparentMy problem with them is not so much that they are GUI tools. It&#x27;s more that they are bloated, low-quality and a bit unpredictable. IMO and IME, of course. They do get the job done, and people can get used to them if they use daily. But if using them is not your daily job (and Stockholm Syndrome hasn&#x27;t set in), they make for a terrible developer experience. They take a lot of time to setup and there&#x27;s often various problems with versioning, for example. All IMO and IME, of course.I used to work in a Cordova&#x2F;PhoneGap&#x2F;Ionic&#x2F;[whatever the name is today] app I had to make those bi-monthly excursions to the codebase that would always take a couple days because of Android Studio or Xcode. Setting the tooling in a new computer or teaching this to a new developer would require a lot of fiddling with version for half a day or more until it worked properly.Sure, if you work on it everyday it doesn&#x27;t suck, but working with multiple apps or working with different things was always a terrible experience. reply yoz-y 13 hours agorootparentI only used Android Studio when it was in beta so can&#x27;t say much about that. But XCode is honestly quite good, not perfect by any means, but especially with Swift and SwiftUI it has some really good features (to wit: live previews).Provisioning and testing purchases is always a mess, but that&#x27;s mostly because the code world meets politics there. reply whstl 11 hours agorootparentAgain, they&#x27;re fine for development as a daily driver. For casual use (occasional maintenance&#x2F;debugging, publishing), not so much. reply sahila 7 hours agorootparentBut that sounds like a function of the complexity involved with mobile development and those tools being a do it everything tool for their development (with Android especially, all the different os versions you can target, the different form factors). What would you like for casual use? replypelagicAustral 11 hours agorootparentprevfor a small window of time, there was an alternative... https:&#x2F;&#x2F;creolabs.com&#x2F; but this is gone now. reply tadfisher 16 hours agoparentprevGoogle has a CLI tool for producing an APK bundle: https:&#x2F;&#x2F;github.com&#x2F;GoogleChromeLabs&#x2F;bubblewrapTutorial here: https:&#x2F;&#x2F;developers.google.com&#x2F;codelabs&#x2F;pwa-in-play reply mksybr 16 hours agoparentprevYou can build on the command line with gradle.I did end up installing Android Studio for the sdk and virtual machine installation, but I&#x27;d assume it could be done on the command line as well. reply elric 15 hours agorootparentI don&#x27;t understand why this was getting downvoted? The parent commenter is right. You can build on the command line with gradle. It will still download hundreds of megabytes worth of dependencies (the Android SDK etc). But at least you don&#x27;t need any GUI tools. reply dmazzoni 10 hours agorootparentIt&#x27;s an SDK for an entire OS, plus the emulator (basically the whole OS in a VM), developer tools and more. It&#x27;s not much different in size than the SDKs for macOS or Windows and it&#x27;s much more cohesive. reply jjnoakes 16 hours agorootparentprevI&#x27;ve downloaded just the command-line tools before and used &#x27;sdkmanager&#x27; to list and download sdk versions and virtual machines, so it is definitely doable without Android Studio, although it isn&#x27;t obvious (or at least it wasn&#x27;t to me). reply mathiasgredal 13 hours agorootparentprevI have done this, but for some reason Android SDK has to be weird, so you have to download the SDK seperately and then create a properties file in to root of the project with the path to the Android SDK. Everyone then also has to have their own version of this file, since the path is likely different. You also have to make sure that everyone downloads the same version of the SDK. (also the path to the SDK cannot have any spaces)Why can it not be like other Gradle dependencies, where Gradle will just download the files automatically? reply anordal 14 hours agoparentprevThe irony is that most people who think they want an app would not see the difference between that and a shortcut to your webpage. reply mouzogu 15 hours agoparentprevjust porting a basic chrome web extension, like 2 js files to safari requires something like 10 GB of Xcode downloads and various other crap.i&#x27;m not doing that. reply holoduke 14 hours agorootparentWhy not if I may ask? Vim only user? Its possible to build ios and android apps with your own build tools. Its a lot true. reply mouzogu 3 hours agorootparenti believe apple intentionally makes it difficult. especially in the case of safari web apps because of their app store monopolies.so i just dont want to support their web browser or their platform in general. as a personal choice.i build some tools for work colleagues using web extensions and i refused to port it to safari. someone else can take that task if they wish. reply freedomben 15 hours agoparentprevWhat do you use for front end for your web app? If you use React or Vue or something that does client-side rendering, you can often turn your app into a PWA fairly trivially by just adding a manifest. That is IMHO definitely the way to go as long as you don&#x27;t need to use native functionality&#x2F;APIs.PWAs are still a little tougher on Apple since Apple holds the reins to their platform very tightly and doesn&#x27;t want apps getting to users without going through \"curation,\" so if iOS is an important market for you and your users will find you through the app store (rather than looking for you in the app store after finding you elsewhere), then a PWA may not be the best choice.If you use server-side rendering, then it will of course be more work, but I&#x27;d still probably go the PWA route and write it in React or Vue. You already know JS so there&#x27;s much less learning, and it&#x27;s the most \"write once run anywhere\" that there is. You&#x27;ll likely have to buy a mac though, although there are services you can \"rent\" one for building&#x2F;signing&#x2F;submitting to Apple.React Native can be a good option as well, especially if you need to call native APIs or must be in the Apple store (Google Play Store can take you as a PWA). Most of your code can be js&#x2F;ts so less learning curve, and you can generate a submittable app package that can go in the Apple store (and of course Google).If you need to make extensive use of native APIs though, then a real native app may be better, though of course you will need a separate one for ios and android, and there&#x27;s a lot of learning to do. And you&#x27;ll definitely have to buy a mac.tldr: a PWA is (probably) the way to go reply stronglikedan 14 hours agorootparent> But because of better monetization options, I sometimes dabble with the idea to build a native mobile app.Likely not in this case. reply Alifatisk 16 hours agoparentprevNot if you use Flutter reply MajimasEyepatch 14 hours agorootparentFlutter is pretty nice, but I have a really hard time trusting Google to continue supporting it. reply bsder 7 hours agorootparentOne of the Flutter primary devs just left Google and is still working on Flutter at his new gig.That&#x27;s a good sign that it&#x27;s getting some momentum outside fo Google.(I agree with you, though. Sadly, the Flutter management team seems to be where Google sends shitty managers for punishment.) reply smallnix 15 hours agorootparentprevDoes anyone use that in production for large B2C applications? reply surajrmal 14 hours agorootparenthttps:&#x2F;&#x2F;flutter.dev&#x2F;showcase has many high profile examples reply Alifatisk 11 hours agorootparentYeah this answers the question pretty much reply maelito 11 hours agorootparentprevThe French railway ticket website, sncf-connect.com is in Flutter.Largest e-commerce service in France. reply quaintdev 15 hours agorootparentprevGoogle pay is written in Flutter reply layer8 16 hours agoparentprevIt’s still the same. You can rent a Mac in the cloud for iOS development though. reply eomgames 16 hours agoparentprevI&#x27;ve had success with react native for deploying web type apps onto both ios and android. Expo really flattens the learning curve, it&#x27;s something to grow out of for sure. I look at it like having an app vs wanting to have an app. reply holoduke 14 hours agoparentprevTry to create a app like behavior in javascript and use a webview in android and ios to wrap your app. We do it like that. You will still have some native parts like push notifications, ads, social logins etc. But your ui render is web. Just make sure you have an app like experience. Doable these days. reply rmbyrro 12 hours agoparentprevPerhaps a progressive web app will work for you reply CamperBob2 13 hours agoparentprevI tried to build an Android app. It required an insane amount of tooling. Hundreds and hundreds of megabytes of stuff. GUI applications you have to use etc.FPGA developers snicker under their breath, but if you look closely you can see the tears welling up in their eyes... reply beretguy 13 hours agoparentprevLook into PWA. reply etchalon 16 hours agoparentprevGoogle has Bubblwrap, which will take any PWA and create an Android wrapper for you: https:&#x2F;&#x2F;developers.google.com&#x2F;codelabs&#x2F;pwa-in-play#0There are tools like that for iOS too, but you absolutely have to have a Mac. reply ederamen 4 hours agoparentprevYes. reply charcircuit 16 hours agoparentprev>It required an insane amount of tooling.The all the extra tooling makes it easier to make Android apps. Nothing is stopping you from downloading the Java JDK and Android SDK and running javac, d8, aapt2, zipalign, and apksigner yourself. reply wiseowise 15 hours agorootparent> Nothing is stopping you from downloading the Java JDK and Android SDK and running javac, d8, aapt2, zipalign, and apksigner yourself.That is the tooling that OP is talking about. reply charcircuit 15 hours agorootparentIt sounded to me that they thought they had to use Android Studio as they were complaining about a GUI. reply izacus 14 hours agoparentprevSo the size of the tooling was about the same size as the your web app pushes on every user?The horror. reply AC_8675309 16 hours agoprevGreat, but don&#x27;t forget to open up the Nintendo eShop and the PlayStation store as well. reply johnnyanmac 16 hours agoparentI think they aren&#x27;t targets due to1) not being a general purpose OS. Sony actually took at away that ability in the PS3 so they aren&#x27;t trying to pretend they do more than play media2) the hardware and software is ephemeral. In 10 years IOS and Android will exist. We will likely be on the PS6 and 2 more generations of Nintendo in that time. There&#x27;s less incentive to bother opening up an OS that is abandoned every generation.3) due to the model of consoles, most of them lose money on sales so they can invoke more software sales. And on top of that, larger studios get direct support from Nintendo&#x2F;Sony. There is negative incentive for a studio to ruin this relationship unless more companies start making consoles themselves. reply AnthonyMouse 15 hours agorootparent> not being a general purpose OS. Sony actually took at away that ability in the PS3 so they aren&#x27;t trying to pretend they do more than play mediaThis is just assuming the conclusion. They&#x27;re general purpose computers that could run arbitrary custom code if their owners weren&#x27;t locked out of them.And so are appliances and HVAC systems and so on, which is exactly why the owners shouldn&#x27;t be locked out of them -- this has significant implications for the entire concept of ownership, right to repair and environmentalism etc. They&#x27;re all general purpose computers, and they should be.> the hardware and software is ephemeral. In 10 years IOS and Android will exist. We will likely be on the PS6 and 2 more generations of Nintendo in that time. There&#x27;s less incentive to bother opening up an OS that is abandoned every generation.But this is making exactly the opposite argument -- it should be opened up because otherwise it will be abandoned and no one else can support it. Likewise, the newer system should be opened up so people can make it run the older games, or the games from other systems from other vendors, whenever possible.> due to the model of consoles, most of them lose money on sales so they can invoke more software sales. And on top of that, larger studios get direct support from Nintendo&#x2F;Sony. There is negative incentive for a studio to ruin this relationship unless more companies start making consoles themselves.This is called a predatory business model, the equivalent of printer makers selling the printer below cost so they can stick you for the ink. There is a serious argument for banning it outright; it&#x27;s certainly nothing we need to worry about protecting. reply johnnyanmac 10 hours agorootparentI don&#x27;t disagree with your arguments, but this story doesn&#x27;t seem to be about forcing an os nor even store to open up. It&#x27;s more about the 30% revenue sharing and how these stores can&#x27;t force studios to submit to it if they are interested in using alternative vendors. That&#x27;s why Google is involved even if you can technically install a dozen other stores (or sideload everything).This won&#x27;t really benefit anyone but the largest studios, even if they open it up to game consoles. But it could be a first step. But a step with a lot less support. reply AnthonyMouse 9 hours agorootparentNobody really cares what one company allows in their store if there is a competitive market for alternate stores with low barriers to entry.Allowing competing stores is a major component of opening up the OS. The competing stores would be able to install and run code on the device, with the permission of the owner rather than the OEM. And suffocating alternatives to paying them the vig is a major reason they close the device to begin with, so without that they&#x27;d be less likely to stand in your way anymore. reply johnnyanmac 8 hours agorootparent>if there is a competitive market for alternate stores with low barriers to entry.But that&#x27;s what the story is about, even with Google:>And although Google permits third-party app distribution platforms, it still generally requires apps to use its billing system.>These effective monopolies on in-app payments can lead to users paying more for the same content or services on mobile devices than on personal computers.Alternate app stores and Android essentially being an \"open OS\" (with forced Google installation on many devices) outside the store wasn&#x27;t enough to protect Google from being involved. And the focus of the rationale is focused on the app purchase and their control over suspected price hikes. reply ksec 13 hours agorootparentprevBy that definition every single model that includes software will need to be opened up. reply AnthonyMouse 12 hours agorootparent> By that definition every single model that includes software will need to be opened up.Sounds good. reply smoldesu 11 hours agorootparentI agree, good luck convincing the FCC. reply AnthonyMouse 9 hours agorootparentFCC commissioners are political appointments. If they&#x27;re reasonable they can be convinced, if they&#x27;re unreasonable they can be replaced. replylambda_lord 15 hours agorootparentprevYour second point is why the stores need to be opened up.Nintendo breaks compatibility almost every generation, so if you want to replay old games you already purchased on a previous console, you have to repurchase the ported versions or buy Nintendo’s subscription service. I’ve dropped hundreds in the eShop but worry I’ll lose access one day, when the Switch is EOL.In comparison, I’ve been able to run my Steam games on multiple devices through the years because PC is a much more open platform. There are multiple shops, so Steam has incentive to keep games forward compatible. reply ksec 13 hours agorootparent>so if you want to replay old games you already purchased on a previous console,I assume you can turn on your old console and replay those games right?Why does the next generation console has to guarantee to work with older games? reply filoleg 15 hours agorootparentprevThat seems like a very uniquely Nintendo problem rather than a modern console problem.Yeah, PS3 was from the era of consoles where backwards compatibility wasn’t as heavily demanded (given that Steam was in its infancy too), so they went with a notoriously and uniquely overcomplicated making games for it (and, by extension, compatibility). But PS4 era and onwards, any digital purchase you made back then for PS4 is accessible on PS5 as well.And hell, even for PS3 digital purchases it is still kind of true. Unfortunately, PS4&#x2F;5 cannot play PS3 games natively, but if you purchased a digital PS3 game back then, you are able to stream it using PS Remote Play on your PS4&#x2F;5.As far as I am aware, a similar thing happened in the Xbox space as well. Xbox One generation and onwards, any digital purchases you made back then are available on the most recent Xbox consoles. And for older games that aren’t natively compatible (and even a bunch of those that are compatible), they provide streaming too (through xCloud). Though don’t quote me on the exact details about how it works for Xbox consoles, as I haven’t used one since the Xbox360 days.Meanwhile, Nintendo resells SNES era games in their “virtual console” section of Switch eShop at a pretty significant premium. reply fomine3 9 hours agorootparent> PS3 was from the era of consoles where backwards compatibility wasn’t as heavily demandedI don&#x27;t think so for PlayStaion. PS3 and PS2 had compatibility to older PS by implement old chip. I think why there&#x27;s no PS3 compatibility is because PS3 unique Cell architecture was dead end. reply filoleg 12 hours agorootparentprevPast the edit cutoff, so here is an important edit.In the 2nd paragraph, i missed a word and meant to say “[…] they went with a Cell chip, making gamedev experience for it notoriously and uniquely overcomplicated.” reply type0 11 hours agorootparentprev> 1) not being a general purpose OS.I don&#x27;t think iOS and Android are general purpose either. Phones and tablets are used for games as much as the consoles if not more in certain demographic reply johnnyanmac 10 hours agorootparentI believe they are definitely general purpose. You can browse the web, do professional work in several industries (spreadsheets for business, media editing programs, tax software, etc), play soke fairly intensive games, and track all kinds of parts of your life. And then you can dock all this to a monitor and use a KB&M and treat it like a ultrathin workstation.I&#x27;m struggling to think of something you can&#x27;t do on a phone&#x2F;tablet these days. Dual monitor support seems to be beyond most devices I tried it with. Maybe some government applications for security purposes. reply Drew_ 6 hours agorootparentYou can do all of those things on a PS5 or Xbox, Sony and Microsoft just don&#x27;t let developers publish apps that let you do that. reply theshrike79 13 hours agorootparentprevI&#x27;d posit that modern consoles are MORE general computing devices than mobile phones.For example the Apple M-series SOC only exists in Apple phones and tablets.Meanwhile the PS5, Xbox Series S&#x2F;X, Steam Deck all use the AMD Zen 2 series CPUs. It&#x27;s basically off-the shelf hardware with generic well-documented interfaces.The only reason we&#x27;re not using the Xbox as a cheap Linux gaming machine is because it&#x27;s absolutely closed up for all hacking. reply fomine3 9 hours agorootparentAnother discussion: Its hardware is almost a PC, so there&#x27;s less reason to force it open, because we can just buy a PC. reply theshrike79 3 hours agorootparentBut it&#x27;s cheaper to get a console than a PC. That&#x27;s why the PS2 Linux version was so popular.I can get a Series X for less than the price of a decent GPU. During the GPU price boom I actually got a PS5 _and_ a Series X for less :) reply blueboo 15 hours agorootparentprevAgreed, mostly, as Id like to point out that the iOS software is abandoned&#x2F;shut down at approximately the same cadence as new consoles are launched. Rare’s the still-available app that was last updated pre-iOS 10…let alone iOS 5 or pre-retina iOS. reply asylteltine 15 hours agorootparentprevThis switch is only useful when it can be hacked. It’s SO much better when you can run whatever you want to run like emulators and other tooling. Or even crazy things, like backing up your saves! reply gyomu 12 hours agoparentprevSony and Nintendo are Japanese companies. From the article:> Japanese companies would be able to run dedicated game stores on iOS devices, as well as use payment systems with lower fees from Japanese fintech companies.It’s not hard to read between the lines. This is all about letting domestic gaming companies like Nintendo and Sony make more money from those mobile platforms.The techie demographic likes to get lost into arguments about the technology and philosophy of computing platforms, but as far as the EU and Japan are concerned it’s just realpolitik to give their domestic companies a leg up. reply bee_rider 15 hours agoparentprevThey should. But, more people have phones than consoles. It is not even that shocking for a phone to be somebody’s only computing device. It is more important, and governments need to prioritize. reply summerlight 15 hours agoparentprevThose are not big enough to bother about and the power dynamic between the platform and its publishers is more even than those App Store&#x2F;Play Store. Remember, regulation takes lots of resources. reply Razengan 15 hours agorootparentWow that’s a heck of a flimsy excuse, whenever this topic comes up reply crazygringo 15 hours agoparentprevYup, this is what bothers me the most. Either there&#x27;s a principle here behind opening app stores or there isn&#x27;t.If we&#x27;re opening them up, then let&#x27;s open them all up.The idea that video games or stores below a certain mega size should be exempt is absurd. reply smoldesu 13 hours agorootparentConsoles ship at a hardware loss, iPhones don&#x27;t. The business comparison has always been a tough stretch, and the functional comparison of an iPhone to an Xbox&#x2F;Keurig&#x2F;dishwasher has always been absurd. Apple&#x27;s service revenue channel is unprecedented, and so far unchallenged. In cases like Apple Music and the App Store, it is unquestionably at-odds with fair competition. Now, countries like Europe and Japan are using their markets as collateral at the negotiation table. Seems fair to me, given that Apple and Google are comfortable treating their userbases the same way. reply crazygringo 10 hours agorootparent> Consoles ship at a hardware loss, iPhones don&#x27;t.This isn&#x27;t true anymore. Consoles don&#x27;t follow the razor-and-blades business model anymore -- that&#x27;s long since gone.Sometimes they don&#x27;t make a profit at launch, but they do soon after in the hardware cycle, which is fine because they sell for years and years. Nor are things like extra controllers or headsets subsidized either.So the idea that consoles need to make up losses via a cut of game sales isn&#x27;t true, and hasn&#x27;t been true for at least a couple of decades.And so there&#x27;s no good reason to treat them differently from smartphone app stores.(The idea comes from the 80&#x27;s and 90&#x27;s, back when it was widely understood to be the business model.) reply klausa 3 hours agorootparentI will eat a hat if XSX or PS5 made money at launch. reply jeroenhd 2 hours agorootparentprevI don&#x27;t see why selling at a loss would warrant exclusion. If anything, that just grants these companies the power to undercut incentives for competition that would allow fair competition.Apple treats their iPhones like Microsoft and Sony treat their consoles: some kind of brand experience that makes you part of a distinguished group of Brand Name customers who enjoy their Brand Name lifestyle. If we&#x27;re preventing Apple and Google from pulling this bullshit, we should take on consoles too.It doesn&#x27;t really matter what functionality consoles offer or not. These things could run Windows if it weren&#x27;t for the DRM that locks them down, and some Chinese stores will sell you boards using the same CPU&#x2F;GPU but running Windows instead. They have HDMI, USB ports, ethernet, and all the hardware to be used as a serious device. Their OS and APIs are a bit weird but I can&#x27;t see a reason why you couldn&#x27;t used them as a basic home PC if you install a browser and an office suite, other than that their vendors won&#x27;t let you download software outside of their walled gardens. reply codedokode 11 hours agorootparentprev> Consoles ship at a hardware lossFirst, it might not be true (because all console contains is a PCB and several chips). Second, if you sell something cheaper than you could it doesn&#x27;t mean you get some kind of privilege and exception from the law. reply smoldesu 10 hours agorootparentThere are definitely cases where console hardware does sell at a profit, but even late in a console&#x27;s lifecycle it probably won&#x27;t make half the hardware margins of an Apple product at launch. They&#x27;re different markets, and while they do have overlapping jurisdiction it doesn&#x27;t contradict legislation like the EU&#x27;s DMA. reply ksec 13 hours agorootparentprev>If we&#x27;re opening them up, then let&#x27;s open them all up.What will happen to Apple Retail, 7-11, Costco and Walmart? reply Exoristos 11 hours agorootparentprevI&#x27;m pretty sure the principle is favoring Japanese businesses. reply pnw 16 hours agoparentprevAll of the recent legislation on this topic, including the EU Digital Markets Act, has a numerical unit cutoff which basically exempts all video game consoles. reply codedokode 11 hours agorootparentAbsolutely unfair. reply modeless 2 hours agoparentprevAs soon as Switch or Playstation are used by >40% of the adult population for multiple hours a day every day on average, sure! reply Jensson 16 hours agoparentprevThere is an order of magnitude difference in number of devices there, smaller brands that ship an order of magnitude less devices isn&#x27;t a target for legislative action. Legislation might cover them but there is no reason to target them specifically since they are too small to matter. reply CharlesW 16 hours agorootparentWhy would that make any difference, unless we want the government to punish success? And if ~140 million Switch consoles sold doesn’t constitute “success”, where is that line? reply notnullorvoid 15 hours agorootparentIt&#x27;s not a punishment of \"success\", it&#x27;s a restriction on monopolistic control of everyday devices which at this point are a basic utility.Game consoles are not ubiquitous devices, and certainly aren&#x27;t a basic utility of modern day living. reply CharlesW 14 hours agorootparentIf you had to classify the iPhone as either a console or a general-purpose computer, I think the answer is obvious. (For good reason: A relatively safe app store is a feature, not a bug, for average users.)Apple doesn&#x27;t have monopolistic control over anything. The smartphone marketplace is full of competition, so consumers have lots of choices in every segment. reply yjftsjthsd-h 13 hours agorootparent> If you had to classify the iPhone as either a console or a general-purpose computer, I think the answer is obvious.Sure, it&#x27;s obviously a computer.> Apple doesn&#x27;t have monopolistic control over anything. The smartphone marketplace is full of competition, so consumers have lots of choices in every segment.How do you figure? If you want a phone, you&#x27;re getting Android or iOS. In practice it is at best a duopoly. reply notnullorvoid 7 hours agorootparentYup it&#x27;s a duopoly when it comes to marketplaces, and there&#x27;s a strong argument to be made that Apple is a monopoly on the phone market as a whole. In many countries iPhones account for 50% or more of the phone market. Where as no company making Android devices controls 50% of the phone market, at least not in the west. reply infotainment 16 hours agorootparentprevIn order to get to a monopoly position, quite a bit of success is generally required. Given that, anti-monopoly laws are by their very nature, punishing “success”.That kind of “success” for an individual company doesn’t necessarily lead to the best outcomes for the customer. reply AnthonyMouse 15 hours agorootparentThis category of law is broadly referred to as competition law, because that&#x27;s what it&#x27;s concerned with.If you had five \"competitors\" but they all conspire to divide up the market between them so they don&#x27;t have to compete with each other, this is problematic in the same way as a monopoly. But that&#x27;s exactly what the game consoles do. Alice has a PlayStation, Bob has an Xbox, Carol has a Switch. If you want to sell your game to Alice, Bob and Carol you can&#x27;t just strike an agreement with one of the console makers because that only allows you to reach a third of the market. You can&#x27;t play Sony and Microsoft against each other to get the lowest fees because you need both of them rather than being able to choose the one offering the best deal.The best argument for why game consoles aren&#x27;t like phones is that a non-trivial number of people have multiple game consoles, whereas hardly anyone carries two phones in their pocket. But even then, there are a large number of people who only have one console -- large enough that game developers still can&#x27;t ignore them -- and few people who have all of them.Likewise, there are companies other than console makers who might like to make a game store, or make one that services all types of devices. Those companies are locked out of the market, even though their presence would be likely to drive down margins.So competition law should be concerned with this, because it&#x27;s limiting competition. reply Jensson 15 hours agorootparent> there are a large number of people who only have one consoleIphone or Android, yeah. Basically nobody only have a Playstation or a Nintendo. reply AnthonyMouse 14 hours agorootparentThere are definitely people who only have one console. Consoles cost money. Some people aren&#x27;t rich. reply Jensson 14 hours agorootparentPhones are game consoles, people who only have one game console today have a phone. reply Larrikin 14 hours agorootparentYou&#x27;re making stuff up. I only have one console because the majority of games outside of Nintendo first party games do not interest me. My cousin has one console because his parents told him he could only get one. My friend only has a PS5 because he does most of his gaming on the computer and wanted a similar experience when he feels like sitting on his couch. My coworker only owns a switch because she grew up wanting to play Mario and Zelda and her parents refused to let her ever get any video or PC games.Not every single person who plays video games is so hardcore about it that they must own the entire generation of consoles. reply Jensson 14 hours agorootparentYou and your friends don&#x27;t have smartphones? If you don&#x27;t consider consoles a different kind of device then you shouldn&#x27;t say you have just one if you have a smartphone as well. reply FridgeSeal 10 hours agorootparentNone of my friends who have PC’s or consoles consider phone (games) a “console” platform worth considering. There’s been no games on phones that have made me consider upgrading or switching. That the phone has games is incidental to its existence, not central. reply AnthonyMouse 14 hours agorootparentprevPhones are power-constrained devices with small screens. Consoles consume >100 watts under load with correspondingly higher performance and are connected to televisions. These are not the same market. They don&#x27;t play the same games.Arguably gaming PCs (i.e. fast PCs with a suitable GPU) are in the same market, but this hasn&#x27;t really added a competitor because Xbox and gaming PCs are both Microsoft, and most people don&#x27;t have gaming PCs either. reply Jensson 14 hours agorootparent> Consoles consume >100 watts under load with correspondingly higher performance and are connected to televisionsNot handheld consoles. And you can connect phones to large screens if you want.> They don&#x27;t play the same games.Genshin impact and fortninte? They could play the same kind of games, they have weaker hardware so graphics wont be there true but they are still devices people play all kinds of games on.> These are not the same marketWhy not? Wasn&#x27;t the original argument that these things are the same and therefore should be regulated the same? How do you differentiate \"gaming console\" from other computers?People will only buy gaming consoles as long as they are better for gaming than phones are, since people already have smartphones. That means that game consoles always face heavy competition from phones and need to stay ahead of them to sell anything at all. reply AnthonyMouse 14 hours agorootparent> Not handheld consoles.Which is why handheld consoles are not in the same market either.> And you can connect phones to large screens if you want.Neither the user interface nor the input method is designed for this.> Genshin impact and fortninte? They could play the same kind of games, they have weaker hardware so graphics wont be there true but they are still devices people play all kinds of games on.There are kinds of games you can&#x27;t play on a phone. Phones aren&#x27;t part of the market for those kinds of games, and you can&#x27;t get out of that by finding some different games that can run on a phone.> Wasn&#x27;t the original argument that these things are the same and therefore should be regulated the same? How do you differentiate \"gaming console\" from other computers?They should be regulated the same because it&#x27;s the same anti-competitive business practice -- tying app distribution to the platform and preventing third party competitors.Far from contradicting the claim, being separate markets is the entire problem -- instead of having a common market for console games or apps in general where anyone can be a distributor for any platform, each platform is segmented into a separate market with only a single distributor.What makes things be in the same market is the ability to substitute them for one another. If you need a wrench and they&#x27;re sold at both Amazon and Walmart, you can substitute one store for the other. But if you need a wrench for your Xbox and only Amazon has wrenches that work on an Xbox whereas Walmart only has wrenches that work on PlayStations, you can&#x27;t get what you need from Walmart anymore so Walmart is out of the market.> People will only buy gaming consoles as long as they are better for gaming than phones are, since people already have smartphones. That means that game consoles always face heavy competition from phones and need to stay ahead of them to sell anything at all.But it&#x27;s trivial for them to do this because they have different design constraints. A phone runs on battery and has to fit in your pocket, so it can&#x27;t use or dissipate >100 watts and it&#x27;s easy to make a console that can which is significantly faster. Then all of the games requiring that level of performance are exclusive to the devices with that level of power consumption. reply smoldesu 13 hours agorootparent> There are kinds of games you can&#x27;t play on a phoneActually, I&#x27;m curious; what are those games?I guess \"GTA 5\" is an acceptable answer. But if you&#x27;re satisfied with 2fps, even that should run through Rosetta&#x2F;Box86 and GPT&#x2F;Proton. Both modern Android and modern iOS devices should have the API coverage to enable DirectX12 via-translation, even if their hardware isn&#x27;t particularly amicable to it.You can play Resident Evil 4 natively on an iPhone. You can play Half Life and Fallout: New Vegas locally on Android. It&#x27;s not really a contradiction of your claim, but I don&#x27;t think anything really stops iPhones and Android phones from providing PC or console-quality game APIs anymore. reply FridgeSeal 10 hours agorootparentApart from being an atrocious experience, in an interface and usage context that is wildly different from the UX “hot oaths” our phones are (rightfully) designed for.I want a different experience when I sit down to my big PC or a PS5. I want to play the games designed for that device and that experience (at the desk, back on the couch). With enough finagling, you can replicate a shallow version of this experience on a phone, but it sucks, the things runs out of power and&#x2F;or gets super hot, and if push comes to shove, the games are the first thing getting ejected from my phone if I need to make space.I can play games on my phone, but the whole experience is nowhere near replacing what I get on, say, Steam. reply AnthonyMouse 12 hours agorootparentprev> But if you&#x27;re satisfied with 2fpsThis is not \"runs\" in a practical sense. It has to be a reasonable substitute for the console.Here&#x27;s the money question: If you&#x27;re the developer of this game, can you reasonably stop selling it for consoles and paying the vig to the console makers by telling people to play it on their phone instead? reply smoldesu 10 hours agorootparent> can you reasonably stop selling it for consoles and paying the vig to the console makers by telling people to play it on their phone instead?Didn&#x27;t people do this a ton? It started with a trickle of poor but sellable ports of stuff like Call of Duty to the Wii and Grand Theft Auto for iPhone. All of that stuff was signed-off by publishers. The ports came through even faster on Nintendo Switch, despite it being both less powerful than most phones and a different architecture from most consoles at the time. Then there&#x27;s even the Steam Deck, which crossed the rubicon of running Windows games without Windows. At no point during any of that history did publishers tell people to stop playing the \"inferior\" ported version. Some might even say they didn&#x27;t care, as long as you bought a full-price copy of the game and enjoyed it.So... yeah. Consoles will exist, and people will port games to them because gamers will buy and own them. But smartphones are simply more popular, and the software pipeline required to get PC games running on the hardware you already own and use exists and is usable today. Console releases haven&#x27;t correlated with quality since forever, the majority of people I know would rather play Fallout: New Vegas on a phone than Fallout: 76 on a PS5. reply AnthonyMouse 10 hours agorootparentOld or low-resource games are a different market, which phones can participate in. But if there wasn&#x27;t a separate market for console games then consoles wouldn&#x27;t exist -- people would just use their phones instead of paying hundreds of dollars for a separate device. That people will pay money for a console when they already have a phone is a simple proof that the phone can&#x27;t replace the console.If consoles were required to be open then they&#x27;d likely merge with PCs, since a console is basically a PC but closed. This is another aspect of the anti-competitive nature of the industry -- if consoles were \"basically a PC\" and open then a non-Microsoft console would be a Linux PC sitting in a hundred million living rooms, which is a threat to Microsoft&#x27;s desktop monopoly, and a major reason they created the Xbox. By subsidizing the sale price of the Xbox and making it back by shaking down game developers, they make an open console an uphill battle because it would have to charge more for hardware and less for games, which reduces initial adoption and the network effect, pressuring its primary competitors to do the same thing and be closed.And then you get game developers targeting PlayStation instead of SteamOS, the latter of which would have made the games also run on any other desktop Linux, and general purpose software developers targeting Windows but not Linux because there aren&#x27;t a hundred million general purpose Linux \"consoles\" in living rooms. reply Rapzid 15 hours agorootparentprevIt&#x27;s punishing success like cutting the grass or trimming the bushes is punishing success. reply Jensson 16 hours agorootparentprevWe want governments to prevent dominant players from cornering the market. Android has billions of active devices, the Switch is a tiny system in comparison so it isn&#x27;t a dominant player.Or are you trying to say that consoles are a separate category of devices and shouldn&#x27;t be compared to number of phones? Then why are you even arguing here, they are separate! So either these consoles are too tiny to be dominant players and therefore doesn&#x27;t need to be regulated like dominant players, or they are a separate category and shouldn&#x27;t be regulated with the same laws as phones. reply CharlesW 14 hours agorootparent> Android has billions of active devices, the Switch is a tiny system in comparison so it isn&#x27;t a dominant player.But it is in console gaming. Even if you include Xbox and PS5, the Switch&#x27;s market share of consoles is far higher than Apple&#x27;s of smartphones. And of course, Switch absolutely dominates if we&#x27;re specifically talking about the handheld game console market.So what I&#x27;m asking is: How is Apple a monopoly in a market where they don&#x27;t dominate and there&#x27;s lots of choice, while Nintendo is not in a market where there are 3 vendors that matter, and they completely dominate the handheld segment? reply Jensson 14 hours agorootparentHow do you define \"console gaming\" that doesn&#x27;t include smart phones? That was the whole point of this sub thread, arguing that consoles are just another general computing device.If you say they are different in a significant way then why would they have the same regulations? replylozenge 10 hours agoparentprevYou can buy multiple consoles. You can&#x27;t practically carry and use multiple phones as they will have different phone numbers. reply averageRoyalty 8 hours agorootparentHave you tried signing into 5 consoles with one PSN&#x2F;XBL login? You&#x27;ll quickly run into issues. reply seanmcdirmid 15 hours agoparentprevIt isn&#x27;t compelling to say that all these competing app stores form a monopoly. You have single someone out or the argument becomes weak. reply gjsman-1000 16 hours agoparentprevNever going to happen. Nobody has any interest in going after the gaming market (1), and the EU DMA was carefully written to not affect game console stores (2).(1) If “phones” are a category in most people’s minds, it’s a two horse race. Most people, however, think of “gaming devices” as the category, not “game consoles” like techies do - in which case, it’s like an eight horse race between PC, PlayStation, Xbox, Nintendo, Steam Deck, smartphones themselves, GeForce Now, etc.Unlike smartphones, where if 2 companies decide to not service you, you’re screwed; you’ve got tons of alternative ways to play, even within most households. Much harder to show anticompetitive interests.(2) One of the provisions of the DMA is that there must be over 10,000 titles for sale. Needless to say, even the prolific Nintendo Switch is under 5,000.Edit: And before anyone objects to me considering PC and game consoles in the same market; think like a lawyer. The very fact that people ask daily, “console or PC?” shows they are in the same market. reply ForkMeOnTinder 16 hours agoprev> And although Google permits third-party app distribution platforms, it still requires apps to use its billing system.Can someone explain this line? If you publish an app on an alternative app store and someone downloads it on their de-googled phone, how in the world would Google prevent it from making a few API calls to Paypal? reply strombofulous 16 hours agoparentThis is incorrect, if you distribute an app outside the play store you do not need to use their payment system, even by the letter of the law. The rule specifically applies to play store apps. It&#x27;s common for developers of more technical apps (like VPN apps) to publish two nearly identical versions - one to the play store that doesn&#x27;t support iap and one to f-droid&#x2F;their website that takes payment via credit card.It&#x27;s possible the people writing this complaint may be referring to the fact that you can&#x27;t link to or reference those options from the play store edition of the app, but I think they might just be misinformed. reply jdiff 16 hours agorootparentThere is a compounding effect of this though, the fact that the Play Store doesn&#x27;t allow this greatly dampens development of libraries that would make it much easier for developers to add this functionality to their apps, making people more likely to rely on the Google&#x27;s payments and just dealing with its cut. reply Dalewyn 15 hours agorootparentI believe it&#x27;s fair:* Unlike Apple App Store on iOS, you are under no obligation to sell on Google Play Store on Android.* If you choose to sell on Google Play Store, it&#x27;s reasonable to \"pay rent\" so to speak.If you don&#x27;t want to accept payments through Google Play Store, you simply don&#x27;t sell through Google Play Store. reply jdiff 14 hours agorootparentYou can believe it&#x27;s fair, what you can&#x27;t do is claim they support aftermarket app stores while they also take actions to stamp them out. You have to acknowledge that the situation Google has created conveniently and heavily discourages aftermarket app stores on multiple levels. You are effectively obligated to publish on the Play Store, and in doing so you face increased maintenance burden for creating non-Play versions. reply Dalewyn 14 hours agorootparentGoogle doesn&#x27;t \"stamp out\" side-loading. They don&#x27;t make it immediately obvious (and they don&#x27;t have to), a user so concerned needs to dive a little into the operating system and permit them, but the option is there for anyone interested.This is in stark contrast to Apple where you may not do anything outside of the One Apple Way(tm), which in this case means you will go through the Apple App Store or pound sand. reply mil22 13 hours agorootparentThey have deliberately and knowingly made it difficult by showing warnings and making users jump through hoops. I think evidence that it was deliberate and intentional was revealed in one of the many lawsuits in the form of meeting notes and reported speech, if I remember right.https:&#x2F;&#x2F;www.theverge.com&#x2F;2023&#x2F;11&#x2F;6&#x2F;23948990&#x2F;and-were-on-to-s... reply jdiff 12 hours agorootparentprevNot stamping out side-loading, although that is heavily impacted by many of the same issues, but stamping out aftermarket software stores.Apps installed from aftermarket stores:- Cannot auto-update themselves, it requires user intervention for every individual app for every individual update.- Cannot update at all if they were initially installed from the Play Store without uninstalling and losing all data in the process.- Require multiple hoops and scary messages for the average user.- Require extra maintenance burdens for the developer who essentially has to maintain two forks of the same application, further complicated by point 2.- The payment issues mentioned upthread.All of this makes aftermarket stores second-class citizens, all the while Google claims it welcomes them with open arms. Aftermarket stores aren&#x27;t the only area where Google does this, either. Plenty of Android-of-yesteryear&#x27;s customizability and openness has atrophied heavily while Google continues to profit off the bitrotting scraps that are left. reply spiderice 13 hours agorootparentprevGoogle knows that they just have to make it inconvenient enough that 99% of people won’t do it (or really even know it’s an option). So yes, if you squint really hard you can kind of make it look like Google is a good guy here. But if we’re talking about how it actually pans out in reality, Google is no better than Apple. replyadmp 15 hours agoparentprevThis appears to be factually incorrect both for apps installed from third-party app stores and from Google Play itself.See \"Alternative billing systems for users\" on https:&#x2F;&#x2F;support.google.com&#x2F;googleplay&#x2F;answer&#x2F;11174377?hl=en-... reply derefr 13 hours agoparentprevI would assume that it means that if you&#x27;re publishing an app on both the Play Store and alternative stores, and your alternative-store versions of the app offer alternative payment methods, then Google will shut down Play Store distribution of your app as punishment for that.Apple (briefly) tried to do something like this previously, where they tried to force apps that offered no free-to-paid conversion through the mobile app, only through the web, to pay the \"Apple tax\" on the subscriptions made through the web, because they were for a backing service that had value for customers almost exclusively due to its use through the mobile app. Nobody was willing to put up with this, though, and they quickly walked it back. reply mmahemoff 15 hours agoparentprevIt’s a commercial&#x2F;legal requirement imposed on developers, not directly enforced through the technology. It comes down to the review process in practice. At some point a human reviewer will need to detect the app is allowing the user to pay with PayPal and therefore block it from distribution.They’ll probably have some technology to help prioritise apps for review if they’re likely to be violating (by scanning the APK statically for PayPal SDKs or running robot scripts to see if they can be presented with a PayPal form). reply spogbiper 15 hours agorootparent> At some point a human reviewer will need to detect the app is allowing the user to pay with PayPal and therefore block it from distribution.But on Android I can just release the .apk or publish to Fdroid app store, etc. I don&#x27;t think Google would be reviewing the app at all. reply mmahemoff 13 hours agorootparentYou can and correct Google won’t review it - that’s why Android shows a warning when you enable sideloading about only using APKs from trusted sources (to my recollection).Those alternative channels typically provide a tiny percentage of active installs compared to Google Play installs, however. reply johnnyanmac 16 hours agoparentprevI took it to be a subtle (but important) grammatical error. I figure it meant \"although you CAN use another store on Google, if you use Google play you need to use it&#x27;s billing system\".But maybe Google has something much more insidious than I expected reply internetter 15 hours agorootparentNo, you are correct reply kmeisthax 12 hours agoprevFor context, the Japanese market is as favorable to Apple as America is. Japan has a lot of very specific features that phones need to support[0] and Apple is usually ahead of Google on implementing them. This is in contrast to the EU where Android dominates because they used to be the only &#x27;cheap&#x27; option.[0] Most notably, Felica, the protocol used by all the contactless payment cards Japan&#x27;s transit systems use. If you&#x27;ve ever been to Japan, you probably have a PASMO or Suica card[1] knocking around somewhere in your drawer. Japanese flipphones have supported that protocol since right when it came out. Apple added it at the same time they added Apple Pay and NFC support in the states.Also, emoji used to be a Japan-only thing that required an NTT or AU SIM until westerners started noticing the funny faces and started writing copy-paste apps to get around the missing keyboard.[1] Suica game but it&#x27;s transit cards instead of watermelons reply fomine3 9 hours agoparent> Japan has a lot of very specific features that phones need to support[0] and Apple is usually ahead of Google on implementing them.It hadn&#x27;t been true until iPhone 7. There were features lacked on iPhone but available on JDM Android phones: Felica, IrDA, TV tuner. Thankfully both supports emoji from early. In 2016, Felica was implemented on iPhone (in far more elegant way than legacy Android Felica). Rest of the features were no longer wanted by people.iPhone had been loved even before iPhone 7 because it works well than many crappy Android phones. Galaxy phones are tend to non-crappy but Korean brand was unpopular in Japan. After iPhone 7, iPhone officially become the national loved phone. reply tehlike 15 hours agoprevI&#x27;m curious how much apple and Google will be allowed to charge for alternative payment methods. In Korea, google and apple (iirc) still can charge 26% of the transaction as their fee, making the change fairly moot (and even counter productive). reply kelthuzad 14 hours agoparentwhen sideloading is finally available I don&#x27;t see how Apple (or Google) would receive any share of developer profits since they can&#x27;t see or control 3rd party payment api calls made by devs reply dagmx 3 hours agorootparentThey could possibly do what Epic do where it’s somewhat honor system and you’re subject to audits if you use their libraries&#x2F;sdks.But I doubt they’d bother. reply clarle 10 hours agorootparentprevThe vast majority of users probably would never side load. The App Store and Play Store is more of a discovery and acquisition channel than anything else. reply tehlike 8 hours agorootparentprevright, when another app store takes off on the platform then their rules would apply.I am mostly thinking the medium term with alternative billing + existing store. reply amadeuspagel 10 hours agorootparentprevSideloading is available now on android. reply codedokode 11 hours agoprevShouldn&#x27;t this apply to Japanese game consoles as well? Why American companies must allow third-party stores, but consoles must not? reply deviantfero 8 hours agoparentComment above said that it&#x27;s because consoles are not general purpose computing devices. reply idle_zealot 8 hours agoprevI wonder how hard Apple is going to fight this. When laws start rolling out and iOS supports 3rd party stores and sideloading, will they region-lock it to just phones purchased in the EU and Japan? Will they tie it to Apple accounts somehow? I guess what I&#x27;m really wondering is what hoops I&#x27;ll have to jump through as a US-located user to be able to sideload on my phone next year. reply octacat 14 hours agoprevWeb platform is already pretty secure&#x2F;privacy oriented (maybe more than the phone apps).But web apps are impossible to install on the phones, because apple&#x2F;google love and push their native apps (love their 30% cut).oh, people would say \"but what is about resources?\"... - many apps are just web-view anyway. reply amadeuspagel 10 hours agoparentWeb apps are easy to install on phones[1].[1]: https:&#x2F;&#x2F;developer.mozilla.org&#x2F;en-US&#x2F;docs&#x2F;Web&#x2F;Progressive_web... reply i5-2520M 12 hours agoparentprevHow are web apps impossible to install? reply octacat 12 hours agorootparentYou have to use browser on mobile to enter them (no way to add an icon for them). + there are some other limitations. There was an article about that.Web platform is pretty capable to be used for app development, it would save a lot of developer hours to not develop for",
    "originSummary": [
      "Japan is planning to introduce regulations that would compel tech giants such as Apple and Google to allow third-party app stores and alternate payment options on their mobile operating systems.",
      "The aim is to address the alleged abuse of their dominant market position in Japan.",
      "The legislation, which will cover app stores and payments, search, browsers, and operating systems, is anticipated to be presented in parliament by 2024."
    ],
    "commentSummary": [
      "Japan is addressing concerns about the app store monopolies of Apple and Google, citing potential antitrust violations.",
      "The discussion focuses on issues related to competition, openness, and user experiences in the app store and gaming console markets.",
      "There is a debate about the regulation of app stores, the future of gaming consoles, and the importance of competition laws to protect consumer interests."
    ],
    "points": 526,
    "commentCount": 275,
    "retryCount": 0,
    "time": 1703609671
  },
  {
    "id": 38773957,
    "title": "A Comprehensive Guide to Hacker News's Hidden Features and Behaviors",
    "originLink": "https://github.com/minimaxir/hacker-news-undocumented/blob/master/README.md",
    "originBody": "A List of Hacker News's Undocumented Features and Behaviors Hacker News, a simple link aggregator owned and operated by Silicon Valley startup incubator Y Combinator, has had many positive effects on SV startups and engineers as a whole. On Hacker News, users receive Karma whenever another user upvotes a submission or comment they made, which incentivizes positive contributions to the community. However, in maintaining its simplicity, many new features and behaviors added over the years on Hacker News are not fully documented other than the occasional comments from staff. This list details some of the hidden norms about Hacker News not otherwise covered in the Guidelines and the FAQ, along with a few bonus features outside of typical HN usage. If there is anything missing/incorrect from this list, feel free to file a GitHub issue/PR. This list has no affiliation with Hacker News, Y Combinator, or any YC-backed company. Table of Contents A List of Hacker News's Undocumented Features and Behaviors Undocumented Features Moderators Downvoting Comments Flagging/Vouching Top Bar Color Merry Christmas! Anti-Voting Manipulation Flame-War Detector Second-Chance Pool Edit/Delete Time Limits Unvote Time Limits Comment Collapsing Shadowbanning Hidden Reply Links Green Usernames Thin Black Bar Behaviors Implicit Downranking of Politics Implicit Downranking of Topics Around Diversity and Inclusion Implicit Downranking of Posts Without URLs Paywalls Perceived Favoritism Toward YC Companies Downranking of Tutorials Bonus Features Hacker News Classic Hacker News Wayback Hacker News on BigQuery Hacker News Lists Public URLS with user-generated content Official RSS feeds Hacker News Search Filter Out Posts Below X Points To-Do Maintainer Undocumented Features Moderators Hacker News currently has one full time moderator: Dan Gackle (dang), and formerly Scott Bell (sctb). Their comment replies provide a pseudo-log of Hacker News moderation. Dan is very responsive when contacted at hn@ycombinator.com, and is the best option for resolving any issues on Hacker News. Downvoting Comments All comments start with a score of 1 point (but in order to prevent bandwagoning, the comment score is not visible to users other than the author). After users reach 501 Karma, they gain the ability to downvote another comment. Downvoted comments (i.e. with a score < 1) reduce their placement on the comment thread and will appear desaturated to other users deemphasize them. There is no upper limit on the score of a comment, but the minimum score is -4 points. Additional downvotes after that still subtract points from the user's Karma, but the comment won't go below -4. You cannot downvote comments which are direct replies to your own comment, and you cannot downvote 24 hours after the original comment was made. Complaining about being downvoted is discouraged and usually results in even more downvotes. If the comment desaturation makes Hacker News difficult to read, you can click on the comment's timestamp to go to its page where the comment will no longer be faded, or you can install the CSS extension discussed here. Flagging/Vouching If a user has 31 Karma, they can flag submissions. Although submissions cannot be downvoted, flags act as a \"super\" downvote and enough flags will strongly reduce the rank of the submission, or kill it entirely (flagging is supposed to be used for submissions which break the site guidelines, but that isn't always the case in practice). A submission that's flagged to death will have a [flagged] tag. Comments behave similarly. A [dead] submission (that does not also show [flagged]) is killed by a moderator or by the software. They will only be shown to users who have showdead enabled in their profile. A submission can simultaneously be [flagged] and [dead]. If a user has 31 Karma, they can also vouch for a [dead] submission/comment. A vouched submission/comment has its rank restored (and potentially improved as the vouch can counteract the effects of flags). Top Bar Color If a user has 251 Karma, they can set the color of the top bar in their profile settings. The default is #ff6600. The Y Combinator logo will change its color to match the top bar. Here's the list of colors from users who have recently posted. Merry Christmas! During Christmas (December 25th) UTC, the Hacker News front page will have a Christmas theme, with a dark red top bar, and alternating red and green submission ranking numbers. Anti-Voting Manipulation The FAQ states \"users should vote for a story because they personally find it intellectually interesting, not because someone has content to promote.\" Indeed, Hacker News utilizes a voting ring detector which will prevent caught submissions from hitting the front page. Due to sites like Product Hunt normalizing the asking for upvotes or other engagement via social media, the implicit asking of upvotes is also done for Hacker News, usually due to ignorance of the Hacker News rule against it. There are very few good reasons to draw attention to a Hacker News submission immediately after it has been submitted. One popular \"trick\" for obfuscating voting manipulation on Hacker News is to link to the Hacker News's /newest page of new submissions (instead of a direct link which would otherwise make voting manipulation obvious), and asking friends to upvote the submission from that page. This trick doesn't actually work. Flame-War Detector The FAQ notes that submission rank is impacted by \"software which downweights overheated discussions.\" A good rule of thumb for this effect is when the number of comments on a submission exceeds its score. Moderators can overrule the downranking for appropriate, not-actually-a-flame-war discussions. Second-Chance Pool Moderators will sometimes rescue a post which didn't receive a lot of upvotes and reset the submission time on the post. (This is also one of the reasons why the FAQ discourages deleting submissions). Relatedly, moderators can also invite users via email to resubmit a post which didn't get much traction. Posts benefiting from this program are visible at https://news.ycombinator.com/pool . Edit/Delete Time Limits After a post or comment is made, it can be edited by the author within 2 hours. A post/comment can be deleted by the author within those two hours, but only if it has no replies, in order to prevent discussion from being lost. In that case, the post/comment cannot be deleted (This can result in a fake [deleted] edit if a person wants to remove their comment in the limit but can't). Users can upvote posts from any time as a \"bookmarking\" feature. Moderators can change the title of a submission at any time. If you need something deleted but you can't, you'll have to message hn@ycombinator.com. Unvote Time Limits After a user votes on a post or comment, it can be unvoted within 1 hour. After that time, the vote becomes permanent. Comment Collapsing Comments can be collapsed by clicking the [+] icon to improve readability.[flagged] comments are sometimes collapsed by default, and moderators can set a comment to automatically be collapsed if necessary (e.g. meta-discussion). When a comment thread is collapsed, the [x more] number on the right indicates the total number of hidden children comments. Shadowbanning Both users and domains can be shadowbanned, where all posts/comments by that user / submissions to that domain will be instantly [dead] and cannot receive votes/comments (but can still be vouched). For accounts with a substantial history on Hacker News, moderators will give warnings before a ban. A good way to tell if a user/domain is banned is to either have another user with showdead enabled check for a series of [dead] content from that source, or view those submissions in Private Browsing/Incognito mode to see if they appear. Users/domains are usually shadowbanned for breaking HN rules/spam. If you feel you are unfairly shadowbanned, contact hn@ycombinator.com. Hidden Reply Links If the comment depth is 3 or more, reply links are withheld until the comments age a while. The amount of aging is a function of the depth. You can get around it by clicking on the comment's timestamp to go to its own page. Green Usernames Accounts which are less than 2 weeks old at time of submitting/commenting will appear with a green username. Thin Black Bar Occasionally, there will be a thin black bar at the top of the top bar, in memoriam of a significant figure in the tech/science community dying. A Hacker News submission about the death will usually be on the front page at that time. Behaviors Implicit Downranking of Politics The Guidelines state that most political discussion is probably off-topic. However, the line between technology and politics is blurred, especially as of recently. Most tech related submissions with a hint of political partisanship will quickly be flagged to death by users (or die a slow death due to the inevitable flame war). dang has commented about this issue many times over the years. Implicit Downranking of Topics Around Diversity and Inclusion Likewise, topics around diversity and inclusion in tech have gained lots of visibility over the past few years. However, despite these discussions not being off-topic, they tend to be flagged to death by users regardless. Unfortunately. (Moderators occasionally unkill such threads if they see it in time, although it rarely sticks). Implicit Downranking of Posts Without URLs Posts without URLs get penalized. If you post with a link and then add the text as a first comment you have more visibility. Paywalls Many news websites have started implementing a paywall for their content, which has caused conflict with Hacker News's \"original source\" rule. As a result, submissions which link to paywalled sites tend to get many comments complaining about paywalls, which are off-topic. Perceived Favoritism Toward YC Companies YC Companies get two notable benefits on Hacker News; they can post jobs ads to the front page (which start off at Rank #6, cannot be voted/commented on, and have a fixed decay rate), and the ability to do a Launch HN when their startup launches out of a YC batch. Currently, there is no evidence that non-job submissions about a YC startup receive preferential treatment on the front page, or kill submissions critical of a YC startup. In fact, the moderators have stated that they explicitly avoid killing controversial YC posts when possible. Additionally, founders of YC companies see each other's usernames show up in orange, which — although not an explicit benefit — does allow fellow YC founders to immediately identify one another in discussions. Downranking of Tutorials HN submissions which are tutorials are downranked by moderators, as they gratify intellectual curiosity less. Bonus Features Hacker News Classic Hacker News allows people to use the old front page ranking algorithm, which only counts votes from early users. Early users are defined as being created before Feb 13, 2008. Hacker News Wayback Hacker News allows users to see what the front page looks like at any point in time, representing a weighted view of the submissions from that 24 hour period. You can also do a wayback view for any user at their registration date by clicking their registration date in their profile. Hacker News on BigQuery If you want to gather large amount of Hacker News data for data analysis/machine learning, you should use the Hacker News dataset on BigQuery, which has submission and comment data up to November 2022 and is much more pragmatic to use than manually scraping data from the Hacker News API. Hacker News Lists Hacker News maintains a list of useful links that allow for primitive filtering by certain types of content. These currently include: /leaders — View a list of users with the most karma /front — Filter front page submissions for a given day (e.g. 2016-06-20), ordered by time spent there /best — View the highest-voted recent links /active — View the links with the most active current discussions /bestcomments — List the highest-voted recent comments /highlights — List curated high-quality comments /noobstories — Show submissions from new accounts /noobcomments — List comments from new accounts /pool — Links selected for a second chance at the front page /shownew — View new Show HN links /invited — List of stories deemed interesting whose author was invited to repost in order to give them a second chance because they didn't catch interest at the first submit. More information here: https://news.ycombinator.com/item?id=20508960 /shownew — View new Show HN links /asknew — Latest Ask HN (text) posts /whoishiring – Monthly \"Who Is Hiring\" threads /launches — Launches of YC startups Public URLS with user-generated content https://news.ycombinator.com/active https://news.ycombinator.com/ask https://news.ycombinator.com/asknew https://news.ycombinator.com/best https://news.ycombinator.com/bestcomments https://news.ycombinator.com/classic https://news.ycombinator.com/favorites?id=[name] https://news.ycombinator.com/favorites?id=[name]&comment=t https://news.ycombinator.com/front?day=[day] https://news.ycombinator.com/from?site=[domain] https://news.ycombinator.com/highlights https://news.ycombinator.com/item?id=[id] https://news.ycombinator.com/leaders https://news.ycombinator.com/newcomments https://news.ycombinator.com/newest https://news.ycombinator.com/noobcomments https://news.ycombinator.com/noobstories https://news.ycombinator.com/over?points=[score] https://news.ycombinator.com/show https://news.ycombinator.com/shownew https://news.ycombinator.com/submitted?id=[name] https://news.ycombinator.com/submitted?id=whoishiring https://news.ycombinator.com/threads?id=[name] https://news.ycombinator.com/topcolors https://news.ycombinator.com/user?id=[name] https://news.ycombinator.com/vouched?id=[name] https://news.ycombinator.com/flagged?id=[name] https://news.ycombinator.com/highlights https://news.ycombinator.com/pool https://news.ycombinator.com/invited Official RSS feeds https://news.ycombinator.com/rss maps the frontpage. https://news.ycombinator.com/showrss maps the Show HN page. Hacker News Search HN Search provides real-time full-text search for Hacker News. The web app is open source and powered by Algolia Search. Filter Out Posts Below X Points Want to catch-up on the best submissions over the last few days? Filter out all posts below a certain threshold with the over?points=100 URL parameter. Examples: over 100 points over 200 points To-Do Add more images/citations Maintainer Max Woolf (@minimaxir, minimaxir on Hacker News since 2012) Max has no affiliation with Hacker News, Y Combinator, or any YC-backed company.",
    "commentLink": "https://news.ycombinator.com/item?id=38773957",
    "commentBody": "A list of Hacker News&#x27;s undocumented features and behaviorsHacker NewspastloginA list of Hacker News&#x27;s undocumented features and behaviors (github.com/minimaxir) 512 points by ssgodderidge 16 hours ago| hidepastfavorite185 comments minimaxir 13 hours agofine, I&#x27;ll update the list today reply dang 7 hours agoparentThis is what happens Larry when you provide a service to the community. reply tech234a 11 hours agoparentprevI figured this list was on the front page again when I was notified that my PR [1] was finally merged :)[1]: https:&#x2F;&#x2F;github.com&#x2F;minimaxir&#x2F;hacker-news-undocumented&#x2F;pull&#x2F;6... reply ZephyrBlu 11 hours agoparentprevGuilt Driven Development reply mayormcmatt 11 hours agorootparentHa, truly! I read his comment, refreshed the GitHub repo and saw the \"changes madeX then push fix lol reply ssgodderidge 13 hours agoparentprevThanks for putting this together. I’ve been on HN for a while (even before getting an account) and learned a ton with this list reply submeta 12 hours agoprevWould like to mention Dang here, he is doing an extraordinary job in keeping this place sane and civilised.I really am wondering how he can read and react to so many comments quickly.Dang, care to elaborate? Do you see a flat list of new comments, review them quickly? Do you use a moderation tool that scans the mood &#x2F; tone &#x2F; aggresiveness of a post? How do you do the screening and the replies? Manually? reply kmeisthax 3 hours agoparentI disagree slightly. Usually HN is fine, but there&#x27;s a lot of weird shifts in site culture. Sometimes you&#x27;ll see comments on political submissions slide way to the right and shit gets flagged. My tinfoil regarding this is that HN is getting freep&#x27;d[0] - a thread gets linked on a conservative or far-right forum and is either flag bombed or spammed with propaganda comments. As mentioned in the \"implicit downrank of DEI\" section, the moderators try to unflag these threads (since they&#x27;re relevant and people want to talk about them) but they just get flagged by the community a second time.If I had access to a better moderated community not being actively attacked by the far right, I&#x27;d use it. But I&#x27;m not aware of any[1].[0] Short for \"Free Republic\" - a conservative political forum that would ballot stuff online polls for the sake of pushing their political agenda.[1] Note that I stopped posting to Reddit after &#x2F;u&#x2F;Spez decided to powertrip to kill a mod protest over third-party API clients. reply sph 2 hours agorootparent> shit gets flagged. My tinfoil regarding this is that HN is getting freep&#x27;d[0] - a thread gets linked on a conservative or far-right forum and is either flag bombed or spammed with propaganda comments.With less aluminium foil: HN has an crude anti-flamewar system that roughly triggers when the ratio comments&#x2F;upvotes is greater than 1, and the post is quickly deranked.Often this means it is a controversial post, but I&#x27;ve seen it happen with two or three posts&#x2F;Ask HN of mine where a vibrant discussion causes them to disappear from the frontpage after 1 hour because commenters were too active and triggered the anti-flamewar system, which is quite disappointing.Please, not everything on the Internet revolves around your political issues. But if it were the case of a thread linked elsewhere and flooded, this system would trigger as well. reply smeej 4 hours agoparentprevI got to be one of the people who learned today that it&#x27;s \"Dan G,\" not [only] \"dang.\" reply core_dumped 12 hours agoparentprevDang is just a 10x moderator reply Zuiii 6 hours agorootparentHe&#x27;s so good that I wouldn&#x27;t be surprised if he was literally 10x moderators. Thank you dang (dangs?) for what you do. reply jader201 2 hours agorootparentNot sure if you were (half) joking, but in case you weren’t:https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38779139(Spoiler alert: it’s all Dan G.) reply revskill 1 hour agorootparentprevEven if you said 100x, he wont upvote you, hehe reply badrabbit 9 hours agoparentprevI used to think so until the shadow restriction&#x2F;moderation stuff. I must agree with the quality part though, just not the methods. But like reddit and other platform, the majority of users don&#x27;t give a shit about this stuff until it&#x27;s their turn. Oh well, shit is. reply Fezzik 3 hours agorootparentI have posted one or two outlandish things whilst enjoying a few too many drinks and I have been properly downvoted; I have never seen or heard of anyone being restricted&#x2F;moderated inappropriately. Any specific examples that come to mind?Edit, to add: I also say ridiculous things when sober, on occasion. I didn’t mean to imply that the only time my mighty brain falters is when I’m inebriated. reply badrabbit 2 hours agorootparentLikewise and I was in the wrong. I hate to beat a dead horse but look at my reply to a sibling comment in this comment thread for an example.My honest opinion: I have geopolitical views that tend to upset ideological people which includes...coughpowerfulcough folks and got stuck with reputation accordingly (whatever, no complaints). I normally avoid politics because in the US at least, I am too contrarian and \"centrist\", but on HN people do engage and correct me or educate me when I am incorrect. At least it used to be so, times are achanging. But tech people keep being political so it&#x27;s hard to avoid. One particular thread about signal or something, I made a point about Silicon Valley tech types presuming things and interfering with politics in other countries (something about signal or other apps enabling one side of politics to win over the other I think), I think that comment was the last straw before the restrictions (can&#x27;t be sure though, just guessing), this must have been in 2022. But as the saying goes \"if a cop tails a car for 500 miles, that car is getting a ticket!\", I am sure I have done plenty more that violate the rules, my only complaint is on how my \"punishment\" was in secret with no opportunity to self-correct. Maybe like reddit, HN is best lurked?I hope to see better times on HN and make dang&#x27;s job easier on my part at least. happy 2024! reply Grimburger 4 hours agorootparentprevShow us some of the stuff you said so we can judge that ourselves?People can still see with Show Dead turned on, over the many years have never seen anything there that wasn&#x27;t either inflammatory, spammy or otherwise very low quality. reply badrabbit 2 hours agorootparentIn my understanding, comments that need moderator intervention would be flagged, not just downvoted. Downvotes and flags are not what I talked about though, this might help with context: https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38773957#38777787HN is the only site where I even comment publicly. No warning restrictions where you don&#x27;t even know if you are restricted == shadow moderarion. Probably best I simply take a hint though, i should spend my time on more productive things anyways. Although, I really do enjoy healthy discourse with people who know the subjects in question far more than I do.HN should either have different rules for non-technical threads or avoid political&#x2F;sensitive topics altogether imho.Moderation isn&#x27;t easy, and I&#x27;m only being so critical because I am personally affected. Chances are this comment I spent time composing won&#x27;t go through and I would have to abandon it because of the restrictions. An unfortunate side effect of it all is others like you can&#x27;t see I am restricted. Or you would reply to me and I can&#x27;t reply back because of those same restrictions.I hope we all can take a step back (especially myself) and and forge a new path where we trest others as we would like to be treated (can&#x27;t ask for more). reply yial 2 hours agorootparentFor what it’s worth, I can see your comment and reply. Though I realize that might not mean you can reply back. replyisoprophlex 14 hours agoprevIt&#x27;s missing \"highlights\" at https:&#x2F;&#x2F;news.ycombinator.com&#x2F;highlights... which, anecdotally, I learned about JUST after getting a comment on there. Imagine my surprise at learning about this (in a manner totally unrelated to the post I wrote that made it to the list), and seeing my own writing as the most recent item on the list.Truman show vibes for sure. reply CharlesW 14 hours agoparentInteresting! Is there something similar for stories? reply dang 7 hours agorootparentNot at present. The links aragonite mentioned all exist, but are determined by upvotes. The &#x2F;highlights page (for comments) is a manually curated list. But if anyone sees a comment that&#x27;s good enough to be on &#x2F;highlights, please let us know at hn@ycombinator.com. We don&#x27;t want to be the only ones finding these things! reply aragonite 13 hours agorootparentprevThere&#x27;s \"best\" (Highest-voted recent links), \"classic\" (Frontpage as voted by ancient accounts) and \"active\" (Most active current discussions):https:&#x2F;&#x2F;news.ycombinator.com&#x2F;lists reply averageRoyalty 8 hours agoprevUnfortunately it still doesn&#x27;t mention \"rate limited\" accounts[0], which is extremely confusing when it happens to you. It&#x27;s happened to me on this account, I can&#x27;t comment more than 5 times per some arbitrary period - I&#x27;m guessing 24 hour UTC.No explanation or information is given - you don&#x27;t even know until HN seems broken and you go searching. dang mentioned that&#x27;s something they&#x27;d like to impove, but given the slow movement of HN features, I doubt it will.Note that none of this is meant as a slight on dang - I respect the work he does and really like the site, but it doesn&#x27;t make it less frustrating. The result will be me rolling a new account (against site guidelines), as I have no other privacy preserving way of handling the issue.0. https:&#x2F;&#x2F;github.com&#x2F;minimaxir&#x2F;hacker-news-undocumented&#x2F;issues... reply StevenNunez 13 hours agoprevGuess I need to figure out how to get more Karma! I didn&#x27;t know there were more features to this site since I&#x27;m essentially a lurker. reply system2 1 hour agoparentThis mindset turns this place into Reddit. I think the features should be enabled after a certain time rather than how likable the person is. reply dsubburam 6 hours agoprevOught the bookmarklet for submitting links[1] be mentioned?Granted not exactly undocumented, as it is linked on the &#x2F;submit page. But I think worth including, as it is not in the Guidelines or FAQ.[1] https:&#x2F;&#x2F;news.ycombinator.com&#x2F;bookmarklet.html reply dang 5 hours agoparentI added that to https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38778450. We&#x27;ll see if minimaxir goes for it :) reply MatthiasPortzel 13 hours agoprevOne undocumented heuristic that I wish was documented better is that repeated submissions to the same domain, especially without discussion activity, leads to an automated shadow ban. This is just from what I’ve observed browsing &#x2F;new with Show Dead turned on. There are lots of people submitting their own blog posts or repeated posts from their favorite news outlets who have been shadow banned. Vouching provides a mechanism for restoring posts of particular quality, but most of these people aren’t trying to spam, they just don’t know this unspoken rule. reply dang 5 hours agoparentThat&#x27;s connected to this guideline (from https:&#x2F;&#x2F;news.ycombinator.com&#x2F;newsguidelines.html): \"Please don&#x27;t use HN primarily for promotion. It&#x27;s ok to post your own stuff part of the time, but the primary use of the site should be for curiosity.\"If people submit primarily their own stuff, our software eventually starts classifying the account as \"promotional\" and starts filtering the posts.People are totally welcome to submit their own work—that&#x27;s great—but to use HN as intended, they should do that as part of a diverse set of submissions on unrelated&#x2F;interesting things. There are too many bloggers&#x2F;marketers&#x2F;writers out there that use HN merely as yet-another-channel for the content they&#x27;re pushing, and that&#x27;s definitely not in the curious spirit which is supposed to animate HN. reply NooneAtAll3 1 hour agorootparentthere&#x27;s a slight difference between \"your own stuff\" and \"same domain\"...I don&#x27;t know how many false positives would fans&#x2F;people with narrow sharing interest make, but slight editing of the guideline might help make them less \"false\" reply raldi 3 hours agorootparentprevAs we used to say at the other site, it’s okay to be a redditor with a website; it’s not okay to be a website with a reddit account. reply aragonite 15 hours agoprevI&#x27;m not sure if this is undocumented, but I&#x27;ve noticed that submitted links sometimes have either the pathname or search parameters portion of their URL truncated or stripped entirely. Sometimes the truncation happens after a slight delay.For example, if you try to submit a link to a particular Google Books page, say the one I just tried:https:&#x2F;&#x2F;www.google.com&#x2F;books&#x2F;edition&#x2F;The_Albert_Einstein_Col...the link will remain intact for a short while before losing its search parameters and turning into a link to the book itself rather than to a page in it. reply Jtsummers 15 hours agoparentPossibly canonical links. HN looks at the submissions and updates the links to the canonical link if present. I&#x27;m on mobile so not able to double check the Google books page at the moment, but if you view source check to see if there&#x27;s a canonical link in there. This also bites some blogging sites that end up with a canonical link to their base http:&#x2F;&#x2F;blog.example.com on all their posts.https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Canonical_link_element reply aragonite 15 hours agorootparentJust checked and you are exactly right (it&#x27;s not in the raw source but the element is found in the DOM tree). This explains the delay also! reply dang 14 hours agoparentprevJtsummers&#x27;s answer is correct, but we turn off canonicalization for some classes of URLs and I think you&#x27;ve found a good example of that. I&#x27;ve exempted Google Books links for the future and put your submitted URL back in https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38774261. reply aragonite 13 hours agorootparentThanks dang! reply XCabbage 15 hours agoprevThe list at https:&#x2F;&#x2F;news.ycombinator.com&#x2F;topcolors is definitely not \"the complete set of colors users have set\" as claimed. I know this because it doesn&#x27;t include my top color (namely #64BFBD). reply sillysaurusx 14 hours agoparentI once submitted this link to the front page, which promptly took down HN because it had no caching.I fired off an email to Scott, frantically telling him \"I think I took down HN. If you’re scrambling to figure out what’s wrong, it’s because so-and-so story is on the front page, and you can fix it by booting it off.\" A minute or two later the site came back to life, so I emailed saying \"oh, never mind. I guess it was something else.\" And to my surprise he emailed back saying you were exactly right, thanks.Presumably they either added caching, prevented it from going to the front page, froze the list for posterity, or it’s still a ticking time bomb that can take down HN again if you happen to submit it. Either way, maybe the explanation of your missing topcolor is there somewhere. reply dr_kiszonka 15 hours agoparentprevSpeaking of the top colors, the repo doesn&#x27;t mention that the color changes to red during Christmas. reply antognini 4 hours agorootparentLooks like it was updated to include that. As someone who is color blind I never noticed that the submission numbers have alternating red and green colors! That was new to me! reply XCabbage 15 hours agoparentprevPerplexingly, my color appeared there shortly after I posted this. Does that link contain colors of recently active users, perhaps? reply dang 14 hours agorootparentYes; users who have recently posted. reply XCabbage 14 hours agorootparentAh, nice - so \"my color doesn&#x27;t appear on the list\" is a problem that fixes itself automatically when you complain on HN about it! reply dang 14 hours agorootparentA suitably recursive property for an internet forum, which I doubt pg had in mind when he wrote that code. reply sillysaurusx 13 hours agorootparentprev(For the curious, this is because HN only loads profiles into memory whenever users post. Then they stay in memory until a server reboot, which seems to happen every day or two.) reply GreenWatermelon 38 minutes agorootparentEverything I hear about HN makes me feel like it&#x27;s a hobby side project hosted on an old laptop in someones basement, and not literally the PR arm of a giant venture capital company. reply TheAceOfHearts 14 hours agoparentprevDidn&#x27;t know about this list. It&#x27;s time for a vibe shift, so after a decade I&#x27;m switching to #2288ff. Thanks for sharing! reply tptacek 15 hours agoparentprevWeird, mine is near the top. I wonder what the sort order is. reply sillysaurusx 14 hours agorootparentIt’s random. I emailed to submit a patch ordering by popularity and adding caching, but got no answer.EDIT: You can see for yourself how it&#x27;s ordered here: https:&#x2F;&#x2F;github.com&#x2F;shawwn&#x2F;arc&#x2F;blob&#x2F;arc3.1&#x2F;news.arc#L2611-L26...This is the original code, which survived untouched till at least 2019 or so. I&#x27;ve wondered if they changed it since then.The key is the (users) function:https:&#x2F;&#x2F;github.com&#x2F;shawwn&#x2F;arc&#x2F;blob&#x2F;arc3.1&#x2F;news.arc#L144-L145This returns the keys of the profs* hash table (short for \"profiles\"). At the time, profs was populated each time the server restarts:https:&#x2F;&#x2F;github.com&#x2F;shawwn&#x2F;arc&#x2F;blob&#x2F;arc3.1&#x2F;news.arc#L88-L91(dir profdir*) essentially returns an `ls` of the directory in which user profiles are stored. (Yes, they are (or were) stored to disk, not a database.) This was one reason it was hard to add a feature to rename users, which Dan somehow figured out.So it takes the `ls` of that directory, giving a list in mostly random order, and loads them one by one, putting them into a hash table, whose iteration order is also random.In modern times, profs* is populated in a lazy fashion whenever users post after a server reboot.Arc uses \"maptable\" to iterate over a hash table, which under the hood used hash-table-for-each: https:&#x2F;&#x2F;github.com&#x2F;shawwn&#x2F;arc&#x2F;blob&#x2F;arc3.1&#x2F;ac.scm#L1082Nowadays in Racket, it probably uses hash-for-each: https:&#x2F;&#x2F;docs.racket-lang.org&#x2F;reference&#x2F;hashtables.html#%28de...It does have a \"try-order?\" option, which gives a deterministic ordering if set. But whenever I set it, I sometimes get strange errors, at least on MacOS. Since it&#x27;s disabled by default, I doubt they set it. Even if they did, it would be ordered alphabetically, which means your topcolor wouldn&#x27;t be anywhere close to the top unless someone else whose name starts with \"a\" also set it.I only know all this because I&#x27;ve spent five years updating Arc for the modern era over at https:&#x2F;&#x2F;github.com&#x2F;shawwn&#x2F;sparcPast discussion: https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38586832 reply gjsman-1000 14 hours agoparentprevI’m grumpy because I am 100% sure that the “automatic downweighing” is applied to me (my comments are often below faded despite multiple upvotes), but it’s never been explained.I’m just guilty without knowing the crime. reply pests 14 hours agorootparentEmail dang he can let you know what&#x27;s going on. reply hamburglar 12 hours agorootparentprevThis actually happened to me when I first signed up and after emailing dang asking why, it went away. I don’t think I ever got any actual explanation but I think it must have been some false positive which, once corrected, would never recur. reply covercash 15 hours agoparentprevMine isn’t on the list either - #9db2cbd reply Jtsummers 15 hours agorootparentDrop the extra `d` and it&#x27;s on there. reply raverbashing 11 hours agoparentprevWell, given the name I assume it&#x27;s a ranking? Though some colors are weird and it would be hard to believe a lot of people use it (like #fafafd or #0082a0 )And yes my choice is there (#ffaa33) reply ilamont 13 hours agoprevThank you for compiling this list. I have been using the site since 2007 and had never looked into the classic page (assumed it was an old stylesheet or something similar). Also had no idea about some of those special URLs such as noobcomments.If a user has 31 Karma, they can also vouch for a [dead] submission&#x2F;comment. A vouched submission&#x2F;comment has its rank restored (and potentially improved as the vouch can counteract the effects of flags).Is this consistent? I recently vouched for something on the New page and it wasn&#x27;t restored.One other thing I would like to ask: I currently bookmark about 20 users whose comments I find especially insightful across a wide range of subject matter expertise. Is there another way&#x2F;better way to do this? reply gnicholas 13 hours agoparent> I currently bookmark about 20 users whose comments I find especially insightful across a wide range of subject matter expertise. Is there another way&#x2F;better way to do this?I don&#x27;t know if it still works, but I used this a few years ago for this exact reason: https:&#x2F;&#x2F;github.com&#x2F;veggiedefender&#x2F;hn-friends reply swyx 9 hours agorootparentval.town has a hnfollow thingy that i use now, but tbh i dont know when it doesnt work reply nyjah 15 hours agoprevThe second chance pool blew my mind the first time I got an email to resubmit. You figure if a post gets no traction, that’s just how it is. But I’ll never forget getting that email. reply susam 13 hours agoparent+1My post https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37956065 got no traction when I posted it and quickly disappeared from \"&#x2F;new\" without many people noticing it. The next day, I received an email that this post was entered into the second chance pool. Then sometime later, it got picked for the front page and led to a good amount of community participation! Thanks to the second chance pool, I received several interesting demo submissions to my hobby project! reply divbzero 13 hours agoprevWhen did Hacker News start applying topcolor to the YC logo? I just noticed it today but am curious if it’s been there for awhile. reply dang 13 hours agoparentI did it a couple days ago because a user had been emailing since 2016 that the orange Y doesn&#x27;t look good when the bar is changed to Christmas red.Btw, that&#x27;s not the record for how long a bug fix has taken. We get there eventually. reply rob 11 hours agorootparentDon’t make me start emailing you every day to update the default font size from 12px to a modern default like 16px. This isn’t 2007 when everybody had 1024x768 monitors. I had to dust my monocle out of storage before adjusting my browser zoom level. reply JorgeGT 13 hours agorootparentprevThis update broke my workflow! reply cantSpellSober 11 hours agorootparentI use [bgcolor=&#x27;#ff6600&#x27;] as a CSS selector for thewrapping the header to remove the header bar color.So this really did break my workflow, hah reply divbzero 8 hours agorootparentThat’s pretty amusing. Does switching the CSS selector to td[bgcolor] do the trick? reply dang 13 hours agorootparentprevI did warn him that people might complain about side effects reply fmx 14 hours agoprevDiscussion from 5 years ago: https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=16437973Nice to see that some of the suggestions from there, like listing the hidden URLS (&#x2F;leaders, etc.) seem to have been implemented. reply dang 14 hours agoparentLet&#x27;s document it shall we:A List of Hacker News&#x27;s Undocumented Features and Behaviors - https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=33076053 - Oct 2022 (68 comments)A List of Hacker News&#x27;s Undocumented Features and Behaviors - https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=30459276 - Feb 2022 (64 comments)A List of Hacker News&#x27;s Undocumented Features and Behaviors (2018-20) - https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=26866482 - April 2021 (255 comments)A List of Hacker News&#x27;s Undocumented Features and Behaviors (2018) - https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=23439437 - June 2020 (266 comments)A List of Hacker News&#x27;s Undocumented Features and Behaviors - https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=20292361 - June 2019 (25 comments)A List of Hacker News&#x27;s Undocumented Features and Behaviors - https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=19212822 - Feb 2019 (183 comments)Hacker News&#x27;s Undocumented Features and Behaviors - https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=16437973 - Feb 2018 (391 comments) reply CharlesW 14 hours agoprevI&#x27;m curious about the full list of automatic headline changes. They seem reasonable, but it&#x27;s worth knowing that if they change a headline for the worse, you can immediately edit the submission and restore the original headline (or an \"in the spirit of\" edit that fits within HN&#x27;s character limit). reply latchkey 16 hours agoprevI&#x27;ve noticed a shadowbanned behavior. If you submit a link that is a banned domain, you will notice that the &#x27;discuss&#x27; link is missing and the post doesn&#x27;t show up at all (but you see it). reply ssgodderidge 15 hours agoparentAny examples? I didn’t know about this one reply Jtsummers 15 hours agorootparentTurn on show dead in your profile and visit the \"new\" link (https:&#x2F;&#x2F;news.ycombinator.com&#x2F;newest). You&#x27;ll see a lot of [dead] links, those are the sort GP is referencing. [flagged][dead] are killed by user action (flags). reply bookofjoe 11 hours agorootparentprevhttps:&#x2F;&#x2F;www.sciencealert.com&#x2F; reply tptacek 11 hours agorootparentThere&#x27;s lot of articles from this site on HN, including recently. But:* It seems low-quality, at times almost like the Weekly World News. On HN, we sometimes replace even high-quality science writing with primary sources (when the paper is recent and the writing is just a summary).* It&#x27;s high-volume, which is problematic in that it incentivizes users to submit lots and lots of stuff from this one site; front page slots are the scarcest resource on HN and everything competes for it, so high-volume low-mid-quality sites tend to get downweighted.* It&#x27;s slathered with ads and bait-y gimmicks.If there&#x27;s something on this site that you think would make a good thread for HN, you can probably just find whatever it&#x27;s blogspamming and submit that instead. reply nadermx 4 hours agoprevWait, so you&#x27;re telling me that after 501 karma points it&#x27;s all just as meaningless as before 501? reply 1vuio0pswjnm7 3 hours agoprevAssuming one has full vouch privileges, why do some flagged comments have vouch links but others can not. Perhaps one can check the source code to answer questions like this, or perhaps the code is constantly changing. reply notatoad 6 hours agoprev>Hacker News allows people to use the old front page ranking algorithm, which only counts votes from early users. Early users are defined as being created before Feb 13, 2008.anybody remember the significance of that date? i apparently joined five days later, and i&#x27;m guessing i was part of some wave of new signups triggered by some event. reply dang 5 hours agoparentIt was \"users who joined in the first year\": https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=2073513. reply stormed 14 hours agoprevA lot of these backend features&#x2F;behaviors are super interesting to read through, especially in regards to the \"flame war detector\" section. I&#x27;ve only ever seen link aggregator websites such as Reddit relying on community moderation & spam detectors rather than anything to improve the quality of the discourse. reply goplayoutside 13 hours agoprevAre there any browser extensions that provide a user tag feature, similar to Reddit Enhancement Suite?There are so many notable tech people on HN, it would be useful to be able to recognize their posts and comments. reply gnicholas 13 hours agoparenthttps:&#x2F;&#x2F;github.com&#x2F;veggiedefender&#x2F;hn-friends reply system2 53 minutes agoprevI don&#x27;t see anything related to IP protection. For example, if I log out and log in with another user, can I upvote a comment? If so, this can lead to vote manipulation. Browser fingerprints are useless if the user is using Adblockers or an extension stripping that information while submitting data.What is preventing vote manipulation here? Signing up is so easy without email so I think there should be something. reply spencerchubb 10 hours agoprevThe &#x27;over&#x27; url parameter is a great feature and I&#x27;m surprised I haven&#x27;t heard about it until now reply dang 7 hours agoprevHere are a few things that could be corrected or added.Corrections:* These URLs aren&#x27;t public - you can only see them if you&#x27;re logged in as $name:https:&#x2F;&#x2F;news.ycombinator.com&#x2F;vouched?id=[name]https:&#x2F;&#x2F;news.ycombinator.com&#x2F;flagged?id=[name]* > Moderators occasionally unkill such threads if they see it in time, although it rarely sticksModerators sometimes turn off the flags on posts like this, in which case the thread will usually stay on the front page.* > Hacker News allows users to see what the front page looks like at any point in timeThis wording makes me squeamish because the &#x2F;front pages don&#x27;t show what the front page looked like at any point in time - they&#x27;re a blended average of the frontpage stories from a particular day. At no point did the front page actually look like that, but that&#x27;s because you can&#x27;t get a 24 hour view from any particular snapshot.Additions:* &#x2F;next (or &#x2F;prev) link on comments jumps to the next (or previous) comment at the same nesting level* &#x2F;root links (on item pages) jump to the top-level GP comment (this may be renamed to &#x2F;top in the future)* &#x2F;context on a comment (when not on the original &#x2F;item page) links to the comment in its place on the original &#x2F;item page* in the RSS section, could include: https:&#x2F;&#x2F;news.ycombinator.com&#x2F;bigrss maps the frontpage and up to 9 pages after that* HN&#x27;s software auto-edits titles to make them less linkbaity, but if it gets the edit wrong, you can change it after you submit (per https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38775366)* the bookmarklet could probably be added, per https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38778787Also, see https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38779156 because that&#x27;s another undocumented aspect. How to sum it up in a sentence? Maybe: \"If you only use HN to submit your own stuff, the software will eventually start filtering your submissions.\" But then I need a second sentence: \"It&#x27;s fine to submit your own stuff, but it should be part of a diverse mix of unrelated&#x2F;interesting things.\" reply minimaxir 4 hours agoparentI&#x27;ll work on adding these. (I know you sent me a list awhile back; I&#x27;ll add those too)Thanks again! :) reply yawnxyz 6 hours agoparentprevDoes your use of \"moderators\" imply \"dang\" is actually a collective of moderators all using one username? reply pvg 5 hours agorootparenthttps:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=28875018 reply dang 5 hours agorootparentprevNo. Comments by dang are always me.It would be a lot easier (in the short run) to have personas and shit like that, but I somehow just know that (in the long run) it&#x27;s not worth it. reply alberth 14 hours agoprevMissing “Classic”It’s missing the relatively recent new list called “Classic”.https:&#x2F;&#x2F;news.ycombinator.com&#x2F;classicClassic: Frontpage as voted by ancient accounts reply n2d4 13 hours agoparentNot relatively recent! The feature is from 2009: https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=607271 reply TheCleric 14 hours agoparentprevI see it there: https:&#x2F;&#x2F;github.com&#x2F;minimaxir&#x2F;hacker-news-undocumented&#x2F;blob&#x2F;m... reply cinntaile 12 hours agoprevAnother undocumented feature is that flagging posts&#x2F;submissions very likely is weighted differently for different users. reply drfuchs 14 hours agoprevI accidentally hit “hide” on an item I want to see. Where’s the documentation on how to unhide? reply erik_seaberg 14 hours agoparentYour view of your profile page https:&#x2F;&#x2F;news.ycombinator.com&#x2F;user?id=drfuchs should have a “hidden” link where each story has an “un-hide” link instead of “hide.” reply drfuchs 13 hours agorootparentThanks. For those wondering how someone may have missed this: Evidently I had not in fact accidentally done a \"hide\", so nothing was in my hidden list, and thus my profile page cleverly didn&#x27;t have a \"hidden\" link at all. reply cwillu 10 hours agorootparentThings expire off the hidden list over time as well, presumably based on when the link itself disappears, but such links will then show up on hn&#x27;s historical view. reply cozzyd 4 hours agoparentprevYeah it&#x27;s very easy to accidentally hide (or flag!) posts on my phone. Wish there was an option for a confirm screen. Every once in a while I check what I&#x27;ve accidentally flagged and unflag it. I&#x27;m sure I&#x27;m not the only one... reply aragonite 14 hours agoparentprevIf you see it on this page (not accessible to anyone except you, logged in), you can just click \"unhide\"https:&#x2F;&#x2F;news.ycombinator.com&#x2F;hidden?id=drfuchs reply drfuchs 13 hours agorootparentVery cool, thanks. How does one become aware of this class of magic? It seems completely undiscoverable, not being in the FAQ, nor in the present \"List of Hacker News&#x27;s undocumented features...\" reply b8 14 hours agoprevThere&#x27;s other moderators than daang and some users help out with site stuff sometimes. reply orenlindsey 12 hours agoprevThis list is pretty nice, I learned a lot from it. reply atomicfiredoll 7 hours agoparentI&#x27;ve lurked here and on other discussion&#x2F;aggregation sites for years (I&#x27;ve apparently been here 10--oof) and only recent started commenting a little more. This whole time I didn&#x27;t realize the top color could be set. I noticed it in my profile a couple days ago and thought it was an entirely new site feature.Seeing all this info I was unaware of laid out in front of me makes me wish there was a docs page linked in the footer, like Tildes has. reply cloudking 13 hours agoprevIdea: 1000 karma should get custom CSS as a setting. reply isoprophlex 13 hours agoparentand a dark mode button ;) reply cloudking 8 hours agorootparentI&#x27;d settle for that too! reply system2 57 minutes agorootparentWhy don&#x27;t you use Dark Reader extension instead? reply javajosh 10 hours agoprevThere is another hidden feature, a \"weak shadowban\" which places your comment lower on the page than default. I think it is a manual flag set by some unknown heuristic, basically a \"we don&#x27;t like you but not enough to ban you\" signal. This happened to me several months ago after a heated political debate and still is in effect (I&#x27;m sure it will happen to this comment, for example). reply matheusmoreira 5 hours agoparentYou&#x27;re rate limited. So am I. Even emailed dang about it. I suppose my case is similar enough: started happening some time after some discussion that went wrong. I really don&#x27;t remember which but I don&#x27;t deny participating in them.Truth be told it&#x27;s probably a good thing for someone like me. HN is addictive and getting kicked out after 5 posts is essentially a better noprocrast setting. I actually thought it was the noprocrast setting at first since at the time I had just begun experimenting with it.It&#x27;s the automatic downranking of posts that hurt me. Noticed it recently. I&#x27;ve been trying to be more selective towards which posts I reply to but it feels pointless since any and all comments immediately sink towards the bottom of the page, unseen.A smaller problem with this system: taking the time to write a comment, submitting it and only then being denied. I actually wasn&#x27;t sure if this comment was going to go through but the timer must have reset. I hesitate to suggest some kind of indicator. It would be very helpful but it would also create a timed reward schedule not entirely dissimilar from those Skinner&#x27;s box simulators they call mobile games. reply lossolo 15 hours agoprevYour ability to download or upvote comments and stories can also be shadowbanned, rendering these actions ineffective. reply ulrischa 15 hours agoprevIs the 501 karma level true? I heard 801? reply jjulius 15 hours agoparentIt&#x27;s 501. reply charcircuit 13 hours agoprevHN also has rate limits on making comments for certain users. The limit is easy to hit and takes hours to go away. This results in having to ignore people&#x27;s questions because it will waste a comment contributing to one&#x27;s rate limit. One can also respond to people by editing one&#x27;s comment, but that has poor ux. reply joecool1029 13 hours agoparentHad it in the past on my acct. It always showed up when I posted on topics about trees and commented faster than normal. Wasn&#x27;t aware it was a thing until emailing HN and dang mentioned it.I&#x27;d maybe suggest updating the message to include some approximate time the limit expires. &#x27;Come back in a few hours&#x27; or something. I say this because it was unusual enough for me to hit that I&#x27;d rewrite my comments thinking it was some kind of word flag I&#x27;d tripped, then try to repost it in 5-10 minutes only to have it fail again. reply unethical_ban 12 hours agoparentprevThis is an annoying feature which I have encountered. I know it is to calm heated discussions, but the ambiguity of the message is belittling. I&#x27;m not overwhelming the database; just tell me how long I have if you&#x27;re going to put me in timeout for a few hours.It isn&#x27;t a frequent issue, but putting duct tape on someone&#x27;s mouth when they&#x27;re talking about something they care about does not calm them down, even if it achieves the primary goal. reply tedunangst 9 hours agoparentprevOr wait a few minutes to answer, because frequently someone else will ask an even better question to reply to. reply xienze 11 hours agoparentprevYou’re talking about the “you’re posting too fast, please slow down” message? IME it’s not a rate limit but a passive-aggressive way of banning someone from a conversation for some amount of time. reply paganel 3 hours agorootparentNot the OP but I think that&#x27;s what the OP had in mind, yeah.It&#x27;s interesting because that limit has made me even more \"radical\" (by the unwritten standards of HN), i.e. when I know that I only have a limited amount of comments per day to make my point I use those comments in a more \"push-y\" manner by responding to the most provocative replies to what I had previously said, while also ignoring replying to comments that are too bland. Which means that I go by the logic of \"why should I reply to this reasonable comment when there&#x27;s this other more radical comment that&#x27;s a lot more thorough and violent when trying to debunk what I had just written? I should use my limited number of comments by answering to the latter, not the former\".One of the many paradoxes of confrontation (\"war\") at work, that is if you regard many of the discussions in here as confrontation (even \"war\") of ideas. reply photochemsyn 14 hours agoprevSeems to be missing comment vote manipulation. Users can see the number of votes their own comments get but can&#x27;t see the number of votes other comments get, or even if their vote changes the vote count on a comment, which allows shadowbanning of unwanted comments by vote count alteration by mods&#x2F;admins. reply godelski 14 hours agoparenthttps:&#x2F;&#x2F;github.com&#x2F;minimaxir&#x2F;hacker-news-undocumented&#x2F;blob&#x2F;m... reply photochemsyn 13 hours agorootparentThat only covers downvoting comments, it doesn&#x27;t mention why comment vote counts aren&#x27;t publicly visible. Keeping comment vote counts invisible makes sense if upvotes on some comments are not being recorded as a form of shadowbanning those comments&#x2F;users. reply tptacek 13 hours agorootparentOther people&#x27;s vote counts are invisible because they led to constant bickering about the merits of any given comment&#x27;s \"spot\" vote count (this is also the reason there&#x27;s a guideline that asks you not to write these kinds of comments, but changing the affordances so you can&#x27;t see them at all was a much more effective solution to the problem).(My confidence in this explanation comes in part from the fact that I&#x27;m pretty sure I had a hand in this coming to pass: https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=2403716).The conspiracist rationale here doesn&#x27;t make much sense, because the site operators also control the displayed comment count and the registration of votes. reply godelski 6 hours agorootparentI thought a lot was about preventing bandwagoning or manipulation. In both directions, but especially downvoting. replybadrabbit 9 hours agoprevAny hidden feature to get past shadow moderation? I know I am not entitled to anything and no one cares unless they are affected directly but it would be nice if \"you&#x27;re posting too fast\" restrictions (could mean 3 or 4 times in one day) showed up before I spend like half an hour composing a reply to someone, not after. But alas, not owed anything, hn is a social engagement site for a VC at the end of the day. But the non-transparent moderation stuff speaks volumes about the culture and all.Will try to post non-FUD, high quality comments and posts (read between the lines lol). reply dang 5 hours agoparentI hear you and am willing to meet you on this, but you simply can&#x27;t post things to HN like \"you are just an ageist tribal presumptive jerk\" (https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38769241) without getting penalized and&#x2F;or banned. I understand that the other person was being provocative, but a lot of this is about how we handle provocation—i.e. it&#x27;s about learning to get hit by provocation (whether intentional or unintentional), absorbing the shock in oneself, and then not responding in kind.You&#x27;re just as welcome on HN as anyone else is, and if you build up a track record of using the site as intended for a while, we can definitely take a look and hopefully take the rate limit off your account. reply badrabbit 3 hours agorootparent@dang, the \"jerk\" part was over the line and I was in the wrong. However, my problem is not with the rules or in the restriction but with the fact that your restrictions where not transparent (shadow moderation) and you didn&#x27;t think simply asking me to refrain would not work. In the past for example, you asked me to avoid brigading and I have avoided it since. You used a humorous comment I made (like I&#x27;ve seen others do once in a while) as an example of low quality comment so I avoid any sort of humor on HN now.I will assume in cases such as what you quoted, you would treat the person I replied to with the same level of non-transparent hostility? If so I will avoid defending myself, else I will just avoid using \"over the line\" terms like \"jerk\" and use more tolerable terms like \"unkind\".> and if you build up a track record of using the site as intended for a whileYou tolerate political topics on HN. On such threads, it seems my problem is that I do not understand what you mean by that. My expectation is either you tell me or others flag my comment if it is inappropriate. I never attack someone&#x27;s character and try to focus on technical analysis of things, what you quoted there was in response to someone attacking my character (will refrain).I will read your rules again. Perhaps I should avoid sensitive topics entirely on HN but truth be told, I make them in anticipation of a healthy discourse (which if you note on that comment thread was had). This is the only site I know of where that is actually possible these days. But I&#x27;d hate to think simply sharing my views is causing such a burden.> You&#x27;re just as welcome on HN as anyone else isI don&#x27;t think that is true. But no worries, I undersrand your perspective well enough.At least I now know that this isn&#x27;t in my head, thanks for confirming. I sincerely hope you reconsider this approach and inform others (if they exist) of your disapproval or measures instead of shadow&#x2F;secret approaches. You absolutley have the right to moderate content and tell people to stop any behavior, just like this.I&#x27;ll seriously consider stopping active participation as a 2024 resolution.Happy new years! reply fragmede 8 hours agoparentprevI believe the hidden feature is to email dang at hn@ycombinator.com and promise you&#x27;ll never ever shit post again. reply badrabbit 3 hours agorootparentBut then I have to use email! You&#x27;re right though. reply hn_throwaway_99 15 hours agoprevMinor nit: I&#x27;ve posted about this before, but I wish folks would not refer to dang as a \"moderator\" or say stuff about \"the HN moderators\". If anything, I think it&#x27;s more appropriate to refer to dang as the site admin.The reason this is a pet peeve of mine is I often see people complaining about \"the HN moderators\" (e.g. \"the HN mods are on a power trip and keep flagging my submissions!\" or some such), where they are clearly taking this conceptual model from Reddit, and they seem to think there is a shadowy cabal of \"mods\" who control what gets downvoted&#x2F;flagged (and, in fairness, this does and can happen in subreddits). In other words, when you see your submissions being flagged or comments being downvoted or flagged, 99.9% of the time it&#x27;s not \"the HN mods\" who are downvoting you. It&#x27;s just other, normal HN users (who have earned 501+ karma as TFA explains) who simply don&#x27;t like what you have to say.Not saying dang never blocks users&#x2F;comments, but it&#x27;s exceedingly rare and I&#x27;ve only seen it for the most obnoxiously egregious behavior after (usually multiple) warnings. reply throwawaysugar 15 hours agoparent> Not saying dang never blocks users&#x2F;comments, but it&#x27;s exceedingly rare and I&#x27;ve only seen it for the most obnoxiously egregious behavior after (usually multiple) warnings.Issuing warnings or reminding people of the guidelines (rules) is also part of being a moderator, so the moniker is perfectly cromulent to describe dang&#x27;s role reply hn_throwaway_99 13 hours agorootparentMy issue isn&#x27;t so much with the word, but it&#x27;s that so often I see tons of users who are clearly bringing preconceived notions about how the moderator role works at Reddit and thinking it works the same here. At the very least there definitely aren&#x27;t moderators, plural, on HN currently by that definition.The other reason I think \"admin\" is a much better word is that it much more clearly indicates that this is a special, privileged role. That vast majority of \"moderation\" on HN is done by normal users with upvotes, downvotes and flagging (and also, of course, by automated systems e.g. with spam filters&#x2F;bot detection).But again, my primary point is that I see users blaming \"the mods\" for why their post is downvoted or hidden, when usually it&#x27;s that people just don&#x27;t want to acknowledge that the community, at large, didn&#x27;t think they were contributing productively to the discourse of the site. reply cowpig 15 hours agorootparentprevI had never heard the word \"cromulent\" and so that led me to a Mirriam Webster article describing how The Simpsons made the word up for a joke and then from there \"seeped into our lexical consciousness\". Worth a read!https:&#x2F;&#x2F;www.merriam-webster.com&#x2F;wordplay&#x2F;what-does-cromulent... reply throwawaysugar 11 hours agorootparentIt&#x27;s way too good a word to be left unused! ;-) That whole episode is hilarious tbh reply Seattle3503 15 hours agoparentprevdang, or someone at HN, can and does silently modify the \"weight\" of a submission. That is why some articles stay on the front page longer, and some slip off the front page in a couple of hours.This is far more control than reddit mods have. Reddit mods have one ability, an that is the ability to remove or not remove a submission, which is a very public action. HN can put its finger on the scales in a way that is much more subtle. reply romafirst3 15 hours agorootparentVery much so and it all happens with zero transparency, explanation or acknowledgement, very sus.Your account can also be effectively shadowbanned with everything you submit going straight to the “dead” pile. Again no explanation and the only way you realize it is cause your submissions slowly die with no comments (and when you view the site in a non logged in fashion your submissions are not there) reply tptacek 15 hours agorootparentThat happens to spammers and serial abusers and, as I understand it, nobody else. People are overwhelmingly told when they&#x27;re banned. reply realusername 15 hours agorootparentI&#x27;ve seen at least one user recently which has all his comments as [dead] automatically and they are pretty normal comments. I did not write down the name but I&#x27;ve seen that only once. reply dang 14 hours agorootparentThis could be because they got caught in a spam filter, or because we banned them and the comments they&#x27;ve posted since then are better. In such cases we&#x27;re always happy to take a look and unban the account if they&#x27;re using HN as intended.There are some weird edge cases, like sometimes a banned user, once they&#x27;re unbanned, will revert to posting abusively. Then we ban them again and they revert to posting good comments again. Who can fathom the psyche of homo internetus.It&#x27;s also common for an account to post both fine comments and abusive comments. It&#x27;s the latter that determine whether we have to ban the account—that&#x27;s how rule enforcement works. But if a banned account posts something good, there&#x27;s no reason why the good post needs to remain [dead]. This is what vouching is for: https:&#x2F;&#x2F;news.ycombinator.com&#x2F;newsfaq.html#cvouch. reply realusername 14 hours agorootparentThanks, I didn&#x27;t know about vouching. I guess there&#x27;s indeed an history behind each ban of a real user. (Not talking about spam of course, it&#x27;s a different thing) reply Jtsummers 14 hours agorootparentprevAssuming no violent (as in promoting) or particularly racist&#x2F;sexist comments in their history, this hits some new accounts if they&#x27;re using a VPN (some particular VPNs, I guess?). Sometimes they get caught up in the same filters used to block spammers&#x2F;abusers from using HN. Or if their first (or one of their first) comments was a link without substantial text, that also gets people put into some kind of \"spammer\" category. You, or they if you tell them, can reach out through the email in the contact link at the bottom to get the ban addressed. reply realusername 14 hours agorootparentI found back the username in my browser history, the name is baybal2, it&#x27;s a fairly old account so I&#x27;m not sure what caused it.I&#x27;m going to contact the moderation indeed, it&#x27;s the first time I was seeing non-spam comments being shadow banned automatically. reply nkurz 11 hours agorootparentYou&#x27;ve found an odd edge-case. baybal2 is banned intentionally, despite the fact that most of his posts are good, and some are great. Unfortunately, the remainder aren&#x27;t. The compromise is that he&#x27;s banned, but users like me vouch for many of his good posts to make them visible. Here&#x27;s a post from Dan about him a couple years ago: https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=29692791. And another from 4 years ago: https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=21195898. reply justsomehnguy 11 hours agorootparentprev> so I&#x27;m not sure what caused itThe behaviour of the account holder. reply censthrowaway23 14 hours agorootparentprevIf only we had some transparent way to know if it’s common or not or if it just happens to bad people we don’t like.Unfortunately we don’t and we just have to trust dang and the benevolent hacker news gods to always have our best interests at heart and only spam us with the content that they want to spam us with and of course their friends and insiders content which we should be grateful to be spammed with. reply pvg 11 hours agorootparentWe do know to a reasonable degree because otherwise the ‘improperly’ banned would make a noticeable fuss. It’s not a common enough occurrence but maybe you have some in mind. reply romafirst3 4 hours agorootparentHuh? How would they know to make a fuss. They aren’t even notified, they are shadow banned. replytptacek 15 hours agoparentprevThere are, I believe, multiple HN moderators; Dan is just the only vocal one. And among all the different sites on the Internet, there are multiple different moderation styles, of which Dan&#x27;s is just one. reply hn_throwaway_99 13 hours agorootparentI don&#x27;t believe that is correct anymore, and I think TFA is right:> Hacker News currently has one full time moderator: Dan Gackle (dang), and formerly Scott Bell (sctb). Their comment replies provide a pseudo-log of Hacker News moderation.I&#x27;ve never seen an anyone else besides dang comment that they are an HN admin in the past 5-ish years or so. reply tptacek 13 hours agorootparentDan is the only one who currently posts publicly as a mod, but if you search his comments, you&#x27;ll see he implies there are others. reply hunter2_ 15 hours agoparentprev> and I&#x27;ve only seen it forSpeculation&#x2F;intuition, but I figure it happens in ways specifically designed to evade the type of detection you&#x27;re basing your claim on, akin to shadow banning, whereas on Reddit there&#x27;s often more visible evidence like corpses of deleted comments. reply gus_massa 15 hours agorootparentYou can see the [dead] comments if you go to your profile and enable \"showdead\". Most of them are very bad, but from time to time you can find a false positive and vouch it. reply hunter2_ 11 hours agorootparentYes, I keep that enabled. I guess I provided poor examples but what I&#x27;m saying is that it seems like on Reddit moderation activities are quite obvious (i.e., a giant chain of deleted comments tends to indicate moderation and elicit resentment) and here they&#x27;re less obvious (there&#x27;s an air of strong moderation but little to point at directly, which is great IMHO as it evades resentment). reply hn_throwaway_99 15 hours agorootparentprevYou can see all of those deleted&#x2F;flagged comments if you turn on \"showdead\" in your options. reply voytec 13 hours agoprev1) I enjoy figuring out how HN works over time by noticing certain patterns. Still haven&#x27;t figured out (please don&#x27;t spoil) why I can&#x27;t downvote certain comments in threads where I see both arrows on other comments. HN feels like games in the past - you&#x27;re given some docs but over time you&#x27;d go \"ooooh\" about something undocumented. Good stuff.2) voting system works well for just users but HN is infested with corporate voting manipulation. It seems that an entity monitoring eg. brand&#x27;s name is able to use just a few 501+ karma accounts to bury, or a few more to strongly boost, any post3) based on what I learned about posts&#x27; scores (more than a grain of salt - as I mentioned - I&#x27;m still figuring things out) it seems a little out of place that sometimes a few hours-old posts (not as old to qualify for the \"second chance\" thing) with barely any comments and points are positioned higher on the top page than younger posts with more activity (higher score and more comments). I&#x27;ve noticed it more than once with substack and my assumption is that HN may be boosting YC-backed products reply tptacek 13 hours agoparentThey do the opposite thing: they&#x27;re extra hands-off with YC-backed things, and quicker to turn off flags on stories critical of YC-backed things.I don&#x27;t see much evidence of voting manipulation (and a lot of evidence of HN being hair-trigger about suppressing vote manipulation). reply shadowgovt 13 hours agoparentprevIt may not even be official corporate manipulation. HN is frequented by a lot of industry insiders, and if they see something that doesn&#x27;t resonate with what they know to be true they&#x27;ll downvote. reply shiroiuma 1 hour agorootparentExactly: there&#x27;s a ton of people here who work at the big ad-tech companies, so of course they&#x27;re going to downvote anything that promotes ad-blocking since that harms their own paycheck in a way.Imagine a discussion forum for medical topics (science, research, etc.) where 1&#x2F;3 the users work for companies that sell homeopathic \"remedies\", and how that would influence moderation there. reply local_crmdgeon 12 hours agoparentprevI don&#x27;t think Dang is advancing YC interests - I think it&#x27;s just bog-standard astroturfing. Especially with the low vote count of most stories, it&#x27;s not hard to make your story pop or disappear. reply lampiaio 13 hours agoparentprev> 2) voting system works well for just users but HN is infested with corporate voting manipulation.You mean to say that next week&#x27;s Kagi thread won&#x27;t be as organic and spontaneous as it purports to be? Impossible. reply cantSpellSober 14 hours agoprevH reply fabian2k 13 hours agoparentI do put a lot of value on privacy, but I actually don&#x27;t really find that problematic. I don&#x27;t have the expectation to be able to just erase everything I wrote online, I assume it is public and persistent. So for things where I want to preserve private aspects I have to take that into account from the start.There are certainly individual cases where some post might turn out to be problematic later, but that&#x27;s more of an exception and can be handled by manual intervention.I do find it annoying in some places how users can just erase their posts without a reason. Because they don&#x27;t just erase their own content, they take the responses to it with them or at least make them less understandable. And those people responding did put some effort into it. reply godelski 14 hours agoparentprevI actually really hate this feature. The landscape has changed and I&#x27;ve been seriously contemplating asking @dang to nuke my account. reply goles 14 hours agorootparentI would greatly encourage you to not nuke (but do contact) despite having had this thought myself.But I appreciate the fact the consequences for everyone are not the same, and potentially even fatal in current times.I believe I understand why it&#x27;s done in the way it&#x27;s done, but also this can drive self-censorship. Which is a growing problem at large.Without speaking out of turn, which may just create more work, they can help.Contact hn@ycombinator.com. reply godelski 6 hours agorootparent> but also this can drive self-censorship.Actually, this is the motivation __to__ nuke. It&#x27;s what I mean about the environment changing. HN is one of the big places that get scraped and everyone is scraping all data that they can these days. It&#x27;s very reasonable to believe you can be de-anonymized much more easily and that it&#x27;s only going to get easier. It&#x27;s not unreasonable to think that someone can mimic my speech and this creates a new vulnerability. I am thinking much more about self-censorship now and being more measured now than I have in the past.I do agree with you that self-censorship is a growing problem. But I&#x27;m not sure how to solve this when we&#x27;re entering a world where it is the language you use that becomes a fingerprint. I appreciate the records because there is a lot of valuable information here but at the same time these records make us vulnerable in a way we haven&#x27;t been before. And that it is in a way that not just worrying about nation states being able to do this but that we&#x27;re posting these records to huggingface. The processing is just getting easier and that&#x27;s about all that&#x27;s needed now. reply goles 4 hours agorootparentThanks for the thoughtful response.Maybe the answer is to let trusted users become polymorphic, but that defeats some of the spirit of the site.I&#x27;ll have to think more on it, get back to you. reply godelski 2 hours agorootparentIt is a hard problem to solve. Switching usernames doesn&#x27;t fix the issue but could be more noise. Even disappearing messages is only noise, but stronger than the former. Probably pretty helpful because I don&#x27;t think people are scraping every day but that could change.Idk, if you have thoughts I&#x27;d love to head. I&#x27;ve been thinking of writing them down and posting to HN but unsure and it feels like one of those things that sounds conspiratorial until years later people will say \"of course this was coming\" lol reply shadowgovt 10 hours agorootparentprev> this can drive self-censorship. Which is a growing problem at large.I suppose it depends on one&#x27;s perspective. We used to call self-censorship \"decorum.\"The nature of human beings didn&#x27;t change when we went online, and sometimes it behooves a person to just read the room before speaking. Because of the karma system, the upvote downvote system, and the user curated flagging system, I would classify Hacker News as a \"decorum-friendly\" message board where a certain amount of pre-filtering is not only necessary but highly encouraged. replyY-bar 12 hours agoprev> Inclusivity […] Unfortunately. (Moderators occasionally unkill such threads if they see it in time, although it rarely sticks).Why would this be described as “unfortunate”? For example tings like LLM:s being trained on flawed&#x2F;biased data is a known real and unfortunate thing which can hurt already marginalised groups in society if used for eg. policy derisions. reply bmicraft 12 hours agoparentIf you quoted the whole thing it would be more apparent:> [...] they tend to be flagged to death by users regardless. Unfortunately.What the author finds unfortunate is the flagging by users, not the moderator action. reply Y-bar 12 hours agorootparentI hope i can agree with you. But are you really sure? The period seems to indicate that the “unfortunate” belongs to the moderator action and not the flagging. reply djur 12 hours agorootparent\"[unfortunate statement]. Unfortunately.\" is idiomatic (if informal) English. \"Unfortunately. [unfortunate statement].\" is not, especially in this case where the second statement is parenthesized. If it was \"Unfortunately:\" or \"Unfortunately,\" that would be different. reply bmicraft 12 hours agorootparentprevIn my mind unfortunately can only ever refer to a previous sentence, not the next one. I read the period simply as a stylistic choice meant as a pause. They could have written it with a dash or a comma:> [...] they tend to be flagged to death by users regardless — unfortunately. reply Y-bar 12 hours agorootparentTo me it is always preceding, like here:> 1. Placement: “Unfortunately” is typically placed at the beginning of a sentence or clause to emphasize the regrettable aspect of the situation. It sets the tone for what follows and ensures clarity in your expression.https:&#x2F;&#x2F;thecontentauthority.com&#x2F;blog&#x2F;how-to-use-unfortunatel...> Unfortunately, my time is limited.https:&#x2F;&#x2F;www.collinsdictionary.com&#x2F;dictionary&#x2F;english&#x2F;unfortu... reply djur 8 hours agorootparentSorry in advance if this is too much detail, but you seem to be interested in the nuance here.\"Unfortunately.\" here is a sentence fragment. It&#x27;s idiomatic (conversational + informal) English to modify a preceding sentence with a sentence fragment. (\"We could have some iced tea. Or lemonade.\") There are cases where a sentence fragment can be used to introduce a sentence (\"Delicious. I ate the whole thing.\") but this is never an adverb (you can say \"Unbelievable. You&#x27;ve done it again\" but not \"Unbelievably. You&#x27;ve done it again\").If one were trying to write \"Unfortunately, there has been an error\" with a long pause, you would write \"Unfortunately... there has been an error\", never \"Unfortunately. There has been an error.\" The latter looks like a typo.That&#x27;s why (as a native speaker&#x2F;reader of American English) I don&#x27;t see any possibility of interpreting the text the way you&#x27;re describing. Hope that helps! replysokoloff 12 hours agoparentprevThe “unfortunately” refers to the preceding text rather than the following.> However, despite these discussions not being off-topic, they tend to be flagged to death by users regardless. Unfortunately. reply Y-bar 12 hours agorootparentThat’s my point, I think. It seems to me that moderators are doing the right thing by HN rules and context by tagging such discussions are not off-topic. Why is it described as “unfortunate” then? reply djur 12 hours agorootparentI think you&#x27;re reading it as \"Unfortunately, moderators occasionally unkill such threads...\" when it&#x27;s meant as \"...they tend to be flagged to death by users regardless, unfortunately\". reply Y-bar 12 hours agorootparentI think so too perhaps and hopefully, but there is no comma As in your example. Just a period suggesting the previous sentence has ended. reply sokoloff 11 hours agorootparentTrue. But the following phrase is set off by both a period and a set of parentheses. I think it’s pretty clear the author’s intent is for there to be a complete sentence about the behavior of HN users and then a complete thought saying that was unfortunate. (Then a parenthetical about the moderator behavior.) replyDiabloD3 14 hours agoprev [–] My favorite is &#x2F;leaders.Shame I&#x27;m not in the top 50 anymore, but there just ain&#x27;t a lot of good content to submit anymore and karma whoring seems like a waste of time. replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The article explores the features, behaviors, and guidelines on the Hacker News platform, including moderators, downvoting comments, and flagging behavior.",
      "It mentions bonus features such as Hacker News Classic and Hacker News Lists.",
      "The article also discusses the availability of the Hacker News dataset on BigQuery for data analysis and machine learning, as well as official RSS feeds, the HN Search feature, and content filtering on Hacker News."
    ],
    "commentSummary": [
      "The summary covers various topics related to the Hacker News platform, including moderation practices, undocumented features, hidden functionalities, user frustrations, color rankings, voting manipulation, banned accounts, and concerns about privacy and self-censorship.",
      "It provides an overview of the functionality, management, and user experiences on Hacker News.",
      "The summary captures the key themes and discussions in the given passages."
    ],
    "points": 512,
    "commentCount": 185,
    "retryCount": 0,
    "time": 1703612847
  },
  {
    "id": 38775439,
    "title": "mRNA-4157 cancer vaccine boosts Keytruda treatment, reduces recurrence by 49%: Phase 2b study",
    "originLink": "https://www.freethink.com/health/cancer-vaccine",
    "originBody": "Moderna’s mRNA cancer vaccine works even better than thought The personalized shot gives a standard melanoma treatment a huge boost. By Kristin Houser December 24, 2023 Fields Cancer Medicine Vaccines Share Copy a link to the article entitled Moderna’s mRNA cancer vaccine works even better than thought Share Moderna’s mRNA cancer vaccine works even better than thought on Twitter (X) Share Moderna’s mRNA cancer vaccine works even better than thought on Facebook Credit: Phicak / Adobe Stock By Kristin Houser December 24, 2023 Fields Cancer Medicine Vaccines Share Copy a link to the article entitled Moderna’s mRNA cancer vaccine works even better than thought Share Moderna’s mRNA cancer vaccine works even better than thought on Twitter (X) Share Moderna’s mRNA cancer vaccine works even better than thought on Facebook By Kristin Houser December 24, 2023 Fields Cancer Medicine Vaccines Share Copy a link to the article entitled Moderna’s mRNA cancer vaccine works even better than thought Share Moderna’s mRNA cancer vaccine works even better than thought on Twitter (X) Share Moderna’s mRNA cancer vaccine works even better than thought on Facebook Adding Moderna’s in-development cancer vaccine to a standard treatment for melanoma dramatically reduces cancer survivors’ risk of death or recurrence, according to newly shared trial data. The challenge: To treat melanoma — the deadliest type of skin cancer — doctors typically start by surgically removing as much of the cancer as possible. They might then administer another treatment, such as chemo or radiation therapy, to kill any cancer cells they missed. Even if a person is cancer-free after this, there’s always a chance of the melanoma coming back, and certain types are considered high-risk for recurrence, including ones that are particularly thick or that had metastasized (spread to other parts of the body) prior to treatment. “This is a pretty significant improvement, a pretty dramatic improvement over standard of care.” Stephen Hoge The cancer vaccine: Moderna and pharma giant Merck are developing an mRNA-based cancer vaccine, mRNA-4157 (V940), for people who’ve had high-risk melanomas removed. The vaccine works by instructing the body to make up to 34 “neoantigens.” These are proteins found only on the cancer cells, and Moderna personalizes the vaccine for each recipient so that it carries instructions for the neoantigens on their cancer cells. The idea behind the vaccine is that, by prompting the body to make these proteins, it can prepare the immune system to quickly identify and attack any new cancer cells bearing them, preventing recurrence. What’s new? In the ongoing phase 2b KEYNOTE-942 study, Moderna and Merck are comparing the cancer vaccine’s ability to prevent melanoma recurrence or death when combined with Keytruda, Merck’s FDA-approved cancer treatment, to Keytruda alone. In 2022, they reported that the combo therapy reduced high-risk patients’ risk of recurrence or death by 44% compared to only Keytruda in the two years after treatment. They’ve now announced that people who received both therapies were 49% less likely to experience recurrence or death a median of three years after treatment compared to people in the Keytruda-only group. They were also 62% less likely to experience distant metastasis or death. “The durability of the responses is really strong — they’re essentially rock solid through this time,” Moderna President Stephen Hoge told Reuters. “This is a pretty significant improvement, a pretty dramatic improvement over standard of care with just Keytruda alone.” “We think that in some countries the product could be launched under accelerated approval by 2025.” Stephane Bancel Looking ahead: The KEYNOTE-942 study is relatively small, with just 157 participants, but Moderna and Merck have already launched a phase 3 trial for the combination cancer therapy that will include more than 1,000 people with high-risk melanoma. The companies are also looking beyond melanoma, launching a phase 3 trial testing the cancer vaccine in people with non-small cell lung cancer — and if these trials go well, it might not be long before the personalized therapy reaches patients. “We think that in some countries the product could be launched under accelerated approval by 2025,” Moderna CEO Stephane Bancel told AFP. We’d love to hear from you! If you have a comment about this article or if you have a tip for a future Freethink story, please email us at tips@freethink.com.",
    "commentLink": "https://news.ycombinator.com/item?id=38775439",
    "commentBody": "Moderna&#x27;s mRNA cancer vaccine works better than thoughtHacker NewspastloginModerna&#x27;s mRNA cancer vaccine works better than thought (freethink.com) 364 points by nateb2022 13 hours ago| hidepastfavorite190 comments pclmulqdq 13 hours agoPast mRNA cancer vaccines haven&#x27;t failed to get through trials because of ineffectiveness. They failed clinical trials in the 2000s and 2010s due to their side effects being too bad. The side effects of mRNA vaccines are getting less and less severe over the years, but the proof of the pudding will come in phase III for this vaccine, as with all the other ones. reply JumpCrisscross 12 hours agoparent> failed clinical trials in the 2000s and 2010sWe didn’t start testing neoantigen mRNA cancer vaccines until 2017 [1].[1] https:&#x2F;&#x2F;www.ncbi.nlm.nih.gov&#x2F;pmc&#x2F;articles&#x2F;PMC8942458&#x2F; reply peyton 12 hours agorootparentIIRC from when Moderna started getting hot back in 2012 the preclinical trials weren’t going well. I think those were running ca. 2004–2005. Moderna took the infectious disease approach. reply mlyle 11 hours agorootparentThis is an oversimplification, but:Moderna mostly took an initial infectious disease approach because extant mRNA formulations at the time provoked too much of an immune response that neutralized the effectiveness of subsequent dosing. There&#x27;s a goldilocks amount of immune response you want-- too little and the immune system doesn&#x27;t learn the antigens you&#x27;d like, but too much makes it miss the mark and focus too much on the formulation of the vaccine itself.For a cancer vaccine that you want to dose multiple times (9 times in this study) to keep up peak immune response this is a problem. For infectious disease, where some early sensitization of the immune system can be enough, it&#x27;s not so bad. reply make3 10 hours agorootparenthuh that sounds exactly like machine learning overfitting vs generalization reply closewith 10 hours agorootparentWell, over-fitting is a term borrowed from biology (and generalisation from economics), so not surprising in grand terms.Maybe the real surprise for some is that not everything was invented in the valley. reply make3 7 hours agorootparentthat&#x27;s definitely not what I meant or implied; it makes sense that the immune system is a classifier of sorts reply shadowgovt 10 hours agorootparentprevInteresting analogy and it may not be wrong.The acquired immune system basically includes a feature where it can squirrel away some of the white blood cells that fought an infection in the past at the site of the infection. This is a subset of the cells that were activated by the chemicals that were found during the infection.But the immune system doesn&#x27;t have a great way to distinguish the chemicals that actually cause illness from ancillary chemicals that may have been present during the infection as well. This is one of the root causes of allergies... you get sick with, say, rhinovirus at the same time that the pollen count is really high and your immune system defeats the rhinovirus but squirrels away both the white blood cells that were activated by the rhinovirus and the ones that were activated by the basically harmless chemicals on the surface of pollen grains. Then the next time allergy season comes around, your nasal passages and lungs fill up with pollen grains and the white blood cells close to the surface that were squirled away due to that old rhinovirus infection activate and trigger your immune system to respond to the pollen. replybluGill 12 hours agoparentprevThe side effects of most cancer treatments are aweful. How do they compare? reply throwup238 8 hours agorootparentMost of them are awful but only in rare cases do they cause permanent damage beyond what is already happening with the cancer and usually the patient can recover after the treatment is stopped.Anything that stimulates the immune system runs the risk of increasing its sensitivity too much till it attacks normal cells and at that point, a lot of horrible things can happen ranging from a new mild allergy to neurological disorders like multiple sclerosis to acute organ failure.Since the point of the therapy is to get immune cells to attack the cancer cells, there&#x27;s the very real risk of instead targeting the tissue the cancer evolved from. reply bsder 7 hours agorootparent> Most of them are awful but only in rare cases do they cause permanent damage beyond what is already happening with the cancerThat&#x27;s simply not true. Chemotherapy, in particular, is ridiculously bad.Chemo brain is a thing. Another example, people have to wear cold packs on their hands and feet in order to minimize the neuropathy that some chemotherapies can cause. Other chemotherapies cause your veins to leak with attendant swelling everywhere. Several years after therapy, some of that swelling still persists. Radiation is horrendously bad at damaging intestinal tissue. etc.Yes, it beats being dead. That&#x27;s about all you can say about it. reply ChrisMarshallNY 7 hours agorootparentI agree. I have not been through it, myself (crosses self), but I live on Long Island, which means that I have watched a lot of people go through it.Most recover nearly completely, but they all suffer some permanent effects, like sun sensitivity, hair loss, digestive problems, anemia, weakened immune system, etc. reply stjohnswarts 2 hours agorootparentWhy would living on long island mean you would watch a lot of people go through it? Is there some some kind of cancer hot spot on long island? Genuinely curious. reply satchlj 12 hours agorootparentprevThe bar for side effects is much stricter for preventative treatments like vaccines. reply JumpCrisscross 12 hours agorootparent> bar for side effects is much stricter for preventative treatments like vaccinesThese vaccines are for treatment. They don’t prevent cancer. They treat it. They’re vaccines because they work by arming your immune system versus doing the fighting themselves. reply Terr_ 11 hours agorootparentA comparison with rabies vaccine may be useful here.On the scope of the entire human body, the virus usually enters first, before the vaccine is used.However on the scope of individual tissues and cells, the vaccine is often the first on the scene, prepping your cells before the (sneaky, slow) virus makes its main attack. reply throwup238 8 hours agorootparentRabies post-exposure prophylaxis is different because it&#x27;s made up of several components: human rabies immune globulin (HRIG) and the vaccine. The vaccine takes a while to work so HRIG is the first line of defense that prevents the virus from entering the nervous system until the immune system is able to fight the virus. reply mcmoor 5 hours agorootparentprevFrom what I&#x27;ve heard it&#x27;s still usually \"preventative\" because we can&#x27;t confirm that the rabies infection will successfully infect until it&#x27;s too late. So we vaccine anyone with (very) likely case. reply throwaway29812 10 hours agorootparentprevAll of this is completely based on the individual case. reply dralley 8 hours agorootparentIf the Rabies virus, or cancer, \"beats\" the vaccine on a significant scale, then it matters little what it is called because you will probably be dead in short order. reply phpisthebest 10 hours agorootparentprevThis further redefinition of the term \"vaccine\" is a huge problemby definition a vaccine should NOT be a treatment, a vaccine has to be preventative, and further personally I believe only Sterilizing immunity should be considered a \"vaccine\"This is very important, because a \"vaccine\" in the US has legal protections other medical drugs, treatments, etc do not get, which is why there is a push to include alot of these things under the definition of \"vaccine\" so now they get the legal shield reply beowulfey 9 hours agorootparentThat&#x27;s not the definition of a vaccine. A vaccine is something that registers our active immune system against an antigen, so that our body detects and eradicates it upon detection. The timing doesn&#x27;t really matter and there are a lot of therapeutic vaccines [1].[1] https:&#x2F;&#x2F;www.ncbi.nlm.nih.gov&#x2F;pmc&#x2F;articles&#x2F;PMC9729511&#x2F; reply jonhohle 8 hours agorootparentI took it as the literal and previously accepted definition was uniformly changed by both government and secular organizations in near unison. As late as January 2021 vaccines were thought of as dead pathogens - https:&#x2F;&#x2F;web.archive.org&#x2F;web&#x2F;20210118194713&#x2F;https:&#x2F;&#x2F;www.merri... reply pseudalopex 6 hours agorootparentMerriam Webster&#x27;s old definition did not exclude therapeutic vaccines. But it did exclude vaccines prepared from viruses.[1]Probably you heard about diphtheria and tetanus vaccines before 2020. They are prepared from toxins.Other definitions did not restrict what a vaccine must contain.[2][1] http:&#x2F;&#x2F;web.archive.org&#x2F;web&#x2F;20190118214500&#x2F;https:&#x2F;&#x2F;www.merria...[2] https:&#x2F;&#x2F;web.archive.org&#x2F;web&#x2F;20160402152957&#x2F;https:&#x2F;&#x2F;www.dicti... reply snowwrestler 5 hours agorootparentprevThe word “vaccine” comes from the recognition that people could gain immunity against smallpox via infection with the less-deadly (and live) virus cowpox.https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;VacciniaThe definition has never been constrained to dead pathogens. reply beowulfey 6 hours agorootparentprevit started as live pathogens (cowpox) so it must have gone through many iterations if that were the case reply labster 6 hours agorootparentLive poliovirus vaccine continues to be used, and continues to cause occasional active cases and transmission. reply johncolanduoni 7 hours agorootparentprevThere were plenty of protein subunit vaccines and a few viral vector vaccines in use in humans prior to the COVID vaccines. Neither of these are dead pathogens. reply phpisthebest 9 hours agorootparentprevIt is now, it has been expanded over the years most recently in response to covid as to include mRNA in the definition of what a vaccine is explicitly to shield these big pharma companies from liability for any harm adverse reactions to mRNA may have to individualsIt is shocking to me that soo many people that normally at against large companies, and against even capitalism do not see the huge danger in classifying these things as something that gives the companies creating them an almost complete liability shield from harms caused by their products.Once something is a \"vaccine\" in the US you lose all ability to sue for most harms caused by said vaccine. reply vaidhy 8 hours agorootparentCitation please? Here is the procedure for getting compensated for vaccine caused issues - https:&#x2F;&#x2F;www.hrsa.gov&#x2F;vaccine-compensationIt should not matter to you whether the compensation is paid by the government or a company, but you can file for compensation and if valid, get compensated for it. reply shibopo 9 hours agorootparentprevI mean the general problem is that the word vaccine has been merged with two meanings of \"creating something so your immune system gets prepped to prevent disease\".Moderna here create something so your immune system gets prepper but it doesn&#x27;t prevent it&#x27;s used to fight.I mean this word came from cow, you are right, it&#x27;s time to redefine some of these terms. reply bawolff 9 hours agorootparentprev> by definition a vaccine should NOT be a treatmentUmm. You have never heard of rabies or tetnus vaccines?This is not a change in definition. reply phpisthebest 9 hours agorootparentI am aware of them, these also should not be called vaccines. reply woodruffw 9 hours agorootparentThis is a bullet you can bite, but it’s inconsistent with your original point: the tetanus and rabies vaccines have long been called vaccines, undermining any claim that we’ve somehow only recently redefined the word “vaccine.”Edit: to be clear, this kind of argument is only slightly less asinine than arguing that a vaccine isn’t a vaccine unless it comes from a cow’s udder. reply dev_tty01 9 hours agorootparentprev>> by definition a vaccine should NOT be a treatment, a vaccine has to be preventative, and further personally I believe...Hmm, I don&#x27;t see the basis for this conclusion. This treatment is acting according to the common definition of a vaccine. Whether before or after disease activation is not part of the definition. As far as your personal belief, I understand that you may feel strongly, but I&#x27;m sorry to say that doesn&#x27;t really enter into articulating an accepted scientific definition.vaccine: any preventive preparation used to stimulate the body’s immune response against a specific disease, ...https:&#x2F;&#x2F;www.dictionary.com&#x2F;browse&#x2F;vaccineCould you be more specific about any particular legal shield? They all have to go through the same phased trials system. I don&#x27;t see any reason to conclude this is a legal shenanigan. What they have created meets the well-known definition of a vaccine. reply AYBABTME 8 hours agorootparentWithout taking a side or another.. I just want to note that the definition you shared starts with \"preventive\" which is the point GP is focusing on. reply futuretaint 8 hours agorootparentI&#x27;m not taking a side either, but seeing as drug companies are shielded from liability w&#x2F; vaccines, if I was a drug company I would be motivated to lobby broadening the definition. it would just be good business. reply satchlj 12 hours agorootparentprev> The cancer vaccine: Moderna and pharma giant Merck are developing an mRNA-based cancer vaccine, mRNA-4157 (V940), for people who’ve had high-risk melanomas removed. reply JumpCrisscross 12 hours agorootparent> for people who’ve had high-risk melanomas removedThis is still treatment. They aren’t administering the vaccine to healthy adults. The population is melanoma survivors with a high risk of recurrence. reply shadowgovt 10 hours agorootparentIsn&#x27;t this hair splitting on definitions? They are there for vaccinating against future instances of cancer after somebody is in remission from previous instances of cancer.It&#x27;s not a generally-distributed vaccine, but there are all sorts of vaccines that aren&#x27;t generally distributed and instead applied only to populations likely to encounter the illness protected against (such as the battery one is exposed to if one travels to particular parts of the world). reply sbelskie 12 hours agorootparentprevThis does not appear to be a preventative treatment. It treats existing melanoma by inducing the body to produce antibodies against it, no? reply kurthr 12 hours agorootparentIt&#x27;s also personalized, which to me says that it&#x27;s unique and each could have different likelihood of autoimmune response. The vaccine works by instructing the body to make up to 34 “neoantigens.” These are proteins found only on the cancer cells, and Moderna personalizes the vaccine for each recipient so that it carries instructions for the neoantigens on their cancer cells. reply satchlj 12 hours agorootparentprevIt is intended for people who’ve had high-risk melanomas removed, to prevent cancer from coming back. reply JumpCrisscross 12 hours agorootparentYou’re both right. It’s preventative, not prophylactic.We’re strict with the latter because they’re administered to healthy adults. That doesn’t apply here. If you took every coronavirus mRNA vaccine side effect and multiplied the severity and frequency by two orders of magnitude, this vaccine would still be worth it for many with melanoma. reply ajb 12 hours agorootparentprevThe reason preventative treatments are held to a higher standard is because they are given to a very large number of people who might never get the disease anyway. Hence, a 1 in 1000 risk is significant. If you are treating people who have had cancer, their risk of getting it again in much higher than 1 in 1000. reply Turing_Machine 11 hours agorootparentRight. There&#x27;s also the fact that the FDA will accept much worse side effects for a cancer treatment than they would for (e.g.) an athlete&#x27;s foot treatment.For a bad cancer like some melanomas, just about anything that doesn&#x27;t kill the patient outright is gonna be on the table. reply nextos 10 hours agorootparentprevNot just antibodies, but it will stimulate a broad class of immune cells.And yes, the name vaccine is misleading. They should use immunotherapy. reply snowwrestler 4 hours agoparentprevI think you might be confusing mRNA vaccines with immunological cancer treatments in general, which have been tried in trials for decades and have occasionally caused horrible side effects in some patients. reply tmountain 12 hours agoparentprevWhat kind of side effects? reply neverrroot 12 hours agorootparentnext [68 more] [flagged] lukeck 12 hours agorootparent> the ones for the CV vaccine were listed on many pages of tiny text (both from Pfizer and from Moderna)The size of the pages side effects were written on says absolutely nothing about what the side effects are or their severity.> Then there are the long term consequences, many studies keep coming out with surprises (recent buzzword “Ribosomal frameshift”Ribosomal frameshift is in itself not a “long term consequence.” It is simply that depending on where translational of a piece of mRNA starts, different proteins can be produced. This is an essential part of how our own (or any other organism’s mRNA) is translated into proteins. other proteins present in a cell can regulate which proteins can be produced - again an essential part of how our cells function. This has been known about for decades.It is one of the main potential areas that could lead to side effects of an mRNA vaccine so understanding what other proteins might be translated and under what circumstances is important.Please don’t fear monger without any actual evidence to back it. reply cscurmudgeon 12 hours agorootparent> The size of the pages side effects were written on says absolutely nothing about what the side effects are or their severity.Naive question: Do they say what the side effects are and their severity in an easily accessible manner somewhere? reply hammock 11 hours agorootparentThe official package insert can be read here: https:&#x2F;&#x2F;www.drugs.com&#x2F;pro&#x2F;moderna-covid-19-vaccine.html reply oezi 11 hours agorootparentprevFrom the sibling comment link I would say, yes:5 WARNINGS AND PRECAUTIONS5.1 Management of Acute Allergic ReactionsAppropriate medical treatment to manage immediate allergic reactions must be immediately available in the event an acute anaphylactic reaction occurs following administration of Moderna COVID-19 Vaccine.Monitor Moderna COVID-19 Vaccine recipients for the occurrence of immediate adverse reactions according to the Centers for Disease Control and Prevention (CDC) guidelines (https:&#x2F;&#x2F;www.cdc.gov&#x2F;vaccines&#x2F;covid-19&#x2F;clinical-consideration...).5.2 Myocarditis and PericarditisPostmarketing data with authorized or approved mRNA COVID-19 vaccines demonstrate increased risks of myocarditis and pericarditis, particularly within the first week following vaccination. For Moderna COVID-19 Vaccine, the observed risk is highest in males 18 years through 24 years of age. Although some cases required intensive care support, available data from short-term follow-up suggest that most individuals have had resolution of symptoms with conservative management. Information is not yet available about potential long-term sequelae.The CDC has published considerations related to myocarditis and pericarditis after vaccination, including for vaccination of individuals with a history of myocarditis or pericarditis (https:&#x2F;&#x2F;www.cdc.gov&#x2F;vaccines&#x2F;covid-19&#x2F;clinical-consideration...).5.3 SyncopeSyncope (fainting) may occur in association with administration of injectable vaccines. Procedures should be in place to avoid injury from fainting.5.4 Altered ImmunocompetenceImmunocompromised persons, including individuals receiving immunosuppressive therapy, may have a diminished response to Moderna COVID-19 Vaccine.5.5 Limitations of Vaccine EffectivenessModerna COVID-19 Vaccine may not protect all vaccine recipients. reply lawlessone 10 hours agorootparentThose last two points are why it is was the right thing to do for ostensibly healthy people to get vaccinated.Because there are people we know it doesn&#x27;t work for and people we don&#x27;t know it doesn&#x27;t work for. reply cempaka 9 hours agorootparentHow did that make it \"the right thing to do\" for healthy people to take the COVID shots? Given their extremely poor effectiveness against transmission, the odds of the groups in those last two points eventually being infected with SARS-CoV-2 were no different with vaccine uptake at 60% or 90% or 99%. What, then, is accomplished by forcing healthy adolescents (for example) to take these shots? reply ruszki 1 hour agorootparentI lost track of COVID research in the past two years. Can you please provide me some good articles about bad transmission effectiveness? I’ve seen this mentioned many times here, but my search seems contradict to that: https:&#x2F;&#x2F;www.ncbi.nlm.nih.gov&#x2F;pmc&#x2F;articles&#x2F;PMC10073587&#x2F; reply neverrroot 1 hour agorootparentConflicting papers have been published on transmission effectiveness. Everyone believes whatever they want.“Similarly, researchers in California observed no major differences between vaccinated and unvaccinated individuals in terms of SARS-CoV-2 viral loads in the nasopharynx, even in those with proven asymptomatic infection.Thus, the current evidence suggests that current mandatory vaccination policies might need to be reconsidered, and that vaccination status should not replace mitigation practices such as mask wearing, physical distancing, and contact-tracing investigations, even within highly vaccinated populations.”https:&#x2F;&#x2F;www.thelancet.com&#x2F;journals&#x2F;laninf&#x2F;article&#x2F;PIIS1473-3... reply ruszki 55 minutes agorootparentI thought that we had some new hard evidence, or revelation. So does this mean we still don’t know, or is there a considerable skew to one side in the statistical meaning, like in the case of eating red meat, or climate change, but as with everything there are contradicting studies? reply neverrroot 21 minutes agorootparentI like to look at the big numbers when it comes to the effectiveness of measures. To me they tell the story. But that requires a lot of work, so I can’t really recommend diving into it.lawlessone 12 hours agorootparentprev> but the ones for the CV vaccine were listed on many pages of tiny text (both from Pfizer and from Moderna).A whole bunch of OTC drugs carry CYA lists like this, it&#x27;s not the gotcha you think it is.> but to me as a healthy 40yo person, with no known immunity issues, not overweight etc.My father is getting a stent put in for a 70% artery blockage. Older of course but skinny, looks after his health, exercises. He had no symptoms like pain or being out of breath. Just high cholesterol.A friend of mine in his early 30&#x27;s healthy nearly got taken out by RSV last year because it aggravated a heart condition he was unaware of.People aren&#x27;t as healthy as they like to convince themselves they are. reply ethanbond 12 hours agorootparentYeah you gotta love the people like “COVID is only super dangerous if you have comorbidities!” Yeah dude have you ever walked around and looked at people? Then have you ever talked even to people who outwardly appear healthy?Comorbidities run amok, known and unknown. reply azan_ 12 hours agorootparentprevmRNA will eventually save the day for everyone, including for perfectly healthy individuals, but to me as a healthy 40yo person, with no known immunity issues, not overweight etc. etc. it is in my opinion still too risky to take just like that. It&#x27;s an interesting take. From what we know so far, the risk from taking vaccine is much smaller than from covid infection. You might counter this with highlighting the \"so far\" part, but keep in mind that many studies keep coming out with surprises not only regarding vaccine side effects, but also negative long term effects of covid. reply OCASMv2 10 hours agorootparentIt&#x27;s not a dichotomy. The vaccine is effectively only for a couple of months so you have to compound the effects of the vaccine PLUS the effects of infection. You also don&#x27;t know if the effects of the vaccine are cumulative with each new booster.And that&#x27;s even assuming it&#x27;s effective in the first place. The \"it reduces symptoms\" mantra is gaslighting since the whole point of its accelerated approval and forced application was for the reduction of transmission. reply OrvalWintermute 10 hours agorootparentprevI&#x27;ll tell that to my colleague that got Covid vaccine induced blood clots, and has been told he will be on blood thinners for life.While this is anecdotal, the amount of people I know personally that have told me they had significant health issues with a Covid vaccine or covid booster is weird. reply throwaway29812 10 hours agorootparentBlood clotting in COVID is higher than instances of Vaccine induced clotting, in every measure. For every example like your colleague, there are many more for COVID itself. reply kemotep 7 hours agorootparentWhich begs the question, how do they know that it was vaccine induced blood clots and not covid or any other possible cause of blood clots?Since at the same time mass vaccination rolled out, essentially all lockdowns were lifted and at least speaking for myself, that is when I caught Covid (and got sick in general again, no colds for almost two years was nice). reply hammock 11 hours agorootparentprev>the risk from taking vaccine is much smaller than from covid infectionDoes that matter if you get covid anyway? reply RoyalHenOil 11 hours agorootparentYes. The vaccine trains your immune system to create antibodies used to fight off the disease. Already having those antibodies when the virus appears means that it will not last as long and the symptoms will not be as severe.Without the vaccine, your body has to create those antibodies while you are already sick, and that takes time. This gives the virus a huge head start. reply shadowgovt 10 hours agorootparentPlus, the trick that this virus uses to kill is that it kicks off a cytokine storm, which is far more likely if your body is racing the virus to eradicate it than if your immune system is already primed and squashes the virus when its infection volume is small and localized.We&#x27;re not just talking the flu-like symptoms here, the most relevant symptoms are \"risk of death decreased.\" reply mewpmewp2 12 hours agorootparentprevThere&#x27;s a key difference here that cancer mRNA vaccine you are taking after you&#x27;ve been diagnosed with cancer.Otherwise for Coronavirus vaccines you couldn&#x27;t argue that risks are much smaller for any given person under any given conditions. E.g. a person could be completely isolated for the next 10 years and have virtually 0% chance of getting Covid-19, so in this case there&#x27;s no calculation that could show a vaccine being with more favourable benefits&#x2F;risks.If there&#x27;s a 1&#x2F;10,000 chance of giving you a sore shoulder that would be worse in the calculations if you are for sure to be isolated from being anywhere near the virus. reply azan_ 12 hours agorootparentOtherwise for Coronavirus vaccines you couldn&#x27;t argue that risks are much smaller for any given person under any given conditions. E.g. a person could be completely isolated for the next 10 years and have virtually 0% chance of getting Covid-19, so in this case there&#x27;s no calculation that could show a vaccine being with more favourable benefits&#x2F;risks.Well yes, in completely absurd and unrealistic situation the risk of taking vaccine might be larger than that of Covid-19. reply mewpmewp2 12 hours agorootparent> Well yes, in completely absurd and unrealistic situation the risk of taking vaccine might be larger than that of Covid-19.What about a person living in simple solitude who works remotely and orders everything in? This is a realistic, non-absurd scenario and they would possibly risk getting Covid-19 on their way to the appointment of getting the vaccine. reply lawlessone 12 hours agorootparent> What about a person living in simple solitude who works remotely and orders everything in? This is a realistic, non-absurd scenario and they would possibly risk getting Covid-19 on their way to the appointment of getting the vaccine.The biggest whine from anti-vaxers was that they were being told they needed a vaccine to do social things they enjoyed , like air travel or coughing on the elderly.I don&#x27;t think you&#x27;re hypothetical neo-hermit would being doing either of these, so itseems unlikely they&#x27;ll be \"forced\" to get a vaccine. reply mewpmewp2 11 hours agorootparentIf you do a lot of social events I agree that calculations will show that you should likely get vaccinated.None the less doesn&#x27;t mean that calculations show that to every one.Whatever \"anti-vaxers\" think doesn&#x27;t change the calculations. reply ethanbond 12 hours agorootparentprevI don’t think anyone would care about such a person opting not to get a vaccine. But this describes, pretty much by definition, a very small portion of a society. reply mewpmewp2 12 hours agorootparentIt&#x27;s not about what anyone cares, but about making calculated decisions. reply ethanbond 12 hours agorootparentWhat reply mewpmewp2 11 hours agorootparentFor an individual it is a formula of should_vaccinate = (risk_of_getting_covid_19 * bad_outcomes) - (bad_outcomes_from_vaccines + risk_of_getting_covid_19_after_vaccine * bad_outcomes_of_covid_19_after_vaccine) > 0On the group level you would have to consider the damage on the group level as well from not getting vaccinated due to increase of covid-19 spread, and increased hospitalisation levels.On the global communication and messaging level I agree you should tell everyone to vaccinate as you can&#x27;t easily provide everyone with those calculators. And not to mention people not being able to come up with correct values for those factors themselves. reply tempestn 10 hours agorootparentHowever, pretty much any real world person doing those calculations and arriving at the conclusion that they&#x27;re better off not getting the vaccine is engaging in motivated rationalization rather than reasoning. Almost no one is actually living as a hermit, unlikely to ever be exposed to covid. And just about everyone who will be exposed will be safer having had the vaccine, considering all the risks and probabilities you listed. reply mewpmewp2 10 hours agorootparentBut then you get to following parameters as well:1. Age. Not all age groups were recommended the vaccine.2. Last time or where you have had Covid-19 at all.3. General pattern of activity and the amount of contacts with other people.4. Last time you have had the vaccine.E.g. in my country through it&#x27;s technical systems didn&#x27;t allow you to get a Covid-19 vaccine when you had the virus within last 6 months. While US recommended the vaccine much shorter period of time after having had Covid-19.> Almost no one is actually living as a hermitAlso consider that in 2021, there were actually many people living as hermits, including I, as I really didn&#x27;t want to get the virus. At the time I was terrified of getting it. In 2020-2021, I did live with my partner, but we worked remotely, worked out outside away from other people and we did cardio&#x2F;gym, but with home made equipment or outside keeping distance to other people.I think it was quite common in 2021. replyneverrroot 12 hours agorootparentprevI would argue that any person isolating for 10 years will for sure have very significant health drawbacks, so that would also have to be factored in. reply mewpmewp2 12 hours agorootparentYou mean in terms of immune system not having been exposed to enough pathogens? reply neverrroot 12 hours agorootparentNot only, psychological consequences, lack of socialization, potentially lack of sun exposure, lack of getting medical checkups or adequate treatment etc. reply ryukafalz 11 hours agorootparentAlong with the health drawbacks of a sedentary lifestyle, unless you&#x27;re vigilant about getting enough intentional exercise to replace the walking around we do over the course of a typical day outside the house. reply mewpmewp2 11 hours agorootparentThat&#x27;s also a weird assumption. The sedentary part. My dream life is owning large amount of land with a house where I can be completely self sustainable, including various automations and this includes having an in-built gym. reply mewpmewp2 12 hours agorootparentprevWould depend on the individual right. Not everyone requires socialisation.Isolation doesn&#x27;t mean lack of sun exposure.Medical checkups would depend on the age and healthiness of the person. reply neverrroot 11 hours agorootparentI was talking about risks in general population, those are not zero, in spite of some individuals who post factum may turn out just fine after 10 years of isolation. reply mewpmewp2 11 hours agorootparentGeneral population wouldn&#x27;t strive for being isolated in the first place though. reply Terr_ 10 hours agorootparentprevNo, it&#x27;s a myth (albeit a popular one) that the human immune system benefits from being \"toughened up\" by actual pathogens. It&#x27;s more about calibration by non-pathogens we co-evolved with.https:&#x2F;&#x2F;www.pnas.org&#x2F;doi&#x2F;10.1073&#x2F;pnas.1700688114 reply mewpmewp2 10 hours agorootparentWhat about the isolated populations, e.g. Sentinelese that are documented to die because they don&#x27;t have the immune system developed to protect themselves against the viruses rest of the World has?Although I would agree that 10 years later in your life is unlikely to create an issue.> The Andaman and Nicobar Islands Protection of Aboriginal Tribes Regulation 1956[9] prohibits travel to the island, and any approach closer than 5 nautical miles (9.3 km), in order to protect the remaining tribal community from \"mainland\" infectious diseases against which they likely have no acquired immunity. The area is patrolled by the Indian Navy.[10] reply Terr_ 9 hours agorootparentI think that confuses:A: What behaviors or environments lead to adult immune systems calibrated to minimize autoimmune dysfunctions.B: Comparing populations that already&#x2F; haven&#x27;t developed inherited countermeasures against certain diseases.C: (In some cases) Specific pathogens where it is less-damaging to encounter them at different phases of life.In particular, I&#x27;d point out that sometimes a population strong against a particular disease does not necessarily have \"better\" immune systems... Sometimes it&#x27;s literally survivorship bias, where all of the vulnerable individuals they used to have already died in tragic ways. replymewpmewp2 12 hours agorootparentprevFor coronavirus maybe, because it&#x27;s not much of danger for you in the first place, but those risks would be thousands of times more acceptable for cancer.There are no side effects observed from mRNA CV vaccines that could be worse when compared to cancers with 1%+ mortality rate.As a reminder you won&#x27;t be taking it before the cancer, you would be taking it after the fact. reply hammock 11 hours agorootparent>those risks would be thousands of times more acceptable for cancer.This might be true if the treatment was thousands of times more effective than the best available alternative treatment. I don&#x27;t know enough about this treatment to know if that&#x27;s the case or not reply neverrroot 12 hours agorootparentprevThere are maybe no effects that we know about right now. Different people have different opinions on what their risk tolerance is, so there could be people that may outright disagree with your statement.And there were even fewer such effects when the vaccines have rolled out back in 2020 with emergency approval, but without being used “in production” beforehand.Now we know much more about the mRNA platform, but I still can’t exclude the possibility entirely for me. So I will take my chances and focus much more on quality of food, on staying fit, reducing stress etc. etc. reply mewpmewp2 12 hours agorootparentYou should do the healthy things to avoid cancer in the first place, but once you are diagnosed with a cancer with high mortality rate, it&#x27;s a whole different calculation in terms of risks vs benefits compared to Covid-19. reply bluGill 12 hours agorootparentMany cancers are caused by random chance, not factors you can do something about. sure, don&#x27;t smoke and all, but odds are still high cancer gets you. reply throwaway29812 10 hours agorootparentExactly. Cancer grows naturally in all of us, every day. reply neverrroot 12 hours agorootparentprevIndeed. My view on vaccines is that they are not a cure but act as prevention agents. reply mewpmewp2 12 hours agorootparentThere&#x27;s multiple vaccines that can also work after you&#x27;ve been exposed to a pathogen. In many such cases risks vs benefits will change by the event.For example rabies vaccines is not given to everyone, however after you get rabies you will be given that vaccine.Vaccine in this case is a less harmful training tool to prepare you for the more harmful pathogen that may still be reaching its peak strength.You can think of vaccines as a training tool. In some cases it might provide you with a response that can completely throw the pathogen out, in other cases it will just be able to fight it better. reply lawlessone 10 hours agorootparent>For example rabies vaccines is not given to everyoneIf you could spread something as bad as rabies by sneezing. You would see actual mandatory vaccinations, not the mild nudge people have been complaining about the last three years. reply jncfhnb 10 hours agorootparentprevThis is a treatment reply lawlessone 12 hours agorootparentprev>You should do the healthy things to avoid cancer in the first placeWhat are those? reply mewpmewp2 12 hours agorootparentThere&#x27;s a certain set of healthy recommendations, but to clarify none of them will absolve the risk completely. reply lawlessone 11 hours agorootparent>There&#x27;s a certain set of healthy recommendationsBut they are all very general and very vague. Eat right. Exercise. Avoiding smoking and alcohol, unless it&#x27;s red wine. Be wealthy Drink Coffee.Sorry just when I see people suggesting healthy lifestyle under this and other articles for things like heart disease mRNA treatments, it generally betrays that they view being sick as a personal failing.1 in 2 people will contract cancer according to the NHS. https:&#x2F;&#x2F;www.nhs.uk&#x2F;conditions&#x2F;cancer&#x2F;The biggest factor with cancer is age. https:&#x2F;&#x2F;www.cancerresearchuk.org&#x2F;health-professional&#x2F;cancer-... reply neverrroot 12 hours agorootparentprevNothing can absolve the risk completely, it’s all about probabilities. And even if the odds are for something, there are outliers and the genes. reply azan_ 12 hours agorootparentprevCan you completely exclude that there are some awful long-term consequences of Covid-19 we do not know about and that could be prevented by vaccine? reply neverrroot 12 hours agorootparentNo, I can’t. Nobody can at this stage unfortunately. reply pfisherman 12 hours agorootparentprevGet ready to see vaccines be used to prevent and cure conditions you that you probably think are outside the scope of what vaccines can address.Projecting further into the future, the world is going to be divided into those with access to and willingness to use biotechnology and those who don’t. Note that I am including things like fitness and health tracking apps under the umbrella of biotech. reply joneholland 12 hours agorootparentprevYou haven’t written anything of substance but antivax fearmongering. reply neverrroot 12 hours agorootparentI have to disagree with this, clearly visible under my profile. In general I try to stay clear of any vaccine debates, in general because of being labeled in certain ways I don’t identify myself with at all. reply robbiep 12 hours agorootparentprevWhere, exactly, are the long term consequences of mRNA vaccination for SARS-CoV-2? Compared with, say, actually developing COVID? (Please note I’m not arguing that receiving a COVID mRNA vaccine is entirely risk free, however on a risk weighted basis receiving a COVID vaccine is much safer than having COVID) reply neverrroot 11 hours agorootparentTo me the biggest risk is still the unknown, as in not enough experience. They have never been used “in production” until 2020.I saved this post as a favorite and set up a reminder in 2 years and in 5 years about it. Hope there will be nothing to add, and we just got ourselves a mighty vaccines platform at zero long term consequences. reply _Microft 12 hours agorootparentprevWhat does „CV“ in „CV vaccine“ mean? reply m0llusk 12 hours agorootparentCV: Corona Virus reply croisillon 12 hours agorootparentthat might explain why i never receive an answer to all the CV i sent reply neverrroot 12 hours agorootparentGood one! replyfrozenport 12 hours agoparentprevDo you have any references? reply squarefoot 13 hours agoprevIf this turns out to work as promised, in a idealistic universe, Moderna gets a big influx of money from all countries in the world and they immediately publish details and surrender any patent allowing the treatment to be produced worldwide and be affordable for everyone. reply lawlessone 12 hours agoparentIt&#x27;s not a general vaccine , but a tailored one aimed at people being treated for cancers that have a high risk of it coming back. So it&#x27;s not something that can just be handed out to everyone.I agree though if a general vaccine is created.I&#x27;m really looking forward to a future where this and heart disease or just merely annoying. reply kurthr 12 hours agorootparentIn particular: The vaccine works by instructing the body to make up to 34 “neoantigens.” These are proteins found only on the cancer cells, and Moderna personalizes the vaccine for each recipient so that it carries instructions for the neoantigens on their cancer cells. reply loceng 12 hours agorootparentAnd let me guess - there&#x27;s a massive lobbying effort to be able to patent each of these neoantigens, if they&#x27;re not already getting patents approved for them?Edit: Ah yes, why patent specifics when you can patent the umbrella which is akin to covering all bases - https:&#x2F;&#x2F;patents.google.com&#x2F;patent&#x2F;US10055540B2&#x2F;en reply blackbear_ 10 hours agorootparentVery unlikely, as the majority of those neoantigens are unique to the patient (that&#x27;s why the vaccine has to be tailored for each of them). reply loceng 10 hours agorootparentAh yes, why patent specifics when you can patent the umbrella which is akin to covering all bases - https:&#x2F;&#x2F;patents.google.com&#x2F;patent&#x2F;US10055540B2&#x2F;en reply blackbear_ 10 hours agorootparentI&#x27;m not sure that patent is worth much to be honest. The general approach has been known and published in research circles for the better part of a decade before the patent was filed, and actually training a machine learning system in that way that gives usable predictions is still an open problem afaik. replyJumpCrisscross 12 hours agoparentprev> a big influx of money from all countriesWe don’t know how valuable this tech is. It could be worth trillions. It could be niche. Risk sharing, not cost, is the currency of deal making.> immediately publish details and surrender any patentmRNA vaccine production methods are tough, e.g. Moderna’s encapsulation technology. Add to that the personalisation required for these treatments, and we’re still far from economies of scale. reply bugglebeetle 12 hours agorootparent> mRNA vaccine production methods are tough, e.g. Moderna’s encapsulation technology…and were invented with publicly-funded research that they’ve now privatized, resulting in large death tolls from vaccine inequity in poor countries. reply reissbaker 9 hours agorootparentIMO they resulted in massive reductions of death across the world. In a world where pharma companies needed to surrender their IP if it was too useful, Moderna wouldn&#x27;t have gotten funded to pioneer this new and difficult technology, and mRNA vaccines simply wouldn&#x27;t exist.Having publicly-funded research doesn&#x27;t mean you can ignore profit either — the public funding for mRNA was that the initial research was done at UPenn, which receives NIH grants. But large research universities don&#x27;t exist on public funding alone — they license the patents they hold to companies to develop them, which is a huge part of their revenue. For the mRNA royalties alone, in 2022 UPenn received over $750 million, which exceeded the total of all of UPenn&#x27;s NIH grants for that year combined. Even in smaller-revenue years like 2020, public funding represents significantly less than half of UPenn&#x27;s revenue sources. Take away UPenn&#x27;s ability to monetize their research, and their ability to fund the fundamental research goes down too.In my view the default isn&#x27;t that diseases cure themselves. The default is people die. If pushing forward the state of the art of vaccine tech didn&#x27;t need money in order to be built, what was stopping anyone else from doing it? And if it does need money, well... Then it needs to be well-funded, meaning it needs to show the potential for profit. I don&#x27;t think charity has proven an excellent model for funding advanced scientific development of this kind. reply bugglebeetle 8 hours agorootparentA long wall of obfuscation and pharmaceutical industry apologetics that can be trivially dismissed by simply pointing at myriad other vaccines that were also publicly-funded, are not constrained by IP, and saved millions of lives.Your last point is also hysterical because yes, the research needs massive investment, the state provides it, and then allows private companies to run off with it and charge them again for the very thing they made possible in the first place. It’s about as insane and corrupt as could ever be imagined.I’ll also highlight that Cuba developed a highly effective COVID vaccine as a poor country, under extreme sanctions, and shared hundreds of millions of doses to other poor countries, which further demonstrates what you claim to be untrue. reply signatoremo 6 hours agorootparent> Your last point is also hysterical because yes, the research needs massive investment, the state provides it, and then allows private companies to run off with itA research is only the beginning. It usually takes many years and lot of investment to bring a drug to the market. There is also high risk that none of the investment will be recouped. That’s the role that private companies can do well. They don’t just “run off with it”.ASML, for example, benefits heavily from EUV research done in the US with public fund. Yet you can’t say they ran off with it. They invested heavily, and it took them many years to perfect the technology. reply bugglebeetle 6 hours agorootparentRefer, again, to the example of Cuba’s COVID vaccine. reply signatoremo 48 minutes agorootparentCheck out the list of drugs that Moderna has in the pipeline:https:&#x2F;&#x2F;www.modernatx.com&#x2F;research&#x2F;product-pipelineAlmost all of them are still long way to go before becoming commercial products, if at all. Does it look like they run off with it as you asserted? Or do they have to shoulder the development cost and risk?One of the drugs is of course their COVID vaccine which is a home run. It’s the potential of such windfall that makes them willing to bet on all of the other drugs in the pipeline.You cited Cuba’s COVID vaccine. How many other drugs do they have in pipeline? Right, not so many compared to all of the drugs developed by private money. Ask yourself, with regard to their COVID vaccine, how extensive was the testing? can they scale the production? how widely can they distribute the vaccine? replyJumpCrisscross 12 hours agorootparentprev> were invented with publicly-funded research that they’ve now privatized, resulting in large death tolls from vaccine inequity in poor countriesThis is an odd complaint for this circumstance.Those poor countries didn’t materially fund these vaccines’ development. And production was fundamentally constrained; nations entered into bidding wars to secure them. In the end, geopolitics dictated which vaccines—if any—poor people got. Covid vaccines were distributed through non-market channels. reply bugglebeetle 12 hours agorootparentIt’s not at all odd because we learned the disastrous, widespread international consequences, of privatization of HIV&#x2F;AIDS treatments in the 90s and 2000s. Treatments were developed with publicly-funded research in rich western countries, hoarded for profit at the cost of mass death in the developing world, which led to increased severity of the disease from unchecked spread, which then spread back into the countries that hoarded them. There is nothing more fundamentally stupid than trying to manage global pandemics with nationalism and IP restrictions in a world with interconnected economies and constant international travel. reply throwaway8877 46 minutes agorootparentWe saw the same in case with covid too. But, ..., the main issue is that those poor countries have increased their population by many orders of magnitude and this makes large scale support incredibly hard. Imagine if African population was still around 200 million people as it was in 50s instead on 1.5 billion it is today. replybawolff 12 hours agoparentprevI imagine an individually customized vaccine would still be expensive even if produced at-cost. Things get cheap when they can be mass produced. reply LorenPechtel 6 hours agorootparentFrom what they said it doesn&#x27;t actually need to be individually produced. Rather, there are 34 possible targets, which ones are actually used depends on the patient&#x27;s cancer. You aim at the targets which the tumor expresses, the others would simply cause side effects for no gain.Thus, instead of custom manufacture your distribution system is 34 separate compounds, the patient receives whatever combination would be best for their tumor. Probably not viable at the ordinary pharmacy level, but viable at the cancer center level or a compounding pharmacy. reply dragonelite 12 hours agoparentprevWe have seen how that went during covid, so dont i wouldn&#x27;t hold your breath on it. The only hope is competition from China and others initiating a race to the bottom. reply bawolff 12 hours agorootparentI mean, it seemed like it worked pretty well with covid. Vaccine was produced very quickly, and in most countries was free. reply croes 12 hours agorootparentIt wasn&#x27;t free, just paid by the government means tax money means money that&#x27;s now missing for other things in countries like Germany because they still are fixated on the 60% debt ceiling based on an Excel error. reply bawolff 9 hours agorootparentI mean the person i was responding to said \"Moderna gets a big influx of money from all countries in the world\" - i assume that cash would come from tax revenue. Money doesn&#x27;t just magically appear. reply oldgradstudent 11 hours agorootparentprevIt was a bit expensive, and it took some time to ramp up production. Still impressive, though.The problem was that almost everyone who got the vaccine also caught the disease. reply blackbear_ 10 hours agorootparentVaccines are not intended to prevent people from getting the disease. Their purpose is to make the body defeat the disease faster while taking less damage. reply oldgradstudent 7 hours agorootparentThis claim would come as a great surprise to the FDA, as they approved it soley for prevention of COVID. reply LorenPechtel 5 hours agorootparentNo vaccine in existence actually prevents infection. Rather, it primes the immune system so the battle is typically a total rout, the invader losing very quickly and the patient never noticing. That&#x27;s what they got against Covid&#x2F;Wuhan and the world actually did achieve herd immunity--against Covid&#x2F;Wuhan. (The Wuhan strain has been extinct for some time.)Unfortunately, it turned out to mutate quite rapidly (like the flu) and soon the battles weren&#x27;t always a rout. It still reduces severity, though, and thus is still used. reply oldgradstudent 5 hours agorootparent> the invader losing very quickly and the patient never noticing.That&#x27;s called preventing Covid. Covid is the disease, not simply an infection.The problem is that the vast majority of people who got the vaccine did in fact notice very well they&#x27;ve had Covid.> That&#x27;s what they got against Covid&#x2F;Wuhan and the world actually did achieve herd immunity--against Covid&#x2F;Wuhan. (The Wuhan strain has been extinct for some time.)The Wuhan strain wasn&#x27;t that relevant even before the vaccines were widely available.> Unfortunately, it turned out to mutate quite rapidly (like the flu) and soon the battles weren&#x27;t always a rout.Which was predicted and expected all along.> It still reduces severity, though, and thus is still used.Only if you take really awful studies seriously.Nobody bothered to test this hypothesis in a proper trial. replynoduerme 12 hours agorootparentprevNot free enough for anticapitalists! reply satchlj 12 hours agorootparentprevIf by \"free\" you mean that taxpayers payed huge amounts of money then sure... reply squarefoot 2 hours agorootparentWhat is wrong in funding everyone&#x27;s health through taxes? I paid a lot of them for years, although living a quite healthy life, then one day everything changed, and in just a couple years I had to get covid vaccines, a vertebral stabilization after an accident, then two stents after a heart attack. All for free. So far, my healthcare taxes have been the best possible investment. reply maxerickson 11 hours agorootparentprevIt was like $30 a dose or less. Super cheap. Egad. reply mb_72 11 hours agorootparentNot to mention saved significant hospital costs as people were unwell but not sick enough to require more intensive and expensive treatment. reply amelius 12 hours agorootparentprevMaybe in 30 years when all the patents have expired. reply DoubleDerper 12 hours agoparentprevi love this idea. worldbank or imf or gates or thiel foundation compensates the company for opening the patent for the benefit of humanity. reply lawlessone 12 hours agorootparentimagine the conspiracy theories though...\"No you can&#x27;t just fight cancer this way, that&#x27;s cheating, drink this completely natural juice made from pulped fruits and vegetables that have been growing separately for 80 million years.\" reply treyd 12 hours agorootparentprevWhat incentive do Gates or Thiel have for doing that? reply lawlessone 11 hours agorootparentI could see Gates doing it. reply loceng 12 hours agoparentprevFree market would automatically cause a big influx of money.A \"big influx of money from all countries in the world\" - if from governments themselves and not individual citizens buying - would in fact be a proof point against its claimed effectiveness, if forcing the otherwise free market (via easily-commonly captured political-government-institutional channels) is what&#x27;s required to drive funding towards it, e.g. regulatory capture to provide profits when they may not actually be deserved-warranted - say by trying to get approval for a fraudulently approved product, e.g. \"... the 26 pharmaceutical companies paid some $33 billion in fines during the 13-year period. The top 11 alone accounted for $28.8 billion\" - https:&#x2F;&#x2F;www.pharmaceuticalprocessingworld.com&#x2F;gsk-pfizer-and...And arguably this is just the tip of the iceberg and the industry hasn&#x27;t been held accountable for most of their fraud since the industrial complex formed.This isn&#x27;t just a problem with the pharmaceutical industry but with clearly corrupt-captured regulators like the FDA - who allowed this fraud to happen to begin with, missing or not checking into whatever lies were presented for the fraud to occur and the products to make it to market.And that&#x27;s why headlines like \"Moderna&#x27;s mRNA cancer vaccine works better than expected\" should be taken with extreme skepticism. reply SV_BubbleTime 9 hours agorootparentVery little about medical development is free market.But otherwise, I always agree with people taking a default being skeptical approach to any medical news. reply loceng 7 hours agorootparentThe current status quo system, sure, but it&#x27;s passion of scientists, researchers, biologists who inevitably make discoveries - it&#x27;s the industrial complex and those wanting to control, mainly for profits sake, that has changed the way it worked and solutions were found early on. reply mberning 12 hours agoprevShould this really be called a “vaccine”? I feel like that is going to give people the wrong impression about what it does. I think labelling it gene therapy, immune therapy, or something like that would be closer to what it actually does.Also really interested to see if people will need and&#x2F;or benefit from periodic re-treatment. I guess they can figure that part out if it makes it to market. reply jph 12 hours agoparentYes this should really be called a vaccine. A vaccine is a preparation that stimulates the body&#x27;s immune response against diseases. For example, a specific vaccine injection (such as a flu shot) can boost the body&#x27;s own immune system to help protect against a specific disease (such as the year&#x27;s flu variant). reply lumb63 12 hours agorootparentAll previous vaccines were aimed at preventing infectious diseases. None of them are (primarily) indicated for preventing recurrence of a non-infectious disease. Whether it meets the technical definition or not, we can probably agree this is fundamentally different to all other vaccines.Also, since no other vaccines are custom to each patient, using the term “vaccine” could cause confusion. Do we really want to give people the idea that they can be “vaccinated against melanoma” when the reality is that they can receive a treatment to prevent their specific melanoma, which they have to have already had, from recurring? It seems laden with confusion. Personally, I’ll just call it immunotherapy. reply jph 7 hours agorootparent> All previous vaccines were aimed at preventing infectious diseases.This is a common misconception, and that&#x27;s whythe U.S. CDC is now emphasizing that vaccines can do more to help people.The most important recent example is the world&#x27;s widespread rollout of COVID vaccines. These are primarily aimed at better protection against severe illness, hospitalization, and death; these are not primarily aimed at preventing an individual from ever being infected by any COVID variant. reply AuryGlenz 2 hours agorootparentWell that’s just plain revisionist history.Name another example. The flu vaccine, maybe? But that’s because they don’t know which strains to target.Both the CDC director at the time and Biden said the vaccine would prevent you from getting COVID.It’s insane we’re changing the definition of words because the COVID vaccines were kinda crappy and that would be bad politically.Not that the vaccine discussed here shouldn’t be called one - it absolutely is. reply chiefalchemist 11 hours agorootparentprevThe idea that marketed vaccines are safe & effective has been well marketed and deeply planted in the minds of the public. It&#x27;s easier to leverage that familiarity than it is to build awareness and trust for \"immunotherapy\". reply owlboy 8 hours agorootparentprevWell, they could at least always say something like “…vaccine, but not like you are thinking. You, like billions of other people, have a misguided and frankly incomplete understanding of the word. …” reply satchlj 12 hours agoparentprevYes the word &#x27;vaccine&#x27; seems misleading to me as well, as people tend to think of vaccinations as ways to prevent contagious illness. I think immune therapy is a much better term. reply maxerickson 11 hours agoparentprevWhy do you wanna call a thing that stimulates an immune response to an antigen a gene therapy?It&#x27;s using the same mechanism as vaccines (exposure to the proteins in question), it&#x27;s utterly bizarre to conflate it with gene therapy.It&#x27;s probably reasonable to call it a vaccine based immunotherapy. reply base698 8 hours agorootparentBecause it transcribes something from a gene? reply DonsDiscountGas 7 hours agorootparentThat&#x27;s not what gene therapy means.>In 1986, a meeting at the Institute Of Medicine defined gene therapy as the addition or replacement of a gene in a targeted cell type. In the same year, the FDA announced that it had jurisdiction over approving \"gene therapy\" without defining the term. The FDA added a very broad definition in 1993 of any treatment that would ‘modify or manipulate the expression of genetic material or to alter the biological properties of living cells’. In 2018 this was narrowed to ‘products that mediate their effects by transcription or translation of transferred genetic material or by specifically altering host (human) genetic sequences’https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Gene_therapy#ClassificationmRNA vaccines don&#x27;t alter the host genome, hence not gene therapy. You could use a broader definition I suppose since (ideally) they lead to the production of certain immune cells; and that would also include all other vaccines. reply justsee 6 hours agorootparentmRNA vaccines are a gene therapy, and a gene therapy does not mean \"must irreversibly change the genome\".Take for instance Moderna&#x27;s own SEC filing discussing mRNA [1], which summarised the regulatory situation:\"Currently, mRNA is considered a gene therapy product by the FDA. Unlike certain gene therapies that irreversibly alter cell DNA and could act as a source of side effects, mRNA-based medicines are designed to not irreversibly change cell DNA; however, side effects observed in gene therapy could negatively impact the perception of mRNA medicines despite the differences in mechanism.In addition, because no product in which mRNA is the primary active ingredient has been approved, the regulatory pathway for approval is uncertain. The number and design of the clinical trials and preclinical studies required for the approval of these types of medicines have not been established, may be different from those required for gene therapy products, or may require safety testing like gene therapy products. Moreover, the length of time necessary to complete clinical trials and to submit an application for marketing approval for a final decision by a regulatory authority varies significantly from one pharmaceutical product to the next, and may be difficult to predict.\"The industry itself was quite open about mRNA being a gene therapy [2].At some point there were clearly industry and marketing concerns, and we saw a sharp u-turn into asserting they were vaccines, and additionally that any claim they were gene therapies was the mark of an ignorant rube.On the marketing front: probably a concern that uninformed memes formed around the \"changes your DNA\" fears, which at the time were unfounded.On the regulatory front: I seem to recall some potentially more rigorous regulatory approval if they were considered gene therapies instead of vaccines.It seems rather clear: they are a gene therapy, and because that adds uncertainty in terms of the regulatory pathways, and consumer acceptance, commercial interests worked hard to recategorise them as a more palatable &#x27;vaccine&#x27;.That the term &#x27;gene therapy&#x27; is being redefined to exclude mRNA seems an exercise in commerce rather than science, given the history available to anyone who cares to look.[1] https:&#x2F;&#x2F;www.sec.gov&#x2F;Archives&#x2F;edgar&#x2F;data&#x2F;1682852&#x2F;000168285220...[2] https:&#x2F;&#x2F;asgct.org&#x2F;publications&#x2F;news&#x2F;november-2020&#x2F;covid-19-m... reply LorenPechtel 5 hours agoparentprevIt primes the immune system to be vigilant for certain patterns. That&#x27;s a vaccine. It is not altering genes, it&#x27;s not gene therapy. It is not otherwise boosting the immune system, it&#x27;s not immune therapy.The fact that it&#x27;s target isn&#x27;t realistically transmissible has no bearing on it&#x27;s nature. We don&#x27;t give people rabies vaccine to people to keep them from spreading it, either. In humans the R0 for rabies is exceedingly close to zero in any society that understands what rabies is. reply Turing_Machine 11 hours agoparentprevIt is definitely not a gene therapy. reply fwungy 12 hours agoprevnext [5 more] [flagged] gumballindie 12 hours agoparentPeople monitored via 5g by the angry agi goddess. Truly, this flat earth of our’s is magnificent. Thanks for reminding me of that. reply guraf 11 hours agorootparentRidiculing people who are worried about long term effects of something that has been in use for only a few years is such an HN thing.There&#x27;s countless precedent for things once touted as being safe to turn out incredibly damaging. We just don&#x27;t know the long term effect yet. Please spare me with the \"but there&#x27;s no mechanism for it to do harm!!!!!\" the same was said for all the other drugs that ended up causing cancer or malformed babies and were only pulled from the market after decades of \"nutjobs\" being dismissed for suggesting a link. reply lern_too_spel 10 hours agorootparentThe vaccines that people are getting ridiculed about have been administered to more than 5 billion people. If there were any such extremely damaging long term effects, we would have seen some already. reply gumballindie 11 hours agorootparentprevThat is true. But I wish there was more evidence presented for it. Crying wolf does more harm than good. reply cubefox 12 hours agoprevThe headline borders on clickbait. The topic is a skin cancer vaccine, not a cancer vaccine. reply JakeAl 11 hours agoprevThe question to ask is why a traditional protein-based platform isn&#x27;t being tested next to the mRNA-based platform. reply DrAwdeOccarim 9 hours agoparentIt’s been tested for decades. The issue is protein-based platforms deliver the protein outside the cell. This means most do not get processed and presented by MHC-1 and 2 the way a genetically encoded therapeutic does. The issue with DNA is it separates the innate immune activation signal (eg cGAS and TLRs) from the presentation so you don’t get co-signaling. This may be the ticket why mRNA could work… reply lame-robot-hoax 10 hours agoparentprevWhy is that the question to ask? reply SV_BubbleTime 10 hours agorootparentMaybe we found out in 20 years there is an mRNA to DNA long term issue? Maybe it’s good to not just one-path cancer treatments? reply fnordpiglet 9 hours agorootparentYou’re aware that mRNA is a fundamental part of DNA based life biology, so there are lots of long term issues, by design of biology. Just probably not the ones you’re thinking of.I would note there’s billions upon billions upon billions invested in developing cancer treatments across hundreds if not thousands of research labs, not just Moderna. This isn’t an all eggs in one basket thing.https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Messenger_RNA reply LorenPechtel 5 hours agorootparentprevOur body has been using mRNA for at least hundreds of millions of years if not billions. (I&#x27;m not going to bother to look up how far back it goes.) We have DNA to mRNA proteins, we do not have mRNA to DNA proteins.Why should our body suddenly start doing something that has never happened? The mRNA vaccines are simply slipping some bogus production orders into the job queue, they go nowhere near the master blueprints. There are various forces that would like you to believe otherwise. Mostly originating in Beijing and Moscow. reply preciz 11 hours agoprevLast company that I will trust with my health. These kind of articles are pushed very frequently to front page. reply lawlessone 11 hours agoparentYeah but you also fall for fake documentaries about how HPV vaccine is dangerous. reply LorenPechtel 5 hours agorootparentOh, but it is!It makes women not so scared of sex, thus making them more likely to end up in the infernal regions. Never mind it if it took them longer to reach their final resting place. reply Euphorbium 12 hours agoprevDo you have to subscribe and get them every month, or they work longer? reply mikewarot 8 hours agoprev [–] I fail to see how anyone ever allowed the terms mRNA and Vaccine to be near each other. It&#x27;s gene therapy, not a vaccine. I wish I hadn&#x27;t been lied to before I, and my family, was injected with this stuff.I guess I should have known better when the Pharma companies were pre-emptively granted immunity for any complications from this treatment. It roasts on the pyre any institutional trust the FDA and CDC had left.As a result of all of this, the control group in this unplanned and reckless experiment is quite small. Its not outside of the realm of possibility to have a \"Children of Men\" situation in a decade or two because of this.It may end up being that only the people who refused this have long and healthy lives. reply LorenPechtel 4 hours agoparentThe reason they got immunity is that the percentage of claims about vaccines that are valid are minuscule. Juries don&#x27;t do well when faced with an obviously harmed \"victim\" and a \"bad guy\" with deep pockets. They see the suffering victim and no evidence to prove yes or no on whether the \"bad guy\" inflicted the harm and tend to side with the victim.The only way the vaccine industry can survive in such a legal climate is to get the claims reviewed by medical people rather than juries. Lawyers hate it because they can&#x27;t woo the jury with the plight of the victim.Society would work a lot better if more systems worked this way. Locally the lawyers utterly hated the system where to bring a malpractice case you had to go before a board of three lawyers and three doctors first. While you could then go to trial with less than 4 votes from that board your chances were minimal when the jury heard what the board said. Effectively that meant you had to convince one of three doctors your claim was sound--if you can&#x27;t do that why do you think you have a case? reply badsandwitch 8 hours agoparentprev [–] mRNA can be used as indirect vaccine, by manipulating cellular factories to produce a foreign object that draws the attention of the immune system.There is no known pathway of persistence in this mechanism, and there is good reason to believe such a mechanism cannot exist as it would be a highly versatile attack vector and viruses would have evolved to be much more virulent by using it. Retroviruses do complex work to infiltrate the DNA repository of a cell, if all it took is to drop some mRNA it would have been exploited and &#x27;patched&#x27; a billion years ago. replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Moderna and Merck have collaborated to develop an mRNA-based cancer vaccine called mRNA-4157.",
      "The vaccine has shown promising results in preventing melanoma recurrence or death when combined with Merck's Keytruda treatment.",
      "In a phase 2b study, the combination therapy reduced the risk of recurrence or death by 49% after three years compared to Keytruda alone.",
      "These positive findings have led to the launch of a phase 3 trial for the combination therapy.",
      "The vaccine may potentially be available for accelerated approval in some countries by 2025.",
      "Moderna and Merck are also exploring the use of the vaccine in other types of cancer, such as non-small cell lung cancer."
    ],
    "commentSummary": [
      "The discussion delves into different aspects of vaccines, such as the efficacy and potential side effects of Moderna's mRNA cancer vaccine.",
      "It also touches on the ongoing debate surrounding the definition of a vaccine and concerns related to the safety and long-term effects of COVID-19 vaccines.",
      "The discussion further explores topics such as blood clotting issues, the significance of vaccination in the context of COVID-19, funding models for vaccine development, personalized vaccines for cancer treatment, drug pipeline progress and challenges, and the role of skepticism within the pharmaceutical industry."
    ],
    "points": 364,
    "commentCount": 190,
    "retryCount": 0,
    "time": 1703620721
  },
  {
    "id": 38771513,
    "title": "Online Shopping Frustration: Website Chaos and Unclear Filters",
    "originLink": "https://www.openmymind.net/Your-Website-Search-Hurts-My-Feelings/",
    "originBody": "I wouldn't mind this huge ad so much if the pancake mix was made with lentil flour...on second thought, that doesn't sound good: And if I complain about not being able to sort by price/quality (or weight), I assume some people will point out that choice adds complexity. But it's hard to understand what value this choice is adding: Since we're talking about facets, I'm curious what category those other 632 products fall under: At costco, a search for \"rice\" yields this amazing facet: Surprise, the search returned 25 results (seriously). I'm particularly intrigued by rice that isn't vegan, lactose free, or gluten free: The sorting made me check wiki to see how much revenue they have ($242 billion): There must be a term in information theory that describes what's going on here: You can't tell, but the 2nd result is also alcohol free (the first 10 results are). For any Singaporean thinking that Alibaba ruined Lazada who ruined Redmart: no. I don't think anyone would be offended if I said e-commerce in the Philippines has a lot of catching up to do. But this first page search result is something else (yes, they sell a lot of other laundry detergent): The inspiration for this post is that nintendo.com lets you filter for 2+ players, but that includes single system, local wireless and online. So there's no way to get a list of games that supports couch coop. Unless you want to check 1680 games that is: Haha, J/K. The search won't return more than 1000 records. And while we're talking about nintendo.com, if you goto Games > Nintendo Switch Games: You'll get 120 games and only 120 games. Of course the list of switch games isn't under Games > Nintendo Switch Games that would be silly. But, for days I've tried and failed to figure out what these 120 games have in common and why they get this special treatment. I can't sleep.",
    "commentLink": "https://news.ycombinator.com/item?id=38771513",
    "commentBody": "Website search hurts my feelingsHacker NewspastloginWebsite search hurts my feelings (openmymind.net) 347 points by earthboundkid 20 hours ago| hidepastfavorite187 comments jwr 15 hours agoI can explain this.About 15 years ago or so, together with a friend, we started a company whose goal was to offer a working product search for E-commerce.Faceting was just one aspect of the problem, a much more pressing issue for non-English speaking countries is that you need a way to deal with declension and various misspellings (especially from people that do not enter accented characters). So we did. Solve the problems, that is. With a pretty good SaaS offering, reasonably easy to integrate into any E-commerce site.And then we discovered that you can have a great product, but selling it is an entirely different problem, and we were not able to solve that problem. It turned out we were competing with marketing budgets: a customer (E-commerce site) could either pay us and improve the customer experience significantly, or they could spend this money on customer acquisition and improve the bottom line right away. Guess which approach won.Another realization was that customer experience might actually not matter much. People are used to crappy websites and will jump through all sorts of hoops if the price is low. So many of our potential customers preferred to pay for customer leads (price comparison sites) and offer low prices, rather than improve customer experience, because customer loyalty isn&#x27;t necessarily a thing these days.Also, if your main acquisition channel is price comparison sites (which is the case in many countries), you don&#x27;t care about search: customers will land directly on product pages (where the price is right) and will endure the crappy search to add some stuff to their carts.We ended up with a bunch of happy customers who stayed with the product for years, but we stopped trying to sell it and moved to other things, it was just too hard to sell.In other words, addressing the OP&#x27;s (excellent) article: your hurt feelings don&#x27;t matter much, and nobody really cares, as long as the customer acquisition funnel works and there is a steady increase in the bottom line numbers. reply thayne 14 hours agoparent> because customer loyalty isn&#x27;t necessarily a thing these days.Probably because so many companies have burned that loyalty for short term gains. It isn&#x27;t worth being loyal to a company if at any time they can decide to throw those loyal customers under a bus to make a quick buck. reply esafak 15 hours agoparentprevI think whether or not it matters is contextual. If I am trying to get work done, or trying to buy something on your site I very much care if search works. UX is a feature.You can acquire customers with lower prices, but can you keep them with a bad UX? reply kamikaz1k 15 hours agorootparentThe answer is clearly yes…It’s a feature but not the only feature. Ppl complain about Google search (arguably their only feature) on HN everyday, but they’re still essentially a monopoly. reply esafak 14 hours agorootparentTheir monopoly status is why they can get away with it; don&#x27;t copy them. And don&#x27;t forget that their early attention to UX, with a spartan UI, and famously fast and good response is what made them succeed in the first place.Users are deciding that chatGPT offers a better search UX in some cases and abandoning Google.So the answer is \"No, unless you are a monopoly, and even then don&#x27;t push your luck.\" reply scotty79 15 hours agoparentprevYou should have pivoted and become price comparison site with good product search. reply tempestn 15 hours agorootparentComparison sites are predominantly not about good search either, but rather affiliate relationships. reply slig 14 hours agorootparentAnd borderline SERP spam tactics to get organic visitors. reply scotty79 14 hours agorootparentprevSo the conclusion must be there&#x27;s exactly zero value in good search. No wonder google gets worse and worse. reply candiodari 11 hours agorootparentNo there is just too much incentive for these sites to sabotage their search ... so they do, and nobody trusts them to actually find what they&#x27;re looking for.Amazon avoided that for a long time, but not anymore (imho). These days there&#x27;s lots Amazon doesn&#x27;t have, and their search seems designed to get you buying faked results. And this is the case for all commercial searches, but still less so for Google. reply realusername 13 hours agorootparentprevThat&#x27;s the same thing for Google indeed, they are getting paid by the search ads so they don&#x27;t care of the quality of what&#x27;s below those.As long as the market share is that high, there&#x27;s no reason to improve anything. reply refulgentis 13 hours agorootparentThis reply is to a comment gently indicating we&#x27;ve arrived at an obviously wrong conclusion.Gliding past that with \"Google [doesn&#x27;t] care [about] search quality\" doesn&#x27;t add much. replyjrm4 15 hours agoparentprevPerfect.While I&#x27;ve never been as deep in the details, my perpetual answer here is:\"You&#x27;re so vain, I bet you think this search is about you?\"I understand it because people are human, but I can&#x27;t help but feel annoyance when people are like \"the search engine isn&#x27;t doing what I want!\"It&#x27;s not designed to DO THAT, it&#x27;s designed to maximize it&#x27;s own sales and we&#x27;re pretty much at the point where we should try to stop pretending otherwise. reply mvdtnz 15 hours agorootparentIt&#x27;s not vanity to complain that a faceted search tool is surfacing completely useless facets populated by obviously-wrong data and actively lying to the user about the number of results in each facet. Or returning completely unrelated products like an umbrella in a search for laundry detergent in the top 3 results.Users should expect better. People with your attitude are not helping. reply binarymax 17 hours agoprevLove this article. Worked as a search relevance consultant for years and these are exactly the problems we work on. Most of the time fixing search is a thankless job. People expect it to work and when it does they take it for granted, but when it doesn’t it damages your reputation and brand.If you want to get started fixing website search relevance I recommend the books “Relevant Search” by Turnbull and Berryman, and “AI Powered Search” by Grainger, Turnbull, and yours truly. Both published by Manning. reply bonecrusher2102 17 hours agoparentYep I work on Search as well -- both of these books are excellent.Website search is... hard. A lot of the faceting still needs to be done by hand. I think there&#x27;s probably some opportunity for LLMs to make some sort of autotagging&#x2F;categorization easier, but there will likely still need to be a human in the loop to verify. reply lukevp 17 hours agoparentprevGreat recommendation on Relevant Search. I worked on an ecom site trying to improve search relevance for years, and it’s an incredibly difficult and challenging problem. Especially because it’s difficult to measure success, and execs would constantly come up with “hey why does this query not return exactly the results I expected” which led to a lot of resources spent optimizing a handful of queries based on executive input instead of based on query frequency and query result conversion metrics. reply storf45 13 hours agoparentprevThanks for the recommendations! I also work on a decently large ecom site and search experience is a difficult problem - especially in aftermarket automotive. reply esafak 15 hours agoparentprev> People expect it to work and when it does they take it for granted, but when it doesn’t it damages your reputation and brand.That&#x27;s the definition of a table stakes feature. It&#x27;s not as if you can not implement it and people won&#x27;t notice! reply Solvency 16 hours agoparentprevI&#x27;m not trying to fix search so what&#x27;s the tldr on why it&#x27;s so hard to do with such constrained databases? The Nintendo one seems insanely egregious. Is this not just a symptom of corporate laziness rather than search problem complexity? reply binarymax 15 hours agorootparentThe searchable content may be constrained, but the queries aren’t ;)Search is hard because you need to anticipate and model the language of all potential searchers and the content.There’s also lots of ambiguity because you’ll only get one or two keywords from the searcher without any other context, and you need to take into account trends and content quality and metadata.Also, in lots of cases search is bad because the product team either doesn’t know or doesn’t care. reply Solvency 14 hours agorootparentBut his examples were about hardcoded filters&#x2F;facets. reply binarymax 14 hours agorootparentThe examples were about a keyword or two (“lentils”, “laundry detergent”, etc) and filters. reply janalsncm 5 hours agorootparentThe reality is that yes there is a long tail of searches people might search for, but for popular searches like “laundry detergent” you can check them manually. You get a lot of coverage just by fixing bugs from the top N queries. reply gukoff 11 hours agorootparentprevHe means the last examples about Nintendo.com replygniv 19 hours agoprevOn the other end of the spectrum, I&#x27;ve been impressed by Netflix search. I would search a name and they do not have that movie, but would suggest a mix of similar movies, movies with the same actors or director. It&#x27;s pretty clever, I just wish they were more explicit about it, as in showing the reasoning behind choosing a result. reply LeifCarrotson 14 hours agoparentUm, no? That&#x27;s not what I want when I search.On the other end of the spectrum, try Digikey:https:&#x2F;&#x2F;www.digikey.com&#x2F;en&#x2F;products&#x2F;filter&#x2F;embedded&#x2F;microcon...or try McMaster Carr:https:&#x2F;&#x2F;www.mcmaster.com&#x2F;products&#x2F;screws&#x2F;socket-head-screws~...Functional, well-tagged, data-driven parametric search. Rockauto.com sells auto parts, which requires a whole different type of search, but they deliver a pretty impressive product as well.I suspect that much of this discrepancy is because there&#x27;s enough of a chance that someone grocery shopping and looking for lentils will think \"Oh, I need pancake mix too\" and click on the ad. Meanwhile, someone looking for a specific grade of M8x25 socket-head cap screws or a low-power Cortex-M0 microcontroller with a particular set of peripherals is unlikely to abandon their work to add a hose clamp or electrolytic capacitor to their cart instead.Market forces seem incapable of delivering a product with a better user experience if that hurts the bottom line even a little bit, they&#x27;re doing gradient descent to local minima and powerless to say \"no\" to finance and climb out of the awful product that results. reply mvdtnz 15 hours agoparentprevNetflix search is possibly the most egregiously bad search in the entire industry, and unfortunately it&#x27;s infecting other players with very stupid ideas. Netflix search is unfaceted, unsortable, unfilterable and worst of all it very often knows what you&#x27;re searching for and instead of returning \"we don&#x27;t have this title\" it will return a bunch of different, loosely related titles. reply thenickdude 6 hours agorootparentWhat&#x27;s worse is that they explicitly know that they don&#x27;t have that title, because they do have that title if you visit from a different region. But they waste your time with showing you irrelevant results instead. reply andirk 16 hours agoparentprevTheir input search works fine, but clicking through the categories seems to surface all the dogshit Netflix-created content first. It&#x27;s like digging through trash.My main issue with the input search is the TV keyboard. There was a Netflix hack day that created a circular keyboard like a dial w&#x2F; the selector a joystick in the middle, so pressing up to 12 o&#x27;clock was A, 12:05 was B, etc. And the joystick would return to center per selection! It was everything I ever wanted in a thing, and now I can&#x27;t even find the article on it. reply nurbl 17 hours agoparentprevSearching google&#x27;s movie service for older movies is annoying because a lot of times you just get a remake. Even if they have the old version! The only way I found to get to the original is to search for some actor in it and scroll... reply WarOnPrivacy 17 hours agoparentprevI had migrated to watching Netflix content on pirate sites for the better experience and stayed for the better experience.> I&#x27;ve been impressed by Netflix search. I would search a name and they do not have that movie, but would suggest a mix of similar movies, movies with the same actors or director.An excellent, early pirate feature. Good on Netflix for adopting it. reply WarOnPrivacy 19 hours agoparentprevSweet! Can Netflix search by age rating yet? reply gniv 18 hours agorootparentStart typing something like age:8 and you&#x27;ll get a relevant category suggestion. reply WarOnPrivacy 17 hours agorootparent>a relevant category suggestionI had waited so long for rating filters, my kids became adults.However, I was wondering what Netflix got after 17 years of development at $215k-$700k per dev - and now I know. reply VeninVidiaVicii 18 hours agorootparentprevage:35 reply DonHopkins 17 hours agorootparentprevYou can type age:!well to get movies that didn&#x27;t age well. reply cykros 16 hours agorootparentJust watched Trading Places again on Christmas Eve. While I wouldn&#x27;t say it&#x27;s aged terribly, I will say this: nobody does blackface like Dan Akroyd. reply pknerd 19 hours agoparentprevDo not want to miss a chance to empty your pockets reply eatonphil 19 hours agorootparentThat doesn&#x27;t make a ton of sense in the Netflix case since presumably it&#x27;s actually more expensive for every additional movie you watch since your subscription price is fixed and each additional movie you watch costs bandwidth and compute? reply lukevp 17 hours agorootparentNetflix wants you to justify the cost in your mind. The bandwidth is free. Netflix is such a high volume of internet traffic that the CDNs actually have edge content caches within the DCs of most major ISPs, so when you stream you’re actually streaming from a box at your ISP (or a cross connect to Netflix) and not costing Netflix any internet-facing bandwidth besides API calls. This setup is cheaper for everyone because the ISP doesn’t have to subscribe as large of an internet pipe because the high bandwidth streaming is siphoned off into private networking and never leaves the DC. reply madeofpalk 19 hours agorootparentprevOn the other hand, people that don&#x27;t watch any content are at the highest risk of cancelling their membership. reply pknerd 19 hours agorootparentprevI will get bored if I am not exposed to what is available on the platform and will get unsubscribed. It&#x27;s like social feed of a social media platform reply leephillips 19 hours agoparentprevI don’t understand. How is it useful for Netflix to show me movies that they don’t have? And how hard is it to implement search over their catalog when they only seem to have about 100 movies? reply pc86 18 hours agorootparentJust because they don&#x27;t have the rights to stream The Office doesn&#x27;t mean they don&#x27;t know what shows and movies they have are enjoyed by people who also enjoy The Office. This is literally the entire point of search - show what you&#x27;re searching for if it exists, and show related things after it. reply cratermoon 17 hours agorootparent> This is literally the entire point of search - show what you&#x27;re searching for if it exists, and show related things after it.That might make sense in an abstract way, but no, that&#x27;s not what people expect. If I&#x27;m searching for something at Netflix, I don&#x27;t want the answer to \"does it exist?\", I want the answer to \"do you have available for viewing?\". reply pc86 16 hours agorootparentSo you search for The Office on Netflix which is only available on Peacock as far as I know. How is a screen that says \"We don&#x27;t have The Office, sorry!\" with no other information at all preferable to this? https:&#x2F;&#x2F;i.imgur.com&#x2F;W6a1hDk.png reply cratermoon 12 hours agorootparentNo, it&#x27;s fine to offer similar products. Good on Netflix for this.In the more general sense of the search topic, I don&#x27;t want a search to return things they don&#x27;t have. If it&#x27;s out of stock, say so prominently. Don&#x27;t make the user click through to the item itself only to see \"out of stock\". reply ry4nolson 13 hours agorootparentprevbut netflix doesn&#x27;t tell you that. if they don&#x27;t have what you searched for, it shows you other things they DO have that are similar. reply ruszki 1 hour agorootparentIt tells you that in the first line of the results, like the sibling comment of yours proves it. replychefandy 19 hours agoprevWhile the overall point holds true, some of the complaints don&#x27;t make sense. Searching \"rice\" for example will probably get you parboiled white rice, instant red beans and rice, chicken and rice canned soup, and rice pudding— many of those facets make prefect sense. If you were searching for rice paper for crafting, or even \"nuts,\" that \"food\" facet would be plenty useful.You can&#x27;t look at one narrow usage path to judge an interface. You can only customize the interface so much for specific use cases before it becomes confusing as hell to use because it lacks consistency between tasks. reply latch 19 hours agoparentThe complaint about \"rice\" is that the tagging is incomplete. \"vegan\" isn&#x27;t a bad facet, but the count is wrong. When you select \"vegan\", most vegan products are removed. I don&#x27;t see how filtering wrong data isn&#x27;t a valid complaint.The problem is obviously that the data isn&#x27;t tagged. But Costco is huge, profitable and doesn&#x27;t have a huge inventory, so why can&#x27;t they tag it? And if the tags are so incomplete, maybe it&#x27;s better not to include them? reply pixl97 18 hours agorootparentTagging products sounds like something a LLM could do rather well with just a little human supervision. reply binarymax 17 hours agorootparentHave worked on this and it’s between 60 and 90% OK in my experiments. You run into lots of fun issues because people don’t write titles and descriptions for machines, they write them to appeal to human emotions to close a sale. reply thaumasiotes 19 hours agoparentprev> Searching \"rice\" for example will probably get you parboiled white rice, instant red beans and rice, chicken and rice canned soup, and rice pudding— many of those facets make perfect sense.Well, no, if I ask for rice, the odds are overwhelming that I&#x27;m looking for rice, in which case chicken and rice canned soup is a totally inappropriate response. It would make as much sense as returning alcohol-free wine when asked for \"alcohol\". reply chefandy 16 hours agorootparentAssuming these sorts of things about your users&#x27; intent causes big usability problems. Trying to predict how users will use search usually ends up more frustrating than not for many people. Most people who use these tools don&#x27;t have a working mental model of how search engines work, so they can&#x27;t troubleshoot easily if your expectations and theirs don&#x27;t line up. This is exactly the sort of \"smart\" feature that developers crow about all the time. Unless you have some way of predicting how individual people will use search boxes then you&#x27;re much better off with something more general. reply Spivak 18 hours agorootparentprevBut chicken and rice canned soup contains rice and the word is part of its logical title.You should be able to unearth the soup with a partial match. What makes the alcohol&#x2F;alcohol-free problem is that alcohol free is one token that can&#x27;t be subdivided. reply lukevp 17 hours agorootparentYou’re describing the problem from a computer scientist POV. OP is describing the problem from a user’s POV. Nobody cares if there’s a subset of their query in the title. That item is not “rice”. No one is searching for “rice” and wants soup. They would search for “soup” then. Tokens aren’t all of equal value in a title, and this is a big part of why ecom search is horribly bad (I have spent years working on ecom search and partnering with vendors trying to improve it. It’s a hard problem.) reply dpig_ 8 hours agorootparentWhat if I want a rice-based ready meal, but don&#x27;t know exactly what kind of meal yet (I&#x27;ll know when I see it), so I&#x27;d like to search rice and see what&#x27;s on offer?Oh! Chicken and rice soup, I could go for that. Cart. reply chefandy 14 hours agorootparentprev> What makes the alcohol&#x2F;alcohol-free problem is that alcohol free is one token that can&#x27;t be subdivided.It&#x27;s not even a problem... it&#x27;s an unfounded assumption that alcohol-free wine wouldn&#x27;t ever be relevant to the user at all. If the only place open on a holiday was Walmart that I didn&#x27;t realize didn&#x27;t sell booze, and I needed some kind of booze to make a pan sauce or a braise, I wouldn&#x27;t necessarily think to search for non-alcoholic booze-- but it would be a very useful substitution for me, and not by accident... they&#x27;re two versions of the same product. I&#x27;d much rather see that they had non-alcoholic beer and wine in my search than completely empty search results.This is why competent companies hire designers to work on interactions like this rather than leaving it up to the implementing developer&#x27;s gut instinct about user needs. If only the tagging could keep up. reply thaumasiotes 17 hours agorootparentprevThat is not a consistent position. If \"alcohol free beer\" doesn&#x27;t contain the word \"alcohol\", then \"chicken and rice soup\" doesn&#x27;t contain the word \"rice\".Moving the argument into the realm of things that matter, chicken and rice soup is not a product that can be called \"rice\", and therefore isn&#x27;t an appropriate response to a search for \"rice\". The title is irrelevant; if you&#x27;re stocking rice with a fancy name and its title doesn&#x27;t include the word \"rice\", that product still should show up in a search for \"rice\". reply Spivak 3 hours agorootparentI don&#x27;t get this position, chicken and rice soup is not the most relevant result for the query rice but it is a match. It should show up in the results somewhere. If I page through all the results of rice I expect to get all the results that conceivably match the query.Yes: chicken and rice, rice cakes, rice paper, cauliflower rice, rice cooker, rice krispies, rice university jersey, mochi, risotto.No: licorice, ricetta tiramisu. replyflenserboy 19 hours agoprevThe point of website search (at least on commerce sites) is not to help the users find what they&#x27;re searching for, but to put other, possibly related, products in front of their eyes. On most sites drilling down through limiters falls apart after about the third or fourth restriction is chosen, often because the sites seem designed to not give three or four clear choices (which is what the user wants to see), but ten or more vaguely related items (is this due to the fear that giving three or four results will imply that the site has a shortage of items to choose from?). Keep marketers & managers away from site design. reply binarymax 17 hours agoparentIt’s both. What you’re referring to is “searchandizing”, and it’s a component of e-commerce for sure. But if you’re not also retrieving what the customer wants you’ll just piss them off and they’ll shop elsewhere. reply timeon 13 hours agoparentprevThis has already roots in real stores where they shuffle stuff so it is harder for you to search for just what you need. It was counter-intuitive for me at first. Because if I can&#x27;t find in store&#x2F;site what I need I just leave. But enough people will buy also other things that store put in their way. So broken search is feature, not bug. reply pixl97 18 hours agoparentprevI may be wrong, but my guess is the site makes its money off those &#x27;promoted&#x27; search terms of most popular&#x2F;most profitable items and the work needed for the long tail doesn&#x27;t seem worth it for most teams. reply cratermoon 16 hours agoparentprevIt&#x27;s the digital equivalent of putting the cans of nacho cheese dip on a little rack in front of the shelves of tortilla chips. reply travoc 16 hours agorootparentIt’s more like asking the clerk where to find the fruit salad and him giving you directions to strawberry ice cream. reply timeon 12 hours agorootparentYou are both right. GP described intent of store, while you have described result. reply flenserboy 16 hours agorootparentprevI can see that. It feels more like the car salesman who says, \"Sure, you can look at the car you actually came here to see, but only after I show you three more we&#x27;d prefer to sell.\" reply jddj 20 hours agoprevI&#x27;ve never been lucky(?) enough to reach a scale or level of domain messiness where the standard method of indexing things naievely in elasticsearch or even typesense breaks down.Especially to such an extent as the umbrella example.What&#x27;s the failure mode here? Microservices that \"own their own data\"? reply ckorhonen 19 hours agoparentThe main failure mode here I find is nuances in more specialized datasets. Elasticsearch is great, powerful and easy to work with once you get the hang of it, but is very open-ended with lots of room to build a sub-optimal solution.Fuzzy matching and boosting often trip people up or lead to folks shooting themselves in the foot relevancy-wise.If you want really great results, you need to spend the time crafting your query, dealing with synonyms (\"pop\" vs. \"soda\"), stemming, typos, negative boosts (\"non-alcoholic\", in the example) etc.It&#x27;s not necessarily hard, just often forgotten or not included in the initial scope. reply blacksmith_tb 16 hours agoparentprevMany if not most ecom sites will be using a vendor for search (e.g. Bloomreach) so they&#x27;re sending a feed of product data to their vendor, just a subset of their product data (which may not have enough metadata for some facets people might reasonably expect). That means there&#x27;s lag, and also that the integration is stiff and brittle, hard to change without time and money. Even then, whether they&#x27;re rolling their own search or using a 3rd party, nine times out of ten their search will be ONLY for products, good luck if you wanted to search for a phrase from their privacy policy; for that or any sort of other non-product content you&#x27;re forced to use Google&#x2F;Bing&#x2F;ddg etc. reply 4pkjai 19 hours agoprevThere’s a restaurant website in Hong Kong that lets you search by region.However it shows restaurants that paid to be featured at the top of the results.So the search results will often contain restaurants that are multiple hours away. reply WaitWaitWha 18 hours agoparentTo some extent, this is consistent with Google&#x27;s mobile Maps version when searching for anything. It returns \"Relevance\" by default, not by \"Distance\".I also noticed the same in several other website where \"relevance\" trumps my sort request (e.g., price, distance). reply core-utility 18 hours agorootparentI hate this with a passion. I can be out somewhere, knowing that there&#x27;s likely a McDonald&#x27;s nearby. I zoom in to my location, type in McDonald&#x27;s, and it zooms out to a 25 mile radius to show me every single McDonald&#x27;s in the area including the one 0.1 miles away (that I then have to zoom all the way back into to get) reply grogenaut 16 hours agorootparentMore favorite of mine is then it decides to fire up the first time user experience for some new features for you or shift into driving mode you thought you disabled. The you click what you think is the nearest MCD and it&#x27;s the one close as the crow flies and on the other side of the freeway so 3 miles away.Search along route sometimes works better and sometimes also zooms out. reply earthboundkid 14 hours agoparentprevThe last time I used Craigslist to apartment hunt, people spam the listings with the name of places 30 minutes away for some reason, as though I would just give up on living in the neighborhood I was searching for if I saw there was something somewhere completely different at a price that wasn&#x27;t even that good. reply welzel 10 hours agoprevI have worked on very large e-Commerce sites and it seems there are 2 common issues:#1 a lot of teams are understaffed, overworked an not aligned, improving search is simply not a priority (at the same times, a lot of people are bored out of their mind, because of inefficient allocation of capacities). And of course, usable analytics data on search does not exist as it is hard to collect...#2 search might be actually ok, but the quality of the catalog data is a nightmare - and the catalog is maintenance by a tiny team that overworked and underpaid. To add injury to insult, there is also no clear accountability established and 5 departments and a couple of vendors play the \"not my problem\" game. Sometimes 10-30% of the catalog is renewed every year and people gave up a long time ago to report problems. reply jzb 19 hours agoprevMy wife prefers to use curbside pickup for groceries, and orders via the Harris Teeter website. It’s a disaster.The site search picks up unrelated items, misses others, and so on. I would complain about the inventory management&#x2F;substitutions too - but since they also have in store shoppers using the same inventory I guess they can’t always be 100% about what’s on the shelf.You’d think a chain that size could do better, but I doubt they have much incentive. The competition is minimal and not any better. reply yurishimo 11 hours agoparentI used to work on a regional grocery chain&#x27;s website (in the Southern USA) for online ordering and fulfillment. It&#x27;s a constant battle. Consider the sheer number of products on store shelves. The average number of products is 30k+. Now, consider that some percentage of those are constantly in flux and potentially unique per store. Holiday items, promotions, marketing from suppliers, overflow, etc etc. Anytime a price is changed or a product brought in&#x2F;out of stock, that line in the database needs to be updated. Product images and nutrition information can also be in flux as companies adjust their products in response to supply chain or financing particulars.At the end of the day, gathering and aggregating all of that crap into a stable API is a chore unto itself. Multiple people&#x27;s jobs. Updating products, associating taxonomies, in-store locations, duplicate locations, optimizing pickup routes, etc etc etc. There is a lot of grunt work here that can&#x27;t be entirely automated away when dealing such a dynamic business.Now you need to build a website that takes all of this data and somehow displays it in a format that is comprehensible and accurate for consumers. Showing relevant recommended products, search keywords, cache and image management (for potentially resizing dynamic supplier photos). Then think about the cart and checkout, two areas and Shopify and Amazon have devoted literal billions of dollars to optimize. Your volume is big enough that using an off the shelf payment provider is off the table; not to mention that you don&#x27;t have the margins anyway. Have you thought about mobile apps and the development effort required there as well?Grocery stores are hard enough as is but doing them online is a totally different beast. There&#x27;s a reason that all of the \"Kroger\" brands use the same exact website with a different logo in the corner. The only players in this space doing it reasonably well are huge conglomerates with money to burn on building the required infrastructure to build the audience. Actually, I just looked up Harris Teeter and they appear to be a Kroger brand as well. TIL. I would say that \"Kroger\" is one of the better ones. Whole Foods is also doing decently well if you can afford the price premium, but I realize for most people that isn&#x27;t a luxury they can afford.The best way to get them to improve is to keep using the service and demanding better (or use a competitor if available). These departments have KPIs. If you get the wrong product, file a complaint at the end of your order process and mark any NPS surveys appropriately. If fulfillment has a lot of bad data from elsewhere in the organization driving their performance scores down, then they are incentivized to get it fixed. I know it sucks, but the only other alternative is to not use it at all and risk is going away forever. reply gosub100 17 hours agoparentprevwe&#x27;re all looking at these as technical mistakes, but it wouldn&#x27;t surprise me at all if, during A&#x2F;B testing, people spend more when search doesn&#x27;t quite work. For a similar reason as why they put milk in the back of the store. optimizing shoppers&#x27; efficiency doesn&#x27;t optimize their profit. reply spcebar 13 hours agoprevFaceting is a deceptively hard problem to solve because it requires understanding the features of the products within the categories of products within the ecosystem of products a seller is selling, and dividing them up based on how a customer is likely to be thinking about the products. If you&#x27;re selling clothing, the customer is likely going to mean something different with the word \"long\" than they would mean if they were searching for foods. Within different categories of clothing the word \"long\" is going to mean different things. reply squarefoot 18 hours agoprevOne thing for sure is that shitty search algorithms in a page showing ads before search results, have the useful side effect of forcing users to refine search, exposing them to more ads before they find what they were searching for. reply nateburke 19 hours agoprevFixing this is the promise of enterprise deployments of LLMs, correct? That you can take the list of initial result texts from a shitty elasticsearch implementation, pass it into an LLM with a prompt \"you are a user looking for , please use common sense to comb through these results and find a few good ones\" and get better intramural search quality.The only other enterprise application I am aware of is applying the homework cheating app to customer facing roles that require email&#x2F;copy generation. reply regularjack 19 hours agoparentImplementing search correctly would solve the problem too, without requiring a square kilometer of GPUs. reply lukevp 17 hours agorootparentIf you have >100k items, “implementing search correctly” is an intractable problem if you don’t have a hundred person team of search relevance optimizers and taggers. Search is terrible until you gather tons of metadata about each item. Search is not nearly as simple as text matching within product titles. An LLM by comparison is peanuts. You could run it overnight and just have it build tags and metadata for all your items and it would probably be 75% accurate and a hell of a lot cheaper. Things can then be fine tuned by people later. reply lightbendover 14 hours agorootparentAt Amazon scale that last x% is a multi-decade problem and still going. It’s intractable full stop. reply pixl97 18 hours agorootparentprevI&#x27;m guessing we&#x27;re at or very near the point where LLMs could tag the data better and cheaper than humans. reply Spivak 18 hours agorootparentprevRight but implementing search correctly often means humans combing through the data and applying common sense to tag the products with their actual facets. So really you&#x27;re just choosing between a square kilometer of office space for cheap labor or GPUs. reply cratermoon 16 hours agorootparent> you&#x27;re just choosing between a square kilometer of office space for cheap labor or GPUs.No, with LLMs you&#x27;re getting both, because a. LLMs need the human-curated data to ingest and b. after training the model is going to get RLHF.Also, a square kilometer of office space still uses much less electricity than a square kilometer of GPUs. reply gumby 19 hours agoprevThese are hilarious and infuriating but also, perhaps, correct?My gf (who used to work in the sector) says a lot of people like to sit in front of the TV and “watch what’s on” — one of the reasons broadcast TV still exists.You see this with sites like Netflix becoming decreasingly useful, urging you to simply click on something rather than consult your curated watch list. Some of them now hide your watch list!She suggests the same applies to shopping: some people just want to push the buy button, which is well known in brick and mortar retail, especially low margin ones like grocery shops: end caps, mid shelves and all sorts of other strategies are used to get you to either buy on impulse or buy the version they’d rather sell of what you have in mind.So for these sites the same applies: show a few options and the customer will simply pick one. Spam the tags, DWIM the customer’s ill-formed, uninformed, or simply unsatisfiable query, and see if the punter buys.For nerds like us this is a pessimization. But perhaps for the majority this works better. reply pixl97 17 hours agoparenthttps:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;OverchoiceMentally when I&#x27;m at a store and I have 3 options to pick from I seemingly enjoy the process better then if I had 6 or 10. Eventually the picking itself becomes work. If it&#x27;s a topic I enjoy like motherboards or RAM then all the choices are fine, I can spend hours on that, but if I&#x27;m picking tomato sauce for a quick and sloppy dish I don&#x27;t want to spend the time selecting. reply alexhsamuel 16 hours agoparentprevWhat you&#x27;re describing isn&#x27;t \"search\", designed to give me information I asked for. It&#x27;s a recommendation engine, designed to induce a behavior in me, whose input is the \"search term\" I entered (plus god knows what other tracking data). The distinction is important: the recommendation engine is designed to get me to buy something (not necessarily what I came to buy), whereas my intention was to buy something (or buy nothing, if they don&#x27;t have it). Calling this recommendation engine \"search\" is IMO deceptive.Maybe if I&#x27;m on Netflix to watch what&#x27;s on, that&#x27;s fine. But e.g. on Amazon, I find \"search\" so adversarial that I often close the tab and go somewhere else, or give up. reply mmahemoff 17 hours agoparentprevYep. YouTube search has become smart about this, blending search with browsing. It typically gives you a handful of reasonably close matches, followed by a whole lot of wildly offtopic suggestions based on subscriptions, watch history, or who even knows. And this happens even when there would be more relevant results to show, i.e., there&#x27;s not even a pretence of trying to give the N most relevant results. The game is to populate search with whatever users will click on. reply pc86 18 hours agoparentprevI&#x27;ve noticed a lot of streaming services will inconsistently show your watch list and&#x2F;or show it in different locations on the home screen. I&#x27;ve long wondered if it&#x27;s a mix of certain services just not being available or taking too long to respond when loading the lolomo (to use Netflix&#x27;s term) and automated A&#x2F;B tests trying to maximize some metric other than \"starts watching something.\" reply plz-remove-card 18 hours agoprevOn officedepot.com when searching for a produect I have found no way to filter by \"in stock at my chosen store\" you have to search, then click through each individual item.The one and only reason I am considering office depot as a place to spend my money is because they&#x27;re local and I can go get whatever cable or thing I need, right now. reply ruined 17 hours agoparentanalytics have confirmed that surfacing inventory status is bad for the customer because it reduces conversion reply lukevp 17 hours agorootparentBad for the store you mean? Because the customer doesn’t convert (buy) something if it says “hurry only 1 left in stock” because everyone knows that store inventory is inaccurate and they ultimately don’t have 1 in stock, they have 0 in stock? reply thaumasiotes 17 hours agorootparentThe way I interpret the comment, it says this:A. [explicit claim; humorous] Analytics confirm that, if you show people whether products are in stock locally, they won&#x27;t buy those products on your website.B. [by implication] This is bad, because it \"lowers\" your sales. So you can&#x27;t display this information.C. [by implication] The retailers are making a mistake - the reason people don&#x27;t buy products on the website when availability information is displayed is that they go to the store and buy the product there.D. [summing up] If you measure something you don&#x27;t care about, you will end up doing things that make you look like an idiot. reply ruined 16 hours agorootparentclose enough reply underlipton 16 hours agorootparentprevThat&#x27;s not a good explanation. Either GGP was being sarcastic or is corpo-brained. GP was confused because, unfortunately, both are equally likely these days. reply ljdtt 17 hours agorootparentprevInteresting. Do you have sources to read more about it? reply Analemma_ 15 hours agorootparentprevIs it good for conversion when I give up out of frustration at your broken store and decide to just get the thing on Amazon instead?This isn&#x27;t just hypothetical: for a while I was using QFC (Kroger) for grocery pickup, but after one too many instances of adding an item to my cart and then only finding out at checkout that a third of my items weren&#x27;t actually available, I said to hell with it and switched to doing all that at Whole Foods instead, since their inventory counts are actually correct. reply javari 17 hours agoparentprevI did just check this, and I found it fairly easy to filter by in-stock at my chosen store. Did you set a preferred store then filter by the \"store pickup\" option? Doing that, I returned results for chair mats with the option to check other local stores. reply driverdan 17 hours agoparentprevThis drives me crazy. Local stock search is different on each store and most don&#x27;t work correctly.Try searching on Walmart to find local stock. You have to click \"In-Store\", then change the Fulfillment Speed to today and tomorrow to find things that are in stock. Except now they also show results from other stores, like Advance Auto Parts, and there&#x27;s no way to filter that garbage out.I tried searching Academy Sports for local stock. The list of items says it&#x27;s in stock but when you go to the individual item it says it&#x27;s OOS.Lowe&#x27;s and Home Depot local stock does seem to work reasonably well, for now. reply DarkSucker 13 hours agoprevIndustry, please give us good search tools like those for researching articles: (\"fog\" near \"coastline\") AND (\"lighthouse\"). Or that offer regular expressions so I can search with precision.I got used to good search tools in grad school, and then came Google. And ever since I&#x27;ve not gotten better than a result salad. Sometimes a tasty salad, but still a salad.And, to be complete, firms like Nerac offer good search tools, but you need to work with someone who performs the search for you (last I used them). reply bozhark 20 hours agoprevEven web search sucks now reply mikro2nd 19 hours agoparentDepends to some degree which search engine you&#x27;re using, even with all the so-called \"AI\" pissing in the soup. reply trgn 18 hours agorootparentI wish there still was decent boolean keyword search out there. reply andirk 15 hours agoprevI&#x27;m currently working on a website search for e-commerce and wow there really is no end to what website search can and should do. The ANDs and the ORs are kind of confusing too. Is it AND between category selections, and OR within?And did anyone else notice the omission of that useless copyright line at the bottom of websites as if we know anything about copyright law? reply AmazingTurtle 15 hours agoprevTo be fair, he searched for \"laundry detergent\" and sorted Name A-Z and was wondering why only 3M stuff appeared. Also I failed to comprehend this one> Of course the list of switch games isn&#x27;t under Games > Nintendo Switch Games that would be silly.Well.. There is a list of games when you open Games > Nintento Switch Games. What is he complaining about? reply latch 30 minutes agoparentIf it&#x27;s supposed to be sorted by name, why does \"Ariel Oxy Bleach...\" show up before \"2-Fold Umbrella...\" but \"Ariel Antibac Jumbo\" shows up after.This isn&#x27;t just bad search, it&#x27;s objectively wrong (and a lot more basic). reply xigoi 15 hours agoparentprev> There is a list of games when you open Games > Nintento Switch Games. What is he complaining about?That the list doesn’t contain all Nintendo Switch games that the site offers and there doesn’t seem to be any clear reason why those particular 120 games were chosen. reply dbetteridge 20 hours agoprevI honestly don&#x27;t understand how some websites have such awful search (Like these examples)You can get better results with a 5minute tsvector in Postgres, At least it will show the words you typed in in the results!And yes I understand it may be a different \"scale\" of products, but do people not test these things? reply pimlottc 19 hours agoparentMy cynical take is that they don’t care what you buy, as long as you buy something. So the more results, the better. Even if they don’t exactly match what you asked for. After all, the absolute worst thing to do would not show any (purchasable) results at all.If you want to get academic with it, you could say something about revealed preferences vs stated preferences (they asked for Coke but they really wanted Pepsi), but I really think they’re just throwing as many suggestions at the user as possible in the hopes they stay engaged on the site and eventually click buy on something. reply monsieurbanana 19 hours agoparentprevI spent a couple of minutes thinking of a good example of search. Google? No more by a long shot, Amazon also a cesspool. Finder finds too many irrelevant files (no matter what I type it returns a huge list of node_modules matches), this one might be on me. Search on Windows I haven&#x27;t bothered in years.The only search tools that actually give me great value are history shell with fzf on the terminal, and grep (and similar like ag or ripgrep) in combination with a fuzzy search frontend (similar to piping grep to fzf).I don&#x27;t know where I&#x27;m going with this. I guess searching just sucks now. reply Podgajski 18 hours agorootparentAmazon is beyond frustrating. The amount of totally unrelated results to a search is so frustrating. And I mean totally unrelated. reply miki123211 19 hours agorootparentprevLaunchbar on Mac and Everything on Windows are pretty great, though they solve very different problems. reply kaibee 19 hours agorootparentprevNewegg had (has? haven&#x27;t used in a while) very good search categories for PC hardware. reply gniv 19 hours agoparentprev> I honestly don&#x27;t understand how some websites have such awful searchGood website search is hard. Funnily in some respects it&#x27;s harder than web search, since there are no popularity signals a la pagerank. (There could be such signals, but you have to work hard to create them.) reply pbhjpbhj 19 hours agoprevI&#x27;ve often fantasised about scraping Amazon just to be able to actually search on the site.eBay too, it&#x27;s OK (compared to Amazon) but most of the items on a search are always \"mousepad [or whatever] suitable for\" the laptop you searched for. Why eBay don&#x27;t clean up their categories and ban sellers I don&#x27;t know, it makes the site so much worse to use.I wonder if anyone is _ever_ hoodwinked by the vast majority of sites that sort by Most Expensive by default?It&#x27;s all part of enshittification presumably, these sites monitor so many metrics it must surely be a profit generator to make search frustratingly bad. reply croisillon 13 hours agoprevI would try and install millions of apps on my iphone if the appstore was properly searchable, alas it’s not and i have like 10 apps installed reply samirillian 20 hours agoprevTangential, but Librivox&#x27;s search leaves a lot to be desired. I love that project, and can mostly just use Google, but it would be a great volunteer project for some good Samaritan with a little more experience enhancing search than me. reply Turing_Machine 16 hours agoprevYears ago, I cobbled up a simple search engine for some FOSS web forum software, mainly for people who were running private instances behind login walls -- i.e., stuff that public search engines couldn&#x27;t crawl.Someone complained that for public instances, Google worked better (this was back before Google started to suck).My response was that if I could write a better search engine than Google in my spare time, I&#x27;d be living a lifestyle considerably more luxurious than the one I actually had. :-) reply ano-ther 18 hours agoprevTagging ins tedious. Especially when it’s not you who searches but lots of other people, all with their specific ways of describing what they are looking for.And yes, the results are between amusing, annoying and infuriating.The “alcohol” example finding mostly comes from alcohol free products advertising this feature in their name - no need to name your regular beer or wine “with alcohol”, at least not in the product name. reply pixl97 17 hours agoparentLeaving myself a note here for when I&#x27;m back behind a computer.I&#x27;m wondering how well something like GPT-4 generates tags with a product description&#x2F;product image. Be interesting to see its accuracy and comprehensiveness. reply nox100 17 hours agoparentprevtagging doesn&#x27;t work period because people will tag things to try to get their product in front of your eyes and add any tag they think is popular entirely unrelated to their \"product\". reply Solvency 14 hours agorootparentSo why not punish them and have bots regularly scan&#x2F;assess tags for relevance. reply okasaki 20 hours agoprevVaguely related but Firefox on android history search (well, autocomplete) is awful. n -> suggests nothing ne -> suggests news.ycombinator.com new -> suggests news.ycombinator.com news -> suggests nothing news. -> suggests nothing news.y -> suggests nothing news.yc -> suggests nothing news.yco -> suggests nothing news.ycom -> suggests nothing news.ycomb -> suggests nothing news.ycombi -> suggests nothing news.ycombin -> suggests nothing news.ycombina -> suggests nothing news.ycombinat -> suggests nothing news.ycombinato -> suggests nothing news.ycombinator -> suggests nothing news.ycombinator. -> suggests nothing news.ycombinator.c -> suggests nothing news.ycombinator.co -> suggests news.ycombinator.com news.ycombinator.com -> suggests news.ycombinator.comhuh??? reply bee_rider 19 hours agoparentI do not understand this phenomenon where search boxes will stop presenting the phrase you want, after you’ve typed the next letter in the word. new -> suggests news.ycombinator.com news -> suggests nothingHow does this happen? It is bizarre, it is the most frustrating little thing I can think of in computers, at least since USB became flip-able. reply RheingoldRiver 19 hours agorootparentI think there is some weighting based on how often you go to a certain result after typing a certain number of letters. For example, since I was just doing Advent of Code and going to related sites:a, ad, adv -> first result is adventofcode.com&#x2F;2023adve -> first result is adventofcode.com&#x2F;2023&#x2F;day&#x2F;1 [honestly, this one baffles me, but I guess I went back to day 1 at some point after typing `adve`]adven, advent, advento -> first result is reddit.com&#x2F;r&#x2F;adventofcodeadventof -> back to adventofcode.com&#x2F;2023I actually *appreciate* this behavior; it means I can better muscle-memory a certain number of letters and go down to the first result when 2 sites have similar names (very annoying for twitter & twitch btw).So I don&#x27;t think the problem is that weights can change as you type more letters; but that an entry can disappear altogether when there are no other suggestions. reply basil-rash 19 hours agorootparentI’d much prefer they took VS Code’s fuzzy non contiguous substring searching algorithm. ra would be reddit.com&#x2F;r&#x2F;adventifcode, same for redadv, or r&#x2F;a, etc. and things like a, ad, a3, etc. would be adventofcode.com&#x2F;2023And if they included the full URL, sheesh… you could effortlessly query against search params, path segments, fragments, and origins all in one intuitive interface. reply monsieurbanana 19 hours agorootparentprevThat&#x27;s been on the back of my mind but I guess I chalked it up to a bug and didn&#x27;t think too much about it.You&#x27;re right though, this is incredibly dumb design. reply pbhjpbhj 19 hours agorootparentprevThere is a logic, you type \"n\" and \"news.yc[...].com\" comes up, so when you type \"ne\" then you&#x27;re presumably (the logic goes) looking for something other than the website that was just presented to you; otherwise you would have chosen that website already rather than continuing to search by adding more letters.In practice people type what they think will turn up the results \"news\", say, and don&#x27;t look at the results until they&#x27;re finished typing. reply bee_rider 19 hours agorootparentI guess that logic isn’t totally absurd. In that case, it would be nice if, under their “current best guess,” a descending list of all the previous “best guesses” was shown.Another annoying aspect to it is that things enter your history as you browse, so the number of characters to hit that point where it shows what you want might shift around. Which totally destroys muscle memory. This isn’t the browser’s fault of course, but it would be nice if they could design around it. reply billyjmc 17 hours agorootparentprevNot quite the same, but iOS gives me trouble with its word suggestions. Usually, it’s contractions that make me curse.‘It’ suggests It, It’s, and Its.‘Its’ suggests Its and Itself.Given that the apostrophe requires accessing a different keyboard overlay, and that only two suggestions are presented (leaving one suggestion slot empty), why not just include the contraction “It’s” in the suggestions? reply wharvle 16 hours agorootparentA few versions back, iOS started sometimes not finding my searches in settings at all.Like I’ll search the full word “passwords”, pausing after each letter, and not get the entry titled “passwords” in my results the whole time. Back out, do it again, this time it’s there by the time I type “pas”.This used to work perfectly, but has been inconsistently broken for years now. Most recent time I saw it was a week or two ago, so it’s still happening. What’s most baffling to me is that a search over such a relatively static dataset (app settings are in there, but those don’t get added or modified often) can be non-deterministic. reply nonbirithm 18 hours agoparentprevIt&#x27;s been that way for years: https:&#x2F;&#x2F;github.com&#x2F;mozilla-mobile&#x2F;fenix&#x2F;issues&#x2F;20351I lost hope it and other issues would be fixed and moved to Chromium on Android. reply malfist 19 hours agoparentprevThe worst offender of this practice has to be the windows start menu. Even when you fully type the executable name, it can show up below the fold under unrelated results.Just try to get to display settings on windows these days through the start menu. reply MSFT_Edging 19 hours agorootparentThe windows search bar will search half the web before showing you a small exe that you&#x27;ve had installed for a year. reply Kubuxu 19 hours agorootparentWhich is why I have disabled that functionality. I also don’t need M$ to know every app I’m running on my PC (although they probably have that data some other way). reply fuzzbazz 19 hours agorootparentprevAfter disabling the web search capability it seems to work properly, for me. reply kakaz 19 hours agorootparentprevIt is by design. You are talking about company which count from 3.11 to 11 going via 98. ... reply malfist 8 hours agorootparentAnd skipping 9 reply fragmede 19 hours agorootparentprevit&#x27;s as easy as start -> run -> \"ms-settings:display\" -> enterwhat? reply Zetobal 19 hours agoparentprevThe firefox awesomebar and the history search are atrocious. Sometimes even if I visited the site in the last 5 minutes there will be no autocomplete available. reply Kubuxu 19 hours agorootparentMake sure to change omnibar settings to prefer history over search. reply Zetobal 18 hours agorootparentWow. That helped quite a bit, thank you. reply jklinger410 16 hours agoprevI have a hot take that Walmart is actually slowly going out of business.They don&#x27;t do big box retailer stuff well (behind glass), they don&#x27;t do local grocery well, and they don&#x27;t even do discounted items well (5 below, Temu).Their stores are garbage and they are asleep at the wheel on their website.It is only a matter of time. reply travoc 16 hours agoparentAll of these things have been true since the beginning of Walmart. Yet here they are, almost the largest retailer on earth. reply scotty79 15 hours agoprevI would really love if there was some browser built-in skinnable system for lists that lets the user control ordering of the list and information visibility on the list, also filtering and grouping.There should be a movement for solutions that put power in the hands of the user. reply renewiltord 18 hours agoprevAll this is unavoidable. But it&#x27;s also temporary. This phase of search won&#x27;t last since we&#x27;re going to replace it with just SKU + inventory plus some fulltext search plus an LLM. An LLM browser assistant could probably solve this problem quite well.Inventory will never be well-tagged. The process change to ensure it would slow down procurement.Ideally, they&#x27;d expose an inventory XML list and we&#x27;d just use our LLM on it.The hard part is how to incentivize by permitting them control over advertising. An alternative is the browser LLM just searching across the page (Arc Browser can do this). reply DonHopkins 17 hours agoprevThat&#x27;s what you get when your AI Powered Search system is implemented by Web3 Experts who just pivoted to AI. reply marginalia_nu 19 hours agoprevFeels like there&#x27;s a pretty widespread problem that goes something like this:* PO: We need search!* PM to architect: Google how to implement search!* Architect: googles \"best search library 2023 java\" Aha! Let&#x27;s use Elasticsearch!* PM to Team: Alright, I need a search feature next sprint. Architect googled for 5 minutes and says we should use elasticsearch!* Team: Alright, integrating elasticsearch in a sprint should work. Bob, you work on loading the data, Eve, you work on rendering the search results.* Team (2-3 weeks later): Another successful value delivery! Alright, what&#x27;s next PM?* PM: PO says we need a chat bot...If it&#x27;s not obvious why this is a problem, getting search to work well requires a lot of tweaking and tuning. You can&#x27;t just slap a search engine into a product and expect it to work well. It&#x27;s hilariously incompatible with the assembly line style of work that many software outfits employ these days. It also requires deep understanding of what a search engine does and how its algorithms work. reply eusto 19 hours agoparentIt&#x27;s more about what the company considers core business and what not. Most often they don&#x27;t see the website as being important enough to the business so they don&#x27;t invest in it.We have gotten used to almost flawless experiences from amazon shopping. Google search finds (or used to) results sometimes almost like magic, etc. The thing is, that&#x27;s the core business. These companies invent new technologies and have huge teams because doing this is so hard.Now take a random supermarket chain. Their knowledge is about physical stores. Their core business has taught them where to open a store and how to arrange things in it so that they maximize their sales in that environment. It&#x27;s very hard and it takes a very long time to shift to an online model. You have to find people with the right competencies and the right leadership to convince the company to do this and this is actually very hard to do.Look at the categories example. That, to me, screams backend database. The company has invested a lot of money into building business intelligence on top of their physical stores. That organization screams \"perfectly curated data warehouse\" and I imagine suggesting something like \"we need to reorganize the way we store data\" is going to be met with blank stares if not full on outrage. reply core-utility 18 hours agorootparent> almost flawless experiences from amazon shoppingIn my opinion its too fuzzy, which is usually the opposite problem I have with website searches (e.g. searching a news site for keywords of an article I know exits, but it&#x27;s taking me too literally)If I search for \"iPhone 14 Pro case\", I don&#x27;t want to see cases for iPhone 15 __, or non-pro models. I&#x27;ve (to my own fault) bought way too many of the wrong product because I search for a specific model and don&#x27;t read the title before ordering, only to realize that Amazon didn&#x27;t give me exactly what I typed in. reply theamk 17 hours agorootparentprev> flawless experiences from amazon shoppingwhat kind of stiff are you buying you call this \"flawless\"? In my experience the Amazon search is worst there is. Search for \"AAA batteries\" and it will offer you AA and even N ones. Why on earth would anyone want that?They even got the basics wrong. The other day I was searching for power bank under $5. Instead, many listings was $10+. How hard is it to get this right?Because of horrible search quality I actively avoid Amazon when I can. reply hunter2_ 18 hours agorootparentprev> almost flawless experiences from amazon shoppingIt&#x27;s pretty good, but there are some longtime flaws that blow my mind, such as the order of results when sorting by price. It never makes sense to me. reply toast0 18 hours agorootparentI think order by price might be ordering by the lowest available price at time of index. This could be an out of date price if the index isn&#x27;t updated when other pricing is updated. It almost certainly doesn&#x27;t include shipping, as that can vary depending on where the item is and where you are so it can&#x27;t be indexed. Filtering by seller doesn&#x27;t change the sorting, in my experience, and I think neither does filtering by condition.So, if someone is selling a used item in poor condition with maximum shipping, that product is going to sort as a low priced item.At least, that&#x27;s my reverse engineered understanding. reply kayodelycaon 19 hours agoparentprevIntellectually, I knew this was common but I never stopped to consider how fortunate I am to work for a company who understands things like search are difficult.It helps we have two internal departments whose primary jobs are data management and analysis. (Real statistics, not “AI”.) reply CharlieDigital 19 hours agoparentprevThere are actually plenty of non-ES products that are way easier to integrate and tune (and get better results with less effort).- Typesense (https:&#x2F;&#x2F;github.com&#x2F;typesense&#x2F;typesense)- Algolia- Google Programmable Search Engine (https:&#x2F;&#x2F;programmablesearchengine.google.com&#x2F;about&#x2F;) reply robertlagrant 18 hours agoparentprevI think if you remove the cariacature that approach isn&#x27;t terrible, assuming there&#x27;s no more suitable technology. Why not have a basic search and iterate, including iterating the technology choice. You can hide it behind a feature flag if you don&#x27;t think it&#x27;s ready. reply marginalia_nu 18 hours agorootparentThe problem is that the sort of stuff you need to tune a search engine to actually perform well doesn&#x27;t look like work. It simply can&#x27;t be broken down into deliveries that provide incremental business value, and throughout the entire process it&#x27;s going to look like a bunch of fiddling with numbers with nothing to show for it. reply toast0 18 hours agorootparentDoes any product feature that requires care and maintenance easily fit the model of delivering incremental value?But for a search engine, showing incremental value should be easy if you have a real product reason to do it. If you expect search to help people find content, then you look at statistics on engagement --- in aggregate did people use search, does it look like they found what they wanted, did they return and use search again. In specific, which queries seem popular and provide good results, which queries seem popular and don&#x27;t provide good results. You provide incremental value by moving queries from the second to the first category, where possible. If you can kind of classify some of the less popular queries, there&#x27;s value in improving results for those too, but classification is also hard.Sometimes the numbers are easy... I participated in an application of search where a support request hit the FAQ before submission --- if search works, there should be fewer tickets and especially fewer tickets where the user can easily solve their own problem. Search in a shopping context should lead to more sales. Etc. reply pixl97 17 hours agorootparentI think you may miss how many technology projects fail in companies. A company may have no internal knowledge or skill to make search better, so they get an outside contracting group that produces a shitty product that doesn&#x27;t improve sales and costs an extraordinary amount for the lack of results.Quite often the company won&#x27;t make any more in those long tail sales, but instead better placement of the products they buy in mass. reply User23 18 hours agoparentprevA big reason why is Search today is an AI Librarian. That AI may be more or less smart, but at the end of the result that&#x27;s what people mean when they say Search. They certainly do not mean a robust query language that will allow them to iteratively refine down a subset of matching documents in a corpus until the desired document is found. reply vasco 20 hours agoprevIf heads of product were forced to use their own products for a full afternoon every week the quality of the web would increase a lot.The amount of high level product people that can&#x27;t demo their own product and cannot find features they hype up in slide decks is depressing. reply marcinzm 18 hours agoparentIn my experience it&#x27;s not just about using a product but understanding how a product functions under the hood. Yelling \"fix it, fix it, fix it, fix it\" in a loop at random teams tends to not actually get things fixed. Instead you&#x27;ll get a lot of blame shifting, a lot of random \"projects\" to improve things and in the end no improvement (or often an even worse experience). In other words you&#x27;ll get corporate politics. Very few executives understand a modern search stack in anything resembling a useful level of detail much less how their teams map to it. reply karaterobot 19 hours agoparentprevI&#x27;ve worked on products with ALL of these exact issues. The answer is usually that the data isn&#x27;t annotated well enough, or that there are not enough developer resources to make special case fixes for things 99% of people don&#x27;t notice or care about (and so are not losing us money, they just annoy us). At the companies I worked for, fixing these problems was not within the remit of the head of product, they were cross-departmental issues and so were in that pernicious category of problems that are highly resistant to change. I assume the reason you think product people are responsible for this is because you aren&#x27;t a product person, but in my experience those are the people who most desperately want this type of shit to be fixed, because it makes them look like clowns. reply whstl 19 hours agorootparentThe reason the responsibility for this is on heads of products is not because they&#x27;re directly responsible for annotating, it&#x27;s because they&#x27;re responsible for knowing what can and will get annotated so the feature they asked for actually works.The work of product person shouldn&#x27;t stop at the idea phase. reply vasco 19 hours agorootparentprevThe responsibility of a product person is that the product works well. This includes working with design and engineering partners as well as any data annotation departments. If the search experience is shit and the head of product doesn&#x27;t think it&#x27;s up to them, they aren&#x27;t being responsible for the product. reply satyrnein 20 hours agoparentprevNot disagreeing, but this example seems like a data&#x2F;tagging problem which is probably some other department. reply pc86 19 hours agorootparentAnd in any sane company, the minute a product person says \"oh this egregious error is a problem for some other department\" they&#x27;d be fired immediately, so it still sounds like a useful exercise.I&#x27;ll be the first admit I don&#x27;t have a ton of respect for the \"product owner\" role which has morphed largely into Tom Smykowski from Office Space[0] but the places I&#x27;ve seen them be actually effective and net positives for an organization are where they have the reach and authority to demand things like this be fixed, regardless of what department is \"in charge.\" IMO a product owner who doesn&#x27;t do this is refusing to do one of the few useful product owner roles.[0] https:&#x2F;&#x2F;www.imdb.com&#x2F;title&#x2F;tt0151804&#x2F;characters&#x2F;nm0726223 reply ellisv 19 hours agorootparentprevIdeally, manufacturers and sellers would provide more, accurate information about products -- but I generally like how Google solved this within Google Maps via crowdsourcing by prompting users to answer questions about places (e.g. does this place have a wheelchair ramp?). I wish there was a better process to collect and store product information so it could be more easily shared. reply vasco 19 hours agorootparentprevWhich would be flagged in the weekly review on the first week? reply prepend 19 hours agorootparentprevI’ve had good results with brute forcing tagging even when it’s not my job.If I use my product and hate it, I make it better. Knowing they some other department sucks for years is no good.I think all this boils down to companies not really giving a flip. reply malfist 19 hours agorootparentprevData tagging makes Walmart throw a huge banner ad for pancakes into your search for lentils? reply marcinzm 19 hours agorootparentThe money from showing a pancake ad make them do that. It&#x27;s not a search problem. They know it&#x27;s not a good experience but they also know it makes them more money than not showing it. The goal is money. Your experience is merely a means to that end.The profit margins on selling products are abysmal for e-commerce. The profit margins on ads are amazing. reply marginalia_nu 19 hours agorootparentprevLentil pancakes are actually a thing to be fair. reply Zetobal 19 hours agorootparentprevI mean if someone tags pancakes as lentils... that said we had great success by using gpt4-v for product tagging tbh. it really is a game changer. reply madeofpalk 19 hours agorootparentprevThe product is bad. Shifting the blame around doesn&#x27;t magically make the product better. reply Gracana 19 hours agoprevThe one that really hurts my feelings is when the search function on a manufacturer’s website doesn’t recognize their own part numbers. Then I get angry and go to google, which gives me unrelated ad results instead. Ah, familiarity. reply kayodelycaon 19 hours agoparentHaving previously worked for a company with this problem, sometimes you need to start pulling other people’s teeth to get them to stop putting data in random fields.We just told our customers to google the part number. I don’t know how these places stay in business. reply pc86 18 hours agorootparentI&#x27;ve never worked at a place where part numbers mattered, but I am very familiar with this mindset of non-technical people that basically boils down to \"just put whatever you want in whatever text field then act befuddled when the system doesn&#x27;t work perfectly.\" reply rascul 19 hours agoprev [–] A little bit related, I wish web sites would stop hijacking the keys for my browser&#x27;s search function. reply andirk 15 hours agoparent [–] Almost every hijacking of keyboard and other default behaviors of our peripherals is lazy and arrogant UI. One exception is auto focusing on a search input element where that view is only used for that search box. Even then it&#x27;s annoying because my browser back&#x2F;forward via keyboard (cmd-left, cmd-right) gets hijacked by the focus. replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The speaker expresses frustration with the overwhelming number of options and lack of organization on various websites.",
      "They question the value of having so many choices and are puzzled by unclear categories and filters.",
      "Specific examples are provided, such as issues with Costco, Lazada, and Nintendo.com."
    ],
    "commentSummary": [
      "Users are expressing frustration with the search functionality on e-commerce websites and platforms, citing issues such as inaccurate results and irrelevant suggestions.",
      "Many companies are seen as prioritizing profit over search quality, leading to a lack of trust from users.",
      "Suggestions for improvement include better tagging and categorization, the use of language model-based learning, and a focus on providing incremental value to users. Privacy concerns and the hijacking of browser functions are also mentioned."
    ],
    "points": 347,
    "commentCount": 187,
    "retryCount": 0,
    "time": 1703596723
  },
  {
    "id": 38777516,
    "title": "Amazon Prime Video to Introduce Ads, Charge Extra for Ad-Free Experience",
    "originLink": "https://www.theverge.com/2023/12/26/24015595/amazon-prime-video-ads-coming-january-29",
    "originBody": "Amazon/ Tech/ Streaming Amazon Prime Video will start showing ads on January 29th Amazon Prime Video will start showing ads on January 29th / Movies and TV shows on Amazon’s streaming service will start getting broken up with ads in January — unless you’re willing to pony up an extra fee each month. By Chris Welch, a reviewer specializing in personal audio and home theater. Since 2011, he has published nearly 6,000 articles, from breaking news and reviews to useful how-tos. Dec 26, 2023, 8:41 PM UTC| Share this story If you buy something from a Verge link, Vox Media may earn a commission. See our ethics statement. Illustration by Alex Castro / The Verge Earlier this year, Amazon announced plans to start incorporating ads into movies and TV shows streamed from its Prime Video service, and now the company has revealed a specific date when you’ll start seeing them: it’s January 29th. “This will allow us to continue investing in compelling content and keep increasing that investment over a long period of time,” the company said in an email to customers about the pending shift to “limited advertisements.” “We aim to have meaningfully fewer ads than linear TV and other streaming TV providers. No action is required from you, and there is no change to the current price of your Prime membership,” the company wrote. Customers have the option of paying an additional $2.99 per month to keep avoiding advertisements. The rest of the email summarizes the many benefits of a Prime subscription — no doubt an attempt to keep customers from cancelling over this decision. Verge readers were none too pleased about the initial news back in September: MosquitoControl This will discourage me from using the service more than encourage me from paying more Amazon Prime currently costs $14.99 each month or $139 annually. (Prime Video can be subscribed to individually for $8.99/month.) The new charge for ad-free streaming would bring Prime to just under $18, and would push standalone Prime Video to just under $12. Related How to cancel your account at Netflix, Amazon Prime, Hulu, and others Amazon also operates Freevee, a free, ad-sponsored streaming service. The company’s email notes that “live event content such as sports, and content offered through Amazon Freevee will continue to include advertising.” The move comes as competing streaming services continue to raise subscription rates across the board and push ads upon customers on their cheapest monthly plans. Disney Plus, Hulu, Max, Netflix, and Paramount Plus all include ads on their most affordable tiers. The monthly cost of Amazon Prime itself isn’t changing, but if you want to preserve the same experience you have today starting on January 29th, you’ll end up paying more. Amazon’s full email follows below. Dear Prime member, We are writing to you today about an upcoming change to your Prime Video experience. Starting January 29, Prime Video movies and TV shows will include limited advertisements. This will allow us to continue investing in compelling content and keep increasing that investment over a long period of time. We aim to have meaningfully fewer ads than linear TV and other streaming TV providers. No action is required from you, and there is no change to the current price of your Prime membership. We will also offer a new ad-free option for an additional $2.99 per month* that you can sign up for here. Prime is a very compelling value. Prime members enjoy a wide range of shopping, savings, and entertainment benefits, including: - More than 300 million items are available with free Prime shipping and tens of millions of the most popular items are available with free Same-Day or One-Day Delivery. - Access to exclusive and broad streaming video content (including Prime Video exclusives like The Lord of the Rings: The Rings of Power, The Boys,Tom Clancy’s Jack Ryan, Citadel, The Wheel of Time, Reacher, and The Summer I Turned Pretty, as well as blockbuster movies such as Air, Creed III, Dungeons & Dragons, Candy Cane Lane with Eddie Murphy, and exclusive live sports including NFL Thursday Night Football). - Access to Prime Video Channels, which provides an unmatched selection of subscription channels like Max, Paramount+ with SHOWTIME, BET+, MGM+, ViX+, Crunchyroll, PBS KIDS, NBA League Pass, MLB.TV, and STARZ—with no extra apps to download, and no cable required. Customers only pay for the ones they want, and can cancel anytime. - The ability to use your Prime shopping benefits—like fast, free delivery, a seamless checkout experience, 24/7 live chat support, and hassle-free returns—on online stores beyond Amazon.com with Buy with Prime. - Exclusive deals and shopping events like Prime Day. - Ad-free listening of 100 million songs and millions of podcast episodes with Amazon Music. - Prescription medications as low as $1 per month and fast, free shipping from Amazon Pharmacy. - Access to unlimited eligible generic prescription medications for only $5 per month (including free shipping) with RxPass from Amazon Pharmacy. - High-quality health care from One Medical for only $9 per month (or $99 annually), with the option to add up to five additional memberships for the family for only $6 per month (or $66 annually) each. - Free two-hour Fresh grocery delivery on orders over $100 (and delivery charges between $6.95 to $9.95 for orders less than $100), and in-store savings on select groceries at Amazon Fresh and Whole Foods Market stores across the U.S. - Unlimited photo storage with Amazon Photos. - Gaming benefits with Prime Gaming. - More than 3,000 books and magazines with Prime Reading. - A free, one-year Grubhub+ membership trial valued at $120 per year, offering unlimited $0 delivery fees on orders over $12. And, you can expect additional features and programs added in the future for our Prime members. As mentioned above, no action is required from you. If you wish to sign up for the ad-free option, you can click here. And, as always, if you have questions about your Prime membership, you can manage your account here. Thank you for being a valued member of Amazon Prime. Sincerely, The Amazon Prime team Most Popular Apple is now banned from selling its latest Apple Watches in the US Amazon Prime Video will start showing ads on January 29th Microsoft Copilot is now available as a ChatGPT-like app on Android Apple appeals US ban on Apple Watch Ultra 2 and Series 9 8 great games for your Switch from 2023 Verge Deals / Sign up for Verge Deals to get deals on products we've tested sent to your inbox daily. Email (required)Sign up By submitting your email, you agree to our Terms and Privacy Notice. This site is protected by reCAPTCHA and the Google Privacy Policy and Terms of Service apply. From our sponsor Advertiser Content From",
    "commentLink": "https://news.ycombinator.com/item?id=38777516",
    "commentBody": "Amazon Prime Video Will Start Showing Ads on January 29Hacker Newspastlogin [dupe] Amazon Prime Video Will Start Showing Ads on January 29 (theverge.com) 328 points by alexzeitler 9 hours ago| hidepastfavorite3 comments hoppyhoppy2 9 hours agoDupe of https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38776591 reply dang 2 hours agoparentComments moved thither. Thanks! reply badrabbit 2 hours agoprev [–] I use a dns adblocker. Both netflix and prime video have prevented me from watching videos because of that (specific error messages about using ad blockers. netflix, mid streaming). I am now enjoying their content and more using alternative means (found a couple of highly underrated recent indie films as a side effect). There has been a lot of innovation on the high seas in recent years for anyone who doesn&#x27;t know. replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Starting from January 29th, Amazon Prime Video will introduce limited advertisements, unless customers opt for an ad-free experience by paying an extra $2.99 per month.",
      "The purpose of this move is to enable Amazon to invest in content and better compete with other streaming services.",
      "While the monthly cost of Amazon Prime will remain the same, the ad-free streaming option will increase the total cost to just under $18, aligning with the approaches of other streaming services like Disney Plus and Netflix, which include ads on their cheaper plans."
    ],
    "commentSummary": [
      "Amazon Prime Video will begin showing ads starting on January 29, causing worries for users who utilize adblockers.",
      "Netflix and Prime Video have previously blocked users from viewing videos when adblocking software is detected.",
      "This move by Amazon may potentially impact the viewing experience for users who prefer an ad-free environment."
    ],
    "points": 328,
    "commentCount": 3,
    "retryCount": 0,
    "time": 1703635919
  },
  {
    "id": 38778390,
    "title": "Apollo 11 Guidance Computer vs. Anker PowerPort Atom PD 2: A Comparison of Processing Power and Features",
    "originLink": "https://forrestheller.com/Apollo-11-Computer-vs-USB-C-chargers.html",
    "originBody": "Apollo 11 Guidance Computer (AGC) vs USB-C Chargers I think it is healthy to compare historical and modern computing. Let's see how the CPUs contained in recent USB-C wall chargers compare to the power of the Apollo 11 Guidance Computer (AGC). The Apollo 11 spacecraft carried 3 humans to the moon and back in 1969. Device Google Pixel 18W Charger Huawei 40W SuperCharge Anker PowerPort Atom PD 2 Apollo 11 Moon Landing Guidance Computer (AGC) Function Charges a phone Charges a phone or maybe a laptop Charges 2 phones or maybe laptopsFly most-of-the-way to moon (CSM) Land on moon (LEM) Take off from moon (LEM) Fly back to Earth (CSM) Microchip(s) Weltrend WT6630P Richtek RT7205 Cypress CYPD4225 Discrete components Clock Speed 10 MHz 22.7 MHz 48 MHz 1.024 MHz RAM 512 bytes \"0.75kB\" 8KB 2048 15-bit words / 4KB if you include the parity bit in each word Program Storage Space 8KB 24KB (Mask ROM + OTP) 128KB Flash 36,864 15-bit words / 72KB if you include the parity bit in each word Instruction Set Intel 8051 (8-bit) Unknown ARM Cortex-M0 32-bit implementing ARMv6-M 16-bit accumulator based Sources ChargerLabs Teardown WT6630P Datasheet ChargerLabs Teardown RT2705 Datasheet ChargerLabs Teardown CYPD4225 Datasheet CPU description Memory Functional overviewInterpretation The most powerful CPU in the table is from the Anker PowerPort Atom PD 2 (CYPD4225). Compared with the Apollo 11 Guidance Computer it runs at ~48 times the clock speed with 1.8x the program space. It's very tempting to claim that the Anker PowerPort Atom PD 2 could fly me to the moon. But we need more analysis. What if the Apollo 11 Guidance Computer contains dedicated hardware which missing in the CYPD4225 and cannot be replicated in software? Let's compare the CYPD4225 to the Apollo 11 Guidance Computer. I'll be using the excellent Virtual AGC documentation as a reference. Speed The Apollo 11 Guidance Computer has a master clock frequency of 1.024MHz. Instructions need at least 12 clock cycles (11.72uS). It operates using one's compliment integer arithmetic. There is no floating point. There is a single accumulator register to perform arithmetic. The Anker PowerPort Atom PD 2 uses a CYPD4225, containing an ARM Cortex-M0 CPU. Luckily for this analysis both the Apollo 11 Guidance Computer and the Cortex-M0 CPUs are very predictable compared to more sophisticated modern CPUs. They have no cache and memory access happens at a constant speed. Instructions execute in-order. They have no hardware floating point. They have no vector/matrix hardware (like AVX, SSE, or NEON). Therefore we can learn a lot by just comparing the cycles of roughly equivalent instructions. Instruction Apollo 11 Guidance Computer Cycles ARM Cortex-M0 Cycles 15-bit Addition and Subtraction 24 (2 * 12) 1 31 Bit Addition and Subtraction 36 (3 * 12) 1 Multiply 36 (3 * 12) 1 - see datasheet Divide 72 (6 * 12) Not Available Branch if zero 24 (2 * 12) 1-4 without CMP, 2-5 with CMP CCS (Count Compare Skip) / CMP 24 (2 * 12) 1 Store accumulator/register to memory (XHC / STR) 24 (2 * 12) 2 Load memory to accumulator/register (XHC / LD) 24 (2 * 12) 2 From the table we see that instructions are usually a proportionate number of low clock cycles. Most of the time each Apollo 11 Guidance Computer instruction takes 12 times more clock cycles than the Cortex-M0. Given that the Cortex-M0 runs at 46.9x the clock speed and that they are otherwise comparable (no cache, no vectors) I claim, for most applications: The Anker PowerPort Atom PD 2 USB-C Wall Charger CPU is 563 times faster than the Apollo 11 Guidance Computer. 563 = (12 Cortex-M0 instructions per 1 Apollo 11 Guidance Computer instruction) * (48 MHz Cortex-M0) / (1.024 MHz Apollo 11 Guidance Computer). After skimming though the Apollo 11 Guidance Computer instruction set, the only important missing Cortex-M0 instruction seems to be division. But each division on the Apollo 11 Guidance Computer takes 72 cycles * (1/1.024MHz) = 70.3 uS. In 70.3uS the CYPD4225 can execute 70.3uS * 48 MHz = 3374 arithmetic instructions. Branches are not too expensive (3 cycles). Hopefully that is enough to implement software division. Memory Program Storage Space This is worth looking at. The Apollo programmers wrote a virtual machine/interpreter for the Guidance Computer because they were struggling with program space. It mattered enough to them to trade speed for increased storage. The Apollo 11 Guidance Computer stores 36,864 15-bit words. The CYPD4225 stores 128KB in flash. It's easy to calculate that the CYPD4225 can store 1.90 times more information than a Apollo 11 Guidance Computer. But does one computer have an advantage in instruction encoding? The Apollo 11 Guidance Computer instructions are 15-bit fixed width instructions. The Cortex-M0 implements the THUMB2 instruction set which has 16-bit instructions and 32-bit instructions. The 16-bit instructions cover most common operations, such as arithmetic, branching, load/store. So it seems most programs can be encoded in mostly 16-bit instructions. How many instructions can a CYPD4225 store? 128KB / 16-bits = 65,536 instructions. The Apollo 11 Guidance computer can store 36,864 instructions. Naively, the Anker PowerPort Atom PD2 can store at most 1.78x more instructions than the Apollo 11 Guidance Computer. Perhaps not all instructions used will be 16-bit. Let's go a bit extreme and say half the instructions will be 32-bit and half the instructions will be 16-bit. Using the following equations where x=16-bit instructions and y=32-bit instructions: 2x + 4y = 128*1024, x = y We get 21,845 32-bit and 16-bit instructions which is 43,690 instructions total -- 1.19x more than the Apollo 11 Guidance Computer. The THUMB2 instruction set used by the Cortex-M0 is designed to be compact. The Cortex-M0 has 12 general purpose registers which can hold arithmetic results compared the single accumulator on the Apollo 11 Guidance Computer. The Apollo 11 Guidance Computer has the advantage that it can do arithmetic and store the result directly to a memory location while the Cortex-M0 requires a separate store instruction. The Cortex-M0 has the advantage that the memory is simpler and requires no bank switching. But the bank switching means that more memory addresses can be encoded directly into arithmetic instructions. This is not straightforward to compare. However, given that the CYPD4225 can hold 1.19x - 1.78x the instructions, I claim that the CYPD4225 should be able to hold an equivalent Apollo 11 Guidance Computer program. Notice that other USB-C chargers have less program capacity than the Apollo 11 Guidance Computer. Going to the moon on 8KB sounds hard. The Apollo 11 Guidance Computer was originally designed with less program storage space but it had to be increased - conditions were not cozy. Therefore, I will not consider other USB-C charger CPUs as candidates for taking me to the moon. Random Access Memory The Apollo 11 Guidance Computer can store 2,048 15-bit words. The CYPD4225 has 8KB of RAM. The Anker PowerPort Atom PD2 has a little over twice the RAM of the Apollo 11 Guidance Computer. One caveat is that the Apollo 11 Guidance Computer mostly computes in terms of 15-bit words. The equivalent Cortex-M0 computations are done in 16-bit words. So in terms of equivalent computational units the CYPD4225 has 4,096 16-bit words vs the Apollo 11 Guidance Computer's 2,048 15-bit words. So exactly twice the words. To The Moon and Back We have our rockets, our modules, some Jonny Kims, and our Anker PowerPort Atom PD2 - are we going to the moon or what? Wikipedia claims the Apollo 11 spacecraft contains 4 computers: 2x Apollo 11 Guidance Computers (examined above). One in the LEM and one in the CSM. 1x Saturn Launch Vehicle Digital Computer (LVDC) 1x Abort Guidance System (AGS) Property Apollo 11 Guidance Computer (AGC) LVDC (Manual) AGS (Design Review) Clock Speed 1.024MHz 2.048MHz 1.024MHz (Search for MHz) RAM 2048 15-bit words 4,096 13-bit words 2,048 18-bit words Program Storage 36,864 15-bit words 32,768 13-bit words 2,048 18-bit words ADD Instruction Duration 23.4uS 82.0uS 10uS (Search for \"Speed -\") MULTIPLY Instruction Duration 35uS 420uS? \"MPH requires five computer cycles\" 70uS Is there a possibility that we can replace multiple computers with a single CYPD4225? I don't think so because the 4 computers were in separate parts of the spacecraft. So we will assume that we need 4 computers to fly an Apollo 11 without significantly altering the design. All the computers on Apollo 11 are less powerful and have less memory than an Anker PowerPort Atom PD 2. I'm going to assume that the LVDC and the AGS do not use any exciting instructions that would be difficult for a Cortex-M0 to perform - besides division. I will hand-wave this away by saying that division probably takes longer than multiplication. And I have listed the multiplication times above. The CYPD4225 can comfortably execute thousands of arithmetic instructions in the time it takes any Apollo 11 computer to execute a single multiply. I claim that we would only need the compute power of 4 Anker PowerPort Atom PD 2 USB-C chargers to get to the moon with the following caveats: The CYPD4225 is definitely not rated for space. I have no idea if it would work in space. I did not examine the peripherals used by the Apollo 11 computers. The CYPD4225 has 30 GPIO signals and talks UART, I2C, and SPI. However, how many peripherals did the Apollo 11 Guidance Computer support? 100? 10? More Googling is needed. And probably the voltage levels from the 1960s are too high to connect to a CYPD4225. Ron Burkey, Owen Smith, and others point out that the LVDC actually contains triply-redundant logic. The logic gives 3 answers and the voting mechanism picks the winner. So it may be fair to claim that you in fact need 3 USB-C chargers to compare against the LVDC. However, I think the redundancy was for reliability and I completely ignoring reliability. And in fact I think any attempt to emulate this voting scheme with 3x microcontrollers with a 4th to tally votes will not make the system any more reliable. But this is proving controversial and it may soon get its own full paragraph summarizing different viewpoints! What does it mean? I was responsible for the overall firmware for Structure Sensor. It used USB charger detection present on most USB chargers at that time - some resistor network between the USB D+ / D- signals. There was no digital communication. The firmware portion was implemented by Evan Fletcher and I think he reported it was easy to do. That was 2012-2013. Now in 2020 USB-C is here. Many USB chargers have a microcontroller with a CPU. Some are less capable than the Apollo 11 Guidance Computer. Some are more capable than the Apollo 11 Guidance Computer. Most of them with at least ~10x faster clock speed. USB-C Power Delivery solves problems and gives us new capabilities. But it is another step toward increasing complexity. It is another firmware and chip to handle during manufacturing. I don't have an alternate proposal for power delivery that: Uses 1 wire in the cable Lets USB-C chargers present arbitrary charge current and voltages Last updated 2020-02-06. Did I miss something obvious? Is the Anker PowerPort Atom PD 2 not going to cut it? forrest@forrestheller.com Thanks my father for looking at this. And thanks to Ron Burkey who looked at a small piece of info I emailed and provided more context. Thanks to Sean Barrett for the correction that Cortex-M0s are pipelined.",
    "commentLink": "https://news.ycombinator.com/item?id=38778390",
    "commentBody": "Apollo 11 vs. USB-C Chargers (2020)Hacker NewspastloginApollo 11 vs. USB-C Chargers (2020) (forrestheller.com) 313 points by raimue 7 hours ago| hidepastfavorite129 comments kens 6 hours ago> Apollo 11 spacecraft contains 4 computersAnalog computers don&#x27;t get the respect they deserve. There&#x27;s one more computer, the FCC. The Flight Control Computer is an analog computer in the Saturn V that controlled the rocket gimbals. It&#x27;s a two-foot cylinder weighing almost 100 pounds. reply kragen 3 hours agoparenti think it&#x27;s (unintentionally) misleading to describe analog &#x27;computers&#x27; as &#x27;computers&#x27;. what distinguishes digital computers from other digital hardware is that they&#x27;re turing-complete (if given access to enough memory), and there isn&#x27;t any similar notion in the analog domainthe only reason they have the same name is that they were both originally built to replace people cranking out calculations on mechanical desk calculators, who were also called &#x27;computers&#x27;the flight control &#x27;computer&#x27; has more in common with an analog synthesizer module than it does with a cray-1, the agc, an arduino, this laptop, or these chargers, which are by comparison almost indistinguishable reply eesmith 1 hour agorootparentWhat do you regard as the first digital, Turing-complete (if given enough memory) computer?ENIAC, for example, was not a stored-program computer. Reprogramming required rewiring the machine.On the other hand, by clever use of arithmetic calculations, https:&#x2F;&#x2F;citeseerx.ist.psu.edu&#x2F;viewdoc&#x2F;summary?doi=10.1.1.37.... says the Z3 could perform as a Universal Computer, even though, quoting its Wikipedia page, \"because it lacked conditional branching, the Z3 only meets this definition by speculatively computing all possible outcomes of a calculation.\"Which makes me think the old punched card mechanical tabulators could also be rigged up as a universal machine, were someone clever enough.\"Surprisingly Turing-Complete\" or \"Accidentally Turing Complete\" is a thing, after all, and https:&#x2F;&#x2F;gwern.net&#x2F;turing-complete includes a bunch of them. reply kragen 1 hour agorootparentlet me preface this with the disclaimer that i am far from an expert on the topicprobably numerous eddies in natural turbulent fluid flows have been digital turing-complete computers, given what we know now about the complexity of turbulence and the potential simplicity of turing-complete behavior. but is there an objective, rather than subjective, way to define this? how complicated are our input-preparation and output-interpretation procedures allowed to be? if there is no limit, then any stone or grain of sand will appear to be turing-completea quibble: the eniac was eventually augmented to support stored-program operation but not, as i understand it, until after the ias machine (the johnniac) was already operationalanother interesting question there is how much human intervention we permit; the ias machine and the eniac were constantly breaking down and requiring repairs, after all, and wouldn&#x27;t have been capable of much computation without constant human attention. suppose we find that there is a particular traditional card game in which players can use arbitrarily large numbers. if the players decide to simulate minsky&#x27;s two-counter machine, surely the players are turing-complete; is the game? are the previous games also turing-complete, the ones where they did not make that decision? does it matter if there happens to be a particular state of the cards which obligates them to simulate a two-counter machine?if instead of attempting to measure the historical internal computational capability of systems that the humans could not perceive at the time, such as thunderstorms and the z3, we use the subjective standard of what people actually programmed to perform universal computation, then the ias machine or one of its contemporaries was the first turing-complete computer (if given enough memory); that&#x27;s when universal computation first made its effects on human society felt reply eesmith 54 minutes agorootparent> is the game?Sure. One of the \"Surprisingly Turing-Complete\" examples is that \"Magic: the Gathering: not just TC, but above arithmetic in the hierarchy \".See https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1904.09828 for the preprint \"Magic: The Gathering is Turing Complete\", https:&#x2F;&#x2F;arstechnica.com&#x2F;science&#x2F;2019&#x2F;06&#x2F;its-possible-to-buil... for an Ars Technica article, and https:&#x2F;&#x2F;hn.algolia.com&#x2F;?q=magic+turing for the many HN submissions on that result. reply thriftwy 2 hours agorootparentprevI wonder why you can&#x27;t make a turing complete analog computer using feedback loops. reply progval 2 hours agorootparentYou can: https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;General_purpose_analog_compute...There is still active research in the area, eg. https:&#x2F;&#x2F;www.lix.polytechnique.fr&#x2F;~bournez&#x2F;i.php?n=Main.Publi... reply kragen 2 hours agorootparenta universal turing machine is a particular machine which can simulate all other turing machines. the gpac, by contrast, is a family of machines: all machines built out of such-and-such a set of partsyou can&#x27;t simulate an 11-integrator general-purpose analog computer or other differential analyzer with a 10-integrator differential analyzer, and you can&#x27;t simulate a differential analyzer with 0.1% error on a (more typical) differential analyzer with 1% error, unless it&#x27;s 100× as large (assuming the error is gaussian)the ongoing research in the area is of course very interesting but a lot of it relies on an abstraction of the actual differential-analyzer problem in which precision is infinite and error is zero reply thriftwy 2 hours agorootparentSure, you cannot easily simulate another analog computer, but this is not the requirement. The requirement is turing completeness, which can be done. reply lisper 1 hour agorootparentIt can? How? reply kragen 1 hour agorootparentas i understand it, with infinite precision; the real numbers within some range, say -15 volts to +15 volts, have a bijection to infinite strings of bits (some infinitesimally small fraction of which are all zeroes after a finite count). with things like the logistic map you can amplify arbitrarily small differences into totally different system trajectories; usually when we plot bifurcation diagrams from the logistic map we do it in discrete time, but that is not necessary if you have enough continuous state variables (three is obviously sufficient but i think you can do it with two)given these hypothetical abilities, you can of course simulate a two-counter machine, but a bigger question is whether you can compute anything a turing machine cannot; after all, in a sense you are doing an infinite amount of computation in every finite interval of time, so maybe you could do things like compute whether a turing machine will halt in finite time. so far the results seem to support the contrary hypothesis, that extending computation into continuous time and continuously variable quantities in this way does not actually grant you any additional computational power!this is all very interesting but obviously not a useful description of analog computation devices that are actually physically realizable by any technology we can now imagine reply thriftwy 1 hour agorootparentprevIt would not be very interesting. You will lose all the interesting properties of analog computer and it would be a poorly performing turing machine. Still it would have those loops and branches necessary, since you can always build digital on top of analog with some additional harness. replyezconnect 2 hours agorootparentprevThey both do the same thing compute an output from given inputs. So they are properly distinguished from each other on how they do the computing. They both deserve the name &#x27;computer&#x27;. reply kragen 2 hours agorootparentonly in the same sense that a machinist&#x27;s micrometer, an optical telescope, an analog television set, an acoustic guitar, a letterpress printing press, a car&#x27;s manual transmission, a fountain pen, a nomogram, and a transistor also &#x27;compute an output from given inputs&#x27;do you want to call them all &#x27;computers&#x27; now? reply adrian_b 2 hours agorootparentThe arithmetic circuits alone, like adders, multipliers etc., regardless if they are mechanical or electronic, analog or digital, should not be called computers.When the arithmetic circuits, i.e. the \"central arithmetical part\", as called by von Neumann, are coupled with a \"central control part\", as called by von Neumann, i.e. with a sequencer that is connected in a feedback loop with the arithmetic part, so that the computation results can modify the sequence of computations, then this device must be named as a \"computer\", regardless whether the computations are done with analog circuits or with digital circuits.What defines a computer (according to the definition already given by von Neumann, which is the right definition in my opinion) is closing the feedback loop between the arithmetic part and the control part, which raises the order of the system in comparison with a simple finite state automaton, not how those parts are implemented.The control part must be discrete, i.e. digital, but the arithmetic part can be completely analog. Closing the feedback loop, i.e. the conditional jumps executed by the control part, can be done with analog comparators that provide the predicates tested by the conditional jumps. The state of an analog arithmetic part uses capacitors, inductors or analog integrators, instead of digital registers.Several decades ago, I had to debug an analog computer during its installation process, before functioning for the first time. That was in a metallurgic plant, and the analog computer provided outputs that controlled the torques of a group of multi-megawatt DC electric motors. The formulae used in the analog computations were very complex, with a large number of adders, multipliers, integrators, square root circuits and so on, which combined inputs from many sensors.That analog computer (made with op amps) performed a sequence of computations much more complex than the algorithms that were executed on an Intel 8080, which controlled various on-off execution elements of the system, like relays and hydraulic valves and the induction motors that powered some pumps.The main reason why such analog computers have become obsolete is the difficulty of ensuring that the accuracy of their computations will not change due to aging and due to temperature variations. Making analog computers that are insensitive to aging and temperature raises their cost much above modern digital microcontrollers. reply kragen 1 hour agorootparentas you are of course aware, analog &#x27;computers&#x27; do not have the &#x27;central control part&#x27; that you are arguing distinguishes &#x27;computers&#x27; from &#x27;arithmetic circuits alone&#x27;; the choice of which calculation to perform is determined by how the computer is built, or how its plugboard is wired. integrators in particular do have state that changes over time, so the output at a given time is not a function of the input at only that time, but of the entire past, and as is well known, such a system can have extremely complex behavior (sometimes called &#x27;chaos&#x27;, though in this context that term is likely to give rise to misunderstanding)you can even include multiplexors in your analog &#x27;computer&#x27;, even with only adders and multipliers and constants; x · (1 + -1 · y) + z · y interpolates between x and z under the control of y, so that its output is conditionally either x or z (or some intermediate state). but once you start including feedback to push y out of that intermediate zone, you&#x27;ve built a flip-flop, and you&#x27;re well on your way to building a digital control unit (one you could probably build more easily out of transistors rather than op-amps). and surely before long you can call it a digital computer, though one that is controlling precision linear analog circuitryit is very commonly the case that analog computation is much, much faster than digital computation; even today, with microprocessors a hundred thousand times faster than an 8080 and fpgas that are faster still, if you&#x27;re doing submillimeter computation you&#x27;re going to have to do your front-end filtering, upconversion or downconversion, and probably even detection in the analog domain reply adrian_b 1 hour agorootparentMost \"analog computers\" have been simple, and even if they usually provided the solution of a system of ordinary differential equations, that does not require a control part, making them no more closer to a complete computer than a music box that performs a fixed sequence.I agree that this kind of \"analog computers\" does not deserve the name of \"computer\", because they are equivalent only with the \"registers + ALU\" (RALU) simple automaton that is a component of a CPU.Nevertheless, there is no reason why a digital control part cannot be coupled with an analog arithmetic part and there have existed such \"analog computers\", even if they have been rarely used, due to high cost and complexity.It is not completely unlikely that such \"analog computers\", consisting of a digital control part and an analog arithmetic part, could be revived with the purpose of implementing low-resolution high-speed machine learning inference.Even now, in circuits like analog-digital converters, there may be analog computing circuits, like switched-capacitor filters, which are reconfigurable by the digital controller of the ADC, based on various criteria, which may depend on the digital output of the converter or on the outputs of some analog comparators (which may detect e.g. the range of the input). reply kens 1 hour agorootparentYou&#x27;re describing a \"hybrid computer\". These were introduced in the late 1950s, combining a digital processor with analog computing units. I don&#x27;t understand why you and kragen want to redefine standard terms; this seems like a pointless linguistic exercise. reply adrian_b 52 minutes agorootparent\"Hybrid computer\" cannot be considered as a standard term, because it has been used ambiguously in the past.Sometimes it has been applied to the kind of computers mentioned by me, with a digital control part and a completely analog arithmetic part.However it has also been frequently used to describe what were hybrid arithmetic parts, e.g. which included both digital registers and digital adders and an analog section, for instance with analog integrators, which was used to implement signal processing filters or solving differential equations.IMO, \"hybrid computer\" is appropriate only in the second sense, for hybrid arithmetic parts.The control part of a CPU can be based only on a finite state automaton, so there is no need for any term to communicate this.On the other hand, the arithmetic part can be digital, analog or hybrid, so it is useful to speak about digital computers, analog computers and hybrid computers, based on that. reply kragen 1 hour agorootparentprevbecause &#x27;computer&#x27; has a meaning now that it didn&#x27;t have 65 years ago, and people are continuously getting confused by thinking that &#x27;analog computers&#x27; are computers, as they understand the term &#x27;computers&#x27;, which they aren&#x27;t; they&#x27;re a different thing that happens to have the same name due to a historical accident of how the advent of the algorithm happenedthis is sort of like how biologists try to convince people to stop calling jellyfish &#x27;jellyfish&#x27; and starfish &#x27;starfish&#x27; because they aren&#x27;t fish. the difference is that it&#x27;s unlikely that someone will get confused about what a jellyfish is because they have so much information about jellyfish alreadymy quest to get people to call cellphones &#x27;hand computers&#x27; is motivated by the same values but is probably much more doomed reply kragen 1 hour agorootparentprevi agree completely; thank you for clarifying despite my perhaps confrontational tonein some sense almost any circuit in which a digital computer controls an analog multiplexer chip or a so-called digital potentiometer could qualify. and cypress&#x27;s psoc line has a bit of analog circuitry that can be thus digitally reconfigured reply justinjlynn 2 hours agorootparentprevWhat&#x27;s wrong with that? They are. We can always make the finer distinction of \"Von Neumann architecture inspired digital electronic computer\" if you wish to exclude the examples you&#x27;ve given. After all, anything which transforms a particular input to a particular output in a consistent fashion could be considered a computer which implements a particular function. I would say - don&#x27;t confuse the word&#x27;s meaning with the object&#x27;s function and simply choose a context in which a word refers to a particular meaning, adapt to others contexts and translate, and simply deal with the fact that there is no firm division between computer and not-computer out in the word somewhere apart from people and their context-rich communications. If the context in which you&#x27;re operating with an interlocutor is clear enough for you to jump to a correction of usage ... simply don&#x27;t; beyond verifying your translation is correct, of course. As you&#x27;re already doing this - likely without realising it - by taking care in doing so consciously you&#x27;re likely to find your communications more efficient, congenial, and illuminating than they otherwise would be. reply shermantanktop 2 hours agorootparentThis is the double-edged sword of deciding to widen (or narrow) the meaning of a term which already has a conventional meaning.By doing so, you get to make a point—perhaps via analogy, perhaps via precision, perhaps via pedantry—which is illuminating for you but now confusing for your reader. And to explain yourself, you must swim upstream and redefine a term while simultaneously making a different point altogether.Much has been written about jargon, but a primary benefit of jargon is the chance to create a domain-specific meaning without the baggage of dictionary-correct associations. It’s also why geeks can be bores at dinner parties. reply derefr 2 hours agorootparentprevWe live in a society (of letters.) Communication is not pairwise in a vacuum; all communication is in context of the cultural zeitgeist in which it occurs, and by intentionally choosing to use a non-zeitgeist-central definition of a term, you are wasting the time of anyone who talks to you.By analogy to HCI: words are affordances. Affordances exist because of familiarity. Don’t make a doorknob that you push on, and expect people not to write in telling you to use a door-bar on that door instead. reply atoav 2 hours agorootparentprevYou are not wrong, yet you are. All of these things are doing computation in a vague, creative sense — sure. But if we call everything that does this or its equivalent a computer we would have to find new words for the thing we mean to be a computer currently.Unilaterally changing language is not forbidden, but if The Culture Wars™ has thought us anything, it is that people are allergic to talking about what they see as mandated changes to their language, even if it is reasonable and you can explain it.Colour me stoked, but you could still just do it unilaterally and wait till somebody notices.However my caveat with viewing everything as computation is that you fall into the same trap as people in the ~1850s did when they wanted to describe everything in the world using complex mechanical devices, because that was the bleeding edge back then. Not everything is an intricate system of pulleys and levers it turned out, even if theoretically you could mimic everything if that system was just complex enough. replynothercastle 5 hours agoparentprevExactly this. A lot of the systems had built in analog computers. It’s a lot cheaper to build then now with electronics but you need more computing power to do things that were previously done mechanically reply hinkley 5 hours agorootparentAnalog computers have to be rebuilt if it turns out the program is wrong though, don’t they? reply coder543 5 hours agorootparentIn the context of this thread, I believe even a digital computer would have to be rebuilt if the program is wrong... :PUnless you typically salvage digital computers from the wreckage of a failed rocket test and stick it in the next prototype. If the FCC is wrong, kaboom. reply Tommstein 4 hours agorootparentPresumably they meant a program being discovered to be wrong before the computer was actually launched. And meant literally building a whole new computer, not just recompiling a program. reply KMag 4 hours agorootparentFor the Apollo Guidance Computer, changing the program meant manually re-weaving wires through or around tiny magnet rings. A good part of the cost of the computer was the time spent painstakingly weaving the wires to store the program. reply neodypsis 3 hours agorootparentThere&#x27;s a very nice video about the assembly lines MIT made just for building the Apollo computer [0].0. https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=ndvmFlg1WmE reply hinkley 4 hours agorootparentprevYeah, though to be fair, some of the programs Apollo ran were on hand woven ROMs, so I may be making too fine a distinction. The program itself was built, not compiled. It if we are comparing with today, it would just be installed, not constructed. reply scaredginger 4 hours agorootparentprevI&#x27;m pretty sure you can perform tests and find defects without actually flying the rocket reply gumby 1 hour agorootparentprevTypically they were reprogrammed by changing the jumpers. The analogous digital change would be replacing the band of cards in a jaquard loom.Much less than “rebuilding”.There have been some hybrids too. reply nothercastle 5 hours agorootparentprevYes though they tend to be mechanically tuned. So like a pneumatic computer or will get tuned to operate into some range of inputs and you probably bench prototype it before you mass produce it reply KRAKRISMOTT 4 hours agoparentprevYou forgot the most important one, the human computers at ground control.Who said women can&#x27;t do math?https:&#x2F;&#x2F;www.smithsonianmag.com&#x2F;science-nature&#x2F;history-human-... reply mulmen 4 hours agorootparent> Who said women can&#x27;t do math?The straw man? reply legostormtroopr 3 hours agorootparentStraw person? reply asylteltine 3 hours agorootparentprev> Who said women can&#x27;t do math?Nobody reply shermantanktop 1 hour agorootparentA quick search will show you many many examples that say otherwise.https:&#x2F;&#x2F;www.dailymail.co.uk&#x2F;news&#x2F;article-524390&#x2F;The-women-ad...Granted, that same search will show you many examples of content accusing unnamed other people of having this attitude.https:&#x2F;&#x2F;www.science.org&#x2F;content&#x2F;article&#x2F;both-genders-think-w...It’s an antiquated notion in my mind, but I don’t think it is a thing of the past. reply interfixus 2 hours agorootparentprev> Who said women can&#x27;t do math?Asimov.https:&#x2F;&#x2F;literature.stackexchange.com&#x2F;questions&#x2F;25852&#x2F;where-d... reply vlovich123 6 hours agoprevIs the weight&#x2F;cost calculus sufficiently improved now that it’s cheaper to shield the processor in it’s entirety rather than trying to rad harden the circuitry itself (much more expensive due to inability to use off the shelf parts & limits the ability to use newer tech)?If I recall correctly this was one of the areas being explored by the mars drone although not sure if Mars surface radiation concerns are different than what you would use in space. reply thebestmoshe 6 hours agoparentIsn’t this basically what SpaceX is doing?> The flight software is written in C&#x2F;C++ and runs in the x86 environment. For each calculation&#x2F;decision, the \"flight string\" compares the results from both cores. If there is a inconsistency, the string is bad and doesn&#x27;t send any commands. If both cores return the same response, the string sends the command to the various microcontrollers on the rocket that control things like the engines and grid fins.https:&#x2F;&#x2F;space.stackexchange.com&#x2F;a&#x2F;9446&#x2F;53026 reply gumby 1 hour agorootparentSeems risky. I remember the automated train control system for the Vienna Hauptbahnhof (main train station) had an x86 and a SPARC, one programmed in a procedural language and one in a production language. The idea was to make it hard to have the same bug in both systems (which could lead to a false positive in the voting mechanism). reply ThePowerOfFuet 38 minutes agorootparentThis is a great technique to avoid common-mode failures. reply jojobas 6 hours agorootparentprevThat sounds way too low. Modern fly-by-wire planes are said to have 12-way voting. reply jcalvinowens 5 hours agorootparent> Modern fly-by-wire planes are said to have 12-way votingDo you have a source for that? Everything I&#x27;ve ever read about Airbus says the various flight control systems are doubly redundant (three units). Twelve sounds like it would be far beyond diminishing returns... reply jojobas 4 hours agorootparentThat was word of mouth. This website says 5 independent computers, of which 2 use different hardware and software so as not to fail in the same fashion.https:&#x2F;&#x2F;www.rightattitudes.com&#x2F;2020&#x2F;04&#x2F;06&#x2F;airbus-flight-cont...I&#x27;d imagine every computer relies on redundant stick&#x2F;pedal encoders, which is how a 12-way notion appeared. reply jcalvinowens 4 hours agorootparentThat blog isn&#x27;t very authoritative, and doesn&#x27;t go into any detail at all.> I&#x27;d imagine every computer relies on redundant stick&#x2F;pedal encoders, which is how a 12-way notion appeared.That&#x27;s disingenuous at best. The lug nuts on my car aren&#x27;t 20x redundant... if you randomly loosen four, catastrophic failure is possible. reply somethingsaid 5 hours agorootparentprevIf you read the link it’s actually two cpu cores on a single cpu die each returning a string. Then 3 of those cpus send the resulting string to the microprocessors which then weigh those together to choose what to do. So it’s 6 times redundant in actuality. reply SV_BubbleTime 5 hours agorootparentThat’s not 6x though.It’s a more solid 3x or 3x+3y, which… if you had a power failure at a chip doesn’t take a 6x to make it 5x. It makes it 4x with the two remaining PHY units because two logical cores went down with one error.The x being physical units, and the y being CPUs in lockstep so that the software is confirmed to not bug out somewhere.It’s 6x for the calculated code portion only, but 3x for CPU and 1-3x for power or solder or circuit board.I know it’s pretty pedantic, but I would call it the lowest form for any quality, which is likely 2-3x. reply dikei 5 hours agorootparentprevIt&#x27;s more complicated than that, in the link, they described it better:>> The microcontrollers, running on PowerPC processors, received three commands from the three flight strings. They act as a judge to choose the correct course of actions. If all three strings are in agreement the microcontroller executes the command, but if 1 of the 3 is bad, it will go with the strings that have previously been correct.This is a variation of Byzantine Tolerant Concensus, with a tie-braker to guarantee progress in case of absent voter. reply mcbutterbunz 5 hours agorootparentI’m curious how often the strings are not in agreement. Is this a very rare occurrence or does it happen often? reply p-e-w 5 hours agorootparentprevI don&#x27;t understand this. If two or more computers fail in the same way simultaneously, isn&#x27;t it much more likely that there is a systemic design problem&#x2F;bug rather than some random error? But if there is a design problem, how does having more systems voting help? reply GuB-42 4 hours agorootparentIt is possible for a random error to affect two computers simultaneously, if they are made from the same assembly line, they may fail in exactly the same way, especially if they share the same wires.That&#x27;s the reason I sometime see that for RAID systems, it is recommended to avoid buying all same disks at the same time, because since they will be used in the same way in the same environment, there is a good chance for them to fail at the same time, defeating the point of a redundant system.Also, to guard against bugs and design problems, critical software is sometimes developed twice or maybe more by separate teams using different methods. So you may have several combinations of software and hardware. You may also have redundant boards in the same box, and also redundant boxes reply adastra22 4 hours agorootparentprevThey are not going to fail the same way simultaneously. This is protecting against cosmic ray induced signal errors within the logic elements, not logic errors due to bad software. reply etrautmann 5 hours agorootparentprevThe multi processor voting approach seeks to solve issues introduced by bit flips caused by radiation, not programming issues. reply gurchik 4 hours agorootparentprevHaving at least 3 computers allows you the option to disable a malfunctioning computer while still giving you redundancy for random bit flips or other environmental issues. reply jojobas 4 hours agorootparentprevWhich is why different sets of computers will run software developed by independent groups on different principles, so that they very unlikely to fail simultaneously. reply kens 5 hours agoparentprevIt&#x27;s always been an option to use shielding rather than rad-hard chips or in combination. RCA&#x27;s SCP-234 aerospace computer weighed 7.9 pounds, plus 3 pounds of lead sheets to protect the RAM and ROM. The Galileo probe used sheets of tungsten to protect the probe relay receiver processor, while the Galileo plasma instrument used tantalum shielding. (I was just doing some research on radiation shielding.) reply adgjlsfhk1 5 hours agoparentprevone thing worth remembering is that a bigger computer runs into a lot more radiation. the cortex m0 is about .03mm^2 vs about .2m^2 for the Apollo guidance computer. as such, the m0 will see about 6 million times less radiation. reply bpye 5 hours agorootparentAren&#x27;t the smaller transistors going to be more susceptible to damage and bit flips though? reply jojobas 6 hours agoparentprevAren&#x27;t high energy space particles a pain in a way that the more shielding you have, the more secondary radiation you generate? reply lazide 5 hours agorootparentIt depends on the type of shielding. For gamma radiation, lead only is a definite problem this way. As is neutron and high speed charged particles&#x2F;cosmic rays.Water less so. reply AnotherGoodName 6 hours agoprevPretty much all USB chips have a fully programmable CPU when you go into the data sheets. It feels silly for simple hid or charging devices but basic microcontrollers are cheap and actually save costs compared to asics. reply hinkley 5 hours agoparentI still want to see postgres or sqlite running straight on a storage controller some day. They probably don’t have enough memory to do it well though. reply lmm 5 hours agorootparentBooting Linux on a hard drive was what, 15 years ago now? reply hinkley 4 hours agorootparentHave you ever tried to google that? reply lmm 4 hours agorootparentYes - up until a few years ago it was easy to find by googling, but now google has degraded to the point where I can&#x27;t manage it. reply winrid 3 hours agorootparentDo you mean this? https:&#x2F;&#x2F;spritesmods.com&#x2F;?art=hddhack&page=1Searched \"run linux on hard drive without cpu or ram\" on Google - third result. replypetermcneeley 5 hours agoparentprevI would also argue that this is another example of software eating the world. The role of the electrical engineer is diminished day by day. reply etrautmann 5 hours agorootparentNah - there are lots of places where you need EEs still. Anything that interfaces with the world. Having programmability does not move most challenges out of the domain of EE. Much of it is less visible than the output of a software role perhaps. reply FredPret 4 hours agorootparentThere will always be problems that can only be solved by an EE, chem eng, mech eng, etc.But the juiciest engineering challenges involve figuring out business logic &#x2F; mission decisions. This is done increasingly in software while the other disciplines increasingly make only the interfaces. reply Almondsetat 2 hours agorootparentprevthe role of the electrical engineer who doesn&#x27;t know a thing about programming is diminished day by day* reply FredPret 4 hours agorootparentprevThe role of the non-software engineer, bit just electrical reply Shawnj2 4 hours agoparentprevWhere I work they were considering using an FPGA over an MCU for a certain task but decided against it because the FPGA couldn’t reach the same low power level as the MCU reply dclowd9901 2 hours agoparentprevYeesh. Beware the random wall wart I guess. reply masklinn 2 hours agorootparentWall warts are not even the biggest worry: https:&#x2F;&#x2F;shop.hak5.org&#x2F;products&#x2F;omg-cable reply jcalvinowens 5 hours agoprev>> others point out that the LVDC actually contains triply-redundant logic. The logic gives 3 answers and the voting mechanism picks the winner.This is a very minor point... but three of something isn&#x27;t triple redundancy: it&#x27;s double redundancy. Two is single redundancy, one is no redundancy.Unless the voting mechanism can somehow produce a correct answer from differing answers from all three implementations of the logic, I don&#x27;t understand how it could be considered triply redundant. Is the voting mechanism itself functionally a fourth implementation? reply kens 4 hours agoparentThe official name for the LVDC&#x27;s logic is triple modular redundant (TMR). The voting mechanism simply picks the majority, so it can tolerate one failure. The LVDC is a serial computer, which makes voting simpler to implement, since you&#x27;re only dealing with one bit at a time. reply somat 2 hours agoparentprevI find it fascinating the two different schools of thought exposed in the LVDC and the AGC.The LVDC was a highly redundant can not fail design. the AGC had no redundancy and was designed to recover quickly if failure occurred. reply tavavex 3 hours agoprevI&#x27;m curious - are there any ways of finding out the precise hardware that&#x27;s used in these small-scale devices that are generally not considered to be computers (like smartphone chargers) without actually having to take them apart? Are there special datasheets, or perhaps some documents for government certification, or anything like it? I&#x27;ve always been fascinated with the barebones, low-spec hardware that runs mundane electronic things, so I want to know where the author got all that information from. reply ssgodderidge 5 hours agoprev> The Anker PowerPort Atom PD 2 USB-C Wall Charger CPU is 563 times faster than the Apollo 11 Guidance ComputerWild to think the thing that charges my devices could be programmed to put a human on the moon reply fuzzfactor 51 minutes agoparentThe proven way to fly people to the moon and back using such low-powered computers was to have a supporting cast of thousands who were naturally well qualified using their personal slide rules to smoothly accomplish things that many of today&#x27;s engineers would stumble over using their personal computers.Plenty of engineers on the ground had no computers, and the privileged ones who did had mainframes, not personal at all.A computer was too valuable to be employed doing anything that didn&#x27;t absolutely need a computer, most useful for precision or speed of calculation.But look what happens when you give something like a mainframe to somebody who is naturally good at aerospace when using a slide rule to begin with. reply oldgradstudent 5 hours agoparentprev> Wild to think the thing that charges my devices could be programmed to put a human on the moonWith a large enough lithium battery, a charger can easily take you part of the way there. reply daxfohl 4 hours agoprevSo in 50 years the equivalent of a gpt4 training cluster from today&#x27;s datacenters will fit in a cheap cable, and it will run over 100 times faster than a full cluster today. reply FredPret 4 hours agoparentComputronium reply ashvardanian 5 hours agoprevRemarkable comparison! I&#x27;m surprised it had only one parity bit per 15-bit word. Even on Earth today we keep two parity bits per 8-bit word in most of our servers.> IBM estimated in 1996 that one error per month per 256 MiB of RAM was expected for a desktop computer.https:&#x2F;&#x2F;web.archive.org&#x2F;web&#x2F;20111202020146&#x2F;https:&#x2F;&#x2F;www.newsc... reply dang 4 hours agoprevDiscussed at the time (of the article):Apollo 11 Guidance Computer vs. USB-C Chargers - https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=22254719 - Feb 2020 (205 comments) reply somat 2 hours agoprevThe ariicle was a lot of fun however I felt it missed a important aspect about the respective computers. IO channels. I don&#x27;t know about the USB charge controllers. But the AGC as a flight computer had a bunch of inputs and outputs. Does a Richtek RT7205 have enough IO? reply jiggawatts 2 hours agoparentThe most powerful chip in the list (Cypress CYPD4126) has 30x general purpose I&#x2F;O pins.[1]AFAIK, this is typical of USB controller chips, which generally have about 20-30 I&#x2F;O pins, but I’m sure there are outliers.The AGC seems to have four 16-bit input registers, and five 16-bit output registers[2], for a total of 144 I&#x2F;O pins total.[1] https:&#x2F;&#x2F;ta.infinity-component.com&#x2F;datasheet&#x2F;9c-CYPD4126-40LQ...[2] https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Apollo_Guidance_Computer#Other... reply kristopolous 6 hours agoprevWhen we go back to the moon, I wouldn&#x27;t be surprised if Zilog-Z80s were a major part of the hardware. Well known, well understood, predictable hardware goes a long way. There&#x27;s a bunch of other considerations in outer space and z80s have proven robust and reliable there. Also I&#x27;d expect a bunch of Kermit and Xmodem to be used as well. reply GlenTheMachine 6 hours agoparentThey won’t be. We will use RAD750’s, the flight qualified variant of the PowerPC architecture. That’s the standard high end flight processor.https:&#x2F;&#x2F;www.petervis.com&#x2F;Vintage%20Chips&#x2F;PowerPC%20750&#x2F;RAD75...The next generation (at least according to NASA) will be RISC-V variants:https:&#x2F;&#x2F;www.zdnet.com&#x2F;article&#x2F;nasa-has-chosen-these-cpus-to-... reply johnwalkr 5 hours agorootparentI wouldn&#x27;t call it the standard, it&#x27;s just used in designs with legacy to avoid the huge cost of re-qualification of hardware and software. It&#x27;s infeasible a lot of times due to cost and power consumption. I work in the private sector in space (lunar exploration actually) and everyone is qualifying normal&#x2F;automotive grade stuff, or using space-grade microcontrollers for in-house designs, with everything from 8-32bit, [1] and ready-made cpu boards[2] for more complex use cases. I&#x27;m sharing just 2 examples but there are hundreds, with variations on redundancy implemented in all kinds of ways too, such as in software, on multiple cores, on multiple chips, or on multiple soft-cpu cores on a single or multiple FPGAs.[1] Example: https:&#x2F;&#x2F;www.militaryaerospace.com&#x2F;computers&#x2F;article&#x2F;16726923...[2] Example: https:&#x2F;&#x2F;xiphos.com&#x2F;product-details&#x2F;q8 reply kristopolous 6 hours agorootparentprevThe 750 is still based on a 27 year old chip and runs at half its clockspeed. The point was that spaceflight is relatively computationally modest. reply dogma1138 6 hours agoparentprevI doubt so and if they will they’ll be abstracted to hell behind modern commodity hardware, Apollo had no bias when it comes to HDI&#x2F;MMIs so astronauts could be trained on the computer interface that was possible at the time.The reason why the controls of Dragon and Orion look the way they do is that they are no far off from modern digital cockpits of jets like the F-22 and F-35 and everyone is used to graphical interfaces and touch controls.Having non intuitive interfaces that go against the bias astronauts and later on civilian contractors already have by using such interfaces over the past 2 decades will be detrimental to overall mission success.The other reason for why they’ll opt to use commodity hardware is that if we are going back to space for real now you need to be able to build and deploy systems at an ever increasing pace.We have enough powerful human safety rated hardware from aerospace and automotive there is no need to dig up relics.And lastly you’ll be hard pressed to find people who still know how to work with such legacy hardware at scale and unless we will drastically change the curriculum of computer science degrees around the US and the world that list would only get smaller each year. We’re far more likely to see ARM and RISC-V in space than z80’s. reply johnwalkr 6 hours agoparentprevI work in this space and z80, kermit and xmodem are not part of the solution. Just because this stuff is simple to the user doesn&#x27;t mean it&#x27;s the best suited, and there&#x27;s a whole industry working on this since the Z80 days. You can buy space-qualified microcontroller boards&#x2F;components with anything from a simple 8 bit microcontroller to a 64-bit, multicore, 1Ghz+ ARM cpu depending on the use case. I&#x27;m sure Z80 has been used in space, but in my time in the industry I&#x27;ve never heard of it.Kermit and xmodem probably aren&#x27;t what you want to use, they are actually a higher level than what is normally used and would require a big overhead, if they even worked at all with latencies that can reach 5-10s. Search for the keyword \"CCSDS\" to get hints about data protocols used in space. reply kristopolous 6 hours agorootparentI worked in it 20 years ago building diagnostic and networking tools ... arm was certainly around but there was also what I talked about. Things probably changed since then.Here&#x27;s kermit in space ... coincidentally in a 20 year old article. Software I wrote supported diagnosing kermit errors.https:&#x2F;&#x2F;www.spacedaily.com&#x2F;news&#x2F;iss-03zq.htmlI guess now I&#x27;m old. reply johnwalkr 5 hours agorootparentThanks for the reference! Kermit could be used locally on ISS or in a lunar mission now that I think about it, but so is&#x2F;could SSH, web browsers or any modern technology. But most space exploration is robotic and depends on communication to ground stations on Earth, and that is fairly standardized. Perhaps kermit will be used on the lunar surface, and that will be a simplification compared to a web browser interface. But for communication to&#x2F;from Earth and Moon, there are standards in place and it would be a complication, not simplification to add such a protocol. reply kristopolous 5 hours agorootparentoh who knows ... I stopped working on that stuff in I think 2006. The big push then was over to CAN and something called AFDX which worked over ethernet. I was dealing with octal and bcd daily, mid 2000s.I have no idea if people still use ARINC-429 or IRIG-B. Embedded RTOS was all proprietary back then for instance, like with VXWORKS. I&#x27;m sure it&#x27;s not any more. I hated vxworks. reply lambda 3 hours agorootparentYeah, I&#x27;m working on a fly by wire eVTOL project, we are using the CAN bus as our primary bus, but there are a number of off the shelf components like ADAHRS that we use that talk ARINC-429 so our FCCs will have a number of ARINC-429 interfaces.But at least for the components we&#x27;re developing, we have basically standardized on ARM, the TMS570 specifically since it offers a number of features for safety critical systems, and simplifies our tooling and safety analysis to use the same processor everywhere.Z80 is pretty retro, and while I&#x27;m sure there may be some vendors who still use it, it&#x27;s got to be getting pretty rare for new designs, between all the PowerPC, Arm, and now RISC-V processors available that allow you to use modern toolchains and so on, I&#x27;d be surprised if many people were doing new designs with the Z80 replyatleta 6 hours agoparentprevIt seems it&#x27;s going to be a new, RISC-V based chip:[1] https:&#x2F;&#x2F;www.zdnet.com&#x2F;article&#x2F;nasa-has-chosen-these-cpus-to-... [2] https:&#x2F;&#x2F;www.nasa.gov&#x2F;news-release&#x2F;nasa-awards-next-generatio... reply monocasa 6 hours agoparentprevI&#x27;m not sure if there&#x27;s a RAD hard z80 variant.They&#x27;ve got their own chips and protocols going back just as far, like https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;MIL-STD-1553 reply kristopolous 6 hours agorootparentThe space shuttle used both z80 and 8086 until it ended in 2011. The international space station runs on among other chips, 80386SX-20s. IBM&#x2F;BAE also has a few RADs based on POWER chips. reply monocasa 5 hours agorootparentDo you have a citation for that?The Space Shuttle Avionics System top level documentation specifically calls out having \"no Z80&#x27;s, 8086s, 68000&#x27;s, etc.\"https:&#x2F;&#x2F;ntrs.nasa.gov&#x2F;api&#x2F;citations&#x2F;19900015844&#x2F;downloads&#x2F;19... reply kristopolous 5 hours agorootparentIntel claims they did. https:&#x2F;&#x2F;twitter.com&#x2F;intel&#x2F;status&#x2F;497927245672218624?lang=en although what&#x27;s that word \"some\" doing in there...And also, sigh, to demonstrate once again that when I worked in space it was 20 years ago, https:&#x2F;&#x2F;www.nytimes.com&#x2F;2002&#x2F;05&#x2F;12&#x2F;us&#x2F;for-parts-nasa-boldly-... (https:&#x2F;&#x2F;web.archive.org&#x2F;web&#x2F;20230607141742&#x2F;https:&#x2F;&#x2F;www.nytim...)Knowing how 8086 timing and interrupts worked was still important for what I was doing in the early 2000s. I don&#x27;t pretend to remember any of it these days. replycontinuational 2 hours agoprevSeems like with cables this powerful, it might make sense for some devices to simply run their logic on the cable CPU, instead of coming with their own. reply codezero 5 hours agoprevWeird question maybe, but does anyone keep track of quantitative or qualitative data that measures the discrepancy between consumer (commercial) and government computer technology?TBH, it&#x27;s kind of amazing that a custom computer from 50 years ago has the specs of a common IC&#x2F;SoC today, but those specs scale with time. reply ajsnigrutin 5 hours agoparentThere is no difference anymore, the only difference is the scale.Back then, consumers got nothing, governments got largge computers (room sized+), then consumers got microcomputers (desktop sized), governments got larger mainframes, consumers got PCs, government got big-box supercomputers,...And now? Consumers get x86_64 servers, governments get x86_64 servers, and the only difference is how much money you have, how many servers can you buy and how much space, energy and cooling you need to run them. well, \"normal users\" get laptops and smartphones, but geek-consumers buy servers... and yeah, I know arm is an alternative. reply creer 2 hours agorootparentLet&#x27;s not go too far either. Money and determination still buys results. A disposable government system might be stuffed with large FPGAs, ASICs and other exotics. Which would rarely be found in any consumer system - certainly not in quantity. A government system might pour a lot of money in the design of these and the cost of each unit. So, perhaps not much difference for each standard CPU and computer node but still as much difference as ever in the rest? reply tavavex 3 hours agorootparentprevI&#x27;d argue that the difference is the price. There is still quite a bit of a difference between average consumer and business hardware, but compute power is cheap enough that the average person can afford what was previously only reserved for large companies. The average \"consumer computer\" nowadays is an ARM smartphone, and while server equipment is purchasable, you can&#x27;t exactly hit up your local electronics store to buy a server rack or a server CPU. You can still get those things quite easily, but I wouldn&#x27;t say their main goal is being sold to individuals. reply codezero 3 hours agorootparentprevI was asking about anyone tracking the disparity between nation-state computing power and commercially available computing power. This seems like something that&#x27;s uncontroversial. reply creer 2 hours agorootparent\"Nation state\" doesn&#x27;t mean \"country\". It certainly doesn&#x27;t mean \"rich country\". reply c0pium 5 hours agoparentprevWhy would you expect there to be one? It’s all the same stuff and has been for decades. reply codezero 3 hours agorootparentI expect nation state actors to have immensely more access to computing power than the commercial sector, is that controversial? reply ghaff 3 hours agorootparentBecause the big ones spend more money. I expect Google etc. has access to more computing power than most nation states do. reply orliesaurus 6 hours agoprev54 years ago - wow - was the Apollo 11 Moon Landing Guidance Computer (AGC) chip the best tech had to offer back then? reply GlenTheMachine 6 hours agoparentYes, given the size, power, and reliability constraints. There were, of course, far more powerful computers around… but not ones you could fit in a spacecraft the size of a Volkswagen Beetle.The Apollo program consumed something like half of the United States’ entire IC fabrication capacity for a few years.https:&#x2F;&#x2F;www.bbc.com&#x2F;future&#x2F;article&#x2F;20230516-apollo-how-moon-... reply db48x 6 hours agorootparentThe AGC was 2 ft³. I believe the volume was written into the contract for development of the computer, and was simply a verbal guess by the owner of the company during negotiations. On the other hand, they had been designing control systems for aircraft and missiles for over a decade at that point so it was not an entirely uninformed guess.The amazing thing is that they did manage to make it fit into 2 ft³, even though the integrated circuits it used had not yet been invented when the contract was written. reply dgacmu 6 hours agoparentprevYes when accounting for size. If you wanted something that was the size of a refrigerator, you could buy a data general Nova in 1969: https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Data_General_Nova8KB of RAM! But hundreds of pounds vs 70lb for the AGC with fairly comparable capability (richer instructions&#x2F;registers, lower initial clock rate).The AGC was quite impressive in terms of perf&#x2F;weight reply kens 5 hours agoparentprevThe Apollo Guidance Computer was the best technology when it was designed, but it was pretty much obsolete by the time of the Moon landing in 1969. Even by 1967, IBM&#x27;s 4 Pi aerospace computer was roughly twice as fast and half the size, using TTL integrated circuits rather than the AGC&#x27;s RTL NOR gates. reply Klaster_1 2 hours agoprevCan you run Doom on a USB-C charger? Did anyone manage to? reply yjftsjthsd-h 2 hours agoparentI feel like I&#x2F;O would be the real pain point there. I suppose if you throw out performance you could forward X&#x2F;VNC&#x2F;whatever over serial (possibly with TCP&#x2F;IP in the middle; SLIP is ugly but so flexible), but that&#x27;s unlikely to be playable. reply jackhack 5 hours agoprevit&#x27;s a fun article, but I would liked to have seen at least a brief mention of power consumption comparison among the four designs. reply ReptileMan 52 minutes agoprevThe great thing about the AI age is that we are once again performance constrained so people start to rediscover the lost art of actually optimizing a program or runtime (the last such age were the waning days of ps2. Those guys made GoW 2 run on 32 megs of ram ... respect) reply m463 6 hours agoprevIt is amazing they were able to miniaturize a computer to fix into a spaceship.Previously calculators were a room full of people, all of which required food, shelter, clothing and ... oxygen. reply Tommstein 4 hours agoprevToo bad the link to Jonny Kim&#x27;s biography is broken (one that works: https:&#x2F;&#x2F;www.nasa.gov&#x2F;people&#x2F;jonny-kim&#x2F;). He has to be one of the most impressive humans who has ever lived. Amongst other things, a decorated Navy SEAL, Harvard medical doctor, and astronaut. Sounds like a kid slapping together the ultimate G.I. Joe. reply SV_BubbleTime 4 hours agoprev>But it is another step toward increasing complexity.I wish more people understood this, and could better see the coming crisis. reply nolroz 6 hours agoprev [–] Hi Forrest! replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The Anker PowerPort Atom PD 2 charger has a significantly faster CPU compared to the Apollo 11 Guidance Computer, estimated to be 563 times faster.",
      "The Anker charger also offers more program storage space than the Apollo 11 computer.",
      "However, it is unclear if the dedicated hardware capabilities of the Apollo 11 computer can be replicated in software, requiring further analysis."
    ],
    "commentSummary": [
      "The discussion explores the comparison between analog and digital computers and their respective advantages and limitations.",
      "It delves into the concept of Turing completeness and the significance of hybrid computers.",
      "The article touches on ongoing research in analog computation, the importance of context and communication in defining terms, and the concept of redundancy in computer systems. Additionally, it mentions the use of different computer chips in space exploration and advancements in computing technology such as the Apollo Guidance Computer and USB-C chargers."
    ],
    "points": 313,
    "commentCount": 129,
    "retryCount": 0,
    "time": 1703643699
  },
  {
    "id": 38777647,
    "title": "Optimizing Learning for Software Developers: Research-backed Strategies and Practical Implications",
    "originLink": "https://cacm.acm.org/magazines/2024/1/278891-10-things-software-developers-should-learn-about-learning/fulltext",
    "originBody": "ACM Sign In Go ACM.org Join ACM About Communications ACM Resources Alerts & Feeds Communications of the ACM Home Current Issue Current Issue: January 2024 10 Things Software Developers Should Learn about Learning On Specifying for Trustworthiness Data Bias Management VIEW TABLE OF CONTENTS News Latest News News Archive Blogs About the Blogs BLOG@CACM Blogroll Blogs Archive Opinion Articles Interviews Opinion Archive Research Latest Research Research Archive Practice Latest Practice Practice Archive Careers Search for Jobs Post a Resume Post A Job Advertise with Us Contact Us Archive The magazine archive includes every article published in Communications of the ACM for over the past 50 years. January 2024 (Vol. 67, No. 1) December 2023 (Vol. 66, No. 12) November 2023 (Vol. 66, No. 11) VIEW MORE ISSUES Videos Home/Magazine Archive/January 2024 (Vol. 67, No. 1)/10 Things Software Developers Should Learn about Learning/Full Text Research 10 Things Software Developers Should Learn about Learning By Neil C. C. Brown, Felienne F. J. Hermans, Lauren E. Margulieux Communications of the ACM, January 2024, Vol. 67 No. 1, Pages 78-87 10.1145/3584859 Comments VIEW AS: Print Mobile App ACM Digital Library Full Text (PDF) In the Digital Edition SHARE: Credit: Lisa Sheehan Learning is necessary for software developers. Change is perpetual: New technologies are frequently invented, and old technologies are repeatedly updated. Thus, developers do not learn to program just once—over the course of their careers they will learn many new programming languages and frameworks. Back to Top Key Insights Just because we learn does not mean we understand how we learn. One survey in the U.S. found that the majority of beliefs about memory were contrary to those of scientific consensus: People do not intuitively understand how memory and learning work.37 As an example, consider learning styles. Advocates of learning styles claim that effective instruction matches learners' preferred styles—visual learners look, auditory learners listen, and kinesthetic learners do. A 2020 review found that 89% of people believe that learners' preferred styles should dictate instruction, though researchers have known for several decades that this is inaccurate.28 While learners have preferred styles, effective instruction matches the content, not learning styles. A science class should use graphs to present data rather than verbal descriptions, regardless of visual or auditory learning styles, just like cooking classes should use hands-on practices rather than reading, whether learners prefer a kinesthetic style or not. Decades of research into cognitive psychology, education, and programming education provide strong insights into how we learn. The next 10 sections of this article provide research-backed findings about learning that apply to software developers and discuss their practical implications. This information can help with learning for yourself, teaching junior staff, and recruiting staff. Back to Top 1. Human Memory Is Not Made of Bits Human memory is central to learning. As Kirschner and Hendrick put it, \"Learning means that there has been a change made in one's long-term memory.\"20 Software developers are familiar with the incredible power of computer memory, where we can store a series of bits and later retrieve that exact series of bits. While human memory is similar, it is neither as precise nor as reliable. Due to the biological complexity of human memory, reliability is a complicated matter. With computer memory, we use two fundamental operations: read and write. Reading computer memory does not modify it, and it does not matter how much time passes between writes and reads. Human long-term memory is not as sterile. Human memory seems to have a \"read-and-update\" operation, wherein fetching a memory can both strengthen and modify it—a process known as reconsolidation. This modification is more likely on recently formed memories. Because of this potential for modification, a fact does not exist in a binary state of either definitively known or unknown; it can exist in intermediate states. We can forget things we previously knew, and knowledge can be unreliable, especially when recently learned. Another curious feature of human memory is \"spreading activation.\"1 Our memories are stored in interconnected neural pathways. When we try to remember something, we activate a pathway of neurons to access the targeted information. However, activation is not contained within one pathway. Some of the activation energy spreads to other connected pathways, like heat radiating from a hot water pipe. This spreading activation leaves related pathways primed for activation for hours.1 Spreading activation has a negative implication for memory1 and a positive implication for problem-solving.32 Spreading activation means that related, but imprecise, information can become conflated with the target information, meaning our recall of information can be unreliable. However, spreading activation is also associated with insight-based problem solving, or \"aha moments.\" Because pathways stay primed for hours, sometimes stepping away from a problem to work on a different one with its own spreading activation causes two unrelated areas to connect in the middle. When two previously unrelated areas connect, creative and unique solutions to problems can arise. This is why walks, showers, or otherwise spending time away from a problem can help you get unstuck in problem solving. In summary, human memory does not work by simply storing and retrieving from a specific location like computer memory. Human memory is more fragile and more unreliable, but it can also offer great benefits in problem solving and deep understanding by connecting knowledge together. We will elaborate further on this in later sections, especially on retrieving items from memory and strengthening memories. Back to Top 2. Human Memory Is Composed of One Limited and One Unlimited System Human memory comprises two main components that are relevant to learning: long-term memory and working memory. Long-term memory is where information is permanently stored and is functionally limitless;1 in that sense, it functions somewhat like a computer's disk storage. Working memory, however, is used to consciously reason about information to solve problems;2 it functions like a CPU's registers, storing a limited amount of information in real time to allow access and manipulation. Working memory is limited, and its capacity is roughly fixed at birth.2 While higher working-memory capacity is related to higher general intelligence, working-memory capacity is not the be-all and end-all for performance.22 Higher capacity enables faster learning, but our unlimited long-term memory removes limitations on how much we could ultimately learn in total.1 Expert programmers may have low or high working memory capacity but it is the contents of their long-term memory that make them experts. As people learn more about a topic, they relate information together into chunks.a Chunking allows the multiple pieces of information to act as one piece of information in working memory. For example, when learning an email address, a familiar domain, such as gmail.com, is treated as one piece of information instead of a random string of characters, like xvjki.wmt. Thus, the more information that is chunked, the larger working memory is functionally.38 Using our computer analogy, our working memory/CPU registers may only let us store five pointers to chunks in long-term memory/disk, but there is no limit on the size of the chunks, so the optimal strategy is to increase the size of the chunks by practicing using information and solving problems. When learning new tools or skills, it is important to understand the cognitive load, or amount of working memory capacity, demanded by the task. Cognitive load has two parts: intrinsic load and extraneous load. Intrinsic load is how many pieces of information or chunks are inherently necessary to achieve the task; it cannot be changed except by changing the task. In contrast, extraneous cognitive load is unnecessary information that, nevertheless, is part of performing the task. Presentation format is an example of how extraneous cognitive load can vary. If you are implementing a database schema, it is easier to use a diagram with tables and attributes than a plain English description—the latter has higher extraneous load because you must mentally transform the description into a schema, whereas the diagram can be mapped directly (see Figure 1). Extraneous load is generally higher for beginners because they cannot distinguish between intrinsic and extraneous information easily. Figure 1. Two ways of presenting the same database schema description with differing extraneous cognitive load. When faced with a task that seems beyond a person's abilities, it is important to recognize that this can be changed by reorganizing the task. Decomposing the problem into smaller pieces that can be processed and chunked will ultimately allow the person to solve complex problems. This principle should be applied to your own practice when facing problems at the edge of or beyond your current skills, but it is especially relevant when working with junior developers and recruits. Back to Top 3. Experts Recognize, Beginners Reason One key difference between beginners and experts is that experts have seen it all before. Research into chess experts has shown that their primary advantage is their ability to remember and recognize the state of the board. This allows them to decide how to respond more quickly and with less effort.15 Kahneman19,b describes cognition as being split into \"system 1\" and \"system 2\" (thus proving that not only developers struggle with naming things). System 1 is fast and driven by recognition, relying upon pattern recognition in long-term memory, while system 2 is slower and focused on reasoning, requiring more processing in working memory. This is part of a general idea known as dual-process theories.34 Expert developers can reason at a higher level by having memorized common patterns in program code, which frees up their cognition. Expert developers can reason at a higher level by having memorized (usually implicitly, from experience) common patterns in program code, which frees up their cognition.4 One such instance of this is \"design patterns\" in programming, similar to previously discussed chunks. An expert may immediately recognize that a particular piece of code is carrying out a sorting algorithm, while a beginner might read line by line to try to understand the workings of the code without recognizing the bigger picture. A corollary to this is that beginners can become experts by reading and understanding a lot of code. Experts build up a mental library of patterns that let them read and write code more easily in the future. Seeing purely imperative C code may only partially apply to functional Haskell code, so seeing a variety of programming paradigms will help further. Overall, this pattern matching is the reason that reading and working with more code, and more types of code, will increase proficiency at programming. Back to Top 4. Understanding a Concept Goes from Abstract to Concrete and Back Research shows that experts deal with concepts in different ways than beginners. Experts use generic and abstract terms that look for underlying concepts and do not focus on details, whereas beginners focus on surface details and have trouble connecting these details to the bigger picture. These differences affect how experts reason but also how they learn. For example, when explaining a variadic function in Python to someone new to the concept, experts might say that it is a function that can take a varying number of arguments. A beginner may focus on details such as the exact syntax for declaring and calling the function and may think that passing one argument is a special case. An expert may more easily understand or predict the details while having the concept explained to them. When you are learning a new concept, you will benefit from both forms of explanation: abstract features and concrete details with examples. More specifically, you will benefit from following the semantic wave, a concept defined by Australian scientist Karl Maton,25 as illustrated by Figure 2. Figure 2. The semantic wave for variadic functions. Following the semantic wave, you continuously switch between the abstract definition and several diverse examples of the concept. The more diverse the examples are, the better. Even wrong examples are beneficial when compared to correct examples to understand why they are wrong,23 such as seeing a mutable variable labeled as non-constant when trying to learn what a constant is. This process is called unpacking. With these diverse examples, you can then (re)visit the abstract definition and construct a deeper understanding of the concept. Deeper understanding stems from recognizing how multiple details from the examples connect to the one abstract concept in the definition, a process called repacking. Problem-solving is (incorrectly) conceived as a generic skill. However, this is not how problem-solving works in the brain. Programming frequently involves learning about abstract concepts. Faced with an abstract concept to learn, such as functions, people often reach for concrete instantiations of the concept to examine, for example, the abs function that returns the absolute value of a number.17 One challenge is that as concepts get more abstract (from values to variables/objects to functions/classes to higher-order functions/metaclasses and eventually category theory), the distance to a concrete example increases. The saving grace is that as we learn abstract concepts, they become more concrete to us. Initially, a function is an abstract concept, but after much practice, a function becomes a concrete item (or chunk) to us and we can learn the next level of abstraction. Back to Top 5. Spacing and Repetition Matter How often have you heard that you should not cram for an exam? Unless, of course, you want to forget everything by the next day. This advice is based on one of the most predictable and persistent effects in cognitive psychology: the spacing effect.10 According to the spacing effect, humans learn problem-solving concepts best by spacing out their practice across multiple sessions, multiple days, and ideally, multiple weeks. The reason spacing works is due to the relationship between long-term and working memory previously described in this article. When learners practice solving problems, they practice two skills. First, matching the information in the problem to a concept that can solve it (such as a filtering loop), and second, applying the concept to solve the problem (such as writing the loop). The first skill requires activating the correct neural pathway to the concept in long-term memory.5 If learners repeatedly solve the same kind of problem, such as for-each loop problems, then that pathway to long-term memory stays active, and they miss practicing the first skill. A common result of un-spaced practice is that people can solve problems, but only when they are told which concept to use.5 While interleaving different types of problems, such as loop and conditional problems, can help, pathways take time to return to baseline, making spacing necessary to get the most out of practice time.10 In addition, the brain needs rest to consolidate the new information that has been processed so that it can be applied to new problems. Going against this time-tested principle, intensive coding boot-camps require learners to cram their problem-solving practice into un-spaced sessions. While this is not ideal, researchers of the spacing effect have known from the beginning that most learners still prefer to cram their practice into as little time as possible.10 For people whose only viable option to learn programming is intensive bootcamps, we can apply the spacing research to maximize their outcomes. To structure a day of learning, learners should limit learning bouts to 90 minutes or less.21 The neuro-chemical balance in the brain makes concentration difficult after this point.21 After each learning bout, take at least 20 minutes to rest.21 Really rest by going for a walk or sitting quietly—without working on other tasks, idly browsing the Internet, or chatting with others. Rest speeds up the consolidation process, which also happens during sleep. Within a learning bout, there are a couple of strategies to maximize efficiency. First, randomize the order of the type of problem being solved so that different concepts are being activated in long-term memory.5 Be forewarned, though, that randomizing the order improves learning outcomes but requires more effort.6 The second strategy is to take short breaks at random intervals to enhance memory consolidation. A 10-second break every 2–5 minutes is recommended.18 Back to Top 6. The Internet Has Not Made Learning Obsolete The availability of programming knowledge changed with the advent of the Internet. Knowledge about syntax or APIs went from being buried in reference books to being a few keystrokes away. Most recently, AI-powered tools such as ChatGPT, Codex, and GitHub Copilot will even fill in these details (mostly accurately) for you. This raises an obvious question: Why is it worth learning details—or anything at all—if the knowledge is available from the Internet within seconds? We learn by storing pieces of knowledge in our long-term memory and forming connections between them.1 If the knowledge is not present in the brain, because you have not yet learned it well, the brain cannot form any connections between it, so higher levels of understanding and abstraction are not possible.1 If every time you need a piece of code to do a database join you search online for it, insert it, and move on, you will be unlikely to learn much about joins. The wisdom of relying on the Internet or AI differs between beginners and experts: There is a key distinction between a beginner who has never learned the details and thus lacks the memory connections, and an expert who has learned the deeper structure but searches for the forgotten fine details.1 There is even some evidence to suggest that searching the Internet is less efficient for remembering information. One study found that information was remembered less well if it was found via the Internet (compared to a physical book).11 Another found that immediately searching the Internet led to worse recall of the same information later, compared to first trying to think of the answer before resorting to searching.14 It seems that searching may rob the brain of the benefits of the memory-strengthening effect of recalling information. There is also the issue of cognitive load discussed earlier. An Internet search requires a form of context switching for the brain; its limited attention and working memory must be switched from the task at hand (programming) to a new cognitive task (searching the Internet and selecting a result or evaluating an AI-generated result). If the required knowledge is instead memorized, then not only is access much faster (like using a cache versus fetching from a hard disk), but it also avoids the cognitive drain of context switching and filtering out extraneous information from the search. So there are multiple reasons to memorize information, despite it being available on the Internet. Back to Top 7. Problem-Solving Is Not a Generic Skill Problem-solving is a large part of programming. One common (but incorrect) idea in software development is to directly teach problem-solving as a specific skill, which can then be applied to different aspects of development (design, debugging, and so on). Thus, problem-solving is (incorrectly) conceived as a generic skill. However, this is not how problem-solving works in the brain. While humans do have some generic problem-solving skills, they are much less efficient than domain-specific problem-solving skills, such as being able to debug programs. While we can learn to reason, we do not learn how to solve problems in general. Instead, we learn how to solve programming problems, or how to plan the best chess move, or how to create a knitting pattern. Each of these skills is separate and does not influence the others. Research into chess found little or no effect of learning it on other academic and cognitive skills, and the same is true for music instruction and cognitive training.36 This inability to transfer problem-solving skills is why \"brain training\" is ineffective for developing general intelligence.29 The one exception to this rule appears to be spatial skills. Spatial skills allow us to visualize objects in our mind, like a Tetris shape, and mentally manipulate those objects, like rotating a Tetris shape. Training these generic skills can improve learning in other disciplines. This phenomenon is so unusual that it has caused much consternation in cognitive and learning sciences.24 Yet, spatial training improves performance on a range of non-verbal skills regardless of initial ability, age, or type of training task.40 Recent work has even demonstrated that spatial training can improve efficiency for professional software developers, likely because they are still learning new concepts.30 Even with this strange exception, the best way to learn how to solve programming problems is still to practice solving programming problems rather than looking for performance benefits from learning chess or other cognitive training. There is a secondary implication here for recruitment. One popular idea for screening programming candidates was to give brain-teaser puzzles, such as how to weigh a jumbo jet. As Google worked out by 2013, this is a waste of time7—there is no reliable correspondence between problem-solving in the world of brain teasers and problem-solving in the world of programming. If you want to judge programming ability, assess programming ability. Back to Top 8. Expertise Can Be Problematic in Some Situations We have discussed many ways in which expertise benefits learning and performance. However, being an expert can also lead to problems. Programmers use tools and aids to be more effective, such as version control systems or IDEs. Such tools can have different effects on beginners and experts. Beginners may get overwhelmed by the amount of options available in professional tools (due to increased cognitive load) and may benefit from beginner-friendly hints on how to use the tool. However, experts find the same hints more distracting than useful because they already know what to do. This is known as the expertise-reversal effect: Hints and guides that help beginners can get in the way of experts and make them less productive. Programmers usually learn multiple programming languages throughout their careers. Knowing multiple languages can be beneficial once they have been mastered, but sometimes transferring knowledge from one programming language to another can lead to faulty knowledge. For example, a programmer may learn about inheritance in Java, where one method overrides a parent method as long as the signatures match, and transfer this knowledge to C++, where overriding a parent method additionally requires that the parent method is declared virtual. These kinds of differences—where features are similar in syntax but different in semantics between languages—specifically hinder the transfer of knowledge.39 Experts often help to train beginners, but experts without experience in training others often do not realize that beginners think differently. Thus, they fail to tailor their explanations for someone with a different mental model. This is known as the expert blind-spot problem: difficulty in seeing things through the eyes of a beginner once you have become an expert. It can be overcome by listening carefully to beginners explain their current understanding and tailoring explanations accordingly. Sometimes, however, knowledge becomes so automated that it is difficult for experts to verbalize it.1 This automated knowledge is why experts have intuitions about how to solve problems or explain their process as, \"I just know.\" In these cases of tacit knowledge, beginners might better learn from instructional materials designed to support beginners, often called scaffolded instruction, or from a peer rather than an expert. A more knowledgeable (but still relatively novice) peer is a highly valuable resource to bridge the gap between beginners and experts. They can help the beginner develop new knowledge and the expert to rediscover automated knowledge. Back to Top 9. The Predictors of Programming Ability Are Unclear The success of learning programming, like most activities, is built on a mix of inherent aptitude and practice. Some people believe it is purely about aptitude—the \"you're born with it\" view—and some believe it is almost entirely about practice—the \"10,000 hours\" idea that only sufficient practice is required for expertise. Both extreme views are wrong, and in this section, we will explore the evidence for the differing effects of aptitude and practice. There has been much research to try to predict programming aptitude but few reliable results. Attempts to produce a predictive test for programming ability have generally come to naught. Research has found that all of the following fail to predict programming ability: gender, age, academic major, race, prior performance in math, prior experience with another programming language, perceptions of CS, and preference for humanities or sciences.35 There was an industry of aptitude tests for programming that began in the 1960s, but as Robins33 summarizes, the predictive accuracy was poor and the tests fell out of use. There is mixed evidence for the importance of years of experience, which relates to practice. There is a correlation between the reputation of programmers on Stack Overflow and their age: Older people have a higher reputation.27 However, a recent study found only a weak link between years of experience and success on a programming task among programmers who were relatively early in their careers,31 suggesting that aptitude may have a stronger effect than experience, at least early in programmers' careers. As in most domains, two factors that weakly predict success in early programming are general intelligence and working memory capacity.4 These factors roughly represent reasoning skills and how much information a learner can process at once. As such, they predict the rate of learning rather than absolute ability. A sub-measure of these two factors, spatial reasoning, is a stronger predictor of success in programming, though still quite moderate.30 Spatial reasoning also predicts success in other science and math fields,24 so this is not programming-specific. Further, these weak-to-moderate correlations largely disappear with increased experience for various reasons. Thus, intelligent people will not always make good programmers, and good programmers need not be high in general intelligence. In short, it is very hard to predict who will be able to program, especially in the long term. Programmers could come from any background or demographic, and links to any other factors (such as intelligence) are generally fleeting in the face of experience. Therefore, in recruiting new programmers, there are no shortcuts to identifying programming ability, nor are there any reliable \"candidate profiles\" to screen candidates for programming ability. Back to Top 10. Your Mindset Matters There is a long-standing idea of a binary split in programming ability: You either can program or you cannot. There have been many competing theories behind this. One of the more compelling theories is the idea of learning edge momentum,33 that each topic is dependent on previous topics, so once you fall behind you will struggle to catch up. A less compelling theory is the idea of a \"geek gene\" (you are born with it or not), which has little empirical evidence.26 As discussed in the previous section, we have recently come to understand differences in programming ability as differences in prior experience.16 Learners who might seem similar (for example, in the same class, with the same degree, completing the same bootcamp) can have vastly different knowledge and skills, putting them ahead or behind in terms of learning edge momentum or, within a snapshot of time, making them seem \"born with it\" or not. A similar effect is found in any highly technical field that is optionally taught before university (for example, CS, physics, and engineering).9 The binary split view, and its effects on teaching and learning, have been studied across academic disciplines in research about fixed versus growth mindsets.12 A fixed mindset aligns with an aptitude view that people's abilities are innate and unchanging. Applied to learning, this mindset says that if someone struggles with a new task, then they are not cut out for it. Alternatively, a growth mindset aligns with a practice view—that people's abilities are malleable. Applied to learning, this mindset says that if someone struggles with a new task, they can master it with enough practice. Attempts to produce a predictive test for programming ability have generally come to naught. As described in Cheryan et al.,9 neither extreme view is true. For example, practically everyone can learn some physics, even if they are not initially good at it. However, practically no one can earn the Nobel Prize in Physics, no matter how much they practice. Between these extremes, we are often trying to figure out the boundaries of our abilities. When teachers and learners approach new tasks with a growth mindset, they tend to persist through difficulties and overcome failure more consistently.12 While the evidence for this effect is strong and intuitive, research suggests it can be difficult to change someone's mindset to be more growth-oriented.8 In particular, there are two common misconceptions about how to promote a growth mindset that prove ineffective. The first misconception is to reward effort rather than performance because a growth mindset favors practice over aptitude. But learners are not stupid; they can tell when they are not progressing, and teachers praising unproductive effort is not helpful. Instead, effort should be rewarded only when the learner is using effective strategies and on the path to success.13 The second misconception is that when someone approaches a task with a growth mindset, they will maintain that mindset throughout the task. In reality, as we face setbacks and experience failure, people skew toward a fixed mindset because we are not sure where the boundaries of our abilities lie. Thus, we must practice overcoming setbacks and failures to maintain a growth-mindset approach.13 A related concept to fixed and growth mindsets is goal orientation. This is split into two categories: approach and avoidance. The \"approach\" goal orientation involves wanting to do well, and this engenders positive and effective learning behaviors: working hard, seeking help, and trying new and challenging topics. In contrast, the \"avoidance\" goal orientation involves avoiding failure. This leads to negative and ineffective behaviors: disorganized study, not seeking help, anxiety over performance, and avoiding challenge. It is important that learners can make mistakes without severe penalties if they are to be directed toward \"approach\" rather than \"avoidance.\" Intelligent people will not always make good programmers, and good programmers need not be high in general intelligence. When learning a new skill or training someone in a new skill, remember that approaching tasks with a growth mindset is effective but also a skill to be developed. Unfortunately, we cannot simply tell people to have a growth mindset and reap the benefits. Instead, nurture this skill by seeking or providing honest feedback about the process of learning and the efficacy of strategies. For mentors, praise areas where a men-tee is making progress and accept that they will make mistakes without chastising them. For learners, reflect on how skills have improved in the past weeks or months when you are doubtful about your progress. Further, expect that a growth mindset will shift toward a fixed mindset in the face of failure, but it can also be redeveloped and made stronger with practice. Feeling discouraged is normal, but it does not mean that you will always feel discouraged. If you feel like quitting, take a break, take a walk, consider your strategies, and then try again. Back to Top Summary Software developers must continually learn in order to keep up with the fast-paced changes in the field. Learning anything, programming included, involves committing items to memory. Human memory is fascinatingly complex. While it shares some similarities with computer architecture, there are key differences that make it work quite differently. In this article, we have explained the current scientific understanding of how human memory works, how learning works, the differences between beginners and experts, and related it all to practical steps that software developers can take to improve their learning, training, and recruitment. Recommendations. We have split up our recommendations into those for recruiting and those for training and learning. For recruiting, we make the following recommendations: There are no good proxies for programming ability. Stereotypes based on gender, race, or other factors are not supported by evidence. If you want to know how well candidates program, look at their previous work or test them on authentic programming tasks. To emphasize a specific point: Do not test candidates with brain-teaser puzzles. At least among young developers, years of experience may not be a very reliable measure of ability. A related recommendation from Behroozi et al.3 is to get candidates to solve interview problems in a room on their own before presenting the solution, as the added pressure from an interviewer observing or requiring talking while solving it adds to cognitive load and stress in a way that impairs performance. For learning and training, we make the following recommendations: Reading a lot of code will help someone become a more efficient programmer. Experts are not always the best at training beginners. Learning takes time, including time between learning sessions. Intense cramming is not effective, but spaced repetition is. Similarly, spending time away from a problem can help to solve it. Just because you can find it through an Internet search or generative AI tool does not mean learning has become obsolete. Use examples to go between abstract concepts and concrete learnable facts. Seeking to succeed (rather than avoid failure) and believing that ability is changeable are important factors in resilience and learning. Further reading. Many books on learning center around formal education; they are aimed at school teachers and university lecturers. However, the principles are applicable everywhere, including professional development. We recommend three books: Why Don't Students Like School? by Daniel T. Willingham provides a short and readable explanation of many of the principles of memory and how the brain works. The Programmer's Brain by Felienne Hermans et al.c relates these concepts to programming and describes how techniques for learning and revision that are used at school can still apply to professional development. How Learning Happens: Seminal Works in Educational Psychology and What They Mean in Practice by Paul A. Kirschner and Carl Hendrick20 provides a tour through influential papers, explaining them in plain language and the implications and linkages between them. The papers cited can also serve as further reading. If you are a software developer you may not have access to all of them; ACM members with the digital library option will have access to the ACM papers, although many of our references are from other disciplines. For more recent papers, many authors supply free PDFs on their websites; you may wish to try searching the Web for the exact title to find such PDFs. Many authors are also happy to supply you with a copy if you contact them directly. Back to Top Acknowledgments We are grateful to Greg Wilson, who was instrumental in initiating and proofreading this paper, and to our other proofreaders: Philip Brown, Kristina Dietz, Jack Parkinson, Anthony Robins, and Justin Thurman. This work is funded in part by the National Science Foundation under grant #1941642. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation. Figure. Watch the authors discuss this work in the exclusive Communications video. https://cacm.acm.org/videos/tenthings-software-developers Back to Top References 1. Anderson, J.R. Cognitive Psychology and its Implications. Macmillan (2005). 2. Baddeley, A. Working memory. Science 255, 5044 (1992), 556–559. 3. Behroozi, M., Shirolkar, S., Barik, T., and Parnin, C. Does stress impact technical interview performance? In Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conf. and Symp. on the Foundations of Software Engineering (2020), 481–492; https://bit.ly/3Sj3AHn 4. Bergersen, G.R. and Gustafsson, J.-E. Programming skill, knowledge, and working memory among professional software developers from an investment theory perspective. J. of Individual Differences 32, 4 (2011), 201. 5. Bjork, R.A. and Allen, T.W. The spacing effect: Consolidation or differential encoding? J. of Verbal Learning and Verbal Behavior 9, 5 (1970), 567–572. 6. Bjork, R.A. and Bjork, E.L. Desirable difficulties in theory and practice. J. of Applied Research in Memory and Cognition 9, 4 (2020), 475. 7. Bryant, A. In head-hunting, big data may not be such a big deal. The New York Times (June 2013); https://nyti.ms/3Msdraa. 8. Burgoyne, A.P., Hambrick, D.Z., and Macnamara, B.N. How firm are the foundations of mind-set theory? The claims appear stronger than the evidence. Psychological Science 31, 3 (2020), 258–267; https://bit.ly/3MsdPp4 9. Cheryan, S., Ziegler, S.A., Montoya, A.K., and Jiang, L. Why are some STEM fields more gender balanced than others? Psychological Bulletin 143, 1 (2017), 1. 10. Dempster, F.N. The spacing effect: A case study in the failure to apply the results of psychological research. American Psychologist 43, 8 (1988), 627. 11. Dong, G. and Potenza, M.N. Behavioural and brain responses related to Internet search and memory. European J. of Neuroscience 42, 8 (2015), 2546–2554; https://bit.ly/49loGL6 12. Dweck, C.S. Mindset: The New Psychology of Success. Random House (2006). 13. Dweck, C.S. and Yeager, D.S. Mindsets: A view from two eras. Perspectives on Psychological Science 14, 3 (2019), 481–496. 14. Giebl, S. et al. Thinking first versus Googling first: Preferences and consequences. J. of Applied Research in Memory and Cognition (2022); https://bit.ly/3QlXun5 15. Gobet, F. and Simon, H.A. The roles of recognition processes and look-ahead search in time-constrained expert problem solving: Evidence from grand-master-level chess. Psychological Science 7, 1 (1996), 52–55; https://bit.ly/40oT4jY 16. Grabarczyk, P., Nicolajsen, S.M., and Brabrand, C. On the effect of onboarding computing students without programming-confidence or experience. In Proceedings of the 22nd Koli Calling Intern. Conf. on Computing Education Research (2022), 1–8. 17. Hazzan, O. Reducing abstraction level when learning abstract algebra concepts. Educational Studies in Mathematics 40, 1 (Sept. 1999), 71–90; https://bit.ly/49lZd4f 18. Huberman, A. Teach & learn better with a \"neuroplasticity super protocol\". Neural Network (Oct. 2021). 19. Kahneman, D. Thinking, Fast and Slow. Macmillan, 2011. 20. Kirschner, P.A. and Hendrick, C. How Learning Happens: Seminal Works in Educational Psychology and What They Mean in Practice. Routledge (2020). 21. Kleitman, N. Basic rest-activity cycle—22 years later. Sleep 5, 4 (1982), 311–317. 22. Kyllonen, P.C. and Christal, R.E. Reasoning ability is (little more than) working-memory capacity?! Intelligence 14, 4 (1990), 389–433. 23. Margulieux, L. et al. When wrong is right: The instructional power of multiple conceptions. In Proceedings of the 17th ACM Conf. on Intern. Computing Education Research (2021), 184–197. 24. Margulieux, L.E. Spatial encoding strategy theory: The relationship between spatial skill and STEM achievement. In Proceedings of the 2019 ACM Conf. on Intern. Computing Education Research, 81–90; 10.1145/3291279.3339414. 25. Maton, K. Making semantic waves: A key to cumulative knowledge-building. Linguistics and Education 24, 1 (2013), 8–22; https://bit.ly/3tXhvZt. 26. McCartney, R. et al. Folk pedagogy and the geek gene: geekiness quotient. In Proceedings of the 2017 ACM SIGCSE Technical Symp. on Computer Science Education (2017), 405–410. 27. Morrison, P. and Murphy-Hill, E. Is programming knowledge related to age? An exploration of stack overflow. In 2013 10th Working Conf. on Mining Software Repositories (MSR), 69–72; https://bit.ly/3Sp5Mgv. 28. Newton, P.M. and Salvi, A. How common is belief in the learning styles neuromyth, and does it matter? A pragmatic systematic review. Frontiers in Education (2020), 5; https://bit.ly/47h0Jmo. 29. Owen, A.M. et al. Putting brain training to the test. Nature 465, 7299 (2010), 775–778. 30. Parkinson, J. and Cutts, Q. Relationships between an early-stage spatial skills test and final CS degree outcomes. In Proceedings of the 53rd ACM Technical Symp. on Computer Science Education 1, (2022), 293–299; 10.1145/3478431.3499332. 31. Peitek, N. et al. Correlates of programmer efficacy and their link to experience: A combined EEG and eye-tracking study. In Proceedings of the 30th ACM Joint European Software Engineering Conf. and Symp. on the Foundations of Software Engineering (Nov. 2022), 120–131. 32. Raufaste, E., Eyrolle, H., and Mariné, C. Pertinence generation in radiological diagnosis: Spreading activation and the nature of expertise. Cognitive Science 22, 4 (1998), 517–546; https://bit.ly/3sidUoq 33. Robins, A. Learning edge momentum: A new account of outcomes in CS1. Computer Science Education 20, 1 (2010), 37–71. 34. Robins, A.V. Dual process theories: Computing cognition in context. ACM Trans. Comput. Education 22, 4 (Sept. 2022); 10.1145/3487055. 35. Rountree, N., Rountree, J., Robins, A., and Hannah, R. Interacting factors that predict success and failure in a CS1 course. ACM SIGCSE Bulletin 36, 4 (2004), 101–104. 36. Sala, G. and Gobet, F. Does far transfer exist? Negative evidence from chess, music, and working memory training. Current Directions in Psychological Science 26, 6 (2017), 515–520; 10.1177/0963721417712760. 37. Simons, D.J. and Chabris, C.F. What people believe about how memory works: A representative survey of the U.S. population. PLOS ONE 6, 8 (Aug. 2011), 1–7; 10.1371/journal.pone.0022757. 38. Thalmann, M., Souza, A.S., and Oberauer, K. How does chunking help working memory? J. of Experimental Psychology: Learning, Memory, and Cognition 45, 1 (2019), 37. 39. Tshukudu, E. and Cutts, Q. Understanding conceptual transfer for students learning new programming languages. In Proceedings of the 2020 ACM Conf. on Intern. Computing Education Research, 227–237; 10.1145/3372782.3406270. 40. Uttal, D.H. et al. The malleability of spatial skills: A meta-analysis of training studies. Psychological Bulletin 139, 2 (2013), 352. Back to Top Authors Neil C.C. Brown is a senior research fellow at King's College London, U.K. Felienne F.J. Hermans is a professor at Vrije Universiteit Amsterdam, The Netherlands. Lauren E. Margulieux (lmargulieux@gsu.edu) is an associate professor at Georgia State University, Learning Sciences, Atlanta, GA, USA. Back to Top Footnotes a. This is not an informal description: the technical term is actually \"chunks.\" b. Parts of Kahneman's book were undermined by psychology's \"replication crisis,\" which affected some of its findings, but not the idea of system 1 and 2. c. Full disclosure: This is written by one of the authors although the other authors recommend it as well. More online: A List of full references and supplementary information is available at https://bit.ly/3G2NPNl. This work is licensed under a Creative Commons Attribution-NonCommercial International 4.0 License. The Digital Library is published by the Association for Computing Machinery. Copyright © 2024 ACM, Inc. No entries found Sign In for Full Access User Name Password » Forgot Password? » Create an ACM Web Account Sign In Article Contents: Introduction Key Insights 1. Human Memory Is Not Made of Bits 2. Human Memory Is Composed of One Limited and One Unlimited System 3. Experts Recognize, Beginners Reason 4. Understanding a Concept Goes from Abstract to Concrete and Back 5. Spacing and Repetition Matter 6. The Internet Has Not Made Learning Obsolete 7. Problem-Solving Is Not a Generic Skill 8. Expertise Can Be Problematic in Some Situations 9. The Predictors of Programming Ability Are Unclear 10. Your Mindset Matters Summary Acknowledgments References Authors Footnotes More News & opinions Google Tests AI Assistant That Offers Life Advice The New York Times The Gamification of Academia Sean Flaherty, Gregg Gordon How to Ace IT Product Localization: The 101 Guide Alex Tray \"> For Authors For Advertisers Privacy Policy Help Contact Us Mobile Site Copyright © 2023 by the ACM. All rights reserved.",
    "commentLink": "https://news.ycombinator.com/item?id=38777647",
    "commentBody": "Things software developers should learn about learningHacker NewspastloginThings software developers should learn about learning (acm.org) 260 points by jarmitage 9 hours ago| hidepastfavorite55 comments xjay 1 hour agoWarning from Daniel Kahneman on \"System 1\" and \"System 2\". [1]> I&#x27;m going to use \"System 1\" and \"System 2\", absolutely as homunculi. [...] They don&#x27;t exist. [...] Don&#x27;t look for them in the brain, because they are not two systems in the brain, of which one does one, and the other does the other. So why am I using this terrible language? I&#x27;m using it because I think it&#x27;s helpful. It fits the way our minds work, and to explain the background of that decision--of why I use \"System 1\" and \"System 2\"--I refer you to a very good book. [...] It&#x27;s by Joshua Foer and it&#x27;s called \"Moonwalking with Einstein\". [2][1] https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=CjVQJdIrDJ0&t=1224s[2] https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Moonwalking_with_EinsteinThe article does put these terms in quotes. reply wisemang 5 hours agoprevI appreciate the succinct detail they go into regarding how the concept of “learning styles” has been debunked. I’ve always considered myself more of a visual learner, and seeing posts here and elsewhere about this supposedly not being a thing has never sat well with me (though I never delved into the actual research).Turns out I do in fact agree with this explanation; that it’s more about what’s being taught that should dictate the how:> While learners have preferred styles, effective instruction matches the content, not learning styles. A science class should use graphs to present data rather than verbal descriptions, regardless of visual or auditory learning styles, just like cooking classes should use hands-on practices rather than reading, whether learners prefer a kinesthetic style or not. reply josephg 29 minutes agoparent> I’ve always considered myself more of a visual learner, and seeing posts here and elsewhere about this supposedly not being a thing has never sat well with me (though I never delved into the actual research).My understanding of the research is that your preference for visual learning is real. But your preference doesn&#x27;t actually translate into better learning outcomes. Ie, you might prefer visual stimulus but the research suggests you&#x27;ll still learn content just as fast if its presented in other mediums. reply cncivubyv 6 minutes agorootparentIs that for the people that didn&#x27;t drop out or are the drop outs included in the stats? reply oatmeal1 5 hours agoparentprevCourses teaching music theory through visual depictions, take note! reply NetOpWibby 4 hours agorootparentThis is why I failed music class in high-school. I was the only non-band student so the work was accelerated. reply jacobolus 4 hours agorootparentThis is why trying to rank&#x2F;judge&#x2F;grade everyone by a uniform standard is almost universally terrible.Students should be encouraged to try their best in every subject, allowed to make the mistakes they are naturally going to make at whatever level they are currently, and helped to improve over time. Punishing people for being less prepared than peers who did more practice or for making ordinary and expected mistakes actively gets in the way of their learning, as well as making them feel terrible. It&#x27;s pretty bad for the students who are more prepared as well, as many of them internalize the idea that they are inherently good at some things and inherently bad at others, which is sometimes temporarily gratifying but often stops them from pushing themselves to try anything new or hard. reply sandos 19 minutes agorootparentI often end up in this concept of the \"real world\" when talking about kids and their up-bringing. Iv&#x27;e always felt school needs to be much more individually tailored, and this connects to my real-world thinking about kids: I envision how the world \"outside\" of the family will treat my kids, and what they need to do well out there. Same thing applies to school, really. And in the real world we do not expect everyone to know the same things, do the same job, or have the same personality or talent as everyone else. So why should schools be like this? Its just stupid. reply WalterBright 1 hour agorootparentprevHaving some pressure to learn things is essential. I discovered that if I audited a course, I never learned much. I needed the pressure of grades. reply johnnyanmac 14 minutes agorootparentI agree and disagree. I agree in that pressure is good, I disagree that audits are bad because of no pressure. Audits mean that is low priority which means that it will fall off when your busy schedule of \"actual\" courses have deadlines. Which is not a refletion of your ability nor interest to learn but of your workload.This is the main reason I hold resentment towards GE&#x27;s. Not because I don&#x27;t want to be a well rounded person, but because when 3-4 other major classes are already crushing you the last bit of \"pressure\" needed is some random music theory or history course quizzing you. I never really got the time to breathe in college, and taking my time woulda been a $20k+ decision on top of the $80k I already had in debt. I literally could not afford to learn properly. reply Jensson 54 minutes agorootparentprevThe problem here was too little testing, not too much. If they tested the students on music before they started they would have been put in classes where they belong and they could have gotten the teaching they needed. reply master-lincoln 56 minutes agorootparentprevA bad mark is not a punishment, just an evaluation of your learning level. reply johnnyanmac 19 minutes agorootparentand can lead to a punishment of repeating a class or entire grade. reply Jensson 12 minutes agorootparentYou are telling me that giving them more education is punishment? No matter how you do it they get left behind, if they continue to get put in classes they aren&#x27;t ready for that is bad as well. reply shrubhub 1 hour agorootparentprevHow do you decide who gets to study a subject full time at a famous institution? reply arvinsim 4 hours agorootparentprevSorry if it is bot clear. Is it a good or bad thing to teach music theory via visual depiction? reply taopai 53 minutes agoparentprevDual coding.The material should present in multiple ways.Visually AND verbally is preferred. reply TibbityFlanders 4 hours agoprevhttps:&#x2F;&#x2F;today.ucsd.edu&#x2F;story&#x2F;a-new-replication-crisis-resear....> Papers that cannot be replicated are cited 153 times more because their findings are interesting, according to a new UC San Diego study> In psychology, only 39 percent of the 100 experiments successfully replicated. reply alphazard 7 hours agoprevIn section 7:> Problem Solving is Not a Generic SkillThere is going to be some difference between solving problems in a specific domain and solving problems generally (which is what TFA argues for). And since we really care about the specific domain of software engineering, it makes sense to pry open that difference when possible.However problem solving in the general case is very close to fluid intelligence and IQ. Some interpretations claim that intelligence in humans is just problem solving, and that problem solving is most of what is captured by g [0]. All problem solving will be positively correlated with all other problem solving, and you would never expect to see someone good at one, but not the other.In section 9 they cite the research on programming ability and its (expected) relation to general intelligence.I&#x27;m not sure how much of a distinction there is to draw here. Psychometrics has historically been filled with attempts to factor out additional clusters from things like g e.g. multiple intelligences. Those findings often fail to replicate. Section 7 seems more like an attempt to draw a distinction without a difference. While section 9 seems like a standard summary of the research (like most things, a mixture of innate intelligence and cumulative experience).[0] https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;G_factor_(psychometrics) reply Kinrany 2 hours agoparentTheir point is that you can&#x27;t learn general problem solving. IQ is not a skill. reply ozim 2 hours agorootparentI would say you can learn general problem solving. There are strategies that help with that an IQ helps with finding patterns.That said it still would be junior level problem solving in unknown domain. Expert will always run circles around newcomers.I most likely could switch fields but I don’t want to spend 5 years getting experience. Even if I have decent generic solving skills it might take me less time but still- I would have to be really interested in the topic. reply Jensson 1 hour agorootparent> Expert will always run circles around newcomers.That isn&#x27;t true, there are many problems less than mediocre experts fails to solve that a smart junior can manage to solve. Experts are great at common problems but not great at rarer ones, non-standard problems depend more on natural ability than experience.Of course smart experts can also solve those problems, but you didn&#x27;t say a smart expert, you just said experts in general.The ability to generalize based on experience requires intelligence. That is why you have so many expert programmers who can&#x27;t solve problems well, because they lack the ability to generalize their experience well enough to apply it to new problems. reply remram 22 minutes agoprevWhy are \"key insights\" presented as an ugly GIF? Just so it can&#x27;t be seen by screen readers? Or maybe robots?If you really want to put your text in an image, can you pick a decent font and not make it blurry? Puzzling. reply vinay_ys 4 hours agoprev> Experts Recognize, Beginners Reason> System 1 is fast and driven by recognition, relying upon pattern recognition in long-term memory, while system 2 is slower and focused on reasoning, requiring more processing in working memory.Interestingly, today, LLMs are augmentation for someone&#x27;s weak system 1, and allowing them to focus solely on strengthening their system 2. LLMs and popular&#x2F;cheap&#x2F;generalizable AI today suck at system 2. So, if you are really good at system 2 and suck at system 1, the next decade is going to be amazing for you. reply Jensson 59 minutes agoparentYou need good system 1 to recognize when the LLM is wrong.But your explanation makes sense, it also helps explain why you see so many post LLM responses they say are correct and proof the LLM can solve the problem, but then the thing they posted is bonkers and wrong. If those people lack a good system 1 it explains all of that, also helps explain which kind of person likes to work with LLMs. reply rramadass 1 hour agoparentprevAlso see Recognition-Primed Decision Model: https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Recognition-primed_decision reply TibbityFlanders 3 hours agoparentprev(removed) reply Always42 2 hours agorootparentcan you post enough about your local LLM setup for me to google&#x2F;rep. this?There is so much out there for LLM&#x27;s parsing is a pain. reply auggierose 1 hour agoprevI like the paper, and I think it makes many valuable points. I especially like the semantic wave model, because that&#x27;s exactly how you learn mathematics. And it also explains why (high-quality) abstractions are so important.But of course everything has to be taken with a grain of salt. For example, their recommendations at the end on how to access papers is not very good. Ever heard of Sci-Hub and VPNs? It is obvious why they cannot mention this in their paper, but it is also equally obvious then that if there was evidence linking race or gender with programming ability, they would not mention it, for pretty much the same reasons.I also don&#x27;t like their example of achieving a Nobel Prize as something that practically no one can attain. Yes, that&#x27;s true, but that is because Nobel Prizes are artificially limited to a few people a year. I think many, many more people can achieve that level of expertise than just a few per year. reply abaymado 2 hours agoprev> Research into chess found little or no effect of learning it on other academic and cognitive skills, and the same is true for music instruction and cognitive training.This inability to transfer problem-solving skills is why \"brain training\" is ineffective for developing general intelligence.I would disagree with this premise, deep work and the forbidden word \"discipline\" are problem solving skills that are learned and need constant training. They are just as important as any other specific skill needed for the subject. Thus, making some problem-solving skills indeed free flowing from subject to subject. reply rand1239 44 minutes agoprevI wonder whether including this article in GPT-5 training data would improve the performance of the model. reply mewpmewp2 8 hours agoprevThe article looks very accurate to me. Having read all of that, I find myself agreeing with most of it, which I think usually would not be the case for me with an article like this. reply thebenedict 7 hours agoparentI was also pleasantly surprised by the quality of this article, especially the nuance around the value of a growth mindset. It&#x27;s consistent with my experience of having had to work pretty hard to learn programming. reply mewpmewp2 7 hours agorootparentIt seems like a clickbait title combined with nuanced points of high quality and relevancy. reply acosmism 8 hours agoprevhug of death: https:&#x2F;&#x2F;web.archive.org&#x2F;web&#x2F;20231227005023&#x2F;https:&#x2F;&#x2F;cacm.acm.... reply emmanueloga_ 5 hours agoparentIt seems they have clouflare support, but must be misconfigured somehow? Also rails? 1.4 secs seems like a long response time, even for a rails app. I guess even the pros get websites wrong sometimes :-p Cache-Control: private, max-age=0, must-revalidate Cf-Cache-Status: DYNAMIC Cf-Ray: 83be9f774919fa8a-SJC Content-Encoding: gzip Content-Type: text&#x2F;html; charset=utf-8 Date: Wed, 27 Dec 2023 03:54:35 GMT Server: cloudflare Status: 200 OK Vary: Accept-Encoding X-Powered-By: Phusion Passenger X-Runtime: 1.42775 reply rTX5CMRXIfFG 5 hours agoprev> a fact does not exist in a binary state of either definitively known or unknown; it can exist in intermediate states. We can forget things we previously knew, and knowledge can be unreliable, especially when recently learned.I’m sorry, what? If I only vaguely understood physics and believed now that the earth is flat, that would neither count as knowledge in some intermediate state. Knowing is binary—you either know or you don’t, no matter how strongly believe what you think. reply BlackFly 2 hours agoparentThis just isn&#x27;t how the verb \"to know\" and the noun \"knowledge\" are commonly used and understood. By your understanding, it is impossible to know anything about the future. Yet, I know my wife will come home in a few hours. Very few people would object to such usage and people would stand by me and agree how unexpected it was if she failed to return. That our knowledge is imperfect surprises few people in everyday usage. reply rTX5CMRXIfFG 31 minutes agorootparentThere&#x27;s much that can be said about how so many words turn out, in fact, to be very poorly understood especially if when using common sense as basis for definition---but the point is, the article is talking about learning from a scientific standpoint, so the colloquial sense of \"knowledge\" is irrelevant. reply Cerium 4 hours agoparentprevAn example: What did you eat for dinner? If you know the answer for sure, go back a day, or another day, etc. At some number of days you will have an idea of what you ate but not be entirely sure.You certainly knew what you ate when you were eating it, but now? reply rTX5CMRXIfFG 4 hours agorootparentIf I don’t know that now but knew it then, then knowledge is still binary is it not? reply RoyalHenOil 2 hours agorootparentYou don&#x27;t have perfect recollection one day and then no recollection whatsoever the next day. Memories become vague as they deteriorate.For example, you may remember something you learned in 4th grade, but you probably don&#x27;t remember exactly how the teacher explained it or what questions about it were on the test. reply master-lincoln 53 minutes agorootparentOk, but then I don&#x27;t know how I was taught that thing, even though I know the thing. Still binary reply rTX5CMRXIfFG 24 minutes agorootparentprevYou don&#x27;t have knowledge if you don&#x27;t have perfect recollection. Put in other words, if you cannot fully remember something, then you do not know it. We need to come terms with that and honestly I might be striking a nerve here because I&#x27;m triggering people&#x27;s Dunning-Kruger.The state of knowing at any point in time is binary, which means that it is possible to un-know something that you used to know. replyzubairq 5 hours agoprevSome good points, programming t00 much can sometimes distort my thinking ! reply bedobi 7 hours agoprevallow me to shamelessly plug something I wrote on the topichttps:&#x2F;&#x2F;gist.github.com&#x2F;androidfred&#x2F;75629dfda63180b6f0a0eaa4...no data, no research :P just anecdote and opinon reply eszed 4 hours agoparentI really like what you&#x27;ve written. It jibes with my own experience and opinions. If I were to expand it a bit further, I&#x27;d point out that there are teaching methodologies that seem to be derived from these paradigms. In one, an apprentice copies the master as well as they are able, sometimes with no explicit instruction at all, until they&#x27;ve filled in the details for themselves; in the other the student memorizes facts and methods, often by rote, until they build up the big picture for themselves.I&#x27;ve learned within (and from) both approaches, and find each - when rigidly followed - to be highly frustrating! In my own pedagogy I try to hold both in mind, and calibrate students&#x27; learning paths accordingly. When they&#x27;re mired in detail, I re-orient them towards the end goal; when they&#x27;re not sure what to do, or how to do it, I guide them through the next step. It&#x27;s a lot more effort, because you have to pay attention to them, and not only the subject, which most teachers would prefer not (or don&#x27;t know how) to do.(On a side note, I&#x27;ll say that - in the field(s?) where I&#x27;m an expert - nearly all of the pleasure comes from refining the last 2% of the details. It&#x27;s never going to be perfect, but it can always be incrementally better. It&#x27;s not \"productive\", certainly in a commercial sense, but it&#x27;s immensely satisfying.) reply rmrf100 2 hours agoprevsometimes it&#x27;s really astonishing to see that, children grow up day by day then quikly understond the logic which hard to them before. reply aksss 1 hour agoprev> “Higher capacity enables faster learning, but our unlimited long-term memory removes limitations on how much we could ultimately learn in total.1 Expert programmers may have low or high working memory capacity but it is the contents of their long-term memory that make them experts.”I’ve always told “kids” that you can learn a lot about systems but with programming and IT systems in general, there is just no substitute for getting the raw mileage of having seen many permutations, iterations, and manifestations. It’s not a dig, but a statement made in the context of encouraging new people to stick with it and not beat themselves up too much when they inevitably get overwhelmed by the scope of their unknowns or roll a critical miss. It’s all about learning, all the time. reply readthenotes1 8 hours agoprevIirc, System 1&#x2F;2 thinking has been largely de-based (i.e., it could be a right model, but the evidence for that is tenuous)https:&#x2F;&#x2F;www.psychologytoday.com&#x2F;us&#x2F;blog&#x2F;a-hovercraft-full-of...Makes me wonder if item (0) should be \"Suspect every one\" (with a nod to Maria Gambrelli, of course) reply jarmitage 8 hours agoparentArticle footnote b. states:\"b. Parts of Kahneman&#x27;s book were undermined by psychology&#x27;s \"replication crisis,\" which affected some of its findings, but not the idea of system 1 and 2.\"Would have been a better footnote if additional references were provided for the latest in that area of discourse. reply quickthrower2 6 hours agoprev> Long-term memory is where information is permanently stored and is functionally limitless; in that sense, it functions somewhat like a computer&#x27;s disk storage.Doesn&#x27;t jibe with my experience. reply selimthegrim 8 hours agoprev500 error reply revskill 6 hours agoprevTo me, intelligence in general is ability of solving things in a generic and abstract way. reply tbwriting 5 hours agoprev [–] This is a fantastic piece. If you find yourself thinking “okay, so the brain is different — now what? What should I actually do to learn better?” I wrote about this last year. Pardon the clickbaity title, HackerNoon changed it up on me. https:&#x2F;&#x2F;hackernoon.com&#x2F;the-four-rs-how-to-become-a-good-prog... replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The article highlights the significance of continuous learning for software developers and presents research-backed insights on human memory and learning that are applicable to the field.",
      "It emphasizes the need to match instructional methods with the content being taught and offers practical implications for enhancing learning outcomes.",
      "The article addresses challenges in programming ability and provides recommendations for recruiting and training individuals, while stressing the importance of maintaining a growth mindset and goal orientation to overcome setbacks and failures. It also underscores the value of providing honest feedback and support to foster skill development and a growth mindset."
    ],
    "commentSummary": [
      "The article challenges the idea of \"learning styles\" and suggests that instruction should be based on the content being taught rather than individual learning preferences.",
      "The comments highlight issues with grading and standardized testing, emphasizing the need for personalized education and the role of expertise and intelligence in problem-solving.",
      "The discussion explores topics such as the limitations of artificial intelligence, the binary nature of knowledge, teaching methodologies, and the significance of both details and the bigger picture in learning. The replication crisis in psychology and the concept of long-term memory are briefly mentioned."
    ],
    "points": 260,
    "commentCount": 55,
    "retryCount": 0,
    "time": 1703637140
  },
  {
    "id": 38775904,
    "title": "The Game Boy Color: Improvements and Innovation",
    "originLink": "https://www.copetti.org/writings/consoles/game-boy/",
    "originBody": "Supporting imagery Model Motherboard Diagram Original Color The original Game Boy. Released on 21/04/1989 in Japan, 31/07/1989 in America and 28/09/1990 in Europe. A quick introduction The Game Boy can be imagined as a portable version of the NES with limited power, but you’ll see that it included very interesting new functionality. The rainbow analysis The immense popularity of this console resulted in a variety of revisions (i.e. Game Boy Pocket, Light, and even in the form of Super Nintendo cartridges). As a matter of fact, the Game Boy brand covers two generations. Within the 4th generation we find the monochrome Game Boy and its revisions, and in the next generation resides the Game Boy Color (shipped after the demise of the Virtual Boy). The good news is that this article covers both ends. So, in the end, you’ll get a good understanding of how the Game Boy works and how its technology subsequently evolved to become the Game Boy Color. CPU Instead of placing many off-the-shelf chips on the motherboard, Nintendo opted for a single-chip design to house (and hide) most of the components, including the CPU. This type of chip is called System On Chip (SoC) and, in this case, it’s been specifically crafted for this console, enabling Nintendo to tailor it to their needs (power efficiency, anti-piracy and extra I/O, among other things). At the same time, this chip can’t be found in any retail catalog, meaning competitors back then had a harder time cloning it. That being said, the SoC found on the Game Boy is referred to as DMG-CPU or Sharp LR35902 [1] and, as the name indicates, it’s manufactured by Sharp Corporation. This company, along with Ricoh (the NES’ CPU supplier), enjoyed a close relationship with Nintendo. The CPU core Within DMG-CPU, the main processor is a Sharp SM83 [2] and it’s a mix between the Z80 (the same CPU used on the Sega Master System) and the Intel 8080. It runs at ~4.19 MHz, which is faster than the average 1-MHz CPU, but remember clock speeds can be deceiving. The DMG-CPU, found on the Game Boy’s motherboard. Now, back when I did the analysis of the Master system, I explained the Z80 is itself a superset of the 8080. So, what does the SM83 actually have and lack from those two? [3] Neither the Z80’s IX or IY registers nor the 8080’s IN or OUT instructions are included. This means that I/O ports are not available. I’m not certain if that’s just a measure to reduce costs, but one thing for sure is that components will have to be completely memory-mapped [4]. Only Intel 8080’s set of registers are implemented. Consequently, there are only eight general-purpose registers, unlike the Z80 with its 16 registers (due to the addition of an ‘alternative’ set). Includes some of the Z80’s extended instruction set (only bit manipulation instructions). Furthermore, Sharp also added a few new instructions that are not present in either Z80 or 8080. They optimise certain operations related to the way Nintendo/Sharp arranged the hardware. One example is the new LDH instruction (meaning ‘load from high RAM’ [5]), which has been specifically included to operate the second half of the memory map (where addresses start at $ff) and, most importantly, fits in a single byte (instead of requiring two or more). The Color effect Original Marked Motherboard of the Game Color [6]. Almost 10 years later, after the abandonment of the Virtual Boy and its cutting-edge hardware, a humble successor arrived: the Game Boy Color. Inside it, there’s a new SoC named CPU CGB that carries a few additions. However, its SM83 CPU core remains the same, except for its doubled clock speed (now running at ~8.38 MHz). It’s hard to think that after a decade Nintendo would still bundle the same CPU, but such decision does come with the following advantages: Developers can re-use their current skills to program the new console. There’s a cost saving by not having to re-design their system to work for a new architecture. Backwards compatibility becomes possible without considerable effort. In fact, Nintendo implemented it by programming two modes of operation on CPU CGB: Normal mode: The SM83 operates at ~4.19 MHz. Dual-speed mode: The SM83 operates at ~8.38 MHz. Albeit, this comes at the cost of adopting outdated technology by late-90s standards. You only have to look at the state of the CPU market to notice the capabilities Nintendo was missing out (to be fair, Nintendo did try with the Virtual Boy). Hardware access The SM83 maintains an 8-bit data bus and a 16-bit address bus, so up to 64 KB of memory can be addressed. The memory map is mainly composed of the following endpoints [7]: Game Pak (the game cartridge) space. Work RAM (WRAM), High RAM (HRAM) and Display RAM (VRAM). I/O (joypad, audio, graphics and LCD). Interrupt controls. These will be explained throughout this article. Memory available Memory architecture of the DMG (original Game Boy). The PPU arbitrates access to VRAM. Nintendo fitted 8 KB of RAM on the motherboard, this is for general purpose use (which they call Work RAM or ‘WRAM’) [8]. Notice that this is four times larger than what the NES included. There’s an additional 127 B of RAM housed in the SoC. It goes by many names (‘H’, ‘High’, ‘Working’ and ‘Stack’ RAM) but the purpose is the same: provide a small space for data that must be accessed instantly, such as the stack. It’s not technically faster to access than general RAM, but it’s an area prioritised for the CPU. You’ll see what this means when you reach the ‘Graphics’ section, where I discuss the DMA component. Expanded memory architecture of the CGB (Game Boy Color). Again, the PPU arbitrates access to VRAM. Later on, with the Color variant, Nintendo enlarged WRAM to 32 KB. However, since the CPU remained unchanged (particularly its addressing capabilities), it’s not possible to connect all the new memory without first overflowing the available address space. To tackle this, Nintendo’s engineers implemented bank switching. Originally found in NES cartridges, the Game Boy Color uses the same principle to access those 32 KB only using 8 KB of memory space. The trick is simple: the last 4 KB can be swapped using eight different banks. Consequently, the CPU bundles an extra register (called SVBK) acting as the bank switcher, this is what developers must use to examine the extended memory. Graphics All graphics calculations are done by the CPU, and then the Picture Processing Unit or ‘PPU’ renders them. This is another component found inside DMG-CPU and it’s based on the predecessor’s graphics chip (of the same name). The picture is displayed on an integrated LCD screen, it has a resolution of 160×144 pixels and, in the case of the monochrome Game Boy, shows 4 shades of grey (white, light grey, dark grey and black). Since the original Game Boy has a green LCD, the picture will look greenish. If you’ve read the NES article before, you may remember that the PPU was designed to follow the CRT beam. However (and for obvious reasons), we got an LCD screen in the Game Boy. Well, the new PPU also follows that methodology as LCDs require to be refreshed too. In doing so, this console will enjoy CRT-based effects that previously enabled NES developers to come up with imaginative content. Organising the content Memory architecture of the PPU. The PPU is connected to 8 KB of VRAM or ‘Display RAM’. In doing so, it also provides arbitrated access to the CPU. Those 8 KB will contain most of the data the PPU will need to render graphics. The remaining materials will instead be stored inside the PPU, as they require faster access rates. The game is in charge of populating the different areas with the correct type of data. Moreover, the PPU exposes registers so the game can instruct the PPU on how that data is organised. Nevertheless, there are many rules to follow (you’ll see it in the following sections). Constructing the frame Let’s see how the PPU manages to draw stuff on the screen. For demonstration purposes, Super Mario Land 2 will be used as an example. Tiles Background Layer Window Sprites Result All Grid Single Multiple tiles. Tiles found in the Pattern Table. The PPU uses tiles as a basic ingredient for rendering graphics, specifically, sprites and backgrounds [9]. Tiles are just 8x8 bitmaps stored in VRAM in a region called Tile set or ‘Tile pattern table’, each pixel corresponds to one of the four shades of grey available. In practice, however, the shades of grey are selected through a ‘colour’ palette. The monochrome Game Boys contain registers that define these palettes. As explained before, there’re only four colours/shades of greys to choose from, so a single 8-bit register can fit a palette of four shades without issue. That being said, the system provides three registers (thus, three programmable palettes) with restricted use (more about this explained later). Furthermore, tiles are grouped into two pattern tables. To build the picture, tiles are referenced in another type of table called a Tile map. This information will tell the PPU where to render the tiles. Two maps are stored to construct different layers of the frame. The next sections explain how tile maps are used to construct the layers. Next » Secrets and Limitations The inclusion of the Window layer and extra interrupts allowed for new types of content and effects. Wobble effect The Legend of Zelda: Link’s Awakening (1993). Spoilers! Horizontal interrupts allowed to alter the frame before being finished. This means that a different scrolling value could be applied at each line, resulting in each row of the frame being shifted at different paces. This achieved an interesting Wobbling effect (I’m not sure that’s the official name of it). The Color additions The Game Boy Color’s PPU behaves as a superset of the original. You will see now what are the additions of the so-called ‘Color’ model of this brand. Modes of operation The Legend of Zelda: Link’s Awakening DX (1998). A hybrid Game Boy Color game running in CGB mode. Super Mario Land 2, as seen from a Game Boy Color. The latter adds a colourised palette and runs in DMG mode. To begin with, for compatibility reasons, the new PPU has two modes of operation. Yet, Nintendo wanted Color users to see enhancements even with the monochrome-only games. Consequently, the two modes of operation are as follows: CGB mode: Extended PPU mode with all the visual improvements that comprise the new Game Boy Color games. DMG mode: Traditional mode with all the extras disabled. However, you’ll see in the ‘Operating System’ section that monochrome games are nonetheless enhanced with colour palettes. Organising the (new) content The CGB motherboard now houses 16 KB of VRAM instead, which is double the original VRAM amount. Due to the CPU’s addressing limitations, this new arrangement is implemented in the form of two 8 KB banks, with a new register (called VBK) acting as a switcher. On the other side, the PPU can access the two banks at the same time. At the end of the day, this means programmers only need to fill the VRAM banks with the help of VBK, and then specify in the tile map the bank where the tile is found, so the PPU can take care of the rest. That being said, what can you do with the extra VRAM? Many things: Store more tiles. Store more palettes, especially coloured ones. Extend the tile metadata space to encode more effects and address extra palettes. The visuals Thanks to the new PPU, programmers can now define colour palettes with 32,768 colors to choose from. Firstly, developers must now populate a new area called Palette Memory, which stores up to eight colour palettes that encode four colours [10]. Each entry fits in a 16-bit value (2 bytes) and only 15 bits are used. Palette Memory is not addressed by the CPU, however, a new register is used as a buffer to write over this memory. This is what the CPU follows to define the palettes. Having said that, Background and Window tiles can reference any of those eight palettes. The same happens with Sprite tiles, except they are constrained to three-colour palettes, as one entry is reserved for the ‘transparent’ colour. The extra space Moving on, Tile sets are now twice as big. Thus, programmers can store double the amount of tiles in VRAM. Background/Window Tile maps have been extended as well, resulting in extra meta-data being encoded. Consequently, expanding the capabilities of these layers. For instance, their tiles can now be horizontally and vertically flipped, saving the game from storing duplicated graphics in VRAM (which, in turn, can be exploited to draw more unique content). Furthermore, CPU CGB also bundles two extra DMA units, both can copy the contents of the Game Pak or WRAM to VRAM, but their timings differ [11]: General-purpose DMA: The transfer will happen at any time and the DMA will get priority over any other memory access. So, programmers need to be careful about when to use this component (i.e. during or outside scanning) and how (the amount of data to copy), as misuse can lead to screen tearing (VRAM access will be blocked during the transfer). Horizontal Blank (H-Blank) DMA: The transfer will only happen during H-Blank periods. This avoids screen artifacts, but can only transfer content in batches of 16 Bytes, and pauses during LCD scanning. Once more, these units offer programmers new possibilities to provide richer content, as they take advantage of periods originally left for idling. Audio The audio system is carried out by the Audio Processing Unit (APU), a PSG chip with four channels [12]. Curiously enough, this is one of the few sections that hasn’t evolved on either model. In fact, it can’t even be sped up, since if you change the speed of the oscillators, you won’t hear ‘better’ sounds, but a higher pitch. Functionality Each channel of the four channels is reserved for a single type of waveform: Pulse Noise Wave Pulse 1 Pulse 2 Complete Oscilloscope view of the pulse 1 channel. Pokemon Red/Blue (1996). Pulse waves have a very distinct beep sound that is mainly used for melody or sound effects. The APU reserves two channels for one pulse wave each. These use one of four different tones constructed by varying their pulse width. The first channel has an exclusive Sweep control available. Due to the limited number of channels, the melody will often be interrupted when effects have to be played as part of the gameplay. This is very noticeable in games like Pokemon Red/Blue when, during a battle, the Pokemon’s cry will overlap all the channels used for music. Next » Secrets and Limitations The mixer outputs stereo sound, so the channels can be assigned to the left side or on the right one, this is only possible to hear from the headphones though! The speaker is mono. Furthermore, the mixer chip is also connected to a dedicated pin on the cartridge, allowing it to stream an extra channel with the condition that the cartridge outputs the analogue sound (only possible with extra hardware). Nevertheless, no game in the market ended up using this feature. Operating System Unlike the NES, which booted straight into the game, the Game Boy was designed to always boot from an internal 256 Byte ROM, and only then jump into the game code. The boot process is as follows [13]: After the console is switched on, the CPU starts reading at address 0x00 (Game Boy’s ROM location). RAM and APU are initialised. The Nintendo logo is copied from the cartridge ROM to Display RAM, and then it’s drawn at the top edge of the screen. If there is no cartridge inserted, the logo will contain garbage tiles. The same may happen if it’s improperly inserted. The logo is scrolled down and the famous po-ling sound is played. The game’s Nintendo logo is matched against the one stored in the console’s ROM. Afterwards, a quick checksum is done on the cartridge’s ROM header to make sure the cartridge is correctly inserted. If any of these checks fail, the console freezes. The console’s ROM is removed from the memory map. CPU starts executing the game. Interestingly enough, the Nintendo logo displayed on the screen is not cleared from VRAM, so games can apply some animation and effects to introduce their own logo. The Game Boy’s boot screen along with 20y, a homebrew demo that fiddles with the logo. The revised sequence The Game Boy Color’s boot screen, the logo doesn’t slide anymore but it features a rainbow effect. Once again, in the case of the Game Boy Color, we find drastic changes in the contents of the ROM. For instance, the size of the ROM is now 2 KB [14] and the routines exhibit new behaviour: The boot sequence will now check if it’s got a Game Boy-only or Game Boy Color game inserted, and then set the corresponding registers to activate DMG or CGB mode. The boot code checks for specific game metadata (within the cartridge’s ROM) that is structured differently for CGB games. In the case a DMG game is inserted, the boot program will also fill Palette RAM with calculated palettes, these are based on a simple but clever algorithm that also relies on the game’s metadata. This is what makes monochrome games look coloured when running on the next-gen console. At this stage, the user may press a button combination that will alter the boot code’s palette of choice. The Nintendo logo check now makes use of HRAM as well. Games Games are written in assembly and they have a maximum size of 32 KB, this is due to the limited address space available. However, with the use of a Memory Bank Controller (mapper), games can reach bigger sizes. The biggest Game Pak (cartridge) found in the market has a 1 MB ROM (or 8 MB, in the case of the Game Boy Color). Game Paks can include a real-time clock and an external battery along with SRAM to hold saves, although all of these are optional. Types of cartridges Due to the two modes of operation the Game Boy Color supported, Nintendo listed three different types of Game Boy games: Game Boy: A type fully compatible with all Game Boy models. It always runs in DMG mode. Game Boy Color enhanced: A hybrid that runs differently depending on the host console. While fully compatible with the monochrome model, it will be visually enhanced (due to running in CGB mode) when running on the Game Boy Color. However, the overall functionality will still be constrained to maintain compatibility with the Game Boy. Game Boy Color exclusive: Only compatible with the Game Boy Color, but it takes full advantage of its hardware. External communications Game Boy Link cable for multiple variants of the console [15]. Since the release of the original console, subsequent revisions miniaturised the socket while keeping the same number of pins. For the first time, games can communicate with other Game Boys using a Game Boy Link cable, which provides multiplayer functionality, for instance. The interface relies on a very primitive type of serial connection. The original Game Boy transfers information at 8 Kbit/second (1 KB/sec), while the Color variant may reach up to 512 Kbit/second (64 KB/sec), the latter speed was known as ‘high speed’ mode. The latter variant is also bundled with an Infrared emitter and receiver (made of an LED and a phototransistor, respectively) [16]. This made it possible to exchange data wirelessly between Game Boy Colors, as seen in games like Pokemon Gold and similar. However, you will find that the system doesn’t implement any communication protocol, only a single register (RP) that encodes the IR sensor’s action (emit or receive), the single bit (0 or 1) being transferred and the last bit received. Nevertheless, to alleviate things for developers, Nintendo did include a reference implementation in the official Game Boy Developer manual [17]. Anti-piracy As you’ve seen in the ‘Operating System’ section, the console will never run a game right away, it first executes a series of checks that prevent the execution of unauthorised cartridges and also makes sure the cartridge is correctly inserted. To be able to pass these checks, games had to include a copy of Nintendo’s logo (in the form of tiles) in its ROM header [18], this way Nintendo could make use of Copyright and Trademark laws to control the distribution, Clever huh?. That being said, more anti-piracy measures can be implemented inside games, like checking the SRAM size (it’s normally bigger in Bootlegs) and checksumming the ROM at random points of the game. That’s all folks",
    "commentLink": "https://news.ycombinator.com/item?id=38775904",
    "commentBody": "Game Boy &#x2F; Color ArchitectureHacker NewspastloginGame Boy &#x2F; Color Architecture (copetti.org) 235 points by ronama 13 hours ago| hidepastfavorite28 comments leshokunin 9 hours agoWhat&#x27;s absolutely brilliant about the development of the original Game Boy is that at the time, Gunpei Yokoi&#x27;s team was facing significant incredulity. \"Why play games when you&#x27;re on the bus, or on the toilet? It&#x27;s going to be so uncomfortable. Play at home on you couch in front of the TV with family and friends. Play at the arcade for cutting edge experience. Nobody wants to change batteries and see just a few shades of grey\".Their vision was to take \"withered technology\" and package it in a simple to use device, with simple, short games.That team single handedly started mobile gaming. Much respect to everyone on it. reply TedDoesntTalk 4 hours agoparent> Gunpei Yokoi&#x27;s team was facing significant incredulity. Why play games when you&#x27;re on the bus, or on the toilet?> That team single handedly started mobile gaming.No. This is revisionist history.Us kids in the 70s already had portable LED games by Mattel that we played on the bus and toilet. I had at least 6 of these, maybe 8. Here’s the earliest one, from 1976:https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Mattel_Auto_RaceThe more popular ones were sports like football, baseball, basketball, etc and they had much more complex rule sets than Auto Race:https:&#x2F;&#x2F;www.ebay.com&#x2F;p&#x2F;2255363696Then in the early 80s we had a slew of portable LCD games like Dungeons and Dragons:https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Dungeons_%26_Dragons_Compute... (1981)“Mattel stated that the game immediately sold out.”None used cartridges but they were so cheap and small (esp. the LCD games), it didn’t matter. You could own a bunch without breaking the bank.Such games are now called “handhelds” to distinguish them from the GameBoy generation of portables, but we did not call them handhelds back then as I recall.My point is a successful market already existed in the 70s and early 80s. Nintendo didn’t do that. They evolved what already existed by shrinking cartridge-based consoles into a portable form factor. Both were already successful, established markets before. reply louthy 9 hours agoparentprevGame & Watch was first, still Gunpei Yokoi, but yeah I’d consider that the start of mobile gaming - at least that’s my memory! reply chrisco255 8 hours agorootparentYeah Game & Watches predate Game Boy by quite a bit. And they were quite successful. reply MBCook 5 hours agorootparentThere’s a reason it was the DMG. Dot Matrix Game(?).It was the first really successful portable system with interchangeable games and a dot matrix screen as opposed to custom LCDs per game like G&W.The Milton Bradley MicroVision came first around 1980. Nintendo referenced it as a precursor. But the screen was 16x16 pixels so ‘graphics’ is kind of misnomer.I think I remember seeing something once where the screen was included with the game, because it was a static LCD sort of like a Game and Watch game.The GB was in the right place, at the right time, from the right company, with the right hardware and software, at the right price point. Instant success. reply leshokunin 5 hours agorootparentprevSame inventor indeed. However the games were mostly just one per device, and basically all used simple \"sprites\" that toggled on&#x2F;off.I think the Game & Watch gameplay feels a lot closer to an electronic toy from the early 80s than what people would consider console gaming.Not to detract from its merit. It&#x27;s just that the Game Boy wasich closer to a NES-like experience. reply MBCook 5 hours agorootparentFor anyone old enough for this to make sense but who hasn’t seen a Game and Watch game: they worked just like the cheap Tiger Electronics LCD games you could get.Just like a simple digital watch, everything that could be on the screen was designed upfront and then the individual bits could just be shown or hidden.Nintendo eventually made three collections of the Game and Watch games for the GB. Each collection was a ton of fun with each game available in classic (faithful) or updated (graphics replaced with sprites, same gameplay) modes.https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Game_&_Watch reply huehehue 3 hours agorootparentThere&#x27;s a fighting game (I think a port of Street Fighter II) in this style and it&#x27;s just wild how they make the static screen elements work with that type of game. reply LanzVonL 3 hours agoparentprevI had a watch with a car racing game like 6 years before Game Boy was a thing. reply Dwedit 11 hours agoprevWhoops, I see Game Boy Color was added in there, but he forgot to revise the part about the cartridge size being only 1MB. Some GBC games reach 4MB in size.edit: There&#x27;s also an 8MB game about Trains, with some short videos and hi-color images in there. reply qingcharles 11 hours agoparentYes, 電車でGO!2 高速編 aka Densha de Go! 2 Kōsoku-hen.https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=S62dSVmLPU0 reply ladyanita22 10 hours agorootparentThis game is technically impressive consdiering it was running in a Z80 basically reply monocasa 11 hours agoparentprevAnd to be fair, I don&#x27;t think there was anything stopping a DMG game from reaching 4MB. The pak has the same aperture size of basically 32k either way, and the bank switching for either 4MB or 1MB is on the cart itself. reply MBCook 9 hours agorootparentRight. I don’t think Nintendo had a mapper to do it at the time. I doubt third parties did.But if you wanted to let a DMG address 2 gigs of memory 32k at a time you could do it.It’s a little hard to imagine a game needing > 1MB if it’s just B&W. But I suppose you could put a ton of text in or do some kind of demo-scene style ‘video’ playback. But no one would have for cost reasons back then. reply duskwuff 9 hours agorootparent> But no one would have for cost reasons back then.Not on the GBC, but GBA had a number of video cartridges, including several full-length movies:https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Game_Boy_Advance_Video reply MBCook 8 hours agorootparentI never used one, but I remember being really surprised the first time I saw one of those in a store. reply itsmartapuntocm 7 hours agorootparentI had a few. The quality was about what you’d expect for a video on a game boy cartridge. reply Dylan16807 3 hours agorootparentprevA single fullscreen image for game boy is almost 6KB. If you were making something pretty you could have several images per level and go over a megabyte without much effort. reply tadfisher 8 hours agorootparentprevI know of the GBC port of Cannon Fodder, which was 32Mb (4MiB) and had an FMV intro. https:&#x2F;&#x2F;youtu.be&#x2F;DkpQrnKcgp4 reply flipacholas 11 hours agoparentprevWhoops indeed. I’ve just added the new info, thanks! reply qingcharles 11 hours agorootparenthttps:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38777017 reply Izmaki 10 hours agoprevThank you for using the Pokémon audio as demonstration. Was a nice trip down memory lane... :) reply Mtinie 7 hours agoprev [–] As an aside, are we at a point in democratized hardware where I could buy the right components and a traced PCB to recreate a Gameboy? reply lancefisher 6 hours agoparentSome people have done this, but you need to transplant at least the CPU from an original. Those chips weren’t built for anyone else. https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;Gameboy&#x2F;s&#x2F;FEAH2VhBRTFunnyPlaying recently came out with an FPGA-based version for an affordable price. https:&#x2F;&#x2F;funnyplaying.com&#x2F;products&#x2F;fpgbc-kit?variant=40858870...They also have a new, populated motherboard for the Game Boy Advance, into which you have to transplant the CPU and RAM. reply bbbbbr 4 hours agorootparentThere is a variation on the new-pcb and components approach called the \"Ultra Boy Colour\" which doesn&#x27;t require any parts from an OEM Game Boy Color.It can use a clone CPU that was manufactured (until recently?) for the \"GBBoy Colour\" (and could also be pulled from the sort of failed SGB-like clone called the \"Extension Converter for GB\"). > I redesigned the Gameboy Color from the ground up > based on the premise that it wouldn&#x27;t require any > components to be harvested from a real GBC, so it > would be compatible with both the original GBC and > clone hardware taken from a GB Boy Colour- since > the KF2007 clone CPU from the GBBC is practically > a drop-in replacement for the CGB CPU found in a > real GBC- no software compatibility issues at all.https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;Gameboy&#x2F;comments&#x2F;w0wrt5&#x2F;no_this_isn... reply fathyb 7 hours agoparentprevIf you don&#x27;t mind your GameBoy being as fast as a supercomputer when it came out, then I think you could get an ARM SBC running a GameBoy emulator on Linux rendering into an OLED screen for less than $150 and a bit of your sanity.If you&#x27;re looking for hardware fidelity, I don&#x27;t think it&#x27;d be in the \"democratised\" price range if you want to build a single unit that fits in a handheld form factor. reply TechSquidTV 6 hours agoparentprevThe approach now would be to use an FPGA. See Analogue Pocket, but other kits exist. reply Waterluvian 7 hours agoparentprev [–] I think you’d at best be buying parts taken from old Game Boys and putting one back together. I think I’ve seen an FPGA Game Boy implementation somewhere, which may be the closest you’d get. replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The Game Boy, a portable version of the NES, was released in 1989 and gained popularity.",
      "It had revisions and two generations, including the monochrome Game Boy and the Game Boy Color.",
      "The Game Boy Color featured a doubled clock speed, backwards compatibility, and added features like bank switching and extended memory.",
      "The system had limited graphics capabilities but included palettes and tiles for visual effects.",
      "It had an audio system with four channels for different waveforms.",
      "The boot process involved initializing RAM, copying logos, and checking cartridges.",
      "The Game Boy Color had different modes for running games and could communicate with other consoles.",
      "Anti-piracy measures were implemented to control distribution."
    ],
    "commentSummary": [
      "The article explores the creation of the Game Boy and Game Boy Color, which faced initial skepticism for the concept of portable gaming.",
      "The team's vision for a portable gaming device revolutionized the industry and popularized mobile gaming.",
      "The article also provides insights into the history of portable gaming devices and highlights the capabilities of the Game Boy."
    ],
    "points": 235,
    "commentCount": 28,
    "retryCount": 0,
    "time": 1703623750
  },
  {
    "id": 38772754,
    "title": "The Piece Table: The Most Elegant and Efficient Data Structure for a Text Editor",
    "originLink": "https://www.averylaird.com/programming/the%20text%20editor/2017/09/30/the-piece-table.html",
    "originBody": "Text Editor: Data Structures Sep 30, 2017 The first step in building my text editor is to implement the core API. If you’re wondering why I want to do this, the original article is here. I researched several data types, and I tried to be language agnostic. I wanted my decision to not be influenced by any particular language, and first see if there was a “best way” out there, solely based on operations. Of course, a “best way” rarely exists. However, in the case of text manipulation and storage, there are some clear “worst ways” and “better ways.” The Worst Way The worst way to store and manipulate text is to use an array. Firstly, the entire file must be loaded into the array first, which raises issues with time and memory. Even worse still, every insertion and deletion requires each element in the array to be moved. There are more downsides, but already this method is clearly not practical. The array can be dismissed as an option rather quickly. By the way: this isn’t a challenge. Please don’t try to find worse ways to manipulate text. A Good Way Another option is a binary tree structure called a rope. Skip to the next section if binary trees aren’t your thing. If you are unfamiliar with binary trees, check out this as a starting point. Basically, the string is split into sections, and stored in the leaves. The weight of each leaf is the length of the string segment. The weight of each non-leaf node is the total length of the strings on its left subtree. For example, in the diagram1 below, node 𝐸 E is a leaf with a string segment 6 6 characters long. Therefore, it has a weight of 6 6, as well as its parent node. However, node 𝐵 B has a weight of 9 9, because nodes 𝐸 E and 𝐹 F together have a length of 9 9. This is a lot more efficient than an array. A rope has two main operations, Split and Concat (concatenation). Split splits one string into two strings at a given index, and Concat concatenates two strings into one. You can preform either an insert or delete with either of these two basic operations. To insert characters, you can split the string once (where you want to insert the content) and concatenate it twice (on either side of the inserted content). Deletions work similarly, by splitting the string twice, and concatenating them again without including the deleted content. There’s a big downside. Using a rope is quite confusing and complicated. It’s difficult to explain even in an abstract manner. Working out the kinks in real life, while still making the code maintainable and readable, seems like a nightmare. What’s more, it still uses a lot of space. It didn’t seem like the best option yet, so I kept looking. A Better Way The Gap Buffer is much simpler than the rope. The idea is this: operations on text are often localized. Most of the time, we’re not jumping all over the document. So, we create a “gap” between characters stored in an array. We keep track of how large the gap is by using pointers or array indices. Let’s examine two cases (using pointers): Insertion The gap is a certain size to begin with. We copy over the inserted content, and if it exceeds the size of the gap, we expand the gap. Deletion We shift the pointers of the gap to include the deleted content. This is makes a lot of sense. We are plagued somewhat by the same issues as an array; under certain circumstances, if we move too far from the gap, every element in the array will have to be moved. However, it’s most likely that this is a rare occurrence for the average user. It is quite possible that the speed gained with most operations will outweigh the inefficiency of certain edge cases. In fact, the editor I’m writing this in – Emacs – uses a gap buffer, and it’s probably the fastest editor I’ve ever used. That fact alone is a pretty convincing argument to use a gap buffer. But if I’m starting from scratch, I want every aspect of the software to be the best option there is. And maybe there’s a better(est) way. The Better(est) Way A couple months ago, my Dad asked me for help with a problem. He was converting one of his books to markdown, and there was an issue with the footnotes. In markdown, footnotes do not automatically number themselves2; they need to be labelled with either a number or some text, like this: [^1] or [^footnote]. The definition is the same, with a colon at the end. He had used pandoc to mostly convert the document, but every footnote had the format [^#]. It was my job to make a script to replace every # with a number, starting from 1 1. Easy, right? Well, that’s what I thought. I whipped up a regex, scanned through the document, and replaced all occurrences of the pattern with an increasing integer. And, it spat out garbage. Why? Because I had made a really, really obvious mistake. The counter doesn’t always take up the same amount of space. The script kept overwriting content, and the offset grew larger the more footnotes were replaced. There’s a simple fix: keep track of how much more space you take up, and add that to your current position in the document. I made that one simple change, and everything worked perfectly. Without knowing it at the time, I had used a Piece Table. The Wikipedia page for the Piece Table is only 8 lines long (Yikes!) Even more concerning, it mentions Microsoft Word among the examples of editors that use piece tables. However, the piece table is a very promising structure. What’s more, at its conception Word was lightning fast with infinite redo/undo, as explained in this interesting article by a Microsoft developer. If you have the time, it’s a cool read. In 1998, Charles Crowley wrote a paper investigating the pros and cons of various data structures used in text editors. His paper includes the structures we covered, like gap buffers, arrays, and ropes. He concluded that – from a basis of speed, simplicity, and structure – the piece table was the leading method. From my point of view, the piece table is also the most elegant solution. We need two buffers: the original file (read-only), and a new file that will contain all of our added text (append-only). Lastly, we have a table that has three columns: file, start, length. This is which file to use (original or new), where the text segment starts in each file (pre-edit), and the length of the segment. Here’s an example: Original File: A_large_span_of_text (underscores denote spaces) New File: English_ File Start Length ----------------------------------- Original 0 2 Original 8 8 New 0 8 Original 16 4 Sequence: A_span_of_English_text Keep in mind that the Start index is not relative to previous edits. This is something that gets handled at runtime by adding the length of each previous edit (like what I did to fix my footnote script). Since the length is already included in the table, this is a trivial step. I like this solution the most because: It’s elegant. Only the minimal amount of information is recorded. It’s simple. We run through the table, and do the necessary operations. We don’t need to re-balance or traverse a binary tree. It’s fast. This solution has the potential to be lightning fast. There are some additional optimizations to take into account, which are explained in detail here The piece table method certainly has its complications, and there are different variations in implementation. It is certainly a daunting task. I’m going to see how far I get. Another article will accompany my attempts to implement the piece table method. Notes P.S. If you’d like to read this article in Russian, Vlad Brown has a translated copy on his website. Thanks Vlad! From Wikipedia ↩ They should! Why aren’t they!?!?! Somebody needs to make that a markdown extension. Every time you want to insert an indexed footnote, you type [^#]. Then, it takes every footnote definition with that format and matches them up. If there’s a mismatch (like a named reference), you wonder why some of your footnotes are missing and fix it. I had to change all of my footnotes just to insert this footnote. It’s crazy. ↩",
    "commentLink": "https://news.ycombinator.com/item?id=38772754",
    "commentBody": "Text Editor: Data StructuresHacker NewspastloginText Editor: Data Structures (averylaird.com) 228 points by ibobev 18 hours ago| hidepastfavorite69 comments Nilithus 14 hours agoVSCode have a blog article talking out their move to using Piece Table as their main data structure. https:&#x2F;&#x2F;code.visualstudio.com&#x2F;blogs&#x2F;2018&#x2F;03&#x2F;23&#x2F;text-buffer-r...Has some good visualizations as well. reply p-e-w 6 hours agoprev> The worst way to store and manipulate text is to use an array.Claim made from theoretical considerations, without any actual reference to real-world editors. The popular Micro[1] text editor uses a simple line array[2], and performs fantastically well on real-world editing tasks.Meanwhile, ropes are so complicated that even high-quality implementations have extremely subtle bugs[3] that can lead to state or content corruption.Which data structure is \"best\" is not just a function of its asymptotic performance. Practical considerations are equally important (arguably more so).[1] https:&#x2F;&#x2F;github.com&#x2F;zyedidia&#x2F;micro[2] https:&#x2F;&#x2F;github.com&#x2F;zyedidia&#x2F;micro&#x2F;blob&#x2F;master&#x2F;internal&#x2F;buffe...[3] https:&#x2F;&#x2F;github.com&#x2F;cessen&#x2F;ropey&#x2F;pull&#x2F;67 reply kragen 5 hours agoparentit&#x27;s talking about an array of bytes, and you&#x27;re talking about an array of lines, which performs acceptably in a much wider range of situations than what&#x27;s being dismissedropes can be complicated but they don&#x27;t have to be. ropey is ten thousand lines of code, and only six years old, so it&#x27;s not surprising that it&#x27;s buggy. https:&#x2F;&#x2F;github.com&#x2F;ivanbgd&#x2F;Rope-Data-Structure-C is 339 lines of code, but it&#x27;s probably not a real-world ropeprobably the most widely used implementation of ropes is the one from the sgi stl, which is in gcc as libstdc++&#x2F;ext&#x2F;rope https:&#x2F;&#x2F;gcc.gnu.org&#x2F;onlinedocs&#x2F;libstdc++&#x2F;libstdc++-html-USER... and libstdc++&#x2F;ext&#x2F;ropeimpl.h https:&#x2F;&#x2F;gcc.gnu.org&#x2F;onlinedocs&#x2F;libstdc++&#x2F;libstdc++-html-USER..., which total 1485 lines of code, about halfway in between the two. in a garbage-collected language, like the cedar language where ropes originated, most of that complexity goes away, and production-quality ropes are only a few hundred lines of code. also c++&#x27;s verbose syntax isn&#x27;t doing brevity any favors herehttp:&#x2F;&#x2F;www.bitsavers.org&#x2F;pdf&#x2F;xerox&#x2F;parc&#x2F;cedar&#x2F;Cedar_7.0&#x2F;09_C... reply userbinator 9 hours agoprevThe worst way to store and manipulate text is to use an array. Firstly, the entire file must be loaded into the array first, which raises issues with time and memory. Even worse still, every insertion and deletion requires each element in the array to be moved. There are more downsides, but already this method is clearly not practical. The array can be dismissed as an option rather quickly.Someone has clearly not heard of Notepad nor used the prolific quantity of text editors for DOS that appeared in the 80s&#x2F;early 90s, a time when memory bandwidths were in the low MB&#x2F;s; or even before that, micros with CP&#x2F;M where memory bandwidths were measured in KB&#x2F;s (and the whole address space was only 64k). Modern systems have memory bandwidths of dozens of GB&#x2F;s.Or perhaps that old saying really applies: worse is better.IMHO an array is all you need for a regular general-purpose text editor. I hate needless complexity in software, but unfortunately others seem to love it. reply chii 7 hours agoparent> an array is all you need for a regular general-purpose text editor.until you try to open up a multi gigabyte log file in notepad, and find that it&#x27;s consuming all your system resources.But opening that same file in sublime text works perfectly, scrolls fast and zero lag. Guess which one is my daily driver? reply anon-3988 2 hours agorootparentReading log files is completely different than constantly manipulating source code? I wish editors have read-only mode that uses data structure that lazily loaded continuous memory. reply kragen 6 hours agoparentprevif you use an array in the naïve way on an 8080 with, as you correctly point out, memory bandwidths on the order of 16 kilobytes per second, as soon as your text file has grown to 48 kilobytes (about 17 printed pages and less than half a floppy disk), inserting a letter near the beginning of it will take multiple seconds; you can type several times as fast as the editor can respond, even with hunt and peckconsequently the editors we used on those computers did not use this approachhowever, a gap buffer is perfectly workable, and preserves most of the array&#x27;s advantages, such as rapid text search. and so emacs continues to use gap buffers to this day reply icsa 6 hours agoparentprevYep.Q: How big is the 90th percentile text file?64KB? That text file fits in some some level of cache and not RAM. An array is just fine most of the time.Use a different purpose-built editor for those multi-GB text files. reply avindroth 9 hours agoparentprevAgreed. Our computers are so powerful, do we really need this additional complexity?And of course, if performance becomes an issue, we can make minor improvements then. reply vlovich123 8 hours agorootparentThat’s why all these programs land these improvements - the performance optimizations were needed. It gets tricky of course because a performance optimization you needed at one time may no longer be needed as HW perf increased or may even be a penalty as HW architecture diverged from the target one that was optimized for. reply afc 14 hours agoprevIn my text editor (https:&#x2F;&#x2F;github.com&#x2F;alefore&#x2F;edge) I use a balanced binary tree containing small chunks (std::vector) of contiguous lines.That works well enough for me: https:&#x2F;&#x2F;asciinema.org&#x2F;a&#x2F;314752This requires loading the entire file into memory, but computers are so fast today that optimizing that away is pointless IMO (as the video shows, a 12MB file with 400k lines can be loaded&#x2F;edited&#x2F;saved reasonably).The tree structure is implemented as a generic container here: https:&#x2F;&#x2F;github.com&#x2F;alefore&#x2F;edge&#x2F;blob&#x2F;master&#x2F;src&#x2F;language&#x2F;con.... The tree is actually deeply immutable; each modification operation returns a new tree (where most contents are shared with the original tree). The leafs contain std::vectors that hold between 128 and 256 lines (save for a few cases). The actual specialization that holds a sequence of lines is LineSequence::Lines here: https:&#x2F;&#x2F;github.com&#x2F;alefore&#x2F;edge&#x2F;blob&#x2F;master&#x2F;src&#x2F;language&#x2F;tex... reply layer8 11 hours agoparent12 MB isn’t necessarily that much. Here is a 96 MB SVG file: https:&#x2F;&#x2F;commons.m.wikimedia.org&#x2F;wiki&#x2F;File:Koppen-Geiger_Map_...Here are genome sequence files that are several hundred MB compressed (the decompressed format is text): https:&#x2F;&#x2F;ftp.ncbi.nlm.nih.gov&#x2F;genomes&#x2F;refseq&#x2F;Finally here is an almost 2 TB XML file: https:&#x2F;&#x2F;wiki.openstreetmap.org&#x2F;wiki&#x2F;Planet.osmThese aren’t the most typical use cases, but it’s nice if an editor can at least handle > 32-bit files (larger than 2 or 4 GB). reply vidarh 14 hours agoparentprevI just use a (Ruby) array of strings for mine, on the basis that I have never on purpose wanted to actually edit a file large enough that this is a problem. Computers keeps getting faster, but files I want to edit really don&#x27;t.If I open a huge one it&#x27;s usually either an accident or because I want to view it, not edit it. It&#x27;s easy enough to support abnormally large files, but it felt to me like a distraction that adds complexity for the sake of a fringe case I was perfectly happy to just define as out of scope.Incidentally, the server process for my editor holds every buffer that I&#x27;ve ever opened (over several years) and not explicitly killed in memory, and it&#x27;s consuming too little for me to bother adding any garbage collection for buffers or similar (it&#x27;s synced to disk, so on reboot I have the full same set of buffers available). We&#x27;re talking many hundred buffers. RAM is cheap and plentiful. reply cdcarter 13 hours agorootparentYou&#x27;ve worked primarily in your own home built editor for years? That&#x27;s pretty cool. reply vidarh 1 hour agorootparentYeah. 6 years or so now. Putting the buffers in a server that checkpoints regularly was key to making it easy to do early - made it really hard to lose any data even when it crashed regularly. reply afc 12 hours agorootparentprevI&#x27;m not the person you directly replied to, but I&#x27;ve worked on my text editor since 2014 and used it exclusively since ~2015. It&#x27;s been a lot of fun! reply atombender 12 hours agoparentprevHow does that perform when the entire 12MB file is a single line?Before you say it&#x27;s unrealistic; sure, it&#x27;s not a normal use case for code, but I frequently find myself wanting to open a huge JSON file, for example, just to perform a search&#x2F;replace, or a minor edit, or copy some fragment, or just look at its contents. Pretty-printing first, or doing the change with a specialized tool like jq, is also something I do, but being able to just load the file into a text editor is often more convenient.In my experience, editors tend to break down on edge cases like these. Editors (e.g. Jetbrains) often turn off syntax highlighting and&#x2F;or edit capabilities on large files, not to mention put an upper limit on the number of cursors. reply afc 12 hours agorootparentThank you for your response. You ask a good question.I expect very long lines should work just fine, but I&#x27;ll give it a try when I have a chance.I represent lines using a virtual class that has a few implementations. IIRC, the line will start as a tree containing 64kb chunks. As edits are applied, these chunks will gradually get replaced with other implementations (of the virtual class) that delegate to the original instances (and the representation is occasionally optimized (which can incur copying) to avoid too much nesting). So simplifying, I think this should just work due to the optimized representation that I use for lines, which doesn&#x27;t require the characters to be contiguous in memory.I fully agree with your observation about how things often break in the corner cases. I actually also put a configurable cap on the number of cursors (e.g., a reg exp search stops after 100 matches).For syntax highlighting I don&#x27;t currently have a cap, but this is ~fine: it just ~wastes a background thread (all syntax parsing is offloaded to a background thread, not blocking the main thread). But yeah, I should probably make this thread give up after a configurable time out. reply JoeyDoey 10 hours agorootparentprevLooks like Codemirror does similar with long&#x2F; huge files https:&#x2F;&#x2F;codemirror.net&#x2F;examples&#x2F;million&#x2F; reply Findecanor 3 hours agoparentprevYou would have to load the entire file either way, even if you&#x27;d mmap() it, to be able to break the lines to find their extents and start offsets.mmap() isn&#x27;t entirely safe under Unix&#x2F;Linux anyway because there is no Mandatory Locking. And it would still be useful only if you operate on the original text encoding. reply SpaghettiCthulu 7 hours agoparentprevIt&#x27;s all fun and games until you&#x27;re loading a multi gigabyte CSV file reply dkjaudyeqooe 7 hours agoprevThere&#x27;s lots of comment here about how this isn&#x27;t needed for \"typical\" editing of smallish files and you can get away with something very simple, but that isn&#x27;t very interesting.What is interesting is the opposite question: how do you enable editing that is bounded only by storage limits?A piece table is a good solution for this since the original file and the added data is immutable, which lends itself to mmap-ing. Then you can implement the piece table and various indexes (line breaks, etc) in a database (such as SQLite) or similar. Overall this gives you consistent performance at any size or number of edits limited only by storage. reply kragen 4 hours agoparentthe original file isn&#x27;t immutable unless you have reflinks or equivalent reply xjewer 8 hours agoprevFrom a linked article https:&#x2F;&#x2F;web.archive.org&#x2F;web&#x2F;20160308183811&#x2F;http:&#x2F;&#x2F;1017.songt...:>It was also possible that characters that were still stored in the document file, had been deleted since those characters were first copied into the file. That meant if a user of the program had typed embarassing or incriminating text in a Word document, saved it and then thought better of publishing such thoughts, and erased them, that once saved text still was recorded in a Word file. If you used an operating system utility to dump the raw content of a fast saved Word file, this stuff became visible to the technically minded nosy person.This fast-save feature is hilarious and reminds me of https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;ACropalypse. Wondering how many awkward stories out there caused by this ”optimization”. reply tedunangst 9 hours agoprevMore reading: https:&#x2F;&#x2F;www.chiark.greenend.org.uk&#x2F;~sgtatham&#x2F;tweak&#x2F;btree.htm... reply behnamoh 12 hours agoprevBut what is a good structure for modern text editors in 2024? Nowadays text editors need to keep track of git commits (by whom, when, which line(s)), reference and footnotes (the author mentions Markdown&#x27;s manual [^...] notation but ideally you&#x27;d want something automatic—even latex isn&#x27;t automatic by default), etc. reply kragen 6 hours agoparentthe best writing i&#x27;ve found on that topic is https:&#x2F;&#x2F;xi-editor.io&#x2F;docs&#x2F;rope_science_00.html reply jph 15 hours agoprevRopes have all kinds of bonus benefits. The Xi editor author has some good notes about text editor rope coding: https:&#x2F;&#x2F;abishov.com&#x2F;xi-editor&#x2F;docs&#x2F;rope_science_01.html reply chucksmash 12 hours agoparentProject site linked from the GitHub[0] is https:&#x2F;&#x2F;xi-editor.io. Linked doc is a mirror of this[1], which was originally written by Raph Linus.[0]: https:&#x2F;&#x2F;github.com&#x2F;xi-editor&#x2F;xi-editor[1]: https:&#x2F;&#x2F;xi-editor.io&#x2F;docs&#x2F;rope_science_01.html reply mcarmichael 12 hours agoprevA nice companion piece, focused on a related probem:\"An Efficient Data Structure For A Hex Editor\" https:&#x2F;&#x2F;www.chiark.greenend.org.uk&#x2F;~sgtatham&#x2F;tweak&#x2F;btree.htm... reply xwowsersx 10 hours agoprevI&#x27;ve been really curious about the algorithms and data structures behind text editors. Does anyone know of a book or YouTube series that does a job slowly building a working text editor? reply ksherlock 10 hours agoparentI don&#x27;t know if it answers your question per se but let me mention: The Craft of Text Editinghttps:&#x2F;&#x2F;www.finseth.com&#x2F;craft&#x2F;https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=13518170 reply xwowsersx 10 hours agorootparentThat looks great indeed, thank you. I assume the concepts are generalizable even though it uses Emacs to concretize them? I ask since I&#x27;m a Vim guy (not trying to start a flame war haha) reply ksherlock 9 hours agorootparentSure. After all, you can use evil-mode to give emacs vi&#x2F;vim keybindings. reply Rusky 15 hours agoprevYou can get the best of piece tables and ropes using a B-tree-based rope- at small sizes, you get a single node that works like a sorted piece table, but you can also scale up to larger documents without the high constant factors of a binary tree. reply josephg 14 hours agoparentYou can also get the best of btrees and gap buffers by putting a gap buffer in each leaf node in the tree. That allows the leaves to be much bigger (400 bytes seems to work well in practice) - and that helps keep the tree itself small and thus in the cpu cache.As the article says, it’s quite a complex data structure though. reply ayhanfuat 13 hours agoprevPrevious discussion from 2017: https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=15381886 (121 comments) reply punnerud 10 hours agoprevWhat about giving it all away to SQLite or similar and treat your document as a table with index, and text as rows with parent references?Then you don’t need to keep it in memory, can have unlimited undo and hand over to complexity to a solution (SQL) than you know with be there the next 20years reply kragen 5 hours agoparentit sounds like a reasonable approach, though some algorithms like string search may be trickier to implement than they would be with a simpler data structure, because a string may cross piece boundaries, maybe even more than onceyou don&#x27;t even need parent references; you can just assign synthetic sequence identifiers to the pieces and rely on sql lexicographical ordering to put them in order. (sql is not really very good with parent references.) this would be handy when you want to annotate the text with things like line numbers and syntax highlighting information. you wouldn&#x27;t get undo for free, though reply thfuran 10 hours agoparentprevWhat happens when someone opens up a 500 MB one-line json blob or something? reply kragen 5 hours agorootparentsql rows don&#x27;t have to be text lines reply jdub 10 hours agoparentprevSadly, no one will use your extremely slow text editor. reply theusus 15 hours agoprevRope has been the hype since years and now author is claim that it was piece table all along. reply kragen 4 hours agoprevif you don&#x27;t have reflinks (or some non-linux equivalent) you&#x27;re going to have to load the entire file into an array when you open it anyway, even if that array isn&#x27;t where you edit itthe performance cost of a dumb array is being oversold here. it&#x27;s true that insertion and deletion are slow in arrays, but let&#x27;s put this in perspective. i&#x27;m displaying my editor on a 2-megapixel display, which is 8 megabytes of pixels because they&#x27;re 32 bits per pixel. drawing a single frame of video involves the same amount of memory traffic as memmove()ing 4 megabytes of text. so until you have a few megabytes, even with a dumb array, ordinary text editing operations will easily support interactive responsivity. even things like refilling paragraphs and indenting or outdenting regions, which might involve hundreds of individual edits, is fine on everyday-sized files. this laptop can do about 20 gigabytes a second to main memory, 10 gigabytes of memmove(), so in 16.7 milliseconds (a 60-hertz frame) it can memmove() 167 megabytes. it isn&#x27;t until you get into things like search-and-replace jobs on big logfiles that you start to feel the painthe complexity of ropes is also being oversold here. as i pointed out in a comment in a subthread, you can make ropes quite complex, but https:&#x2F;&#x2F;github.com&#x2F;ivanbgd&#x2F;Rope-Data-Structure-C is 339 lines of code. it&#x27;s probably not a real-world rope, though; it has far too much memory overheadprobably the most widely used implementation of ropes is the one from the sgi stl, which is in gcc as libstdc++&#x2F;ext&#x2F;rope https:&#x2F;&#x2F;gcc.gnu.org&#x2F;onlinedocs&#x2F;libstdc++&#x2F;libstdc++-html-USER... and libstdc++&#x2F;ext&#x2F;ropeimpl.h https:&#x2F;&#x2F;gcc.gnu.org&#x2F;onlinedocs&#x2F;libstdc++&#x2F;libstdc++-html-USER..., which total 1485 lines of code, about five times larger. c++&#x27;s verbose syntax isn&#x27;t doing brevity any favors herein a garbage-collected language, like the cedar language where ropes originated, most of that complexity goes away, and production-quality ropes can be only a few hundred lines of codehttp:&#x2F;&#x2F;www.bitsavers.org&#x2F;pdf&#x2F;xerox&#x2F;parc&#x2F;cedar&#x2F;Cedar_7.0&#x2F;09_C...another somewhat popular rope implementation is librope, in c https:&#x2F;&#x2F;github.com&#x2F;josephg&#x2F;librope which is 684 lines of code and looks to be production-qualitythere&#x27;s an unrelated librope-ocaml in debian (and opam); it&#x27;s 1029 lines of code but nothing in debian depends on it. it looks quite full-featured; the documentation says it &#x27;has all non-deprecated operations of String&#x27; http:&#x2F;&#x2F;chris00.github.io&#x2F;ocaml-rope&#x2F;doc&#x2F;rope&#x2F;Rope&#x2F;in ur-scheme i used the simplest, dumbest kind of rope for building up output text (input was parsed into s-expressions character by character): an output text was either a string, a pair (representing the concatenation of its arguments), or nil (representing the empty string). this is very similar to erlang&#x27;s io lists. the only operations supported were (constant-time) concatenation (implemented as cons) and (linear-time) flattening into a string; this took 27 lines of scheme because the subset of scheme i implemented it in was quite limited. http:&#x2F;&#x2F;canonical.org&#x2F;~kragen&#x2F;sw&#x2F;urscheme&#x2F;finseth&#x27;s book is worth reading https:&#x2F;&#x2F;www.finseth.com&#x2F;craft&#x2F;, tatham&#x27;s notes on b-tree ropes https:&#x2F;&#x2F;www.chiark.greenend.org.uk&#x2F;~sgtatham&#x2F;tweak&#x2F;btree.htm... are worth reading, and raph levien&#x27;s notes on xi and ropes https:&#x2F;&#x2F;xi-editor.io&#x2F;docs&#x2F;rope_science_00.html are worth reading. chris wellons&#x27;s post about gap buffers https:&#x2F;&#x2F;nullprogram.com&#x2F;blog&#x2F;2017&#x2F;09&#x2F;07&#x2F; is a little bit silly but has animations reply guidoism 15 hours agoprevHonestly an array isn’t really the worst… for smallish files on modern processors. Sequential memcpy is pretty dang fast and pointer indirection can be slow.Just sayin’ reply eludwig 11 hours agoparentWorked on a homegrown Mac wsywyg editor back in the 90s. Arrays worked perfectly. If you are assuming that files fit in memory, using BlockMove() was very, very fast indeed.I can see if you need to edit multi-gig log files and things will not fit in memory, but for small files, array is totally fine.There were other tricks that were done back then to keep the number of single char inserts down to a minimum while typing. Like reading chars into a small buffer during fast typing and then inserting all the keystrokes at once as soon as you had the time. reply whazor 11 hours agoparentprevYes. When typing normally there is no slowness because you’re swapping one string for another.But when pressing enter, pasting a bigger snippet, or deleting lines the slowdown is less noticeable. When you type a word, the deltas are tiny and it is easy to process for your brain. While the changes in multiple lines happen less often, so waiting a couple of frames doesn’t bother you much.This is at millions of lines. With small files it is indeed too fast. reply wavemode 10 hours agoparentprevAn array and nothing else? It very quickly stops making sense to copy the entire tail section of the file every time the user inserts text in the middle. reply userbinator 9 hours agorootparentA conservative estimate for memmove() speed on a computer from even a decade ago would be 1GB&#x2F;s. That&#x27;s 1ms to move 1MB. All humans won&#x27;t notice a latency below 10ms. reply kragen 4 hours agorootparentthis is true, and a good point; the machine i&#x27;m typing this on is closer to 20 gigabytes a second. but, for example, if i refill a longish paragraph in emacs, it might easily do dozens of insertions; if i indent or outdent a block of code, it might do hundreds. it&#x27;s easy to imagine a situation where i have a 13-megabyte mail.log open in emacs and want to do a search-and-replace to reduce the noise level in it, maybe making 12895 edits. this currently takes less than a second (emacs uses a gap buffer), but if each of those edits took 6 milliseconds it would have been a minute or two reply ehaliewicz2 9 hours agorootparentprevIf it was an issue, and a simple gap buffer isn&#x27;t fast enough, you could do a linked list of gap buffers. Would still be pretty simple, and probably very fast. reply billforsternz 11 hours agoparentprevI was amused that this approach was dismissed without analysis as impractical. Sure it was impractical in living memory when CPUs and memory were many many orders of magnitude slower. reply naniwaduni 13 hours agoparentprevThis sort of data structure evaluation mostly starts to make sense when you approach interactive large-file editing as a soft-realtime problem. reply 082349872349872 13 hours agoparentprevand if an array doesn&#x27;t work, a gap buffer isn&#x27;t much additional complexity.all of my C homebrew editors have been gap buffer based, and I told myself I&#x27;d reimplement if I ever wanted to edit something large enough for that to be a problem — but it never was. reply Hammershaft 15 hours agoprevEmacs is the fastest editor the author has worked with!!?? reply TacticalCoder 13 hours agoparentTo be fair on a modern setup it is fast. Sure it&#x27;s not nano or vi(m) but compared to Electron-based stuff, it&#x27;s plenty fast.Five years ago: time emacs -Q -eval &#x27;(kill-emacs)&#x27;would be slow at 320 ms.But on a \"recent\" AMD 7700X that&#x27;d be 80ms.That&#x27;s starting emacs, evaluating elisp code, and exiting Emacs.Surely if you can start Emacs, execute elisp, code and exit in 80ms it shouldn&#x27;t lag too much to insert a character.Now, I know, some modes can be slow. But Emacs in recent version as seen lots of changes enhancing its performances. And hardware have become really very fast.YMMV but my Emacs is really incredibly responsive.P.S: I was already using Emacs on a 486 back in the \"Eight Megabytes And Constantly Swapping\" days. The joke was fun and true. Nowadays Emacs is actually one of the lightest sofware I use. reply cb321 10 hours agorootparentYeppers. The days of waiting a minute or more for emacs to initialize itself are long gone for most. Still marginally slower than nvim or vim for me (on a 2016 vintage i7-6700k on Linux under X11 inside of a sucksless `st` terminal): $ tim &#x27;vim --clean -c quit&#x27; &#x27;nvim --clean -c quit&#x27; &#x27;emacs -nw -Q -eval \"(kill-emacs)\"&#x27; (3.45 +- 0.27)e-04 (AlreadySubtracted)Overhead (7.929 +- 0.052)e-03 vim --clean -c quit (8.329 +- 0.031)e-03 nvim --clean -c quit 0.05302 +- 0.00023 emacs -nw -Q -eval \"(kill-emacs)\"There can still be a human-noticeable difference when you fire up emacs on some 2.5 MiB org-mode \"active document\" and it takes over 1000 milliseconds (a whole heartbeat! omg! &#x2F;scarcasm) while vim still takes under 20 ms. And, similarly, vim&#x2F;nvim with a lot of plugins loading can push up their start-up times as well.EDIT: BTW, chances are good that the 4X reduction from 320ms to 80ms over 5 years you are seeing is in large part due to ahead-of-time native compilation of elisp (https:&#x2F;&#x2F;www.gnu.org&#x2F;software&#x2F;emacs&#x2F;manual&#x2F;html_node&#x2F;elisp&#x2F;Na... ) and libgccjit&#x2F;etc. not just CPU&#x2F;DRAM improvement. reply 9front 7 hours agorootparentprevEven under WSL emacs is fast: [9front@RIPPER ~]$ time emacs -Q -eval &#x27;(kill-emacs)&#x27; real 0m0.193s user 0m0.136s sys 0m0.010s reply tom_ 14 hours agoparentprevThere are other editors?! reply fleur-de-lotus 14 hours agoparentprevHe never tried anything else. For speed, vi is number one, followed by sublimetext. reply TwentyPosts 13 hours agorootparentEven the suggestion that vim is fast is funny to me.Vim feels sluggish compared to NeoVim.NeoVim feels sluggish compared to Helix. reply smitty1110 10 hours agorootparentThat&#x27;s why GP said &#x27;vi&#x27;, not &#x27;vim&#x27; or &#x27;neovim&#x27;. It&#x27;s an important distinction. You can still get old-school vi on most distributions, and it is far smaller and lighter than vim. In my experience, it feels snappy even if you&#x27;re in an ssh session to a server in a different timezone. reply behnamoh 12 hours agorootparentprev> NeoVim feels sluggish compared to Helix.Of course, if you offer only a portion of NeoVim&#x27;s features the speed improvement should not be surprising. reply TwentyPosts 12 hours agorootparentAt the same time, NeoVim lacks many, many of Helix&#x27; features unless you use plugins. That&#x27;s fine, of course, plugins and customization are part of what makes NeoVim so great.Anyway, can you name a few NeoVim features you miss in Helix? I still use NeoVim now and then, so I&#x27;d be happy to learn more, even if it&#x27;s something small. reply beautron 10 hours agorootparentprevHow does Vim feel sluggish? Everything in Vim feels instantaneous to me. reply TwentyPosts 9 hours agorootparentGranted, it&#x27;s been long enough I don&#x27;t even quite remember the exact issues I had, so take it with a grain of salt. It was definitely primarily weird edge cases, and might&#x27;ve been related to plugin use.This is one area where NeoVim afaik comes out on top: Vimscript isn&#x27;t fast, and NeoVim allows Lua plugins via the embedded LuaJit, and those should be significantly faster. reply medo-bear 11 hours agoparentprevAs an emacs user I find it offensive that he tried other editors reply PH95VuimJjqBqy 5 hours agoprev [–] > In fact, the editor I’m writing this in – Emacs – uses a gap buffer, and it’s probably the fastest editor I’ve ever used. That fact alone is a pretty convincing argument to use a gap buffer.I know for a fact that vim is faster than Emacs and I&#x27;m pretty sure it uses an array (although I don&#x27;t know that for a fact). reply kragen 4 hours agoparent [–] i thought vim used an array of lines, not an array of bytes, but it turns out it&#x27;s more like an array of pages; https:&#x2F;&#x2F;www.free-soft.org&#x2F;FSM&#x2F;english&#x2F;issue01&#x2F;vim.html describes it, https:&#x2F;&#x2F;github.com&#x2F;vim&#x2F;vim&#x2F;blob&#x2F;e723c42836d971180d1bf9f98916... is the implementation replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The author is developing a text editor and evaluating different data structures for the core API.",
      "Options examined include an array, a rope (a binary tree structure), a gap buffer, and a piece table.",
      "The author concludes that the piece table is the best choice due to its elegance and efficiency."
    ],
    "commentSummary": [
      "The debate centers around the optimal data structure for a text editor, specifically whether an array is the best choice.",
      "Various data structures such as ropes, balanced binary trees, and gap buffers are analyzed for their performance and ability to handle different file types.",
      "The discussion also delves into the challenges of managing large files and compares the efficiency of different text editors, specifically Emacs, NeoVim, and Helix, in terms of speed and performance."
    ],
    "points": 228,
    "commentCount": 69,
    "retryCount": 0,
    "time": 1703605695
  },
  {
    "id": 38773426,
    "title": "Private equity acquisitions harm patient care, study finds",
    "originLink": "https://medicalxpress.com/news/2023-12-quality-declines-private-equity-hospitals.html",
    "originBody": "December 26, 2023 Editors' notes This article has been reviewed according to Science X's editorial process and policies. Editors have highlighted the following attributes while ensuring the content's credibility: fact-checked peer-reviewed publication trusted source proofread Quality of care declines after private equity takes over hospitals, finds nationwide analysis by Harvard Medical School Credit: Unsplash/CC0 Public Domain Patients are more likely to fall, get new infections, or experience other forms of harm during their stay in a hospital after it is acquired by a private equity firm, according to a new study led by researchers at Harvard Medical School. The research, published Dec. 26 in JAMA, is among a handful of recent nationwide analyses of how private equity takeovers affect the quality of patient care in hospitals. The increases are seen in conditions or outcomes deemed preventable and are key measures of hospital safety and quality. The findings come amid growing concerns about private equity's increasing role in U.S. health care, with $1 trillion invested in the past decade. \"We had previously found that private equity acquisitions led to higher charges, prices, and societal spending,\" said Zirui Song, associate professor of health care policy and medicine in the Blavatnik Institute and director of research in the Center for Primary Care at HMS. \"Now, we're learning that there are also downstream concerns for the clinical quality of care delivered to hospital patients.\" The researchers said the findings are alarming because they may reflect bottom-line incentives overshadowing patient care and safety. \"Hospital success is measured not only in dollars or the number of patients who pass through the doors, but also in lives saved, complication rates, patient satisfaction, and a number of other quality and safety metrics,\" said HMS research fellow Sneha Kannan, a physician in the Division of Pulmonary and Critical Care at Massachusetts General Hospital. \"We need to make sure we fully understand the costs and benefits of this prominent new force in health care.\" The economic repercussions of private equity acquisitions are not a new concern. Previous studies by Song and co-author Joseph Dov Bruch of the University of Chicago indicate that this high-debt, for-profit financial model of hospital ownership may also lead to increased spending and other economic implications. Many have expressed concerns about hospital bankruptcies under private equity ownership that often leave underserved populations with limited access to care. But up until now, the effects of private equity deals on patient health and quality of care have remained understudied and poorly understood. Why private equity is different \"When health systems buy hospitals, they generally do not use borrowed money,\" said Song, who is also an internal medicine physician at Mass General. \"In contrast, the classic private equity buyout uses a small amount of cash, but a large amount of debt.\" A private equity firm raises some capital from investors and borrows the rest, putting debt on the acquired hospital with its physical assets, such as land and buildings, as collateral for the loan. The acquired hospital must then generate revenue to pay down that debt. Private equity generates revenue by charging management fees to its investors—commonly, pension funds, endowments, and other institutions or individuals—as well as by focusing on high-revenue procedures, cost-cutting, reorganization, and financial engineering. One argument in favor of private equity investments is that many struggling hospitals need capital and management expertise. However, most private equity buyouts are of successful operations. Private equity firms want to buy going concerns that are able to take on debt and generate revenue in the short run. These financial pressures can create perverse incentives favoring profit over patients, the researchers say. Private equity and quality of care For this study, the researchers examined insurance claims data for all fee-for-service Medicare hospitalizations from 2009 to 2019, totaling more than 600,000 hospitalizations at 51 private equity hospitals and more than 4 million hospitalizations at 259 similar hospitals not acquired by private equity. The hospitals not acquired by private equity served as the control group to control for other factors that may have affected outcomes. The researchers compared how often patients experienced certain outcomes before and after the hospital was acquired by private equity. For example, they looked at how often patients fell while in the hospital or how often they developed an infection after a procedure or a surgery. The team also analyzed the makeup of the patient populations and various other outcomes such as how often patients died, how long they stayed at the hospital, and how often they ended up readmitted after leaving the hospital. After a hospital was acquired by private equity, admitted Medicare patients had a 25% increase in hospital-acquired complications, compared with patients admitted before acquisition. Patients also had 27% more falls and 38% more bloodstream infections caused by central lines, which are temporary surgically inserted ports that allow easy intravenous access for patients receiving repeated drug infusions or other treatments. The increase was seen despite private equity hospitals' placing 16% fewer central lines than before the buyout. All of these results were calculated while taking into account changes, trends, and patterns over the same period of time at peer hospitals not owned by private equity to isolate the differences that were due to the change in ownership. Curiously, the study found a small drop in hospital deaths at private equity hospitals. This, the researchers said, may be due to social and demographic factors—private equity patients were younger and less disadvantaged than those at peer hospitals not owned by private equity. It may also be due to patients getting transferred more often out of private equity hospitals. When the researchers followed patients longer after discharge, the small decrease in deaths dissipated within a month after leaving the hospital. Framework for policy solutions Policymakers, insurance companies, and public sector bodies have grown increasingly concerned about protecting patients and societal resources from the effects of private equity transactions. Earlier this year, Song and Christopher Cai, a HMS clinical fellow in medicine at Brigham and Women's Hospital, outlined such a policy framework in a JAMA viewpoint article, which included regulating fraud and abuse, increasing antitrust oversight, reducing moral hazard (such as by lowering the debt used in acquisitions), protecting against inflated prices, and transparency in reporting of private equity acquisitions. Currently, only private equity acquisitions over $111.4 million must be reported. This threshold may capture many hospital acquisitions but leaves out most acquisitions of physician practices. \"Private equity firms have historically operated in the shadows in health care,\" Kannan said. \"Going forward, it's important to lift the veil and increase transparency.\" And both researchers and policymakers should be rigorous in their efforts to understand how private equity changes health care operations and the downstream consequences, the authors cautioned. \"Patients and providers, investors and taxpayers, employers and insurers, all have a stake in this,\" Song said. \"Understanding what the corporatization of health care delivery means is a goal shared by many across society.\" More information: Changes in Hospital Adverse Events and Patient Outcomes Associated with Private Equity Acquisition, JAMA (2023). DOI: 10.1001/jama.2023.23147. jamanetwork.com/journals/jama/ … 1001/jama.2023.23147 Journal information: Journal of the American Medical Association Provided by Harvard Medical School Citation: Quality of care declines after private equity takes over hospitals, finds nationwide analysis (2023, December 26) retrieved 27 December 2023 from https://medicalxpress.com/news/2023-12-quality-declines-private-equity-hospitals.html This document is subject to copyright. Apart from any fair dealing for the purpose of private study or research, no part may be reproduced without the written permission. The content is provided for information purposes only.",
    "commentLink": "https://news.ycombinator.com/item?id=38773426",
    "commentBody": "Quality of care declines after private equity takes over hospitalsHacker NewspastloginQuality of care declines after private equity takes over hospitals (medicalxpress.com) 217 points by rntn 17 hours ago| hidepastfavorite139 comments alwaysrunning 16 hours ago\"After a hospital was acquired by private equity, admitted Medicare patients had a 25% increase in hospital-acquired complications, compared with patients admitted before acquisition. Patients also had 27% more falls and 38% more bloodstream infections caused by central lines, which are temporary surgically inserted ports that allow easy intravenous access for patients receiving repeated drug infusions or other treatments.The increase was seen despite private equity hospitals&#x27; placing 16% fewer central lines than before the buyout.\"I am not sure who the private equity firms are, but I thought it was alarming when insurers started purchasing hospitals, urgent care facilities and even your standard family med doc, as they would be negotiating with themselves on how much profit they could make off each appointment. But I didn&#x27;t consider lower standards when it comes to patient care, this adds a whole different dimension. reply Voultapher 16 hours agoparentYou know, it&#x27;s maybe a bad idea to privatize some things https:&#x2F;&#x2F;www.economist.com&#x2F;by-invitation&#x2F;2023&#x2F;07&#x2F;10&#x2F;mathew-la... water in the U.K got more expensive and the service worse. A terrible deal for everyone but a handful of private equity investors. reply klipt 16 hours agorootparentWater is a monopoly right? Seems obvious that privatizing a monopoly doesn&#x27;t help, because there&#x27;s no competition incentive to keep prices down and service up. reply albert180 13 hours agorootparentHospitals are a monopoly too, when you have an emergency and need care now, you can&#x27;t compare prices or get quotes for the care, as you don&#x27;t know even what you will need reply aeternum 8 hours agorootparentThis is not well-known but the vast majority of hospital visits are non-emergency.People used to have choice but now due to 1950s tax code, your employer chooses a health conglomerate which chooses an insurance company which influences the billing allowances which influences the hospital profitability which is now so complicated that often only PE firms are willing to take the investment risk.The incentives and market feedback is now so distorted that there is little to no signal. Healthcare in the US is often used as an example of failed free markets, but it is very far from a free market. reply AnotherGoodName 15 hours agorootparentprevI sometimes think the \"anti-government run anything\" mantra is just people who are easily swayed by propaganda reciting a line. It&#x27;s easy to take singular examples of government failure and write articles claiming that everything must be owned by private equity because government is dysfunctional.But do you know what? I used to pay 1&#x2F;4 the price for electricity when i lived in an area with a local county run power company compared to my new nearby location that has PG&E as the only option. As in government run utilities were literally 1&#x2F;4 the price per kwh and yet i&#x27;m supposed to buy into this \"GOVERNMENT IS INEFFICIENT LOL COMMUNISM\" propaganda that just doesn&#x27;t appear true in the slightest. I mean this is consistent across the board.I know the baby boomer generation were vulnerable to this anti-socialism propaganda but can we please start looking at things objectively and push back against this privatization that is almost always a way for politicians to enrich themselves. Sigh. reply jaybrendansmith 13 hours agorootparentMy township recently sold my local sewer line to a private company, after privatizing my water line to the same company. Interestingly, there was a major scandal a few years ago where the private company had allowed grey water to mix with the fresh water running to everybody&#x27;s tap, and a few people got sick. There seemed to be a major public outcry but the company did not pay a price for the error, and were rewarded with the sewer business. As someone who maintains a SaaS business, this seemed unforgivable to me, the equivalent of my company failing in their one job, to keep the servers online. At the same time, my water bill has just about doubled, and my sewer bill has now gone to be around three times. I now pay 4.5 times what I did at my old residence with public utilities, for the exact same service. Is everybody on the take? reply kriops 11 hours agorootparentprevGame show idea: Is it a malfunctioning market, or just regulatory capture? reply Gibbon1 12 hours agorootparentprevI think the anti-government everything must be privatized thing is pushed because it&#x27;s an opportunity to divert large flows of money into wealthy peoples pockets. The head of a publicly run company might make as much as a sr tech bro. The head of a privatized one can divert tens of millions a year.Same as attempts to privatize social security. You got a cash flow of $1.4T&#x2F;year and it drives the lords of capital bonkers they can&#x27;t get their hands on it. reply skfingngihg 14 hours agorootparentprevThe government is less efficient at serving its stake holders than for-profit corporations.The government creates the conditions for PGE to be so bad. I’m defecting from the grid this year, PGE’s high prices mean my s system will pay for itself in less than a year and will likely be more reliable.battery and solar companies saved me from PGE and they can save you. The government perhaps can but likely won’t. Extrapolate for yourself what this means about the world and what organizations should command the most resources. reply cool_dude85 12 hours agorootparentThe government told PG&E not to fix its broken ass infrastructure and cause a catastrophic fire? I think if you examine the historical record, you will find that the opposite occurred. The government told PG&E not to have 100-year-old infrastructure in fire prone areas, and PG&E decided to send dividends to its hedge fund owners instead of doing so. reply figassis 12 hours agorootparentprevSeems a bit passive no? The government has a job, but your way of handling it is the opposite of holding it accountable. Government is made of people and they will be as inefficient as allowed to be. reply rqtwteye 15 hours agorootparentprevSeems it’s the perfect target for investors. Competition is for suckers. reply Sharlin 15 hours agorootparentprevOh, but it does help. Just not the general population. reply giantrobot 16 hours agorootparentprevNo one wants to learn the lesson that while government administration of some things might be \"inefficient\", any efficiencies brought about by privatization will be turned into margins by the private entity rather than savings to the end customers. Private entities need to maintain margins and will cut service to maintain them. reply Frost1x 15 hours agorootparentYea the argument that efficiency gains are passed to consumers is a tired old and often false argument. The only time efficiencies are passed to consumers are when there is real healthy competition and consumers are supporting that diversity. That&#x27;s when in order to remain competitive, efficiency needs to be passed to the consumer.Even in those situations it&#x27;s not guaranteed. Markets seem to arise at or duplicate similar solutions and even if it&#x27;s inefficient it&#x27;s not worth he risk to find better alternatives for consumers because the competitive edge vs risk vs reward just isn&#x27;t a winning mix worth pursing.I&#x27;m increasingly convinced that most forms of free market systems work best once we have commoditization of a solution and it&#x27;s just a matter of reproducing that solution, and doing so isn&#x27;t too capital intensive. As you start to move away from these parameters, markets get more and more consumer hostile and self focused making the argument for them increasingly weak. reply rqtwteye 15 hours agorootparentThere needs to be a focus on competitive markets instead of free markets. Competition is mostly good for customers but competition needs to be created by regulating the market to some degree. Otherwise markets will almost always be taken over slowly by the biggest players that don’t like competition. reply brayhite 11 hours agorootparentIt’ll never happen in the US. Neither political party with power will break away from the corporate interests that determine what laws and regulations will pass.The last candidate who ran on small donors and wore a proud badge of being anti-corporate interests lost twice. Politicians are figuring out how much money can be made by both acquiring legislative influence and power and accepting donations to steer their platforms, as well as passing legislation and policy that helps siphon public funds into private companies that said politicians have a financial stake in.Changing private equity power requires benevolence, and there’s very little of that left amongst those with political power in the US. reply lelandfe 15 hours agorootparentprevNot to mention that the private companies will work to avoid said competition. Little wonder why ISPs refer to creating networks where one already exists as “overbuilding” reply lotsofpulp 13 hours agorootparentWhen I look around, there are multiple businesses usually near each other that compete. Home Depot and Lowes, Walmart and Target, Burger King and McDonald’s. ATT&#x2F;Verizon&#x2F;Tmobile.But it is possible that if you pay to build out an entirely new network to a place where Comcast has a monopoly, and the customers would rather pay Comcast $5 less than whatever the lowest price you can charge is, then you would see negative returns by paying to build that network (since the cost of building out a network like that is so astronomical, and Comcast obviously does not have to pay it so their COGS is much lower). reply JumpCrisscross 15 hours agorootparentprev> most forms of free market systems work best once we have commoditizationIt’s almost the opposite. Free markets didn’t work in the pre-industrial era because everything was commoditised. That made returns on capital paltry, which in turn made it more “profitable” to conquer capital than build it.After industrialisation, we repeatedly saw innovation—though not necessarily creativity—in market economies outperform that in centralised ones. reply JumpCrisscross 16 hours agorootparentprev> Private entities need to maintain margins and will cut service to maintain themThis is a false dichotomy. Public v private is so often framed as a political agency versus free-market bonanza. In reality, we have independent agencies and public-private partnerships. But on the private side, we also co-operatives, a model which seems well-suited for community hospitals. reply Arn_Thor 12 hours agorootparentPPPs are often a mirage. As long as there&#x27;s a private actor involved, some of the input (money) will be siphoned off as profits. That&#x27;s the height of inefficiency. Where the alternative is a competent, responsive and accountable government, the government will allocate the resources more effectively, and all \"inefficiency\" goes into working people&#x27;s pockets. This is not always practical, for example in situations where only private businesses have highly specialized knowledge, but for basic infrastructure the private sector offers nothing the government couldn&#x27;t do just as well. reply phkahler 15 hours agorootparentprevOr how about just non-profit? Regardless of the options, private equity seems like the worst one and should probably be illegal.Taking \"profit\" from keeping people \"not dead\" seems as close to extortion as you can get without actually being so. reply JumpCrisscross 15 hours agorootparent> how about just non-profit?“If the public health goal is to improve hospital care, then focusing on things like for-profit or nonprofit status is a distraction” [1].> Taking \"profit\" from keeping people \"not dead\" seems as close to extortionModern medicine is expensive. For profit or not, expenses must be covered.We’re better off incentivising people to become doctors and nurses and pharmaceutical researchers. The problem is all the nonsense being eaten up in administration, administration largely paid for by price obfuscation.[1] https:&#x2F;&#x2F;www.hsph.harvard.edu&#x2F;news&#x2F;press-releases&#x2F;hospitals-c... reply phkahler 14 hours agorootparent>> “If the public health goal is to improve hospital care, then focusing on things like for-profit or nonprofit status is a distraction”Fair enough. So let&#x27;s just ban for-profit healthcare so we can stop being distracted by that whole thing. reply albert180 13 hours agorootparentprevStill you manage to waste 10% on Administrative Expenses and have the worst health outcomes in G7 but the highest expenses. Maybe learn a thing or two from northern Europe which has 100% public healthcare delivery reply giantrobot 9 hours agorootparentprev> Modern medicine is expensive. For profit or not, expenses must be covered.There&#x27;s a ton of bullshit that goes into \"medical expenses\" in the US. Not the least of which is the inability for people to pay for care and end up in the ER for urgent non-emergency services because ER services are required to be provided. Hospitals need to foot the costs of those people that literally can&#x27;t pay. Those costs get amortized over every other service (plus plain old margin and gauging) increasing the overall costs.Besides eliminating unpaid emergency service public health coverage would let people get regular medical care without plunging into debt so they&#x27;d go sooner before conditions become acute. Private insurance could even still exist, but as a premium add-on to statutory coverage.More doctors and nurses is just supply side economics applied to healthcare. Those jobs will appear naturally with the entire population completely covered. Rural hospitals could even open back up since they&#x27;d be serving people with actual coverage. replyFomite 16 hours agoparentprevOne of the big problems is that infection control and antibiotic stewardship don&#x27;t have billable procedures most of the time, and are thus viewed as cost centers by hospital leadership. reply ublomate 16 hours agorootparentAnd therein lies the most significant problem with unbounded capitalism. Capitalism is a very, very powerful tool, because it redefines everything in terms of money. This enables the magic of specialization and trade, but it also causes situations like this where greedy and&#x2F;or thoughtless people chip away at the margins of things like safety and let other people suffer the consequences. reply JumpCrisscross 15 hours agorootparent> Capitalism is a very, very powerful tool, because it redefines everything in terms of moneyWell, in terms of capital. For the segments of the economy we put into the market. Also, this is more a feature of industrialisation than capitalism—socialist economies still consider resource allocation in generalisable terms. reply amelius 15 hours agorootparentprev> Capitalism is a very, very powerful tool, because it redefines everything in terms of money.But only when it is convenient. For example school teachers or police officers can work their brains out but cannot profit from it like entrepreneurs do. For many people unbounded capitalism is a very shitty proposition. reply saulpw 12 hours agorootparentTeachers and police officers don&#x27;t benefit by definition because they do not have capital. Capitalism benefits the people who start private schools and private jails. Capitalists take advantage of labor in order to increase their own profits. reply rootusrootus 14 hours agoparentprev> I thought it was alarming when insurers started purchasing hospitals, urgent care facilities and even your standard family med docI am skeptical of that conclusion. Kaiser Permanente gets good results, and they are an HMO: the very definition of an insurer owning the hospital&#x2F;urgent care&#x2F;family doc. reply chiefalchemist 16 hours agoparentprev> After a hospital*A* hospital. Single. I&#x27;m certainly not going to defend private equity. But if we&#x27;re going to do a takedown, let&#x27;s do it correctly. Cherry picking what is likely an outlier is not doing it correctly. reply arcaen 10 hours agorootparentNot sure where you got the idea that the study was only of a singular hospital, seeing as the study outline details claims for 600k hospitalizations in 51 private-equity hospitals are compared to 4m comparable hospitalizations in 259 non-private-equity hospitals. reply chiefalchemist 14 hours agorootparentprevSome things never change. So typical of HN. A comment lacking in critical thinking is added. The punters jump on to further the false narrative. And...the correction to that misguided affair is down voted. Y&#x27;all are funny. Ignorant, but funny. reply orwin 2 hours agorootparentSorry, I avoid saying that on HN, but if you read the article and not just skimmed through it, you could have clicked on the link to the study here:https:&#x2F;&#x2F;jamanetwork.com&#x2F;journals&#x2F;jama&#x2F;fullarticle&#x2F;2813379?gu...And discover why your comment was downvoted. I apologize I did not leave a comment to explain why I was down voting, I just did not want to say &#x27;look, this person can&#x27;t read&#x27;, it&#x27;s tacky (sorry for still doing it btw) reply dopylitty 16 hours agoprevThis is such a silly quote:> \"Hospital success is measured not only in dollars or the number of patients who pass through the doors, but also in lives saved\"As if profits are the primary goal of hospitals and saving lives&#x2F;treating patients is just an afterthought.The capture of all life sustaining efforts by for-profit entities is probably the number one problem in the western world which is driving all others. Climate change, famine, homelessness, death from preventable disease.None are caused by unsolvable problems but instead by the inability of the for-profit&#x2F;market based system to provide for human needs. reply TaylorAlexander 15 hours agoparentI think a lot about how hard it is for us to imagine a better world. My foot kind of hurts. It’s been this way for years and I’ve changed shoes to something that’s better but I’d love to have it looked at by a podiatrist. But also every time I visit the doctor, despite having insurance, I end up having to pay a lot of money. So maybe I don’t see a doctor about my foot. It’s fine.But in a better world I wouldn’t have any reason not to get my foot checked up on. Maybe there is something that could be done for it and maybe it makes my life better. Or it’s fine and I get peace of mind and stop worrying.Now multiply this across the entire population. What if everyone had lower barriers to health care. The amount of population wide well being would increase a lot. And that’s the part I think we usually have a hard time conceptualizing when we think about a better world. But it’s a very real possibility! reply Dalewyn 15 hours agoparentprevManhours cost money, equipment and infrastructure cost money, supplies and consumables cost money.If the primary goal of a hospital is to save lives, which it is, then the hospital needs to generate sufficient revenue to enable that goal. reply phkahler 15 hours agorootparentNo doubt. But skimming some of the revenue for profit goes directly against that reply JumpCrisscross 15 hours agorootparent> skimming some of the revenue for profit goes directly against thatNon-profit hospitals don’t fare better or worse on health outcomes than for-profit hospitals [1]. They are run less efficiently [2]. If you’ve been on a non-profit Board, particularly a service-model non-profit, this makes sense.[1] https:&#x2F;&#x2F;www.hsph.harvard.edu&#x2F;news&#x2F;press-releases&#x2F;hospitals-c...[2] https:&#x2F;&#x2F;www.ncbi.nlm.nih.gov&#x2F;pmc&#x2F;articles&#x2F;PMC7439627&#x2F; reply Dalewyn 15 hours agorootparentprevProfit is necessary to invest in the future. If revenue matches cost leading to a gross profit of zero, you can only ever keep whatever it is you have today with no room to advance or adapt or even maintain. reply Arn_Thor 12 hours agorootparentprevExcept, if it&#x27;s tax funded the hospital does not need to generate revenue to enable that goal. It can focus on saving lives, and maybe even do preventative care which lowers overall health costs for society by dealing with problems early on. As a society, that saves money, leaving everyone better off reply hlieberman 16 hours agoprevSeems obvious. Private equity is all about sacrificing everything on the altar of cost cutting, quality be damned. Why should we be surprised they do the same to the hospitals they buy? reply dv_dt 16 hours agoparentIt’s pretty clear to me that the overriding economic issue of our time is how do we systematically realign delivering quality and efficiency with profits. It’s pretty obvious that there are major breakdowns in both the current practices of laissez-faire and of regulation. Its seems to be an exception when a company delivers on all three of quality, efficiency, and profits. reply tremon 10 hours agorootparenthow do we systematically realign delivering quality and efficiency with profitsWe don&#x27;t. We should just acknowledge that certain sectors of the economy (including healthcare and education) are fundamentally incompatible with private profit. These sectors should be ring-fenced with their own managed budgets and money flowing out of the system should be subject to 100% tax rate. reply VancouverMan 15 hours agorootparentprev> how do we systematically realign delivering quality and efficiency with profits.Free entry into the market, along with competition, are the free market forces that naturally balance those attributes over time.Keep in mind that within a given market, there can be multiple different balances at once, depending on the varying needs and priorities of the buyers and sellers involved.> It’s pretty obvious that there are major breakdowns in both the current practices of laissez-faire and of regulation.Even if it may be somewhat freer than in places like Canada or the UK, for example, the US health care sector shouldn&#x27;t be considered a real free market or laissez-faire in any meaningful way.The US health care sector is highly regulated. This significantly reduces competition, and also makes it extremely costly and difficult for new entrants to participate in the markets.Regulation-imposed distortions that decrease competition and impede market entry should be expected to result in the inefficiencies and other problems we see.Fixing the problem is rather simple: remove the regulations, allow free market entry and competition again, and put the onus back on market participants to choose for themselves what&#x27;s best for them and their own particular situations. reply kevin_b_er 14 hours agorootparent> Fixing the problem is rather simple: remove the regulations, allow free market entry and competition again, and put the onus back on market participants to choose for themselves what&#x27;s best for them and their own particular situations.This is a statement that cannot be farther from the truth.There is a great asymmetry of information and a high pressure situation that leads to difficulty in researching and deciding on the best choice. It is a system that, without regulation, leads to exploitation.The regulations exist because of the abuses, because of the immoral behavior it caused without them.The only motivation to remove them is a profit motive. Avarice at the sake of lives. It is not &#x27;regulations&#x27; that are a problem, but the immoral avarice living at the heart of healthcare in the US. reply hlieberman 15 hours agorootparentprevExcept that, in basically every other developed nation, healthcare is _more_ regulated, if not nationalized, and has better performance for substantially lower cost.The invisible hand(job) can&#x27;t save you from areas where there is no significant profit to be made. In many cases, it&#x27;s most cost effective to simply let people die. That&#x27;s the problem with focusing exclusively on profit motives in healthcare. reply kelseyfrog 15 hours agorootparentprevWhat do we do if we think it&#x27;s immoral and unethical to make people choose between money and their lives? What&#x27;s the plan B? reply asadotzler 15 hours agorootparentprevRight, because who cares about quality and outcomes, a race to the bottom FTW. reply pylua 15 hours agorootparentprevWhat regulations are you referring to that we can remove ? reply mint2 14 hours agorootparentprevErrr that just causes a dystopian race to the bottom. Why would one think otherwise? reply drewcoo 16 hours agoparentprevIn a world of so many distractions, what&#x27;s not a surprise?Also, I think unless people are trained to question everything, they tend to flow with the dominant narrative. Private equity makes health care more efficient!I&#x27;d love to know how to get people to focus on potential problems and ask questions. reply pc86 16 hours agorootparent\"Private equity makes health care more efficient\" isn&#x27;t the dominant narrative by any stretch. reply faeriechangling 16 hours agoprevThe free market is the most efficient system we have for optimising profits. Not sure why people think that means it&#x27;s a good model for hospitals. reply darth_avocado 15 hours agoparentWhat we have is not a free market. We have a system that has been consolidated and pushed towards high inefficiencies by pharma companies, insurance companies and hospital conglomerates.A free market would be one where government hospitals exist alongside private for profit hospitals. A free market would have insurance as an optional convenience, not a mandatory one. A free market would have one price for medicine regardless of how you pay for it, where paying with insurance wouldn’t cost $34 when paying out of pocket and using GoodRx costs $2. A free market would be one where opening a private practice for a doctor would not be prohibitively expensive for family medicine because they need to hire 5 administrators to deal with the entangled mess of insurance payments. reply poncho_romero 15 hours agorootparentI’d love to see evidence supporting the claim that a free market would make these things come to pass. Otherwise, this comes across as a list of things you’d like to see, with “the free market” standing in as the just-so story that will magically right all that is wrong reply verall 13 hours agorootparentI wanted to say that the charitable interpretation is that these things are basically prerequisites for the market to be \"free\" enough for any invisible hand efficiencies to arise - low barrier to entry, consumers are cost-conscious and have free choice, no anti-competitive bundling between provider, pharmacy, and insurance services.But then I hit F5 and see that they explicitly feel these things will just come about given a \"free market\". Oh well. reply darth_avocado 15 hours agorootparentprevI’d like to see evidence supporting it doesn’t work. If something hasn’t been done before, it’s really hard to provide evidence without actually doing it. I unfortunately cannot change it by my own, so I cannot provide evidence on what would hypothetically happen if we ban lobbying, get politicians motivated to allow a competitive free market, have the government participate in healthcare and do bunch of other things.I can provide examples of different models of public&#x2F;private healthcare existing in the same market, if health insurance and healthcare being more accessible and being cheaper across the world. All of these systems have more than one aspect of my “wishlist” if not all. https:&#x2F;&#x2F;www.news-medical.net&#x2F;health&#x2F;Healthcare-Systems-Aroun... reply derbOac 15 hours agorootparentprevNot to mention all the territorial regulatory capture on the provider side.I&#x27;m supportive of greater \"publicizing\"(?) of healthcare but I also agree that there are many, many, many ways in which competition could be increased to actual improve free market dynamics. reply kevin_b_er 14 hours agorootparentprevThe freest market is one that consolidates. The highest profits are made by controlling all the resources, after all. reply darth_avocado 13 hours agorootparentYes that is correct. In theory, in the freest market, there would only be one company that does everything and generates infinite profits.I’m responding to the “free market” that most “free market” advocates refer to: a market where companies are free to do whatever they want as long as it makes them money and also have regulations only when it benefits them. reply klipt 16 hours agoparentprevFor markets with lots of competition it&#x27;s pretty good at optimizing customer satisfaction at reasonable prices too. Consider food and electronics.It fails with monopolies though. reply thinkingtoilet 16 hours agorootparentNot even monopolies, literally no market forces. If you have a heart attack or a stroke, it doesn&#x27;t matter what the prices listed for care are or customer reviews. You get to a hospital as fast as you fucking can. There are literally no market forces in play a lot of the time. Also, if you don&#x27;t live in a big city, there may only be a single hospital by you that provides the treatment you need. Again, no market forces. reply JumpCrisscross 15 hours agorootparentIt’s useful to separate medicine from ER in healthcare discussions. In ER, there is no market. In healthcare, we have a deeply-flawed market. But in most zip codes, there is some competition. reply cameldrv 15 hours agorootparentIt&#x27;s being reduced though. A lot of private practices are being bought out by big (in many cases for-profit) hospital or doctor&#x27;s groups, and the doctors are put on salaries instead of owning their practices. In many areas there is now only one big group. Naturally this can be highly profitable. reply bakedoatmeal 15 hours agorootparentprevThe ED is the primary source of hospital admissions so I don’t think the two are that separable. reply JumpCrisscross 15 hours agorootparentIn part due to access-to-care issues. More competition in medicine should increase access thereby reducing ED use. One that comes to mind are in-house visits, something expensive for no good reason in America. reply aplusbi 16 hours agorootparentprevFood is heavily regulated though, in a large part because markets failed to prevent food-borne illnesses. reply darth_avocado 15 hours agorootparentIt is kind of regulated. Enforcement is really poor though. reply manicennui 15 hours agorootparentprevThe food in the US is terrible and contributes directly to many of our health problems. reply FrustratedMonky 15 hours agorootparentprevIt also fails for in-elastic demand.Nobody while having a heart-attack stops and thinks about shopping around for a lower price. \"hmmm, about to die, let me see if I have any coupons\".Take insulin for example, companies can charge whatever they want, what are customers going to do? Just die? reply klipt 15 hours agorootparent> Nobody while having a heart-attack stops and thinks about shopping around for a lower price. \"hmmm, about to die, let me see if I have any coupons\".Well in theory that&#x27;s the point of insurance. You comparison shop insurance before having a heart attack, then if an attack happens you&#x27;re covered.Of course many people just wouldn&#x27;t buy insurance unless forced ... many breadwinners die leaving their families nothing because they don&#x27;t think of buying life insurance. reply snarkypig 13 hours agorootparentAbsolutely, I&#x27;m personally super excited about my ability to choose between my single work-provided health insurance that tries to maximize the amount of money they get for a doctor&#x27;s visit where the doctor doesn&#x27;t even try to help me.And I agree, we should punish those who don&#x27;t understand the system. Let them die for their ignorance. reply FrustratedMonky 12 hours agorootparentprevIf you get insurance through work, there is zero choice.You are locked into you&#x27;re companies decisions.You may say, &#x27;well you can opt out&#x27;, but the deck is really stacked against you trying to go it alone. This is one of the ways that employees are &#x27;locked in&#x27; to jobs and become immobilized from seeking better jobs. The insurance is a type of &#x27;capture&#x27;. reply faeriechangling 13 hours agorootparentprevFood is a terror-nightmare that can only be fixed by completely rethinking our values. We are literally seeing lifespan decline as food manufacturers addict their customers, capture regulators, destroy the environment, and pretty much single handedly reduce life expectancy. In many ways, we have regressed to the point of having a food system that works worse than it did under the better periods of communism.Electronics I think has been mostly successful as a regulated free market. reply DaveExeter 12 hours agoparentprev>The free market is the most efficient system we have for optimising profits.Um, the whole point of the free market is to destroy profit!To paraphrase Jeff Bezos, your profit is my opportunity. reply edgefield 16 hours agoprevI’m struggling to understand how the standard PE model works, particularly the debt component. PE firms seem to target mediocre businesses, load them with debt, and then harvest returns and eventually the assets. Why would a bank loan money for such an arrangement, given how often these businesses end up in bankruptcy (Toys R Us as an example)? Is the debt collateralized and bundled with better performing debt or sold off to an unwitting buyer? reply JumpCrisscross 16 hours agoparent> PE firms seem to target mediocre businesses, load them with debt, and then harvest returns and eventually the assetsPrivate equity no longer necessarily involves large amounts of debt, i.e. LBOs. Most managers have an economic effect they deploy to bring efficiency to an industry. The first popular one was deconglomeratisation. Then capital structure management. Digitisation and supply chain management followed. In each phase, they delivered then overcorrected.Hospitals initially started on the scale side. The thesis was that the biggest cost centre is administration, so if you linked together the administration of many hospitals you could reduce costs compared to private practices. This initially panned out. But hospitals are natural monopolies; those initial theses were rapidly corrupted.> given how often these businesses end up in bankruptcyPrivate-equity backed companies tend to be more resilient, not less [1].[1] https:&#x2F;&#x2F;siepr.stanford.edu&#x2F;news&#x2F;private-equity-firms-show-re... reply Projectiboga 16 hours agoparentprevIt is a mix of tax loopholes. They can load up on debt, pull cash out. The big scam is share buybacks, they use the loans to by up their own stock. That was illegal from the 30a-80s, the Regan administration legalized it and it&#x27;s been a race to the bottom. The other part is banks create money at multiples of their deposits. Fractional reserve lending. This creating money and then charging interest that isn&#x27;t created is the root of this mess. They system worked well when there was fresh land and slaves to grab. It has continued w industrial growth. But now we are running out of oil to pump and other resources are starting to become scarce. This whole private equity is becoming a shackle on our economic system, almost 24% of the economy is controlled by private equity, and a chunk of that is carry forward profits which aren&#x27;t taxed until withdrawn. They get to sell not pay capital gains and go into the next one. Good luck trying that yourself. reply bigbuppo 15 hours agoparentprevBuy the company, saddle it with the debt used to acquire it, take all its real estate and transfer it to a separate company, make the company rent from the company that owns the real estate at inflated prices. If its something like a software company you convert its software to a rental model and keep jacking up the price so that now an annual subscription costs the same as the perpetual license price from three years ago with 25 years of maintenance. Sure, you just lost 98% of your customers, but really, you just need one or two customers that can&#x27;t leave you to keep it running in perpetuity. In the case of a retail store, though, you just run the store into the ground over the next year or two and now your real estate division is the #1 creditor of the retail chain and gets first pick of the liquidation and it now owns the real estate, which it then sells for 10x its real value to a developer who will bribe the local approval board to turn the area that is zoned commercial only due to flood risk is now overpriced homes or condos. When everything floods the victims that bought houses or condos will have to move elsewhere.Or, your PE firm is actually owned by a foreign country and they don&#x27;t care about profit as their long term goal is wrecking your country&#x27;s economy. reply cameldrv 15 hours agoparentprevThere are different playbooks, but one is to take a declining business and cut unprofitable parts until it becomes (at least temporarily) profitable. The debt has a higher interest rate that reflects the risk of the strategy not being successful (hence being called junk bonds).Even in the case of bankruptcy though, the bondholders may still come out OK. Toys R Us actually made about 5 billion in debt payments in the years it was privately owned, and it took about 5 billion in debt as part of the buyout. The bondholders may not have gotten the return they were expecting, but they mostly got their principal back.All of that said, the real problem as I see it with PE is that it&#x27;s just so exploitative. The only thing that matters is the investor&#x27;s money. For example, one very common strategy after a buyout is to cut quality in various forms. People may have a positive quality impression of a store or a brand, and then keep buying it even after the PE company cuts quality. It takes them a while to realize that the product they&#x27;re buying is not what it once was, and so the PE firm is making money by tricking people into buying bad products.The hospital case is just an extreme example of this where the cuts in quality lead to people getting sick and dying. reply tdullien 15 hours agoparentprev\"it depends\" - but there are several components at play:PE benefited greatly from a long-term decline in interest rates. The amount of debt a company can service at a given profitability is directly related to the current prevailing interest. So as long as interest rates drifted down, PE firms could buy, load with debt to be paid out as dividend, and sell again, sometimes to the next PE buyer.Secondly, banks will not hold this debt directly on their books, but either sell bonds directly (the low interest environment led to some life insurers and other long term investors to buy pretty risky corporate debt) or repackage them with (hopefully) uncorrelated debt to obtain better ratings (price).There&#x27;s an argument that the success of PE funds had everything to do with them being a macro bet on falling interest rates. reply JumpCrisscross 15 hours agorootparent> an argument that the success of PE funds had everything to do with them being a macro bet on falling interest ratesIt’s incorrect. The leveraged buyout, for example, found its footing in the high-rate environment of the early 1980s. reply tdullien 15 hours agorootparentDoesn&#x27;t that have more to do with Michael Milken pioneering junk bonds, which then vastly expanded the available credit?As in: The fact that the 80s had comparatively high interest rates doesn&#x27;t matter to the argument, as the necessary infrastructure to issue and trade high yield corporate debt quickly didn&#x27;t exist - so in some sense effective interest rates dropped from infty to something, enabling the entire LBO model.And since then, with the exception of the recent hiccup, the long term trend in interest rates has been downward?What&#x27;s your read? reply JumpCrisscross 15 hours agorootparent> Doesn&#x27;t that have more to do with Michael Milken pioneering junk bondsCapital was uniquely available in the 1980s. But Milken was a symptom, not the cause. The booming American economy provided the fuel, but digitisation turbocharged the engine: issuing, pricing and trading securities, in particular bonds, became easier very quickly. (This is why your stereotypical trader from the 80s has an accent and is uncouth. They replaced blue-blooded bankers who had run bonds, calculating prices and yields by hand using tables.)Put another way, America is “unusually good at creating tradeable claims on the profits and revenues that its economy generates” [2]. Computers amplified that strength and prompted massive opportunities in reshaping the economy.> with the exception of the recent hiccup, the long term trend in interest rates has been downwardYes, this is a function of increasing stability and time horizons [1]. That said, the relevant frame is a fund lifespan, usually 5 to 10 years. (Unless you’re Warren Buffett.) In those intervals, the long-term signal is dwarfed by short-term noise.[1] http:&#x2F;&#x2F;www.economist.com&#x2F;news&#x2F;finance-and-economics&#x2F;21598651...[2] https:&#x2F;&#x2F;www.bankofengland.co.uk&#x2F;-&#x2F;media&#x2F;boe&#x2F;files&#x2F;working-pa... reply tdullien 12 hours agorootparentI haven&#x27;t checked in depth, but ... while the Fed funds rate has oscillated a bit (\"noise\") the curve for 10- or 30-year t-bills or mortgages is much smoother, and newly issued junk bonds will track those more than the Fed funds rate.And I&#x27;d be surprised if in the period between 1980 and 2021 you could find a 10 year interval that didn&#x27;t exhibit significantly lower rates at the end than at the beginning, and only a select few 5-year periods.You seem to have the viewpoint that this had nothing to do with the historical performance of PE funds? replyaquafox 16 hours agoprevI don&#x27;t understand why governments allow to privatize hospitals or other public ressources, often with the reasoning that otherwise they wouldn&#x27;t be profitable. We don&#x27;t expect other public services like fire departments or road construction to make money. Why should hospitals be treated differently? reply derbOac 15 hours agoparentMy sense (as someone with generations of family in healthcare) is, at least in the US, there&#x27;s a massive gap between what the healthcare system was like decades ago and how it functions now. At one point in the distant past there was probably a better balance of population size and number of providers, service demand and provider type, a greater number of more independent and locally run hospitals and clinics, less regulation about certain important issues, more permanent employee positions with more standard employer benefits, and so forth and so on.At least in some areas of the US there used to be public hospitals, run at the local (state or county level) but they were essentially privatized or defunded to the point where they had to close or become privatized.I could give lots of examples of this, but things now are less local, more controlled by centralized, private but national entities, more regulation, more consolidation of clinics and hospitals, and so forth and so on.I&#x27;m for putting more central resources in public control (government insurance, more public clinics and hospitals) but also believe there needs to be more deregulation of important elements and more competition to drive down costs on the supply end.In my opinion, there&#x27;s a lot of hidden factors driving up healthcare costs, in the form of insulating providers and hospital and clinic chains from competition, taking choice aware from consumers, decreasing transparency about costs, as well as \"hiding\" public funding of healthcare that&#x27;s already happening. It feels a bit like, in the US at least, we have this system that likes to pretend it&#x27;s 1960, at least insofar as doing so benefits certain people and groups. reply ImHereToVote 15 hours agoparentprevBecause there is no practical way for rich people to use luxury fire departments that deprive the filthy proles of protection, or luxury roads for that matter. reply 1vuio0pswjnm7 3 hours agoprevThere is a journalist who was covering this for years who once said in an interview that she could determine which ER Departments were owned by PE by searching the job listings for ER staff in the area. Ads placed by PE were easy to spot. She said when travelling she would deliberately avoid those hospitals. reply selimnairb 16 hours agoprevLBOs should probably be illegal. They seem to ruin everything they touch. Also, we should probably nationalize a lot of the entities PE has taken control of. Might be a nice way to back into nationalizing a lot of industries that have no business being private. reply szundi 16 hours agoprevThere are stuff that the government should doc, like public healthcare, military etc. reply aubanel 15 hours agoprevA datapoint I find impressive on the debate \"What should we privatize\": UK Rail privatisation seems to have worked really well, as shown in this graph: https:&#x2F;&#x2F;upload.wikimedia.org&#x2F;wikipedia&#x2F;commons&#x2F;9&#x2F;9f&#x2F;GBR_rail... reply deckiedan 11 minutes agoparentI suspect there are a lot of other factors to take into consideration here:- population growth over that time - percent of people able to afford cars and fuel over that time - % of those numbers that are in London ( and other big cities) - so the number of people able to afford to live in London compared to jobs there... Many people were priced out, and so have to commute, and pollution taxes and parking prices are now there to discourage cars - how much train line maintenance has been dropped, and will be coming up as needing replacement \"soon\" but not done - how much subsidy the private companies are actually getting still....Trains in the UK are really not great at all... Compared to the rest of Europe anyway. reply bigbuppo 15 hours agoparentprevHow is this relevant to private equity in healthcare being the equivalent of throwing patients into a meat grinder and then charging them for it? reply thinkingkong 16 hours agoprevIm endlessly fascinated by the cultural angle on technology of well being. We have enough resources to solve poverty, education, and access to medicine yet social determinism is the driving force in all these areas.There have been recent calls for the human population to hit a trillion people (by billionaires). The reasoning is we’d “have more einsteins” but personally Im way more worried about the genius physicist working in an amazon shipping center. reply armchairhacker 15 hours agoparent> We have enough resources to solve poverty, education, and access to medicine yet social determinism is the driving force in all these areas.Do we?We have enough food, but not the logistics to distribute it. I doubt we produce enough of some medicines because they have constant shortages. Whether we have enough education depends on how you define it: I believe we have enough to give everyone access to online resources, but we have an education crisis in the US which is driven by unmotivated students, and the factors which are causing the lack of motivation cost much more (old-style teaching, large classes with disruptive students, poor school infrastructure, poverty at home).It&#x27;s a social problem because I&#x27;m sure even if we had enough resources, there are people who would get in the way of distributing them, because they don&#x27;t want people to be educated or feel they don&#x27;t deserve assistance. Already we have countries which block parts of the internet, and countries at war which block aid. But I don&#x27;t think it&#x27;s only a social problem. reply stuartjohnson12 15 hours agoparentprev> We have enough resources to solve poverty, education, and access to medicineAccess isn&#x27;t binary, nor is it ever \"solved\". Every person in wealthy 1st world countries has access to enough food that they won&#x27;t starve, access to secondary education to 16&#x2F;18, and emergency medical care. I&#x27;m sure you would consider those things insufficient.A right asserted without any degree is pretty useless. reply lostlogin 16 hours agoparentprev> by billionairesI think you are right, the billionaires want minimum wage workers who are making less than a living wage. reply jmclnx 16 hours agoprevI remember my parents and Aunts&#x2F;Uncles talking about the new at the time trend of for-profit health care in the 70s. Back then all the hospitals were non-profit, many supported by religious groups.They pretty much predicted the current situation for care, but missing out a bi on the tech side of healthcare. reply chank 14 hours agoprevIs there any well known instance where a private equity firm taking over a business&#x2F;service resulted in a net positive for customers and not just the firm being a leech? reply sgnelson 4 hours agoparentI would add another caveat: \"and for the employees\" as one of the key plays by PE seems to be to just lay as many people off as they possibly can get away with. reply supertrope 13 hours agoparentprevDell went private for a bit so they could pivot without having Wall St complain about a few unprofitable quarters. reply api 16 hours agoprevIf we do enter an era of slowing actual growth, we will really have a problem with the financial industry trying to cannibalize the entire economy to find \"alpha\" somewhere.Everything in our economy could work just fine without a lot of growth except finance, which absolutely cannot work and collapses completely unless number always goes up forever. reply lotsofpulp 11 hours agoparent>Everything in our economy could work just fine without a lot of growth except financeWhich includes all taxpayers, since all those government employees are paid with the \"growth\" in the form of underfunded defined benefit pension and retiree healthcare. Or the people counting on their 401k numbers to go up. reply hnbad 16 hours agoprev> \"When health systems buy hospitals, they generally do not use borrowed money,\" said Song, who is also an internal medicine physician at Mass General. \"In contrast, the classic private equity buyout uses a small amount of cash, but a large amount of debt.\"This is a red herring. Health systems have different metrics for success than \"the market\". The former, ideally balances QoC with costs. The latter by definition only cares about ROI in terms of profits (either from revenue or from playing hot potato with other buyers). There&#x27;s no reasonable argument why we should expect private equity to result in anything but a degradation of QoC if it doesn&#x27;t have a considerable negative impact on profitability or resale value (and that&#x27;s even ignoring situations in which a loss may be desirable or acceptable). reply spacedcowboy 16 hours agoprevtbh, my experience is the other way around. When a hospital locally ran out of money and was taken over by the county, the quality of care dropped drastically.My wife had a good experience giving birth in our local hospital 12 years ago, but this year, after having been taken over, they completely screwed her brain up in what should have been a routine procedure. 10 months later and she’s still suffering. i’m not sure she’ll ever recover :( reply michaelmrose 15 hours agoparentFirst let me say I&#x27;m sorry for your wife and I hope she recovers. Respectfully that hospital may not be an apt comparison because it was taken over because it was already failing under private ownership whereas PE is more apt to buy something because it is functional and therefore profitable.It seems likely that both causes are an example of failure of private ownership wherein the county is being blamed only because it was left holding the bag. reply nine_zeros 16 hours agoprevWhen the metric is \"return on investment\" let me assure you that all the PE investors, execs, managers, individuals involved are only looking to eek out more profit - quarter after quarter.If it ever comes down to helping a patient (or employee) vs a quarter with lower than expected growth, they will predictably not help the patient (or employee) in favor of meeting that quarter&#x27;s target. reply pierat 16 hours agoprevWhy aren&#x27;t VC&#x27;s and PE also found directly complicit when physicians and nurses are found guilty&#x2F;liable of $badthing?In the US, we love faulting individuals doing $badthing, but refuse to look at policymakers when they knowingly do so.And in the end, the policymakers play musical chairs or Pokemon (gotta buy them all), and do the same company after company. reply pc86 16 hours agoparentI&#x27;m trying to take this comment as an honest question in good faith and not as political bait or \"rich ppl bad.\"Some random PE investor or analyst isn&#x27;t the one putting needles in people&#x27;s arms or writing prescriptions, so does it make sense to send the analyst to jail when someone dies from negligent causes in a hospital? I don&#x27;t think it does. It probably doesn&#x27;t make sense to send anyone to jail unless there&#x27;s actual malice involved but if you&#x27;re looking to punish someone for something it probably makes sense to look first at the person actually pushing the drug or doing the procedure. reply asadotzler 15 hours agorootparentWhen PE takes over and cuts resources or otherwise \"optimizes\" leading to bad outcomes, the medical staff are on the hook for those bad outcomes but the PE firm that caused the cuts that caused the bad things are not. Make sense? reply pierat 15 hours agorootparentprevLet me see if I can elaborate further.PE company comes in to a med facility. We have proof that when they move in, they make new policies. These policies have a demonstrable proof that they reduce standard of care, cause X% more incidents, and Y% more deaths.From the article is:> After a hospital was acquired by private equity, admitted Medicare patients had a 25% increase in hospital-acquired complications, compared with patients admitted before acquisition. Patients also had 27% more falls and 38% more bloodstream infections caused by central lines, which are temporary surgically inserted ports that allow easy intravenous access for patients receiving repeated drug infusions or other treatments.We have demonstrable proof that policies put in place have directly caused this harm. I&#x27;m not trying to alleviate blame when a med professional does something wrong and causes an injury. However when we see increases like \"25% increase in hospital acquired complications\", tells me that there is underlying policy that caused a significant increase in injuries&#x2F;deaths.We can point towards a policy that causes mass injury, yet the policy people are beyond reproach. Instead, the 25% more injuries are looked at as an individual bad action. Now we should be looking at the individuals and how their decisions caused a bad thing to happen.... however when we see trends from multiple people towards the same reasoning, and the reasoning is \"policy\", then absolutely those policymakers should be in scope.I would also look at policy decisions of 2nd derivative effects too. However those can take quite a bit of time to appear. Again, if policymakers want the responsibility of policy, ethics, legality, and direct result of policies should also be what they look at and are responsible for. reply fallingfrog 13 hours agoprevWhat?? Why are we allowing private equity to take over hospitals? That’s a life and death public service! You don’t just throw it to the wolves like that. reply turnsout 16 hours agoprev [–] Wait, I thought the free market would solve everything! &#x2F;s reply skfingngihg 16 hours agoparentWhy? You can’t even build a hospital or practice medicine without government permission. It’s an industry ripe for regulatory capture. reply lostlogin 16 hours agorootparentAre you suggesting that less regulation would make healthcare better? reply pc86 15 hours agorootparentI think they&#x27;re pointing out that heavily regulated industries are very easy to fall prey to regulatory capture, which benefits incumbents at the expense of users and those trying to enter the industry from outside. Healthcare in the US is nowhere close to a free market, so it seems perfectly reasonable to question the GP if it&#x27;s asked in good faith (it&#x27;s not). reply JumpCrisscross 16 hours agorootparentprev> suggesting that less regulation would make healthcare better?No, but more competition would. If we want to have private healthcare, we have to actually let free markets work. Our current system is the worst of both worlds. reply FireBeyond 15 hours agorootparentprev> You can’t even build a hospital ... without government permission.You are talking about Certificates of Need which are investigations to show that an area needs a new facility...The concept of a CoN was lobbied for by hospitals and healthcare providers who wanted \"guarantees\" of profits before they&#x27;d commit to building new facilities. The CoN process helps existing facilities get a say in whether a new hospital should be allowed in the area. The government itself generally doesn&#x27;t care. reply jokethrowaway 16 hours agoparentprevI&#x27;m sure hospitals owned by private equities still have regulations and face little competition.They just shouldn&#x27;t exist.It would be interesting to compare small private clinics performance vs hospitals. reply TarasBob 16 hours agoparentprev [–] It does if the government is not involved. reply seadan83 15 hours agorootparentThe most pure form of free-market is a black market. Without any government, the free market does not have a mechanism to prevent monopolies (AKA absolute winners). Further, many markets do not even exist without the government. EG: there is no Tesla without the department of Energy [1]. The housing market would be totally different without Freddie & Fannie Mae, the banking system would also be totally different without FDIC insurance.Capitalism creates bubbles & winners (aka monopolies). Government has to be involved to keep an even playing field and to smooth out the bubbles. No industry in the US has had no government involvement. All industries are involved with government in some form - if only to enforce contract law, or for more physical matters like using public roads, buying electricity from government run electrical companies, benefitting regulation that requires phone providers to provide service, etc..[1] https:&#x2F;&#x2F;www.energy.gov&#x2F;lpo&#x2F;tesla reply nobodyandproud 16 hours agorootparentprevAny actual data to prove that assertion? reply thefaux 16 hours agorootparentprev [–] How exactly does the market work without the government enforcing contracts and property rights? reply xyzzy4747 16 hours agorootparent [–] Lots of cryptography and security guards? reply nobodyandproud 16 hours agorootparent [–] So threat of violence? In other words, a defacto government? reply xyzzy4747 16 hours agorootparent [–] The most profitable companies can afford the most security. It would probably work to some extent. I imagine it would mostly be for defensive purposes and to protect employees but you never know. reply JumpCrisscross 16 hours agorootparent> most profitable companies can afford the most security. It would probably work to some extent.Monopoly on violence requires enforcing that monopoly. Such a firm would also stomp out its competition. You’re describing government-run healthcare by an unelected government. reply seadan83 15 hours agorootparent> You’re describing government-run healthcare by an unelected government.Government run healthcare, the healthcare where you just go to the hospital and then you just get treated.. (?) Government run healthcare does not have a great reputation for doing cutting-edge health care, but on the other hand it does have a good reputation for taking care of people for basic needs. How is that describing a monopoly on violence? Repairing broken bones is the same as breaking bones? (If the implied argument is that health care is so heavily rationed, that people are left untreated, tantamount to violence, I would beg you to look at the statistics of which countries have more complications due to untreated illnesses (hint, the US is high on that list [1]; or we could look at health outcomes per dollar spent, US is very far down on that list, US spends multiples of other countries for the same outcome))I would think a more apt description and example of a monopoly on violence would be narco-states. Otherwise the example is a well functioning government, and that is demonstrated in the police force and judicial system (not healthcare).When considering the absence of the government monopoly on violence, drug markets are the closest thing there is to a true free-market. And yes, there contract enforcement is contingent upon a bunch of people standing behind you with guns.[1] https:&#x2F;&#x2F;www.cbsnews.com&#x2F;news&#x2F;medical-care-costs-americans-sk... reply nobodyandproud 15 hours agorootparentIn your haste, I think you missed a key bit: “… by an unelected government.”Or do you mean an unelected government has a good track record of meeting basic needs as well? reply nobodyandproud 16 hours agorootparentprev [–] Sure but as a profitable company not accountable to customers, what stops them from giving poor service at a premium? reply xyzzy4747 16 hours agorootparent [–] The customers would buy less of their services probably. reply seadan83 15 hours agorootparentThat implies having a choice to buy less or more of a service. Healthcare often does not give that choice. reply krapp 15 hours agorootparentprev [–] Why would companies that claim their own monopolies on violence and have no laws to answer to other than their own allow customers to do so? What&#x27;s to stop companies from blacklisting such customers or collaborating to fix prices?What&#x27;s being described here is what anarcho-capitalism and pure free markets always devolve into - the mafia. You don&#x27;t get to just shop around for a better mafia if you don&#x27;t like the one running your town. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "A nationwide study conducted by Harvard Medical School reveals that the quality of care diminishes after hospitals are acquired by private equity firms.",
      "Patients in these hospitals are at a higher risk of preventable conditions and adverse outcomes, such as falls and infections.",
      "The findings highlight the potential conflict between profit-driven motives and patient care and safety, raising concerns about the impact of private equity in healthcare."
    ],
    "commentSummary": [
      "Private equity firms have an impact on hospitals and the healthcare industry, with concerns about a decline in the quality of care after acquisitions.",
      "There is a debate about the pros and cons of privatization in various industries, including healthcare, and the role of government regulation in this context.",
      "The efficiency of private equity and the challenges of implementing a free market model in healthcare are also discussed, along with potential negative consequences of profit-driven motives in the sector."
    ],
    "points": 217,
    "commentCount": 139,
    "retryCount": 0,
    "time": 1703609659
  },
  {
    "id": 38770693,
    "title": "UK Achieves Historic Carbon Emission Reduction, Unnoticed by Public and Politicians",
    "originLink": "https://www.spectator.co.uk/article/uk-becomes-first-g20-country-to-halve-its-carbon-emissions/",
    "originBody": "Coffee House Fraser Nelson UK becomes first G20 country to halve its carbon emissions 26 December 2023, 9:47am Text settings Text size Small Medium Large Line Spacing Compact Normal Spacious Comments Share Share Fraser Nelson UK becomes first G20 country to halve its carbon emissions Copy link Linkedin Messenger Email A major milestone has just been passed: Britain has become the first major country to halve its carbon emissions. The rapid pace of UK environmental progress means that our output is now below 320 million tonnes – less than half the 652 million tonnes of our 1970 peak. This is in spite of Britain now having a far larger population than 50 years ago and an economy more than twice the size. Had things gone the other way – if our carbon emissions had doubled, for example – this would be front page news. But I’m not sure you can expect to read about this good news anywhere other than The Spectator. There are no campaign groups tracking it, no politicians likely to trumpet it. The info is tracked by the Global Carbon Project and is one of many metrics collected in the energy section of The Spectator data hub. Here it is, showing (as you’d expect) a drop during the lockdowns, but by 2022 showing a drop driven by efficiencies that takes our emissions lower in a normal year than they were when the economy was being shut down. Already a subscriber? Log in Keep reading with a free trial Subscribe and get your first month of online and app access for free. After that it’s just £1 a week. There’s no commitment, you can cancel any time. SUBSCRIBE Try a month free Cancel any time Or Unlock more articles REGISTER Written by Fraser Nelson Fraser Nelson is editor of The Spectator Comments Share Share Fraser Nelson UK becomes first G20 country to halve its carbon emissions Copy link Linkedin Messenger Email Topics in this article Politics Society",
    "commentLink": "https://news.ycombinator.com/item?id=38770693",
    "commentBody": "UK becomes first G20 country to halve its carbon emissionsHacker NewspastloginUK becomes first G20 country to halve its carbon emissions (spectator.co.uk) 205 points by beejiu 23 hours ago| hidepastfavorite259 comments ricardo81 22 hours agoAs noted in another comment, some of this can be attributed to manufacturing being done elsewhere and the UK becoming more of a service economy.Though there are indicators like emissions per kWh of electricity generated that show promise. https:&#x2F;&#x2F;grid.iamkate.com&#x2F; ... over the past 10 years the value has went from 475g of CO2 per kWh to ~153g. A contentious part of that value no doubt is that biomass imported from North America is considered carbon neutral. Add in the complications of new growth vs old growth, transportation and the time-sensitive nature of man-made climate change it can perhaps be seen as less advantageous as figures may suggest.All the same I&#x27;m optimistic, particularly in Scotland where a disproportionate amount of green electricity is harvested by population. Our National Grid is playing catch up trying to accommodate intermittent sources and price accordingly. reply dukeyukey 4 minutes agoparent> some of this can be attributed to manufacturing being done elsewhere and the UK becoming more of a service economy.Worth remembering the UK still manufactures more than France, and had a comparable amount of manufacturing as a percentage of GDP as other Western countries. reply ajsnigrutin 21 hours agoparentprev> As noted in another comment, some of this can be attributed to manufacturing being done elsewhere and the UK becoming more of a service economy.This would be an interesting metric... instead of carbon emissions being calculated by the location of \"chimneys\", a better metric would be a total carbon emissions by all consumpton of the country. Sure, EVs are clean if you buy them in (eg.) UK, but all the polution is done elsewhere, from the dirty lithium production and dirty rare earths production, energy intensive metalworks, etc., but the car is still bought and used in UK, and the UK consumer is the cause for those emissions.Also many even local policies don&#x27;t take that in effect, because many things where we &#x27;force&#x27; users into &#x27;greener alternatives&#x27; are not so green if you count the manufacture (which is done elsewhere, and again, noone cares about that). In many cases, extending the life of a device (car, appliance,...), even with bad energy usage and&#x2F;or emissions is still greener (looked globally) than buying a new appliance&#x2F;car and disposing of the old. Sure,the new small EV is \"green\", but driving your old VW golf for 5 more years could have been greener (again, depends on the distance driven, etc.). reply helsinkiandrew 21 hours agorootparent46% of &#x27;British Peoples&#x27; carbon emissions are released overseas (2016)https:&#x2F;&#x2F;www.wwf.org.uk&#x2F;sites&#x2F;default&#x2F;files&#x2F;2020-04&#x2F;FINAL-WWF...> In 2016, 54% of the UK’s carbon footprint was domestically sourced with the remaining 46% coming from emissions released overseas to satisfy UK consumption. The overseas proportion of the UK’s carbon footprint increased substantially – from just 14% in 1990 – thus reducing the scope of UK climate policy to affect emissions associated with consumption. reply xorcist 19 hours agorootparentprev> if you count the manufacture (which is done elsewhere, and again, noone cares about that)Please stop feeding that myth. There is a lot of anti-green propaganda built on it. The area of research is called lifecycle analysis and is one of the most heavily studied recently, precisely because we need facts to base an energy transition on. There is plenty of published and reviewed research. reply dan-robertson 21 hours agorootparentprevThe search term you’re looking for is “consumption based emissions”. There the uk in 2021 was down >30% from its peak, though the number for this year may be lower. Example: https:&#x2F;&#x2F;ourworldindata.org&#x2F;grapher&#x2F;consumption-co2-emissions... reply gambiting 21 hours agorootparentprev>>the UK consumer is the cause for those emissions.Sure, but counting it as UK emissions would mean you count it twice, no? If other countries that mine lithium report their own CO2 emissions then if you add the CO2 cost of using electronics made with lithium and used in UK to UK&#x27;s CO2 emission, that means you have counted the same emissions twice,no? reply kitbrennan 20 hours agorootparentWhile there&#x27;s a lot of nuance to this: in carbon accounting it is standard practice to account for the same emission more than once. Carbon inventories are broken out into three different scopes, the first two scopes concern fuel and electricity generation emissions, and the third scope includes everything in your value chain (both upstream and downstream). Therefore the country producing the EV battery would report on the emission in their Scope 1 and Scope 2 inventory, and the country utilising that EV battery would report it within their Scope 3 inventory.It may seem odd to double account, but the goal of carbon accounting is not to ascribe blame to an emission (since ascribing blame is a never-ending game of finger pointing), it is to make every business&#x2F;consumer responsible. The country creating the emission needs to be incentivised to decarbonise, and the country consuming that emission needs to be incentivised to decarbonise their full - including international - supply chains. reply belorn 17 hours agorootparentThey do the same double accounting for green credits. If a country produce green energy it get accounted there. If they then sell that energy it get accounted a second time by the country who buys it. The exporter (possible a separate company from the producer) can even account it a second time, as can the consumer in the end.Basically every time green energy has changed hand there are two new credits being created, and with a international energy market there is a lot of opportunities for energy to change ownership. reply gambiting 20 hours agorootparentprevAh - that&#x27;s really interesting, didn&#x27;t know this. But that means that UK should already be counting CO2 emissions of products sold there(where known), right? reply kitbrennan 20 hours agorootparentYes, this is where the *nuance* mentioned at the start of my reply comes in.It *should* include all scopes, including upstream emissions from purchased goods and services from abroad. But in many countries their country-level carbon inventories still have huge gaps.UK Legislation implements the GHG Protocol scope system in the UK&#x27;s carbon accounting regulations that businesses must follow for reporting their emissions (e.g. SECR), and government guidance for calculating emissions all follow the scope system too (e.g. BEIS Conversion Factors guidance). So it is very disappointing if the UK&#x27;s carbon accounts has got gaps (but I wouldn&#x27;t be surprised). reply IMTDb 18 hours agorootparentprev> It may seem odd to double account, but the goal of carbon accounting is not to ascribe blame to an emissionIt&#x27;s definitely how it is used by activists &#x2F; NGO etc tho [1].Another strange practice is to scale the CO2 production with the price of goods. Eg: it is assumed that you produce 10x more CO2 when you buy a $100 bottle of wine than if you buy a $10 bottle of wine. This makes no sense at all from an environmental perspective, but the conclusion you can draw from this are aligned with the political views of the people producing those reports.1: https:&#x2F;&#x2F;www.oxfamamerica.org&#x2F;explore&#x2F;stories&#x2F;top-5-ways-bill... reply tialaramex 18 hours agorootparentSure, for example I had accounting done as part of a study which also fitted meters (which I wanted anyway) and read the data (which I would likely have freely agreed to but obviously does need formal permission) and their baseline basically go well, about 60% of your income isn&#x27;t accounted for in these days so we assume you turned that money into carbon emissions at our default rate. Actually the money was just sat in a bank account, which AFAIK doesn&#x27;t cause net carbon emissions.For individuals I don&#x27;t think this approach works very well, but over a population I can believe it comes out in the wash. reply KMag 17 hours agorootparent> Actually the money was just sat in a bank account, which AFAIK doesn&#x27;t cause net carbon emissions.That depends on your definitions. I&#x27;m not sure about U.K. reserve ratios, but in the U.S., about 90% of your bank deposit goes back out as loans that hopefully increase economic activity. The majority of that activity probably isn&#x27;t carbon-neutral. reply okeuro49 15 hours agorootparentNeither the UK nor the US have a reserve requirement.https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Reserve_requirementIn addition, banks in modern times have never had to wait for deposits to extend loans. reply KMag 13 hours agorootparentHow did I miss the reserve requirement in the US going to zero in 2020? I was living overseas at the time, and I know I had more pressing things to consider, but still...> In addition, banks in modern times have never had to wait for deposits to extend loans.Are you referring to the discount window, the repo market, and interbank lending, or the fact that reserve requirements are typically averaged over a couple of weeks? In these cases, (as long as there&#x27;s a reserve requirement), a person adding to a bank does still enable more lending, no? Or, are you saying that in modern times, even with non-zero reserve requirements, commercial banks usually operate with excess reserves, so reserves are not the limiting factor on lending? reply okeuro49 1 hour agorootparentThis is what I read from the Bank of England:> The reality of how money is created today differs from the description found in some economics textbooks> Rather than banks receiving deposits when households save and then lending them out, bank lending creates deposits.https:&#x2F;&#x2F;www.bankofengland.co.uk&#x2F;-&#x2F;media&#x2F;boe&#x2F;files&#x2F;quarterly-... replyLargeTomato 11 hours agorootparentprevOn the 1x vs. 10x point, I believe the unit quantity is derived by finding the manufacturing costs (in CO2) for a given process and then extrapolating. No one is counting the grams of CO2 of your cheeseburger. They&#x27;re counting the CO2 of the whole farm. reply mirekrusin 21 hours agorootparentprevIt needs to be done at planet level, otherwise it&#x27;s emission shifting, not reduction. Moving it from one place to another on the same planet doesn&#x27;t reduce emissions. reply abhaynayar 20 hours agorootparentprevIronically you have written \"twice, no?\" twice. reply patmorgan23 21 hours agorootparentprevIf you are trying to total all CO2 admissions? Yes.If you&#x27;re trying to determine which countries have the biggest impact it could be a valuable metric. reply dan-robertson 21 hours agoparentprevTrade adjusted emissions are also down. The percentage due to trade has increased over time: https:&#x2F;&#x2F;ourworldindata.org&#x2F;grapher&#x2F;share-co2-embedded-in-tra...Note: 1. the US hasn’t seen such a big change so this thinking doesn’t really carry over to there.2. The chart is a percentage of domestic production so decreasing domestic production while holding emissions due to trade constant would make the line go up. reply throw0101b 21 hours agoparentprev> As noted in another comment, some of this can be attributed to manufacturing being done elsewhere and the UK becoming more of a service economy.Consumption-based emissions are also down for the UK from a peak of 750 Mt down to 500 Mt:* https:&#x2F;&#x2F;ourworldindata.org&#x2F;consumption-based-co233% is still pretty good. reply asdajksah2123 19 hours agoparentprevBritain has been one of the highest trading nations in the world for centuries. That&#x27;s what their colonies were for.So I don&#x27;t think it&#x27;s as simple as British manufacturing all having moved overseas. A lot of British consumption was being manufactured overseas even before the last few decades.It&#x27;s entirely possible that this is because of that, but I don&#x27;t think one should be so quick to jump to that conclusion specifically for a country that is famous almost entirely for their \"Empire\". reply riffraff 17 hours agorootparentBritish manufacturing is still in decline, e.g. Bloomberg published a graph yesterday showing how the British car industry output shrunk by half since Brexit in 2016. reply nvm0n2 12 hours agorootparentBrexit didn&#x27;t happen in 2016, just the vote to begin the process, so the fact that Bloomberg are making this claim is misleading. Additionally when Brexit actually did get implemented it&#x27;s had no effect on trade, in fact trade with the EU is up slightly. But that&#x27;s all expected for anything to do with the UK in the media these days. They will keep lying and manipulating people on this topic forever. reply kolektiv 12 hours agorootparentYou don&#x27;t need a final legislative change to impact inward investment and manufacturing location when everyone knows it&#x27;s coming. The Brexit vote hit investment confidence and manufacturing long before the final withdrawal agreement because the writing was on the wall and manufacturers avoid risk.\"No effect on trade\" is definitely not true, I&#x27;ve personally had multiple exporting clients go bust due to trade restrictions and even if their volume of trade is exactly replaced, the balance and make up of trade is different (it isn&#x27;t just that, however). reply nvm0n2 11 hours agorootparentNo it didn&#x27;t. The economy was growing at the time of the vote and continued to grow after it, a bit faster even. Unemployment hit record lows.You may be thinking of the predictions of the remain campaign which very heavily featured the idea that even the vote itself, without any actual legal changes, would trigger a recession due to \"uncertainty\". But that was wrong, it never happened.Check the trade stats for yourself. Trade is down but that&#x27;s because of the huge damage of lockdowns and other terrible policies. Brexit would show up as a reduction in trade with the EU faster than elsewhere, but that didn&#x27;t happen, the proportion remained about the same. This data comes from the government itself. Seeing any Brexit effect in the recorded data just isn&#x27;t possible.That&#x27;s surprising given that even most leave campaigners accepted that there would be some impact, no but in the end it seems the single market was basically a mirage. It never did much for services which is the bulk of the UK economy.Again, if you don&#x27;t believe me, check the data for yourself. reply duckmysick 11 hours agorootparentprevThis report from the UK Parliament, says otherwise: https:&#x2F;&#x2F;commonslibrary.parliament.uk&#x2F;research-briefings&#x2F;cbp-...The report admits that the trade was affected by Brexit and other factors and it&#x27;s difficult to separate one cause from the other. The report also notes that the way data is tracked and compiled changed since Brexit.The trade with the EU in pounds was highest in 2022, and lower in 2020 and 2021 compared to 2019. The report says it&#x27;s not adjusted to inflation. In terms of % of the total trade, it&#x27;s the lowest ever in 2022. reply vondur 14 hours agorootparentprevI assumed that the colonies were there to provide the raw materials which were shipped to British factories. The finished goods were then exported to the colonies and other countries. reply flohofwoe 21 hours agoparentprevI wonder if it&#x27;s a similar effect like after the German reunification, but caused by Brexit instead? Total emissions for Germany as a whole dropped by around 35% during the 90&#x27;s because the non-competitive East German industry was essentially shut down overnight. 35% emission drop looks great in headlines, but only if one ignores the economic side effects (of turning the lives of 16 million people upside down).PS: looking at the graph the sharp downward turn already started in 2006, so probably not Brexit... reply tonyedgecombe 17 hours agorootparentIt really started with the \"dash to gas\" [1] in the nineties. We had gone through some painful labor disputes in the coal industry which the government was keen to avoid. Alongside that the generating industry was privatised. This encouraged a lot of investment in gas. Also we were getting a ton of grief from Norway about all the acid rain from sulphur which had to be dealt with [2].The Tory government which has been in power since 2010 was somewhat sceptical about wind power and shut down new onshore development when they came in. Fortunately their coalition partner negotiated the continued development of offshore which has been huge since then.[1] https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Dash_for_Gas[2] https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Acid_rain reply LargeTomato 11 hours agorootparentHi, sorry but how can the Tories have coalition partners? The UK has 3 major parties and the other 2 are generally the coalition, right? I&#x27;m not too familiar with UK politics. reply midasuni 22 hours agoparentprevBiomass is a small impact. The next big impact will be ever increasing wind power and the shift from gas to electric for heating. reply londons_explore 22 hours agorootparentDrax is huge, and now burns canadian imported wood rather than coal which is was built for because it can be considered carbon-free. reply tonmoy 22 hours agorootparentFarmed wood should actually be considered carbon neutral. Other than transportation cost and the carbon emitted during harvesting, it’s a legitimate way of capturing carbon from the atmosphere and then just emitting the same carbon back reply robin_reala 22 hours agorootparentThe problem is that at this point we need to be actually capturing carbon and not re-emitting it. reply gambiting 21 hours agorootparentCapturing carbon is literally a fantasy. Other than growing forests there is no hope we can invent a technological way out of our carbon emissions by capturing them. Companies which promise otherwise are essentially a scam.And growing forests is a viable option but the problem is that the carbon is only captured for as long as you don&#x27;t burn the wood or let it rot, it has to be either buried or turned into things which ultimately won&#x27;t release their CO2 back into the air. reply shaky-carrousel 19 hours agorootparentGrow a forest, convert the wood into charcoal, then store it in a salt mine. reply Retric 18 hours agorootparentConverting to charcoal is pointless, just burry biomass which is how we got fossil fuels in the first place.A dump beats every commonly proposed method of carbon sequestration because people pay you to take it and all you need to do is prevent that carbon from entering the atmosphere. reply _3u10 20 hours agorootparentprevIf we can increase our CO2 concentrations in the atmosphere we can become carbon neutral faster as increased concentrations drive forest growth and allow plants to grow in previously desert regions. reply youngtaff 18 hours agorootparentprevFarmed wood shouldn’t be considered carbon neutral as there’s transportation and other factors to includePlus we should avoid burning captured carbon reply ricardo81 22 hours agorootparentprevDrax which burns biomass is self-styled as \"the UK&#x27;s largest renewable power station\" reply midasuni 22 hours agorootparentRight now it’s generating less power than Solar - in the depth of winter, 8%Gas is 50% higher at 12% and that doesn’t include the gas usage in keeping homes warmWind (32%) imports and nuclear are all higher than gas for the electricity generation, let alone imported wood generation. reply ricardo81 22 hours agorootparentIt&#x27;s not ideal to compare at any point in time. It&#x27;s a clear day (and midday), not long after a storm has passed generating record levels of wind energy - during a national holiday when demand is always low (and prices often go into negative territory)I&#x27;m not disagreeing what&#x27;ll be the next impact, just saying Drax and biomass is perhaps worth more than the statement of it being &#x27;small impact&#x27;. reply distcs 21 hours agoparentprev> As noted in another comment, some of this can be attributed to manufacturing being done elsewhereThis is an excellent point. Are there reports similar to TFA that accounts for this? I mean a report that calculates the total demand a country is putting on carbon emissions (directly or indirectly) will be more insightful. reply londons_explore 22 hours agoparentprev> Our National Grid is playing catch upNah - they&#x27;re thinking of playing catch-up in 5-10 years time when they&#x27;ve done the paperwork to get permission to start catching up... reply baby 21 hours agoparentprevThere’s a real problem of transparency and opacity of actual numbers when it comes to contribution to global warming, diseases, and other issues at scale. It’s a shame and it’s not clear how we can do better without some real technological breakthrough in measurement reply _3u10 20 hours agorootparentMaybe just measure CO2 in the atmosphere? And maybe put international pressure on the largest emitters? (China and India) reply netsharc 19 hours agorootparentChina is where Walmart gets their cheap crap (and Apple their iPhones ), tax the CO2 and the people shopping at Walmart get mad that everything&#x27;s more expensive. Slash, the manufacturers look for other countries where maybe the CO2 expulsion is under the limit, or the reporting isn&#x27;t that robust... Because hey if they&#x27;re not competitive price-wise, the customers look elsewhere for their cheap crap... reply snowpid 19 hours agorootparentje vous presente https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Carbon_Border_Adjustment_Mecha... reply thworp 19 hours agorootparentprevAh yes, clearly it&#x27;s only those people shopping at Walmart that would not be happy about price increases. More than that, it&#x27;s only that cheap crap that they buy that would get more expensive. Enlightened people like you will just stand above it and buy nothing but organic, locally sourced crap! reply _3u10 18 hours agorootparentWith a high carbon footprint due to the price local labour is paid and the amount of stuff they purchase. When you buy from China and India you know people are only getting paid enough to just buy food and a small place to live, drastically reducing their carbon footprint. reply nvm0n2 12 hours agorootparentprevThat&#x27;s measured already since the 60s. You never see any reference to it because what it shows is a constant rate of increase that is entirely unaffected by literally anything you can think of. Rise of China? No impact. Lockdowns and the global air travel shutdown? No impact. Renewables revolution? Invisible. Great recession? Of course not. And all the international conferences and agreements.... well, you get the picture.This fact is discouraging which is why the media and government enforce a blanket silence about it. Along with many other aspects of climate science. reply mytailorisrich 21 hours agoparentprevI note that the article mentions that the peak was in 1970, perhaps earlier than most countries.I think that this may be linked to North Sea gas and conversion from coal to natural gas. And also the decimation of industry that occurred during the 70s and 80s. reply ricardo81 21 hours agorootparentIndeed, great points.Generally these articles use 1990 as a frame of reference. To be fair based on the initial graph shown alone, the UK has almost reached 50% of 1990 values also, though perhaps other countries have already achieved 50% of 1970 values. reply Podgajski 18 hours agoparentprevThere’s a Chinese saying, that if you apply weights and measures people will use the weights and measures to cheat and steal.This time we stop measuring emissions and just focus on reducing emissions. And that would mean reducing consumption. reply pjc50 18 hours agorootparentI&#x27;m as much of an anti-Taylorist as anyone, but how do you even know whether emissions are reduced without measurement?Reducing \"consumption\" (of what? measured how?) is probably impossible in a democratic economy - that&#x27;s known as a recession and tends to cause a change of party. reply Podgajski 17 hours agorootparentWe have a seal what increases emissions: driving, flying, buying things, heating, cooling,… I mean, it’s pretty obvious.Focusing on those, rather than trying to quantify, the emissions, released, is much easier. It’s much easier for me to know how much I buy then to try to find out how much admissions are omitted between the things I buy.For example, trying to figure out the emissions created by buying a Tesla for a high mileage, gas powered car. What really matters most? The best answer is buying neither of them. Second best sensor is buying and driving it much less, those both matter more than how much admissions are created by the creation or use these vehicles.They still arguing about how much emissions are caused by elect versus high-efficiency, gas, powered vehicles. In fact, we have something that easily quantifiable versus something that’s almost unquantifiable.We all just need to do a lot less. But that’s why people focus on emissions and not use, because it’s the thing they don’t want to hear. reply physicles 17 hours agorootparent“We just need to do less”“But that’s called a recession. It won’t happen”“WE JUST NEED TO DO LESS”For better or worse, in a democracy anyone who tells the populace to scale back their lifestyle will not get elected. “We need to do less” is not a solution. It’s wishful thinking.What other solutions are there? Make our current lifestyle less carbon-intensive over time. Yeah it’s not as good as perfect collective action, but it has the advantage of actually being possible. replydoingtheiroming 19 hours agoprevReading the comments, it seems that absolutely no one is happy about this. It is either a lie, insufficient or a pointless response to a non-problem.But it’s a clear achievement. An achievement with lots of nuance. And one that runs counter to many strongly held and arguably ideological positions on all sides of the political spectrum.Reading responses to it here and on Twitter has been rather depressing. reply ZeroGravitas 17 hours agoparentIt&#x27;s not helped that the messenger is a highly polarising source that piggybacks on the facts to a) attack the mainstream media as unpatriotic liars and b) downplay the severity of climate change that they&#x27;ve been actively denying, because of the success of policies they&#x27;ve actively fought against at every turn. reply nvm0n2 12 hours agorootparentThe Spectator often makes valuable points that are hard to accept but true. One of them is that British climate efforts are pointless, it accounts for such a tiny percentage of world emissions that it could drop to zero and be a rounding error. Therefore the \"success\" of this policy is a mirage. Yes the UK had reduced emissions. No it won&#x27;t have any effect on the climate. Both these things can be true simultaneously: the policy succeeded and failed. reply ZeroGravitas 31 minutes agorootparentNo, it&#x27;s cynical sophistry aka bullshit.Meanwhile, The Spectator has employed some of the worst climate deniers in british journalism, who specialize in the production of exactly that commodity.https:&#x2F;&#x2F;www.desmog.com&#x2F;james-delingpole&#x2F;> James Delingpole announced in The Spectator that he was going to “put his money where his mouth is” and invest in a fund named Cool Futures with the aim of short-selling renewable energy stocks. Delingpole describes climate change as an “outrageous scam” and says he will bet “on the Big Short principle” and call “this rigged market’s bluff.”...In June 2016, Joanne Nova finally published about the new fund on her own blog, asking readers to donate money to get the Cayman Island-based hedge fund started. The Cool Futures Fund Management needs at least $375,000 to get off the ground, of which it has already raised $42,530, reported Daily Koshttps:&#x2F;&#x2F;www.desmog.com&#x2F;ross-clark&#x2F;> In a Spectator article titled “The true cost of renewable energy”, Clark wrote that “the price of green energy is a form of terrible segregation, where the rich will have access to light and heat, and those who need it most, the poor, will shiver in the dark”.3https:&#x2F;&#x2F;www.desmog.com&#x2F;toby-young&#x2F;> Young criticised the youth climate strike movement in a Spectator column where he claimed the protestors just wanted “a day off school”. He wrote that Greta Thunberg had been “living on another planet for the past 16 years” and concluded the article by writing: [25]“If children really must wag their fingers at older generations for some imaginary sin, I wish they’d do it at the weekend. Better yet, they could combine it with picking up litter, which really might do something for the environment. The fact that so many students have been taken in by Greta Thunberg’s crude propaganda is an argument for raising the voting age to 21, not lowering it to 16 reply doingtheiroming 15 hours agorootparentprevDon’t shoot the message. reply burkaman 18 hours agoparentprevLike it or not (and I don&#x27;t like it), that is the culture of this website. Positive comments are actively discouraged, and even if you want to make one they are difficult to write. To criticize you only need to find one thing wrong, but to substantively support you really need to understand the whole subject. reply s1artibartfast 15 hours agorootparentIn my opinion, there has also been an eternal summer effect following the reddit protests, which manifests as 1) more anti-curious doomerism and joke comments reply alephnerd 12 hours agorootparentI feel the pandemic also changed the tone on here, though the Reddit protest definetly made things worse on here. reply bitcharmer 14 hours agorootparentprevI guarantee you if that was USA the comments would be much more positive. HN is as biased and indoctrinated as Reddit simply because Americans are the largest group of commenters here reply muhaaa 21 hours agoprevWhen UK exported its industry to China and imports the stuff it needs from China then yeah the CO2 emitted in UK halfed. When you add the CO2 emissions happing in China which are related to UK consumption then the number wont be impressive at all. reply throw0101b 21 hours agoparent> When you add the CO2 emissions happing in China which are related to UK consumption then the number wont be impressive at all.Consumption-based emissions are also down for the UK from a peak of 750 Mt down to 500 Mt:* https:&#x2F;&#x2F;ourworldindata.org&#x2F;consumption-based-co233% is still pretty good. reply dan-robertson 21 hours agoparentprevCan you write down what you think the number would be? reply martyvis 22 hours agoprevWhile a little orthogonal to this, Tom Scott&#x27;s latest video [1] with him climbing a power transmission tower concludes with discussion as to how the generating sources have gradually been moving from the centre of the country, away from coal mine fuelled generators to nuclear, gas and wind generation near the coast.[1] https:&#x2F;&#x2F;youtu.be&#x2F;F0JDK_71yDg?si=XkJZBFH4nVR0GBTV reply manojlds 18 hours agoparentIt was interesting to visit Battersea Power Station in London...which is now a mall. reply tialaramex 15 hours agorootparentLondon&#x27;s fossil fuel power stations hadn&#x27;t actually produced power for decades. Their redevelopment (as the Tate Modern art gallery in Bankside, and as a mixed development with a mall in Battersea, not to mention housing developments in several more sites) is more recent, but I think they were all shut in the 1980s or earlier. reply mpweiher 22 hours agoprevNo mention of France, who have lower emissions.They just never went up as much, because they converted to nuclear much earlier. reply ponorin 20 hours agoparent... except the carbon intensity (carbon emission per electricity generated) of France stayed basically flat throughout the last 20 years, and it actually slightly increased in 2022 compared to years before, likely due to corrosion issues in reactors and droughts which led to reduced nuclear output. Are they being complacent because they already are \"ahead of the game?\"Also I find it interesting that Germany gets the brunt of the critic for not using nuclear even though they managed to steadily reduce carbon intensity (at least pre covid) of their grid, meanwhile Japan, whose carbon intensity shot up after 2011 because of a certain event and still hasn&#x27;t returned to pre-2011 level, gets a pass since it apparently killed no people with radiation during the process. reply vladvasiliu 18 hours agorootparent> except the carbon intensity (carbon emission per electricity generated) of France stayed basically flat throughout the last 20 years, and it actually slightly increased in 2022 compared to years before, likely due to corrosion issues in reactors and droughts which led to reduced nuclear output. Are they being complacent because they already are \"ahead of the game?\"Well, it stayed flat because it was already very low to begin with. Only Sweden was below that, and they are the only countries which didn&#x27;t \"improve\". See [0] for the CO2 emissions to produce 1 kWh of electricity in the EU. It stops at 2020, so it doesn&#x27;t say anything about your point on 2022. But it shows that Germany, even though they reduced CO2 emissions by a hefty amount, are still quite terrible compared to France, emitting more than three times the CO2 per kWh.[0] https:&#x2F;&#x2F;www.statistiques.developpement-durable.gouv.fr&#x2F;editi... reply ZeroGravitas 17 hours agoparentprevYou are clearly talking about only electricity production.The article is talking about all sectors.The UK and France have both reduced their per capita intensity over the last few decades but after an early lead by France, the UK has caught up and may well overtake. reply adrianN 22 hours agoparentprevFrance is currently reducing the share of nuclear in favor of renewable energy. They don’t plan to build enough new reactors to even replace what they currently have and electricity demand will increase a lot in the future as transportation, heating and industry are switched away from fossil fuels. reply mpweiher 22 hours agorootparentThis was the plan a couple of years ago.Didn&#x27;t work out, so it is no longer the plan.They are still investing somewhat in renewables, mostly off-shore wind, but the focus has shifted back to nuclear expansion. reply mp05 18 hours agorootparentMy life at the holiday table has gotten easier when someone inevitably brings up climate change in whatever capacity. I simply state that, until the mainstream consensus is \"build the hell out of nuke plants\", I don&#x27;t want to hear about it. As cool as it was to strap DC motors to your car, they still rely on a carbon-fueled grid and I&#x27;m still amazed at the horror on some faces when reminded of the fact.Anyway, another example of France keeping it real yet again. reply matthewdgreen 17 hours agorootparent“Build the hell out of nuclear plants” isn’t going to have any impact, and having this attitude means you aren’t informed about the state of the world. There are two types of energy technology right now: those that can be constructed in a factory and deployed on an exponential curve (so far that’s solar and wind, and maybe battery storage in the future) and those that aren’t on this trajectory. Unless someone figures out modular reactors, and gets them deployed by the thousands quickly, nuclear isn’t going to matter.This chart (which I keep posting, apologies HN) helps to illustrate the dynamic: https:&#x2F;&#x2F;www.dropbox.com&#x2F;scl&#x2F;fi&#x2F;9ijkxxhuscrr3x22tewk2&#x2F;File-De... reply belorn 14 hours agorootparentThere are three kind of power plants being built in Europe right now. Wind, gas and solar. Even in countries like Sweden the building plans and permits being given out right now is wind, gas and solar.Battery storage, nuclear and reverse hydro might be built in the future. They are all potential technology that cost too much compared to wind, gas and solar.A rather large contextual note is that wind, gas and solar get practically all the government subsidies, so there is a uphill fight if investors wanted to invest into some of the \"future stuff\". Wind get most subsidies, solar next, natural gas (and oil) third. Many gas and oil plants in Europe is actually government funded through out, as they operate as reserve energy. They get subsidies just to exist, and then get subsidies when they balance the grid, and last they can get subsidies again since prices during high demand and low supply can become more than the market is willing to bear. reply ViewTrick1002 9 hours agorootparentNuclear energy gets a enormous subsidy by the government taking about 90-99% of the risk insurance.This is for the US but equivalent laws exist in all countries. For example in Sweden it is capped to about $1.5B which is laughable.https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Price%E2%80%93Anderson_Nucle... reply belorn 8 hours agorootparentIn Europe the subsidies for nuclear is marginal and significant less per watt produced than wind, solar and gas.Talking about insurance in isolation is practically meaningless. Hydropower do not cover risk insurance of flooding, nor does fossil fuel pay insurance against pollution and global warming. The government or private citizens takes that risk, covering 100% of the costs. Wind and solar do not cover risk of forest fires or damages to endangered species. Wind farms on the ocean do not cover risks to changes in currents, and only need to cover a limited survey in term assessing risk to the environment.In a perfect world all energy production would cover all social and environmental risks of their commercial operation. Intermittent energy sources would then have to pay insurance on the cost of blackouts, as well as the full bill of the reserve energy plan that the government currently pays for. Paying all those through taxes only hide the true cost of energy production away from the consumer. reply ViewTrick1002 11 minutes agorootparent> In Europe the subsidies for nuclear is marginal and significant less per watt produced than wind, solar and gas.Gas pays carbon taxes through the ETS system, although not equivalent to the cost of climate change. Existing renewables get some subsidies, new built barely anything.> Wind and solar do not cover risk of forest fires or damages to endangered species. Wind farms on the ocean do not cover risks to changes in currents, and only need to cover a limited survey in term assessing risk to the environment.That sounds like coping. Trying to build a false equivalency to paint the huge nuclear subsidy gets as insignificant. On one side we have clear examples like Fukushima costing at least ~$150B to clean up compared to for example the $1.5B insurance limit in Sweden. A 99% subsidy on insurance cost. On the other we have vague references and scaremongering.> Intermittent energy sources would then have to pay insurance on the cost of blackouts, as well as the full bill of the reserve energy plan that the government currently pays for. Paying all those through taxes only hide the true cost of energy production away from the consumer.How much should the French nuclear industry pay for having half their fleet off-line right as we went through a huge energy crisis in Europe?https:&#x2F;&#x2F;www.nytimes.com&#x2F;2022&#x2F;11&#x2F;15&#x2F;business&#x2F;nuclear-power-fr... mpweiher 16 hours agorootparentprev1. That turns out not to be the case. Yes, nuclear currently is not built very efficiently, but the efficiency of the plants is so amazing that they are quite competitive.2. There are several companies that are planning to build reactors in factories. Rolls-Royce is one. I was wondering where they&#x27;re going to put all those reactors, and the only reasonable answer I could find was upgrading old coal plants.In fact, that appears to be exactly the plan:https:&#x2F;&#x2F;www.msn.com&#x2F;en-gb&#x2F;money&#x2F;other&#x2F;rolls-royce-in-talks-t...Pretty cool, when you think of it: all that big and heavy custom infrastructure is already in place. The reactor is built in a factory and shipped there.And there are a LOT of coal powered plants in the world about to be or already decommissioned. reply matthewdgreen 16 hours agorootparentI am not anti-nuclear. I am anti-1970s-era-nuclear because I see no path by which large bespoke concrete reactors with enormous containment vessels are going to achieve the grid transformation we need on the timescales we need it by. At least not without a WWII level of government involvement, which doesn’t seem politically practical (not even in China, which is doing better than most.)Modular reactors are at least theoretically feasible. The only question right now is whether they’ll make it from theory to multi-TW deployment before battery storage eats their economic lunch. Over a 20-30 year timeframe that’s not an easy call, though I would personally bet on batteries. reply mpweiher 14 hours agorootparentI agree that factory-built reactors are a great way forward, and with so many companies now working on the problem, it seems very likely that at least one will make it happen.I wish we already had them, because then a lot of these discussions would be moot.However, even with the current somewhat crazy approach to building, nuclear energy is still quite competitive, and without peer for providing reliable, stable grid-level power.Which we absolutely need. reply ZeroGravitas 17 hours agorootparentprevYou realise that you are living the cliche of the uninformed but highly opinionated relative at holiday dinners?Your relatives are almost certainly in the right, just by vaguely following whatever the mainstream media is saying, even if they can&#x27;t penetrate your carefully crafted wall of internalised propaganda you&#x27;ve thrown up. reply mp05 15 hours agorootparent> You realise that you are living the cliche of the uninformed but highly opinionated relative at holiday dinners?What the hell are you going on about? If someone offers their unwarranted take they developed using talking points from \"The Week\", then I will readily admit my ignorance on the topic and insist I outsource this thought process to the French government. If nuke plants are, have been, and will continue to be heavily utilized for moving their electrons, then I&#x27;m going to need every \"green energy\" evangelist to be able to tell me why the French are actually massive idiots and in reality, we should all have hamster wheels at home hooked to the grid or something.> Your relatives are almost certainly in the right, just by vaguely following whatever the mainstream media is saying, even if they can&#x27;t penetrate your carefully crafted wall of internalised propaganda you&#x27;ve thrown up.Do you assume all people are this conniving? I&#x27;d much rather talk about hobbies and achievements with family at Christmas dinner, not energy policy. reply ZeroGravitas 13 hours agorootparentI was about to ask if you&#x27;d even read the post I replied to, then realised you wrote it.Dis you?> My life at the holiday table has gotten easier when someone inevitably brings up climate change in whatever capacity. I simply state that, until the mainstream consensus is \"build the hell out of nuke plants\", I don&#x27;t want to hear about it. As cool as it was to strap DC motors to your car, they still rely on a carbon-fueled grid and I&#x27;m still amazed at the horror on some faces when reminded of the fact. reply mpweiher 15 hours agorootparentprevEr...exactly the opposite.It is the relatives that are following what the mainstream media is saying, which just happens to be wrong, but has been fortified with a wall of propaganda that feels just right. reply adrianN 22 hours agorootparentprevOh yeah? How many new reactors are they planning to build in the next twenty years? How many reach EOL in that time? reply mpweiher 21 hours agorootparentOh yeah? Oh yeah!6 initially, 8 more after that. And they will extend the life of their existing fleet.\"Macron calls for nuclear &#x27;renaissance&#x27; to end the France&#x27;s reliance on fossil fuels\"https:&#x2F;&#x2F;www.euronews.com&#x2F;green&#x2F;2022&#x2F;02&#x2F;11&#x2F;macron-calls-for-n...\"At COP28, Countries Launch Declaration to Triple Nuclear Energy Capacity by 2050, Recognizing the Key Role of Nuclear Energy in Reaching Net Zero\"https:&#x2F;&#x2F;www.energy.gov&#x2F;articles&#x2F;cop28-countries-launch-decla... reply ViewTrick1002 21 hours agorootparentPolitical grandstanding in the face of reality.> Schneider: Most of them are countries that are already operating nuclear power plants and have their own interest in trying to drag money support, most of which by the way would go into their current fleets. Take EDF [France’s state-owned utility company], for example. Through the French government, EDF is lobbying like mad to get support from the European Union—European taxpayers’ money—for its current fleet. It’s not even for new construction, because the French know that they won’t do much until 2040 anyway. There is also another aspect that is related and that illustrates how this pledge is completely, utterly unrealistic.> The pledge to triple nuclear energy capacity is not to be discussed first in terms of pros or cons, but from the point of view of feasibility. And from this point of view, just looking at the numbers, it’s impossible. We are talking about a target date of 2050, which is 27 years from now. In terms of nuclear development, that’s tomorrow morning. If we look at what happened in the industry over the past 20 years since 2003, there have been 103 new nuclear reactors starting operation. But there have been also 110 that closed operation up until mid 2023. Overall, it’s a slightly negative balance. It’s not even positive. Now if you consider the fact that 50 of those new reactors that were connected to the grid were in China alone and that China closed none, the world outside China experienced a negative balance of 57 reactors over the past 20 years.https:&#x2F;&#x2F;thebulletin.org&#x2F;2023&#x2F;12&#x2F;nuclear-expert-mycle-schneid...Meanwhile China is barely building nuclear anymore. China added more wind and solar the past nine months than all of its nuclear reactors under construction will provide. Yes, that includes capacity factor.https:&#x2F;&#x2F;twitter.com&#x2F;yo_ean&#x2F;status&#x2F;1718633487454904718 reply mpweiher 21 hours agorootparentYeah, \"anti nuclear advocate Mycle Schneider pooh poohs most recent move towards more nuclear\" probably isn&#x27;t the grand revelation you make it out to be.The fact that most of the world under-invested in nuclear the last two decades or so is well-known. Extrapolating past trends linearly into the future is ... unwise.Particularly when there has been a significant policy shift, not in small part due to the disaster of the German \"Energiewende\".The Tragedy of Germany’s Energy Experimenthttps:&#x2F;&#x2F;www.nytimes.com&#x2F;2020&#x2F;01&#x2F;08&#x2F;opinion&#x2F;nuclear-power-ger...Germany’s Energiewende: A Disaster In The Makinghttps:&#x2F;&#x2F;www.thegwpf.org&#x2F;publications&#x2F;germanys-energiewende-a...Germany’s Energy Disaster 20 Years Laterhttps:&#x2F;&#x2F;www.americanexperiment.org&#x2F;germanys-energy-disaster-...Germany’s Energy Crisis Dispels Several Mythshttps:&#x2F;&#x2F;www.forbes.com&#x2F;sites&#x2F;michaellynch&#x2F;2022&#x2F;08&#x2F;31&#x2F;germany...\"Much of its problem is self-inflicted and demonstrates the perils of populist but irrational energy policy.\"As for China:\"As of February 2023, China has 55 plants with 57GW in operation, 22 under construction with 24 GW and more than 70 planned with 88GW. \"https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Nuclear_power_in_ChinaThat is almost a tripling in both numbers and capacity. Because you need some reliable power. reply adrianN 21 hours agorootparentprevFrance has nearly sixty reactors that all need to be replaced in the next thirty years… reply mpweiher 20 hours agorootparentPart of the \"nuclear renaissance\" is extending the life of the current fleet. reply lispm 16 hours agorootparentWhich costs a lot of money. Which is also needed to build new ones.https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Nuclear_renaissance\"Since about 2001 the term nuclear renaissance has been used to refer to a possible nuclear power industry revival, driven by rising fossil fuel prices and new concerns about meeting greenhouse gas emission limits.\"Just like today.Since 2001 and now, mostly nothing happened which could be seen as an \"nuclear renaissance\" in France. Other than the EPR having extreme cost increases and extreme construction times.Why should the \"nuclear renaissance\" happen, this time? Given that the situation is only worse: engineers are aging, plants are aging, current nuclear technology&#x2F;companies are not delivering, money is tight, ... reply mpweiher 16 hours agorootparent> Why should the \"nuclear renaissance\" happen, this time?Dunno...maybe because, as the article you cite mentions, the term was used in 2001 to refer to a hypothetical.\" ... a nuclear renaissance is possible ... \"So there was no \"that time\". Whereas \"this time\" it is not some hypothetical that could happen, but officially declared government policy.Macron presents France’s long-term ‘nuclear-heavy’ energy planhttps:&#x2F;&#x2F;www.euractiv.com&#x2F;section&#x2F;energy&#x2F;news&#x2F;macron-presents...Macron announces ‘nuclear renaissance’ — and papers over past mishapshttps:&#x2F;&#x2F;www.politico.eu&#x2F;article&#x2F;emmanuel-macron-papers-nucle...Macron pushes &#x27;French nuclear renaissance&#x27;https:&#x2F;&#x2F;www.dw.com&#x2F;en&#x2F;macron-calls-for-french-nuclear-renais...Did I write \"declared\" government policy? My bad: declared and enacted government policy:French Parliament passes law to accelerate construction of new nuclear reactorshttps:&#x2F;&#x2F;www.lemonde.fr&#x2F;en&#x2F;economy&#x2F;article&#x2F;2023&#x2F;05&#x2F;17&#x2F;french-...French parliament votes nuclear plan with large majorityhttps:&#x2F;&#x2F;www.reuters.com&#x2F;world&#x2F;europe&#x2F;french-parliament-votes...Note the \"large majority\". To be precise it was \"approved with 402 votes in favour and 130 against.\" That&#x27;s a whopping 75% of the vote. Doesn&#x27;t seem particularly controversial.France&#x27;s Assemblée Nationale adopts &#x27;nuclear acceleration&#x27; bill on first readinghttps:&#x2F;&#x2F;www.lemonde.fr&#x2F;en&#x2F;france&#x2F;article&#x2F;2023&#x2F;03&#x2F;21&#x2F;france-s...So that&#x27;s what&#x27;s different. Hypothetical possibility vs. declared and enacted government policy.You&#x27;re welcome. reply lispm 15 hours agorootparentIt&#x27;s an all-government show now. EDF had earlier this year 64.5 Billion Euro debt. It sells electricity to the consumer, operates the grid, operates the power plants, build the power plants. Their mismanagement is amplified by the lack of competition. It hides the large losses of the electricity sector. Now they also pay for the \"favor\" to construct a nuclear power plant in the UK, where they carry the rising losses, which the Chinese now are refusing to pay. They would need to increase the electricity price to be profitable, but that&#x27;s impossible, just in case the politicians want to be reelected -> debt goes to the taxpayer. I wonder how the finance all this debt? French debt is now at > 3000 billion Euro. 115 % debt&#x2F;gdp.It&#x27;s a mostly state owned monopoly on electricity. It&#x27;s not a market-oriented energy landscape at all. Energy socialism.https:&#x2F;&#x2F;www.euractiv.com&#x2F;section&#x2F;energy-environment&#x2F;opinion&#x2F;...> declared and enacted government policyDelivery is the problem. See the main tasks of a) keeping the aging and outdated reactors online, b) building new plants, c) developing new types of plants, d) innovating and e) financing the whole show with government money.They are so desperate, such that EDF is still cooperating with Rosatom&#x2F;Russia... reply mpweiher 12 hours agorootparentI love it how you don’t even bat an eyelid when one of your outlandish claims turns out to be made up out of thin air…and just move on to the next one.A wonderful Gish Gallop.. reply lispm 11 hours agorootparentI love how you go on, having exposed that you even don&#x27;t know the difference between a powerplant and a reactor. All the while you claim others to be &#x27;comically incorrect&#x27;. You should really tone down and don&#x27;t say that anti-nuclear people are clueless, while you are yourself demonstrating a lot of ignorance. reply mpweiher 1 hour agorootparentGood grief.In common usage, the two terms are sometimes used interchangeably, particularly in Germany where plants often consist of a single reactor. But yeah, that bit of terminological sloppiness is a total game changer...about what exactly?Pretty much all your claims are, in fact, comically incorrect:1. nuclear is too expensive.It is not. It is clearly and obviously cheaper than renewables. See France v. Germany electricity prices.Taking the most expensive outlier in cost and claiming that to be the norm is comically incorrect. Well, if you assume an honest mistake, which admittedly is difficult with that level of wrong. So OK: not comically incorrect, just plain old dishonest.2. nuclear is unsafeIt is not. It is either among the safest or the safest form of energy production we have.3. nuclear takes too longIt does not. Average time to build is 7.5 years, consistently. France converted their electricity production to nuclear in 20 years, Germany hasn&#x27;t managed half of that in 20 years with renewables.Again, focusing on a few outliers that take a very long time and claiming that to be the norm is comically incorrect. Or just dishonest see above.4. nuclear is on the way outThere is a massive expansion going on. It was on the way out a few years back, but the catastrophe that is the German Energiewende made non-ideologically captured countries realize they need reliable grid-level supply. Russian gas backup isn&#x27;t it.But you obviously know better than those countries as to what they are actually doing.Again, either comically incorrect or dishonest.5. nuclear waste storage is unsolvedDo tell the countries that have built long-term storage that they haven&#x27;t. And tell the facilities that are doing the intermediate storage just fine that it isn&#x27;t working.This one I wouldn&#x27;t classify as just plain old incorrect, not comically so, as it is somewhat tricky to navigate through all the misinformation and find out what&#x27;s going on. reply lispm 22 minutes agorootparent> particularly in Germany where plants often consist of a single reactorWe were arguing about nuclear power in Japan. In 2011 Fukushima had one of the largest plants in the world, with six reactors. Many powerplants in Japan had more than one reactor. But you did not know that?You also never seemed to have heard of the events at the other reactors, like the four ones in Daini.Your list reads like that one of a fanboy, it has very little to do with reality.mp05 18 hours agorootparentprevSounds like a lot of great, high-paying jobs to me. reply adrianN 17 hours agorootparentSure, but who’s gonna pay for them? reply mpweiher 16 hours agorootparentInfrastructure tends to pay for itself.Whereas with solar and wind, we ship the money to China. reply Symbiote 14 hours agorootparentWind power equipment is mostly made in Europe (Denmark, Germany and Spain). reply mpweiher 14 hours agorootparentChinese manufacturers dominate wind power, taking 60% of global marketChina leads in renewable energy as Europe falls behindhttps:&#x2F;&#x2F;asia.nikkei.com&#x2F;Business&#x2F;Energy&#x2F;Chinese-manufacturer...China becomes solar energy superpower, dominates 80% of supply chainhttps:&#x2F;&#x2F;interestingengineering.com&#x2F;innovation&#x2F;china-becomes-... reply ZeroGravitas 12 hours agorootparentEuropean made wind still dominates European installs.China supplies its own massively growing domestic market as well as others.It&#x27;s starting to make inroads into Europe, Japan and other places where politicians have sabotaged the renewable transition to the detriment of local manufacturing, but that&#x27;s a relative recent phenomenon for wind, slightly behind the pace blazed by the same countries on solar. reply adrianN 15 hours agorootparentprevI prefer the cheaper infrastructure that gets the job done. Nuclear is notoriously expensive. reply mpweiher 14 hours agorootparentIt&#x27;s actually not.\"Expensive\" French nuclear electricity is around half the price of \"cheap\" German electricity.The numbers that get passed around the anti-nuke fan-communities are usually completely bogus, for example one uses the price of the Vogtle plant in the US, pretty much the plant with the most time + cost overruns, as the \"cost of nuclear\".With that level of wrong, it is hard to attribute it to just incompetence and not malice.p.s.: As the biggest part of the cost of a nuclear plant currently is finance, i.e. interest payments, time delays = cost overruns. reply adrianN 5 hours agorootparentNuclear energy in France is heavily subsidized with tax money. reply mpweiher 1 hour agorootparentIncorrect.Nuclear energy in France is heavily used to subsidize other parts of the economy. reply adrianN 43 minutes agorootparentEDF had to be bought by the state because it failed to make any profits. The price of nuclear power in France is regulated, which is why EDF failed to make profits. The actual cost for EDF to produce power is secret. The financials leave zero room for new construction, barely cover maintenance and there is insufficient money for decommissioning existing plants. realusername 21 hours agorootparentprev> France is currently reducing the share of nuclear in favor of renewable energy.That was the plan indeed until about 2020 when they realised that renewables could not sustain the French demand. The Russian war was the final nail in the coffin of the all renewable plan.The law reducing the share of nuclear then got scrapped and the custom electricity market applied there negociated in the EU designed to prop-up competitors is in grand danger of being scrapped as well. reply DrBazza 22 hours agoprevEmissions shift with manufacturing. And for Western countries that means outsourcing to China. reply ZeroGravitas 22 hours agoparentIn general, this isn&#x27;t as big a difference as many claim. On the other hand, the UK is probably an outlier in this regard and exhibits one of the largest such effects, so the reduction given as 50% in the article is more like 35% once imports are accounted for.Some comparitive graphs in this article:https:&#x2F;&#x2F;ourworldindata.org&#x2F;co2-gdp-decoupling reply midasuni 22 hours agoparentprevThey do address that and mention consumption, and the U.K. is down 32% including import, second to Italy. reply mpweiher 22 hours agorootparentRecessions help with emissions. Like in Germany. reply throw0101b 20 hours agoparentprev> Emissions shift with manufacturing. And for Western countries that means outsourcing to China.Consumption-based emissions are also down for the UK from a peak of 750 Mt down to 500 Mt:* https:&#x2F;&#x2F;ourworldindata.org&#x2F;consumption-based-co2 reply AlecSchueler 22 hours agoparentprevSo much this. Is there anywhere where we can see emissions by what is consumed rather than what is produced?It&#x27;s pure politics to outsource everything to China and India then complain that they&#x27;re the biggest culprits. reply iamthemonster 22 hours agorootparentIt&#x27;s easy to google this data, see Figure 1 on this link, from official UK statistics: https:&#x2F;&#x2F;www.gov.uk&#x2F;government&#x2F;statistics&#x2F;uks-carbon-footprin... reply throw0101b 20 hours agorootparentprev> Is there anywhere where we can see emissions by what is consumed rather than what is produced?https:&#x2F;&#x2F;ourworldindata.org&#x2F;consumption-based-co2 reply verve_rat 22 hours agorootparentprevThe narrative that manufacturing was outsourced to China and India and those same were then blamed for carbon emissions removes agency from China and India.If they want manufacturing industry, then they should acknowledge the externalities that come with that. reply AlecSchueler 22 hours agorootparentBut we want the things they manufacture. I&#x27;m arguing for sharing the agency, not putting it all on the UK and others. The climate doesn&#x27;t care about borders. There&#x27;s a global demand for manufacturing, we just don&#x27;t want it in our own backyards. reply midasuni 22 hours agorootparentprevIt is mentioned in the article and they include figures. I’m not convinced by the reliability of those figures, given the carbon intensity of the imports (from a higher carbon grid in China and from the transport)Other sources put U.K. imports at 30% of total energy use, but you need to include the environmental impact too. Better to import 100MWh of goods from Iceland or Norway than 10MWh from China or India. reply _3u10 20 hours agoparentprevChinese people get paid a lot less so they have a much smaller carbon footprint. It’s an environmental and economic win-win situation. reply okeuro49 16 hours agoprevWas recently reading about new Viking Link interconnect between UK and Denmark.Scheduled to come online soon.https:&#x2F;&#x2F;www.viking-link.com&#x2F; reply asdajksah2123 19 hours agoprevThe ridiculous and incompetent Tories of today might be changing this, but historically the UK has been fairly unique in that even their right wing party (the Conservative party) are fairly pro green policies. Conservation of the environment in the UK has been considered a Conservative party tradition and the UK Conservatives did not find it difficult to shift from environmental conservation at the local level to the global level (i.e. also acting against climate change). In fact, Boris Johnson was one of the biggest promoters of rewilding and reducing emissions.Unfortunately the political incompetents running the Conservative party today have taken the result of a single local issue in a single election where they did pretty badly but did not do as badly as the worst case scenario and decided that it means the British public is for worse transit, worse environmental policies, and less government incentive to reduce energy costs and greening the grid.I&#x27;m hopeful that whenever this ridiculous conservative party is forced to call an election (within the next year or so), the larger British public will punish them for it and get rid of the trash that has taken over the party. reply ascorbic 18 hours agoprevThe UK has made good progress with renewables, but measuring from a peak 1970 the overwhelming majority of that is going to be down to the move away from coal. In 1970, most power generation and home heating used coal. Even cooking used coal gas. Oil and gas were found in the North Sea in the 1970s, the coal industry was killed by Thatcher in the 1980s and the privatised energy companies moved most power generation to gas during the 90s. The biggest remaining coal-fired power station (Drax) was converted to biomass in the past decade. reply 6stringmerc 22 hours agoprevIt’s quite impressive how financially penalizing automobile use like in London - sure you can drive as long as you lop off a testicle for His Majesty - and they also have a significant train system.Eventually the math will work its way to the point where Nations will have to admit their Airlines are the biggest polluters in their purview and it will be hard conversation.Frankly there’s a very easy avenue to significantly address two issues:Emissions and Global Pilot ShortagesIn 2024 if passenger aviation went down by 25% in the United States the GDP would improve by a statistically noticeable amount.I should write an article about this…3rd generation aviator (no license due to handicap) reply lancebeet 22 hours agoparentIt has always fascinated me how incredibly car-dependent the UK is, given how small, densely populated and flat it is. The UK seems to have the perfect conditions for getting rid of the automobile entirely, but in reality public transport in the UK (at least outside of London, but sometimes even there) falls short, even compared to much larger and more scarcely populated nations. reply pjc50 18 hours agorootparent> getting rid of the automobile entirelySaying \"entirely\", or \"all\" or \"every\" or \"always\" always results in the pile of replies you see where people come up with counterexamples.But the UK was built without the car; we had thousands of years of pre-modern economy, and the peak of the Empire was entirely done by rail and ship. That serves as an existence proof for ways of life in which many people do not permanently own a car.Car dependency was a choice of the 1960s and the Beeching era. reply 00deadbeef 21 hours agorootparentprevPlenty of people here don&#x27;t need cars. Granted they&#x27;re mostly people in the cities and large towns. I&#x27;m almost 40 and don&#x27;t know how to drive. Buses and trains can get me everywhere I want to be.One third of British households have no car.The USA has 908 vehicles per 1000 people; the UK just 600. reply tonyedgecombe 17 hours agorootparent>One third of British households have no car.I&#x27;d like to believe that but judging by my neighbours I&#x27;m struggling to. reply dukeyukey 8 hours agorootparentDepends where you live - I&#x27;m in London, and probably 1&#x2F;10th of my friends have a car, at most. reply gadders 17 hours agorootparentprevI can understand people not owning a car, but not learning to drive seems short sighted. Driving is a life skill like swimming. reply madeofpalk 21 hours agorootparentprev> The UK seems to have the perfect conditions for getting rid of the automobile entirelyHow do you get your dog to the vet?Or, we&#x27;re going away to a country cottage for christmas. How do we get all our stuff there? reply sdflhasjd 20 hours agorootparent> How do you get your dog to the vet?A taxi? I don&#x27;t know how uber or whatever work, but you can just ask for a taxi to take your dog somewhere reply ilikehurdles 17 hours agorootparentThis gets “rid of the automobile entirely,” how, exactly? reply CatWChainsaw 20 hours agorootparentprevIf niche situations like those are the only ones this person needs a car for, it might be better for them to rent a car for situations like those, rather than keep and maintain a vehicle full time. Still reducing car use by a lot even if it&#x27;s not getting rid of it \"entirely\". reply madeofpalk 16 hours agorootparentI agree - car hire&#x2F;share&#x2F;rent works out great for these \"one offs\", but it&#x27;s worth noting that these \"niche situations\" can happen pretty often, and it&#x27;s entirely at odds with \"getting rid of the automobile entirely\". reply CatWChainsaw 11 hours agorootparentAt a certain point you&#x27;d have to think to yourself, I have \"niche situations\" every week, maybe I actually do need to have my own car. And that&#x27;s not the \"niche situation\" OC was describing. reply dfgfek 20 hours agorootparentprevnext [2 more] [flagged] sdflhasjd 19 hours agorootparentNormally I would agree with the sentiments but cars just rust and disintegrate, depreciating in value more than any belonging I can immediately think of.I was financially better off before I got a car. A lot of people could save money by using alternatives. replypaganel 21 hours agorootparentprevThe only good examples inside of Europe of countries where you can get by train to almost everywhere, in time, are very far and few between.Switzerland is of course in there, maybe Netherlands, maybe Belgium, not sure about Denmark, certainly not France nor Germany. Rail transport outside of Paris which does not involve the TGV is absolutely crap, meaning expensive (unless you purchase your tickets 3 months in advance or some crazy stuff like that) and it doesn’t get you to where you want, unless you want to get from Paris to one of the big other big French cities, but in that case you’d take the TGV anyway. reply lancebeet 21 hours agorootparentMy claim is not that public transport is perfect everywhere else in Europe, only that the preconditions for good public transport are much better in the UK than in most other countries, such as France or Germany, yet it isn&#x27;t vastly superior. In my personal experience, public transport in the UK is definitely worse than in Switzerland, the Netherlands and Denmark, but I would argue it&#x27;s worse than in Germany, France and even Sweden. Mind you, I don&#x27;t restrict this to trains, buses make more sense in scarcely populated areas and their reliability tends to be highly affected by local politics. reply 00deadbeef 21 hours agorootparentThe UK does have good public transport. It just doesn&#x27;t have amazing public transport. I would disagree public transport in Germany is better. Deutsche Bahn trains are notorious for being late. reply okeuro49 16 hours agorootparent> Deutsche Bahn trains are notorious for being late.Statements like this don&#x27;t make sense unless you take into account cultural differences in what \"late\" means.In Germany people think that a 5-10 minute delay is unacceptable. In the UK trains are so frequently late and cancelled that when a train is 5-10 mins late, it&#x27;s considered on time. reply 00deadbeef 14 hours agorootparentNo that&#x27;s not true. UK punctuality figures are based on actual lateness, not perceived lateness. If a train is more than 15 minutes late in the UK you&#x27;re entitled to compensation on a sliding scale up to one hour when you&#x27;re entitled to a full refund of your ticket. reply dukeyukey 8 hours agorootparentprevI don&#x27;t know about that, I take trains in the UK a lot, and the _vast_ majority arrive exactly when expected. reply bluGill 21 hours agorootparentprevEkeryone complains about their local system. What does the data show? reply chupasaurus 18 hours agorootparentThat 34.8% of long-distance trains were late in 2022. [0][0] https:&#x2F;&#x2F;ibir.deutschebahn.com&#x2F;2022&#x2F;fileadmin&#x2F;pdf&#x2F;dufe_2022.p... reply nmfisher 21 hours agorootparentprevSpain&#x27;s rail system was great during my (admittedly very short) visit. reply adrianN 22 hours agoparentprevMaybe eventually, but afaik aviation is something like 4% of emissions currently. A lot, but not our biggest concern. Probably also not the area where you get the biggest CO2 reduction per money invested. reply est31 22 hours agorootparentYour figure is accurate for the global scale. In the USA it&#x27;s also around 3%.In the UK, it&#x27;s 8% though: https:&#x2F;&#x2F;www.theguardian.com&#x2F;business&#x2F;2023&#x2F;feb&#x2F;28&#x2F;scientists-... reply Closi 22 hours agorootparentThis is right in percentage terms - but also worth noting that the UK produces 4.7 tonnes of co2 per capita vs USA&#x27;s 14.4 metric tons per capita.i.e. taking your 8% and 3% figures along with the tonnes of co2 above would mean in raw CO2 terms, the USA is producing more tonnes of aviation carbon per capita (i.e. per person) than the UK.The UK would be at 2.6% if it consumed &#x27;other carbon&#x27; at USA rates (i.e. 14.4 metric tons per capita), but it is 8% because it consumed 4.7 tonnes per capita. reply closewith 22 hours agorootparentprevThe USA excludes international aviation emissions while the UK includes them in their figure (the only country to do so). Domestic UK aviation emissions are tiny. reply Eavolution 21 hours agorootparentWhat domestic flights even are there? The only ones I&#x27;m aware of are Belfast to assorted cities in England. reply janekm 20 hours agorootparentThere&#x27;s over 182 domestic routes flown in the UK (Heathrow - Edinburgh is the most popular one): https:&#x2F;&#x2F;commonslibrary.parliament.uk&#x2F;domestic-flights-in-the... reply jenscow 20 hours agorootparentprevI know at least Manchester, Birmingham, Newcastle, Glasgow and Edinburgh all connect to London. reply Digit-Al 20 hours agorootparentI&#x27;ve flown to Newquay as well. reply Closi 18 hours agorootparentprevLoads of people fly from London&#x2F;Birmingham to Edinburgh&#x2F;Glasgow and visa-versa (it&#x27;s often cheaper and faster than the train).Then obviously NI as you mentioned. reply stevedh 20 hours agorootparentprevI&#x27;ve flown Newcastle to Southampton (and back) a few times. reply tonyedgecombe 17 hours agorootparentprevThe interesting thing is that even in the USA only 50% of people fly in any one year. reply closewith 22 hours agoparentprev> Eventually the math will work its way to the point where Nations will have to admit their Airlines are the biggest polluters in their purview and it will be hard conversation.If we&#x27;re at that point, then we&#x27;ll have already won the battle against emissions. Aviation is tiny compared to land transport, sea transport, heating, manufacturing, etc. For aviation to become the biggest polluter, we&#x27;ll need all the above to reduce emissions by 90+%. reply midasuni 22 hours agoparentprevFunny I drove in london a couple of weeks ago just fine. Stayed in a hotel near docklands, later drove to the British library and parked there for a few hours.Yes there was a financial sting from parking, last I checked NCP wasn’t owned by the government though.Even if I had gone into the very centre of london the cost of parking far outweighs the cost of the congestion charge. reply closewith 22 hours agorootparentMost people driving in the ULEZ and congestion charge zones arent paying for parking. reply dukeyukey 8 hours agorootparentAlthough the ULEZ only charges a vanishingly small number of vehicles, and the congestion charge only affects the most central part of the city.Driving in most of London is no more expensive than other UK cities. reply est31 22 hours agoparentprevThe UK is surrounded by water, and ships are a comparatively slow means of transportation. reply londons_explore 22 hours agorootparentThere is a train to europe and can get you there in ~60 mins. reply ageitgey 21 hours agorootparentThe Eurostar is great and I&#x27;ve taken it quite a few times. But realistically, it is way too expensive right now to seriously compete with flying for most people. It is typically 2x-3x as expensive as flying from Heathrow&#x2F;Gatwick to CDG, for example. That really adds up if you are traveling with multiple people. We really need more services and more competition. reply tonyedgecombe 17 hours agorootparentThere is a new operator offering a service, I think it starts in 2025. reply andyjohnson0 21 hours agorootparentprev> There is a train to europe and can get you there in ~60 mins....from London. Most of the country, and a large majority of the population, isn&#x27;t in London. reply throwaway167 19 hours agorootparentAnd nor will those 60 minutes take you through all of Europe.You seem overly focused on London. reply andyjohnson0 12 hours agorootparent> And nor will those 60 minutes take you through all of Europe.Because mainland Europe is larger than the uk?> You seem overly focused on London.Not really. I live a long way from London and haven&#x27;t been there for many years. I do, however, know how things work in this country. reply tailspin2019 22 hours agorootparentprevNitpick: you&#x27;re already in Europe even before getting on that train.But we know what you mean... :) reply defrost 22 hours agorootparentprevHigh bandwidth though.Speed is somewhat irrelevant here as what matters is how many ships per day they can process. reply ramesh31 22 hours agorootparent>Speed is somewhat irrelevant hereNo, it&#x27;s not, because people. I really don&#x27;t see a future where anyone gives up civil aviation for the climate. More likely a combination of electrification, biofuel, and hydrogen will be the solution. reply closewith 22 hours agorootparentIt&#x27;s already happening with TEN-T on the continent. Even though slower, travel by train in generally city centre to city centre and much less hassle, so where feasible can supplant aviation. reply defrost 22 hours agorootparentprevWhat does civil aviation have to do with the bulk transport of goods by massive container and bulk carrier ships?Ship journey time is irrevelant to volumetonnage moved as the limiting factors are port capacity and water approach limitations.A ship journey is a long pipe, you can pop 6 ships a day in one end and have 6 ships a day arriving at the other end. reply closewith 22 hours agorootparentKind of irrelevant to reducing aviation emissions, given that people only pay for air freight when speed is a priority. replyramesh31 22 hours agoparentprev>In 2024 if passenger aviation went down by 25% in the United States the GDP would improve by a statistically noticeable amount.How does that make any sense? You propose to generate more economic activity by... reducing the amount of economic activity occuring? reply bluGill 21 hours agorootparentA lot of what people fly for could be done as a phone call, or so many claim. That would return the cost of the trip to something else. reply throwaway167 19 hours agorootparentIncome&#x27;s circular. Airline related employees etc would have less. While others get the substitution. But where&#x27;s the net increase? reply bluGill 19 hours agorootparentThose employees would need to get a new, more productive job reply tonyedgecombe 17 hours agorootparentprevServices get cheaper so we consume more of them. See Jevon&#x27;s paradox. reply _3u10 20 hours agoparentprevDoubtful they will just switch to SAF and pass on the cost. reply youngtaff 18 hours agorootparentNot enough plant oils to make sufficient quantities of SAF so somehow we need to synthesise it from atmospheric CO2 reply tonyedgecombe 17 hours agorootparentThe airline industry looks like the least likely to decarbonise to me. I&#x27;m extremely sceptical about any messaging from them that purports to indicate they have any intention in that direction. reply andrepd 22 hours agoparentprevDefinitely not penalising enough. reply AmericanChopper 22 hours agoparentprev> Eventually the math will work its way to the point where Nations will have to admit their Airlines are the biggest polluters in their purview and it will be hard conversation.I can’t see how this would ever happen. Passenger aviation is ~2% of global carbon emissions. There is no climate model that relies on cutting passenger aviation to meet its targets. reply closewith 22 hours agorootparentDomestic aviation is 2% of global emissions. The Paris Accords didn&#x27;t require any country to accept responsibility for emissions from international flights and only the UK does so (and only beginning in 2023). reply rjknight 22 hours agoprevhttps:&#x2F;&#x2F;archive.is&#x2F;AHFlh reply orenlindsey 18 hours agoprevGood on them. More countries should follow. reply markburns 22 hours agoprevNot read past the paywall. Could anyone let me know if this statement is clarified?> no politicians likely to trumpet itIrrespective of whether or not it&#x27;s outsourcing to China and so not quite the truth, I can&#x27;t see why politicians wouldn&#x27;t lay claim to it. Isn&#x27;t it their modus operandi to find positive news they could attribute to their own efforts? reply 93po 1 hour agoparentOften there&#x27;s an archive.whateverTLD link in comments, there&#x27;s one here:https:&#x2F;&#x2F;archive.is&#x2F;AHFlh reply OscarCunningham 22 hours agoparentprevThe current government is very unpopular, but they won a small by-election by opposing London&#x27;s ultra low emissions zone. So they&#x27;re now shifting to an anti-environmental stance in the hope that will save them in the next election (they were hardly pro-environment previously). reply ZeroGravitas 21 hours agoparentprevThis is probably down more to the lack of overlap between people who care about carbon accounting and those who celebrate petty jingoistic \"victories\" like being the first nation in the G20 to achieve collectively agreed targets. reply midasuni 22 hours agoparentprevThis government is leaning heavily into culture wars, they are unlikely to attach themselves to something they are painting as “woke” (cleaner air in school playgrounds, lack of smog, etc) reply mellosouls 22 hours agoprevCan&#x27;t read due to paywall but maybe worth pointing out this is seemingly an opinion piece by the editor of this conservative and climate-skeptic magazine.I did a quick Google and couldn&#x27;t find a more neutral news item with appropriate context so I&#x27;ll be taking the headline with a pinch of salt until that. reply midasuni 22 hours agoparentFor those unaware and to give an idea of leanings, the magazine used to be edited by Boris Johnson.The tone of the article is very much “climate change is a problem but the U.K. is a tiny amount of it and it won’t be impacted by the U.K. magically cutting all emissions, the only influence the U.K. has is in showing emissions can be cut, and it’s doing a good job on that. It also mentions cuts to use of fertiliser and other non co2 issuesTo be fair it’s a solid point. reply dan-robertson 21 hours agorootparentTo be fair to Boris, he did push a reasonable amount on various green issues, set some long term policies, targets, etc, while he was pm. reply youngtaff 18 hours agorootparentprevYeh, but reducing CO2 emissions generally also results in the reduction of other pollutants so it’s still worthwhile doing to get cleaner airPlus switching to non-fossil fuel energy sources reduces dependency on the Middle East autocracies reply traceroute66 21 hours agoprevnext [6 more] [flagged] rodlette 20 hours agoparenthttps:&#x2F;&#x2F;paulgraham.com&#x2F;disagree.html calls this level of critique \"DH1. Ad Hominem.\".> Of course he would say that. He&#x27;s a senator.> Of course [they] would say that. [They&#x27;re Tory leaning]. reply traceroute66 20 hours agorootparent> Of course [they] would say that. [They&#x27;re Tory leaning].Except this bit is true.Post-Brexit, the incumbent Tory government has been desperate to frame everything in a \"UK first\" or \"Great British\" light, even if it is patently not true or some massaging of the data is required to make it appear so.Therefore, whilst I am not necessarily disputing the headline statement, the point I am making is that it would have been nice to see a link to an impartial, independent website (preferably with raw data attached) rather than a known Tory-leaning rag.And in relation to emissions in particular, just look at the way the present government fought the Uxbridge by-election on the basis of a complete pack of scaremongering lies about ULEZ (Ultra Low Emission Zone). That was in August 2023, and a leopard doesn&#x27;t change its spots. reply rodlette 17 hours agorootparent> And in relation to emissions in particular, just look at the way the present government fought [...]. That was in August 2023, and a leopard doesn&#x27;t change its spots.The article and topic aren&#x27;t overtly political, no need to make it. reply dan-robertson 21 hours agoparentprevIsn’t the current government’s position that the green policies (from the ~previous government…) were bad and should be undone. Doing well at emissions reduction is maybe not something the government would want to see applauded? reply rjsw 20 hours agorootparentThe subtext could be that emissions are down so it will be fine to \"max out\" North Sea oil production. reply Brthrw 22 hours agoprevIf this is what victory looks like I&#x27;ll take the rising sea please. reply piuantiderp 18 hours agoparentIf you have to be told that you are \"akstually winning!\" then it&#x27;s a very sad win, if any at all. reply ilikehurdles 17 hours agorootparentNobody in the general sense is better off but let’s all celebrate because some bureaucrats’ climate targets were met.Degrowth is a suicide cult. reply piuantiderp 18 hours agoprevTLDR; people in the UK are poorer by half. reply kingkongjaffa 22 hours agoprev1. It&#x27;s behind a paywall so if others have a link that would be great2. The UK is (now) almost [80% services industries](https:&#x2F;&#x2F;www.ons.gov.uk&#x2F;economy&#x2F;economicoutputandproductivity...) not building anything.3. With that in mind it&#x27;s kind of bad it&#x27;s taken us this long to get to 50% of the 1970 output. reply Luc 22 hours agoparentPrepend archive.is&#x2F; to the URL. The images are cut off but you can open them in a new tab to see the whole image.I would also recommend https:&#x2F;&#x2F;ourworldindata.org&#x2F;co2-emissions . You can compare the UK to other countries. It seems to be doing rather well (though it has huge historical emissions), e.g. the UK is at 4.6 tonnes CO2 per person, while Germany is at 8. reply mpweiher 22 hours agorootparentWell, Germany is doing particularly horribly.The \"Energiewende\" was a huge failure, due to its reliance on Russian Gas, even before the Russian Gas thing blew up in our faces and we had to scramble to buy up all the gas on the international market. reply adrianN 22 hours agorootparentIn what way was it a failure? CO2 per kWh had been going down steadily. Germany just built less renewables than the UK. reply mpweiher 21 hours agorootparentCoal use went UP the last two years, not down. This year it&#x27;s down, in part due to the recession reducing overall demand. And the recession is in large part due to high energy prices.Germany has the 2nd highest electricity prices in the EU. AND the 2nd highest CO2 emissions per unit of energy. That&#x27;s after 20 years of \"Energiewende\". If that isn&#x27;t a failure, I don&#x27;t know what is.And yes, emissions do go down. It&#x27;s just that \"yes, renewables produce some electricity some of the time\" is not exactly success. reply ViewTrick1002 21 hours agorootparentIn large part due to making up the French nuclear shortfall.https:&#x2F;&#x2F;www.nytimes.com&#x2F;2022&#x2F;11&#x2F;15&#x2F;business&#x2F;nuclear-power-fr... reply mpweiher 21 hours agorootparentFrance had planned inspection and maintenance shutdowns that took longer because they had deferred those during the COVID years.Scheduled for the summer when electricity consumption is lowest and PV generation is highest.Because, well, you actually can schedule power generation with a nuclear power plant.And that was the only year. In 2023 France is exporting to Germany again. reply ViewTrick1002 20 hours agorootparentNow you are misrepresenting facts to present a rosy picture. Is it that hard to go by the facts? Or does nuclear power require lies to be sold?They found unexpected corrosion cracks in critical cooling pipes across much of the fleet. It was not planned or deferred maintenance.We lasted the entire winter with half the French nuclear energy fleet off line, when it was needed the most. France was an importer of German coal the entire winter.Now we are back, but we need to keep in mind that fleets of nuclear plants share the issues. Exactly like when we ground fleets of commercial aircraft. The failure modes are correlated. reply mpweiher 20 hours agorootparentNothing that I wrote was wrong.Now they actually found a problem during these routine inspections, so they took longer than anticipated. Though that&#x27;s kind of why you have inspections: so you find problems before they become real problems. Which is what happened here.And again, the reason it was so many power stations at the same time is that maintenance was deferred during the COVID years (and the chronic underinvestment the last decades). So stuff accumulates. But again, this is because they were able to choose when to do this.Which is pretty much the opposite of how it was portrayed in the press in Germany and how it stuck in the minds of anti-nuclear advocates. reply ViewTrick1002 19 hours agorootparentGood to know that they chose to do the safety critical maintenance during winter and coinciding with a fossil gas supply crisis due to war in Europe.Smart thinking! reply mpweiher 16 hours agorootparentIncorrect.They chose to do it in summer. Because they found more than they had anticipated, it took longer.You&#x27;re welcome. reply ViewTrick1002 16 hours agorootparentIs it so hard to stick to the truth when promoting nuclear power?They unexpectedly found corrosion cracks and immediately shut down the plant. Based on this they precautionar shut down several plants. Then as fast as possible they inspected the rest of the fleet and shut down the offending reactors.They did not plan it. reply mpweiher 14 hours agorootparentIs it so hard to stick to the truth when trying to denigrate nuclear power?(Narrator: yes it is)The shutdowns for the inspections and maintenance were planned. Not for a single plant, for a lot of plants. The inspections found a problem. The shutdowns were extended so they could be fixed, in the original plants and in other plants that might also be affected.The shutdowns, the inspections and the maintenance were planned.What they found was obviously not planned. If you could plan for what you find during an inspection, you wouldn&#x27;t need an inspection. That&#x27;s why you inspect.End of story. replyadrianN 21 hours agorootparentprevRenewables account for something like 40% of electricity production in Germany. That’s a lot more than „some be energy some of the time“. replyDataDaemon 20 hours agoprev\"to halve it&#x27;s carbon emissions... and the economy\". Congrats UK, soon carbon free with people on the streets. reply __natty__ 22 hours agoprevThat&#x27;s pretty cool. Now I&#x27;m waiting for India and China. reply ViewTrick1002 21 hours agoparentChina is the world leader in building out renewables. They are ahead. reply toomuchtodo 19 hours agorootparentThis. China built more wind in 2023 than total UK generating capacity, and more solar than total US solar generating capacity. reply andrepd 22 hours agoparentprevIndia and China manufacture the goods that the developed world wants. You can&#x27;t just shift your carbon-intensive industry overseas and then go smirking smugly about how those developing nations emit so much.If you want to be honest, a good metric is CO2 per capita for goods consumed, not manufactured. reply verve_rat 22 hours agorootparentIf you go wholly off of consumption then you remove responsibility for the manufacturing to improve.Turns out the world is one giant complex system, there are no simple answers. reply novolunt 22 hours agoprevWhy don&#x27;t you talk about the British nuclear leakage that caused severe nuclear pollution in European waters? The British are really smart. They control the world&#x27;s media and say the best about everything, but they are the initiators of the current geopolitical disputes around the world. reply hbrav 22 hours agoparentWell you seem to feel very strongly about it, so I assume you&#x27;d be a good person to tell us about these leaks. Do you have data &#x2F; links you want to provide? reply phantomathkg 21 hours agoparentprevCan you back it up with some sources on the following 3 points? * nuclear * control of the media * starter of geopolitical disputes. reply 10xDev 17 hours agoparentprevThe UK doesn&#x27;t think for itself much anymore when it comes to geopolitical disputes. They copy the decision making of the US almost always.Have a look at this: https:&#x2F;&#x2F;www.aljazeera.com&#x2F;wp-content&#x2F;uploads&#x2F;2023&#x2F;12&#x2F;INTERAC... reply petesergeant 21 hours agoparentprev> The British … control the world&#x27;s mediaWith our secret mind rays from our lizard king, yes reply gumballindie 22 hours agoparentprevThe uk does have a problem with massaging statistics. Healthcare being one. I wouldnt be surprised if the same happened with co2 emissions. reply londons_explore 22 hours agoprevThis figure likely came at a large cost to the economy. GDP&#x2F;capita isn&#x27;t looking good compared to the rest of the western world...Turns out when you tell everyone they have to sit in a barely-heated house and shouldn&#x27;t drive their car or fly, GDP goes down.Eg. https:&#x2F;&#x2F;twitter.com&#x2F;MortenORavn&#x2F;status&#x2F;1538160446150156292 reply ricardo81 21 hours agoparentUnderstandable point but it would seem that the price of natural gas has had a far greater bearing on the economy.e.g. the \"energy price guarantee\" of last Winter cost something of the order of ~£30bn to the taxpayer, all due to the national grid&#x27;s pricing system being largely dependent on the cost of natural gas, and the UK&#x27;s relatively higher reliance on natural gas for heating. reply room271 19 hours agoparentprevPoor GDP is a complex question to answer but lowering the heating by a degree doesn&#x27;t seem like the best attempt at exploring it(!).Transport might be a better part of the explanation, but more because of the lack of investment in decent transportation beyond London (the recent HS2 cancellation being one example).For me, I suspect poor GDP performance is due to a lack of investment&#x2F;success in manufacturing and science, in part because of an over-emphasis on the city&#x2F;finance and other (in-part) rent-seeking activities which has sucked in talent and energy, but also because of a general incompetence of government over the last 20 years. reply dukeyukey 8 hours agoparentprevGDP per capita in the UK is on-par with France and within a few percentage points of Canada. It&#x27;s really not an outlier at all. reply dan-robertson 21 hours agoparentprevLots of European countries and the United States have had emissions fall. The lack of productivity growth is relatively unique to the U.K. (and some countries like Italy) reply dukeyukey 8 hours agorootparentNo it isn&#x27;t - UK productivity is on-par with Canada and Australia, and consistently above New Zealand and Japan. reply adrianN 22 hours agoparentprevDid GDP go down 50%? I don’t think so. reply londons_explore 22 hours agorootparentno - far from it. But any GDP loss today compounds over generations. reply closewith 21 hours agorootparentWhat an odd mix of short and long term thinking. Obviously the destruction of the environment will also have a compounding effect on GDP over generations. reply LargeTomato 11 hours agorootparentYup, so we&#x27;re trading one thing for another. Nothing is so cut and dry. reply closewith 10 hours agorootparentWell, there might be other externalities not captured by GDP as a result of destroying the environment.Further comment would involve breaching HN guidelines. replymytailorisrich 22 hours agoparentprevThis is indeed the typical spin article.While in itself it is good that emissions are going down, most of the underlying reasons and explanations for this 50% drop are actually negative or unrelated to \"environmental progress\". reply closewith 21 hours agorootparentHow so? reply mytailorisrich 21 hours agorootparentI think the peak was achieved earlier than most countries because of the switch from coal to natural gas when North Sea gas started to be exploited, combined with the decimation of industry at the same time and later. Further down the line, the economy has stalled since 2008.Sure renewables are growing like they do everywhere, but overall the energy policy, if there is one at all, is chaotic.So, to me, praising \"environmental progress\" and how well the UK is doing is pure spin. reply gumballindie 22 hours agoparentprevThe masses demand taxes. Likely poverty will increase further as a result. No disposable income means no spending means no economy. But good luck explaining that to communist sympathisers. reply 1 more comment... GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The UK has become the first major country in the G20 to cut its carbon emissions in half, with its current output below 320 million tonnes.",
      "This achievement is significant as it demonstrates the country's progress in reducing its carbon footprint, despite having a larger population and economy.",
      "Surprisingly, this accomplishment has received little attention from campaign groups and politicians, and is not being actively promoted. The data can be accessed on The Spectator data hub, tracked by the Global Carbon Project."
    ],
    "commentSummary": [
      "Critics argue that the UK's reduction in carbon emissions is partially due to outsourcing manufacturing and transitioning to a service-based economy, raising concerns about the exportation of emissions from goods consumed in the UK.",
      "Carbon accounting practices, the impact of Brexit on manufacturing, and the potential for double-counting of green energy production are discussed.",
      "The article explores the role of nuclear energy, wind power, and other renewable sources, as well as the effectiveness and convenience of public transportation. Different opinions are expressed regarding the impact of environmental measures on the economy, with concerns about political bias and the credibility of the source."
    ],
    "points": 205,
    "commentCount": 259,
    "retryCount": 0,
    "time": 1703588461
  },
  {
    "id": 38772382,
    "title": "Get Unique Activity Ideas for the Holiday Break with Whataaabout.com",
    "originLink": "https://www.whataaabout.com/",
    "originBody": "Hi HN, I’ve been working on whataaabout.com with a friend, while learning to code. It&#x27;s a fun little website for those short on time but seeking new experiences. It started from a question, \"when was the last time you did an experience you had never done before?\". As human beings we need some degree of novelty, to expose ourselves to the unfamiliar and keep learning throughout our lives. So to add a spark to my daily routine and keep novelty coming my way, I started collecting ideas of interesting and unusual activities I’d like to try out sooner or later. The main selection criteria is things that take a short amount of time and are not too demanding, nor location-specific. I organized them based on categories like uniqueness, humans involved, location, price, time required, and others.I hope you like it, and I’d be happy to hear your thoughts, as well as any cool activity ideas you might have. Cheers!",
    "commentLink": "https://news.ycombinator.com/item?id=38772382",
    "commentBody": "Whataaabout.com – unique activity ideas for the holiday breakHacker NewspastloginWhataaabout.com – unique activity ideas for the holiday break (whataaabout.com) 202 points by erkjs 18 hours ago| hidepastfavorite83 comments Hi HN, I’ve been working on whataaabout.com with a friend, while learning to code. It&#x27;s a fun little website for those short on time but seeking new experiences. It started from a question, \"when was the last time you did an experience you had never done before?\". As human beings we need some degree of novelty, to expose ourselves to the unfamiliar and keep learning throughout our lives. So to add a spark to my daily routine and keep novelty coming my way, I started collecting ideas of interesting and unusual activities I’d like to try out sooner or later. The main selection criteria is things that take a short amount of time and are not too demanding, nor location-specific. I organized them based on categories like uniqueness, humans involved, location, price, time required, and others.I hope you like it, and I’d be happy to hear your thoughts, as well as any cool activity ideas you might have. Cheers! eterm 16 hours agoSome things I noticed:The list is fairly short (70 activities total). Once filtered it would be easier to simply display all matching activities.Many of the \"Low\" cost activities have significant upfront costs. I wouldn&#x27;t describe Archery as \"low cost\", nor describe Astronomy or a board game night as Free activities!It was easier just to read the raw JSON than browse the site. ( The activities are https:&#x2F;&#x2F;www.whataaabout.com&#x2F;activities.json and seem to be static, not based on parameters. ) reply erkjs 16 hours agoparentThanks for the feedback! It is short yes, mostly because I didn&#x27;t want to spend more time on it before showing it to somebody. If the feedback is positive I&#x27;ll add more. About displaying all activities, it&#x27;s a good point, I though it was more fun to get one at a time, but I might add the display all as another option. And about the categories, I hear you, but take it with a grain of salt, those depends on many factors. For example I consider board game night free because where I am there are many pubs with games you can play. Same with archery, I&#x27;ve been in places where with 20euros or less you can try it for one hour..so yes, it depends. reply throwup238 13 hours agorootparentI&#x27;d remove \"Try CBD\" and \"Space Cake\" once you&#x27;ve got more options. The latter is still illegal in many places and for the former is just gauche. reply wspeirs 13 hours agorootparentPlus the CBD one appeared when I had the \"Kid Friendly\" filter selected :-| reply erkjs 13 hours agorootparentSeriously or are you trolling? If so, it&#x27;s some strange bug, Kids friendly is not a category assigned to that activity reply throwup238 12 hours agorootparentWell it is FDA approved to treat epilepsy in children :-) reply pests 10 hours agorootparentDidn&#x27;t HN just get in an uproar over T Mobile disallowing marketing texts where the topic was illegal in one state but legal in another?So do we want legal-in-some states items or not? reply throwup238 10 hours agorootparentWho is \"we\"? There&#x27;s more than one person commenting on HN and they don&#x27;t always agree. reply pests 2 hours agorootparentThe people who I describe, which were a bunch. reply CPLX 10 hours agorootparentprevIt’s also commonly used for treating parenthood in parents. reply klyrs 9 hours agorootparentNo, we use THC for that, CBD is for physical back pains. replyquickthrower2 9 hours agorootparentprevCBD = Central Business District too. Worth a try! reply gala8y 15 hours agorootparentprevI really like the idea and the site looks nice. It&#x27;s good the list is short as of now, as quality of suggestions is crucial. I&#x27;ll be absolutely looking at it, sometimes I get really too narrow in focus. reply erkjs 14 hours agorootparentI appreciate it! reply arunan 13 hours agoparentprevI kinda like that it shows only one activity at a time. If one doesn&#x27;t like it, they can always spin for a new one. Makes it better than the typical, google seo farmed lists. Also, let&#x27;s you focus on one and make a decision.In regards to the costs, again, what&#x27;s low cost for one might be different for another. My archery club allows you to show up and try it for a session (or two) before enrolling. Astronomy nights don&#x27;t cost us anything or much fuel, if at all. Been to plenty of board game nights where there were no entrance cost.To share some contrasting opinions. reply singhrac 8 hours agorootparentI agree, I like being forced to make a decision on an activity sequentially. Prevents optimization, which is a common way to spin around doing nothing. reply bongobingo1 4 hours agoparentprev> a board game night as Free activities!https:&#x2F;&#x2F;boardgamegeek.com&#x2F;boardgamecategory&#x2F;1120&#x2F;print-play&https:&#x2F;&#x2F;boardgamegeek.com&#x2F;forum&#x2F;974620&#x2F;bgg&#x2F;design-contestsBGGs site is always a bit confusing to me to navigate, but searching for \"pnp\" or \"print and play\" and a contest year will probably surface a lot of good \"free\" (or at least, free to print....) You can at least certainly get a bunch printed for cheaper than 1 boxed game and if you dont like half of them, no drama.Shutup and sit down has done a few round ups of solo-print-and-plays https:&#x2F;&#x2F;inv.tux.pizza&#x2F;search?q=shut+up++and+sit+down+print+a.... I&#x27;m sure there are people on here that can recommend some +1-player pnp games. reply oniony 0 minutes agorootparentBesides, board games are dirt cheap entertainment. Assuming a cost of 50 of your local currency, split between four people for entertainment for perhaps six to ten hours is hard to beat. And you can typically sell the games to recover 50% of what you paid.In fact, if you buy used, and resell when you&#x27;re bored of the game, if can be pretty much free. reply diggan 15 hours agoparentprev> I wouldn&#x27;t describe Archery as \"low cost\"Obviously the range is very broad, but around my parts you can get away with archery for about ~15 EUR per person (and up). That&#x27;s not very expensive. What is \"low cost\" also obviously depends on the person. reply Moto7451 13 hours agorootparentYup. About $20 USD where I grew up, with several archery ranges at public parks. I’d probably spring $20 for a few more niceties. reply bacheaul 5 hours agorootparentHaving bought basic archery gear for 3 people in the last year, unless you&#x27;re talking about a toy longbow, $20 is not going to get you anywhere. Even buying the absolute necessities for a basic barebow recurve setup, let&#x27;s list things off:Riser, plunger, arrow rest, limbs, string, arrows, finger tab. That&#x27;s the absolute bare minimum. You&#x27;ll also probably want an arm guard unless you don&#x27;t value your forearms. You can&#x27;t do this for less than US$150-$200, unless you find someone basically giving away second hand equipment. reply 4ggr0 4 hours agorootparentThey&#x27;re talking about rental archery, aka. \"I give you $20 and in turn can use your bow and arrow for an hour.\" reply pimlottc 16 hours agoparentprevNot sure about archery and astronomy, but free board game meetups are pretty common. There’s normally plenty of people bringing games that you don’t need to bring any yourself. reply Alex3917 13 hours agorootparentOr just play Go online. reply tuukkah 11 hours agorootparentAnd Board Game Arena has many more online board games (non-premium) for free. reply quickthrower2 9 hours agoparentprevAt a basic level astronomy requires you to wait until twilight or dusk, and then tilt your head up :-), which costs nothing. You can also enhance it with an night sky app which are usually free too. If you have binoculars hanging around grab them. reply NewJazz 16 hours agoparentprevFor me astronomy night would be much longer than an hour, and yeah not free because of fuel costs.More like 3-4 hours, and low (but not zero) cost. reply samstave 2 hours agoparentprev>>It was easier just to read the raw JSON than browse the site*this is hilarious. Like when us it guys have to call a call center triage desk \"yes, i already rebooted....\" reply knowknowledge 17 hours agoprevIs there any way to get a permanent link? If I wanted to share an idea with friends, how would I do that? reply erkjs 16 hours agoparentThere&#x27;s no way now, for a quick one, I&#x27;d suggest to just take a screenshot reply burrish 1 hour agoprevI don&#x27;t really like that \"trying CBD\" was in the list reply eluketronic 16 hours agoprevThe first suggestion that appeared for me was “Trying CBD”. Is that really a “unique activity idea for the holiday break”? reply bongobingo1 4 hours agoparentMore exciting than mine, \"doing an expense report\" or \"checking your subscriptions\". reply samstave 2 hours agorootparent\"contemplate how that spider you can see in the corner when youre on the toilet, can survive so long without any critters in her web?\"\"spend ten minutes attempting to come up with a name for your dying plant, because if you just remember to talk to it - it wont leave you like everyone else has\"\"Promise yourself youll fold the laundry after you rest your eyes for a few minutes\"I&#x27;m pretty good at this game! reply sirsinsalot 14 hours agoparentprevSounds good to me. Each to their own. It&#x27;d be boring list if designed to avoid value judgement. reply zenapollo 16 hours agoparentprevIf you do like 80-100mg, yes it def can be! Add freestyle yoga or deep house-cleaning and you’re in for a good day. reply bicx 12 hours agorootparentI often suspect it’s the trace amounts of remaining THC that make higher doses of CBD a good time. reply NewJazz 16 hours agorootparentprevHow do you clean deep house music? The stuff I listen to is fairly clean as is? reply mf2hd 16 hours agorootparentTry something with dirty stinky bass. reply erkjs 16 hours agoparentprev:D sorry about that! Well, it could be... but no, not ALL of them are, of course reply samstave 2 hours agoparentprevhaha mine was \"lockpicking\"I make custom lock picks was member of TOOOL - and have a few cool tricks up my sleeve:https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=u3I6lbpF68Qhttps:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=qwD54DiYprw reply o0-0o 12 hours agoprevThis list includes taking drugs? Do you think it should have an age acceptance for terms and conditions? reply pests 10 hours agoparentCaffeine is a drug yet there&#x27;s the fad of collecting donations via \"buy me a coffee\". Or as you might call it \"buy drugs for me\".Not all drugs are bad, not all drugs are illegal. reply leke 4 hours agorootparentSpace cakes are where I&#x27;m from. reply burlesona 6 hours agoprevI would definitely make permalinks to each activity and make the “surprise me” button navigate so you can hit back.This is pretty neat! I think it’s actually nice that the list of activities is not enormously long, because at a certain point it would just feel like too much work to dig through an entire dictionary of possible things to do. So, if you wanted to keep working on it, I might focus more on refining what you have, for example you could have “how to get started” links on each activity. reply jacquesm 17 hours agoprevNeat! In the same spirit I&#x27;m trying to put together a minimalist gaming platform for kids that want to learn how to code. The gratification of a game is such that messing with the code if you have access to the source is a given and so far all my kids have taken to hacking on the games almost as fast as they did to playing them. Mission accomplished, nothing like seeing a 7 year old modifying a game and uploading it to a microcontroller, I didn&#x27;t expect that end result but I&#x27;m very happy to take it. reply erkjs 16 hours agoparentSounds great! I could probably use it myself, I&#x27;m still learning the basics. reply jacquesm 16 hours agorootparentNeat, I am working on the thing right now and will soon post an article on how it is all put together. What really interests me is to lower the barrier to programming as far as possible but not to have it be some visual stuff where you click things together. reply imafish 16 hours agoprevThe name is off to me. More natural:- whaaatabout- whatabooout reply erkjs 15 hours agoparentOuch! I was not sure about it. Is english your first language? I&#x27;m asking out of curiosity, I&#x27;m italian reply tikwidd 6 hours agorootparentIt&#x27;s because the first vowel in &#x27;about&#x27; is an unstressed schwa. It sounds unnatural in English to stress or lengthen a schwa.(The only exception I can think of is &#x27;evennn&#x27;. e.g. Bob and Sue are at a dinner party with friends. Bob tells everyone that he likes all vegetables. Sue knows he doesn&#x27;t like broccoli, so she nudges Bob and says \"evennn...?\". This makes sense if &#x27;even&#x27; in fact contains a syllabic nasal consonant rather than schwa: &#x2F;&#x27;ivn=&#x2F;.)That said, I don&#x27;t think it&#x27;s a big deal! reply cole-k 15 hours agorootparentprevNative English speaker, I agree with the parent.I don&#x27;t think it&#x27;s a big deal, though. The site itself is more important than the name.Edit: If I were to say a drawn-out \"what about\" to someone I would probably say \"what abooout\" but both suggestions sound okay to me. reply joshuamcginnis 16 hours agoprevMy first three recommendations: shooting guns, consuming marijuana brownies, axe throwing. Oh my. reply bicx 12 hours agoparentJust don’t do them all at the same time, and it should be pretty fun. reply fundad 15 hours agoparentprevThis reads like those SEO spam headlines that link to a slideshow. reply erkjs 16 hours agoparentprevOh wow, you got lucky! These are probably the most hardcore ones reply sentientslug 14 hours agoprevNot so sure about this. I like the idea but for my chosen filter combo (which would be a relatively common one) there seem to only be three activities? reply erkjs 13 hours agoparentI&#x27;ll add more activities, I wanted to collect feedback before spending too much time on it reply kevinwang 16 hours agoprevI looked at a bunch of the ideas, they were all pretty good! reply erkjs 16 hours agoparentThank you, I appreciate it! I spent quite some time selecting and organizing them reply glennonymous 11 hours agoprevA couple of thoughts: I’m unable to go back with my browser’s ’Back’ key. not being able to revisit an activity reduces the site’s usefulness. Also, I’d recommend adding a feature that lets users add activities. reply erkjs 10 hours agoparentNoted, thank you! reply hn_user82179 14 hours agoprevI adore this! It already has neat ideas that never would&#x27;ve occurred to me, and indoor&#x2F;outdoor or active&#x2F;sedentary are the options I&#x27;d prefer to turn on&#x2F;off when deciding what to do. Really looking forward to you adding more suggestions! reply erkjs 13 hours agoparentHappy to hear it, will do! reply nxobject 10 hours agoprevI love this! People here like to think that the time for small quirky and lean webpages is gone - well, this is it here and now. reply pants2 12 hours agoprevThis is awesome! I have a list of activities somewhere I need to suggestIt would be cool to have daytime&#x2F;night filters too if we&#x27;re looking for something to do at night! reply hn_throwaway_99 13 hours agoprevPerhaps this is a bit \"old man yells at cloud\", and this is not particularly directed at you (as I see it all over the Internet these days) but I&#x27;ve grown to despise the use of fake, AI-generated \"stock images\" for sites like this. They make me want to get off the Internet altogether.The reason, though, that I think it&#x27;s especially bad for your project is because the whole purpose of your site is to try something new. I like the saying \"Be brave enough to be bad at something new\". So many of us get into a vein (often subconsciously) of always comparing ourselves to others because basically all of the examples we see online are the 0.001% experts that make it to the top of some feed algorithm. So, for example, why use a \"perfect model specimen\", complete with fake veins, for your \"Bouldering\" example, https:&#x2F;&#x2F;www.whataaabout.com&#x2F;imgs&#x2F;bouldering.png? Why use a \"Hallmark card perfection\" example of a (fake) smiling grandmother and her daughter, https:&#x2F;&#x2F;www.whataaabout.com&#x2F;imgs&#x2F;familyTree.png, for \"Creating a family tree\"? An almost fairytale scene for \"Scavenger Hunt\", https:&#x2F;&#x2F;www.whataaabout.com&#x2F;imgs&#x2F;scavengerHunt.png (though apparently some of the cars in the lower right are about to be sucked into another dimension).Your site should be about people embracing the joy of the reality of doing something new, not more fantasy land bullshit that doesn&#x27;t exist.Sorry, will get off my soapbox now and go back to eating my soup. reply erkjs 12 hours agoparentIt&#x27;s a fair point and I partially agree with you. The reason why the images are this way is on one side because that&#x27;s what the AI comes up with, which is clearly biased (but it took me long enough just to get to a photorealistic look). And on the other side because I think a more aspirational look will push you more than a strictly realistic one. If I see the muscled guy bouldering I think how I could become, and it just looks \"easier\". While if I see a regular person who can barely hold onto the wall I just think of how bad the experience might be and could end up not even considering it. reply hn_throwaway_99 12 hours agorootparent> And on the other side because I think a more aspirational look will push you more than a strictly realistic one. If I see the muscled guy bouldering I think how I could become, and it just looks \"easier\". While if I see a regular person who can barely hold onto the wall I just think of how bad the experience might be and could end up not even considering it.This is the part I pretty strongly disagree with you. There is a reason \"Comparison is the thief of joy\" is a popular saying. People are pretty quickly going to be exposed to the reality of the situation, at which point the gap between their fantasy and reality will be a huge demotivator.In any case, there are lots of free stock image sites you can use to find images that actually depict reality. Not saying they aren&#x27;t staged, but there are lots that I think would be \"friendlier\" to newbies, which is what your site is about. reply shermantanktop 10 hours agorootparent> There is a reason \"Comparison is the thief of joy\" is a popular saying.New to me! But it’s great and I will use it. A related one that I use (with my kids) is “Don’t compare your insides to other people’s outsides.” reply bicx 12 hours agoparentprevFor such a small project done while learning to code, it was probably a choice between stock&#x2F;AI images like these, or no images at all.I think it looks fine, and ads some visual cues for what the activity actually is. reply hn_throwaway_99 12 hours agorootparentNot saying they aren&#x27;t staged, but I still think reality is still a better option, e.g.:https:&#x2F;&#x2F;www.pexels.com&#x2F;search&#x2F;bouldering&#x2F;https:&#x2F;&#x2F;www.pexels.com&#x2F;search&#x2F;grandmother%20and%20grandchild...https:&#x2F;&#x2F;pixabay.com&#x2F;images&#x2F;search&#x2F;scavenger%20hunt&#x2F; reply erkjs 12 hours agorootparentThese actually looks great, I didn&#x27;t know about Pexels. I&#x27;ll have a look, thank you! reply y-curious 16 hours agoprevSuggestion: reset filters button reply erkjs 16 hours agoparentMakes sense, thank you! For now, I&#x27;d suggest to reload the page for a quick reset reply bonsn 9 hours agoprevReally cool can definitely use something like this, thanks! Even more activities would be great.Suggestions:- Perhaps a way to view all activities? or activities within the specified selections rather than having to use the suprise- I noticed repeating activities before hitting the 70 limit, perhaps some memory? reply your_friend 11 hours agoprevThe website led me to a Wikipedia article with a list of cognitive biases.That was fun, thank you. reply erkjs 10 hours agoparentYou are welcomed! reply praveen9920 12 hours agoprevPerhaps adding a page for users to suggest things would improve location specific stuff. reply serf 16 hours agoprevjust give me a button so that I can look at the list, or pre-cache the future results. it&#x27;s obnoxious to spam a button over and over again on a website with poor performance from click-til-load. reply legerdemain 12 hours agoprevI got \"visit the post office,\" \"go to the supermarket,\" and \"defrost something from the freezer.\"...Oh wait, that&#x27;s just my \"to do\" list for today. reply oriettaxx 12 hours agoprevwow, super cool! reply cwp1989 11 hours agoprev [–] The woodworking stock photo is… funny lol replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The author has developed a website called whataaabout.com with the purpose of offering ideas for quick and distinctive experiences.",
      "The website aims to introduce variety into the author's daily routine and foster a mindset of lifelong learning.",
      "Users are encouraged to provide feedback and suggestions on the activities organized on the website."
    ],
    "commentSummary": [
      "Users have provided feedback about the short list of activities and misleading cost categories on the website Whataaabout.com, as well as bugs and inappropriate activity suggestions.",
      "The creator plans to add more activities based on positive feedback, and users have generally given positive feedback on the website, with suggestions for additional features and more realistic images.",
      "There are discussions on the use of CBD as a treatment for epilepsy in children and suggestions for various activities during the holiday break, including trying CBD and creating a minimalist gaming platform for children to learn coding. Users also provide feedback on a website that provides random activities, suggesting improvements and more variety."
    ],
    "points": 202,
    "commentCount": 83,
    "retryCount": 0,
    "time": 1703603588
  },
  {
    "id": 38774165,
    "title": "Using Make: Simplifying the Makefile Process in Linux",
    "originLink": "https://text.causal.agency/001-make.txt",
    "originBody": "MAKE(7) Miscellaneous Information Manual MAKE(7) NAME Using Make – writing less Makefile DESCRIPTION Let's talk about make(1). I think an important thing to know about make(1) is that you don't need to write a Makefile to use it. There are default rules for C, C++ and probably Fortran. To build foo from foo.c, just run: make foo The default rule for C files uses the CFLAGS variable, so you can set that in the environment to pass flags to the C compiler: CFLAGS=-Wall make foo It also uses LDLIBS for linking, so you can add libraries with: LDLIBS=-lcurses make foo Obviously writing this every time would become tedious, so it might be time to write a Makefile. But it really doesn't need much: CFLAGS += -Wall -Wextra LDLIBS = -lcurses foo: Assigning CFLAGS with ‘+=’ preserves the system default or anything passed in the environment. Declaring foo as the first rule makes it the default when ‘make’ is run without a target. Note that the rule doesn't need a definition; the default will still be used. If foo is built from serveral source files, unfortunately a rule definition is required: OBJS = foo.o bar.o baz.o foo: $(OBJS) $(CC) $(LDFLAGS) $(OBJS) $(LDLIBS) -o $@ This rule uses LDFLAGS for passing linker flags, which is what the default rule does. The ‘$@’ variable here expands to ‘foo’, so this rule can be copied easily for other binary targets. If some sources depend on a header file, they can be automatically rebuilt when the header changes by declaring a dependency rule: foo.o bar.o: foo.h Note that several files can appear either side of the ‘:’. Lastly, it's always nice to add a clean target: clean: rm -f $(OBJS) foo I hope this helps getting started with make(1) without writing too much Makefile! EXAMPLES The example Makefile in its entirety: CFLAGS += -Wall -Wextra LDLIBS = -lcurses OBJS = foo.o bar.o baz.o foo: $(OBJS) $(CC) $(LDFLAGS) $(OBJS) $(LDLIBS) -o $@ foo.o bar.o: foo.h clean: rm -f $(OBJS) foo AUTHORS june@causal.agency This document is produced from mdoc(7) source available from https://git.causal.agency/src/tree/www/text.causal.agency Causal Agency September 17, 2018 Causal Agency",
    "commentLink": "https://news.ycombinator.com/item?id=38774165",
    "commentBody": "Using Make – writing less MakefileHacker NewspastloginUsing Make – writing less Makefile (causal.agency) 201 points by todsacerdoti 15 hours ago| hidepastfavorite178 comments heads 10 hours agoMake is the grilling of the hacker world. You just want to light up some quality lumpwood charcoal and get a nice smokey crust on some steaks, cook up some hamburgers, and crisp up some chicken for your guests over the course of an afternoon……but oh no there is a constant stream of inquisitive folk coming over, beer in hand, suggesting cool adjustments &#x2F; hacks &#x2F; “improvements” to your setup. With grilling it’s actually pretty easy to defend oneself from this unsolicited advice. There’s only one grill after all. It’s not like it’s some company wide grill that anyone can make changes to solely by convincing one other person to sign off on their PR &#x2F; MR &#x2F; changeset.I’ve seen some very “clever” makefiles (and cmake lists.txt) over the years. It is hacker catnip for all manner of Jackson Pollock ad hoc scripting rubbish and I lament every commit that did anything except make the makefile less complex! All new software engineers should be given a shiny ball to play with when they get bored with their day job, lest they decide to take out their playful creativity on the build system instead. reply m463 8 hours agoparentmake is very useful. The idea that things depend on each other and are built as a hierarchy is important.But it is the opposite of beauty, or elegance, or organization.Honestly all the millions of man months that have gone into writing makefiles should have gone into making make more usable.for example:all the implicit rules in this article? Nobody¹ uses them, they should just be explicit.You shold explicitly distinguish between a target and a file and a directory. don&#x27;t let them depend on each other helter-skelter.indent, tabs, spaces. what a mess.requiring continuation lines and juggling return codes?macros that break with whitespace?I could go on, but ugh.I actually like cmake better than make. Just having IF statements and a flow makes things more understandable. But yeah, you trade one set of problems for another.look up \"modern cmake\" and you&#x27;ll see the nitty-gritty between nice theory and ugly practice.[1] for practical purposes reply dmd 7 hours agorootparentApparently the reasoning behind a bunch of Make’s warts is that by the time Stuart Feldman realized the problem, it already had half a dozen users, and, well, he wasn’t gonna go and break it for them! reply interroboink 7 hours agorootparentI don&#x27;t see why there couldn&#x27;t be (even today!) some statement like \"require version XYZ\", which Make could notice, which would enable new features and nicer syntax. Something like \"editions\" in Rust, though the idea is much older.For some reason, people prefer to add more layers on top of Make rather than improving the core language and tool. Maybe there&#x27;s some good reason? reply m463 6 hours agorootparentI think people treat make like a dmv trip or taxes.They want to get in, get it done, and get out.But who wants to improve the DMV? reply matheusmoreira 4 hours agorootparentprev> rather than improving the core language and toolGNU Make has been steadily gaining new functions though. I&#x27;ve been reading the manual again recently and there were a few functions I did not recognize: let and intcmp. The new integer comparison function is particularly notable since people have gone to some rather incredible lengths to implement arithmetic in make:https:&#x2F;&#x2F;gmsl.jgc.orgIt&#x27;s also possible to write native plugins for GNU Make to load at runtime. New functions can be implemented this way. It works just like C extensions for other scripting languages. There&#x27;s also Guile extension support but unfortunately it doesn&#x27;t seem to allow new function definitions. reply alwillis 3 hours agorootparentprevAll I can say about these critiques: I learned make a few years ago and it wasn&#x27;t nearly as bad as it was made out to be.As someone who cares about the aesthetics of code, I thought I would hate using make but I ended up enjoying writing modern makefiles.When make was created back in the day, I get why certain choices were made, like with virtually all Unix tools of that era. But we don&#x27;t have to use them in the same way. In general, our tooling is so much better that the so-called warts of make are more like minor annoyances than show stoppers. reply masspro 12 hours agoprevQuasi-tangent: I was once hit by a coworker doing first code review on a new repo with \"hmm...I&#x27;m not familiar with using Makefiles as a project management tool. So something something [don&#x27;t remember] we should replace that.\" It struck me as weird because I don&#x27;t see `make` as a build tool so much as automating shell script snippets you would&#x2F;could type at the command line. From that POV I conceptually see Makefiles as like a `bin&#x2F;` folder full of little scripts, honestly the main advantage for me being every command runs with working dir being where the Makefile is; no need to write a `set -euo pipefail; HERE=\"$(dirname \"$0\")\"` dingleberry at the top of every hypothetical `bin&#x2F;` script. And having incremental rebuild is sugar on top when it&#x27;s possible to write :)I don&#x27;t see `make` as a build tool because I believe from experience that is where the road to hell begins. But it is convenient to have `make` call CMake&#x2F;Cargo&#x2F;pip&#x2F;whatever. Plain `make` can build, `make test` can test, `make fmt` can auto-format... Even better if you alias `m` to `make` in your shell. reply gregwebs 12 hours agoparentYour coworker&#x27;s experience is more principled: Make is a mediocre tool for executing commands. It wasn&#x27;t ever designed for that. Although it is pretty common to see what you are mentioning in projects because it doesn&#x27;t require installing a dependency.For a repo where an easy to install (single binary) dependency is a non-issue, consider using just. [1] You get `just -l` where you can see all the command available, the ability to use different languages, and overall simpler command writing.[1] https:&#x2F;&#x2F;github.com&#x2F;casey&#x2F;just reply hathawsh 11 hours agorootparentWith all due respect, I don&#x27;t understand the first part of your comment. Make&#x27;s core purpose is to execute commands, isn&#x27;t it? How was it not designed to execute commands? reply nine_k 11 hours agorootparentNo, no. Make&#x27;s principal purpose is to put a set of files into a desired state. It can \"make\" a particular file by invoking a dependent graph of commands that produce that file from other files. It checks timestamps and only run steps where the resulting files are older than some of the (transitive) source files.You can invoke it by naming a named rule, not a file, but the logic will remain.If this is not what you want to be doing, and if your Makefile is full of .PHONY targets, you likely need a Justfile instead, or (worse) plain shell scripts. reply KajMagnus 10 hours agorootparentI also don&#x27;t like Make for running commands.I was using Make for running commands, but it interprets all command line parameters as build targets, which was annoying. And Bash scripts in Makefiles aren&#x27;t type safe, became messy.Nowadays I use Make only for building. And Deno + Typescript instead, for running commands & scripts.(Hadn&#x27;t heard about Just — the scrips aren&#x27;t type safe though?) reply maleldil 9 hours agorootparent> Hadn&#x27;t heard about Just — the scrips aren&#x27;t type safe though?Just runs code written in other languages. It&#x27;s `sh` by default, but you can define anything (e.g. `python3 -c` to run Python scripts). reply BeetleB 7 hours agorootparentprevI tell people not to use make if there&#x27;s no need for a dependency graph.Just don&#x27;t.And do just :-) reply twaw 3 hours agorootparentYeah, use make -j8 instead. reply alwillis 11 hours agorootparentprev> Make&#x27;s core purpose is to execute commands, isn&#x27;t it? How was it not designed to execute commands?Some people on HN believe that if you use make primarily as a task runner that you&#x27;re doing it wrong and should use something else. I don&#x27;t agree.Most users are using multiple aspects of make: as a task runner and as something that handles the dependency graph when building something digital.As a web developer, I used npm scripts, grunt, gulp, etc. to manage web builds but now I only use make and have gotten off the merry-go-round of web build tools. reply TheSoftwareGuy 7 hours agorootparentWhat’s the point of using another tool to manage your shell scripts though? Why not just use shell scripts reply Izkata 1 hour agorootparentMakefiles coordinate your shell scripts, using a dependency tree you&#x27;d have to reimplement if you did it purely with shell scripts.For example:A) You have a recipe that installs dependencies if they&#x27;re not up-to-date (such as \"pip install\").B) You have a recipe that compiles stuff if it&#x27;s not up-to-date; it&#x27;s made to depend on A.C) \"make\" can be an interface recipe that depends on B and does nothing else.D) \"make test\" can be an interface recipe that depends on B, then runs tests.E) \"make run\" can be an interface recipe that depends on B, then actually runs the code &#x2F; a webserver to interact with.Nowadays whenever I put one of these together there&#x27;s 2 versions of A (one for python, one for node), 1+ versions of B (webpack build without watch, maybe others depending on the project), 3 versions of D (\"test-js\", \"test-python\", and \"test\" that does both, and each of them only requires the relevant parts of A and B).Makes it trivial to ensure you&#x27;re up-to-date after a \"git pull\" without having to waste time waiting on things that don&#x27;t need updating. reply alwillis 3 hours agorootparentprev> What’s the point of using another tool to manage your shell scripts though? Why not just use shell scripts.When you create a Makefile, you&#x27;re describing a dependency graph of what depends on what and the order tasks need to take to produce a particular set of digital artifacts--you&#x27;re not writing shell scripts.Turns out there are all kinds of edge cases and foot guns that make handles for you that you&#x27;d otherwise have to deal with if you wrote a bespoke shell script.Plus make is battle tested and has been around since the dawn of Unix--there&#x27;s literally no build scenario it can&#x27;t handle.I hadn&#x27;t used make until a few years ago but it&#x27;s been one of the best investments I&#x27;ve made in my workflow. reply alwillis 10 hours agorootparentprevOut of the box with no configuration, Fish shell provides target completions for a Makefile in the current directory [1].[1]: https:&#x2F;&#x2F;fishshell.com&#x2F;docs&#x2F;current&#x2F;interactive.html reply sime2009 11 hours agorootparentprevA similar tool is `task` https:&#x2F;&#x2F;taskfile.dev&#x2F; . It is quite capable and also a single executable. I&#x27;ve grown to quite like it. reply wirrbel 11 hours agorootparentI prefer task over just, while I am not a huge fan of YAML, we now use it everywhere so it just makes sense to not learn yet another DSL for Just and just use YAML. reply bravura 11 hours agorootparentHow do task and just compare to SCons?Although SCons is Python (which is a pro or con depending upon your perspective), it has strong dependency management. Or is the argument that dependency management is part of build, not general project maintenance? reply wirrbel 10 hours agorootparentStarting out from the blog post, it talks essentially about Make as it was intended, as a build system to compile programs with. Make maps this to the task of producing a file from input files, which are written down in the form of rules in a Makefile. a key ingredient from make is that it checks for timestamps on disk for the source files and updates targets only if the source files have been modified after the targets have been built.If you go a bit further down this route, you end up with build tools that generate the compilation rules for you in some form: These are Automake&#x2F;CMake&#x2F;Meson and SCons. I did use scons years ago and it was nice, but its definitely completely lost its market share. IIRC Scons does this without generating Makefiles.Task and Just are following a different route. The problem people have solved by using a \"hack\" in Makefiles (PHONY targets), so that you can easily run \"sub-commands\" in Make (make install_deps, etc). It would never occur to me to use Scons in that space.Btw. a third option is to use a shell script like the following (POSIX-shell compatible actually). sub_install_deps() { set -e -x # ... } sc=$1 case $sc in \"\"\"-h\"\"--help\") sub_help ;; *) shift \"sub_${sc}\" \"$@\" if [ $? = 127 ]; then echo \"Error: &#x27;${sc}&#x27; is not a known sc.\" >&2 echo \" Run &#x27;${prog_name} --help&#x27; for a list of known scs.\" >&2 exit 1 fi ;; esac reply MrOxiMoron 12 hours agorootparentprevjust is great, I add it everywhere, just test, just run, just fix, just shell. just works ;-) reply mort96 11 hours agorootparentprevHonestly Make is a pretty decent tool for executing shell snippets plus some lacklustre dependency resolution stuff on top. reply masspro 12 hours agorootparentprevThis looks really nice. But then should `j` alias to `just` or to `jobs`? Non-starter. &#x2F;s reply hathawsh 11 hours agoparentprevI really like GNU Make because it has a hidden superpower: the \"-j\" parameter enables instant, easy parallelization. I have a project with many subprojects and I use \"-j16\" to invoke the same command in all of the subprojects, 16 at a time. It saves a lot of time and it works for all commands that don&#x27;t touch other subprojects.Like you, I use Make as a front end to lower level build tools. It seems to fit that role well. reply maccard 8 hours agorootparentI think this is a great example of tools being stuck in the 90&#x27;s, actually. Make should be parallel by default.I get it, I do. I know there&#x27;s all sorts of legacy reasons, but my 4k resolution screen with 12GB vram and 32 cores still runs make build serially, outpitx to an 80x24 window and is bottlenecked down by stdout flushing, all because we can&#x27;t change defaults. reply stouset 8 hours agorootparentAnd annoyingly, I don’t think there’s a way to tell `make` that a particular `Makefile` should be built with multiple threads by default. So you have to specify `-j` every time. reply ttyprintk 10 hours agorootparentprev‘-j -l $MAXLOAD’ is even more polite on shared systems. Avoids most runaways, but not all. reply t43562 11 hours agoparentprevMake is declarative - that&#x27;s roughly the point of it. You give it rules and dependencies and variables and it&#x27;s supposed to work out what to do.If your build is extremely simple and fast then it cannot add value. reply 01100011 12 hours agoprevNice to see some love for make.Sure, make syntax sucks. It&#x27;s arcane and hard to google much of what is going on. (Shower thought... a tool that lets you view a Makefile, giving explanatory tooltips for syntax elements would be great).But make is widespread, ancient and eternal. It isn&#x27;t going anywhere, despite decades of new tools trying to take its place. It does many jobs relatively well.I&#x27;d go so far as to say that the platonic ideal of development is:make # Build the projectmake install # Install the projectmake clean # Clean the project build dir(s)and whatever new tool you use to actually build things should live behind make. You should have a really good reason to deviate from this. It&#x27;s approachable and familiar to most devs(well, ok, older C&#x2F;C++ devs). Sure, write your own custom tool that builds everything in a chroot jail or docker container, has sub tools for installing, running tests, etc... but please, put it behind a make frontend. reply ParetoOptimal 12 hours agoparent> the platonic ideal of development is:> make # Build the project> make install # Install the project> make clean # Clean the project build dir(s)IMO, that last step of cleaning is very not platonic ideal. However... maybe a reality of incremental development?The first step for me in a lot of cases is just trying a project to see if it&#x27;s worth my time, typically that would just have been:cd &#x2F;tmp git clone http:&#x2F;&#x2F;foo&#x2F;some&#x2F;project.git cd project # build steps, perhaps in a good case your make example above, a dockerfile, or nix flakeThese days though I&#x27;m ecstatic when I see a Nix flake because I can reduce that to: nix run github:some&#x2F;projectThen for development I could even avoid cloning things myself (and sometimes do) with: nix develop github:some&#x2F;project cd &#x2F;tmp unpackPhaseMake some change: genericBuildI guesss all this to say my platonic ideal includes getting more busy work out of the way and at least in the case of building, not having to worry about cache affecting reproducibility.I guess all of the above is to try and express my different idea of platonic ideal here and how Nix gives it to me (and could perhaps to you, for a cost). reply layer8 10 hours agorootparentIf nothing else, make clean is useful to recover disk space not needed anymore, which can be substantial. It’s like deleting a cache or a tmp directory. It can also help to fix the build process by resetting it if something got stuck in the wrong state, but that’s not necessarily the primary purpose. reply hawski 11 hours agoparentprevFor make there is much better option than googling: https:&#x2F;&#x2F;www.gnu.org&#x2F;software&#x2F;make&#x2F;manual&#x2F;make.html and hit Ctrl+F. You can even easily search for things like $ But make is widespread, ancient and eternal. It isn&#x27;t going anywhere, despite decades of new tools trying to take its place. It does many jobs relatively well.Agreed--make is eternal. One thing that&#x27;ll probably be true: in a post-apocalyptic future, make will still be around and still be useful. reply matheusmoreira 9 hours agoparentprev> But make is widespread, ancient and eternal.Yeah. The GNU Make is literally everywhere, it&#x27;s too useful and it&#x27;s got too many good features. GNU Make is worth learning and using just because of this. Having to deal with non-make systems is seriously annoying.> It does many jobs relatively well.I wonder if anyone else other than me was insane enough to try and use GNU Make to manage dotfiles? I even blogged about it.https:&#x2F;&#x2F;www.matheusmoreira.com&#x2F;articles&#x2F;managing-dotfiles-wi...Gotta be really careful with stuff like that. GNU Make is simultaneously a rather lisplike metaprogrammable language and a turing tarpit. I screwed up a personal project once because I got sidetracked essentially reinventing a fraction of autoconf in pure GNU Make code.Now I try to keep things as simple as possible. I still understand the makefile so I suppose it&#x27;s simple enough. reply alwillis 9 hours agorootparent> I wonder if anyone else other than me was insane enough to try and use GNU Make to manage dotfiles?Not a bad idea IMHO… thanks for the blog post. I&#x27;m tempted to try something similar. reply matheusmoreira 9 hours agorootparentThe real makefile that I use and wrote about has some features that I didn&#x27;t get around to describing in the blog post. Also has a ton of comments.https:&#x2F;&#x2F;github.com&#x2F;matheusmoreira&#x2F;.files&#x2F;blob&#x2F;master&#x2F;GNUmake...The metaprogramming template I described is used to implement XDG Base Directories. The links take the XDG variables into account while the real files live in their default locations inside the repository. reply bombcar 5 hours agoparentprevgradle is just a makefile with bells on and you’ll never convince me otherwise reply bigdict 14 hours agoprevThe article reads I think an important thing to know about make(1) is that you don&#x27;t need to write a Makefile to use it. There are default rules for C, C++ and probably Fortran.And then it goes against its own spirit... You don&#x27;t need this: OBJS = foo.o bar.o baz.o foo: $(OBJS) $(CC) $(LDFLAGS) $(OBJS) $(LDLIBS) -o $@Just do: foo: foo.o bar.o baz.oIt respects LDFLAGS and LDLIBS. reply Karellen 13 hours agoparentBut the OBJS allows for an easy `clean` target, defined further down the article.Personally, I&#x27;d have gone with foo: $(OBJS) $(CC) $(LDFLAGS) -o $@ $ The example Makefile in its entirety: CFLAGS += -Wall -Wextra LDLIBS = -lcurses OBJS = foo.o bar.o baz.o foo: $(OBJS) $(CC) $(LDFLAGS) $(OBJS) $(LDLIBS) -o $@ foo.o bar.o: foo.h clean: rm -f $(OBJS) fooBecome: CFLAGS += -Wall -Wextra LDLIBS = -lcurses OBJS = foo.o bar.o baz.o? foo: $(OBJS) foo.o bar.o: foo.h clean: rm -f $(OBJS) foo reply Brian_K_White 13 hours agorootparentprevAh got it. Agreed. reply gumby 14 hours agoprevWith gnu make you don&#x27;t even need OBJs for a simple directory in which all the .c files are compiled: .SECONDEXPANSION foo: $$(patsubst %.c,%.o,$$(wildcard *.c)) $(CC) $(LDFLAGS) $^ $(LDLIBS) -o $@Admittedly you can end up with short, very general Makefiles that look like they were a Prolog program written in TECO. But the advantage is that they are general so don&#x27;t need to be fiddled with as your project grows.I make small Makefiles like this that ensure my local build environment is correct for the package, then call cmake. reply adrian_b 13 hours agoparentUsing these and other similar features of GNU make it is possible to write a generic Makefile that works for any software project.It appears that almost nobody reads the manual of GNU make, despite the fact that it is extremely instructive.I have read the GNU make manual once, about 25 years ago. Then I have written a set of small Makefiles that I have used in all my software projects forever, until now, with only extremely small changes during the years, e.g. when new compiler options have appeared, or when I have added compilers for additional CPU targets or operating system targets or additional programming languages.It is possible to make such generic Makefiles that will work in any software project.For example, in the simplest case, when the source files are in a single directory and you use that directory also for building the project, copying a small template Makefile that includes the appropriate generic Makefile for the target CPU and operating system is enough so that \"make\" will identify all the kinds of source files that exist in the directory, generate the necessary dependence rules, invoke the appropriate compilers and build the executable.For any more complex project, only a minimum of information needs to be added to the template Makefile. For instance, when the build is done in another directory than the one with sources, or when there are multiple directories with source files, a list of directories with source files must be added into the template Makefile. Lists of non-standard libraries, non-standard library directories and non-standard include directories may be added. Additional options can be specified, e.g. building a shared library, not an executable file.Besides these, nothing specific to the project needs to be written. Adding or deleting source files or renaming source files do not need any changes in the Makefile, but after such changes a \"make clean\" is recommended.Instead of this simple and obvious approach, more than 99% of the software projects that I have seen use absolutely horrible huge and unmaintainable Makefiles, or they use \"make\" replacements like \"cmake\", which are much worse than the traditional \"make\", so I fail to understand why anyone would want to use them. reply dagmx 13 hours agorootparentI think you’re overstating the ease of use of make.How does one discover dependencies in a cross platform way with Make without writing the logic themselves to stay up to date with platform changes? Or picking up configuration options from dependencies.How does one make use of Ninja with make? Or discover changes to sdk paths for new platforms?You end up having to duplicate that logic across every repo that needs it. Along the way you accrue lots of variance.In the end you get something that is difficult to debug and scale. That’s why so many projects moved to cmake. It scales better. In much the same way that you can technically do anything you need with C, but other languages add ergonomics and scalability that people value more. reply nocombination 12 hours agorootparentFunny to mention cmake when GNU Autotools handles all of these dependency resolution with grace using only a few lines of autoconf.CMake script syntax is not very great, and debugging is no better than sprinkling printf&#x27;s throughout... So, why not use Autotools? &#x2F;bin&#x2F;sh has been the norm for most build processes, to the point where the later-designed YaML syntax is _effectively_ the SAME as a Makefile with &#x2F;bin&#x2F;sh statements running the pipeline procedures.CMake came along and tried to fix complexity in Autotools while adding kitchen-sink baggage along the way. reply dagmx 11 hours agorootparentI’m not specifically advocating for CMake , though it is my preference for many reasons like faster uptake of multiple platform and compiler features.I was merely pushing back on the person perplexing on why people don’t just use Make. reply angra_mainyu 2 hours agorootparentWhy cmake and not meson? reply adrian_b 12 hours agorootparentprevDiscovering the dependencies depends on the compilers, not on the platform.I have stopped using MSVC many years ago, so I do not know how it handles dependencies, but with gcc or clang that works regardless of platform, with compiler-specific options.Moreover, software for one platform can be built on another platform. What matters is only the platform used for building, where the compilers are hosted, not the target platform for the built software.In general, I extract all platform specific definitions, like the names of the compilers and their command-line options in platform-specific Makefiles, which include a generic Makefile with platform-independent definitions, rules and make targets. Any platform-specific details, like SDK paths, are encapsulated in the definitions of certain \"make\" variables.While I have not needed to update the platform-independent generic Makefile for decades, the platform-dependent sections need updates from time to time, but that is not done for every software project, but perhaps once per year or more seldom, when significant new compiler versions, standard libraries or other new software tools become available.I have not seen yet any reason for using Ninja. Perhaps it may be faster when building something like chrome, but because chrome is likely to be the slowest-compiling software project known to mankind, it is hard to tell how much is gained by Ninja. For software projects of more typical sizes I have not seen noticeable advantages of Ninja. Traditional make can keep all the cores of a CPU 100% busy, so there is no way any other building system can be appreciably faster. There are some very bad Makefiles that are intrinsically slow, but that is a problem of those Makefiles, not of \"make\" in general.Using a minimum project-specific Makefile section can only simplify debugging, not making it difficult.I have never seen any evidence for the claim that cmake scales better. On the contrary, at least in all the open-source software projects that I have ever seen cmake is a major source of bugs in the building process that nobody knows how to fix. (i.e. after some software updates cmake fails to find files that exist, presumably due to some errors in the cmakelists that appear to be arcane enough that they are hard to find) I have never seen in projects that use traditional make such errors that are so frequent in all the projects that use cmake.I have a lot of experience in building software projects, because for many decades, with the exception of a few professional programs without alternatives, I have been using only programs that I compile from sources. Most projects have very bad Makefiles, which are very hard to maintain, i.e. to change when files are added, deleted, moved or renamed, but at least they build the software projects reliably. Whenever cmake is used, building a new version is always an adventure with unpredictable outcome. reply dagmx 12 hours agorootparentI should rephrase, by dependencies I mean third party dependencies. That is on the build system not the compiler.And make tends to fall significantly behind Ninja in my experience for cold builds. Here’s a post from someone else that echoes my experience https:&#x2F;&#x2F;david.rothlis.net&#x2F;ninja-benchmark&#x2F; but for a sufficiently complex project even a well tuned make build is about 20% slower on CI, which is about 10-20m per build.It also seems like you’re giving make the benefit of the doubt when it comes to “most being badly written but my own is great”, while simultaneously deriding cmake without giving it the same benefit.But to your larger point, and going back to my C vs other languages comparison (let’s pick C++ for arguments sake). Yes Make can do it all, but even by your own statements it requires doing everything correctly and there’s significant room for error. CMake makes the trade off of abstraction for a more consistent experience with minimal work.This is similar to C and C++, where yes C can (1) do much better in the very very specific right hands, but (2) it can also become an unwieldy mess. C++ with RAII may seem perplexing from the lens of only accepting the former and not the latter. It lets people spend their time elsewhere on the system. reply t43562 11 hours agorootparentNinja excludes a lot of features of make which are expensive to implement and cause it to be slower to parse so naturally in certain situations ninja runs faster than make. If your build time is actually affected by how long it takes to read the makefiles then this is important (e.g. building Android). reply adrian_b 11 hours agorootparentWhen the makefiles are well organized, all of them have just a few lines, with a few definitions, perhaps at most ten lines, but usually less, and they include a single bigger common Makefile, with most of the definitions, all the rules and all the make targets.So reading all the makefiles should take a negligible time, even in a big project.I have seen too many projects with huge makefiles and with much more makefiles than necessary, but all of those are examples of misuse of make, which are easier solved by an efficient use of make, instead of by replacing make with another tool.For instance, there are many projects with a Makefile in each directory with source files, which is a very bad choice that multiplies the number of makefiles. Makefiles should exist only in a dedicated build directory, not in source directories, except for very small projects with a handful of source files.The best way IMO, is to have a single very small Makefile for each final file that must be built, e.g. executable file or library, in a dedicated subdirectory of the build directory. This allows for a maximum simplification of the Makefiles and their number and size is independent on the number of source directories and on the number of source files. reply t43562 10 hours agorootparentMake does a lot more than just reading makefiles - sometimes it re-reads them as a result of something that happened in the makefile itself.It&#x27;s also doing a lot of variable expansion and if the makefile uses macros like define....endef to generate rules it has to expand those.Make is also often trying all sorts of pattern rules to see if they will allow it to fulfill dependencies. This takes time.If makefiles are small and you run make on each one separately then you miss dependencies across modules and you get the syndrome where program A doesn&#x27;t get rebuild when library B changes.So there&#x27;s a price to pay for everything. Ninja is a lot faster because it just doesn&#x27;t have some of these features and you can make a big makefile where dependencies work properly with it and not pay the price of slow parsing.I worked on builds that took 12 hours on a large cluster of build machines where parsing was 45 minutes - so it matters. The more general solution would be to use a packaging system like the Linux distributions but even that doesn&#x27;t save you from every problem as I also found out when working with OBS on a linux phone distribution. reply adrian_b 11 hours agorootparentprevThe link provided by you shows correctly that Ninja is indeed faster in the cases when almost nothing is recompiled, so the command execution time is dominated by the dependency evaluation time, which is faster in Ninja.However, the same link shows that even for relatively large projects the time difference is less than a second, which matches my experience.Ninja requires much more effort for using, as it is not a complete build system, but just the execution component, and except for projects of chrome size that effort is not worthwhile for a subsecond gain in certain scenarios.I cannot imagine how a \"tuned\" make build can be 20% slower, except when the project is not really tuned, but it is badly organized. A \"tuned\" Makefile means that the execution time for \"make\" is negligible in comparison with the execution times of the compilers and linkers. The time spent in \"make\" should be measured in seconds an most, never in minutes.Only a very slow file system can cause \"make\" to be much slower than Ninja, because the dependencies for \"make\" are stored in many \".d\" files, equal in number with the source files. On modern SSDs or RAM disks, that is never a problem.Perhaps you are right about CMake. I cannot be certain whether CMake is good or bad, because I have never created a CMake project myself, I have only built CMake projects created by others. Nevertheless, there certainly is a problem with the CMake users that I have not seen yet one that can explain which are the advantages of CMake. All the tutorials that I have ever seen about CMake were showing how to do in a complex way things that can be done in a simple way with GNU make, so I never had any reason to investigate any further.What I know for sure is that much fewer understand how to use CMake than how to use make, because the frequency of bugs in CMake projects is much greater.I would like to see how CMake can provide minimal work. With GNU make, creating a new very simple project needs only copying a template Makefile in the source directory. For a more typical software project, the template Makefile must be edited to add a list of directories, those that must be searched for source files (I actually write the list as a prefix directory plus a list of subdirectories for it, as that is how I normally structure the projects).For external dependencies, up to 3 lists may be added, of libraries, include directories and perhaps of library directories, which should suffice.That is all. Nothing more. I normally build each target file of a project, e.g. executable or library, in a separate subdirectory of the build directory, so by default it is not necessary to write it in the Makefile, because the name of the subdirectory will be used for the target. The root of the build directory contains a Makefile that I never change and which descends in each subdirectory to build its target.How can be CMake more simple to use than this? All CMake projects that I have ever seen were much more complicated and they were very difficult to modify, with a very large number of project-specific details that have no place in the project configuration files, because the building system should be able to deduce them.All the examples of using CMake that I have ever seen have demonstrated a tool with a much lower level of abstraction than GNU make and which is tedious to use.GNU make does not need any such thing like the CMakeLists that are ubiquitous with CMake. reply dagmx 10 hours agorootparent> GNU make does not need any such thing like the CMakeLists that are ubiquitous with CMake.Then what is a Makefile?And if your dependencies only amount to looking for source files, then that’s fine. Many are more complex than that and include configuration options themselves.Again, I think you’re hyper indexing on just your own setup and ignoring anything outside of that, while simultaneously hand waving the number of bad makefiles out there and any shortcomings to drive home that Make is simplest.Meanwhile by your posts, you’ve had to create an entire scaffolding of support around your makefiles to make it appear that simple. How is that any different than CMake or any other build system&#x2F;generator? In the end you’ve just orchestrated your own version of such a thing and are decrying any other system as overly complex or just needing adjustments.I believe we’re just talking past each other, but I’d really encourage you to consider that the reason you’re confused why people don’t just use Make is perhaps well founded and rooted in the number of things you have outright dismissed or don’t care about for your own projects. I’m not even specifically advocating for Cmake, but there’s no great mystery why so many projects use it or other build systems; it mostly lets them shift the responsibility of configuration to the system and focus on their task at hand. Again, exactly the same as C vs C++&#x2F;Rust&#x2F;etc… replyt43562 11 hours agorootparentprevIn this kind of situation the templates tend to become complicated stores of knowledge and choices about how to build something for a particular platform and there are usually some complications about maintaining them.It all depends on what you want out of the system - the ability to build against multiple versions of a specific linux distribution or compatibility across unix or even to non-posix operating systems. So the complexity exits from the generic parts where you say: PROGRAM:= bob SOURCE:=fred.c LIBRARIES=gtk include build_program_template_$(PLATFORM).mkand enters the templates for building on a specific platform where you have to decide if that&#x27;s gtk4 or gtk2 and how to tell the program what it can&#x2F;cannot do on that specific platform.If you&#x27;re the one building the code and deciding where to do it that&#x27;s cool. For people who want to port your code to some other platform, cmake or autoconf will help discover what needs to be done on some specific plaform, check that the needed features&#x2F;libraries are on the platform and construct a build with all the features it can support on that platform turned on and the ones it cannot cope with turned off.They automatically do part of the job you might be doing manually when maintaining templates. This is useful when you&#x27;re giving the build to someone who just wants it to work and doesn&#x27;t know the ins and outs like you do. But of course they are complicated and for all the great functionality they are far more complicated to fix when they don&#x27;t work :-D reply zabzonk 12 hours agorootparentprevi have a couple of old blog articles that cover the basics of writing a generic makefile for c++ here: https:&#x2F;&#x2F;latedev.wordpress.com&#x2F;2014&#x2F;11&#x2F;08&#x2F;generic-makefiles-w... reply IshKebab 2 hours agorootparentprevYeah Make can be used for \"any software project\" if your idea of a complex project is that it has multiple source directories! reply bigdict 13 hours agorootparentprevLove this, I aspire to build a similar system myself one day. reply bigdict 14 hours agoparentprevJust do: .SECONDEXPANSION foo: $$(patsubst %.c,%.o,$$(wildcard *.c))No need to tell Make how to invoke a C compiler for the billionth time. reply rurban 13 hours agorootparentBut then you need the name it GNUmakefile when you use incompatible GNU extensions.BSD makefiles do look better. reply JadeNB 14 hours agoparentprev> a Prolog program written in TECOI know we don&#x27;t do general-purpose \"I like this\" comments on HN, but, as a Christmas treat to myself, I&#x27;m just going to say how happy this little sentence fragment makes me. reply gumby 5 hours agorootparentThanks! I aim to please.As it happens I used to program in both Prolog and TECO, in the same period of time but of course not on the same code bases. reply lhamil64 13 hours agoparentprevI admittedly don&#x27;t have a ton of experience with make, but what&#x27;s the point of doing this vs just throwing the compile commands in a shell script? reply nrclark 11 hours agorootparentImagine you had all your build commands in a shell-script.Now imagine that you want to run individual steps from time-to-time, instead of running your whole script start to finish. So maybe you write functions that represent each group of commands, and call the function based on an input argument.Now imagine that some of your build-steps have dependencies on other build-steps, and you want to be able to say \"run step X, plus whatever else you have to do first.\" Or maybe you want to retry a failed build from wherever it left off, instead of having to re-run the entire thing.Now imagine that you&#x27;d like to have some level of isolation between build-steps, so that one step doesn&#x27;t mess the other one up.As you add more and more things to your build script, you&#x27;ll find yourself writing an implementation of Make pretty quickly.Now- sometimes - your build fails halfway though, and you want to resume from where-ever it failed. S reply dpkirchner 13 hours agorootparentprevThe biggest practical difference is that make can test to see if a file needs to be rebuilt before proceeding. reply adrian_b 11 hours agorootparentYes, exactly this.When compiling a project once from a clean state make has no advantage over a shell script that would use a tool like \"parallel\" to distribute compilation jobs over all available CPU cores.However writing such a script for parallel compilation is more complex than writing a simple Makefile executed with \"make -j N\".As you say, the main reason of existence of \"make\" is the acceleration of the recompilation of a project after incremental changes in the source files, when only the files that depend on the changes are recompiled, saving time. reply t43562 11 hours agorootparentprevMake is declarative. You tell it how to make an A from a B and a B from a C or a D and it then works out that it needs to do \"CBA\" or \"DBA\" depending on whether it finds a D or a C in the filesystem.The more possible variations you have to cater for the worse build scripts get.If you try not to redo work that you did 5 minutes ago that&#x27;s still valid then build scripts become even more complicated whereas the makefile doesn&#x27;t change at all.Other tools do declarative even more than make does but as they get easier to understand they tend to lose abilities. reply f1shy 13 hours agorootparentprevMake will automatically do all the steps, and stop if any one fails. Of course you van do manually, but is much more work. reply dagmx 12 hours agorootparentThe irony is that the same argument would work against Make in the description further up vs other build systems like CMake etc.Not to take away from your correct answer of course reply gumby 5 hours agorootparentThat’s because CMake calls make or an equivalent (ninja, xcodebuild…) that actually does all the dependency checking! reply Brian_K_White 13 hours agoparentprevGlobbing is possible but unwise. You do want a variable with an explicit list of sources, even while trying to make things as automatic and dynamic as possible. reply adrian_b 13 hours agorootparentWhy would you want that?In decades of programming in very varied environments I have never compiled any software project otherwise than by using globbing, so that I have never needed to waste time to write the name of any source file in a Makefile.I have never seen any reason to do otherwise.For instance, using special compilation flags for a certain source file, different from the others, is something that I consider a mistake. Whichever is the goal, it certainly can be achieved by other means (e.g. pragmas or attributes).Moreover, the dependencies for any file must always be generated automatically, they must never be written explicitly, which removes the main reason why in bad Makefiles the names of the source files are written individually. reply gumby 5 hours agorootparentThe problem with globbing is gratuitous matches. If a.cc depends on l.h and m.h, and b.cc only depends on m.h, doing a glob in the dependencies ( $(wildcard…) ) will unnecessarily rebuild b.o if l.h changes. reply an_d_rew 12 hours agorootparentprevGlobbing means that the directory has to be scanned on every buildOnce you start getting to multiple directories and recursive scans, the build time begins to grow enormously and usually unexpectedly.Not a problem at all for small or even medium size projects on a fast machine.But once you start building very large projects with multiple sub-projects, it generally slows the machine to a crawl on every build. reply Brian_K_White 7 hours agorootparentI said unwise not inefficient.Globbing is a security and reliability hole. You don&#x27;t ever really know what got included or will get included, you just assume you do, and you are right only most of the time and only by luck not by actually ensuring it.Saying things like \"only the files I expect should ever be in this dir\" is just wrong swiss cheese thinking. (you never said that, but another commenter essentially did) reply adrian_b 12 hours agorootparentprevWhen the project is so big that this is a problem, you can make an additional make target, e.g. \"make file_lists\" that would use globbing and store the lists into a file that will be used by any other make commands.The bigger a project is, the more important is to use only globbing and never write explicitly any file name lists.However, I believe that this, the slowness of globbing for big directories, might be a strictly Windows-specific problem, if it exists. At least on Linux and with XFS (which has B-tree directories) I have never seen globbing to take any noticeable time, even on directories with ten thousand files or more, where it still appears to be instantaneous at the human reaction time scale.There is no reason to ever do recursive scans. All the directories with source files of a project must be scanned to search for any source files located there, but only once. reply dataangel 13 hours agorootparentprevglobs don&#x27;t even prevent you from special casing one file to have different flags reply adrian_b 12 hours agorootparentTrue, but this should better be avoided, because in such cases reading the source files is not enough to understand what they do.You must also be aware that they are compiled in a different way and you must search a frequently too big and obfuscated Makefile to discover which is the applicable compilation rule.Unfortunately I have seen many projects that had used such tricks and it was always painful for maintenance to discover how they were supposed to work. reply dataangel 13 hours agorootparentprevGlobbing is how you avoid pointless busy work. If there&#x27;s a file in the directory that shouldn&#x27;t be built, it shouldn&#x27;t be there. reply Brian_K_White 7 hours agorootparentAnd nothing that shouldn&#x27;t happen ever does. I guess you also include \".\" in $PATH, even roots. reply adrian_b 12 hours agorootparentprevWhen you want to exclude a file from compilation without deleting it, it is enough to change its extension, e.g. from \".c\" to \".c_\".Alternatively, you could move such a file to a subdirectory \"Attic\" or the like.Any of these solutions would ensure that globbing will no longer include that file in the list of source files, without needing to update any project configuration files. reply rochak 12 hours agoprevGonna get downvoted to oblivion for saying this but I haven&#x27;t hated a tool more than make. reply bxparks 12 hours agoparentNot downvoting you. Just curious to know why? In my experience, as long as the Makefile is less than about 100 lines, it&#x27;s the most useful workflow system ever. Because it&#x27;s available almost everywhere. After about 100 lines, yeah, it is not great. And that idiotic tab character. I know the historical reason why tab was used. I wish they had fixed it to accept both tabs and spaces a long time ago. reply nickelpro 12 hours agorootparentIt&#x27;s a poor, verbose, arcane solution in every problem space it purports to solve.It was fine in its era, but that&#x27;s a long time ago. It is completely unsuitable as a project management tool and a poor task runner.Also its ubiquity is overhyped. There&#x27;s no build platform where I can&#x27;t download the tools I need, that&#x27;s the entire point of a build platform, and most such platforms come with far more advanced tools pre-installed. reply t43562 10 hours agorootparentMake has few dependencies so it&#x27;s a way to build an operating system up from ground 0. Any tool&#x2F;language that wishes to be generally useful in an OS doesn&#x27;t want to be built by something that is only available much further up the tree like python.So it gets used a lot and it&#x27;s only a language for dependencies and rules. It doesn&#x27;t do \"project management\". Its the simplicity of what it&#x27;s trying to that saves it from ever becoming irrelevant - because it can be made to fit almost any use case. It&#x27;s not a special tool for enforcing one structure or building only one language - something that seems highly regressive to me but which is adopted by many languages now. reply nickelpro 10 hours agorootparent> Any tool&#x2F;language that wishes to be generally useful in an OS doesn&#x27;t want to be built by something that is only available much further up the tree like python.This is a non-priority, an unreal use case. I can&#x27;t build GCC without a functioning C++ compiler and a fairly sophisticated OS environment and that&#x27;s fine. Real-world use cases for bootstrapping a build environment from rubbing two sticks together are so exceptionally rare as to be near fictional.Even if we allow that such cases exist, they are unicorns, not a thing to design tools around.> It&#x27;s not a special tool for enforcing one structure or building only one language - something that seems highly regressive to me but which is adopted by many languages now.This is what makes it so unsuitable. A tool that can make no assumptions about its application is less and less useful a tool. A bread knife is better at cutting bread than a plain 10\" kitchen knife, a boning knife better for deboning, etc.We live in a world where for every language there are build tools that know far, far more about the needs of that language environment than make, and thus are far better suited. reply t43562 10 hours agorootparentMake exists and is used widely and not because everyone&#x27;s too inept to understand your points.The \"assumption-making\" build tools are the unicorns that inevitably cannot dominate because they&#x27;re not generally applicable. The more they assume the more niche they become. reply nickelpro 4 hours agorootparent> The \"assumption-making\" build tools are the unicorns that inevitably cannot dominate because they&#x27;re not generally applicable.They already do dominate. In the last bastion where make can be said to be popular (C) make has already lost to CMake and its usage shrinks every year [1]The situation is moving even faster for C++ [2]And for literally every other language in the systems programming space (C#, Swift, Rust, D, Go, Zig), make was never a player to begin with.[1]: https:&#x2F;&#x2F;www.jetbrains.com&#x2F;lp&#x2F;devecosystem-2022&#x2F;c&#x2F;#which-proj...[2]: https:&#x2F;&#x2F;www.jetbrains.com&#x2F;lp&#x2F;devecosystem-2022&#x2F;cpp&#x2F;#Which-pr... reply t43562 33 minutes agorootparentcmake generates makefiles (or ninja) so it doesn&#x27;t replace them. It replaces autotools - it works out what features a system has and what build flags to enable and from a generic description of your program it generates makefiles or ninja.build.There&#x27;s nothing fun whatsoever about fixing build problems with cmake - because you have to understand make&#x2F;ninja AND cmake. It&#x27;s slightly easier to understand than autotools, though, where one has the same problem. reply 0x1ceb00da 7 hours agorootparentprevMake is popular because many old projects use it and it comes pre-installed on Linux. It&#x27;s a cultural thing. reply t43562 24 minutes agorootparentIt works on an enormous number of platforms with few dependencies. People have got these projects which build on everything from embedded platforms to mainframes and it has taken effort from many different people to achieve that.So you might come up with some new thing that \"Works For Me\" and now all those people who are peacefully using the tool on their whatever platform have to fiddle with it to make it work again - or the person who made it work is no longer around and that platform loses support for the new versions.When you say \"cultural\" you make it sound like people do things because they are ignorant and resistant to change but it&#x27;s worth at least considering that their point of view is quite different. replywokwokwok 9 hours agorootparentprevYou see, this is why I personally hate make: automake.As you say, a small elegant makefile is lovey.That is not the sort of makefile you get from the autoconf&#x2F;automake family, and (ha!), good luck if something goes wrong.The reality is that a small simple makefile is not sufficient to build real software.The problem is fundamentally that make doesn’t compose well.Most real build systems such as cmake, scons, cargo, etc. provide primitives for dealing with complex messy situations like “oh today I’m using visual studio not clang” or “does this platform have stdbool.h?” or “is the flag for disabling a warning different on this compiler?”…and a way of composing that tasks into smaller ones (eg. add_subdirectory() and find_package()).Make does not.Make is a simple tool for simple tasks.The reason people don’t like it, in my experience, is they have had to work with a make based project that has grown beyond the trivial size and it’s become a nightmare.It makes easy things easier, and hard things much harder.If worked on SPAs where they used make instead of webpack; but it’s a stupid solution. It doesn’t do live reloading, or any of the other pipeline stuff… but by gosh they tried!You know what I like about clojure? It’s a simple tool that scales well for both simple and complex tasks.Make simply doesnt.…and that is reflected in the reality, which is that increasingly build tools are moving away from it and towards others, even for the low level tasks that people used to “generate makefiles” for eg. to ninjaI really am not a cmake fan; but there absolutely no denying it works very well in many real world situations, where trivial “-lsqlite” flags in a naive makefile don’t and cant work. reply bufio 9 hours agorootparenthttp:&#x2F;&#x2F;9p.io&#x2F;sources&#x2F;plan9&#x2F;sys&#x2F;src&#x2F;mkfile reply wokwokwok 7 hours agorootparentYou can find an exception to any rule, but you&#x27;ll find, in general, that its both self-evident and unsurprising that make is on the decline, and being replaced by other tools.plan9 exists in an enviable position of only needing to build in a few controlled environments.For example, http:&#x2F;&#x2F;9p.io&#x2F;sources&#x2F;plan9&#x2F;sys&#x2F;src&#x2F;libmach&#x2F;mkfile reads: CFLAGS=$CFLAGS -I&#x2F;sys&#x2F;src&#x2F;cmdCute. Non-portable. A perfect example of how make is useful in trivial or contrived circumstances only.&#x2F;shrugTry reading the llama.cpp makefile, which is an example of an excellent cross platform makefile that works very well: https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;blob&#x2F;master&#x2F;Makefile^ this is what many real Makefiles look like, and this one is excellent and well maintained.Many are worse.Make is good when you only have easy problems to solve (like, specifically, only having to build one specific constrained environment for one specific compiler). reply bxparks 8 hours agorootparentprevWoah, what is that @{ construct in that mkfile? I cannot find it in the GNU Make Manual (https:&#x2F;&#x2F;www.gnu.org&#x2F;software&#x2F;make&#x2F;manual&#x2F;make.html). I use backslash-before-newline for multi-line loops like that. reply enriquto 10 hours agorootparentprev> I wish they had fixed it to accept both tabs and spaces a long time ago.I have good news for you: this is already the case! In all modern versions of make you can use semicolons instead of tabs.You just write target : dependencies ; ruleinstead of target : dependencies ^Iruleif, for some reason, you hate the beautiful tab characters. reply bxparks 9 hours agorootparentHa, what if the &#x27;rule&#x27; is 5 lines long?Many times, a Makefile snippet in an HTML page or a PDF file has had the tab character auto-magically morphed into spaces. So when it is copied, it will be a syntax error in a Makefile. Even copying Makefile snippets from one terminal window to another will eat those tab characters. Fortunately my vim editor is configured to handle most of those edge cases now, but it took me... I don&#x27;t know... 10-15 years to get that right.My personal rule is that control characters which are visually indistinguishable from other characters have no place in source code which are meant to be written, read, and copied by human beings. So yeah, that includes thecharacter. And yes, I do think the Golang people got that wrong, although the &#x27;go fmt&#x27; command largely solves that problem. Except when reading Go source code on web sites which don&#x27;t handle tabs well, and you are nested 6 levels deep, and half the code is unreadable because it&#x27;s bled off the right side of the page. reply twaw 3 hours agorootparentIn my editor, tabs are shown with reddish background, so they cannot be mismatched with spaces. Fix your editor. reply enriquto 7 hours agorootparentprev> control characters which are visually indistinguishable from other charactersThis is a text editor configuration issue, not a character choice issue. replykristjansson 7 hours agoprevOutside of simple C projects, a Makefile should be just a “here’s how to hold it” reference for the rest of your build system. Shoehorning in all of the build complexity is going to be overwrought, but a good “make dev; make clean; make build; make release” is a very welcome entry point for new or infrequent contributors reply raggi 14 hours agoprevIf you want to write less makefile, you don&#x27;t need to define foo. `foo` is an implicit target if foo.c is present.You can use make in this form without a makefile even: ~ % echo &#x27;#include void main() { printf(\"OHAI\\\"); }&#x27; > ohai.c ~ % make CFLAGS=-DDEFINE_ME_A_MAKEFILE ohai cc -DDEFINE_ME_A_MAKEFILE ohai.c -o ohai ~ % .&#x2F;ohai OHAI reply gumby 14 hours agoparentIn that case why call make at all? reply raggi 14 hours agorootparentCC, CFLAGS and LDFLAGS conventions, and freshness checks (if you do it twice, it won&#x27;t build again as it performs an mtime check on the source). reply bb88 11 hours agoprevThis page just makes me miss manpages in general.Not every manpage was great, but many were, and many could also be formatted, printed out, and placed into a binder for easier reading.What I enjoyed most about manpages is that the people that cared to write good documentation on things like bash.If you have groff installed you can: man -Tpdf man >man.pdfThe only problem with groff is that it doesn&#x27;t support system standard fonts. reply enriquto 11 hours agoparentWhy do you speak in the past tense? Man pages are alive and well, and we use them every day. reply ttyprintk 10 hours agoparentprevOn recent vim, :Man crisply delivers within the same editor tab. reply yegle 13 hours agoprevThe author should mention the automatic variables: https:&#x2F;&#x2F;web.mit.edu&#x2F;gnu&#x2F;doc&#x2F;html&#x2F;make_10.html#SEC94Specifically, in one of the examples, to list all dependencies as the input to `cc`, you can write `$^` instead of `$(OBJS)`. reply djbusby 13 hours agoparentOBJS is a little more clear to what&#x27;s happening than $^I&#x27;m not a fan of magic symbols. Hard to remember, especially across languages. reply slau 10 hours agorootparentMake actually has some of the more sensible shortcuts, which are easy to remember$@ -> the target (like an email address)$^ -> the dependencies (look up)$ only the first dependency (look up sign, but rotated 90s left to point at the first one) reply dextercd 9 hours agoprevI wish people actually defined the dependencies between .c and .h files and .h and other .h files. When a project doesn&#x27;t do this, I always and up doing a `make clean && make` after getting stuck on a weird bug because I just assume it&#x27;s not rebuilding something it should.Nowadays I use CMake and am really happy with it. reply wruza 5 hours agoprevBack in 2000s I was all in the bash make unix camp. That was for ten years or so. Now I realize that it was just a religion, at least on my part. Python and Node and their libs are thousands of times better than all that nonsense (including arcanes like CMake).I have a whole folder of personal tools I’ve written in bash, and had a similar set for make. Some of them I don’t want to touch, but when I have to, I just rewrite them in Node with a little io&#x2F;ipc helper library I created and it feels like a breath of fresh air. reply psyclobe 5 hours agoprev30 year veteran here, funny enough I barely know anything about make. My first job we were building our own build utility because we had to build on windows netware, dos, Linux… soon after that it was all cmake, and I never had to use a make file for much of anything.Only recently have I come to be kinda fond of its insanely simple syntax but really make hasn’t been a requirement to native development since as long as I’ve been around getting paid. reply bsdpufferfish 14 hours agoprevIt&#x27;s unfortunate that most programmers only experience with make is automake&#x2F;autoconf. This is fantastic. reply NelsonMinar 14 hours agoparentAutoconf, that&#x27;s the thing that spends time making sure that my software can be built on BeOS and A&#x2F;UX? reply asveikau 14 hours agorootparentMore like some obscure pre-c89 stuff. But it takes actual work and testing to get that working, so most people have a configure script that checks for all that stuff but when it&#x27;s time to compile it only actually works on their specific Linux distro. reply im3w1l 13 hours agorootparentprevI have a vivid memory of autoconf. I set it up best as I could. Someone ran .&#x2F;configure. It triggered a new run of autoconf that failed because of mismatching autoconf version. Why it triggered a new run? Because of mtime checks. Why did they say it needed to be rerun? Because git doesn&#x27;t preserve mtime. I guess it&#x27;s just assumed that everyone distributes their sources as a tarball. I spent many hours trying to figure out how to bypass those issues, and I can&#x27;t remember if I ever succeeded.These days I just use Rust instead. reply dgfitz 8 hours agorootparent> I spent many hours trying to figure out how to bypass those issues, and I can&#x27;t remember if I ever succeeded.Might I suggest “got-restore-mtime” for this, upstream in Debian and Ubuntu already.Did you really spend hours on this? reply e12e 7 hours agorootparentThis?https:&#x2F;&#x2F;manpages.org&#x2F;git-restore-mtime reply dgfitz 5 hours agorootparentSure. Are you trying to imply the existence of a manpage means something ubiquitous? reply im3w1l 3 hours agorootparentprevWell I wanted to create an easy and familiar process for people installing from source, so I wanted it to be roughly git pull && .&#x2F;configure && make && sudo make install. So I didn&#x27;t consider restoring mtimes into that flow, just looked for options for autoconf to not regen.Also I think it took quite a while to even realize the mtimes were the issue from an error messages, as I was not very knowledgeable about autoconf &#x2F; automake. reply bufio 9 hours agorootparentprevautoconf&#x27;s principal purpose is to waste time. reply ataylor284_ 12 hours agoprevMake&#x27;s killer feature was conditionally rebuilding based on changed dependencies. Back in the day, it was easy for a medium to largish software project to take many hours, or even days to fully rebuild. C and C++ were especially bad, especially \"every file transitively includes every header\" type projects. Make saved a lot of that pain, even if linking still sucked.I love make and still spin up a minimalist Makefile like in the article regularly for tiny projects, but I&#x27;d hate to have to actually maintain one for a real world project. reply ataylor284_ 11 hours agoparentYou could do something like this to get dependencies almost for free: .depends: $(SRCS) $(CC) $(CFLAGS) -MM $(SRCS) -o .depends -include .depends reply t43562 10 hours agoparentprevI see people now with long javascript builds that waste time doing many things repeatedly - it&#x27;s quite ironic. Eventually people find crap to fill up the performance that&#x27;s available.The problem is that now the tools aren&#x27;t designed as composable bits that you can really parallelise with a makefile. C&#x2F;C++ are designed that way and if they didn&#x27;t use header files I&#x27;d consider them perfect from the build system&#x27;s point of view :-). Golang and java for example are a nuisance. reply Too 1 hour agorootparentA lot can be said about js tooling but honestly it works good once it is set up. Most modern js tooling provides --watch flags and hot module reloading. Vite is one popular option. Just save one file and the change instantly takes effect in the browser, even on a large project. Never seen any makefile based c++ project come close to that experience. reply t43562 46 minutes agorootparentI&#x27;m glad you had success. It wouldn&#x27;t be difficult to do that with inotify to run make when a file changed. OTOH with webpack and transpiling and various other things I have watched people in my company take quite a long time to see a single change. I think they haven&#x27;t sorted out the build right and I&#x27;m not enough of a javascript expert to do it for them.They prioritise various features over productivity and I think that&#x27;s a mistake. reply andreynering 12 hours agoprevIf you&#x27;re looking to an alternative, you could take a look at Task:https:&#x2F;&#x2F;taskfile.dev&#x2F; https:&#x2F;&#x2F;github.com&#x2F;go-task&#x2F;task reply Arcuru 12 hours agoparent+1 to this recommendation. Everyone likes to point to ‘just’ as a makefile replacement, but having tried both I slightly prefer Task. reply Augenstern 14 hours agoprevI like the man page format each post has on this website. A little hard to read, but very unique. reply edoardo-schnell 14 hours agoparentYeah, 2023 and reflowing documents still isn&#x27;t a thing. Yay reply Conscat 14 hours agorootparentI read this on mobile and it felt pretty tedious. I wonder if maybe some minimal HTML could make the paragraphs reflow properly while still looking like a manpage everywhere. reply Karellen 14 hours agorootparentprevWhich is ironic, given that man pages do reflow on different-width terminals and have done since the &#x27;90s. reply 082349872349872 14 hours agorootparentprevWhat sort of keyhole are you peering through that needs to reflow 72-character text lines? reply plugin-baby 14 hours agorootparentA phone? Your comment is 3 lines for me. reply 082349872349872 14 hours agorootparentJust checked; both my comment and TFA appear full width on my phone. Bog-standard Android. reply dxdm 13 hours agorootparentMaybe other people do not use the same setup as you do. reply 082349872349872 13 hours agorootparentThat much is obvious given that a vanilla phone setup has no trouble with even 90-character lines. Not my business to stop anyone making things hard on themselves, though, so good luck you all!(do we have anyone who reads HN over a morse clacker?) reply pests 13 hours agorootparentI have vision problems, I increase zoom a lot in desktop and mobile.What do you have against people different than youself? reply 082349872349872 10 hours agorootparentNothing; I wished you luck! I imagine people with screenreaders have an even tougher time...(mobile I agree would be hopeless with zoom [I avoid reading on mine even without zoom], but does your desktop zoom mean even TFA&#x27;s vt100-friendly 72-chars needed reflowing?) reply pests 10 hours agorootparentI don&#x27;t always increase zoom but TFA is def too small on desktop. I just increase until its readable, that being said, the article is comfortable for me at about 200-250%. Reflow issues seem to occur at 300% - so no issue for me but I could see some people going that high. replyesafak 14 hours agoparentprevThis is supposed to be functional, not art work. Make it legible; easy to digest.I guess it&#x27;s fitting for a tool (make) that is also unergonomic by today&#x27;s standards. reply Augenstern 13 hours agorootparentI wouldn&#x27;t use this format myself either, but that&#x27;s the fun with personal websites: you can do whatever you want. There&#x27;s no law that says stuff needs to be functional. People who dislike it will just move on. reply mosselman 12 hours agorootparentprevTypography isn’t set. It _is_ about legibility and therefore proper typography _is_ functional.This site simply isn’t as legible as it could be. I am not suggesting it needs to be “pretty”. A monospaced Markdown file would be more legible. reply regurfvjtdf 11 hours agoprevThere’s so much of it in this thread I couldn’t possibly reply to all of it, but wow. Lot of make&#x2F;shell wankery going on. No one is impressed with your pipe. reply Tyr42 11 hours agoprevI honestly wish that make&#x27;s default rules were available to import and view the source of, but we&#x27;re disabled by default.Then a beginner could see what happens in a straightforward way, and then import the C rules. And we could all stop importing the VCS rules which aren&#x27;t useful anymore. reply spc476 9 hours agoparentI don&#x27;t know about POSIX make, but GNU make you can run \"make -p -f&#x2F;dev&#x2F;null\" to get a dump of all the built in rules it has. And if you don&#x27;t want to run those, you can always use \"make -r\" to have it ignore built in rules. reply hgs3 11 hours agoprevYou can use Autotools to generate your Makefiles. Autotools gets a bad rep, but I&#x27;ve found it perfectly fine for simple projects. To get started you only need two files, configure.ac and Makefile.am, you can ignore all the autogenerated stuff. reply assbuttbuttass 14 hours agoprevFor larger programs, the compiler can also generate the dependency tree for incremental compilation: https:&#x2F;&#x2F;www.gnu.org&#x2F;software&#x2F;make&#x2F;manual&#x2F;html_node&#x2F;Automatic... reply datadeft 12 hours agoprevIs there any reason to use make over ninja and gn? reply Too 2 hours agoparentNinjafiles are not designed to be edited by hand, it expects more sane, language-aware tooling to generate the dependency graph, for ninja to execute. https:&#x2F;&#x2F;ninja-build.org&#x2F;manual.html#_design_goalsThough you could also argue that writing makefiles by hand is equally foolish. Use a tool designed for the job and not some duct tape that “works” everywhere but barely keeps things together. The example in the OP is missing handling of headers #including other headers, just to give one obvious example. reply spicybright 12 hours agoparentprevI&#x27;m a seasoned developer for the past 15-ish years and I haven&#x27;t heard of ninja or gn before.I tried typing them in my command line (macos 12.4) and I would need to install them to use them.I was taught make in CS101 in college and the majority of other people&#x27;s projects I&#x27;ve looked at use it.So the advantage is ubiquity and familiarity. Which can be useful if your code base is going to have a large number of people working on it. reply nickelpro 11 hours agorootparent> I&#x27;m a seasoned developer for the past 15-ish years and I haven&#x27;t heard of ninja or gn beforeThen you also stopped learning about developments in your field 15 years ago.> I tried typing them in my command line (macos 12.4) and I would need to install them to use them.Ok? And? We don&#x27;t pre-installed dev tools on consumer operating systems. You won&#x27;t find valgrind or vcpkg either.> I was taught make in CS101 in collegeThe tools we taught to beginners over a decade ago are perhaps not suitable in all cases, in fact they are suitable in very few cases. Not much Pascal usage either anymore.> the majority of other people&#x27;s projects I&#x27;ve looked at use it.Then you live in a very tiny bubble. reply baq 56 minutes agorootparentYou seriously compare whatever this ninja thing is to valgrind with a straight face?There’s value in not overloading your cognitive capacity with the most recent fads, there’s enough serious tools and tech to learn. reply spicybright 6 hours agorootparentprevNo need for personal attacks here, I&#x27;m just answering based on my experience. reply nickelpro 4 hours agorootparentYou&#x27;re the one who made your personal experience the grounding for your defense of make, not me.There&#x27;s no way to respond other than to point out you&#x27;ve had a very bizarre, isolated 15 years if the only thing you encounter regularly is make. In the one language ecosystem where make is still popular (plain C), it&#x27;s a minority that only comes in second overall for build tools [1][1]: https:&#x2F;&#x2F;www.jetbrains.com&#x2F;lp&#x2F;devecosystem-2022&#x2F;c&#x2F;#which-proj... reply datadeft 12 hours agorootparentprevNinja was super easy to pick up even after using make for some time (10+ years). GN is just a ninja generator that is optional.https:&#x2F;&#x2F;gn.googlesource.com&#x2F;gn&#x2F;+&#x2F;main&#x2F;docs&#x2F;quick_start.mdhttps:&#x2F;&#x2F;ninja-build.org&#x2F; reply baq 12 hours agoparentprevYeah, if you have no effing idea what ninja or gn are reply tekknolagi 9 hours agoparentprevThis is why I wrote https:&#x2F;&#x2F;bernsteinbear.com&#x2F;blog&#x2F;ninja-is-enough&#x2F; :) reply t43562 10 hours agoparentprevninja is faster at parsing large makefiles and I&#x27;m reasonably certain it&#x27;s because it lacks certain expensive features of make.Do you need those features? You&#x27;d have to read about make to find out but it boils down to there being not much performance difference for small builds so make might have an edge there. When you get to android sized builds the reading of makefiles can take minutes and ninja wins there enough to make it worth sacrificing the features. reply WesolyKubeczek 13 hours agoprevMake is fantastic. I’m using it to rebuild and run containers in my development environment, and to build a full repository of RPM packages (that my project needs and which CentOS Stream lacks). I suppose that if javascript tooling was more amenable to process one file at a time, I could do parallel transpiling and bundling of a hefty frontend with make as well.I also use its metaprogramming and late evaluation capabilities a lot, heavily inspired by the tricks Buildroot is doing.If you are mindful of its restrictions (it works with “words” separated by spaces, thus paths with spaces in them are a problem unfixable without severely breaking backward compatibility; and your targets need to be expressible as files with meaningful modification timestamps), it’s a very powerful tool. It deserves more appreciation. reply grungyneer 1 hour agoparentI too have found it handy for managing containers, container networks, and images. It&#x27;s prevented me from using docker-compose as much as I pronanly should. reply markhahn 12 hours agoprevnice: friendly but also succinct.how many people are using systems like cmake (or worse) who would be fine with just make (used right)? reply gosub100 13 hours agoprevI say this as someone with 20+ years of experience in C and C++: Using make is a huge waste of time in 2023 (and forward). Learning the intricacies of make actively blocks you from Getting Things Done.Yeah, make is kind of neat because it has this functional what_I_want : how_to_get_it syntax, but it doesn&#x27;t actually work that way. It&#x27;s a huge waste of time to try to get it working so unless you meet the following caveats, make is not for you:- you love trivia like \"who was the 19th president of the US?\"- on a road trip, you stop to look at historical markers- you love steampunk&#x2F;tube amplifiers&#x2F;making pasta from scratch&#x2F;listening on vinyl&#x2F;home brewing beerIf those sound like you, you&#x27;re the type who won&#x27;t regret wasting a couple days getting make running and getting cut on the sharp edges. Most of us who just want to get there will be better with cmake, meson, or hand-rolling a script using a modern language (inb4 \"make does more than compile code\" - yeah, so does ruby or python or any other script, without making false promises or wasting your time) reply spicybright 11 hours agoparent3 paragraphs of rambling for \"make is a waste of time because of sharp edges\"Can you actually articulate what those sharp edges are? reply gosub100 10 hours agorootparentyes, I can reply t43562 10 hours agoparentprevIf meson fits your project it&#x27;s worth using it - it could turn into a straightjacket later but changing to something else isn&#x27;t the end of the world if it cost little effort to implement it in the first place.As for trying to use a \"modern\" language to write scripts.......well if they&#x27;re not declarative and don&#x27;t understand dependencies then how can they be better?make is no paragon - it just contains solutions to lots of problems that exist in build systems. The fact that those problems exist and make has ways to solve them isn&#x27;t make&#x27;s fault. Everyone might perhaps want builds to \"just work\" so they can ignore them and such people write scripts and use the concept of \"rebuild and clean\" a lot and it works until the build gets big and slow.\"modern\" tools get written with many simplifying assumptions and become slowly complicated as they hit all the problems make hit long ago and try to find ways to solve them. reply bufio 9 hours agoparentprev\"Make actively blocks you from Getting Things Done.\" --Quote from someone suggesting CMake. reply mathematicaster 12 hours agoparentprevLost me at \"making past from scratch\". reply jjgreen 11 hours agorootparentIn some sense, don&#x27;t we all make the past from scratch? [stares into middle distance] reply amelius 12 hours agoprev [–] Things get messier when you start to add automatic dependencies. replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The make(1) command in Linux can be used to build C, C++, and Fortran files without the need for a Makefile.",
      "Environment variables like CFLAGS and LDLIBS can be used to pass compiler and linker flags to the make command.",
      "Including a rule in the Makefile allows for automatic rebuilding of source files when header files change, and adding a clean target can remove object files and binaries."
    ],
    "commentSummary": [
      "The discussion revolves around the challenges and frustrations of using Make as a build automation tool.",
      "Some argue that Make is complex and needs to be more user-friendly, while others advocate for its use and highlight its benefits.",
      "Alternative tools like CMake and Ninja are mentioned, with differing opinions on their effectiveness, and the discussion delves into dependency management, parallelization, and Makefile organization."
    ],
    "points": 201,
    "commentCount": 178,
    "retryCount": 0,
    "time": 1703613954
  },
  {
    "id": 38775505,
    "title": "The SAT's Misleading Question: Counting Rotations & Brilliant Online Learning",
    "originLink": "https://www.youtube.com/watch?v=FUHkTs-Ipfg",
    "originBody": "- In 1982, there was one SAT question that every single student got wrong. Here it is. In the figure above, the radius of circle A is 1/3 the radius of circle B. Starting from the position shown in the figure, circle A rolls around circle B. At the end of how many revolutions of circle A will the center of the circle first reach its starting point. Is it A, 3/2, B, three, C, six, D, 9/2, or E, nine? SAT Questions are designed to be quick. This exam gave students 30 minutes to solve 25 problems, so about a minute each. So feel free to pause the video here and try to solve it. What is your answer? I'll tell you right now that option B, or three, is not correct. When I first saw this problem, my intuitive answer was B, because the circumference of a circle is just two pi r, and since the radius of circle B is three times the radius of circle A, the circumference of circle B must also be three times the circumference of circle A. So logically, it should take three full rotations of circle A to roll around circle B. So my answer was three. This is wrong, but so are answers A, C, D, and E. The reason no one got question 17 correct is that the test writers themselves got it wrong. They also thought the answer was three. So the actual correct answer was not listed as an option on the test. Mistakes like this aren't supposed to happen on the SAT. For decades, it was the one exam every student had to take to go to college in the US. It had a reputation for determining people's entire futures. As a newspaper from the time stated, \"If you mess up on your SAT tests, you can forget it. Your life as a productive citizen is over. Hang it up, son.\" Of 300,000 test takers, just three students wrote about the error to the College Board, the company that administers the SAT, Shivan Kartha, Bruce Taub, and Doug Jungreis. - I did a lot of math problems when I was young for the competitions. I probably did thousands of math problems and I read it and I was amazed how badly it's worded. I just put three down. I figured that's what they wanted. - The three students were confident none of the listed answers were correct, and their letters showed it. As a director at the testing service recalled, they didn't say they had come up with possible alternative answers or that maybe we were wrong. They said flat out, \"You're wrong,\" and they proved it. - I discussed it with some other people and said, I think there was a mistake, and they mostly said, \"No one cares.\" I wrote a letter to the Educational Testing Service. It was a little while later they called us and said I was correct. - Here is their argument. The simplest version of this problem is with two identical coins. These have the exact same circumference. So by our initial logic, this coin should rotate exactly once as it rolls around the other. So let's try it. Okay. But wait, we can see it's already right side up at the halfway point. So if we finish rolling it around the other coin, it'll have rotated not once, but twice. Even though the coins are the exact same size There are no tricks here, you can try it for yourself, and I'll do it again slowly. That's one, two. This is known as the coin rotation paradox. This paradox also applies to question 17. I've made a to scale model of the problem. One useful tip for standardized tests, even though they say their images are not to scale, they almost always are. So when we roll circle A around circle B, we can see that it rotates once, twice, three times, and four times in total. So the correct answer to this question is actually four. Once again, the circle rotates one more time than we expected. To understand this, let's wrap this larger circle in some ribbon And I'll make it the same length as the circumference, and then I will stick it down to the table as a straight line. I'm adding some paper here so there's something for this to roll on. And now it rolls one, two, three times. What's happening when we turn this straight path into a circular one is that circle A is now rolling the length of the circumference and it's going around a circle. The shape of the circular path itself makes circle A do an additional rotation to return to its starting point. So this is the general solution to the problem. Find the ratio between the circumferences of circle B and circle A and then add one rotation to account for the circular path traveled. But there is a way to correctly get three. Let's count the rotations of circle A from the perspective of circle B looking out at A. We can see circle A rotates one, two, three times. And it doesn't matter which circle you are looking from, to circle A, it also rotates three times to come back to its starting position around circle B. Similarly, from the perspective of the coins, we can see that the outer coin only rotates once as it rolls around the inner coin. Using the perspective of a circle is just like turning the circle's circumference into a straight line. It's only as external observers that we actually see the outer circle travel a circular path back to its starting point, giving us the one extra rotation. But there's even another answer. If you look closely at question 17, it asks how many revolutions circle A makes as it rolls around circle B back to its starting point. Now, in astronomy, the definition of a revolution is precise. It's a complete orbit around another body. The earth revolves around the sun, which is different from it rotating about its axis. So by the astronomical definition of a revolution, circle A only revolves around circle B once. It goes around one time. Now, other definitions of a revolution do include the motion of an object rotating about its own axis. So one isn't a definitive answer, but the wording of this question is extremely ambiguous if you can justify at least three different solutions. After reviewing the letters from the students, the College Board publicly admitted their mistake a few weeks later and nullified the question for all test takers. - They said they were discounting the problem and they were calling us because they were gonna tell the news and they thought that we should be warned that the news might contact us. I did a bunch of phone interviews and NBC News, they came to my school. They said I was right and they were discounting it. So that was great. - But there's more to the explanation. - It's easy to get an intuitive reason, but it's really hard to formally prove that the answer is four. I could give you some proofs if you want. - Well, that would be wonderful. I think that would be, we'd appreciate that for sure. - I have a whiteboard because I'm a mathematician, so I just happen to have a whiteboard here. Hold on. Can you see that? - Yep. - It turns out that the amount the small circle rotates is always the same as the distance the center travels. All right, so why is this true? Suppose you had a camera and the camera was always pointed at the center. So in your movie, it looks like the center doesn't move. In the real world, the center is going around the circle. Let's say it's going at some speed V. What's the velocity of this point? It's zero, and that's because it's rolling without slipping. If it had any component in that direction, that's what slipping would be. I mean, this is something I think they should have spelled out in the problem, but when you change your frame of reference, the relative velocities don't change. In the movee, the center always has velocity zero. So this point would have to have velocity negative V. So that means the speed that this is turning is the same as the speed the center is moving. So if they always have the same speed, they have to go the same total distance. The total distance this turns has to be the same as the total distance the center moves. In this problem, the center of the small circle goes around a circle of radius four. So the total distance that the center moves is eight pi. What's the total amount that the small circle rotates? It rotates four times, and its circumference is two pi. It's the same number. If it rolls without slipping, the total distance the center travels is the same as the total amount it turns. - And this is always true. Take a circle rolling without slipping on any surface from a polygon to a blob, on the outside or the inside, the distance traveled by the center of the circle is equal to the amount the circle has rotated. So, just find this distance and divide it by the circle circumference to get how many rotations it's made. This is an even more general solution than our answer to the coin paradox where we just took our expected answer, which we'll call N, and added one, and it reveals where this shortcut comes from. If a circle is rolling continuously around a shape, the circle center goes around the outside, increasing its distance traveled by exactly one circumference of the circle so the distance traveled by the circle center is just the perimeter of the shape plus the circle's circumference. When we ultimately divide this by the circle circumference to get the total number of rotations, we get N plus one. If a circle is rolling continuously within a shape, the distance travel by the circle center decreases by one circumference of the circle, making the total number of rotations N minus one. If the circle is rolling along a flat line, the distance travel by the circle center is equal to the length of the line which, divided by the circle circumference, is just N. This general principle extends far beyond a mathematical fun fact. In fact, it's essential in astronomy for accurate timekeeping. When we count 365 days going by in a year, 365.24, to be precise, we say we're just counting how many rotations the earth makes in one orbit around the sun. But it's not that simple. All this counting is done from the perspective of you on earth. To an external observer, they'll see the earth do one extra rotation to account for its circular path around the sun. So while we count 365.24 days in a year, they count 366.24 days in a year. This is called a Sidereal year, Sidereal meaning with respect to the stars where an external observer would be. But what happens to that one extra day? A normal solar day is the time it takes the sun to be directly above you again on earth. But the earth isn't just rotating, it's orbiting the sun at the same time. So in a 24-hour solar day, earth actually has to rotate more than 360 degrees in order to bring the sun directly overhead again. But Earth's orbit is negligible to distant stars. To see a star directly overhead again, Earth just needs to rotate exactly 360 degrees. So while it takes the sun exactly 24 hours to be directly above you again, a star at night takes only 23 hours, 56 minutes, and four seconds to be above you again. That's a Sidereal day. This explains where the extra day goes in the Sidereal year. If we start a solar day and a Sidereal day at the same time, we'd see them slowly diverge throughout the year. After six months, the Sidereal day would be 12 hours ahead of the solar day, meaning that noon would be midnight, and it would keep moving up until it's finally one full day ahead of the solar day, at which point a new year and orbit begins. 365.24 days that are each 24 hours long are equal to 366.24 days that are each 23 hours, 56 minutes, and four seconds long. So it makes no sense to use Sidereal time on earth, because six months down the line, day and night would be completely swapped. But equally, it's useless to use solar time while tracking objects in space, because the region you're observing would shift between say, 10:00 PM one night and 10:00 PM the next night. So instead, astronomers use Sidereal time for their telescopes to ensure that they're looking at the same region of space each night. And all geostationary satellites, like those used for communication or navigation, they use Sidereal time to keep their orbits locked with the Earth's rotation. So the coin paradox actually explains the difference between how we track time on earth and how we track time in the universe. The rescoring of the 1982 SAT wasn't all good news. With question 17 scrapped, students' scores were scaled without it, moving their final result up or down by 10 points out of 800. Now, while that doesn't seem like much, some universities and scholarships use strict minimum test score cutoffs. And as one admissions expert put it, \"There are instances, even if we do not consider them justified, in which 10 points can have an impact on a person's educational opportunities. It might not keep someone out of law school, but it might affect which one he could go to.\" This mistake didn't only cost points off the exam. According to the testing service, \"Rescoring would cost them over $100,000, money that came outta the pockets of test takers. The question 17 circle problem was far from the last error on the SAT. But errors are likely the least of their concerns these days. I mean, the SAT is slowly becoming a thing of the past. After COVID-19, nearly 80% of undergraduate colleges in the US no longer require any standardized testing. And that 1982 exam, well, it didn't turn out too badly for some. How did you do on your math SAT, if I can ask? - I got an 800. Even before that, it was clear I was gonna go into math. I did math competitions. I really liked math. - Do you end up writing any math questions these days? - A while back I wrote problems for a math competition. - And were you careful with how you wrote them, the wording? - I hope so. I tried - Today's deep dive on one SAT question proves there's no substitute for hands-on exploration to understand and appreciate the everyday phenomena of our world. But you don't have to observe earth from space, or make cardboard cutouts to get hands-on with math, science, and cutting edge tech. In fact, you can do it from anywhere right now with this video sponsor, Brilliant, and you can get started for free. Just go to brilliant.org/veritasium. Brilliant will make you a better thinker and problem solver in everything from science to math, data programming, technology, you name it. Just set your goal and Brilliant will design a path to get you there. If you liked today's video, I highly recommend you check out one of my favorite courses on Brilliant, which is Scientific Thinking. Scientific Thinking takes you on an interactive tour of our physical world. You'll engage with key scientific principles and theories from simple machines like gears and pulleys to Einstein's special theory of relativity. Whether you're comparing circuits to understand voltage and current, playing snooker to learn the rules of collisions, or even planning your itinerary for an intergalactic music festival on a space time diagram, you will learn by doing and foster a deep understanding without getting bogged down in heavy mathematical formulas. They even have a version of the coin paradox. And by the end, this course will change how you think about the world around you. Beyond science, Brilliant has a massive library of content in math, data science, programming, and technology, all with that same hands-on approach that makes concepts incredibly easy to understand. And the best part is you can learn with Brilliant on the go. It's like having interactive versions of our videos in your pocket. To try everything Brilliant has to offer for free for 30 days, just visit brilliant.org/veritasium. I will put that link down in the description. And the first 200 of you will get 20% off Brilliant's annual premium subscription. So I wanna thank brilliant for sponsoring this video and I want to thank you for watching.",
    "commentLink": "https://news.ycombinator.com/item?id=38775505",
    "commentBody": "Veritasium: The SAT Question Everyone Got Wrong [video]Hacker NewspastloginVeritasium: The SAT Question Everyone Got Wrong [video] (youtube.com) 194 points by goplayoutside 12 hours ago| hidepastfavorite157 comments js2 9 hours agoThis is an 18 minute video about an SAT problem from 1982. If you&#x27;re not sure you want to commit to it, here&#x27;s a contemporaneous NYT article:https:&#x2F;&#x2F;www.nytimes.com&#x2F;1982&#x2F;05&#x2F;25&#x2F;us&#x2F;error-found-in-sat-que...(Gift link.)And a recent Scientific American article:https:&#x2F;&#x2F;www.scientificamerican.com&#x2F;article&#x2F;the-sat-problem-t...(Both of these are linked from the video description itself.)I&#x27;ll bet Marilyn vos Savant wouldn&#x27;t have got this one wrong[1]. :-)[1]: https:&#x2F;&#x2F;web.archive.org&#x2F;web&#x2F;20130121183432&#x2F;http:&#x2F;&#x2F;marilynvos... reply ramraj07 7 hours agoparentWhile I share your disdain for unnecessarily long YouTube videos, Veritasium often (not always) gets a pass from me. I have been instant clicking pretty much every video he’s released in the past few months and always came out having learned a lot and satisfied.The only other channel I would give the same pass is neo. reply js2 2 hours agorootparentI intended no shade on Veritasium and my comment that the video is 18 minutes long wasn&#x27;t an expression of disdain, merely a statement of fact. Rather, my intent was to offer an alternative for folks who prefer to read instead of or before watching a video. reply TaylorAlexander 5 hours agorootparentprevYou may be interested to see this critique of Veritasium which cooled my views on the channel significantly since I saw it:https:&#x2F;&#x2F;youtu.be&#x2F;CM0aohBfUTc reply jnsaff2 3 hours agorootparentThe thing that put me off veritasium was the electricity video which was immediately controversial.Looking at just the video he left the feeling that all electricity being taught is completely wrong while actually he was talking about some edge case and failed to put it into the proper context. He did not mention at all that the proportion of what he was talking about was essentially irrelevant unless you are an electronics board designer. reply ploxiln 2 hours agorootparentEE details relevant to electronics board designers are kinda important and interesting, and I have a lot of sympathy for \"technically correct\". But that veritasium video was a trick question, and a misleading video. It subtly shifted between ideal model to real-world effects to smugly show how most skilled EEs are wrong about electricity, but Derek knows the truth.In a simplified circuit model it would work like a circuit, in a as-real-world-as-possible model it would always be dominated by interference. In a carefully-segregated-semi-real model, it would be less than 1% on after 1&#x2F;c seconds, would you really call that on? If it was, then 100% on would be way too much current!So many minutes spent without explaining that the real world is very complicated and EEs apply different models to different situations as appropriate, and it&#x27;s silly to say that only the field matters not the electrons because of course the electrons cause the field while the field moves the electrons, and the original question is a trick because it&#x27;s an idealized thought experiment which tries to trick you into applying the wrong model but that&#x27;s silly because it&#x27;s an idealized thought experiment which wouldn&#x27;t work in the real world so it&#x27;s only purpose is to exercise a simplified model ...Great fodder for a series of viral-y videos from multiple youtubers though. (electroboom&#x27;s response was pretty good IMHO) reply quailfarmer 2 hours agorootparentprevHah, that video really ticked me off at the time. Technically true, but only in the least meaningful way possible. reply moffkalast 45 minutes agorootparentYeah I really liked the AlphaPheonix video[0] where he actually built the thing in a field and explained the entire effect properly.I&#x27;m not entirely sure if Derek didn&#x27;t really understand what he was presenting in that video or deliberately based it on a really misleading bullshit gotcha to troll people and stir debate for clicks, and neither reflects very well on him.[0] https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=2Vrhk5OjBP8 reply Jackson__ 1 hour agorootparentprevThe one that I really couldn&#x27;t stand was the \"How to slow aging\" one, in which he interviews a man, who in my opinion clearly had some sort of cosmetic surgery done on himself to appear younger. He talks about how to reverse aging in mice, and then randomly claims he actually reversed aging _in himself_.[0]For anyone following science in mice, this seems like an extremely huge red flag. That together with the cosmetic surgery and a clear financial incentive for the him to lie here, seeing as he is selling a book about it, was triggering just about every bullshit detecting neuron in my brain.Veritasium however seems perfectly fine with it, and still posted the video.I haven&#x27;t ever seen anyone else bring this up, so if anyone wants to share their own hot take on this, I&#x27;d be interested to know :)[0] https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=QRt7LjqJ45k reply hoten 3 hours agorootparentprevThis guy does hour long videos on many large popular YouTubers. I tried watching one but it was so. drawn. out. Is he just creating controversy from nothing? reply deaddodo 1 hour agorootparentHe&#x27;s like Turkey Tom, but instead of just recapping YouTubers&#x2F;YT Drama, which is already the reality TV version of YouTube content, he wants to treat it like hard hitting journalism. Which means he overinflates and exaggerates everything. reply beaned 2 hours agorootparentprevYeah, pretty much. reply gnicholas 5 hours agorootparentprevGiven the length of the video (50+ mins), could you give us a TLDR? What aspects of the channel does the critique take aim at? reply nojs 4 hours agorootparentHe claims Derek’s Waymo video was a paid PR piece and too favourable to the company. He also spends 2 minutes explaining why you need Surfshark VPN reply ramraj07 4 hours agorootparentWhich is obvious to anyone with half a brain though. So what. We have discretion to choose which of his videos to watch. I doubt he got paid by the laws of thermodynamics to make his entropy video. reply j4yav 3 hours agorootparentSure, but there are lots of good science content creators out there and I&#x27;d rather just watch one where I don&#x27;t have to worry about it. The Waymo video is over the top credulous, even for something they were being paid to review. In general, the idea of building a channel based on \"the element of truth\", establishing yourself as an educator, and then shamelessly selling your credibility in that way is kind of a turn off. At least I&#x27;m not going to see you as credible in the same way anymore, and others feel the same.That said, the channel is more popular than ever so who knows. reply moffkalast 42 minutes agorootparentprevHe&#x27;s just in the pocket of Big Physics! &#x2F;s reply gnicholas 2 hours agorootparentprevSo it&#x27;s just about the one video? It&#x27;s not about technology reviews in general, or any of the math stuff (like what was linked in the video above)? reply jraph 6 hours agorootparentprev> The only other channel I would give the same pass is neo.Nice, thanks for the mention, I&#x27;ll check this out. At first sight, typically looks like stuff I would enjoy. reply ramraj07 4 hours agorootparentI would start with their Thai Cave rescue video. I scratch my head how this guy is not on tv. reply holoduke 48 minutes agorootparentIs someone still watching tv? Nobody in my neighborhood watches linear tv anymore. Already for many years. Maybe some elder people? reply EE84M3i 3 hours agorootparentprevIt&#x27;s almost 2024. He seems to do very well on YouTube and has a lot more freedom in his niche. Why would he want to be on TV? reply drdrey 4 hours agorootparentprevhis Kowloon Walled City is incredibly well made reply corethree 3 hours agorootparentprevFirst 10 seconds into the video I know one major reason working against him. He has a bit of lisp. Personally I don&#x27;t mind, but I&#x27;d be lying if I said his videos would be better without it.Fortunately we have youtube. reply alister 8 hours agoparentprevThe New York Times says:The choice was given among these answers: (a) 3 over 2. (b) 3. (c) 6. (d) 9 over 2. (e) 9. &#x27;&#x27;The answer to this question should have been 4, not 3. Daniel B. Taylor, the College Board&#x27;s executive vice president for operations, said as a result of the flawed question, he anticipated score adjustments of 10 points, up or down.I&#x27;m wondering how they corrected the scores since the correct answer was not in the list of choices. I&#x27;m assuming that they simply excluded the question -- so if there were 154 questions in total, they recalculated as if there were only 153 questions. I&#x27;d like to imagine that the three students who reported the error got an extra boost to their scores, but probably not. reply chatmasta 7 hours agorootparentAccording to the video, they rescored all 300,000 affected tests after excluding the question. reply sverhagen 1 hour agorootparentI found the notion pretty shocking how deeply the rescoring affected people. If the test writers couldn&#x27;t even get it right, why fault the test takers. I get it... rules... but it still feels unfair at some angle. reply perryizgr8 9 minutes agorootparentThey should have done max(old_score, re_score) for all test takers. That&#x27;s the only fair way. Although I don&#x27;t know if SAT grades on a curve, which wouldn&#x27;t play well with this method (I think?). reply hightrix 9 hours agoparentprevThank you so very much. Reading this article was so much more valuable to me as I would have never watched an 18 min video to show a single question that needed 1 paragraph to discuss. reply jjnoakes 8 hours agorootparentThe video does go further - it contains some interesting content around how this concept relates to the earth&#x27;s orbit around the sun, the discrepancy between the duration of an earth day and a 360 degree rotation about the earth&#x27;s axis, and why astronomy days are not the same length as earth days. reply jraph 7 hours agorootparentYeah, the SAT question is almost an excuse to speak about this things, and you also get to see an interview with one of the concerned candidates. Very enjoyable video. reply 2muchcoffeeman 5 hours agorootparentThere’s also an interesting note about when the specified answers in the question would have been right, the point of view of the centre circle. reply jjnoakes 3 hours agorootparentI actually didn&#x27;t think that part belonged in there. Considering a rotating point of view seemed contrived to me. reply 2muchcoffeeman 44 minutes agorootparentSurely the intended meaning of the question belongs in the discussion?You’re right, it’s contrived. But I automatically tried to compare lengths too. So it’s a good lesson in making sure that your simplification of the question still does what the question asks. replyfnordpiglet 8 hours agorootparentprevAlso note you can speed it up. I usually watch YouTube on 1.5 speed, so this is a 12 minute video for me ;-) reply drc500free 11 hours agoprevWhat makes sense to me is to think about something that DOESN&#x27;T roll.Suppose I start in Greenwich, walk - without rolling - down the prime meridian to the south pole, up the international date line to the north pole, and back down the prime meridian to Greenwich.How many rotations do I go through? One. I get a full rotation because I&#x27;ve followed the earth&#x27;s curvature all the way around the globe once, even though I&#x27;m walking straight without rolling.So the answer is \"how many rotations due to rolling\" plus \"one bonus rotation for passing around the curvature of the circle.\" reply lordnacho 9 hours agoparentThe way I thought about it was to ask what happens if you just have a coin going around a point, which would be the natural end point of reducing the size of the second coin. That of course would give you 1, so a good first hypothesis is that the total is just however many spins the two coins go, plus 1. reply HarHarVeryFunny 5 hours agoparentprevYes, but that intuition doesn&#x27;t seem to work for the case where small disk is rolling around the inside of large one, not outside. What&#x27;s the intuition for why we then subtract one rotation rather than adding one ?! reply HenryPrickett 4 hours agorootparentVery basically, to go the same direction, a small disk on the inside and the outside of a large disk need to spin opposite directions. When you&#x27;re on the outside, your spinning direction agrees with the direction you go around the circle (i.e. the rotations add). When you&#x27;re on the inside your spinning direction is the opposite of the direction you go around the circle (the rotation subtracts). reply ahepp 1 hour agorootparentprevIt’s concave rather than convex? It’s bending into itself instead of away from itself? reply jodrellblank 7 hours agoparentprevI like this; it gives me a good way to think about the \"from the circle&#x27;s view\" - Earth center looking out at you sees your feet close and your head far away, always; you don&#x27;t rotate at all. reply jagthebeetle 10 hours agoparentprevThat intuition makes a lot of sense to me, especially if I picture the person&#x27;s frame of reference.Kind of reminds me (a layman) of winding numbers. I suppose there are topologically inspired variations of this problem that might be even more \"paradoxical\" (or perhaps just silly). If you moonwalk the second half, you undo your rotation? Or if you follow specially designed subterranean tunnels, you can end up doing 0 or negative rotations! reply charcircuit 1 hour agoparentprev>How many rotations do I go through?0. Walking in a circle is different from spinning. They are 2 separate things that should be counted separately.Adding them together should be a type error. reply pbj1968 7 hours agoprevI’m no genius, but I used to score well on such tests as a youngster. I was handed an IQ test circa ~1996 and got about two thirds through it before I found a question where none of the responses were correct. I brought it to the teacher’s attention. “There is no way you could possibly be right, this test was reviewed first!”Anyway, blame it on photocopy errors or whatever, but the answers were wrong. Still mad about it.I took another one ~2007 when I was looking to be a marketing person for a construction equipment rental company. This one was about 30 questions over 60 minutes. I got to question 26, which was some extraordinarily complicated question about how many cuts would it take to chop down a large board into the pieces you needed. After I wasted several minutes methodically writing this out, I realized the true test was realizing it was a time waster question one should skip. 27-30 were far simpler and then the buzzer ran out on me. reply WalterBright 6 hours agoparentI approach tests by going to the next question if the answer isn&#x27;t immediately solvable. Then, when reaching the end, I go back and spend more time on the ones I hadn&#x27;t solved. That maximizes the score.I apply that technique everywhere. For example, when working the bug list, I&#x27;ll solve the easy problems first, and the toughest last. That also maximizes the score, as the bug submitter&#x27;s only care is if the bug is fixed, not how hard it is to fix. reply remram 1 hour agorootparentGiven a fast enough supply, this means you never look at the difficult bug reports at all. reply sverhagen 1 hour agorootparentIn (software) engineering you want to take on enough of the hard stuff early on, to make sure you&#x27;re not painting yourself into a corner with naive solutions... reply WalterBright 43 minutes agorootparentThere is always the concern that one has made a fundamental error. Can&#x27;t worry about it too much, though, or one will never get anywhere. reply WalterBright 1 hour agorootparentprevIt also clears the mind if one clears out the easy ones first. It&#x27;s like when I feel like working on the car, the first thing is clean the garage.(If the reports come in faster than they can be fixed, of course one can never catch up regardless of the algorithm.) reply nick__m 4 hours agorootparentprevAlmost completely off topic but you are good example of nominative determinism ! reply WalterBright 3 hours agorootparentIn the Air Force, my dad&#x27;s assigned office mate was Major Smart. A line of my ancestors were the Dumbles. reply LanceH 2 hours agorootparentWhile in the Marines, I knew of a Major Hazzard and a Captain Justice. I really wanted to be there when they first met. reply anigbrowl 6 hours agoparentprevthe true test was realizing it was a time waster question one should skipI remember something similar on the LSAT. Odious practice; if you get a wicked problem in a professional context, you don&#x27;t have the option of just blowing it off. Of course, you could say it&#x27;s about learning to prioritize with limited resources, but that&#x27;s meta-gaming the applicants and selecting for corner-cutters. There&#x27;s a place for corner cutting; in emergency situations it&#x27;s sometimes the right thing to do. The problem is that selecting for corner cutters also incentivizes exploiting the system for less noble motives. reply contravariant 1 minute agorootparentWorst one I ever saw was a question to prove something that was vacuously true if you got the preceding question correct.People spent ages trying to find their mistake. I was one of the first to finish because I quickly concluded I wasn&#x27;t going to find the mistake (after checking a mere 3 times). reply kleene_op 1 hour agorootparentprevI don&#x27;t see the problem with a corner cutter question located around the end of a test.It filters people into:- those who don&#x27;t manage time efficiently by investing too much energy on a problem.- those who did cut the corner and as a result got an edge with a few more questions solved- the genius ones, who solved everything, in whatever orderIf the objective of a test is classifying candidates and time management is one of the metric considered, then this kind of testing pattern absolutely makes sense.I was one of those who never cut corners and just focused on the problem at hand until failure, and now that I am working and need to get things done I wished my teachers trained me that skill. reply az226 7 hours agoparentprevMight not even have been a gotcha. They interlace draft questions all the time. They don’t count toward your score. reply pedrosorio 7 hours agoparentprev> Still mad about it.Why? reply fsckboy 7 hours agorootparentthey told him it was an IQ test but it was actually a psychological profile to diagnose his anger issues reply mrfox321 6 hours agorootparentprevFor the same reason why he brought up his iq tests in the first place.Because he self-identifies with some definition of iq. reply orenlindsey 11 hours agoprevThe guy who figured this out (who Veritasium interviewed) is crazy smart.Also, a lot of kids math problems (middle school and below) are super vague. I get that they&#x27;re designed to teach a concept, but they could do it in a more exact&#x2F;precise (idk what the word is) way. reply makeitdouble 10 hours agoparentThe vagueness is seen as a feature, not a bug. The kid is supposed to read between the lines and find out what the problems are really about.That&#x27;s straight how my teachers would explain it to us, and you see that line of thinking in the \"problem solving\" bits where the text is intentionally made harder to decrypt.Reading skills and shared background assessments are baked into most math tests, even if that&#x27;s not the focus of it. reply monkeydreams 9 hours agorootparent> The vagueness is seen as a feature, not a bug. The kid is supposed to read between the lines and find out what the problems are really about.Pity the poor child who is capable of thinking both inductively and reductively. Such lax testing standards reminds me of the story of the WWII cryptographer in Cryptonomicon who is asked to work out how long a ship would take to travel down the river, comes up with a new theory of fluid dynamics, and flunks the test. reply mhh__ 10 hours agorootparentprevPractically I think that just means that the kids end up getting taught vast quantities of \"exam technique\" rather than content.I taught myself the essence of calculus when I was 14 or 15 - there were holes in my knowledge but it took I&#x27;d estimate 5 years for regular education to catch up to the intuition I built then (on a houseboat on the Thames even, sounds romantic except for the fumes) reply atoav 2 hours agorootparentDepends on the kid. Don&#x27;t you think? For some the repetition is the only chance they get to find a pattern.I think where \"traditional\" education truly sucks tho (unless you are blessed with a gifted teacher), is giving you an overwiew of how the pieces fit together and what is behind each of them. reply makeitdouble 9 hours agorootparentprevYes. Kids moving countries hit that wall at full speed, and have to learn the exam system and quirks on top of the actual content.The interesting aspect to me was how they need to take a step back and look at it as a set of made up conventions, when they might have just absorbed it as universal truth otherwise. reply refulgentis 10 hours agorootparentprevI was homeschooled through 6th grade, entering regular school, it was striking just how \"slow\" everything seemed.Conversely, I struggled immensely for a year and a half, once my 3 year lead on material ran out.The difference in quality and speed in something you&#x27;re self-motivated to do and are doing at your own pace, versus something you&#x27;re being told to do at someone else&#x27;s pace, even with the same person in both cases, is quite astounding.Or maybe that&#x27;s the viewpoint of someone with ADD, and most people can keep steady progress _and_ lead balanced lives. I&#x27;ll never really know. reply irjustin 9 hours agorootparentprevThis has been a huge contention point of Common Core and why so many kids had such huge problems with the overall test. Hard questions that seem&#x2F;potentially-are vague, but are hard questions first.Personally, I&#x27;m a big fan of what Gates was attempting to do, but I have teachers in my family who couldn&#x27;t get rid of Common Core from their schools fast enough. Debates with them were never good and me, not being an operator, wouldn&#x27;t dare tell them how to do their jobs.But, I understand difficult, standardized testing isn&#x27;t a good answer, but really how do you get a whole nation to up-skill? reply csense 8 hours agorootparentDe-centralize and de-nationalize schooling. It&#x27;s clear the feds have no idea how to make the education system work. Let the states have ~50 different curricula, or individual schools to have ~hundreds or ~thousands of different curricula. Even if they&#x27;re bad on average, some of them will probably figure out how to make an actually good US school by sheer luck. Once we have that, the others have a model to copy. reply EnigmaFlare 2 hours agorootparentSchools and teachers have ways to make sure improvements don&#x27;t happen. These include - same pay regardless of performance, not hiring or firing based on performance, and teachers don&#x27;t want to learn or do anything differently, especially if it&#x27;s even slightly harder than what they already do. They&#x27;re not really professionals that improve or have any career growth besides management and accumulating number of years experience. Schools don&#x27;t care either, nor do governments bother to incentivize them to care.Except in, say, China, where none of that applies and education works great. reply rtsil 8 hours agorootparentprev> Once we have that, the others have a model to copy.That is a bold expectation. Some schools will have an incentive to just have as much pupils pass the exam by lowering the bar, other will be incentivized to just teach the bare minimum, others won&#x27;t have enough money if the model of excellence is expensive (and it probably will be).And the model that applies to, say, a child who grew up with college-educated parents won&#x27;t necessarily apply to the child from a blue collar family.That is just to say that this is a hard problem. My intuition is that the only way to achieve the best education (excluding the case of hypergifted people) seems to be by throwing money at it through personalized education and tutoring and solving the child&#x27;s other life problems, i.e. the wealthy kid model, but obviously that&#x27;s not realistic when it comes to mass education. But maybe mass education (at the level we seem to expect from it) isn&#x27;t realistic.Or maybe we should reduce the number of subjects taught in school to math, language and physical education. reply chongli 7 hours agorootparentprevThe problem with trying to make a good school is that strong students pretty much teach themselves and their performance far exceeds any effects from good teaching methods. If any school starts to show decent results then word gets around and it becomes a magnet school. Magnet schools attract wealthy and high-performing students, which totally swamp the effects of the teaching method you&#x27;re trying to study. reply anigbrowl 5 hours agorootparentprevExcept that as soon as someone says &#x27;hey X works, here&#x27;s the data to prove it&#x27;s all the rent seekers who have gotten happy with the alternatives will fight against its adoption. When I was younger I used to buy into the &#x27;laboratories of democracy&#x27; concept, now I think it&#x27;s more like &#x27;meth labs of bureaucracy.&#x27; reply snowwrestler 6 hours agorootparentprevCongrats: what you’re asking for is exactly how things work now! The feds do not set currricula, each state does independently. Common Core is an opt-in, state-led initiative, which was based on the idea of examining the latest research and practices in education across the country, and adjusting curricula to match. reply kemotep 8 hours agorootparentprevI haven’t been in public school in 15 years but there was 2 public school districts, a public-private magnet school associated with the State University and a Private Catholic school with radically different curriculum in my hometown. Even between the two high schools in the same district there was different courses and funding because of the property taxes funding specific schools and one school was on the nice side of town.As far as I understand, most public schools are decentralized, at least the State level, if not further. Are you claiming that is not the case currently? reply tsimionescu 3 hours agorootparentprev> Even if they&#x27;re bad on average, some of them will probably figure out how to make an actually good US school by sheer luck. Once we have that, the others have a model to copy.How could anyone know that, if each school has its own curriculum and exams? If you got 4 gold stars out of 5 on your reading comprehension and I got three palm leaves out of 4 at literature interpretation, which one of us is getting a better education? reply hoten 3 hours agorootparentThe ones who get into better colleges. reply tsimionescu 1 hour agorootparentThe there is a de facto national curriculum, defined by the good college entrance criteria. reply irjustin 8 hours agorootparentprev> De-centralize and de-nationalize schooling. It&#x27;s clear the feds have no idea how to make the education system work. Let the states have ~50 different curricula, or individual schools to have ~hundreds or ~thousands of different curricula. Even if they&#x27;re bad on average, some of them will probably figure out how to make an actually good US school by sheer luck. Once we have that, the others have a model to copy.Sadly, we already know what works, individualized&#x2F;custom attention that exposes the kid to a lot of fields. A worse way to put it, more money == better education. And this is largely true even if specifically wrong.But once you try to scale that up, everything falls apart. reply throwaway2037 4 hours agorootparentprevI am not defending \"Common Core\", but why didn&#x27;t this happen before Common Core existed?Also: Many highly advanced nations have a national-level curriculum. What is different and why does it work? reply thfuran 7 hours agorootparentprev>Let the states have ~50 different curricula,There are several states in which that would be absolutely disastrous. reply whoopdedo 10 hours agorootparentprevWhich is a big problem I have with these sort of tests. A problem that exists in most I.Q.-like assessments. Rather than evaluating how well the student is able to use their brain it&#x27;s instead filtering those who were coached to think the same way as the test writers. That creates homogenity which is why I believe there are cultural divides in test results.Like the Cold War era stories where spies use seemingly innocuous or nonsensical code phrases to identify each other. \"The sparrow flies at midnight. Calculate the velocity it would need to reach Berlin in 7 days.\" reply kuroguro 10 hours agorootparentprevI think vagueness is kindof ok in math&#x2F;physics questions - if you think about it there&#x27;s usually and interpretation that makes more sense, but I doubt most of them are made that way on purpose.Where I really hate to see it is humanities e.g. psychology tests&#x2F;surveys - if after reading a question I immediately think of 3 different interpretations I just think it&#x27;s a bad test. And if I spend any more time thinking about it I get almost nowhere. reply makeitdouble 9 hours agorootparent> there&#x27;s usually an interpretation that makes more senseYes, I see it as \"background\" part, or \"common sense\" perhaps.I&#x27;s funny when for instance you have grocery questions about little Jimmy having 3 bags with 4 buns in each, so how many sausages does he need to make hotdogs with all the buns ?That&#x27;s cute and straightforward as long as you know what a hotdog is, which is common sense for the question writer.I don&#x27;t think that&#x27;s something that needs to be (or can be) changed, as long as school and exams are seen as a formative step, and not a single chance you&#x27;ll have to make a decent living. Kids in the later group will have a hard time either way. reply Aeolun 9 hours agorootparent> That&#x27;s cute and straightforward as long as you know what a hotdog is, which is common sense for the question writer.Even if you don’t know what a hotdog is, it’s indicated you use sausages and buns to make them. The most straightforward method of doing that would lead to the correct answer.There may be kids that figure that’s too simple and assume they need two or three sausages per bun, but those will be the minority. reply dr_kiszonka 9 hours agorootparentprevI always found that \"feature\" annoying because students who knew the material well could fail questions testing its mastery due to imprecise instructions. (My personal pet peeve is not making it clear whether \"or\" is inclusive or exclusive.) reply hahajk 6 hours agorootparentI can&#x27;t think of a situation in which \"or\" would ever be used in an exclusive sense unless it was explicitly called out as an XOR or included \"but not both\". reply tsimionescu 3 hours agorootparent\"Is it A or B?\"\"Do you want the burger or the ribs?\" reply peyton 10 hours agorootparentprevI’m thankful I was doing competition math by that age and had access to more rigorous materials than these “readiness” assessments. reply mewpmewp2 11 hours agoparentprevConsidering how little educators are paid, I&#x27;m amazed as to how much they have been able to come up with and do already, though. reply dudeinjapan 11 hours agoparentprevI think you are looking for the word “cromulent”. reply plibither8 3 hours agorootparentDid you just learn this word too from another recent thread on HN? I did, haha! reply BeetleB 7 hours agoparentprev> The guy who figured this out (who Veritasium interviewed) is crazy smart.Eh. I knew about it before I took the SAT. It was in various brain teasers books. reply krackers 12 hours agoprevThis was also the infamous amc 2015 \"clockblock\" questionhttps:&#x2F;&#x2F;artofproblemsolving.com&#x2F;wiki&#x2F;index.php&#x2F;2015_AMC_10A_... reply CrazyStat 11 hours agoparentI remember having the same question (probably with different sized circles) in our local high school math team competition ca. 2002. I got it wrong. reply genocidicbunny 9 hours agorootparentI distinctly recall having to solve a version of this in my 2nd grade math class. Only about half the class got it right. reply chunsj 5 hours agoprevWhy should we take the meaning of revolution in this problem not as the contacting point to meet the circle again, but just as the pointing the same direction? I think this is the source of confusion here.I&#x27;m not the native speaker of English and I might take the meaning of \"revolution\" very absurdly here. Just curious and I have to ask this. :-) reply scotty79 1 hour agoparentHe mentions that the word revolution has specific meaning in astronomy which doesn&#x27;t make any sense in the context of this problem. reply jhncls 9 hours agoprevThe coin around coin problem has its own wikipedia page, although some people want to delete it:https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Coin_rotation_paradox reply caditinpiscinam 7 hours agoparent> although some people want to delete itI hope they don&#x27;t. The argument for removing it is basically that if you understand geometry, then the conclusion is obvious; but since this is an area where most people&#x27;s understanding of geometry is incorrect (myself included, before today), the article is valuable. I don&#x27;t see how it&#x27;s different from articles on other mathematical paradoxes (Simpson&#x27;s paradox, for example). reply alister 8 hours agoparentprevI love the Wikipedia animations -- they are usually extremely helpful -- but when I&#x27;m trying to read the article, I find them distracting. I usually move another window on top to obscure the animations so I can concentrate on the article. But there must be a better way. Is there a hotkey to stop animations? For Firefox in particular?EDIT: I don&#x27;t want to stop animations permanently with about:config; I just want to pause and un-pause. reply jhncls 8 hours agoparentprevThe same paradox is also featured prominently as part of a Mathologer video [0] and an article in Scientific American [1].[0]: https:&#x2F;&#x2F;youtu.be&#x2F;oEN0o9ZGmOM?t=523s[1]: https:&#x2F;&#x2F;www.scientificamerican.com&#x2F;article&#x2F;the-sat-problem-t... reply ecshafer 8 hours agoprevMy answer to this after reading the question was 1, because by definition it is one revolution. So I thought that was trick, and the answer essentially is none of the above. But there is a lot more nuance here than just that with multiple definitions of revolution versus rotations, and sidereal time is where this answer can get weird. reply HarHarVeryFunny 6 hours agoparentThe only ambiguity there would be revolution&#x2F;rotation about what center - the small disk&#x27;s own center or the center of the large one. It seems pretty obvious they are expecting you to calculate something, so assuming this is a trick question and the answer is 1 would seem a risky interpretation, and anyways wasn&#x27;t one of the answer options. reply matt-attack 6 hours agorootparentBut isn’t that precisely the distinction between rotation and revolution already provides? When I first read it I thought, well they used the word revolution so clearly the answer is 1. If they had stated “rotations” then the answer is obviously 4.It’s pretty ridiculous they couldn’t use the right word when the distinction was taught back it elementary school during the discussion of the motion of the earth. reply HarHarVeryFunny 6 hours agorootparentBut why is it obviously 4 ? Is it equally obvious that if the small disk was rolling around the inside of the large disk then the answer would be 2 ?If you already know the relationship between distance traveled by the center of the small circle and number of rotations, then of course both answers (for outside of circle, or inside) are obvious, but the inside case doesn&#x27;t seem intuitive.btw, it seems they were intending \"3\" to be the right answer, but the wording isn&#x27;t even close to what it&#x27;d have to be for that to be the right answer! reply pmayrgundter 11 hours agoprevNone of the explanations gave me an intuition for it except the circle rolling down a straight line with the same length as its circumference.A roll down the line will rotate the circle once. Coming back the same. But rolling around one corner will add half a circumference, and another half for rolling around the other. So you get 2 * 0.5 extra circumferences, and so + 1 C. Somehow that helps with the other polynomials for me too. Super cool. reply krackers 10 hours agoparentTriangle is probably easier to see (120 deg rotation when you turn the corner). Then you can see that the key is the sum of exterior angles, and this holds as you increase num sides. In the limit, you get a circle. reply cm2187 9 hours agorootparentHaving watched the video, yours is the only intuitive explaination that talks to me. reply beautron 9 hours agorootparentprevOh wow! Hadn&#x27;t heard it like this before, but now I like this way the best. I can finally see clearly where that \"extra\" rotation comes from. reply wodenokoto 1 hour agoparentprevFor my inner eye, I can see that the circle will rotate half a circumsphere to get around the corner.But I can’t really explain why. It’s the same distance.I get that when a road turns left, it’s longer on the right side, but these are lines that have no width and so there’s no stretch and by definition we’ve kept the length the same. reply jodrellblank 7 hours agoparentprevThe best intuition for me was the YouTube comment that said to imagine the rolling coin&#x2F;circle is picked up on its edge, in 3D, rolling like a unicycle wheel. It turns forwards to roll and measure distance, and it turns sideways to travel around the circular path. i.e. \"N\" rotations for circumference distance on one axis (the straight line distance) plus one full rotation on a different axis around the circle. When the coin is lying flat, both have to happen on the same axis. reply KMag 11 hours agoparentprevDo you mean polygon, or did I miss something relating to a polynomial solution? reply jeffbee 10 hours agoparentprevTo me the intuitive explanation is that if they were gears, with shafts fixed in a block, the small wheel would turn 3 times per turn of the big wheel, but in the given scenario the small gear also has to go around the big one, hence the R&#x2F;r+1 solution. reply Aeolun 9 hours agorootparentYeah. I think that’s the intuition I have about this too. It rotates once by virtue of going around, and three times by virtue of the size of the big circle.If you imagine a very tiny circle it’d still rotate a full time. reply HarHarVeryFunny 7 hours agorootparentprevYes, but that line of thought doesn&#x27;t really help when the small disk is rolling around the inside of the large disk, not the outside. reply haltist 6 hours agoparentprevThis is the right way to think about it. You get half at the beginning and half at the end so overall you get an extra rotation. reply mlcrypto 11 hours agoprevThe perspective from inside the circle was mind blowing. Can that be an analogy to relativity & time? reply emmet 11 hours agoparentSuch a clever way of showing the mechanic! Tried to picture it in my head first and then he just did it for me reply TarasBob 10 hours agoprevThe way I think about it is imagine the circle in the middle is very tiny compared to the circle that rolls. reply medler 9 hours agoparentI like this perspective as well. If the coin is rotating around a point (radius 0), the point on the coin edge touching the point will not move at all, yet the coin will make one rotation before returning to its original position. reply jncfhnb 10 hours agoprevI think the explanation is kind of messy tbh.People are going to have one of two interpretations. If a coin rolls around another coin, is the distance traveled equal to the edge along the the static coin? Or is it the distance traveled by the center of the rotating coin?If it’s defined by the former, the answer is 3 because the circumference is 3x as long as that of the traveling coin. If it’s defined by the latter then it’s 4 because the circumference of the base coin buffered out by the radius of the traveling coin is 4x as long that of the traveling coin.I vote 3 personally reply orenlindsey 10 hours agoparentThe point is that the question is vague and is open to multiple different \"correct\" answers. SAT questions should only have one correct answer, with no possibility of any others. reply HarHarVeryFunny 6 hours agorootparentIt&#x27;s not just that it was poorly worded, but rather poorly conceived&#x2F;analyzed. Apparently they had wanted the right answer to be \"3\", and therefore basically asked the wrong question (which would been a bit awkward to word!). reply jncfhnb 10 hours agorootparentprevThe point is that the question is sort of interesting and the history is just fluff for YouTube clicks and video length. The context is irrelevant. reply snowwrestler 6 hours agoparentprevThe text of the question did not ask about distance, it asked about revolutions.“In the figure above, the radius of circle A is one-third the radius of circle B. Starting from position shown in figure, circle A rolls around circle B. At the end of how many revolutions of circle A will the center of circle A first reach its starting point?” reply omeze 9 hours agoparentprevYea, I think the difference between “how many rotations” and “how far is the center of the small disk traveling” is pretty big. I was completely lost as to how you “add one” until I watched what the video called a “rotation”. It would have never occurred to me to call the coin being upright a rotation… the Art of Problem solving question in another comment thread was more clearly worded for that scenario reply HarHarVeryFunny 7 hours agorootparentThe question wording was about \"revolutions\" of the small disk. The distance traveled by the small disk center (= 4x it&#x27;s own diameter) was only used as a way to calculate the number of revolutions, but I&#x27;m still not getting the intuition for how each diameter of distance traveled by the disk center equals one rotation regardless of the shape of path followed (it&#x27;s obvious for a flat line of course). reply medler 10 hours agoparentprevIt depends on the viewer’s perspective. From the coin’s point of view it rotates three times, but to an outside viewer it rotates four times. I think the outside viewer’s perspective is far more intuitive, so four is really the “most” correct answer to me. reply jncfhnb 7 hours agorootparentPerhaps. But I think the most intuitive is noting that a coin’s perimeter should touch the other coin exactly one full time during a rotation. reply sillysaurusx 10 hours agoprevThere is an intuitive way to see why this is true.Picture a circle rolling around the outside of another. Easy to picture, right? Even if it’s not clear how many times it rotates, you can still visualize it.Now imagine the outer circle has to spin around the inside of the first circle.Stack two quarters, and try to spin the top quarter around the inside of the bottom quarter.You can’t do it. There’s no room. If they’re identical size, the top quarter can’t spin \"around\" the inner edge of the bottom quarter.Now put a penny on top of a quarter. Spin that around. Not along the outside, but along the inside of the quarter. You’ll see the penny is tracing a very tight circle. There’s not much room.Now put the penny next to the quarter and spin it around the outer edge of the quarter. It goes easily. Plenty of room.If you do the naive calculation, you’d get the same answer for both cases. But clearly there’s a difference.The difference is more apparent if you imagine the coins had little teeth, like gears. If you go around the outside, it’ll go just fine. But if you hollow out the quarter and put teeth along the inside, and try to spin the penny around in that, you’ll find that it doesn’t make anywhere close to a full rotation each time you go around. reply whoopdedo 9 hours agoparentThis becomes even more apparent when you turn the circumference of the fixed circle into a straight line. (As the video suggested but didn&#x27;t show in the same way.) The center of the rolling circle will trace a line parallel and equal length to the fixed line, whether above or below it. If you reform the circle the outer line will be too short to form a complete circle, and the inner line will overlap itself. reply medler 10 hours agoprevThe application to sidereal time is where this really gets interesting. It’s far from a trivial question of semantics or a mathematical trick. It’s something astronomers have known about for ages, since the amount of time it takes for the Earth to make one rotation is different from the perspective of distant stars than from the perspective of the sun. reply fsckboy 6 hours agoparentwhat is perplexing to me is that I&#x27;ve been thinking about the sidereal problem for the last or so...(because I heard on a podcast that the earliest sunset occurs more than a week before the solstice and the latest sunrise occurs after the solstice, but still the solstice is the shortest day, and they said something about the tilt of the earth and I was thinking that was not explanatory, so I&#x27;ve been idly noodling about it)...and I just got this problem wrong! didn&#x27;t even occur to me :) in my defense, the thumbnail for this video does not state the question so I wasn&#x27;t sure what I was supposed to do, but guided by the multiple choice I selected 3but talk about having my brain primed for the question... doh! reply whartung 9 hours agoparentprevIndeed. This was my favorite part of the video. The discussion of the problem and SAT part are interesting, but when it delves into the sidereal time aspect, it just flies off into a direction where never saw coming and presented some fascinating aspects I’d never had need to consider. reply vood 5 hours agoprevI&#x27;m surprised that only 3 people reported the error.While problem is not trivial, my initial reaction was \"it can&#x27;t be 3, it&#x27;s too obvious\". And it seems that a lot of people in the comments here do get to the right answer on their own.So, why not hundreds of reports? reply crazygringo 5 hours agoparent> my initial reaction was \"it can&#x27;t be 3, it&#x27;s too obvious\".The SAT has math questions ranging from easy to hard-ish. Plenty of the questions have very obvious answers.Unlike the AP Physics 2 exam, for example. reply elbasti 9 hours agoprevQuestion: the outside circle is a tire made of rubber, which wears down at a known rate.Which tire wears down more:A tire that rolls around a circle of circumference D.A tire that rolls down a flat surface, total distance D? reply johnfn 9 hours agoparentWell, surely these are the same. Right? Haha of course they are. This is silly. Right? RIGHT?!? reply Aeolun 9 hours agoparentprevJust because it’s spinning more does not mean it wears out faster though.Maybe tires are more resistant to rolling on curvatures? reply scotty79 53 minutes agorootparentI&#x27;d say they are less resistant to going on positive curvatures. It&#x27;s like a bum that goes deeper into the tire, deforming tire more which should result in more wear than driving on even surface. reply bobby_table 11 hours agoprevThe way I think of this is: If instead of having a small circle rolling around a larger one, think of a circle with ANY non-zero radius rolling around one with a zero (or really really small) radius. You’ll always end up with at least one rotation, regardless of how small the other circle is. So +1 makes sense. reply gumby 8 hours agoprevInteresting: I took the SAT in 1982 and don’t remember this problem. They don’t give you your % right, just percentile and a mysterious score between 200 and 800 for some reason. Does 800 mean 100%? reply gnicholas 4 hours agoparentWhen I took the SAT a couple decades later, there were multiple sittings per year. Is it possible you took it at a different time? You should be able to check the news coverage and see which sitting it refers to. reply zdw 6 hours agoparentprevI got an 800 on the Math portion of the SAT, but the results also said I missed a question (this was mid-90&#x27;s), so maybe that one was thrown out?(1480 combined, did a prep class but didn&#x27;t pay much attention to the language bits) reply gumby 5 hours agorootparentBack in 1982 our school’s entire SAT prep was: “bring only #2 pencils. Sharpen them before you come. Fill the bubble completely and don’t leave any other marks. If you can eliminate one answer, it’s worth picking one randomly, else skip the question.” I don’t remember that there was any SAT prep culture like there is these days. Nobody had heard of ACT — perhaps it didn’t exist yet.There were apparently about 50 of us in my year at MIT who had 800&#x2F;800, and apparently that was pretty standard. I don’t think I was the only one in my HS class of 39. reply hatthew 7 hours agoparentprevanecdata from years ago: 100% is 800, and leaving one question blank is 780. reply scotty79 1 hour agoprevSecond part of the video is way more interesting, where he discusses how this toy problem actually relates to how we define what a year is depending on whether we look at the stars or at what&#x27;s on Earth. reply ad404b8a372f2b9 7 hours agoprevIt&#x27;s odd to me that anyone would come up with 3, to me it was immediately very apparent from the diagram that the closest answer was 9&#x2F;2. reply HarHarVeryFunny 6 hours agoparentI guess if you&#x27;re smart enough to realize that 4 is the right answer, but isn&#x27;t one of the choices, then the next question is \"what were they thinking?\", which is why 3 would be the smart guess to get your answer marked as correct. reply elwell 9 hours agoprevWhat if the large circle rotates at the same rate that the small circle &#x27;orbits&#x27; it? reply xorcist 10 hours agoprevDoes the video spoil the solution too or is it only the question? reply orenlindsey 10 hours agoparentWell, there&#x27;s not really one solution. Watch the video to see what I mean. reply fsckboy 6 hours agoparentprevthe video starts out with the question and then says you can pause if you want to work on it. Then it the next thing it says is something about the answer. reply belltaco 4 hours agoprevI did a eyeball ball park estimate and figured the closest answer was 9&#x2F;2 and was right. reply goodpoint 1 hour agoparentYes, it&#x27;s pretty easy to just imagine the rotation and guess it&#x27;s somewhere around 4. The last part of the rotation is more difficult to imagine. reply lostmsu 11 hours agoprev [–] The title sort of spoils it as it becomes obvious you need to look for some issue with the trivial number solution. reply anamexis 11 hours agoparent [–] I think any video about it would spoil it, because you wouldn&#x27;t make a video about a trivial number problem. replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "In 1982, there was a mistake on the SAT exam where students discovered an incorrect answer to a test question.",
      "The article introduces the concept of counting rotations, explaining its applications in fields like astronomy.",
      "It discusses the diminishing importance of standardized testing and recommends the online learning platform, Brilliant, for enhancing critical thinking and problem-solving skills. The article offers a free trial and acknowledges Brilliant for sponsoring the video."
    ],
    "commentSummary": [
      "The discussion touches on various topics such as criticisms of a YouTube video, flaws in standardized tests, challenges in math education, improving the education system, and the concept of rotations in geometry.",
      "Users contribute their opinions, experiences, and insights on these subjects.",
      "This discussion provides a platform for engaging in meaningful conversations and exchanging ideas on diverse educational topics."
    ],
    "points": 194,
    "commentCount": 157,
    "retryCount": 0,
    "time": 1703621055
  },
  {
    "id": 38777460,
    "title": "Interactive Map Makes Chicago's Zoning Code More Accessible",
    "originLink": "https://secondcityzoning.org",
    "originBody": "Toggle navigation By DataMade Zoning map Zoning districts Zoning rules About What is this? 2nd City Zoning is an interactive map that lets you: find out how your building is zoned learn where to locate your business explore zoning patterns throughout the city To make Chicago's zoning code digestible by humans, we took inspiration from one of our favorite games: Sim City 2000. It started with the color scheme: green for residential, blue for commercial and yellow for industrial. (This oversimplifies things a bit, read the full story.) From there we got a little carried away. Graphics, sounds, music, oh my. We had a lot of fun making this, so dive in and explore! Ok, thanks! Read more » Find zone (find me) Search Reset Zoning type (what's that?) Residential Commercial Industrial Planned Development Transportation Parks and Open Space Sound effects Off Music Play Stop Sim City Theme Moogy City City Shimmy Questions? Email us! +- Streets Building addresses Satellite Zoning Hover over an area LeafletTerms & Feedback, © CARTO Data accurate as of Oct 25, 2023. By DataMade, Derek Eder, and Juan-Pablo Velez.",
    "commentLink": "https://news.ycombinator.com/item?id=38777460",
    "commentBody": "2nd City ZoningHacker Newspastlogin2nd City Zoning (secondcityzoning.org) 193 points by michaefe 9 hours ago| hidepastfavorite75 comments tptacek 8 hours agoThis is very neat. A thing that&#x27;s a little crazymaking in all of these maps though: one of the most important things you&#x27;re looking at in a zoning map is the distinction between single-family lots (exclusively for detached houses) and multi-family lots. You have to zoom in to the single-neighborhood level to see that distinction here (between paler green and darker green), and it&#x27;s subtle.It&#x27;s nowhere nearly as bad as my neighboring municipality&#x27;s zoning map, though:https:&#x2F;&#x2F;www.oak-park.us&#x2F;sites&#x2F;default&#x2F;files&#x2F;zoning&#x2F;2021-02-2...I&#x27;m convinced this map was colored specifically to obscure that distinction.The links to zoning variance documents in the planned development (red) zones are a really nice touch! reply zappb 6 hours agoparentOak Park has a history behind the zoning. Not a pleasant one. reply kasey_junk 6 hours agorootparentSo does all of Chicago. reply ChadNauseam 2 hours agorootparentI don&#x27;t know what you&#x27;re referring to, but I&#x27;m curious as I moved to Chicago relatively recently. Could you elaborate? reply martijnvds 2 hours agorootparentMost likely: racism&#x2F;segregation. reply zappb 5 hours agorootparentprevYeah it’s kind of shameful to be honest. reply tptacek 6 hours agorootparentprevYou don&#x27;t want to get me started on this. reply nikanj 52 minutes agoparentprevThe phrasing I liked the best was ”the area colored pale green are reserved for families who qualify for a seven-digit mortage” reply kasey_junk 6 hours agoparentprevAre you sure it’s not just that there isn’t enough gradients to tell?Within 1 block of my house there are townhomes, a mansion, single family homes, 2 parks, mid rises, a hotel, real commercial, a high rise, an sro and a school.The commercial, parks and hotel are accurately represented but only 1 set of townhouses are otherwise. And that’s the set that takes up a whole block.Otherwise the mixed blocks all come up as single family. Which might just be a fidelity problem. reply arthurdenture 4 hours agorootparentIs it possible that many of those uses predate the current zoning? There are numerous buildings in Chicago that would not be permitted by their current zoning. reply tptacek 5 hours agorootparentprevOh, the data is good, I just think there has to be a better color scheme for the residential areas (it needs to be green to fit the SimCity aesthetic, but there must be something else you can do). reply kasey_junk 5 hours agorootparentI’m not convinced the data is good. At least in my neighborhood the zoning requirements are address by address and I’d be easily convinced coming up with a coloring scheme that attributes everything correctly is a hard computer science problem. reply jdksmdbtbdnmsm 3 hours agorootparent>I’d be easily convinced coming up with a coloring scheme that attributes everything correctly is a hard computer science problem.It&#x27;s not a computer science problem at all. It&#x27;s literally just a matter of having a uniue color for each type of zone. reply tptacek 5 hours agorootparentprevI&#x27;m just saying, to look at this map, you&#x27;d think Beverly and Lincoln Park were comparably zoned. reply inferiorhuman 4 hours agorootparentprevI like the idea of not using color to encode a high level of detail. SC2K breaks it down into dense and light zones (and changes the color accordingly) and that&#x27;s it.There should be more detailed info in the tooltip for each parcel. reply lukas099 3 hours agorootparentprevThat’s crazy to me that you have all that within a block of where you live. Are you in an East Coast city or something? reply kasey_junk 6 hours agorootparentprevThe parks and school are ~mostly correct as well. reply cobertos 7 hours agoparentprevWhat would you want more of our of that map? It looks like a lot of zoning maps I have seen but I can&#x27;t think of any way to improve them other than consistency and a better viewing medium (not PDF) reply inferiorhuman 7 hours agorootparentIt took me a moment but I think OP is trying to distinguish areas where single family homes are allowed&#x2F;required from those where multi-family units are allowed. reply db48x 7 hours agoparentprevWhat&#x27;s wrong with your neighborhood’s zoning? reply devilbunny 7 hours agorootparent> car repair, machine shops, furniture makingNone of which is really unbearable to live somewhat near to (unlike heavy industrial, they do shut down at night), but they&#x27;re often pretty locally polluting. Brake dust, metal shavings, wood finishes with VOC&#x27;s, paints, and then all sorts of chemicals used as solvents for cleaning, rust removers, etc. reply tptacek 6 hours agorootparentprevA lot, but my critique is of the map itself, not of the zoning. reply db48x 5 hours agorootparentAh. I agree that the map is not amazing :). Color choices could be better too. reply tptacek 5 hours agorootparentFun challenge: find the non-SFZ residential on that map. You&#x27;re not allowed to use the eyedropper tool. replyanigbrowl 6 hours agoprevThe originally submitted title (&#x27;Chicago zoning in the style of SimCity&#x27; or words to that effect) was much more informative. reply sedatk 2 hours agoparentTo be fair, this is a great web site, but the it doesn&#x27;t look like SimCity 2000 at all apart from some icons here and there. I&#x27;m glad that SimCity inspired the web site&#x27;s creators, but nowhere near to call the web site \"SimCity style\". Maybe I&#x27;m missing something? reply k1t 2 hours agorootparentYou&#x27;re probably missing the fact that SimCity and SimCity 2000 are different games. In particular the original was a 2D overhead view (like TFA), but 2000 was a 3D isometric view.Surprisingly only 4 years apart though. reply sedatk 2 hours agorootparentThe creators of the web site specifically mention SimCity 2000 flavor for their inspiration. But regardless, let&#x27;s assume that they wanted to imitate the original SimCity: having a 2D street map doesn&#x27;t make something automatically SimCity style. Otherwise, every map app would be SimCity style. Again, great site. I see some icons from SimCity, but I don&#x27;t see a web page in SimCity style, hence my objection to mentioning \"SimCity style\" in the title. reply ssgodderidge 9 hours agoprevHere’s the GitHub repo [1] for those who are code-curious.https:&#x2F;&#x2F;github.com&#x2F;datamade&#x2F;second-city-zoning reply CrendKing 8 hours agoprevWait, so commercial zones are supposed to be only in strips and around street, and industrial zones are not actually far away from residential? Guess I always played SC2000 wrong back in the days. reply db48x 7 hours agoparentPutting industrial far away from your residential is a classic mistake that causes all kinds of traffic problems, but also the industrial in a real city is going to mostly be relatively innocuous. It’ll be car dealerships, car repair, machine shops, furniture making, data centers, warehouses, etc, etc. In Sim City industrial was always giant factories with belching smokestacks, steel foundries, chemical plants, coal gas, and so on. reply rifty 5 hours agorootparentSomething I enjoy in SimCity2000 is that you can avoid the traffic problems by not making a complete network of road connections between the residential and the jobs. This works because commuters can &#x27;spawn&#x27; from a single residential tile despite pulling from the pool of residents on the entire map! So you can zone single residential tiles on isolated industrial road networks for minimal traffic issues. reply quickthrower2 8 hours agoparentprevThe trick was to use railway and no roads! reply samstave 6 hours agorootparentWhat does Patchouli smell like in binary? reply spanktheuser 4 hours agorootparentLike foresight. reply vkou 7 hours agoparentprevYou want to keep your industrial zones away from nice residential ones. It&#x27;s fine for the paupers to be breathing in zinc. reply hughesjj 5 hours agorootparentindustrial revolution intensifieseast sideeeeehttps:&#x2F;&#x2F;www.theguardian.com&#x2F;cities&#x2F;2017&#x2F;may&#x2F;12&#x2F;blowing-wind-... reply zbrozek 9 hours agoprevWow this is amazing. Clicking on zones is also more informative than I expected. Massive kudos. Also, it&#x27;s nice touch to let me turn on SimCity music. Fantastic all around. reply resonantjacket5 6 hours agoprevPretty nice website of chicagos zoningFyi there’s an initiative to create a nationwide (USA) zoning map https:&#x2F;&#x2F;www.zoningatlas.org&#x2F; reply burlesona 9 hours agoprevVery cool visualization! A meta comment here is that you look at the shapes of the zoning districts and they make no real sense, except that they were applied as the best fit to whatever happened to be on each lot at the time the codes was adopted. It’s sad how much our local governments main job is to try and freeze everything in place and prevent change. reply jackcosgrove 8 hours agoparentIf you look at the north and south sides where there are large swathes of green residential neighborhoods, you can see they are regularly divided by blue commercial corridors along the cardinal directions. Chicago&#x27;s grid design was adapted from the original Northwest Ordinance, which divided the land into square mile sections. Those blue strips are one mile apart, and in denser neighborhoods an additional blue strip exists midway at the half mile mark. This organizing principle is what made Chicago a city of neighborhoods, since each section or quarter section is bounded by commercial thoroughfares on each side, and the neighborhood turns inward. The section line and half section line thoroughfares are wider roads which permit more traffic volume. I think it&#x27;s a good compromise between facilitating traffic flow and also isolating it from the contained neighborhoods where the bulk of people live.As far as freezing the zoning in place, that&#x27;s not really true as downtown areas were once like the neighborhoods and were subsequently upzoned as density and desirability increased. The original zoning plan is intact in the neighborhoods because there just hasn&#x27;t been a desire or need to upzone them (yet). Chicago is known as a relatively pro-development city which is flexible on things other cities are inflexible on. reply hammock 9 hours agoparentprev>look at the shapes of the zoning districts and they make no real sense???Commercial zoning aligns with the major streets of the grid, per design.Industrial zones along the Chicago River, where barges can access, and rail lines.Residential everywhere else.What doesn&#x27;t make sense? reply chongli 8 hours agoparentprevIt’s sad how much our local governments main job is to try and freeze everything in place and prevent change.That&#x27;s not really what the local government&#x27;s main job is supposed to be. Local government is supposed to be a venue for regular people to participate in the kinds of everyday decisions that affect their lives. But then it ends up mainly representing local property owners and developers who are its most interested (in the sense of stake or investment, not curiosity) constituents. It is these special interests who try their hardest, not necessarily to prevent change, but to protect and grow their investments. reply alexb_ 7 hours agoparentprevThe zoning makes a lot of sense. The place where white people live is zoned to have nice, single-family houses with a huge yard that takes up a ton of space and is given outsized say in local politics. The place where black people live is zoned to be as underfunded and crowded as possible on the other side of the highway.Mission accomplished for the local governments who decided how zoning should work. reply davidw 7 hours agorootparentThis is sarcastic, but it&#x27;s very much how it works in a lot of places, and it no accident.This book is a nice deep dive into how that came about: https:&#x2F;&#x2F;www.hachettebookgroup.com&#x2F;titles&#x2F;richard-d-kahlenber... reply alexb_ 4 hours agorootparentI&#x27;m being \"sarcastic\" in that I completely agree with you. reply shiroiuma 5 hours agorootparentprevWhy is this modded down? It&#x27;s absolutely the truth. The local governments here are doing exactly what their (more affluent and politically connected) constituents are demanding of them. reply csdvrx 9 hours agoparentprev> It’s sad how much our local governments main job is to try and freeze everything in place and prevent change.I wish we had mixed zoning outside the rare cities like NYC: it would offer at least one scalable option to reduce urban sprawl - say when lots are demolished. reply devilbunny 7 hours agorootparentPlenty of mixed zoning in New Orleans. Unsurprisingly for a French-founded city, the older areas (not just the French Quarter) use the traditional shops-on-ground-floor-living-quarters-above method. And they have a lot of small commercial streets within their neighborhoods, which are demarcated more by the larger divided streets (many of which had streetcar lines running down the space between - in most places, it&#x27;s the \"median\", but in NO, it&#x27;s the \"neutral ground\". No, I have no idea why they call it that - never lived there but go quite often). reply jackcosgrove 6 hours agorootparentThe neutral ground was so named because it demarcated neighborhood boundaries between French (Acadian) and American neighborhoods. reply alexb_ 7 hours agoprevhttps:&#x2F;&#x2F;gisapps.chicago.gov&#x2F;mapchicago&#x2F; has more information. Your city probably also has a GIS page - just look up \"gis \" and you will probably find it. reply jbullock35 7 hours agoparentIn general, that link does have more information. But if one wants to learn about how individual blocks or areas are zoned, the OP&#x27;s site (https:&#x2F;&#x2F;secondcityzoning.org) is much easier to use and seems to offer more information, too. reply theogravity 6 hours agoprevSearch button doesn&#x27;t work for me on mobile safari. Using the find me link does, but hard to tell if the map was centered around my address or not. reply kasey_junk 6 hours agoparentI thought so too but there just wasn’t a visual indication that it did anything. The map definitely moved. reply theogravity 5 hours agorootparentFind me actually showed me the zoning, but not search. reply cmason 4 hours agoprevWhy are all the `y`s missing from the text in Chrome but not in Safari? reply DeIlliad 7 hours agoprevThis is something I&#x27;ve been meaning to do with my city to show how bad residential zoning is. Kudos to whoever made this. reply freitzkriesler2 8 hours agoprevJokes on us, the mayor set all of the budgets to 50% and the transportation manager was right: we regret it. reply yreg 7 hours agoparentDismiss petitioner reply veucast 6 hours agoprevwow, impressive. Colors could be better, but overall, it&#x27;s pretty informative and cool. reply TMWNN 8 hours agoprevA Reddit post by me:>A Detroit News columnist playing the city-management computer game SimCity \"found that Godzilla attacking the city in the 1972 Detroit scenario caused less destruction than the mayoralty of Coleman Young\".reply tptacek 8 hours agoparentThat&#x27;s not really fair; he was mayor from the early 1970s all the way into the 1990s, a period that captured the peak of panic selling and white flight (not just in Detroit but across the midwest) and the deindustrialization and hollowing-out of the Detroit auto industry. reply csdvrx 9 hours agoprevPolitical take: even if many people would like mixed zoning in dense urban centers (with commerce at the street level, and housing above), the familiarity of zoning from games like SimCity makes mixed zoning less legible.Laws mandating parking space and accessibility requirements add to the problem to make it unfixable: any relaxation (allowing mixed zoning at time t) could create future legal and social problems (not enough street parking at time t+N) reply hibikir 7 hours agoparentZoning in other countries is easy to visualize, even though it&#x27;s completely alien to the Sim City ways: I recall being disappointed in the first game, and SC2000, because there was no way to make my home city. Residential zoning? My hometown doesn&#x27;t have such designation at all, apartment buildings have commercial underneath in almost all cases, and office buildings and apartments end up sharing walls.Not that Sim City ever took traffic or parking even remotely seriously: Even in modern city builders, nobody attempts to represent the amount of space dedicated to surface lots in the US, because it&#x27;d look awful. It&#x27;s always a fun exercise to, go to Google Maps, take a picture of the downtown of, say, St Louis, and then coloring every surface parking, or multi-level building solely dedicated to housing cars for commuters and sports events attendees: It&#x27;s well over 30% of the surface area. Then people wonder why the city is dead after 5 pm reply steveklabnik 7 hours agorootparent> Not that Sim City ever took traffic or parking even remotely seriouslyHilariously, it did: https:&#x2F;&#x2F;www.theverge.com&#x2F;2013&#x2F;5&#x2F;9&#x2F;4316222&#x2F;simcity-lead-desig...> \"Our game was going to be really boring if it was proportional in terms of parking lots,\" SimCity lead designer Stone Librande says reply bobthepanda 8 hours agoparentprevThat is because American zoning is often use-prescriptive.Japanese zoning is pretty easy to visualize because all zoning types are defined by intensity level, where high intensity zones also allow all lower intensity uses. So rather than a series of venn diagrams, the structure is more like a pyramid. reply mminer237 5 hours agorootparentAmerican zoning is typically—though not always—cumulative (aka Euclidean) as well. I believe Chicago is an exception to the norm though. The problem is also that people just don&#x27;t often want to live in a commercial or industrial zone even if they have the right. reply bobthepanda 4 hours agorootparentHousing in less ideal places makes up the bedrock of affordable housing. The reality is that eventually a city becomes too large to house everyone in a house with a large yard on a cul-de-sac.The entire affordability crisis is driven by bougie tastes pricing people out by eliminating the entire bottom rung of the market. In many places the starter home simply doesn’t exist anymore. reply tptacek 8 hours agoparentprevHow does \"not enough street parking\" creating legal problems? Most of the north side of Chicago is mixed-use, medium-density, and street-parking reliant; it works fine (these are some of the most valuable and economically vital in the city). reply bobthepanda 7 hours agorootparentthere is a common conception in America that there is a &#x27;right&#x27; to parking availability, which is not really a thing anywhere else.in practice, &#x27;not enough parking&#x27; often means &#x27;not enough parking, that I am personally willing to pay for, in a location that I am willing to park in&#x27;. which could mean anything from &#x27;all the spots are taken within a half mile&#x27; to &#x27;I don&#x27;t really want to pay for a $2 garage one block over because the street parking is free&#x27; reply toast0 6 hours agorootparentThere&#x27;s some sort of idea that everyone has to like what I like. Personally, I dislike paying for parking, and I dislike being in a setting thick with people and buildings... So I&#x27;m going to be out in the suburbs or exurbs or semi-rural and not bother you all in the urbs.Sometimes there is some reason for me to go into the city, and I&#x27;ll try not to drive the whole way, because it will upset me when I have to pay for parking when I get there. But if that works, I think that&#x27;s part of the point. reply kasey_junk 6 hours agorootparentI don’t think this is an issue of everyone needing the same preferences so much as a straight forward economic one. In the US we dramatically subsidize cars.One of those subsidies is free parking. That preference might not even be obvious if you don’t point out what are negative outcomes from that subsidy (for instance many progressives will demand parking mandates to get housing built, trading 1 real preference, equitable housing for one that they don’t prefer, cheap ice ownership). reply aflag 8 hours agoparentprevcities: skylines 2 has mixed zones and it&#x27;s easy to visualise. Just a matter of picking a new colour. reply DonHopkins 8 hours agoprev [–] Where&#x27;s the Disasters menu?I think I found Godzilla!https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=jSYNfFh24i4 reply jackcosgrove 8 hours agoparent [–] Dunno, but it looks like someone built an arcology at Marina City. replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "2nd City Zoning is an interactive map designed to simplify Chicago's zoning code, inspired by Sim City 2000 and utilizing color-coded zoning districts.",
      "Users can navigate the map to understand zoning patterns, determine their building's zoning status, and identify ideal locations for their business.",
      "The project was developed collaboratively by DataMade, Derek Eder, and Juan-Pablo Velez, and features graphics, sounds, and music to enhance user engagement."
    ],
    "commentSummary": [
      "The discussions center around the issues with a zoning map in Chicago, including criticism of its clarity and data accuracy.",
      "There is debate regarding color schemes and design choices for the zoning map.",
      "Participants also discuss the influence of zoning on inequality, the potential for mixed zoning, SimCity, search button issues, parking in cities, and the impact of parking subsidies."
    ],
    "points": 193,
    "commentCount": 75,
    "retryCount": 0,
    "time": 1703635330
  }
]
