[
  {
    "id": 39670922,
    "title": "JSON Canvas: Empowering Infinite Canvas Data Organization",
    "originLink": "https://jsoncanvas.org/",
    "originBody": "logo.svg readme An open file format for infinite canvas data. Infinite canvas tools are a way to view and organize information spatially, like a digital whiteboard. Infinite canvases encourage freedom and exploration, and have become a popular interface pattern across many apps. The JSON Canvas format was created to provide longevity, readability, interoperability, and extensibility to data created with infinite canvas apps. The format is designed to be easy to parse and give users ownership over their data. JSON Canvas files use the .canvas extension. JSON Canvas was originally created for Obsidian. JSON Canvas can be implemented freely as an import, export, and storage format for any app or tool. This site, and all the resources associated with JSON Canvas are open source under the MIT license. spec/1.0 Learn more: Readme Spec GitHub JSON Canvas × { \"nodes\": [ { \"id\": \"logo\", \"type\": \"file\", \"x\": 36, \"y\": 48, \"width\": 1264, \"height\": 35, \"file\": \"logo.svg\" }, { \"id\": \"readme\", \"type\": \"file\", \"x\": 36, \"y\": 240, \"width\": 1264, \"height\": 256, \"file\": \"readme.md\" }, { \"id\": \"spec\", \"type\": \"file\", \"x\": 600, \"y\": 140, \"width\": 1268, \"height\": 175, \"file\": \"spec/1.0.md\" }, { \"id\": \"nav\", \"type\": \"text\", \"x\": 336, \"y\": 36, \"width\": 1264, \"height\": 88, \"text\": \"Learn more:- [Readme](/)- [Spec](/spec/1.0)- [GitHub](https://github.com/obsidianmd/jsoncanvas)\" } ], \"edges\": [ { \"id\": \"edge-logo-nav\", \"fromNode\": \"logo\", \"fromSide\": \"right\", \"fromEnd\": \"none\", \"toNode\": \"nav\", \"toSide\": \"left\", \"toEnd\": \"arrow\" }, { \"id\": \"edge-readme-spec\", \"fromNode\": \"readme\", \"fromSide\": \"right\", \"fromEnd\": \"none\", \"toNode\": \"spec\", \"toSide\": \"left\", \"toEnd\": \"arrow\" } ] } Copy code Download file Toggle output Zoom out Zoom in Reset",
    "commentLink": "https://news.ycombinator.com/item?id=39670922",
    "commentBody": "JSON Canvas – An open file format for infinite canvas data (jsoncanvas.org)678 points by nickmain 16 hours agohidepastfavorite136 comments kepano 15 hours agoWhoa! Didn't expect this to bubble up to the top of HN. Some context about why we created JSON Canvas: https://obsidian.md/blog/json-canvas/ We just released it today, so this is still a very nascent project. A little over a year ago we released Obsidian Canvas. The .canvas file format has felt stable enough to give it a name and resources that other apps can freely use. See the original https://news.ycombinator.com/item?id=34066824 The spec is conservative, and definitely does not support many features canvas apps will want to implement (yet). The purpose of giving JSON Canvas a name and site is to encourage an interoperable ecosystem to grow around this format. We're definitely looking for feedback of all kinds! It's great to see all the suggestions already shared in this thread because it starts to provide a roadmap for how this could become a more useful format for other apps. reply remram 14 hours agoparentWho was involved in this open spec? Or was it built by Obsidian with a hope that it works for everyone else? Which existing formats were considered before building your own, e.g. SVG/Excalidraw/draw.io/...? reply eichin 13 hours agorootparentYeah, didn't see any discussion in the other thread, do you have any notes on why this isn't SVG? reply npunt 12 hours agorootparentIt makes sense if you see Obsidian as the starting point - it's a document store. While other canvas products may be more graphics-oriented, Obsidian's is about laying out documents and objects and providing simple relationships between them. For this purpose, JSON's probably a lot easier to work with than XML/SVG. reply remram 8 hours agorootparentObsidian's own file format might have merit, but posting it as an \"open file format\", \"created to provide (...) interoperability, and extensibility to data created with infinite canvas apps\"... and then publishing 1.0 without consulting with anybody else... is rather egregious, I think. I agree with you that posting Obsidian's spec to start the conversation would have been welcome, but this is not what they did. reply npunt 7 hours agorootparentObsidian's philosophy is file over app [1] and releasing a spec for their Canvas feature is fulfilling that promise. It's a strictly positive 'today is better than yesterday' thing for them to have done so. You shouldn't need to ask permission or consult anyone to do this. That's a silly bar, 'egregious' even. When markdown was created, Gruber wasn't asking all his friends making text editors whether they'd support it. He just released it, and others chose to adopt it on its merits. I don't meant this to sound harsh, but just to call it as I see it - your comment is basically creating a 'damned if you do, damned if you don't' scenario and an example of why people building things shouldn't worry much about comments in forums. [1]: https://stephango.com/file-over-app reply gcr 7 hours agorootparentI’m glad Obsidian is publishing this, even though the spec clearly isn’t ready yet. If I had the final say for how this format were to be publicized, I would probably just include the JSON schema “as-is” in the Obsidian developer docs. Then, it’s just as usable by third parties without making promises about interoperability. But Obsidian is thinking ahead here. They’ve intentionally given us a duck to poke at[1], knowing we’d offer feedback and critique. If the goal is to get people to imagine a better implementation, preying on HN’s “I could implement this so much better” sense is one hecking smart way to get the community to pitch in. 1: https://bwiggs.com/notebook/queens-duck/ reply rahoulb 1 hour agorootparent> Then, it’s just as usable by third parties without making promises about interoperability. I'd say this is the point. If it's \"published as part of Obsidian\" it implies Obsidian can break it at any time and the interop anyone else has built will need updating. But \"published as a standalone spec\" means Obsidian is saying \"we won't change this without warning\" (at the very least). reply npunt 6 hours agorootparentprevYeah, whether or not it's intentional, the approach taken is one that will generate a lot of feedback. I haven't seen anyone mention it in this thread, but my immediate reaction to seeing JSON Canvas was that it looks like a spiritual sibling to Markdown, a famously (and intentionally) informal spec. There's a real wisdom to this informal spec approach, and it's worked incredibly well for Markdown despite grumblings about its lack of standardization. I imagine this is the Obsidian team's intention with JSON Canvas. Obsidian has really benefitted from the informality of Markdown, which meant they were free to extend it in ways that made sense to the product (with things like cross-page ^ references, yaml frontmatter, etc), without triggering the Spec Police. The same perhaps applies here to Canvas products. reply rat9988 8 hours agorootparentprevI'm not sure I agree with you here at all. They can start a format that serves their own purpose, and make it open for interoperability and extensibility sake. Nothing wrong with it. reply haswell 7 hours agorootparentprevThis is an odd bit of gatekeeping and I really don’t understand your position here. Calling this egregious seems odder still. It suggests that companies that choose to publish how they do things have some obligation to do so under a specific framework or shouldn’t do so unless they’re willing to get public buyin. Many of the best projects that become defacto standards start as a solution to a very specific and real problem that had no public input. If JSON canvas isn’t a good solution for broad adoption/interoperability, it won’t become one. The world is not harmed by its release, and at worst, it’s now far easier for people building software to understand and make tools that can interact with Obsidian. At best, it becomes a solid foundation and option for tools going forward. Obsidian didn’t have to do anything here. I’m glad they did. reply blowski 4 hours agorootparentAnd indeed, there are many projects that spend so long trying to gather and process feedback that it becomes a talking shop with no useful output. Better to share something early and then iterate. reply naikrovek 11 hours agorootparentprevSVG is great and you are the first person I have ever seen who would (and did) imply that SVG might be or would be better than JSON for this kind of thing. SVG really stretches the “human readable” thing to its limit, and I personally would not adopt an SVG-like format for anything that isn’t SVG. reply eichin 9 hours agorootparentLooking at the samples, this isn't human readable either (long opaque id strings for linking dominate the syntax) and both syntaxes have vast amounts of punctuation and whitespace. And neither of them are particularly human writable. But something svg-based would have lots of existing options for additional processing in consistent ways, that would be ad-hoc for a json-based syntax. (Given that this is coming from obsidian, I kind of expected something actually markdown-like - which would certainly be a challenge, but not an impossible one.) Granted, this notation is a little higher level than SVG, so the model mismatch might be a bigger problem anyway? reply littlestymaar 3 hours agorootparentprevSVG is a very poor format by all measures so if you can start from scratch for your use case you should definitely avoid it. I'm still glad SVG exists because it's available on the web for free, but this file format is really a mess (very hard to parse, the feature set is so broad it's never entirely implemented and the implantations diverge making it surprisingly hard to support multiple browsers). reply Idiot211 10 hours agorootparentprevRelevant XKCD: https://xkcd.com/927 reply kepano 10 hours agorootparentI can't believe it took this long for someone to post this haha reply tylerdurden91 2 hours agoparentprevGreat work & thanks for doing this. I built a library at AWS for a general canvas editor called Diagram Maker. It recently got archived so I stood up a fork here: https://github.com/sameergoyal/diagrammer and the data format we use is strikingly similar. Check it out here: https://sameergoyal.github.io/diagrammer/?path=/docs/docs-us.... The key differences are panels, workspace & editor. I dont actively work on the project outside of bugs, but maybe there are ways to collaborate here, like moving my project to use & extend the JSON canvas spec. reply altairprime 14 hours agoparentprevFor those having difficulty viewing the spec at the linked jsoncanvas.org site, the spec/1.0 pane on the right has the scrollbar hidden; there's more content if you scroll that pane down. reply hiccuphippo 11 hours agorootparentClick the spec link, it shows the same content in a full page. reply desmondl 14 hours agoparentprevI agree with a lot of comments that it's minimal, but in my opinion that is a good thing. I'm a big fan of Obsidian, and of the things I like about it is the data source is all markdown files. Markdown is meant to be very lightweight and portable, and overcomplicating it will limit adoption and extensibility (imagine markdown vs pdf). JSON Canvas seems to follow in that spirit by being very lightweight, so a lot of implementation details (i.e. how are files rendered, what file formats are supported, etc), edit tags, etc) are left open to implementation. Markdown and JSON are meant to be non-opague file formats that prioritizes portability and human readability over other features. An application format like Sqlite has a lot of benefits over markdown, but it loses the benefits of text based formats like being compatible with git and is less portable. What I would like to see is a convention for extending the node and edge definitions, similar to frontmatter in markdown files- something that is not required for basic rendering but is a nice-to-have for applications to consume) - that way portability between apps of varying complexity can be maximized while still allowing for more complex features that some apps might implement. Markdown has the benefit of supporting extensions (for example like tables in GFM) - apps that are not compatible can still render the unsupported markup. But there should be an explicit way to extend open JSON formats. Some feedback off the top of my head and from reading the comments: 1. *Specifying node ordering*. Obsidian seems to just send whatever is last touched to the top, but this makes a common feature in many apps (move up/down/back/front) more difficult to impement 2.*More explicit definition for line shape*. Adding a way to \"bend\" a line in a specific way. Useful for creating more complex diagrams. 3. *Relations between nodes*. Group nodes contain child nodes, but the spec doesn't specify how the child nodes are defined. I would expect it to have a `children` property to nest nodes. Obsidian seems to implicitly link nodes to groups based on whether their bounds intersect. This makes it difficult to implement some common features: a. nodes that exist outside of the bounds of its group, for example a node that \"floats\" just outside of the edge of the group's borders. b. nodes that are not part of a group even though it exists within the bounds of that group. There are many different ways for a canvas app to extend the spec to implement those features, but it seems like something that should be defined in the spec to maximize portability 4. *Extensibility.* Either explicitly support or provide a standard for defining more styles for nodes and edges, such as stroke width, stroke style, rotation, etc. It seems like \"color\" should be a part of this as well, rather than being an explicit property of a node. 5. *Embeds.* Supporting \"embeds\" as a node type. I even think the \"file\" node should be redefined as `embed` with a `uri` property to support different schemes (`file://`, `oembed://`, `https://`) and maybe a `mime-type` (`text/markdown`, `image/webp`). The file node's \"subpath\" property seems to be only relevant for markdown files, so I think that should be an extension rather than an explicitly defined. 6.*YAML* :) (Should just seemlessly convert from json, but yaml is more readable than json) Being able to design standards that evolve over time and making tough decisions about what what to make explicit and what to leave implicit is a skill I want to improve on as a developer this year. Does anyone have any resource recommendations or best practices to recommend for me to research? reply exceptione 13 hours agorootparent> 6. YAML Please don't, it has one of the most confusing syntax out there with lists and maps, and it won't do well for parsing. reply desmondl 13 hours agorootparentI haven’t had any issues with yaml in markdown frontmatter or openapi specs. What kind of issues do you see with list and maps that make you against yaml? I agree that for computers and consistency json is preferred. I already use a linter for my markdown files so I would do the same with yaml to keep lists and maps consistent reply cstrahan 12 hours agorootparentYAML is kind of like C++: > You like C++ because you're only using 20% of it. And that's fine, everyone only uses 20% of C++, the problem is that everyone uses a different 20% :) https://eli.thegreenplace.net/2009/10/17/the-c-bashing-seaso... The YAML footguns are too numerous to reproduce here, so here are some sources: https://stackoverflow.com/questions/3790454/how-do-i-break-a... https://www.arp242.net/yaml-config.html https://noyaml.com/ YAML isn't terrible if you only ever have to read what you wrote. Now consider that there are 63 different ways to write multi-line strings in YAML -- how many of those have you committed to memory? Yeah... now throw 10-100 developers into the mix, each with their own favorite alternative syntaxes -- good luck making sense of your YAML. reply desmondl 12 hours agorootparentYeah true, I'm starting to remember the headaches with yaml when I was using kubernetes or cloudformation.... reply bbkane 12 hours agorootparentprevPoint taken, but you can mitigate a lot of this with yamllint. reply 8n4vidtmkvmk 3 hours agorootparentThat's pretty sad that you need to lint your config lang. reply exceptione 12 hours agorootparentprevIn the past I had to craft yaml files. Sometimes I needed quotes for a string, sometimes I had to put in a dash in front of a key, or just not. You basically needed to have the whole schema in your head. There can only be so much nesting before you lose track of what item belong to which parent. Copying some yaml structures over to another level requires care, as the result might look correct, but the white space parser thinks otherwise. I have lost hours of debugging yaml files when a dash was missing somewhere or when I needed one more leading space. The parser accepts it happily, but half of the typical javascript programs will only detect things are wrong when it has already executed on half of your spec. The other half will just run with input that wasn't intended that way. I remember writing artillery.io test specs where all those problems pop up. Now the good thing from JSON is JSON Schema. The latest spec allows you to specify quite advanced validations. Yaml has no such thing. As to your remark: Yaml for front matter is defensible, as you dont have deeply nested structures. Though, as an obsidian user you want to make sure your front-matter is conforming to your own schema. That would require writing a json spec and then have your yaml internally converted to json before handing it over to the validator. A spec is worthless if you cannot validate against it. Json and xml have a good story there. I concede that yaml is more human-readable than json without an editor. Correctness is the holy grail though. reply chatmasta 13 hours agorootparentprev> Markdown and JSON are meant to be non-opague file formats that prioritizes portability and human readability over other features I don't think human readability is a critical feature of JSON at this point. If that's your priority, you can use YAML. Readable JSON is nice because for small files you can read or edit small sections of it, and it's easy to debug when manipulating it with machine code. But there are plenty of cases where a huge JSON file is still useful even if it's barely human readable. My heuristic has always been: use YAML if you expect humans to create the file (or maintain large chunks of it), otherwise use JSON. For example, Kubernetes config is YAML because humans create it from scratch, and it would suck to do that with JSON. Whereas package.json is JSON because machine code initializes it and humans only make minor edits to specific fields. In the case of this canvas format, I wouldn't expect humans to create the file from scratch, so use JSON over YAML. Then the question is, will humans even care about reading the raw JSON? Probably not. So why not use something like SQLite or Protobuf? The most compelling reason would be that humans writing code to interface with the format can use parsing tools from their language's standard library. reply desmondl 13 hours agorootparentYep, I think the compelling reason of humans writing code is key here. SQLite would make it less accessible for people to write external tooling to integrate with an obsidian vault. There are lots of existing and open that support diffing/parsing/syncing/manipulating json, while with sqlite you have to not only know sql but support another application’s database schema, which third party developers are less likely to do reply naikrovek 11 hours agorootparentprev> I don't think human readability is a critical feature of JSON at this point. If that's your priority, you can use YAML. Wow you have kinda lost the plot on a few things. JSON was designed to be human readable and writable. YAML was designed to be a human readable format for the automated interchange of data between automated systems. Human writability was neither a goal for YAML nor its intended use. Like everyone else on the frakking planet, you’ve misunderstood what YAML was intended and designed for. YAML was never intended for human-written configuration storage, which is what everyone used it for the instant after they became aware of it. YAML can bite you very hard if you misunderstand it. JSON is simply invalid if you misunderstand it when writing it. If you don’t need human readability, use a binary format. Binary formats are so freaking fast compared to literally any structured text format, past, present, or future. High speed and low latency matter and binary formats make both of those easier. If you need to inspect the binary data, write a viewer using the code you use to read it. It’s a lot simpler than people believe it to be. I find Protobuf to be more of a hassle than writing the code myself, and protobuf is very easy to use, and I’m quite a moron. Binary stuff is not hard. reply louthy 12 hours agorootparentprev> I agree with a lot of comments that it's minimal, but in my opinion that is a good thing The purpose of a spec is to specify, and if you don’t specify and leave things open to interpretation, then that completely defeats the purpose. Anybody who’s worked with a poorly defined spec knows exactly how bad this can be. A good example would be the shambles that is the HL7 spec used in healthcare. A former colleague had a phrase for this: “once you’ve seen one HL7 message… you’ve seen one HL7 message”. Which really highlights the issue of a standard that’s open to interpretation. The issues raised (in the comments here) seem to hint at a lack of specificity. That is something that they should really look at improving. I think overall any group that tries to come up with a standard that can unify a field should be lauded and supported. But perhaps calling this a 0.1 release, and taking the feedback on board, would be the best way forward. reply gcr 14 hours agoprevThis is a great idea. For now though, the spec is under-specified and ambiguously terse. A few points that could be clarified: - How do coordinates work? Does +Y point up (OpenGL) or down (web)? Is the origin meaningful? What are the units - how does this interact with font sizes? High-DPI displays? - What’s the difference between a file node and a URL node linking to file://./? Are files supposed to be transcluded? What filetypes are allowed? The home page seems to have an image — was this done using a file node or a markdown node with anelement? - What HTML tags are allowable in markdown? Is JavaScript allowed? - Why does the group node allow a background image? If both image and color are specified, which takes precedence? How are children of the group specified? A couple feature requests for extensibility or interoperability with Excalidraw and TLDRaw and friends: drawings / scribbles, predefined shapes like circle or rectangles, ability to specify fill style, edge width, transparency, ability to embed images, more detailed placement for connector start/end points, etc. reply SpaghettiCthulu 9 hours agoparent> Is the origin meaningful? I assume no. > What are the units Arbitrary. The website says pixels, but the demo lets you zoom in and out, so I think defining the unit as pixels is pretty meaningless, except as a hint to the viewer of the initial scale for the canvas. Even then I can see good reasons for a viewer program to just ignore that and use whatever initial scale allows everything to fit on screen. > how does this interact with font sizes? Seems like the font size can't be changed, but I'd imagine it's a specific number of units. > High-DPI displays? Not sure what kind of answer you're looking for here. You can just scale everything, so support for High-DPI displays would be up to the viewer program. reply gcr 7 hours agorootparentI know this isn’t intended as a document presentation format, but for a consistent layout, it would be a good idea to specify the ratio between pixels (for layout) and font size (in pt). Otherwise, viewers would render text inconsistently-each node would be some ratio too big or too small for its contents. reply gitgud 4 hours agoparentprev> Is the origin meaningful? I assume yes, as it makes placement of things much easier if they're all relative to a known origin. The limits of the canvas would be \"realistically infinite\" in all directions though I assume... reply 8n4vidtmkvmk 3 hours agorootparentI don't know if \"realistically infinite\" does or doesn't work here. A lot of video games with huge worlds start to break down with big coordinates due to loss of precision in the floats I presume. If using integers, I'd cap it at 2^53 to align with js's max safe integer which I think is just a double. reply asa400 15 hours agoprevI've worked a few jobs now where application data was stored in text files of various kinds (homegrown as well as well-defined formats, JSON included) and it pretty quickly becomes a mess when you start talking about modifying it over time, evolving its schema, validating it in the face of end-user edits, ensuring threads don't write to it concurrently, etc. This strikes me as exactly the type of application data that would benefit from being represented in SQLite. Of course, JSON is a `JSON.parse` away, but now you're building your own...everything else. Storage/retrieval, validation, querying/filtering/indexing, graph traversal, etc. It's all yours. There's so many benefits to building this kind of thing in SQLite. You get data validation, atomic transactions in memory and on disk, a high-level query interface, lazy loading (i.e., only load nodes at most 2 edges away), triggers (when I delete this node, automatically delete edges that point to it), and a stable on-disk format, to say nothing of the fact that SQLite itself is just about the most stable software there is. By the way, no disrespect to JSON Canvas, it looks like good work, just trying to offer the perspective of someone who has done stuff like this in the past. reply simonw 14 hours agoparentI'm usually the first person to suggest SQLite for just about anything, but in this particular case I do feel like JSON is a better default format. Interacting with SQLite from different programming languages is easier than most other formats, but you still need a SQLite binding. They're available for every language but that's still a not-completely-trivial dependency. I expect most tools that people build against JSON Canvas will run in a web browser. Adding SQLite as a dependency means you need SQLite running in WebAssembly - totally possible, and even officially supported these days (the SQLite team run their own WASM builds now) but still a sizable piece of extra complexity over just using JSON.parse(...) Also: SQLite files aren't very easy to diff, so they're not great for collaboration in version control. JSON is better for that. I'm 100% with you on the schema changes and versioning challenge. The best way to address this IMO would be for the spec to include a top-level \"version\": key which indicates the version of the spec that a file was created against. Handled carefully and introduced right at the start of the project this could ensure an ecosystem grows up around the standard such that older spec versions can always be opened by newer implementations, and any implementation can fail-fast if it is given a file that it doesn't yet know how to handle. reply asa400 14 hours agorootparentValid points and I generally agree! I think you're probably right that the SQLite dependency is possibly too much for some applications to pull in, though I will ask why we're so often willing to pull in more or less anything else regardless of weight (DOM manipulation, state management, animation, etc.) but a robust data layer is often a bridge too far. I wish I knew why this is, as it seems to be one of the larger cultural differences between folks who work mainly \"on the backend\" and \"on the frontend\". > Also: SQLite files aren't very easy to diff, so they're not great for collaboration in version control. JSON is better for that. Yeah, if we're talking about diffing the literal file itself, then that changes things. At that point, we're not just talking about a storage format, we're talking about interchange as well. In that case I'd ask - of course this is application specific, not general - what data are we putting on the wire? Where does that data live? For example, if the main state state lives in a your browser instance, and you ship updates (i.e., \"CREATE\", \"EDIT\", \"DELETE\" or some such) back and forth between collaborators, then diffing the state of whatever you have is fairly easy, SQLite or JSON or whatever else. But if we're shipping the actual file itself over the wire or attempting to version control it, then you're absolutely right and diffing the SQLite file is inferior. There are some interesting tradeoffs in this space. This is a fun discussion! reply desmondl 13 hours agorootparentI think you're talking about the trade offs between supporting features like \"DOM manipulation, state management, animation, etc.\" and \"shipping updates\" out of the box, versus only storing the data as simple files and leaving everything else to the implementation. Sqlite as an application file format is great [1], but for a knowledge base / note taking app the benefits are not worth the tradeoffs in my opinion. Sqlite is more performant more performant and provides lots of built-in features. However, most note taking users do not have enough notes or files to benefit from that performance. Sqlite will also lock the user into the application, whereas a \"pack of files\" can be used in the shell as a text editor. Using markdown files + a open json format has the benefit of being supported by multiple applications (e.g. sometimes i open my obsidian vault in vscode), while a sqlite database would need a proprietary schema coupled with a single application I prefer an open file format that isn't tied to a vendor. A \"data bridge\" might handle syncing and diffing more efficiently than plain files, but it is still tied to the vendor. For example, I prefer not to pay for Obsidian Sync, and I'm able to use a git plugin and storing my files on nextcloud to sync between my devices. This leverages existing tech without having to implement features from the ground up [1] https://www.sqlite.org/appfileformat.html reply eviks 5 hours agorootparentExcept the markdown files are tied to a vendor outside of trivial formatting since it's a simplistic underpecified format without extensions, so all these obsidians specify their own extensions to add complexity, which your vscode does not support (and neither would git diff help to see data change in a sea of formatting changes) And this spec is for complicated layouts, not trivial notes you're comparing it to, so your intuition from simple notes doesn't translate to this use case reply asa400 13 hours agorootparentprev> I think you're talking about the trade offs between supporting features like \"DOM manipulation, state management, animation, etc.\" and \"shipping updates\" out of the box, versus only storing the data as simple files and leaving everything else to the implementation. I'm not sure I understand. Can you clarify? > Sqlite is more performant more performant and provides lots of built-in features. However, most note taking users do not have enough notes or files to benefit from that performance. For a graph with lots (thousands+) of nodes/edges, SQLite is probably capable of being more performant than a JSON file, depending on whatever specific kind of performance we're measuring. That said, to me, the most interesting thing that SQLite gives for applications when compared to flat files is data integrity, via transactions and schemas/constraints. Performance is nice but not even close to the most interesting thing about SQLite in most applications. Performance has never been the reason I've chosen SQLite over flat files for my applications. > Sqlite will also lock the user into the application, whereas a \"pack of files\" can be used in the shell as a text editor. Using markdown files + a open json format has the benefit of being supported by multiple applications (e.g. sometimes i open my obsidian vault in vscode), while a sqlite database would need a proprietary schema coupled with a single application In a sense you're right about this! I'll grant it's easier to open a JSON file in vscode and edit it if you already know vscode and JSON. That said, SQLite is in the public domain with a well-defined, stable format and there are countless free and open source database editors/viewers out there. SQLite is self-describing, also. You open `sqlite3` and type `.schema` and it shows you the database schema. You enter a query and get some results. It's all right there. So, while a database might have a schema that was designed for a particular application, that doesn't mean you as the end user can't tinker with it, and the number of people who know SQL is rather large. reply apitman 13 hours agoparentprevSQLite is awesome, but it's still an order of magnitude more complex than JSON. If it wants to be the one file format to rule them all, we're going to need high quality and heavily used implementations in most languages. Adding C sqlite to a golang project adds a significant hit to build times and cross-compilation/static linking complexity[0]. When I looked into the native Go implementations of sqlite I came away with the feeling it wasn't worth the tradeoffs compared to using the C version, but now I still have to deal with the issues above. I haven't looked deeply into how sqlite works, but my instincts tell me the reason we don't have high quality implementations in every language is because it's actually too complex to treat as a protocol. I would love to see something fill the void between plain text and sqlite. [0]: https://www.arp242.net/static-go.html reply mlunar 10 hours agorootparentCheck out https://github.com/zombiezen/go-sqlite if you're interested in trying out Sqlite in Go again. Nice interface, negligible compile time impact, fast, compiles without CGO. It's very comfortable. I agree that going from text to sqlite is a bit of a hurdle, especially if you're not writing C :) reply chrisweekly 14 hours agoparentprevCrucial context: its provenance is embedded infinite canvas in Obsidian (amazing markdown-based notes app++), which supports JS but has no external datastore per se. SQLite is fantastic, but inappropriate for this use case. reply asa400 14 hours agorootparentOh I read the post and a few of the other linked posts so I'm aware. If we're storing a file, we're already storing a file of some kind, so we're just talking about what kind of file it is. I'm just talking about a different set of tradeoffs one can make if that file is something other than JSON. reply LordDragonfang 13 hours agorootparent>If we're storing a file, we're already storing a file of some kind It's not just a file \"of some kind\", it's a text file. That's one of Obsidian's key selling points - that a vault is simply a collection of text files. The \"text\" part is important to both Obsidian's philosophy and the majority of its users. reply asa400 12 hours agorootparentSure! And in a lot of ways this is about values more than technology. reply exceptione 13 hours agoparentprevSQLite has no real types. Use SQLite if data quality is of no concern. The applicability domain of SQLite is far smaller than people think. If you want to improve on JSON, you would have to go into an other direction. Maybe something like postgis would be helpful for extremely large canvasses. JSON Schema is pretty powerful by the way. Checkout the documentation. SQLite is absolutely no match there. What Sqlite could bring is speed, but I dont see how in the contxt of canvas it would be of any help here. reply asa400 12 hours agorootparentSQLite's types are quite loose by default, no doubt. There is a somewhat recent STRICT mode that strengthens them: https://www.sqlite.org/stricttables.html reply samatman 12 hours agorootparentExactly so. SQLite also has a rich collection of CHECK constraints which can raise errors if data is not to your liking in some fashion, this includes validating JSON. Not a JSON schema, admittedly, although (just like for Postgres) this is available as an extension. https://github.com/asg017/sqlite-jsonschema Saying that SQLite doesn't have \"real types\" is simply false. If one doesn't want to learn how to use a tool, blaming it for that failure is poor form. reply exceptione 11 hours agorootparentThe author of SQLite is quite open about it. The lack of typing has been part of the design from the beginning. Sqlite has hardly any types: INT INTEGER REAL TEXT BLOB ANY Of course one can program all kind of check constraints, like one can program all kinds of value validations in javascript. Unfortunately, that is not the same as typing. Sqlite lacks typing because, as the sqlite author explains in the docs, flexibility is the goal. He continues with \"But other developers are aghast\", and so strict tables where born, but you can clearly see this cannot overcome real concerns. Try to look for the DATETIME datetype in that list. Deep bow to sqlite, its design goal was to be the ini file replacement and it has outperformed itself on that one. Thanks for the extension link. Although constraints are not reuable type definitions, they would still be helpful in this context. Pity that json doesn't have a type for dates, one has to rely on string formats: https://json-schema.org/understanding-json-schema/reference/... reply samatman 6 hours agorootparent> Of course one can program all kind of check constraints, like one can program all kinds of value validations in javascript. I don't consider this a valid distinction where databases are concerned. If you define a datatype in an ordinary SQL database, and try to pass it invalid data, it will fail at runtime. How else could it work? There's no compile-time interaction between the value and the database. If you define a field as BOOLEAN in Postgres, then the value must be 0 or 1, or the database will refuse to write it and return an error. In SQLite this is spelled INTEGER NOT NULL CHECK (col_name = 0 or col_name = 1). More verbose? Yes. Identical semantics? Also yes. It would certainly be nice if SQLite had a datetime validator that could be used as a check constraint! Hold, up, I got you fam: CHECK(date IS strftime('%Y-%m-%d', date)). If you need a different format, those are,, available. I guess it has a date type after all! Learn something new every day. reply asa400 5 hours agorootparentHeh, I haven't seen that particular datetime constraint before, thanks for that! reply wizzwizz4 8 hours agorootparentprevAren't these the types? NULL INT(EGER) REAL TEXT BLOB (INTEGER PRIMARY KEY) reply toddmorey 13 hours agoparentprevAgree that SQLite is a great local format for this sort of thing, but seems to get tricky when trying to sync across clients without conflicts. I've seen most CDRT schemes working with json documents rather than SQLite, but curious if there's solid conflict-free syncing out there for SQLite files being used inside applications? reply mch82 9 hours agoparentprevThe SQLite docs have a great page on the use case `asa400` describes. SQLite as an Application File Format, https://sqlite.org/appfileformat.html reply eviks 5 hours agorootparentWhat's the best example of this format used for complex docs, has anyone implemented their OpenDocument replacement idea? reply apitman 13 hours agoprevObsidian is one of the few closed-source applications I would consider relying on, due to their commitment to building around simple, open file formats. Sure, they could screw me over and start charging absurd amounts of money for their app, but high quality open source alternatives would pop up immediately. Meanwhile, as long as they don't screw me over, it's unlikely an open source alternative is going to be able to catch up to a profitable business that keeps their users happy. It's an interesting approach, focused on incentive alignment, which is the best way to ensure quality long term. reply chatmasta 13 hours agoparentI love Obsidian. I wish they would consider open sourcing the application. It doesn't even seem in conflict with their monetization plans, because they're already distributing the app for free, and making money with things like \"Obsidian publish.\" They've got enough critical mass and a sufficiently thriving ecosystem of community plugins that they could only stand to benefit from open sourcing the core app. See Mattermost for an example of a similarly positioned product that is fully open source. reply dugite-code 9 hours agoparentprevThe big gap I would love to be solved is a, preferably selfhosted, browser based view into my notes. That way I could access my notes from computers you can't or won't install obsidian on. If it was open source that would be more likely to happen reply Brajeshwar 8 hours agorootparentDo you mean how your notes are connected, and organized? Otherwise, they are plain-text Markdown files. Any app that renders Markdown should be able to do it. In-fact, I don’t really like Obsidian on Mobile, so I use iA-Writer to edit/view the Markdown files that I managed with Obsidian on the Desktop. reply dugite-code 5 hours agorootparent> Any app that renders Markdown should be able to do it. Sure but I can't interact with them the same way as I do in Obsidian. It's an electron app so it's already heavy on the web based tech. Currently I export my notes as a webpage and edit using my Nextcloud instance. It works, but it's not very nice. reply desmondl 13 hours agoparentprevI completely agree. Even if they completely tank I can open my obsidian directory in a text editor or command line and still use it. I would still have access to features that are common in other apps like full text search or plain file sync. Attachments are just files in the filesystem that can be opened in any image viewer. Basically if i can’t use obsidian anymore i can still use my notebook and take notes without implementing or finding new software reply iainmerrick 15 hours agoprevThis looks a little pre-1.0, it's quite short on detail. For example: file (required, string) is the path to the file within the system. What kind of path, within what system? It's not clear that the 'file' type couldn't just be another kind of 'link'. If various fields like 'background' were defined to be URLs, that would offload a lot of complexity onto existing web specs. reply gvx 15 hours agoparentYeah, other details are missing too. For example backgroundStyle: > cover fills the entire width and height of the node. Does that work like the CSS background-size: cover; or background-size: 100% 100%;? > ratio maintains the aspect ratio of the background image. Does that mean CSS cover? contain? Something else? reply wongarsu 14 hours agorootparentColors can be specified in hex format, e.g. \"#FFFFFF\". Six preset colors exist, mapped to the following numbers: 1 red 2 orange 3 yellow 4 green 5 cyan 6 purple I'm sure everyone will infer the same color codes here. Maybe the file format isn't meant to reproduce the exact same look in different software, but merely communicate user intent. Your guess is as good as mine. reply gcr 14 hours agorootparentI’d almost prefer string literals for color with constants like “red”, “blue”, etc. that compatible implementations could theme as they see fit (eg terminal emulators). Perhaps that’s the intent behind the numeric constants but string literals would convey that better IMO reply jakelazaroff 16 hours agoprevI really like the idea of a format for interchange between infinite canvas apps, but the preset colors and list of node types makes this spec feel strangely opinionated. You could build something like Kinopio [1] but not much beyond that. It looks really promising though! I'm definitely interested in seeing this grow. [1] https://kinopio.club reply iainmerrick 15 hours agoparentThinking about it, I'm not sure any interchange spec would leave useful room for innovation. This isn't like EPS or PDF or something where the kind of output is well-defined (printable graphics + text) and the innovation is in the editing interface. The innovation in canvas apps is surely in the semantics of the nodes themselves; but if you add some new node type with special behavior, how can you usefully export that to other apps? You could make it a black box that can round-trip safely, but that doesn't seem very useful. Maybe if it's something like an HTML embed or iframe? reply gcr 13 hours agorootparentIt would be fun to go the postscript route and have the nodes be specified by some bytecode on some VM that lays the objects out on the canvas while specifying their editable properties. Then, conforming implementations could render any document just by following the instructions, while editors that actually understand them can provide their own high-level control. The trick is keeping it editable, which postscript doesn’t do well. Example: if the language is strong enough to, say, implement force-directed node layout, an editor that doesn’t understand it could still add nodes and they would move around according to the document author’s wishes whereas perhaps the original editor might have more powerful editing capabilities. reply cmgriffing 14 hours agorootparentprevI think a baseline spec would be really useful. I like to think of it like the unist ecosystem for ASTs. Unist provides a baseline spec that compatible tools can use to comb an AST. Then, specific AST tools like hast for HTML or sast for CSS/SCSS can add their own metadata on top. I'm imagining an ecosystem of \"adapters\" that would help you translate some of the metadata across providers. reply heleninboodler 15 hours agorootparentprevMy very first thought about this format was that someone is definitely going to pervert it to be used as a graphics format, and for that it's woefully inadequate. I decided to hold my tongue since it's not the creators' fault that someone will misuse it, but I can totally see people adding optional ad hoc fields to nodes to allow for example all sorts of fancy line styles. reply treflop 15 hours agoprevSince a whiteboard is much more a visual thing, I imagine the spec should spend a lot of space defining the visual elements like specifying the control points of the Beizer curves because where a line is drawn and what it overlaps matters a lot on a whiteboard. But to be a total downer, this spec looks like an extremely rudimentary graph file format, of which there are already like a hundred and all define more visual aspects than this spec. reply xamde 14 hours agoparentFrom my Analysis, GraphML, although specified in XML, seems to be one of the most widely used exchange formats, especially with the yWorks extensions for yEd. reply lovasoa 15 hours agoprevAs a maintainer of an open source JavaScript infinite canvas application [1], I was very interested, and now I am a little disappointed. The set of supported objects on the canvas is quite minimal. [1] https://github.com/lovasoa/whitebophir reply apitman 13 hours agoparentIt's an early revision. Now sounds like the perfect time for you to share your experience and suggestions to improve it. reply eviks 5 hours agorootparent0.1 is early revision, 1.0 is long past that prefect improvement time reply heleninboodler 15 hours agoprevSuggestion: add some metadata at the top level, including a bounding box that includes all nodes so that you can encapsulate a canvas and include it within another context before having created all the elements within it. It's redundant information and it needs to be kept in sync with the nodes and edges, but it's very useful for applications working with your data. Also, this is not very json-ish, but optimizing your serializer so your metadata is always written first is pretty handy for embedding, since it allows you to use a pull parser and do useful things before the entire doc is parsed. (e.g. picture a huge doc being embedded and starting out as just a box and having its elements filled in async. You can't do that well if you don't know the bounding box ahead of time) reply kijin 6 hours agoparent+1 for bounding box. It would save a lot of computation when trying to determine the initial scale for the canvas, especially if text is involved. It would also make the format useful for fixed-size canvases, not only infinite ones. reply dleeftink 7 hours agoprevGood to see this moving ahead, but doesn't a well-defined graph format already exist in Argdown? [0] While its renderer uses auto layouting instead of user defined coordinates, the principle of using Markdown files instead of JSON seems more appealing to me considering the Obsidian/git workflow. Besides Argdown, Markdown itself provides some built-in features that can be utilised to construct graphs. Coordinates for instance, can be stored as HTML comments or link alt texts, e.g. `[node](# (\"x:25,y:50\"))`. Edge, shape types and other data could similarly be stored in alt text fields as serialised JSON or in separate blocks using link reference definitions. [1] One step further, Markdown lists could be used to store subtrees while cycles as Obsidian block links. This also allows you to encode ancestral, sibling and descendant relations: - [root](# (\"x:25,y:50\")) - leaf - [link](#^id) - another leaf^id You'd then be able to interleave prose and graph structures in a single file rather than dealing with two separate parsing structures. Even better, the end result would still be Markdown compliant. [0]: https://argdown.org/ [1]: https://github.github.com/gfm/#link-reference-definition reply ramses0 16 hours agoprevThe spec basically fits on a page: https://jsoncanvas.org/spec/1.0/ Summary: \"node: { type: ..., x/y/color }; edge: { from/to: ..., color/label/... }\" Refreshingly simple, especially paired with their \"gif of usage\": https://obsidian.md/canvas reply Caddickbrown 16 hours agoprevJust realised this is by the Obsidian guys! Good on them! reply simonw 15 hours agoprevIt would be neat if the README or spec included links to some real-world examples of files - would be easier to start playing with building simple tools on top of this if there was already an example file to start experimenting with, without me having to learn Obsidian first. UPDATE: Figured out how to create one: 1. Install and then open Obsidian 2. Click the \"Create new canvas\" icon - third down of the icons on the left 3. Add some stuff to the canvas - I double clicked to create a few boxes, put some text in them and then dragged lines between them 4. In the ... menu on the top right click \"Reveal in Finder\" You can then open the file it reveals in a text editor to see the JSON Canvas format. reply jakelazaroff 14 hours agoparentIf you hit \"toggle output\" on the bottom right, it shows a JSON Canvas representation of the page's content. reply simonw 13 hours agorootparentIs that an icon? I can't seem to find it (Obsidian 1.4.16 on macOS) reply jakelazaroff 13 hours agorootparentSorry — I mean on the bottom right of https://jsoncanvas.org! reply politelemon 14 hours agoprev> The JSON Canvas format was created in hopes of providing longevity, readability, interoperability, and extensibility to data created with infinite canvas apps. If I'm reading between the lines, this is only supported by Obsidian (as it's by Obsidian)? Considering the complexities and 'malleability' of infinite canvas tools, it would have been prudent to have involved or approached some of the major players in this space, like Excalidraw, Draw.io, Microsoft, Figma. Or at least started at version 0.1 and once it gained a wider consensus, release 1.0. reply catapart 14 hours agoprevOh sweet! I was in the middle of building my own version of a node-graph component (ref: https://github.com/catapart/Magnit.NodeGraph), but the `canvas.js` implementation from this JsonCanvas repo is exactly what I was trying to build. I really, really like how elegantly uncomplex it is. Could not have been more exactly what I was looking for. So thanks for this! As far as the spec, I don't really like the idea of forcing well-known types for the nodes. A generic spec should allow for entirely generic nodes that can represent themselves to consuming functions with a 'type' property as a key, as well as arbitrary data types linked to arbitrary nodes. For instance: one of my use cases is an 'addition' node, which would take two number values and produce a number value. This node would also use an entrance execution pin as well as an exit execution pin. If the spec were to include a 'pin' data type and capture the type keys and labels for pins, those pins could be stored as a list on the node. Then, the type property could just tell the executing context how to route the node data and the pin properties would bring type safety to the functional inputs passed to the mapped function. Anyway, I assume all of that is out of scope for initial offerings, but that's my two cents on a generalized node spec. Regardless, thanks again for the sweet, simplistic node graph implementation! reply wongarsu 14 hours agoprevBased on the title I would have expected this to be about describing images on an infinite canvas, where different parts have different zoom levels. This is instead about whiteboard-style graphs. Which is useful, but I find the branding \"An open file format for infinite canvas data\" to be confusing. Nothing in it implies whiteboards or graphs to me. The fact that the canvas is infinite doesn't even have an obvious influence on the file format, apart from the absence of a canvas size property. reply theogravity 15 hours agoprevIt would be nice to see actual examples. We use an infinite canvas (https://switchboard.app), but it's hard for me to map the objects we have on our virtual-desktop-like-canvas with the file format described here. The items on our infinite canvas are more akin to a desktop app, where most objects are application windows. reply m12k 15 hours agoparent> It would be nice to see actual examples If you click \"Toggle output\" on the linked page, you can see the code for the page itself reply theogravity 15 hours agorootparentThanks. The available types definitely do not map to what we do. It seems the format is more suited for displaying DAGs. reply GuB-42 9 hours agoprevWhy make so many fields required? Node: only id, x, y would be necessary. This would allow for point nodes. We could even imagine letting go of x and y to signify that the position of the node is not fixed and could be recomputed in real time by the program. Even ids could be optional, why require them if they are not referenced? An added bonus of having point nodes is that you get freehand drawings for free: every stroke is a series of connected dots. Maybe it is an anti-feature though, depending on your vision. reply mcqueenjordan 9 hours agoprevExploring the concept of files > app deeper, it would be interesting if we were able to foster a culture of website apps writing to local storage with files (in a similar manner to Obsidian), and if we had a common format for doing so, with an open-source daemon that sync'd writes to and from that directory to e.g. some other folder. That would unlock ownership of data even in web apps. The daemon could be app-agnostic and just dutifully sync all the things. reply adamfeldman 15 hours agoprevThis was released (and originally implemented) by Obsidian: https://obsidian.md/blog/json-canvas (see also https://news.ycombinator.com/item?id=39670684) reply mstipetic 15 hours agoparentI would highly recommend using this plugin with it - https://github.com/rpggio/obsidian-chat-stream It has basically changed the way I interact with LLMs and research and plan things reply tiborsaas 15 hours agoprevI really like that you commit to keep this stable and open. Do you plan to make the TypeScript definition part of this new site? https://github.com/obsidianmd/obsidian-api/blob/master/canva... For me it's easier to read TS format. reply egeozcan 14 hours agoparentI didn't know about this and made ChatGPT prepare me a typescript definition form the markdown: https://gist.github.com/egeozcan/27db06f6771dcf214f0f92bce8c... :) Perhaps I shouldn't be too lazy to google things. reply Macha 14 hours agoprevOne of the big reasons I use Obsidian is the data portability - while it provides some nice enhancements, if Obsidian went evil tomorrow and I had to use my notes in VS Code tomorrow, then even reduced to just \"a folder full of markdown\", I'd get 90% of the value without relying on any convoluted importers/exporters. I've stayed away from their Canvas feature largely because it is.. not that. Not because the Obsidian developers have kept it locked down in some crazy proprietary format, but like a JSON file representing the canvas is pretty useless without something to interpret it and these days Obsidian is still the only implementation. So I kind of hope this takes off. Having a second source available would make me feel a lot more comfortable trying out the canvas feature. reply chatmasta 12 hours agoparentI've stayed away from their Canvas feature because the Excalidraw plugin for Obsidian is unbelievably good. I love it. reply darkteflon 12 hours agoprevHow interesting. I was just this weekend looking to implement a DAG/multitree-based tasks app as a side project, and my first thought was to do it as an obsidian plugin with D3 for rendering. I ended up canning that idea because it felt like a bridge too far to cram nodes and links into a human-readable format like markdown. I went with SQLite and a TUI instead. Seeing this spec emerge makes me think that Obsidian might be a good place for that kind of plugin after all. It’s becoming something of an emacs, Obsidian is! reply Retr0id 15 hours agoprevIf ids are unique strings, then why are `nodes` and `edges` arrays, as opposed to id-string-keyed maps? reply eropple 14 hours agoparentThis is always a tough spot in JSON, I think. You want the `id` in the node or edge object, but you also want uniqueness. I don't think there's a great way to get this any be able to just `JSON.parse()` and go without further fixup. reply Retr0id 14 hours agorootparentWhat do you concretely gain from embedding the id within the object? reply Macha 14 hours agorootparentYou have a reference to an object, maybe found by filtering the list based on co-ordinate intersection, and you need its ID to put somewhere else (e.g. in the \"to\" field of an edge). Sure, all your find/filter methods could return (ID, object) or just return an ID and make you do a second lookup to find the object, but at the kind of object counts we're talking about for this implementation, a small amount of extra memory to have the ID as a field in exchange for a nicer API is a reasonable tradeoff. reply desmondl 13 hours agoparentprevAn object in JSON is meant to be unordered [1], so arrays are better if you want order to be preserved. [1] https://www.json.org/json-en.html reply Retr0id 13 hours agorootparentSure, but why do you want order to be preserved? The spec never says the order of nodes is pertinent. Is there undocumented z-index behavior? reply desmondl 13 hours agorootparentSometimes you have nodes that overlap each other, so you want to control whether or not a node is in front of or behind another node. Though yes, they could have explicitly defined a z-index or defined a convention on how the ordering should work (first nodes top and last nodes bottom or vice versa?). It's interesting to think about the trade offs between explicitly defining these things vs. leaving the application to implicitly make the choice. JSON Canvas seems to be designed to use in tandem with markdown files in a note taking app, so it makes sense why they opted for the more implicit design to be similar to markdown reply Retr0id 13 hours agorootparentprevLooking at their reference impl, it does seem that array index implies z-index, in which case fair enough, I suppose an array does make the most sense - they should probably document the z-indexing though! reply Retr0id 15 hours agoparentprevA couple of other nit-picks: - I'd like to see the `file` node have a mime type field. - Why limit links to URLs, as opposed to URIs? - You might want to put a version field in the top level! reply regus 15 hours agoprevWhat is the upside of using an infinite canvas? I tried playing around with them in the past but I think my brain's mental model of how to parse data is incompatible with \"clutter\" (for lack of a better term) of an infinite canvas. reply aragonite 15 hours agoparentCheck out this video from one of my favoriate infinite canvas softwares, especially beginning around 1:25 (which I'm deep-linking to): https://youtu.be/GblI7GI0jQ4?t=85 (Alas, the original desktop app (iMapping, in Java) as shown in the video is no longer being developed, and now they are only developing the web app (Infinitymaps) only run in the browser, which imo is not the best fit for infinite canvas apps which can be very resource-intensive) Also see the app author's own all-purpose mega-map that he used to organize everything: https://youtu.be/bTQWL5wmdZY?si=6VrnPIErOzasisEe reply DrDeadCrash 13 hours agorootparentImo it would need to show varying levels of information density depending on scale. Otherwise, as another commenter stated it's overwhelming.... reply allenu 14 hours agoparentprevI've always wanted to build an infinite canvas idea to try it out, but whenever I've used implementations of it (such as in the Muse app), it just feels wrong. It feels like a beautiful way to interact with essentially fractal information, but in practice it doesn't quite work to me. I agree with you that I don't think it maps well to the mental model of the brain. Seeing the youtube video link in the sibling comment, when the user is completely zoomed out and can see everything, I just feel overwhelmed looking at it all. Maybe it's the nature of the boxes having different scales that you can't compare them as easily to each other compared to just a regular canvas where things just place in two dimensions. Each time you zoom into a canvas, the transition causes a lost sense of place and space, akin to walking into a doorway to a room and wondering why you walked there to begin with. reply ivanjermakov 13 hours agoprevActual spec: https://jsoncanvas.org/spec/1.0/ reply ianbicking 15 hours agoprevThere's some questions about interoperability, which I imagine to be hard since any canvas app involves viewing and editing and probably has unique entities and approaches which other apps wouldn't be able to handle. BUT, I imagine this being more useful for creating non-frontend tools. For instance a server that returns subsets of nodes for a particular viewport. Or something that may index nodes, or produce search results. Or tools that simply generates canvases from other data as a one-way operation. reply skadamat 14 hours agoprevCurious to know which projects & companies were involved in creating this. Ideally an open standard has a bunch of folks involved & informed of the design process so we get a design that lasts! Braintree payments did this well when they were still a startup. They had to collaborate with their direct competitors to create a standard for the entire POS induustry. What prevents us from doing this here as well? reply dekhn 15 hours agoprevIs this infinite canvas similar to QGraphicsView? I've been looking for a good equivalent to QGraphicsView and HTML Canvas is not it (HTML Canvas is a raster image, QGraphicsView is a size-independent fixed-sized, scrollable canvas with indexed objects). reply tonyarkles 12 hours agoparentWould it be possible to bang SVG into what you’re looking for? It’s a little bit quirky but I’ve used it to draw some pretty wild diagrams with a small amount of JS for pan and zoom. reply raggi 15 hours agoprevI would love to see this and excalidraw converge feature & spec wise. reply desmondl 13 hours agoparentAny reference for obsidian's specs? The closest thing I could find is this: https://docs.excalidraw.com/docs/codebase/json-schema but it seems to be really minimal and doesn't go into detail on what properties belong in elements reply airstrike 14 hours agoprevThis is neat, I'll definitely try it out. I do have a related question: what are people using to implement high performance canvases on React these days? reply WuxiFingerHold 5 hours agoparentThe canvas in Obsidian is as the whole app very well made. I wondered what they are using as well. My guess is https://www.xyflow.com/, which is for drawing nodes. More general purpose would be http://fabricjs.com/. Or very low level https://pixijs.com/. reply account-5 11 hours agoprevIn a similar vein I prefer Treesheets to this sort of thing. I like grids better. reply RyanHamilton 13 hours agoprevI would like to take this opportunity to implore everyone to not invent a new format. Avoid it at all costs. First try to follow an existing standard or say that you will support a subset. When you invent a new standard, you start small and say oh users just need to know this and look it's so simple. Then you find all the edge cases, add all the features and voila you have created a half baked implementation of an existing standard. Only if you can explicitly say many reasons why you haven't used standard X or Y create something new. To take this specific example, some of it feels very similar to HTML. label, links, sections, groups, anchors, background fill options...I would have been tempted to define as a subset of HTML that is supported. Then if I wanted JSON, say how JSON maps to HTML. voila suddenly everything is standardly named and creatable. This means backgroundStyle is replaced by background-size = cover or contain. It means that those six preset colors are replaced by all HTML standard colors. Voila no one needs to learn different concepts or definitions. Try that existing standard, try 2 more and only if they don't work invent a new one. Please. I say this as someone that inherited standards invented by teams that I then had to try and train hundreds of users on. Funnily enough the previous people left when the coding was done, without teaching the users. THey probably left to implement version 2 elsewhere. :) reply mywacaday 13 hours agoprevFYI only the spec canvas is black on chrome mobile with dark mode enabled. reply dorian-graph 11 hours agoprevI was hoping this would include infinite _nesting_! reply darkest_ruby 11 hours agoprevIs that what nodered uses for its diagrams? reply CubsFan1060 15 hours agoprevIs there any control over the routing of the edges? reply iJohnDoe 7 hours agoprevI’m confused by the comments. This seems really cool. Are there other solutions that take JSON and do the same thing just as easily? reply mock-possum 15 hours agoprevLove obsidian and it’s cute that the site itself is presented as a ‘canvas’ interface element, but… not a great experience on mobile (iOS/safari) - pinch to zoom and drag to scroll are both a bit busted, the zoom ui controls can disappear altogether, and there’s no ui indication that the box on the right, that contains the details of the spec, is actually a scrollable container (ie no scroll bar visible) reply subtra3t 15 hours agoparentReally? I used the website on Chrome on my Samsung Galaxy M12 (a fairly average smartphone) and the experience was fine. reply curtisblaine 13 hours agoprevWhat I would expect from an infinite canvas app would be a streamable data structure, that can be rendere on demand on a \"virtual\" canvas - let's say you have a window from 0,0 to 100,100 - you can query the data structure to give you only the nodes in the viewport + the nodes connected to them. reply cyanydeez 11 hours agoprev [–] need, but on mobile I want to double tap to zoom and fit content reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "JSON Canvas is an open file format designed for infinite canvas data, enabling users to visualize and arrange information spatially for improved organization.",
      "It offers longevity, readability, interoperability, and extensibility for data created using infinite canvas applications, using the .canvas extension, and can be integrated into various tools and applications freely.",
      "Originally developed for Obsidian, JSON Canvas files, like logo.svg and readme.md, are open source under the MIT license, with navigational links like spec/1.0.md for additional details."
    ],
    "commentSummary": [
      "Obsidian has introduced JSON Canvas, an open file format for infinite canvas data, aiming to simplify laying out documents and objects with simple relationships.",
      "Users have mixed reactions, with some expressing concerns about the lack of pre-release consultation, while others appreciate Obsidian's effort to collect community feedback.",
      "Discussions highlight the comparisons between JSON Canvas and Markdown, emphasizing the importance of open file formats, data portability, and collaborative development in canvas applications."
    ],
    "points": 678,
    "commentCount": 136,
    "retryCount": 0,
    "time": 1710177742
  },
  {
    "id": 39673087,
    "title": "Webb and Hubble align on Universe's expansion rate",
    "originLink": "https://www.esa.int/Science_Exploration/Space_Science/Webb/Webb_Hubble_confirm_Universe_s_expansion_rate",
    "originBody": "Science & Exploration Webb & Hubble confirm Universe’s expansion rate 11/03/2024 17891 views 16 likes ESA / Science & Exploration / Space Science / Webb Webb measurements shed new light on a decade-long mystery. The rate at which the Universe is expanding, known as the Hubble constant, is one of the fundamental parameters for understanding the evolution and ultimate fate of the cosmos. However, a persistent difference, called the Hubble Tension, is seen between the value of the constant measured with a wide range of independent distance indicators and its value predicted from the afterglow of the Big Bang. The NASA/ESA/CSA James Webb Space Telescope has confirmed that the Hubble Space Telescope’s keen eye was right all along, erasing any lingering doubt about Hubble’s measurements. NGC 5468 – Cepheid host galaxy One of the scientific justifications for building the NASA/ESA Hubble Space Telescope was to use its observing power to provide an exact value for the expansion rate of the Universe. Prior to Hubble’s launch in 1990, observations from ground-based telescopes yielded huge uncertainties. Depending on the values deduced for the expansion rate, the Universe could be anywhere between 10 and 20 billion years old. Over the past 34 years Hubble has shrunk this measurement to an accuracy of less than one percent, splitting the difference with an age value of 13.8 billion years. This has been accomplished by refining the so-called ‘cosmic distance ladder’ by measuring important milepost markers known as Cepheid variable stars. However, the Hubble value does not agree with other measurements that imply that the Universe was expanding faster after the Big Bang. These observations were made by the ESA Planck satellite’s mapping of the cosmic microwave background radiation – a blueprint for how the Universe would evolve structure after it cooled down from the Big Bang. The simple solution to the dilemma would be to say that maybe the Hubble observations are wrong, as a result of some inaccuracy creeping into its measurements of the deep-space yardsticks. Then along came the James Webb Space Telescope, enabling astronomers to crosscheck Hubble’s results. Webb’s infrared views of Cepheids agreed with Hubble’s optical-light data. Webb confirmed that the Hubble telescope’s keen eye was right all along, erasing any lingering doubt about Hubble’s measurements. The bottom line is that the so-called Hubble Tension between what happens in the nearby Universe compared to the early Universe’s expansion remains a nagging puzzle for cosmologists. There may be something woven into the fabric of space that we don’t yet understand. Does resolving this discrepancy require new physics? Or is it a result of measurement errors between the two different methods used to determine the rate of expansion of space? Comparison of Hubble and Webb views of a Cepheid variable star Hubble and Webb have now tag-teamed to produce definitive measurements, furthering the case that something else – not measurement errors – is influencing the expansion rate. “With measurement errors negated, what remains is the real and exciting possibility that we have misunderstood the Universe,” said Adam Riess, a physicist at Johns Hopkins University in Baltimore. Adam holds a Nobel Prize for co-discovering the fact that the Universe’s expansion is accelerating, owing to a mysterious phenomenon now called ‘dark energy’. As a crosscheck, an initial Webb observation in 2023 confirmed that Hubble’s measurements of the expanding Universe were accurate. However, hoping to relieve the Hubble Tension, some scientists speculated that unseen errors in the measurement may grow and become visible as we look deeper into the Universe. In particular, stellar crowding could affect brightness measurements of more distant stars in a systematic way. The SH0ES (Supernova H0 for the Equation of State of Dark Energy) team, led by Adam, obtained additional observations with Webb of objects that are critical cosmic milepost markers, known as Cepheid variable stars, which can now be correlated with the Hubble data. “We’ve now spanned the whole range of what Hubble observed, and we can rule out a measurement error as the cause of the Hubble Tension with very high confidence,” Adam said. The team’s first few Webb observations in 2023 were successful in showing Hubble was on the right track in firmly establishing the fidelity of the first rungs of the so-called cosmic distance ladder. Astronomers use various methods to measure relative distances in the Universe, depending upon the object being observed. Collectively these techniques are known as the cosmic distance ladder – each rung or measurement technique relies upon the previous step for calibration. But some astronomers suggested that, moving outward along the ‘second rung’, the cosmic distance ladder might get shaky if the Cepheid measurements become less accurate with distance. Such inaccuracies could occur because the light of a Cepheid could blend with that of an adjacent star – an effect that could become more pronounced with distance as stars crowd together on the sky and become harder to distinguish from one another. The observational challenge is that past Hubble images of these more distant Cepheid variables look more huddled and overlapping with neighbouring stars at ever greater distances between us and their host galaxies, requiring careful accounting for this effect. Intervening dust further complicates the certainty of the measurements in visible light. Webb slices through the dust and naturally isolates the Cepheids from neighbouring stars because its vision is sharper than Hubble’s at infrared wavelengths. “Combining Webb and Hubble gives us the best of both worlds. We find that the Hubble measurements remain reliable as we climb farther along the cosmic distance ladder,” said Adam. The new Webb observations include five host galaxies of eight Type Ia supernovae containing a total of 1000 Cepheids, and reach out to the farthest galaxy where Cepheids have been well measured – NGC 5468, at a distance of 130 million light-years. “This spans the full range where we made measurements with Hubble. So, we’ve gone to the end of the second rung of the cosmic distance ladder,” said co-author Gagandeep Anand of the Space Telescope Science Institute in Baltimore, which operates the Webb and Hubble Telescopes for NASA. Together, Hubble’s and Webb’s confirmation of the Hubble Tension sets up other observatories to possibly settle the mystery, including NASA’s upcoming Nancy Grace Roman Space Telescope and ESA’s recently launched Euclid mission. At present it’s as though the distance ladder observed by Hubble and Webb has firmly set an anchor point on one shoreline of a river, and the afterglow of the Big Bang observed by Planck from the beginning of the Universe is set firmly on the other side. How the Universe’s expansion was changing in the billions of years between these two endpoints has yet to be directly observed. “We need to find out if we are missing something on how to connect the beginning of the Universe and the present day,” said Adam. These findings were published in the 6 February 2024 issue of The Astrophysical Journal Letters. More information Webb is the largest, most powerful telescope ever launched into space. Under an international collaboration agreement, ESA provided the telescope’s launch service, using the Ariane 5 launch vehicle. Working with partners, ESA was responsible for the development and qualification of Ariane 5 adaptations for the Webb mission and for the procurement of the launch service by Arianespace. ESA also provided the workhorse spectrograph NIRSpec and 50% of the mid-infrared instrument MIRI, which was designed and built by a consortium of nationally funded European Institutes (The MIRI European Consortium) in partnership with JPL and the University of Arizona. Webb is an international partnership between NASA, ESA and the Canadian Space Agency (CSA). Release on esawebb.org Contact: ESA Media relations media@esa.int Like Thank you for liking You have already liked this page, you can only like it once!",
    "commentLink": "https://news.ycombinator.com/item?id=39673087",
    "commentBody": "Webb and Hubble confirm Universe's expansion rate (esa.int)525 points by thunderbong 13 hours agohidepastfavorite202 comments physicles 5 hours agoThe article mentions the cosmic distance ladder, which is one of my favorite things in all of science. How do we know how far away the really far stuff is? It's non-trivial and I find the history fascinating. It all started with knowing the distance from the earth to the sun. Nobody had a clue until Richer and Cassini got within 10% in 1672. Then we nailed it down in 1769 with James Cook's voyage to Tahiti, the primary purpose of which was to observe the transit of Venus from the other side of the world. From there if you know basic geometry, you can observe the nearby stars shift a bit when the earth goes around the sun (parallax), but that only works to about 10k light years. Then, we discovered a couple unbelievably convenient astrophysics hacks: Cepheid variables (Henrietta Swan Leavitt, 1908) and Type 1A supernovae (Subrahmanyan Chandrasekhar, 1935, the namesake of the Chandra X-Ray Observatory). These allowed us to move out a couple more rungs on the ladder. From there, the relationship between redshift and distance becomes significant and that takes us to the edge. https://www.uwa.edu.au/science/-/media/Faculties/Science/Doc... reply luc4sdreyer 34 minutes agoparent> Nobody had a clue In the 3rd century BC, Aristarchus calculated that the Sun was between 18 and 20 times farther away from the Earth than the Moon, and proposed the Heliocentric model as a result. The true value is instead approximately 400 times. But it's incredible given that he didn't have lenses, the value of Pi, and that the Geocentric model was considered correct until 1800 years after his death. https://en.wikipedia.org/wiki/Aristarchus_of_Samos#Distance_... Nice video about the cosmic distance ladder by Terence Tao: https://www.youtube.com/watch?v=7ne0GArfeMs reply risfriend 9 minutes agoparentprevThe distance was also known in 16th century as per Hindu hymn of hanuman chalisa - https://hinduism.stackexchange.com/questions/10370/did-our-a... reply tails4e 1 hour agoparentprevI've often thought about this myself. I'm sure scientists involved are aware of the compounding errors with each step and build that in, but I'd love to see an analysis that breaks that down. When I first saw it I thought the errors due to cephids must be a large component of uncertainty, but really I've no idea how well contained that is. reply rexer 3 hours agoparentprevGreat comment! Maybe you can help me with a book recommendation? I was recently looking for a book which was basically your comment, but more in depth and covered the last couple thousand years. I wanted a to read about the history of astronomy - yknow, what was the state of the art in, say, 1350 or whatever. If you know of anything, I’d be super interested! reply tails4e 1 hour agorootparentTerence Tao did a great lecture on this, https://youtu.be/kY1gfrhNUIg?si=9u9k8of6-jRybwCG reply physicles 2 hours agorootparentprevUnfortunately I don't have any books to recommend. I don't remember where I learned about Cepheid variables and type 1a supernovae (maybe science shows, maybe youtube, ...) but I learned about the transit of Venus stuff on a big Wikipedia rabbit hole one evening. I think the pre-quantum mechanics era for physics and astronomy is super interesting. People figured out so much with such primitive tools, and it's all very accessible and easy to understand. reply severila 36 minutes agorootparentprevYou might be interested in \"Unrewarded\" by \"Ben Moore\" which has an interesting take by telling the history of astronomy through the lives of those that made these discoveries but were not awarded a Nobel Prize. reply zuzun 30 minutes agorootparentprevBig Bang: The Origin of the Universe by Simon Singh. reply hummingn3rd 3 hours agoparentprevIf you speak French or don't mind translating, there is this great video that goes through these techniques in layman's terms https://www.youtube.com/watch?v=FGwmAEMabm4&t=1 reply tzs 12 hours agoprevA little background based on a few articles about this plus my recollection of PBS Space Time videos on this: There are at least two ways to try to figure out the rate of expansion of the universe (which is called the Hubble Constant). • From variations in the cosmic microwave background (CMB) which are the result of certain conditions in the early universe it is possible to figure out what the expansion rate should be now. • From looking at very distant galaxies and noting how far away they are and how fast they are receding from us the expansion rate can be calculated. Theory says that these should give the same expansion rate. When the rates were first found using those two methods they gave different results, but the error bars on both were large enough to overlap. People expected then that further refinement of both methods to decrease the error bars would converge to some common value. That did not happen. Refinement of the CMB measurements got to 67 +/- 0.5, and refinement of the galaxy distance/speed method got to 73 +/- 1. Those do not overlap. This non-overlap between the possible ranges given by the two methods is called the Hubble tension, and it is one of the most irksome problems in cosmology. Possible explanations include: • Some sort of error in how we measure the variations in the CMB. • Some sort of error in the distant galaxy distance or speed measurements, which until the James Webb telescope were almost entirely Hubble telescope measurements. • We're missing something in our understanding of the physics. These new results add a bunch of data from the James Webb telescope, which observes in different wavelengths than Hubble. These results fit with the Hubble measurements. They do not resolve the Hubble tension. What they do is remove most doubt that the distant galaxy results involve some sort of Hubble measurement error. I believe cosmologists are pretty confident of the CMB measurements, and so this will be interpreted as telling us that the Hubble tension is not just a problem with our measurements. There is either physics that we got wrong or physics we need to discover. reply xeonmc 10 hours agoparentCould it be that nearby galaxies are akin to small angle approximations in spacetime trajectory, but as you get really far away (e.g. CMB) the perspective distortion increases hyperbolically? I notice that if you normalize the Hubble data by their Lorentz factor you get back a constant expansion rate: https://www.desmos.com/calculator/llhnja1ocb reply scotty79 59 minutes agorootparentProbably accidental? Redshifts can be way higher than 1 that in your scaling is a magic value. https://en.wikipedia.org/wiki/List_of_the_most_distant_astro... reply rhelz 9 hours agorootparentprevFor real? That is a very interesting observation. reply throwawaymaths 10 hours agoparentprevThere's a 'fringe' theory that the CMB is not the echo of the big bang but rather the redshifted black body radiation of intergalactic dust (apparently the numbers are about right). Although the primary proponent of this theory is trying to justify some sort of cyclic universe model with it, if my understanding is correct it would still be compatible with a 'standard' big bang that has the \"longer\" (aka galactic expansion) timeframe. And in any case physics still needs to explain what happened to that blackbody radiation. reply theoreticalmal 7 hours agorootparentI can’t believe it. First they take Pluto from us, next they’re going to tell me the static on my tv set isn’t the remnants of the Big Bang, but just hot dust? My childhood is crumbling! reply orlp 2 hours agorootparentYour comment made me realize how some younger people have no idea of, or at least no first-hand experience of static on TV and radio since it is all digital nowadays. reply brabel 15 minutes agorootparentI have a super modern flatscreen. Sometimes, when it can't connect to the laptop it shows what I believe is \"simulated\" static noise! I love that :D reply LilBytes 1 hour agorootparentprevMy great grandparents had a black and white TV when I was a toddler, and I'm 36! The amount of change millennials and the generations before and after us have seen is boggling. I was out in country Victoria, Australia a few months back and the internet was TERRIBLE. I'm talking, JPG's loading line by line terrible. And this was on 'alleged' 4G. I felt pretty nostalgic for all of 2-3 minutes before I started pulling my hair out. I feel nostalgic about it again now. reply denton-scratch 32 minutes agorootparentprevSee also https://en.wikipedia.org/wiki/White_Dot reply taneq 5 hours agorootparentprevThe hot dust is also remnants of the Big Bang, if that helps. Of course, so are we. :) reply sebzim4500 10 hours agorootparentprevHow could that be true? Wouldn't that smear the spectrum so that it no longer so matches the black body radiation of a single object of a set temperature? reply throwawaymaths 9 hours agorootparentNo. According to the standard big bang theory the CMB should look like black body radiation. As for smearing, IIRC black body curve has the central limit property reply lagrange77 9 hours agoparentprevInteresting, thanks! Out of curiosity: Can we actually differentiate between the expansion of space itself and the drifting of objects within it in the same direction? reply mr_mitm 14 minutes agorootparentYes, the \"drifting\" component you speak of is called the peculiar velocity. It's responsible for the \"finger of god\" effect if we don't account for it (galaxies appear to be aligned along the line of sight of the observer) and becomes negligible at large enough distances. I guess technically it's hard to differentiate because it's usually random, but it enters the error analysis and is accounted for. reply dmd 8 hours agorootparentprevYou would have to come up with an explanation for such drifting to be occurring across the entire visible universe in precisely the same way, across billions and billions of light years. reply xeonmc 8 hours agorootparentJust a fringe speculation without any physical basis: maybe the Big Bang emits a 4-dimensional spherical shell of matter at c? With the direction in which we are traveling at c called “time”, and the other three which we're not moving through called “space”; diverging rays having spatial velocity relative to each other equal to tan(alpha) i.e. flat projection; and at any given point on the 4-sphere's 3D surface everything appear to be diverging in 3D because the emission is spherically symmetric; and trajectories sufficiently paraxial to us fits the small angle approximation but those diverging widely has much more apparent perspective distortion, with an error ratio of tan(alpha)/sin(alpha) = sec(alpha) = 1/Lorentz_Factor? reply jabits 1 hour agorootparentThis is a really interesting thought. I will be noodling on this tonight. Thanks reply dmd 8 hours agorootparentprev> without any physical basis Exactly - the \"universe is expanding\" explanation has the benefit of having tons of physical evidence for it, whereas your explanation has the drawback of being meaningless not-even-wrong word salad. reply root_axis 6 hours agorootparentI think if you're going to dismiss the comment in such a flippant and condescending manner you should at least explain what's wrong with it. reply dvsfish 6 hours agorootparentprevWhile its probably totally wrong, I think your tone and general attitude to people throwing around imaginative and creative ideas is extremely unproductive and frankly rude. If you want to stifle creative discussions, this is exactly how you'd do it. I found the post to be very stimulating regardless of its validity. And it's not like they're trying to spread it as gospel. There was a lot of disclaimer. reply arandomusername 6 hours agorootparentprevI think we observed some of the galaxies moving away from us at rates faster than speed of light, multiple times faster, which wouldn't work with objects drifting away if we hold by that speed of light is max speed matter can move at. reply Kerbonut 6 hours agoparentprevCould it be the rate of expansion is not constant? reply michaelsbradley 11 hours agoparentprev\"Humason assembled spectra of the nebulae and I attempted to estimate distances.\" So wrote Hubble of his colleague Milton Humason in 1935 by which time spectra had been obtained for over 150 nebulae. Hubble was a stern warner of using the Doppler effect for galaxies and argued against the recessional velocity interpretation of redshift, convincing Robert Millikan, 1923 recipient of the Nobel Prize for Physics and director of physics at the California Insitute of Technology, that the redshift interpretation as an expanison of the universe was probably wrong, the year before both of their deaths in 1953. Hubble ended his book Observational Approach to Cosmology[+] with the statement:...\"if the recession factor is dropped, if redshifts are not primarily velocity-shifts, the picure is simple and plausible. There is no evidence of expansion and no restriction of time-scale, no trace of spatial curvature, and no limitation of spatial dimensions. Moreover, there is no problem of internebular material. The observable region is thoroughly homogeneous; it is too small a sample to indicate the nature of the universe at large. The univers might even be an expanding model, provide the rate of expansion, which pure theory does not specify, in inappreciable. For that matter, the universe might even be contracting.\" [+] https://ned.ipac.caltech.edu/level5/Sept04/Hubble/paper.pdf source: https://plasmauniverse.info/people/contributors.html reply xwolfi 10 hours agoparentprevhttps://www.sciencedaily.com/releases/2023/12/231201123626.h... reply rf15 11 hours agoparentprevThis, together with the endless philosophising around dark energy, dark matter and whatnot paints a pretty strong arrow towards our models having some flaws when it comes to their large-scale application. I hope to live long enough to see where we made our mistake and get a better model. reply cowgoesmoo 11 hours agorootparentWe know for sure that there are issues with our current theories. Our two best theories, general relativity and quantum mechanics, are not compatible with each other. https://en.wikipedia.org/wiki/General_relativity#Relationshi... reply dclowd9901 9 hours agorootparentInteresting to think we only got the “middle” size physics right, but we can’t reconcile it with micro or universal physics. reply ethbr1 8 hours agorootparentI think about it as getting practical / low-tech observable physics right, then expanding out from there.planetary have very important applications! But I'd argue not nearly as many as \"Here are all the formula that govern a falling apple.\" reply superjan 3 hours agorootparentprevAnd don’t forget cosmic inflation. reply pishpash 11 hours agorootparentprevAlmost every model we build of more trivial things based on observation always turns out to be not really right. I cannot imagine why one of the universe that has had multiple version updates in the last 100 years to not also be grossly mistaken. I also don't expect the full model to be simple or beautiful. We may be thinking wishfully based on massive extrapolation and cutting corners to suit our narrow view into the world. reply qup 2 hours agorootparentIt's probably spaghetti code reply CuriouslyC 10 hours agorootparentprevI think the truth is likely that we live in a very complex universe that can be approximated by these sweeping laws in general, but is very \"messy\" close up. Consider that while the universe roughly obeys probabilities, it also has will and intent (to a degree we can debate in another conversation). reply kaashif 7 hours agorootparentWhat do you mean by will and intent? I guess if humans have will and are part of the universe, then the universe has it too, but I don't think that's what you meant. reply hinkley 10 hours agorootparentprevI still have $20 down on \"We are living in a universe inside a black hole.\" reply hughesjj 10 hours agorootparentI mean effectively we are regardless. The cosmological event horizon is a thing. Also, I forget off the top of my head, but there's some oddities depending on your chosen frame of reference when looking @ hawking radiation (skip to part about relativity of the vacuum) that I think could apply to the cosmic event horizon as well https://youtu.be/isezfMo8kWQ?si=5m_L6JtZ7p7Ls6xH reply JohnMakin 12 hours agoprevThis sort of brushes on it but for a long time there was hope at resolving the Hubble Tension by saying that Hubble telescope’s measurements were incorrect, because that would be the most simple explanation. This was not the case, so if anything, the mystery deepens. I don’t know for certain but I believe Hubble’s estimation has been widely accepted for a while though, because we’ve been using the 13.8 billion cosmological age estimate ever since I started brushing up my layman's understanding of the subject. reply perihelions 12 hours agoparentHere's a graph of the contradictory measurements (JWST data not yet included), https://en.wikipedia.org/wiki/Hubble%27s_law#Determining_the... (caption: \"Value of the Hubble constant in (km/s)/Mpc, including measurement uncertainty, for recent surveys[54]\") reply orra 12 hours agoparentprev> we’ve been using the 13.8 billion cosmological age estimate ever since I started brushing up my layman's understanding of the subject. I remembered the age of the universe as as 13.7 billion years, but I wasn't sure why that was. Well, the initial WMAP results in 2003 supported an age of 13.7 billion years. Later results nudged this upwards to 13.8 billion years. Of course, all the results have error bars. reply explaininjs 12 hours agorootparent> Of course, all the results have error bars. Not to mention the underlying philosophical assumptions, such that the \"rate of time\" has been constant across all of... time. Aka: How do we know a \"year\" 13 Billion \"years\" ago bears any resemblance to one now? What would it mean for it not to? reply Veserv 10 hours agorootparentWell for one, if the laws of physics are not time invariant (i.e. the laws of physics are not the same at all points in time) then energy can be created or destroyed [1]. So that would be quite a shocker. [1] https://en.m.wikipedia.org/wiki/Noether%27s_theorem reply hnfong 4 hours agorootparentSaying \"X is true because otherwise, it would be quite a shocker\" isn't really a proof. It's almost an appeal to common sense. (which, admittedly, is often the best argument we have). IMHO there's often a bit of over-confidence among scientists about the universe being 13 billion years old and what happened during the Big Bang, if it just relies on such a common sense argument. I know it's unscientific to suggest maybe laws of physics are not time invariant across such scales, because until we have a time machine we can't test this theory, but then flipping the argument (that laws of physics are definitely time invariant) is also technically unscientific -- we're basically assuming this without strong evidence. This goes back to the old debate on the problem of induction in science, (see David Hume, Karl Popper, etc.) and I think it isn't emphasized enough in modern discussions that, perhaps, there's a small chance that these foundational concepts in physics could be invalid. reply Veserv 3 hours agorootparentI did not present proof. They asked what it would mean and a consequence of physical laws not being time invariant would be that energy does not need to be conserved. To the extent that relates to epistemology that would be more like anti-proof I guess? It does not prove that the laws are time invariant, rather it raises the bar to demonstrate that the laws are not time invariant because that means energy need not be conserved. You can not get one without the other without attacking even deeper fundamentals of modern scientific models. So you must either demonstrate the linked claims, which are pretty foundational themselves, or you must overturn basically everything; both of which demand very robust evidence. reply explaininjs 8 hours agorootparentprevDo you think the current amount of energy in the universe is 0? If not, how was it created? reply baq 1 hour agorootparentThat's a completely different question, to which the answer is 'we don't know'. reply itishappy 9 hours agorootparentprevDoesn't dark energy do exactly that? 70% of the energy of the universe doesn't seem to want to play by our rules! reply digging 8 hours agorootparentYes, but my understanding is that dark energy doesn't play by the same rules, it's an exception. I certainly can't explain why but also it may not be known exactly why, given dark energy is an unexplained phenomenon. reply dartos 11 hours agorootparentprevI’d assume that we have some notion of how the laws of physics have changed, if at all, since the Big Bang. We measure time in vibrations of a cesium isotope IIRC reply yongjik 11 hours agorootparentFun fact: the Oklo reactor, a naturally occurring nuclear reactor that was active more than a billion years ago, was used to test if physical constants were the same in ancient times. https://en.wikipedia.org/wiki/Natural_nuclear_fission_reacto... reply drjasonharrison 10 hours agorootparentAnd we can split hairs and conclude that for \"the last two billion years, on this planet, in this galaxy, the physics affecting nuclear decay have not changed\" It's great to know that say dating using carbon-14 decay is still useful over those time ranges on planet earth (I don't know if that is something we care about given that fossils don't tend to contain much carbon, but coal and oil deposits are around 400 million years old). I don't want to imply that this is too small of a sample size, but I will imply that nuclear decay, and the movement of galaxies across the universe might be unrelated. Don't know. Not sure how we'd measure that. Supernova observations would tell us about nuclear fusion and it's limits. Does it tell us about nuclear fission? I don't know. reply db48x 11 hours agorootparentprevNo, we measure the frequency of vibrations of _light_, not of a type of atom. Specifically, it is light emitted by cesium atoms that are transitioning from one specific energy state to another specific energy state. Although this is arbitrary, it is highly reproducible and would give precisely the same measured lengths of time at any point since the big bang. reply elashri 11 hours agorootparentThat assumes that fundamental laws of physics did not change (will not change). This is what we believe and have no evidence otherwise. This is important since we rely on measuring atomic transitions of cesium atoms which itself were formed/forming billions of years after the big bang itself. The laws of physics invariance under time is a core to our understanding. It would be very disrupting if we found otherwise. reply not2b 10 hours agorootparentRight, but most deviations one can think of (like, changes over time to physical \"constants\") would have an observable effect, so ancient galaxies would look much more different from modern galaxies and spectra wouldn't look the same other than a red shift, which moves all the lines in a uniform way. reply fooker 11 hours agorootparentprevPhilosophical indeed, as it's impossible to define the idea of a rate of time, when the idea of rate is defined in terms on time itself. reply explaininjs 4 hours agorootparentYes, an external reference clock is needed. If a computer chugs along doing { counter++; } at 1 clock cycle per clock cycle for 13.7 billion clock cycles, it will think 13.7 billion clock cycles have passed when counter is 13.7 billion. On the other hand, if a computer chugs along at one clock cycle per clock cycle for 1 clock cycle, and reads &counter and sees 13.7 billion, it will think 13.7 billion clock cycles have passed. Either way it's perfectly capable of introspecting it's source code and logically stepping back until the memory location was 0 to see how many clock cycles would have been required to reach it's current state, but that sort of reasoning is completely devoid of meaning without both perfect knowledge of what the true start state was, and a guarantee that no external influences have occurred. Here in reality, we know neither the our start state nor our isolation level, but the hubris of many is too string to not at least try finding some logical step-back functions and iterating them until they don't know how to go any further, then proudly proclaiming that \"The Start\". (after all - how could it not be The Start, look, I can iterate the inverse of my step-backward function from then to now and it matches! QED!) reply jMyles 4 hours agorootparentI agree with you 100% on your assessment of what can be known, but I think I disagree nearly as strongly on your assessment of the humanity that springs from attempting to know it. To exist in the state you describe - with neither start state nor isolation level measurable in any tamper-proof way - and to yet still dedicate one's life to observing and pondering the complexity of the resulting cosmos is, to my eye, laudable and beautiful. Where you see hubris, I see humility. Everyone who attempts to expand the corpus of human understanding of cosmology knows that the endgame is somewhere short of perfection. And yet they are inspired to carry on. It seems to me that matters of state, economics, medicine, technology, and many other fields will benefit from a similar disposition. reply speak_plainly 11 hours agorootparentprevTime in this context is just an arbitrary measurement. Like extrapolating the calendar back to the Big Bang which is when space/time began, another way to think about time. reply dwattttt 11 hours agorootparentprevHow do we know ? Observations we can make plus models that relate those to the things we can't observe directly. reply vlovich123 10 hours agorootparentUnfortunately we can’t observe fundamental things like “what are the rules of physics and time at the beginning of the universe?”. We look for clues and make large assumptions, but given that the universe experienced a 10^78 factor expansion during the Big Bang, assuming that actually happened, then why would it make sense to assume that the rules of the universe today are the rules for the very early beginning of the universe? A strand of DNA would become 10 light years. Given that relativity redefined our understanding of basic physics but only applies as we approach the speed of light, it would stand to reason that the rules of physics would be different from our current models based on today’s observations when the matter of the universe is packed much more tightly together. reply itishappy 9 hours agorootparentEntirely reasonable assumptions! Our models match surprisingly well though... The CMB has a blackbody spectrum that aligns with predictions, we see galaxies more or less when and where we'd expect them, stellar populations look like what we'd expect for a universe made of hot hydrogen, and more! It's not quite perfect, but modern physics explains stuff really quite well even billions of years ago! reply vlovich123 6 hours agorootparentThe inflationary epoch where it expanded by 10^78 in volume happened in the first 10^-32 seconds. The furthest galaxy we can see (fairly poorly) is 300M years after the Big Bang. It's likely if time or the rules were different, 300M years was enough for things to mostly die down to steady state. And as you say, they match more or less but those errors could easily hide remnants of when things were different. Of course, these are all numbers that assume the Big Bang theory is correct which is difficult to impossible to falsify since we can't possibly observe or test anything from that long ago. We'll have to wait to see if refinements to our model that clear up contradictions change what we think about the beginning of the universe and other boundary conditions. reply explaininjs 8 hours agorootparentprevModern physics is guided by those observations, they can't be then used as an argument for its veracity. Let's say I walk into a room and observe someone writing a tally mark on a chalk board once every second. I count 4x10^17 tally marks. I might assume that 4x10^17 seconds ago that same person entered the room and started tallying. I might even observe for the next 4x10^17 seconds they continue to tally. Heck I might even see a recorder going that when I play back at what I assume to be 1x speed, has chalk scratches at regular intervals for 4x10^17 seconds. I still don't have any actual evidence that they started those 8x10^17 seconds ago. reply wavefunction 10 hours agorootparentprevI have had the same thought that primordial reality had the same timestream as us but it was much longer, like the first \"year\" of reality was far longer than one year today, just the thinking \"could time have been shorter or longer, why not\" reply denton-scratch 13 minutes agorootparent> the first \"year\" of reality was far longer than one year today This talk about the \"rate of time\" doesn't make any sense to me. A second takes one second, and always has. Isn't this like asking whether the length of a metre might have changed over time? Or the mass of a kilogramme? It looks to me like a category error. reply arcastroe 11 hours agoprevI've asked this question before, but I don't think I received a good answer, so figured I'd try asking again. How do we know that galaxies are accelerating away from us and not moving at constant speed? People often point to the observation that the further away a galaxy is, the faster it appears to be moving away from us, implying acceleration. However, wouldn't we expect to see the same observation even without any acceleration? Imagine there are some objects in space all moving in random directions and speeds, relative to Earth. After long enough time, all objects will appear to be moving away from Earth, even if they were moving towards it initially. And after long enough time, the objects that move fastest should be farthest away, by the simple definition of speed! In short, even if galaxies weren't accelerating, we would still see that the further away a galaxy is, the faster it recedes. reply narag 9 hours agoparentHow do we know that galaxies are accelerating away from us and not moving at constant speed? There's a more basic question: How do we know the galaxies are moving? It seems (I haven't seen any other response, like... ever) that we have one and only one way to measure the speed of galaxies: the red shift. It's impossible to triangulate those huge distances and the time scale would also be a barrier, so no way of confirming the red shift calculations with a different method. That means that if the red shift was caused by any other effect, say the light \"degrading\" after millions of years of travelling the void, all the calculations would be invalid. I've asked about this many times and the answers are in the line of \"we don't know any other reason for the light to red shift\" and \"the current theoretical frame is consistent\", even if there isn't any other measure to be consistent with. There was a prediction (expansion is related to Big Bang) that the far away galaxies, being younger, would have a different composition. This prediction seems to be failing, but advances in instrumental could give us a more precise answer in the future. reply k7sune 8 hours agorootparent“Degrading” sounds very intuitive to me. Can the frequency of the light waves simply slow down over a very long distance/time? Or maybe the speed of light simply slows down over an unimaginably long distance? We don’t have any model to describe such behavior, but everyday objects around us all slow down one way or another, what makes light so different? reply Sprocklem 5 hours agorootparentIIRC, this was one of the explanations proposed when the existence of a red shift was first noted: that the light is somehow slowly losing its energy over very long distances, becoming “redder” as it did so. It ultimately lost out to the dark energy / space-time expansion theory, although I do not recall why. Presumably there was some observation that precluded “degrading” light from being the sole explanation. reply semi-extrinsic 3 hours agorootparentThere are several challenges for \"tired light\", or indeed any theory that's an alternative to expansion of the universe. The theory has to explain why the light gets redshifted, but does not get blurred, and the spectral lines do not get broadened. This severely restricts the type of interactions possible. Also the theory has to explain the consistency between redshifts within our own galaxy, to that of far-away galaxies. reply pigpang 1 hour agorootparentMy theory is that redshifting caused by gravitational background noise. I did calculation somewhere on HN or Youtube few months ago and numbers are of same magnitude. reply hnfong 4 hours agorootparentprevhttps://en.wikipedia.org/wiki/Tired_light reply ganzuul 2 hours agorootparentprev> \"we don't know any other reason for the light to red shift\" https://en.wikipedia.org/wiki/G%C3%B6del_metric There are versions of this which do provide another reason for red shift. Personally I keep this in mind as a means to free my thinking from a single narrative. reply garrettgarcia 5 hours agorootparentprevThe American astronomer, Halton Arp, had a theory he called \"intrinsic redshift\". My limited understanding is that he saw evidence of \"very close\" and \"very far away\" structures that are connected to each other in space, which makes no sense. He theorized that redshift may also be indicative of the age of a galaxy, rather than only indicating velocity. The interpretation of redshift as velocity is also the primary reason cosmologists think the universe is expanding. reply the-mitr 3 hours agorootparentHis book Seeing Red which raises several interesting and troublesome questions for the standard big bang model is worth a read. reply __turbobrew__ 5 hours agorootparentprevDegrading light sounds like an interesting idea to me, you could call it the “cosmological damper” if you will. Thought experiment: imagine you have light particles in a perfectly circular orbit around a black hole. Does the light ever fall into the black hole or does it orbit for eternity? reply oneshtein 4 hours agorootparentThere a whole set of theories, called \"Tired light\" theories, which tries to explain Cosmological Red Shift by degradation of photons with time. Buy they require whole set of different cosmological principles: a medium for light propagation is required to dump lost energy into, no Big Bang. But even with a tired light theory, galaxies are accelerating toward attractors, see https://arxiv.org/pdf/1702.02483v1.pdf https://www.youtube.com/watch?v=NpV0GQo3P0c reply javajosh 6 hours agorootparentprev>It's impossible to triangulate those huge distances Galaxies are BIG. Andromeda is faint, but the same angular size as the moon. It's 2.5Mly away, but it's also 150kly across. Over a long enough time line you could do triangulation on it. In fact it's moving toward our galaxy, but very, very slowly compared with its diameter at 110kps. But yeah, in theory you could do triangulation on it over a very long period of time. reply at_a_remove 5 hours agorootparentprevYou're not going to get a simple answer because the answer is quite complex. Astronomy is the paleontology of photons. You should take an astronomy course if you really want to know, but essentially a \"ladder\" was built of distances, starting with the very near and slowly building outward using various techniques and discoveries of physics as they became available. This is called the cosmic distance ladder. You start with stellar parallax, then after that you go farther with \"standard candles\" (particular types of variable stars). But then you have to get even further out, where you can no longer see an individual star, and then you rely on specific breeds of supernovae. Only then do you get to redshift, and compounding tons of data from step three seems to verify the redshift estimates. By the time you get to the Hubble constant, it was a huge rift between two communities over what was still a factor of two difference. It's quite fascinating, but I can't really dump out an entire book into a comment. reply pdonis 9 hours agoparentprev> How do we know that galaxies are accelerating away from us and not moving at constant speed? More precisely, we see that galaxies started accelerating away from us a few billion years ago; before that they were decelerating (moving away from us but with the \"speed\" decreasing instead of increasing). > People often point to the observation that the further away a galaxy is, the faster it appears to be moving away from us, implying acceleration. That observation tells us that the universe is expanding, but by itself it does not tell us whether the expansion is accelerating or decelerating or neither. So you are correct that that observation alone is not sufficient to show that the expansion is accelerating. What we look at to see how the expansion rate changes with time is a comparison of three pieces of observed data, galaxy by galaxy: redshift, brightness, and angular size. The relationship between these three quantities is what cosmologists use to construct a model of the expansion history of the universe, which in turn tells us things like what I said above, that the expansion has been accelerating for the last few billion years but before that it was decelerating. reply AlecSchueler 9 hours agorootparentBy few billion are you talking like 3 billion? Why the change? reply pdonis 8 hours agorootparentMeaning, why did the expansion change from decelerating to accelerating a few billion years ago? Because that was when the density of matter, which had dominated the dynamics until then, became smaller than the density of dark energy, which has dominated the dynamics since then. The dark energy density doe not change with time, but the density of matter decreases as the universe expands. reply 1d22a 6 hours agorootparentHow is is that dark energy density does not change with time? Surely the total amount of dark energy has to be constant (energy can't be created or destroyed, and all that), and then as the universe expands, that's then the same amount of energy over a larger volume, right? reply EnterpriseTell 8 hours agoparentprev>In short, even if galaxies weren't accelerating -Galaxies are not accelerating, space is expanding. > Imagine there are some objects in space all moving in random directions and speeds, relative to Earth... -No, In your scenario then end result would be a most static average distance between all objects in the universe. As an infinite number of objects come from infinite distances, there would ALWAYS be objects in the neighborhood. I think what you're imagining is a bunch of objects in a box, give them random vector and then remove the box. If they maintain course, all will eventually move outside the original box boundaries, and away from each other. (not the way the universe is). They know space is expanding. The primary mechanism we know this is the speed with with we measure an object (moving away) is redshifted. Objects at the same distance from Earth, but opposite regions of space are moving at the ~SAME measured velocity. There simply is no existing theory which can account for what we are seeing besides space expanding. I'm not big on thinking we understand it all, but in this particular measurement, there is basically zero doubt. Space is expanding, which has the affect of accelerating all objects in the universe away from you, with an acceleration relative to the distance. The more space between you, the more opportunity to expand. reply munksbeer 4 minutes agorootparentWhat does \"space is expanding\" mean? That the distance between objects is increasing? How can you tell the difference between \"space expanding\" and \"objects moving in a non-expanding space\"? Is there any way to tell the difference or is it just that all objects are moving away at the exact speeds that satisfies the \"space expanding\" explanation and nothing else? But then, I'm back to what does \"space is expanding\" mean? What is doing the expansion? reply ijustlovemath 9 hours agoparentprevOne thing that is not often mentioned is that this effect only applies outside the local supergroup; within the supergroup gravity overrides the expansion of spacetime and holds us together (for now!) reply digging 10 hours agoparentprevIt's because the movement is ascribed to the expansion of space itself, not the individual galaxies. We don't have any reason to believe galaxies are moving in random directions at random speeds (not at scales that explain the redshifts we call the expansion of the universe). In your explanation, I think we'd expect to see some very distant, very slow-moving galaxies moving toward us. And there may be some very fast-moving galaxies close to us that just started really far away. Objects would be entering our local universe from outside it, and that simply doesn't happen. reply arcastroe 10 hours agorootparent> In your explanation, I think we'd expect to see some very distant, very slow-moving galaxies moving toward us Thank you, but I suppose I'm not really questioning the big bang piece. My question was mostly in regards to the continued acceleration piece. Feel free to disregard the \"in random directions\" part of my original post. I'm picturing more of an explosion in empty space. A firework or granade of sorts. Any individual dust/shard of the explosion still sees all other objects moving away from it and the rest of my question stands. But I suppose this would imply a \"center\" to the explosion, which I've also heard is not the case. Theres a few other comments offering more clarity to the acceleration piece. Thank you everyone! reply pdonis 9 hours agorootparent> I'm picturing more of an explosion in empty space. No, that's not what the big bang is. > this would imply a \"center\" to the explosion, which I've also heard is not the case That's correct. The big bang does not work like anything ordinary that you are used to imagining. The math is straightforward and unambiguous, but there is no good ordinary language description that corresponds to the math. reply lp4vn 6 hours agorootparentAs I understand it, before the big bang the whole observable universe was contained in a small sphere and then it started to expand metrically. Is this interpretation correct? Another thing: suppose I point a laser beam to the space and by chance this laser beam never finds any kind of matter in its way, where is this laser going to? To an infinite void? Is it correct to say that stars radiate energy to the infinite then? reply pigpang 1 hour agorootparentIt's just an interpretation. Your interpretation is similar to the Big Bang model of visible Universe expansion. If you can convince us that your model is better than other models, then we will use your model, but nobody can prove than a model is correct, unless we will find a hidden recorder somewhere which was turned on for few dozens of billion years. Photon will hit something, or will travel until it will be redshifted to obvilion, or will travel until end of the medium (photon is a wave, so it waves something). reply alexb23 7 hours agorootparentprevI find helpful this analogy of the space-time (4-dimension) expansion from the big bang: the surface (2D) of an expanding bubble. YMMV. reply snowwrestler 10 hours agoparentprev> After long enough time, all objects will appear to be moving away from Earth, even if they were moving towards it initially. And after long enough time, the objects that move fastest should be farthest away, by the simple definition of speed! Sure, but to argue that this explains what we observe today, you would need to show that as of today it has been “long enough,” which is its own can of worms to open. You might say “obviously it has been long enough for full sorting, because we observe a fully sorted data set of speed correlated with distance.” But that would be begging the question. reply runeb 11 hours agoparentprevTheir light is more red shifted the farther away they are. I'm no expert on this, but I believe in a constant-speed scenario they would have equal red shift no matter the distance reply px43 10 hours agorootparentAs I understand it, if the expansion was constant, farther away stuff would still be more red shifted. Stuff twice as far away appears to be moving twice as fast. It helps me to imagine the expansion of a metal cookie sheet, where the two edges are moving apart faster relative to each other compared to the speed that they're moving away from the center. The surprising bit is that the far away stuff seems to be even more red shifted than that, so we're not just expanding, but the rate of expansion seems to be accelerating. reply willis936 10 hours agorootparentprevThe assumption made here is that relative velocity is the only method that would redshift light. Gravitational redshift is a thing and our model of gravity is incomplete. reply pdonis 8 hours agorootparent> The assumption made here is that relative velocity is the only method that would redshift light. Not in our actual model of the universe, no. The redshift of light is determined by the spacetime geometry and the worldlines of the emitter and receiver. That is a general formula that works in any spacetime. > Gravitational redshift is a thing Not for the universe as a whole, no. Gravitational redshift is only meaningful in certain kinds of spacetimes, namely stationary spacetimes (which, roughly speaking, describe objects that either don't change with time at all, or which are periodic, like a rotating planet or star). The spacetime that describes our universe as a whole is not stationary and there is no meaningful concept of gravitational redshift. > our model of gravity is incomplete In the sense that we do not have a quantum theory of gravity, yes. But that does not affect anything under discussion here. Our current theory of gravity, GR, works fine for treating the expansion of the universe and whether or not it is accelerating. reply willis936 8 hours agorootparent>Our current theory of gravity, GR, works fine for treating the expansion of the universe and whether or not it is accelerating. Then why are there phantoms in the data that need dark matter and dark energy to make the supposed working model fit them? reply pdonis 8 hours agorootparentI'm not sure what you mean by \"phantoms in the data\". The distribution of stress-energy is a free parameter in GR; it has to be inferred from observations. The terms \"dark matter\" and \"dark energy\" are just names for, respectively, \"stress-energy that acts like the matter we can see, but we can't see it\", and \"stress-energy that acts like a cosmological constant\". Neither of those things poses any problem for GR, since both types of stress-energy are allowed for in the theory. \"Dark matter\" poses a problem for particle physicists, who have so far been unable to find any fundamental particles that would produce the observed properties. \"Dark energy\" only poses a problem if for some reason you don't like having a nonzero cosmological constant. reply willis936 8 hours agorootparent>\"Dark matter\" poses a problem for particle physicists, who have so far been unable to find any fundamental particles that would produce the observed properties. It's clear to me why we haven't made any new discoveries in cosmology in the past two decades. It's this exact attitude of \"the model is the truth\". All models are wrong. The data can help you improve it, but you have to at least want to improve it. reply jMyles 4 hours agorootparent> we haven't made any new discoveries in cosmology in the past two decades Is that true? At least for my hobbyist understanding of the progress of cosmology, quite a lot seems to have happened in the past two decades. Confirmation of the Higgs Boson at CERN [0] kept me up all night to watch the press conference; I found it extremely exciting. (Maybe you count this strictly as observational particle physics and not cosmology, but I might appeal for it to be allowed in the context of your critique). And what of TFA? Isn't what we're reading now a new discovery in cosmology? What about the rush of exoplanet discoveries? What about the dramatically different galactic properties now observed in increasingly strange corners of the observable universe (including some which perhaps give insight into some of the properties of \"dark matter\" or whatever it ends up being)? 0: https://home.web.cern.ch/news/news/physics/new-results-indic... reply oneshtein 3 hours agorootparentYes, it's true. Mainstream science refuses to accept anything radically new because of huge baggage. Nobody wants to look stupid, then relearn, recalculate, republish, reteach everything, or lose their tenures, grants, etc. It's why science advances in small incremental steps. AFAIK, there is a team of scientists secretly working on radically new set of theories (I got contact but cannot join because of war). reply sophacles 6 hours agorootparentprevWhat do you think they do now? How do you propose they do it differently? What evidence do you have that what they are doing now doesn't work, and does the all the evidence of how they work support your hypothesis? Be detailed, because your comment just has some motivational speaker nonsense but no depth. For example, in the last 20 years cosmology has: + Refined its model of stellar formation based on observational data of the number of planets found observationally, and used this to validate and invalidate several model adjustments. + Observed galaxies that appear not to have dark matter, and by their existence and behavior validate some theories of dark matter, and validated others, which predict such galactic behavior. (e.g. some theories attempting to update gravity). + Run simulations of stellar and glactic formation that predicted structures in the universe that were later observed. Everywhere they look they are finding things the models don't explain well, and refining the models - that is literally using the data to improve the models. If you think you can come up with something better, then do it - all you gotta do is make up some mumbo jumbo and write down any old equation. It probably should: - provide the same results as were observed when the plugging in the experimental parameters of existing experiments. - explain \"wierd stuff\" in the data that existing models couldn't. - predict future observations of the known phenomena with the same or better accuracy as the old model - predict currently unobserved and unpredicted phenomena Go ahead and take a stab real quick - I'm sure you can do it. I mean Gallieo did it, so did Newton and Einstein. Next up is willis936. reply hnfong 4 hours agorootparentPeople can be right about problems with a process or way of thinking without being the next Einstein. There's no need to get personal. reply colechristensen 10 hours agoparentprevIf your idea was the case there would always be new things from very far away heading towards us, this is not the case. If the universe is flat and infinite there would be no end in supply of new galaxies with all velocities and you would always have the same mix as the initial mix of velocities. That’s not what we see. reply NemoNobody 10 hours agorootparentI'm so glad \"the infinite universe\" as an idea is finally falling off. It works great in the Hitchhikers Guide, I love the floopy mattresses and planets that grow screwdrivers but nothing real is infinite. I didn't even realize how many people held that belief til that article about how the universe isn't as big as we thought reply colechristensen 9 hours agorootparentHuh? Every evidence points to a flat infinite universe. Nothing but speculation points to anything otherwise. reply Steuard 11 hours agoparentprevYou're right: the \"more distant galaxies are moving away faster\" point is just Hubble's original observation of an expanding universe. It's not an argument for cosmic acceleration. (If you see people making that claim, they're probably either speaking carelessly or not experts themselves.) The conclusion that the expansion is accelerating was a quite recent result: 1990s, I believe. It's based on careful measurements of supernova explosions of a type with computable intrinsic brightness in increasingly distant galaxies, and the exact pattern seen in their apparent redshifts vs. apparent brightness. It was a shocking discovery when it came out, with two separate teams announcing the result pretty much neck and neck. There's also independent and compatible evidence for acceleration from the exact pattern of variations in the temperature of the cosmic microwave background seen at different points in the sky. reply bitcurious 7 hours agorootparentCould we use gravity lensing to directly measure the difference in redshift of the same star at different points in time? reply AnimalMuppet 10 hours agoparentprevI'm not an expert, but I think it's like this: If the universe were expanding uniformly, we would see galaxies moving away from us. The further away they are, the faster they would move. Distance and velocity would have a linear relationship, with the Hubble Constant as the scaling factor. But what we actually see is that, if we measure precisely enough, galaxies further away are moving faster than that. The conclusion is that the expansion is accelerating. reply pdonis 8 hours agorootparent> galaxies further away are moving faster than that No, you have it backwards. Accelerating expansion means, roughly speaking, that we see galaxies further away moving away slower than a \"uniform\" expansion would predict. Remember that we are seeing galaxies further away as they were a longer period of time ago--so \"accelerating expansion\" means the universe was expanding slower then, when the light was emitted, than it is now. Actually, though, we don't observe the distance to a galaxy directly. We infer it from other observations. The actual observed quantities are redshift, brightness, and angular size, and the relationship between those three observed quantities is what tells us the expansion history of the universe. reply pfannkuchen 41 minutes agoprevI don’t really understand how we can make conclusions about the entire universe when literally all of the data is collecting from one point in space. Couldn’t there be local effects that obfuscate global behavior? Is there an implicit caveat asterisk on all such statements that is like “as far as we can possibly tell from the data we have” and the reality is we really, really don’t know for sure? reply luc4sdreyer 19 minutes agoparent>from one point in space - We have multiple observatories on and around the planet - The Earth is moving around the Sun - The Sun is moving around the centre of the galaxy - The galaxy is moving towards the great attractor, etc The Copernican principle states that humans, on the Earth or in the Solar System, are not privileged observers of the universe, that observations from the Earth are representative of observations from the average position in the universe. This has been tested in various ways: https://en.wikipedia.org/wiki/Copernican_principle#Tests_of_... A nice PBS spacetime video about the topic: https://www.youtube.com/watch?v=q-6oU3jXAho You can also ask how do we know that the laws of physics haven't changed over time. We don't. But at some point you have to make a few basic assumptions in order to have any hope of making scientific progress. reply harywilke 11 hours agoprevDr.Becky goes over this in a video [0] from a year ago about the divergent results obtained by the two main ways we measure the rate of expansion. Cosmic Microwave Background Vs. Supernovae. As the accuracy of each method has improved, the end results have diverged. [0] 'theJWST just made the \"Crisis in Cosmology\" WORSE' https://www.youtube.com/watch?v=hps-HfpL1vc&t=858s reply pigpang 2 hours agoparentUnfortunatelly, title of article on HN contradicts content of article, so many HN readers skip article because of \"confirmed\" (in other words: nothing new). IMHO, a Tired Light theory will better explain facts, but it will require paradigm shift, so there will be a lot of resistance before revolution. reply luc4sdreyer 2 minutes agorootparent> but it will require paradigm shift, so there will be a lot of resistance before revolution. I think the main point of resistance is the incompatibility with observations. All Tired light models have been falsified: https://en.wikipedia.org/wiki/Tired_light#Specific_falsified... If you're aware of a model that can fit some or all of our observations, please share it! reply merek 2 hours agoprevFor those interested in the cosmic distance ladder, David Butler has an excellent youtube series \"How far away is it?\", detailing the methods used to estimate distance along the cosmic distance ladder, the history, and examples. I highly recommend. https://www.youtube.com/playlist?list=PLpH1IDQEoE8QWWTnWG5cK... reply jon_adler 2 hours agoparentThank you. This series of videos is extremely helpful for understanding the concepts referred to in the article. reply acyou 10 hours agoprevEven though the title is copied from the article, we should change the title to \"New data indicates the Webb and Hubble telescopes agree on the universe's expansion rate, but not with the cosmic microwave background measurement based expansion rate\" reply russdill 3 hours agoparentEven the article seems confusing at first. The new measurements don't seem to be shedding light on anything or removing any confusion. They just confirm that hubble measurements already made. The Hubble tension remains as confounding a problem as ever. reply arbitrage 10 hours agoparentprevThat's a mouthful. The title should be \"Hubble Tension almost certainly not caused by measurement error.\" reply ta8645 7 hours agoprevDon't know why, but I recently got this gentleman's channel in my feed, who believes that there is no expansion at all: https://youtu.be/TGpjGVNVYEg?t=397 It's beyond my depth to explain why he's wrong. reply pmayrgundter 1 hour agoparentAlexander Unziker. Also Eric Lerner, Pierre Robitaille reply 3cats-in-a-coat 2 hours agoprevThe universe is not expanding. The atoms are getting smaller. reply pantulis 1 hour agoprevPlay player Euclid. reply sudom82 7 hours agoprevThis article is titled \"Webb and Hubble confirm Universe's expansion rate\", however I don't see the expansion rate actually listed there, or in the comments. I see a mention of age, but not the rate. Does anyone know the rate? I clicked through to the paper itself[1], but wasn't able to interpret it from the details I could see [1]: https://iopscience.iop.org/article/10.3847/2041-8213/ad1ddd reply russdill 3 hours agoparentIt seems like a better title would be \"Webb confirms Hubble's measurement of Universe's expansion rate\" reply westurner 12 hours agoprevHubble's law: https://en.wikipedia.org/wiki/Hubble%27s_law Expansion of the universe: https://en.wikipedia.org/wiki/Expansion_of_the_universe : > While objects cannot move faster than light, this limitation only applies with respect to local reference frames and does not limit the recession rates of cosmologically distant objects Given that v is velocity in the opposite direction, and c is the constant reference frame speed of light; do we account for velocity in determining whether light traveling at c towards earth will ever reach us? v - c c v + c > c if v>0 Are tachyons FTL, is there entanglement FTL? How far away in light years does a mirror in space need to be in order to see dinosaurs that existed say 100 million years ago? reply blackbear_ 11 hours agoparent> do we account for velocity in determining whether light traveling at c towards earth will ever reach us? As far as I know this is not necessary because the speed of light is constant regardless of the velocity of both the source and the observer (this is Einstein's special relativity: https://en.m.wikipedia.org/wiki/Special_relativity) reply a_random_canuck 7 hours agoparentprev> do we account for velocity in determining whether light traveling at c towards earth will ever reach us? No, because the speed of light is constant for all observers. From our frame of reference on earth, light from distant receding galaxies is always moving towards us at exactly the speed of light, c. Those galaxies also observe the light moving away from them at exactly c. That seems contradictory and unintuitive, that two observers moving away from each other both measure light moving c relative to themselves, but that’s reality. It’s another measurement that changes: if c is always constant, then it must be the passage of time and the distance travelled that we observe differently. reply pigpang 2 hours agorootparent> That seems contradictory and unintuitive, that two observers moving away from each other both measure light moving c relative to themselves, but that’s reality. Light is a wave. Photon is a complex thing (Hopfion?), but it's a wave, so it waves something, a medium. Speed of wave propagation in a medium is constant. IMHO, it's intuitive. reply arbitrage 10 hours agoparentprevTachyons aren't a thing. Tachyons are sci-fi nonsense. Nothing in the universe can travel faster than the speed of light. This does not hold for the universe itself. It can and does expand faster than the speed of light, using specific reference frames (i.e., big enough). So, space can increase FTL. Particles do not travel faster than light tho, that is nonsense. reply nsilvestri 12 hours agoprevThis URL is a stub, and the full article can be read at https://www.esa.int/Science_Exploration/Space_Science/Webb/W... reply dang 8 hours agoparentThanks! We've changed to that from https://www.esa.int/ESA_Multimedia/Images/2024/03/Webb_Hubbl... above. reply cwillu 12 hours agoparentprevAdded bonus: it has 50% less animated “responsive” design. reply jjbinx007 10 hours agoparentprevI wonder if the mods can change the main article link to the one you provided instead? reply dang 8 hours agorootparentIndeed we can! reply gregorymichael 12 hours agoprevWhat are the implications? reply kromem 12 hours agoparentBasically it means that we can't regard the Hubble result as a mismeasurement and the age of the universe seems to be different depending on how you measure it. From the article: \"The bottom line is that the so-called Hubble Tension between what happens in the nearby Universe compared to the early Universe’s expansion remains a nagging puzzle for cosmologists. There may be something woven into the fabric of space that we don’t yet understand.\" reply techwiz137 11 hours agorootparentI just think it means the expansion rate is not a constant, but a variable. reply rihards__ 4 hours agorootparentI kind of agree with this conclusion. Before we know better it can be just that spacetime was expanding at a different rate (we still would need at least one another Planck that operates in roughly same range to confirm this). Hubble wavelenght range - 0.1 to 0.8 μm Webb wavelenght range - 0.6 to 28.3 μm Planck wavelenght range - 330 to 10000 μm My understanding is that Planck was observing photons that have happened much more earlier. reply washadjeffmad 12 hours agorootparentprevSounds like they don't want to spoil everyone's research grants! reply jdiff 11 hours agorootparentOutside of the tinfoil, it just sounds like the universe is complex and not always predictable. reply harywilke 11 hours agorootparentIt's amazing that barely a 100 years ago The Great Debate in astronomy was weather the Milky Way was the extent of the universe or things like Andromeda were their own 'island universes'. In the 1920s, Edwin Hubble showed that Andromeda was far outside the Milky Way by measuring Cepheid variable stars. These are the same stars that we are measuring today in this debate. https://en.wikipedia.org/wiki/Great_Debate_(astronomy) reply chadcmulligan 11 hours agorootparentAnd not much longer before that the discussion was how long would the sun last - 5,000 years or so was the estimate if it was a big ball of burning gas (source: A scientific American article I read, wish I could find it again, hoping someone here knows) reply yokoprime 10 hours agorootparentI think it’s this one you’re referring to https://www.scientificamerican.com/article/experts-doubt-the... reply chadcmulligan 10 hours agorootparentThat's the one, thanks, I've been looking for it for ages reply washadjeffmad 11 hours agorootparentprevI meant it as a comment on the phrasing. Potentially jeopardizing a field's cash flow is a legitimate worry, and I see a few have felt that, as well. reply bigbillheck 11 hours agorootparentThere's a whole lot of open problems in cosmology, nobody's going to be out of work if they solve this one. reply mr_mitm 12 hours agoparentprevEither we live in an unusually under dense region of the universe or our models are wrong (\"new physics\"). reply amethyst 12 hours agorootparentOr it's a simulation and someone keeps pushing changes to production. reply cwillu 12 hours agorootparentWhich would also count as new physics. reply yreg 11 hours agorootparentWith even more literal meaning of new. reply ducttapecrown 11 hours agorootparentprevAll the expert software engineers agree this is the most likely explanation. Have physicists looked into this? reply NateEag 8 hours agorootparent> All the expert software engineers agree this is the most likely explanation. That's quite a strong claim. I'm skeptical. Sources? > Have physicists looked into this? They shelved it right next to \"God Made The Universe\" in the \"Unfalsifiable Propositions\" section, under the title \"Grad Students Made The Universe.\" reply kaashif 7 hours agorootparentI'm reading their comment as a joke about how software engineers tend to overestimate their own expertise on things like physics and are not actually anywhere close to experts. Software engineers presenting weird pseudo science as serious physics is one way this manifests. I could be wrong. reply lajr 12 hours agorootparentprevI wonder if their introspection is good enough to have our population on a Grafana dashboard somewhere reply imzadi 11 hours agorootparentprevSomeone keeps running gparted on our partition reply queuebert 12 hours agorootparentprevSomewhere aliens are making fun of how shoddy our simulation is coded. reply brcmthrowaway 12 hours agorootparentprevIt's definitely a simulation at this point reply dbtc 9 hours agoprevCan someone help me understand - point me toward some reading or ELI5 - what is the universe expanding into? (Or probably, why is that question not formulated well?) reply andreareina 8 hours agoparentOne of the quantities described by Einstein's equations of General Relativity is the metric tensor—a matrix of matrices—that describes how \"far\" things are in space and time given the stuff in the local environment. One of the things that the equation tells us is that objects that are not gravitationally bound will tend to get farther away from each other as time passes. We call this the expansion of space. As far as we can tell, there isn't anything \"outside\" the universe is expanding \"into\"; distances just become larger somehow. reply digging 8 hours agoparentprevThe universe is not expanding into anything. It is infinite. However, new space is being created in the voids between objects. It isn't super intuitive or particularly easy to grasp, things are just getting farther apart. reply ChrisMarshallNY 11 hours agoprevThe thing that strikes me, is the detailed image description. That looks like good fodder for an ML visitor. reply beaned 7 hours agoprevStupid question: how do we know that the universe is strictly exponentially expanding, and not both expanding and contracting in perpetuity like a sin wave? And could such an idea have anything to do with the Hubble tension? reply quijoteuniv 3 hours agoprevIs it 42? reply rmbyrro 11 hours agoprevThe scale of the universe sometimes feels terrifying to me reply swader999 11 hours agoparentIt's not if you consider we can only move at less than the speed of light or that we go on forever. reply modelofdemocray 8 hours agoprevit's not going to expand faster than what you're able to observe reply luxuryballs 11 hours agoprevthose are some pretty old energy waves reply m3kw9 11 hours agoprevIf we can travel faster than light does it blow up the theory that the universe expands? Because if we can travel faster than light the universe is theoretically be infinite if I can go to a point pas the furthest reaches of stars/matter. reply not2b 9 hours agoparentIf we can travel faster than light, that would blow up all the theories that say that we cannot. And FTL travel would imply time travel (if in your reference frame, something is traveling faster than light, there's another reference frame where it is going backwards in time). A lot of science fiction just sweeps this under the table and pretends that we have Newtonian absolute time to go along with the FTL travel. reply m3kw9 8 hours agorootparentIf we can do FTL it may just prove that light speed can be achieved by brute force, or an alternative method. No laws needs to be broken and new laws can be found. Like quantum physics and classical physics. reply FredPret 12 hours agoprevUnbelievable how much we can work out from just the odd photon hitting us from somewhere in the great unknown. reply daxfohl 12 hours agoparentIt's astounding to me that space is so empty that in the billions of light years from here to the edge of the universe, there's orders of magnitude less total interference than what you get from a small cloud. It's also astounding that celestial objects emit enough photons that thousands per second travel within the arc that goes the distance from the star to somewhere inside the radius of your pupil. And if this wasn't the case, then we'd never even be aware of any of this. reply suzzer99 3 hours agorootparentAlso that the light and gravitational waves from colliding neutron stars can travel 100 million light years and arrive at earth within a second of each other. That's a mind-bogglingly small amount of drag. reply CobrastanJorji 11 hours agoparentprev\"The odd photon hitting us\" is also a pretty good description of eyesight, radio, and fiber optics. reply fooker 11 hours agorootparentClearly the even photon is for parity checking and error correction ;) reply golemotron 12 hours agoparentprevOccam's Razor does a lot of heavy lifting. reply eikenberry 12 hours agoprevnext [4 more] [flagged] mr_mitm 12 hours agoparentAlmost all claims any serious scientist makes concern the observable universe. Also, these are very smart people. You'd be surprised what we can work out with a few very reasonable assumptions and the observations we made. The dismissiveness really isn't warranted. reply eikenberry 11 hours agorootparentSo you're saying the scientific media is where the \"dumb people\" are, not the scientists themselves. While I agree, it doesn't really change the point that these sorts of headlines/articles are heliocentric in nature and mischaracterize our understanding of the universe. IE. I'm criticizing pop-science, not science. reply xcv123 11 hours agoparentprevhttps://en.wikipedia.org/wiki/Observable_universe reply mjfl 11 hours agoprevIt is very fortunate that the universe is expanding. This provides a virtually unlimited source of energy. reply digging 8 hours agoparentWithout being able to harvest dark energy, it's actually the exact opposite. reply puzzledobserver 6 hours agorootparentWhat would a hypothetical dark energy engine look like? reply mjfl 5 hours agorootparenttwo black holes in highly elliptical orbit, so they they nearly collide when they come together. at the center of mass is a cloud of iron atoms. When the black holes nearly collide tidal forces across iron nuclei rip them apart. These ripped apart atoms can then be re-harvested and used as fuel in a fusion engine. The energy loss from ripping the atoms apart is then recovered by the black holes by taking advantage of dark energy when they go apart, which increases the distance between them, increasing the gravitational potential energy. reply undersuit 6 hours agorootparentprevIt would harness dark energy. /s We haven't detected dark matter or dark energy outside of their visible effects on the larger universe. Maybe we can't interact with it with baryonic matter. A dark energy engine made out of dark matter would be invisible. reply mjfl 7 hours agorootparentprevThe extra energy comes from the increase in gravitational potential energy between objects. The accelerating increase in this potential energy implies the total energy of the universe is increasing. I’m not talking about harvesting dark energy directly. reply antod 6 hours agorootparentI'm no physicist, but isn't the gravitational potential energy between two objects inversely proportional to the distance between them? reply mjfl 6 hours agorootparentit is, in the negative direction though. So increasing the distance increases the potential energy by making -1/r closer to zero. reply antod 5 hours agorootparentHeh, I had to go off and read some stuff to get my head around what \"negative\" potential energy would even mean (high school physics was decades ago). So... the negative is just a convention to represent work done against the \"field\", and positive is work done by the field? ie more of a vector than actually being negative energy? I think I get that bit now. So now I'm wondering if that still applies to an expanding universe vs eg a rocket leaving earth. If things aren't moving further apart by work (ie force x distance) being done against their gravitational fields so much as space time expanding, is there an increase in potential energy? And then if objects are moving apart faster than escape velocity, could that still be seen as increasing potential energy? I think I'm confusing myself further... reply fsckboy 10 hours agoprev [–] given that a telescope conceived while Bill Clinton was president in the 1990's got named so as to complete the name \"Webb Hubble\", let's please please please call the next telescope \"Chelsea\". https://pagesix.com/wp-content/uploads/sites/3/2020/02/chels... https://www.wnd.com/wp-content/uploads/2015/10/Webb-Hubbell_... (Chelsea Clinton's resemblance to Clinton family friend Webb Hubble has prompted speculation...) reply yen223 9 hours agoparent [–] I now really want to know what prompt was used to generate this text reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The Hubble constant, crucial for determining the Universe's expansion rate, has sparked debates from conflicting measurements.",
      "The James Webb Space Telescope's validation of the Hubble Space Telescope's measurements offers fresh perspectives on the expansion rate.",
      "This confirmation prompts inquiries into hidden variables affecting the expansion rate, driving scientists to delve deeper into research with more precise data from both telescopes."
    ],
    "commentSummary": [
      "The discussion explores the universe's expansion rate and historical methods to measure celestial distances, referencing historical figures and cosmological concepts.",
      "Various theories on the universe's expansion, redshift phenomenon, and galaxy ages are debated, emphasizing the continual exploration of physics laws and the universe's complexity.",
      "Limitations in current cosmological models and the importance of refining cosmology further are also addressed."
    ],
    "points": 525,
    "commentCount": 202,
    "retryCount": 0,
    "time": 1710190175
  },
  {
    "id": 39670102,
    "title": "Python/Cpython Introduces Disabling the GIL",
    "originLink": "https://github.com/python/cpython/pull/116338",
    "originBody": "python / cpython Public Notifications Fork 28.5k Star 58.8k Code Issues 5k+ Pull requests 1.5k Actions Projects 27 Security Insights New issue Jump to bottom gh-116167: Allow disabling the GIL with PYTHON_GIL=0 or -X gil=0 #116338 Merged colesbury merged 11 commits into python:main from swtaarrs:cpython-disable-gil Merged gh-116167: Allow disabling the GIL with PYTHON_GIL=0 or -X gil=0 #116338 colesbury merged 11 commits into python:main from swtaarrs:cpython-disable-gil +163 −1 Conversation 20 Commits 11 Checks 31 Files changed 12 Conversation Member swtaarrs commented • edited In free-threaded builds, running with PYTHON_GIL=0 or -X gil=0 will now disable the GIL. #116322 and #116329 track follow-up work to re-enable the GIL when loading an incompatible extension, and to disable the GIL by default, respectively. In order to support re-enabling the GIL at runtime, all GIL-related data structures are initialized as usual, and disabling the GIL simply sets a flag at startup that causes take_gil() and drop_gil() to return early. With PYTHON_GIL=0 set, I spot-checked a few tests and small programs that don't use threads. They all seem to run fine, and very basic threaded programs work, sometimes. Trying to run the full test suite crashes pretty quickly, in test_asyncio. Issue: Add a mechanism to disable the GIL #116167 29 25 swtaarrs added the topic-free-threading label swtaarrs requested a review from colesbury bedevere-app bot mentioned this pull request Add a mechanism to disable the GIL #116167 Open swtaarrs added the skip news label Member Author swtaarrs commented !buildbot nogil bedevere-bot commented 🤖 New build scheduled with the buildbot fleet by @swtaarrs for commit ac24233 🤖 The command will test the builders whose names match following regular expression: nogil The builders matched are: x86-64 MacOS Intel ASAN NoGIL PR x86-64 MacOS Intel NoGIL PR ARM64 MacOS M1 Refleaks NoGIL PR ARM64 MacOS M1 NoGIL PR AMD64 Ubuntu NoGIL Refleaks PR AMD64 Ubuntu NoGIL PR AMD64 Windows Server 2022 NoGIL PR Allow disabling the GIL with PYTHON_GIL=0 … 9e03999 In free-threaded builds, running with `PYTHON_GIL=0` will now disable the GIL. Follow-up issues track work to re-enable the GIL when loading an incompatible extension, and to disable the GIL by default. In order to support re-enabling the GIL at runtime, all GIL-related data structures are initialized as usual, and disabling the GIL simply sets a flag that causes `take_gil()` and `drop_gil()` to return early. With `PYTHON_GIL=0` set, I spot-checked a few tests and small programs that don't use threads. They all seem to run fine, and very basic threaded programs work, sometimes. Trying to run the full test suite crashes pretty quickly, in `test_asyncio`. swtaarrs force-pushed the cpython-disable-gil branch from ac24233 to 9e03999 Compare Member Author swtaarrs commented !buildbot nogil bedevere-bot commented 🤖 New build scheduled with the buildbot fleet by @swtaarrs for commit 9e03999 🤖 The command will test the builders whose names match following regular expression: nogil The builders matched are: x86-64 MacOS Intel ASAN NoGIL PR ARM64 MacOS M1 Refleaks NoGIL PR AMD64 Windows Server 2022 NoGIL PR AMD64 Ubuntu NoGIL PR x86-64 MacOS Intel NoGIL PR AMD64 Ubuntu NoGIL Refleaks PR ARM64 MacOS M1 NoGIL PR corona10 reviewed View reviewed changes Member corona10 left a comment • edited Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more. Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Good to add test case in this file too. cpython/Lib/test/test_cmd_line.py Line 896 in 72714c0def test_int_max_str_digits(self): Member corona10 commented • edited Ah ignore above comments, this PR only referencing the environment variable so not proper for this case. You can just add the testcase for the environment variable, still worth to adding it. 1 Member corona10 commented At not for this PR, since this feature is not officially announced, but in some moment, you should write about the environment variable at https://github.com/python/cpython/blob/72714c0266ce6d39c7c7fb63f617573b8f5a3cb2/Doc/using/cmdline.rst to get user feedback. 1 swtaarrs added 3 commits Don't add 'enable_gil' to test_embed in normal builds a53ed2e Set enable_gil properly when PYTHON_GIL=1 e0b8484 Add test for PYTHON_GIL in test_cmd_line 4f43a87 Member Author swtaarrs commented !buildbot nogil bedevere-bot commented 🤖 New build scheduled with the buildbot fleet by @swtaarrs for commit 4f43a87 🤖 The command will test the builders whose names match following regular expression: nogil The builders matched are: x86-64 MacOS Intel ASAN NoGIL PR ARM64 MacOS M1 NoGIL PR AMD64 Ubuntu NoGIL PR x86-64 MacOS Intel NoGIL PR ARM64 MacOS M1 Refleaks NoGIL PR AMD64 Ubuntu NoGIL Refleaks PR AMD64 Windows Server 2022 NoGIL PR swtaarrs marked this pull request as ready for review swtaarrs requested a review from ericsnowcurrently as a code owner bedevere-app bot added the awaiting review label swtaarrs requested a review from vstinner Member Author swtaarrs commented you should write about the environment variable ... to get user feedback What would be a good time to do that? Maybe once the GIL is disabled by default, so the description doesn't have to change? colesbury reviewed View reviewed changes Python/initconfig.c Outdated Show resolved Hide resolved Fix else brace formatting 9e9d925 Contributor colesbury commented • edited I would document the environment variable now, when it's added. For reference, we already document the --disable-gil configure flag. I don't think it needs to be extensive, but should cover: Only available in free-threaded builds 0 forces the GIL to be disabled 1 forces the GIL to be enabled New in version 3.13 The behavior when then environment variable is not set is what's going to change in the future. We can document that later. 4 Document PYTHON_GIL environment variable 5053661 Member Author swtaarrs commented After discussion in Discord, I'm also going to add a -X option to go with the environment variable. swtaarrs marked this pull request as draft bedevere-app bot removed the awaiting review label swtaarrs added 2 commits Merge remote-tracking branch 'upstream/main' into HEAD 6d62fbb Add -X gil option, add to sys.flags, modify test to cover env var… … 971a9a1 … and option swtaarrs changed the title gh-116167: Allow disabling the GIL with PYTHON_GIL=0 gh-116167: Allow disabling the GIL with PYTHON_GIL=0 or -X gil=0 swtaarrs added 2 commits Fix link to -X gil 89b1150 Fix PYTHON_GIL versionchanged line 16fbd44 swtaarrs marked this pull request as ready for review swtaarrs requested a review from gpshead as a code owner bedevere-app bot added the awaiting review label colesbury requested a review from erlend-aasland ericsnowcurrently approved these changes View reviewed changes Member ericsnowcurrently left a comment Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more. Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment LGTM I've left one minor comment about an unclear test. It really isn't a blocker, as the test is passing, but it would still be nice to have a response. Lib/test/_test_embed_set_config.py Outdated Show resolved Hide resolved bedevere-app bot added awaiting merge and removed awaiting review labels erlend-aasland approved these changes View reviewed changes corona10 approved these changes View reviewed changes Clarify test_flags in normal builds 8d39bbc colesbury approved these changes View reviewed changes Hide details View details colesbury merged commit 2731913 into python:main 34 checks passed bedevere-app bot removed the awaiting merge label Member vstinner commented This is both exciting and very scary. Congrats @swtaarrs and @colesbury :-) 2 4 6 This comment was marked as off-topic. Sign in to view This comment was marked as off-topic. Sign in to view This comment was marked as off-topic. Sign in to view python locked as spam and limited conversation to collaborators Sign up for free to subscribe to this conversation on GitHub. Already have an account? Sign in. Reviewers colesbury ericsnowcurrently corona10 erlend-aasland vstinner gpshead Assignees No one assigned Labels skip news topic-free-threading Projects None yet Milestone No milestone Development Successfully merging this pull request may close these issues. None yet 10 participants",
    "commentLink": "https://news.ycombinator.com/item?id=39670102",
    "commentBody": "gh-116167: Allow disabling the GIL (github.com/python)433 points by freediver 17 hours agohidepastfavorite218 comments dvas 17 hours agoExtra links for the no gil work for anyone else curious about this [0], [1]. [0] Multithreaded Python without the GIL https://docs.google.com/document/d/18CXhDb1ygxg-YXNBJNzfzZsD... [1] Github repo https://github.com/colesbury/nogil reply tentacleuno 15 hours agoparentFurther context on noGIL in general: https://hn.algolia.com/?dateRange=all&page=0&prefix=true&que... reply Hugsun 16 hours agoprevIt will be very exciting to see how much faster they'll be able to make vanilla python. The value proposition is being challenged by the plethora of tools aiming to alleviate those issues. Speed improvers like Mojo, pytorch, triton, numba, taichi come to mind. There are so many different attempts at solving this problem that that the last time I wanted to try one of them, I found myself overwhelmed with options. I chose taichi which is pretty fun and easy to use, although somewhat limited in scope. reply nerpderp82 15 hours agoparentMojo should be viewed as an attack on the Python ecosystem due to it being a superset. It can consume Python, but it itself is not Python. Taichi is really underrated, it works across all platforms (including Metal), has tons of examples and the code is easy to write. And lastly, it integrates with the ecosystem and doesn't displace it. https://github.com/taichi-dev great demo reel of what Taichi can do, https://www.youtube.com/watch?v=oXRJoQGCYFg https://www.youtube.com/watch?v=WNh4Q7-OSJs https://www.taichi-lang.org/ reply boxed 2 hours agorootparentMojo is not a superset. Not even close. They say they are AIMING for it to be a superset OF THE SYNTAX. This is a subtle yet enormous difference to being a superset of the language itself. reply its-summertime 11 hours agorootparentprevCython is also a superset, is Cython also guilty of such crimes? reply Intralexical 9 hours agorootparentCython is dependent on CPython. Cython outputs Python extension modules, which can only be used when imported into a standard CPython environment, and which interoperate cleanly with the rest of the Python ecosystem. Mojo explicitly does the opposite, allowing Mojo to use Python but requiring Mojo to be in control, while making it hard/impossible for code written in Mojo to benefit code written in Python: > Our long-term goal is to make Mojo a superset of Python (that is, to make Mojo compatible with existing Python programs). […] Mojo lets you import Python modules, call Python functions and interact with Python objects from Mojo code. […] > As shown above, you can call out to Python modules from Mojo. However, there's currently no way to do the reverse—import Mojo modules from Python or call Mojo functions from Python. […] > This pattern doesn't work because you can't pass Mojo callbacks to a Python module. > Since Python can't call back into Mojo, one alternative is to have the Mojo application drive the event loop and poll for updates. No comment on whether this should be viewed as an attack. https://docs.modular.com/mojo/manual/python/ reply KerrAvon 14 hours agorootparentprevI think \"attack\" is a bit much; C++ isn't an attack on C. reply Kranar 14 hours agorootparentWhile Ken Thompson never used the word attack, he certainly didn't have a positive opinion of the language or of Bjarne Stroustrup either in terms of his technical contributions or his handling of C++ adoption: https://gigamonkeys.wordpress.com/2009/10/16/coders-c-plus-p... reply hn_throwaway_99 13 hours agorootparentThanks for posting that, I thought it was a great read (as someone who last used C++ probably about 25 years ago...) Given that so many of the criticisms were about C++ being over-complicated, I do worry about languages just becoming more and more difficult over time as everyone wants their pet feature added, but due to backwards-compatibility concerns old/obsolete features are rarely removed. For example, take Java. I think that a ton of goodness has been added to Java over the decades, and for people who have been working with it throughout, it's great. But it feels like the learning curve for someone just getting involved with Java would be really steep, not just because there is just a ton of stuff, but because without having the context of the history and how things were added over time (usually with an eye towards backwards-compatibility) it feels like it would be hard to wrap your head around everything. If you're writing your own new program that's not really a problem as you can just stick to what you know, but if you're getting into an existing codebase that could use lots of different features it feels like it could be daunting. It's been quite a while since I've programmed in Java, so I'm just speculating, but would be curious how other folks relatively new to the language in production environments find the learning curve. reply progmetaldev 8 hours agorootparentI haven't used Java since v1.5, roughly around 2005. I do use C# quite a bit, and have since v1.1 (I got into .NET through VB.NET at v1.0, as I had just learned VB through schooling). I look back at all the features that have been added over time, and since I have followed along as it has developed, I embrace the changes. They have made my code more concise and easier to read, with less boilerplate. When I think about someone brand new getting into the language, I truly hope they have a mentor. I can't imagine having to work with someone new to the language, and not being given plenty of time to get them up to speed with both current codebases, as well as newer ones (and I haven't even adopted the newest language versions unless it was suggested to me through Resharper, and I looked up the feature and found how it made my code easier to understand). I feel like the comparison with Java is similar, as both Oracle and Microsoft are large corporations that largely control the language and ecosystem, while also having open source implementations. C# has made some more major changes to the language itself than Java, but both have diverged quite a bit from what they were back in 2005. I'll find out next year, as my son goes into high school, and they are offering Java software development classes. I got Apple Basic on the Apple ][e, QuickBasic, and a bit of C++ in high school, and graduated in 1997. I wouldn't be surprised if he's going to be dealing with Java v1.5 instead of the latest features. At least I'll have a motivation to learn the latest features if he enjoys it and keeps working at it. reply flakes 2 hours agorootparent> I wouldn't be surprised if he's going to be dealing with Java v1.5 instead of the latest features. Well I certainly hope not… LTS for Java 5 probably hit end of life when he was 2. I’d say most likely the version being installed is Java 17, or 11 if theyre really dated. Java 8 would likely be the absolute minimum, mainly due to the industry being so slow to migrate away from it. Newer programmers are unlikely to ever know what Java felt like before generics and lambdas existed. reply dimitrios1 13 hours agorootparentprevI was doing primarily go development since it was first released up until a few years ago when the pandemic allowed me the opportunity to move into a full time remote gig doing primarily Java development, so I can answer this as I hadn't done Java at that point for over 10 years, so I felt completely new (what Java I did before that, I was mostly trying to not use Java by using play framework or jruby on rails) As someone in the boat you mentioned (sort of) the short answer is modern Java development for 90% of tasks is not complicated at all: it's very much like any programming language used in a bizdev/corp environment -- you are mostly using a framework and a bunch of DSLs. Almost everyone uses Intellij and Gradle for IDE and build, and Junit5 or Spock for unit testing. I passed a technical interview mostly on Spring Framework concepts knowing almost nothing about it, nor having ever used it in production by simply just having the documentation open while I was being interviewed so I could look up the answers. Any language that is popular is going to have frameworks with decent documentation that help you be productive quickly, so I just jumped in doing Spring. The java stuff came as needed, or I referenced something like Effective Java (great book), or a Baeldung article. Java world has made some great strides since the 2000's and early 2010s of XML chaos. It took a while, but I feel like it's in a really good spot and getting better. As an aside, if it hasn't been mentioned to you before, if you like simplicity in a language, but still incredibly productive, you might enjoy Go. reply progmetaldev 7 hours agorootparentI truly appreciate your answer, as someone that primarily does C#, but learned Java first back around 2003 to 2005. More so because my son is going to be learning Java next year in high school, and I know he'll be looking to me for help, and I'd like to at least be at the same language version as he's learning. I've also been looking at Go quite a bit, especially from a lot of commentary even recently on YouTube, as well as plenty of time during work hours to experiment with languages I'm not familiar with. I've been deciding between Python and Go, and I picked up Python very quickly for the language, but ultimately I think it's the libraries and ecosystem around both that will decide it for me. Just good to see another vote for Go, especially when comparing to Java. I feel like I'm in the same boat as you seem to be, where it's all just syntax and learning frameworks, while having enough experience at this point to also pick up the smaller details while quickly being effective. reply dimitrios1 5 hours agorootparentNo problem. I am also a huge Dream Theater fan. reply richrichie 9 hours agorootparentprevThanks for sharing. > perhaps Erik Naggum, scourge of Usenet, was right when he said: “life is too long to know C++ well.” I feel similar about Rust. I have read “the book”. Did a couple of small projects. It is also sort of committee administered language that sucks up tiny features like a giant vacuum cleaner sweeping the streets and changes every hour. reply kelnos 4 hours agorootparent> that sucks up tiny features like a giant vacuum cleaner sweeping the streets and changes every hour. Can you provide some (preferably recent) examples? My experience has been the opposite. It feels like new features are given a lot of thought and deliberation, and stablizing even something small can take upwards of a year. I will agree that today's Rust is different from Rust 1.0, but I don't see that as necessarily a bad thing. More that they were able to come to a stable 1.0-ready core fairly early on, and have been adding on the more tricky parts bit by bit since then. reply android42 14 hours agorootparentprevI wasn't sure whether to agree with this or not, so I finally took a slightly closer look at Mojo just now. This depends on how they license it going forward, and whether they make it open, or use being a superset as a way to capture then trap python users in their ecosystem, and I don't think we have a certain answer which path they'll take yet. The way they let you mix python compatible code with their similar but more performant code [1] looks interesting and provides a nice path for gradual migration and performance improvements. It looks like one of the ways they do this is by letting you define functions that only use typed variables which is something I would like to see make its way back to CPython someday (that is optionally enforcing typing in modules and getting some performance gains out of it). [1] https://en.wikipedia.org/wiki/Mojo_(programming_language)#Pr... reply Intralexical 9 hours agorootparent> It looks like one of the ways they do this is by letting you define functions that only use typed variables which is something I would like to see make its way back to CPython someday (that is optionally enforcing typing in modules and getting some performance gains out of it). This is already how Cython and MYPYC work. You add standard PEP-484 type annotations, and they use that to infer where code can be compiled to native instructions. https://cython.readthedocs.io/en/latest/src/tutorial/pure.ht... reply oivey 4 hours agorootparentprevThe typing thing is really unnecessary and a step backwards, IMO. Added typing is maybe great for managing a code base, but it isn’t necessary for performance with a compiler that can do type inference. reply nerpderp82 14 hours agorootparentprevI didn't mention C++ at all. Was that my argument? This thread is about Python, the GIL. Mojo was brought up as a way to speed up Python code. C++ predates on C in a similar way to how Mojo predates on Python. At least C++ has extern C. https://docs.modular.com/mojo/manual/python/#call-mojo-from-... > As shown above, you can call out to Python modules from Mojo. However, there's currently no way to do the reverse—import Mojo modules from Python or call Mojo functions from Python. One way street. Classic commons harvesting. reply kelnos 4 hours agorootparentI think the person you're replying to is just trying to use an analogous example; you didn't need to bring it up. Regardless, I think it's a bit alarmist and overly aggressive to assume nefarious intent. Have the developers acted in ways such that this reputation is deserved? Also, little OT, but it took me unreasonably long to understand that you meant \"predates\" as the verb form of \"predator\", not as in \"comes before chronologically\". The phrase \"preys on\" may be more clear. reply objektif 15 hours agorootparentprevNever heard of taichi before looks promising. Do you know any shop that uses it for prod code? reply fragmede 15 hours agorootparentETH Zurich is using it for their physics sim courses, University of Utah is using it for simulations (SIGGRAPH 2022), OPPO (they make smart devices running Android), Kuaishou uses it for liquid and gas simulation on GPUs. Lots of GPU accelerated sim stuff. https://www.taichi-lang.org/ https://www.researchgate.net/publication/337118128_Taichi_a_... https://github.com/taichi-dev/taichi reply vlovich123 16 hours agoprevDoes anyone know why the biased reference counting approach described in https://peps.python.org/pep-0703/ just has a single thread affinity requiring atomic increments/decrements when accessed from a different thread? What I’ve seen other implementations do (e.g. various Rust crates implementing biased reference counting) is that you only increment atomically when moving to a new thread & then that thread does non-atomic increments/decrements until 0 is hit again and then an atomic decrement is done. Is it because it’s being retrofitted into an existing system where you have a single PyObject & can’t exchange to point to a new thread-local object? reply colesbury 15 hours agoparentWe could implement ownership transfer in CPython in the future, but it's a bit trickier. In Rust, \"move\" to transfer ownership is part of the language, but there isn't an equivalent in C or Python, so it's difficult to determine when to transfer ownership and which thread should be the new owner. We could use heuristics: we might give up or transfer ownership when putting an object in a queue.SimpleQueue, but even there it's hard to know ahead of time which thread will \"get\" the enqueued object. I think the performance benefit would also be small. Many objects are only accessed by a single thread, some objects are accessed by many threads, but few objects are exclusively accessed by one thread and then exclusively accessed by a different thread. reply vlovich123 15 hours agorootparentI think you would do it on first access - “if new thread, increment atomic & exchange for a new object reference that has the local thread id affinity”. That way you don’t care about whether an object actually has thread affinity or not and you solve the “accessed by many threads” piece. But thanks for answering - I figured complexity was the reason a simpler choice was made to start with. reply orf 14 hours agorootparentBut this would now make the reference count increment require a conditional? It’s a very hot path, and this would cause a slowdown for single-threaded Python code. reply vlovich123 13 hours agorootparentIt's already taking a conditional. Take a look at the PEP: if (op->ob_tid == _Py_ThreadId()) op->ob_ref_local = new_local; else atomic_add(&op->ob_ref_shared, 1Learn golang or rust instead. Bad take. Learn golang and rust and python. You should use the language which is suited to the task, sometimes that's golang sometimes that's python and sometimes it's rust. It's impressive that the python team as a whole continues to improve in such big ways after more than 30 years of development. It's more impressive that the python team managed to navigate 2to3 and come out stronger. reply takeda 6 hours agoparentprevRust, sure, but typing in golang isn't superior to python's type annotations. Also Rust became a quite popular tool for python extensions, where you can offload performance to rust and business logic to python. reply alfalfasprout 6 hours agoparentprevBad take. One doesn't just migrate massive ecosystems to \"go or rust\". Python, like it or not, is the lingua franca of ML/AI and science. Worse, as a user of Go/Rust it's very disingenuous to expect the same kind of iteration ability in those languages as you get in Python. With tools like pyright now + the work on nogil everyone benefits from this using Python. reply Cacti 6 hours agoparentprevbut Python has my libraries, and go and rust do not. >> 99% of my compute is offloaded to compiled BLAS or CUDA. types are enough. and memory and type safety are not terribly relevant for my use cases. reply Dowwie 17 hours agoprevWe are now one step closer to PythonOTP reply misoukrane 16 hours agoprevFinally, looking forward to the benchmarks of many tools! reply xuhu 14 hours agoparentPEP-703 predicted in June 2023 an overhead of 15% when running with NoGIL: https://discuss.python.org/t/pep-703-making-the-global-inter... reply protomikron 17 hours agoprevAlthough this is nice, the problems with the GIL are often blown out of proportion: people stating that you couldn't do efficient (compute-bounded) multi-processing, which was never the case as the `multiprocessing` module works just fine. reply ynik 16 hours agoparentmultiprocessing only works fine when you're working on problems that don't require 10+ GB of memory per process. Once you have significant memory usage, you really need to find a way to share that memory across multiple CPU cores. For non-trivial data structures partly implemented in C++ (as optimization, because pure python would be too slow), that means messing with allocators and shared memory. Such GIL-workarounds have easily cost our company several man-years of engineer time, and we still have a bunch of embarrassingly parallel stuff that we still cannot parallelize due to GIL and not yet supporting shared memory allocation for that stuff. Once the Python ecosystem supports either subinterpreters or nogil, we'll happily migrate to those and get rid of our hacky interprocess code. Subinterpreters with independent GILs, released with 3.12, theoretically solve our problems but practically are not yet usable, as none of Cython/pybind11/nanobind support them yet. In comparison, nogil feels like it'll be easier to support. reply ebiester 16 hours agorootparentAnd I guess what I don't understand is why people choose Python for these use cases. I am not in the \"Rustify\" everything camp, but Go + C, Java + JNI, Rust, and C++ all seem like more suitable solutions. reply KaiserPro 12 hours agorootparent> but Go + C, Java + JNI, Rust, and C++ all seem like more suitable solutions. apart from go (maybe java) those are all \"scary\" languages that require a bunch of engineering to get to the point that you can prototype. even then you can normally pybind the bits that are compute bound. If Microsoft had been better back in the say, then c# should have been the goto language of choice. It has the best tradeoff of speed/handholding/rapid prototyping. Its also statically typed, unless you tell it to not be. reply snovv_crash 3 hours agorootparent#pragma omp parallel for gets you 90% of the potential performance of a full multithreaded producer/consumer setup in C++. C++ isn't as scary as it used to be. reply oivey 16 hours agorootparentprevNotably, all of those are static languages and none of them have array types as nice as PyTorch or NumPy, among many other packages in the Python ecosystem. Those two facts are likely closely related. reply abdullahkhalids 16 hours agorootparentPython is just the more popular language. Julia array manipulation is mostly better (better syntax, better integration, larger standard library) or as good as python. Julia is also dynamically typed. It is also faster than Python, except for the jit issues. reply oivey 15 hours agorootparentPreaching to the choir here. Julia’s threading API is really nice. One deficiency is that it can be tricky to maintain type stability across tasks / fetches. reply znpy 15 hours agorootparentprev> It is also faster than Python, except for the jit issues. I was intrigued by Julia a while ago, but didn't have time to properly learn it. So just out of curiosity: what's the issues with jit and Julia ? reply cjalmeida 14 hours agorootparentThe \"issue\" is Julia is not Just-in-Time, but a \"Just-Ahead-of-Time\" language. This means code is compiled before getting executed, and this can get expensive for interactive use. The famous \"Time To First Plot\" problem was about taking several minutes to do something like `using Plots; Plots.plot(sin)`. But to be fair recent Julia releases improved a lot of it, the code above in Julia 1.10 takes 1.5s on my 3-year old laptop reply jakobnissen 15 hours agorootparentprevJulia's JIT compiles code when its first executed, so Julia has a noticable delay from you start the program and until it starts running. This is anywhere from a few hundred milliseconds for small scripts, to tens of seconds or even minutes for large packages. reply shiroiushi 4 hours agorootparentI wonder why they don't just have an optional pre-compilation, so once you have a version you're happy with and want to run in production, you just have a fully compiled version of the code that you run. reply aoanla 1 hour agorootparentEffectively, it does - one of the things recent releases of Julia have done is to add more precompilation caching on package install. Julia 1.10 feels considerably snappier than 1.0 as a result - that \"first time to plot\" is now only a couple of seconds thanks to this (and subsequent plots are, of course, much faster than that). reply samatman 16 hours agorootparentprevIf only there were a dynamic language which performs comparably to C and Fortran, and was specifically designed to have excellent array processing facilities. Unfortunately, the closest thing we have to that is Julia, which fails to meet none of the requirements. Alas. reply rmbyrro 16 hours agorootparentIf only there was a car that could fly, but was still as easy and cheap to buy and maintain :D reply esafak 16 hours agorootparentprevWhy do people use python for anything beyond glue code? Because it took off, and machine learning and data science now rely on it. I think Python is a terrible language that exemplifies the maxim \"worse is better\". https://en.wikipedia.org/wiki/Worse_is_better reply nottorp 10 hours agorootparentTo quote from Eric Raymond's article about python, ages ago: \"My second [surprise] came a couple of hours into the project, when I noticed (allowing for pauses needed to look up new features in Programming Python) I was generating working code nearly as fast as I could type. When you're writing working code nearly as fast as you can type and your misstep rate is near zero, it generally means you've achieved mastery of the language. But that didn't make sense, because it was still day one and I was regularly pausing to look up new language and library features!\" Source: https://www.linuxjournal.com/article/3882 It doesn't go for large code bases, but if you need quick results using existing well tested libraries, like in machine learning and data science, I think those statements are still valid. Obviously not when you're multiprocessing, that is going to bite you in any language. reply rmbyrro 16 hours agorootparentprevSome speculate that universities adopted it as introductory language for its expressiveness and flat learning curve. Scientific / research projects in those unis started picking Python, since all students already knew it. And now we're here reply spprashant 14 hours agorootparentI have no idea if this is verifiably true in a broad sense, but I work at the university and this is definitely the case. PhD students are predominantly using Python to develop models across domains - transportation, finance, social sciences etc. They then transition to industry, continuing to use Python for prototyping. reply zamadatix 16 hours agorootparentprevPeople choose Python the use case, regardless what that is, because it's quick and easy to work with. When Python can't realistically be extended to a use case then it's lamented, when it can it's celebrated. Even Go, while probably the friendliest of that buch when it comes to parallel work, is on a different level. reply pillusmany 16 hours agorootparentprev\"Ray\" can share python objects memory between processes. It's also much easier to use than multi processing. reply ptx 16 hours agorootparentHow does that work? I'm not familiar with Ray, but I'm assuming you might be referring to actors [1]? Isn't that basically the same idea as multiprocessing's Managers [2], which also allow client processes to manipulate a remote object through message-passing? (See also DCOM.) [1] https://docs.ray.io/en/latest/ray-core/walkthrough.html#call... [2] https://docs.python.org/3/library/multiprocessing.html#manag... reply pillusmany 15 hours agorootparentShared memory: https://docs.ray.io/en/latest/ray-core/objects.html reply ptx 15 hours agorootparentAccording to the docs, those shared memory objects have significant limitations: they are immutable and only support numpy arrays (or must be deserialized). Sharing arrays of numbers is supported in multiprocessing as well: https://docs.python.org/3/library/multiprocessing.html#shari... reply jononor 9 hours agorootparentprevI think that 90 or maybe even 99% of cases has under 1GB of memory per process? At least it has been the case for me the last 15 years. Of course, getting threads to be actually useful for concurrency (GIL removed) adds another very useful tool to the performance toolkit, so that is great. reply vita7777777 16 hours agoparentprevOn the other hand, this particular argument also gets overused. Not all compute-bounded parallel workloads are easily solved by dropping into multiprocessing. When you need to share non-trivial data structures between the processes you may quickly run into un/marshalling issues and inefficiency. reply kroolik 17 hours agoparentprevManaging processes is more annoying than threads, though. Incl. data passing and so forth. reply pillusmany 16 hours agorootparentThe \"ray\" library makes running python code on multi core and clusters very easy. reply kroolik 15 hours agorootparentAlthough its great the library helps with multicore Python, the existence of such package shouldnt be an excuse not to improve the state of things in std python reply smcl 16 hours agorootparentprevInteresting - looking at their homepage they seem to lean heavily into the idea that it's for optimising AI/ML work, not multi-process generally. reply pillusmany 16 hours agorootparentYou can use just ray.core to do multi process. You can do whatever you want in the workers, I parse JSONs and write to sqlite files. reply liuliu 17 hours agoparentprev`multiprocessing` works fine for serving HTTP requests or do some other subset of embarrassingly-parallel problems. reply skrause 16 hours agorootparent> `multiprocessing` works fine for serving HTTP requests Not if you use Windows, then it's a mess. I have a suspicion that people who say that the multiprocessing works just fine never had to seriously use Python on Windows. reply rmbyrro 15 hours agorootparentProbably a very small minority of Python codebases run on Windows, no? That's my impression. It would explain why so many people are unaware of multiprocessing issues on Windows. I've never ran any serious Python code on windows... reply ptx 16 hours agorootparentprevWhy is it a mess? What's wrong with it on Windows? reply colatkinson 12 hours agorootparentAdding on to the other comment, multiprocessing is also kinda broken on Linux/Mac. 1. Because global objects are refcounted, CoW effectively isn't a thing on Linux. They did add a way to avoid this [0], but you have to manually call it once your main imports are done. 2. On Mac, turns out a lot of the system libs aren't actually fork-safe [1]. Since these get imported inadvertently all the time, Python on Mac actually uses `spawn` [2] -- so it's roughly as slow as on Windows. I haven't worked in Python in a couple years, but handling concurrency while supporting the major OSes was a goddamn mess and a half. [0]: https://docs.python.org/3.12/library/gc.html#gc.freeze [1]: https://bugs.python.org/issue33725 [2]: https://docs.python.org/3.12/library/multiprocessing.html#co... reply fulafel 6 hours agorootparentRe (1), are there publicly documented cases with numbers on observed slowdowns with it? I see this mentioned from time to time, but intuitively you'd think this wouldn't pose a big slowdown since the system builtin objects would have been allocated at the same time (startup) and densely located on smaller nr of pages. I guess if you have a lot of global state in your app it could be more significant. Would also be interesting to see a benchmark using hugepages, you'd think this could solve remaining perf problems if they were due to large number of independent CoW page faults. reply skrause 16 hours agorootparentprev* A lack of fork() makes starting new processes slow. * All Python webservers that somewhat support multiprocessing on Windows disable the IOCP asyncio event loop when using more than one process (because it breaks in random ways), so you're left with the slower select() event loop which doesn't support more than 512 connections. reply jcranmer 16 hours agoparentprev> as the `multiprocessing` module works just fine. Something that tripped me up when I last did `multiprocessing` was that communication between the processes requires marshaling all the data into a binary format to be unmarshaled on the other side; if you're dealing with 100s of MB of data or more, that can be quite some significant expense. reply karmasimida 16 hours agoprevThis is exciting, can't wait reply rnmmrnm 16 hours agoprevMore than seeing it in main, I'm happy for the \"python thread slow\" meme officially going away now. reply agent281 9 hours agoparentI doubt that Python will ditch the meme. The fundamental model of dynamic dispatch using dictionaries on top of a byte code interpreter is pretty slow. I wouldn't expect it to get within 2x of JavaScript. reply DinaCoder98 9 hours agorootparentJavascript may not have an official bytecode, but is it not also based on the same concept of using dictionaries to dispatch code and slow as a result? I certainly had always filed it away as \"about as fast as python\" in my head. Why else would it rely on evented i/o? reply agent281 7 hours agorootparentYou are correct, but they have (1) all of the money in the world as the fundamental programming language of the Internet and as a result (2) they have a state of the art tiered JIT for dynamic languages. The blood of countless PhD students flows through v8. I don't know if python will get the same treatment. reply imtringued 1 hour agorootparentprevOk but given enough cores even python code will run into memory bandwidth problems rather than be bottlenecked by memory latency. reply scubbo 16 hours agoparentprevI wish I had your optimism. Thoughtless bandwagon-y \"criticism\" is extraordinarily persistent. reply bmitc 14 hours agorootparentIt isn't thoughtless. I'm working in Python after having come from more designed languages, and concurrency in Python is an absolute nightmare. It feels like using a language from the 60s. An effectively single threaded language in 2024! That's really astonishing. reply scubbo 11 hours agorootparentIf your criticism isn't thoughtless, then that's not what I'm complaining about. Specifically, I'm annoyed about people who _just_ say \"Python isn't fast enough, therefore it's not suitable to our use-case\", when their use-case doesn't require significant speed or concurrency. If you thoughtfully discount Python as being unsuitable for a use-case that it's _actually_ unsuitable for, then good luck to you! reply mlyle 9 hours agorootparentPython has been too often just a -bit- too slow for my use cases; the ability to throw a few cores at problems more easily is not going to eliminate this criticism from me but it's sure going to diminish it by a large factor. reply nextlevelwizard 13 hours agorootparentprevmost software doesnt need multi threading. most times people cry about pythons performance then write trivial shit programs that take milliseconds to run in python as well reply cwalv 4 hours agorootparentPython being slow, and working to speed python programs up, helped me immensely to build a mental model for what makes programs slow. After learning C in school, when I first learned how python was implemented, I was shocked that it was even usable. reply bmitc 12 hours agorootparentprevNearly every time I've interactive with Python, its execution speed is absolutely an issue. reply nextlevelwizard 3 hours agorootparentPlease do give an example. I see is people crying how python is slow and then use a proper fast programming language to write code that gets executed so few times that even if python was 100x slower it wouldn't matter or the program is so trivial that python's speed definitely isn't an issue. I have even sometimes seen people stop using a tool when they find out they were written in python - now all of a sudden they are unusably slow. Then they try to justify it by writing some loop in their favourite proper fast language and tell me how fast that tight loop is or they claim that some function is X times faster, but when I actually compile it and run something like hyperfine on it and python version the difference is hardly ever X since there is already so much more over head in a real world. reply samatman 16 hours agorootparentprevThere's no need to pretend Python has virtues which it lacks. It's not a fast language. It's fast enough for many purposes, sure, but it isn't fast, and this work is unlikely to change that. Faster, sure, and that's great. reply rmbyrro 16 hours agorootparentAlthough true, it doesn't mean they can't improve its performance. Working with threads is a pain in Python. If you want to spawn +10-20 threads in a process, it can quickly become way slower than running a single thread. Removing the GIL and refactoring some of the core will unlock levels of concurrency that are currently not feasible with Python. And that's a great deal, in my opinion. Well worth the trouble they're going through. reply bb88 15 hours agorootparentWorking with threads is a pain regardless of which language you use. Some might say: \"Use Go!\" Alas: https://songlh.github.io/paper/go-study.pdf After a couple decades of coding, I can say that threading is better if it's tightly controlled, limited to usages of tight parallelism of an algorithm. Where it doesn't work is in a generic worker pool where you need to put mutex locks around everything -- and then prod randomly deadlocks in ways the developer boxes can't recreate. reply jcranmer 15 hours agorootparent> After a couple decades of coding, I can say that threading is better if it's tightly controlled, limited to usages of tight parallelism of an algorithm. This may be a case of violent agreement, but there are a few clear cases where multithreading is easily viable. The best case is some sort of parallel-for construct, even if you include parallel reductions, although there may need to be some smarts around how to do the reduction (e.g., different methods for reduce-within-thread versus reduce-across-thread). You can extend this to heterogeneous parallel computations, a general, structured fork-join form of concurrency. But in both cases, you essentially have to forbid inter-thread communication between the fork and the join parameters. There's another case you might be able to make work, where you have a thread act as an internal server that runs all requests to completion before attempting to take on more work. What the paper you link to is pointing out, in short, is that message passing doesn't necessarily free you from the burden of shared-mutable-state-is-bad concurrency. The underlying problem is largely that communication between different threads (or even tasks within a thread) can only safely occur at a limited number of safe slots, and any communication outside of that is risky, be it an atomic RMW access, a mutex lock, or waiting on a message in a channel. reply bmitc 14 hours agorootparentprev> Working with threads is a pain regardless of which language you use. That's not true at all. F#, Elixir, Erlang, LabVIEW, and several other languages make it very easy. Python makes it incredibly tough. reply amethyst 14 hours agorootparent> Python makes it incredibly tough. I disagree, Python makes it incredibly easy to work with threads in many different ways. It just doesn't make threads faster. reply bmitc 14 hours agorootparentIn what way? Threading, asyncio, tasks, event loops, multiprocessing, etc. are all complicated and interact poorly if at all. In other languages, these are effectively the same thing, lighter weight, and actually use multicore. If I launch 50 threads with run away while loops in Python, it takes minutes to laumch and barely works after. I can run hundreds of thousands and even millions of runaway processes in Elixir/Erlang that launch very fast and processes keep chugging along just fine. reply rmbyrro 14 hours agorootparentprevThe whole purpose of threads is to improve overall speed of execution. Unless you're working with a very small number of threads (single digits), that's a very hard to achieve goal in Python. I wouldn't count this as easy to use. It's easy to program, yes, but not easy to get working with reasonably acceptable performance. reply bb88 5 hours agorootparentAnd the python people would just point to multiprocessing...which works pretty well. reply bmitc 2 hours agorootparentWhich has its own set of challenges and yet another implementation of queue. reply rmbyrro 14 hours agorootparentprevIt's not such a big pain in every language. And certainly not as hard to get working with acceptable performance in many languages. Even if you have zero shared resources, zero mutexes, no communication whatsoever between threads, it's a huge pain in Python if you need +10-ish threads going. And many times the GIL is the bottleneck. reply Shog9 9 hours agorootparentprevThis is where Python's GIL bit me: I was more than familiar with how to shoot myself in the foot using threads in other languages, and careful to avoid those traps. Threads spun up only in situations where they had their own work to do and well-defined conditions for how both failure and success would be reported back to the thread that requested it, along with a pool that wouldn't exceed available resources. Like every other language I've used this approach with, nothing bad happened - the program ran as expected and produced correct results. Unlike every other language, spreading calculations across multiple cores didn't appreciably improve performance. In some cases, it got slower. Eventually scrapped it all, and went with an approach closer to what I'd have done with C and fork() decades ago... Which, to Python's credit, was fairly painless and worked well. But it caught me off-guard, because with asyncio for IO-bound stuff, it didn't seem like threads really have much of a purpose in Python, other than to be a tripwire for unwary and overconfident folks like myself! reply bb88 5 hours agorootparentNot disagreeing. The only case for threading in python is for spinning something to handle IO. But now with async even that goes away. reply heinrich5991 15 hours agorootparentprevConcurrency with rayon in Rust isn't pain, I'd say. It's basically hidden away from the user. reply KaiserPro 12 hours agorootparentprev> If you want to spawn +10-20 threads in a process, it can quickly become way slower than running a single thread. as you know thats mostly threads in general. Any optimisation has a drawback so you need to choose wisely. I once made a horror of a thing that synced S3 with another S3, but not quite object store. I needed to move millions of files, but on the S3 like store every metadata operation took 3 seconds. So I started with async (pro tip: its never a good idea to use async. its basically gotos with two dimensions of surprise: 1 when the function returns, 2 when you get an exception ) I then moved to threads, which got a tiny bit extra performance, but much easier debugability. Then I moved to multiprocess pools of threads (fuck yeah super fast) but then I started hitting network IO limits. So then I busted out to airflow like system with operators spawning 10 processes with 500 threads. it wasnt very memory efficient, but it moved many thousands of files a second. reply scubbo 15 hours agorootparentprevThis is entirely fair, and I wish I'd been a little less grumpy in my initial reply (I assign some blame to just getting over an illness). Thank you for the gentle correction! That said - I think it's fair to be irritated by people who write Python off as entirely useless because it is not _the fastest_ language. As you rightly say - it's fast enough for many purposes. It does bother me to see Python immediately counted out of discussions because of its speed when the app in question is extremely insensitive to speed. reply Affric 10 hours agorootparentIt’s all about values. I have been on teams where Python based approaches were discounted due to “speed” and “industry best practice” and then had the very same engineers create programs that are slow by design in a “fast” language and introduce needless complexity (and bugs) through “faster” database processes. Like you said, it’s the thoughtless criticism. The meme. I am happy for Python to lose in a design analysis because it’s too slow for what we are building; I am loathe to let it lose because whoever is doing the analysis with me has heard it’s slow. Which is to say, I get what you’re saying. I think people have been a little ungenerous with your comment. reply scubbo 9 hours agorootparent> I think people have been a little ungenerous with your comment. Eh - I engaged with a fraught topic in a snarky way without clarifying that I meant the unintuitive-but-technically-literally-accurate interpretation of my words. Maybe some people have been less-generous than they could have been, but I don't begrudge it - if I look sufficiently like a troll, I won't complain when I get treated like one. Not everyone has the time and mental fortitude to treat everyone online with infinite patience and kindness - I know I sure don't. Thank you for the support, though! reply wongarsu 15 hours agorootparentprevIn some ways the weakness even was a virtue. Because Python threads are slow Python has incredible toolsets for multiprocess communication, task queues, job systems, etc. reply fragmede 11 hours agorootparentprev\"Faster, sure\" seems unnecessarily dismissive. That's the whole point of all this work. reply nick238 12 hours agorootparentprevMaybe it'll shut up \"architects\" who hack up a toy example in , drop it on a team to add all the actual features, tests, deployment strategy, and maintain, and fly away to swoop and poop on someone else. Gee thanks for your insight; this API serves maybe 1 request a second, tops. Glad we optimized for SPEEEEEED of service over speed of development. reply markhahn 16 hours agorootparentprevYou seem to be implying that there is something inherently slow to Python. What? This topic is an example: a detail of one particular implementation, since GIL is definitely not inherent to the language. Just the usual worry about looseness of types? reply doctorpangloss 15 hours agorootparentThere are worse hills to die on than this. But the Python ecosystem is very slow. It's a cultural thing. The biggest impact would be completely redoing package discovery. Not in some straightforward sense of \"what if PyPi showed you a Performance Measurement?\" No, that's symptomatic of the same problem: harebrained and simplistic stuff for the masses. But who's going to get rid of PyPi? Conda tried and it sucks, it doesn't change anything fundamental, they're too small and poor to matter. Meta should run its own package index and focus on setuptools. This is a decision PyTorch has already taken, maybe the most exciting package in Python today, and for all the headaches that decision causes, look: torch \"won,\" it is high performance Python with a vibrant high performance ecosystem. These same problems exist in NPM too. It isn't an engineering or language problem. Poetry and Conda are not solutions, they're symptoms. There are already too many ideas. The ecosystem already has too much manic energy spread way too thinly. Golang has \"fixed\" this problem as well as it could for non-commercial communities. reply pphysch 15 hours agorootparentThe \"Python ecosystem\" includes packages like numpy, pytorch & derivatives which are responsible for a large chunk of HPC and research computing nowadays. Or did you mean to say the \"Python language\"? reply doctorpangloss 13 hours agorootparent> The \"Python ecosystem\" includes packages like numpy, pytorch & derivatives which are responsible for a large chunk of HPC and research computing nowadays. The \"& derivatives\" part is the problem! Torch does not have derivatives. It won. You just use it and its extensions, and you're done. That is what people use to do exciting stuff in Python. It's the manic developers writing manic derivatives that make the Python ecosystem shitty. I mean I hate ragging on those guys, because they're really nice people who care a lot about X, but if only they could focus all their energy to work together! Python has like 20 ideas for accelerated computing. They all abruptly stopped mattering because of Torch. If the numba and numpy and scikit-learn and polars and pandas and... all those people, if they would focus on working on one package together, instead of reinventing the same thing over and over again - high level cross compilers or an HPC DSL or whatever, the ecosystem would be so much nicer and performance would be better. This idea that it's a million little ideas incubating and flourishing, it's cheerful and aesthetically pleasing but it isn't the truth. CUDA has been around for a long time, and it was obviously the fastest per dollar & watt HPC approach throughout its whole lifetime, so most of those little flourishing ideas were DOA. They should have all focused on Torch from the beginning instead of getting caught up in little manic compiler projects. We have enough compilers and languages and DSLs. I don't want another DataFrame DSL! I see this in new, influential Python projects made even now, in 2024. Library authors are always, constantly, reinventing the wheel because the development is driven by one person's manic energy more than anything else. Just go on GitHub and look how many packages are written by one person. GitHub & Git, PyPi are just not adequate ways to coordinate the energies of these manic developers on a single valuable task. They don't merge PRs, they stake out pleasing names on PyPi, and they complain relentlessly about other people's stuff. It's NIH syndrome on the 1m+ repository scale. reply fragmede 13 hours agorootparentyeah. like xkcd 927 to the nth degree. reply sneed_chucker 15 hours agorootparentprevCPython is slow. That's not really something you can dispute. It is a non-optimizing bytecode interpreter and it makes no use of JIT compilation. JavaScript with V8 or any other modern JIT JS engine runs circles around it. Go, Java, and C# are an order of magnitude faster but they have type systems that make optimizing compilation much easier. There's no language-inherent reason why Python can't be at least as fast as JavaScript. reply cozzyd 6 hours agorootparentJavaScript doesn't have to contend with a plethora of native extensions (which, to be fair, are generally a workaround for python slowness). reply mixmastamyk 15 hours agorootparentprevI've read that it can't even be as fast as JS, because everything is monkey-patchable at runtime. Maybe they can optimize for that when it doesn't happen, but remains to be seen. reply sneed_chucker 13 hours agorootparentI've heard similar claims but I don't think it's true. JavaScript is just as monkey-patchable. You can reassign class methods at runtime. You can even reassign an object's prototype. Existing Python JIT runtimes and compilers are already pretty fast. reply maple3142 9 hours agorootparentPython is probably much more monkey patchable. Almost any monkey patching that JavaScript supports also works in Python (e.g. modifying class prototype = assigning class methods), but there are a few things that only Python can do: accessing local variables as dict, access other stack frames, modifying function bytecode, read/write closure variables, patching builtins can change how the language works (__import__, __build_class__). Many of them can make a language hard to optimize. reply imtringued 1 hour agorootparentprevYou can always use optimistic optimization strategies where you profile the fast path and optimize that. When someone does something slow, you tell them to stop doing it if they want better performance. reply oivey 15 hours agorootparentprevPython is inherently slow. That’s why people tend to rewrite bits that need high performance in C/C++. Removing the GIL is a massively welcome change, but it isn’t going to make C extensions go away. reply IshKebab 15 hours agoparentprevWell, technically it still won't be able to use the full power of threads in many situations because (I assume) it doesn't have shared memory. It'll presumably be like Web Workers / isolates, so Go, C++, Rust, Zig, etc. will still have a fundamental advantage for most applications even ignoring Python's inherent slowness. Probably the right design though. reply Difwif 13 hours agorootparentWhy would you think it's not shared memory? Maybe I'm wrong here but by default Python's existing threading implementation uses shared memory. AFAIK we're just talking about removing the global interpreter lock. I'm pretty sure the threading library uses system threads. So running without the GIL means actual parallelism across system threads with shared memory access. reply IshKebab 12 hours agorootparentYeah I think you're right actually. Seems like they do per-object locking instead. reply znpy 15 hours agoparentprevI still hear the \"java slow\" meme from time to time... Memes are slow to die, sadly. Some people just won't catch on with the fact that java has had just-in-time compilation for like 15 years now (it was one of the first major platforms to get that), has had a fully concurrent garbage collector for a number of releases (zgc since java 11) and can be slimmed down a lot (jlink). I work on low-latency stuff and we routinely get server-side latencies in the order of single to low double-digit microseconds of latency. If python ever becomes fully concurrent (python threads being free of any kind of GIL) we'll see the \"python slow\" meme for a number of years... Also doesn't help that python gets updated very very slowly in the industry (although things are getting better). reply PhilipRoman 13 hours agorootparentI think java being slow has less to do with the implementation (which is pretty good) and more to do with the culture of overengineering (including in the standard library). Everything creates objects (which the JIT cannot fully eliminate, escape analysis is not magic), cache usage is abysmal. Framework writers do their best to defeat the compiler by abusing reflection. And all these abstractions are far from zero cost, which is why even the JDK has to have hardcoded special cases for Streams of primitives and ByteBuffers. Of course, if you have a simple fastpath you can make it fast in any language with a JIT, latency is also generally not an issue anymore, credit where credit is due - java GCs are light years ahead of everything else. Regarding jlink - my main complaint is that everything requires java.base which already is 175M. And thats not counting the VM, etc. But I don't actively work with java anymore so please correct me if there is a way to get smaller images. reply RyEgswuCsn 15 hours agorootparentprevI feel Java deserves better. When Python finally gets true thread concurrency, JIT (mamba and the like), comprehensive static analysis (type hints), and some sophisticated GC, and better performance, people will realise Java have had them all this time. reply thorncorona 13 hours agorootparentGraalVM is a pretty magical tool reply helsinki 17 hours agoprevWhile the title is correct, it is a bit misleading, because disabling the GIL breaks the asyncio tests. It's like saying the engine can be removed from my car. Sure, it can, but the car won't work. reply Kranar 17 hours agoparentWell this release will break any code that uses threads. The goal of this particular release is to work for thread-free programs. reply neilkk 16 hours agorootparentThis isn't correct. TFA said that small threaded programs had been run successfully, but that the test suite broke in asyncio. Async I/O and threads are two different things, and either can be present in real code without the other. reply wolletd 16 hours agorootparent\"small threaded programs had been run successfully\" I have ran a lot of programs containing race conditions successfully many times until I ran into an issue. reply Kranar 16 hours agorootparentprevNot quite sure what your comment means exactly or how it implies what I said is incorrect. At any rate, test_asyncio contains a lot of tests that involve threads and specifically thread safety between coroutines and those tests fail. As far as async I/O and threads being distinct, I mean sure that is true of a lot of features but people mix features together and mixing asyncio with threads will not work with this particular release. reply ollien 17 hours agorootparentprevHow do single-threaded programs benefit from a lack of GIL? reply Kranar 17 hours agorootparentThey don't benefit much from a lack of GIL, perhaps a small reduction in overhead. This feature is a first step towards being able to disable the GIL completely. It is intended to be implemented in a very conservative manner, bit by bit and so for this first step it should work for thread free code. reply kjqgqkejbfefn 17 hours agorootparentprevDisabling the GIL can unlock true multi-core parallelism for multi-threaded programs, but this requires code to be restructured for safe concurrency, which isn't that difficult it seems: > When we found out about the “nogil” fork of Python it took a single person less than half a working day to adjust the codebase to use this fork and the results were astonishing. Now we can focus on data acquisition system development rather than fine-tuning data exchange algorithms. https://peps.python.org/pep-0703/ reply kjqgqkejbfefn 16 hours agorootparent>We frequently battle issues with the Python GIL at DeepMind. In many of our applications, we would like to run on the order of 50-100 threads per process. However, we often see that even with fewer than 10 threads the GIL becomes the bottleneck. To work around this problem, we sometimes use subprocesses, but in many cases the inter-process communication becomes too big of an overhead. To deal with the GIL, we usually end up translating large parts of our Python codebase into C++. This is undesirable because it makes the code less accessible to researchers. reply actionfromafar 16 hours agorootparentMaybe they should look in to translating parts of their code base to Shedskin Python. It compiles (a subset of) Python to C++. reply logicchains 14 hours agorootparentHow's it different from Cython, which compiles a subset of Python to C or C++? reply actionfromafar 13 hours agorootparentShedskin has stricter typing, and about 10-100 times performance vs Cython. reply TylerE 16 hours agorootparentprevSpeed. Admittedly not quite as much so the way this patch is implemented, since it just short circuits the extra function calls, doesn’t omit them entirely. reply 0cf8612b2e1e 16 hours agorootparentRemoving the GIL results in slower execution. Without the guarantees of single thread action, the interpreter needs to utilize more locks under the hood. reply TylerE 13 hours agorootparentNot in single threaded code. reply 0cf8612b2e1e 13 hours agorootparentUmm, yes it does? For the longest time, Guido’s defense for the GIL was that all previous efforts resulted in an unacceptable hit to single threaded performance. Read PEP-703 (https://peps.python.org/pep-0703/#performance) where the performance hit is currently 5-8% reply TylerE 10 hours agorootparentThat's to make it thread safe without the GIL. If you only care about single thread there's all kinds of stuff you can do. reply jononor 9 hours agorootparentHow to ensure there are no other threads, confidently enough that one can turn thread safety off? reply Kranar 8 hours agorootparentFrom when I was reading the proposal, the idea is that until a C extension is loaded, you can assume that there are no other threads. Then when a module is loaded, by default you assume that it uses threads but modules that are thread free can indicate that using a flag, so if a module indicates it's thread free then you continue running without the thread safety features. reply gtirloni 17 hours agorootparentprevIt could remove the locking/unlocking operations. reply sapiogram 16 hours agorootparentRemoving the GIL requires more locking/unlocking operations. For single-threaded program, it's a performance penalty on average: https://peps.python.org/pep-0703/#performance reply Retr0id 17 hours agorootparentprevDoesn't removing the GIL imply adding back new, more granular locks? reply fiddlerwoaroof 17 hours agorootparentSort of, but the biased reference counting scheme they’re using avoids a lot of locks for the common case. reply protomikron 17 hours agorootparentprevThey don't. reply OskarS 17 hours agorootparentprevReally, any code? I thought they were adding fine-grained locks to the python objects themselves? Are you saying that if I share a python list between two threads and modify it on one and read it on the other, I can segfault python? reply Kranar 17 hours agorootparentWith this particular release, yes it will segfault. But down the road what you state is correct, this is just a first step towards that goal. reply thibaut_barrere 17 hours agorootparentprevCouldn’t it work if each threads only touch thread-specific data structures? reply petters 16 hours agoparentprevYou also need to compile Python with a special flag activated. It’s not only an environment variable or a command line option. reply ollien 17 hours agoparentprevI mean, you're not wrong, but also it's a huge feat to provide a toggle for a major feature like the GIL. Though, if it's just asyncio that's broken, perhaps it's not like removing your engine, but rather your antilock brakes :) EDIT: > [the test synchronous programs] all seem to run fine, and very basic threaded programs work, sometimes Perhaps this is closer to removing the oil pan reply znpy 16 hours agoparentprev> While the title is correct, it is a bit misleading, because disabling the GIL breaks the asyncio tests. It's like saying the engine can be removed from my car. Sure, it can, but the car won't work. You're not supposed to drive a car that hasn't got out of the research and development laboratory either, so there's that. reply pharrington 16 hours agoparentprevBeing able to remove the engine from my car with the push of a button would be a pretty amazing feature! reply jagged-chisel 16 hours agorootparentAnalogy breaking down and all, but … Only as long at it’s as easy to put back in reply TylerE 17 hours agoparentprevThis comment feels very disingenuous because non-threaded programs do in fact work. reply whalesalad 16 hours agoprevA great video tour of the GIL - https://www.youtube.com/watch?v=Obt-vMVdM8s reply lbj 16 hours agoprev[flagged] dang 16 hours agoparent\"Please don't post shallow dismissals, especially of other people's work. A good critical comment teaches us something.\" \"Don't be snarky.\" https://news.ycombinator.com/newsguidelines.html reply lbj 16 hours agorootparentFair enough, my apologies. reply dang 5 hours agorootparentAppreciated! reply behnamoh 16 hours agoprev [–] I've been programming in Python for over 6 years now and every week I learn something new. But recently I've been thinking about moving to a more capable language with proper concurrency for backend API requests (FastAPI sucked). I also want types, so Elixir is not in the picture. I dabbled in Rust a bit. Although I was able to get the hang of things and build a CLI tool pretty quickly, I'm worried I'll have to deal with numerous quirks later if I keep using Rust (like numerous string types). Is that something to be worried about if all I want from Rust is Python+Types+Concurrency? reply spprashant 14 hours agoparentAs someone in a similar boat, just pick Golang. People often dislike the basic syntax, explicit error checking and the lack of algebraic data types. I did. Rust just seems like it offers so much more and you fear missing out on something really cool. But once you get over it, you realize Golang has a good type system, concurrency model, package manager that's not pip, fast compile times, and static binaries. For most cases it will also offer great performance. It has everything you need to build APIs, CLI tools, web servers, microservices - pieces which will form the building blocks of your software infrastructure. I have heard numerous stories of people being productive in Go in a few days, sometimes even hours. If Python is 0 quality of life and Rust is a 100, Golang gets you all they way up to 80-90. That last bit is something you might never need. Rust is a great language and something I hope to be in proficient someday, but I ll save it for where I actually need that last microsecond of performance. reply imbusy111 16 hours agoparentprevSounds like you want Julia. It looks like Python, but also has, what you ask for. You can even run Python from Julia, so that alleviates the problem with a lack of libraries somewhat. reply seabrookmx 10 hours agoparentprevGoLang as others suggested is a good pick. If you're in the \"I don't like coloured functions\" camp it's likely your best bet for web workloads. If you're not a fan of GoLang's spartan syntax and are cool with async/await, C#/dotnet core is a great experience on all platforms. IMO it has the best async/await implementation (it originated there) on top of a multi-threaded event loop. ASP.NET is a great web framework and it has great library support for everything else. As someone who avoids \"traditional\" ORMs (Hybernate, Django) I really like Dapper. reply archargelod 8 hours agoparentprevYou might wanna try Nim [0]. Nim is a statically-typed compiled language with very pythonic syntax. It's easy to learn, especially if you already know Python, because Nim's stdlib is heavily inspired by it. For multithreading in Nim see: Weave - https://github.com/mratsim/weave Malebolgia - https://github.com/Araq/malebolgia [0] - https://nim-lang.org reply sparks1970 16 hours agoparentprevGolang? Pretty easy to pick up coming from Python and proper concurrency. reply scubbo 16 hours agoparentprevPython supports types! https://www.mypy-lang.org/ reply cmeacham98 16 hours agorootparentThis seems a bit like saying \"JavaScript supports types!\" because of typescript. reply ralphist 16 hours agorootparentIt's not a separate language, you can just start typing your programs right now. reply nextlevelwizard 13 hours agorootparentexcept nothing enforces your types at run time, you can have typi hints all you want and everyone else cn ignore them reply aunderscored 12 hours agorootparentSame exists with laws. And in most languages if I want to avoid the type system and hand you a goldfish instead of an int, I can. It just may take more effort. Other language will blow up too if you hand them strange things. Just like python those languages have ways to verify you're not passing goldfish, You just may need more or less effort to use them reply nextlevelwizard 3 hours agorootparentThats a lot of words for: yeah, you are right reply nick238 12 hours agorootparentprevPython actually has type safety though, as you can't do `'1' + 1` like in JS (not that a linter wouldn't scream at you). If I hear another \"I compileso I know it will work, but you can't do that in Python\" I'll lose it. Having the compiler not complain that the types match is not effing \"testing\". reply scubbo 11 hours agorootparent> Having the compiler not complain that the types match is not effing \"testing\". It absolutely is - it's just testing at a _very_ low level of correctness, and is not sufficient for testing actual high-level functionality. reply wiseowise 15 hours agorootparentprevBoth true. What’s wrong with them? reply posix_monad 15 hours agorootparentprevTypes are way more mainstream in the JS ecosystem than they are in the Python ecosystem. If you want a \"scripty\" language with types, then TypeScript is a reasonable choice. reply jondwillis 16 hours agoparentprevSwift or Kotlin might be what you’re looking for but nobody uses Swift for backend really, and I’m not sure about Kotlin. reply kuschku 16 hours agorootparentKotlin has basically replaced java for many spring shops. It's really common in backend nowadays. reply rightbyte 16 hours agoparentprev> Python+Types+Concurrency Sounds like Groovy. But I wouldn't recommend it. Also the career padding hype is gone. reply docc 15 hours agoparentprevi moved from python timo Rust and i really like it. moving to a staticly typed language is bit of a mind fuck but cargo is amazing and so is the speed and portability of rust. reply neonsunset 9 hours agoparentprevStrong recommendation to look into ASP.NET Core with C#. It gives static typing, easy first-class concurrency, it runs on all platforms and now can be AOT compiled if that's your thing, if not - can still produce just a single executable if you choose to. Also its CLI is nice and similar to cargo. Naturally, you won't be having performance issues. reply willcipriano 16 hours agoparentprev [–] What sucked about FastAPI? reply behnamoh 16 hours agorootparent [–] Not fast enough! I used it to call llama.cpp server but it would crash if requests were \"too fast\". Calling the llama.cpp server directly solved the issue. reply ralphist 16 hours agorootparentDid you get \"connection reset by peer\" when you sent a bit too many requests perchance? I've never found the source of that in my programs. There's no server logging about it, connections are just rejected. None of the docs talk about this. reply behnamoh 15 hours agorootparentI don't remember the exact error name but the FastAPI server would just freeze. reply dekhn 14 hours agorootparenta freeze is not a crash. reply KaiserPro 12 hours agorootparentprevInteresting, I've used fastAPI to serve many thousands of requests a second (per process) for a production system. How were you buffering the requests? reply jononor 9 hours agorootparentprevYou ran FastAPI app proper server like Uvicorn? reply dumdedum 15 hours agorootparentprevThis sounds like a layer 8 problem reply dekhn 16 hours agorootparentprev [–] Honestly, that doesn't sound right. I'm curious what you mean by crash thoguh. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "A new feature in Python/Cpython enables disabling the Global Interpreter Lock (GIL) through PYTHON_GIL=0 environment variable or -X gil=0 option.",
      "This feature is accessible in free-threaded builds and involves additional work to re-enable the GIL when incompatible extensions are loaded.",
      "Test cases have been introduced to confirm the effectiveness of this new capability."
    ],
    "commentSummary": [
      "GitHub discussions focus on enhancing Python's speed by debating the possible disabling of the Global Interpreter Lock (GIL).\"",
      "Comparisons are drawn between Python, Java, and C++; diverse tools like Mojo, Pytorch, and Taichi are discussed.\"",
      "The conversation delves into language complexity, evolution, learning curves, concurrency, and alternatives such as Go, Rust, and C#, highlighting trade-offs in speed, ease of use, and performance when choosing a programming language.\""
    ],
    "points": 433,
    "commentCount": 218,
    "retryCount": 0,
    "time": 1710174077
  },
  {
    "id": 39672450,
    "title": "Diffusion Models: A New Theoretical Perspective",
    "originLink": "https://www.chenyang.co/diffusion.html",
    "originBody": "Diffusion models from scratch, from a new theoretical perspective Diffusion models have recently produced impressive results in generative modeling, in particular sampling from multimodal distributions. Not only has diffusion models seen widespread adoption in text-to-image generation tools such as Stable Diffusion, they also excel in other application domains such as audio/video/3D generation, protein design, robotics path planning, all of which require sampling from multimodal distributions. This tutorial aims to introduce diffusion models from an optimization perspective as introduced in our paper. It will go over both theory and code, using the theory to explain how to implement diffusion models from scratch. By the end of the tutorial, you will learn how to implement training and sampling code for a toy dataset, which will also work for larger datasets and models. In this tutorial we will mainly reference code from smalldiffusion. For pedagogical purposes, the code presented here will be simplified from the original library code, which is on its own well-commented and easy to read. Training diffusion models Diffusion models aim to generate samples from a set that is learned from training examples, which we will denote by 𝐾 . For example, if we want to generate images, 𝐾 ⊂ 𝑅 𝑐 × ℎ × 𝑤 is the set of pixel values that correspond to realistic images. Diffusion models also work for 𝐾 corresponding to modalities other than images, such as audio, video, robot trajectories, and even in discrete domains such as text generation. In a nutshell, diffusion models are trained by: Sampling 𝑥 0 ∼ 𝐾 , noise level 𝜎 ∼ [ 𝜎 min , 𝜎 max ] , noise 𝜖 ∼ 𝑁 ( 0 , 𝐼 ) Generating noisy data 𝑥 𝜎 = 𝑥 0 + 𝜎 𝜖 Predicting 𝜖 (direction of noise) from 𝑥 𝜎 by minimizing squared loss This amounts to training a 𝜃 -parameterized neural network 𝜖 𝜃 ( 𝑥 , 𝜎 ) , by minimizing the loss function 𝐿 ( 𝜃 ) = 𝐸 ⁡ ‖ 𝜖 𝜃 ( 𝑥 0 + 𝜎 𝑡 𝜖 , 𝜎 𝑡 ) − 𝜖 ‖ 2 In practice, this is done by the following simple training_loop: def training_loop(loader : DataLoader, model : nn.Module, schedule: Schedule, epochs : int = 10000): optimizer = torch.optim.Adam(model.parameters()) for _ in range(epochs): for x0 in loader: optimizer.zero_grad() sigma, eps = generate_train_sample(x0, schedule) eps_hat = model(x0 + sigma * eps, sigma) loss = nn.MSELoss()(eps_hat, eps) optimizer.backward(loss) optimizer.step() The training loop iterates over batches of x0, then samples noise level sigma and noise vector eps using generate_train_sample: def generate_train_sample(x0: torch.FloatTensor, schedule: Schedule): sigma = schedule.sample_batch(x0) eps = torch.randn_like(x0) return sigma, eps Noise schedules In practice, 𝜎 is not sampled uniformly from the interval [ 𝜎 min , 𝜎 max ] , instead this interval is discretized into 𝑁 distinct values called a 𝜎 schedule: { 𝜎 𝑡 } 𝑡 = 1 𝑁 , and 𝜎 is instead sampled uniformly from the 𝑁 possible values of 𝜎 𝑡 . We define the Schedule class that encapsulates the list of possible sigmas, and sample from this list during training. class Schedule: def __init__(self, sigmas: torch.FloatTensor): self.sigmas = sigmas def __getitem__(self, i) -> torch.FloatTensor: return self.sigmas[i] def __len__(self) -> int: return len(self.sigmas) def sample_batch(self, x0:torch.FloatTensor) -> torch.FloatTensor: return self[torch.randint(len(self), (x0.shape[0],))].to(x0) In this tutorial, we will use a log-linear schedule defined below: class ScheduleLogLinear(Schedule): def __init__(self, N: int, sigma_min: float=0.02, sigma_max: float=10): super().__init__(torch.logspace(math.log10(sigma_min), math.log10(sigma_max), N)) Other commonly used schedules include ScheduleDDPM for pixel-space diffusion models and ScheduleLDM for latent diffusion models such as Stable Diffusion. The following plot compares these three schedules with default parameters. A comparison plot of different diffusion schedules Toy example In this tutorial we will start with a toy dataset used in one of the first diffusion papers [Sohl-Dickstein et.al. 2015], where 𝐾 ⊂ 𝑅 2 are points sampled from a spiral. We first construct and visualize this dataset: dataset = Swissroll(np.pi/2, 5*np.pi, 100) loader = DataLoader(dataset, batch_size=2048) Swissroll toy dataset For this simple dataset, we can implement the denoiser using a multi-layer perceptron (MLP): def get_sigma_embeds(sigma): sigma = sigma.unsqueeze(1) return torch.cat([torch.sin(torch.log(sigma)/2), torch.cos(torch.log(sigma)/2)], dim=1) class TimeInputMLP(nn.Module): def __init__(self, dim, hidden_dims): super().__init__() layers = [] for in_dim, out_dim in pairwise((dim + 2,) + hidden_dims): layers.extend([nn.Linear(in_dim, out_dim), nn.GELU()]) layers.append(nn.Linear(hidden_dims[-1], dim)) self.net = nn.Sequential(*layers) self.input_dims = (dim,) def rand_input(self, batchsize): return torch.randn((batchsize,) + self.input_dims) def forward(self, x, sigma): sigma_embeds = get_sigma_embeds(sigma) # shape: b x 2 nn_input = torch.cat([x, sigma_embeds], dim=1) # shape: b x (dim + 2) return self.net(nn_input) model = TimeInputMLP(dim=2, hidden_dims=(16,128,128,128,128,16)) The MLP takes the concatenation of 𝑥 ∈ 𝑅 2 and an embedding of the noise level 𝜎 , then predicts the noise 𝜖 ∈ 𝑅 2 . Although many diffusion models use a sinusoidal positional embedding for 𝜎 , the simple two-dimensional embedding works just as well: Two-dimensional 𝜎 𝑡 embedding Now we have all the ingredients to train a diffusion model. schedule = ScheduleLogLinear(N=200, sigma_min=0.005, sigma_max=10) trainer = training_loop(loader, model, schedule, epochs=15000) losses = [ns.loss.item() for ns in trainer] Training loss over 15000 epochs, smoothed with moving average The learned denoiser 𝜖 𝜃 ( 𝑥 , 𝜎 ) can be visualized as a vector field parameterized by the noise level 𝜎 , by plotting 𝑥 − 𝜎 𝜖 𝜃 ( 𝑥 , 𝜎 ) for different 𝑥 and levels of 𝜎 . Plot of predicted 𝑥 ^ 0 = 𝑥 − 𝜎 𝜖 𝜃 ( 𝑥 , 𝜎 ) for different 𝑥 and 𝜎 In the plots above, the arrows point from each noisy datapoint 𝑥 to the “clean” datapoint predicted by the denoiser with noise level 𝜎 . At high levels of 𝜎 , the denoiser tends to predict the mean of the data, but at low noise levels the denoiser predicts actual data points, provided that its input 𝑥 is also close to the data. How do we interpret what the denoiser is learning, and how do we create a procedure to sample from diffusion models? We will next build a theory of diffusion models, then draw on this theory to derive sampling algorithms. Denoising as approximate projection The diffusion training procedure learns a denoiser 𝜖 𝜃 ( 𝑥 , 𝜎 ) . In our paper, we interpret the learned denoiser as an approximate projection to the data manifold 𝐾 , and the goal of the diffusion process as minimizing the distance to 𝐾 . This motivates us to introduce a relative-error approximation model to analyze the convergence of diffusion sampling algorithms. First we introduce some basic properties of distance and projection functions. Distance and projection functions The distance function to a set 𝐾 ⊆ 𝑅 𝑛 is defined as d i s t 𝐾 ( 𝑥 ) := min { ‖ 𝑥 − 𝑥 0 ‖ : 𝑥 0 ∈ 𝐾 } . The projection of 𝑥 ∈ 𝑅 𝑛 , denoted p r o j 𝐾 ( 𝑥 ) , is the set of points that attain this distance: p r o j 𝐾 ( 𝑥 ) := { 𝑥 0 ∈ 𝐾 : d i s t 𝐾 ( 𝑥 ) = ‖ 𝑥 − 𝑥 0 ‖ } If p r o j 𝐾 ( 𝑥 ) is unique, the gradient of d i s t 𝐾 ( 𝑥 ) , the direction of steepest descent of the distance function, points towards this unique projection: Proposition Suppose 𝐾 is closed and 𝑥 ∉ 𝐾 . If p r o j 𝐾 ( 𝑥 ) is unique, then ∇ 1 2 d i s t 𝐾 ( 𝑥 ) 2 = d i s t 𝐾 ( 𝑥 ) ∇ d i s t 𝐾 ( 𝑥 ) = 𝑥 − p r o j 𝐾 ( 𝑥 ) . This tells us that if we can learn ∇ d i s t 𝐾 ( 𝑥 ) for every 𝑥 , we can simply move in this direction to find the projection of 𝑥 onto 𝐾 . One issue with learning this gradient is that d i s t 𝐾 is not differentiable everywhere, thus ∇ d i s t 𝐾 is not a continuous function. To solve this problem, we introduce a squared-distance function smoothed by a parameter 𝜎 using the softmin operator instead of min . dist 𝐾 2 ( 𝑥 , 𝜎 ) := softmin 𝜎 2 𝑥 0 ∈ 𝐾 ‖ 𝑥 0 − 𝑥 ‖ 2 = − 𝜎 2 log ⁡ ( ∑ 𝑥 0 ∈ 𝐾 exp ⁡ ( − ‖ 𝑥 0 − 𝑥 ‖ 2 2 𝜎 2 ) ) The following picture from [Madan and Levin 2022] shows the contours of both the distance function and its smoothed version. Smoothed distance function has continuous gradients From this picture we can see that ∇ d i s t 𝐾 ( 𝑥 ) points toward the closest point to 𝑥 in 𝐾 , and ∇ dist 2 ( 𝑥 , 𝜎 ) points toward a weighted average of points in 𝐾 determined by 𝑥 . Ideal denoiser The ideal or optimal denoiser 𝜖 ∗ for a particular noise level 𝜎 is an exact minimizer of the training loss function. When the data is a discrete uniform distribution over a finite set 𝐾 , the ideal denoiser has an exact closed-form expression given by: 𝜖 ∗ ( 𝑥 𝜎 , 𝜎 ) = ∑ 𝑥 0 ∈ 𝐾 ( 𝑥 𝜎 − 𝑥 0 ) exp ⁡ ( − ‖ 𝑥 𝜎 − 𝑥 0 ‖ 2 / 2 𝜎 2 ) 𝜎 ∑ 𝑥 0 ∈ 𝐾 exp ⁡ ( − ‖ 𝑥 𝜎 − 𝑥 0 ‖ 2 / 2 𝜎 2 ) From the above expression, we see that the ideal denoiser points towards a weighted mean of all the datapoints in 𝐾 , where the weight for each 𝑥 0 ∈ 𝐾 determines the distance to 𝑥 0 . Using this expression, we can also implement the ideal denoiser, which is computationally tractable for small datasets: def sq_norm(M, k): # M: b x n --(norm)--> b --(repeat)--> b x k return (torch.norm(M, dim=1)**2).unsqueeze(1).repeat(1,k) class IdealDenoiser: def __init__(self, dataset: torch.utils.data.Dataset): self.data = torch.stack(list(dataset)) def __call__(self, x, sigma): x = x.flatten(start_dim=1) d = self.data.flatten(start_dim=1) xb, db = x.shape[0], d.shape[0] sq_diffs = sq_norm(x, db) + sq_norm(d, xb).T - 2 * x @ d.T weights = torch.nn.functional.softmax(-sq_diffs/2/sigma**2, dim=1) return (x - torch.einsum('ij,j...->i...', weights, self.data))/sigma For our toy dataset, we can plot the direction of 𝜖 ∗ as predicted by the ideal denoiser for different noise levels 𝜎 : Plot of direction of 𝜖 ∗ ( 𝑥 , 𝜎 ) for different 𝑥 and 𝜎 From our plots we see that for large values of 𝜎 , 𝜖 ∗ points towards the mean of the data, but for smaller values of 𝜎 , 𝜖 ∗ points towards the nearest data-point. One insight from our paper is that the ideal denoiser for a fixed 𝜎 is equivalent to the gradient of a 𝜎 -smoothed squared-distance function: Theorem For all 𝜎 > 0 and 𝑥 ∈ 𝑅 𝑛 , we have 1 2 ∇ 𝑥 dist 𝐾 2 ( 𝑥 , 𝜎 ) = 𝜎 𝜖 ∗ ( 𝑥 , 𝜎 ) . This tells us that the ideal denoiser found by minimizing the diffusion training objective 𝐿 ( 𝜃 ) is in fact the gradient of a smoothed squared-distance function to the underlying data manifold 𝐾 . This connection is key to motivating our interpretation that the denoiser is an approximate projection. Relative error model In order to analyze the convergence of diffusion sampling algorithms, we introduced a relative error model which states that the projection predicted by the denoiser 𝑥 − 𝜎 𝜖 𝜃 ( 𝑥 , 𝜎 ) well approximates p r o j 𝐾 ( 𝑥 ) when the input to the denoiser 𝜎 well estimates d i s t 𝐾 ( 𝑥 ) / 𝑛 . For constants 1 > 𝜂 ≥ 0 and 𝜈 ≥ 1 , we assume that ‖ 𝑥 − 𝜎 𝜖 𝜃 ( 𝑥 , 𝜎 ) − p r o j 𝐾 ( 𝑥 ) ‖ ≤ 𝜂 d i s t 𝐾 ( 𝑥 ) when ( 𝑥 , 𝜎 ) satisfies 1 𝜈 d i s t 𝐾 ( 𝑥 ) ≤ 𝑛 𝜎 ≤ 𝜈 d i s t 𝐾 ( 𝑥 ) . In addition to the discussion about ideal denoisers above, this error model is motivated by the following observations. Low noise When 𝜎 is small and the manifold hypothesis holds, denoising approximates projection because most of the added noise is orthogonal to the data manifold. When added noise is small, most of noise is orthogonal to tangent space of manifold. Under the manifold hypothesis, denoising is approximately projection. High noise When 𝜎 is large relative to the diameter of 𝐾 , then any denoiser predicting any weighted mean of the data 𝐾 has small relative error. When added noise is large compared to diameter of data, denoising and projection point in the same direction We also perform empirical tests of our error model for pre-trained diffusion models on image datasets. The CIFAR-10 dataset is small enough for tractable computation of the ideal denoiser. Our experiments show that for this dataset, the relative error between the exact projection and ideal denoiser output is small over sampling trajectories. Ideal denoiser well-approximates projection onto the CIFAR-10 dataset under relative-error model Sampling from diffusion models Given a learned denoiser 𝜖 𝜃 ( 𝑥 , 𝜎 ) , how do we sample from it to obtain a point 𝑥 0 ∈ 𝐾 ? Given noisy 𝑥 𝑡 and noise level 𝜎 𝑡 , the denoiser 𝜖 𝜃 ( 𝑥 𝑡 , 𝜎 𝑡 ) predicts 𝑥 0 via 𝑥 ^ 0 𝑡 := 𝑥 𝑡 − 𝜎 𝑡 𝜖 𝜃 ( 𝑥 𝑡 , 𝜎 𝑡 ) Intuition from the relative error assumption tells us that we want to start with ( 𝑥 𝑇 , 𝜎 𝑇 ) where d i s t 𝐾 ( 𝑥 𝑇 ) / 𝑛 ≈ 𝜎 𝑇 . This is achieved by choosing 𝜎 𝑇 to be large relative to the diameter of 𝐾 , and 𝑥 𝑇 sampled i.i.d. from 𝑁 ( 0 , 𝜎 𝑇 ) , a Gaussian with variance 𝜎 𝑇 . This ensures that 𝑥 𝑇 is far away from 𝐾 . Although 𝑥 ^ 0 𝑇 = 𝑥 𝑇 − 𝜎 𝑇 𝜖 𝜃 ( 𝑥 𝑇 , 𝜎 𝑇 ) has small relative error, the absolute error d i s t 𝐾 ( 𝑥 ^ 0 𝑇 ) can still be large as d i s t 𝐾 ( 𝑥 𝑇 ) is large. In fact, at high noise levels, the expression of the ideal denoiser tells us that 𝑥 ^ 0 𝑇 should be close to the mean of the data 𝐾 . We cannot obtain a sample close to 𝐾 with a single call to the denoiser. Sampling process iteratively calls the denoiser based on 𝜎 𝑡 schedule. Thus we want to iteratively call the denoiser to obtain a sequence 𝑥 𝑇 , … , 𝑥 𝑡 , … 𝑥 0 using a pre-specified schedule of 𝜎 𝑡 , hoping that d i s t 𝐾 ( 𝑥 𝑡 ) decreases in concert with 𝜎 𝑡 . 𝑥 𝑡 − 1 = 𝑥 𝑡 − ( 𝜎 𝑡 − 𝜎 𝑡 − 1 ) 𝜖 𝜃 ( 𝑥 𝑡 , 𝜎 𝑡 ) This is exactly the deterministic DDIM sampling algorithm, though presented in different coordinates through a change of variable. See Appendix A of our paper for more details and a proof of equivalence. Diffusion sampling as distance minimization We can interpret the diffusion sampling iterations as gradient descent on the squared-distance function 𝑓 ( 𝑥 ) = 1 2 d i s t 𝐾 ( 𝑥 ) 2 . In a nutshell, DDIM is approximate gradient descent on 𝑓 ( 𝑥 ) with stepsize 1 − 𝜎 𝑡 − 1 / 𝜎 𝑡 , with ∇ 𝑓 ( 𝑥 𝑡 ) estimated by 𝜖 𝜃 ( 𝑥 𝑡 , 𝜎 𝑡 ) . How should we choose the 𝜎 𝑡 schedule? This determines the number and size of gradient steps we take during sampling. If there are too few steps, d i s t 𝐾 ( 𝑥 𝑡 ) might not decrease and the algorithm may not converge. On the other hand, if we take many small steps, we need to evaluate the denoiser for as many times, a computationally expensive operation. This motivates our definition of admissible schedules. Definition An admissible schedule { 𝜎 𝑡 } 𝑡 = 0 𝑇 ensures 1 𝜈 d i s t 𝐾 ( 𝑥 𝑡 ) ≤ 𝑛 𝜎 𝑡 ≤ 𝜈 d i s t 𝐾 ( 𝑥 𝑡 ) holds at each iteration. In particular, a geometrically decreasing (i.e. log-linear) sequence of 𝜎 𝑡 is an admissible schedule. Our main theorem states that if { 𝜎 𝑡 } 𝑡 = 0 𝑇 is an admissible schedule and 𝜖 𝜃 ( 𝑥 𝑡 , 𝜎 𝑡 ) satisfies our relative error model, the relative error can be controlled, and the sampling procedure aiming to minimize distance converges. Theorem Let 𝑥 𝑡 denote the sequence generated by DDIM and suppose that ∇ d i s t 𝐾 ( 𝑥 ) exists for all 𝑥 𝑡 and d i s t 𝐾 ( 𝑥 𝑇 ) = 𝑛 𝜎 𝑇 . Then 𝑥 𝑡 is generated by gradient descent on the squared-distance function with stepsize 1 − 𝜎 𝑡 − 1 / 𝜎 𝑡 d i s t 𝐾 ( 𝑥 𝑡 ) / 𝑛 ≈ 𝜎 𝑡 for all 𝑡 Coming back to our toy example, we can find an admissible schedule by subsampling from the original log-linear schedule, and implement the DDIM sampler as follows: class Schedule: ... def sample_sigmas(self, steps: int) -> torch.FloatTensor: indices = list((len(self) * (1 - np.arange(0, steps)/steps)) .round().astype(np.int64) - 1) return self[indices + [0]] batchsize = 2000 sigmas = schedule.sample_sigmas(20) xt = model.rand_input(batchsize) * sigmas[0] for sig, sig_prev in pairwise(sigmas): eps = model(xt, sig.to(xt)) xt -= (sig - sig_prev) * eps Samples from 20-step DDIM Improved sampler with gradient estimation Next, we use our interpretation to derive a new efficient sampler. Since ∇ d i s t 𝐾 ( 𝑥 ) is invariant between 𝑥 and p r o j 𝐾 ( 𝑥 ) , we aim to minimize estimation error 𝑛 ∇ d i s t 𝐾 ( 𝑥 ) − 𝜖 𝜃 ( 𝑥 𝑡 , 𝜎 𝑡 ) with the update: 𝜖 ¯ 𝑡 = 𝛾 𝜖 𝜃 ( 𝑥 𝑡 , 𝜎 𝑡 ) + ( 1 − 𝛾 ) 𝜖 𝜃 ( 𝑥 𝑡 + 1 , 𝜎 𝑡 + 1 ) Intuitively, this update corrects any error made in the previous step using the current estimate: Our gradient estimation update step This leads to faster convergence compared to the DDIM sampler, as seen from the samples on our toy model lying closer to the original data. Samples from 20-step gradient estimation sampler Compared to the default DDIM sampler, our sampler can be interpreted as adding momentum, causing the trajectory to potentially overshoot but converge faster. Sampling trajectories varying momentum term 𝛾 Empirically, adding noise during the generation process also improves the sampling quality. In order to do so while sticking to our original 𝜎 𝑡 schedule, we need to denoise to a smaller 𝜎 𝑡 ′ then add back noise 𝑤 𝑡 ∼ 𝑁 ( 0 , 𝐼 ) . 𝑥 𝑡 − 1 = 𝑥 𝑡 − ( 𝜎 𝑡 − 𝜎 𝑡 ′ ) 𝜖 𝜃 ( 𝑥 𝑡 , 𝜎 𝑡 ) + 𝜂 𝑤 𝑡 If we assume that 𝐸 ⁡ ‖ 𝑤 𝑡 ‖ 2 = ‖ 𝜖 𝜃 ( 𝑥 𝑡 , 𝜎 𝑡 ) ‖ 2 , we should choose 𝜂 so that the norm of the update is constant in expectation. This leads to the choice of 𝜎 𝑡 − 1 = 𝜎 𝑡 𝜇 𝜎 𝑡 − 1 1 − 𝜇 and 𝜂 = 𝜎 𝑡 − 1 2 − 𝜎 𝑡 ′ 2 where 0 ≤ 𝜇 = 1 mu : float = 0., # Requires mu in [0, 1) xt : Optional[torch.FloatTensor] = None, batchsize : int = 1): xt = model.rand_input(batchsize) * sigmas[0] eps = None for i, (sig, sig_prev) in enumerate(pairwise(sigmas)): eps, eps_prev = model(xt, sig.to(xt)), eps eps_av = eps * gam + eps_prev * (1-gam) if i > 0 else eps sig_p = (sig_prev/sig**mu)**(1/(1-mu)) # sig_prev == sig**mu sig_p**(1-mu) eta = (sig_prev**2 - sig_p**2).sqrt() xt = xt - (sig - sig_p) * eps_av + eta * model.rand_input(batchsize).to(xt) yield xt Large-scale examples The training code above not only works for our toy dataset, they can also be used to train image diffusion models from scratch. See this example for an example of training on the FashionMNIST dataset to get a second-place FID score on this leaderboard: Samples from a diffusion model trained on the FashionMNIST dataset The sampling code works without modifications to sample from state-of-the-art pretrained latent diffusion models: schedule = ScheduleLDM(1000) model = ModelLatentDiffusion('stabilityai/stable-diffusion-2-1-base') model.set_text_condition('An astronaut riding a horse') *xts, x0 = samples(model, schedule.sample_sigmas(50)) decoded = model.decode_latents(x0) We can visualize the different effects of our momentum term 𝛾 on high resolution text-to-image generation. Samples from a pretrained stable diffusion model Other resources Also recommended are the following blog posts on diffusion models: What are diffusion models introduces diffusion models from the discrete-time perspective of reversing a Markov process Generative modeling by estimating gradients of the data distribution introduces diffusion models from the continuous time perspective of reversing a stochastic differential equation The annotated diffusion model goes over a pytorch implementation of a diffusion model in detail Citation If you found this blog useful, please consider citing our paper: @article{permenter2023interpreting, title={Interpreting and improving diffusion models using the euclidean distance function}, author={Permenter, Frank and Yuan, Chenyang}, journal={arXiv preprint arXiv:2306.04848}, year={2023} }",
    "commentLink": "https://news.ycombinator.com/item?id=39672450",
    "commentBody": "Diffusion models from scratch, from a new theoretical perspective (chenyang.co)291 points by jxmorris12 14 hours agohidepastfavorite36 comments ycy 12 hours agoAuthor here, when I tried to understand diffusion models I realized that the code and math can be greatly simplified, which led to me writing this blog post and diffusion library. Happy to answer any questions. reply godelski 10 hours agoparentAs a researcher, there's a lot of diffusion blogs that I do not like. But I actually really do like this one! It does a great job at getting to the meat of things, showing some of the complexities (often missing) but without getting lost or distracted. I especially like the discussion of trajectories as this is motivating to many things I think a lot of people struggle with (e.g. with schedulers). That can be hard to write. Albeit not as complete, I think this is far more approachable than the Song or Lilian's blogs (great resources too, but wrong audience). Great job! I'm actually going to recommend this to others. FWIW, a friend of mine (and not \"a friend of mine\") wrote this minimal diffusion awhile back that I've found useful that's a bit more \"full\" w.r.t DDPM. More dropping here since I saw others excited to see code and it could provide good synergy here: https://github.com/VSehwag/minimal-diffusion/ reply DinaCoder98 9 hours agorootparent> FWIW, a friend of mine (and not \"a friend of mine\") What? Do you mean a coworker or something as compared to a friend? reply lucubratory 8 hours agorootparentThey are clarifying that \"a friend of mine\" is not a euphemism for themselves, because it's more common to use it as a euphemism for yourself than it is to actually talk to strangers online about your friend's life, problems, opinions etc. reply DinaCoder98 1 hour agorootparentOh, huh. I now recognize what you're referring to but I never would have realized that without being explicitly told so. Thank you. reply thomasahle 11 hours agoparentprevYour `get_sigma_embeds(batches, sigma)` seems to not use its first input? Did you mean to broadcast sigma to shape (batches, 1)? reply ycy 10 hours agorootparentMy intention was to omit the details of batching in the blog post for clarity of exposition, and I'll update the post accordingly. Sorry for the confusion, but you can see the full implementation here: https://github.com/yuanchenyang/smalldiffusion/blob/main/src... reply 3abiton 9 hours agoparentprevI am curious if any of these concepts derive somehow from some physics principles? Like the same way neural networks are modeled after biological neural networks? Maybe you have some insights on that conception? reply ks2048 7 hours agoparentprevThis looks great! How long does it take (on what hardware) to train the toy models? Such as the `fashion_mnist.py` example? Thanks. reply ycy 7 hours agorootparentThe 2d toy models such as the swissroll takes 2 mins to train on the CPU, whereas the fashionMNIST model takes a couple hours on any modern GPU reply xchip 12 hours agoparentprevYour post is awesome and is explaining something nobody else did, thanks! reply porphyra 7 hours agoprevAnother great post is also called Diffusion Models From Scratch: https://www.tonyduan.com/diffusion/index.html That goes into a lot more mathematical detail but is also accompanied by a minimal, very easy to understandHow long before more legible code or code-like constructs (or markup?) start to displace arcane mathematical notation? My best guess is never. The problem is that math is a language itself. In fact, I'd more accurately call it a family of languages. People often confuse it for an inherent thing because most of the time we use it in applied ways for subjects like physics, and the sciences. But clearly those people have never taken abstract algebra or developed their own toy algebra (or group, ring (or rung), or field). You might say they have an _ideal_istic notion of mathematics. That said, I am absolutely sympathetic to the complexity of mathematics and there is so much that is difficult to grasp. What's worse, is that when this abstract gobbledy goop finally clicks it's so abundantly obvious that it deceives you into thinking you've known it all along. I think this contributes to the difficulty of teaching the subject because those qualified have forgotten the struggles they themselves have had (to be fair, human psychology plays a big role here too because if we only remember the struggles we'd have difficulty finding the passion and beauty). But I think our best hope is to find a unified mathematics. The obsession people have with things like set theory and category theory is that you can see this interconnectedness. You get this deep feeling that you're looking at the same things manifesting in different ways. Remember that a lot of mathematics is converting your system into another system (we might call this an isomorphic mapping) such that your problem is easier to solve in the other system (think about how we might want to go from Cartesian coordinates to polar, but now abstract that concept out more than your friend that took a few tabs of acid). You zoom out move around, and zoom back in, and maybe a few more times (but you have to keep track of your path). > Squiggles and lots of Greek letters seem like an anachronism. Do you have another suggestion? Because I don't see how code, English, or literally any other thing that can be written down is anything more meaningful than \"a bunch of squiggles.\" There's only so many simple, and importantly, distinguishable squiggles that we can write down. Luckily we have a tool to exactly determine this ;) The shitty part of mathematics is also the best part, in that you have to embrace the abstractness of it all. But of course human brains weren't designed for this. (Yes, there are some of us trying to figure out how to get machines to think with that extreme level of abstractedness at its core. No, this is not how computers already \"think.\") So yeah, I empathize with you. Quite a lot. There's so much more that I want to know about this thing too. But it is often seemingly impenetrable. All I can tell you is that it just looks that way and isn't. Everyone has the capacity but finding the right way up this mountain (that I'm nowhere near Terry, and he's nowhere near the summit) can be just as hard as the climb itself. The best way is persistence though. Just remember that you don't play video games because they are easy. Sometimes you gotta just trick yourself into pushing through. And some of the best advice I can give is revisiting old topics. So much of what you think you've known actually has a tremendous amount of depth to it. Coming back with fresh eyes and a better understanding of the larger picture can help you see all the beauty in it, even if this is sometimes fleeting. I'm sorry to have vomited so much, but you know, Stockholm Syndrome is a bitch. reply zrm 27 minutes agorootparent> Remember that a lot of mathematics is converting your system into another system Claude Shannon had something to say about this as I recall. > Do you have another suggestion? Because I don't see how code, English, or literally any other thing that can be written down is anything more meaningful than \"a bunch of squiggles.\" The difference is that the squiggles appear on your keyboard. If you use modern language to construct a variable name, the name can be suggestive of what it is to someone already fluent in natural language. Multi-character names also reduce collisions -- there are only so many Greek letters. And then the research paper doesn't show an inscrutable glyph as a JPEG that you can't even copy and paste into a search engine if you don't already know what it represents. reply GaggiX 13 hours agoparentprev>i'd love an extension of this for the diffusion transformer All you need to do is replace the U-net with a transformer encoder (remove the embeddings, and project the image patches into vectors of size n_embd), and the diffusion process can remain the same. reply swyx 12 hours agorootparentseems too simple. isn't there also a temporal dimension you need to encode? reply GaggiX 11 hours agorootparentFor the conditioning and t there are different possibilities, for example the unpooled text embeddings (if the model is conditioned on text) usually go in the cross-attn while the pooled text embedding plus t is used in adaLN blocks like StyleGAN (the first one), but there are many other different strategies. reply acekingspade 10 hours agoprevGood article, but I feel that it misses an important property of diffusion models that they model the score function (derivative of log prob) [1] and that diffusion sampling is akin to Langevin dynamics [2]. IMO these explain why it's easier to train these models than GANs, because of an easier modeling objective. [1] https://yang-song.net/blog/2021/score/ [2] https://lilianweng.github.io/posts/2021-07-11-diffusion-mode... reply ycy 10 hours agoparentYes, these blog posts offer a different perspective on diffusion models from the \"projection onto data\" perspective described in this blog post. You can view them as different ways of interpreting the same training objective and sampling process. In our perspective, diffusion models are easier to train because instead of predicting the gradient of the _exact_ distance function, the training objective predicts the gradient of a _smoothed_ distance function. Sampling the diffusion model is akin to taking multiple approximate gradient steps. To gain a deeper understanding of diffusion models, I encourage everyone to read all of these blog posts and learn about the different interpretations :) reply strangecasts 11 hours agoprevSuper interesting! Immediately reminded of Iterative alpha-(de)Blending [1] which also sets out to set up a conceptually simpler diffusion model, and also arrives at formulating it as an approximate iterative projection process - I think this approach allows for more interesting experiments like the denoiser error analysis, though. [1] https://arxiv.org/pdf/2305.03486.pdf reply skybrian 12 hours agoprevThis is a nice explanation of the theory. It seems to be dataset-independent. I'm wondering about the specifics of generating images. For example, what is it about image generators makes it hard for them to generate piano keyboards? It seems like some better representation of medium-distance constraints is needed to get alternating groups of two and three black notes. reply Vecr 11 hours agoparentIt's the finger problem, you've got to get the number, size, angle, position, etc. right every single time or people can tell very fast. It's not like tree branches where people won't notice if the splitting positions are \"wrong\". reply dr_dshiv 7 hours agoprevIs part of the idea of diffusion that you get a huge amount of training data? Like, you get to contrast all of these randomly diffused images with the undiffused image? reply adamnemecek 11 hours agoprevAll machine learning models are convolutions, mark my words. reply sva_ 10 hours agoparentI think you posted this a few times, maybe you want to elaborate on this point? I have trouble seeing reinforcement learning as convolution, for example. reply adamnemecek 9 hours agorootparentIt's a Fredholm equation which in turn is a convolutional equation. reply carlthome 8 hours agorootparentNext you're gonna say that polynomial multiplication is also just convolution of the arguments. reply adamnemecek 8 hours agorootparentIt's not the standard convolution. reply hotdogscout 11 hours agoprev [–] There's a secret society using the comments on this post to send a message do not Google reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The tutorial explores diffusion models for generative modeling, specifically sampling from multimodal distributions, encompassing theory, implementation, and training code.",
      "It emphasizes training neural networks to predict noise direction, different noise schedules, and denoising techniques for data manifold projection.",
      "The paper introduces an efficient sampler using gradient estimation, discusses the DDIM sampler for enhanced sampling quality, and provides training code for image diffusion models, exhibiting promising outcomes on the FashionMNIST dataset."
    ],
    "commentSummary": [
      "The blog post author and diffusion library creator offer a new theoretical perspective on diffusion models, praised for its clarity by users who recommend it to others.",
      "Discussions within the post cover the concept of diffusion models, including trajectories, code implementation, mathematical notations, training times for toy models, and the relationship between diffusion models and Langevin dynamics.",
      "Users also comment on challenges related to generating images, the substantial training data needed, and engage in conversations about machine learning model nature and convolution equations."
    ],
    "points": 291,
    "commentCount": 36,
    "retryCount": 0,
    "time": 1710186212
  },
  {
    "id": 39666993,
    "title": "Building Decentralized E-Book Search Engine with Glitter",
    "originLink": "https://github.com/j2qk3b/ebook-demo/blob/main/tutorial.md",
    "originBody": "Building An Open Source Decentralized E-Book Search Engine Recently, I was recommended an ebook search website called Liber3 by a friend, which uses ENS domain names. They have created an e-book search website running on ENS and IPFS. Then, I conducted some analysis on their network requests and found that they appear to be using Glitter, a decentralized database service built with Tendermint. Unfortunately, because Liber3 has not made their source code available, after reviewing Glitter's documentation and open datasets stored on it, I decided to implement an open-source community version myself. This way, everyone can create their own e-book search website. Step 1 - Initialize the Project First, create a new project and install Glitter SDK. This SDK allows you to easily connect to the Glitter network and obtain metadata of e-books. JavaScript npm install @glitterprotocol/glitter-sdk Python pip install glitter-sdk Step 2 - Connect to the Network Next, generating a client so that the application can interact with Glitter network. We initialize an LCDClient instance through Glitter SDK and configure the relevant parameters JavaScript import { LCDClient, MnemonicKey, Coins, Numeric } from '@glitterprotocol/glitter-sdk'; const XIAN_HOST = \"https://api.xian.glitter.link\" const CHAIN_ID = \"glitter_12000-2\" const mk = new MnemonicKey({ mnemonic: \"lesson police usual earth embrace someone opera season urban produce jealous canyon shrug usage subject cigar imitate hollow route inhale vocal special sun fuel\" }); const client = new LCDClient({ URL: XIAN_HOST, chainID: CHAIN_ID, gasPrices: Coins.fromString('0.15agli'), gasAdjustment: Numeric.parse(1.5), }) const dbClient = client.db(key); Python from glitter_sdk.client.lcd import LCDClient from glitter_sdk.core import Numeric, Coins from glitter_sdk.key.mnemonic import MnemonicKey XIAN_HOST = \"https://api.xian.glitter.link\" CHAIN_ID = \"glitter_12000-2\" mk = MnemonicKey( \"lesson police usual earth embrace someone opera season urban produce jealous canyon shrug usage subject cigar imitate hollow route inhale vocal special sun fuel\" ) client = LCDClient( chain_id=CHAIN_ID, url=XIAN_HOST, gas_prices=Coins.from_str(\"0.15agli\"), gas_adjustment=Numeric.parse(1.5)) dbClient = client.db(mk) Step 3 - Build Search Function The search function is the core of the application. We define a search function that accepts the user's query keywords, constructs a query statement, and sends it to the Glitter network. JavaScript import { MatchPhraseQuery, queryStringPrepare, prepareSQL } from '@glitterprotocol/glitter-sdk'; const queries = []; const query = 'Come Prima' queries.push(new MatchPhraseQuery('title', `${query}`)); const queryStr = queryStringPrepare(queries); const sql = `SELECT ipfs_cid, title, author, extension, language, publisher, year, filesize, _score, _id FROM library.ebook WHERE query_string(?) LIMIT 0, 200`; const newSql = prepareSQL(sql, queryStr); const sqlData = await dbClient.query(newSql); Python from glitter_sdk.util.parse_query_str import * from glitter_sdk.util.parse_query_str import * from glitter_sdk.util. parse_sql import * queries = [] query = \"Come Prima\" queries.append(MatchPhraseQuery(\"title\", query)) query_str = query_string_prepare(queries) sql = \"SELECT ipfs_cid, title, author, extension, language, publisher, year, filesize, _score, _id FROM library.ebook WHERE query_string(%s)\" sql = prepare_sql(sql, [query_str]) rst = db.query(sql) Step 4 - Display Search Results After building search function, we need to display search results on front-end interface. This includes designing a interface to show the basic information of e-books and providing some interactive elements so that users can easily browse and select the books they are interested in. With these four steps, we can build a e-book search engine, providing users with an efficient and convenient platform for retrieving e-book resources. You could publish a compiled version of this website to IPFS network, then you are having a decentralized ebook search engine hosted on IPFS accessible via any IPFS gateway. The entire source code is available here in this repo.",
    "commentLink": "https://news.ycombinator.com/item?id=39666993",
    "commentBody": "Building an Open Source Decentralized E-Book Search Engine (github.com/j2qk3b)267 points by j2qk3b 22 hours agohidepastfavorite34 comments boredumb 20 hours agoMany moons ago I wanted to do something similar for AI data sets and models over IPFS. I don't know the future for IPFS but I do hope the essence of a p2p data sharing infrastructure becomes more accessible to help individuals tackle some of the issues with large datasets with less hardware on hand. https://github.com/JakeKalstad/IPFSPytorchDataset https://github.com/JakeKalstad/load_ipfs_pytorch_model reply Mortiffer 20 hours agoprevCould you detail how you populate the search index and what you expect the memory limits to be? reply droopyEyelids 19 hours agoprevThe title got me really excited that they were doing full text search. Boy that would be an awesome project. Zlib and Google Books do it, but it would be great to have a open source version that everyone could contribute to, and provided access to full texts reply raybb 19 hours agoparentOpenLibrary does provide search access to full texts. For example: https://openlibrary.org/search/inside?q=%22institutional+thi... It is open source and they're always looking for contributors. I think they'd especially welcome help improving search! https://github.com/internetarchive/openlibrary/ reply mellutussa 11 hours agoparentprevI think a distributed OCR project is needed. Problem is that a lot of books are PDF scans and missing raw text. OcrMyPdf does a pretty good job of is but it's cpu intensive. reply greggsy 7 hours agorootparentI'd wager that there are several players in the AI market who have already scraped and OCR'd every book and magazine on zlib and libgen to feed into training models. Google are almost certainly piped everything they have in Google Books into their models, before some future legal case says they can't. Won't take long before the open community starts doing the same. reply devops000 19 hours agoprevCool! Could be used for torrent searching? Like running web torrent with video streaming and a decentralized search engine. reply j2qk3b 19 hours agoparentYes! Try this one: https://anybt.eth.limo/ I will build an open sourced version too! reply hanniabu 18 hours agorootparentNice to find eth.limo being used in the wild reply ValleZ 18 hours agoprevIs this an actual search engine or just a front end which builds “select from” queries? reply carlosjobim 17 hours agoprevThere's 13 search engines in a dozen if you only want book title or author. What's lacking is a search index of the content of e-books. Something that will soon be incredibly important in the face of generative AI. Somebody here on HN told me it only takes a laptop to index the content of millions of books, while other people say the scope is almost impossible. Is there any project working on this? reply dmotz 14 hours agoparentI have a side project that aims to organize your ebook highlight collections with on-device semantic search. [1] Right now it only indexes your own content but I'd like to add a mode that allows you to share your collection and let others find relevant ideas via semantic search -- a discovery platform for ideas found in books. It's open source if you want a sense of how it works now. [2] [1] https://emdash.ai/ [2] https://github.com/dmotz/emdash reply dredmorbius 10 hours agoparentprevThe size of the index is far more dependent on the search text than the number of searched items. I believe Google documented some of this in its early days, noting that a search index returns the relevant metadata matching a specific query. The query space itself is largely based on both raw keywords and tuples (2- or 3-word ngrams if memory serves, though I'm hazy on this), the latter meeting some minimum frequency requirement. Longer search terms can be constructed from shorter ngrams. A typical advanced native-tongue English vocabulary is about 40,000 words. An expansive dictionary might contain fewer than 250,000 words, including obsolete ones. Mapping a vocabulary to works citing those words is relatively straightforward. Ngrams experience combinatorial expansion, but are still a reasonably constrained space. And we now have well over a quarter-century's experience indexing written content at Web scale. A laptop could probably make a decent cut at providing a useful index of many millions of books, though you'd probably want a somewhat larger system for a more comprehensive index, in particular to rank-index the search space, which is probably the more considerable challenge. reply carlosjobim 6 hours agorootparentThat was a great reply, and made me understand how these things work a lot better. Thank you! reply dredmorbius 3 hours agorootparentThe key concept here is the inverted index:reply myco_logic 16 hours agoparentprevDepends on how beefy that laptop is... I've been doing some local LLM stuff at work recently, and even with the amazing advances in quantization lately, doing that kind of stuff on a ThinkPad is feasible, but still strongly inferior to just renting out a VPS with a couple 4090/H100s for several hours. The biggest thing with summarizing stuff is that most local LLM models often don't have very big context-windows, so they have trouble with larger texts like even a short Vonnegut novel (I was just testing em' with summarizing GitHub issues, and even with a 16k token context window they still sometimes struggle if there are a lot of comments). There are probably smarter people than I who could get this working on a Raspberry Pi though... ;) reply CWuestefeld 16 hours agoparentprevI believe that Calibre, the popular and free ebook management tool, now supports indexing the content all books in your library. reply bt1a 17 hours agoparentprevPerhaps the initial creation of the index is indeed something that an average laptop could accomplish, but I'd imagine that frequently updating the index and serving requests against it would be compute-intensive. I have nothing to back this up but speculation. Would love to learn more! reply MrThoughtful 19 hours agoprevWhat on earth is this about? \"I was recommended ... Liber3 ..., which uses ENS domain names ... running on ENS and IPFS ... they appear to be using Glitter ... a ... service built with Tendermint.\" This sounds like a signal from outer space to me. In a language used in a different galaxy. I tried that Liber3 thing, but whatever I do, I get \"Oops! Something went wrong. Please refresh or try again later\". What is this all about? reply WolfeReader 19 hours agoparentThe title is the de-jargonized version. It's a set of instructions to build an open-source ebook search engine. (Admittedly there is still some jargon in that description, but not to the level of naming specific libraries.) The bulk of the article is implementation details, helpfully hyperlinked. reply droopyEyelids 19 hours agoparentprevnext [2 more] [flagged] pvg 18 hours agorootparenthttps://hn.algolia.com/?dateRange=all&page=0&prefix=true&que... reply throwawayyyyyy2 18 hours agoprevAnd then realize it has existed for almost 15 years and it's called libgen.rs reply spondylosaurus 17 hours agoparentAnna's Archive is even better! reply brevitea 14 hours agorootparentIMO, the more the merrier. That's the joy of decentralization and P2P. reply tamimio 12 hours agorootparentprevIt seems they are using flask in their code, just to show you don’t to go crazy with your stack to build useful software. reply Dudhbbh3343 15 hours agoprevnext [2 more] [flagged] bastawhiz 15 hours agoparentSounds like they're on ipfs with a metadata database on some web3 system. reply neilv 16 hours agoprev [–] This seems to be intended for IP piracy. Clarifying that in the title would help. I'm trying to encourage publishers and authors to offer legitimate sales of DRM-free ebooks, so would prefer we try not to have the term \"ebook\" associated with piracy. reply citruscomputing 11 hours agoparentIt does seem to be! Isn't that cool? reply sureglymop 15 hours agoparentprevIt's a search engine... What about it makes it specific to IP piracy? I actually understand your point well but I think it's even more important not to group in any legitimate use of technology with illegitimate use of it. Especially considering recent events (lawsuits over Yuzu and Dolphin emulators). reply jrm4 8 hours agoparentprevSure, but by the same argument, libraries seem to be intended for IP piracy (or more precisely, the thing that drives most of this, which is \"people reading books for free\") as well. reply RamblingCTO 16 hours agoparentprev [–] Since when are ebooks piracy? I think that might only be you reply neilv 15 hours agorootparent [–] Title is \"Building an Open Source Decentralized E-Book Search Engine\", and screenshot seems to suggest piracy. reply t-3 13 hours agorootparentNothing about the title suggests piracy, and the screenshot doesn't show download links - hell, there aren't any actual Harry Potter books in a search for \"Harry Potter\". Even if it were searching for files, free and legal ebooks are ubiquitous, no copyright infringement necessary to make it a worthwhile endeavor. reply WolfeReader 12 hours agorootparentprev [–] Please be specific about how the screenshot advocates piracy. (Also, a personal preference: never use the phrase \"seems to suggest\" again; if you're going to make an accusation, be honest enough to actually make it.) reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "An open-source decentralized e-book search engine, inspired by Liber3, is being developed using Glitter for its database services.",
      "The project aims to help users set up their e-book search websites on IPFS, allowing efficient access to e-book resources with a decentralized platform.",
      "The article outlines the steps for initializing the project, connecting to the Glitter network, implementing the search function, and displaying search results on the front-end interface, with available source code for reference."
    ],
    "commentSummary": [
      "An open-source decentralized e-book search engine is in development on GitHub, focusing on peer-to-peer data sharing and full-text search capabilities in open source projects.",
      "Related projects on OCR, semantic search for e-books, and indexing process are mentioned, with potential implications for AI and generative models.",
      "Concerns and confusion about IP piracy related to the project are expressed by some users during the discussion."
    ],
    "points": 267,
    "commentCount": 34,
    "retryCount": 0,
    "time": 1710158205
  },
  {
    "id": 39671218,
    "title": "Kdenlive 24.02: Enhanced Performance and New Features",
    "originLink": "https://kdenlive.org/en/2024/03/kdenlive-24-02-0-released/",
    "originBody": "Kdenlive 24.02.0 released by Farid AbdelnourMar 11, 2024News, Releases7 comments The team is thrilled to introduce the much-anticipated release of Kdenlive 24.02, featuring a substantial upgrade to our frameworks with the adoption of Qt6 and KDE Frameworks 6. This significant under-the-hood transformation establishes a robust foundation, shaping the trajectory of Kdenlive for the next decade. The benefits of this upgrade are particularly noteworthy for Linux users, as improved Wayland support enhances the overall experience. Additionally, users on Windows, MacOS, and Linux will experience a substantial performance boost since Kdenlive now runs natively on DirectX, Metal, and Vulkan respectively, replacing the previous abstraction layer reliance on OpenGL and Angle, resulting in a more efficient and responsive application. This upgrade brings significant changes to packaging, featuring the introduction of a dedicated package for Apple Silicon, the discontinuation of PPA support and an enhanced method for installing the Whisper and Vosk speech-to-text engines. While a significant effort has been invested in providing a stable user experience in this transition, we want to acknowledge that, like any evolving software, there might be some rough edges. Some known issues include: themes and icons not properly applied in Windows and AppImage, text not properly displayed in clips in the timeline when using Wayland and a crash in the Subtitle Manager under MacOS. Worth noting also is the temporary removal of the audio recording feature pending its migration to Qt6. We appreciate your understanding and encourage you to provide feedback in this release cycle so that we can continue refining and improving Kdenlive. In the upcoming release cycles (24.05 and 24.08), our development efforts will concentrate on stabilizing any remaining issues stemming from this upgrade. We’ll also prioritize short-term tasks outlined in our roadmap, with a specific emphasis on enhancing performance and streamlining the effects workflow. In terms of performance enhancements, this release introduces optimized RAM usage during the import of clips into the Project Bin. Furthermore, it addresses Nvidia encoding and transcoding issues with recent ffmpeg versions. To safeguard project integrity, measures have been implemented to prevent corruptions. Projects with non-standard and variable frame rates are not allowed to be created. When rendering a project containing variable frame rate clips, users will receive a warning with the option to transcode these clips, mitigating potential audio-video synchronization issues. Users can now enjoy the convenience of an automatic update check without an active network connection. Glaxnimate animations now default to the rawr format, replacing Lottie. Furthermore, we’ve introduced an FFv1 render preset to replace the previously non-functional Ut Video. And multiple project archiving issues have been fixed. Beyond performance and stability we’ve managed to sneak in several nifty quality-of-life and usability improvements, the highlights include: Subtitles This release introduces multiple subtitle support, allowing users to conveniently choose the subtitle from a drop-down list in the track header. A subtitle manager dialog has been implemented to facilitate the import and export of subtitles. Now, in the Import Subtitle dialog, you have the option to create a new subtitle instead of replacing the previous one. Speech-to-Text The Speech Editor, our text-based editing tool that enables users to add clips to the timeline from selected texts, now includes the option to create new sequences directly from the selected text. Effects The initial implementation of the long awaited easing interpolation modes for keyframes has landed. Expected soon are easing types (ease in, ease out and ease in and out) and a graph editor. The Gaussian Blur and Average Blur filters are now keyframable. Rendering Added the option to set an interpolation method for scaling operations on rendering. Quality-of-Life and Usability Added the option to apply an effect to a group of clips by simply dragging the effect onto any clip within the group. Conveniently move or delete selected clips within a group using the Alt + Select option. Added a toggle button to clips with effects to easily enable/disable them directly from the timeline. Added list of last opened clips in Clip Monitor’s clip name Added the ability to open the location of the rendered file in the file manager directly from the render queue dialog.. The Document Checker has been completely rewritten following the implementation of sequences. Now, when you open a project, Kdenlive checks if all the clips, proxies, sequences, and effects are loaded correctly. If any errors are spotted, Kdenlive seamlessly sorts them out in the project files, preventing any possible project corruptions Added the ability to trigger a sound notification when rendering is complete. Full changelog Fix multitrack view not exiting for some reason on tool switch (Qt6). Commit. Fix qml warnings. Commit. Show blue audio/video usage icons in project Bin for all clip types. Commit. See issue #1816 Multiple fixes for downloaded effect templates: broken link in effect info, empty name, cannot edit/delete. Commit. New splash for 24.02. Commit. Subtitles: add session id to tmp files to ensure 2 concurrent versions of a project don’t share the same tmp files. Commit. Fixes bug #481525 Fix title clip font’s weight lost between Qt5 and Qt6 projects. Commit. Fix audio thumbnail not updated on replace clip in timeline. Commit. Fixes issue #1828 Refactor mouse position in the timeline to fix multiple small bugs. Commit. Fixes bug #480977 Subtitle import: disable ok button when no file is selected, only preview the 30 first lines. Commit. Fix wrong clip dropped on timeline when subtitle track is visible. Commit. See bug #481325 Fix track name text color on Qt6. Commit. Ensure we don’t mix title clips thumbnails (eg. in duplicated clips). Commit. Fix scopes and titler bg on Win/Mac. Commit. Fix incorrect item text. Commit. Fix extract frame from video (fixes titler background, scopes, etc). Commit. Make AVFilter average and gaussian blur keyframable. Commit. Ensure we always load the latest xml definitions for effects. Commit. Fix composition paste not correctly keeping a_track. Commit. Ensure custom keyboard shortcuts are not deleted on config reset. Commit. Fix crash after changing toolbar config: ensure all factory()->container actions are rebuild. Commit. Try to fix white monitor on undock/fullscreen on Windows / Mac. Commit. Fix sequence copy. Commit. See bug #481064 Fix pasting of sequence clips to another document messing clip ids. Commit. Fix python package detection, install in venv. Commit. See issue #1819 Another pip fix. Commit. Fix typos in venv pip. Commit. Venv: ensure the python process are correctly started. Commit. Add avfilter dblur xml description to fix param range. Commit. Fix typo. Commit. Correctly ensure pip is installed in venv. Commit. Fix undocked widgets don’t have a title bar to allow moving / re-docking. Commit. Ensure pip is installed inside our venv. Commit. Fix Qt6 dragging clips with subtitle track visible. Commit. Fixes bug #480829 Subtitle items don’t have a grouped property – fixes resize bug. Commit. See bug #480383 Fix Shift + resize subtitle affecting other clips. Commit. Speech to text : switch to importlib instead of deprecated pkg_resources. Commit. Multi guides export: replace slash and backslash in section names to fix rendering. Commit. Fixes bug #480845 Fix moving grouped subtitles can corrupt timeline if doing an invalid move. Commit. Fix sequence corruption on project load. Commit. Fixes bug #480776 Fix sort order not correctly restored, store it in project file. Commit. Fixes issue #1817 Ensure closed timeline sequences have a transparent background on opening. Commit. Fixes bug #480734 Fix Arrow down cannot move to lower track if subtitles track is active. Commit. Enforce refresh on monitor fullscreen switch (fixes incorrectly placed image). Commit. Fix audio lost when replacing clip in timeline with speed change. Commit. Fixes issue #1815 Fix duplicated filenames or multiple uses not correctly handled in archiving. Commit. Fixes bug #421567. Fixes bug #456346 Fix multiple archiving issues. Commit. Fixes bug #456346 Do not hide info message on render start. Commit. Fix Nvidia transcoding. Commit. See issue #1814 Fix possible sequence corruption. Commit. Fixes bug #480398 Fix sequences folder id not correctly restored on project opening. Commit. Fix duplicate sequence not creating undo entry. Commit. See bug #480398 Fix drag clip at beginning of timeline sometimes loses focus. Commit. Fix luma files not correctly checked on document open, resulting in change to luma transitions. Commit. Fixes bug #480343 [CD] Run macOS Qt5 only on manual trigger. Commit. Fix group move corrupting undo. Commit. Fixes bug #480348 Add FFv1 render preset to replace non working utvideo. Commit. Fix possible crash on layout switch (with Qt in debug mode), fix mixer label overlap. Commit. Hide timeline clip effect button on low zoom. Commit. Fixes issue #1802 Fix subtitles not covering transparent zones. Commit. Fixes bug #480350 Group resize: don’t allow resizing a clip to length < 1. Commit. Fixes bug #480348 Luma fixes: silently autofix luma paths for AppImage projects. Try harder to find matching luma in list, create thumbs in another thread so we don’t block the ui. Commit. Fix crash cutting grouped overlapping subtitles. Don’t allow the cut anymore, add test. Commit. Fixes bug #480316 Remove unused var. Commit. Effect stack: don’t show drop marker if drop doesn’t change effect order. Commit. Try to fix crash dragging effect on Mac. Commit. Another try to fix monitor offset on Mac. Commit. Optimize some of the timeline qml code. Commit. Fix DocumentChecker model directly setting items and incorrect call to columnCount() in index causing freeze in Qt6. Commit. Fix clip monitor not updating when clicking in a bin column like date or description. Commit. Fixes bug #480148 Ensure we also check “consumer” producers on doc opening (playlist with a different fps). Commit. Fix glaxnimate animation not parsed by documentchecker, resulting in empty animations without warn if file is not found. Commit. Fix NVidia encoding with recent FFmpeg. Commit. See issue #1814 Fix clip name offset in timeline for clips with mixes. Commit. Better way to disable building lumas in tests. Commit. Don’t build lumas for tests. Commit. Fix Mac compilation. Commit. Fix data install path on Windows with Qt6. Commit. Fix ridiculously slow recursive search. Commit. Fix start playing at end of timeline. Commit. Fixes bug #479994 Try to fix mac monitor vertical offset. Commit. Don’t display useless link when effect category is selected. Commit. Fix save clip zone from timeline adding an extra frame. Commit. Fixes bug #480005 Fix clips with mix cannot be cut, add test. Commit. Fixes issue #1809. See bug #479875 Fix cmd line rendering. Commit. Windows: fix monitor image vertical offset. Commit. Fix project monitor loop clip. Commit. Add test for recent sequence effect bug. Commit. See bug #479788 Fix tests (ensure we don’t try to discard a task twice). Commit. Blacklist MLT Qt5 module when building against Qt6. Commit. Fix monitor offset when zooming back to 1:1. Commit. Fix sequence effects lost. Commit. Fixes bug #479788 Avoid white bg label in status bar on startup. Commit. Fix qml warnings. Commit. Fix clicking on clip fade indicator sometimes creating a 2 frames fade instead of defined duration. Commit. Improved fix for center crop issue. Commit. Fix center crop adjust not covering full image. Commit. Fixes bug #464974 Fix various Qt6 mouse click issues in monitors. Commit. Disable Movit until it’s stable (should have done that a long time ago). Commit. Fix Qt5 startup crash. Commit. Add time to undo action text. Commit. Fix cannot save list of project files. Commit. Fixes bug #479370 Add missing license info. Commit. [Nightly Flatpak] Replace Intel Media SDK by OneVPL Runtime. Commit. [Nightly Flatpak] Fix and update python deps. Commit. [Nightly Flatpak] Switch to Qt6. Commit. Fix editing title clip with a mix can mess up the track. Commit. Fixes bug #478686 Use Qt6 by default, fallback to Qt5. Commit. Fix audio mixer cannot enter precise values with keyboard. Commit. [CI] Require tests with Qt6 too. Commit. Add FreeBSD Qt6 CI. Commit. Apply i18n to percent values. Commit. Show GPU in debug info. Commit. Prevent, detect and possibly fix corrupted project files, fix feedback not displayed in project notes. Commit. Fixes issue #1804. See bug #472849 [nightly Flatpak] Add patch to fix v4l-utils. Commit. Update copyright to 2024. Commit. [nightly flatpak] fix v4l-utils once more. Commit. [nightly Flatpak] v4l-utils uses meson now. Commit. Don’t crash on first run. Commit. [nightly flatpak] Try to fix v4l-utils. Commit. [nightly flatpak] Cleanup. Commit. Get rid of dropped QtGraphicalEffects. Commit. Fix qml warnings. Commit. Qt6: fix subtitle editing in timeline. Commit. Fix subtitles crashing on project load (incorrectly setting in/out snap points). Commit. Test project’s active timeline is not always the first sequence. Commit. Ensure secondary timelines are added to the project before being loaded. Commit. Ensure autosave is not triggered when project is still loading. Commit. Show GPU name in Wizard. Commit. Avoid converting bin icons to/from QVariant. Commit. [Nightly Flatpak] Update deps. Commit. Fix Qt6 audio / video only clip drag broken from clip monitor. Commit. Fix rubber select incorrectly moving selected items when scrolling the view. Commit. Port away from jobclasses KIO header. Commit. Fix variable name shadowing. Commit. When switching timeline tab without timeline selection, don’t clear effect stack if it was showing a bin clip. Commit. Fix crash pressing del in empty effect stack. Commit. Ensure check for HW accel is also performed if some non essential MLT module is missing. Commit. Fix closed sequences losing properties, add more tests. Commit. Don’t attempt to load timeline sequences more than once. Commit. Fix “Sequence from selection” with single track. Commit. Refactor code for paste. Commit. Fix timeline groups lost after recent commit on project save. Commit. Ensure we always use the correct timeline uuid on some clip operations. Commit. Qt6: fix monitor image vertical offset. Commit. Always keep all timeline models opened. Commit. See bug #478745 Add animation: remember last used folder. Commit. See bug #478688 Fix KNS KF6 include. Commit. Add missing include. Commit. Refresh effects list after downloading an effect. Commit. Fix crash searching for effect (recent regression). Commit. Fix audio or video only drag of subclips. Commit. Fixes bug #478660 Fix editing title clip duration breaks title (recent regression). Commit. Glaxnimate animations: use rawr format instead of Lottie by default. Commit. Fixes bug #478685 Effect Stack: remove color icons, fix mouse wheel seeking while scrolling. Commit. See issue #1786 Fix timeline focus lost when dropping an effect on a clip. Commit. Disable check for removable devices on Mac. Commit. [CD] Use Qt6 templates instead of custom magic. Commit. Fix type in Purpose KF version check. Commit. Fix dropping lots of clips in Bin can cause freeze on abort. Commit. Right click on a mix now shows a mix menu (allowing deletion). Commit. Fixes bug #442088 Don’t add mixes to disabled tracks. Commit. See bug #442088 Allow adding a mix without selection. Commit. See bug #442088 Fix proxied playlist clips (like stabilized clips) rendered as interlaced. Commit. Fixes bug #476716 [CI] Try different approach for macOS signing. Commit. [CI] Signing test, explicitly source env for now. Commit. Camcorder proxies: ensure we have the same count of audio streams and if not, create a new proxy with audio from original clip (Fixes Sony FX6 proxies). Commit. Fix typo. Commit. Fixes issue #1800 [CI] Re-enable Flatpak. Commit. [CI] More fixes for the signing test. Commit. [CI] Fixes for the signing test. Commit. [CI] Add macOS signing test. Commit. [CI] Fix pipeline after recent renaming upstream. Commit. Qml warning fixes. Commit. Add subtitle manager to project mneu. Commit. Fix groups tests. Commit. Fix transparency lost on rendering nested sequences. Commit. Fixes bug #477771 Fix guides categories not applied on new document. Commit. Fixes bug #477617 Fix selecting several individual items in a group. Commit. Add import/export to subtitle track manager. Commit. Drag & drop of effect now applies to all items in a group. Commit. See issue #1327 New: select an item in a group with Alt+click. You can then perform operations on that clip only: delete, move. Commit. See issue #1327 Consistency: activating an effect in the effects list now consistently applies to all selected items (Bin or Timeline). Commit. Cleanup assets link to documentation. Commit. Check MLT’s render profiles for missing codecs. Commit. See bug #475029 Various fixes for python setup. Commit. Fix Qt6 compilation. Commit. FIx incorreclty placed ifdef. Commit. Start integrating some of the new MLT keyframe types. Commit. Various fixes for python venv install. Commit. Fix missing argument in constructor call. Commit. Fix crash on auto subtitle with subtitle track selected. Commit. Fix python install stuck. Commit. Improve timeline clip effect indicator. Commit. See issue #445 Work/multisubtitles. Commit. Fix some issues in clip monitor’s last clip menu. Commit. Various fixes and improved feedback for Python venv, add option to run STT on full project. Commit. Text corrections. Commit. Fix typos. Commit. If users try to render a project containing variable framerate clips, show a warning and propose to transcode these clips. Commit. Fix qml warning (incorrect number of args). Commit. Fix qt6 timeline drag. Commit. Flatpak: Use id instead of app-id. Commit. Fix audio stem export. Commit. Add link to our documentation in the effects/composition info. Commit. Qt6: fix monitor background and a few qml mouse issues. Commit. Rename ObjectType to KdenliveObjectType. Commit. We need to use Objective C++ for MetalVideoWidget. Commit. When pasting clips to another project, disable proxies. Commit. Fixes issue #1785 Remove unneeded lambda capture. Commit. Fix monitor display on Windows/Qt6. Commit. Cleanup readme and flatpak nightly manifests. Commit. [Nightly Flatpak] Do not build tests. Commit. Fix tests broken by last commit. Commit. Add list of last opened clips in Clip Monitor’s clip name. Commit. Add Craft Jobs for Qt6. Commit. [CI] Switch to new template include format. Commit. [CI] Add reuse-lint job. Commit. Chore: REUSE linting for compliance. Commit. Don’t check for cache space on every startup. Commit. Don’t allow creating profile with non standard and non integer fps from a clip. Commit. See issue #476754 Remove unmaintained changelog file. Commit. Automatically check for updates based on the app version (no network connection at this point). Commit. Fix project duration for cli rendering. Commit. Fix clips with missing proxy incorrectly loaded on project opening. Commit. Fix compilation with KF < 5.100. Commit. Add undo redo to text based edit. Commit. Check and remove circular dependencies in tractors. Commit. Fixes bug #471359 Hide resize handle on tiny clips with mix. Commit. Fix minor typos. Commit. Adapt to new KFileWidget API. Commit. Fix mix not always deleted when moving grouped clips on same track. Commit. Fix python venv for Windows. Commit. Fix timeremap. Commit. Fix replace clip keeping audio index from previous clip, sometimes breaking audio. Commit. See bug #476612 Create sequence from selection: ensure we have enough audio tracks for AV groups. Commit. Fix timeline duration incorrect after create sequence from timeline selection. Commit. Add a Saving Successful event, so people can easily play a sound or show a popup on save if wanted. Commit. See issue #1767 Fix project duration not updating when moving the last clip of a track to another non last position. Commit. See bug #476493 Update file kdenlive.notifyrc. Commit. Duplicate .notifyrc file to have both KF5 and KF6 versions. Commit. Don’t lose subtitle styling when switching to another sequence. Commit. Fixes bug #476544 Port from deprecated ksmserver calls. Commit. Allow aborting clip import operation. Commit. Ensure no urls are added to file watcher when interruping a load operation. Commit. Fix crash dropping url to Library. Commit. When dropping multiple files in project bin, improve import speed by not checking if every file is on a remote drive. Commit. Fix titler shadow incorrectly pasted on selection. Commit. Fixes bug #476393 Sequences folder now has a colored icon and is always displayed on top. Commit. Fix Qt5 compilation. Commit. Fix Qt5 compilation take 3. Commit. Fix Qt5 compilation take 2. Commit. Fix Qt5 compilation. Commit. Fix some Qt6 reported warnings. Commit. Fix pasted effects not adjusted to track length. Commit. Python virtual env: Add config tab in the Environement Settings page, minor fixes for the dependencies checks. Commit. [Qt6] We need to link to d3d on Windows. Commit. Convert license headers to SPDX. Commit. Use pragma once for new monitor code. Commit. Fix Qt6 build on Windows. Commit. Text based edit: add font zooming and option to remove all silence. Commit. Move venv to standard xdg location (.local/share/kdenlive). Commit. Whisper now has word timings. Commit. Use python venv to install modules. Commit. Fix timeline preview ignored in temporary data dialog. Commit. Fixes bug #475980 Improve debug output for tests. Commit. Correctly prefix python scripts, show warning on failure to find python. Commit. Qt6 Monitor support. Commit. Speech to text: fix whisper install aborting after 30secs. Commit. Don’t try to generate proxy clips for audio with clipart. Commit. Clip loading: switch to Mlt::Producer probe() instead of fetching frame. Commit. Multiple fixes for time remap losing keyframes. Commit. [CI] Increase per test timeout. Commit. Add secondary color correction xml with renamed alphasp0t effect, fix effectgroup showing incorrect names. Commit. Add png with alpha render profile. Commit. See issue #1605 Fix Mix not correctly deleted on group track move. Commit. See issue #1726 Cleanup commented code. Commit. Fix setting default values is never executed. Commit. Cleanup param insert and placeholder replacement. Commit. Move render argument creation to a function. Commit. Move project init logic out of renderrequest. Commit. Use projectSceneList() for both cli and gui rendering. Commit. Use active timeline for rendering. Commit. Adapt to KBookmarkManager API change. Commit. Small cleanup. Commit. Properly initialize projectItemModel and bin playlist on render request. Commit. Revert “Properly initialize projectItemModel and bin playlist on render request”. Commit. Fix for renamed frei0r effects. Commit. Fix rendering with alpha. Commit. Rotoscoping: don’t auto add a second kfr at cursor pos when creating the initial shape, don’t auto add keyframes until there are 2 keyframes created. Commit. Fix description –render-async flag. Commit. Fix keyframe param not correctly enabled when selecting a clip. Commit. Fix smooth keyframe path sometimes incorrectly drawn on monitor. Commit. Allow setting the default interpolation method for scaling operations on rendering. Commit. Fixes issue #1766 Don’t attempt to replace clip resource if proxy job was not completely finished. Commit. Fixes issue #1768 Properly initialize projectItemModel and bin playlist on render request. Commit. Rename render params, don’t load project twice. Commit. Remove accelerator on timeline tab rename. Commit. Fixes issue #1769 Print render errors for cli rendering too. Commit. Minor cleanup. Commit. Improve exit code on failure. Commit. [cli rendering] Fix condition for subtitle. Commit. Show documentchecker warning only if relevant. Commit. Fix printing of documentchecker results. Commit. [cli renderer] Ensure x265 params are calculated. Commit. Custom clip job: allow using current clip’s frame as parameter. Commit. Properly adjust timeline clips on sequence resize. Commit. Remove unused debug stuff. Commit. Fix project duration not correctly updated on hide / show track. Commit. Custom clip jobs: handle lut file as task output. Commit. Allow renaming a timeline sequence by double clicking on its tab name. Commit. Fix resize clip with mix test. Commit. Fix resize clip start to frame 0 of timeline not correctly working in some zoom levels,. Commit. Remember Clip Monitor audio thumbnail zoom & position for each clip. Commit. Asset List: ensure favorite are shown using a bold font. Commit. Fix asset list using too much height. Commit. Switch Effects/Compositions list to QWidget. Commit. Drop unused and deprecated qmlmodule QtGraphicalEffects. Commit. Fix warning. Commit. Fix multiple audio streams broken by MLT’s new astream property. Commit. Fixes bug #474895 Custom clip jobs: ensure we never use the same output name if several tasks are started on the same job. Commit. Custom clip jobs: ensure script exists and is executable. Commit. Fix dialogs not correctly deleted, e.g. add track dialog, causing crash on exit. Commit. Ensure clips with audio (for exemple playlists) don’t block audio when inserted on video track. Commit. Ensure translations cannot mess with file extensions. Commit. Fix another case blocking separate track move. Commit. Fix grabbed clips cannot be moved on upper track in some cases. Commit. Final blocks for enabling render test suite: add synchronous option to exit only after rendering is finished, add option for render preset (use H264 as default). Commit. Implement #1730 replace audio or video of a bin clip in timeline. Commit. Fix cppwarning. Commit. Fix move clip part of a group on another track not always working. Commit. Fix playlist count not correctly updated, allowing to delete last sequence. Commit. Fixes bug #474988 Fix motion-tracker Nano file name and links to the documentation. Commit. Stop installing kdenliveui.rc also as separate file, next to Qt resource. Commit. Library: add action to open a library file in a File manager. Commit. Fix tests and possible corruption in recent mix fix. Commit. Correctly highlight newly dropped files in library. Commit. Fix threading issue crashing in resource widget. Commit. Fixes issue #1612 Fix freeze on adding mix. Commit. See issue #1751 Make Lift work as expected by most users. Commit. Fixes bug #447948. Fixes bug #436762 Fix load task discarding kdenlive settings (caused timeline clips to miss the “proxy” icon. Commit. Fix multiple issues with Lift/Gamma/Gain undo. Commit. Fixes bug #472865. Fixes bug #462406 Fix freeze / crash on project opening. Commit. COrrectly update effect stack when switching timeline tab. Commit. Drop timeline guides, in favor of sequence clip markers. Commit. Optimize RAM usage by not storing producers on which we did a get_frame operation. Commit. Fix guide multi-export adding an extra dot to the filename. Commit. Open the recursive search from the project file location. Commit. Inform user about time spent on recursive search. Commit. Allow open contained folder in job queue dialog. Commit. Read input and output from command line. Commit. Correctly process configurable render params. Commit. Fix crash on subclip transcoding. Commit. Fixes issue #1753 Fix audio extract for multi stream clips. Commit. Correctly set render params for headless rendering. Commit. Ensure some basic parts are built with headless rendering. Commit. Remove unneeded setting of CMake policies, implied by requiring 3.16. Commit. Fix detection/fixing when several clips in the project use the same file. Commit. Render widget: show warning if there is a missing clip in the project. Commit. DocumentChecker: Enable recursive search for clips with proxy but missing source. Commit. Fix rnnoise effect parameters and category. Commit. Fix minor typo. Commit. Fix zone rendering not remembered when reopening a project. Commit. Add missing test file. Commit. Various document checker fixes: fix display update on status change, allow sorting in dialog, hide recreate proxies if source is not available, add test for missing proxy. Commit. Project Bin: don’t draw icon frame if icon size is null. Commit. Fix clips with empty resource not detected by our documentchecker code. Commit. Fix document checker dialog not enabling ok after removing problematic clips. Commit. Document checker dialog: fix selection, allow multiple selection, limit color background and striked out text to a specific column. Commit. Show fade value on drag. Commit. Fixes issue #1744 If copying an archived file fails, show which file failed in user message. Commit. Don’t incorrectly treat disabled proxy (-) as missing. Commit. Fixes issue #1748 Fix minor typo. Commit. Fix box_blur xml. Commit. Add new “preserve alpha” option to box blur. Commit. Transcoding: add option to replace clip in project (disabled for timeline sequence clips). Commit. See issue #1747 Add notr=”true” for text that should not be translated. Commit. When an MLT playlist proxy is missing, it should be reverted to a producer, not stay in a chain. Commit. Adapt to kbookmarks API change. Commit. Adapt to KNotifcations API change. Commit. Try to auto fix path of LUT files on project opening. Commit. Automatically fix missing fonts (like before). Commit. Remove unused ManageCapturesDialog. Commit. [DCResolverDialog] Improve UI. Commit. Fix recursive search and “use placeholder”. Commit. [REUSE] Remove duplicated entry in dep5. Commit. Chore(REUSE): Further linting. Commit. Chore(REUSE): Add headers in data/effects/update. Commit. Chore(REUSE): Add headers in src/ui. Commit. Chore(REUSE): Add missing licence texts. Commit. Chore(reuse): Add missing IP info. Commit. Chore(REUSE): Add SPDX info to CMakelists.txt files. Commit. Add missing include (fix qt6 build). Commit. Don’t duplicate KF_DEP_VERSION + remove unused REQUIRED_QT_VERSION. Commit. Fix configure qt6. Commit. [ColorWheel] Show real color in slider instead of black and white. Commit. See issue #1405 Add QColorUtils::complementary. Commit. Add some accessibility names for testing. Commit. Add option to export guides as FFmpeg chapter file. Commit. See bug #451936 [Rendering] Further restructuring. Commit. [DocumentResource] Fix workflow with proxies. Commit. Try to fix tests. Commit. [DocumentChecker] Fix and polish after refactoring. Commit. [DocumentChecker] Refactor code to split logic and UI. Commit. [DocumentChecker] Start to split UI and backend code. Commit. Add our mastodon on apps.kde.org. Commit. Fix typo not installing renderer. Commit. Fix tests. Commit. Delete unused var. Commit. Initial (yet hacky) cli rendering. Commit.",
    "commentLink": "https://news.ycombinator.com/item?id=39671218",
    "commentBody": "Kdenlive 24.02 open source video editor released (kdenlive.org)258 points by jrepinc 16 hours agohidepastfavorite53 comments d416 9 hours ago15 years ago I bought a cheap laptop, removed windows to install Ubuntu 6, took it backpacking through Asia for the next 6 months and started editing my travel video on Kdenlive [1] at the tail end of it. It was my first real leap into Linux and Kdenlive was so far ahead of anything else I never looked back. Props to the community for such a solid product and for making me fall in love with open source. [1] https://youtu.be/6ZCL4Jr9qs8?si=QTp9fEpgLwP5REYU reply tombert 13 hours agoprevWithout getting into the philosophy of open source vs proprietary and the like, but just as someone who very occasionally needs to edit video on Linux, how does Kdenlive compare to something like Lightworks or the Blender video editor? On Linux for the last 6 or 7 years, I've used Lightworks just because the free version has been good enough for what I've needed for nearly everything, and for the few times I need higher resolution I have just paid like $20 for one month of \"premium\". I didn't have any objective criteria for choosing Lightworks, other than \"it's been used for 'real' movies!\" That said, I'd prefer to keep things open source, so I'd rather move to a FOSS thing like Kdenlive or Blender, if they're comparable (or better!) than Lightworks. Anyone have experience with all three? reply rozap 13 hours agoparentI use Kdenlive about once or twice a year to edit/compile racecar footage. The UI has been friendly enough to relearn each time because I use it so infrequently. I've never had any complaints about it, though I'm not doing anything crazy. But it's extremely intuitive and even as a new user it doesn't seem to get in my way, which is just about the highest praise I can give to a piece of interactive software. Unfortunately I can't compare against Lightworks or Blender, because I tried Kdenlive and it worked great, and there was no reason to search for alternatives. reply DANmode 13 hours agorootparentOT, but: what type of racecars? =D reply pilaf 7 hours agoparentprevI've used Kdenlive, Shotcut, Blender and Olive [1]. They all have strenghts and weaknesses, so I choose which one to use depending on what I'm trying to do, or sometimes I use two of them through a single video project. One thing to note is that Kdenlive and Shotcut both use the MLT video editing framework [2] under the hood, so their capabilities and constraints are very close to each other's. That said, their UIs are their own and some things may be easier to do in one over the other, may be a matter of personal preference. AFAIK Shotcut is developed by the same people who built MLT, but I don't think that gives it any particular advantage. Also both of these apps have the largest ready-made effects toolbox out of the four apps I mentioned at the top. Blender's VSE (video sequence editor) is great if you need fine-tuned 2D animations of elements because you can use all the same awesome keyframing tools you'd use for 3D animation, but it's severely lacking in other aspects, especially in the effects dept (you can crop, blur, mask, but not much else). For some reason you can't use Blender's compositor node system with video, which would enable many more capabilities if possible. There's also a steeper learning curve if you've never used Blender before because its UI breaks many conventions. Olive is a newcomer that doesn't get enough attention, but IMHO it was at one point the most promising OSS video editor out there. Sadly the developer works on it on his free time, and he's recently said that he's pausing development because he doesn't have the resources to work on it any more. I'm really hoping a miracle happens. There's two versions of Olive, 0.1 and 0.2 which is a complete rewrite. Both versions are good, but they work pretty differently. What got me excited about 0.2 is that its effects are node-based (unlike MLT-based editors which are stack-based), which enables far more advanced editing, although you probably wouldn't need that unless you're working on something quite ambitious. 1: https://www.olivevideoeditor.org/ 2: https://www.mltframework.org/ reply tetris11 13 hours agoparentprevBlender video editor is extremely feature rich in terms of the keyframing effects you can do with it, the only problem being: it's slow, and it doesn't always sync audio with video even when you ask it to. With Kdenlive, it's frustrating to add a simple block of text (you can have to create an overlay, and then configure that overlay, and place it roughly where you want), but the final rendering takes only minutes and it's stable enough to work with for larger projects. Can't comment on Lightworks. reply depingus 12 hours agorootparent> Blender video editor is ... slow Might be time to think about trying Blender again. This was posted here on HN about a month ago. https://aras-p.info/blog/2024/02/06/I-accidentally-Blender-V... reply soogwoog 10 hours agorootparentI tried blender video editor just the other day, and this problem was still there -to the point that I actually went out and found kdenlive the same day! reply mixmastamyk 7 hours agorootparent> upcoming Blender 4.1 (which just became beta and can be downloaded from… reply MikeTheGreat 13 hours agoparentprevI don't have experience with any of those, but I have used ShotCut [1], which is free an open-source. It works well enough on those rare occasions I've needed it (e.g. editing a video of my child playing an instrument for their class). If you're looking for \"FOSS\" and \"easy to use\" ShotCut might be worth looking at. [1] https://en.wikipedia.org/wiki/Shotcut reply foobarbecue 12 hours agoparentprevMy experience has been: Kdenlive is full-featured and weird to use. Blender is missing features and weird to use. Openshot is simple and easy, but crashes. shortcut is simple and easy and does not crash. reply foobarbecue 7 hours agorootparentUgh, Shotcut, not Shortcut reply redwall_hp 13 hours agoparentprevIn my experience, Kdenlive and Shotcut are more or less on par with iMovie. Da Vinci Resolve works on all three major OSes and is actively usurping the prosumer market previously held by Premiere and Final Cut Pro. (It even has an advanced, node-based compositing tool for visual effects and motion graphics.) It's not open source, though it's free unless you need some features aimed more at serious commercial stuff. reply haunter 12 hours agorootparent>unless you need some features aimed more at serious commercial stuff or simply H264/H265 which is a dealbreaker for a lot of people. reply snailmailman 8 hours agorootparentThe free version has h264 and h265, as far as I can tell. I rendered a mp4, h264 video in resolve today. My first time using the software and it was surprisingly capable. (I needed to blur a moving face in a video) reply jasomill 5 hours agorootparentOn Mac and Windows, the free version of Resolve supports H.264 and H.265 (using OS-supplied codecs IIRC). Linux H.264 and H.265 support is Studio-only[1] (presumably because royalties). If you don't care about any other Studio features, converting to/from an intermediate codec — DNxH[DR], say — on import/export using another tool like FFmpeg seems like a reasonable workaround for most applications. Since Resolve doesn't support H.264/H.265 passthrough (on any platform), the only generation loss added by this approach will be from DNxH[DR] encoding, which, assuming a sufficiently high bitrate for the intermediate codec, should be minimal. Note that even Resolve Studio doesn't support AAC audio on Linux[2], so, for many H.264/H.265 projects, you'd end up with pre-/post-conversion steps even if the video codecs were supported. [1] https://documents.blackmagicdesign.com/SupportNotes/DaVinci_... [2] https://documents.blackmagicdesign.com/SupportNotes/DaVinci_... reply haunter 5 hours agorootparentprevNot in Linux, only in Mac and Windows reply IshKebab 11 hours agorootparentprevUnfortunately iMovie is still in a different league to Kdenlive or Shotcut. Think iPhone vs iPAQ. Do Kdenlive or Shotcut even support GPU rendering yet? Blender is probably still the best open source video editor by quite some margin. reply cultofmetatron 1 hour agorootparent> Unfortunately iMovie is still in a different league to Kdenlive or Shotcut. Think iPhone vs iPAQ. if you need a feature set like iMovie, why not use davinci resolve? its available on linux. not open source but neither is imovie reply Daub 4 hours agoparentprev> Without getting into the philosophy of open source vs proprietary and the like, but just as someone who very occasionally needs to edit video on Linux, how does Kdenlive compare to something like Lightworks or the Blender video editor? Blender features a node-based compositor (Like Nuke) and an NLE (movie editor). Both these tools are fine when used within a Blender workflow, but they are troublesome to use in a stand alone capacity. In my opinion, the most important feature of an NLE is its media optimization strategy without which editing is a chore. I now use DaVinci by Blackmagic, which is free. They inherited the amazingly efficient proxy system from the DaVinci colour grader. To this Blackmagic added their own codec voodoo which is not insubstantial. DaVinci and Blackmagic footage together run as smooth as butter. reply kristopolous 8 hours agoparentprevkdenlive is the easiest. It has annoying bugs but they're relatively easy to work around. Of course, like any open source project, part of the reason why those bugs exist is my fault for not engaging with the project ... but let's ignore that reply kps 12 hours agoparentprevAs someone who rarely edits video and doesn't need or want to become an expert, but had one to do last year, I ended up using Kdenlive. I know I also tried Shotcut and Openshot (because they're on my machine) but not why I rejected them, and I don't remember whether I tried DaVinci. reply charcircuit 9 hours agoparentprevBoth kdenlive and blender are not that great at working with text which makes them hard to recommend for serious use. reply garyiskidding 17 minutes agoprevBeen using Kdenlive for a while now and it is really good for all of the basic video editing i need. So far I've been updating it manually. Would anyone know if there is an auto-update feature available within Kdenlive? reply afarviral 10 hours agoprevKdenlive is great! I just used this recently to create some transitions with transparency. I normally use DaVinci Resolve but I couldn't figure out how to export a video with a straight alpha channel, only premultiplied (it only lets you do this for individual clips, not compound clips or a whole timeline). I think it's fair to say Kdenlive exposes much more power to the user, but I felt that I had a bit of learning to do to understand a few things which have opinionated defaults in programs like Davincei Resolve. For instance, I had to use a composition between two tracks and choose the type of compositing to use between them in order to get transparency exported, whereas in other programs this would default to add or multiply which behaves in a predictable way with less effort (but with less control). In saying that it feels like Kdenlive is an incredible amalgamation of very powerful plugins and parts! I wish it had a bezier curve editor for things like fades or tweaking keyframe animations. After a while I switched to blender (since I used it to create the source footage) but it was a little glitchy, the video somehow getting out of sync with animated opacity and it confused me how you \"Render Animation\" in blender which is the same option as for rendering your (3D) Scene in blender, and where to find the option to turn off the video editor composition which overrides this as soon as you start adding clips to the video editor. It doesn't really compare as it's just blender, with all it's intricacies, and video editing bolted on to it, whereas Kdenlive is just about video editing. Anyway, great work! reply pteraspidomorph 4 hours agoparentI subscribe this comment, non-linear interpolation using curves would be extremely useful. Another obvious thing I often miss is the ability to change the length (duration) of static image clips in the project bin (currently? - haven't tried the latest version yet - the only way is to set a length in the settings before importing). It seems like it doesn't matter at first, since it's a static image, but start adding effects and keyframes and any resizing done in the timeline can mess everything up. reply afarviral 4 hours agorootparentThat's another great suggestion, and probably a pretty easy contribution to tackle for someone. I'll make a mental note to look in to it next time I'm on holiday. reply gorjusborg 15 hours agoprevI accidentally stumbled onto kdenlive when I needed to crop a video clip years back. It's really good; the developers should be proud. reply PeeMcGee 14 hours agoparentLikewise, was looking for something with a workflow similar to Sony Vegas and came across Kdenlive. I mostly just needed to perform some simple cuts and fades with multiple audio and video tracks, and was very impressed by its capabilities and how intuitive it felt to use. reply freedomben 12 hours agorootparentSame. I was a Vegas guy through the late 90s/early 00s, and then got out of the area for a few years. When returning I really just wanted Vegas like from before and tried all kinds of stuff. It was very surprising to me how hard it was to find! Kdenlive is really, really great and is super intuitive to me. reply PeeMcGee 10 hours agorootparentWhen I was looking into it I was discussing with someone how Sony's refusal to meet the market led to Vegas being the top 5 most pirated software. I learned that Sony sold it to Magix during a big selloff, who apparently thrived by simply switching to a realistic pricing model and resuming maintenance. reply atum47 15 hours agoprevI've been using kdenlive ever since i switch to Linux back in 2016. All my YouTube videos are a combination of simple screen recorder and kdenlive, although i don't put much effort in them reply wkat4242 14 hours agoparentI always use OBS for screen recording but otherwise the same story :) I always use KDE as my primary desktop and love it. It's amazing. Kdenlive is only one of the many amazing apps they produce. And I'm proud to donate to them monthly. reply freedomben 12 hours agorootparentSame. Most of my videos aren't public, but I use this stack for everything. Linux base, OBS for recording, and Kdenlive for editing. Do you do green screen? I've had a horrible time finding a web camera that doesn't mess up the green screen by auto color correcting. It ends up putting a weird tint on the video that looks awful. Any web cam recommendations that work good with linux/obs? reply wkat4242 8 hours agorootparentNo, I don't record myself directly. I usually use it together with a HDMI capture module to record meetings. My work only allows recording Teams meetings to Microsoft Stream which has really crap quality and we can't download from it so I can't edit them, and we can't install any software like OBS directly on our laptops. Sometimes I need to archive trainings for later use. So I use the HDMI out to record it from another computer. Then I can edit the video to make it more to the point, add some on-screen popups etc. Then I share those on MS Stream (because unfortunately that is the platform we have to use despite it being crap). It's a bit of a workaround but it does the job. The cheap Aliexpress HDMI dongle does the job admirably, it gets really really hot but it survives the day :) But green screen yeah no never tried that, sorry. reply nickstinemates 14 hours agoparentprevBasically same, on Windows too. OBS for recording, kdenlive for editing. reply avinassh 15 hours agoparentprevhow does it compare with ScreenFlow, which I have seen most people recommend for YouTube / Screencasts. I am a newbie, so I am learning to make videos and edit them. reply atum47 15 hours agorootparentNever used screen flow so i don't have any comparing parameters reply focusedone 15 hours agoprevI use Kdenlive once a week for a quick top/tail edit. Always amazed at how well it runs on low-end hardware. Excited to see how the new version feels under Wayland. reply mikae1 3 hours agoprev> The initial implementation of the long awaited easing interpolation modes for keyframes has landed. Yes! I'm a professional editor and I've waited years for this. Seems it's not quite there, but they're getting close. At work I have to use Premiere I have to use Premiere and Resolve, but at home I'm on this long quest to open source everything. The lack of easing has held me off from Kdenlive even for home use. Nothing looks more amateurish than those stiff linear animations and transitions. reply amatecha 8 hours agoprevKdenlive is awesome. If any contributors read this-- thank you!! reply snvzz 4 hours agoprevDear web developer: Please do not override browser scroll behavior. reply hn_acker 10 hours agoprevI've used Shotcut in the past but not Kdenlive. Shotcut doesn't support subtitle files at all, so I'm quite pleased to learn that Kdenlive supports input and output of multiple subtitles files now. reply xnx 15 hours agoprevCool to see continued work on this. At various points I've tried ShotCut, OpenShot, and Blender's video editor, but haven't found a clear favorite. reply unstruktured 11 hours agoprevBig fan of kdenlive. I switched to it after one too many openshot bugs. Admittedly I always want kde apps to be good but in this case it actually is. Openshot was easier to learn though, personally. reply 2Gkashmiri 10 hours agoprevA few years ago a big complaint against kdenlive was that it would crash all of a sudden, randomly and general instability and generally buggy. Is that the case with say 24.02 or has that thing been improved? reply pteraspidomorph 4 hours agoparentThe one crash I regularly experienced about a year ago had to do with saving and reusing effect stacks. I'm not sure if that still happens as I stopped using that feature entirely. Far less regularly, I've seen crashes when replacing video files used in the project. I also recommend setting the framerate of a project first thing and then never touching it again. reply seabrookmx 5 hours agoparentprevI moved to Resolve because of this. It's maddening when you're new to editing and lose an hour's worth of work due to a crash like this. The UI and features were plenty for my use case otherwise. reply thebiglebrewski 12 hours agoprevCongrats - Kdenlive is an amazing program and awesome example of open source software gone right! reply shmerl 14 hours agoprevNice editor. I'm not sure how it compares to professional ones, but it was more than enough to make some video compositions when I needed it and it was easy to use and figure out. reply ggambetta 15 hours agoprev [4 more] [flagged] jdboyd 14 hours agoparentThe free version has this issue \"The free version of DaVinci Resolve on Linux does not support h.264 and h.265.\" Having to transcode to dnxhr is a bummer. I should probably just buy a dongle though. reply unpopularopp 14 hours agoparentprev [3 more] [flagged] bornfreddy 14 hours agorootparent [–] It's not just that. With kdenlive (awesome software, btw, kudos to developers!) I can be pretty sure it will still be around next time I need it. With proprietary software, who knows? Maybe Adobe will buy them and start charging exorbitant monthly fees. Or they will go out of business, or... reply wkat4242 14 hours agorootparent [–] Yeah I've been bitten by that before. Invest a ton of time in learning a tool and then they drop the free tier or neuter it. Or drop the whole thing altogether like 123D Design :( reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The latest version of Kdenlive, 24.02.0, is out with significant improvements, enhanced performance across different operating systems, packaging modifications, and bug fixes.",
      "New features in this update include multiple subtitle support, keyframe easing interpolation modes, group clip effects, and various bug fixes and enhancements.",
      "Development is focusing on stabilizing remaining issues, with the ability for users to check for updates without an active network connection, alongside commits addressing bugs, improving performance, user experience, and project stability."
    ],
    "commentSummary": [
      "The recent release of Kdenlive 24.02, an open-source video editor, is commended for its easy-to-use interface and features, drawing comparisons to Lightworks and Blender.",
      "DaVinci Resolve is suggested for professional editing, while Shotcut offers a no-cost and user-centric alternative.",
      "Linux users value Kdenlive for its efficiency and open-source ethos, although some raise stability issues, emphasizing the importance of evaluating software's long-term sustainability."
    ],
    "points": 258,
    "commentCount": 53,
    "retryCount": 0,
    "time": 1710179215
  },
  {
    "id": 39669167,
    "title": "Airbnb prioritizes renter privacy by banning indoor security cameras",
    "originLink": "https://www.theverge.com/2024/3/11/24097107/airbnb-indoor-security-camera-ban",
    "originBody": "Security/ Tech/ Apps Airbnb is banning indoor security cameras Airbnb is banning indoor security cameras / The rental app is also introducing new disclosure rules for outdoor security cameras and noise decibel monitors. By Emma Roth, a news writer who covers the streaming wars, consumer tech, crypto, social media, and much more. Previously, she was a writer and editor at MUO. Mar 11, 2024, 2:33 PM UTC Share this story If you buy something from a Verge link, Vox Media may earn a commission. See our ethics statement. Nick Barclay / The Verge Airbnb will no longer allow hosts to use indoor security cameras, regardless of where they’re placed or what they’re used for. In an update on Monday, Airbnb says the change to “prioritize the privacy” of renters goes into effect on April 30th. The vacation rental app previously let hosts install security cameras in “common areas” of listings, including hallways, living rooms, and front doors. Airbnb required hosts to disclose the presence of security cameras in their listings and make them clearly visible, and it prohibited hosts from using cameras in bedrooms and bathrooms. But now, hosts can’t use indoor security cameras at all. The change comes after numerous reports of guests finding hidden cameras within their rental, leading some vacation-goers to scan their rooms for cameras. Airbnb’s new policy also introduces new rules for outdoor security cameras, and will now require hosts to disclose their use and locations before guests book a listing. Hosts can’t use outdoor cams to keep tabs on indoor spaces, either, nor can they use them in “certain outdoor areas where there’s a great expectation of privacy,” such as an outdoor shower or sauna. Additionally, listings will have to disclose noise decibel monitors, which hosts might use to measure whether there’s a party going on in their rental — something that Airbnb banned in 2022. “These changes were made in consultation with our guests, hosts, and privacy experts, and we’ll continue to seek feedback to help ensure our policies work for our global community,” Juniper Downs, Airbnb’s head of community policy and partnership, says in a statement. Airbnb hosts will have until the end of April to remove the security cameras from inside their listings. If a guest reports the presence of an indoor camera after that, Airbnb says it will investigate and that it could remove the host’s listing or account as a result. The new policy still can’t control the presence of hidden cameras, but it will at least offer some peace of mind knowing that rule-abiding hosts can no longer put cameras anywhere in their rentals. Most Popular Most Popular British monarchy rocked by bad Photoshop job Airbnb is banning indoor security cameras Samsung’s new midrange Galaxy A55 arrives with improved security and materials Oscars 2024 winners: Oppenheimer and Christopher Nolan lead the way Porsche Taycan Turbo GT sets new lap record at Laguna Seca Verge Deals / Sign up for Verge Deals to get deals on products we've tested sent to your inbox daily. Email (required)Sign up By submitting your email, you agree to our Terms and Privacy Notice. This site is protected by reCAPTCHA and the Google Privacy Policy and Terms of Service apply. From our sponsor Advertiser Content From",
    "commentLink": "https://news.ycombinator.com/item?id=39669167",
    "commentBody": "Airbnb is banning indoor security cameras (theverge.com)258 points by matbilodeau 18 hours agohidepastfavorite427 comments endisneigh 18 hours agoIn my experience Airbnb isn’t even cheaper, just has more “interesting” housing available. Back to the topic - removing listings isn’t a sufficient enough punishment imho. And how would you even prove it from the guests side? Take a picture I suppose? But how do you stop bad actors? Not sure what the correct solution is. Someone who wants to spy can purchase a spy camera which is very abundant these days. reply bombcar 18 hours agoparentAirBNB \"when it started\" was quite likely to find you quirky and cheap options. Now it's commoditized and a business, and the quirky and cheap options are few and far between. Part of the degradation is both the company, the sellers, and the buyers - back in the very beginning it wasn't well known and most of the customers were well behaved - I'm not sure that's entirely the case now, as the customer base has grown. reply casenmgreen 17 hours agorootparentI'm a digital nomad. One luggage, no permanent home, been fifteen years. I used to use AirBnB all the time. They gradually become more and more, well, \"large company\". Governments gradually made AirBnB illegal or effectively illegal, largely or wholly killing off AirBnB in given cities, or countries. For example, it used to be about impossible to get into Amsterdam, because of rent control and renting regulations; no supply of places to let. Then AirBnB came along, and everyone and their dog let places on AirBnB. You could get in, and at a good price. Then AirBnB was essentially banned, and now you can't get into Amsterdam. It was always that there were a lot of people offering places who didn't know how to price what they had; so you'd see a lot of properties, but a lot were crazy money. Still, if there were enough, there would be enough places at sane prices you'd get somewhere. These days, I still look at AirBnB, but I see that their fees have continually risen over time, and are now like 10% or 15%, and there's just no content. I was in Paris last couple of months. There was nothing viable on AirBnB, unless you wanted to pay several thousand a month. There was one what I concluded was a scam, a very dodgy letting agency, who had lots of apartments, all with no ratings, but very bad reviews on-line. I think they were continually deleting and remaking their lets on AirBnB, to get rid of negative reviews. Finally, AirBnB itself, regarding \"large company\", became unreliable as a service, in that I never knew, when I came to us it, if log-in would still work. I recall the first time log-in failed for no obvious reason, and the and the only option was \"email support and we'll contact you in a few days\" - and I was looking to move in about two weeks time. After that, I put up my own HTTPS proxy, which I now use whenever I use AirBnB, to avoid AirBnB suspending my account for a few days, until support get back to me. I also recall one episode about ten years ago where I had to phone support. It was a three hour long screaming nightmare of hell and madness. So - fees are now rather high, support don't bother - anything but support - I had to backdoor my own net traffic to use the service, no viable apartments in most locations. It was great, but now it's really not. The one and critical thing AirBnB got right was building into their platform the expectation owners would offer discounts for stays over a week, or over a month. I don't see this on other platforms, and it makes pricing on other platforms crazy. If I come and stay for three months, I expect a discount for giving full occupancy over that time. If you don't offer that, you're off the menu. reply Doxin 56 minutes agorootparent> For example, it used to be about impossible to get into Amsterdam, because of rent control and renting regulations; no supply of places to let. Then AirBnB came along, and everyone and their dog let places on AirBnB. You could get in, and at a good price. Then AirBnB was essentially banned, and now you can't get into Amsterdam. Slightly off topic, and not aimed at you in particular but at tourists in general: For the love of god visit other cities than Amsterdam. Amsterdam is expensive and overcrowded. There's a whole country worth of interesting places to visit instead. reply throwaway2037 21 minutes agorootparentWill you say the same for Paris, London, Rome, Milan, New York, Miami, Los Angeles, San Francisco, Beijing, Tokyo, Hongkong, Singapore, and Seoul? All of those cities (and more!) fit this template: \"X is expensive and overcrowded.\" Yeah, and they have lots of interesting culture so people want to visit. They are willing to pay the high prices and put up with crowds. reply bombcar 17 hours agorootparentprevAll of the above makes total sense (to me) when you realize that people don't know how to price rentals at all - just like the average person doesn't know how to price restaurant food. When you know truly how to price (and cost!) something, you know when and where you can offer discounts. If cost to acquire a tenant is $2k, then the discount to preserve a good tenant should be about $2k per tenant turnover time. reply unethical_ban 9 hours agorootparentAlmost like Uber was marketed, AirBnB was not initially sold as a \"make primary income off this\". It was a way for people with excess assets (spare car and some time, spare room in a big house, etc) to make some money and the companies were the broker. Turning Airbnb into a full-blown rental market with people and LLCs buying houses just for AirBnB was the downfall. At least, that's my interpretation. reply mlrtime 7 hours agorootparentI think Paris tried to curb this buy allowing Airbnb, but you can only rent out max 50% of the year. This makes it so local owners can make some money, but corporations wouldn't be able to get the yield they need at 50% occupancy. reply kshacker 4 hours agorootparentAnd did it work? I ask because corporates usually figure out how to use the rules to their advantage and I am now trying to sniff through how they would play this. reply RussianCow 3 hours agorootparentYeah, doesn't this just end up increasing short term rental prices across the board? It's not like there is a shortage of demand for lodging in Paris. reply brailsafe 13 hours agorootparentprev> the average person doesn't know how to price restaurant food Could you be more specific? reply xboxnolifes 12 hours agorootparent\"It would cost less for me to buy the groceries and make this at home\". reply brailsafe 10 hours agorootparentPerhaps it's a bit pedantic, but that seems less like pricing restaurant food and more like pricing food in general, which is totally valid if you're on a fixed budget and considering how much to spend for food. reply ghaff 12 hours agorootparentprevNot that I always could or would care to go to the effort for a fancy meal, but in general it's going to be a lot cheaper to prepare most meals at home. Or maybe I'm misunderstanding you. reply xboxnolifes 10 hours agorootparentOf course it'll be cheaper to make most meals yourself. Especially since the imagined price doesn't tend to include rent, kitchen labor, service staff labor, or depreciating costs of kitchen appliances, which all go into the price of restaurant meals. reply ghaff 5 hours agorootparentI'm living in a house anyway. I'm not paying myself. Appliances last decades. It would cost me money to drive to a restaurant. With rare exceptions (e.g. high end kitchen you don't use much and eating out really cheaply) there's no world where the costs of preparing meals at home are higher than eating out every meal. reply brazzy 3 hours agorootparentThe point is that many people expect them to be priced similarly because they think of the ingredients as the main costs (because those are the only costs they actually see when cooking for themselves), which they are not. reply adolph 16 hours agorootparentprev> people don't know how to price rentals And \"helping\" them is a lawsuit magnet: Big landlords are colluding to raise rents, D.C. lawsuit alleges https://news.ycombinator.com/item?id=38155840 reply bombcar 16 hours agorootparentIf I had a group of graduate students at my beck and call I'd love to try to factor out what effect single-family rentals have had on the rental market in toto - because once you remove appreciation, they're often losing money each month, which means they're subsidizing the renters (and willing to do so because they're making it up on highly-leveraged appreciation). reply RussianCow 3 hours agorootparent> because once you remove appreciation, they're often losing money each month I'm curious if you have a source for this? Intuitively, it makes sense, but I haven't actually seen any figures that support this. Although my guess is that data would be hard to come by. reply downWidOutaFite 13 hours agorootparentprevWhy would the math be different for apartment buildings? They also appreciate and they usually have better accountants to make sure no money is left on the table if it's available. reply bombcar 12 hours agorootparentApartment buildings get handled in a business-like manner, but are also apartments (which have downsides for the renters). My theory is that single family homes get rented below what it costs to rent them and the landlords make it up via appreciation. It's pretty obvious that there are places like this, where the rental prices are substantially below 1% of the purchase price (one of the \"rule of thumbs\"). reply cryptoxchange 3 hours agorootparentI think it’s safe to call this a fact in markets like the Bay Area. In college three friends and I rented a 1000sqft 2 bed 2 bath apartment with one parking spot for $X, then the next year a 2000sqft 4 bed 4 bath 2 car garage for $1.15X. Per zillow, the owner was renting it to us for 0.3% of the purchase price. The only way for them not to be losing their shirt is crazy appreciation. reply lazyasciiart 15 hours agorootparentprevThe lawsuit is about helping them using non-public data from other companies. Nothing stops the platform from making all the rental data public and giving the same recommendations. reply cultofmetatron 2 hours agorootparentprev> The one and critical thing AirBnB got right was building into their platform the expectation owners would offer discounts for stays over a week, or over a month. I'm also a digital nomad. new country every 1-3 months and I completely agree. I'd love to find an alternative but the one month discount is critical for keeping my costs fairly low. booking doesn't even allow you to book over 30 days. reply scheme271 2 hours agorootparentI think a lot of landlords are worried about letting anyone stay more than 30 days. That's usually when tenant's rights kick in and removing someone becomes a complicated process with housing court and potentially months without rent. reply DenseComet 16 hours agorootparentprevWhat have you been using in place of AirBnB? reply scarface_74 13 hours agorootparentprevI think it’s a good thing that laws are being passed to make it mostly impossible for AirBnbs to be in residential areas and to make it easier for residents to find affordable houses instead of tourists reply jjjjj55555 13 hours agorootparentI don't trust the laws. I think they're either implemented on behalf of the hotel chains, or else they only entrench people who already got \"permits\" for short term rentals, which now become basically golden tickets. I seriously doubt we'll see rents drop significantly anywhere. reply scarface_74 10 hours agorootparentAnd that’s how it should be - a hotel is a business in a commercial zone with professional management, pays taxes and doesn’t negatively affect home affordability reply mlrtime 7 hours agorootparentAnd why shouldn't I be able to rent out my apartment for a week if I go on vacation? This was supposed to be the intent of Airbnb and is still a valuable resource if available. reply rtpg 1 hour agorootparentI think the idea is that your neighbors didn't sign up to live next to total randos. How valid that argument is is up for debate. And it's not like you can't still rent out to friends or the like. Just totally financializing and anonymizing that interaction turns it into a different sort of interaction. Social trust is a thing, and avoiding having _every single human interaction_ pass through some broker and be financialized is not the worst. But really... a touch of social proof never hurts. reply squokko 4 hours agorootparentprevWhy shouldn't your neighbors be able to use their powers as voters to lobby their government to pass laws preventing their quiet suburban street from being used as a distributed hotel? reply username332211 2 hours agorootparentYour framing is such a good defense of the arbitrary and thoughtless use and abuse of political power. \"Use your power as a voter\", what could go wrong? Why shouldn't our neighbors use their powers as voters to keep their peaceful suburban street from being used as housing for migrants, minorities and other undesirables? I'd bet that'll affect housing desirability far more than short term rentals. Why shouldn't they use their power as voters to ban the construction of affordable homes in their vicinity? Why shouldn't we use our power as voters to proscribe alcohol in their town? What could go wrong? Your power as a voter isn't supposed to be the power to arbitrary meddle in other people's business. reply squokko 49 minutes agorootparentYes, all of those should be legal. And other voters can oppose them. For example, by electing national politicians who can pass laws like the Civil Rights Act or the Fair Housing Act. reply mplewis 2 hours agorootparentprevWhat do you think a community is if not a group of people who decide on a common way to share resources and build harmony? reply Brian_K_White 3 hours agorootparentprevThis. I hate staying in hotels and love varied and individual places like airbnb has, but this is simply valid. As a consumer, it has to be good enough if there are only some areas where it's allowed, and some where it's not. Or maybe allowed but only up to a certain density or percentage. Anywhere it would be allowed without limit I would probably not want anyway, just like a hotel. Most airbnbs are run like little hotels now anyway and I hate most of those, but at least there is variety and plenty are good still. reply TulliusCicero 4 hours agorootparentprevIt'd be better if there was enough housing to support normal residential uses AND Airbnb, but it's easier to just blame Airbnb for problems. reply hn_throwaway_99 17 hours agoparentprev> In my experience Airbnb isn’t even cheaper, just has more “interesting” housing available. I think it depends. For relatively short stays of one or two people, a hotel is nearly always a better deal in my opinion (for short stays the cleaning fee alone can be a big percentage of the total price). But for week or longer stays with a family or larger, they usually come out ahead given multiple bedrooms and the fact that the cleaning fee is amortized over more nights (not to mention access to a kitchen can save a ton of money). In this segment, though, they're really no different from VRBO or a host of other vacation rental providers that have been around for a long time. reply ghaff 13 hours agorootparentThe whole house thing for a group is where I see Airbnb (or VRBO) etc. being a big win over most hotel options. But for most singles or couples traveling, predictability and lack of issues is pretty much the priority. That's not to say I only stay in chain hotels--I've stayed in plenty of B&Bs and small inns--but I don't like rolling the dice on that aspect of my travel more than I need to. reply Brian_K_White 2 hours agorootparentMyself, wife, and most of both of our whole families find all hotels lifeless and impersonal and uninteresting. Sure nice ones are nicer than bad ones, and it's fun to do that once in a while just as an experience, but generally the nicest hotel with impeccable staff isn't prefferable to a cozy appartment even if it has a few problems. The dice roll is way better. Even the bad ones once in a while are at least different and fun to mock. It's funny how almost the whole families have the same gut response to stuff like that. Look at something and what is the most immediate natural reaction to an idea or the look of a thing. And some other family might all have the same some other natural reaction. Like one person finds gold attractive and another finds it gawdy, and so do their whole family & friends groups. For me, hotels are only good for work. That's the only time the predictability matters. I avoid scheduling as much as possible on vacation. There are events of course but I don't try to tetris pack the days. But for a few days or a week at a customer, then a hotel just something I need like a highway rest stop bathroom. reply lotsoweiners 7 hours agorootparentprevAbsolutely. Most of the Airbnb/VRBO that we’ve done was to accommodate 12-15 people including 5-7 children. Only other time I do Airbnb is for specialty situations like a beach house where we can have a big patio with a grill etc. reply EVa5I7bHFq9mnYK 15 hours agoparentprevAirbnb has a normal fridge, a microwave, a washing machine, a dishwasher and 700-100 sq ft area. A similarly priced hotel room will have 250 sq ft area and none of the above amenities. reply al_borland 3 hours agorootparentThis all depends on the AirBnB. I stayed in one in Tokyo that had a kitchen and washing machine, but no TV. I had never stayed anywhere without a TV before (hotel or AirBnB), so I suppose it’s my fault for assuming it would have on. I was very disappointed, as I was planning to watch at least some Japanese TV in the evenings and instead just had to watch whatever was on my iPad. With a hotel, especially bigger chains, there are certain things a person can count on. With AirBnB I feel like I need to be a detective and keep a list of all my requirements, even ones I thought were default standards. Also, at the Tokyo AirBnB, once I got there I found a sign in the bathroom that essentially told me AirBnB wasn’t allowed in the building, so if anyone asked who I was, I had to lie and give the prepared backstory. I’ve never had to lie to skirt the law/rules when staying at a hotel. My next trip I’m staying in a hotel, and it has a kitchen as well (with a sink and range), so I can go to the store and prepare food if I want. It’s not 1,000sqft, but how much space do I really need for a couple weeks? My goal is to get out and do things, not stay inside the whole time. I’ve also had many hotels with a fridge and a microwave. The fridge might not be full size, but again, for a week or two, how much space does a person really need? reply ghaff 13 hours agorootparentprevSuite hotels have all that except maybe the washing machine and are often pretty comparable in price to \"regular\" hotel rooms. Those tend to be my preference although I use the refrigerator more than the rest of the kitchen. reply TulliusCicero 4 hours agorootparent> Suite hotels have all that except maybe the washing machine and are often pretty comparable in price to \"regular\" hotel rooms. I have not found this to be the case at all. Getting a suite with a kitchen usually involves paying a large premium to a regular hotel room. reply klausa 3 hours agorootparentThey don't mean upgrading to a suite at a \"regular\" hotel; there are chains specializing in renting out \"home-style\" rooms (\"extended-stay\" is the industry term, I believe), and I've seen them referred to suite hotels sometimes. The biggest chains have these sub-brands in the space, based off three minutes of googling/wikiing: https://en.wikipedia.org/wiki/Residence_Inn_by_Marriott https://en.wikipedia.org/wiki/Staybridge_Suites https://www.hyatt.com/development/ourbrands/hyatthouse https://en.wikipedia.org/wiki/Homewood_Suites_by_Hilton I never stayed at one, so can't comment how accurate the OP was re/price. reply TulliusCicero 3 hours agorootparentI was thinking of those options too. I do tend to look for them when I'm searching for hotel rooms. reply scarface_74 13 hours agorootparentprevI literally stayed in hotels from October 2022 - October 2023 permanently with my wife except for two months between January and February last year as we flew around the country. We generally stayed in Homewood Suites, Home2Suites, Embassy Suites and Hyatt Places. Most Hyatt Places didn’t have washer and dryers onsite. We had to either Uber to take our clothes or use a wash and fold delivery service. The rest of the places did. The rooms are usually around 400 square feet. Homewood Suites and Home2Suites have dishwashers and full refrigerators. Home2Suites you have to ask for a “burner” reply jupp0r 16 hours agoparentprevIt's not about the price. Hotel suites with multiple bedrooms are extremely hard to find or cater more to the super luxury segment. reply zeroonetwothree 17 hours agoparentprevIt’s much cheaper for larger spaces. If you want a 2 bedroom living space Airbnb is probably 3-4x less than a hotel. reply jupp0r 16 hours agoparentprevUnless somebody is making serious efforts and running hidden data wires, spy cams are pretty easy to find by the average nerd equipped with a cheapish spectrum analyzer by their RF signature. reply bmicraft 13 hours agorootparentAre they? Many are wifi connected, but record onto an sd card, which means they shouldn't be easily detectable, except when accessed, right? reply jupp0r 10 hours agorootparentEven if they are not sending data, there will be periodic communication between the access point and the device. Wifi power saving modes are pretty advanced and described here: https://howiwifi.com/2020/06/25/power-save-methods/. reply ransom1538 18 hours agoparentprev\"In my experience Airbnb isn’t even cheaper, just has more “interesting” housing available.\" I still, can not wrap my head around this. The idea behind airbnb was to rent out unused parts of your house. You STILL can not do this. Motel 6 is cheaper all day everyday. I will keep giving my $80 to Motel 6, but, I KNOW there is someone out there that has a room and needs $80. The original problem still exists. I can't pay $120 + $30 in cleaning fees, plus do your house chores Sarah. reply smelendez 17 hours agorootparentMy suspicion is those cheap single rooms were never worth it for the hosts once you consider the long tail of things that can go wrong. One bad guest who upsets the other residents and leaves the property in bad condition will sap your earnings and throw off your life. Airbnb might eventually reimburse you for damage, but there’s still your time and aggravation. Also any household maintenance issue, like an internet outage or finicky toilet or a microwave on the fritz becomes an emergency if it’s an amenity you promised to a guest. Even realizing the guest towels are getting a little worn might be a cause for panic. Motel 6 has a handyman on call, maybe the owner or a relative, and can take a room out of rotation if they’re not booked solid, but Joe Airbnb is stuck begging a plumber to come out on short notice or rushing to Target before check in time. reply ghaff 13 hours agorootparentI suspect that people in general are very bad about factoring in fully amortized costs--especially for expensive things that may be unpredictable and likely only happen once in a great while. I was having a discussion not that long ago about how a house that's paid off actually costs a lot of money. reply shagie 14 hours agorootparentprev> My suspicion is those cheap single rooms were never worth it for the hosts once you consider the long tail of things that can go wrong. They stay too long, and now they're a tenant rather than a guest - and you can't evict them. https://www.sfgate.com/travel/article/airbnb-tenant-squatter... https://www.airbnb.com/help/article/805 reply pjc50 17 hours agorootparentprev> The idea behind airbnb was to rent out unused parts of your house. Airbnb raised rents to to point where either people have no space or those spare rooms are filled with longish-term tenants, at lower void rates. Or their children had to move back in because all the inner city housing is now AirBnbs. reply squokko 3 hours agorootparentprevIs it so surprising that Motel 6 is more operationally efficient? That Sarah needs more money than Motel 6 to allow a unknown stranger into her living space and to clean up after them, rather than a gigantic hotel chain with decades of experience and thousands of locations? reply sschueller 15 hours agoprevI find it sad how Airbnb has taken a market that worked (in many places) and re introduced all the problems that have been resolved through decades of regulations being put in place for hotels etc. Chances of finding a camera in a hotel room are near zero while at Airbnb you have no idea what kind of pervert the renter is. At this point prices for Airbnb have rissen so high why not get a hotel? It's like we don't learn at all from history or just like to forget so we can make an extra buck. reply al_borland 3 hours agoparentThis sounds like a lot of tech companies that look to shake up an old industry. They say they’re starting from first principles and try to reinvent every wheel. It usually ends up meaning pain for the customers as they rediscover why the old players in the industry do things the way they do them, and then act like they discovered something new. reply ilrwbwrkhv 13 hours agoparentprevMost people these days get a hotel anyways. Also I only took a airbnb once in my life. Anybody who takes airbnbs regularly has for sure slept on somebody else's dried body fluids. reply matsemann 2 hours agorootparentI'd wager the chance is higher in a hotel. Source: worked at a hotel. reply mlrtime 7 hours agoparentprevSounds like an ad from the Hotel lobby. People should still be able to rent out their homes if they want. reply AlecSchueler 21 minutes agoprevDefinitely always scan your rooms. I volunteer with helping women who were filmed involuntarily for sexual purposes and the level of voyeuristic content from AirBnb hosts has been skyrocketing. reply mholt 18 hours agoprevI'm worried this ban will only affect honest hosts. Anyway, what Airbnb should really ban is hosts charging a mandatory cleaning fee when also requiring their guests to do the cleaning. We just stayed at one this weekend, and like almost all our previous bookings, the host asked us to: - Take off all bedding and towels and start a load of laundry - Do all the dishes and start the dishwasher (but required to hand-wash some items) - Collect and take out all the trash to the curb - Clean the grill/stove/oven And we still had to pay a $200 cleaning fee when booking the place. A hotel would never require this of us or charge us extra. reply UniverseHacker 17 hours agoparentAt this point the cleaning fees are just a scam to make the listing fee seem artificially lower. People did the same thing on eBay by making the items almost free but the shipping expensive, until they fixed the eBay interface, e.g. by letting you sort by price + shipping and showing shipping fees before you click. reply Scoundreller 17 hours agorootparent> People did the same thing on eBay by making the items almost free but the shipping expensive, until they fixed the eBay interface There’s a longer story to this: eBay used to exempt shipping fees from its commissions. So sellers would be screwing eBay and, being an efficient market, as it often is on eBay, the savings would be passed onto the buyer. I’d sell games for 1 cent with $10 shipping (cost to me: ~$2) and put the shipping cost in the title for clarity. If you didn’t like it, fine, pay more to buy from someone else. Now they charge it on the whole purchase, which sucks when they take a ~15% commission on sales taxes and shipping and sellers can’t easily tack on a premium to these. So they get lower net proceeds when selling to someone that has high sales taxes or shipping fees. This really discourages sellers from shipping internationally where those costs are higher. Now that eBay charges percentage fees for “promoting” your item and getting visibility, bulk sellers will often post the exact same item, unpromoted, for less if you can be bothered to go through their “other items for sale”. Have seen some wild examples where a seller sold one item at very different prices. They must have had items with 50% promotion fees, 25%, 5% and 0% (evidently passed onto the customer through different pricing) to capture the whole market. reply _puk 16 hours agorootparentAs a user I always avoided those low cost, high postage listings as it just seemed a good way for a seller to avoid refunding the full amount. Refund would always be minus postage costs, so you'd get 1 cent back for your $10.01 purchase when returning (i.e. no point) reply Scoundreller 16 hours agorootparentI think it’s up to seller’s policy if it’s a buyer’s remorse return situation. I find it hard to believe eBay wouldn’t make the buyer whole in a damaged/not as described case. Been a while since it came up for me (always sold stuff as “final sale” and if I avoid buying fragile stuff). Once had a hard time repaying a buyer that returned an item where I didn’t describe it correctly (screw you apple for having nearly identical LCDs but with slightly different camera cable lengths) reply EasyMark 16 hours agorootparentprevThanks for the history. I was wondering why I was seeing less crazy rates on shipping. I had no idea it wasn't on the whole purchase price as that seemed obvious to me. I figured it was sellers hoping people wouldn't notice the s&h until the got to checkout reply Scoundreller 16 hours agorootparent> I figured it was sellers hoping people wouldn't notice the s&h until the got to checkout There was a lot of that too. I’m sure it’s sometimes emotional: sellers wouldn’t want to “subsidize” shipping, so they’d fix the shipping fee to what the max could be for anywhere in the country. What grinds me as an international buyer is when sellers offer “free” shipping domestically, but don’t subsidize int’l shipping by the same dollar amount. Though there can be other time costs for int’l shipping. It depends. reply usaar333 17 hours agorootparentprevCleaning fees exist to charge a fixed fee for a reservation, regardless of the number of days. There's more overhead for a reservation beyond just \"cleaning\". The most consumer friendly thing would be for AirBnB to just explicitly have a fixed \"stay\" fee and a per-day fee. That said, AirBnB's site shows the total cost for a stay when browsing, so I don't really see this as particularly deceptive. I don't care how the host itemizes the bill as long as I know what I'm paying when selecting a place. reply callalex 17 hours agorootparentIt only shows total price in markets where it is legally required to. Meaning they took the time to implement the feature and actively decided to be shitty to their customers when they are given the choice. reply achates 16 hours agorootparentAre you sure? I'm in the US and it displays the entire price (including cleaning fees) for me. reply kibwen 16 hours agorootparentprev> they took the time to implement the feature and actively decided to be shitty to their customers when they are given the choice Ah, but companies have a fiduciary duty to their shareholders to be as shitty to their customers as they can get away with. reply shagie 15 hours agorootparentTo help put an end to this meme... Burwell v. Hobby Lobby Stores, Inc. - https://www.law.cornell.edu/supremecourt/text/13-354 > While it is certainly true that a central objective of for-profit corporations is to make money, modern corporate law does not require for-profit corporations to pursue profit at the expense of everything else, and many do not do so. For-profit corporations, with ownership approval, support a wide variety of charitable causes, and it is not at all uncommon for such corporations to further humanitarian and other altruistic objectives. Many examples come readily to mind. So long as its owners agree, a for-profit corporation may take costly pollution-control and energy-conservation measures that go beyond what the law requires. A for-profit corporation that operates facilities in other countries may exceed the requirements of local law regarding working conditions and benefits. If for-profit corporations may pursue such worthy objectives, there is no apparent reason why they may not further religious objectives as well. reply tareqak 3 hours agorootparentIt would be nice for me as customer to see what sorts of objectives any given company values: a sort of barcode scanner / brand scanner that travels up the tree of parent companies and returns the objectives for each one. reply walterbell 3 hours agorootparentB Corporations have a certification process: https://www.bcorporation.net/en-us/find-a-b-corp/ reply tuwtuwtuwtuw 16 hours agorootparentprevNo they don't. reply dsr_ 16 hours agorootparentMany companies act as though they believe that they have a fiduciary duty to not think further than the next quarter's results. Are you happy now? reply ghaff 14 hours agorootparentCompanies obviously do care (a lot) about their current quarterly results. But if they literally did not care about what happens in the future, they could cut a ton of costs. They probably have a ton of engineering and marketing that isn't essential to the next quarterly earnings. reply tuwtuwtuwtuw 14 hours agorootparentprevSuch companies would fire all employees directly unless they are required to complete the current quarter. I haven't seen many sustainable businesses completely giving up all revenue for all future quarters just to improve the current quarter. It would be a very poor business decision. It sounds like a meme and I'll name it Enterprise Kamikaze. reply objektif 15 hours agorootparentprevDang what does your flamewar indicator say about these? reply Scoundreller 17 hours agorootparentprev> That said, AirBnB's site shows the total cost for a stay when browsing This depends on which site you’re browsing on. I don’t think US does this, but you can browse the AU or Canada version and set your price in $USD because those geographic “editions” have to post the all-in cost up-front. reply bonestamp2 17 hours agorootparentYes, and I think the only exception might be California where it is now required that prices include previously hidden fees such as \"resort fees\" (and \"landing fees\", etc for airlines). reply usaar333 15 hours agorootparentprevI'm in California and see it reply septic-liqueur 17 hours agorootparentprevI haven't checked the platform lately but from what I remember the first search view did not show the total (plus cleaning) fee and only when you advance to book you would see the final cost. While not a scam, it definitely feels like a dark pattern. reply ghaff 17 hours agorootparentprevAgreed. That said, not that I stay at AirBnbs more than rarely but there's definitely been an ooching in both the per-stay fees and the expectations for cleaning. I suspect some of this is that housekeeping services have probably gone up in cost a lot. I thought my housekeeper was sort of an expensive luxury. It is but after talking to some people, the rates I pay are actually pretty modest. But to many other comments, there's probably at least some element of advertising a relatively low nightly price and tacking on a stay fee that may not be as immediately obvious. reply __derek__ 17 hours agorootparentprevAirbnb sellers can charge a fixed fee directly by setting minimum stay lengths or by charging different nightly rates based on the length of stay (e.g., weekly and monthly stay discounts). reply usaar333 15 hours agorootparentFormer isn't the same thing and is in fact less efficient. Is latter actually possible on the platform? Edit: At most latter is possible with complex rule sets. But I'm really not sure: https://www.airbnb.com/help/article/2061 reply Scoundreller 17 hours agorootparentprevWhat if I just want to stay 3 days and I’m willing to pay the fixed fee anyway? What I do see is some booking automation that will dynamically set minimum stay lengths to avoid filling, say, a 3 day gap with a 1 or 2 day booking if it’s far enough out and set minimum stays to the duration of the gap. reply lucisferre 16 hours agorootparentSimple they just have a minimum fee, book whatever days you want. reply EasyMark 16 hours agorootparentprevSo put it in the price rather than \"fees\". That always feels highly dishonest to me. reply matsemann 14 hours agorootparentHow would you make that work? I rent out our cabin. If you stay 1 day or 7 days, it's the same amount of work for me and same costs in cleaning. That's impossible to price fairly without having a flat fee. In my market the customer sees the total price up front. I think this is the best solution for all parties. reply biomcgary 17 hours agorootparentprevAirbnb is a marketplace with a much wider range of \"products\" than standard hotels. I have found stays that fit my needs far better than any hotel ever could. I am both a host on Airbnb and frequent guest. Airbnb's UI needs to guide prospective guests toward specifying what they are looking for so that prices (and price sorting) actually meets their expectations. If you search for a specific date range with # of guests, pets, etc., the sort order needs to be on all-in costs, not daily cost. Maybe they could offer pre-baked \"personas\" - e.g., I am looking for a hotel alternative vs I am looking for a unique experience. reply nox101 15 hours agorootparentI've used Airbnb 25 times. 12 of those times had issues. Issues like \"parking space included\" - show up, no parking \"wifi included\" - show up, wifi is sitting next to window and stealing from neighbor listed on quiet road - after booking address is changed to unit on busy road heater is so loud it sounds like a vacuum cleaner making getting any sleep hard I no longer use AirBnB period. Hotels have extra rooms, reception, cleaning, etc. I'm not saying hotels are perfect but at least for me, the positive experience rate is 90% vs AirBnB which is 50% AirBnb also lost the price wars, at least as the places I've looked. reply archagon 14 hours agorootparentFor what it’s worth, I’ve used Airbnb around 60 times and found it a much better experience than a hotel 9/10 times. Even putting aside amenities such as kitchen and washer/dryer access, hotels always feel like unnerving liminal spaces to me. Very few hotels I’ve stayed at have felt “homey,” which puts a significant psychological damper on my trip. reply ghaff 12 hours agorootparentI guess I don't expect a short-term rental in an urban locale (especially) to be \"homey.\" I'm looking for well-located, clean, and comfortable (enough). There are some brands that I find more welcoming than others but, in general, I'm not really looking for the hotel to be an important part of my experience with some exceptions. reply archagon 12 hours agorootparentIn contrast, some of my favorite travel memories involve unique Airbnbs. People around the world have created some incredible living spaces, and hotels don't let you experience that aspect of travel. reply yurishimo 15 hours agorootparentprevIt’s good for consumers this way I think. The hotels were forced to compete and I think overall are better than they were 20 years ago. For normal people, they are not aware of this different form of rented lodging and if you’re hosting a big family vacation or a retreat, Airbnb is not a bad option. It’s the bottom of the Airbnb market that is completely screwed up. reply jerlam 15 hours agorootparentprevYour persona feature is interesting but I feel that every Airbnb owner would also game it so they end up in the category with the highest prices, just like they're gaming the reviews. Hotels have a star rating so that I never go to a two-star hotel expecting a four-star experience. These star ratings are done independently, and I don't trust Airbnb to provide objectivity when the owners are going to strongly push back against an \"unfair\" rating, and Airbnb will capitulate especially for superhosts and others who have influence. reply tentacleuno 17 hours agorootparentprevNo, they've worked around that too -- now sellers typically include a [completely unrelated] 0.99p item as an option in the listing, which then moves them to the top of the price-sorted view. reply Scoundreller 17 hours agorootparentOr on aliexpress, on items you’d usually buy multiples of, set the shipping to $cents for the first item, then it jumps to $$$ once you add a second to your cart. Sometimes I’ve just bought individual items from several sellers as a workaround. reply mitthrowaway2 16 hours agorootparentUsually when that happens, the shipping date jumps too, because it's chosen a faster shipping method. I'm not sure why but I'm not 100% convinced it's deliberate deception. However they also don't show shipping prices in the search view, and display the lowest-price configuration instead of the range. reply Scoundreller 16 hours agorootparentSome of these would be tiny items. But yeah, on larger items, sometimes the shipping price changed dramatically once you add the 8th item to your cart. I wish users could order stuff by sea for a discount. Sometimes I’m buying camping or sports stuff for another season and don’t need it coming airmail. Arriving months later can actually be beneficial. But I understand, too many people will lose their minds. reply xnx 16 hours agorootparentprevI'm not a fan of how AirBnB has displayed pricing in the past, but don't lose sight of how bad hotel pricing is: taxes and fees not displayed until the checkout page, \"resort\" fees added at any time, etc. There are no good guys here. reply netsharc 16 hours agorootparentMaybe that's the US hotel booking experience, I live in Europe and haven't encountered those dumb fees. reply Symbiote 15 hours agorootparentThere are occasionally local tax rates that differ depending on the type of traveller, e.g. no tax for business travellers, €2/night for tourists. I've mostly seen it in Germany. It's generally written somewhere, but on aggregation sites like hotels.com and booking.com it seems the hotel can't alter the advertised price directly. Example: https://www.hotels.com/ho119759/maritim-hotel-koln-cologne-g... > A city tax is imposed by the city of Cologne. Business travellers with proof of business-related travel are exempt from this tax. For more details, please contact the property using the information on the reservation confirmation received after booking. > An effective city/local tax rate of 5.00 per cent will be charged reply BeFlatXIII 14 hours agorootparentprevThis is also how airlines work these days. Separate airfare from fees and baggage, then trick travelers with low sticker prices. reply ghaff 11 hours agorootparentMost of the many taxes and fees associated with air tickets are built in. Not checked luggage in particular in many cases although I don't pay that on my usual airline and rarely have it anyway. Otherwise, unless I want to upgrade, the price it gives me is what I pay. reply mellutussa 16 hours agorootparentprevThis is why I can't use AliExpress. Ordering by price+shipping is truly an awful experience. The people in charge of that feature take incompetence to a new level. reply pjerem 16 hours agorootparentpreveBay could have fixed this a long time ago. Most marketplaces (at least in my country) manage the shipping transparently (by taking the shipping fee and providing a pre paid label to the seller). Idk in the US but I know nobody who still use eBay for anything, I don’t know how they are still alive. reply EasyMark 16 hours agorootparentI fix old electronics things like phonos, radios, and other sound related things as a hobby and resell(mostly 60s through 90s). Ebay is still great (well \"good\") for old parts and selling my warez. Although I do sell more locally as I've gotten some word of mouth, and it's more business than I can do with my day job. Anyway ebay still works for older products/parts for electronics and I reckon other fixable things. reply ghaff 11 hours agorootparentThey're still sometimes useful for old parts and the like. But there was a period when I used them all the time--admittedly as something of a novelty. Now, it's maybe a once a year sort of thing. Global supply chains have made a lot of the used market pretty tough for things you can just buy new. (I admittedly work a lot harder to get rid of stuff than acquire it these days.) reply Scoundreller 16 hours agorootparentpreveBay can now show the shipping cost tailored to your zip/postal code, and shows it up-front. eBay is an undertold story in failure (to innovate). For sure, still a going concern, but odd of such a large company taking the Craigslist approach of “let’s stay small and just do enough to keep it working comfortably for all of us” eBay practically invented online shopping at scale and had a huge head start but failed to maintain marketshare. It’s still the catch-all for products that don’t fit in other marketplaces. For me, it’s a place to buy a lot of Amazon items but at a lower price (at some convenience cost). Or niche/used items. Amazon has overly regionalized (sometimes I can get items in Canada from Amazon.com cheaper than amazon.ca for example), while eBay is still a generally global interface. reply Perepiska 16 hours agorootparentprevI remember a wave of $1 iPhone charging cables on Amazon! (with expensive delivery cost :) reply ModernMech 17 hours agorootparentprevAnd those were a twist on the old infomercial trick: Get a second one absolutely free! (just pay a \"small\" shipping and handling fee that is as much as a second one) reply kloch 16 hours agorootparentThe \"But wait, there's more!\" device predates infomercials. It goes at least as far back as the \"Atlantic city boardwalk pitch\" from the early 20th century: https://en.wikipedia.org/wiki/K-tel > K-tel was founded by Philip Kives,[4] a demonstration salesman from Oungre, Saskatchewan.[5][6] Kives had worked at a number of jobs as a young man, including selling cookware door-to-door and in a department store, and as a pitch-man on the Boardwalk in Atlantic City. reply hn_throwaway_99 17 hours agorootparentprevCleaning fees aren't a \"scam\" - most property owners really do need to pay someone to clean the property at the end of a stay. But, as another commenter said, they can be used as a way to encourage longer stays, as with most places you only pay a single cleaning fee regardless of the length of your stay. But, again, it's just a fact that these rentals are usually just cleaned between guests (as opposed to a hotel when cleaning is done daily so it can be rolled into the nightly rate). reply idiotsecant 17 hours agorootparentNo, scam is the right word. It could pretty easily be rolled into the advertised nightly rate. reply victorbjorklund 16 hours agorootparentLets say their cost for cleaning is 100 usd. Without cleaning they the price per night is 150 usd. How much should the price increase per night to cover the cost of cleaning? So either too low for people staying one night or too high for people staying multiple nights. Why is it better? reply nimih 16 hours agorootparentThey should estimate the average stay length of their guests and amortize the cleaning fee appropriately, the same way that many other businesses (including real B&Bs, in my experience) handle fixed costs. In theory, AirBnb should be very good at automating the process (they have large amounts of very granular, market specific, historical rental data), but honestly, if that amount of planning and bookkeeping is too much for the rental operator, maybe running a B&B is not the best career fit for them. reply ghaff 16 hours agorootparentIt's fairly routine for B&Bs to have a 2-night minimum on weekends but, yes, they typically also just have a nightly charge (which may vary depending on day of week) and don't have a fixed per-stay adder. AirBnBs have definitely normalized a significant per-stay charge that isn't the norm in the hotel/inn business. reply idiotsecant 15 hours agorootparentprevSomehow hotels that don't clean every night figured this out like a million years ago. Averaging reply matsemann 14 hours agorootparentA hotel with 100 rooms can average. Me with my single room can not. If you don't like it, fine, just don't use it. But pricing it to what people actually use, is fairest for everyone. reply usaar333 17 hours agorootparentprev> It could pretty easily be rolled into the advertised nightly rate. No it can't because the hosts are trying to charge a fixed fee per reservation. reply bonestamp2 17 hours agorootparentThey can still take that fixed fee and do what some hotels do and average the total cost of the stay across the number of nights booked to get a \"nightly rate\". reply matsemann 14 hours agorootparentYou can't do that on airbnb. You can only set a flat fee and a nightly fee. However, in my market customers only see the total, calculated to an avg nightly price. So it all works out and is fair for everyone. reply usaar333 15 hours agorootparentprevI don't think hosts have the ability to set variable nightly fees like that. reply micromacrofoot 17 hours agorootparentprevIf they need a fixed fee they should set bounds on the duration of stay. They keep shifting the burden to users when it should be on hosts. AirBnb hosts have had their hands on the scale for too long now because the company is terrified of a mass exodus of hosts (many of whom act just as entitled as your average landlord, who will beat every penny out of you). It used to be a much better service for the users. reply hn_throwaway_99 17 hours agorootparentBut there aren't bounds on the duration of the stay. Many places will take you for 1 day or 4 weeks, and there is still just a single cleaning fee. FWIW I think pretty much every vacation rental I booked pre-AirBnB (or, for that matter, pre-Internet) charged a cleaning fee in a similar manner. reply micromacrofoot 16 hours agorootparentRight, I'm suggesting duration bounds as a host-focused solution to a problem that's been frustratingly shifted to users. Better for the user experience to limit the stay to 3 nights (and thus removing it from results for single nights) instead of trying to put a $200 cleaning fee on an overnighter, IMO. reply smith7018 17 hours agorootparentprevOP is right that it's not necessarily a _scam_ but is more so deceptive advertising. You're also right that they should just show total cost of your stay rather than the nightly rate pre-fees. It's wild to me that Airbnb hasn't done this because it's one of the worst parts of their service that has pushed me (and others I've talked to) back to hotels. reply LordAtlas 15 hours agorootparentIf you switch to the Australian AirBnB site, you can see an all-inclusive price because they are required by law to do so there. reply hn_throwaway_99 17 hours agorootparentprevNo, because math. As others have stated it can't be rolled into the nightly rate because it is only charged once per reservation. At least last time I booked on AirBnB it was clearly displayed as part of the total reservation price if you entered dates. reply idiotsecant 15 hours agorootparentI stay at hotels all the time that don't do a cleaning on the first night. How could they possibly figure out the complex calculus of what to charge? Oh right, averaging. reply hn_throwaway_99 12 hours agorootparentAs another user replied, hotels have a cleaning staff that they just pay a salary to - it is, for the most part, a fixed cost. The owner of a single rental/room really is usually paying a flat fee every time they need their placed cleaned, and thus can have widely different costs based on whether someone stays 1 night or 28. They simply don't have the luxury of averaging their costs over a wide base, no matter how many times you put that word in italics. reply namdnay 17 hours agorootparentprevIsn’t it? When I search on airbnb the rates shown on the map include everything. It’s only once you open the page for a specific listing that the price gets broken down reply s1artibartfast 17 hours agorootparentprevI get an itemized bill from the grocery store, but that doesn't make it a scam. reply greedo 17 hours agorootparentIt's like putting a bunch of bananas in your basket, thinking the display price ($10 of course) is the real price, only to find out at the checkout, that it's $10 per banana... reply s1artibartfast 17 hours agorootparentFees are stated up front, and completely to be expected. I think getting shocked by cleaning fees is like getting shocked that there is a sales tax.. Every time you go to the store. Showing the total with cleaning fees at the end is inconvenient and user hostile, but in no way a \"scam\". reply interestica 15 hours agorootparentprevWhat if they charged you a fee because they now have to restock the bananas that you removed from their display? reply s1artibartfast 14 hours agorootparentit would be annoying and stupid, but if it was a regular and expected part of the bill every time I went shopping, I dont think it would be a \"scam\". I think scam implies that something is deceptive, but not everything that annoys me is a scam. If users are completely aware of something and expect it, it is hard to claim they are being duped, in my opinion. reply UniverseHacker 17 hours agorootparentprevIt's a scam when the cleaning fee is as much as the advertised nightly rate, and they require me to clean everything still. reply LordAtlas 15 hours agorootparentprevIt's a scam if a cleaning fee is charged and the renters are ALSO expected to do a truckload of cleaning themselves, for which the fee is presumably charged. And a less charitable interpretation is that it's a scam because it's deceptive advertising to show a lower nightly rate. reply EasyMark 16 hours agorootparentprevI've rented out a couple rooms in my house in the past for airbnb during special events in Austin to make some hobby money, shared my kitchen, they have a private bathroom. It doesn't cost $200 to clean. I can clean up after them typically in less than an hour. reply ipaddr 14 hours agorootparentIt cost you nothing but time and some supplies. If you asked a random person to clean it, they would refuse. So you have to pay them. Minimums for cleaning would be 3 hours. At 50 an hour plus taxes and travel $200 doesn't sound unreasonable. reply yabatopia 10 hours agorootparentI think nearly all the house cleaners will be extremely surprised to hear that they earn $50 an hour. Even more, they get all their travel-related costs reimbursed! Oddly enough, that's not what they see on their payslip. And that's assuming they get a payslip at all. reply mrguyorama 13 hours agorootparentprevThe people cleaning your hotel room who can barely speak english and get skiddish if you are wearing a law enforcement uniform definitely don't make $50 an hour. If you can't efficiently get cleaning services for your rented out room, maybe you should get out of the hotel business! reply matsemann 2 hours agorootparentWait, your point is that those paying their cleaners well, above the board and with a contract should go out of business? But those paying cash, no taxes, low salary abusing people, should remain in business? What an insane take. reply ipaddr 6 hours agorootparentprevUnless you have a pipeline of the undocumented then you have to go through a company. reply hn_throwaway_99 13 hours agorootparentprevFWIW I've never seen $200 cleaning fees on AirBnB for a room-in-a-house accommodation, but if I saw that then yes, I'd call BS. I've only ever seen fees that high for whole-home rentals. reply hilux 14 hours agorootparentprevWhen the cleaning fee frequently exceeds the per-night rate, AND the guest is expected to do a significant amount of cleaning(!) ... that's a scam. reply eli 17 hours agorootparentprevThe fee is obviously much more than they're paying the person who actually does the cleaning. reply marssaxman 16 hours agorootparentHow is that obvious? Paying $200 for turnover service seems perfectly reasonable; when my wife and I ran an AirBnB, we paid our cleaners $250 per visit. reply partiallypro 17 hours agorootparentprevIt's not always a scam but requiring guests to clean while they are also paying a cleaning fee is what makes it a scam. reply cratermoon 17 hours agorootparentprev> Cleaning fees aren't a \"scam\" - most property owners really do need to pay someone to clean the property at the end of a stay. I'm willing to pay a cleaning fee (though really it should be rolled into the cost of the stay) but not also be required to do the cleaning myself. reply throwway120385 17 hours agorootparentIf you rent an apartment in my neck of the woods, then this is exactly what the law is. If your landlord charges a cleaning fee you can feel free to let them clean it up at the end of the rental period. reply lazyasciiart 16 hours agorootparentThe only reason anyone does the cleaning is because the host can then leave a poor review. Does the law cover this angle? reply throwway120385 11 hours agorootparentWell, if I'm a landlord I'll often call two or three of the tenant's last landlords and ask them if they left the place in decent condition, if they were a good tenant, etc.. So there is a review system for tenants as well. I'll also use someone's credit rating and derogatory notes on credit reports as a proxy for how diligent they are at following a contract. And finally nobody here rents anything long-term without a very high-quality contract either purchased boilerplate or custom-written by a lawyer. So yeah, the law and custom both cover this angle. reply ipaddr 14 hours agorootparentprevThe law allows you to make multiple accounts. Creating additional cards could help here. reply trimethylpurine 16 hours agorootparentprevI think it's the time constraints on the dishwasher. Basically there's two levels of cleaning service. Everything else can be cleaned in two hours, but then there is waiting on the dishwasher. The cleaning crew bills to wait for the dishwasher to run and dry (2-4 extra hours). Just look for higher priced rentals, they will roll that cost in. For me, I'd usually rather pay a smaller cleaning fee because I don't usually use the dishes. reply Scoundreller 17 hours agoparentprevI’d say if it’s not mentioned in the ad up-front, you don’t have to do any of that. I’ve found high cleaning fees to sometimes be an intentional discouragement to shorter bookings, but sometimes I’ve paid it because it was still the best option and I like that they maintained the option. Sucks that the only way to push on fixed costs (it’s not just cleaning) is via the cleaning fee. reply notyourwork 17 hours agorootparentThis! If I show up after checking in and get a message asking me to do a bunch of things, I'm going to give them a hard no and point to the booking. reply culopatin 16 hours agorootparentprevThat’s how you get a bad review and charges for $800 to replace a napkin. Airbnb will side with the host, not you. reply Scoundreller 16 hours agorootparentReviews of buyers don’t matter much anymore since almost the entire platform is “instant booking”. Dunno what kind of rules exist on the host side to block bookings under a certain rating. I know eBay’s are garbage because they’re not in the business of blocking sales. reply dymk 16 hours agorootparentprevThis doesn't actually happen even discounting the hyperbole. Airbnb sides with guests for little things. And hosts are much more affected by a bad review than a guest is. reply culopatin 15 hours agorootparentOf course not a napkin, but it’s happened to me that they pushed made up charges on to me. So you can’t tell me it doesn’t happen. A friend of mine came to visit me and stayed at a place for one night. They hit him with smokers damage. Not only he doesn’t smoke but also he was there only to sleep. How Do you even fight that? reply ipaddr 14 hours agorootparentCharge back. reply tensor 16 hours agoparentprevI've also noticed that now some hosts are able to use external services and require a security deposit not managed through airbnb. Supposedly these are only via \"airbnb approved platforms,\" but I still don't like it as there is no real oversight. For example, I found one that wanted almost $1000 as a deposit, and had a whole bunch of rules by which they would charge you extra. Not cleaning dishes, or not well enough would ding you $12 per item! Not putting the keys back in the lockbox on time would be $28/h. There was even a db meter that supposedly didn't record audio. The issue is where is the line? What do they consider clean a dish \"well enough\"? What is the db limit? Can I see the meter? I just avoided it as it sounded too irritating to deal with and with the deposit outside of airbnb there is no room for argument. reply EVa5I7bHFq9mnYK 15 hours agorootparentThat's even worse with Booking (half of their listings are now Airbnb-like rentals). After I paid the full non-refundable amount, I was requested to send an extra deposit, pass video-based verification, holding a passport next to my face, explain the purpose of my travel etc. reply ipaddr 14 hours agorootparentIssue a Chargeback. reply standardUser 17 hours agoparentprevI've stayed at dozens of AirBnBs and I've never had an onerous list of chores or a noticeably high cleaning fee, so these complaints always baffle me a little. Are people reading the listing thoroughly? Are they choosing a place with lots of ratings and reading all of the reviews? I've definitely had some bad AirBnbs over the years despite my careful selection process, but my complains are usually about noise or a lack of supplies (extra towels and blankets for example). reply jamiek88 17 hours agorootparentI literally lived in airbnbs for three years traveling across the whole of the contiguous United States and never once had this either. I did have them ask to put the trash can outside at some of them and most want you to remove your food from the fridge / freezer but that is about it. Just checked my account, we stayed at 167 places in the last 5 years. reply Arelius 17 hours agorootparentMaybe it's more common in larger group stays? The type you might not be visiting as much as an individual living in Airbnbs? reply jamiek88 17 hours agorootparentWwe were a couple with two dogs renting whole houses as a rule but yeah that’s possible. reply drdaeman 16 hours agorootparentprevSame here. 16 stays so far (most long-term, 1+ months), so not much, but all with zero issues. Hosts usually ask to do the most basic stuff - check that everything is closed and turned off, sometimes take the trash out, and possibly collect the used towels and drop them in the bathroom. Stuff I'd do anyway without being asked. I remember having a request to run a washing machine once - and sure thing I did. It's just a push of a button, not like I'm doing laundry by hand, and if a couple minutes of my time (that otherwise would be spent circling around the place and quadruple-checking if I packed everything I've brought with me) saves someone half an hour then I'm happy for a quick and meaningful distraction. As for the dishes and kitchen utensils - some hosts ask, some don't, but I wouldn't leave them dirty either way, that's just common sense and basic respect to the property. Don't remember seeing any outrageous cleaning fees - although I haven't really bothered to check the fee structure, all I care about is the grand total and whenever it fits my budget - the rest is simply irrelevant to me. Airbnb used to suck about not showing the total amount right away (which led to this cleaning fee fiasco), but I believe it's long fixed. Then, I typically spend about a week's worth of evenings carefully going over the listings, multiple times. Airbnb's search is mostly a joke, one can only find a decent place by setting only the most basic filters then methodically going through the listings checking if they're accurate, have a decent number of photos (listing descriptions are useless, have to actually see the kitchen, shower and \"dedicated workspace\"), favorable reviews, no obvious red flags, checking surroundings on the maps and so on. Either way, for the long-term stays a good Airbnb house or apartment beats a hotel (YMMV). And ever for the shorter-term (1-2 week) stays I always checked the hotels and always ended up picking an Airbnb because it was a more attractive option. reply hilux 14 hours agorootparent> Then, I typically spend about a week's worth of evenings carefully going over the listings, multiple times. This is the key. You're valuing your time at nothing. Which is fine, if it works for you. It doesn't work for me. I want to do a quick search, using the basic parameters, and to be able to trust my search without going full CSI. Increasingly, with Airbnb (also booking.com, also eBay, and now even Amazon), the results of a quick search cannot be trusted. In the case of Airbnb, hotels offer a \"more reliable\" alternative for the busy person. reply drdaeman 4 hours agorootparentIf you know a service that has a working search over finer details (like a fridge size or having an ergonomic chair or path to a grocery store), and wide availability - please tell me. I’d probably ditch Aribnb instantly, I have no loyalty to it or anything. It’s not good at all, but I’m not aware about any alternatives that can provide equally good results. Having a good stay that I will enjoy is what I value of my time against. If I’m going to spend 2 months somewhere, I think spending a week isn’t worse than hating something every day for those two months. reply hklgny 17 hours agorootparentprevI used to feel the same way, but over the past few months have started experiencing it quite a bit. I think it started in more touristy areas I didn't tend to go to, and now is spreading all over the platform. It's a terrible experience. last 2 trips I've gone on I've ended up in hotels because all of the airbnb's had cleaning fees close to the nightly rate and an actual chore list for check out. no thanks. reply Salgat 17 hours agoparentprevThere is no situation where cameras are acceptable in another person's living space, so whether this only affects honest hosts is irrelevant; as long as it reduces usage, it is a good thing. Imagine hotels arguing for allowing cameras in their suites because they are \"honest\". reply rolph 15 hours agorootparentimagine a hotel charging you because when you obstruct the camera with some object, your video [profile data] couldnt be provided to a data broker reply lazyasciiart 15 hours agorootparentprevI believe the argument is that dishonest hotels will simply say \"Yep, we got rid of cameras too!\" without actually doing so. reply Salgat 14 hours agorootparentYou could never trust a host to respond honestly about that anyways, so it's a moot point. AirBNB up until recently never acted on indoor cameras, and didn't care if a user reported it even if they felt the host hid/misled that information. reply lotsofpulp 14 hours agorootparentprevI have never heard of hotels putting cameras inside rooms. reply klinquist 15 hours agoparentprevI'm a host. I charge a $200 cleaning fee and my cleaners charge me $250, so I lose money on cleaning. I ask guests to: * Strip the beds (otherwise, I find that people re-make beds and it's hard to tell if they have been slept in), * Load and start the dishwasher (the dishwasher takes 4 hrs, and the cleaners travel to my remote house, and I want to make sure they can finish in about that time) * Take the trash down to the garage (to avoid ants) I pay a neighborhood kid to take the trash bins from the garage to the street on trash day. I successfully manage my place remotely (3hrs away) via a good cleaner and home automation (detailed here: https://www.linquist.com/airbnb/automation). 4.99 stars, superhost. reply MissTake 13 hours agorootparentI find that abhorrent myself. What YOU pay for your cleaners is up to you. Stop nickel and dimming your customers - if you charge a cleaning fee then there's zero moral rights in my mind that you have to ask customers to also clean. Just up the nightly rates and build it in. When's the last time you stayed at a Hotel and found you got hit with a 'Room Cleaning\" fee? What's next - a fee for allowing Heating to be used? A fee to allow water? Heavens above. reply klinquist 12 hours agorootparentMy cleaners travel 25 mins each way to my remote home and spend 5 hours cleaning. In a hotel, with identical setups in each room, they can do a lot more work in that time. My checkout list is clearly defined in my listing, which is doing great based on my ratings and bookings. The cleaning fee is not hidden. I would re-evaluate if I were not successful. reply chgs 44 minutes agorootparentDoesn’t sound like it’s your home, sounds like it’s your business. reply ihumanable 10 hours agorootparentprevI'm confused, if the dishwasher takes 4 hours and the cleaners clean for 5 hours, why do the guests have to start the dishwasher? reply klinquist 8 hours agorootparentAre we arguing over the difficulty of loading soap and pressing a button? Yes, it would be possible for the cleaner to do it, and I don't negatively review a guest who doesn't. But it makes things much easier if the cleaner can simply unload upon arrival... and once again, helps keep pests at bay. My property is in the woods and it can be a challenge. reply MissTake 10 hours agorootparentprevThat’s entirely your problem. And sure, your listing may be doing “great” now, but what if Airbnb change the rules as they may start looking at doing? Sounds to me that your model may not be sustainable in the medium to long term. Life’s full of “we did like gangbusters last year until…” reply klinquist 8 hours agorootparentIf it stops being sustainable, I'll change my policy. I'm willing to change. I'm always revisiting my policies / listing / amenities / etc. reply EVa5I7bHFq9mnYK 15 hours agorootparentprevAs you spelled out your reasons, it makes sense. But usually, you don't go over reasons with guests, and they feel like the host is treating them as his poor relatives doing chores. It's totally different at a hotel, where guests feel like they're being served. reply klinquist 14 hours agorootparentI do go over the reasons in the automated message you receive the night before checkout. reply MissTake 9 hours agorootparentprev“ I ask guests to: * Strip the beds” “ But an unreasonable task is like strip the bed…” Airbnb CEO Brian Chesky, 2022 https://finance.yahoo.com/news/airbnb-ceo-guests-shouldnt-ha... reply madaxe_again 3 hours agorootparentprevI’m a host, two properties in the U.K. I charge no cleaning fee. I don’t ask guests to do anything other than lock the door behind them when they leave. It’s really simple to manage - price it in. Our nightly rate is our nightly rate is our nightly rate. We have a two night minimum on our city property, so it doesn’t end up astronomical - and when we have single nights spare we open them up at a higher price point. On the other property, we have a one week minimum, as it’s in the countryside, larger, and cleaning costs more. We also price in stuff like welcome champagne, tea, coffee, cooking supplies, firewood, you name it - we give our guests everything they would have at home, and enough to throw together a meal on their first evening if they get in late. Our margin is >50% on both. Price it in. reply gnicholas 3 hours agorootparentThis approach makes sense. Not everyone will take it, both because they're trying to compete on (apparent) price and because they don't want to 'force' guests to pay for things that they might have been willing to do themselves. But it's good that there are hosts out there who bundle all of these services together, like they would be in a hotel or resort. For many people, this is what they're looking for. They don't want to do chores when they're supposed to be on vacation. And because there are all kinds of hosts on the platform, the types of guests who don't want to have services bundled in can book a lower-priced listing that requires guests to do more work (but not pay as much). reply lotsofpulp 14 hours agorootparentprev>(otherwise, I find that people re-make beds and it's hard to tell if they have been slept in), That is why any half decent hotel has protocol to always change all the linens anytime someone rents a room. reply klinquist 14 hours agorootparentI have 6 beds. Change 6 beds when one is used? Talk about energy use and wear/tear on the linens themselves. Hotels have cleaning \"scale\" on their side. I'm also operating a large home with a full kitchen, balcony, garage, etc. 8 people can comfortably stay. It ends up being a LOT cheaper AND more comfortable than a hotel for 6-8 people. reply madaxe_again 3 hours agorootparentWe just have a card with a couple of chocolates (bribe) on each bed asking that if it is used, the card is removed, and if it isn’t used, it’s left in place. If there’s any doubt, the linens get changed. Means the guests don’t have to do anything by default, the environment takes one less insult, and we save on linen hire costs. reply lotsofpulp 14 hours agorootparentprev>Change 6 beds when one is used? Talk about energy use and wear/tear on the linens themselves. Depends how much you want to risk a guest sleeping in dirty linens. Maybe you have very trustworthy people you can rely on to accurately gauge if a bed has been used or not, maybe your guests do not mind the risk either in exchange for cost/environmental savings. I prefer the most foolproof option when it comes to sleeping in between linens or using towels on my body. reply klinquist 14 hours agorootparentMy existing solution of asking guests to pull the linens off their beds seems to be working great :). reply scarface_74 13 hours agorootparentprevWhy wouldn’t you clean the sheets regardless after each stay? But everything else seems reasonable. If a host left a note explaining the reasoning behind the dishwasher and the trash, I would understand completely. reply klinquist 12 hours agorootparent6 beds (comprising of 4 different sizes of beds/sheets) make it quite wasteful if a bed is not used. It already takes ~5 hours to clean the home. reply lotsoweiners 7 hours agorootparentEwww…hosts like you are the reason Airbnb gets a bad rap. I don’t think any reputable hotel would find it ok to not clean the sheets on both queen beds in a room if only one “looked” used. Logistics of cleaning should be built into your pricing and scheduling and not be a chore or something the guests have to think about. reply matsemann 2 hours agorootparentYou've never seen hotel staff clean a room, lol. You'd be surprised. reply scarface_74 10 hours agorootparentprevI would take the side of sanitation than taking a chance on having used sheets on the bed reply klinquist 10 hours agorootparentMy existing solution does, by asking guests to remove the sheets. Obviously, if no sheets are removed, they would all be cleaned. reply scarface_74 10 hours agorootparentAnd if your guest don’t remove the sheets after they sleep on them? reply klinquist 8 hours agorootparentI pay my cleaners to make sure the home is clean. Simple as that. reply moralestapia 15 hours agorootparentprevTbh, those are problems that you have to solve, somehow. reply matsemann 14 hours agorootparentAnd they've done it. As long as these expectations are spelled out before booking, it's not a scam and completely fine. If a renter doesn't like it, just don't book. reply klinquist 14 hours agorootparentprevGiven my airbnb rating and constant positive feedback, I'd say that my existing solution is sufficient. reply MissTake 13 hours agorootparentGive it time. I'd not be surprised to read that Air BnB are going to curtail this behavior next. The only reason you're doing this is to game the system. Be honest and charge more. reply therealdrag0 13 hours agorootparentNo the logistics and economics don’t work to run an Airbnb the same as a hotel. Why would Airbnb add regulations that make their customers less profitable or financially viable? reply MissTake 9 hours agorootparentMeanwhile: https://www.rentalscaleup.com/airbnb-adds-2-more-to-guest-fe... This change, affecting international bookers, will make it more expensive to rent, thus potentially affecting a hosts bottom line. This may not affect you, but it will affect others. And remember, as a host, you’re not the customer. The person booking is the customer. You? You’re just a supplier. reply MissTake 11 hours agorootparentprevEr, because they can. And do. As evidenced by the recent changes. The point is, you should either charge cleaning costs and not require the customer to do anything other than leave it in a “used” state - or - not charge cleaning costs but require the customer at least does some basic cleaning. Charging and also requiring customers to clean is unethical in my book. reply therealdrag0 10 hours agorootparentPardon what recent changes? reply MissTake 10 hours agorootparentYou don’t think the subject of this story isn’t a change of their rules and regulations? reply therealdrag0 9 hours agorootparentI don’t see how security cameras impacts the economics of running an Airbnb. reply MissTake 9 hours agorootparentSome owners were using them to record damage or vandalism to a property. Without them that could make some properties more expensive to maintain and repair. reply klinquist 12 hours agorootparentprevHow am I not honest? The listing shows my cleaning fee and checkout list. Everything is transparent. reply MissTake 11 hours agorootparentBe honest and build the fee into your rates. Why do you not do that? What is your rationale for charging it separately? reply klinquist 11 hours agorootparentFirst, note that Airbnb shows it \"built into the rates\" when you put in the number of days you are staying when making a reservation. See this article from 2022: https://www.theverge.com/2022/11/7/23444561/airbnb-total-pri... But to answer your question - because it is a fixed cost per stay. If you stay two nights @ $250/night, you'll pay an additional $200 cleaning fee (so that would be $350 night if I were to \"build cleaning in\") If you stay 6 nights @ $250/night, you'll still only pay an additional $200 cleaning fee (so it would be $283/night if were to \"build cleaning in\"). Airbnb doesn't allow me to charge a different $ based on the number of nights stayed. But they are showing an all-inclusive cost to the user when the user is browsing with their # of days, so there is nothing hidden... and to your \"give it time\" statement, it's been over 5 years... reply MissTake 10 hours agorootparentFirst off: https://www.nerdwallet.com/article/travel/airbnb-has-a-plan-... And secondly if you reread my post you’ll see that I think it’s morally wrong to charge a cleaning fee AND expect guests to clean as well. That’s double dipping. reply matsemann 2 hours agorootparentMy cleaning fee is for washing the bedding, mopping the floors, wash inside the oven, was inside the refrigerator, vacuum all floors, wash all floors, clean every surface, scrub the shower, scrub the toilets, clean the sink and mirror, etc etc. None of which I expect guests to do. So it's not double dipping. You've been explained why you're wrong ten times now, and still drone on about it. Maybe airbnb just isn't for you? Book a more expensive hotel instead, and then those thinking my cabin is a good deal can do so and be happy. reply MissTake 2 minutes agorootparentThis is a public forum. If I have a strong feeling on something, why do you feel I should not talk about it? It is your opinion that I am wrong. It is not a fact that I am wrong. You say you have a cabin - I’ve booked Cabins in the Hocking Hills area of Ohio (not via Airbnb). They’ve never had cleaning fees. If they can handle this, why can’t you? lukan 17 hours agoparentprev\"I'm worried this ban will only affect honest hosts.\" EMF meters(like in the airplane to spot mobiles) are quite cheap and if you find an active one, you can probably sue the host, but at least reporting should have some effect, even though no instaban. \"If a guest reports the presence of an indoor camera after that, Airbnb says it will investigate and that it could remove the host’s listing or account as a result.\" I do think that many hosts are scared of loosing the income, so they will comply. reply autoexec 16 hours agorootparent> EMF meters(like in the airplane to spot mobiles) are quite cheap and if you find an active one, you can probably sue the host In a house with a bunch of smart meters/appliances, bluetooth devices, a router, indoor electrical wiring, nearby power lines, cell signals, AM/FM radio, etc wouldn't the house be constantly flooded with EMF radiation pretty much everywhere? reply lukan 14 hours agorootparentThey all emmit on different frequencies. But I have not made a test yet, how well it works with a cheap one (I bought a simple one some weeks ago). The advanced ones can be tuned to find very specific bands. reply noodlesUK 17 hours agorootparentprev> like in the airplane to spot mobiles What are you referring to here? reply renewiltord 17 hours agorootparentYeah I've often forgotten and left my ipad on cellular or my phone non-aircraft-moded. No one has ever said anything so it's unlikely they have automatic detection. reply lukan 17 hours agorootparentIt also never happened to me, but I heard of stories, where the stewardess walked around with one. But they might have gotten less paranoid about it(since nothing ever happened?). Chances of bad interference are probably very low. reply chgs 39 minutes agorootparentI’ve flown over a thousand times and have rarely used “airplane mode”. I’ve never seen any cabin crew using anything to “detect” cellular activity or even mentioning phones. reply ptero 17 hours agorootparentprevI think (although not sure) that cell phone on planes ban was much less about interference with plane electronics and more about the poor handling of a rapidly moving phone on ground cell networks at the time. Enough flying phones would cause a significant performance degradation. The issue of handling fast switching was solved, but any \"because security\" or \"because safety\" laws tend to stay for a long time. reply dawnerd 15 hours agorootparentprevI fly a lot and haven't head of this at all. Sounds like one of those stories that spread from someone thinking they saw something when in reality it was a crew with a phone checking seats for passengers. reply lukan 17 hours agorootparentprevPeople not turning on the airplane mode on their devices while in a plane. Sometimes they check (never happened to me though, but I avoid flying). reply outside1234 17 hours agorootparentModern mobiles are not using frequencies anywhere close to those used by the plane for nav / comms anymore so this is not an issue (though your battery will run to zero as it frantically tries to find a tower) reply shagie 15 hours agorootparenthttps://simpleflying.com/why-passengers-must-put-phones-into... > There is another reason sometimes cited too for the early introduction of mobile phone restrictions, and it is related to the potential impact on cellular ground equipment. > A mobile device operating at an altitude, and moving at speed, can see multiple cell towers at the same time. This will block frequencies used by these towers, with much more activity than they were designed to handle from ground-based devices. In the US, the ban on electronic devices in flight was initially put in place by the US Federal Communications Commission (FCC), not the FAA, for this reason. https://www.faa.gov/travelers/fly_safe/information > The FCC and FAA ban cell phones for airborne use because its signals could interfere with critical aircraft instruments. Devices must be used in airplane mode or with the cellular connection disabled. You may use the WiFi connection on your device if the plane has an installed WiFi system and the airline allows its use. You'll note the FCC in there. While the aircraft instrumentation is the most immediate \"reason for people to do this\" that people can identify, disrupting ground communications is also there. Unfortunately, people are not that likely to do it based on \"you'll inconvenience people using the cell phone towers that you fly over.\" reply notyourwork 17 hours agorootparentprevIMO phone battery is relatively fine. I've forgotten to enable airplane mode on a few flights. reply ghaff 17 hours agorootparentI try to remember but I know I don't always and the battery has been relatively fine. And over the course of hundreds of flights I've never once seen or heard a flight attendant come around to tell a passenger to put their phone into airplane mode when I'm sure that maybe the majority of people just ignore the instruction to do so. reply mrgoldenbrown 17 hours agoparentprevYou can avoid those listings if you want. You are only responsible for the tasks they specify up front in the listing under \"rules\". If they didn't list it up front, before you book, you don't have to do it. If they try to spring some tasks on you upon arrival, that's their problem. reply wdb 17 hours agoparentprevYeah these days it is cheaper to rent a spot at apartment hotel. Might pay $75/night for it but to need to do any cleaning reply ezfe 17 hours agoparentprevI always refuse to do everything except load and start dishwashers, if I filled it, take the trash out. I have 15 reviews, ALL positive. reply EasyMark 16 hours agoparentprevI just move on when I see requirements like that. Some of those I do as a matter of course (clean any food mess/dishes if I have a kitchen). I think my mom beat that one into me. I'm definitely not doing laundry with a house cleaning fee is tacked on. Probably why I stay at hotels 95% of the time. reply mx_03 16 hours agoparentprevNext time book a hotel then. The only way things will change is if all of use refuse to keep giving them our money. reply kube-system 17 hours agoparentprevHonestly I think those requirements are perfectly reasonable for the originally intended purpose of the site, which is people renting out their personal homes. I have stayed in several Airbnbs that had the owner's clothes in the closet and pictures of their kids on the wall, and in those situations, it just seems rude to leave dirty dishes in the sink. These places aren't hotels. I agree that doing laundry is a bit onerous in clearly dedicated rentals, but in my experience those tend to have fewer requirements and tend to not care if you miss a couple steps unless you trash the place. reply jupp0r 16 hours agorootparentThat's all fine. The $200 cleaning fee on top of that isn't. The $200 cleaning crew will do the dishes no problem. reply kube-system 16 hours agorootparentDepends on the size of the property. reply notyourwork 17 hours agorootparentprevThose types of Airbnb's are long gone, now its entirely commercial. reply kube-system 16 hours agorootparentThey are not gone, I book them all the time. They're hard to find, but they exist. reply archagon 14 hours agorootparentHow are they even hard to find? They’re one of the three top level filter options. reply kube-system 14 hours agorootparentThat's not really what I'm talking about. Yes, the \"room in someone's home\" is a subset of \"people's personal homes\", but some of my best Airbnb experiences were entire homes that were someone's personal home (or duplex style arrangements), that were being rented out on a temporary basis. There is no filter for this. reply sourcecodeplz 16 hours agoparentprevVacuum, mop the floors, scrub the toiles and sinks, clean the dust / hairs. Of course there is a cleaning fee. reply dcsommer 17 hours agoparentprevAirBnB should tie the cleaning fee to your rating, perhaps a cleanliness dedicated rating. reply lucisferre 16 hours agoparentprevThey should ban any hosts that require guests to do cleaning to be honest. reply matsemann 2 hours agorootparentWhy? Lots of people would rather do a little themselves to get the price down. Why should your preference dictate how everyone does it? And it varies from market to market what's the norm. For instance for cabins in Norway, the norm for a long time was that the renter brought their own bedding and cleaned everything, mopped the floor etc before leaving. Long before airbnb was a thing. Should an American company decide that this Norwegian way of doing it is no longer ok..? reply scosman 17 hours agoparentprevRe: will only affect honest hosts It still probably helps reduce surveillance a fair bit: - honest host who currently disclosed cameras will stop - dishonest hosts who are worried about getting caught will stop (most aren’t tech savvy enough to hide cameras, catching them isn’t hard, and they don’t want to risk getting kicked off platform) - really dishonest hosts who hide cameras will get kicked off platform if caught (and potential civil/criminal cases) Not perfect but a good move and should help. reply chgs 35 minutes agorootparentA host can’t complain about you breaking the camera if they didn’t say there’s one there. cctv cameras in non public places and with no oversight (including video doorbells etc) are a cancer on society reply barbazoo 15 hours agoparentprevNo come on, some of those are crazy, no one is expected to clean the oven after normal use. I've seen this lots but I've never actually done any of the things they ask me to. Bringing out the garbage? Sure, but only if it's overflowing while I'm there. I will start the dishwasher. We've also never received a bad review so I think at least where we stayed, while those things are explained in their handbook or wherever there's no downside to just not doing them. reply booleandilemma 17 hours agoparentprevA $200 cleaning fee is more expensive than a night at many Manhattan hotels. Actual hotels. With full service. Is airbnb even worth it? reply tensor 16 hours agorootparentNot for a night no. It's more of a replacement to renting a service apartment for longer periods of time (e.g. a week or more). reply archagon 14 hours agorootparentprevFWIW, the last hotel I stayed at in Manhattan was $300 a day and had no service. And this was a big name hotel too, not some budget option. reply nradov 17 hours agoparentprevWhy would customers even put up with this type of abuse? I don't get it. Just stay somewhere else. If I'm staying with a friend or relative for free then I'm happy to pitch in with household chores, but if I'm paying then screw that. I honestly can't understand why anyone would use AirBnB when there are so many better options available. reply deadbabe 17 hours agorootparentAirBnB is the only decent affordable option for a large group of people who want to stay together. Other than that, I see no point. reply SergeAx 10 hours agoparentprevWhat will they do if you refuse? reply moralestapia 15 hours agoparentprevWhy do you have to do all that, though? reply dangus 15 hours agoparentprevI don’t understand why this is a worry. There’s no such thing as an in-room security camera in a hotel. Airbnb should have had a clear ban on indoor security cameras a long time ago. You might as well say that “laws only apply to law-abiding people.” That’s not a reason to not have laws. reply sneak 17 hours agoparentprevIf you’re paying the cleaning fee anyway, why would you do any of that stuff? reply tgsovlerkhgsel 17 hours agorootparentHave you watched Black Mirror's \"Nosedive\"? There is a rating system for guests, and losing \"social credit\" will make using the platform harder. This implied threat makes people \"behave\", with both benefits (the worst actors will be filtered out and can't repeatedly impose the cost of their actions on society) and drawbacks (feeling pressured to always give 120%, fear of bad ratings, getting excluded through no fault of your own if you just get unlucky). reply mholt 17 hours agorootparentprevBecause hosts can leave bad reviews of guests. This happened to us once when we were new to Airbnb, we had that exact thought: \"We paid a cleaning fee, we're pretty tidy anyway let's just let them do their job.\" But we got a bad review and now almost every place we book, I get a phone call asking about the bad review and I have to explain... \"We paid a cleaning fee, we're already very tidy people, and we were stressed and rushed getting our family with young kids out the door on time. The mess they're complaining about was probably 5 minutes of sweeping, but now we are even more meticulous when checking out.\" reply groggo 17 hours agorootparentprevMost of those things are pretty common courtesy... The cleaning fee is for deeper stuff like vacuuming, scrubbing the sink, toilets, etc... And yeah I think it also captures the one-time effort of the \"turnover\", where the host has to do the laundry, remake the beds, and a bunch of small things you might not notice. I also hated it when the cleaning fee wasn't added until the end, which was very bait-and-switch-y. But as long as it's upfront it's ok. It would be nice if more hosts just offered rooms though. If you don't think of it as a hotel, then it doesn't need that deep clean and doesn't need the cleaning fee. I just want a quick place to crash for a night sometimes... reply loeg 17 hours agorootparentprevThe hosts could leave you a bad review which can make it harder to book in the future. reply dawnerd 15 hours agorootparentWith instant booking isn't that pretty much pointless now? I'd rather have a low rating than be scammed. reply AlexandrB 15 hours agorootparentprevI love how we went from \"the customer is always right\" to this. /s reply bongodongobob 17 hours agorootparentprevBecause they'll charge you for it and ding your rating. reply deadbabe 17 hours agoparentprevOne hack you can do is tell the host you have a disability and will not be able to carry out those tasks. reply VeejayRampay 17 hours agoparentprevgo to a hotel then, they have paid cleaning staff to take care of that stuff for you reply statguy 17 hours agoparentprevYou knew this when making the booking? Why didn’t you book a hotel instead? Airbnb or the host have no incentive to change if people like you keep going back. They will change only when it starts to hurt their bottom line. reply Cheer2171 17 hours agorootparent> You knew this when making the booking? You know about the cleaning fees at booking, but not necessarily the \"rules\" about cleaning. reply mrgoldenbrown 12 hours agorootparentAnything not disclosed in \"house rules\" section at the time of booking is not your responsibility. reply swatcoder 16 hours agoparentprevYour family was asked to do about 17 minutes of convenient, easy preparatory tidying that most considerate people would do even without being asked (as in a hotel or when visiting a friend). Yet you benefited from coming into a home that, presumably, had all of its floors vaccuumed, its kitchen and bathroom surfaces sanititized, its linens folded and/or readied, etc. Whether it was the host or a service, somebody followed up behind you and likely spent at least several hours preparing the place for the next guest. And instead of it being bundled into your daily room rate like in a hotel, since it's only being done once per stay, it was applied as a per-stay cleaning fee. That's eminently fair. There are about a thousand of things to complain loundly about with AirBnB and countless things in life more broadly -- being asked to tidy up a little bit or to pay a per-stay fee to cover cleaning? Not really a compelling one. reply noslenwerdna 16 hours agorootparentI think the point is it should be part of the upfront cost when searching, rather than something that shows up at the end. reply swatcoder 16 hours agorootparentYes, that is one of the many valid complaints one might have with AirBnb and AirBnb browsing is much less frustrating in the markets that have forced them to do that. But I don't read that as what the above commenter was complaining about. reply linehedonist 14 hours agorootparentprevYou empty the trash from your hotel room? Really? reply swatcoder 12 hours agorootparentIf there's an appropriate place for me to do so, yeah. And if there's not (as often), I at least gather all the trash together and make that task as easy as possible for whoever's stuck with the thankless job of cleaning up after me. It's wild and disheartening that people here feel justified in not doing basic tidying of whatever mess they made when traveling or that it's some big demand of a host to ask them to. reply ohnoitsahuman 17 hours agoparentprevIt's hard to gauge cleaning effort: if a guest uses every dish in the kitchen without doing a wash up, they cleaning crew could be there for hours. But if a guest is tidy, then the cleaning fee should be nothing. The hard part is that you need to reserve and pay for a cleaning crew as if it was always a disaster. reply alistairSH 17 hours agorootparentyou need to reserve and pay for a cleaning crew as if it was always a disaster. Yet hotels manage this fine - it's just baked into the price. I don't have a problem paying a cleaning fee. I just don't want to pay the fee AND be required to do 90% of the cleaning myself. I'm happy to put the dishes in and then start the dishwasher. I'm not going to wait around for it to finish and then put them away. I'm happy to pile the used towels and bedding in a single spot, but I'm not going to do the laundry. I'll pick up any trash and wipe down the kitchen, but I'm not going to take out the trash or do a deep clean of the kitchen. Etc. reply cameldrv 17 hours agorootparentThis is the fundamental problem with AirBnb as a business instead of a side gig. The hotel has massive economies of scale in cleaning and other things, because they have dozens of rooms on site. If one room gets totally trashed, they just won’t use that one until they get around to cleaning it. If a guest checks out late or wants to extend their stay, they just put the next guest in a different room. Also the cleaning crew is already on site. People are always checking out and checking in. The crew just pushes their cart to the next room instead of driving across town, packing and unpacking all of their stuff, and making sure to hit a precise time window between checkout and checkin. reply dawnerd 15 hours agorootparentThis may be true for hosts with one property but it's becoming more and more common for management companies to own a lot of properties on airbnb. The whole economies of scale have been figured out. reply alistairSH 17 hours agorootparentprevAbsolutely, there's a lot more risk in renting a single unit vs a hotel. The fees should be higher. The check-in/check-out timing should be stricter and with more time in between. Etc. reply nick12r55t1 17 hours agorootparentprevMy wife and I have an airbnb and this is exactly how we structure our check-out. It still costs $250 per turnover for a reliable cleaning crew to come and clean the 3br, 3ba two level house. reply temporarara 16 hours agorootparent$250 sounds like a lot of cleaning, multiple hours of cleaning by one person for sure, of course depending on your location. But that really gotta be some mansion or your customers are really the absolute worst if that's always required. Meanwhile my friend who does this stuff changes the sheets and towels and wipes the tables together with some light vacuuming and that's basically it, only a few times per year he does a more thorough cleaning. And he does it by himself since he has reasoned that he wants to check the apartment personally every time it's used",
    "originSummary": [
      "Airbnb is prohibiting indoor security cameras starting April 30th to prioritize renters' privacy.",
      "Hosts must now reveal outdoor security cameras and noise decibel monitors' usage and locations.",
      "The update follows instances of guests discovering hidden cameras, prompting Airbnb to safeguard privacy by restricting hosts from installing cameras in their rentals."
    ],
    "commentSummary": [
      "Airbnb users are engaging in discussions about various topics such as the platform's changes, increased fees, limited listings, and reliability concerns.",
      "Debates include regulatory issues in cities like Paris, Airbnb's influence on rental markets, and the comparisons between Airbnb and hotels regarding advantages and disadvantages.",
      "Users are also addressing matters like cleaning fees, guest obligations, security measures like cameras, and the differing pricing models of Airbnb and regular hotels, expressing worries about concealed fees and pricing strategies, as well as the difficulties encountered by hosts and guests."
    ],
    "points": 258,
    "commentCount": 427,
    "retryCount": 0,
    "time": 1710169705
  },
  {
    "id": 39666865,
    "title": "Teable: Open-Source Fusion of Postgres and Airtable",
    "originLink": "https://github.com/teableio/teable",
    "originBody": "FeaturesSpreadsheet-like interface All you want is here • Cell Editing: Directly click and edit content within cells.• Formula Support: Input mathematical and logical formulas to auto-calculate values.• Data Sorting and Filtering: Sort data based on a column or multiple columns; use filters to view specific rows of data.• Aggregation Function: Automatically summarize statistics for each column, providing instant calculations like sum, average, count, max, and min for streamlined data analysis.• Data Formatting: formatting numbers, dates, etc.• Grouping: Organize rows into collapsible groups based on column values for easier data analysis and navigation.• Import&#x2F;Export Capabilities: Import and export data from other formats, e.g., .csv, .xlsx.Multiple ViewsVisualize and interact with data in various ways best suited for their specific tasks.• Grid View: The default view of the table, which displays data in a spreadsheet-like format.• Form View: Input data in a form format, which is useful for collecting data.• Coming soon: Kanban View, Calendar View, Gallery View, Gantt View, Timeline View.Super Fast Amazing response speed and data capacity• Millions of data are easily processed, and there is no pressure to filter and sort• Automatic database indexing for maximum speed• Supports batch data operations at one timeFull-featured SQL Support Seamless integration with the software you are familiar with• BI tools like Metabase PowerBi...• No-code tools like Appsmith...• Direct retrieve data with native SQLPrivacy-First• Bring your own database (coming soon)Real-time collaboration • No need to refresh the page, data is updated in real-time",
    "commentLink": "https://news.ycombinator.com/item?id=39666865",
    "commentBody": "Teable – Open-Source No-Code Database Fusion of Postgres and Airtable (github.com/teableio)242 points by bieberChen 22 hours agohidepastfavorite83 comments Features Spreadsheet-like interface All you want is here • Cell Editing: Directly click and edit content within cells. • Formula Support: Input mathematical and logical formulas to auto-calculate values. • Data Sorting and Filtering: Sort data based on a column or multiple columns; use filters to view specific rows of data. • Aggregation Function: Automatically summarize statistics for each column, providing instant calculations like sum, average, count, max, and min for streamlined data analysis. • Data Formatting: formatting numbers, dates, etc. • Grouping: Organize rows into collapsible groups based on column values for easier data analysis and navigation. • Import/Export Capabilities: Import and export data from other formats, e.g., .csv, .xlsx. Multiple Views Visualize and interact with data in various ways best suited for their specific tasks. • Grid View: The default view of the table, which displays data in a spreadsheet-like format. • Form View: Input data in a form format, which is useful for collecting data. • Coming soon: Kanban View, Calendar View, Gallery View, Gantt View, Timeline View. Super Fast Amazing response speed and data capacity • Millions of data are easily processed, and there is no pressure to filter and sort • Automatic database indexing for maximum speed • Supports batch data operations at one time Full-featured SQL Support Seamless integration with the software you are familiar with • BI tools like Metabase PowerBi... • No-code tools like Appsmith... • Direct retrieve data with native SQL Privacy-First • Bring your own database (coming soon) Real-time collaboration • No need to refresh the page, data is updated in real-time MitPitt 18 hours agoI would love to hear some comparison points with: nocodb, baserow, grist. These are all very similar and open-source, does teable bring anything new to the table? reply Cilvic 18 hours agoparentWas the pun intended? I'm also curious to see a comparison between the above, in particular trying to understand who is the most open source in their features + expected to not start feature capping soon. reply bieberChen 15 hours agoparentprevThis pun is interesting; the 'new' in Teable is literally the 'e'. I've addressed the product comparison questions in a public reply, hoping it's helpful. reply MitPitt 24 minutes agorootparentWhere can I see it? reply anoy8888 18 hours agoparentprevThey are not true open source if they have a pricing page reply sneak 18 hours agorootparentThey’re not true open source if they have ridiculous nonfree licenses like the AGPL which is just an EULA in disguise (and probably isn’t enforceable as a result). reply j45 7 hours agorootparentDo you have a better licensing solution for companies who resell and profit on open source and don’t give back commits or support? reply nsonha 18 hours agorootparentprevlol what reply nagstler 9 hours agoprevCongrats! It's a very useful use-case, BTW we are building an open-source no-code data activation from Postgres to Airtable and many more destinations. Would love to watch the repo ;) (https://github.com/Multiwoven/multiwoven) reply neeleshs 17 hours agoprevCongratulations on the launch! This looks pretty neat, though there are many similar products. When non-technical/semi-technical people are exposed to the database, the biggest issues come from two areas * their lack of grasp on data models, and modeling in general. This stumps them every time they see bridge tables, many-to-ones, and joins. Or when they need to answer a question, and the answer is not obvious from the base tables * databases in general are very normalized, cryptically named (tables and columns) and have too much evolutionary baggage (both from a schema and data point of view) - except for in new/small systems. These then become organizational problems rather than tooling problems. reply ccorcos 16 hours agoparentI totally agree. I’m very interested in some kind of data modeling representation that’s a lot more user-friendly. The core problem as I understand it is the difference between references and values. reply kgodey 17 hours agoprevCongratulations on launching, it's nice to see more open source products in this area (I work on https://mathesar.org/). Feel free to reach out if you'd like to talk and compare notes. reply hodanli 17 hours agoparentyour demo seems to be offline reply kgodey 16 hours agorootparentThanks, taking a look. We just moved it to new infrastructure and we're still working out the kinks. Edit: should be fixed, let me know if there are more issues. reply jskherman 18 hours agoprevHow does this compare to NocoDB[0] which is already quite established? [0]: https://github.com/nocodb/nocodb reply bieberChen 18 hours agoparentNocoDB is a pretty impressive product. In fact, the idea for creating Teable emerged around the same time NocoDB was released. The challenge with Teable is our desire to provide a fully No-Code and real-time collaborative experience on the product interface, which contrasts with NocoDB's approach of exposing more database details to the user. Both have their pros and cons. Additionally, Teable supports developers by offering open database connections and database permission management, a concept inspired by Supabase. This allows both developers and users to create on the same platform. reply christoff12 15 hours agoprevI've been waiting on Xata to allow a direct SQL connection to the underlying db to take advantage of the Postgres connector in Plasmic for rapid product development -- that Teable allows this is a killer feature for me. reply tudorg 13 hours agoparentHey, SQL over HTTP is possible now in Xata, and direct Postgres access will be possible _real soon_. reply okcdz 22 hours agoprevI've deployed this on my server, and I have to say, it's incredible. I've been looking for an open-source alternative to Airtable for a long time, but none of them met my requirements. reply pif_ 19 hours agoparentBaserow[0] is really good! [0]: https://baserow.io/ reply egeozcan 18 hours agorootparentBaserow is open-core, and some views and JSON-export are behind a subscription. Not a bad thing per-se but needs a mention. reply bieberChen 21 hours agoparentprevI'm thrilled to hear that! reply abdullahkhalids 13 hours agoprevIs there any functionality that allows me to easily create a no-code form on my website and link to a database table? Additionally I want to use the API to do things when certain tables are updated. Example, send an email (using my own code) when a new row is added. How easy is something like this? reply warthog 19 hours agoprevWhat are the differences to Glide's data-grid? https://glideapps.github.io/glide-data-grid/?path=/story/gli... reply bieberChen 19 hours agoparentGlideGrid is an outstanding canvas grid component (interestingly, it was initially used by us before we decided to create our own version to enhance the user experience). Teable, on the other hand, is a fully-featured no-code platform aimed at empowering those without technical backgrounds to efficiently work with Postgres (a relational database). reply Gys 10 hours agoprevI like it. Do you have a roadmap? There are a lot of 'coming soon' features :) What I miss in these kind of tools is a way to 'mold' the UI into something more specific. A kind of scripting maybe. For example I would like an 'edit' button on each row to open an edit form, opening a form for editing. Similarly an 'Add record' button at the bottom. Or better, an 'Add contact' button if its the Contacts table. And how about having a view that always opens with a certain filter, for example the Purchases table showing only purchases by customer A if I click on the button 'Purchases' in the Customers table in the row of customer A. reply Cilvic 18 hours agoprevCan you share the motivation behind it? Why did you open source it? Is there a commercial version where some feature will be paid? reply gorkemcetin 14 hours agoprevI loved what you have done out there - it is clean and neat. I wish you all the success albeit the crowded space (disclaimer: I work for Retable). reply rareitem 13 hours agoprevYou should maybe remove the 'X' symbol in your FAQ section because it looks as if the the things listed there are not supported. E.g 1. I see 'Does Teable support SQL queries?' 2. I click on the list item 3. The accordion menu expands, the '+' symbol turns to 'x', as if to tell that Teable does not support SQL queries. reply bieberChen 8 hours agoparentyou are right, I'll fix it~ reply jitl 18 hours agoprevHow do you handle schema changes like adding columns or changing the type of columns? Is the schema fixed at design time by a database admin, or can the end-user add/remove columns, create tables, and introduce relationships? reply bieberChen 18 hours agoparentThat is what I want to share. When users create fields in the visible table, it directly creates columns in the corresponding Postgres table. There's a mapping between the fields on the UI and the physical columns. At the same time, relationships are a major highlight; the \"link field\" of the Teable will create logical foreign keys or junction tables between physical tables to maintain the relationship. reply resoluteteeth 18 hours agoprev\"Postgres-Airtable Fusion\" sounds like it runs on top of airtable. If this is not the case I suggest rephrasing this (e.g. to \"airtable alternative based on postgres\") Also there are a number of other products in this space so it would be useful to have some sort of comparison. reply bieberChen 18 hours agoparentI must say, this suggestion is fantastic, super grateful! reply lxe 16 hours agoprevNicely done! Easy to get started and super intuitive. Some bugs here and there (clicking the plus button is giving 404 on my end for example), but seems solid. Congrats. reply bieberChen 16 hours agoparentThank you! reply bieberChen 15 hours agoprevIn the comments, everyone is interested in the core differences between Teable and similar products. Here, I'll compare the products mentioned. Please note that since I'm not a deep user of the other products, there might be inaccuracies, and I welcome corrections. Grist I had heard of Grist before but never actually experienced it. A quick look at Grist through videos showed that its dynamic spreadsheet capabilities are incredibly powerful, complete with fixed field types, making it excellent for organizing structured data. It seems to have an edge in flexibility, and compared to Airtable, it might be more akin to Smartsheet. According to its official documentation, Grist's Pro Plan offers up to 100k rows, indicating that queries and calculations are processed on the frontend or in memory, which typically makes it challenging to scale data rows further. This is a problem that Airtable also faces. Baserow and NocoDB, my impression is that Baserow's features are relatively more stable, and it started commercializing earlier, being among the first batch of open-source Airtable alternatives. Baserow initially had a limit on the number of rows, but this year's updates seem to have significantly increased its data capacity. Notably, Baserow does not support Bring Your Own Database or query by SQL, but it offers a seamless scrolling table interface, unlike NocoDB, which requires pagination. In terms of other functionalities, both have their strengths. My assessment aligns with what I found on Baserow's official forum and comparisons with NocoDB. Teable Compared to similar products, Teable invests heavily in its table format UI, striving for seamless scrolling, copy-pasting, batch editing, and other quick table operations, which we believe are key to saving users' time. Therefore, we developed our Canvas table rendering component to achieve perfection. Meanwhile, batch operations pose a significant challenge for database compatibility, but we see this as a necessary investment. Additionally, Teable supports developers by offering open database connections and database permission management, a concept inspired by Supabase. This allows both developers and users to create on the same platform. What we think the future of no-code products look like 1. An interface that anyone can use to build applications easily. 2. Easy access to data, letting users grab, move, and reuse their information as they wish. 3. Data privacy and choice, whether that's in the cloud, on-premise, or even just on your local. 4. It needs to work for developers too, not just non-tech users. 5. It should handle lots of data, so it can grow with your business. 6. Flexibility to integrate with other software, combining strengths to get the job done. 7. Native AI integration to takes data automation to a whole new level. reply johnchristopher 12 hours agoparentLooks super fast ! I am just dabbling in nocode platforms, so far I only spent an evening per solution: undb, baserow, nocodb. Nocodb has an url and an email field type that enforces URL and email. Do you plan to add something like that ? reply bieberChen 8 hours agorootparentIn the SingleLineText field, you can choose to show as email, Any format verification (regex) will be introduced in the future. reply johnchristopher 2 hours agorootparentOh, I missed that, thanks ! reply christoff12 14 hours agoparentprevI'm a big believer in easy no code abstractions without black box restrictions on the code underneath, particularly for \"power users\" who run into the limitations of no code tools after a few iterations (particularly on a cost per function level). reply culiao 16 hours agoprevCan you join data from various tables and build a view? Also - my team prefers airtable because of the calendar view.... would be great to see. reply bieberChen 5 hours agoparentThere is currently no \"join\" capability, but there is a \"Link Field\" to handle table relationships. We also found the calendar view useful, as it sits in the middle of the requirements priorities reply zem 19 hours agoprevcan I use this as a collaborative grid component in my own project? that is, I don't want any of the no-code, spreadsheet etc features, I just want part of the frontend of my own app to be a grid that multiple people can edit, persisted in a database table that my own phoenix/rails/whatever process also has access to. reply bieberChen 19 hours agoparentA more optimal approach is to utilize the SDK to use the table as a React component within the application. This capability will be iterated later. Teable's UI interface is all built on top of a standalone SDK. In theory, both component and collaborative capabilities can be exposed externally, but it will take some time. reply zem 18 hours agorootparentthanks! i've been hoping for this functionality for years but none of the airtable replacements so far has provided it. reply bieberChen 19 hours agoparentprevTechnically feasible, if you deploy Teable under the same domain with your own app, the user sessions can be shared then you can use an iframe to embed the table into your application, and backend services can access table data within Teable through SQL or API. reply snisarenko 19 hours agoprevThis looks pretty nice. Whats the pricing for the cloud version if I am bringing my own postgres database ? reply bieberChen 18 hours agoparentIn fact, we haven't seriously discussed commercialization yet; our main focus is on perfecting the basic functionalities. But I believe the price will definitely be very reasonable. reply jstummbillig 17 hours agorootparentThis is a non-trivial issue for any db product. I am not going to integrate this into my business, unless I feel very certain about what I can expect. A couple of month ago, for example, Budibase introduced a fundamental license restructure, that (for our specific commercial use case and user structure) moved it from interesting to out of the question. The product itself is useful and thoughtfully designed (as Teable is shaping up to be), I am sure there is a good reason for why they did it, and I am not even putting it past them that something could have been \"worked out\" — but this to me is completely unacceptable level of stress, when I have maybe already deeply integrated your tool into my business. reply mgummelt 14 hours agoparentprevPlato (https://plato.io) is free for up to five users, then $20/user/month (Disclaimer: I'm a founder) reply nextworddev 13 hours agoprevWhen should I use this over tools like, say, Retool? reply bieberChen 8 hours agoparentIt's kind of like collaborating directly on a database (Through the Grid that has been prepared) rather than building an application interface reply d0100 18 hours agoprevI can't scroll the grid vertically using Firefox 124.0b3 reply bieberChen 18 hours agoparentDon't worry, it's been noted! reply jongjong 7 hours agoprevIs this only for building internal tools? Or is it possible to build apps which collect data from end users with support for auth and access control? It seems that there are a lot of such platforms for internal apps. E.g. Retool, Airtable. Are there any no-code tools that support building external-facing apps? Apps that can collect data from untrusted external users? Something that combines the features of Wufoo and Airtable in a single cohesive and flexible service? reply bieberChen 5 hours agoparentTeable can currently share forms to get data from external users. You can also integrate with other lowcode tools to build external application interfaces reply bram2w 18 hours agoprevWhat are the main differences compared to Baserow (https://baserow.io/)? reply foxbee 18 hours agoprevWow - did you just completely rip off Attio's website: Your website: https://teable.io/ Attios website: https://attio.com/ --- Also, what makes this different from: Budibase (cofounder) https://budibase.com Baserow https://baserow.com Nocodb https://nocodb.com reply bieberChen 17 hours agoparentI'm truly sorry if our actions have caused any offense. Our team consists of 5 freelance programmers without any designers on board, and without any funding, so we had to tackle the design aspect ourselves. Initially, we planned to use a colorful gradient theme, but found the UI coordination too challenging for us. It was then that we came across the beautiful simplicity of shadcn, and decided to go with a black and white theme. We did look at various black and white themed websites on the market, including Notion and Attio, but I assure you, we did not plagiarize any images. Thank you for your critique. We will make it our priority to adjust our design to be more unique as swiftly as possible. reply sigmarule 17 hours agorootparentDon't apologize for using a theme for your landing page. And don't apologize to a competitor who is throwing shade and linking to their competing product. reply byteofbits 17 hours agorootparentprevI think it's a bit strange to reply directly that you did not plagiarise any images when the opening table cell background is a direct lift of the SVG from our site... reply haswell 7 hours agorootparentAs a neutral observer, it’s hard to make sense of what exactly you’re saying was copied. What is the opening table cell background? Is this something in the app code itself, or something that made it onto the public site? And this SVG was custom made by your company? reply nolongerthere 18 hours agoparentprevLooks like they used framer.com based on a pop up I see on the bottom right, so maybe that’s a template? reply djfdat 18 hours agorootparentThey should really think about changing out the theme asap. It may not be their fault that they used the same template and their logos look similar, but being the new entrant, they should definitely establish their own brand look. reply esafak 17 hours agorootparentThey should not be thinking about branding at this stage. They should be thinking about differentiation, and delivering value. Getting in front of customers. Their landing page looks perfectly adequate from an aesthetic perspective. reply bieberChen 17 hours agorootparentprevSure we will, this week. reply sigmarule 17 hours agorootparentIn all honesty, don't. Seriously, this is bad advice. Nobody is going to visit your website and say \"woah, this looks like Attio's website - I'm out!\" with, evidently, the exception of a few folks from Attio. The website looks good, you and the other company aren't truly direct competitors so branding conflicts are not much of a concern, and if you both truly just derived your designs from a root common theme then I don't know why this is even being brought up here, unless the other commenters were unaware that their design was derived from a template. There are undoubtedly things significantly more worth spending time on as a very early startup then redesigning your website to appease a few people on HN, who were (assuming the common template bit is true) in the wrong for raising this issue to begin with and and should be updating their comment with this context. EDIT - apologies, the OP here is not from Attio which was my assumption and would've made the OP's post unnecessary but an understandable reflex to seeing a doppelganger of their own website. If you check the OP's profile to see which company they're _actually_ from you will certainly realize that this entire comment chain should be fully ignored. It's pretty shitty, actually. reply foxbee 16 hours agorootparentI think I was pretty open in my comment (cofounder). They copied Attio's SVG image and everything. My issue was not the aesthetics but the fact they copied another organization's work. Surely, you don't think that's right? reply sigmarule 14 hours agorootparentAre you serious, man? You edited your comment, twice. We both know your comment did not include \"(cofounder)\", but did include unnecessary jabs at your your competitor (\"this makes me lose all faith in our credibility.\") when you posted it. I've used Budibase and think it's a great product, and couldn't have anything but respect for the people behind it. You're above this. EDIT - Apologies, you did indeed include \"(cofounder)\" in your original post, I just missed it (based on bing's cached page.) Regardless, this is not the way to deal with competitors, and frankly your product speaks for itself. And perhaps obscene amounts of torrenting in my teenage years has permanently skewed my moral compass for these things, but I really don't care about table cell background svg theft. reply foxbee 13 hours agorootparentI'm sorry to hear about your years of torment. reply fkyoureadthedoc 17 hours agoparentprev> Also, what makes this different from With no prior knowledge of any of these and briefly glancing through each site, the main thing that sticks out to me is that Teable is free as in has no pricing page (for now at least) reply foxbee 17 hours agorootparentThe others are open source reply mistermann 19 hours agoprevCan it do embeddable sub-forms and grids, like good ole MS Access? Like, I've got a grid on the main form showing multiple rows, I click on a row and it refreshes two sub forms, one showing a form view of additional fields of the selected record, and the other shows multiple rows from a child table? reply bieberChen 19 hours agoparentIf I understand correctly, this feature is currently supported. You can try the Sales CRM template https://template.teable.io/t/recHJEzSkIZ1IpjltNZ . In the first table named Opportunities, if you expand the first row, that might be exactly what you're looking for. reply mistermann 7 hours agorootparentThat's not quite as functional as MS Access could do (though, maybe it's possible but you don't have a demo), but it is quite nice. This whole app looks extremely promising, for my purposes it looks very close to something that I could build a huge database on top of, I am going to keep it in mind and try to get some time to play with it, I hope you guys keep moving it forward because it is slick! With adequate features, I could see this being something that consultants could use to go in and write impressive custom database software in a month or so for companies like used to be so common, I think it would be a great way for junior people to get into the IT business, with a few caveats of course. reply ramraj07 19 hours agoprev [–] This is exciting! What limits do you typically suggest? airtable had like a 50k record max or something laughable, so you also have similar limits?? reply bieberChen 18 hours agoparent [–] In fact, Teable has no limit on the number of rows; you can always trust Postgres . Here's a table with 1 million rows [0], where you can experience the response speed as well as the feedback from filtering and sorting. [0]: https://app.teable.io/share/shrVgdLiOvNQABtW0yX/view reply egeozcan 18 hours agorootparent [–] I cannot scroll the example. I'm using Firefox. reply bieberChen 2 hours agorootparentFixed~ reply jszymborski 14 hours agorootparentprev [–] I'm not having that problem on FF v123 on Linux Mint reply egeozcan 13 hours agorootparent [–] I have the same version but on Windows reply cyanydeez 10 hours agorootparent [–] can't scroll on mobile kiwi browser reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The software provides a spreadsheet-like interface with various features like cell editing, formula support, data sorting, filtering, aggregation functions, data formatting, grouping, and import/export capabilities.",
      "Users can view data in multiple formats such as grid, form, and soon-to-be-introduced options like Kanban, Calendar, Gallery, Gantt, and Timeline views.",
      "The software ensures fast response speed, high data capacity, supports SQL operations, prioritizes privacy, and enables real-time collaboration without requiring page refresh."
    ],
    "commentSummary": [
      "Teable is an open-source, no-code database combining Postgres and Airtable, offering a spreadsheet-like interface, formula support, data manipulation functions, and full SQL capabilities.",
      "Users are evaluating Teable against similar options like nocodb, baserow, and grist, focusing on its Postgres integration and usability.",
      "Discussions include potential enhancements, pricing, ethical issues like logo similarity, and detailed comparisons with other tools, alongside concerns about bugs and technical problems in various browsers."
    ],
    "points": 242,
    "commentCount": 83,
    "retryCount": 0,
    "time": 1710157323
  },
  {
    "id": 39672111,
    "title": "The Bias of Describing Others Negatively Increases with Later Encounters",
    "originLink": "https://suchscience.org/people-encountered-later-in-a-sequence-described-more-negatively/",
    "originBody": "A new study finds that the later we meet someone in a sequence, the more negatively we describe them New research finds that unconscious bias could disadvantage people who happen to be evaluated later in a sequence, whether it's job applicants, contestants on a reality show, or Tinder dates. DOUGLAS HEINGARTNERMARCH 11, 2024 Imagine you’re the 20th candidate to interview for your dream job, or the 10th hopeful stepping onto the stage to audition for a coveted role. You’re just as qualified and just as talented as those who went before you. But according to a new study, you might be at a surprising disadvantage, thanks to an unconscious bias in how we perceive and describe others. It found that the later we encounter someone in a sequence, the more negatively we tend to describe them. The study was published in the Journal of Personality and Social Psychology on February 29, 2024. The Serial Position-Negativity Effect The researchers coined this phenomenon the “serial position-negativity effect.” They hypothesized that when we sequentially encounter people, we focus on distinct attributes that differentiate each new person from those we’ve already met. And because distinct attributes tend to be negative in the grand scheme of things, our descriptions of later-encountered people become increasingly negative. To test this, the researchers conducted a number of studies. In one, they had 992 participants (recruited from Prolific Academic) describe 20 people based on their Facebook profile pictures. The participants described the first few individuals quite positively, using an average of 6.2 positive words each. But as they progressed through the sequence, their descriptions became significantly more negative, dipping to an average of just 4.7 positive words by the 20th person. In another experiment, 987 participants (about evenly split between male and female, with an average age of 42) were shown short video clips of women introducing themselves on the popular TV show The Bachelor. In these clips, each woman tried to make a memorable first impression on the bachelor, often in creative and attention-grabbing ways. As the study participants progressed through the sequence of videos, their descriptions of the women became increasingly negative: the tenth woman was described significantly more negatively, on average, than the first woman, despite the fact that the order of the videos was randomized for each participant. And their descriptions also became increasingly specific over time. As participants encountered more women, they focused more on what made each new woman stand out, leading to more unique and ultimately more negative descriptions. Related Posts: Example of Bias: Surprising Ways It Sneaks into Everyday Decisions Implicit Bias Unwound: The Surprising Everyday Impact LinkedIn for older workers (55 and up): using a younger photo definitely helps Further Directions The study opens up many possibilities for future research. The researchers next want to examine how our personal quirks and the groups we belong to might amp up or tone down this negative bias towards people we meet later on. They also want to see how long these snap judgments stick around, and how they might be throwing a wrench into things like job interviews and performance reviews in the real world. Implications The research suggests that this unconscious bias could disadvantage people who happen to be evaluated later in a sequence, whether it’s job applicants, contestants on a reality show, or Tinder dates. So the next time you’re in a situation where you’re meeting a lot of new people sequentially, whether a networking event or speed dating night, keep in mind that your impressions may be tainted by this subtle cognitive bias. Study information: Journal: Journal of Personality and Social Psychology Publication Date: February 29, 2024 (Online ahead of print) DOI: 10.1037/pspa0000383 Title: “Differentiation in social perception: Why later-encountered individuals are described more negatively“ Authors: Alex Koch: Booth School of Business, University of Chicago Andrew Bromley: Booth School of Business, University of Chicago Johanna Woitzel: Department of Psychology, Ruhr University Bochum Hans Alves: Department of Psychology, Ruhr University Bochum",
    "commentLink": "https://news.ycombinator.com/item?id=39672111",
    "commentBody": "The later we meet someone in a sequence, the more negatively we describe them (suchscience.org)241 points by amichail 14 hours agohidepastfavorite104 comments airstrike 12 hours agoAt Kellogg they used to say it's best to either be the first to be interviewed or the last—assuming you're a decent candidate and well prepared to begin with. First because you set the benchmark, and your outstanding qualities become \"requirements\" for the candidates that follow to meet that benchmark. Last because of recency bias, so whatever qualities you have are better recalled by interviewers. Everyone else becomes somewhat forgettable. (I would guess they mentioned a study about it, but it's been 10 years and I don't have the reference handy.) In my experience as an interviewer, everyone in the middle does get somewhat mixed up together, especially when I had less than 5 minutes to scribble notes and reset between candidates. But I would modify \"first\" and \"last\" to \"towards the start\" and \"towards the end\" e.g., I will (subconsciously) more easily benchmark candidates against a very strong \"second interview of the day\" than against a lackluster \"very first candidate\", if that makes sense. Said differently, whoever is the first candidate that hits it out of the park becomes the benchmark. And whoever happens to be the best relatively strong candidate is more easily recalled than other relatively strong candidates. Thank you for coming to my TED talk reply makeitdouble 10 hours agoparentA decent way to normalize this effect is to take relatively detailed notes of the exchanges, and make the decision a few hours or a day later rereading the notes. One of the issues we had interviewing very young candidates (like fresh out of college) was how they all looked awkward, even in their outfit, and few had any kind of confidence during the 15~20 min we spent with them, with some saying really weird things (stuff like \"I'm really good at the internet\"). But obviously non of that matters long term, we assumed they'd probably all fit in fine once in. Putting some distance and picking up the good and bad stuff from the transcript helped a lot to get past the weird impressions, including the order we saw them and how tired we were when we saw them. To some point. reply kjqgqkejbfefn 9 hours agorootparent>a few hours or a day later rereading the notes. https://onlinelibrary.wiley.com/doi/abs/10.1002/ejsp.52 >The durability of anchoring effects >Recent research suggests that judgmental anchoring is mediated by a selective increase in the accessibility of knowledge about the judgmental target. Anchoring thus constitutes one instance of the judgmental effects of increased knowledge accessibility. Such knowledge accessibility effects have repeatedly been demonstrated to be fairly durable, which suggests that the effects of judgmental anchoring may also persist over time. Consistent with this assumption, three experiments demonstrate that judgmental anchors influence judgment even if they were present one week before the critical judgment is made. In fact, the magnitude of anchoring was found to remain undiminished over this period of time. reply makeitdouble 4 hours agorootparentThanks, that was pretty interesting. I didn't get access to the full text, but had a look at other papers from the same researcher [0] on what kind of methodology they use. In the case of recruiting, I think the main factor when moving the decision further down the line is the change in information (\"a selective increase in the accessibility of knowledge about the judgmental target\"), in two specific ways: - we actually remember less about the subject, for better or worse. A candidate might have had a weird look, and the notes are probably impacted by that bias, but we can look back at their coding test without that impression and come out with a slightly different conclusion. - we get to compare to other subjects in a different order. In particular, that helps catching weird expectations. For instance if every candidates has been falling through the same trap, it helps give them a pass and assume the question was at fault. If we had to do that in real time, only the last few ones would get a kinder judgement. [0] https://www.researchgate.net/publication/11394075_The_Mallea... reply from-nibly 7 hours agorootparentprevI think the problem is you are still going to write your notes down with increasing negativity. reply makeitdouble 4 hours agorootparentInterestingly enough, yes, but you also understand it reading the notes. For instance they become sparser and sparser, or tendencies arise. I would compare it to reviewing one's code a few hours later. We're still in a similar mindset, but there's a bit more distance, and we also catch the bits that don't make sense when reading back afterwards. That works even better when exchanging notes afterwards. reply azinman2 5 hours agorootparentprevDid you hire? Did they work out? reply makeitdouble 4 hours agorootparentYes, we hired a few that stood out. They were indeed kinda weird for a few months, some got blander afterwards and some stood a lot more, but all in all they were meeting the bar we had in mind, and the ones that really grew weren't those we expected at first. In particular we had people who's surface personality were completely different from what we perceived during the interview. Not in any way that made it hard to work with them, but moving from university to a corporate environment was just enough of a gap to change their behavior in significant ways. I think hiring fresh graduates is way harder in that respect, and we were happy to have some flexibility in the work culture. One of the guy moved from a super rural area to be thrown into the megalopolis, and it was a real journey, we had the funniest late to work excuses (\"couldn't find my bike cause I parked it near the neighbor's house and he moved it in their garage thinking it was his son's\" -- later found his house had a bike parking behind the building) reply weinzierl 10 hours agoparentprevWhat I learned in my years as proposal manager was that it is always best to get the first slot amongst the other bidders for your presentation and the last for the negotiation. reply kjqgqkejbfefn 9 hours agoparentprevHere's a study by someone from Kellogg university on this very subject: https://www.jcr-admin.org/preprints/chernev-preprint.pdf >Semantic Anchoring in Sequential Evaluations of Vices and Virtues >How do people evaluate sequentially presented items? Prior research suggests that sequential evaluations are subject to anchoring biases, such that the values of subsequently evaluated alternatives are assimilated toward the initially considered option. The present research argues, however, that sequential valuations often lead to contrast rather than assimilation effects, whereby values of the subsequently estimated alternatives are distanced from the initially evaluated option. These contrast effects are attributed to semantic anchoring, which stems from evaluating conceptually related options classified into opposing categories (e.g., vices and virtues). reply dhosek 10 hours agoparentprevI remember while hiring for a new team member commenting about the first candidate that we interviewed that if we had seen him later in the sequence we likely would have hired him but coming first, he just seemed “ok.” reply daxfohl 6 hours agoparentprevSeems like you could interview half the candidates and get a lot of time back. reply johnnyanmac 5 hours agorootparentOr none of them because you already had an internal hire ready in the wing anyway. reply bschne 11 hours agoparentprev(how) do you (try to) correct for this with interviews? reply OJFord 10 hours agorootparentSomeone's bound to mention it in this thread (or I hope they do, because I thought it was great but can't remember the name or the details) - but there's some formula that's roughly like: N = total applicants 1. Interview N/10, decline no matter what 2. Hire the next person who's better than everyone seen so far As I recall it was slightly more complex than that, perhaps only to give it an impression of being rigorous theory, but that was the gist of it. reply wiml 10 hours agorootparentThis only applies if you have to make a hire/skip decision immediately after each interview and can't go back later. (The formulation I usually see is a princess choosing a suitor — for pride/face reasons, she can't pass on a suitor and then go back if it turns out he was the best of the bunch.) That's not typically how job interviews work, though. I think the magic number is 1/e of the applicant pool (e = Napier's constant, 2.7...) reply vikingerik 9 hours agorootparentprevThat mathematical problem (the secretary problem) is often cited, but it's not how real hiring works. The problem defines the only success criterion as hiring the single top candidate, and everything else fails. That's not at all what you're looking for in the real world - there are a spectrum of results, where any one in the top decile will be great and the next decile only slightly lesser and so on. reply bschne 2 hours agorootparentOne-dimensional ranking is one aspect of how it breaks in real-world hiring, but I think almost the bigger might be that the secretary problem assumes you need to decide immediately and can't just wait around until you've seen the whole batch of applicants. reply Dylan16807 1 hour agorootparentThe main difference is that you're trying to solve the problem of turning an interview into a way to choose between candidates, while the secretary problem assumes you already figured that out and is trying to solve something different entirely! reply bschne 2 hours agorootparentprevThis is the secretary problem [1] from optimal stopping theory, but it doesn't apply to hiring because you can wait until the end to make a decision once you've seen everyone. The main problem here indicated by OP isn't explore/exploit, it's that the ordering of candidates seemingly distorts their rating of them. 1. https://en.wikipedia.org/wiki/Secretary_problem reply navane 10 hours agorootparentprevI think you're looking for the Secretary problem. And the solution is almost what you said, but instead of N/10 it's a bit over N/3. reply begueradj 4 hours agoparentprevInteresting. Do you have any explanation for that ? reply gumby 9 hours agoparentprevSeems like any mathematician interviewing candidates would know the secretary problem (https://en.wikipedia.org/wiki/Secretary_problem) and thus going first would be the worst option. reply avianlyric 8 hours agorootparentThe secretary problem only applies if you have to reject candidates before you’ve interviewed them all. Specifically it’s formulated as each candidate needs to be either accepted or rejected immediately after their interview. That situation obviously doesn’t apply if you’re interviewing many candidates on the same day and comparing them to each other. As a result to optimal strategy for the secretary problem also doesn’t apply. reply selcuka 8 hours agorootparentprevFortunately real world employers usually don't have to hire on the spot. reply babyshake 11 hours agoparentprevWhat's the conventional wisdom for something like a demo day? Try to go first? reply airstrike 8 hours agorootparentTheir recommendation was to either be first or last. I guess my own twist is you don't have to be the very first or last—you have to be great and to try not to be right in the middle of the pack. But above all, do well and don't overthink this stuff as there's no scientific rigor. It doesn't help to be first or last and suck at it. reply cyanydeez 11 hours agoparentprevnow to make it interesting, place the middle candidates right after lunch to see if having lunch improves the middle stretch reply patcon 13 hours agoprevPut another way, we have a tendency to like ppl, but once we calibrate, we start penalizing ppl. But without context, we round up. We seek positive attributes first before resorting to negative, and only in aggregate. It's a local-first favouring tactic. local networks (maybe small-world) are maybe more stabilized by the up-rounding. Or maybe it's also more about calibration over time. Most of our social technology is about not being shitty to one another at scale (or maybe \"in massive sequence\"), so this seems aligned with my understanding of the world. The work of progress is to \"not be shitty\" at increasing scale (of population, idea complexity, levels of abstraction), and we build institutions that mostly try to do that. Though I think digital has kinda failed on that mission lately, which is another conversation Seems kinda nice and adaptive and optimistic even. Though yes, downsides in a society that lives at scale, if not mitigated by process or social/digital tech. reply petsfed 11 hours agoparentIt seems like maybe abstraction runs contrary to the concept of not being shitty, because while abstraction is meant to neutrally essentialize people, as often as not, the abstraction process is used to identify aberrations for individual treatment, rather than to streamline creating individual treatments for all. Put another way, we typically tend to sort first for assholes, then maybe sort for other things later. reply patcon 7 hours agorootparentAh interesting. I guess when I think of abstraction, I think of a boundary over which complexity is reduced, and therefore information is lost. My more personal sense is that a ton of human misery comes from poorly chosen abstraction. Or abstraction where there should be none. The idea of \"the tyranny of the database\" was where I first encountered this: > At a higher, more semantic level, a subtle distortion in how we perceive reality took hold: things that were hard to represent in databases became alternately devalued and fetishized. [...] Once in awhile a technical counter-current would take hold and try to push back on the tyranny of the database, but the general trend held firm: if it does not fit in the database, it does not exist. > You may not think you know this world of databases, but you live in it. [...] Every time a customer service assistant shrugs and says “computer says no” or an organization acts in crazy, inflexible ways, odds-are there’s a database underneath which has a limited, rigid view of reality and it’s simply too expensive to fix the software to make the organization more intelligent. https://medium.com/humanizing-the-singularity/by-the-end-of-.... reply mym1990 4 hours agorootparentprevOr: people who are hiring their future co-workers sort for people who will not make their life hell(or their co-workers life hell), and then evaluate for actual expertise. reply coldtea 12 hours agoparentprev>Put another way, we have a tendency to like ppl, but once we calibrate, we start penalizing ppl. But without context, we round up. Put it another way: we're shallow, primitive, creatures. reply patcon 8 hours agorootparentHeh I like the sentiment, but this feels to me a little bit odd, like saying \"evolution is a cold ruthless inhumane process\". There is no human (the root of all things \"humane\") without the evolution being critiqued by the very human values it made possible :) What you're calling \"primitive\" (perhaps with judgement) is part of the system that perpetuates our collective socio-biological process in poorly understood ways. For all we know, liking ppl until we're at scale is highly adaptive (in an information-theoretic sense) for sociality as a whole, and not some broken primitivity :) reply coldtea 23 minutes agorootparentWell, evolution doesn't care anyway if there's a human - much less a humane human, it's indeed a \"cold ruthless inhumane process\". Evolutionarily speaking we could be replaced by cockroaches in a heartbeat if the conditions arise reply sctb 12 hours agorootparentprevIf that's true then I would put even less stock into these just-so stories. reply tqi 12 hours agoprev> The participants described the first few individuals quite positively, using an average of 6.2 positive words each. But as they progressed through the sequence, their descriptions became significantly more negative, dipping to an average of just 4.7 positive words by the 20th person. I don't have access to the study, but I'd be curious why they chose to count positive words as opposed to just asking people to rate on a numerical scale? My impression is that sentiment scoring via bag-of-words is not a particularly robust method, especially in 2024. It also sounds like they didn't normalize by description length, so outcome could just as easily be because people's responses got shorter as time went on due to fatigue? (also, this is a nit with the article rather than the study, but given the methodology I think it is important to distinguish between becoming less positive and becoming more negative, and in this case I would not describe using fewer positive words as \"became significantly more negative\") reply balderdash 12 hours agoparentYeah - antidotally I feel like earlier in a hiring process people are apt to expound on candidates fit for portions of the role and in later stages it basically just thumbs up or down and only more exposition in the case of disagreement reply p1necone 11 hours agorootparentYeah, when you don't have anything to compare them with you need to describe them. But once you have some strong candidates to compare them with everyone just becomes \"are they better or worse than X\". reply CRConrad 12 hours agorootparentprevWhat do you need an antidote against? (“Anecdotally”, I think.) reply hinkley 12 hours agorootparentMaybe this is why some companies have hierarchical hiring processes. The first round is a basic filter. By the time you get to the final round you're only thinking about a dozen people. reply d0gsg0w00f 8 hours agoparentprevI'd imagine this is tricky. Today everyone in white collar culture is so guarded with language so as not to offend. You have to have an extremely sensitive ear to understand that mildly positive words actually mean extremely negative. reply nkurz 7 hours agoparentprev> I don't have access to the study Full study is here: https://osf.io/s2zv8/download/?format=pdf reply autoexec 10 hours agoparentprevI'd really need to see this study reproduced several times before I take it seriously at all. Ideally there'd also be other similar but non-identical studies pointing to the same effect. reply ethanwillis 11 hours agoparentprevLikert scales have their own problems. reply tqi 10 hours agorootparentI don't think any of the common problems[1] are relevant for this specific application (understanding the importance of serial position). Like I said, I'd be curious to understand the motivation behind their methodology choice. [1] https://uk.surveymonkey.com/mp/likert-scale-pros-cons/ reply kjqgqkejbfefn 9 hours agoprevThis is called https://en.wikipedia.org/wiki/Anchoring_effect More specifically, sequential anchoring: >Consider the following scenario: A reviewer evaluates many unqualified applicants for a university program successively, and the next applicant to be reviewed is an average (borderline admit-reject) applicant. Because the evaluator is influenced, or anchored, by recently made decisions, this borderline applicant might be admitted to the program. On the other hand, when an evaluator is anchored by having reviewed many qualified applicants, the same or a similarly borderline applicant might be rejected (Figure 1). In this scenario, individual fairness, stating that individuals with similar characteristics should be treated similarly [8], is impaired, and wrong or inconsistent decisions can have a consequential impact. Source: https://dl.acm.org/doi/fullHtml/10.1145/3491102.3517443 reply dkarras 30 minutes agoprevThis was kinda known I think. I remember also reading about blood sugar levels and risk taking behavior. If you are interviewing / auditioning just before people's lunch time, tough luck. They are decision fatigued, hungry, low blood sugar so their risk taking threshold is significantly higher than how it was when they started, so if they thought there were promising candidates from when they were fresh, well rested and not hungry, it is unlikely that you will surpass them even if you are objectively better. If you are their first after the lunch however, that would again be a very good spot. reply abeppu 13 hours agoprevI think a flip side I've seen is when the first applicant through an interview loop has basically no chance of getting an offer because (a) people know they're not calibrated and (b) \"surely the odds are low that the very first person would be the right person\" This kinda means it's a waste of the candidate's time unless they're _also_ just interviewing for practice. reply pcthrowaway 12 hours agoparentIt really depends. A lot of teams don't want to waste their time reviewing a bunch of candidates and will pick the first person who impresses them. Not saying they won't go through with whatever other interviews they had scheduled, but if the first person knocks it out of the park and the remaining 3 candidates who make it through the interview cycle that week are less impressive, they'll just make an offer to the first person rather than continue interviewing reply mym1990 4 hours agoparentprevBut the candidate usually never knows what position they are in line. Sure the person on the other side of the table could divulge, but the candidate shouldn't really trust that information. A person that is aware enough to know they are not calibrated should also not make the mistake of attributing higher/lower odds to something that is mutually exclusive, assuming that the interviews are not systematically set up in a way to put worse qualified candidates first. reply bongodongobob 11 hours agoparentprevI just accepted an offer last week. I was told I was the first to interview and they cancelled the others because I was a perfect fit. Interviewed Friday morning, offer came Friday afternoon, I accepted that evening. /anecdata reply CSMastermind 6 hours agoparentprevThis is probably BS pseudo-science but I read a book years ago that talked about the four primary types of shoppers: 1. Tightwads - spend as little as possible (maximize savings) 2. Spendthrifts - spend everything they have (maximize pleasure) 3. Optimizers - who want the best deal possible (money for value) 4. Satisfiers - who set a bar then pick the first option that meets it (minimize decision time) You see these behaviors in all sorts of decision-making processes as well, so if the hiring manager is a Satisfier type and the first person who walks through the door is great, they'll likely hire them. If the hiring manager is a tightwad or an optimizer though they'll almost certainly force many candidate through to collect data before making a decision and that first candidate has little chance unless they're truly exceptional. reply unsupp0rted 12 hours agoparentprevMultiple times I've been the first candidate in an interview loop (out of say 3 or 4 candidates) and gotten picked. reply eastbound 12 hours agorootparentYes, it works as long as it’s a batch. Won’t work if the next candidate they see is 15 days later. reply mym1990 4 hours agorootparentThis is likely only because the initial candidate was already thrown out of the pool, so the 15 day later candidate might as well become first in line at that point. reply axus 11 hours agoparentprevReminds me of the old trope, \"We wouldn't want to hire someone with bad luck.\" reply mathgradthrow 12 hours agoprevAlternate explanation. The more bachelor contestants you watch, the more patterns of shitty behavior you identify. One could conceive of a more representative population than influencers trying to make money on the bachelor. reply yumraj 13 hours agoprevHmm… I would think that there are two forces at play here: 1) the ones who come first are not compared against any of the latter ones, so make a better impression 2) the ones who come last are still in memory, especially if results are all tabulated in the end. So, if the above is true, and there is no guarantee that it is, I would have expected an inverted bell curve. reply alekseiprokopev 9 hours agoprevMaybe it is somehow connected to so called \"Baby duck syndrome\" https://en.wikipedia.org/wiki/Imprinting_(psychology)#Baby_d... reply karaterobot 10 hours agoprevWould love to see this get reproduced. Until then, I'm not sold. Feels like people have been arranging themselves in sequence for hundreds of thousands of years at least; you'd think somebody would have noticed if this effect were strong enough to be measured. reply faeriechangling 12 hours agoprevI'll hold my breath until I see this result replicated a few times using different methods. This reminds me a lot of https://datacolada.org/98 and if I think about it, the data would be trivial to fake. reply kjqgqkejbfefn 9 hours agoprevAlso this threads needs a touch of AI, we're on HN ! >AI-Moderated Decision-Making: Capturing and Balancing Anchoring Bias in Sequential Decision Tasks https://dl.acm.org/doi/fullHtml/10.1145/3491102.3517443 reply jollyllama 14 hours agoprevStrange, because I have met people who always like the last person they spoke to best. reply unsupp0rted 12 hours agoparentI think it depends a lot on priming, IQ, memory capacity, ability to notice details, etc. Donald Trump tends to like the last person he spoke to best, but probably because that's the person who comes to mind when asked, and anyway he doesn't really care one way or the other. Unless the topic of conversation was himself: Donald Trump, in which case he cares a lot, will remember details, and will like whichever person said the nicest things about him, regardless of sequencing. reply mathgradthrow 12 hours agoparentprevMaybe they weren't speaking to bachelor contestants. reply Exoristos 10 hours agoprevLet's be clear that this was a sequence of unpleasant characters. A reality-TV game show is not the same as a sequence of job interviews. There may not be much to extrapolate here. reply xyzelement 12 hours agoprevA related concept I noticed is that we are \"wired\" to identify with whoever is presented as the protagonist, even when the information is readily available to recognize them as the antagonist. EG - I recently watched Peter Pan for the first time, as an adult. Peter is basically a horrible person who deludes and kidnaps children, puts them in incredibly dangerous situations, doesn't care about people other than to make himself look good (eg, he fought the pirates who had kidnapped the Indian princess, then he got so distracted w himself she nearly drowned in the tide). On the other hand, we're told that Hook and the pirates are the bad guys but really Hook is just responding to the fact that Peter had maimed him and fed his hand to a crocodile. There's no indication of anything bad Hook has done prior or since, other than trying to \"get\" Peter for having done that. The protagonist of Peter Pan is actually the dad who was trying to protect the kids from nonsense but even he got derailed by the wife, therefore leaving the kids vulnerable to all that has happened to them. But if you ask most people who are familiar with the story, they'll react to the superficial presentation that Pan is whimsical and aspirational for children while the Dad represents the dull adult world to be escaped from, or something like that. The crux is that the narrative form suggests how we should feel about whom even if that makes no sense. I think this is what the article here is talking about as well in a different manifestation. reply bazoom42 12 hours agoparentPedantic comment perhaps, but “protagonist” does not mean “good guy” or “admirable”. It just means main character in a story. reply bbarnett 12 hours agoparentprevWhile I get what you're saying, context counts. Hook is a pirate. By definition, that's a murdering, raping thief, whos main income is boarding honest ships, slaughtering their crew, etc. Piracy is/was punishable by death. The story is old, but at the time written, anyone would be a hero for maiming, or killing a pirate. reply CrazyStat 10 hours agorootparent> The story is old, but at the time written, anyone would be a hero for maiming, or killing a pirate. I’m going to contest this. Peter Pan first appeared in 1902[1], by which time the romanticization of pirates was well under way. The Pirates of Penzance, for example, debuted in 1879[2]. [1] https://en.wikipedia.org/wiki/The_Little_White_Bird [2] https://en.wikipedia.org/wiki/The_Pirates_of_Penzance reply bongodongobob 11 hours agoparentprevCmon. Hook is a pirate trying to kill a child. reply xyzelement 10 hours agorootparentPan is not a child, but a predator who \"refused to grow up\" - and the whole \"trying to kill him\" is clearly the result of Pan cutting Hook's hand off and sadistically feeding it to a crocodile (IE, while we don't know what the battle between Hook and Pan was about, the fact that Hook celebrated his win by maiming Hook and humiliating him this way is a good datapoint into his character.) It seems like you're on Pan's side, which I think is in line with the point the comment you're replying to makes. reply tiffanyh 13 hours agoprevRecency Bias This seems to be the opposite of \"recency bias\", which is interesting - if true. https://en.wikipedia.org/wiki/Recency_bias reply arduanika 11 hours agoparentSure, but I learned about this after primacy bias, and now both effects seem very salient to me. I forget if there were others in between. reply gumby 9 hours agoprevI changed my name when marrying and moved from the end of the alphabet to the middle. I wondered what effect it would have: am I now more likely to get a cookie before they run out? But what if it’s bullets — they should run out before they get to me. Spoiler: I’ve not been able to notice any difference. reply thinkingemote 13 hours agoprevCould be worth mixing this with memory. This study had people rating as they went. If they rated after seeing everyone perhaps the ones later would be less negative? reply xyst 10 hours agoprevIn a rapid fire succession, I can see this happening. I personally call this “I just want to get this over with” bias. In an interview setting, I have never completed more than 1-2 interviews in a single day so this doesn’t apply. Maybe hiring managers or recruiters have this bias more than me. reply zaphirplane 6 hours agoprevNot a single comment on how this applies to dating or marriage. I get the sequence is within a short period of time reply anon115 1 hour agoprevdoes the same thing happen with dating? reply nkurz 10 hours agoprevA version of the full paper seems to be available here: https://osf.io/s2zv8/download/?format=pdf reply barbatoast 14 hours agoprevThis is why I prefer to do class presentations first reply hawski 13 hours agoparentIn my experience it is fatigue first and failure of the teacher to keep the time limit second. reply catlikesshrimp 13 hours agoparentprevhmmm, I used to like to go 2nd back then. The first one might have (mostly) \"technical difficulties\" that get solved afterwards. The latter you go, the more desperate the group will become. Fatigue and whatnot. reply dividefuel 12 hours agorootparentAgreed, I like to go in the first ~20% but not actually first. That gives you a chance to quickly make any changes from good qualities you noticed in the first ones, but get it done before fatigue sets in. And it's just nice to have it out of the way so you don't have to stress! reply freitzkriesler2 13 hours agoprevUnrelated but sort of not. If you can don't ever do a job interview after lunch on a Friday. It will never go well. No one wants to be there and is thinking about the weekend. reply quotemstr 11 hours agoprevThis is exactly the sort of popular-psychology thing that fails to replicate, isn't it? I'm pattern-matching it against \"ego depletion\" and the hungry-judge theory, both of which failed to replicate. reply kjqgqkejbfefn 9 hours agoparentNo this is well studied, although you're right this is a \"popular-psychology thing\" since ctrl+f \"anchoring\" returns nothing in the discussed study paper. See: https://www.cambridge.org/core/journals/judgment-and-decisio... >A number of studies have shown the robustness of the anchoring effect. Anchors can influence judgment even after weeks or months (Reference MussweilerMussweiler, 2001; Reference Yoon, Fong and DimokaYoon & Fong, 2019). The anchoring effect is present even in experts in the judgmental domain (Reference Englich and MussweilerEnglich & Mussweiler, 2001; Reference Englich, Mussweiler and StrackEnglich et al., 2006). An anchor can influence subsequent judgment even if it is clearly implausible (Reference Strack and MussweilerStrack & Mussweiler, 1997) or when it is compared to a different object (Reference Frederick and MochonFrederick & Mochon, 2012; Reference Mochon and FrederickMochon & Frederick, 2013). reply faeriechangling 6 hours agoparentprevI too take an absolutely defensive attitude towards results like these which don't get replicated half the time and are trivial to fake. I'm full on \"assume it's fake or bunk until proven otherwise\" when it comes to behavioural psychology because the field has such systemic integrity and reproducibility problems. There's no real systemic replication or anti-fraud efforts so why should I trust anything coming from this field? I'm not trusting any result like this unless it comes from a meta-analysis really. I have heard of this general idea before, that the first and last to interview are more likely to get a job, but the plausibility of the finding doesn't make it any less suspect. reply CoastalCoder 7 hours agoparentprevWell, crap. My next interview with a major CPU manufacturer is (drum roll) this Friday afternoon. Here's hoping someone with a backhoe doesn't check for fiber conduits. reply sebastianconcpt 11 hours agoprevI'll put this over here and slowly and silently walk backwards... Makes me wonder if the function wouldn't end up being like hypergamy. Makes me wonder (as a follow up) if turns up to be like hypergamy, then there is a link between human economics and sexual behavioral drivers. reply sciencesama 12 hours agoprevFamiliarity brings fondness !! reply RcouF1uZ4gsC 11 hours agoprev> To test this, the researchers conducted a number of studies. In one, they had 992 participants (recruited from Prolific Academic) describe 20 people based on their Facebook profile pictures. Alternative interpretation: Paid participants who most likely just want to do the study and get paid get more and more cranky the more pointless words they have to write about unknown people’s Facebook profiles. How well this actually reflects phenomenon in the real world is actually unknown. reply samatman 12 hours agoprevI question describing this as a cognitive bias, let alone an unconscious one, and the experimental protocol used here is awful for attempting to show that it is. The examples they give: Job interviews, dating, auditions, are all cases of search for the best out of a set of possibilities. The earliest candidates have the initial advantage of there being few in the set, or none, for the first candidate. Since the purpose is to compare the entire set, by definition, the first one is the best you've seen. Even if they objectively aren't great, the rating at that time will tend to be more positive. That isn't a bias, it's a best-effort judgement in the face of incomplete information. You'd need a crystal ball not to do this. What they're measuring is immediate impressions, while what they should be measuring is decisions made after the search concludes. I've certainly had the experience of interviewing a candidate for a job and telling myself \"yeah, might be ok\", then a later candidate comes by and I adjust that to \"no, this one is much better\". If they want to show that being first or early in such a sequence makes it more likely someone will get picked, they've failed to do so with this protocol. reply eszed 3 hours agoparentI had the same thought regarding decisions vs impressions. For what it's worth, my preference during my time as a working actor was to audition first thing after lunch. I figured (based largely on my own experiences as an auditor) that the first people in the day would be forgotten / calibrated against, and those later in the morning would be victims of fatigue, whereas directly after lunch I'd have the best chance to make the best impression on fresh auditors. I had objectively excellent rates of success, so figured that method was good enough.I don't think that generalizes to more extensive interview processes. Auditions are weird: you get a \"hello\", then five minutes (max) to do two monologues, or one monologue and a song. It's basically a cattle-call, and very hard to stand out. Sometimes the \"hello\" is actually the most important thing, sometimes what you're wearing is what makes the best impression. As an actor you have essentially zero control, and yet that gives you utter and complete freedom. Still: first thing after lunch. Works every time, except when it doesn't. reply thereticent 12 hours agoparentprevI can't tell from the linked article whether the ratings/descriptions were made immediately, but if so you are correct. It's just not applicable to those examples. This only applies in a \"hired on the spot\" type of evaluation. I'm a psychologist, and though HN usually tends to be tough on psychology as a science, I'm amazed that this paper is being discussed so earnestly. Social science in particular should require a bit more skepticism. reply cushpush 12 hours agoprevFirst Come First Served? reply dumbfounder 8 hours agoparentFirst In Last Out. reply greatgib 12 hours agoprevImagine the huge impact this has on the fairness of jugement in trials? Especially in current time where justice is understaffed and judges have to work long hours... This is quite scary in my opinion. reply mxkopy 12 hours agoprevI wonder if this is more due to the pressure to be descriptive than because of any sort of internal dialogue. If I’m being asked to do a study I feel that I need to contribute by adding something ‘new’ and ‘substantive’ for each question (which ultimately leads to choosing negative qualities to describe a person), but if I weren’t expected to have an answer I would’ve had more boring thoughts about the questions. reply datavirtue 13 hours agoprevMy guess is: familiarity. The first person is part of the algorithm that is applied to all the others, making them more familiar. Familiarity trumps. reply User23 10 hours agoprevI wonder if this explains some of the data correlating promiscuity with divorce. reply ekianjo 11 hours agoprev> You’re just as qualified and just as talented as those who went before you. there is no such thing as people with identical profiles reply Aeglaecia 9 hours agoprev [2 more] [flagged] acdha 9 hours agoparent [–] HN doesn’t need casual sexism and that’s just going to derail a more substantial conversation. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "A recent study in the Journal of Personality and Social Psychology reveals an unconscious bias towards individuals evaluated later in a sequence, leading to more negative perceptions in varied scenarios like job interviews, reality shows, or dating contexts.",
      "The research indicates a \"serial position-negativity effect,\" where participants exhibit increasingly critical descriptions as they meet more people sequentially, potentially impacting sequential evaluations.",
      "Future investigations will explore how personal characteristics and group interactions shape this bias and the duration of these rapid judgments."
    ],
    "commentSummary": [
      "Meeting candidates later in the interview sequence can result in more negative evaluations due to judgmental anchoring.",
      "Decision-making during interviews can be impacted by personal traits, fatigue, and language used, reflecting biases in evaluations.",
      "Strategies must be adopted to reduce biases in hiring processes, and additional research is necessary to validate these findings."
    ],
    "points": 241,
    "commentCount": 104,
    "retryCount": 0,
    "time": 1710183940
  },
  {
    "id": 39668962,
    "title": "Onedoc Labs Unveils Innovative PDF Generation Tool",
    "originLink": "https://github.com/OnedocLabs/react-print-pdf",
    "originBody": "Hey HN, we’re the co-founders of Onedoc (https:&#x2F;&#x2F;www.onedoclabs.com&#x2F; ), and the original contributors to the open-source library react-print-pdf (https:&#x2F;&#x2F;github.com&#x2F;OnedocLabs&#x2F;react-print-pdf ) which lets developers design and generate PDF documents automatically. Here’s a demo video: https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=MgfCyOyckQU&t=3sBillions of PDFs are generated daily: invoices, contracts, receipts, reports, you name it. Developer time gets wasted producing these basic documents because there are no good-enough tools to design and generate PDFs.We previously worked at giant firms, where documents (especially PDFs) were central to most workflows. We got asked to generate automated trade confirmations for our customer’s counterparties. We could not find any tool other than outdated libraries offering poor control over layout and the generation process. In the end, we just created our own—basically bringing web technologies to PDFs. That was the genesis of Onedoc.PDF creation has two phases: design (specifying content and layout) and generation (producing the actual PDF file). Onedoc lets you do both simply and automatically.Design: we have an open-source library called \"react-print-pdf\" (https:&#x2F;&#x2F;github.com&#x2F;OnedocLabs&#x2F;react-print-pdf ) that allows you to design a document the same way you would design a website. It supports Tailwind CSS components, Chakra UI components, and recently also built LaTeX and Markdown components. The latter let you write text in Markdown style, and include formulas using LaTeX syntax, directly within a React component.Generation: we have an API (https:&#x2F;&#x2F;docs.onedoclabs.com&#x2F;api-reference&#x2F;introduction ) and Node.js SDK (https:&#x2F;&#x2F;docs.onedoclabs.com&#x2F;quickstart&#x2F;nodejs ) that render your designs into PDFs.The choice of renderer significantly affects the accuracy of the resulting PDF. For example, exporting a webpage into PDF will often result in a layout that differs from the original webpage. We ensure that what you designed is what you get, and therefore you have 100% control over the entire layout of your document including margin, style, etc. We can do that because we built the react-print-pdf library to match the HTML&#x2F;CSS to PDF rendering tool we have.Once you have generated your document, you can either store it on your local system or, if you want, use our platform (https:&#x2F;&#x2F;app.onedoclabs.com&#x2F; ) to host your document online. If you use us, you’ll also get analytics over your documents.Our main product is an API, but you can try it on our website directly (https:&#x2F;&#x2F;www.onedoclabs.com&#x2F;) using our playground without any installation or sign-up. Our pricing is usage-based: per document generated. The pricing is degressive: the more documents you generate, the less you pay per document. If you don’t want to pay for PDF generation, you can still generate as many documents as you want, but with a watermark on the margin.It’s been fun to see what our users are building with our open-source library (components, templates, etc.) and our API. We have a website (https:&#x2F;&#x2F;react-print.onedoclabs.com&#x2F;) dedicated to the open-source library where we post the templates submitted by the community. Some early power users built simple web apps (CV&#x2F;Resume generator, NDA and Invoice generator). We are excited to show our product to the HN community and look forward to your feedback!",
    "commentLink": "https://news.ycombinator.com/item?id=39668962",
    "commentBody": "Onedoc (YC W24) – A better way to create PDFs (github.com/onedoclabs)229 points by AugusteLef 19 hours agohidepastfavorite146 comments Hey HN, we’re the co-founders of Onedoc (https://www.onedoclabs.com/ ), and the original contributors to the open-source library react-print-pdf (https://github.com/OnedocLabs/react-print-pdf ) which lets developers design and generate PDF documents automatically. Here’s a demo video: https://www.youtube.com/watch?v=MgfCyOyckQU&t=3s Billions of PDFs are generated daily: invoices, contracts, receipts, reports, you name it. Developer time gets wasted producing these basic documents because there are no good-enough tools to design and generate PDFs. We previously worked at giant firms, where documents (especially PDFs) were central to most workflows. We got asked to generate automated trade confirmations for our customer’s counterparties. We could not find any tool other than outdated libraries offering poor control over layout and the generation process. In the end, we just created our own—basically bringing web technologies to PDFs. That was the genesis of Onedoc. PDF creation has two phases: design (specifying content and layout) and generation (producing the actual PDF file). Onedoc lets you do both simply and automatically. Design: we have an open-source library called \"react-print-pdf\" (https://github.com/OnedocLabs/react-print-pdf ) that allows you to design a document the same way you would design a website. It supports Tailwind CSS components, Chakra UI components, and recently also built LaTeX and Markdown components. The latter let you write text in Markdown style, and include formulas using LaTeX syntax, directly within a React component. Generation: we have an API (https://docs.onedoclabs.com/api-reference/introduction ) and Node.js SDK (https://docs.onedoclabs.com/quickstart/nodejs ) that render your designs into PDFs. The choice of renderer significantly affects the accuracy of the resulting PDF. For example, exporting a webpage into PDF will often result in a layout that differs from the original webpage. We ensure that what you designed is what you get, and therefore you have 100% control over the entire layout of your document including margin, style, etc. We can do that because we built the react-print-pdf library to match the HTML/CSS to PDF rendering tool we have. Once you have generated your document, you can either store it on your local system or, if you want, use our platform (https://app.onedoclabs.com/ ) to host your document online. If you use us, you’ll also get analytics over your documents. Our main product is an API, but you can try it on our website directly (https://www.onedoclabs.com/) using our playground without any installation or sign-up. Our pricing is usage-based: per document generated. The pricing is degressive: the more documents you generate, the less you pay per document. If you don’t want to pay for PDF generation, you can still generate as many documents as you want, but with a watermark on the margin. It’s been fun to see what our users are building with our open-source library (components, templates, etc.) and our API. We have a website (https://react-print.onedoclabs.com/) dedicated to the open-source library where we post the templates submitted by the community. Some early power users built simple web apps (CV/Resume generator, NDA and Invoice generator). We are excited to show our product to the HN community and look forward to your feedback! Perz1val 0 minutes agoIf I would want to put an image in footer on every page, would it reuse the same resource? How do you do shadows, spam rectangles or attach an image + mask, or maybe you bake that into the image itself? Many pdf tools are so bad at, the result can be even more than x10 in size and I don't even mean saving entire pages as JPEGs reply egnehots 14 hours agoprevThe main issue is conflating templating and pdf generation. Using html to pdf solutions allow to do the templating in html, where it is pretty much a solved issue. And as many said, headless chrome is a robust html to pdf solution, even though it feel like a hack. But, yeah, there seems to be a lack of awareness about these options within corporations. So, kudos to you for addressing a genuine problem! reply pedro120 14 hours agoparentIndeed, we aim at bundling this in a way that makes it easy and obvious for enterprises to build their PDFs that way. reply plopz 13 hours agoparentprevThe problem with chrome is the performance, it is very slow and uses a bunch of memory. There was a neat post here awhile ago about generating pdfs faster https://news.ycombinator.com/item?id=39379690 reply AugusteLef 13 hours agorootparentIndeed, speed is an issue (and it's hard to tackle). Additionally, when using Chrome, what you see is not always what you get. The layout often doesn't match expectations, especially with complex elements. It's ok for simple use cases, but for professional and scalable solutions, you usually need to switch to something else!! reply egnehots 11 hours agorootparentYes, that's a hard issue for arbitrary/user-provided HTML pages. But with templates under your control, the context is different. Your designers do not have to trust Chrome; they can preview, tweak using print media queries, and provide robust templates that print to acceptable PDF. The speed issue is also real, but you can scale horizontally by spawning other chromium instances (with gotenberg containers for ex). Not really efficient but it can certainly help alleviate most of the load... reply AugusteLef 11 hours agorootparentAbsolutely! It's all about finding the right balance between granting users complete control over the layout and restricting it to ensure specific use cases through templates. As for scaling, indeed, horizontal scaling is an option, but there's a limit to its practicality. It would be interesting to conduct a resource/time analysis over the various solutions available on the market in different situations/use cases reply yencabulator 13 hours agoparentprevTypst is a typesetting language that makes programmatic layout and processing JSON input pretty darn simple. I make invoices by having a Typst template read line items from a JSON file. https://github.com/typst/typst reply adfaure 11 hours agorootparentJust spent my Sunday creating my invoice template in typst as well. I enjoyed it, and I could do what I wanted quickly! reply AugusteLef 7 hours agorootparentNever used it actually, is it in between LaTeX and Markdown with the possibility to process JSON input? reply yencabulator 7 hours agorootparentI'd describe Typst as somebody wanted a TeX reimplementation in Rust with better syntax and modern ideas, and made that their Masters projects. It's markdown-like (but definitely not exactly CommonMark, don't expect that), with # acting as escape into the world of layout and programming. Templates are just functions that return a datatype called a content block, and so on. https://typst.app/docs/tutorial/writing-in-typst/ reply AugusteLef 7 hours agorootparentOk very interesting, i'll give it a try! Thanks for sharing reply midenginedcoupe 16 hours agoprevI've also spent much longer than I'd like on this same problem. Having a lightweight-enough service to convert html->pdf on the fly, with good fidelity, and that can create an accessible pdf seems to be impossible. If you can nail accessible PDFs then you'd open up a very big government market. reply AugusteLef 16 hours agoparentWe felt the same, and that's precisely why we built this tool! The key, as you mentioned, is fidelity, especially for designing complex layouts. We hope to bring something new and valuable to the table. And yes, documents are central to many industries including government, legal, banking etc. reply dmazzoni 16 hours agorootparentCan you directly answer whether your tool generates tagged PDFs? Of course, you can't guarantee that the resulting document is 100% compliant because you can't enforce that the input is valid, but are you at least outputting a complete tag tree with as much semantics as possible given the input? reply Titou325 15 hours agorootparentYes, Onedoc generates tagged PDFs as long as you add a `title` property to the API call to make the PDF UA/1 compliant. reply matteason 18 hours agoprevReally interesting product. I do agree that the pricing seems steep ($0.25/document on Pro on the most generous tier) but I don't know enough about pricing B2B products to know if that would be a blocker. I agree that HTML -> PDF can be a really powerful tool. I worked on the UK government's tool to generate energy efficiency labels for consumer goods [0] and we ended up doing PDF generation with SVG templates, using Open HTML to PDF for the conversion. That ended up working very well, though as you allude to there can be some gotchas (eg unsupported CSS features) that you need to work around. A few questions: - Do the rendered documents support PDF's various accessibility features? - How suitable is this for print PDF generation? For example, what version of the PDF spec do you target? What's your colour profile support like? Do you support the different PDF page boxes (MediaBox, CropBox, BleedBox, TrimBox, ArtBox)? [0] https://github.com/UKGovernmentBEIS/energy-label-service [1] https://github.com/danfickle/openhtmltopdf reply Titou325 17 hours agoparentThe pricing does go down for larger volumes and is something we still have to narrow down to the exact place that makes sense to companies and is also viable. - We do not force PDF/* profiles down to the user, but it seems that for most of them PDF/UA-1 would be a sensible default. We can extract most of the tags from the HTML semantics by themselves which makes it much easier. - We target the PDF 1.7 spec. Color profiles can be changed and you can use a custom .icc profile, with the corresponding embedding restrictions based on the document format. MediaBox is supported through the @page size property. Bleed, trim and marks can be added using vendor specific css properties. We don't support ArtBox yet but this is something we can look into! So far none of our customers really wanted to take this out to a real print shop, but we would be glad to help people go down this route :) reply dmazzoni 16 hours agorootparentSo are you saying that you don't output tagged PDFs now? For those who don't know, if you use Chromium's print-to-pdf feature you get a tagged PDF. And it's scriptable from the command-line too. reply AugusteLef 14 hours agorootparentAs mentioned in another comment, \"Onedoc generates tagged PDFs as long as you add a `title` property to the API call to make the PDF UA/1 compliant.\"! Hope it helps reply ak217 17 hours agoprevFYI: the open source state of the art in this area is Playwright (the successor to Puppeteer) with Paged.js (https://pagedjs.org/). I highly recommend that everyone check out and donate to paged.js, it's a fantastic project with lots to like. It certainly blows commercial alternatives like Prince XML out of the water. That forms a solid foundation that I find it hard to imagine paying for. The things where you might still command a premium are basically safety mechanisms/CI checks/library components that ensure the PDF renders correctly in the presence of variable-length content, etc. as well as maybe PDF-specific features like metadata and fillable forms. Naive ways to format headers, footers, tables/grids/flexboxes etc. often fail in PDFs because of unexpected layout complications. So having a methodology, process, and validation system for ensuring that a mission critical piece of information appears on a PDF in the presence of these constraints could be attractive. reply caesil 16 hours agoparentI think https://github.com/diegomura/react-pdf is closer to what this company is doing. In fact their open source library, https://github.com/OnedocLabs/react-print-pdf, seems like a higher-level library that sits above react-pdf. Reminds me a lot of the set of react-pdf based components I built for a corporate job where letting users create PDFs was a huge part of the value proposition. They're solving a really cool problem, actually, because building out into certain difficult use cases like SVG support was a huge pain. reply Titou325 17 hours agoparentprevWe are currently experimenting with this approach. A good thing about paged.js is that we would be able to provide hot-reload and live preview of files without actually converting to PDF. Your second point is very interesting, seems like some kind of .assert('text').isVisible() API. We may want to dig into that further! reply rudasn 13 hours agorootparentOr maybe some visual diffing based on expected output, based on the template/layout/theme used, since you'd want to perform this check on every pdf generated in prod (that has real, sensitive data) , not just in CI or testing mode, if you're aiming for critical docs. Cool project btw, congrats for the launch! reply cyanydeez 9 hours agorootparentsafer would be out of band pdf-image conversion the OCR WHen dealing with layouts and assurance, I would go way out to verify as close to print as possible. there's also a discussion about color to black/grayscale printing where you want a document to stay in character at grayscale. these would be premium features I'd think. reply timvdalen 17 hours agoparentprev(How) does it handle CMYK and print PDFs? I see images of printed books created by Paged.js, were these post-processed, or printed using a printer that does a best-effort RGB conversion? reply ak217 17 hours agorootparentI'm not sure - we don't do color correction on our PDFs because we don't have photos in them and color rendering is not mission critical - but paged.js is focused on the concern of layout for print media. I would imagine color rendering can be solved orthogonally to what paged.js does for you, as long as you specify the color data in CSS. I'm pretty sure paged.js will pass it through without messing with it, so you're good if the browser that Playwright/puppeteer is driving supports the correct color profile when emitting the PDF. I honestly don't know if browsers have sufficient support for that when emitting a PDF, though. Overall you're right that color correction is another area where you could probably command a premium. reply timvdalen 15 hours agorootparentIt's certainly an area with more depth than I anticipated when I first started getting into it. Adobe is still pretty much the only one that can get a PDF compliant with print standards. As far as I know, there's no way to currently get colors adhering to print color profiles in CMYK out of browsers. Indeed, if color correctness isn't mission critical, I can imagine that going with Paged.js can be a nice experience! (Edit: in my experience so far, it's been really really hard to 'correct' colors from an existing PDF in a way that gets a satisfying end result---the colors are usually muted/washed out) reply ak217 15 hours agorootparentI was curious and searched around and found this presentation: https://www.w3.org/Graphics/Color/Workshop/slides/Erias.pdf You're right - although many of the building blocks are there, it appears there is no way to specify a colorspace or print profile when asking Chrome to emit a PDF (and I doubt the other browsers are any better). Skia (the PDF rendering engine that Chromium uses) actually supports colorspace transforms, but Chromium doesn't seem to hook that up to CSS or even support non-RGBA colors in its rendering pipeline. reply Sytten 6 hours agoprevDefinitely a problem I experienced. Big fan of browserless.io. Though I didnt see any comment on the biggest problem in this space: SSRF. Most HTML-to-PDF are deeply insecure and I am more than happy to pay someone else to deal with isolation and security. Report generators are often used to leak cloud secrets via the metadata API. reply AugusteLef 6 hours agoparentTrue. Security is a significant concern, and in our discussions with businesses, we realised that most of them do not want any kind of data leaving their own systems. This is especially true in the biotech/healthcare industry, but also in legal and banking. That's why we're considering an on-premises solution for the future (as we're focusing on B2B). However, I assume most people were talking about personal use cases or non-sensitive documents, hence the fact that no one mentioned SSRF (yet ;)). reply throw03172019 4 hours agoparentprevAlso big fan of browserless. Do you run it yourself? We run the browserless docker containers on prem. reply Brajeshwar 18 hours agoprevMay be this is just me but this looks extremely costly to me! It will cost $2,500 to generate 50,000 PDFs. Are edits/corrections additional cost? reply jot 16 hours agoparentIt sounds like this is as advanced as DocRaptor[1]. They have what I consider to be the best PDF generation API, giving complete control over the documents you need to create. The pricing is similar. If you'd rather do it for free weasyprint[2] is the best open source alternative. Another more affordable option you might want to consider is Urlbox[3]. (Disclosure: I work on this) Urlbox's rendering engine is based on Chrome. It's been refined over the last 11 years to render pages as images or PDFs[4] that look great. I was a customer for 5 years before I joined the team. Everything we'd tried before Urlbox was a disappointment. Urlbox probably can't match the power of either Onedoc or DocRaptor, but pricing starts at less than $0.01 per document and drops significantly with scale. If your PDF looks great when saving as PDF in Chrome it should look identically brilliant with Urlbox. [1]: https://docraptor.com [2]: https://weasyprint.org [3]: https://urlbox.com [4]: https://urlbox.com/html-to-pdf reply Titou325 18 hours agoparentprevThis is a good point, and we are still trying to figure out how to price things fairly. Depending on the type of PDF, whether it is a simple receipt or a large multi-pages report, associated costs are very different on our side. At this time, we rely on other proprietary software that we are aiming to replace but that incur high costs on our side as well. Edits and corrections on generated PDFs is not provided as the PDFs are signed as-is, however you can attach the metadata to the PDF and rerender with the modifications. reply mediaman 17 hours agorootparentAs a point of reference on pricing, convertAPI charges $0.05 per document conversion at their most expensive tier, and with any level of fixed commitment ($80 - $300 per month) it goes down to $0.016-0.006 per document. Their PDF conversion is pretty good (I use it for PPT/Word -> PDF conversion), though your product is obviously different and has different/better capabilities for programmatic PDF creation. Still, a reference point. Pricing page: https://www.convertapi.com/prices reply passion__desire 18 hours agorootparentprevEdits would be limited to certain pages but may spill over (e.g. tables) so the whole PDF need not be generated. Only edited pages can be inserted back to previously generated PDFs. Could be an optimization to reduce cost. reply snadal 18 hours agoparentprevI second this. Maybe I'm missing something in the value proposition, but we already generate PDFs from .docx/.html templates using open source libraries and Docker microservices. Do not misunderstand. A Stripe for generating PDFs can be great, but for a small team, $0.50/PDF is way more than I can afford (after all, you can create a small number of PDFs without too much fuss). Maybe you are oriented towards large companies? reply AugusteLef 16 hours agorootparentIndeed, and as you mentioned, open-source libraries are always an option. It's worth noting that our open-source library assists in document design, allowing freedom in renderer choice. While the open-source library is aimed at individuals, our API targets businesses of any size. Our pricing can be as low as $0.05 per PDF for high-volume or annual commitments. Additionally, we offer cloud hosting for your documents for up to 90 days, and our pricing includes analytics. reply adnans 17 hours agoparentprevWe use https://www.api2pdf.com/pricing/ and it's priced per bandwidth and usage - ($.001 per mb bandwidth and $0.00019551 per second of computation) You can choose which API to use: Headless Chrome, Wkhtmltopdf, Libreoffice, etc. reply Leoko 18 hours agoprevI had to deal a lot with PDF generation over the past few years and I was very unhappy with the eco-system that was available: 1. HTML-to-PDF: The web has a great layout system that works well for dynamic content. So using that seems like a good idea. BUT it is not very efficient as a lot of these libraries simply spin up a headless browser or deal with virtual doms. 2. PDF Libraries (like jsPDF): They mostly just have methods like \".text(x, y, string) which is an absolute pain to work with when building dynamic content or creating complex layouts. This was such a pain point in various projects I worked on that I built my own library that has a component system to build dynamic layouts (like tables over multiple pages) and then computes that down to simple jsPDF commands. Giving you the best of both worlds. Hope this makes somebody's life a bit easier: https://github.com/DevLeoko/painless-pdf reply Crowberry 17 hours agoparentI'm with you.. We ended up writing a similar wrapper around https://github.com/jung-kurt/gofpdf library. We haven't open sourced it yet. But it's made it a lot easier to deal with rendering a PDF, especially over pagebreaks ect. reply Leoko 17 hours agorootparentYes, page breaks are probably the most significant difference between the layout of a web page and a PDF document, and thereby a major drawback when using HTML-to-PDF. There is little to no tooling for this in the web. If you want granular control over how your PDF will look with content that is more than one page long, you will have a hard time using html. reply Titou325 17 hours agorootparentWe actually provide helpers to do that in our React library https://react.onedoclabs.com/components/shell#pagebreak CSS actually implements the break-before property to control this https://developer.mozilla.org/en-US/docs/Web/CSS/break-befor... which is also supported by the Print to PDF dialog in modern browsers. reply Titou325 17 hours agorootparentprevWe actually provide helpers to do that in our React library https://react.onedoclabs.com/components/shell#pagebreak CSS actually implements the break-before property to control this https://developer.mozilla.org/en-US/docs/Web/CSS/break-befor... which is also supported by the Print to PDF dialog in modern browsers. reply pedro120 17 hours agorootparentprevThat's what we are trying to solve at Onedoc, we want developers to be able to have full control over the PDF layout as they write content. react-print is built with the intention of creating the illusion that React was meant for PDFs. reply Gualdrapo 18 hours agoprevIt seems TeX/LaTeX is a major inspiration in this, though there can be seen some room for improvement for details like hyphenation, expansion/protusion and microtypography. Not sure if/how a web engine can reach to those points but still it seems this has a potential niche and market outcome, so congrats. Though personally I wish stuff like ConTeXt was more popular and approachable - to my humble knowledge their Lua backend seems to have huge potential, I am doing my invoices with ConTeXt/Lua. reply Titou325 18 hours agoparentIt definitely is! Typesetting quality was the main reason we chose not to go down the Puppeteer/headless browser route but rather use a completely separate engine where typography is a first-class citizen. We like LaTeX, but even for advanced users laying things out can be a difficult thing. Given that documents are a frontend, we wanted to bring the same tools frontend developers already use. reply petern81 15 hours agoprevThis is a good problem to tackle. The hours i've sunk... reply AugusteLef 15 hours agoparentWe spent many hours designing and generating PDFs at our previous venture.. terrible experience. Which is why we're now focused on solving this issue! reply staffors 14 hours agoprevI see that you support page breaks and headers and footers and stuff which is very cool. Is there some form of widow/orphan control when text wraps from one page to the next? How do you handle things like a large table that is longer than the length of a page? reply staffors 14 hours agoparentAlso, do you different paper sizes (A4 and Letter)? reply Titou325 14 hours agorootparentWe support the size[1] property and the widows and orphans[2] spec for both your needs :) [1]: https://developer.mozilla.org/en-US/docs/Web/CSS/@page/size [2]: https://developer.mozilla.org/en-US/docs/Web/CSS/orphans reply ffpip 17 hours agoprevLove the demo on the homepage with the render button. Really helps explain the product! reply AugusteLef 16 hours agoparentThanks! We try to make our product as accessible as possible for anyone to use (or at least to test). It's good to hear that our efforts have been worthwhile! reply Nathanba 4 hours agorootparentThe text says \"Instantly generate dynamic documents based on real-time data.\" but when I changed the react code to give the QR code a red color and clicked render it took far longer, maybe 5-6sec to render again. reply debarshri 2 hours agoprevWhat would be interesting is that if users generate PDFs via your library, embedding a machine format that allows parsing the PDF via your library is made easy, could become a game changer. reply marceldegraaf 17 hours agoprevWe're using Gotenberg[1] to convert a rendered web page (with Elixir/Phoenix, in our case) to PDF. Works like a charm and we can use our existing frontend code/styling (including SVG graph generators) which is a huge bonus. 1: https://gotenberg.dev/ reply Titou325 17 hours agoparentWe actually experimented with Gotenberg! Ultimately it is a layer on top of Chromium for conversion and we were dissatisfied with the results. I am curious so as to how are you handling assets and other static media / attachments: do you embed everything in a single HTML file or do you use some kind of bucketing system to resolve URLs? reply marceldegraaf 15 hours agorootparentGreat question! We actually just use the static assets (stylesheets, images) from our public asset CDN. The generated HTML points to the latest version of those assets, which means we can always use all the latest styling/assets in our generated PDF files. To give you an idea, this is the kind of PDF files we generate that way: https://assets.walterliving.com/documents/walter-charlotte-d... reply kornhucker 18 hours agoprevSuper interesting and potentially a fit for a project I'm working on right now. What are the benefits of going this route vs styling your page for print (ex. tailwind print modifier) and relying on the browser's print dialogue? reply Titou325 17 hours agoparentThere is both commonalities and differences! Both approaches rely on web technology to provide the layout and are flexible in terms of frameworks and integrations. Where things differ is that we don't actually use a browser under the hood. This allows a much better control over typesetting and layout - and you can do it on the server. We have also more controls over the outputted PDF and the ability to use more advanced features such as form fields or embedding other files and metadata in the PDF. reply fasteddie31003 18 hours agoprevIs this just a wrapper around Puppeteer that renders a pdf? I do this currently with an AWS lambda that has a chrome-aws-lambda layer. reply Titou325 18 hours agoparentWe use a dedicated HTML to PDF engine (such as PrinceXML) rather than building on top of a browser. Main issue with browser-backed implementations is that PDFs are often of subpar quality. However, the main good thing is you can rely on the latest CSS features. In the end, what was the main decisive factor is the support for the PrintCSS and PagedMedia specifications, which have been completely discarded by major vendors and only implemented by specific engines. reply winter-day 18 hours agoprevCongrats! My career has also revolved around PDF generation (once for federal compliance at large companies, second for scrubbing data from PDFs for HIPAA compliance and then generating a new pdf based on the scrubbed data). I think I've seen your tool around, I ended up creating a workflow that generated LateX scripts then converted them to pdfs, and the second a python library. The most difficult aspect for our tools was formatting - the pdfs were generally 60-100 pages and tables could show up anywhere and break the page/formatting. Quite curious to see how your company will grow, good luck! reply DutchHugo 17 hours agoparentCurious, which python library did you use to convert to PDFs? currently looking into a couple options myself reply stormfather 16 hours agorootparentweasyprint isn't terrible reply travelinmyblood 13 hours agoprevFirst reaction - congrats guys, this is a problem I have in my own business. Second reaction - the pricing is way over the top and the model is unusual. In your own pitch you talk about the volume of documents created every day. How does that square with per document pricing? reply AugusteLef 13 hours agoparentThanks! We're fine-tuning our pricing model and realize we have some work to do in this area hahaha! Indeed, at a certain scale, per-document pricing becomes almost impossible (we're talking about millions of documents generated daily). As noted in another comment, costs vary significantly depending on the PDF type, from simple receipts to large multi-page reports, especially since we currently rely on other proprietary software that incurs high costs. In the future, we aim to offer more than just document generation (like e-signature, analytics, hosting, editor, etc.) and hope to move away from \"per document\" pricing for high volumes. That said, our open-source library allows anyone to design a document and use their preferred renderer for PDF conversion, with all the pros and cons each solution provides. There are more comments about pricing providing additional information; feel free to dive in if you have any comments or questions reply rahhulk7 14 hours agoprevThis looks interesting! Especially the Markdown and LaTeX components in react-print-pdf. Could be a great way to streamline technical documentation generation in codebases. Would love to see some examples of those in action. reply AugusteLef 14 hours agoparentIndeed it could be a very interesting use case. While we are more \"Selling Shovels\" it could be interesting to explore this use case and maybe build a simple demo out of it! And yes, as a big fan of LaTeX myself (I used to do all my research reports on overleaf), we wanted to be able to integrate formulas, code and more into your document very simply. Glad you like it ! reply ska 11 hours agoparentprevFWIW I’ve had some good results for technical documentation in RST markdown with sphinx for generation. You can develop latex header details for detailed templating for pdf output, etc. while keeping the html more simple if you want . reply AugusteLef 11 hours agorootparentThanks for the tip, I will take a look at it asap reply roastedpillows 13 hours agoprevThe pricing is a little expensive. Have you heard of https://htmldocs.com/ I've been using them for a year now and it's free reply jjslocum3 14 hours agoprevDoes Onedoc retain any visibility into, or in any way use or reserve the right to use any content created using its API in any way? Obviously, calling an API means sending document contents to Onedoc. reply AugusteLef 14 hours agoparentWe do not. We are also working on getting SOC2 compliant as soon as possible. More about security here: https://docs.onedoclabs.com/ressources/security (especially how we use temporary buckets). Also, you can chose rather to host you generated documents on our platform or to store it on your local system. But indeed, calling an API means sending documents contents to Onedoc in a way or another. We aim to provide a self-hosted solution in the future to solve this issue reply Crowberry 17 hours agoprevThis looks really interesting! One of the main reasons we've opted to writing a more complex rending code is for speed. We're getting around 500ms for a single document, which is (last I tested) quicker than any headless chrome setup. How long does it take to render using your API? :) reply pedro120 17 hours agoparentRendering time scales with the length / complexity of the document. At the moment, our self-serve API renders slower than a headless chrome setup. We are working on speeding this up as it is currently in the order of seconds. reply Crowberry 13 hours agorootparentAlright, thanks! reply kwhinnery 15 hours agoprevLooks awesome, will keep this in mind - every so often you need to create complex documents in code, and it's always a pain. Doing it with a familiar modern programming interface would be nice. reply AugusteLef 15 hours agoparentExactly, that's one of the main reasons we began working on this. We aim to bring the modern web technologies used for website design into the document world. This includes enabling the use of React and, of course, Tailwind, Chakra UI, etc. reply anonymouse008 16 hours agoprevHmm interesting... I just went through this user experience on iOS generating PDF invoices locally. I attempted the HTML > PDF route, but Webkit is thorny wrt to layouts (as you mentioned). I did settle in with drawing everything from the ground up > which with LLMs wasn't as hairy as it used to be, even got a little Swift framework out of the deal. Am I understanding the docs correctly that you don't have a local library available (the SDKs are just calling the APIs right?)? Mind going through why you chose a remote API? reply Titou325 15 hours agoparentYou are right in the sense we do not provide a local library. We considered the option but would have brought a lot of challenges to accommodate the various runtimes and device capabilities. This may come at a later stage once we have built our own rendering engine though reply Oras 18 hours agoprevThis is definitely a huge market. Are you targeting React developers only? I've successfully used html2pdf in the past, but looking again at their Github, it seems there has been no update in the last three years. I think SOC2 is a must to start engaging with companies. Most PDFs will have sensitive data, and not many companies will feel comfortable sending customer data to a 3rd party platform, so you need security measures and certifications. Good luck! reply Titou325 14 hours agoparentWe actually take HTML as an input to our API converter. The React tooling is mostly to ease the barrier with most frontend codebases, as well as leverage the existing ecosystem of components. It seems that these conversion engines are massive pieces of work that require a lot of upkeep, partly because CSS is a living spec but also because of the sheer number of edge cases. We are already working on SOC2 as this has been a recurring ask, and indeed documents almost always contain PII. reply canterburry 15 hours agoprevMay I ask, why do we still need PDFs? I know they are still popular, I just don't understand why. reply Titou325 15 hours agoparentThere are many reasons behind it, to name a few: files are self-contained(*) and easily portable, can guarantee some security features, the format is easily extended, and the ecosystem is very large. It seems that a better format should exist, but the fact that PDF is the de-facto for portable documents make it unlikely things can change overnight. reply cpr 18 hours agoprevSo are you using PrinceXML for your \"completely separate engine where typography is a first-class citizen\"? reply Titou325 17 hours agoparentYes, we use an API layer on top of PrinceXML with additional polyfills to support modern features. This is a meh solution but it allowed us to iterate quickly and get to work with customers without building a full blown PDF engine firsthand. However building this engine ourselves is the key to reduced latency and overall better feature support. But we need to engage with our users first and see exactly where we should head first :) reply cpr 12 hours agorootparentIsn't PrinceXML pretty much up to date? What's missing? reply jjmaestro 13 hours agoprevJust out of curiosity, as I've seen a few comments also mentioning PrinceXML. Is OneDoc an API, wrapper, etc, on top of PrinceXML? Or is it a completely new rendering engine? Thanks! reply AugusteLef 13 hours agoparentAs of today we are building our solution on top of PrinceXML/DocRaptor which is considered \"to be the best PDF generation API, giving complete control over the documents you need to create\" (cf. another comment). As we started working on this solution less than 2 months ago, building our own renderer was not an option. But once we have validated the idea, we are definitely going to work on our own renderer to have 100% control over the workflow, and also to be able to offer a better pricing model! reply kvakkefly 17 hours agoprevFunny name! The reason I find it funny is I know some people who made Doconce: https://github.com/doconce/doconce :D reply esafak 17 hours agoparentAre they still developing it after the founder's passing? reply nbittich 1 hour agoprevAnother way to create pdfs would be a better title imho. reply dazh 17 hours agoprevGlad to see people building in the PDF space, which as a format is unfortunately both awful and ubiquitous. Are you planning to build any support for programmatically filling out existing PDF forms? That's a huge pain point our product is facing that doesn't seem easy to solve. reply wonger_ 16 hours agoparentI'm facing that same pain point of programmatic PDF filling. I noodled around in the PDF format and learned it's a bit difficult to deal with fonts and formatting. But I think this client-side library works well enough, as a start: https://pdf-lib.js.org/#fill-form I've also heard of one paid API that I forgot but seemed to work well, and this related service https://www.jotform.com/, and I also considered porting some server-side libraries to WASM. One day I'll collect all the libraries and findings in a blog post. Are you looking to programmatically fill any PDF form by detecting the fields? Or are you filling one known PDF template? reply kodt 14 hours agorootparentYears ago I needed to programmatically fill PDFs and used this library to achieve it. Funny it has the same name as what you linked: https://www.pdflib.com/ It is a paid commercial product however. reply nip 14 hours agoparentprevFor programmatic filling of PDFs, have a look at DocSpring: https://docspring.com reply pedro120 17 hours agoparentprevYes, our focus is on programmatic interactions with PDFs, form filling is on our roadmap, alongside programmatic digital signature and many more. reply dazh 17 hours agorootparentAmazing, is there anywhere I can follow along to find out when form filling will be available? reply pedro120 17 hours agorootparentSure! Feel free to join our Discord, we post announcements as soon as new features are released. You can also ask for features, we prioritise these requests with enterprise customer's in our development roadmap. reply azmodeus 14 hours agoparentprevWhat are you looking for in programmatic pdf filling? reply mstijak 15 hours agoprevCongratulations on the launch — it looks fantastic! My company is also developing a similar product. We've chosen to create a visual report designer that enables end-users (non-developers) to create and tweak PDF reports, and integrate with the existing IT infrastructure via the API. Our experience is that users want changes in reports very often and that it's best to allow them do it on their own. https://www.cx-reports.com reply Titou325 14 hours agoparentReally like your approach! We tried to keep things tied to code as much as possible rather than dealing with complex interfaces between changing inputs and outputs. Most legal and tech teams we talked to pointed to the fact that CI/CD would quickly become unbearable when decoupling documents and code implementation. What is your approach on that? reply mstijak 14 hours agorootparentWe offer comprehensive import/export functionality, ensuring seamless transfer of reports between environments. Moreover, workspaces allow you to segregate test and production environments or create unique environments for each client, allowing easy report customization. While reports are simply JSON files, which could theoretically be stored on the file system and checked in, doing so would hurt the flexibility we're trying to achieve. reply gtirloni 16 hours agoprevI wonder what YC expects from such investments (considering the multitude of FOSS solutions in this area). reply Titou325 16 hours agoparentWhile this may sound a bit counterintuitive (maybe?) we actually pivoted to this field based on YC input and discussions they have had with their previous companies. The multitude of FOSS solutions in this area indicates this is a real problem people are willing to spend time on, and yet there is no go-to solution and every team we have talked to selected different tools based on a very specific requirement. This may not mean success, it means that game is not over in the documents field :) reply gtirloni 16 hours agorootparentThanks for the perspective. Indeed, this is an area with real demand. I haven't evaluated YC's recent startups but I trust they do know a bit about what has a better chance in the market. Best of luck :) ps.: As someone with very minimal PDF needs personally and at work, I'd say the beautiful templates are what caught my attention the most. reply axhl 9 hours agoprevneat. are there similar services / libs for generating word docs? this is a recurring problem for many reply AugusteLef 9 hours agoparentWe do not work with docx and never did it myself but: \"DOCX Template API is a tool that allows you to dynamically generate MS Word documents by replacing custom properties using a JSON object that contains your data.\" I assume this is more or less similar to what you are looking for (?) reply axhl 8 hours agorootparentYes - sadly the pricing and docs aren’t the most competitive or intuitive. Hoping someone in the thread has experience with alternatives. Very best of luck with your launch reply breadwinner 17 hours agoprevHow is this better than writing out an HTML file, then using headless chrome to export to PDF, like this: \"C:\\Program Files\\Google\\Chrome\\Application\\chrome.exe\" --headless --disable-gpu --print-to-pdf=C:\\temp\\foo.pdf --no-margins --print-to-pdf-no-header C:\\temp\\test.mhtml reply _puk 17 hours agoparent\"you can already build such a system yourself quite trivially by getting an FTP account, mounting it locally with curlftpfs, and then using SVN or CVS on the mounted filesystem.\" https://news.ycombinator.com/item?id=8863 reply Titou325 17 hours agoparentprevThis brings its own set of challenges. Headers and footers are strictly limited in terms of features, you cannot add footnotes, the notion of page spreads is harder to implement. Then you need to combine that with having a Chrome instance at hand + exposing the needed assets for URL resolution. Definitely not difficult let alone impossible, but not the easiest way to get started :) reply breadwinner 16 hours agorootparentThe easier way costs $0.05 cents per page. Imagine sending an invoice to your customer and the invoice itself costs 5 cents per page! That's prohibitively expensive for many applications. I wouldn't consider any solution that costs more than 1 cent per page. reply Titou325 16 hours agorootparentWe bill per document, so the number of pages wouldn't impact the pricing. A 5 pages invoice would come at 1 cent per page. However, it seems that each and every company has different needs and the pricing may or may not make sense for them. There are alternative billing options that we are considering but we want to keep it easy to grasp rather than go into billing kilobytes or ms of execution. We would be more than happy to discuss use cases and see what can work for each company :) reply acoyfellow 16 hours agoprevReminds me of BrewPDF.com reply BrandiATMuhkuh 16 hours agoprevCongrats on the launch! What's the main advantage over pspdfkit? reply Titou325 16 hours agoparentIt is similar to pspdfkit. We add an abstraction layer over the HTML and assets hosting to make it easier to use without having to think too hard about security and serving assets. We also hope to keep the focus on the PDF generation part rather than expanding super-horizontal style to provide all imaginable PDF tools at the expense that none is really good. reply ramon156 17 hours agoprevCan we not have an alternative to PDFs? I get that they're more standardized but why would everyone let adobe have the hammer for a file type that's so important reply Titou325 17 hours agoparentWe quite agree on this - but getting a new alternative out will require a significant critical mass before it can be of any interest. While PDF has its challenges, it remains a light portable format and its security features make it a good fit for binding documents. The ecosystem, although it is dominated by Adobe, also includes other major players and existing integrations. The way we look at it is PDFs allows embedding of other files and metadata. It is easy to provide a platform where we can enrich PDFs to display different contents than the one in the PDF itself. If this gets interesting enough, we can then phase out the PDF in the first place. But this is a long way ahead. reply rapatel0 17 hours agoparentprevPDF is an incredibly (stupidly) extensible format. There are tons of government forms that (sadly) bake in complex workflows into PDF forms. Given that the whole world has been running on PDFs for decades it's makes more sense to leverage the existing infrastructure and move it towards something more functional over time. Introducing a new format will just lead to another format the achieves 0.5% marketshare and then is abandoned after a few years. Microsoft basically forcing people to use XPS in windows (>70% market share of computing) still wasn't able to achieve meaningful usage or change. I expect that PDFs will not go away for 20 years at least, but who knows reply breadwinner 17 hours agoparentprevPDF is an open format in the sense that you don't need to pay Adobe a license fee for generating PDFs, or for reading and rendering PDFs. The format is fully documented, although the specification is controlled by Adobe. reply nradov 17 hours agoparentprevFor supply chain workflows the ASC X12 Electronic Data Interchange (EDI) industry standard works much better than PDFs. Unfortunately, despite being around for decades in has only been adopted by forward thinking organizations such as Walmart. Most smaller companies and their vendors still haven't implemented EDI. https://developer.walmart.com/home/us-edi/ reply calvinmorrison 17 hours agorootparentInsanity. EDI is the only place where people are regularly still paying for message by the kilobyte, where unsecured FTP over the open internet is still a norm, and where entire cottage industries exist to support AVOIDING using EDI. Source: I work in EDI. it's a pain in the rump. Also, EDI is really only good for things like PO's, shipping notices, invoices, sales orders, etc. reply ochrist 15 hours agorootparentYou don't have to pay for message by the kilobyte. This is only true if you use an external vendor for the conversion or use a VAN for transmission: https://en.wikipedia.org/wiki/Value-added_network Source: Worked in EDI for a few years reply calvinmorrison 14 hours agorootparentyou absoultely don't need to use a VAN, yet a LOT of people do. Even when they're not using a VAN for comms, they might pay a VAN to host their FTP. The whole thing is backasswards. reply oldandboring 16 hours agorootparentprev> Also, EDI is really only good for things like PO's, shipping notices, invoices, sales orders, etc. Don't forget health insurance claims, eligibility & benefits, and prior auth requests! reply ochrist 15 hours agorootparentEDI is used in a lot of situations for machine-to-machine communications, but outside USA I believe EDIFACT is much more used (X12 is mostly used in USA). Today many EDIFACT documents have been converted to ebXML: https://en.wikipedia.org/wiki/EbXML Source: Worked in EDI for a few years reply nvr219 17 hours agoparentprevYeah let's give XPS another go. reply devsda 17 hours agorootparentGiving credit where it's due, I can appreciate Microsoft for introducing XPS as an alternative to pdf. There was a time, when not every software had \"export to pdf\". So, having a \"print to pdf\" meant installing (often pirated) Adobe Acrobat or installing a sketchy free(ware) printdriver software downloaded from sourceforge. MS adding xps print driver to windows enabled sharing docs consistently (within windows ecosystem) without resorting to hacks. I don't know why it didn't catch up. May be it was the general mistrust of anything MS, it arrived too late or it was something else. reply AugusteLef 16 hours agorootparentIndeed, we need to give credit to MS for what they did. However, it didn't catch up as you mentioned, maybe due to timing, skepticism toward MS, or the complexity of moving from Adobe to MS for PDF management. I will dig a bit into it and come back later if I find anything interesting. reply baggy_trough 14 hours agoprevThis is definitely a somewhat painful process. I have done it with puppeteer / chromium on Debian, and it works very well after the headache of figuring it out. Having to pay 50 cents per PDF and deal with a 3rd party vendor would not provide value for our needs. reply ketanmaheshwari 17 hours agoprevI just wanted to add that if you want to convert plaintext files to pdf, vim has a builtin feature to do so: vim filename.txt -c \"hardcopy > filename.psq\" && ps2pdf filename.ps #convert ps to pdf reply airbreather 17 hours agoprevare you doing this with pdfmarks? reply AugusteLef 17 hours agoparentNo, we don't currently do that. However, we are considering adding metadata to PDFs, and using pdfmark could be very helpful! reply cratermoon 17 hours agoprevThe problem with using Tailwind is that I can't just say Some Heading. As noted in the Tailwind documents \"All heading elements are completely unstyled by default, and have the same font-size and font-weight as normal text.\"[1] Most of the time when I'm writing HTML I want a set of default styles for the most common elements, It's tedious and error-prone to have to specify a class every single time. 1 https://tailwindcss.com/docs/preflight reply Titou325 17 hours agoparentMakes total sense. There is no real requirement to use Tailwind to create the PDFs, we just have grown accustomed to Tailwind :) If you don't use thetag, the browser defaults are used to generate the PDF. reply admissionsguy 16 hours agoprevYou cannot make this up, generating PDFs is now an enterprise product. reply AugusteLef 15 hours agoparentEditors such as Overleaf, and those offered by MS and Adobe, have been around for a long time. Recently, companies like Pandadoc and Docusign have started offering services around PDFs (generation or other aspects of their lifecycle). It might seem odd, given our long history with PDFs, but I believe there's still much to be done with these documents. They're everywhere—invoices, tickets, reports, etc.—yet the technology for generating and managing them hasn't evolved much in years. Our approach is to apply the same modern technologies used for web design to document design. reply dmazzoni 16 hours agoparentprevWhat do you mean \"now\"? It has been for years. It's a huge business. reply admissionsguy 15 hours agorootparentWhen I was first hired 15 years ago my first task was to create a PDF report. It was easy back then in PHP+fPDF. Two years ago I was hired to work on a Heroku-hosted NodeJS app. I was surprised to find that generating a PDF turned out to be substantially more difficult task, requiring running a browser emulator or connecting to an external service. And now, seeing PDF generation as a premium pay-as-you-go product is just too much. reply AugusteLef 15 hours agorootparentMakes sense. Actually, if you keep the layout/content very simple, aren't constrained by throughput, and don't need to integrate dynamic data or other similar processes, then simple FOSS could indeed get the job done! That's exactly why we developed the open-source library react-print-pdf reply kodt 14 hours agorootparentprevThere have long been free HTML-PDF options around, but at the same time commercial products that offered more features. Some examples are opening an existing PDF and modifying it, adding to it, merging PDFs, or filling PDF forms. reply patrick4urcloud 17 hours agoprev [–] very nice ! reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Onedoc Labs, founded by ex-employees from major corporations, developed an open-source library, react-print-pdf, for automated PDF document creation.",
      "The platform provides an API, Node.js SDK, and layout control for document design, along with analytics, offering pricing based on document generation and volume discounts.",
      "Users can explore the tool on the website without registration, as the company eagerly shares the product with the Hacker News community for input."
    ],
    "commentSummary": [
      "The conversation delves into tools such as Onedoc, react-pdf, and DocRaptor for PDF generation, covering pricing, layout, and accessibility.",
      "Users explore considerations for form filling, metadata, CSS styling, and XPS, emphasizing the need for modern and user-friendly PDF solutions.",
      "The discussion underscores the consistent demand for reliable PDF generation tools and the enduring popularity of PDFs as a widely accepted format."
    ],
    "points": 229,
    "commentCount": 145,
    "retryCount": 0,
    "time": 1710168776
  },
  {
    "id": 39675807,
    "title": "Trangram: Free Motion Graphics and SVG Animator",
    "originLink": "https://www.trangram.com",
    "originBody": "Trangram is a free one-stop platform to create, and share motion graphics and svg animations with a free built-in powerful editor which is a fusion of Adobe Illustrator and animation tools.",
    "commentLink": "https://news.ycombinator.com/item?id=39675807",
    "commentBody": "I made a free animator. Think Adobe Illustrator but for animation (trangram.com)224 points by trangram 6 hours agohidepastfavorite59 comments Trangram is a free one-stop platform to create, and share motion graphics and svg animations with a free built-in powerful editor which is a fusion of Adobe Illustrator and animation tools. mittermayr 21 minutes agoOne quick comment: I was threatened and on the brink of being sued by one of Facebook's bigger law firms, because my domain was called \"***gram.com\" — I went through a couple of calls with them, and they confirmed that the only issue was the use of the word \"gram\". I tried to stick to my guns, knowing they had nothing (legally) on me, but eventually had to give in once they told me to research the ongoing cases they have with all the other domain owners (which you can all find online). Some of which have been going through lawyer-warfare for over three years (at the time). They basically said, if I have the power to sustain this, they would sue and we'll clear this in court, if I don't want to go through this, I should back off and use a different domain. So, while I hate saying this, be warned, I was warned and ignored it, and eventually, no matter how tall you feel, that call may come quicker than you think. Keep a second domain around ready to go, make sure you can rebrand. Just my word of (super sad) advice. Looks great though! reply notsahil 4 minutes agoparentAny reference to it? I couldn't find anything about Facebook taking action on a domain ending with gram. Was your domain similar to instagram? Because telegram is still out there. reply Chazprime 0 minutes agorootparentThis is one of them that I remember from 2016: https://www.theguardian.com/media-network/2016/dec/02/battle... reply junto 3 hours agoprevBack in the day I used to do quite a lot of Macromedia Flash work. It’s uncannily similar but a modern take. I’ve often wondered why no one has come up with a new product in this space. I think the long term demise of Flash has put off anyone even trying. There are so many great uses for animations on the web, even if we don’t need full blown user interfaces of them and intro screens like we did back in 2002. Great job! reply Closi 2 hours agoparentTotally agree! The major feature missing compared to Flash would be library and component support - i.e. the ability to create reusable animated graphics that you can drag onto the canvas (with infinite nesting). i.e. you can animate a bird with flapping wings, then drag 3 copies onto your sky. reply lukan 1 hour agorootparentIn flash you could also select any element and add a onclick handler. Bam, a button. Or play a different animation of a subelement on mouseover e.g. bird.flapWings() And quite some other things .. This is not a flash replacement and also does not aim to be one. reply Closi 1 hour agorootparentTo clarify, I mainly meant the major animation feature that let you do more complex animations compared to this tool. reply albert_e 1 hour agorootparentprevYes I absolutely loved that. I wish Microsoft PowerPoint built some of that so we could use it for light weight animations and story telling. reply jonplackett 41 minutes agoparentprevAbsolutely agree. The interface was what made flash so good because it let non-coders make things. So sad that Adobe were unwilling / unable to just make it output html5 instead of swf. But Adobe so where software goes to die so… reply ramraj07 2 hours agoparentprevIsn’t adobe animate just flash ? reply lukan 2 hours agorootparentNo, since flash is no longer around. Meaning you can still create all the animations and games, but then you will have to try to port it to js and canvas (via easeljs). And that did not work very nicely last time I tried it. reply eloeffler 2 hours agorootparentThere is also ruffle which seems to be doing a very good job at reviving Flash animations. The Internet Archive is using it to preserve the profound cultural heritage of early millenial flash animations. https://www.theverge.com/2020/11/19/21578616/internet-archiv... reply illwrks 20 minutes agoprevI’ve not looked at the tool, but are animations driven by CSS, and if so do you embed the CSS into the SVG? Where I work we’re looking at SVG animation tools for some graphics and the majority appear to be lottie-like tools dependent on JavaScript and other plugins which we can’t use. reply jarek-foksa 8 minutes agoparentFor pure SVG-based animations you should take a look at Boxy SVG: https://boxy-svg.com/blog/21 reply lukko 15 minutes agoprevWell done for shipping! I'm really impressed but the tool - it's intuitive and like a very light-weight After Effects. I think just re-designing the landing page would make a huge difference, very quickly. I think currently it lets down what is a well-crafted design / animation tool. Get rid of the white and teal, and the default system fonts. The looom site is a lovely example (https://www.iorama.studio/looom). reply Mackser 23 minutes agoprevThis looks really well done! I was able to create a simple animation easily even though I don't know anything about animation software. By the way, your \"Features\" page is really well made. I would showcase those features on the homepage. I didn't find the list of features until I clicked the link in one of your comments on HN. You can also make your project more appealing on the homepage by including a looping demo animation (to see what's possible) and a screenshot of the tool. Best of luck with the project! reply JSavageOne 3 hours agoprevThis looks awesome. Some suggestions though: - Would be awesome to be able to open an existing animation (eg. like any of the ones showcased). It's a built overwhelming for a noobie opening up to a blank editor page. - Could also consider putting a tutorial video Anyways I'll have to play around with this. reply scosman 2 hours agoprevOpened it, made an animation in 10 seconds. On mobile. With zero experience with animation software. Well done Any chance of Lottie export? I think I would use this if there was. reply santiagobasulto 1 hour agoprevThis is great. I have actually written about how this is a great opportunity: an \"excalidraw for animations\". Try to make the people land immediately on the editor instead of the landing page. And focus on the sharing aspects so it gets viralized. Public you're aiming to? reply t09i209ba893 2 hours agoprevVery nice. One question: I see the submission mentions SVG animation but I don't see any way of exporting as SVG. Did I miss something, or was this just meant as a plan for the future? reply weinzierl 41 minutes agoprevA good SVG animation tool is dearly missing. I wish Inkscape could do it or there was an Inkscape-like animation tool, that does not hide but rather embraces the underlying SVG code. Can you tell how you approach to this is? Can you just export SVG animations or even import and edit without destroying the original structure? reply shubhamjain 4 hours agoprevThis is awesome. Great job! I love the simplicity of the whole thing. I was able to figure out the tool in a minute. Compare that to professional software, which can have a scary complexity. I think it'd be a great educational tool. Kids would love to experiment with it. reply trangram 3 hours agoparentYour words mean a lot to me. In designing and implementing Trangram, ease-of-use ranks high on my list of priorities, alongside functionality and safety. Just as you mentioned, one of my primary motivations was to facilitate the creation of educational materials, which I was particularly inspired by experiences while watching The Power Of A Mathematical Picture https://www.youtube.com/watch?v=6vnMT70HOxc&list=PLOxODW9vlV... I've designed Trangram as more than just an editor; it's a platform intended to democratize content sharing. Every individual should have the opportunity to disseminate valuable content to others, whether through a simple link or embedded HTML code (allowing for easy integration on personal blogs or websites). My aspiration is for Trangram to evolve into a secure and enriching environment for all, including children, fostering a space where learning flourishes. reply piva00 1 hour agorootparentGood luck on your mission! As a kid/teenager who grew up creating animations and games on Flash (from v2 to v5) I've always felt that since then only the people really into animation have been able to play with it, the tools are not approachable, you need to learn a lot before you can do the basics which were quite straightforward with Flash. I really hope Trangram gets some traction so you have the time and resources to further develop it, it's a tool that has potential to make the world a nicer place :) reply meekaaku 1 hour agoprevLooks very nice. Is this on webgl? Speaking as a non-professional JS programmer, where can I read about how to architect a program like this where you select a tool, then that tool lets you draw (or do something) on canvas. Another tool, has a different behavior to mouse click/movement while initial placement and later editing of the nodes. Architect it in such a way that its not a long list of if statements for state management. The recently shared Eloquent Javascript[0] had chapter 19 making a pixel editor. That seems to be a good approach, but then I wouldn't know any better. Any recommended reading or small size sourcecode that one could read and learn? [0] https://eloquentjavascript.net/19_paint.html reply tamimio 1 hour agoprevSlightly off topic: What are the tools for making animations in the webpage, in html/canvas format not embedded videos? Sometimes I see some sites with nifty animations that animate while scrolling too and I wonder how it’s done. reply imp0cat 45 minutes agoparentDo you mean scrolljacking? reply wfme 1 hour agoprevLooks great and works well. Some small suggestions: - Please add some keyboard shortcuts for common actions, i.e., cmd/ctrl + z to undo (+ shift to redo), delete/backspace to delete, cmd/ctrl + d to duplicate, cmd/ctrl + a to select all. - Increase rotate cursor affordance - it's currently relatively tiny. reply HanClinto 4 hours agoprevReally impressive work!! Ever since the demise of Flash I've felt a strong lack of such animation tools. Very nicely done! reply trangram 3 hours agoparentThank you for the kind words. I hope you guys will find it useful. reply neurostimulant 1 hour agoprevThis is great! If you add lottie export, I could see this tool become popular among web and mobile app devs. reply abmmgb 2 hours agoprevJust tried it on my mobile, responsive and well designed. I like how neat and uncluttered the site is. The editor feels intuitive and easy to use. Like the demos on the homepage. This must have been a massive project very impressive!! reply kylegalbraith 1 hour agoprevThis is super cool and was very easy to get going. Missing keyboard shortcuts is pretty annoying and would make this even simpler. I hopped in hoping it was like a Google Drawing, which it's pretty close, but the lack of keyboard shortcuts for copying/pasting, selecting all, etc -- makes it hard to move fast. reply cududa 54 minutes agoparentThis is a ridiculously complex application dude. I’m sure they’ll get to keyboard shortcuts when they get to it reply tupolef 2 hours agoprevGreat job! It took me 5 min to redo a animated logo that took me a few hours 3 years ago with Adobe. I will use it. reply quaestio 3 hours agoprevIt would be cool to be able to use the middle mouse button to pan and zoom the canvas, like most CAD software allows. Can the units of the artboard be set - for example to mm? reply maxiwer 2 hours agoprevI'm elated that you used Angular. Because I've been b*tching about no one uses it :D reply est 55 minutes agoprevneed a free drawing tool instead of pen/line reply dkarras 2 hours agoprevThis is cool, and \"Adobe Illustrator for animation\" is perhaps Adobe After Effects. reply ramraj07 2 hours agoparentNo, it’s adobe flash or whatever the hell they call it nowadays. reply verdverm 1 hour agoprevIs audio possible, and if not, plans for it? reply justinclift 3 hours agoprevJust opened the editor, switched to line mode, and drew a line. It seems to have automatically switched to selection mode, rather than keeping in the mode I deliberately switched to (line mode). That's unexpected, and probably not what most users would expect. Any chance of getting that changed, so the mode only changes when the user does it? --- Also, is it Open Source? :) reply lukan 2 hours agoparent\"Also, is it Open Source?\" No, also: \"We reserve the right to modify this Agreement or its terms related to the Services at any time at our discretion.\" So it is currently free of charge, but may one day contain tracking without warning, or paid membership? The buisness plan (how to make money) is not clear to me. And this would be a major blocker for me to use if for anything serious, when one day I might loose access to my data, unless I pay an unspecified amount. You also cannot export your work to a common vector format, so will be bound by this website forever. If you are looking for something Open Source, there is the wick editor: https://www.wickeditor.com/ (but developement is on hold) reply justinclift 2 hours agorootparentYeah. Not being FOSS is a blocker from my point of view as well. Already been through that exercise with Adobe and the Flash ecosystem. No interest at all in a repeat. reply rekado 1 hour agorootparentHere is a free software animator: https://www.synfig.org/ Development is kinda slow, but this also means that it still works just as well as when I made a little short movie 15 years ago. reply lukan 2 hours agorootparentprevYeah, me too. Why, just why didn't Adobe open source (and vastly improve in terms of security) the flash player and rather let it die. It was such an awesome ecosystem. reply 5- 2 hours agoprevvery nice! unless i'm missing something, you don't support export to any vector and/or animated formats besides your own? reply asteroidz 30 minutes agoparentCertainly a bit disappointing that Lottie or SVG format export doesn't exist. Hopefully a work in progress? reply KingOfCoders 4 hours agoprevLike Macromedia Director? I remember creating tcp ip network flow animations with that one. reply trangram 3 hours agoparentI'm not familiar with Macromedia Director. So I did a quick search and find that both tools can do a basic positions, scale, rotation and color animation just by creating shapes and keyframes within few clicks and draggings. Trangram, however, is more than that, it supports (1) morphing between two shapes (actually any shapes you can create or imagine) without extra efforts (2) motion path, which means you can define an arbitrary route and let your object move along that path easily (3) parent link, which means linked object can move, rotate and scale together There're many more exciting features I don't mention here, you can check them from Trangram about page below. https://www.trangram.com/about Thanks so much for your comment and letting me know about Macromedia Director. reply Brajeshwar 3 hours agorootparentWell, the power of Macromedia Director was not those animations — but, a scripting language called Lingo[1]. In 1999, I did a trial Kiosk for the State Bank of India[2] to be installed at a popular landmark in Bombay, called the Wockhardt Hospitals[3]. I used Macromedia Director to let users navigate for information, akin to an ATM but for info. A partnership led to including quite a few animations of Cardiology and similar info-animation. The ability of Macromedia Director and then Flash to see what you are doing immediately was what draw me into the world of scripted visuals. 1. https://en.wikipedia.org/wiki/Lingo_(programming_language) 2. https://en.wikipedia.org/wiki/State_Bank_of_India 3. https://en.wikipedia.org/wiki/Wockhardt_Hospitals reply mahomedalid 4 hours agoparentprevI came here to say the exact same thing, it remind me to Macromedia Flash animations. reply seabrookmx 3 hours agorootparentIt still exists. Macromedia Flash became Adobe Flash became Adobe Animate: https://www.adobe.com/ca/products/animate.html Nice to see some free alternatives popping up. reply HanClinto 4 hours agorootparentprevYes, very much reminds me of that -- this looks great! reply barlog 3 hours agoprevwhat an awesome. I made it in 5 seconds and published it, then it was embarrassed to be listed on the top page. Let's publish it unlisted! user/dai reply begueradj 2 hours agoprevGreat work reply exodust 2 hours agoprevAll the examples say \"post unavailable\" for me in Firefox desktop. (edit: ESR browser version slightly out of date, worked after updating. I wonder what new browser feature is being used?) https://www.trangram.com/post/65bdd64d51a9f658905fa662 reply self_awareness 1 hour agoprev [–] Looks good, although I'm a little bit sad about this web application trend. I mean I get the ease of deployment -- just open the webpage and use it. But this application is just temporary; it won't exist after 15 years; it will change into something else, or it will cease to exist. It will be impossible to use older version of it. It won't be possible to preserve it, emulate it. If the server is off, the application dies. In contrast, it's still possible to install the original Lotus 1-2-3, an application from 1983. The amount of applications that I've seen which aren't available anymore (even to be seen) is staggering. I see it as a waste of resources. Webapps have their advantages, but I see their core trait as ephemeral -- that might be sometimes a good thing, but I see it as a disadvantage. reply mittermayr 17 minutes agoparent [–] I agree, this is also going to get much worse. I recently dipped back into some of the younger SaaS communities (Twitter is particularly crazy) and it's just madness. They make something, (ab)use anything they can find to push this in front of people and promote it, and if it doesn't throw cash within the minute, it will be left for dead. I'd even argue that some of these tools have a lifetime of a month or less. With AI, this is typically the moment when they run out of their first OpenAI top up and start to abandon. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Trangram is a free platform enabling users to design and distribute motion graphics and SVG animations through a robust editor, reminiscent of Adobe Illustrator and animation software."
    ],
    "commentSummary": [
      "Trangram, a free platform for creating motion graphics and SVG animations, is under legal threats regarding its domain name, sparking discussions on its resemblance to Flash and ideas for enhancements.",
      "Users admire Trangram for its user-friendly interface and educational value, but they express worries about lacking features and the long-term viability of web-based applications."
    ],
    "points": 224,
    "commentCount": 59,
    "retryCount": 0,
    "time": 1710213351
  },
  {
    "id": 39670035,
    "title": "Speedometer 3.0: New Browser Benchmark for Web App Responsiveness",
    "originLink": "https://browserbench.org/announcements/speedometer3/",
    "originBody": "Announcing Speedometer 3.0: A Shared Browser Benchmark for Web Application Responsiveness by contributors from Apple, Google, Microsoft, and Mozilla March 11, 2024 Since the initial version of the Speedometer benchmark was released in 2014 by the WebKit team, it has become a key tool for browser engines to drive performance optimizations as users and developers continue to demand richer and smoother experiences online. We’re proud to release Speedometer 3.0 today as a collaborative effort between the three major browser engines: Blink, Gecko, and WebKit. Like previous releases (Speedometer 2 in 2018 and Speedometer 1 in 2014), it’s designed to measure web application responsiveness by simulating user interactions on real web pages. Today’s release of Speedometer 3.0 marks a major step forward in web browser performance testing: it introduces a better way of measuring performance and a more representative set of tests that reflect the modern Web. A New Governance Model This is the first time the Speedometer benchmark, or any major browser benchmark, has been developed through a cross-industry collaboration supported by each major browser engine: Blink/V8, Gecko/SpiderMonkey, and WebKit/JavaScriptCore. It’s been developed under a new governance model, driven by consensus, and is hosted in a shared repository that’s open to contribution. This new structure involves a lot of collective effort: discussions, research, debates, decisions, and hundreds of PRs since we announced the project in December 2022. A Broader Range of User Experiences Speedometer 3 adds many new tests. We started designing this new benchmark by identifying some key scenarios and user interactions that we felt were important for browsers to optimize. In particular, we added new tests that simulate rendering canvas and SVG charts (React Stockcharts, Chart.js, Perf Dashboard, and Observable Plot), code editing (CodeMirror), WYSIWYG editing (TipTap), and reading news sites (Next.js and Nuxt.js). We’ve also improved the TodoMVC tests: updating the code to adapt to the most common versions of the most popular frameworks based on data from the the HTTP Archive. The following frameworks and libraries are included: Angular, Backbone, jQuery, Lit, Preact, React, React+Redux, Svelte, and Vue; along with vanilla JavaScript implementations targeting ES5 and ES6, and a Web Components version. We also introduced more complex versions of these tests which are embedded into a bigger DOM tree with many complex CSS rules that more closely emulate the page weight and structure from popular webapps today. Taken together these exercise a more broad and representative cross section of the engine, providing new opportunities to optimize JS, Layout, CSS, Graphics, and DOM APIs in order to improve user experience on the Web. Take a look at this page for more details about the tests themselves. Improvements to the Test Runner The test runner itself in Speedometer 3 has been improved to measure more of the work the browser does in response to user actions, such as painting and asynchronous tasks. Speedometer 2.0 measured the time to run a test script synchronously as \"sync\" time, as well as any additional work before a 0 second timer fires as \"async\" time. However, this missed some work browser engines have to do to update the rendering of a web page. In Speedometer 3.0, we are able to measure this previously-missing rendering work, which creates more opportunities to optimize real-world content. To do this, we measure a test script within a requestAnimationFrame callback as \"sync\" time, and a 0 second timer scheduled in a second requestAnimationFrame fires as \"async\" time. This async time is guaranteed to include work from timers in the test itself, as well as page rendering by the browser engine. These changes greatly improve the accuracy of the benchmark, and translate into real-world improvements for users as engines optimize this previously-missing work. There are some more behind the scenes improvements as well. There’s improved developer tooling so browser engineers can better understand results, profile, and customize the test. We redesigned the test runner architecture to make it easier to write and maintain complex test cases. And there are many code quality improvements and migrations to modern features that weren’t broadly available when Speedometer 2.0 was released, such as native promises, async / await, classes, and modules. Improving Web Performance The primary goal of Speedometer 3 is to reflect the real-world Web as much as possible, so that users benefit when a browser improves its score on the benchmark. It has already had some success at this before publicly launching, with core optimizations in each major engine throughout the last year turning into responsiveness improvements for users across the Web.",
    "commentLink": "https://news.ycombinator.com/item?id=39670035",
    "commentBody": "Speedometer 3.0: A shared browser benchmark for web application responsiveness (browserbench.org)208 points by cpeterso 17 hours agohidepastfavorite146 comments awesomekling 17 hours agoThis is fantastic! Speedometer 1.0 was a breath of fresh air, and 2.0 was a much-needed refresh, but it's really been showing its age in recent years. 3.0 looks like a solid upgrade with many new kinds of sub-tests, contemporary frameworks, etc. I'm looking forward to sucking at this, and then slowly and systematically improving. :^) reply bgrins 8 hours agoparentGood luck! FYI, there's a hidden developer menu that's handy for browser developers to change the number of iterations, select specific tests, etc: https://browserbench.org/Speedometer3.0/?developerMode, and ?startAutomatically avoids needing to click the button to start the tests. reply aeyes 13 hours agoparentprevI'd love some videos seeing you try to improve the score or trying to get it to run :^) reply nateb2022 7 hours agoprevOn my M2 MBA (16GB RAM, 256GB SSD): REGULAR MODE Safari 17.3.1: 27.1 Safari 17.3.1 (Private): 26.2 Firefox 123.0.1 (uBlock Disabled, Enhanced Tracking Protection Disabled): 29.5 Firefox 123.0.1 (w/ uBlock Enabled, ETP Enabled): 27.1 LOW POWER MODE Safari 17.3.1: 17.37 Safari 17.3.1 (Private): 16.99 Firefox 123.0.1 (uBlock Disabled, Enhanced Tracking Protection Disabled): 20.0 Firefox 123.0.1 (w/ uBlock Enabled, ETP Enabled): 17.8 Safari has 0 extensions installed, and Firefox 0 extensions installed besides uBlock Origin. Benchmarks were run with each browser as the sole application open and plugged in to a power supply. reply internetter 6 hours agoparentFor a more diverse view, as seen from my M2 MBA 8gb RAM: Speedometer 3.0: Arc: 22.6, Orion: 19.6, Safari: 19.0, Chrome: 22.6, Firefox: 20.7 Speedometer 2.1: Arc: 408, Orion: 467, Safari: 481, Chrome: 404, Firefox: 478 No changes beyond stock browser. No extensions beyond stock install. Battery. reply wafriedemann 2 hours agoparentprevI get 31 in Safari, 20 in Firefox (M2 Mac Mini base model, Safari no extensions, Firefox with Ublock and some about:config adjustments) reply ChrisArchitect 16 hours agoprevRelated annoucement post: Improving Performance in Firefox and Across the Web with Speedometer 3 https://hacks.mozilla.org/2024/03/improving-performance-in-f... reply om2 12 hours agoparentAnother related announcement, with a bit more detail on specifics of the benchmark changes (and some history of Speedometer): https://webkit.org/blog/15131/speedometer-3-0-the-best-way-y... reply Vinnl 17 hours agoprevPretty neat: > This is the first time the Speedometer benchmark, or any major browser benchmark, has been developed through a cross-industry collaboration supported by each major browser engine: Blink/V8, Gecko/SpiderMonkey, and WebKit/JavaScriptCore. reply TylerE 17 hours agoparentnext [6 more] [flagged] rockdoe 17 hours agorootparentAre there any small engines that have enough of the web implemented that they can run it? reply aryonoco 6 hours agorootparentThe only other (semi) alive browser engine today is Servo, originally by Mozilla (and the reason Rust was created for), which is these days a Linux Foundation project funded by Igalia. There are small web engines anymore. Every other one, from khtml to presto to trident, is dead. reply TylerE 16 hours agorootparentprevOpera? reply abhinavk 13 hours agorootparentOpera uses Chromium so it's covered under Blink/V8. reply None4U 16 hours agorootparentprevPresto hasn't been used for years reply CharlesW 16 hours agoprevTry it here: https://browserbench.org/Speedometer3.0/ Very unscientific results using a Mac Studio - Chrome: 20.4, Safari: 17.9, Firefox: 20.1. Safari on an iPhone 13 Pro Max - 16.5. reply ksec 9 hours agoparentOn my Early 2015 MBP Safari 17.4. : 5.52 Chrome 122 : 6.25 Firefox 123 : 7.26 The results pretty much confirms my general feeling about how the browser behaves on my machine as well. Where Firefox being the fastest. And 22.5 on my iPhone 14 on iOS 17.4 That is my smartphone is ~4x faster than my laptop. reply gilgoomesh 8 hours agoparentprevI'm getting 27.7 in Safari 17.4 on an M1 Pro MacBook. I'm puzzled how you got so low on a Mac Studio. reply andreasley 8 hours agorootparentAnother Mac Studio M1 with Safari 17.4: 22.7 with Content Blocker 28.7 without Content Blocker reply CharlesW 7 hours agorootparentThat was it, thanks! I thought I'd controlled for this by using a Private Window, but I'm getting 25.9 with extensions disabled (compared to 17.9 in my original post). reply om2 15 hours agoparentprevWhat Safari version are you using? For me, with 17.4, Safari is ahead of Chrome and Firefox, though it is close if you use dev channel. reply CharlesW 15 hours agorootparentmacOS 14.4 for the Mac Studio tests, and iOS 17.4 for the Safari-on-iOS test. reply enra 5 hours agoparentprevPC i7 13700k (chrome) - 29.8 iPhone 15 Pro (safari) - 23.1 MacBook Air M1 (chrome) - 25.9 MacBook Air M1 (safari) - 18.0 reply vikingtoby 15 hours agoparentprevnext [13 more] [flagged] CharlesW 15 hours agorootparentIs it though? https://wpt.fyi/interop-2024 reply kouteiheika 13 hours agorootparent> Is it though? In my experience it's the buggiest browser out of the big three, and is often missing basic features like e.g.: https://caniuse.com/?search=opus Supported in Firefox for *12 years* now, in Chrome for 10, still no support in Safari. They only \"support\" Opus audio in their special snowflake '.caf' container, which is super buggy and the last time I checked no open source program could even generate Opus '.caf' files that could be played by Safari on all Apple platforms. I ended up writing a custom converter which takes a standard '.opus' file and remuxes it on-the-fly (I only store '.opus' files on my server) into Safari-compatible '.caf' files, taking special care to massage it so that it avoids all of their demuxer/decoder bugs. You shouldn't have to do this to have cross-browser high quality audio! reply CharlesW 13 hours agorootparent> You shouldn't have to do this to have cross-browser high quality audio! You don't, because HE-AACv2 is as universal as MP3 and better than Opus at low bitrates. That said, Safari for macOS and iOS plays all the examples at https://opus-codec.org/examples/ except the last. reply kouteiheika 12 hours agorootparent> That said, Safari for macOS and iOS plays all the examples at https://opus-codec.org/examples/ except the last. That's because none of those samples are Opus files, except the last one. It even says so on the page. > You don't, because HE-AACv2 is as universal as MP3 and better than Opus at low bitrates. No. I did evaluate it before picking Opus. It only beats Opus at very low bitrates, and open source encoders for AAC suck. reply CharlesW 12 hours agorootparent> That's because none of those samples are Opus files, except the last one. Ooof, I didn't even imagine that the official examples were WAV files. Here's an Opus audio file that plays fine in Safari on macOS and iOS: https://kur-static.biblica.com/audio/GEN_001.webm (Note: I have no idea what this content is, but could not find any English Opus content in the wild.) > …and open source encoders for AAC suck. Yeah, the disparity is a real bummer. reply kouteiheika 23 minutes agorootparent> Here's an Opus audio file that plays fine in Safari on macOS and iOS Yeah, that has Opus packed into a Matroska container (which people usually use only for videos and not pure audio). I suppose that's another good way of getting around the problem! reply no_way 12 hours agorootparentprevJust go to home page https://wpt.fyi/ see chart \"Browser-specific failures are the number of WPT tests which fail in exactly one browser.\" Safari leads by a longshot with over 3800 tests failing only in Safari. Firefox has 1700 and Chrome less which kinda correlates to my own personal development experience. reply agust 12 hours agorootparentprevInterop is only a tiny subset of the entire suite of WPT tests, and it only contains tests that all vendors agreed upon, so no browser will look bad in Interop. If you look at the full WPT test suite [1], you'll see that Safari is by far the one failing the biggest number of tests, i.e. the most buggy browser. The Safari team likes to use Interop to trick people into thinking Safari is as good as the others. It's just a PR play. [1] https://wpt.fyi/results/?label=experimental&label=master&ali... reply CharlesW 11 hours agorootparentFor a less biased result, use Stable: https://wpt.fyi/results/?label=master&label=stable&aligned > If you look at the full WPT test suite [1], you'll see that Safari is by far the one failing the biggest number of tests, i.e. the most buggy browser. In Safari's case, most WPT test fails mean \"hasn't been implemented yet\". > Interop is only a tiny subset of the entire suite of WPT tests, and it only contains tests that all vendors agreed upon… Exactly. If you're happy building \"Works with Chrome\" web apps, Safari is not for you. reply agust 10 hours agorootparent\"Browser-specific failures are the number of WPT tests which fail in exactly one browser.\" From wpt.fyi In other terms, WPT test failures for Safari means Safari has bugs or unsupported features that both Firefox and Chrome do not have. As for Interop, it focuses on a specific, very limited areas, like \"scrolling\" or \"subgrid\" and is in no way representative of the overall feature set of a browser. So no, contrary to what you're implying, it's not that Chrome is too advanced, or doing too much, it's really Safari that is buggy and lagging behind both Chrome and Firefox (by a lot). reply CharlesW 10 hours agorootparent> In other terms, WPT test failures for Safari means Safari has bugs or unsupported features that both Firefox and Chrome do not have. Yep! Safari is not the browser for people who need cutting-edge features, especially not for ones still at the proposal stage. reply baggy_trough 15 hours agorootparentprev> Nevertheless, it’s still a garbage browser. That seems like quite an absurd statement. Why do you think so? reply riquito 16 hours agoprevLink to the actual test https://www.browserbench.org/Speedometer3.0/ reply ornornor 15 hours agoparentOn Firefox iOS: open in a new window because you won’t be able to press back to come back to this discussion easily. reply freediver 12 hours agoprevThis collaboration is pretty exciting. I would expect that the teams of all three rendering engines (WebKit, Blink, Gecko) have done whatever they could to improve performance for the launch and that there won't be any outliers at the beginning with all of them having similar performance. But the title of future performance king is up for grabs! And now we have a de-facto standard for browser performance benchmarking. reply uncharted9 2 hours agoprevRyzen 5600H + RX 5500M laptop, AC plugged in with power mode in Windows set to \"Best performance\", Windows 11 Home 23H2 Chromium (122.0.6261.112) without any extensions:16.7 ± 0.34 Brave Beta (122.0.6261.111): 12.8 ± 0.62 Brave Beta (122.0.6261.111) Private: 15.9 ± 0.58 Floorp (11.10.5 based on Firefox ESR 115): 7.40 ± 0.20 Floorp (11.10.5 based on Firefox ESR 115) Private: 7.91 ± 0.19 Librewolf (123.0-1): 8.41 ± 0.19 Librewolf (123.0-1) with uBlock Origin disabled: 8.86 ± 0.17 reply elecman 26 minutes agoprevEdge results on my laptop:13 (all extensions disabled) Private: 20 reply lapcat 16 hours agoprevThis may be a dumb question, but what do the scores even mean? Is this explained anywhere? Neither https://browserbench.org/Speedometer3.0/about.html nor https://browserbench.org/Speedometer3.0/instructions.html appear to explain it. Are lower scores better, or higher scores? reply tux3 16 hours agoparentHigher is better. The analogy is speed. You want more speed. It's not a physical speed, just a benchmark number. Think of it as arbitrary units, which allows you to compare different version of browsers on the same machine. reply julienwaj 16 hours agorootparentIf the analogy isn't working for you, you can see the actual durations when you click on \"Details\". reply lapcat 16 hours agorootparentprev> You want more speed. On the other hand, premature optimization is the root of all evil. > Think of it as arbitrary units, which allows you to compare different version of browsers on the same machine. That's precisely the problem. It's arbitrary, meaningless. Without any physical units, I don't know what's good or bad, fast or slow. And why do the scores go from 0 to 140 when the web browsers are all getting approximately 20? reply treyd 15 hours agorootparent> On the other hand, premature optimization is the root of all evil. The web ecosystem is extremely mature and widely used. The workloads are fairly well understood. It is a magic unit, but the factors that go into it have a lot of thought from real-world scenarios. Bringing up \"premature optimization\" is completely irrelevant because that's not what this is, it's about as far as you can get from that. reply lapcat 15 hours agorootparent> that's not what this is, it's about as far as you can get from that. I don't know what it is. How exactly does the score relate to the experience of the web browser user? I'm a browser extension developer, and I've occasionally had people ask me about Speedometer scores, but I have no idea what they're supposed to mean or what to tell these people. reply om2 15 hours agorootparentSpeedometer measures web app responsiveness. Roughly, it simulates a series of user operations on web apps built with various frameworks (as well as vanilla JS), and measures the time it takes to complete them and paint the results to the screen. The score is a rescaled version of inverse time - if it goes up, that implies the browser can handle more user operations per second, or alternately, it takes fewer milliseconds to complete a user operation in a complex web app. reply lapcat 14 hours agorootparent> Speedometer measures web app responsiveness. We know that, but you haven't said anything specific about scores other than higher scores are faster, in an abstract sense, which has already been established. reply IainIreland 14 hours agorootparent\"The score is a rescaled version of inverse time\" is the key here. If you run all the tests in half the time, your Speedometer score will double. If your score improves by 1%, it implies that you are 1% faster on the subtests. (There are probably some subtleties here because we're using the geometric mean to avoid putting too much weight on any individual subtest, but the rough intuition should still hold.) reply IainIreland 15 hours agorootparentprev(I work on SpiderMonkey.) Benchmarking is hard. It is very easy to write a benchmark where improving your score does not improve real-world performance, and over time even a good benchmark will become less useful as the important improvements are all made. This V8 blog post about Octane is a good description of some of the issues: https://v8.dev/blog/retiring-octane Speedometer 3, in my experience, is the least bad browser benchmark. It hits code that we know from independent evidence is important for real-world performance. We've been targeting our performance work at Speedometer 3 for the last year, and we've seen good results. My favourite example: a few years ago, we decided that initial pageload performance was our performance priority for the year, and we spent some time trying to optimize for that. Speedometer 3 is not primarily a pageload benchmark. Nevertheless, our pageload telemetry improved more from targeting Speedometer 3 than it did when we were deliberately targeting pageload. (See the pretty graphs here: https://hacks.mozilla.org/2023/10/down-and-to-the-right-fire...) This is the advantage of having a good benchmark; it speeds up the iterative cycle of identifying a potential issue, writing a patch, and evaluating the results. reply lapcat 14 hours agorootparentThis doesn't say anything about what the scores mean. 21 is apparently better than 20, but how much better? You could say \"1 better\", tautologically, but how does that relate to the real world? Driving a car 1 mile per hour faster may be better, in a sense, but even if you drove 24 hours straight, it would only gain you 24 total miles, which is almost negligible on such a long trip. Nobody would be impressed by that difference. reply Vinnl 17 minutes agorootparentIain explained that in a reply to your other comment: https://news.ycombinator.com/item?id=39672279 > \"The score is a rescaled version of inverse time\" is the key here. > If you run all the tests in half the time, your Speedometer score will double. If your score improves by 1%, it implies that you are 1% faster on the subtests. > (There are probably some subtleties here because we're using the geometric mean to avoid putting too much weight on any individual subtest, but the rough intuition should still hold.) reply bigfudge 13 hours agorootparentprevI guess that’s why it’s fairly interesting to see scores thrown out in this thread on random hardware. It’s anexdata, but gives a sense of the spread/variance of scores for common platforms. I don’t think this is a number that is ever going to make much sense for consumers to use because without this sort of context it’s just going to be like the spinal tap ‘this one goes to 11’ sort of problem. reply charcircuit 13 hours agorootparentprevIt means it is 5% faster. You are overcomplicating it. reply lapcat 12 hours agorootparentPercentages are rarely informative without an absolute reference. A 5% raise for someone who makes $20k per year is $1k, whereas a 5% raise for someone who makes $200k is $10k, which would be a 50% raise for the former. reply Vinnl 15 hours agorootparentprevThey say something about the speed of the browser, so it doesn't really make sense to ask extension developers about it, I don't think. Possibly that your extension might make the browser slower, so you could compare scores with and without the extension and see whether it negatively affects performance? (Although I'm not sure it can necessarily tell you anything about to what extent it affects performance, only that it does.) reply lapcat 15 hours agorootparent> Although I'm not sure it can necessarily tell you anything about to what extent it affects performance Exactly. reply julienwaj 16 hours agorootparentprevThe score goes from 0 to 140 so that there's some room for when the computers will get faster. When we started working on this, all browsers were maxed at 140, so the computation got changed. reply jeffbee 12 hours agorootparentI thought the front page goes to 140 just because it is modeled after actual GM dashboard speedometers produced ~1960-1990, sometimes having range 0-85 MPH, or 0-140km/h in metric markets. reply IainIreland 11 hours agorootparentYes. The speedometer graphic was inherited from Speedometer 2. When Speedometer 2 was released, scores were in a reasonable car-speed range. The combination of hardware and software improvements meant that early versions of Speedometer 3 (which includes a subset of Speedometer 2 tests) were consistently scoring above 140, so we adjusted the scaling factor (IIRC, by ~20x) to give plenty of room for future improvements. reply om2 11 hours agorootparentprevNothing actually stops the score from going higher than 140, it will just max out the visual dashboard at that point. On Speedometer 2, Safari on M3 Macs ended up over 500. At scores that high it’s harder to have intuition, thus the changed scale of the new test. reply havaloc 9 hours agoprevPlatform Wars Results Samsung S24 Galaxy - 13.8 (very new, with the good processor) iPhone 13 Mini - 22.3 iPhone 15 Pro - 23.1 MBA M2 - 24.2 (Safari) Win 11 i9-13950HX - 26.8 (Edge) reply MBCook 5 hours agoparentWow. A S24 is that far behind? Does the browser make a difference? I figured the iPhones would be faster but not 2x. reply acdha 5 hours agorootparentMy iPhone 11 scores 16.8, which really highlights how much of a lead Apple opened up during Qualcomm’s complacent decade. I think that matters less for benchmark wars between tribes, fun as those always are, than as a stark reminder that any of us building for the public should remember than an S24 is _really_ fast for an Android phone and your median user probably bought whatever was on sale a couple years ago. That means that if you’re one of the many developers using an iPhone which isn’t roughly a decade old, you have no idea how your app feels to the median user because your device can run so much more code before it feels sluggish. reply HJain13 6 hours agoparentprev> Samsung S24 Galaxy Curious: Is it the Exynos version (Rest of the world) or the Snapdragon version (US, China)? reply internetter 6 hours agorootparentThey appear to be in the United States reply bee_rider 6 hours agoparentprevComparing a 3 year old iPhone mini to the current Galaxy is a Platform Warcrime. reply sedatk 6 hours agoparentprevIt crashes on my iPhone 14 Pro. reply Spivak 8 hours agoparentprevYeah this tracks, Apple clearly have sold their souls to the devil to get the performance they have on iOS. It's basically the sole reason I have an iPhone. reply bscphil 5 hours agoprevI see really insane outliers on some tests, some of the time, and this seems to kill the score on my platform (Firefox 122 on Linux x86_64). As an example \"NewsSite-Next\" has 8 out of 9 repetitions between 215 and 236 ms, but 1 out of 9 (iteration 5) is 1884 ms. This is such a radical outlier I have trouble believing it could be a browser bug. Visually, the interface seems to get \"hung\" when switching between tests sometimes. I don't have a great explanation for that. This specific issue in this case in with NewsSite-Next/NavigateToUS which reports a 144.5% variance as a result of this one outlier. I see several others like this in the results, although none quite as extreme. reply tambourine_man 5 hours agoprevAnd yet no one cared to make a responsive CSS to the site. Try to read it in a phone. This kind of thing drives me nuts. And it’s just text, it’s not like it’s rocket science. reply kleiba 14 hours agoprevBy far the biggest speed complaint I have about Firefox is not in everyday use, but whenever I restore a previously saved session - it basically stops reacting for a few minutes(!) before eventually I can use it again. I suppose it's due to the anti-virus interfering with some kind of memory image or whatever but whatever it is, it's so annoying. reply gruez 4 hours agoparentAre you a tab hoarder by any chance? I get 2-3s of lag when restarting with a few dozen tabs open, but nowhere near \"a few minutes\". reply WarOnPrivacy 14 hours agoparentprev> but whenever I restore a previously saved session - it basically stops reacting for a few minutes(!) I haven't run into anything like this; you may be an outlier. I interact with ~7 Firefox instances (Win) each week. Each has diff configs and plugins. reply emayljames 14 hours agorootparentYeah, I've used on linux/win/mac restoring sometimes 20+tabs and never had that issue. reply Lacusch 12 hours agoparentprevI've the same problem on Linux with no antivirus installed reply murat124 16 hours agoprevGot \"Infinity\" after testing my Firefox Dev Edition 123b9. Is this because of my FF config because my browser is perhaps blocking something (e.g. canvas, fingerprint, etc) or any result north of 140 is considered infinity? reply gruez 4 hours agoparent>because my browser is perhaps blocking something (e.g. canvas, fingerprint, etc) or any result north of 140 is considered infinity? I vaguely remember there's a privacy protection that rounds timer information. eg. all timers get rounded to the nearest 100ms. If you have a bunch of tests that take less than 100ms to complete, those tests might seemingly complete at the same time they start, which causes them to have infinite score. reply julienwaj 16 hours agoparentprevDo you see anything (errors or something else) in the web console? reply julienwaj 16 hours agorootparentPlease file a bug there if necessary => https://github.com/WebKit/Speedometer/issues/new :-) reply dist-epoch 10 hours agorootparentprevI also got Infinity, on Firefox 123.0.1, with a bunch of privacy extensions. There is only one warning in the Console: Ignoring ‘preventDefault()’ call on event of type ‘wheel’ from a listener registered as ‘passive’. react-dom.production.min.js:29:112 reply tdudhhu 16 hours agoprevOn Firefox mobile I got a score of 3. So while Firefox is now super fast the performance of webapps might still be very bad on mobile. And I think this applies to all modern browsers: they are fast at rendering very slow webapps and websites. reply kiwijamo 15 hours agoparentProbably hardware dependent too as I get 8.11 on my fairly old Samsung S21. My newer laptop gets 14.9 so I'm not sure whether I agree performance is \"very bad\" on mobile -- it may be less performant but that is surely to be expected given the hardware constraints on mobile. YMMV. reply vient 13 hours agorootparentGot 7.89 on laptop with Core Ultra 155H in battery mode, 17.9 in AC mode. Zenfone 10 shows 7.75. All in Firefox. reply wswope 15 hours agoparentprevIf you’re on iOS, Apple gates Firefox from using JIT JS compilation which massively hinders performance. E: I was wrong/extremely out-of-date - it does have JIT but relies on the Safari/Webkit implementation. In ancient versions of iOS, the WebView widget that third-party browsers were forced to use had JIT disabled, but that’s long since changed. reply ornornor 15 hours agorootparentFor this benchmark I get 12.0 in FF and 13.9 on safari. I’m glad it’s not so big of a gap as I already pay a penalty for not wanting to use safari on iOS (in terms of integration with iOS and usability from Apple’s artificial limitations on third party browsers) reply capitainenemo 15 hours agorootparentprevIt's more than that right? They have to use all of webkit. So it's pretty much a reskinned Safari in terms of layout/rendering/JS. reply om2 15 hours agorootparentprevThat’s not accurate. Firefox and all third party WebKit apps get the same JOT as Safari. reply capitainenemo 15 hours agorootparentRight, but expecting the same behaviour from \"Firefox\" on iOS as on desktop is just not going to happen, since they have no control over the core engine. It's why, in general using iOS devices for cross-browser testing is pretty useless. reply om2 12 hours agorootparentThis is a fair point, though it is possible for app-level things that the browsers do to regress performance from the baseline pure engine level. In this case, I think the 3 score must be either very old/low-end Android hardware or a measurement error. I don’t think any iOS browser gets 3.x scores, on even remotely modern hardware. reply mattlondon 15 hours agoparentprevApprox 6.3 and 6.7 in chrome and Firefox respectively on my low-end Pixel 6a reply SushiHippie 13 hours agorootparentWeird I have the Google Pixel 8 Pro and get a score of 4.83 in Fennec and 6.61 in Vanadium (hardened chromium fork). reply jeppesen-io 14 hours agoparentprev11.6 on s23 Sounds like old hardware or some other issue reply dvngnt_ 11 hours agoparentprev4.7 brave on a pixel 7 reply nusl 15 hours agoparentprevAre you on iOS or Android? reply WarOnPrivacy 14 hours agoparentprev10.9 on Win10 2017 Xeon @ 4.3GHz w/ 64GB. This instance of Ffx has had ~50 tabs (across 5 containers) open for a couple of weeks. What do these values represent? I can guess the last. Unsure of the first 2. 96.84 ±(5.0%) 4.87 ms reply cristianer 10 hours agoprevNice, an update well received by enthusiast. reply sib 10 hours agopreviPhone 15 Pro Max; iOS 17.4; Safari: 26.2 in a private tab Pixel 8 Pro; Android 14 QPR2; Chrome: 9.47 in an incognito tab reply havaloc 9 hours agoparentI was debating between a Galaxy S24 and a Pixel for an Android test device. 13.8 vs 9.47 on a Pixel is not a good result for Google's custom chip. reply kossTKR 17 hours agoprevHopefully at some point actual click latency will be fixed in general after some dark decades. Still incredible that a gameboy or an 80's computer with a CRT feels more responsive than most devices these days. Bring back tactility. I'm convinced the choppiness and weird waits are actually psychologically stressing us out. That's why good keyboards + old low latency OS'es or typewriters are so soothing to use. reply coldblues 17 hours agoparentLatency will always be an issue as long as developers use web technologies. Nothing beats native. reply CharlesW 16 hours agoparentprev> Hopefully at some point actual click latency will be fixed in general after some dark decades. Meaning, the 300ms delay (if the site developer does no optimization) with mobile browsers? reply tentacleuno 15 hours agorootparentContext for the 300ms delay: https://developer.chrome.com/blog/300ms-tap-delay-gone-away/ reply CharlesW 15 hours agorootparentSome other ways to mitigate: https://www.sitepoint.com/5-ways-prevent-300ms-click-delay-m... reply olliej 16 hours agoparentprevWhat is the latency being referred to here? I don’t see noticeable lag on pressing/tapping buttons or other ui components in day to day browsing, even on my quite old iPhone. There are obviously ways to make delays in web content anyway (user action->synchronous network request being the canonical one), but assuming there’s nothing silly like that lag isn’t an issue I’ve noticed. Actual execution latency is something I worked on for many years in JSC, and so there are a lot of engine optimizations to reduce that latency as much as possible (the interpreter itself, the interpreter performance, byte code caches, hilarious amounts of lazy parsing and source skipping, etc) so even the first time you have a ui element trigger code there shouldn’t be any significant delay. Obviously if a developer makes poor choices there’s only so much you can do, but by and large there aren’t that many bad things a web developer can do that a native dev can’t also do (and devs in both environments frequently do :-/). reply iknowstuff 16 hours agoprevCrashes, or gets killed, by Safari on iOS 17.3 on iPhone 15 Pro. reply CharlesW 15 hours agoparentTry it with a Private tab. Mine did the same until I did that, after which I got 16.5 on an iPhone 13 Pro Max. reply jeffbee 12 hours agoprevPosting just because nobody else has posted one this high: Mac mini M2, macOS 14.4, Chrome 122: 30.2 ± 1.6 Good ol' Apple Silicon. reply mccr8 11 hours agoparentI got 35.7 ± 2.3 on a MacBook Pro M3, Chrome 122. reply jeffbee 10 hours agorootparent33.3 with an Intel 14900k and Ubuntu. And people will sit there and try to tell you that computers aren't getting faster any more. reply 12345hn6789 14 hours agoprevWhy is the scale 0-140? My modern windows 10 desktop using FireFox latest gives 15.0/140 with no other programs running besides FF and discord. Surely 15 is horrible in that context? I have 1 extension, ublock origin, allowed to run on the site by default. I have never felt performance has ever lacked, outside of a few outlier sites (youtube, facebook, twitch). But those are tightly coupled with their (crappy) implementations. reply troupo 12 hours agoprev> The primary goal of Speedometer 3 is to reflect the real-world Web as much as possible, so that users benefit when a browser improves its score on the benchmark. As with any other benchmark its results will be interpreted incorrectly and will have little effect on real world. Google already has vast amounts of real-world data. The end result? \"Oh, you should aim for a Largest Contentful Paint of 2.5 seconds or lower\" (emphasis mine): https://blog.chromium.org/2020/05/the-science-behind-web-vit... Why? Because in real world the vast majority of sites is worse. Browsers are already optimised beyond any reasonable expectation. Benchmarks like these focus on all the wrong things with little to no benefit to the actual performance of real-life web. Make all benchmarks you want, but then Google's own Youtube will load 2.5 MB of CSS and 12 MB of Javascript to display a grid of images, and Google's own Lighthouse will scream at you for the hundreds of errors and warnings Youtube embed triggers. Edit: Optimise all you want, and run any benchmarks you want for the \"real world\", but performance inequality gap will still be there: https://infrequently.org/2024/01/performance-inequality-gap-... Optimise all you want, and run any benchmarks you want for the \"real world\", but Lighthouse will warn you when you have over 800 DOM nodes, and will show an error for more than 1400 DOM nodes (which are laughably small numbers) for a reason: https://developer.chrome.com/docs/lighthouse/performance/dom... reply ramones13 4 hours agoparentSpeedometer 3 is designed to handle the real world you describe. Edge’s post has some details - https://blogs.windows.com/msedgedev/2024/03/11/contributing-... reply MBCook 5 hours agoparentprevThe last two versions have lead to demonstrable speedups in major browsers. Why wouldn’t this? Bad developers (or management dictates) will be bad no matter what. That’s not a reason to give up. reply atlas_hugged 14 hours agoprevIn iOS, all browsers (at the moment) use Safari under the hood. Imagine my surprise to see these noticeable differences in some of them. Vivaldi:12.2 Brave:18.1 Safari:18.2 Chrome:19 Firefox Focus:21 reply freediver 13 hours agoparentThis is because browsers on iOS do not 'use Safari' but use WebKit and there is huge amount of browser app software built on top of it, which will contribute to variance on benchmarks (and also to these being very different browsers ultimately). reply havaloc 15 hours agoprev13.1 on MBA 15\" M2. reply RantyDave 11 hours agoprevIn descending order.... MacBook Pro, M2 Pro, 16GB, plugged in, external display: Safari=31.2 Chrome=29.4 iPhone 12 mini, plugged in: Safari=19.4 HP Z2 mini (i7): Edge=15.9 Panasonic Toughbook CF19 (win 10): Edge=4.7 Chrome=5.6 Galaxy Tab S5e: Chrome=2.2 Oculus Quest 2: browser crashed Tizen TV: displayed, wouldn't run Nintendo 2DS: displayed, no css, wouldn't run reply MBCook 5 hours agoparent> Nintendo 2DS You brave fool. I love that you tried it. reply RantyDave 4 hours agorootparentIt was the tv that took the most effort. reply sedatk 13 hours agoprevOn my machine, Firefox got 12.3, and Edge (Chromium) got 12.8. I don't believe that the performance characteristics of these two are that close unless I'm missing something. For example, audio players on Edge stutter a lot while Firefox plays them smoothly. An example is: https://deepsid.chordian.net/ I believe Edge is slower not because Chromium is slow, but because of Microsoft's overreaching efforts on energy conservation. Machine: AMD 5950X, 32GB RAM, 3080 GPU, Windows 11 Pro 23H2 Firefox v123.0.1 Edge v122.0.2365.80 EDIT: Interesting, I tried both in private windows later to bypass extensions, and Edge got 10.8 this time, Firefox got 16.9. I now have more questions. reply rezonant 13 hours agoparentI ran v3 on my machine while listening to \"Voyage\" by \"Yahel & Eyal Barkan\" in Chrome and doing a bunch of background stuff. The background stuff took up about 20% of my CPU. While testing, the music played perfectly without any buffer underrun pops. Ran it in each browser one at a time while the music played in Chrome. Chrome 122.0.6261.112: 21.3 +/- 0.64 Edge 122.0.2365.80: 20.1 +/- 0.78 Firefox 121.0.1: 18.5 +/- 0.75 Machine specs: Intel Core i9 12900k (24 core) / 64GB RAM / 3080Ti / Windows 11 Pro 23H2 After finishing the tests, I played that same song on Firefox and Edge. Both Firefox and Edge played it perfectly. > audio players on Edge stutter a lot while Firefox plays them smoothly I'm curious about what could be leading to this inconsistency as I use Web Audio for a number of projects, so I have a bit of a vested interest. It is notoriously easy to do WebAudio wrong or to do just a bit too much computation which leads to buffer underruns (pops). It also may have a lot to do with specific tracks on DeepSID, could you share some tracks that perform inconsistently for you? reply sedatk 12 hours agorootparentAny track plays completely garbage on Edge. The beat skips, the sound cuts off. This one for instance: https://deepsid.chordian.net/?file=/MUSICIANS/F/Fate/World_R... I think Edge's problems come from some kind of power efficiency setting, not necessarily performance-related. (Like a low-granularity JS timer, or something like that) EDIT: Turning off all efficiency settings on Edge didn't make any difference: 11.0 reply rezonant 10 hours agorootparentDo you have your Windows power plan on Performance? Maybe thats the difference? Definitely would be a bummer if web audio didn't work reliably on the default power plan (which is Balanced iirc) reply sedatk 6 hours agorootparentIt’s on High Performance, yes. reply sedatk 6 hours agoparentprevOther interesting notes: - Firefox on WSL2 on the same machine gets 10.1 despite that the rendering must be horribly slow since it goes through Remote Desktop layer. - Firefox gets a 25% speed boost when I disable 1Password extension. Disabling it on Edge makes no difference. reply mdasen 12 hours agoparentprevAudio wouldn't be going via the DOM or JS, right? I know that Firefox has its own codec support and that Safari on Mac uses different AV stuff than other browsers. I don't think that AV stuff would be tested by Speedometer. reply sedatk 1 hour agorootparentIt's more than channeling audio files to browser's codecs though. A SID player for example runs a 6502 CPU emulator and a SID chip emulator on the browser using JS. So, it's problematic in such scenarios. Otherwise, I can watch Youtube videos, or listen to Internet radios without issues. reply rezonant 11 hours agorootparentprev> don't think that AV stuff would be tested by Speedometer It probably isn't, but fwiw yes web audio is controlled by JavaScript. Doing it right means using web audio worklets, which is a special purpose JS context that has no access to your main page context. reply CraftThatBlock 14 hours agoprevSome of my results: Desktop Firefox: 25 Desktop Chrome: 26 Laptop Firefox: 16 Laptop Chrome: 20 Laptop Safari: 21 Phone Firefox: 12 Phone Chrome: 10 --- Desktop: 5900X, 3090, Linux Laptop: M1 Pro 14\" Phone: S24 Ultra Ran all tests in private window to avoid extensions, and gave a minute to cool between tests. Laptop/phone was plugged in. reply hedgehog 12 hours agoparentM1 MacBook Air: Safari: 24.1 Firefox: 21.7 Extensions can really slow things down: Safari w/ Ghostery: 6.83 Safari w/ AdBlock: 13.0 reply aPoCoMiLogin 13 hours agoparentprevchrome 27.4 firefox 26.3 desktop: 7800xd3, 2060, linux reply butz 14 hours agoprevWe should stop \"speed-shaming\" browsers and focus on websites, which negate all performance improvements made by browser developers by adding more useless features. reply apfsx 12 hours agoprevI tested this with Firefox stable release and Brave stable release, 3 runs on each. Same exact extensions across both. Highest scores across tests: Firefox: 6.34 +- 0.31 Brave: 11.3 +- 0.37 on Ryzen 9 7940HS + RTX 3060 mobile Which really sucks since I highly prefer Firefox but this past week I've been trying out Brave and I think its noticeably faster and smoother to me. Even with the reduced speed I'm still swayed toward Firefox for the customization factor you can achieve with userchrome.css file. reply wolverine876 12 hours agoparentThe 'same' extensions on each browser control for your experience, but not for the browsers' performance: The browsers have the same or very similar APIs for the extensions but that is just the interface; each browser executes the extension's instructions differently (a lot or a little - I don't know the browsers' code). The same extension will impact Brave's performance differently than it will impact Firefox's. In other words, the same extension is not, in this sense, the 'same' on each browser. In this sense, an extension is part of the user experience, like a website. The Speedometer test suite doesn't include those extensions (I assume) and that is the experience the browsers are optimized for. The parent's test doesn't represent that; it does represent their desired experience, of course. reply muizelaar 8 hours agoparentprevWhat's your score in Firefox with extensions disabled? reply apfsx 3 hours agorootparentFirefox extensions disabled: 16.8 +- 0.59 Brave extensions disabled: 19.0 +- 0.88 Interesting because I have only 5 extensions. The heaviest extension seems to be Dark Reader which causes over 5 point changes. reply denysonique 12 hours agoparentprevEven if you have the exact same extensions the fact that you have an old Firefox profile may be hindering the results. Try comparing with a fresh Firefox profile with the same extensions. reply apfsx 12 hours agorootparentThe Firefox profile I'm using is no more than 3 weeks old. Fresh install of Windows was done around that time. reply fddrdplktrew 12 hours agoprevI'm more worried about Firefox' stability these days... reply dylan604 15 hours agoprev [8 more] [flagged] Timon3 15 hours agoparentWhen that metric is \"performance in real workloads\", I can't imagine it ever becoming irrelevant. Just look at their new tests: > In particular, we added new tests that simulate rendering canvas and SVG charts (React Stockcharts, Chart.js, Perf Dashboard, and Observable Plot), code editing (CodeMirror), WYSIWYG editing (TipTap), and reading news sites (Next.js and Nuxt.js). > We’ve also improved the TodoMVC tests: updating the code to adapt to the most common versions of the most popular frameworks based on data from the the HTTP Archive. The following frameworks and libraries are included: Angular, Backbone, jQuery, Lit, Preact, React, React+Redux, Svelte, and Vue; along with vanilla JavaScript implementations targeting ES5 and ES6, and a Web Components version. We also introduced more complex versions of these tests which are embedded into a bigger DOM tree with many complex CSS rules that more closely emulate the page weight and structure from popular webapps today. Improving these benchmark results will at least partially make those libraries faster in the real world, and most likely also many additional libraries and workloads. reply coldpie 15 hours agorootparentThe article even links to real-world performance measurements that are completely separate from the benchmark. I have no idea what the purpose of the OP's comment was. I guess they just wanted to blurt out the first cynical thing that came to mind. Awesome contribution. reply dylan604 14 hours agorootparentI love the fact that an honest question is met with such hostility. I knew there was a quote, but was unable to think of it well enough for a proper search. It is easier to find an answer from other people with just a shard of detail that search engine will not find as there's no SEO for the broken fragment. I'm so happy to see this place is alive and well with the attitude to support a curious mind. What a tosser reply mos_basik 14 hours agorootparentI've probably spent too much time on the internet, then, because I definitely wouldn't have interpreted your original post as an honest question without having seen the clarification in your followup comment. Probably a defense mechanism built from past pain caused by my assuming good faith and then being ridiculed from the non-honest-question-asker. But with that nastiness out of the way - I looked back at your original question and thought \"seems like the kind of query Kagi would eat for breakfast\". Here's what it responded to your question: https://kagi.com/search?q=What%27s+the+quote+about+optimizin... Specifically, the \"what\" at the start and the \"?\" at the end triggered the LLM-powered quick answer at the top (and it passes the smell test for correctness). To Google's credit, it also returns reasonable results for this question. reply coldpie 14 hours agorootparentprevSo, why are you bringing up the quote in this context, then? reply dylan604 14 hours agorootparentBecause it's directly what spurred the thought? What, I'm supposed to post it randomly? reply psychoslave 15 hours agoparentprev [–] Probably Goodhart's law: when a measure becomes a target, it ceases to be a good measure. To my mind, it means that the metric becoming the main focus, it makes easy to forget the original relevant goal and even works against it. That is not the case here. https://en.m.wikipedia.org/wiki/Goodhart's_law reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Speedometer 3.0, a collaborative browser benchmark created by Apple, Google, Microsoft, and Mozilla, is now available to assess web app responsiveness with new user interaction simulations.",
      "Developed under a fresh governance structure, it incorporates contributions from major browser engines to enhance accuracy in measuring browser tasks, potentially benefiting users with performance optimizations.",
      "The benchmark strives to mirror the contemporary web landscape, offering diverse tests and optimization avenues to enhance user experience."
    ],
    "commentSummary": [
      "The discussion centers on browser performance benchmarks like Speedometer 3, showcasing real-world web application responsiveness.",
      "Users analyze scores across browsers and devices, identifying browser bugs, audio compatibility, and browser-specific failures.",
      "Insights on incremental score enhancements, developers' challenges in optimizing apps for diverse devices and browsers, and concerns about extensions influencing browser performance are shared, enriching the discourse on improving browsing experiences."
    ],
    "points": 208,
    "commentCount": 146,
    "retryCount": 0,
    "time": 1710173838
  }
]
