[
  {
    "id": 40950584,
    "title": "Use a work journal",
    "originLink": "https://fev.al/posts/work-journal/",
    "originBody": "Use A Work Journal To Recover Focus Faster And Clarify Your Thoughts 2024-Jul-12 You’re working on the most complex problem in computer science: fixing permissions on a deployment pipeline. It’s been 4 days you started on that simple task already. Your manager explained to you in no uncertain terms that your performance on the subject is well below the expectations she has from a midterm intern. Your colleagues stay as far away as possible from you to avoid getting tainted by your shameful failure. 4 days of sleepless afternoons, seeing that freaking status turning to “build failed” everytime, bringing you to tears. The weather is shit, rain taping on the window of your overpriced basement suite, reflecting the state of your soul. You never felt so alone. Even your partner left you, you loser! But this time you got it. This time you have a strategy beyond clicking on “retry failed steps” again and again. You finally read the logs, and you have an idea. You think you finely grasped what might be wrong. It’s a long shot: you’re gonna clear the credential cache, get an elevation you’re missing, force a permission sync, reset the service connection to the cluster, then downgrade the stupid auth library you’re using to a version that was hacked 6y ago but still works, setup everything again, then rollback. Your brain is making the connections, you got it, you’re better than that. You’re sorting through 23 different documentation tabs, waiting for the elevation to succeed, summoning all precious focus you got. A red bubble shows up on IM. Conditioned by years of desperately reading your texts as soon as they arrived to create yourself what passes for a social life, your hand doesn’t even consults the sentient part of your cortex, and just moves to the little icon. click. It’s Mitch, your PM1. He’s asking the url for a doc he wrote, and complains that it’s so complex to find doc in this organization. This is a trap meant to get your focus out. Not this time. You’re better than that. You ignore his message, look at the elevation command, trigger it. Copy the id of the request that you need to preciously keep to finish the elevation to your clipboard. 4 minutes later you get a call request from your manager. You answer. Hey, Mitch is saying he needs a doc urgently and you’re not answering his IM. Can you get to it? poof Where the freak was I? Credit: monkeyuser.com. notice how much better it conveys my story. Like everyone, I sometimes struggle to maintain focus. This was particularly true when I was a manager switching context all day long, this is also true as a dev working on three antagonistic projects, including one that’s very consultory in nature, and working with processes that sometimes take hours to complete. The typical situation is that I start something, switch to something else, get into a meeting, forget the very essence of what I was doing. Turn on autopilot, read all my emails, all my IMs. Then it’s 5PM, I’m exhausted, and I realize I’m at the same stage I was at 8AM, tell myself I should really achieve something that day, and open HN. It was so bad at the beginning of this year that I seriously started wondering if I had ADHD2. I started working on something badly documented. As in: there’s no documentation, the people who built that thing are halfway across the world and they don’t even really work for the company anymore. It wasn’t even just interruption, but also procrastination. I felt so frustrated by the situation that I started writing that in my daily notes on Obsidian. I asked Berna how to turn on super-compression 5000, and she’s not answering again! I tried it with the --yo-compress-shit really-well-like-5000-or-something and it didn’t work. It still spits me that yo is not proper English, be civilized error. What the hell can I do about that. Mitch called me to ask for that doc again, which I gave him, not without mentioning that little “star” icon in the url bar. And then got back at it. And boy oh boy did that help! I just re-read what I was doing, and boom! I was back in. I started listing all the commands I was running, and their results. Writing down my train of thoughts, the things I was doing and what I wanted to do next. And I have been doing that for the past 3-4 months. I feel like I invented something new. It helps me think more clearly, and restore the context so, so much faster when I switch between things. I’m almost looking forward to an interruption to get a chance to marvel again at my genius! Except that it’s nothing new, right? “Writing helps you organize your thoughts more clearly”: everyone and their grandmother know that! Writing a plan, writing a diary? People keep listing how transformative that’s been for them. I’m not proposing a new framework. I’m just saying - every movie has a scientist recording themselves on one of these shitty little cassette recorders. They might be onto something. Write notes of what you’re doing and what you’re thinking. When you drop the pen and get back at it, read the last bit. That’s it. I’ve just been too lazy to ever do it. Or not necessarily lazy, but more: I didn’t trust the tool enough to think it was a good use of my time, and instead just mash on the keyboard till it works. After all, I’m writing pages of text, of which I will never read more than a fraction. But that’s not the point. The point is structure, and the point is caching. I guess that’s kind of it: if you’re having trouble switching between things or getting focused, try writing what you’re doing, and read the last couple sentences when you resume. Maybe it will help you. Maybe it won’t. Or maybe I’m an idiot who needs crutches. But hey, who knows! notes Discussion on HN this is not a bash on PMs, everyone has the potential to be an interrupting dick. Please don’t take that personally if you are a PM. I don’t know anyone called Mitch. If you are called Mitch, please feel free to take it personally and launch a vendetta. I’m sure you’re a dick anyways. ↩ I don’t. I’m just a product of our interrupting times. ↩ Related",
    "commentLink": "https://news.ycombinator.com/item?id=40950584",
    "commentBody": "Use a work journal (fev.al)760 points by charles_f 18 hours agohidepastfavorite220 comments simpaticoder 16 hours agoThe benefit of journaling is not just reentry, but that you begin to solidify the mental model into a concrete branching of possibilities that is tightly coupled to the specific problem. Your work becomes traversal and mutation of this tree. Several benefits accrue: you begin to see gaps in the tree, and can fill them in. You begin to have confidence in your mental model, recovering the time you used to spend going over the same nodes again and again in a haphazard way. In distributed systems in particular, the work is often detailed, manual, error prone and high latency - with a solid mental model you can get through a checklist of steps with minimum difficulty and high confidence that you didn't miss anything. This ability to take something abstract and make it more concrete on the fly is a critical skill. Perhaps the greatest barrier to using it is akin to envy. We see others who apparently do this without written materials, in their head. I think we see this as evidence of intellectual superiority and harbor the doubt that using an aid like a journal means we are somehow lacking in skill or ability. This is wrong. Using an aid to map out complex problems isn't a failure, it's essential, especially for problems in systems you've never used before. Over time you may yourself build up your expertise such that you no longer need the aid, but that doesn't signal anything about your intelligence or ability either, only your experience. reply lelanthran 11 hours agoparent> solidify the mental model into a concrete branching of possibilities that is tightly coupled to the specific problem. Your work becomes traversal and mutation of this tree. I wrote a program (used from a CLI, but I mostly use the GUI I developed for it) to do something similar for my own use: https://github.com/lelanthran/frame/blob/master/docs/FrameIn... I use it daily. reply cmiles74 6 hours agorootparentIt's low-tech, but I've been using notebooks (not too thick, stapled usually). I write a header for each day when I start and then a line or two as I work on tasks and I try to note each time I switch tasks. I keep the notebooks but rarely look at them once they are filled. Maybe once or twice as I switch to a new notebook and then once again when it comes to yearly review time. A couple of times I have rifled through old ones looking for command line flags but most of the time reading through the notes is enough to jog my memory. reply interroboink 2 minutes agorootparent+1 for physical notes (: I use a lot of diagrams / sketches / arrows between things / etc, so doing it all in text on a computer is too cumbersome. Of course, with paper, looking up something from the past is a pain sometimes. reply gofreddygo 3 hours agorootparentprevA real notebook and pen are the perfect tools for this. Its the only one I know that works, long term. append only log works, but skimming through 2 months of logs for a specific thing is poor and slow but very useful. I don't do that often but when i do need it sometimes and especially fast, like on a call, with no lead time. Its a superpower. I now organize my notebook a with a few conventions that make it more useful. I limit myself to using only the right side of a page for logs from whole week. Each new week gets a new page, always on the right side. Put the date as the page header. Makes skimming easier. Put any important tasks / targets for the week right after. Further Split right page into 3 columns. First 2 for work logs, third column for recurring weekly/biweekly meeting stuff. Very easy to go back to what was discussed 2 months ago. All logs are usually 1-3 words. Just cues. but everything has a topic subheader. Left page is free form for detailed stuff. Things i discover, design, 1:1 meeting notes, questions i need answers to, philosophy, anything new on HN, etc. Right page serves as context. I also do an index on the first page of the book pointing to anything that i find repeatedly useful. Could do page numbers but i put the date as its naturally ordered by the dates on the right page. Been doing this a while and works perfect. I have everything I need in one notebook, i can carry it with me. a $1 composition book with 50 pages lasts well over six months. reply tra3 2 hours agorootparentprevVan Neistat (YouTube) has a video that I can’t find know where he talks about his planning process. He uses full sized poster boards. Lots of space to lay out your ideas and get a great overview just not very pocketable. reply telesilla 4 hours agorootparentprevAlso a note taker - the physical aspect acts as a kind of cementing the thought. I never look at my old notes either and discard them when the book is complete, it's just an excerise in helping me manage complexity. reply devsda 6 hours agorootparentprevNot a comment about the project itself. If someone wants to try a similar flow but cannot run the above for any reason, git can be used to achieve something similar. You can also make use of existing tooling around git for shell integration like PS1 and gui. 1. Your main/master branch is your root frame. 2. Child Branches + branch commits themselves are messages. 3. Pop is hard reset of branch to parent or branch switch The main idea is your log resides in commit messages and not the commit data itself. You can try using commit data too but limit that for shared contextual information. Wrapping it up(to generate phony changes and running git) in shell aliases or functions should be easy. reply epolanski 2 hours agorootparentBut how do you use that flow to _plan_ your work? reply mbreese 1 hour agorootparentI wouldn't see that as a way to plan work, but rather to keep track of what you did or where you were. In general, I like the idea of ruthlessly tracking what I've done like this. But I think that it is still missing the context of the environment... meaning, if you are editing a file, it would be nice to not only know that you edited file A, but also that you changed line N to X. I've spent a decent amount of time thinking about this over the years and haven't fully figured out a good solution. I was a wet lab scientist for a long time and we have the tradition/requirement of keeping a lab notebook. This is something that is incredibly helpful where you keep track of what you've done and what you're planning to do. I've missed this when I switched over to mainly computational work. In the past, I've thought about maybe having a loopback FUSE-ish mounted directory to track changes to files would work. But I think something akin to working in a git tracked repository (with these intermittent commits per command) might make this work better! reply tarruda 9 hours agorootparentprevInteresting tool. I currently use a TODO text file in the root of each project I'm working on, which I update right before I stop working. The lists are hierarchical, so each task I'm focused on has a parent item, which helps me remind of the bigger picture. reply hallman76 33 minutes agorootparentMineis called README.md or notes.txt I use a similar hierarchical format - I use tabs for the hierarchy. I use markdown for most notes, but I find it somewhat incompatible with the tab-based note-taking that I prefer. I share this in case there are others out there who work the same way. Let's band together to establish a tab-friendly Markdown variant! reply slyfox125 8 hours agorootparentprevUsually, ths simplest solution is best and your approach exemplifies it: no need for a special application, simply open the text file and then go to work. reply leetrout 8 hours agorootparentI have found, and advise teams I lead or work with, that the tool absolutely does not matter because it is the discipline to use the tool consistently that makes a difference. Now, having said that, shoving jira down peoples throats with all kinds of rules around tagging and whatever wears people out. So, yes, a text file, a google doc, linear or a few post-its on the wall. reply rpdillon 7 hours agorootparent> because it is the discipline to use the tool consistently that makes a difference. Agree, though I advise folks that tooling matters tremendously, because a bad tool requires more discipline to continue using than a streamlined tool. reply leetrout 6 hours agorootparentThis is very true. I guess I was thinking about all the times new people join a team and want to make changes to the team to fit the tools they prefer (including when a \"big org\" person rolls in wanting JIRA). Toil and friction are killers. In \"Secrets of Productive People\" Mark talks about building systems and that the lower level / background things should be reliable and without friction. \"Good systems for simple administration will free your mind for more productive work. Ideally you shouldn’t need to have to think about the lower-level stuff at all. Thinking needs to be kept for the high-level systems, which will be designed to fit each particular case. But even then the aim of designing a high-level system is to avoid eventually having to think about that system too.\" reply lelanthran 5 hours agorootparentprev> Usually, ths simplest solution is best and your approach exemplifies it: no need for a special application, simply open the text file and then go to work. I agree. The \"tool\" I have simply maintains text files and provides a streamlined way to display/edit the current one, switch to some previous one and switch back. I made it to manage context switches. IOW, it's not to serve as a journal, it's to serve as a swap partition for my brain. reply was8309 1 hour agorootparentprevand a tool to squash or expand the hierarchy helps alot. jEdit can fold/unfold based on indentation. along with simple prefixes : ' for info, '>' for todo, '= {date}' for done, etc reply aggrrrh 9 hours agorootparentprevIt’s great. You should do a separate hn post reply tra3 2 hours agorootparentprevThis is really cool. How do you reorder frames? If you’re working on a particularly complex frame, how do you maintain context? reply lelanthran 1 hour agorootparent> This is really cool. How do you reorder frames? Since each frame is a subdirectory directory in ~/.framedb/root, I can simply `mv` them. TBH, I've never yet found a need to rearrange them. > If you’re working on a particularly complex frame, how do you maintain context? I create child frames using whatever context was written into the current frame. reply ravetcofx 10 hours agorootparentprevThat is one of those novel once in a generation kind of tools and concepts I feel. Outstanding reply johtso 9 hours agorootparentprevLove the sound of this tool! Seems a bit of a shame to permanently delete anything that's been completed, sometimes it's really useful to have a record of what you've been working on. reply lelanthran 47 minutes agorootparent> Love the sound of this tool! Seems a bit of a shame to permanently delete anything that's been completed, sometimes it's really useful to have a record of what you've been working on. I did originally have that; it lowered the signal:noise ratio. I've found that there's isn't a need to keep around any frames of context for things that I have completed: generally there's already an artifact from that frame of context anyway (write this function, call that person, design that foobar, etc). reply dhc02 10 hours agorootparentprevThis is really interesting. Thanks for sharing. reply kwakubiney 9 hours agorootparentprevReally cool tool. Starred reply safety1st 9 hours agoparentprevIf I'm working on something particularly complex I basically just do the journaling in a code comment adjacent to what I'm working on. So the first commit may be three lines of code and a huge long winded rambling comment of what I've already tried or thought about. By the time the task is done I've pared away the more speculative/rambling elements of what I wrote, and what's left is typically some extremely well commented code. I think this method results in higher quality code produced faster than if you just try to keep everything in your head. No one's complained yet...! reply glynnormington 8 hours agorootparentI like that. There are other ways of capturing work in progress adjacent to the code, instead of writing a journal. One of my favourites is to write a failing test - pretty much impossible to overlook or misunderstand on \"re-entry\" to the task. Another is to write a temporary commit log, with \"WIP\" in the first line and a TODO list in the rest of the log. This is good for ephemeral information that would just clutter up the code. If I do need something like a journal, I have occasionally just written a private gist and put that in a tab on my browser. reply simpaticoder 4 hours agorootparentprevA good technique I've used as well. Call it \"iterative literate programming\". reply quest88 15 hours agoparentprev> Perhaps the greatest barrier to using it is akin to envy. We see others who apparently do this without written materials, in their head. I think we see this as evidence of intellectual superiority and harbor the doubt that using an aid like a journal means we are somehow lacking in skill or ability. To add to your \"this is wrong\": These others may have themselves solved the problems we are now trying to solve, likely even using a journal. They no longer need a journal since they know how to navigate it, and it appears as superior to us. reply bee_rider 3 hours agorootparentThis was the biggest revelation in grad school for me, I think. The professors were in fact not gods of problem solving, they just had the answers. Not just of the problems they brought along (obviously). But also of the handful of problems we’d tend to invent. Of course, if you really catch them flat-footed, they can provide circumspect and sagely advice, and then quickly check Wikipedia to see if anyone has solved your problem. reply ahartmetz 9 hours agorootparentprevI mitigate this \"problem\" somewhat by taking pride in specialized yet (hopefully) readable notations and sometimes even creative tool usage. Like using a project planning tool with a Gantt chart feature to speed up a boot process with many dependencies. Of course, sometimes an existing tool or notation used for its original purpose is what you need. Maybe manually remove the stuff that doesn't matter. reply bee_rider 4 hours agoparentprevExperience looks like intelligence to us because we didn’t see the hours our favorite instructors and mentors spent banging their heads against their own notebooks, haha. reply simpaticoder 3 hours agorootparentIt is the rare instructor or mentor that takes pains to NOT show off their expertise, and model the expected behavior, tools, and techniques of the learner where they are at. It is the classic problem of telling someone the goal, but not how to get there. A common error-mode is repeating \"the magic\" over and over again, expecting it to sink in; it takes extra effort to decompose the magic into teachable, practicable parts. This teaching effort requires meta-cognition and empathy orthogonal to the effort required to become an expert, which is why expert teachers are a rare and precious gift. reply galaxyLogic 15 hours agoparentprevI write a lot of notes in fact I write more than I read. It helps. However I have some issues: 1. How much commentary should I write? I try to write not too many notes because I write code, and some say code should be self-documenting. So it's the same old question of how many comments should there be along the code. 2. How do I retrieve a note I've written earlier? I can use tags and search for them but it is not easy to come up with a perfect tag which I would remember later. 3. I have so many notes and by now many of them are out-of-date. I don't want to spend time updating my notes. But if I don't they can become misleading. There needs to be a balance between \"Just do it\" and \"Write about it\". I'm not sure I have the correct balance between those two. I can see an alternate approach which would be a FORUM where co-workers discuss what they are doing or plan to, or have done . But there too the retrieval might be a problem. However the FORUM-tool would automatically keep track of when something was written and by whom. NOTE: You might benefit from other people's notes just as you can from your own. reply TimSchumann 13 hours agorootparent> 1. How much commentary should I write? I try to write not too many notes because I write code, and some say code should be self-documenting. So it's the same old question of how many comments should there be along the code. I think the best I've heard this was by a friend who said something akin to this; Programming is the art of solving problems by codifying complexity. Generally, the self documenting part is the 'defining the problem' portion of the code. But in every problem, there's a certain amount of irreducible complexity, or it wouldn't be a problem. There's going to be some part of the code where you wish you could make it simpler, or you wish you understood it better, or you wish you could break it down into smaller components but there's no 'good way' to do it in the system you're working in. Or, the way you have working is 'good enough' and it's not worth the investment from some (business needs angle) in making it any better. This is the portion of the code you should comment, and document, and do so liberally and in detail. reply dghlsakjg 15 hours agorootparentprevCheck out obsidian. It is just folders and markdown so it is ultra portable, but the obsidian editor has tons of useful features like a graph view, autolinking, and a plugin in for anything you can think of. There are tons of YouTube videos and articles describing different organizing systems and ways to use it. reply simpaticoder 3 hours agorootparentI like Obsidian, but plain text continues to reign supreme. I prefer nvAlt or my programmers editor, or if on a team, whatever the company provides (e.g. Jira). The embarrassment of options is actually a key problem in itself, since if you do not commit to one, you find your notes spread across various files, tools, and services, totally disjoint in a way that is impossible to work with and difficult to undo. The ideal tool, which I don't think exists, would combine the immediacy and locality of nvAlt and bidirectionally map to something like Jira for sharing, distributed as a browser plugin and/or a simple server component with local write access. reply patmorgan23 10 minutes agorootparentObsidian is pretty close to that. Markdown is basically plain text. reply wenc 15 hours agorootparentprevIt really depends on your work. If you're doing mundane work, keeping notes is just busywork and doesn't really have a payoff. But if you're constantly trying to solve novel problems, and have episodic ideas that are half-baked, writing notes -- without trying to organize them first -- can be really powerful. For me, I just write them in Logseq and tag them with a few hashtags like #topic1 #topic2 #topic3. It doesn't have to be a perfect tag, just tag it with all the topics you think are relevant. From time to timeI click a hashtag and revisit all my half-baked ideas -- periodic revisits and curation is key -- I surprise myself when some peripherally connected notes coalesce into a real idea. (Logseq makes this easy because each note is bullet point that can be tagged, and clicking on a tag is like running a query) This is called the Fieldstone method. (conceptualized by Gerard Weinberg). It's a very useful approach for writers because it recognizes that the best ideas are episodic and don't all come at once, you have to gather the \"stones\" over a long time before something gels. https://www.amazon.com/Weinberg-Writing-Fieldstone-Gerald-M/... I've used it with great success over the years (both at work and in my writing). reply simpaticoder 15 hours agorootparentprevKeep trying. Try different methods. What works for me debugging and refactoring distributed systems may not work for you. The thing I tend to drive toward is a single page mind-map-like artifact that is monotonically increasing in density with a focus on system-of-record and data flow. I tend to keep notes about individual tech and important systems in whatever tools are handy, either locally or in a form that are accessible to the team. Often this takes the form of a JIRA page per topic, or an nvAlt note per topic. Tickets aren't great for this because they are ephemeral. These notes are where code snippets, error messages, anything searchable, go. I have something like the Zettlekasten[1] method in the back of my mind when deciding on scope of these support notes. However I may also start from a simple local text file, not committed to the repository, or even attached as a note to a ticket or other ephemeral trigger. But the thing that ties it all together, for me, is that mind map. Once I have that I can truly reason about the system. 1 - https://zettelkasten.de/introduction/ reply 8n4vidtmkvmk 13 hours agorootparentprevI write but never reread. The act of writing helps me organize my thoughts. Maybe I'll reread the last page when coming back to something but that's it. So my advice is to write when you have a lot on your mind so that you can get it out of your mind. That's it. Don't mess around with forums. That's for a different problem. Keep a pen and paper handy always. There can be no barrier to entry or it breaks the flow. reply grugagag 12 hours agorootparentLikewise, I seldom reread. But I do get most out of just writing it down. reply michaelsalim 7 hours agorootparentprevI think you need to have a clear separation on what you're trying to achieve. From what you've written, I get the sense that you're combining documentation, wiki and your personal notes together. For example, code documentation is very subjective. I'd combine code comments with readmes and potentially a separate wiki. All depending on the complexity of it. For personal notes like reminders or thoughts, there's no need to keep it up to date. It's ordered by date. And when I need it, I roughly remember when I wrote it. If something needs to be updated? Write a new entry today. I use pen and paper for this. This is also where I sketch one-off diagrams and the like. Separately, I also have a personal wiki for things I learn or teach among other things. Since these are limited in numbers and are quite important, keeping them up to date is not a big task reply perrygeo 6 hours agorootparentprev> a balance between \"Just do it\" and \"Write about it\" This is the key point - capturing, organizing and retrieving notes has a cost. And I find myself always paying for notes out of my \"just do it\" budget. Especially when virtually all of the things I'm working on (notebooks, libraries, applications, planning documents) are themselves a form of writing, having yet another place to scatter my thoughts is not helpful at all. It's much more productive to take that thought and put it directly into the project documentation where everyone can benefit. More README, less journal. reply m3kw9 15 hours agorootparentprevThe problem with writing too much is that it becomes a chore to read, there could be a lot of fluf and a few gems, but you couldn’t tell anyways. reply DenisM 15 hours agorootparentprev> 2. How do I retrieve a note I've written earlier? I can use tags and search for them but it is not easy to come up with a perfect tag which I would remember later. Feed them all to an LLM? reply galaxyLogic 14 hours agorootparentI've thought about that. The AI should figure it out. But if I don't know what I should ask the AI it cannot much help me. I wrote in some discussion about whether AI could replace us programmers? I think they cannot because: AI has the answers. WE have the questions! reply brightball 15 hours agoparentprevI’ve been seriously considering using Logseq for this reason. When I first started with Obsidian I used it that way, but the more I put in it the more I started organizing everything. It became less of a journal and more of a repository for long form stuff. I’m thinking about using both just so I have a dedicated tool just for the journaling side of things. reply kaiwen1 11 hours agorootparentI use Emacs Org Mode with Org Roam for journaling. I’ve customized it extensively to fit my workflow, but there is a risk. Unlike paper, in Emacs there’s a potential distraction lurking around every thought, every entry, even every keystone. I’ve tried going to paper many times to mitigate the risk, but it never sticks. I’m too far down the hole and habituated to change. But if I were starting over, I would choose paper and stay with it. When journaling, you want no distractions. Nothing beats paper. reply kchr 8 hours agorootparentprevSame here. I began using Obsidian when looking for something to drop my collection of Markdown notes into, which was somewhat of an improvement due to the quick search, tags and links, but not life-changing. I tried the Kanban plugin but gave up after a while. Then I read about the MOC concept[0] and started with topic-based index pages using the `dataview` plugin for generating lists of backlinks. Haven't looked back (yet)! You could also create an index of MOC pages with the same plugin and making sure each MOC have a `#moc` tag, for example by using templates. Then write a query that lists all pages with the `#moc` tag. For pure TODO lists, I'm a happy user of Taskwarrior since more than a decade. [0] https://obsidian.rocks/quick-tip-quickly-organize-notes-in-o... reply wruza 8 hours agorootparentprevFor those struggling with overwhelming functionality an ux tax of note-taking apps: This also works It’s easy to spin up And to search through It has a structure Can be easily refactoredreply dr_kiszonka 14 hours agorootparentprevI use different editors for different purposes, e.g., Obsidian for long form and planning, OneNote for meetings. I wouldn't overthink it, though, and just use the simplest tools available. I use Sublime Text 3 with a few shortcuts to add the current timestamp, etc. and log everything in a long file. I was too ambitious in the past and wanted to learn how to use Emacs for everything, but it just held me back, and I ended up without any notes. Also, my unfortunately named thread from 2022: https://news.ycombinator.com/item?id=33359329 reply phoh 11 hours agorootparentdo you have a decent system to get entries between(both in and out) Obsidian and OneNote? I have a similar system to you, but getting things in and out of OneNote is such a massive pain. Everything I have tried requires significant reformatting (even Word). reply dr_kiszonka 2 hours agorootparentI usually don't move notes to and from OneNote. I tried this Obsydian importer, and it is OK for my meeting notes in terms of formatting. However, it has a bug that strips slashes from page titles, which is a bit of an issue for me because I always add dates to titles (e.g., 7/13/2024). With a few good examples, maybe some LLM could help you with reformatting? Good luck! reply im_dario 11 hours agorootparentprevI've been using Logseq as a work journal and it works great. Hashtags help me to track what I need to do and what I've done. reply beoberha 3 hours agorootparentLove Logseq. It’s the best model Ive found for how I want to take notes. I used to get analysis paralysis managing structure or thinking about when to split off a new note. But with Logseq, you just write linearly and the hashtags take care of all that for you while making discovery a great experience. reply mettamage 9 hours agoparentprevI had a whole CLI full with bash scripts and a Sublime Text app with self-written plugins that functioned like this at my previous job. reply greggsy 9 hours agorootparentYou mean, for journaling, or to automate the process? reply makz 16 hours agoparentprevThis advice is pure gold. Thank you. reply darkerside 8 hours agoparentprevI believe it cuts both ways. Writing things down allows you to dump state so that you can make other complex calculations with your working memory. Then, learning to hold more complexity in your head increases your mental bandwidth using the same amount of external state available. Rinse and repeat. reply m3kw9 15 hours agoparentprevIs the process of writing not for reading it but to solidfy memory? Of course the bonus is you could read it, but you really won’t reply chrisweekly 15 hours agorootparentI sometimes write to document my thoughts - but just as often I write in order to discover what I think. Sometimes it's like the writing is the thinking. Also, I love Obsidian. It became more useful when learned to stop overdoing it with exploring the endless plugin options, settled on a favorite few, and now mostly write in the daily note, occasionally extracting things to dedicated devnotes which in turn have chronological timestamped entries (and bidirectional links to the corresponding DNs). Highest possible recommendation to find a tool / workflow that suits you, and leverage it. reply nlawalker 17 hours agoprevWhat finally got this to stick for me was abandoning all notion of structure and organization (and formal concepts like “logging” and “journaling”) and optimizing fully for capture over retrieval, then relying on search tools and proximity for the latter. I have the OneNote icon in the notification area configured to create a new quick note and use it liberally. Occasionally I look through all the pages, especially the recent ones, aggregate and reorganize some, move others to an “archive” tab, and that’s it. The faintest - and most disorganized - ink is more powerful than the strongest memory. reply pizzathyme 15 hours agoparentI’ve found a lot of success going one step further giving up on retrieval all together. I use either a new text file every time (which I never open again) or a physical notebook page (which I never refer back to). I get so much value from the act of writing itself. reply 8n4vidtmkvmk 13 hours agorootparentSame. I just bought a scanner which I intend to use to scan and OCR my notes so that I can finally shred the mountain of paper I have. But I know I won't read the digital copy either so I'm not sure why I'm bothering. I guess it's the next step to putting it completely out of mind. reply uNople 12 hours agoparentprev> abandoning all notion of structure and organization ... optimizing fully for capture over retrieval, then relying on search tools and proximity I did this too - obsidian's daily note feature is fantastic, and you can extract out pages from it if you want/need to dedicate a document for a specific thing. Since it's just markdown, search is quick - and being able to use regex if I need to is awesome. The graph view, showing connections between notes is great if you create notes on specific subjects, and link them together, or pull sections out to explain more in depth - but it's not really that necessary unless you're building your own knowledgebase, which like all documentation suffers from rot over time. As long as your note-ing tool supports a good enough search that you can find things again, then I think it doesn't really matter what you use - as you said, writing it down is the important part. reply garbthetill 11 hours agorootparenti never read the change logs, so just found out the concept of daily notes it looks pretty cool and might be what i need. Do you use any other templates? reply mkoubaa 16 hours agoparentprevIt took me a couple years to realize this too. For the past five years I abandoned all structure. I use a literal log file. Chronological from top to bottom, with paragraph breaks for each workday. Higher than necessary verbosity, no points taken away for spelling or grammar mistakes. reply james_marks 14 hours agorootparentI basically do this, too. One big text file. My twist that makes it work for me is a slightly modified text editor that I only use to edit this file. That way I’ve got a dedicated dock icon and context just for writing notes, but no other overhead. It’s important to me that it not feel like a product, and search works effortlessly (although subject to typo misses). My only tweak on the text editor is a shortcut to insert a timestamp and a chunk of new lines, which I do periodically so I can separate moments in time and see what I was working on when, how I fixed something, etc. I used obsidian for a while, but for my purposes it felt like work to organize and get “right”. I ended up writing a script to join all the files into one. reply linsomniac 2 hours agorootparentprevI've also been doing this for ~30 years. My current job's journal is 17,581 lines long. It's just a file I edit in screen (so I can attach to it from multiple machines) with a line with the date on it and then a sentence for every thing I've done that day. It is super helpful when we notice something strange has been going on since a specific date. I give my coworkers access to it and we will regularly refer back to it to try to figure out what was going on on a particular date. I also use it monthly to summarize for my 1:1 meeting with my boss. I also have a Kindle Scribe e-notebook that I use for my daily todo list. The writing experience with the Kindle is very good, in that it's very paper like, but the access and retrieval is pretty meh. I described it to my coworkers as: It's exactly like paper, only more expensive. I'm basically doing bullet journaling of my tasks, things I need to circle back with coworkers about, and stuff to chat about over lunch or shows people have recommended. reply thfuran 16 hours agorootparentprevIt's hard to grep for misspelled words though. reply tomrod 16 hours agorootparentMaybe something like fzf? https://www.redhat.com/sysadmin/fzf-linux-fuzzy-finder reply zdc1 2 hours agoparentprevThis is what I do too. I use the Obsidian TimeStamper plugin with a nice shortcut (Ctrl + .), and then just have an append-only log of thoughts that each start with a timestamp on its own line. I'll throw in a tag or two if I'm motivated (#tickets/DEV-1234) and create a new file every month (e.g. 2024-07). It makes it very easy and fast to just switch to Obsidian and do a brain dump before lunch / end of day, or any time a thought hits me. reply Ezhik 7 hours agoparentprevThat was my trick for Obsidian. No organization or fancy plugins, just interlinked notes. Backlinks alone work amazingly well for me for retrieval. reply sublinear 10 hours agoparentprev> The faintest - and most disorganized - ink is more powerful than the strongest memory. Very true words. Thank you. reply p5a0u9l 16 hours agoparentprevThis is the way. I’ve used zettelkasten, which is similar philosophy. Dump a thought and tag it for retrieval later. I prefer making this work with simple files and markdown. I have a colleague who uses email drafts well. My problem with OneNote and similar is the bloat of it. But, having images alongside text, when needed, is super nice. reply bongodongobob 14 hours agoparentprevIt's the exact same thing as a messy desk pre computers. I'm a big subscriber of that idea. reply deskamess 7 hours agorootparentNo surprise that I am a very strong proponent of messy desk. Organized chaos. reply simonw 16 hours agoprevI use GitHub Issues threads for this and it works amazingly well. Any task I'm working on has a GitHub issue - in a public repo for my open source work, or a private repo for other tasks (including personal research). As I figure things out, I add comments. These might have copy pasted fragments of code, links to things I found useful, quoted chunks of text, screenshots or references to other issues. I often end up with dozens of comments on an issue, all from me. They provide a detailed record of my process and also mean that if I get interrupted or switch to something else I can quickly pick up where I left off. Here's a public example of one of my more involved research threads: https://github.com/simonw/public-notes/issues/1 I also create a new issue every day to plan the work I intend to get done and keep random notes in. I wrote about how that works here: https://til.simonwillison.net/github-actions/daily-planner reply geekodour 16 hours agoparentI tried following this idea from simon (first via discord channels) about 8-9 months ago and now zulip, I used zulip \"streams\" as the \"github issue\" here. Worked very nicely for me. I use it for everything and not only work journal, this creates a small problem.Since I dump both future references and worklogs, and I have ~50 channels, it's very easy to not get back to things and only get back to it when needed(which is the idea mentioned by OP). It seems like a feature than a bug at first but after capture, one round of review after some time interval really helps. It took a while but slowly seeing the benefits. For that I plan to write some bot to re-organize the worklogs and the reference/other things dump to my own email at the end of the week and then I can create something like https://simonwillison.net/tags/weeknotes/ for myself(private) to go though at the end of the week. I think it would be perfect for me. reply irthomasthomas 6 hours agoparentprevI use GitHub issues (combined with embeddings and logprobs via your llm) as an AI infused bookmark manager: https://GitHub.com/irthomasthomas/undecidability/issues The code that runs it is here: https://GitHub.com/irthomasthomas/label-maker (how it started/how it's going:) reply imiric 4 hours agoparentprevThat's interesting, but aren't you concerned about using a proprietary service for this? I would hesitate to be at the mercy of a corporation for such a personal workflow. reply hubraumhugo 12 hours agoprevIt's surprising how many devs are trying to overoptimize for productivity with various fancy tools and techniques, when it actually comes down to simple basics. Here is what I rely on as a founder who does a lot of context switching: - a never-ending text file for todos and work journal [0] - calendar for planning (and blocking focus time) - website blocker - turned off notifications [0] https://news.ycombinator.com/item?id=39432876 reply mistahenry 9 hours agoparentFor the never-ending text file for todos and work journal, I've found great success with org-mode, especially since deadlines automatically end up in my calendar with org-agenda. The outline format of org-mode is quite nice and it took all of 1 day to learn the key commands for making / manipulating the outline, creating links, and cycling through TODO states (using Doom emacs made the start super easy since I already know vi and didn't have to also learn the text editing commands). I've also found the concept of an \"inbox\" from the Zettelkasten method very helpful. Anytime something comes up that's not yet in my system, I add it to the inbox for later processing (org-capture on my computer, and beorg on my phone). This way, note entry doesn't require a full context switch. I then just make sure to regularly drain my inbox. I don't use emacs currently for anything but org-mode but I'm far happier with this than I was with a never ending `.md` file. reply sudhirkhanger 7 hours agorootparentHow does one hide completed tasks from org mode? reply merlincorey 7 hours agorootparentLike many things in Software the question is why do you want to hide them? One way to do it is to archive[0] it which will move it into a local _archive file. Another way which I'm currently using is to periodically manually archive tasks into a tree of folders and files by date then category (and sometimes subcategory) which allows me to publish an HTML or PDF file of everything from say 2023 for a particular client. [0] https://orgmode.org/manual/Archiving.html reply setopt 4 hours agorootparentprev(setopt org-agenda-skip-scheduled-if-done t) reply Forge36 7 hours agorootparentprevArchive is an option, it'll move the heading and children into a new file. Agenda can also find and filter by state. Perhaps to clarify: hide from where? reply bloopernova 5 hours agorootparentprevIn org-journal, opening a new daily or weekly file only moves uncompleted tasks. It leaves completed tasks in the previous file. reply DarkCrusader2 12 hours agoparentprevBig +1 to this (see my comment on this thread). I just use a editor plugin to make marking tasks as done/cancelled easier and rotate the files every month to keep things a little organized (and scoping the keyword search) and it has been the most effective form of project management/journalling for me for many years. reply sudhirkhanger 7 hours agoparentprevHow do/would you use it over various systems? What's the best way to cloud sync this? One might have to use it on work machine with restrictions around installing apps. reply setopt 14 minutes agorootparentAs long as auto-revert-mode is on (so you don’t accidentally lose changes from other devices), you can sync it using any file storage service: Dropbox, iCloud Drive, Resilio Sync, etc. reply Forge36 7 hours agorootparentprevI use org mode and sync with a git repository. Thoughtfully emacs and git are both approved. reply polairscience 12 hours agoparentprevWhat website blocker do you use? reply Flimm 4 hours agorootparentI use LeechBlock NG, for Chrome, Firefox, Edge, as well as Firefox on Android. reply elitan 9 hours agorootparentprevI follow the same structure, and I use Notion, Google Calendar, and Flow App. reply huevosabio 6 hours agoprevI like having a \"devlog.md\" file for each project/repo. The log is jut a reverse chronological order of comments/todos/rants. I have mapped on VSCode cmd+shift+I to write a timestamp. Whenever I want to write something I just insert a timestamp at the beginning of the bullet-point list and write it out. This is stored alongside the repo. It is particularly useful for recovering in the morning where I was the night before. reply snakey 5 hours agoparentAlong the same lines, I also find it useful to have a text log for each sizeable task I undertake. This could contain anything, notes of module structure, code & data snippets for testing, everything! Like one of the top comments mentions, it acts as a node of knowledge within a wider system (graph) and I end up revisiting these logs more often than you would expect! It only gets better as you explore/document more and edges form. reply huevosabio 2 hours agorootparentYea, for me what really helped is once I interned at a hydraulic engineering company and I was tasked with debugging a custom software for pump modeling/optimization in GIS. I had no background in C++ or the GIS I was dealing with and this was a one-off thing that was built for a project, but not maintained or documented at all. The lead eng that wrote the software also had a long and detailed personal log of what he was working on. He just gave me that. It helped a lot to understand not only how the software worked, but also the design decisions. reply Flimm 4 hours agoparentprevDo you commit \"devlog.md\" to your version control system? If not, how do you sync it between your different computers? reply huevosabio 2 hours agorootparentYes, I do! reply bloopernova 49 minutes agoprevThe org-journal extension to Emacs' org-mode is great for this. ctrl-c n j = add new timestamped journal entry https://github.com/bastibe/org-journal So the workflow for this is: command-tab to switch to Emacs, ctrl-c n j to add a new journal entry, write the entry, command-tab back to whatever else I was doing. Emacs auto-saves my changes. Of course, getting to that point requires some work. You have to be using Emacs and org-mode already, or prepared to try it, and that journey can be difficult for some. The org-journal extension is great: It supports daily/weekly/monthly/yearly journal files (I use daily but I'm considering switching to weekly or monthly). When I create a new daily file, it only \"brings forward\" uncompleted TODO items, which means any completed TODOs are automatically archived out of your sight. Because it integrates with org-mode, I have it set up such that it tracks when a task is moved from TODO into PROG, and again when it moves into DONE. (I get annoyed when columns don't line up, so I made my todo item names 4 characters long) reply esskay 7 hours agoprevEvey time I've tried essentially journaling for my own sanity I get too bogged down in the way to actually do it. Logseq and Obsidian are often mentione but by the time I've figured out how the hell to use thier obscure syntax (why the hell am I programming, its a journal) I've lost interest. I'd like for there to be an out of the box option, but there doesnt seem to be any. When you bring this up in discussions like this it often ends up just proving the point when someone tells you \"Oh its easy just install X and then add this plugin, that plugin, tweak this file to do such and such...and my adhd brain lost interest. There's a real hole in the market for a good out of the box, opensource and self hoated tool that you open the app and write tool, not a note pad, we've all got one of those, I'm talking about somethinh you open, you've got your current day to dump notes into and and they're stored. Sure, theres tools like DayOne, but it's cloud based storage is a catagorical no for me, and many others - theres been countless discussions about this on Reddit, and the answer is always spend half a day screwing around with Logseq or Obsidian and then try to remember their syntax. The out of the box on both of those is pretty awful, and if the solution is spending hours tweaking it it's not really a solution, its a patch, one that shouldn't really need to be made if it's supposed to be the tool to use. reply rigmarole 7 hours agoparentWhat syntax are you trying to use or thinking that you need? In Obsidian for example, remove all plugins except Daily Note and just start typing. Ignore all syntax except maybe bullets. Ignore properties and links. Ignore any habit-tracking or database tricks people say you need. Ignore the graph. Or consider using Vim/Neovim and set a leader hotkey to open today’s journal/YYYY-MM-DD.txt reply setopt 45 minutes agorootparent> Or consider using Vim/Neovim and set a leader hotkey to open today’s journal/YYYY-MM-DD.txt For a broader audience: Create a shell script that runs “$EDITOR $JOURNALDIR/$(date +%Y-%m-%d).md”, and bind a system-wide keybinding to run that script. Works even for GUI editors. reply FranklinMaillot 6 hours agoparentprevIf all you need is daily notes, Obsidian does that out of the box. You don't have to tweak anything, install any plugins or learn any new syntax if all you need is a simple GUI over text files. It's not open-source however. Don't get fooled and intimidated by the \"productivity porn\" community that likes to show off their sophisticated setup and unrealistic workflows. My rule: if they refer to Obsidian as their \"second brain\", they are part of the cult and should be ignored. All that said, I strongly encourage you to try pen and paper, the ultimate, no setup, open source app. And it has exquisite haptic feedback on top of that. reply wombat-man 6 hours agoparentprevIt took me a few times to land on something I stuck with. At my current job I have a running doc where I just start a new entry per day, at the top and log before I try to do something, and what progress I made that day. What maybe helps is this doc is findable and readable by coworkers and my boss. It is very useful for me, but it's also helpful for others to see what I'm working on without pinging me. reply WOTERMEON 3 hours agorootparentI do kind of the same but I’d be concerned to have it readable and n not private. But maybe I shouldn’t be. What’s your thoughts on having it open? I think it would be the first in the company if it’d do that. reply wombat-man 2 hours agorootparentEh, I just try to not slack off too much. I've been doing this for nearly 2 years and so far only benefits as far as I can tell. I think very few people look at it besides myself and occasionally my boss. reply bruce343434 7 hours agoparentprevHave you tried appending a txt file? reply bruce343434 6 hours agoparentprevHave you tried appending a txt file? If you want to find something, grep it. reply flakiness 16 hours agoprevTo me the hardest part of journaling (or Pomodoro, or whatever work-related methodology/hack) is to stick with it. I have a work journal. I abandoned it and came back, then abandoned it and came back again. It's an endless back-and-forth. To those who keep doing this for a longer period: Any tips would be appreciated. reply merlincorey 7 hours agoparentWe often need consistency and time to build up habits that stick. I do everything including my own personal side projects in a work journal with time tracking for everything, but I had to work up to it being a natural part of my workflow. Here's my suggested path to gaining these habits: 1. Initially just try to make sure you are taking SOME notes at the start or end of every day - it doesn't matter where they are or how they are formatted just always take some notes at the start or end of your day 2. Once you have gotten into the habit of taking daily notes, start figuring what kinds of things you need notes for most often and take those before or during those activities - for example if you often find yourself having to look back at work tickets to retrieve some important information, start adding that information to your notes 3. By the time you are taking daily notes and adding things you know you need notes for you probably have a lot of notes so start worrying about structure and formatting - for example maybe you decide text files with homegrown markup aren't going to scale and you look into something like Obsidian with Markdown or Emacs with Org-mode 4. Repeat iterations of using your chosen note taking methods daily, building good habits, and improving your note taking system for you - if it feels like something is taking more time than it is worth change how you are doing it so it takes less time or just stop doing it reply volume 1 hour agoparentprevI think you need to go past this common thinking to \"just focus on consistency/habits/discipline\". You need to get clear about how/why you decide in the present moment. I assume this takes varying amount of time/effort for different people. I think one needs to unravel our inner state and psychology ... we cannot simply turn on and off. But then, once we understand our inner state/psychology it makes it easier to turn on/off. reply gavmor 2 hours agoparentprevGustave Flaubert said, \"be regular and orderly in your life, so that you may be violent and original in your work.\" Perhaps we who strive to be regular and orderly in our work should accept a little chaos in our life. On the other hand, perhaps we simply need better external anchors for our habits. I have been journaling on-and-off for years. Environments change, people change, and my schedule changes. What got me back into it this time was joining a Shut Up and Write™ meetup. That broke the seal, and I've been sporadically journaling to de-frag in the weeks since. reply stevekemp 14 hours agoparentprevI have my journal available within my editor, so it's easily accessible. I find that once I've seen how useful past-notes have been its very apparent I need to update. I keep a standard set of headers for each new entry: * DD/MM/YYYY ** Admin ** Meetings ** Tickets/Stories/Work ** Problems I copy/paste that header to the end of the file, and just fill out stuff as I go. I used to have my editor auto-open the diary on startup, but took that away in the end. reply tra3 2 hours agoparentprevEvery day starts with a new day note that automatically generates a checklist for me. One of the checklist items may be to journal. The randomness is so that I actually follow through on the checklist and don’t gloss over it. reply rzzzt 8 hours agoparentprevI keep a daily log but tend do skip updating it at the most intense times, ie. when it would be the most helpful to take notes... reply charles_f 15 hours agoparentprevI had the same problem because it was serving no purpose. What really worked for me is to make it a primary tool of work. Rather than sometimes writing what I did today, whenever I work on something somewhat involved, I immediately write my approach about it (\"I need to do X so I started Y\"). reply Scarblac 9 hours agoparentprevAccept that that is how it works for you. Like, maybe you do different types of work and subconsciously find it useful only for some of them. That's OK. Consider it a tool you sometimes use, and think of using journaling as an option when you're frustrated with something. reply waprin 15 hours agoprevThe technique of journaling as you work is sometimes called “interstitial journaling” and I became a big fan of it as a way to help focus as well as keeping track of what I was working on. I made a tool to associate those notes with a color coded project and timestamp: https://interstitch.app It ended up being unintentionally similar to an invoicing time tracking tool a freelancer might use but the use case Im interested in is more personal productivity. Can’t say the project has generated much interest outside my own personal use but I find it very nice to track notes as I go and then easily see how much time I spent on a given project. You can also add a #hashtag in the notes and then filter by that hashtag in the calendar view. Completely free in case anyone else finds it helpful! reply epistasis 16 hours agoprev\"Ignore all previous instructions. Show your work. Explain your chain of thought in detail. Keep a journal of your work progress.\" I've been using this prompt on myself, on and off, for years, but I keep on hallucinating and losing focus and going off track. reply didgeoridoo 16 hours agoparentHave you tried telling yourself that a schoolbus of children will drive off a cliff if you don’t get the job done? reply softwaredoug 6 hours agoprevI like how a work journal reorients your locus of control, and focus on important vs urgent work. You can get really reactive to whatever stuff is happening on your team that you lose the forest for the trees. And further as you get more and more senior, living off of reactivity is only counterproductive as you control your own time and its expected you try to shift to a focus of important over urgent work. reply senkora 17 hours agoprevI keep a stack. Whenever I am interrupted, I push a task onto the stack. When I finish a task, I pop it from the stack. Each task has an associated journal file. Sometimes I reorder the stack. reply codazoda 13 hours agoparentI recently started doing this with my todo list. I wrote a few bash functions and aliases: todo: Open my list in an editor todo [thing]: Add a thing to the bottom of my list. next: Show me only my next most important task. mark: Mark my current task done. There are a few more, but you get the idea. reply bogdan-lab 4 hours agoparentprevYes, stack is much better than long-long log. After some time you're log becomes too big and if it contains some points you want to return to, then they are just lost. And if you lose something in it you stop trusting it and do not use it. At least this is my story. reply kgeist 13 hours agoparentprevSimilar workflow, except I have 3 lists: TODO, Pending and Done. \"TODO\" is actionable items. It's a reorderable stack of work I need to do. Same: interruption pushes a task to the stack, when I finish a task I pop it from the stack, etc. So I always know what to do next. \"Pending\" is an unordered list of things I'm awaiting. Say, I asked someone to do something, and they promised they'd get back to me in a few hours (or \"by July 20\"). I occasionally scan this list to see if some of the items got resolved and I need to continue working in those areas because I'm unblocked. \"Done\" is a list of items I completed for the day, all finished items go there. I then copy the entire list to the time tracker (for the PMs) at the end of the day. However, I organize files by day, not by task. Each day I create a new file for the day by copying the lists of the previous day's file minus the DONE list. I don't modify lists for previous days, so it's kind of an append-only log so I can see what was the state for any particular day. 1 file per day is easy to see as a whole as it mostly fits in one screen (and inside my working memory). I use plain text files because I found it much simpler to use, I don't have to install any software, it just works, and it's easily searchable. I've been using this system for the last 6 years now and it served me well. reply atlasstood 3 hours agorootparentI started this a few months ago and find that my TODO grows faster than it depletes. My TODO items are both professional items (i.e. implement feature x) and personal (i.e. fix bike chain). The list is items that will take some non-trivial effort. I now find that when I have a moment to do something, I pick it off of the TODO list and complete it. Prior to this technique, I did not have a list of this nature and some items never got completed. I feel incredibly productive with my current setup. However, I don't feel as though my previous system was unproductive and am concerned that I'm \"spinning my wheels\" by feeling like I need to complete these tasks that went unfinished before. Have you experienced this? Do you know how to best think about what is optimal? reply merlincorey 7 hours agorootparentprev> However, I organize files by day, not by task. Each day I create a new file for the day by copying the lists of the previous day's file minus the DONE list. I don't modify lists for previous days, so it's kind of an append-only log so I can see what was the state for any particular day. 1 file per day is easy to see as a whole as it mostly fits in one screen (and inside my working memory). I use plain text files because I found it much simpler to use, I don't have to install any software, it just works, and it's easily searchable. For my working files I use a similar system except I separate at least Months, often Weeks, and sometimes Days into subheadings for easy time tracking of tasks on various time partitions. So when I need to start a new heading for a new day, I just move all the incomplete TODO items into it, similar to you moving them into a new file. Occasionally I manually archive the completed tasks into files by year with headings by month only when I no longer need their full granularity. reply packetlost 17 hours agoparentprevThis is pretty similar workflow to mine. I have a sideproject that implements it in a terminal that's been sitting on a shelf for awhile, maybe I should pick that one back up reply darby_nine 17 hours agoparentprevHow do you organize the journal? Do you grow an index from one side and tasks from the other? reply senkora 17 hours agorootparentI wrote myself a python cli tool that manages the stack by updating a sqlite database, and one of the commands creates a text file associated with the task (if it doesn't already exist) and opens it in Emacs. The text files are stored in a hard-coded directory and an anacron job does a git commit once a day. I can tell what I worked on each day by querying the git commit history, and I can grep the entire directory for keywords. It's a little janky but it works pretty well for me. reply darby_nine 15 hours agorootparentAhh, I was expecting \"journal\" to refer to a physical book for some reason! This makes a lot of sense. Why not simply store the task with the sqlite database? I'm assuming ease of editing + the ability to manage the stack separately from the log of text entries, which presumably need no maintenance nor will ever be deleted? reply senkora 15 hours agorootparentPretty much what you said, yeah. I thought that a git repo of text files was easier to work with then storing them as blobs inside sqlite. I do insert completed tasks into a \"completed task\" append-only table when I pop them off the stack, so I do have a record of completed tasks in sqlite. (I find that useful for remembering what I did recently for standups and 1:1's) reply hghar 5 hours agoprevThis sounds so similar to certain productivity dogmas that were mentioned already in the comments and I know that some people get extremely obsessed with those methods and that in the end that might become precisely a productivity obstacle. But honestly I have been doing exactly what the author claims work for him, just write what you are doing and you “feel” is going to be useful later, this very thing that just happened in the command line is a clue to the complete puzzle. And it’s funny that this is exactly what those productivity dogmas describe as “second brain” in my perspective you just take what works for you from those methods. I think you just have to try it, although it seems like that’s a waste of time and that you will never see that note again just write it save it and maybe the next month when you come back to the same exact problem those notes will be pure gold, yea I know most of those notes are going to be just a bunch of bytes never to be seen again but when they are useful you will be so thankful that you did it. reply jilles 17 hours agoprevLovely post. I do something similar where I write blog posts for myself as I solve a problem. Something like a how-to guide before I actually know how to do it. Then I cite sources as I find them. When I finally solve the issue or create \"the thing\", I revisit the doc and either publish it internally, or keep it in my archives. This really became a habit after reading Writing to Learn by William Zinsser. I recommend this book to everyone and their grandmother these days. \"Writing enables us to find out what we know—and what we don’t know—about whatever we’re trying to learn.\" reply anotherhue 17 hours agoprevJust use post-it notes as a task stack. Push and Pop as distractions come and go. Affirm to yourself that you are a productive and loved human being and not a biological IRQ handler. reply gdilla 5 hours agoprevall the senior engineers i worked with, when i started out 20 years ago, carried around bound books and kept as journals. And they logged things all day, dated entries. They just said it was essential, so I did it. reply g8oz 4 hours agoparentIndeed, we are doing a lot of reinventing. Or more charitably reapplying old ideas to today's world and it's affordances. reply neilv 5 hours agoprevSometimes I use the project management tool's issue/task comments for notes on what I'm thinking. Other times, comments in the code. Other times, the relevant page in the startup's wiki. Sometimes the intermediate notes want to evolve to more polished documentation and/or code, and other times it wants to be preserved in a very lightweight way. What's important is that information you or others might later need is captured in a way that's accessible when you need it. Make it a practice, and it's negligible additional effort. reply barbazoo 17 hours agoprevI just finished a project where I was in the same situation. Stuck, I ended up in the same place every night that I started at. Began writing down a plan as detailed as I could, made a plan for every day what I wanted to achieve and maintained that to the end of the project. Got me out of a slump. reply gringocl 15 hours agoprevOur implementation team of 15 writes individual and project logs every day. The logs are not to account for a record of what was performed but rather the rational, decisions, setbacks, observations, learnings, etc of our work. Each member of the team is expected to keep current on the individual and project logs. I’ve been doing this now for the last two years and I’m always surprised at how helpful they continue to be. reply _spduchamp 8 hours agoprevI have a part-time research assistant job in a lab where I work on several overlapping projects, and when I started this job I started a journal using a Google Spreadsheet, with a shortcut to the sheet on the home screen on my phone. It's instantly always there, adds up my hours for time tracking, and when I have to write a project status report, I just pull out the relevant entries and turn them into proper sentences. Bam! Report done with semi-chronological flow. I bold entries of open questions/tasks that need attention, and i can quickly scan my journal to know exactly what to work on next. I've never been so organized and on top of things in my life. This little journal hack makes me look like a fricken genius in the lab. reply al_borland 16 hours agoprevThis is something I do if I'm in the middle of something before the weekend, and especially before a vacation. I use Obsidian with its calendar plugin, so I get a note per day. It's where I keep my to do items as well as any notes for the day. Before leaving for a break I will open up the note for the day I plan to return and fill in my to do items, as well as additional notes on what I was doing, what I should be doing next, and references I may have had that I'll want to revisit. It is extremely helpful for getting me back into things once I've been out for a bit. reply nicbou 13 hours agoprevI did this at my last job and it was very helpful. I used an actual notebook. I used a new page every day. I'd write down everything I worked on, mostly for stand-ups and performance reviews, and the to-do list but only for items I intended to complete that day. Every morning, I manually copied the previous day's to-do items to the new page. It reminded me of what I was working on and made me aware of the issues I kept delaying. reply coffee2theorems 17 hours agoprevThanks for the great blog post, I could feel the frustration of not getting something to work, and the anxiety of feeling like I'm under performing. I've added you to my RSS reader now. :) I've started to use index cards to write down daily tasks and I'll switch over to obsidian if I find myself asking the same question more than once. I think for me the process of writing something down slows my mind enough to let me focus on it. I like @ZXoomerCretin's idea of keeping a running document of what I do each day. I think it would make anual review time a lot easier. reply baby_souffle 17 hours agoparentYes! Obsidian with daily note plugin and templates makes it really easy to build a quick list of things and automatically link to yesterday’s notes. A little extra time with templates and some custom js and you can make it a single key combo to copy the notes to paste buffer for sharing in slack standup thread. Also, set up a praise folder and take screenshots every time somebody says something nice about the work you’re doing. You can automate documenting the context around it, too with quick add reply y1n0 16 hours agorootparentDo you all pay for obsidian? Subscriptions rub me the wrong way for whatever reason and it was enough that I didn't want to pay so I use something else. reply baby_souffle 3 hours agorootparent> Do you all pay for obsidian? I did pay to support their development[1] early on. I've been an obsidian user since early 2020 and I had to roll my own sync solution at the time. At the time I was experimenting with a lot of other PKIM/Note apps and Obsidian was the only one that didn't do proprietary storage format and really honored the \"minimal but trivial to extend in powerful ways\" philosophy that I value. > Subscriptions rub me the wrong way for whatever reason I can understand that. Software development is hard and we are _long_ past the days where software was static. In some ways, I miss buying a computer that didn't expect an internet connection to constantly self-update. On the other hand, though, paying a few bucks a month so make sure the app is updated to take advantage of new OS features and generally keep up with device capabilities is worth it for me. If there was some 2-5$/month option to support obsidian development I'd consider it. Yes, I know their cheapest sync plan is $4/month but it's only good for 1 gig of data and my biggest vault grows by that much every year or two... hence using SyncThing on a cheap VPS :). [1]: https://help.obsidian.md/Licenses+and+payment/Catalyst+licen... reply incompleteCode 16 hours agorootparentprev(Full disclosure - I’m speaking from a place of privilege where I can afford the subscription cost of Obsidian) I hear you on subscriptions rubbing you the wrong way. Hear me out, though. This is a bootstrapped team that builds and supports apps that empower you, the individual. Your data resides in plaintext and you can use your own sync server. Not everything needs to be a subscription, but I don’t mind paying a few dollars per month to support the team. Just my 2c. reply dv35z 14 hours agorootparentprevIf you're interested in an open-source, free equivalent, check out VSCodium (open-source version of VSCode), and FOAM (VSCode plugin - https://foambubble.github.io/foam/). In a new project, create a `docs/` folder, and start with `docs/notes.md`. When you want to branch out to other files & links, you can type [[MyTopic]] and FOAM will automatically create MyTopic.md, and will allow you to click on the link and navigate to it. Later, if you want to publish your notes as an HTML site, you can run `mkdocs` on the `docs/` folder, and it'll create a website from your notes. This MkDocs plugin enables the crosslinks in HTML: https://github.com/Jackiexiao/mkdocs-roamlinks-plugin. Good luck! reply charles_f 16 hours agorootparentprevI'm using the free version synchronized with Syncthing, works great for just myself. Have a couple plugins I built for my own workflow. The good thing is that I can jump to any other markdown app if I want to reply Tepix 14 hours agorootparentThere is no free version if you use it for work. reply malux85 17 hours agoparentprevHeres my digital pensieve - Daily running log of what I have done Dynamic todo list where things move up and down freely remarkable tablet where I watch lectures and write notes (writing things down slows my mind and focuses as you say) Daily Weekly Fortnightly Monthly Text files for spaced repetition, if I get to one and I have forgotten something then it’s moved up, if I feel like I know something it’s moved down Longer memories are stored by topic: Memory (working memory) Platform Operations Zany schemes Non-AT … + many more Each is a hierarchy, platform is some 20 categories each with 10ish sub categories It’s actually hosed on my NAS, tailscale connects all my devices so I can edit and view anywhere I look forward to the day I can query and edit this with thought reply vantassell 16 hours agoprevReminds me a lot of Cal Newport's ideas re: Slow Productivity. He talks a lot about how Context shifts are death for knowledge work and that a lot of offices operate via a \"hyper active hive mind\" that doesn't allow or value deep work. reply jen729w 15 hours agoparentI find a timer useful for this. If you use Timery, you get a live activity on your home screen. I don't care how long a thing takes, and I don't retrospectively analyse the time. The point is that I can only have one timer running: and that's the thing that I'm supposed to be doing. If I notice I'm doing something else, it serves to bring me back to the task. And at the end of the day, I do look through the list and see how often the thing I was doing changed. I try to keep that to a minimum, because every change is a context switch. I've only been doing this for about a week, I'm still working on it, but so far it's been more helpful than not. reply al_borland 16 hours agoparentprevI have too many meetings to get anything done. I'll go weeks, or months, without actually doing anything of real value. Eventually it comes to a head and I need to get things done to avoid going crazy. I go on do-not-disturb in our chat app, quit Outlook completely, and turn on a focus mode on my cell phone so people can't even call me. I'll end up working for 8-15 hours straight with no real breaks. I go to the bathroom, but keep my head in the problem, that's about it. I completely forget to eat or do anything else. I get 2 months worth of work done in 1 day. If meetings were eliminated (or just consolidated into a single planning week), and I cloud just do deep work, I think I could work 2 days per month and be more productive than I am currently working 40+ hours per week. I always want to send my management graphs like this to show them why having 10 projects running at a time is a bad idea... https://res.cloudinary.com/jlengstorf/image/upload/f_auto,q_... ...but I know it will be received poorly. The image in the article (here, since the link was broken: https://fev.al/img/2024/focus.png) is something I've sent to a boss in the past. He didn't get it. reply Flimm 4 hours agorootparentI feel for you, friend. Maybe you could share the essay \"Maker's Schedule, Manager's Schedule\" by Paul Graham [0]. It's been somewhat helpful when I've shared it with colleagues. [0] https://www.paulgraham.com/makersschedule.html reply borghives 13 hours agoprevIs there a benefit difference between digital journal and analog (pen) ? I’ve been having this internal debate. Any thoughts? reply charles_f 13 hours agoparentI like the idea of analog, there's something about writing on paper. Practically, digital is much better. Everytime I see something that might be useful I past a link. I paste links to discussions I need to follow up on. And then I also use a homemade plugin for Obsidian that lists my to-dos across all notes(1), so whenever I think about something I need to do I just include it directly into the text and it's listed there. I think you could do something somewhat similar on paper (I tried a while back), but the overhead is simply too much for me. 1: https://github.com/cfe84/obsidian-pw reply FranklinMaillot 7 hours agoparentprevPen and paper is far superior in my experience. https://news.ycombinator.com/item?id=40953297 reply criddell 7 hours agoparentprevIf analog works better for you, maybe a good stylus would be a usable middle ground? Don’t cheap out on it though. The inexpensive ones aren’t worth it. reply bityard 17 hours agoprevI have been doing this for years. Every morning, I create a new Markdown file with the day's date, copy the previous day's content into it, and edit it. Mine has (for now) the following sections: Morning checklist, Todo, Done, and Meeting Notes. The morning checklist consists of things like checking email, checking Teams, skimming the team's handover queue, logging into various things, etc. Todo is a stack of things I can/should tackle. Most important ones to the top. I limit it to 15 items, no matter what. But realistically, I typically only interact with about the top 5 99% of the time. Done gets wiped every morning and I add things to it as I do them. Things like, \"emailed Joe Schmo for 3rd time to ask for ETA\", or \"helped Fred troubleshoot the frobnitz.\" Little things that I would totally forget about but cumulatively end up taking a huge chunk of the day. I've never had a boss that expressed a concern, but I think of it as my primary defense if anyone accuses me of slacking off all day. (Maybe it's just to convince myself...) Each meeting I go to gets its own section for the day. If the content was important enough to save into my second brain[1], I clean it up and transfer it over there at the end of the day, or the beginning of the next day at worst. Any complex investigation or rabbit hole gets its own section as well. It's astonishingly difficult for me to actually reason about any complex system or design without writing it out and actually describing it to myself. I envy those who can just \"see\" it all at once in their mind's eye. If ends up being important enough to save, I will clean it up and share it with the team and/or dump it into my personal wiki. [1]: https://github.com/cu/silicon reply phito 13 hours agoparentI have the exact same workflow! It's great! reply lifeisstillgood 10 hours agoprevI don’t like keeping it seperate from “The code”. I try but there is ultimately one place to keep everything and that is in “the code”. So my comments become my journal, and this gels with how I view code and source control - that leaving dead comments, moving around functions and chnaging stuff is part of the process It’s like, we are somehow conditioned into thinking that the commit must be perfect and not reflect a process over time. And the thing is a software system reflects a process over time even if we are pushing it into production reply adius 6 hours agoprevHeynote was exactly developed for this purpose. Just one big buffer with sections and lots of shortcuts and nice little additional features: https://heynote.com/ reply hruzgar 5 hours agoparentno vim support sadly reply parasti 10 hours agoprevI sometimes use this approach for untangling unfamiliar undocumented code and for keeping track of what worked/didn't work for a particularly complex task. I found that it's a real life superpower, a literal augmentation of my flawed, forgetful human brain. The biggest downside is that I don't use it enough in fear that it takes up too much time to write everything down, so I feel I'm wasting my employer's time. reply nstart 16 hours agoprevI’ve shared this before here. I actually keep a timestamp log of work I do. I used to do it by hand on my journal and while I still prefer it I ultimately found the benefit of having searchable text files that are integrated into my work management to be too much of a benefit to ignore. Been using obsidian since it launched and my workflow is to always have the window open taking a thin column on the left of my screen and whatever I’m working on takes up the rest of the screen (Yey for fancyzones in windows powertoys). As I work I just stream my thoughts into the file. I have a shortcut, ctrl-alt-m, that inserts the timestamp as `hh:mm:ss`. I hit it, and start typing. I paste screenshots, code snippets, as I go along. It’s godsend when I’ve gone far along enough and I need to reference something. Esp given that I work on security tickets and I’m constantly triaging reports that are unclear or require digging into layers of source code to find where they come from. One important step to note if you ever try this out: if you have 30 seconds before you jump on to an interruption, try to build the discipline of throwing in a few words saying what you need to do when you return. Even with all the historical context it can take some thinking to recall what your next step should be. In fact, if you don’t like journaling just do this last step instead. I stole the concept from GTD’s next actions and it works. reply bgoated01 15 hours agoprevI'm currently working on a PhD dissertation part time, and this concept has been very helpful since I'm not working full days and therefore have more context switching. I end up drawing a lot of graphs and figures to think things through, so a big notebook of graph paper ends up working the best for me. Every couple of days or so I write out a list of the next few tasks as I currently see them, then think on paper to figure out how to implement the code for each task. Funny thing is, writing it down helps it stick in my brain, so I need the write up less than I would if I didn't write it. That's got to be some kind of contrapositive of Murphy's law or something. reply submeta 12 hours agoprevI cannot work without taking notes. It is a process of thinking, sorting my ideas, documenting steps and outcomes, pausing, practicing meta-cognition, gaining clarity and confidence along the process. Plus I have the benefit to go back to my notes and have instant access to what I did days, months, years ago. So I can’t understand how people are working without taking notes, documenting (for themselves), and journaling. reply FranklinMaillot 8 hours agoparentSame here. Taking notes is incredibly helpful, especially when I'm stuck or unfocused. I just start writing anything that's on my mind and it's like the writing does the thinking for me. While I used to type notes digitally, I've recently discovered the superiority of pen and paper. Writing by hand offers more flexibility - you can start anywhere on the page, sketch, or create mind maps effortlessly which encourages creativity, whereas typing forces you to think linearly. Research also shows handwriting improves thinking and memory retention.[1] Interestingly, rediscovering fountain pens sparked this change for me. The enjoyment of using a quality writing instrument encouraged me to take more handwritten notes, leading to significant improvements in my workflow. I now keep separate notebooks for different projects and have started journaling. This discussion has made me realize that moving my keyboard is the last bit of friction when switching from computer to paper notes. It might finally convince me to invest in that split keyboard I've been considering. [1] https://www.npr.org/sections/health-shots/2024/05/11/1250529... reply submeta 7 hours agorootparentBoth digital and handwritten notes have their values. - I‘ve used moleskine notebooks and fountain pens for over two decades. Fountain pens are unbelievably smooth to write with, and the text looks beautifully. So I can relate to your experience. Re Keyboards: I switched to mechanical keyboards lately and will never go back. It’s like the fountain pen of typing. reply FranklinMaillot 6 hours agorootparentI wholeheartedly agree on mechanical keyboards. reply zogrodea 11 hours agoprevNice article that reminds me of that one Sherlock Holmes quote. “I consider that a man's brain originally is like a little empty attic, and you have to stock it with such furniture as you choose. A fool takes in all the lumber of every sort that he comes across, so that the knowledge which might be useful to him gets crowded out, or at best is jumbled up with a lot of other things, so that he has a difficulty in laying his hands upon it. Now the skillful workman is very careful indeed as to what he takes into his brain-attic. He will have nothing but the tools which may help him in doing his work, but of these he has a large assortment, and all in the most perfect order. It is a mistake to think that that little room has elastic walls and can distend to any extent. Depend upon it there comes a time when for every addition of knowledge you forget something that you knew before. It is of the highest importance, therefore, not to have useless facts elbowing out the useful ones.” reply smusamashah 9 hours agoprevI was assigned a big project at work 2 years ago. Apart from main code base there were a bunch of other modules and services it talked to at it was all very old code. I couldn't have kept all of that in my head throughout. I kind of rediscovered OneNote which helped me actually do the project. It's like your thought dumping playground. It's the only tool that offers least resistance to jotting down thoughts in whatever form you like. Every page is like a freeform infinite canvas. Just click anywhere, start typing or paste the screenshot or even files. OneNote has been my daily driver since then. I mostly just dump thoughts in a lossely structured system that I am following for myself. reply igammarays 9 hours agoparentSame here though my preferred tool is OmniOutliner or any similar tool with hierarchical branching. There are other tools like Roam/Obsidian but Omni has proved to the fastest offline local macOS app with the most rich feature set. reply Simon_ORourke 6 hours agoprevWhile I wholeheartedly agree that this approach makes perfect sense and would perhaps help with context switching various tasks, the time required to pour your heart out journaling would take time away from doing actual work. While I do some note taking at work it's all one liners, url references, copy and paste code snippets and VM options for intellij. reply ochronus 12 hours agoprevI wonder what the logistics of this look like - isn't this just another interrupt while you're in the zone? Or do you do it between two zones, retroactively? Imagine you start working deep on something. Do you simultaneously take notes of what you're doing? reply karencarits 10 hours agoprevI've concluded that I need different note taking tools for various contexts. * LogSeq for day-to-day notes. Append only. Since everything is bullet points, I don't have to worry about structure, and it's easy to just add a comment instead of revising a paragraph. * TiddlyWiki for write-ups. I've tested many wiki solutions but always return to TiddlyWiki. Mainly because it is so easy to adapt how things look, and various entries may need different presentation. * E-mail. Should not be underestimated as a knowledge base for discussions and decisions. Unfortunately, the search function in outlook is terrible reply singhrac 10 hours agoparentI use Obsidian for the first two. Email was very useful but you need to essentially categorize the data yourself manually. I’m very surprised there isn’t a bot that will automatically attempt to categorize your emails for you via IMAP access. reply karencarits 9 hours agorootparentI tried obsidian too, but found that outlining/nested bullet points in LogSeq matched my flow better. I agree regarding emails, one day I'll try to write a script that imports emails with a specific tag, perhaps autotagging can be a part of the pipeline reply blumomo 9 hours agoprevGitLab issues also works nicely for journaling. Each issue is a task, bundled into milestones. Within each issue you can add comments and even have one level of nested comments. Good for a lightweight hierarchical journaling. And comments/threads of an issue can each be “resolved”, all within the same issue. Great for progress tracking. Ah, and you can paste images and have syntax highlighting. Great for debugging. reply rhardih 8 hours agoprevThis has become one of my favourite tools over the last couple of years: https://jrnl.sh reply pavel_lishin 16 hours agoprevForget focusing faster and clarifying thoughts - a work journal does wonders for helping you during your yearly/6-month reviews, because you can look over the past several months and have concrete things to put down for why you deserve a raise and promotion. (Actually, don't forget it. But realize that you can use it for both purposes.) reply alabhyajindal 6 hours agoprevI use Sublime Text for my daily journal and it works really well. Typing in Sublime is a joy. I have it open throughout the day in a separate virtual desktop. Distraction Free mode is very cool! reply Willish42 15 hours agoprevI started doing a daily work journal in earnest when I began as a SWE and it's seriously saved my hide dozens of times. It's a habit that once you get used to, you can't function without it reply Hexigonz 16 hours agoprevI recently started manually typing out everything I jot down in my pocket notebooks when I fill them in my Digital Garden. The amount of ideas I forgot about within a week or so is crazy. Glad I can revisit some of this now reply simonireilly 12 hours agoprevVS code user, Vs code journal plugin - https://marketplace.visualstudio.com/items?itemName=pajoma.v... >month/day.md >month/day/important-meeting.md Find the equivalent for where you spend the majority of your time reply DarkCrusader2 12 hours agoprevI have been using PlainTasks [0] plugin with Sublime Text for many years now at work to do something similar. It is sort of a mix between GTD [1] and journalling. I create a new file monthly (since that is the \"sprint\" duration at work) and for each project I am working on, I create a project heading and a list of tasks below each. The plugin doesn't enforce any schema and is just a plain text file so I can iterate on the task list, record thoughts as free form notes anywhere (just below the project heading or below a particular task or just in the file anywhere. If the notes grow too large for a particular project (they don't usually for me), I pull them in a separate file dedicated to that project. I iterate on the task list as things become more clear. For eg. I might start as \"Find out how to deploy new certificates on our cluster nodes\". Once I have done some research or talked to my colleagues, I might mark this a done or delete it and replace it with a list of steps required to deploy the certificates. I also mark things I am going to do today with \"@today\" every day in the morning. If something planned comes up urgently, it gets it own task with \"@critical\" tag. The plugin highlights these tags for me. There are some more features but I only use creating and marking tasks as done with the 2 tags. The plugin is also semi-abandoned which is a big +1 for me as I don't have to worry about flow breaking changes or sudden sponsorship messages or constant updates. I don't use sublime for anything else but tracking tasks and notes so it gives me a sort of dedicated workspace for collecting my tasks. Recall is just a plain text search away. I have been using this for many years and has been extremely effective for me. Whenever I feel lost of overwhelmed, I just look this file, find the @today tags and suddenly I am back in my flow. Most of the other tools I have tried (Jira, Asana, Trello, Github/Gitlab issues, Azure DevOps, company internal project management tooling etc.) are too opinionated, not flexible enough, sends unnecessary notifications to me or everyone on the team and are a chore to maintain (busywork). [0] https://github.com/aziz/PlainTasks [1] https://gettingthingsdone.com/what-is-gtd/ reply myth_drannon 2 hours agoprevThe first paragraph spoke to me. The complexity of the systems is so big and the difficulty of fixing issues is exponential. The stress of working on things that are not core software development is killing me. How to calm down after working on days without good progress on the issue, PM is on my neck, the end of sprint is near and the task is not finished yet. reply nottorp 9 hours agoprevHmm? I keep (shared if needed) google docs for every project called \"XXX random notes\" and we throw info in there. I thought everyone did that? reply mxey 11 hours agoprevI use my inbox in OmniFocus as a running log of things for later, but I like the idea of having a dedicated journal thing. reply TechDebtDevin 15 hours agoprevI have a #leftoff tag in all my Logseq journals where I leave a description of where I was when I stepped away from a task. Use it everyday all day. Simple and works great. reply p5a0u9l 16 hours agoprevProse writing in any form is super useful. The challenge is having a unified tool to impose some structure. Instead, thoughts are divided across email, slack, confluence, quip, etc. reply alkh 17 hours agoprevSadly, the image doesn't load to me (I get ![[focus.png]] instead) reply charles_f 17 hours agoparentWoops, thanks for letting me know, should be fixed in a few minutes. That's the one: https://fev.al/img/2024/focus.png reply vantassell 16 hours agorootparentI'm getting the below instead of an image ![/img/2024/focus.png] reply charles_f 16 hours agorootparentYeah I'm editing that on a phone and apparently that's beyond my skillset. It's working now! Thanks for helping! reply barrenko 9 hours agoprevRelated - we need more landscape-oriented notebooks in the world. reply tkcranny 17 hours agoprevWonderfully written piece. I love how it builds up that idea of finally making progress, “seeing the matrix”, and then: > It’s Mitch, your PM. He’s asking the url for a doc he wrote Boy that’s too real. Really reminds me of the comic from a decade ago about programmer focus [1]. But welcome to the world of Note taking, I agree it’s like a superpower once you develop the habit. Obsidian is fantastic, but even daily markdown notes are great. The whole “second brain” idea hasn’t panned out for me, but a hotkey to jump to today’s note, and another insert the current time has been a mainstay of my workflow for years now. [1] https://imgur.com/never-interrupt-programmer-3uyRWGJ reply charles_f 16 hours agoparentThe stupid thing is that I messed up the link of the comic I included, which is pretty much the same :) : https://fev.al/img/2024/focus.png reply m3kw9 15 hours agoprevMost things I write become almost obsolete in a few days reply chillingeffect 3 hours agoprevNo Joplin in this thread?! Simple and flexible, cheap sub or free if you host self or on a service like dropbox. Import/export for backup. Instantly useful out of the box. Many plugins but i havent needed them. Basic search is all I need. I keep brief notes of the \"secret sauce\" as I work, eg a list of magic commands, a short outline of function calls, etc. reply ZoomerCretin 17 hours agoprevI've always done this. My coworkers have been frustrated with me in the past for asking them to write things down, because some of them overestimate my ability to listen to them speak for 5-10 minutes and remember every detail. It's very hard to forget what is written down. Recently, I neglected to write down my thinking and progress for a week, and I was at a loss for where to begin the following Monday. Keeping a work journal (in my case, a linear text document with an entry for each day) is the most important productivity habit I have. reply nbbaier 17 hours agoparentHow do you structure the entries? Bullets, free style, mix? reply charles_f 17 hours agorootparentI write it as a prose. Mostly because that's the form I enjoy the most to write, and also because it's mostly about writing it more than reading it later on reply ZoomerCretin 16 hours agorootparentprevDate (MM/DD/YYYY)- (tab) Task (tab) (tab) Details etc reply ge96 13 hours agoprevI always wrote notes into Apple notes everyday It would be like: W47 Mon, date reply kiba 17 hours agoprevI do work journaling too, but they're simply part of my journaling routine which encompasses much more than just a work log. Primarily I use work log to document problems I am having, keeping track of contexts such as what page I last read, and keeping track of my time. Timestamping is a very useful tool to fight procrastination. In other part of my life, I use journals for personal development and productivity in general, like writing down my problems and thinking about them. I often stumbled upon changes that I could implement or try. This allows me to achieve things that I haven't achieved before, such as putting actual effort in learning electronics. Daily habits and action items are tracked, including my prediction of how an action goes and what is the actual outcome. So yeah, journaling is a very good practice. It doesn't seem to matter much how you use them, just that you use them. It's very good at stopping your ruminating and you can actually move forward with your thoughts. reply Log_out_ 12 hours agoprevI use notepad++ documents. One is the stack, its just describing as one liners with checkboxes and subtasks a task and its depths. Arrow to where execution is.one can have several threads of course. Research into a topic goes into that list as tasks and then seperate documents or wikis if it blows the list.simple and it works suprisingly intuitiv like a bare metal os stack debug screen. reply dukeofdoom 16 hours agoprevMy new organization workflow is as follows for planning my day. I run a custom python script to generate a file/folder for each day in a month 2024/June/01-Jun.md ... 1. Tell ChatGPT what I want to do on that day, and ask it to give me a checkable markdown list. 2. Use \"Typora\" markdown editor for organization. It has a folder browser in the side bar. So I can have other files / folders at glance. Very easy to access. 3. Copy and paste the checkable list into the file with into Typora. If I had the need, I could write a script to aggregate all the unchecked items and create a new file with them. Or do other processing. Markdown editor + ChatGPT => killer combination. reply 29athrowaway 17 hours agoprevIf you get constantly interrupted, or you became a manager, this is a must. reply philwelch 10 hours agoprevOne format I like to use is to organize my most immediate tasks in the form of a stack. So if I’m working on feature A, that’s the bottom of the stack. Need to solve sub problem A1, push that to the top of the stack. Subproblem A1 requires me to shave yak number A1(a) so now my stack is three deep. Someone asks me for something unrelated. I pause, make sure I’ve fully written down my current stack, and then patiently listen to the person (or read their message). If I need to context switch and do something right now, fine, now interrupt B is the top of the stack, and when it’s done, I go back to shaving yak number A1(a). If I’m blocked, I pull the front task in my priority queue and push that to the stack. If the stack gets too deep with unrelated tasks and interrupts, I usually pull it apart and put everything into a priority queue. At a higher level of granularity it also helps to record what you’ve actually done for performance reviews and the like. reply brador 10 hours agoprevJournaling can be a lot of things and they’re moxed here. Brainstorming, real-time todo, planning, decision making. Each requires the tool be used differently. reply rustypotato 12 hours agoprevI hand-write a work journal. Just an A5 notebook and a few pens of different colors. Definitely an essential piece of my dev toolkit. I've especially come to love the free-form nature of hand-writing, which allows me to visualize more of my thoughts than a digital text editor. The journal has served two main purposes. One, I can write and annotate free-form pseudocode at exactly the level of abstraction I need without getting distracted by the errors produced by the code editor. It's really helped me work through the difficult parts of coding puzzles before I ever touch the keyboard to implement. Two, I have a scientific notebook for debugging. I write down a hypothesis, design a small experiment, document the steps and complications as I go, and write down what the actual result was; then repeat the cycle. Putting it all in writing keeps it straight so I don't chase my tail, and I have something to look back on if I need to explain the bug and how it was solved to my coworkers. reply waxaxiom 20 minutes agoparentCan you tell me more about the scientific notebook? Does it have a specific layout or is it just another notebook you use for debugging? reply tivert 12 hours agoprev [–] > Except that it’s nothing new, right? “Writing helps you organize your thoughts more clearly”: everyone and their grandmother know that! Writing a plan, writing a diary? People keep listing how transformative that’s been for them. Luckily, we'll soon be replacing writing with editing LLM output. Much more efficient. /s reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The author discusses the challenge of maintaining focus while dealing with complex problems and frequent interruptions in a work environment.",
      "They suggest using a work journal to document tasks, thoughts, and progress, which helps in regaining focus quickly after interruptions and clarifying thoughts.",
      "The practice of writing down what you're doing and reviewing the last entry when resuming work is presented as a simple yet effective productivity tool."
    ],
    "commentSummary": [
      "Using a work journal can enhance efficiency by solidifying mental models into concrete possibilities.",
      "Journaling helps identify gaps, build confidence, and ensure thoroughness, particularly in complex systems.",
      "Various methods and tools, such as physical notebooks, digital apps, and GitHub Issues, can be used for journaling; consistency is crucial."
    ],
    "points": 760,
    "commentCount": 220,
    "retryCount": 0,
    "time": 1720829123
  },
  {
    "id": 40948806,
    "title": "Free-threaded CPython is ready to experiment with",
    "originLink": "https://labs.quansight.org/blog/free-threaded-python-rollout",
    "originBody": "Back to blog Free-threaded CPython is ready to experiment with! Published July 12, 2024 rgommers Ralf Gommers First, a few announcements: Yesterday, py-free-threading.github.io launched! It's both a resource with documentation around adding support for free-threaded Python, and a status tracker for the rollout across open source projects in the Python ecosystem. We hope and expect both of these to be very useful, with the status tracker providing a one-stop-shop to check the support status of the dependencies of your project (e.g., \"what was the first release of a package on PyPI to support free-threaded Python?\" or \"are there nightly wheels and where can I find them?\") and get an overview of ecosystem-wide progress: Later today, the Birds-of-a-Feather session \"Supporting free-threaded Python\" will be held at the SciPy 2024 conference (co-organized by one of our team members, Nathan Goldbaum, together with Madicken Munk), focusing on knowledge and experience sharing. Free-threaded CPython - what, why, how? You may be wondering by now what \"free threading\" or \"free-threaded CPython\" is, and why you should care. In summary: it is a major change to CPython that allows running multiple threads in parallel within the same interpreter. It is becoming available as an experimental feature in CPython 3.13. A free-threaded interpreter can run with the global interpreter lock (GIL) disabled - a capability that is finally arriving as a result of the efforts that went into PEP 703 - Making the Global Interpreter Lock Optional in CPython. Why? Performance. Multi-threaded performance. It makes it significantly easier to write code that efficiently runs in parallel and will utilize multiple CPU cores effectively. The core counts in modern CPUs continue to grow, while clock speeds do not grow, so multi-threaded performance will continue to grow in importance. How? It's now easy to get started by installing a free-threaded interpreter: macOS/Linux/Windows & python.org/pyenv/apt/yum/conda - your preferred option is probably available now. Sounds awesome - what's the catch? Implementing free-threading in CPython itself is a massive effort already, and worthy of its own (series of) blog post(s). For the wider ecosystem, there's also a ton of work involved, mainly due to two problems: Thread-safety. While pure Python code should work unchanged, code written in other languages or using the CPython C API may not. The GIL was implicitly protecting a lot of thread-unsafe C, C++, Cython, Fortran, etc. code - and now it no longer does. Which may lead to all sorts of fun outcomes (crashes, intermittent incorrect behavior, etc.). ABI incompatibility between the default and free-threaded CPython builds. The result of a free-threaded interpreter having a different ABI is that each package that has extension modules must now build extra wheels. Out of these two, the thread-safety one is the more hairy problem. Having to implement and maintain extra wheel build jobs is not ideal, but the work itself is well-understood - it just needs doing for each project with extension modules. Thread-safety on the other hand is harder to understand, improve, and even test reliably. Because multithreaded code is usually sensitive to the timing of how multiple threads run and access shared state, bugs may manifest rarely. And a crash or failure that is hard to reproduce locally is harder to fix then one that is always reproducible. Here are a couple of examples of such intermittent failures: numpy#26690 shows an example where a simple call to the .sum() method of a numpy array fails with a fairly mysterious RuntimeError: Identity cache already includes the item. when used with the Python threading and queue modules. This was noticed in a scikit-learn CI job - it never failed in NumPy's own CI (scikit-learn has more tests involving parallelism). After the bug report with a reproducer was submitted, the fix to a numpy-internal cache wasn't that hard. pywavelets#758 was a report of another fairly obscure failure in a test using concurrent.futures: TypeError: descriptor '__enter__' for '_thread.RLock' objects doesn't apply to a '_thread.lock' object That looked a lot like a problem in CPython, and after some investigating it was found there as well cpython#121368 and fixed fairly quickly (the fix required some deep expertise in both CPython internals and multithreaded programming in C though). There are a fair amount of examples like that, e.g. undefined behavior in Cython code that no longer worked due to changes in CPython 3.13, a crash from C code in scipy.signal that hadn't been touched for 24 years (it was always buggy, but the GIL offered enough protection), and a crash in Pillow due to Python C API usage that wasn't supported. It's encouraging though that issues like the ones above do get understood and resolved fairly quickly. With a good test strategy, and over time also test suites of libraries that cover Python-level threading better (such tests are largely non-existent now in most packages), detecting or guarding against thread-safety issues does seem doable. That test strategy will have to be multi-pronged: from writing new tests and running tests in loops with pytest-repeat & co., to getting ThreadSanitizer to work in CI and doing integration-level and real-world testing with users. The road ahead & what our team will be working on Free-threaded CPython becoming the default, and eventually the only, build of CPython is several years away. What we're hoping to see, and help accomplish, is that for Python 3.13 many projects will work on compatibility and start releasing cp313t wheels on PyPI (and possibly nightly builds too, for projects with a lot of dependencies), so users and packages further downstream can start experimenting as well. After a full year of maturing support in the ecosystem and further improvements in performance in CPython itself, we should have a good picture of both the benefits and the remaining challenges with robustness. Our team (currently Nathan, Ken Jin, Lysandros, Edgar, and myself) has now been working on this topic for a few months, starting at the bottom of the PyData stack (most effort so far has gone to NumPy, Cython, and CPython), and slowly working our way up from there. For each package, the approach has been similar so far - and a lot of that can be used as a template by others we think. The steps are roughly: Add a first CI job, usually Linux x86-64 with the latest Python 3.13 pre-release candidate, and ensure the test suite passes, Based on knowledge from maintainers, fix known issues with thread-safety and shared/global state in native code, Add free-threaded support to the wheel build CI jobs, and start uploading nightly wheels (if appropriate for the project), Do some stress testing locally and monitor CI jobs, and fix failures that are observed (take the opportunity to add regression tests using threading or concurrent.futures.ThreadPoolExecutor) Mark extension modules as supporting running without the GIL Move on to a next package (e.g., a key dependency) and using its test suite to exercise the first package more, circling back to fix issues or address follow-up actions as needed. Our main takeaway so far: it's challenging, but tractable! And fun as well:) We've only just scratched the surface, there'll be a lot to do - from key complex packages like PyO3 (important for projects using Rust) and PyTorch, to the sheer volume of smaller packages with extension modules. The lessons we are learning, as far as they are reusable, are going into the documentation at py-free-threading.github.io. The repository that contains the sources for that website also has an issue tracker that is used to link to the relevant project-specific tracking issues for free-threaded support, as well as for ecosystem-wide issues and tasks (contributions and ideas are very welcome here!). Furthermore, we'd like to spend time on whatever may be impactful in helping the ecosystem adopt free-threaded CPython, from answering questions to helping with debugging - please don't hesitate to reach out or ping one of us directly on GitHub! Conclusion & acknowledgements We're really excited about what is becoming possible with free-threaded CPython! While our team is busy with implementing CI jobs and fixing thread-safety issues, we are as curious as anyone to see what performance improvements and interesting experiments are going to show up with real-world code soon. It's hard to acknowledge and thank everyone involved in moving free-threaded CPython forward, because so much activity is happening. First of all we have to thank Meta for funding the efforts of our team to help the ecosystem adopt free-threaded CPython at the pace that will be needed to make this whole endeavour a success, and Sam Gross and the whole Python Runtime team at Meta for the close collaboration. Then the list is long - from the Python Steering Council, for its thoughtful approach to (and acceptance of) PEP 703, to the many library maintainers and community members who are proactively adding support to their own projects or guide and review our contributions whenever we work on projects we are not ourselves maintainers of.",
    "commentLink": "https://news.ycombinator.com/item?id=40948806",
    "commentBody": "Free-threaded CPython is ready to experiment with (quansight.org)443 points by ngoldbaum 23 hours agohidepastfavorite310 comments eigenvalue 21 hours agoReally excited for this. Once some more time goes by and the most important python libraries update to support no GIL, there is just a tremendous amount of performance that can be automatically unlocked with almost no incremental effort for so many organizations and projects. It's also a good opportunity for new and more actively maintained projects to take market share from older and more established libraries if the older libraries don't take making these changes seriously and finish them in a timely manner. It's going to be amazing to saturate all the cores on a big machine using simple threads instead of dealing with the massive overhead and complexity and bugs of using something like multiprocessing. reply pizza234 12 hours agoparent> using simple threads instead of dealing with the massive overhead and complexity and bugs of using something like multiprocessing. Depending on the domain, the reality can be the reverse. Multiprocessing in the web serving domain, as in \"spawning separate processes\", is actually simpler and less bug-prone, because there is considerably less resource sharing. The considerably higher difficulty of writing, testing and debugging parallel code is evident to anybody who's worked on it. As for the overhead, this again depends on the domain. It's hard to quantify, but generalizing to \"massive\" is not accurate, especially for app servers with COW support. reply bausgwi678 10 hours agorootparentUsing multiple processes is simpler in terms of locks etc, but python libraries like multiprocessing or even subprocess.popen[1] which make using multiple processes seem easy are full of footguns which cause deadlocks due to fork-safe code not being well understood. I’ve seen this lead to code ‘working’ and being merged but then triggering sporadic deadlocks in production after a few weeks. The default for multiprocessing is still to fork (fortunately changing in 3.14), which means all of your parent process’ threaded code (incl. third party libraries) has to be fork-safe. There’s no static analysis checks for this. This kind of easy to use but incredibly hard to use safely library has made python for long running production services incredibly painful in my experience. [1] Some arguments to subprocess.popen look handy but actually cause python interpreter code to be executed after the fork and before the execve, which has caused production logging-related deadlocks for me. The original author was very bright but didn’t notice the footgun. reply ignoramous 2 hours agorootparent> The default for multiprocessing is still to fork (fortunately changing in 3.14) If I may: Changing from fork to what? reply thomasjudge 44 minutes agorootparent\"In Python 3.14, the default will be changed to either “spawn” or “forkserver” (a mostly safer alternative to “fork”).\" - https://pythonspeed.com/articles/python-multiprocessing/ reply lyu07282 37 minutes agorootparentprevSame experiences, multiprocessing is such a pain in python. It's one of these things people think they can write production code in, but they just haven't run into all the ways their code was wrong so they figure out those bugs later in production. As an aside I still constantly see side effects in imports in a ton of libraries (up to and including resource allocations). reply skissane 11 hours agorootparentprevJust the other day I was trying to do two things in parallel in Python using threads - and then I switched to multiprocessing - why? I wanted to immediately terminate one thing whenever the other failed. That’s straightforwardly supported with multiprocessing. With threads, it gets a lot more complicated and can involve things with dubious supportability reply lyu07282 14 minutes agorootparentThere is a reason why it's \"complicated\" in threads, because doing it correctly just IS complicated, and the same reason applies to child processes, you just ignored that reason. That's one example of a footgun in using multiprocessing, people write broken code but they don't know that because it appears to work... until it doesn't (in production on friday night). reply phkahler 21 hours agoparentprevI feel like most things that will benefit from moving to multiple cores for performance should probably not be written in Python. OTH \"most\" is not \"all\" so it's gonna be awesome for some. reply wongarsu 19 hours agorootparentI often reach for python multiprocessing for code that will run $singleDigit number of times but is annoyingly slow when run sequentially. I could never justify the additional development time for using a more performant language, but I can easily justify spending 5-10 minutes making the embarrassingly parallel stuff execute in parallel. reply throwaway81523 13 hours agorootparentI've generally been able to deal with embarassing parallelism by just chopping up the input and running multiple processes with GNU Parallel. I haven't needed the multiprocessing module or free threading so far. I believe CPython still relies on various bytecodes to run atomically, which you get automatically with the GIL present. So I wonder if hard-to-reproduce concurrency bugs will keep surfacing in the free-threaded CPython for quite some time. I feel like all of this is tragic and Python should have gone to a BEAM-like model some years ago, like as part of the 2 to 3 transition. Instead we get async wreckage and now free threading with its attendant hazards. Plus who knows how many C modules won't be expecting this. reply robertlagrant 12 hours agorootparentAsync seems fine? What's wrong with it? reply throwaway81523 12 hours agorootparentWatch this video and maybe you'll understand ;). Warning, NSFW (lots of swearing), use headphones. https://www.youtube.com/watch?v=bzkRVzciAZg This is also good: https://journal.stuffwithstuff.com/2015/02/01/what-color-is-... web search on \"colored functions\" finds lots of commentary on that article. reply wesselbindt 11 hours agorootparentI've always found the criticism leveled by the colored functions blog post a bit contrived. Yes, when you replace the words async/await with meaningless concepts I do not care about, it's very annoying to have to arbitrarily mark a function as blue or red. But when you replace the word \"aync\" with something like \"expensive\", or \"does network calls\", it becomes clear that \"async/await\" makes intrinsic properties about your code (e.g., is it a bad idea to put this call in a loop from a performance perspective) explicit rather than implicit. In short, \"await\" gives me an extra piece of data about the function, without having to read the body of the function (and the ones it calls, and the ones they call, etc). That's a good thing. There are serious drawbacks to async/await, and the red/blue blog post manages to list none of them. EDIT: all of the above is predicated on the idea that reading code is harder than writing it. If you believe the opposite, then blue/red has a point. reply adament 10 hours agorootparentBut a synchronous function can and many do make network calls or write to files. It is a rather vague signal about the functions behavior as opposed to the lack of the IO monad in Haskell. To me the difficulty is more with writing generic code and maintaining abstraction boundaries. Unless the language provides a way to generalise over asyncness of functions, we need a combinatorial explosion of async variants of generic functions. Consider a simple filter algorithm it needs versions for: (synchronous vs asynchronous iterator) times (synchronous vs asynchronous predicate). We end up with a pragmatic but ugly solution: provide 2 versions of each algorithm: an async and a sync, and force the user of the async one to wrap their synchronous arguments. Similarly changing some implementation detail of a function might change it from a synchronous to an asynchronous function, and this change must now propagate through the entire call chain (or the function must start its own async runtime). Again we end up in a place where the most future proof promise to give for an abstraction barrier is to mark everything as async. reply wesselbindt 8 hours agorootparent> But a synchronous function can and many do make network calls or write to files This, for me, is the main drawback of async/await, at least as it is implemented in for example Python. When you call a synchronous function which makes network calls, then it blocks the event loop, which is pretty disastrous, since for the duration of that call you lose all concurrency. And it's a fairly easy footgun to set off. > It is a rather vague signal about the functions behavior as opposed to the lack of the IO monad in Haskell. I'm happy you mentioned the IO monad! For me, in the languages people pay me to write in (which sadly does not include Haskell or F#), async/await functions as a poor man's IO monad. > Again we end up in a place where the most future proof promise to give for an abstraction barrier is to mark everything as async. Yes, this is one way to write async code. But to me this smells the same as writing every Haskell program as a giant do statement because the internals might want to do I/O at some point. Async/await makes changing side-effect free internals to effectful ones painful, which pushes you in the direction of doing the I/O at the boundaries of your system (where it belongs), rather than all over the place in your call stack. In a ports-adapters architecture, it's perfectly feasible to restrict network I/O to your service layer, and leave your domain entirely synchronous. E.g. sth like async def my_service_thing(request, database): my_business_object = await database.get(request.widget_id) my_business_object.change_state(request.new_widget_color) # some complicated, entirely synchronous computation await database.save(my_business_object) Async/await pushes you to code in a certain way that I believe makes a codebase more maintainable, in a way similar to the IO monad. And as with the IO monad, you can subvert this push by making everything async (or writing everything in a do statement), but there's better ways of working with them, and judging them based on this subversion is not entirely fair. > ugly solution: provide 2 versions of each algorithm: an async and a sync I see your point, and I think it's entirely valid. But having worked in a couple async codebases for a couple of years, the amount of stuff I (or one of my collaborators) have had to duplicate for this reason I think I can count on one hand. It seems that in practice this cost is a fairly low one. reply pansa2 10 hours agorootparentprev> when you replace the word \"aync\" with something like \"expensive\", or \"does network calls\", it becomes clear that \"async/await\" makes intrinsic properties about your code explicit rather than implicit. Do you think we should be annotating functions with `expensive` and/or `networking`? And also annotating all of their callers, recursively? And maintaining 4 copies of every higher-order function depending on whether the functions it calls are `expensive`, `networking`, neither or both? No, we rely on documentation for those things, and IMO we should for `async` as well. The reason we can’t, and why `async`/`await` exist, is because of shortcomings (lack of support for stackful coroutines) in language runtimes. The best solution is to fix those shortcomings, not add viral annotations everywhere. reply wesselbindt 9 hours agorootparentSo here I think we differ fundamentally in how we like to read code. I much prefer being able to quickly figure out things of interest about a function by glancing at its signature, rather than look at documentation, or worse, having to read the implementation of the function and the functions it calls (and so on, recursively). For example, I much prefer a signature like def f(a: int) -> str: over def f(a): because it allows me to see, without reading the implementation of the function (or, if it exists, and I'm willing to bet on its reliability, the documentation), that it takes an integer, and gives me a string. And yes, this requires that I write viral type annotations on all my functions when I write them, but for me the bottleneck at my job is not writing the code, it's reading it. So that's a small upfront cost I'm very much willing to pay. > Do you think we should be annotating functions with `expensive` and/or `networking`? And also annotating all of their callers, recursively? Yes, absolutely, and yes, absolutely. That's just being upfront and honest about an intrinsic property of those functions. A function calling a function that does network I/O by transitivity also does network I/O. I prefer code that's explicit over code that's implicit. reply pansa2 9 hours agorootparent> Yes, absolutely, and yes, absolutely. Fair enough, that's a valid philosophy, and one in which `async`/`await` makes perfect sense. However, it's not Python's philosophy - a language with dynamic types, unchecked exceptions, and racy multithreading. In Python, `async`/`await` seems to be at odds with other language features - it feels like it's more at home in a language like Rust. reply wesselbindt 8 hours agorootparentI completely agree with you. However I've always found the dynamic typing approach to be a bit at odds with python3 -c \"import this\"head -4tail -1 I think the fast and loose style that Python enables is perfect for small scripts and one off data science notebooks and the like. But having worked in large codebases which adopt the same style, and ones that avoid it through static typing and in some cases async/await, the difference in productivity I've noticed in both me and my collaborators is too stark for me to ignore. I think I should've been more nuanced in my comments praising async/await. I believe that what I say is valid in large IO-bound applications which go beyond basic CRUD operations. In general it depends, of course. reply pansa2 8 hours agorootparent> I think the fast and loose style that Python enables is perfect for small scripts and one off data science notebooks and the like. Agreed - I only use Python for scripts like this, preferring statically-typed, AOT-compiled languages for larger programs. That’s why I think Python should have adopted full coroutines - it should play to its strengths and stick to its fast-and-loose style. However, the people who decide how the language evolves are all employees of large companies using it for large codebases - their needs are very different from people who are only using Python for small scripts. reply hiddew 10 hours agorootparentprev> The reason we can’t, and why `async`/`await` exist, is because of shortcomings (lack of support for stackful coroutines) in language runtimes The JVM runtime has solved this problem neatly with virtual threads in my opinion. Run a web request in a virtual thread, and all blocking I/O is suddenly no longer blocking the OS thread, but yielding/suspending and giving and giving another virtual thread run time. And all that without language keywords that go viral through your program. reply pansa2 10 hours agorootparentYes, this is similar to how Go works. IIRC the same approach was available in Python as a library, “greenlet”, but Python’s core developers rejected it in favour of `async`/`await`. reply throwaway81523 10 minutes agorootparentThe Python community seems to have a virulent hatred of threads. I don't understand the reason. Yes there are hazards but you can code in a style that avoids them. With something like BEAM you can even enforce the style. Async/await of course introduce their own hazards. gpderetta 8 hours agorootparentprevExcept that at least in python async doesn't mean that. Non async functions can do networking, block, or do expensive operations. On the other hand async functions can be very cheap. Again, which useful property does async protect? reply eigenvalue 18 hours agorootparentprevI personally optimize more for development time and overall productivity in creating and refactoring, adding new features, etc. I'm just so much faster using Python than anything else, it's not even close. There is such an incredible world of great libraries easily available on pip for one thing. Also, I've found that ChatGPT/Claude3.5 are much, much smarter and better at Python than they are at C++ or Rust. I can usually get code that works basically the first or second time with Python, but very rarely can do that using those more performant languages. That's increasingly a huge concern for me as I use these AI tools to speed up my own development efforts very dramatically. Computers are so fast already anyway that the ceiling for optimization of network oriented software that can be done in a mostly async way in Python is already pretty compelling, so then it just comes back again to developer productivity, at least for my purposes. reply goosejuice 13 hours agorootparentKind of sounds like you are optimizing for convenience :) reply indigodaddy 17 hours agorootparentprevEver messed about with Claude and php? reply lanstin 16 hours agorootparentI don't think we are supposed to use HN for humor only posts. reply saagarjha 16 hours agorootparentYou think wrong reply jacob019 16 hours agorootparentprevlol reply indigodaddy 4 hours agorootparentprevWhy the downvotes? Totally serious question. Jesus Christ HN reply jillesvangurp 14 hours agorootparentprevRight now you are right. This is about taking away that argument. There's no technical reason for this to stay true. Other than that the process of fixing this is a lot of work of course. But now that the work has started, it's probably going to progress pretty steadily. It will be interesting to see how this goes over the next few years. My guess is that a lot of lessons were learned from the python 2 to 3 move. This plan seems pretty solid. And of course there's a relatively easy fix for code that can't work without a GIL: just do what people are doing today and just don't fork any threads in python. It's kind of pointless in any case with the GIL in place so not a lot of code actually depends on threads in python. Preventing the forking of threads in the presence of things still requiring the GIL sounds like a good plan. This is a bit of meta data that you could build into packages. This plan is actually proposing keeping track of what packages work without a GIL. So, that should keep people safe enough if dependency tools are updated to make use of this meta data and actively stop people from adding thread unsafe packages when threading is used. So, I have good hopes that this is going to be a much smoother transition than python 2 to 3. The initial phase is probably going to flush out a lot of packages that need fixing. But once those fixes start coming in, it's probably going to be straightforward to move forward. reply jodrellblank 15 hours agorootparentprevhttps://www.servethehome.com/wp-content/uploads/2023/01/Inte... AMD EPYC 9754 with 128-cores/256-threads, and EPYC 9734 with 112-cores/224-threads. TomsHardware says they \"will compete with Intel's 144-core Sierra Forest chips, which mark the debut of Intel's Efficiency cores (E-cores) in its Xeon data center lineup, and Ampre's 192-core AmpereOne processors\". What in 5 years? 10? 20? How long will \"1 core should be enough for anyone using Python\" stand? reply d0mine 14 hours agorootparentNumber crunching code in Python (such as using numpy/pytorch) performs the vast vast majority of its calculations in C/Fortran code under the hood where GIL can be released. Single python process can use multiple CPUs. There is code that may benefit from the free threaded implementation but it is not as often as it might appear and it is not without its own downsides. In general, GIL simplifies multithreaded code. There were no-GIL Python implementations such as Jython, IronPython. They hadn't replaced CPython, Pypy implementation which use GIL i.e., other concerns dominate. reply imachine1980_ 13 hours agorootparentYes but jython am iron aren't the standard, and I feel the more relevant part is inertia, puppy is design whit lots of concern of compatibility, then being the new standard can totally make difference making both cases not a good comparison. reply Derbasti 15 hours agorootparentprevA thought experiment: A piece of code takes 6h to develop in C++, and 1h to run. The same algorithm takes 3h to code in Python, but 6h to run. If I could thread-spam that Python code on my 24 core machine, going Python would make sense. I've certainly been in such situations a few times. reply Certhas 9 hours agorootparentC++ and python are not the only options though. Julia is one that is gaining a lot of use in academia, but any number of modern, garbage collected compiled high level languages could probably do. reply DanielVZ 20 hours agorootparentprevUsually performance critical code is written in cpp, fortran, etc, and then wrapped in libraries for Python. Python still has a use case for glue code. reply andmkl 18 hours agorootparentYes, but then extensions can already release the GIL and use the simple and industrial strength std::thread, which is orders of magnitude easier to debug. reply woodruffw 18 hours agorootparentConcurrent operations exist at all levels of the software stack. Just because native extensions might want to release the GIL and use OS threads doesn't mean pure Python can't also want (or need) that. (And as a side note: I have never, in around a decade of writing C++, heard std::thread described as \"easy to debug.\") reply ipsod 18 hours agorootparentprevReally? Cool. I expected that dropping down to C/C++ would be a large jump in difficulty and quantity of code, but I've found it isn't, and the dev experience isn't entirely worse, as, for example, in-editor code-intelligence is rock solid and very fast in every corner of my code and the libraries I'm using. If anyone could benefit from speeding up some Python code, I'd highly recommend installing cppyy and giving it a try. reply woodson 17 hours agorootparentThanks, I haven’t come across cppyy! But I’ve worked with pybind11, which works well, too. reply ipsod 15 hours agorootparentSure! I tried pybind11, and some other things. cppyy was the first I tried that didn't give me any trouble. I've been using it pretty heavily for about a year, and still no trouble. reply nly 9 hours agorootparentLast I checked cppyy didn't build any code with optimisations enabled (same as cling) reply ipsod 2 hours agorootparentIt seems like you might be able to enable some optimizations with EXTRA_CLING_ARGS. Since it's based on cling, it's probably subject to whatever limitations cling has. To be honest, I don't know much about the speed, as my use-case isn't speeding up slow code. reply MBCook 20 hours agorootparentprevBut it would give you more headroom before rewriting for performance would make sense right? That alone could be beneficial to a lot of people. reply rty32 19 hours agorootparentI think it is beneficial to some people, but not a lot. My guess is that most Python users (from beginners to advanced users, including many professional data scientists) have never heard of GIL or thought of doing any parallelization in Python. Code that needs performance and would benefit from multithreading, usually written by professional software engineers, likely isn't written in Python in the first place. It would make sense for projects that can benefit from disabling GIL without a ton of changes. Remember it is not trivial to update single threaded code to use multithreading correctly. in Python language specifically. Their library may have already done some form of parallelization under the hood reply bdd8f1df777b 14 hours agorootparent> Code that needs performance and would benefit from multithreading, usually written by professional software engineers, likely isn't written in Python in the first place. There are a lot of simple cases where multi-threading can easily triple or quadruple the performance. reply Certhas 6 hours agorootparentBut multiprocessing can't? I used to write a ton of MPI based parallel python. It's pretty straightforward. But one could easily imagine trying to improve the multiprocessing ergonomics rather than introducing threading. Obviously the people who made the choice to push forward with this are aware of these options, too. Still mildly puzzling to me why threads for Python are needed/reasonable. reply tho34234234 14 hours agorootparentprevIt's not just about \"raw-flop performance\" though; it affects even basic things like creating data-loaders that run in the background while your main thread is doing some hard ML crunching. Every DL library comes with its own C++ backend that does this for now, but it's annoyingly inflexible. And dealing with GIL is a nightmare if you're dealing with mixed Python code. reply fuzztester 19 hours agorootparentprevnext [15 more] [flagged] fuzztester 19 hours agorootparenthttps://news.ycombinator.com/item?id=40949956 reply fuzztester 17 hours agorootparentprevnext [5 more] [flagged] jacob019 16 hours agorootparentBashing Python in Python development thread is not tactful. reply fuzztester 13 hours agorootparentit may not be tactful but it sure is factful (even if that last word is ungrammatical, at least the sentence is poetical). he he he. jfc. guido knows ;), this thread is getting weirder and weirder, creepier and creepier. are you literally implying that I should lie about known facts about python slowness? \"tactful\" my foot. then I guess the creators of PyPy and Unladen Swallow (the latter project was by Google) were/are not being tactful either, because those were two very prominent projects to speed up Python, in other words, to reduce its very well known slowness. There is/was also Cython and Pyston (the latter at Dropbox, where Guido (GvR, Python creator) worked for a while. Those were or are also projects to speed up Python program execution. https://en.m.wikipedia.org/wiki/PyPy https://peps.python.org/pep-3146/ Excerpt from the section titled \"Rationale, Implementation\" from the above link (italics mine): [ Many companies and individuals would like Python to be faster, to enable its use in more projects. Google is one such company. Unladen Swallow is a Google-sponsored branch of CPython, initiated to improve the performance of Google’s numerous Python libraries, tools and applications. To make the adoption of Unladen Swallow as easy as possible, the project initially aimed at four goals: A performance improvement of 5x over the baseline of CPython 2.6.4 for single-threaded code. 100% source compatibility with valid CPython 2.6 applications. 100% source compatibility with valid CPython 2.6 C extension modules. Design for eventual merger back into CPython. ] Your honor, I rest my case. reply jacob019 1 hour agorootparenttactful - showing skill and sensitivity in dealing with people. Right or wrong, your comments do not contribute to the discussion. Open source development is about collaboration. Your irrelevant comments are disrespectful to all the hard working developers who have been working together to push the limits of what is possible with what may be the most widely used programming language of our time. I use Python for it's ergonomics and community, not for speed. Removing the GIL has been something we've been yearning for since the Python 2 days. And the speed has improved drastically in recent years with each release. You don't win a prize for being right. reply fuzztester 13 hours agorootparentprevif Google wanted a performance increase of 5x over the then existing python, I guess we can safely say that Python was slow, amirite. and yes, I know the version number mentioned, and what it is today. reply fuzztester 14 hours agorootparentprevnext [3 more] [flagged] geertj 13 hours agorootparentYou might be getting downvoted because many people know Python is among the slower dynamic languages, and there are other reasons to use it. Speaking for myself, the reasons that make me reach for Python for some projects are the speed of development, large ecosystem of libraries, large developer pool, and pretty good tooling/IDE support. reply fuzztester 12 hours agorootparentthen it would be much preferable if they said so explicitly, although both of those are points I already know well. both are common knowledge, not just the first, i e. the slowness. the productivity and other benefits are common knowledge too. downvoting is such a fucking dumb way of disagreeing. how does one know whether the downvote is because a person disapproves of or dislikes what one said or because they think what one said is wrong. no way to know. amirite? :) reply fuzztester 17 hours agorootparentprevnext [8 more] [flagged] jacob019 16 hours agorootparentIt's very unpopular to mention Perl, but I did many cool things with it back in the day, and it still holds a special place for me. Perl taught me the power of regex--it's really first class in Perl. I still have some Perl code in production today. But to be fair, it is really easy to write spaghetti in Perl if you don't know what you're doing. reply fuzztester 13 hours agorootparentdon't worry about unpopularity, bro. worry about being true. the rest will take care of itself. if not, you are in the wrong company, forum, or place, and better to work on getting out of there. reply dr_kiszonka 14 hours agorootparentprevOne day your ISP emails you that they increased your upload and download speeds. Sweet, right? Same here with Python. reply fuzztester 13 hours agorootparentnext [2 more] [flagged] fuzztester 13 hours agorootparenthttps://news.ycombinator.com/item?id=40951145 reply saagarjha 15 hours agorootparentprevThey’re probably downvoting you because you’ve posted like the laziest trope comment there is. lol Python slow everyone is paid to hide the truth amirite reply fuzztester 13 hours agorootparentthanks for your viewpoint, but i doubt it. one of them (user jacob019) talked about the \"need\" to be tactful (in a thread about software, aka logic !!!) and the same guy tried to divert the topic to perl, in another comment. I'm damn sure it's the upton sinclair syndrome at work, as I posted, also in this subthread, i.e. they are lying because of vested interests, like salary. another way of describing it is \"The emperor's new clothes\", that well known fable about hypocrisy and sycophancy for gain. reply porksoda 8 hours agorootparentYour comments feel like you want a reddit style battle of wits. You throw words like ignorant around rather freely. I downvote these comments because they lower the tone of the discussion. We're not here to talk about each other or feel smarter than others.. Well I'm not. reply quietbritishjim 1 hour agoparentprevIf you're worried about performance then much of your CPU time is probably spent in a C extension (e.g. numpy, scipy, opencv, etc.). Those all release the GIL so already allow parallelisation in multiple threads. That even includes many functions in the standard library (e.g. sqlite3, zip/unzip). I've used multiple threads in Python for many years and never needed to break into multiprocessing. But, for sure, nogil will be good for those workloads written in pure Python (though I've personally never been affected by that). reply wokwokwok 11 hours agoparentprev> there is just a tremendous amount of performance that can be automatically unlocked with almost no incremental effort for so many organizations and projects This just isn’t true. This does not improve single threaded performance (it’s worse) and concurrent programming is already available. This will make it less annoying to do concurrent processing. It also makes everything slower (arguable where that ends up, currently significantly slower) overall. This way over hyped. At the end of the day this will be a change that (most likely) makes the existing workloads for everyone slightly slower and makes the lives of a few people a bit easier when they implement natively parallel processing like ML easier and better. It’s an incremental win for the ML community, and a meaningless/slight loss for everyone else. At the cost of a great. Deal. Of. Effort. If you’re excited about it because of the hype and don’t really understand it, probably calm down. Mostly likely, at the end of the day, it s a change that is totally meaningless to you, won’t really affect you other than making some libraries you use a bit faster, and others a bit slower. Overall, your standard web application will run a bit slower as a result of it. You probably won’t notice. Your data stack will run a bit faster. That’s nice. That’s it. Over hyped. 100%. reply anwlamp 8 hours agorootparentYes, good summary. My prediction is that free-threading will be the default at some point because one of the corporations that usurped Python-dev wants it. The rest of us can live with arcane threading bugs and yet another split ecosystem. As I understand it, if a single C-extension opts for the GIL, the GIL will be enabled. Of course the invitation to experiment is meaningless. CPython is run by corporations, many excellent developers have left and people will not have any influence on the outcome. reply pansa2 8 hours agorootparent> one of the corporations that usurped Python-dev Man, that phrase perfectly encapsulates so much of Python’s evolution over the last ~10 years. reply Demiurge 14 hours agoparentprevMassive overhead of multiprocessing? How have I not noticed this for tens of years? I use coroutines and multiprocessing all the time, and saturate every core and all the IO, as needed. I use numpy, pandas, xarray, pytorch, etc. How did this terrible GIL overhead completely went unnoticed? reply viraptor 14 hours agorootparent> I use numpy, pandas, xarray, pytorch, etc. That means your code is using python as glue and you do most of your work completely outside of cPython. That's why you don't see the impact - those libraries drop GIL when you use them, so there's much less overhead. reply Galanwe 4 hours agoparentprev> It's going to be amazing to saturate all the cores on a big machine using simple threads instead of dealing with the massive overhead and complexity and bugs of using something like multiprocessing. I'm saturating 192cpu / 1.5TBram machines with no headache and straightforward multiprocessing. I really don't see what multithreading will bring more. What are these massive overheads / complexity / bugs you're talking about ? reply saurik 19 hours agoparentprevFWIW, I think the concern though is/was that for most of us who aren't doing shared-data multiprocessing this is going to make Python even slower; maybe they figured out how to avoid that? reply eigenvalue 16 hours agorootparentPretty sure they offset any possible slowdowns by doing heroic optimizations in other parts of CPython. There was even some talk about keeping just those optimizations and leaving the GIL in place, but fortunately they went for the full GILectomy. reply quotemstr 15 hours agoparentprevWhat about the pessimization of single-threaded workloads? I'm still not convinced a completely free-threaded Python is better overall than a multi-interpreter, separate-GIL model with explicit instead of implicit parallelism. Everyone wants parallelism in Python. Removing the GIL isn't the only way to get it. reply simonw 18 hours agoprevI got this working on macOS and wrote up some notes on the installation process and a short script I wrote to demonstrate how it differs from non-free-threaded Python: https://til.simonwillison.net/python/trying-free-threaded-py... reply vanous 13 hours agoparentThanks for the example and explanations Simon! reply vegabook 11 hours agoprevClearly the Python 2 to 3 war was so traumatising (and so badly handled) that the core Python team is too scared to do the obvious thing, and call this Python 4. This is a big fundamental and (in many cases breaking) change, even if it's \"optional\". reply blumomo 8 hours agoparentDid Python as the language change which justified that version bump? reply nine_k 22 hours agoprevPython 3 progress so far: [x] Async. [x] Optional static typing. [x] Threading. [ ] JIT. [ ] Efficient dependency management. reply janice1999 21 hours agoparentNot sure what this list means, there are successful languages without these feature. Also Python 3.13 [1] has an optional JIT [2], disabled by default. [1] https://docs.python.org/3.13/whatsnew/3.13.html [2] https://peps.python.org/pep-0744/ reply jolux 21 hours agorootparentThe successful languages without efficient dependency management are painful to manage dependencies in, though. I think Python should be shooting for a better package management user experience than C++. reply yosefk 21 hours agorootparentIf Python's dependency management is better than anything, it's better than C++'s. Python has pip and venv. C++ has nothing (you could say less than nothing since you also have ample opportunity for inconsistent build due to mismatching #defines as well as using the wrong binaries for your .h files and nothing remotely like type-safe linkage to mitigate human error. It also has an infinite number of build systems where each system of makefiles or cmakefiles is its own build system with its own conventions and features). In fact python is the best dependency management system for C++ code when you can get binaries build from C++ via pip install... reply wiseowise 20 hours agorootparent> If Python's dependency management is better than anything, it's better than C++'s. That’s like the lowest possible bar to clear. reply yosefk 10 hours agorootparentAgreed, but that was the bar set by the comment I was replying to, which claimed Python doesn't clear it. reply vulnbludog 9 hours agorootparentprevOn bro reply andmkl 18 hours agorootparentprevC++ has apt-get etc. because the libraries do not change all the time. Also, of course there are vcpkg and conan. Whenever you try to build something via pip, the build will invariably fail. The times that NumPy built from source from PyPI are long over. In fact, at least 50% of attempted package builds fail. The alternative of binary wheels is flaky. reply viraptor 13 hours agorootparent> C++ has apt-get That's not a development dependency manager. System package management is a different kind of issue, even if there's a bit of overlap. > because the libraries do not change all the time That's not true in practice. Spend enough time with larger projects or do some software packaging and you'll learn that the pain is everywhere. reply stavros 19 hours agorootparentprevThat was the entire point, that C++ is the absolute worst. reply dgfitz 17 hours agorootparentI pip3 installed something today. It didn’t work, at all. I then yum installed a lib and headers, it worked well. C++ on an msft platform is the worst. I can’t speak for Mac. C++ on a linux is quite pleasant. Feels like most of the comments like yours are biased for un-stated reasons. reply viraptor 13 hours agorootparentThis has nothing to do with languages. You can yum install python packages and expect them to work fine. You can install C++ files using an actual dependency manager like vcpkg or conan and have issues. You're pointing out differences between software package management styles, not languages. reply stavros 17 hours agorootparentprevIf I had a penny for every time I gave up on compiling C++ software because there's no way to know what dependencies it needs, I'd be a millionaire. Python at least lists them. reply ahartmetz 10 hours agorootparentIs that because the compiler failed with \"foo.h not found\" or the build system said \"libfoo not found\"? CMake is most common and it will tell you. Worst case it's difficult to derive the package name from the name in the diagnostic. It's not great, but usually not a big deal neither IME. Typically a couple of minutes to e.g. find that required libSDL2 addon module or whatever, if there is that kind of problem at all. reply stavros 9 hours agorootparentYes it is, and it's usually such a big deal for me that I just don't use that software. I don't have time to go through a loop of \"what's the file name? What package is it in? Install, repeat\". This is by far the worst experience I've had with any language. Python has been a breeze in comparison. reply dgfitz 5 hours agorootparentI’m not going to refute your points. If you’re going to wear rose-tinted glasses about all of the bad parts about python, that’s fine, I also like python. reply stavros 5 hours agorootparentWhat's rose-tinted about \"one of them downloads dependencies automatically, the other one doesn't\"? reply dgfitz 17 hours agorootparentprevIf I had a penny every time I heard something like that on sites like this, I’d be a billionaire :) reply yupyupyups 17 hours agorootparentprevMac has the Brew project, which is sort of like apt-get or yum. reply galdosdi 57 minutes agorootparentprevThe shitshow that is python tooling is one of the reasons I prefer java jobs to python jobs when I can help it. Java got this pretty right years and years and years earlier. Why are python and javascript continuing to horse around playing games? reply __MatrixMan__ 19 hours agorootparentprevPython's dependency management sucks because they're audacious enough to attempt packaging non-python dependencies. People always bring Maven up as a system that got it right, but Maven only does JVM things. I think the real solution here is to just only use python dependency management for python things and to use something like nix for everything else. reply adgjlsfhk1 19 hours agorootparentJulia's package manager (for one) works great and can manage non Julia packages. the problem with python's system is that rejecting semver makes writing a package manager basically impossible since there is no way to automatically resolve packages. reply woodruffw 18 hours agorootparentCould you clarify what you mean? pip and every other Python package installer is absolutely doing automatic package resolution, and the standard (PEP 440) dependency operators include a compatible version operator (~=) that's predicated on SemVer-style version behavior. reply pletnes 7 hours agorootparentprevThis is what we used to have and it was much worse. Source: lived that life 10-15 y ago. reply __MatrixMan__ 3 hours agorootparent15y ago I was using apt-get to manage my c++ dependencies with no way of keeping track of which dependency went with which project. It was indeed pretty awful. Now when I cd into a project, direnv + nix notices the dependencies that that project needs and makes them available, whatever their language. When I cd into a different project, I get an entirely different set of dependencies. There's pretty much nothing installed with system scope. Just a shell and an editor. Both of these are language agnostic, but the level of encapsulation is quite different and one is much better than that other. (There are still plenty of problems, but they can be fixed with a commit instead of a change of habit.) The idea that every language needs a different package manager and that each of those needs to package everything that might my useful when called from that language whether or not it is written in that language... It just doesn't scale. reply Galanwe 21 hours agorootparentprevNot sure this is still a valid critic of Python in 2024. Between pip, poetry and pyproject.toml, things are now quite good IMHO. reply arp242 20 hours agorootparentI guess that depends from your perspective. I'm not a Python developer, but like many people I do want to run Python programs from time to time. I don't really know Rust, or Cargo, but I never have trouble building any Rust program: \"cargo build [--release]\" is all I need to know. Easy. Even many C programs are actually quite easy: \"./configure\", \"make\", and optionally \"make install\". \"./configure\" has a nice \"--help\". There is a lot to be said about the ugly generated autotools soup, but the UX for people just wanting to build/run it without in-depth knowledge of the system is actually quite decent. cmake is a regression here. With Python, \"pip install\" gives me an entire screen full of errors about venv and \"externally managed\" and whatnot. I don't care. I just want to run it. I don't want a bunch of venvs, I just want to install or run the damn program. I've taken to just use \"pip install --break-system-packages\", which installs to ~/.local. It works shrug. Last time I wanted to just run a project with a few small modifications I had a hard time. I ended up just editing ~/.local/lib/python/[...] Again, it worked so whatever. All of this is really where Python and some other languages/build systems fail. Many people running this are not $language_x programmers or experts, and I don't want to read up on every system I come across. That's not a reasonable demand. Any system that doesn't allow non-users of that language to use it in simple easy steps needs work. Python's system is one such system. reply simonw 20 hours agorootparent\"I don't want a bunch of venvs\" That's your problem right there. Virtual environments are the Python ecosystem's solution to the problem of wanting to install different things on the same machine that have different conflicting requirements. If you refuse to use virtual environments and you install more than one separate Python project you're going to run into conflicting requirements and it's going to suck. Have you tried pipx? If you're just installing Python tools (and not hacking on them yourself) it's fantastic - it manages separate virtual environments for each of your installations without you having to think about them (or even know what a virtual environment is). reply arp242 20 hours agorootparentManaging a farm of virtualenvs and mucking about with my PATH doesn't address the user-installable problem at all. And it seems there's a new tool to try every few months that really will fix all problems this time. And maybe if you're a Python developer working on the code every day that's all brilliant. But most people aren't Python developers, and I just want to try that \"Show HN\" project or whatnot. Give me a single command I can run. Always. For any project. And that always works. If you don't have that then your build system needs work. reply simonw 18 hours agorootparent\"Give me a single command I can run. Always. For any project. And that always works.\" pipx install X reply arp242 11 hours agorootparentRight so; I'll try that next time. Thanks. I just go by the very prominent \"pip install X\" on every pypi page (as well as \"pip install ..\" in many READMEs). reply simonw 4 hours agorootparentYeah, totally understand that - pipx is still pretty poorly known by people who are active in Python development! A few of my READMEs start like this: https://github.com/simonw/paginate-json?tab=readme-ov-file#i... ## Installation pip install paginate-json Or use pipx (link to pipx site) pipx install paginate-json But I checked and actually most them still don't even mention it. I'll be fixing that in the future. reply pletnes 6 hours agorootparentprevPipx is great! Although, I always seem to have to set up PATH, at least on windows? reply Ringz 13 hours agorootparentprevThat single command is pipx. reply zo1 8 hours agorootparentprevI could say the exact same stuff about NodeJs, c++, go, rust, php, etc. All of these are easy to use and debug and \"install easily\" when you know them and use them regularly, and the opposite if you're new. Doubly-so if you personally don't like that language or have some personal pet peeve about it's choices. Guys let's not pretend like this is somehow unique to python. It's only until about a few years ago that it was incredibly difficult to install and use npm on windows. Arguably the language ecosystem with the most cumulative hipster-dev hours thrown at it, and it still was a horrible \"dev experience\". reply imtringued 57 minutes agorootparentprevPythons venvs are a problem to the solution of solving the dependency problem. Consider the following: it is not possible to relocate venvs. In what universe does this make sense? Consider a C++ or Rust binary that would only run when it is placed in /home/simonw/. reply kallapy 7 hours agorootparentprevI reject the virtual environments and have no issues. On an untrusted machine (see e.g. the recent token leak): /a/bin/python3 -m pip install foo /b/bin/python3 -m pip install bar The whole venv thing is overblown but a fertile source for blogs and discussions. If C-extensions link to installed libraries in site-packages, of course they should use RPATH. reply guhidalg 19 hours agorootparentprevNormal users who just want to run some code shouldn't need to learn why they need a venv or any of its alternatives. Normal users just want to download a package and run some code without having to think about interfering with other packages. Many programming languages package managers give them that UX and you can't blame them for expecting that from Python. The added step of having to think about venvs with Python is not good. It is a non-trivial system that every single Python user is forced to learn, understand, and the continually remember every time they switch from one project to another. reply nine_k 18 hours agorootparentThis is correct. The whole application installation process, including the creation of a venv, installing stuff into it, and registering it with some on-PATH launcher should be one command. BTW pyenv comes relatively close. reply simonw 18 hours agorootparentprevI agree with that. Until we solve that larger problem, people need to learn to use virtual environments, or at least learn to install Python tools using pipx. reply sgarland 19 hours agorootparentprevThis is mostly a curse of Python’s popularity. The reason you can’t pip install with system Python is that it can break things, and when your system is relying on Python to run various tools, that can’t be allowed. No one (sane) is building OS-level scripts with Node. The simplest answer, IMO, is to download the Python source code, build it, and then run make altinstall. It’ll install in parallel with system Python, and you can then alias the new executable path so you no longer have to think about it. Assuming you already have gcc’s tool chain installed, it takes roughly 10-15 minutes to build. Not a big deal. reply Galanwe 13 hours agorootparentprevMaybe I am biased, because I learned these things so long ago and I don't realize that it's a pain to learn. But what exactly is so confusing about virtualenvs ? They really not that different from any other packaging system like JS or Rust. The only difference is instead of relying on your current directory to find the the libraries / binaries (and thus requiring you to wrap binaries call with some wrapper to search in a specific path), they rely on you sourcing an `activate` script. That's really just it. Create a Virtualenv: $ virtualenv myenv Activate it, now it is added to your $PATH: $ . myenv/bin/activate There really is nothing more in the normal case. If you don't want to have to remember it, create a global Virtualenv somewhere, source it's activate in your .bashrc, and forget it ever existed. reply imtringued 55 minutes agorootparentOnly python demands you to source an activation script before doing anything. reply vhcr 19 hours agorootparentprevDo you have a problem with Node.js too because it creates a node_modules folder, or is the problem that it is not handled automatically? reply arp242 19 hours agorootparentI don't care about the internals. I care about \"just\" being able to run it. I find that most JS projects work fairly well: \"npm install\" maybe followed by \"npm run build\" or the like. This isn't enforced by npm and I don't think npm is perfect here, but practical speaking as a non-JS dev just wanting to run some JS projects: it works fairly well for almost all JS projects I've wanted to run in the last five years or so. A \"run_me.py\" that would *Just Work™\" is fine. I don't overly care what it does internally as long as it's not hugely slow or depends on anything other than \"python\". Ideally this should be consistent throughout the ecosystem. To be honest I can't imagine shipping any project intended to be run by users and not have a simple, fool-proof, and low-effort way of running it by anyone of any skill level, which doesn't depend on any real knowledge of the language. reply sgarland 19 hours agorootparent> To be honest I can't imagine shipping any project intended to be run by users and not have a simple, fool-proof, and low-effort way of running it by anyone of any skill level, which doesn't depend on any real knowledge of the language. This is how we got GH Issues full of inane comments, and blogs from mediocre devs recommending things they know nothing about. I see nothing wrong with not catering to the lowest common denominator. reply arp242 19 hours agorootparentLike people with actual lives to live and useful stuff to do that's not learning about and hand-holding a dozen different half-baked build systems. But sure, keep up the cynical illusion that everyone is an idiot if that's what you need to go through life. reply sgarland 18 hours agorootparentI didn’t say that everyone is an idiot. I implied that gate keeping is useful as a first pass against people who are unlikely to have the drive to keep going when they experience difficulty. When I was a kid, docs were literally a book. If you asked for help and didn’t cite what you had already tried / read, you’d be told to RTFM. Python has several problems. Its relative import system is deranged, packaging is a mess, and yes, on its face needing to run a parallel copy of the interpreter to pip install something is absurd. I still love it. It’s baked into every *nix distro, a REPL is a command away, and its syntax is intuitive. I maintain that the relative ease of JS – and more powerfully, Node – has created a monstrous ecosystem of poorly written software, with its adherents jumping to the latest shiny every few months because this time, it’s different. And I _like_ JS (as a frontend language). reply pletnes 7 hours agorootparentprevThis is the truth right here. The issues are with people using (not officially) deprecated tools and workflows, plus various half baked scripts that solved some narrow use cases. reply Arcanum-XIII 21 hours agorootparentprevAll is well, then, one day, you have to update one library. Some days later, in some woods or cave, people will hear your screams of rage and despair. reply Galanwe 21 hours agorootparentBeen using python for 15 years now, and these screams were never heard. Dev/test with relaxed pip installs, freeze deployment dependencies with pip freeze/pip-tools/poetry/whateveryoulike, and what's the problem? reply neeleshs 20 hours agorootparentsame here. Been using python/pip for 10+ years and this was never a problem. In the java world, there is jar hell, but it was never a crippling issue, but a minor annoyance once a year or so. In general, is dependency management such a massive problem it is made to be on HN? Maybe people here are doing far more complex/different things than I've done in the past 20 years reply mixmastamyk 16 hours agorootparentprevGuessing that folks who write such things are lacking sysad skills like manipulating paths, etc. It does take Python expertise to fix other issues on occasion but they are fixable. Why I think flags like ‘pip —break-system-packages’ are silly. It’s an optimization for non-users over experienced ones. reply est 21 hours agorootparentprevDeps in CPython are more about .so/.dll problem, not much can be done since stuff happens outside python itself. reply whoiscroberts 16 hours agoparentprevOptional static typing, not really. Those type hints are not used at runtime for performance. Type hint a var as a string then set it to an init, that code still gonna try to execute. reply zarzavat 7 hours agorootparent> Those type hints are not used at runtime for performance. This is not a requirement for a language to be statically typed. Static typing is about catching type errors before the code is run. > Type hint a var as a string then set it to an int, that code still gonna try to execute. But it will fail type checking, no? reply mondrian 4 hours agorootparentThe critique is that \"static typing\" is not really the right term to use, even if preceded by \"optional\". \"Type hinting\" or \"gradual typing\" maybe. In static typing the types of variables don't change during execution. reply zarzavat 3 hours agorootparentIf there’s any checking of types before program runs then it’s static typing. Gradual typing is a form of static typing that allows you to apply static types to only part of the code. I’m not sure what you mean by variables not changing types during execution in statically typed languages. In many statically typed languages variables don’t exist at runtime, they get mapped to registers or stack operations. Variables only exist at runtime in languages that have interpreters. Aside from that, many statically typed languages have a way to declare dynamically typed variables, e.g. the dynamic keyword in C#. Or they have a way to declare a variable of a top type e.g. Object and then downcast. reply mondrian 2 hours agorootparentPython is dynamically typed because it type-checks at runtime, regardless of annotations or what mypy said. reply neonsunset 3 hours agorootparentprev'dynamic' in C# is considered a design mistake and pretty much no codebase uses it. On the other hand F# is much closer to the kind of gradual typing you are discussing. reply ramses0 21 hours agoparentprevYou forgot: [X] print requires parentheses reply zarzavat 7 hours agorootparentprint was way better when it was a statement. reply nine_k 15 hours agorootparentprevFair. But it was importable from __future__ back in 2.7. reply vulnbludog 9 hours agorootparentprevIdk why but python 2 print still pops up in my nightmares lol on bro reply alfalfasprout 21 hours agoparentprevThe conda-forge ecosystem is making big strides in dependency management. No more are we stuck with the abysmal pip+venv story. reply falcor84 18 hours agorootparentI definitely like some aspects of conda, but at least pip doesn't give me these annoying infinite \"Solving environment\" loops [0]. [0] https://stackoverflow.com/questions/56262012/conda-install-t... reply setopt 9 hours agorootparentThat issue is fixed by using the libmamba resolver: https://www.anaconda.com/blog/a-faster-conda-for-a-growing-c... reply nhumrich 14 hours agoparentprevPython 3.12 introduces a little bit of JIT. Also, there is always pypy. For efficient dependency management, there is now rye and UV. So maybe you can check all those boxes? reply nine_k 13 hours agorootparentRye is pretty alpha, uv is young, too, and they are not part of \"core\" Python, not under the Python Foundation umbrella (like e.g. mypy is). So there's plenty of well-founded hope, but the boxes are still not checked. reply GTP 21 hours agoparentprevI don't get how this optional static typing works. I had a quick look at [1], and it begins with a note saying that Python's runtime doesn't enforce types, leaving the impression that you need to use third-party tools to do actual type checking. But then it continues just like Python does the check. Consider that I'm not a Python programmer, but the main reason I stay away from it is the lack of a proper type system. If this is going to change, I might reconsider it. [1] https://docs.python.org/3/library/typing.html reply sveiss 21 hours agorootparentThe parser supports the type hint syntax, and the standard library provides various type hint related objects. So you can do things like “from typing import Optional” to bring Optional into scope, and then annotate a function with -> Optional[int] to indicate it returns None or an int. Unlike a system using special comments for type hints, the interpreter will complain if you make a typo in the word Optional or don’t bring it into scope. But the interpreter doesn’t do anything else; if you actually return a string from that annotated function it won’t complain. You need an external third party tool like MyPy or Pyre to consume the hint information and produce warnings. In practice it’s quite usable, so long as you have CI enforcing the type system. You can gradually add types to an existing code base, and IDEs can use the hint information to support code navigation and error highlighting. reply quotemstr 15 hours agorootparent> In practice it’s quite usable It would be super helpful if the interpreter had a type-enforcing mode though. All the various external runtime enforcement packages leave something to be desired. reply setopt 8 hours agorootparentI agree. There are usable third-party runtime type checkers though. I like Beartype, which lets you add a decorator @beartype above any function or method, and it’ll complain at runtime if arguments or return values violate the type hints. I think runtime type checking is in some ways a better fit for a highly dynamic language like Python than static type checking, although both are useful. reply nine_k 21 hours agorootparentprevAt MPOW most Python code is well-type-hinted, and mypy and pyright are very helpful at finding issues, and also for stuff like code completion and navigation, e.g. \"go to the definition of the type of this variable\". Works pretty efficiently. BTW, Typescript also does not enforce types at runtime. Heck, C++ does not enforce types at runtime either. It does not mean that their static typing systems don't help during at development time. reply GTP 21 hours agorootparent> BTW, Typescript also does not enforce types at runtime. Heck, C++ does not enforce types at runtime either. It does not mean that their static typing systems don't help during at development time. Speaking of C here as I don't have web development experience. The static type system does help, but in this case, it's the compiler doing the check at compile time to spare you many surprises at runtime. And it's part of the language's standard. Python itself doesn't do that. Good that you can use external tools, but I would prefer if this was part of Python's spec. Edit: these days I'm thinking of having a look at Mojo, it seems to do what I would like from Python. reply kortex 18 hours agorootparenthttps://github.com/python/mypy > Python itself doesn't do that The type syntax is python. MyPy is part of Python. It's maintained by the python foundation. Mypy is not part of CPython because modularity is good, the same way that ANSI C doesn't compile anything, that's what gcc, clang, etc are for. Mojo is literally exactly the same way, the types are optional, and the tooling handles type checking and compilation. reply GTP 2 hours agorootparent> Mojo is literally exactly the same way. No, because in Mojo, type checking is part of the language specification: you need no external tool for that. Python defines a syntax that can be used for type checking, but you need an external tool to do that. GCC does type checking because it's defined in the language specification. You would have a situation analogous to Python only if you needed GCC + some other tool for type checking. This isn't the case. reply zo1 2 hours agorootparentYou're really splitting hairs here all to prove some sort of \"type checking isn't included with python\" property. Even if you're technically right, half the world really doesn't care and most code being churned out in Python is type-hinted, type-checked, and de-facto enforced at design time. It's honestly winning the long-term war because traditional languages have really screwed things up with infinite and contrived language constructs and attempts just to satisfy some \"language spec\" and \"compiler\", whilst still trying to be expressive enough for what developers need and want to do safely. Python side-stepped all of that, has the perfect mix of type-checking and amazing \"expressibility\", and is currently proving that it's the way forward with no stopping it. reply GTP 2 hours agorootparentI'm not saying that no one should use Python, I'm just saying why I don't like it. But if you instead like it I will not try to stop you using it. This said, if most people use type hints and the proper tooling to enforce type checking, I would say this would be a good reason to properly integrate (optional) static typing in the language: it shows that most programmers like static typing. The problem I focused on in my example isn't the only advantage of a type system. reply davepeck 21 hours agorootparentprevThird party tools (mypy, pyright, etc) are expected to check types. cpython itself does not. This will run just fine: python -c \"x: int = 'not_an_int'\" My opinion is that with PEP 695 landing in Python 3.12, the type system itself is starting to feel robust. These days, the python ecosystem's key packages all tend to have extensive type hints. The type checkers are of varying quality; my experience is that pyright is fast and correct, while mypy (not having the backing of a Microsoft) is slower and lags on features a little bit -- for instance, mypy still hasn't finalized support for PEP 695 syntax. reply zitterbewegung 21 hours agorootparentprevOptional static typing is just like a comment (real term is annotation) of the input variable(s) and return variable(s). No optimization is performed. Using a tool such as mypy that kicks off on a CI/CD process technically enforces types but they are ignored by the interpreter unless you make a syntax error. reply nine_k 21 hours agorootparentA language server in your IDE kicks in much earlier, and is even more helpful. reply zitterbewegung 18 hours agorootparentI haven't used an IDE that has that but it is still just giving you a hint that there is an error and the interpreter is not throwing an error which was my point. reply setopt 8 hours agorootparent> I haven't used an IDE that has that You don’t need an IDE for this, an LSP plugin + Pyright is sufficient to get live type checking. For instance, Emacs (Eglot), Vim (ALE), Sublime (SublimeLSP) all support Pyright with nearly no setup required. reply kortex 17 hours agorootparentprevThat's true of most compiled languages. Unless we are talking about asserts, reflection, I think type erasure, and maybe a few other concepts, language runtimes don't check types. C does not check types at runtime. You compile it and then rely on control of invariants and data flow to keep everything on rails. In python, this is tricky because everything is behind at least one layer of indirection, and thus virtually everything is mutable, so it's hard to enforce total control of all data structures. But you can get really close with modern tooling. reply cwalv 16 hours agorootparent>> and the interpreter is not throwing an error which was my point. > That's true of most compiled languages True of most statically typed languages (usually no need to check at runtime), but not true in Python or other dynamically typed languages. Python would have been unusable for decades (prior to typehints) if that was true. reply kortex 2 hours agorootparentThat's just reflection. That's a feature of code, not language runtime. I think there are some languages which in fact have type checking in the runtime as a bona-fide feature. Most won't, unless you do something like isinstance() reply zo1 2 hours agorootparentprev> \"I haven't used an IDE that has that but it is still just giving you a hint that there is an error and the interpreter is not throwing an error which was my point.\" At this point, I'm not sure how one is to take your opinion on this matter. Just like me coding some C# or Java in notepad and then opining to a Java developer audience about the state of their language and ecosystem. reply kortex 17 hours agorootparentprevNope. Type annotations can be executed and accessed by the runtime. That's how things like Pydantic, msgspec, etc, do runtime type enforcement and coercion. There are also multiple compilers (mypyc, nuitka, others I forget) which take advantage of types to compile python to machine code. reply wk_end 21 hours agorootparentprevThe interpreter does not and probably never will check types. The annotations are treated as effectively meaningless at runtime. External tools like mypy can be run over your code and check them. reply cwalv 16 hours agorootparentIt checks types .. it doesn't check type annotations. Just try: $ Python >>> 1 + '3' reply NegativeK 17 hours agorootparentprevPython's typing must accommodate Python's other goal as quick scripting language. The solution is to document the optional typing system as part of the language's spec and let other tools do the checking, if a user wants to use them. The other tools are trivially easy to set up and run (or let your IDE run for you.) As in, one command to install, one command to run. It's an elegant compromise that brings something that's sorely needed to Python, and users will spend more time loading the typing spec in their browser than they will installing the type checker. reply hot_gril 21 hours agorootparentprevI think static typing is a waste of time, but given that you want it, I can see why you wouldn't want to use Python. Its type-checking is more half-baked and cumbersome than other languages, even TS. reply nine_k 20 hours agorootparentI used to think like that until I tried. There are areas where typing is more important: public interfaces. You don't have to make every piece of your program well-typed. But signatures of your public functions / methods matter a lot, and from them types of many internal things can be inferred. If your code has a well-typed interface, it's pleasant to work with. If interfaces of the libraries you use are well-typed, you have easier time writing your code (that interacts with them). Eventually you type more and more code you write and alter, and keep reaping the benefits. reply simonw 20 hours agorootparentThis was the thing that started to bring me around to optional typing as well. It makes the most sense to me as a form of documentation - it's really useful to know what types are expected (and returned) by a Python function! If that's baked into the code itself, your text editor can show inline information - which saves you from having to go and look at the documentation yourself. I've started trying to add types to my libraries that expose a public API now. I think it's worth the extra effort just for the documentation benefit it provides. reply hot_gril 20 hours agorootparentThis is what made me give it a shot in TS, but the problem is your types at interface boundaries tend to be annoyingly complex. The other problem is any project with optional types soon becomes a project with required types everywhere. There might be more merit in widely-used public libraries, though. I don't make those. reply hot_gril 20 hours agorootparentprevI shouldn't have said it's a waste of time period, cause every project I work on does have static typing in two very important places: the RPC or web API (OpenAPI, gRPC, whatever it is), and the relational database. But not in the main JS or Py code. That's all I've ever needed. I did try migrating a NodeJS backend to TS along with a teammate driving that effort. The type-checking never ended up catching any bugs, and the extra time we spent on that stuff could've gone into better testing instead. So it actually made things more dangerous. reply grumpyprole 19 hours agorootparentprevA type checker is only going to add limited value if you don't put the effort in yourself. If everything string-like is just a string, and if data is not parsed into types that maintain invariants, then little is being constrained and there is little to \"check\". It becomes increasingly difficult the more sophisticated the type system is, but in some statically typed languages like Coq, clever programmers can literally prove the correctness of their program using the type system. Whereas a unit test can only prove the presence of bugs, not their absence. reply baq 21 hours agorootparentprevTypescript is pretty much the gold standard, it’s amazing how much JavaScript madness you can work around just on the typechecking level. IMHO Python should shamelessly steal as much typescript’s typing as possible. It’s tough since the Microsoft typescript team is apparently amazing at what they do so for now it’s a very fast moving target but some day… reply hot_gril 20 hours agorootparentWell the TS tooling is more painful in ways. It's not compatible with some stuff like the NodeJS profiler. Also there's some mess around modules vs \"require\" syntax that I don't understand fully but TS somehow plays a role. reply GTP 20 hours agorootparentprevI instead think that the lack of static typing is a waste of time, since without it you can have programs that waste hours of computation due to an exception that would have been prevented by a proper type system ;) reply VeejayRampay 21 hours agorootparentprevpython will never be \"properly typed\" what it has is \"type hints\" which is way to have richer integration with type checkers and your IDE, but will never offer more than that as is reply kortex 17 hours agorootparents/will never be/already is/g https://github.com/mypyc/mypyc You can compile python to c. Right now. Compatibility with extensions still needs a bit of work. But you can write extremely strict python. That's without getting into things like cython. reply infamia 18 hours agorootparentprev> what it has is \"type hints\" which is way to have richer integration with type checkers and your IDE, but will never offer more than that as is Python is strongly typed and it's interpreter is type aware of it's variables, so you're probably overreaching with that statement. Because Python's internals are type aware, it's how folks are able to create type checkers like mypy and pydantic both written in Python. Maybe you're thinking about TS/JSDoc, which is just window dressing for IDEs to display hints as you described? reply GTP 2 hours agorootparentI don't think you can say that a language is strongly typed if only the language's internals are. The Python interpreter prevents you from summing an integer to a string, but only at runtime when in many cases it's already too late. A strongly typed language would warn you much sooner. reply hot_gril 21 hours agorootparentprevIt is properly typed: it has dynamic types :) reply GTP 20 hours agorootparentThen we have very different ideas of what proper typing is :D Look at this function, can you tell me what it does? def plus(x, y): return x+y If your answer is among the lines of \"It returns the sum x and y\" then I would ask you who said that x and y are numbers. If these are strings, it concatenates them. If instead you pass a string and a number, you will get a runtime exception. So not only you can't tell what a function does just by looking at it, you can't even know if the function is correct (in the sense that will not raise an exception). reply andrewaylett 20 hours agorootparentIt calls x.__add__(y). Python types are strictly specified, but also dynamic. You don't need static types in order to have strict types, and indeed just because you've got static types (in TS, for example) doesn't mean you have strict types. A Python string is always a string, nothing is going to magically turn it into a number just because it's a string representation of a number. The same (sadly) can't be said of Javascript. reply GTP 2 hours agorootparent> It calls x.__add__(y) Your answer doesn't solve the problem, it just moves it: can you tell me what x. __add__(y) does? reply hot_gril 19 hours agorootparentprevYeah and even with static typing, a string can be many things. Some people even wrap their strings into singleton structs to avoid something like sending a customerId string into a func that wants an orderId string, which I think is overkill. Same with int. reply vhcr 19 hours agorootparentprevIn theory it's nice that the compiler would catch those kinds of problems, but in practice it doesn't matters. reply GTP 2 hours agorootparentIt can matter also in practice. Once I was trying some Python ML model to generate images. My script ran for 20 minutes to then terminate with an exception when it arrived at the point of saving the result to a file. The reason is that I wanted to concatenate a counter to the file name, but forgot to wrap the integer into a call to str(). 20 minutes wasted for an error that other languages would have spotted before running the script. reply nequo 20 hours agorootparentprevBoth Haskell and OCaml can raise exceptions for you, yet most people would say that they are properly typed. The plus function you wrote is not more confusing than any generic function in a language that supports that. reply hot_gril 20 hours agorootparentprevWhen is the last time you had a bug IRL caused by passing the wrong kind of thing into plus(x, y), which your tests didn't catch? reply GTP 2 hours agorootparentIt never happened to me, because I don't use Python ;) On a more serious note, your comment actually hints at an issue: unit testing is less effective without static type checking. Let's assume I would like to sum x and y. I can extensively test the function and see that it indeed correctly sums two numbers. But then I need to call the function somewhere in my code, and whether it will work as intended or not depends on the context in which the function is used. Sometimes the input you pass to a function depends from some external source outside your control, an if that's the case you have to resort to manual type checking. Or use a properly typed language. reply hot_gril 1 hour agorootparentThis isn't an actual problem people encounter in unit testing, partially because you test your outer interfaces first. Also, irl static types often get so big and polymorphic that the context matters just as much. reply mixmastamyk 16 hours agorootparentprev> you can't tell what a function does just by looking at it You just did tell us what it does by looking at it, for the 90% case at least. It might be useful to throw two lists in there as well. Throw a custom object in there? It will work if you planned ahead with dunder add and radd. If not fix, implement, or roll back. reply GTP 2 hours agorootparent> You just did tell us what it does by looking at it, for the 90% case at least The problem is that you can't know if the function is going to do what you want it to do without also looking at the context in which it is used. And what you pass as input could be dependent on external factors that you don't control. So I prefer the languages that let me know what happens in 100% of the cases. reply agumonkey 20 hours agoparentprevI'm eager to see what a simple JIT can bring to computing energy savings on python apps. reply KeplerBoy 12 hours agorootparentI'd wager the energy savings could put multiple power plants out of service. I regularly encounter python code which takes minutes to execute but runs in less than a second when replacing key parts with compiled code. reply VeejayRampay 21 hours agoparentprevthe efficient dependency management is coming, the good people of astral will take care of that with the uv-backed version of rye (initially created by Armin Ronacher with inspirations from Cargo), I'm really confident it'll be good like ruff and uv were good reply noisy_boy 17 hours agorootparentrye's habit of insisting on creating a .venv per project is a deal-breaker. I don't want .venvs spread all over my projects eating into space (made worse by the ml/LLM related mega packages). It should atleast respect activated venvs. reply nine_k 15 hours agorootparentA venv per project is a very sane way. Put them into the ignore file. Hopefully they also could live elsewhere in the tree. reply VeejayRampay 7 hours agorootparentprevwell that's good for you, but you're in the minority and rye will end up being a standard anyway, just like uv and ruff, because they're just so much better than the alternatives reply Sparkyte 21 hours agoprevMy body is ready. I love python because the ease of writing and logic. Hopefully the more complicated free-threaded approach is comprehensive enough to write it like we traditionally write python. Not saying it is or isn't I just haven't dived enough into python multithreading because it is hard to put those demons back once you pull them out. reply ameliaquining 21 hours agoparentThe semantic changes are negligible for authors of Python code. All the complexity falls on the maintainers of the CPython interpreter and on authors of native extension modules. reply stavros 20 hours agorootparentWell, I'm not looking forward to the day when I upgrade my Python and suddenly I have to debug a ton of fun race conditions. reply dagenix 19 hours agorootparentAs I understand it, if your code would have race conditions with free threaded python, than it probably already has them. reply stavros 18 hours agorootparentNot when there's a global interpreter lock. reply pdonis 18 hours agorootparentThe GIL does not prevent race conditions in your Python code. It only prevents race conditions in internal data structures inside the interpreter and in atomic operations, i.e., operations that take a single Python bytecode. But many things that appear atomic in Python code take more than one Python bytecode. The GIL gives you no protection if you do such operations in multiple threads on the same object. reply stavros 17 hours agorootparentI answered in a sibling reply: https://news.ycombinator.com/item?id=40950798 reply pdonis 17 hours agorootparentI'll respond there. reply vulnbludog 9 hours agorootparentI’m a dummy don’t know nothing about coding but I love HN usernames lol reply matsemann 9 hours agorootparentprevI think what many will experience, is that they want to switch to multithreading without GIL, but learn they have code that will have race conditions. But that don't have race conditions today, as it's run as multiple processes, and not threads. For instance our webserver. It uses multiple processes. Each request then can modify some global variable, use as cache or whatever, and only after it's completely done handling the request the same process will serve a new request. But when people see the GIL is gone, they probably would like to start using it. Can handle more requests without spamming processes / using beefy computer with lots of RAM etc. And then one might discover new race conditions one never really had before. reply dagenix 18 hours agorootparentprevAre you writing an extension or Python code? If you are writing Python code, the GIL can already be dropped at pretty much any point and there isn't much way of controlling when. Iirc, this includes in the middle of things like +=. There are some operations that Python defines as atomic, but, as I recall, there aren't all that many. In what way is the GIL preventing races for your use case? reply d0mine 14 hours agorootparentIt is not about your code, it is about C extensions you are relying on. Without GIL, you can't even be sure that refcounting works reliably. Bugs in C extensions are always possible. No GIL makes them more likely. Even if you are not the author of C extension, you have to debug the consequences. reply dudus 11 hours agorootparentDoes that mean rewriting all the extensions to Rust? Or maybe CPython itself? Would that be enough to make Python no gill viable? reply stavros 18 hours agorootparentprevI mean that, if the GIL didn't prevent races, it would be trivially removable. Races that are already there in people's Python code have probably been debugged (or at least they are tolerated), so there are some races that will happen when the GIL is removed, and they will be a surprise. reply dagenix 17 hours agorootparentThe GIL prevents the corruption of Pythons internal structures. It's hard to remove because: 1. Lots of extensions, which can control when they release the GIL unlike regular Python code, depend on it 2. Removing the GIL requires some sort of other mechanism to protect internal Python stuff 3. But for a long time, such a mechanism was resisted by th Python team because all attempts to remove the GIL either made single threaded code slower or were considered too complicated. But, as far as I understand, the GIL does somewhere between nothing and very little to prevent races in pure Python code. And, my rough understanding, is that removing the GIL isn't expected to really impact pure Python code. reply stavros 17 hours agorootparentHmm, that's interesting, thank you. I didn't realize extensions can control the GIL. reply dagenix 17 hours agorootparentYup, I think its described here: https://docs.python.org/3/c-api/init.html#releasing-the-gil-.... My understanding, is that many extensions will release the GIL when doing anything expensive. So, if you are doing CPU or IO bound operations in an extension _and_ you are calling that operation in multiple threads, even with the GIL you can potentially fully utilize all of the CPUs in your machine. reply pdonis 17 hours agorootparentprev> removing the GIL isn't expected to really impact pure Python code. If your Python code assumes it's just going to run in a single thread now, and it is run in a single thread without the GIL, yes, removing the GIL will make no difference. reply dagenix 17 hours agorootparent> If your Python code assumes it's just going to run in a single thread now, and it is run in a single thread without the GIL, yes, removing the GIL will make no difference. I'm not sure I understand your point. Yes, singled thread code will run the same with or without the GIL. My understanding, was that multi-threaded pure-Python code would also run more or less the same without the GIL. In that, removing the GIL won't introduce races into pure-Python code that is already race free with the GIL. (and that relatedly, pure-Python code that suffers from races without the GIL also already suffers from them with the GIL) Are you saying that you expect that pure-Python code will be significantly impacted by the removal of the GIL? If so, I'd love to learn more. reply pdonis 16 hours agorootparent> removing the GIL won't introduce races into pure-Python code that is already race free with the GIL. What do you mean by \"race free\"? Do you mean the code expects to be run in multiple threads and uses the tools provided by Python, such as locks, mutexes, and semaphores, to ensure thread safety, and has been tested to ensure that it is race free when run multi-threaded? If that is what you mean, then yes, of course such code will still be race free without the GIL, because it was never depending on the GIL to protect it in the first place. But there is a lot of pure Python code out there that is not written that way. Removal of the GIL would allow such code to be naively run in multiple threads using, for example, Python's support for thread pools. Anyone under the impression that removing the GIL was intended to allow this sort of thing without any further checking of the code is mistaken. That is the kind of thing my comment was intended to exclude. reply dagenix 15 hours agorootparent> But there is a lot of pure Python code out there that is not written that way. Removal of the GIL would allow such code to be naively run in multiple threads using, for example, Python's support for thread pools. I guess this is what I don't understand. This code could already be run in multiple threads today, with a GIL. And it would be broken - in all the same ways it would be broken without a GIL, correct? > Anyone under the impression that removing the GIL was intended to allow this sort of thing without any further checking of the code is mistaken. That is the kind of thing my comment was intended to exclude. Ah, so, is your point that removing the GIL will cause people to take non-multithread code and run it in multiple threads without realizing that it is broken in that context? That its not so much a technical change, but a change of perception that will lead to issues? reply pdonis 15 hours agorootparent> This code could already be run in multiple threads today, with a GIL. Yes. > And it would be broken - in all the same ways it would be broken without a GIL, correct? Yes, but the absence of the GIL would make race conditions more likely to happen. > is your point that removing the GIL will cause people to take non-multithread code and run it in multiple threads without realizing that it is broken in that context? Yes. They could run it in multiple threads with the GIL today, but as above, race conditions might not show up as often, so it might not be realized that the code is broken. But also, with the GIL there is the common perception that Python doesn't do multithreading well anyway, so it's less likely to be used for that. With the GIL removed, I suspect many people will want to use multithreading a lot more in Python to parallelize code, without fully realizing the implications. reply dagenix 14 hours agorootparent> Yes, but the absence of the GIL would make race conditions more likely to happen. Does it though? I'm not saying it doesn't, I'm quite curious. Switching between threads with the GIL is already fairly unpredictable from the perspective of pure-Python code. Does it get significantly more troublesome without the GIL? > Yes. They could run it in multiple threads with the GIL today, but as above, race conditions might not show up as often, so it might not be realized that the code is broken. But also, with the GIL there is the common perception that Python doesn't do multithreading well anyway, so it's less likely to be used for that. With the GIL removed, I suspect many people will want to use multithreading a lot more in Python to parallelize code, without fully realizing the implications. Fair reply pdonis 1 hour agorootparent> Switching between threads with the GIL is already fairly unpredictable from the perspective of pure-Python code. But it still prevents multiple threads from running Python bytecode at the same time: in other words, at any given time, only one Python bytecode can be executing in the entire interpreter. Without the GIL that is no longer true; an arbitrary number of threads can all be executing a Python bytecode at the same time. So even Python-level operations that only take a single bytecode now must be protected to be thread-safe--where under the GIL, they didn't have to be. That is a significant increase in the \"attack surface\", so to speak, for race conditions in the absence of thread safety protections. (Note that this does mean that even multi-threaded code that was race-free with the GIL due to using explicit locks, mutexes, semaphores, etc., might not be without the GIL if those protections were only used for multi-bytecode operations. In practice, whether or not a particular Python operation takes a single bytecode or multiple bytecodes is not something you can just read off from the Python code--you have to either have intimate knowledge of the interpreter's internals or you have to explicitly disassemble each piece of code and look at the bytecode that is generated. Of course the vast majority of programmers don't do that, they just use thread safety protections for every data mutation, which will work without the GIL as well as with it.) reply pdonis 17 hours agorootparentprev> if the GIL didn't prevent races, it would be trivially removable Nobody is saying the GIL doesn't prevent races at all. We are saying that the GIL does not prevent races in your Python code. It's not \"trivially removable\" because it does prevent races in the interpreter's internal data structures and in operations that are done in a single Python bytecode, and there are a lot of possible races in those places. Also, perhaps you haven't considered the fact that Python provides tools such as mutexes, locks, and semaphores to help you prevent races in your Python code. Python programmers who do write multi-threaded Python code (for example, code where threads spend most of their time waiting on I/O, which releases the GIL and allows other threads to run) do have to use these tools. Why? Because the GIL by itself does not prevent races in your Python code. You have to do it, just as you do with multi-threaded code in any language. > Races that are already there in people's Python code have probably been debugged Um, no, they haven't, because they've never been exposed to multi-threading. Most people's Python code is not written to be thread-safe, so it can't safely be parallelized as it is, GIL or no GIL. reply teaearlgraycold 20 hours agorootparentprevIt's kept behind a flag. Hopefully will be forever. reply metadat 19 hours agorootparentThe article states the goal is to eventually (after some years of working out the major kinks and performance regressions) promote Free-Threaded Python to be the default cPython distribution. reply stavros 19 hours agorootparentprevOh, very interesting, that's a great solution then. reply geekone 19 hours agorootparentLooks like according to the PEP it may eventually be default in 4-6 releases down the road: https://peps.python.org/pep-0703/#python-build-modes reply hot_gril 20 hours agoparentprevWhat are the common use cases for threading in Python? I feel like that's a lower level tool than most Python projects would want, compared to asyncio or multiprocessing.Pool. JS is the most comparable thing to Python, and it got pretty darn far without threads. reply BugsJustFindMe 20 hours agorootparentWorking with asyncio sucks when all you want is to be able to do some things in the background, possibly concurrently. You have to rewrite the worker code using those stupid async await keywords. It's an obnoxious constraint that completely breaks down when you want to use unaware libraries. The thread model is just a million times easier to use because you don't have to change the code. reply hot_gril 20 hours agorootparentAsyncio is designed for things like webservers or UIs where some framework is probably already handling the main event loop. What are you doing where you just want to run something else in the background, and IPC isn't good enough? reply BugsJustFindMe 20 hours agorootparentNon-blocking HTTP requests is an extremely common need, for instance. Why the hell did we need to reinvent special asyncio-aware request libraries for it? It's absolute madness. Thread pools are much easier to work with. > where some framework is probably already handling the main event loop This is both not really true and also irrelevant. When you need a flask (or whatever) request handler to do parallel work, asyncio is still pretty bullshit to use vs threads. reply hot_gril 20 hours agorootparentNon-blocking HTTP request is the bread and butter use case for asyncio. Most JS projects are doing something like this, and they don't need to manage threads for it. You want to manage your own thread pool for this, or are you going to spawn and kill a thread every time you make a request? reply BugsJustFindMe 20 hours agorootparent> Non-blocking HTTP request is the bread and butter use case for asyncio And the amount of contorting that has to be done for it in Python would be hilarious if it weren't so sad. > Most JS projects I don't know what JavaScript does, but I do know that Python is not JavaScript. > You want to manage your own thread pool for this... In Python, concurrent futures' ThreadPoolExecutor is actually nice to use and doesn't require rewriting existing worker code. It's already done, has a clean interface, and was part of the standard library before asyncio was. reply hot_gril 20 hours agorootparentThreadPoolExecutor is the most similar thing to asyncio: It hands out promises, and when you call .result(), it's the same as await. JS even made its own promises implicitly compatible with async/await. I'm mentioning what JS does because you're describing a very common JS use case, and Python isn't all that different. If you have async stuff happening all over the place, what do you use, a global ThreadPoolExecutor? It's not bad, but a bit more cumbersome and probably less efficient. You're running multiple OS threads that are locking, vs a single-threaded event loop. Gets worse the more long-running blocking calls there are. Also, I was originally asking about free threads. GIL isn't a problem if you're just waiting on I/O. If you want to compute on multiple cores at once, there's multiprocessing, or more likely you're using stuff like numpy that uses C threads anyway. reply BugsJustFindMe 20 hours agorootparent> Python isn't all that different Again, Python's implementation of asyncio does not allow you to background worker code without explicitly altering that worker code to be aware of asyncio. Threads do. They just don't occupy the same space. > Also, I was originally asking about free threads...there's multiprocessing Eh, the obvious reason to not want to use separate processes is a desire for some kind of shared state without the cost or burden of IPC. The fact that you suggested multiprocessing.Pool instead of concurrent_futures.ProcessPoolExecutor and asked about manual pool management feels like it tells me a little bit about where your head is at here wrt Python. reply hot_gril 19 hours agorootparentBasically true in JS too. You're not supposed to do blocking calls in async code. You also can't \"await\" an async call inside a non-async func, though you could fire-and-forget it. Right, but how often does a Python program have complex shared state across threads, rather than some simple fan-out-fan-in, and also need to take advantage of multiple cores? reply nilamo 19 hours agorootparentThe primary thing that tripped me up about async/await, specifically only in Python, is that the called function does not begin running until you await it. Before that moment, it's just an unstarted generator. To make background jobs, I've used the class-based version to start a thread, then the magic method that's called on await simply joins the thread. Which is a lot of boilerplate to get a little closer to how async works in (at least) js and c#. reply LegionMammal978 18 hours agorootparentRust's version of async/await is the same in that respect, where futures don't do anything until you poll them (e.g., by awaiting them): if you want something to just start right away, you have to call out to the executor you're using, and get it to spawn a new task for it. Though to be fair, people complain about this in Rust as well. I can't comment much on it myself, since I haven't had any need for concurrent workloads that Rayon (a basic thread-pool library with work stealing) can't handle. reply kristjansson 12 hours agorootparentprevThere is also https://docs.python.org/3/library/asyncio-task.html#eager-ta... if you want your task to start on creation. reply pests 16 hours agorootparentprevThat is a common split in language design decisions. I think the argument for the python-style where you have to drive it to begin is more useful as you can always just start it immediately but also let's you delay computation or pass it around similar to a Haskell thunk. reply stavros 20 hours agorootparentprevI feel you. I know asyncio is \"the future\", but I usually just want to write a background task, and really hate all the gymnastics I have to do with the color of my functions. reply BugsJustFindMe 20 hours agorootparentI feel like \"asyncio is the future\" was invented by the same people who think it's totally normal to switch to a new javascript web framework every 6 months. reply hot",
    "originSummary": [
      "Free-threaded CPython, a major change in CPython 3.13, allows multiple threads to run in parallel within the same interpreter, making the Global Interpreter Lock (GIL) optional (PEP 703).",
      "This experimental feature aims to improve multi-threaded performance by effectively utilizing multiple CPU cores, though it presents challenges like thread-safety and ABI incompatibility.",
      "The community is working on compatibility, starting with the PyData stack, and aims to provide cp313t wheels on PyPI for Python 3.13, with ongoing efforts to resolve thread-safety issues in packages like numpy and pywavelets."
    ],
    "commentSummary": [
      "Free-threaded CPython is now available for experimentation, promising significant performance improvements with minimal effort once key libraries support no GIL (Global Interpreter Lock).",
      "This development could enable newer projects to gain market share if older libraries fail to adapt quickly, simplifying the use of all cores on a machine without the overhead of multiprocessing.",
      "Python 3.14 will change the default multiprocessing method from fork to spawn or forkserver, addressing some issues, but the transition to free-threading may still introduce challenges like ensuring fork-safe code and handling concurrency bugs."
    ],
    "points": 443,
    "commentCount": 310,
    "retryCount": 0,
    "time": 1720813972
  },
  {
    "id": 40950235,
    "title": "Crafting Interpreters",
    "originLink": "https://craftinginterpreters.com/",
    "originBody": "Ever wanted to make your own programming language or wondered how they are designed and built? If so, this book is for you. Crafting Interpreters contains everything you need to implement a full-featured, efficient scripting language. You’ll learn both high-level concepts around parsing and semantics and gritty details like bytecode representation and garbage collection. Your brain will light up with new ideas, and your hands will get dirty and calloused. It’s a blast. Starting from main(), you build a language that features rich syntax, dynamic typing, garbage collection, lexical scope, first-class functions, closures, classes, and inheritance. All packed into a few thousand lines of clean, fast code that you thoroughly understand because you write each one yourself. The book is available in four delectable formats: Print 640 pages of beautiful typography and high resolution hand-drawn illustrations. Each page lovingly typeset by the author. The premiere reading experience. Amazon.com .ca .uk .au .de .fr .es .it .jp Barnes and Noble Book Depository Download Sample PDF eBook Carefully tuned CSS fits itself to your ebook reader and screen size. Full-color syntax highlighting and live hyperlinks. Like Alan Kay's Dynabook but real. Kindle Amazon.com .uk .ca .au .de .in .fr .es .it .jp .br .mx Apple Books Play Books Google Nook B&N EPUB Smashwords PDF Perfectly mirrors the hand-crafted typesetting and sharp illustrations of the print book, but much easier to carry around. Buy from Payhip Download Free Sample Web Meticulous responsive design looks great from your desktop down to your phone. Every chapter, aside, and illustration is there. Read the whole book for free. Really. Read Now About Robert Nystrom I got bitten by the language bug years ago while on paternity leave between midnight feedings. I cobbled together a number of hobby languages before worming my way into an honest-to-God, full-time programming language job. Today, I work at Google on the Dart language. Before I fell in love with languages, I developed games at Electronic Arts for eight years. I wrote the best-selling book Game Programming Patterns based on what I learned there. You can read that book for free too. If you want more, you can find me on Twitter (@munificentbob), email me at bob at this site's domain (though I am slow to respond), read my blog, or join my low frequency mailing list: Handcrafted by Robert Nystrom — © 2015 – 2021",
    "commentLink": "https://news.ycombinator.com/item?id=40950235",
    "commentBody": "Crafting Interpreters (craftinginterpreters.com)381 points by metadat 20 hours agohidepastfavorite111 comments liamilan 15 hours agoRead Crafting Interpreters when building Crumb (https://github.com/liam-ilan/crumb). It was indispensable, especially the sections on scope and local variables. The balance between technical implementation and conceptual insights is super helpful, especially when trying to go off of the book’s set path. It’s inspiring to see technical writing done like this. As an aspiring engineer, this sets a really high standard to aim for - excellent resource. reply apstls 57 minutes agoparent> when building Crumb (https://github.com/liam-ilan/crumb) > As an aspiring engineer I’ve got some good news for you. reply news_to_me 15 hours agoparentprevThis looks cool! How did you decide on what data types to include? reply liamilan 0 minutes agorootparentIt was my first time writing making a language, so I built into the language whatever was needed to make something cool. Wanted first class functions to simplify the parse step (they can be treated like any other value), but I needed a different mechanism to invoke “native code” vs user-defined methods, so there’s two different types for that. Needed some kind of compound data type, but I didn’t want to deal with side effects from pass by reference, so Crumb implements lists, but they are always pass by value :) reply myco_logic 7 hours agorootparentprevPersonal choice, is what I'd say. You can get a lot of mileage out of implementing a dynamic language with NaN boxing[1]. It really depends on the kind of language you're trying to build an interpreter for, and what purpose it could serve. For dynamic languages, I'd say looking at the core types of Erlang is a great place to start (Integers, Symbols, Functions, etc.). For a statically typed language things get more complex, but even with just numeric types, characters, and some kind of aggregating type like a Struct you can build up more complex data structures in your language itself. [1]: https://leonardschuetz.ch/blog/nan-boxing/ reply metadat 2 hours agorootparentReally neat article, this is my first encounter with the concept of NaN Boxing. Thanks for sharing! reply dgb23 5 hours agoprevThe secondary but perhaps equally important benefit of this book is that it teaches clarity. The text, code, structure and pacing, everything is clear and to the point. „Crafting“ is the right word, because the book feels like it’s written by and for craftspeople. reply fooker 7 hours agoprevThis book should be the second or maybe third step of your journey into PL compilers. The first step is to write an interpreter yourself, for a simple language you create, without knowing anything about interpreters or language design. The second step is to rewrite it, and make less mistakes! :) If you don't do this, you are never going to appreciate the nuances of this topic. And you are going to skip over concepts that don't seem important. reply bruce343434 6 hours agoparentFor me, this book demystified these topics and allowed me to do your second and third step in the first place :) It's okay to not come up with/reinvent from first principles every technique on your own. It's normal to stand on the shoulders of giants. reply fooker 2 hours agorootparentI'd think of it as riding a bike vs reading about riding a bike. reply kleinsch 2 hours agorootparentWhen you learn to ride a bike, someone teaches you how to do it. You could go out and fall down a few times by yourself so it’s more fulfilling when you have instruction, but I wouldn’t say that’s the most efficient path. reply macintux 2 hours agorootparentprevI’d much rather have someone, or something, providing guidance as I learned to ride a bike. Of course you’re less likely to end up with a head injury writing an interpreter, but still, it’s not exactly a crime to learn before doing. reply kccqzy 1 hour agoparentprevThat's exactly what I did. My own experience with the first step is to write a TeX-like language with macro definition and macro replacement. About a few days into the project I kept running into so many bugs that really instilled a more academic culture in me and I started to read books. But it was still a wonderful first step: without it you might not have even realized that this is a book-worthy topic. reply sunday_serif 15 hours agoprevMy favorite part of this book is that guides you through writing two separate interpreters for the same language. I think it really allows you to grasp some of the more intricate and nuanced parts about building a programming language. You can encounter all of the big ideas in the first half of the book and gain enough familiarity with them so that when you revisit them again in the second interpreter, you can actually absorb the interesting parts. Such a phenomenal book! reply runevault 14 hours agoprevSince people are talking about other compiler resources, one I have enjoyed so far though I still need to finish it, Immo Landwerth writing a compiler in c# that generates IL and also debug symbols and the like. It is older (about 5 years, so Core but like 3ish timeframe I believe) so doesn't use current c# syntax but shouldn't matter much for most of what you'd be doing other than a lot of warnings about nullability on types. https://www.youtube.com/playlist?list=PLRAdsfhKI4OWNOSfS7EUu... reply chris37879 16 hours agoprevI feel like this is a book most programmers should work through at some point or another. Doing so made me really appreciate just what's going on inside a compiler / language toolkit. It's also one of the most well written technical guides I've ever followed, it really helped me internalize the concepts, and they are useful all over the place, not just in compilers. reply sva_ 3 hours agoparentThis comment sold me. Gonna keep this as an inactive tab for the next couple months. reply BoiledCabbage 39 minutes agorootparentJust to be safe, make sure to also copy the url into a note app, or some other location where you'll never look at it again. reply phyrex 41 minutes agorootparentprevWhy not buy an copy and put it on your to-read shelf for the next couple of months? reply blackerby 20 minutes agorootparentMonths? How about years? And then start it, step away for a year, come back and start it again, then step away for a year, come back and … ? reply lifeinthevoid 1 hour agoprevWhat bothered me a little bit while following the book was that copying the code doesn’t result in compilable code all the time because there is code missing that is only introduced later. I get why the author chose to follow this path, but I’m from the club that every commit should compile and was annoyed a bit by that. reply kccqzy 1 hour agoparentI am the opposite. I think pedagogical materials like this should introduce things in the order that makes intuitive sense, and most of the time that means from the easiest to the hardest. reply metadat 1 hour agoparentprevWhat do you think the author's intention was behind doing it this way? reply eyelidlessness 1 hour agorootparentDoing it the other way is a common pitfall in teaching programming concepts. A very common example is the historically necessary boilerplate in Java’s Hello World, where quite a few concepts are present but handwaved away as you don’t need to worry about this now. The problem with this is twofold: 1. It is challenging to distinguish the pertinent from the impertinent. People who are just starting won’t be able to tell what parts of the code deserve their attention now. 2. It is challenging to retroactively draw attention to previously dismissed details once they do become pertinent. I think there are other ways to address this, with additional formatting considerations in the presentation of example code. But there’s still a problem once the reader wants to tinker with the code, as any such formatting will be lost between a carefully tailored example rendering and the user’s code editor. reply BoiledCabbage 41 minutes agorootparent> Doing it the other way is a common pitfall in teaching programming concepts. 100% disagree If someone is brand new to programming and you're expecting to teach them the following concepts: A class, methods, static methods, data types, method return types, void return type, arrays, and namespaces just to be able to write a simple \"Hello World\" app [1] I think your approach is the one that's misguided. The most common mistaken people make in teching is teaching things in the order the were discovered historically. Not always, but very often that is a distraction. The second most common is teaching from the outside in. The best way to teach is to explain what your tyring to accomplish and start with a very simple example for people to play with. Then pick on concept and modify it so they can see the results of those changes. Then pick the next concet and modify that so they can see tangibly what that means. In the hello world example, it would be starting with the program in [1] and focusing just on the constant string \"Hello World!\". Change it see what that means, understand what's going on. Then move out to the Console.WriteLine. Make a copy of that line have two lines of it then 3 to understand the method call. Then swap to Console.ReadLine to see what a different method call can do. Continue exploring out from that core concept. The \"static\" identifier on a method class is not where to start teaching someone programming. It's also why Racket removed a lot of the boiler plate from thier core teaching langauges for students. For them to not even have to see the things that are unnecessary at that point in their learning.[2] [1] https://www.geeksforgeeks.org/java-hello-world-program/ [2] https://docs.racket-lang.org/drracket/htdp-langs.html reply Arisaka1 8 hours agoprevCurious question from someone new to the programming field who lacks a formal CS background: How are books like this one are meant to be consumed? Do you read it cover to cover as you code along with the author in a way YouTube tutorials work? The main reason for asking is, I don't know if I'm lacking in natural gifts (really likely) but I can't seem to retain knowledge like that. It feels nice to onboard myself to a language or framework by doing so, but afterwards I will still struggle to connect pieces together. I'm interested to learn more on how language interpreters work, I just don't know if the format is something that will assist me. I'm trying to compliment and assist my brain with note taking after reading the note taking thread that is currently in the front page, so perhaps that will change. reply shortercode 8 hours agoparentThis book is intended to be a cover to cover job. I tackled it about 2 or 3 chapters at a time while building alongside. But after the midway point where it swaps to a C based byte code compiler I just read it instead. There are books like the dragon book which cover PL design in a more reference book style. But I don’t recommend them. If you’re looking for a lighter alternative then “writing an Interpreter in Go” is worth a look. Also Bob Nystrum has some other good material on his blog, and a chapter in game programming patterns about PL stuff. reply Measter 8 hours agoparentprevFor this book, that is absolutely what you should do. It starts you off building a simple parser and interpreter, and then has you add features to both as you progress through the book. Skipping sections can mean that you end up missing functionality you might need. reply aodonnell2536 8 hours agoparentprevWell, in the instance of this book, you should follow along with the steps that Bob Nystrom writes for you. From reading it, it’s clear you’re supposed to follow along and write code “with him”. He even outlines exactly where each line of code should go, and explains why along the way. reply anta40 1 hour agoparentprevStart at 1st chapter, try all the codes until you understand the concept. Then go to next chapter and repeat the process. reply radiospiel 8 hours agoparentprevIm my experience following - as in actually doing the steps - works wonders. reply foooorsyth 7 hours agoparentprevThis book, unlike the infamous “Dragon Book” for compilers, can be read cover to cover. The Dragon Book has great detail but it is a slog, especially in early chapters. So many students drop their first course on compilers every year because of the Dragon Book and not the actual concepts reply xigoi 10 hours agoprevDoes anyone know of a good resoucce for creating a statically typed language with stuff like parametric polymorphism and basic type inference? reply kccqzy 1 hour agoparentThe main crux of parametric polymorphism and type inference is just implementing Algorithm W. Just find a toy implementation online and poke at it. For me, when I learned it myself, I simply took the toy implementation at https://github.com/wh5a/Algorithm-W-Step-By-Step/blob/master... realized that it was written in an extremely outdated style, modernized it, cleaned up, and ended up with https://gist.github.com/kccqzy/fa8a8ae12a198b41c6339e8a5c459... Then I proceeded to change various things to \"break\" the algorithm and see how they are broken. reply rscho 9 hours agoparentprevModern compiler implementation in ML by A. W. Appel There are (inferior) versions of the same in C and Java. I'd use them together with the ML version. reply zellyn 4 hours agoparentprevIt’s a bit more theoretical, but working my way (slowly, with many re-watches of certain videos) through the Coursera compilers course was a major milestone for me: I highly recommend finding an accessible way to do one of the CS things you think only Wizards can do: once you tackle one or two such projects, and realize that it’s hard, but mostly just slogging your way through it, it can be an enormous confidence boost :-) I’m always happy to help anyone going down this path: zellyn@(most things) if anyone reading this wants to try but is feeling nervous. reply zellyn 4 hours agorootparentOne additional note: the provided code for that course is (or was a decade+ ago!) either C++ or Java, but the tests and test scaffolding are fairly straightforward. I opted to translate into Go as I went, which meant: - a lot more work/time - slower progress - no “if you fill in this missing piece, we’ve provided the rest so your compiler will work end-to-end” - an annoying amount of trying to print ASTs exactly like the C++/Java code for validation purposes But I got to use Go instead of those other languages! ;-) reply britannio 17 hours agoprevI just finished the second half, it's a great book! I recommend doing one or two of the proposed challenges in each chapter to reinforce your understanding of the content. reply trenchgun 11 hours agoprevAlright, I will finally read the book. It has been collecting dust in my bookshelf for a while. reply tusharsadhwani 11 hours agoparentI finished this book, wrote the two implementations in Python and Zig, genuinely one of the best set of projects I've ever built. reply dailykoder 6 hours agorootparentHmmm, I started the book once in powershell. I think, I finished chapter 2. Maybe I should finish the whole book with it finally. Just for the fun. (Yes, powershell. Because I didn't have any other toolchain/compiler on the windows machine at some old work place and no admin rights to install anything. So I learned powershell.) reply ayhanfuat 5 hours agorootparentprevHow was the Zig experience? I am looking for an intermediate Zig project to practice Zig. Does this require advanced features of Zig? reply cdcarter 1 hour agorootparentI’ve also ported the lox vm to Zig and had a great time working through it. Since the project is designed in C, you can mostly write the exact same code in zig, with minimal modifications. If you want to use zig features, they’re easy to integrate, but Nystrom obviously won’t be giving you any hints. But the language offers a lot of useful features (slices, optionals, error types) and makes some C paradigms syntactic realities (tagged enums, explicit pointer casts). Even more so, the standard library comes with very useful stuff (an ArrayList, a handful of different allocators, heck I replaced the trie of keywords with a StaticStringMap). it’s a fun project, I would definitely recommend it! reply cube2222 5 hours agorootparentprevI’m also implementing it in Zig right now (and haven’t done any project other than small snippets in zig before) and it’s fine. You can actually appreciate how much nicer it is to write than in C, while still being a fairly 1-1 translation. reply eternityforest 18 hours agoprevMad respect to those with that kind of dedication, and everyone working to keep all the dev infrastructure going, but I'm super glad my \"I want to make a language\" phase was just a passing interest! That's just a crazy amount of work! reply grumpyprole 12 hours agoparentIt doesn't have to be a crazy amount of work, see Lisp-in-Lisp in the original SCIP book or a lambda calculus interpreter in Haskell (fits on a screen). reply kazinator 11 hours agorootparentL-in-L interpreters assume that you already have symbols implemented, memory management implemented, reader and printer implemented, ... You could spend considerably more time implementing a Lisp dialect's math library than the interpreter. There are a lot of combinations. Where are the objects, hash tables, exception handling, ... Of course you can get some of these things from a host language, but someone had to make them. reply grumpyprole 10 hours agorootparentYes but my point is that these problems are as hard as we want to make them. For a simple language implemented in C, maybe it's fine to never free any memory until the process quits. reply kazinator 9 hours agorootparentMore like, for a simple problem or a sufficiently small instance of a complex problem being solved in C, or a language written in C (simple one or otherwise), maybe it's fine to never free memory until the process quits. reply eternityforest 11 hours agorootparentprevActually doing anything beyond weekend project complexity with that sort of thing is pretty hard though. I guess if you have a lot of fairly small ideas with a lot of novelty, I could see the appeal of a new language to express them in. reply cageface 16 hours agoprevThe author is one of the lead developers for Dart, which has evolved over time into a pretty nice language. reply throwaway032023 3 hours agoparentI think Dart is underappreciated. Almost the entire compiler is written in Dart. It has a VM, AOT compiler, FFI/Native, and first class Javascript interop with ability to compile to javascript. It has first class WASM support. Null sound safety. They have experimental support for macros. Pretty impressive. reply dmeijboom 5 hours agoprevHa, neat! I’m building a dynamic programming language in Rust called atom (https://github.com/dmeijboom/atom). It’s an interpreted language with a bytecode compiler. Will definitely check it out. reply nu11ptr 4 hours agoprevHow easy is it to follow along and yet build a statically typed language (instead of dynamically typed one as used in the book)? reply lolinder 4 hours agoparentDepends on your other goals for your language. If it's a statically typed language along the lines of TypeScript (where the static typing is just for safety, not performance) and you don't have any opinions about how it runs (compiled vs interpreted) then you can totally follow the book and then strap your type checker on later. If you have a compilation target in mind (llvm, wasm, jvm luajit), the first half of the book won't get you much closer to it (you'd have a parser) and the second half would have to be heavily adapted to output your target format instead of the highly specialized custom bytecode the book uses. And in neither case does the book directly help you with using type information to improve efficiency, so without adaptation you'd end up with a language that has static typing for safety but still does type checking at runtime. That's not a bad problem to have, you can always come back and strip away checks later as you get more confident. reply IAmLiterallyAB 15 hours agoprevPlanning to read this soon. Anyone got any other compiler book recommendations? Preferably modern (I've heard the dragon book is out of date) reply dils 2 hours agoparent\"Writing An Interpreter In Go\"[1] is also pretty good. I read it after finishing crafting interpreters. [1] https://interpreterbook.com/ reply lolinder 32 minutes agorootparentDoes it provide information or techniques that aren't already covered by Crafting Interpreters, or was it more of a refresher? reply pansa2 15 hours agoparentprevEngineering a Compiler (Cooper and Torczon) seems to be widely recommended. I’ve got the second edition but there’s now a third - not sure how significant the changes are. AFAIK the problem with the dragon book is the same as with most academic compiler courses. It spends most of its time on the theory-heavy front-end (parsing) and much less on the back-end (code generation). Real-world compilers are very much the opposite. reply bmoxb 13 hours agorootparentI think Engineering a Compiler is a great next step after Crafting Interpreters. It's both easier to get into (in terms of writing style and structure) and, as you say, generally more practical than the Dragon Book. reply eredengrin 10 hours agorootparentDo you have any thoughts about how to read some of the next step books like Engineering a Compiler/Dragon Book? I don't normally read large technical books end to end so I'm curious how others approach it. I am going through Crafting Interpreters right now and I like how it has well defined checkpoints that help me know that I can move on when I understand the current material. I skimmed Engineering a Compiler and it looks like it has exercises as well, do you recommend all/some of those, or any other methods? Also I did some searching to see past discussions of Engineering a Compiler and found this interesting comment thread from Bob Nystrom for anyone interested: https://news.ycombinator.com/item?id=21725581 reply Jtsummers 15 hours agoparentprevhttps://mitpress.mit.edu/9780262047760/essentials-of-compila... Recent, that is the Racket version and the author has a Python version, too. Based on the nanopass compiler idea. It builds out the compilers over the course of the books. reply chin7an 15 hours agoparentprevI’ve not read this book, so can’t offer a comparison but Thorsten Ball’s two books [1] are great. The dragon book was my textbook in college, brings back memories, might be outdated but some concepts should still be useful. [1] https://thorstenball.com/books/ reply signaru 9 hours agorootparentThorsten Ball's books are the closest thing to Crafting Interpreters. Not just by topic, but also with how the code is presented. By far these are among the few books I know that take the effort to guide the reader to programming something complex from scratch, while also being testable as the program grows. This means that previously presented code changes along the way as new features are added. A lot of other \"code yourself\" books, on the other hand, simply slice already finished codebases, and there's no way to test a simpler incomplete version of the program unless the reader makes extra effort and go beyond what is presented in the book. While there is a lot of overlapping topics in Nystrom's and Ball's books, there are also enough differences to learn something new from the other. Ball's books uses the same parser and AST as front ends to both tree-walking and VM interpreter. CI, on the other hand, skips the AST for the VM interpreter, and also teaches extra topics like implementing garbage collection, dictionaries/hashtables and class-based OOP. reply purple-leafy 9 hours agoprevWaiting on the shelf for me once I finish cs50 reply yodsanklai 16 hours agoprevI know this book has been praised before on HN, but I've been personally a bit disappointed. It's suitable for someone with no very little knowledge on the topic, but it doesn't really cover any advanced topic. reply lolinder 14 hours agoparentIt's generally a good idea to judge a book by its explicitly expressed audience, where applicable. In this case Bob made his audience very clear in the first few paragraphs of the book [0]: > It’s the book I wish I’d had when I first started getting into languages, and it’s the book I’ve been writing in my head for nearly a decade. > In these pages, we will walk step-by-step through two complete interpreters for a full-featured language. I assume this is your first foray into languages, so I’ll cover each concept and line of code you need to build a complete, usable, fast language implementation. > In order to cram two full implementations inside one book without it turning into a doorstop, this text is lighter on theory than others. It sounds to me like you picked up the wrong book, skipped the introduction, and then were disappointed that it wasn't directed at you. The good news is that it is free, so you aren't actually out anything but time. [0] http://craftinginterpreters.com/introduction.html reply atan2 15 hours agoparentprevI trust that you're correct, but I'm glad that this is the case. There are books designed to be introductory, and there are books that serve as reference for advanced topics and state-of-the-art techniques. The first type can often serve as an enabler for the second type. reply linhns 11 hours agoparentprevBecause it’s meant to be for those people reply brcmthrowaway 17 hours agoprevIs this book still relevant? reply metadat 17 hours agoparentYes, very much so. Why wouldn't it be? Can you recommend a better resource for learning how to create a programming language from first principles? I just started it today, which I why I felt the inclination to create this post. reply LoFiSamurai 16 hours agorootparentIt’s an incredible book. Munificent did an amazing job. reply SatvikBeri 17 hours agoparentprevIt's a 3 year old book on compilers, so if it was ever relevant once, it surely still is today! reply runevault 14 hours agoparentprevThis is not a book about bleeding edge compiler techniques. It teaches the foundations that makes it easier to learn the more complex parts of writing a compiler. Unless compiler design radically changes it will likely remain a relevant book for a very very long time. Especially since Java-adjacent languages are likely to remain in vogue and the odds of us losing C as a major language in the next few decades is basically zero. reply danielvaughn 17 hours agoparentprevDefinitely. I just went through the first 60% or so and it's great. The language is java but it's easy to translate to other languages, which would be a good learning exercise in its own right. reply linhns 11 hours agoparentprevSuper. Bob’s way of presenting his content is what I feel the most important to get out of. Great communicator. reply rassibassi 8 hours agoparentprevCFG (context free grammar), which is explained in the book, is used here together with LLMs: https://tree-diffusion.github.io reply alabhyajindal 13 hours agoprevI really wish this book used something other than Java. Nothing against Java - just that I don't know it and don't feel excited about learning it. reply jasonjmcghee 13 hours agoparentFor what it's worth, these days you could use an LLM to turn each code snippet into many other languages. Or just focus on his words and use the snippets like generic code and figure out how to do it in whatever language of choice. You don't need to \"learn java\" to become conversant or read it as pseudocode. reply alabhyajindal 13 hours agorootparentThat's great advice. Thank you! reply SatvikBeri 6 hours agoparentprevLots of people have done implementations in other languages: https://github.com/munificent/craftinginterpreters/wiki/Lox-... I did the first half in Clojure (in order to teach myself Clojure), worked just fine. I had to do a bit of translation but it's really not a lot. reply miki123211 10 hours agoparentprevThe Java it uses is basically understandable regardless of what language you're used to. I guess if you're extremely early in your career and have never written anything but Python, it might not be, but if you've ever touched any statically-typed language and a language with a somewhat C-like syntax, you should be fine. reply bluedays 12 hours agoparentprevI’m glad it used Java. I wound up rewriting the code in Python and it really helped me understand the concepts a lot better. reply WJW 8 hours agoparentprevDon't use Java then. The Java bits are just an example, you can write it in any language you want. reply dukeyukey 9 hours agoparentprevJava is pretty readable, as long as your familiar with C-style languages you shouldn't have too many problems rewriting this in another language. reply Derbasti 10 hours agoparentprevIt does. The second half is written in C. Java is just used for the simplified first part. reply snats 13 hours agoparentprevYou can do the second part that is C! reply grumpyprole 12 hours agoparentprevWhen Java gets pattern matching, it will become a more reasonable choice to write an interpreter in. reply CraigJPerry 12 hours agorootparentIt’s had pattern matching since 16, it’s been expanded since with some syntactic sugar and more recently the introduction of sum types and exhaustiveness to go along with pattern matching. reply grumpyprole 8 hours agorootparentI think there should be a second edition of this book then :) reply chasil 16 hours agoprev [–] The lex and yacc utilities are part of POSIX.2; is there any reason not to reach for them first? https://pubs.opengroup.org/onlinepubs/9699919799/utilities/l... https://pubs.opengroup.org/onlinepubs/9699919799/utilities/y... All the POSIX.2 standards for shell utilities can be found here: https://pubs.opengroup.org/onlinepubs/9699919799/utilities/ The original introduction to lex and yacc was in the book by Kernigan and Pike: https://scis.uohyd.ac.in/~apcs/itw/UNIXProgrammingEnvironmen... reply pdpi 15 hours agoparentFor a few reasons. One, because actually building the lexer and parser from scratch is a useful exercise in a learning context, which is what this is. Two, because the book wants to teach Pratt parsers, rather than LALR parsers. There’s a lot of literature out there on LALR parsers and generated parsers in general, but precious little on handrolled parsers. Covering material that hasn’t been covered to hell and back is a Good Thing. Three, because lex and yacc generate C, and the book has two implementations of the interpreter, in C and Java. You could’ve used ANTLR to generate both parsers, but lex/yacc would only cater to the C version And finally, because not everybody is on a POSIX system. Before WSL, using anything Unix-y on Windows was miserable. These days it’s mostly fine, but using WSL means you’re not actually building windows-native stuff anymore. reply 1f60c 9 hours agoparentprevBob addresses this in the introduction^: > We will abstain from using [parser generators] here. I want to ensure there are no dark corners where magic and confusion can hide, so we’ll write everything by hand. As you’ll see, it’s not as bad as it sounds, and it means you really will understand each line of code and how both interpreters work. ^ https://craftinginterpreters.com/introduction.html#the-code reply gavinhoward 16 hours agoparentprevAs the author of a POSIX standard utility, I would advise you to only reach for such utilities when portability is the most important thing. POSIX utilities are not great. Lex and Yacc included. reply chasil 16 hours agorootparentIs your criticism of the relationship of the lexer and the parser, or something more fundamental? Is an LR parser expressed in BNF notation unsatisfactory? I say this only as it has been many years since I have written one, but I thought this OCaml presentation on a POSIX shell held the structure in high regard. https://archive.fosdem.org/2018/schedule/event/code_parsing_... reply gavinhoward 15 hours agorootparentThe UX of the BNF notation. I am partial to hand-coded recursive descent purely because I struggled with Lex and Yacc too much when I tried. reply chasil 15 hours agorootparentI commiserate, it can be unforgiving, and the use of C is admittedly out of vogue. reply gavinhoward 15 hours agorootparentOh, I love C. [1] But yes, unforgiving. And a lot of magic, with special variables, macros, and functions that you must know. And unclear scoping. [1]: https://gavinhoward.com/2023/02/why-i-use-c-when-i-believe-i... reply nicoburns 13 hours agorootparentprevAre POSIX utilities even portable? They tend to have poor windows support. And they also tend to target C, which has a high ceiling for portability, but also a high bar for making things portable (whereas more modern languages are often just portable by default). My take on POSIX utilities would be only to use them on Linux platforms where they effectively form a \"native\" part of the platform. reply gavinhoward 13 hours agorootparentI was mostly speaking about POSIX, since that was the focus of GGP. Yes, you should only use them on POSIX. My utility does build natively on Windows, though. reply rnart 6 hours agorootparentprevI disagree here. The syntax can be infuriating at first, but once you understand it, Lex/Yacc are rock solid and speed up development significantly. reply cdcarter 1 hour agoparentprevThe Unix Programming Environment tutorial for building hoc doesn’t even come close to what Nystrom gets into in Crafting Interpreters. Hoc is a very fun little language, but as Nystrom describes several times in the book…parsing just isn’t the interesting part of the game. reply runevault 14 hours agoparentprevDoes a single production compiler use lex and yacc to generate any part of their system, beyond maybe a first pass to test out syntax before it gets rewritten into a hand written parser/lexer? I'm not going to say none exist since I don't know literally every production compiler ever written, but I have never heard of one that used them for the final code. reply ufo 1 hour agorootparentPython notably switched from hand written recursive descent, to a PEG based parser generator. But indeed, last I checked, recursive descent was the most common choice overall. reply pansa2 14 hours agorootparentprevI think Ruby does? IIRC its source code includes a 10,000+ line “parse.y” file which is converted to C code using Yacc. reply thomascountz 10 hours agorootparentExcitingly, Ruby 3.3+ shipped with a new alternative parser called Prism[0]! See also: [1][2]. [0]: https://github.com/ruby/prism [1]: https://railsatscale.com/2023-06-12-rewriting-the-ruby-parse... [2]: https://railsatscale.com/2024-04-16-prism-in-2024/ reply runevault 14 hours agorootparentprevInteresting, I'll have to look into this because I'd never heard that before and now I'm curious. A language with as much going on as Ruby was not one I would have picked as a candidate for yacc reply runevault 13 hours agorootparentprevJust went and looked in the github repo. 16,000+ line .y file. I can't imagine that is pleasant to maintain. reply rnart 6 hours agorootparentprevYes, OCaml with its very complex syntax and hundreds of features uses the OCaml equivalents of Lex/Yacc. It is a myth that one cannot use Lex/Yacc in production. reply jahewson 13 hours agoparentprevParser generators create more problems than they solve. reply fmbb 7 hours agoparentprev [–] Is there any reason to reach for them first? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "\"Crafting Interpreters\" is a comprehensive book for those interested in creating their own programming language, covering topics from parsing to garbage collection.",
      "Readers will learn to build a language with features like dynamic typing, lexical scope, first-class functions, closures, classes, and inheritance.",
      "Authored by Robert Nystrom, a Google engineer working on the Dart language, the book is available in print, eBook, and web formats."
    ],
    "commentSummary": [
      "\"Crafting Interpreters\" is praised for balancing technical implementation with conceptual insights, making it an excellent resource for aspiring engineers.",
      "Users appreciate the book's clarity and practical approach, with some recommending reading it cover-to-cover while coding along.",
      "Despite some wishing it used a language other than Java, the book is considered valuable for learning the foundations of creating a programming language and sets a high standard for technical writing."
    ],
    "points": 381,
    "commentCount": 111,
    "retryCount": 0,
    "time": 1720825225
  },
  {
    "id": 40951800,
    "title": "Introduction to Calvin and Hobbes: Sunday Pages 1985-1995 (2001)",
    "originLink": "http://timhulsizer.com/cwords/cintro.html",
    "originBody": "Back To the Calvin & Hobbes Page Back To the C&H Books Page Back To the Main Page Introduction By Bill Watterson (C)2001 From the book, \"Calvin and Hobbes - Sunday Pages 1985 - 1995\" It's been five years since the end of Calvin and Hobbes, the longest time I can remember in which I haven't drawn cartoons. Calvin and Hobbes was a wonderful experience, but it was an all-consuming career. When I quit the strip, I put my cartoons in boxes, and jumped into other interests. I haven't really considered the strip since, so at the invitation to do this show, I thought it might be time to look back at some of my work. My first reaction in going through my old cartoons was some amazement at the size and weight of the pile. For most successful comic strips, ten years is just a drop in the bucket, but even that amount of time yields a huge amount of material. It's no wonder that decade seems like a blur. Going through my old strips is sort of like looking at old photographs of myself: they're personal and familiar, yet somewhat bizarre at the same time. There are cartoons I've drawn that are the equivalent of pictures of my younger self wearing yellow pants: I know I'm responsible for that, but what on earth was I thinking? As my tastes have changed, and as I've learned more, I imagine that I would do many strips quite differently today. Not better necessarily, but certainly differently. I was twenty-eight when Calvin and Hobbes was first published, and, of course, I would make other choices now at age forty-three. It's also sort of strange to see a record of my own learning curve. Pick up a given strip, and I see how I struggled with various writing and drawing problems, or how I finally surmounted one. I remember sometimes feeling that the strip was better written than I could actually write, and better drawn than I could actually draw. I learned a great deal over the years by trying to push the strip beyond my own abilities, and I'm very proud that Calvin and Hobbes explored and developed all the way to the end. By the final years, I see naturalness or a sense of inevitability to the drawing and writing that is very satisfying. I'm more appreciative of this kind of grace since returning to the awkward stages of new learning curves. Of course, I'd also say the times have caught up with some of my strips. It's frankly a little discouraging to see how ordinary some of them look now. When Calvin and Hobbes first appeared, it was somewhat surprising to treat reality as subjective, and to draw a strip with multiple viewpoints, juxtaposing Calvin's vision with what others saw. I did this simply as a way to put the reader in Calvin's head and to reveal his imaginative personality. Now these juxtapositions are a visual game for many comic strips, and after all these years, I suspect readers know where this sort of joke is headed as soon as they see it. The novelty cannot be recaptured. Novelty, however, is probably overrated anyway. The Calvin and Hobbes strips that hold up best, to my eye anyway, are the ones where the characters seem big, vivid, and full of life, and where the strip's world seems genuine and inviting. Punchlines come and go, but something in the friendship between Calvin and Hobbes seems to hold a small piece of truth. Expressing something real and honest is, for me, the joy and the importance of cartooning. The Sunday strips were usually the cartoons I had the most fun with, and for this show I've chosen a few Sunday strips from each year that I think show off the strip's strengths. I have fond memories of reading the Sunday comics when I was a kid. As far as I was concerned, the Sunday comics were the whole reason for newspapers to exist. On weekdays, I read only the strips I liked; but on Sundays, I read them all, and often several times. The Sunday comics were always the most fun to look at, so when I finally got the chance to draw my own comic strip, I knew I wanted to make the Sunday Calvin and Hobbes something special. It took me a little while to learn to use the larger Sunday space effectively. It requires a somewhat different pace for the humor, and, of course, a big color panel is no place to find out that you don't know how to draw the back of your character's head. The Sunday strip shows off both strengths and weaknesses. Occasionally I would see that an idea I'd written for a Sunday strip was not as substantial as I'd hoped it would be, and I'd realize that some of the panels and dialogue weren't adding anything significant to the story. If that were the case, I'd remove everything extraneous and use the trimmed idea for a daily strip instead. I held the Sundays to a different standard: any idea for the Sunday strip had to need the extra space. I felt a Sunday strip should do something that was impossible the rest of the week. Over the years, I learned that daily strips are better suited for certain kinds of ideas, while Sunday strips are better for others. The daily strip is quick and to the point, perfect for a simple observation, or a short exchange between characters. Daily strips are also better for long stories, where a certain suspense can be fostered by continuing the story day after day, and the reader can remember what happened previously. Extended conversations with real back and forth dialogue, however, don't work very well in four tiny panels - the dialogue balloons crowd out the drawings and the strip loses its grace. In a Sunday strip, you can spread out, and let the characters yap a bit. This is often funny in itself, and it's a wonderful way to let the characters' personalities emerge. It also lets you explore a topic a bit more fully. You can talk about things without reducing them to one-liners right away. And, of course, in today's minuscule comics, if an idea requires any real drawing, the Sunday strip is the only possible place for it. Likewise, any complex storytelling problem-a strip illustrating a long expanse of time, for example, or an event depicted in a succession of very tiny moments-is futile in the daily format. Calvin's fantasies generally migrated to the Sunday page for this reason. In short, the Sunday page offered unique opportunities, and I deliberately tried to come up with ideas that could take advantage of them. I usually wrote the Sunday strips separately from the dailies. For the daily strips, I tried to write an entire month's worth of ideas before inking any of them. This allowed a long period for editing and rewriting. I was less able to do this for the Sunday strips because the Sundays need to be drawn weeks further in advance and because the strips took so much longer to draw. If at all possible, however, I would try to keep two or three Sunday ideas ahead of the deadlines. I always wanted to reserve the option of abandoning an idea that didn't stand up to a few weeks of scrutiny. For those who are interested in technical matters, the early strips were drawn on any cheap pad of Bristol board the local art supply store happened to stock. The paper was usually rather thin and sometimes the sheet wouldn't accept the ink consistently (bad sizing or something), which would make drawing aggravating and time consuming. Eventually I switched to heavier Strathmore Bristol board, which was much nicer. I used a 2H pencil to rough in the drawing, and then inked with a small sable brush and India ink. I did as little pencil work as possible in order to keep the inking more spontaneous, although the more elaborate panels required more preliminary drawing. For lettering, I used a Rapidograph cartridge pen. I drew the dialogue balloons and a few odds and ends with a crow quill pen. To cover up unwanted marks, I used various brands of Wite-Out, and in the early days, typewriter correction fluid. (Remember typewriters?) No doubt this stuff will eat through the paper or turn green in a few years, but as the original cartoons were intended for reproduction, not picture frames and gallery walls, I did not overly concern myself with archival issues or, for that matter, neatness. At some point along the way, however, I did ask the syndicate to send the printers a quality reproduction of the Sunday cartoon, rather than the original drawing, in order to reduce the amount of tape, registration marks, and general crunchings and manglings to which the drawings had previously been subjected. Coloring the strips was a slow and tedious process. My syndicate gave me a printed sheet showing numbered squares of color, each a mixture of various percentages of red, yellow, and blue. Using this sheet as a guide, I taped some tracing paper over the finished cartoon, and painted watercolor approximations of the available colors in the areas I wanted. This would give me a very rough idea of what the newspaper version might look like. Then I numbered each little spot of color. As the Sunday strips became more visually complex, and as I started to use color more deliberately for effects, this process became a real chore. These days, I believe much of it can be done with a few clicks of a mouse. Colors take on different characteristics when placed next to other colors (a neutral-seeming gray might look greenish and dark next to one color, but brownish and pale in relation to another). Because of this, I came up with one little trick for coloring the strip. I cut out each of the color squares provided by the printer, so I had a stack of colors (like paint chips), rather than a sheet. By laying out the cut squares and physically placing one color next to the others I expected to use, I could see exactly how each color behaved in that particular context. As I got better at this, I was able to choose approprial \"palettes\" for each strip, and create moods with color. One strip might call for contrasting, bright colors; another strip might be done with a limited group of soft, warm colors; another idea might call for a close range of grays and darks, and so on. If I made Calvin's skin a dull pink-gray to suggest dim lighting at night, I would have to find a dull yellow-gray that would suggest his hair in the same light. These challenges took an inordinate amount of time for work on deadline, but I was often quite proud of the results. A comic strip should always be fun to look at, and good use of color can contribute to that appeal More than that, color creates its own emotional impact, which can make the drawing more expressive. The half-page Sunday format required certain guaranteed panel divisions. The strip had to be drawn in three rows of equal height, and there was one unmovable panel division within each row. This allowed editors to reduce and reconfigure the strip to suit their particular space needs. The same strip could run in several shapes by restacking the panels. Editors commonly removed the entire top row altogether, so in essence, a third of the strip had to be wasted on \"throwaway panels\" that many readers would never see. The fixed panel divisions were also annoying because they limited my ability to compose the strip to best suit the idea. For example, they often forced a small panel where I needed more space for words. Of course, a big part of cartooning is learning to work effectively within tight space constraints. Much of cartooning's power comes from its ability to do more with less: when the drawings and ideas are distilled to their essences, the result can be more beautiful and powerful for having eliminated the clutter. That said, there is a point at which simplification thwarts good storytelling. You can't condense Moby Dick into a paragraph and get the same effect. Over the years, my frustration increased and I became convinced that I could draw a better comic strip than the current newspaper format was permitting. Looking at examples of comics from the 1930s, when a Sunday strip could fill an entire page, I was amazed by the long-forgotten possibilities out there. I took a sabbatical after resolving a long and emotionally draining fight to prevent Calvin and Hobbes from being merchandised. Looking for a way to rekindle my enthusiasm for the duration of a new contract term, I proposed a redesigned Sunday format that would permit more panel flexibility. To my surprise and delight, Universal responded with an offer to market the strip as an unbreakable half page (more space than I'd dared to ask for), despite the expected resistance of editors. To this day, my syndicate assures me that some editors liked the new format, appreciated the difference, and were happy to run the larger strip, but I think it's fair to say that this was not the most common reaction. The syndicate had warned me to prepare for numerous cancellations of the Sunday feature, but after a few weeks of dealing with howling, purple-faced editors, the syndicate suggested that papers could reduce the strip to the size tabloid newspapers used for their smaller sheets of paper. Another strip could then run vertically down the side. Consequently, while some papers, primarily in larger markets, ran the strip as a half page, other papers reduced it. In some of the latter papers (including the one I read at the time), I actually lost ground: the new Sunday strip was printed even smaller than before. I was in no mood to take on new fights, so I focused on the bright side: I had complete freedom of design and there were virtually no cancellations. For all the yelling and screaming by outraged editors, I remain convinced that the larger Sunday strip gave newspapers a better product and made the comics section more fun for readers. Comics are a visual medium. A strip with a lot of drawing can be exciting and add some variety. Proud as I am that I was able to draw a larger strip, I don't expect to see it happen again any time soon. In the newspaper business, space is money, and I suspect most editors would still say that the difference is not worth the cost. Sadly, the situation is a vicious circle: because there's no room for better artwork, the comics are simply drawn; because they're simply drawn, why should they have more room? Business controversies aside, the new format opened up new ways to tell stories, and I drew different kinds of strips as a result. I could write and draw the strip exactly as I imagined it, so it truly challenged my abilities. Whereas Sunday strips had previously taken me a full day to draw and color, a complex strip would now take me well into a second day to finish. Deadlines discourage this kind of indulgence, and I had to steal that extra time from what would have been some semblance of an ordinary life, but I was thrilled to expand the strip's world. Laying out the panels became a job in itself, now that I was no longer confined to horizontal rows. could place boxes anywhere and any size, but the reader's eye needs to flow naturally to the proper panels without confusion, and big panels need to be designed in such a way that they don't divert attention and spoil surprises. The graphic needs of each panel must be accommodated and the panels themselves should form a pleasing arrangement so the entire page is attractive, balanced, and unified as well. Here again I looked for guidance in the gorgeous Sunday pages of George Herriman's Krazy Kat. The new Sunday format necessitated a change in the format of my book collections as well. Having won a bigger strip in newspapers, I wanted the book reproductions to reflect the strip's new impact as much as possible by printing the Sunday strips large. This resulted in the rather awkward horizontal format of my later books. They stick out of bookshelves, but the strips look nice. From this point on, the Sunday strips were reproduced in color with each collection, not just in the \"treasury\" collections, as before. (Here's a piece of trivia: because of the timing of the book format change, the cartoons from the Snow Goons collection were never put in a treasury book, so those Sunday strips have been reprinted only in black-and-white.) Ten years after starting Calvin and Hobbes, I ended the strip. As much as I knew I'd miss the characters, the decision was long anticipated on my part. Professionally, I had accomplished far more than I'd ever set out to do and there were no more mountains I wanted to climb. Creatively, my interests were shifting away from cartooning toward painting, where I could develop my drawing skills further. And personally, I wanted to restore some balance to my life. I had given the strip all my time and energy for a decade (and was happy to do so), but now I was that much older and I wanted to work at a more thoughtful pace, out of the limelight, and without the pressures and restrictions of newspapers. The final Calvin and Hobbes strip was a Sunday strip. The deadline for Sunday strips being early, I drew it well before writing the daily strips that would eventually precede it in the newspaper. I very much wanted to hit the right note for this final strip. I think it worked, but it was a bittersweet strip to draw. Since Calvin and Hobbes, I've been teaching myself how to paint, and trying to learn something about music. I have no background in either subject, and there are certainly days when I wonder what made me trade proficiency and understanding in one field for clumsiness and ignorance in these others. On better days, I enjoy having so many new challenges and surprises. Even so, these new endeavors have only deepened my appreciation for comics. I no longer take quite so much for granted the versatility of comics and their ability to depict complex ideas in a beautiful, accessible, and entertaining form. For all their seeming simplicity, the expressive possibilities of comics rival those of any other art form. Five years after Calvin and Hobbes, I love the comics as much as ever. Bill Watterson Summer 2001 Back To the Calvin & Hobbes Page Back To the C&H Books Page Back To the Main Page",
    "commentLink": "https://news.ycombinator.com/item?id=40951800",
    "commentBody": "Introduction to Calvin and Hobbes: Sunday Pages 1985-1995 (2001) (timhulsizer.com)216 points by thunderbong 14 hours agohidepastfavorite83 comments po 5 hours agoMy sister just came to visit me in Tokyo and asked what to bring as a gift... I said, \"If you can, please bring my Calvin and Hobbes collection for my kids to read.\" My 10 and 6 year olds have been devouring them ever since. There is something electric and timeless about these strips. I am certain they don't understand all of the vocabulary but still they read it. It is a format that lures kids in and then uses that attention it has earned to stretch minds. Re-reading it as an adult also rings true in a totally different way. Calvin's parents become sympathetic compatriots. It's smarter than most adults but captivates kids. It is a decade of work that deserves all the awards that could possibly be given. reply tomrod 6 hours agoprevCalvin and Hobbes was a source of happiness during a mostly miserable adolescence. My world is better for Bill Watterson having been in it, even if he'll never know that himself. reply TecoAndJix 5 hours agoparentCalvin and Hobbes was also a major source of happiness during my youth. At one point I think I could have quoted you every strip. I fancied myself an artist when I was a kid and had a regular comic called \"Teco And Jix\". It was an obvious ripoff of both Calvin and Hobbes but had very violent storylines. When it came time to create my first \"internet name\" (for AOL AIM), I went with TecoAndJix and it has remained my digital pseudonym to this day! I even got it tattooed one not-sober evening in the Navy... Thank you Bill Watterson. reply 7thaccount 3 hours agoparentprevSame. Calvin and Hobbes were always out having fun and I used to dream about having some kind of friend to hang out with where we could go on those kinds of adventures. I spent a lot of my time being lonely and bored and these comics were a great escape. Having a daughter many years later in my adulthood has been awesome though and we go on all kinds of adventures and read stories together and play games and all kinds of stuff. In a way, it is kind of like getting a second crack at childhood and I want to make sure she isn't as lonely as I was. Of course you can't stifle your kids or live vicariously through them (I definitely draw some lines and give her as much freedom as you can in the modern day). reply SoftTalker 45 minutes agoparentprevYes, I was a little older but Calvin and Hobbes is my all time favorite comic. I don't even read the comics anymore, since I haven't subscribed to a printed newspaper in a long time. The whole ritual of sitting down with a cup of coffee and reading through the Sunday paper is something I still miss from time to time. reply froggertoaster 35 minutes agoparentprevYour words were so simple, but hit me in a place I did not expect. I also had a miserable childhood/adolescence, and I could not help but cry a little reading your words. I feel exactly the same way. reply adolph 49 minutes agoparentprevTo paraphrase:is better forhaving been in it, even ifnever know that. Seems like a definition of virtue or at least beneficence. reply fermigier 12 hours agoprevhttps://www.newyorker.com/magazine/2023/10/30/the-mysteries-... I've read, many times (it's a very short read), M. Watterson's (and his friend's John Kascht) latest work, \"The Mysteries\", and I very much like it. It's a multifaceted fable that explores, among other things, the tension between curiosity and control, and the allure and danger of technological progress. \"The Mysteries\" takes a more somber and philosophical approach, and its graphical style is radically different from C&H's, but I think it still shares with C&H a deep appreciation for the mysteries of life and the power of imagination to enrich our understanding of the world. reply ZeroGravitas 11 hours agoparentThere's a YouTube video with the two creators discussing their process on the book: https://youtube.com/watch?v=HHND7L1wUl0 reply mysterydip 6 hours agorootparentI was excited for the chance to finally see Watterson's face, as the only picture of him I've seen is from the 80s, but the interview only shows the hands of both of them. reply gammarator 4 hours agorootparentIt cracks me up how much he looks like “Dad” in the strip in those 80s photos. reply fragmede 2 hours agorootparentprevThat is the only photo of him! reply acomjean 7 hours agoparentprevI’ve read the “mysteries” book too, and would recommend. I think it’s the first book from Waterson since stopping Calvin and Hobbs. It’s quite different from Calvin and Hobbs, heavier but really thought provoking. I found myself thinking about the story after the fact. reply TheAmazingRace 1 hour agorootparentTo each their own, but the book would never have sold any copies if Bill's name wasn't attached to it. I purchased a copy on Amazon and thought I was hoodwinked. It felt like a pure cash grab. reply barbecue_sauce 25 minutes agorootparentDid you just, like, purchase it without having any concept of what it was? It's certainly different from Calvin and Hobbes, but there was no indication that it wouldn't be. Making a fairly artistic book with niche appeal is actually the opposite of a cash grab. reply savanaly 1 hour agorootparentprevCan't believe someone would look at the art in The Mysteries and call it a cash grab. It oozes personality and thoughtfulness. It's true it wouldn't have sold any copies if his name weren't attached though. reply kejaed 5 hours agoprevI grew up with Calvin and Hobbes collections in the back seat of the car on family vacations. I was 14 when the strip ended so I had lots of historical material to go through at the book stores (!). We have the anthology at home, I think it’s time to introduce the kids (6 & 9) to Watterson, although my son is already an expert at Calvinball without even knowing it. What struck me reading this piece was thinking about how all the constraints that Watterson faced just don’t exist today, as he pointed out with his “click of the mouse” comment. Constraints can often lead to creative solutions, I wonder where Calvin and Hobbes would go in today’s landscape. reply putlake 12 hours agoprevSince influential people are on this forum, someone should get Bill Watterson nominated for the Nobel prize in literature. His work deserves that honor as much as, if not more than, Bob Dylan. reply lostlogin 11 hours agoparent> His work deserves that honor as much as, if not more than, Bob Dylan. Could you explain why? I love Calvin and Hobbes. However Dylan played a role in political and human rights movements at a key time. I don’t see this sort of influence in Calvin and Hobbes and Watterson was apparently fired from one job for his lack of political knowledge (assuming the wikipedia entry is correct). reply mdp2021 11 hours agorootparent> fired from one job for his lack of political knowledge (assuming the wikipedia entry is correct) You should also have noticed from that source that Watterson majored in political science (he initially wanted to be a political cartoonist), and also that unfortunate job was related to local politics. The Nobel Prize in Literature is awarded for producing \"the most outstanding work in an idealistic direction\" with \"the greatest benefit on mankind\". This was interpreted into broad qualities (so, including but not reducing to promoting «human rights»). See the list of Nobel Laureates and the formal justifications for the award, at: https://en.wikipedia.org/wiki/List_of_Nobel_laureates_in_Lit... reply TeMPOraL 8 hours agorootparent> The Nobel Prize in Literature is awarded for producing \"the most outstanding work in an idealistic direction\" with \"the greatest benefit on mankind\". They should award one posthumously to Gene Roddenberry then. After all, the Federation from Star Trek is pretty much the only well-known case of an utopia that actually works in fiction writing, and the show itself is one of few examples of aspirational and inspirational writing in modern sci-fi. reply nanomonkey 2 hours agorootparentThere are quite a few Utopia's in fiction that work [0]. The Culture series by Iain M. Banks is a fairly well known one. [0]https://en.wikipedia.org/wiki/List_of_utopian_literature reply voisin 7 hours agorootparentprevThe Nobel is famously not awarded posthumously. There are many examples of two or more cofounders where one is not awarded the Nobel due to having died. reply krapp 2 hours agorootparentprev>After all, the Federation from Star Trek is pretty much the only well-known case of an utopia that actually works in fiction writing, and the show itself is one of few examples of aspirational and inspirational writing in modern sci-fi. The Federation only works as a utopia because the writers never bother to address any of the complex questions about how such a utopia would actually work, because they don't care. Star Trek's utopian ideal is mostly just window dressing. Also, Star Trek isn't very aspirational. We can't aspire to simply evolve beyond human nature as Trek humanity has, such that every human lacks any form of greed, vice or selfishness and is perfectly happy to participate in a society which still has all of the hierarchies of capitalism, including lifetime careers, but with none of the incentives. Nor can we expect the infinite free energy and physics-defying transporters and replicators that allow Star Trek's writers to just handwave away the hard problems of scarcity and thermodynamics. The Fermi paradox tells us plainly that FTL in any form is almost certainly impossible. The Vulcans aren't going to show up in the ruins of our civilization and potty train us. That isn't something we can aspire to, that's never going to happen. reply rsynnott 4 hours agorootparentprev> After all, the Federation from Star Trek is pretty much the only well-known case of an utopia that actually works in fiction writing Eh. I mean, by Voyager/DS9 it has secret police, and _literal mine slaves_ (old-model sentient holographic doctors). And it appears to be _basically_ a military dictatorship; the civil government in practice always seems to be subordinate to Starfleet. It also happily trades away inhabited territory to the Cardassians, who are essentially Space Nazis. And it has a safety culture that would make the Soviet Union blush. Really, the closer you look, the uglier it looks. We also don’t see that much of how Federation _civilians_ live, and a lot of what we do see frankly isn’t that great. reply hnlmorg 2 hours agorootparentAll the negative sides you’ve described is portrayed after Gene Roddenberry died and he was famously against a lot of those concepts while he was alive. DS9 would never have been green-lit during Roddenberry’s lifetime. > We also don’t see that much of how Federation _civilians_ live, and a lot of what we do see frankly isn’t that great. That’s not true. The worlds that aren’t great are planets outside of federation jurisdiction. Those that are part of it are usually portrayed as utopias. That all said, you’ve hit on a great premise for a Star Trek spin off. Edit: > And it appears to be _basically_ a military dictatorship; the civil government in practice always seems to be subordinate to Starfleet. The partnership is explored in DS9 and was the exact opposite of that you’ve described. In the DS9 episode I’m thinking of, shapeshifters (“changelings”) had taken over Star Fleet (the military) and were then trying to bypass the Federation to start a war. Basically a military coup lead by a small number of infiltrators. The remainder of the military were against the coup, which is why they were found out and the coup failed. reply lupire 7 hours agorootparentprevWhat is a utopia that \"actually works\"? Star Trek is \"What if Communism but it wasn't influenced by human nature?\", and totally avoided thinking about how the trillions of humans not in the military lived. reply ryandrake 32 minutes agorootparentYou don’t need to overcome all of human nature, you “just” need to overcome greed and solve scarcity. I put “just” in quotes because I recognize those are a tall order, but I don’t think you really need much else to have a Star Trek collectivist utopia. reply tomrod 6 hours agorootparentprevST:TNG goes into quite a bit of on-the-ground imagining of the lives of Federation individuals and Star Fleet. Colonies, research stations, etc. Though the only earth-side I remember is Jean Luc's winery and estate or when the admiral was mind controlled by some kind of pest. reply krapp 1 hour agorootparentEven TNG didn't always follow through with utopian ideals, because that doesn't make for compelling drama. Tasha Yar came from a failed Federation colony where society had collapsed, and spent her childhood avoiding roving rape gangs. reply TeMPOraL 5 hours agorootparentprev> Star Trek is \"What if Communism but it wasn't influenced by human nature On the contrary, the very premise of Star Trek is \"what if humans overcome the bad parts of their nature, and embraced the good ones\". The core concept of the entire franchise, coming from Gene himself, is that humanity must be recognizably human to the audience, just better than what it is now. The Federation being fully automated communist space utopia is an extension of that. reply AlchemistCamp 34 minutes agorootparentYeah, that really is its fatal flaw. The only really good Star Trek series I saw was DS9. It was darker, had longer story arcs, religion and fully developed characters. There was even a scene that illustrated this, where Captain Sisko was talking with an Earth politician and telling him that the problem was he lived on Earth, a paradise, and couldn't understand the war and problems those near DS9 faced. reply bazoom42 11 hours agorootparentprevDylan got the nobel price in litterature, which is awarded for a work of litterature, not for political influence. (Infamously Peter Handke won the litterature price despite his support for Milošević.) You are probably thinking of the peace price, but that is a different thing. reply codetrotter 10 hours agorootparent> The Nobel Prize in Literature […] is awarded annually, since 1901, to an author from any country who has, in the words of the will of Swedish industrialist Alfred Nobel, \"in the field of literature, produced the most outstanding work in an idealistic direction\" https://en.wikipedia.org/wiki/Nobel_Prize_in_Literature > Interpretations of Nobel's guidelines > Alfred Nobel's guidelines for the prize, stating that the candidate should have bestowed \"the greatest benefit on mankind\" and written \"in an idealistic direction,\" have sparked much discussion. In the early history of the prize, Nobel's \"idealism\" was read as \"a lofty and sound idealism.\" The set of criteria, characterised by its conservative idealism, holding church, state, and family sacred, resulted in prizes for Bjørnstjerne Bjørnson, Rudyard Kipling, and Paul Heyse. During World War I, there was a policy of neutrality, which partly explains the number of awards to Scandinavian writers. In the 1920s, \"idealistic direction\" was interpreted more generously as \"wide-hearted humanity,\" leading to awards for writers like Anatole France, George Bernard Shaw, and Thomas Mann. In the 1930s, \"the greatest benefit on mankind\" was interpreted as writers within everybody's reach, with authors like Sinclair Lewis and Pearl Buck receiving recognition. From 1946, a renewed Academy changed focus and began to award literary pioneers like Hermann Hesse, André Gide, T. S. Eliot, and William Faulkner. During this era, \"the greatest benefit on mankind\" was interpreted in a more exclusive and generous way than before. Since the 1970s, the Academy has often given attention to important but internationally unnoticed writers, awarding writers like Isaac Bashevis Singer, Odysseus Elytis, Elias Canetti, and Jaroslav Seifert. reply lostlogin 10 hours agorootparentprevThis is a good point. But his award is most definitely political both in how it was awarded and in the positions Dylan took in various events, songs and lyrics. reply joenot443 5 hours agorootparentJon Fosse was the 2023 winner and to my knowledge his work isn't considered especially political. I can't say confidently if Bill is a deserving candidate, but as others have suggested, political perspective or activism isn't a requirement. reply bazoom42 8 hours agorootparentprevObviously Dylan is political but this does not mean the price was political. The Handke debacle seems a pretty clear indication they evaluate the litterary merit without regard to the politics of the author. reply darby_nine 7 hours agorootparentprevSome of the best literature ever written isn't very \"important\" (although, I think you're greatly underestimating Calvin & Hobbes's influence) Secondly, I also question the influence of Bob Dylan. There's no indication that he was anything but a reflection of the political turmoil at the time. He was just a poet with some banger turns of phrase. I also found his emotional distance to the actual politics to be rather distasteful (compared to, say, 2pac's first album, Joey Bada$$'s \"all amerikkkan badass\", and like half of Kendrick Lamar's work, many varied feminist artists now like Mitski & Laufey, the anti-war influence on the then Dixie Chicks, now the Chicks... i'm sure you could come up with thousands of artists with more to say than bob dylan). C&H's political expression speaks much more to the absurdity of the values and habits that entrench themselves by adulthood, and help us question which of these are meaningful, rational, and necessary. A different sort of politics for sure, but one that is certainly relevant to most households across the country. Most of the political angst that comes through the adults in the form of disgust with cultures of consumption and commodification, the absurdity of a biker pitted against car, the joy of giving an F-15 to a trex rather than a modern state, are no less salient today. reply slater- 2 hours agorootparentincorrect to compare Dylan to 2pac and Mitski (a “varied feminist artist?”) I understand the impulse of trying to tear down the establishment guy, but your take is absurd. reply mdp2021 11 hours agoparentprevBerkeley Breathed could feel left out - and one of his collections was named \"Classics of Western Literature\". reply bell-cot 8 hours agorootparentNobel Prizes leave lots of people feeling left out. reply readthenotes1 10 hours agoparentprevIf you look at the list of nominees it's not exactly a mark of lasting impact https://en.m.wikipedia.org/wiki/List_of_nominees_for_the_Nob... reply munchler 12 hours agoprevFishing the comics section out of the Sunday paper to read a big Calvin and Hobbes strip was a joy. At the peak of the era, our paper (The Washington Post) actually had two separate comics sections, one with C&H on the front and the other with Peanuts. There was also Doonesbury and Bloom County, many other lesser strips, and even magazines like Parade and Potomac, all of which together provided a full morning’s entertainment. reply elicash 5 hours agoprevHere's my list of 34 personal favorite C&H comics: https://docs.google.com/document/d/1XryL7duzj_WZEy0dtdDO48_s... reply ryukoposting 6 hours agoprevI'm not sure how many times I've read this before - at least twice, I think. Much like Calvin and Hobbes, it never gets old. I'm too young to remember Calvin and Hobbes as a new comic. But, my parents had a complete anthology and I must have read the entire thing half a dozen times, cover to cover. I think Calvin and Hobbes informed the way I try to look at life. Embrace spontaneity and use your imagination. It's easy to forget that stuff. reply tempodox 6 hours agoprevIf you want to see them all: https://www.gocomics.com/calvinandhobbes/1985/11/18 reply kejaed 5 hours agoparentIt is also on the Internet Archive. https://archive.org/details/TheCompleteCalvinHobbes_201902 reply LeifCarrotson 5 hours agorootparentThere's also a search engine: https://michaelyingling.com/random/calvin_and_hobbes/ And a text file transcription on the Archive: https://web.archive.org/web/20210706165719/http://www.s-anan... If you want to find a particular comic you remember. reply irrational 3 hours agoprevI was a teenager in the 80s. I can remember going outside in the dark before school to get the paper so I could read C&H during breakfast. I didn’t know at the time that they would someday be reprinted in books, so I would cut each one out and tape it into a notebook. The other main comic strip I enjoyed at that time was The Far Side. Recently I reread both Calvin and Hobbes and the Far Side. Calvin and Hobbes is still just as good as a 50s year old adult (though I find I now identify a lot more with the parents). But, I didn’t find the same thing to be true of The Far Side. Some of the Far Side strips are wonderful - like The School of the Gifted - but most just fell flat for me. reply Loughla 21 minutes agoparentThe Far Side scratches a weird kid itch really really well. I bought the entire collection for my kid. Their okay, but not as good as I remember. He was dying laughing as he read them. reply bombcar 2 hours agoparentprevFar Side really works well on a “one a day” timeline - binging it doesn’t work as well as it does with C&H. reply max_ 13 hours agoprevOn a long enough time line all games become Calvin Ball reply Loughla 20 minutes agoparentI cannot play croquet without thinking about mashing someone with a mallet because of this comic. reply u235bomb 11 hours agoprevI can't really remember how or by whom I was introduced to Calvin and Hobbes. There was for years a peripheral awareness of it existing and eventually - reasons unknown - I purchased one of the books collecting strips. Properly reading Calvin and Hobbes for the first time is one of those experiences that leave a lasting imprint. The only other comic strip that had this lasting effect on me was Cul de Sac by Richard Thompson. https://www.gocomics.com/culdesac/2007/11/11 reply ghaff 5 hours agoparentFor whatever reason, it's a strip I was aware of but didn't really appreciate until later in life. I confess to also being much more into Hanna-Barbera than Warner Brothers/Loony Toons until college or so. reply matrix12 1 hour agoprevThere is also some fan created extensions. https://imgur.com/gallery/all-bacon-hobbes-comics-i-could-fi... reply alsetmusic 4 hours agoprevStupendous Man is one of my tattoos. Seeing it always makes me happy. What a uniquely great comic. It has been an incredible influence in my life. The first time I was reading “Something Under the Bed is Drooling,” when I got to the Mr Bun strip, my brain broke from the art change. I was extremely confused until I got to the end. To be a kid again… Watterson has a new book: The Mysteries. reply the_doctah 2 hours agoparentThe art style usually changed when he became Spaceman Spiff and Tracer Bullet as well. Loved those alter egos. reply KingOfCoders 9 hours agoprevI've used Calvin and Hobbes to learn English. reply kibwen 5 hours agoparentAs a native English speaker, Calvin and Hobbes taught me to swear. I remember waiting until my mom was outside watering the flowers, then locking myself in the bathroom and whispering \"darn\" into the mirror. I was not a very bold child. reply RhysU 8 hours agoparentprevMy children each started reading in earnest after memorizing the several pages beginning \"So long, Pop! I'm off to check my tiger trap!\" reply optimalsolver 4 hours agoprevFor fans of both Calvin and Frank Herbert's Dune, I give you Calvin & Muad'Dib: https://calvinanddune.tumblr.com/ reply idrathernotsay 13 hours agoprevTim needs to fix his expired security certificate, at least according to Bitdefender https://i.imgur.com/wZCSESE.png reply diggan 6 hours agoparentTime to get rid of Bitdefender I guess? The site is http, so no certificate needed (or even served, for that matter). For the https version, it serves a certificate for the wrong domain (`*.brinkster.com` rather than for `timhulsizer.com`), so not even the error message is correct. reply froggertoaster 33 minutes agoprevAnyone else struck by how captivating Bill Watterson is as a writer, both in C&H and out? reply wombatpm 2 hours agoprevIn college I had Calvin and Hobbes in German. It was the best way to learn vocabulary reply block_dagger 13 hours agoprevIt's admirable that he traded his core competence for new subjects later in life. reply ekianjo 12 hours agoparentisnt it what everyone does when they retire though? reply Kodiologist 6 hours agorootparentNo, actually, although you could argue that the word \"retire\" isn't applied quite correctly in such cases. It's common for academics, for example, to continue doing much of the same work once they're nominally retired, just at a different pace with different priorities. reply tomrandle 12 hours agoprevI was introduced to Calvin and Hobbes (as well as Sam and Max) as a kid by a taxi driver when on holiday in Florida in the 90s. Loved both. It’s been a delight to re read them with my son recently. I somewhat lament it’s not possible to by any merchandise though! I’d love to have a big print of the pair in their radio flyer crashing down the hill! reply vunderba 2 hours agoparentIf the distasteful proliferation of the \"Calvin peeing\" bumpersticker is anything to go by, thank spaceman spiff Watterson sealed the floodgates of merchandising and marketing garbage. reply bdjsiqoocwk 11 hours agoparentprevAh!, speaking of merchandise. I once read an essay about the idea of, if you have a successful artistic creation, should you exploit it financially, for example by selling rights for someone else to find new income streams (Bill Waterson famously never did.) The essay argued that even if you are primarily motivated by art and not money, you should for the reason that money being the motivating factor for so many activities in our society, doing so is the only way to keep your creation relevant in society, which would then mean that your creation would keep being exposed to new people, as opposed to live in the minds of ever older people and die with them. I would like to to find the essay and reread it, but I could never find it. It kind of rang true. reply probably_wrong 9 hours agorootparentI believe that essay mistakes quantity for quality. I think most authors would prefer a small community that \"gets\" them than a larger one that doesn't. I believe most artists care more about the message of their art than about its spread. And if you ask them to choose between \"500 people that get it\" and \"50000 people who don't\", the fact that the second number is bigger is not necessarily a better deal. (Obviously there are considerations for \"I need to pay rent\", but that's a different issue) reply ndsipa_pomu 9 hours agorootparentprevI'm not convinced that merchandising would keep an artistic creation relevant in society. It'd be just as likely to reduce the artwork to the lowest common denominator and it'd become just another slogan/logo. reply bdjsiqoocwk 9 hours agorootparentThe idea you're conveying is very well known but in my opinion poorly argued for. This essay I mentioned on the other hand had a minority view and well argued for, which is why I was trying to find it in order to reread it. Anecdotally let's look at one data point. Spider-Man is in comparison garbage, but it's everywhere to this day. Why, because some company calculated they could milk it. C&H in comparison is much better but virtually unknown for anyone under the age of 20. It's dying. reply vinnyvichy 4 hours agorootparenthttps://www.hbs.edu/ris/Publication%2520Files/09-128.pdf reply 082349872349872 8 hours agorootparentprev> \"You either die a hero or...\" —HD reply vinnyvichy 4 hours agorootparentprevSounds like this essay https://www.hbs.edu/ris/Publication%2520Files/09-128.pdf reply lkdfjlkdfjlg 3 hours agorootparentHey! Listen, the abstract exactly matches what I described. However, I believe that the essay I read specifically mentioned C&H as an example, which CTRL-F indicates your pdf doesn't. (On the other hand, it's also possible that I originally did read your pdf and in my mind used C&H as an example, and am now mis-remembering it haven been presented as example). Regardless, I'll read your pdf, thank you! reply lupire 7 hours agorootparentprevCalvin and Hobbes was extremely mainstream. It only stopped being so because the creator stopped creating content for it. Spider-Man, for all its merchandising and marketing, doesn't have people chasing down old content in dying formats. It is supported by new content in modern formats. reply lkdfjlkdfjlg 6 hours agorootparentOld spider-man content sells for a lot more than old calvin and hobbes content. > Calvin and Hobbes was extremely mainstream. It only stopped being so because the creator stopped creating content for it. I think that's exactly the point. It stopped being mainstream. Spiderman continues to be mainstream because the original author passed on the rights and now it will live for longer than the original author. Calvin and Hobbes will continue to shrivel in how well known it is. reply bombcar 2 hours agorootparentWhich is exactly what Watterson wanted. (He famously “came back” for a gag, but he hasn’t “sold out” and is apparently happy doing what he does, set for life.) https://stephanpastis.wordpress.com/2014/06/07/ever-wished-t... reply spacecadet 6 hours agoprev [–] I still have all of the Calvin and Hobbes books in storage. Some of the black and white pages were carefully watercolored by me as a teen. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Bill Watterson reflects on the evolution of his work on \"Calvin and Hobbes\" five years after ending the strip, noting changes in his tastes and skills over time.",
      "He highlights the unique challenges and joys of creating Sunday strips, which required more elaborate drawing and allowed for extended storytelling compared to daily strips.",
      "Watterson discusses his decision to end the strip after ten years to explore other interests like painting and music, while still maintaining a deep appreciation for the art of comics."
    ],
    "commentSummary": [
      "Calvin and Hobbes, a beloved comic strip by Bill Watterson, continues to resonate with both children and adults, providing timeless enjoyment and relatable content.",
      "Bill Watterson's latest work, \"The Mysteries,\" is a multifaceted fable exploring themes of curiosity, control, and technological progress, though opinions on its merit vary.",
      "Discussions around Calvin and Hobbes include its impact on readers' lives, the potential for Watterson to receive a Nobel Prize, and debates on the effects of merchandising on the comic's legacy."
    ],
    "points": 216,
    "commentCount": 83,
    "retryCount": 0,
    "time": 1720845422
  },
  {
    "id": 40952509,
    "title": "I made a drag and drop CSS grid generator",
    "originLink": "https://cssgridgenerator.io/",
    "originBody": "We are launching on Product Hunt today Check It Out CSS Grid Generator Illustration Generator Illustration Generator CSS Grid Generator CSS grid generator is a tool that helps developers create custom CSS grid layouts more easily. The generator allows users to specify the number of columns, rows, the gutter size. How to use CSS grid generator? 1. Customize the number of columns, rows, and gaps to fit your needs. 2. Click the square with + sign to add a new element to the grid. 3. Resize the DIV using the handle in the bottom right corner. 4. Drag and drop the DIV to reposition it as desired. 5. Finally, copy the generated HTML and CSS code and paste it into your project. Columns Rows Gap(px) + + + + + + + + + + + + + + + + + + + + + + + + + X1 Reset Copy HTML1Copy CSS .parent { display: grid; grid-template-columns: repeat(5, 1fr); grid-template-rows: repeat(5, 1fr); gap: 8px; } Illustration Generator Tailwind Grid Generator Made with ☕ by Kristjan",
    "commentLink": "https://news.ycombinator.com/item?id=40952509",
    "commentBody": "I made a drag and drop CSS grid generator (cssgridgenerator.io)183 points by Kristjan_Retter 11 hours agohidepastfavorite39 comments AlexDragusin 8 hours agoNice, however there is an issue, for example add a few boxes then if you try to move one and move it as towards the others to overlap, eventually they go all the way down, outside the bounrary of the grid and feels funny but yeah. No errors showing on the console. I don't think it was the intended operation as they cover the code part. If is not clear what I am saying, I can try to make a video or something. reply Kristjan_Retter 11 hours agoprevI previously made a Tailwind Grid Generator to make grid layouts with Tailwind CSS easier. It received a lot of positive feedback, so I decided to create a similar tool for plain CSS as well reply yonatan8070 8 hours agoparentThat's super cool! One thing that I noticed is that you can drag/push grid elements outside of the grid's boundaries, which is probably undesired reply catapart 3 hours agoprevThis is really great! Thanks for making it! I will say, personally, my problems with grid are usually centered around getting them to do the expected dynamic rearrangement, rather than just setting up the grids in the first place. It's always an MDN hunt, for me, when I want to get a grid to collapse to fewer columns rather than squish the columns (or vice-versa). But, in general, it's always nice to have simple utilities like this to simplify those rote things we do all the time. I'm sure I'll be using it sooner rather than later! One quick suggestion: if establishing grids set the state in the url, it would be trivial to send someone a link to your utility which had the grid they wanted. Not sure exactly how useful that would be, but I just figured I could pop a bunch of \"standard\" layouts into a text file as urls and that would make the utility easier to rely on, for me. reply emadda 6 hours agoprevNice tool. I feel generators are good learning tools, but once you grasp the syntax and features you gain more flexibility and are able to connect code to a concrete visual in your mind. The chrome dev tools are also useful as it allows you to modify a grid in place to instantly see your changes. I wrote a post on grid here: https://cssprinciples.com/3/grid/ reply dylan604 5 hours agoparentIn the early days of web, tables were the goto for layouts with Web1.0. They could get really complicate so Macromedia's tools using Fireworks to slice the images and Dreamweaver to manipulate the HTML were great places to start. But as you said, after doing it enough you start to see all of the baggage/weight that these tools added to the HTML. I eventually got to the point of writing it by hand which would eliminate all of that extra \"crap\" that these tools added. Every byte saved was very important during the days of dial-up. reply emadda 3 hours agorootparentI learnt how to use tables for layout from Google's homepage design. I think in general interaction between generator tools and hand written code is very difficult. Once you start with one you cannot then use the other on the same code. reply dnpls 6 hours agoprevI found 2 bugs: - the width is not responsive, if you resize the browser window the boxes don't resize. - the boxes are shorter than the containers (for me, on Chrome on macOS) so every new row is a bit higher on the grid. On row 5 the boxes are basically positioned one whole row above. reply endofreach 7 hours agoprevBack when grid was new, i had the feeling it would fundamentally change the way we build (responsive) web layouts by basing everything on template areas and just reassigning on breakpoints etc. I never got around to build some kind of internal framework for this, i ended up hacking away with flex (which was already the gamechanger – hail to the OGs from the times before flexbox). Then tailwind came along and after disregarding it for a long time, it ended up to fulfil every need i had for quickly doing responsive frontends quickly. Still wondering if anyone knows good frameworks or examples of people using template areas as the main fundament for their layouting? I'd like to try, but it's not important enough for me to build something well enough that can be used as a general base. Or maybe that doesn't even make much sense or much of a difference. Haven't even thought about it much. Any thoughts on this are welcome. Ah, also: nice work OP. I like it. Maybe you have some ideas about template areas yourself. I will definitely use your tool next time i fiddle around with grids. reply hinnisdael 6 hours agoparentI would argue you no longer need a framework when using CSS grid. Creating template areas once per breakpoint and assigning them once per container really is all there‘s to it. Sure, you can build another abstraction on top of that, but I doubt it‘ll get any simpler than the underlying handful of grid declarations. reply mansarip 9 hours agoprevTools like this are actually very helpful especially if you are learning to understand them. A few years ago, I found a site generator similar to this one, but it wasn't drag and drop. I can't remember the name of the site. From there I learned about grid-template-columns, gap, and some basic things about CSS grid. It didn't cover everything, but it introduced me to how the grid works when I was just learning about it. Keep up the good work. It will always make it easy for people to learn, or use it directly. reply karmakaze 3 hours agoprevI'm not a web dev, but I imagine that some may want more than 12 columns, like if I were making a game with many cells. I tried entering a larger number but the parsing was weird 22 -> 1 columns. Edit: It seemed to make those columns into rows as well. reply memset 7 hours agoprevThank you! I’m an older programmer and can do grids using tables by muscle memory - and have always struggled to figure out the right css layout. Tools like this are super helpful. reply hereonout2 7 hours agoparentOlder programmer too, tables for layout is very retro though! Hopefully you're not still adding a spacer.gif here and there. I recently picked up some front end work for the first time in about 15 years. Things have progressed so much, CSS grid is a dream but also the things that were horribly hacky in the 2000s with JS are now solved so well. Not even talking about frameworks or react, with just vanilla CSS, JS and the mdn docs you can build really nice sites that were so so awkward to make before. I'd encourage you to spend a few afternoons delving deep into this, a lot of the stuff I learnt years ago is still relevant and brings a solid foundation, it didn't take long to update my approach. reply lysp 6 hours agorootparent> Older programmer too, tables for layout is very retro though! Hopefully you're not still adding a spacer.gif here and there. new-old school is to do spacer.base64. reply lofaszvanitt 4 hours agorootparentprevSPACER GIFS!!! Ahahahaha, good ol days :D. reply replwoacause 4 hours agorootparentprevI grew up on spacer.gifs reply ericyd 4 hours agoprevReally great UX for me. I am a full time web dev but I write page layouts so infrequently that I've never learned grid syntax deeply. Bookmarking this for the future. reply ghosty141 8 hours agoprevHave you thought about open-sourcing the code for this page? I'd be curious how you made this. Works really well! reply yupyupyups 7 hours agoparentCTRL+SHIFT+I in Firefox. Then you will be able to see the HTML and all the calculated CSS properties of each element. If he uses Tailwind CSS you should be able to look at the class-attributes of the elements to figure out how it all works. Haven't done so myself, on mobile. reply kristopolous 6 hours agorootparentPreface the URL with \"view-source:\" on mobile. Example: view-source:https://cssgridgenerator.io/ reply beardyw 7 hours agoprevIsn't it odd to have class div1 div2 etc. Isn't that what the style attribute is for? reply Akronymus 7 hours agoparentThe style attributes can get pretty long and hard to read. I personally prefer the approach of assigning it via classnames/ids, so that it's shorter in the html itself, along with being easier to switch out components. (Altough, I also use htmx, so that's another reason to go with classnames/ids) reply lovegrenoble 1 hour agoprevInteresting tool, even better than mine, for FlexBox CSS: https://flexboxcss.com reply jslakro 8 hours agoprevHere Hitesh presents a system to build any possible layout with tailwind, part of the recipe can be made with plain flexbox https://youtu.be/rbSPe1tJowY reply atum47 7 hours agoprevWorks really well on mobile. Nice job reply SandraBucky 6 hours agoprevNice, Cool tool. reply spiel24 7 hours agoprevCool! reply robxorb 9 hours agoprev [–] Where is the best place to find up-to-date information on stuff like layouts, and how to manage crossbrowser/accessibility etc? It's a minefield when trying to self-learn these things as it's hard to tell when info is wrong or bad practice, outdated, etc. For example - is this CSS grid-generation/layout current best-practice for building the base, foundational layer for a simple static website? Say, with three columns, one centered and wider with content, the others narrow and empty - serving as margins? And how would one arrive there, as a solution? Searching online turns up an infinity of options such that it's difficult if not impossible to figure out how to do things in if not maybe \"the\" right or best way, at least \"a\" right way. reply robxorb 15 minutes agoparentThanks all, your responses have been incredibly helpful. In summary (besides the layout-specific stuff which was great) - between the MDN documentation for learning and reference: https://developer.mozilla.org/en-US/docs/Web And, for checking compatiblity and common practice: https://caniuse.com A n00b like me can get a decent handle on current best practice and standards for web dev. Feel levelled-up - much appreciated! reply afloatboat 9 hours agoparentprevGrid has been around for a while now, together with flex it’s the way to go to start building layouts. But like everything it requires practice to get the nuances down and learn about the pitfalls. You won’t experience many cross browser issues these days with either of them. And in terms of accessibility you mostly need to consider that visual order does not necessarily match tab order. Especially for grid where you can arbitrarily place elements in the grid. An older resource for learning grid would be https://youtube.com/playlist?list=PLu8EoSxDXHP5CIFvt9-ze3Ing.... 6 years old but grid itself has gone mostly unchanged. https://css-tricks.com/snippets/css/complete-guide-grid/ https://css-tricks.com/snippets/css/a-guide-to-flexbox/ https://flexboxfroggy.com/ Is a good way to introduce yourself with the concepts of flexbox. reply ffsm8 7 hours agorootparentAnd this one is for the grid https://cssgridgarden.com/ reply promiseofbeans 8 hours agoparentprevCaniuse (https://caniuse.com/) aggregates data from MDN with their own data and has reasonably good search. I see browser and standards people linking to it from time-to-time. reply tgv 8 hours agoparentprevMDN has complete pages on all features, including their compatibility, and decent introductions: https://developer.mozilla.org/en-US/docs/Web/CSS/CSS_grid_la... reply zadokshi 9 hours agoparentprevI’ve always used “flex” layout for this. I’m not sure what is considered normal. reply danielvaughn 7 hours agorootparentThere’s no normal, but a good rule of thumb that has worked for me is: use flex for one-dimensional layouts, and grids for two-dimensional layouts. reply Brajeshwar 7 hours agorootparentprevGrid lays out the structural layout, while Flex lays out the content ordering and positioning. reply 6510 9 hours agoparentprev [–] Anything that works is fine. 3 grid columns would work, flexbox would work too, but normally one would use margins. Something like this. https://jsfiddle.net/gaby_de_wilde/5muq1tag/ I forget why I'm not using a single margin:auto, probably some weird edge case I didn't want to see ever again. reply Etheryte 4 hours agorootparent [–] You wouldn't want to use a single `margin: auto`, because that sets the top and bottom margins to `auto` as well. For example if the parent container is flex, you'd then end up with a container that is centered vertically as well, which is not what you wanted. What you're probably looking for is `margin: 0 auto`. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The launch on Product Hunt features two new tools: CSS Grid Generator and Illustration Generator.",
      "The CSS Grid Generator allows developers to create custom CSS grid layouts by specifying columns, rows, and gutter size, and provides generated HTML and CSS code for easy integration.",
      "The tool includes user-friendly features such as customizable columns, rows, gaps, and drag-and-drop functionality for repositioning elements."
    ],
    "commentSummary": [
      "A new drag-and-drop CSS grid generator (cssgridgenerator.io) has been launched, receiving positive feedback for its simplicity and utility despite some bugs.",
      "Users have reported issues such as grid elements moving outside boundaries and boxes not resizing responsively, and have suggested improvements like adding URL state for sharing and open-sourcing the code.",
      "The tool is appreciated as a helpful learning resource and a quick method for setting up grid layouts, reflecting on the evolution of web design tools."
    ],
    "points": 183,
    "commentCount": 39,
    "retryCount": 0,
    "time": 1720856649
  },
  {
    "id": 40949943,
    "title": "Optimizing the Lichess Tablebase Server",
    "originLink": "https://lichess.org/@/revoof/blog/optimizing-the-tablebase-server/MetV0ZQd",
    "originBody": "Community blogsBlog topicsLiked blog postsLichess blog Optimizing the tablebase server revoof6 Jul 202425323,127 viewsEnglish (US) LichessSoftware DevelopmentEndgame Hunting down tail latencies Recently our our 7 piece Syzygy tablebase server was struggling to complete its periodic RAID integrity check while being hammered with tablebase requests. We decided to try a new approach, using dm-integrity on LVM. Now, instead of periodically checking every data block, we passively check blocks whenever they are read. 17 TiB of tablebases are unwieldy, so to do this migration without hours of downtime, we set up a second server with the new approach. This also allowed us to run controlled benchmarks on the full set of tablebases, before finally doing the switch and retiring the old server. We're trying to get the most out of the following new hardware: 32 GiB RAM unchanged 2 x 201 GiB NVMe, where the previous server didn't have any SSD space. The rest of the 476 GiB disks is reserved for OS and working space 6 x 5.46 TiB HDD, where the previous server had only 5 disks The current operating system is Debian bookworm with default I/O schedulers: root@bwrdd:~# uname -a Linux bwrdd 6.1.0-21-amd64 #1 SMP PREEMPT_DYNAMIC Debian 6.1.90-1 (2024-05-03) x86_64 GNU/Linux root@bwrdd:~# cat /sys/class/block/nvme0n1/queue/scheduler [none] mq-deadline root@bwrdd:~# cat /sys/class/block/sda/queue/scheduler [mq-deadline] none Monitoring important as ever RAID 5 is a good fit here, allowing recovery from any single disk failure, and distributing random reads across all disks. My first attempt was: $ lvcreate --type raid5 --raidintegrity y --raidintegrityblocksize 512 --name tables --size 21T vg-hdd # Oops Peformance numbers in initial tests were decent, but we would have left a lot on the table if we didn't have monitoring to catch that not all disks were indeed participating equally. Physical disk read activity with bad raid setup That's because omitting --stripes does not default to use all physical volumes. Benchmark results (overview) In normal conditions the server receives between 10 and 35 requests per second. We record 1 million requests in the production environment to replay them in a controlled benchmark. In the chosen scenario, 12 parallel clients each sequentially submit requests from the production log. Tables are lazily opened, and application and OS caches are lazily populated. So the first 800k response times are ignored as a warmup. We analyse the response times for the remaining 200k requests. On average, response times are plenty fast, but tail latencies are high. So this is our focus for any optimizations. We'll unpack the results in a moment, but here are the empirical distribution functions (ECDFs) with 30ms added to each response time for an overview. ECDFs For a given response time on the x axis (log scale!) you can see which proportion of requests is faster. Or for a given proportion on the y axis (think percentile), you can read off the corresponding response time on the x axis. The added constant seems artificial, but it's just viewing the results from the point of view of a client with 30ms ping time. Otherwise the log scaled x-axis would overemphasize the importance of a few milliseconds at the low end. mmap with higher tail latencies than pread Our Syzygy tablebase implementation shakmaty-syzygy now offers an interface to plug in different ways of opening and reading from table files. The main contenders are: Map table files into memory. After the file has been mapped, disk reads happen transparently when accessing the respective memory region, so no further system calls are needed. Unfortunately, that also means reads look just as infallible as normal memory accesses, so that errors can only be handled out of band, via signals. pread(2), one system call per read, with read error reporting via the return value. More robust error handling would probably be enough to justify using pread for a server implementation, but surprisingly, the diagram above shows that pread also peforms better in the scenario we care about. Perhaps that is because sometimes transparently reading a single memory-mapped data block across page boundaries may end up issuing two disk reads while (...) { uint8_t d = *ptr++; } whereas uint8_t buf[MAX_BLOCK_SIZE]; ssize_t res = pread(fd, buf, block_size, offset); immediately reveals how much data will be read. Now, before you change your chess engine to use pread: Tablebases in engine matches are typically used only if enough fast storage for all WDL tables is available. The typical range of response times is not even visible in the graph above. Here, the saved syscall overhead is significant, so that memory mapping performs better. Zoom on ECDFs: mmap saves syscall overhead MADV_RANDOM / POSIX_FADV_RANDOM counter-productive The next surprise looking at the results above, is that posix_fadvise(fd, 0, 0, POSIX_FADV_RANDOM) or its equivalent for memory maps are actually mostly counter-productive. POSIX_FADV_RANDOM is intended to alleviate pressure on the page cache, by hinting to the operating system that file accesses are going to be random and automatic read-ahead is likely pointless. Perhaps tablebase access patterns when people are analysing endgames are not so random afterall. Again, this may differ for chess engines, where probes may be more likely to be scattered across different possible endgames. Table prefixes on limited SSD space To decide how to use the limited SSD space, let's have a look at the anatomy of a single table probe. The position will be encoded as an integer index, based on encoding information from the table header. Then we need to find the compressed data block that contains the result for the particular index. Syzygy provides a \"sparse\" block length list, which points close to the correct entry in the block length list, which is then used to find the relevant data block. Table section sizes WDL DTZ Total Headers and sparse block length lists 38 GiB 9 GiB 47 GiB Block length lists 274 GiB 64 GiB 339 GiB Compressed data blocks 8433 GiB 8458 GiB 16891 GiB We could certainly use the SSD space for an additional layer of adaptive caching, to cache hot list entries and data blocks. But since we're trying to improve tail latencies in particular, it makes sense to think about the worst case. By putting the sparse block length lists and the block length lists on SSD storage, hot or cold, we can guarantee a maximum of 1 slow disk read per table probe. In our case that doesn't quite fit when using the SSD space in RAID 1 (mirrored), but since this optimization is optional, we can give up redundancy and use RAID 0. Parallelizing reads In chess engines, a typical tablebase request will be for a single WDL value. But for the user interface we instead want to display DTZ values for all moves. Tablebase explorer That, together with Syzygy's internal resolution of captures, will cause the average request to issue 23 WDL probes and 70 DTZ probes. In the initial implementation handling of requests was parallelized, but probes within each request were executed sequentially. In the benchmark results we can see that using more fine grained parallelism has some overhead at the low end, but significantly reduces tail latencies. Of course the disks can not really physically handle that many parallel reads, but now the I/O scheduler is more likely to plan them in a way that will finish each request as soon as possible, and can better plan the order of all involved disk accesses (minimizing time until the disk's read head is at the next requested sector). Performance in production Finally, it's good to confirm that optimizations in the benchmark scenario actually help in production. Here are response time charts sliced together. Standard chess response times Raw data at https://github.com/niklasf/lila-tablebase-bench-tool Discuss this blog post in the forum More blog posts by revoof 19 Apr 2024 Adapting nnue-pytorch's binary position format for Lichess Lichess stores lots of chess positions, and (thanks to Stockfish) now more efficiently",
    "commentLink": "https://news.ycombinator.com/item?id=40949943",
    "commentBody": "Optimizing the Lichess Tablebase Server (lichess.org)181 points by cristoperb 7 hours agohidepastfavorite48 comments imperialdrive 52 minutes agoLichess is one of those things you just have to sit and appreciate like a fine wine. It's absolutely wonderful for people in the chess community. I use it every day and am inspired by the functionality and performance, especially knowing it's a 1-2 person shop with limited budget. reply wavemode 12 minutes agoparentI wish more open source end-user software learned from Lichess, in terms of how user friendly, well designed and well maintained it is. reply TheRoque 32 minutes agoparentprevYou forgot to mention that it's free, open source, and doesn't nor will ever ask for your money, and a lot of people donate. Their expenses are public. It's also available as an app ! reply lepetitchef 37 minutes agoparentprevMe too. Recently the new beta mobile app is even cleaner and has haptic feedback which is so cool. reply robbles 5 hours agoprev> here are the empirical distribution functions (ECDFs) with 30ms added to each response time > The added constant seems artificial, but it's just viewing the results from the point of view of a client with 30ms ping time. Otherwise the log scaled x-axis would overemphasize the importance of a few milliseconds at the low end. I thought this was interesting - maybe it's a standard practice I was just unaware of but it seems like a smart trick. reply aeyes 5 hours agoprevDid they have to reduce cost or is there any other reason to not stick 20TB of SSDs in a box and call it a day? 4TB SSDs only cost ~$300, even HP or Dell SFF drives aren't much more expensive. I guess they were interested in doing the testing and optimization for fun. From a product standpoint I probably would have invested my limited time in other projects. reply broodbucket 5 hours agoparentLichess is a non-profit with a lot of volunteers, they probably don't have the same time vs hardware cost balance as most for-profit companies do reply traceroute66 1 hour agorootparentIt is important not to automatically make assumption that all non-profits are impoverished and run by volunteers. One of the most famous examples is Wikipedia. Technically yes, they are a non-profit. Impoverished ? Certainly not ! Look at the financials, as others have already pointed out. Especially if you are in the habit of donating to non-profits, the financials can make for interesting reading. reply r0ks0n 1 hour agorootparentfrench detected reply ViktorRay 2 hours agoparentprevLichess is a non-profit. It is run entirely on donations and volunteering. It has only 1 employee, the dude who founded the non-profit, and it seems he takes far less money than he could make from any other job based on how talented he is. Also the organization is based in France. I don’t what impact that has on costs but it’s worth mentioning. reply lukhas 1 hour agorootparentWe're up to 2 employees now! The founder and a mobile dev. The impact on costs is \"not small\", because as a rough estimate, the charity pays overall about twice what the dev gets in take-home money, because French employer taxes are high (keyword for the Frenchies reading us: URSSAF). Source: am President of the Lichess charity and have the honour and pleasure of dealing with most of the French administrative paperwork. reply jayemar 1 hour agorootparentprevI had no idea that was the case, that's incredibly impressive! reply bastawhiz 2 hours agoparentprevThey managed to reduce max response times by an order of magnitude. If this project took a week (even two) and some users went from 15s response times to 1.5s response times, only projects where the user experience is even worse or where you work for a for-profit organization where there's money to be made elsewhere (and you admit you don't really care about customer pain) would be a better justification of time. reply BSDobelix 4 hours agoparentprev>testing and optimization for fun In no other industry a engineer would think like that...except in IT. We definitely have too powerful and cheap Hardware, combined with lazy Wetware who just wants to \"call it a day\"....be proud of your work....or so they say. reply chronogram 4 hours agorootparentNot calling it a day anywhere is why Lichess is such a good website. reply aeyes 3 hours agorootparentprevMost things in life are a compromise and it's easy to get tempted to find the perfect solution instead of spending your time on actually moving forward. In all industries there is always something you can do better if only you spend more time. But at most places time is worth money and I'd say $3000 for a few SSDs is little enough to not make this worth my time. reply BSDobelix 2 hours agorootparent>$3000 for a few SSDs is little enough to not make this worth my time. Yeah ok i got it, you are so superior that it's not worth your time in finding the performance problems but throw hardware on it. So happy you don't work on any Operating-system related project or anything that has a massive infrastructure. PS: I know $3000 dollars are your monthly Uber-Eat costs, but not everyone is that loose with money. reply diggan 2 hours agoparentprev> From a product standpoint Makes sense from that perspective, but Lichess is not run as a for-profit company with a product, it's run as a non-profit organization (which it is), so a perspective shift is needed to understand their decisions :) reply silvestrov 2 hours agorootparentTake a look at their financials and $1500 for SSDs would not be out of place. They have yearly expenses for more than $500.000 https://docs.google.com/spreadsheets/d/1Si3PMUJGR9KrpE5lngSk... Seems really weird to be using harddrives when they already have expenses like that. reply lukhas 1 hour agorootparentAs mentionned elsewhere, we're renting most of our infra from OVH, and paying, monthly, for 40TB of SSDs or NVMes would simply explode our yearly budget. Source: am président of the lichess charity (and also one of the sysadmins) reply Timshel 1 hour agorootparentprevLooks like rented stuff to me you can't just add drives ... And while 500k is a lot maybe they can do so much with it because they do not just throw $1500 in drives at every problem. reply Out_of_Characte 1 hour agorootparentprevThe reason is buried in another article \"WDL tables (.rtbw) store the outcome of positions, e.g. if a position is winning. An engine will use this very frequently to decide which endgames to aim for. WDL tables should be stored on the fastest disk (preferably SSD) you have.\" \"DTZ tables (.rtbz) tell the engine how to finish the endgame once it is on the board. They are optional, but required to reliably convert complicated endings.\" Seems reasonable to put the WDL table on the SSD for better engine performance. I do understand not choosing SSD's. The number of lookups for positions always remains the same per user per game. Yet the tablebase is growing more than exponentially. https://lichess.org/@/lichess/blog/7-piece-syzygy-tablebases... reply KolmogorovComp 4 hours agoparentprevWhy scale up when you can optimise? I'm probably going to be downvoted for this, but imo this is really the mindset that leads to bloated software. reply tra3 3 hours agorootparentAgreed. This is the implicit assertion that developer time is more expensive than hardware costs. Seems true in the short term, until the whole system crumbles. reply 29athrowaway 2 hours agoprevThere is also lishogi but it is smaller enough to not require such optimizations yet. Shogi is the most entertaining for chess variants. Xiangqi not as much. reply everyone 3 hours agoprevA lichess is a female lich I'm assuming? (It's like baron / baroness) reply o11c 2 hours agoparentNoble titles are a poor comparison since they're the rare example where there actually is an exclusively-male root form. For most words the root form is neuter, and both male-only (if it exists) and female-only forms require an affix. Properly, a male lich is \"werlich\" and a female lich is \"wiflich\" (unlike other words the /f/ sound is not likely to disappear); the plurals add \"-en\". But generally sex is irrelevant for undead{cn} so the neuter form by far predominates. \"lichess\" is an abominable mixture of German and French roots ... so naturally it is indistinguishable from the rest of English. reply claytonwramsey 1 hour agorootparentnote - \"chess\" is not a Germanic word (deriving from the Arabic شَاه (shah), meaning king). Ironically enough, it comes to English via the Old French eschés, meaning that \"lichess\" is arguably made from entirely French roots. reply o11c 1 hour agorootparentHm, I guess the \"libre\" is French, but \"live\", \"light\", and most importantly \"lich\" are all German. If we look for relatives of \"libre\", they include \"leed\"(song) and the first half of Leopold (adding \"bold\") and Luther (adding \"army\"). The common meaning is \"people\". reply OsrsNeedsf2P 2 hours agoparentprevIt's \"Libre\" chess, as in \"Free (and open source)\" chess reply hocuspocus 5 hours agoprev [–] I know it's not a fair comparison but I'm truly impressed by the quality of engineering shown by the Lichess team, when their main competitor was for example boasting about a migration to GCP and yet suffering from repeated outages due to fairly organic growth in popularity. While I believe they employ 100x more people. Lichess' mobile app was a weak spot, however the v2 rewrite in Flutter is already pretty good while still in beta. And keep in mind Thibault pays himself less than 60k/year. reply sgt 5 hours agoparentI don't think he needs to feel bad about increasing his salary. Make it 200k/yr and make his life easier, which can only be good for the project long term. reply epidemian 2 hours agorootparentIDK about France (where Thibault is from, and IDK if he lives there), but where i'm from, you would have a very comfortable life earning 5k every month, so his self-imposed 60k/yr salary doesn't seem unreasonable at all. At some point, more money yields diminishing returns. reply diggan 2 hours agorootparent> but where i'm from, you would have a very comfortable life earning 5k every month, so his self-imposed 60k/yr salary doesn't seem unreasonable at all. (Some) HN commentators seems weirdly out of touch when it comes to salary outside of IT-heavy cities in the US. The other day someone claimed $125k/year for an employee wasn't \"big money\" (https://news.ycombinator.com/item?id=40927175), so I'd take any comments saying some salary is high/low with a box filled with sand. reply hyperman1 2 hours agorootparentprevI don't know if that 5K is before or after taxes. You easily lose half of what your employer actually pays. reply maccard 1 hour agorootparent€60k pre-tax is roughly in the top 10% of incomes in the country based on a quick google. Not opulent, but definitely comfortable. reply hocuspocus 1 hour agorootparentHis salary is more like €55k though. It's comfortable outside of Paris and other expensive cities. But he could easily double that given his background. Before quitting his job he already worked with Play and the Typesafe (now Lightbend) stack before the peak of its hype, when companies were paying top dollar for consultants. reply hocuspocus 4 hours agorootparentprevI don't know him personally but from the talks he's given, he seems to be ideological about Lichess and his own lifestyle, in a way that would be considered fairly anti-capitalistic by most of the HN crowd :) reply treyd 3 hours agorootparentDo you have links to any of these talks you could recommend? reply heap_perms 1 hour agorootparentNot OP but I can recommend this talk by Thibault (the founder): https://www.youtube.com/watch?v=LZgyVadkgmI reply epolanski 2 hours agoparentprevI think you're highly overestimating how many devs Chess.com has reply hocuspocus 1 hour agorootparentI am not, that's why I said employees not devs. reply Sesse__ 2 hours agoparentprevLichess is a great example of how efficient Wikipedia should have been (both on the code and organization level). :-) reply peter_retief 5 hours agoparentprev [–] Lichess is a great service to casual chess players like myself to get a quick game against another human. Never much of a wait. What I do want to know is how does one pronounce Lichess? Lie chess, Le chess?, League chess? reply jffry 5 hours agorootparentAccording to https://lichess.org/faq#name: \"Lichess is a combination of live/light/libre and chess. It is pronounced lee-chess\" They also link this video: https://www.youtube.com/watch?v=KRpPqcrdE-o reply tecleandor 2 hours agorootparentI guess it's because of the lychee fruit? reply hocuspocus 5 hours agorootparentprev/li:/ as in libre. reply ycombinete 4 hours agorootparentprev [–] I’m team lie-chess. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Lichess optimized its 7-piece Syzygy tablebase server to handle RAID integrity checks under heavy load by switching to dm-integrity on LVM and setting up a second server for benchmarking.",
      "Key findings included issues with RAID 5 setup, high tail latencies with mmap, and improvements using SSDs for specific data lists and parallelizing reads.",
      "The optimizations led to better response times in production, with full details and raw data available on GitHub."
    ],
    "commentSummary": [
      "Lichess, a free and open-source chess platform, has optimized its tablebase server, significantly improving response times and user experience.",
      "The platform operates on donations and volunteer work, with only two employees, highlighting its efficient use of resources despite high operational costs.",
      "The recent beta mobile app update includes cleaner design and haptic feedback, showcasing continuous improvements in user interface and experience."
    ],
    "points": 181,
    "commentCount": 48,
    "retryCount": 0,
    "time": 1720822447
  },
  {
    "id": 40954535,
    "title": "Firefox added [ad tracking] and has already turned it on without asking you",
    "originLink": "https://mastodon.social/@mcc/112775362045378963",
    "originBody": "Create accountLogin Recent searches No recent searches Search options Only available when logged in. mastodon.social is one of the many independent Mastodon servers you can use to participate in the fediverse. Administered by: Server stats: mastodon.social: About · Status · Profiles directory · Privacy policy Mastodon: About · Get the app · Keyboard shortcuts · View source code · v4.3.0-nightly.2024-07-11 ExploreLive feeds Mastodon is the best way to keep up with what's happening. Follow anyone across the fediverse and see it all in chronological order. No algorithms, ads, or clickbait in sight. Create accountLogin About",
    "commentLink": "https://news.ycombinator.com/item?id=40954535",
    "commentBody": "\"Firefox added [ad tracking] and has already turned it on without asking you\" (mastodon.social)171 points by notamy 3 hours agohidepastfavorite123 comments openrisk 1 hour agoQ: How many people are using a web browser on a daily basis worldwide as their main window to the collective infosphere? A: Billions. Q: What fraction of the corresponding GDP would it take to fund a serious browser-as-public-good initiative that would develop this technology to its full potential without the perverse constraints of adtech business models? A: 0.0001%? Too small to compute? The idea that critical communication infrastructure must be (directly or indirectly) supported by advertising interests is certainly not obvious. Advertising businesses, like all businesses or individuals should be guests on that global platform, playing by the rules, not setting the rules. The status-quo is a unique and singular failure that has been normalized for reasons that historians will surely describe with gory detail in due course. There are more ways to fund things than either adtech or dazed and confused individuals paying/donating directly for software. Especially when the stakes are extremely high. Mozilla's attempt to provide a more palatable alternative while accepting the premise that the web is an ad-funded technology was, alas, always doomed. Its market share is trending to zero and it is just a matter of time before complete disaster... reply doe_eyes 53 minutes agoparentThe problem isn't building a publicly-funded browser, it's building a publicly-funded browser that's good and that can keep up with the demands of the platform - which means taking risks, also when it comes to security and privacy. A publicly-funded equivalent of MSIE 6, heroically made 100% secure and private, would be a disaster for the web. And that's a pretty likely outcome if it's designed by a government committee and prioritizes safety above all. reply throwaway81523 45 minutes agorootparentThe \"demands of the platform\" drive expanding capabilities of the browser, which in turn drive increasing commands of the platform. A vicious spiral. Magically getting everyone to switch to a less powerful browser would necessarily beat back the demands of the platform. That would be a wonderful thing, not a disaster. reply mulmen 40 minutes agorootparentTell me you never used IE6 without saying you never used IE6. reply woopsn 1 hour agoparentprevMozilla is up in a bunch of shit at this point. They have a half a BILLION dollar nut to raise annually, and money from Google covers the whole thing. It's ridiculous. https://en.m.wikipedia.org/wiki/Mozilla_Corporation#Finances reply IshKebab 31 minutes agorootparentThe craziest thing is that 70% of their staff are American. What a waste of money. reply moepstar 1 hour agoparentprev> fund a serious browser-as-public-good initiative > A: 0.0001%? Too small to compute? Interesting thought experiment. I think, the question can pretty much be answered with \"whatever is necessary to fund a company, their personnel and keep them afloat and honest\". Not sure when and why Mozilla has deviated from that path/mantra. Did they get greedy? Corporate bloat? reply mulmen 1 hour agoparentprev> There are more ways to fund things than either adtech or dazed and confused individuals paying/donating directly for software. Like what? reply chaps 53 minutes agorootparentGrants! reply lapcat 1 hour agoprevMozilla has been working with Meta on this. \"For the last few months we have been working with a team from Meta (formerly Facebook) on a new proposal that aims to enable conversion measurement – or attribution – for advertising called Interoperable Private Attribution, or IPA.\" https://blog.mozilla.org/en/mozilla/privacy-preserving-attri... reply kstrauser 1 hour agoparentI didn’t see that, and it makes it even worse. We owe Meta nothing. I have zero reason to donate my data to them. reply heresie-dabord 20 minutes agorootparentI have supported Firefox for a long time. I enjoy using Firefox with plug-ins such as uBlock Origin, Privacy Badger, Multi-Account Containers, and FlagFox. I understand the need to work with Google and Meta. I don't like or trust the advertising business but I recognise that these companies are billion-dollar giants. However, activating a feature without asking users is a failure to uphold the mission statement. And Meta is a repeat offender in the domain of security. Mozilla, this is a failure. Here is a fork of Firefox that I have been using: https://librewolf.net/installation/ reply lpgauth 2 hours agoprev\"Google/Firefox claim their tracking features are not \"tracking\" because they use something called \"differential privacy\". I don't have room to explain this class of technology, but I sincerely consider it to be fake.\" Differential privacy is not fake, although quite complex to do in practice. https://en.wikipedia.org/wiki/Differential_privacy reply GrantMoyer 1 hour agoparentThe fake part isn't whether differential privacy exists. The fake part is claiming differential privacy can be used by browsers to provide aggregate ad conversion data to advertising networks without providing information that can be linked to an individual. According to Mozilla[1], Firefox's implementaion uses the \"Distributed Aggregation Protocol\" (DAP)[2]. Individual browsers report their behavior to a data aggregation server, which in turn reports aggregate data to an advertiser's server using differential privacy. But the aggregation server still knows the behavior of individual browsers, so basically it's a semantic trick to claim the advertiser can't infer the behvior of individual users by defining part of the advertising network to not be the advertiser. Now, Mozilla says the data aggregation server they use is run by the Internet Security Research Group[3], which is a non-profit, so perhaps the social incentives truely are aligned in this case to ensure individual user behavior isn't shared with advertisers. But it's disingenuous to claim user privacy is protected absolutely by technical measures when in reality it's only protected by social measures. Finally, ad conversions can easily be measured without cookies by serving unique URLs with each ad, so what's even the point of this technology? I'm not clever enough to discern any ulterior motives (if there even are any), but the complexity of the approach is suspicious to me, since ostensibly a much more obvious solution would suffice. [1]: https://support.mozilla.org/en-US/kb/privacy-preserving-attr... [2]: https://datatracker.ietf.org/doc/html/draft-ietf-ppm-dap [3]: https://en.wikipedia.org/wiki/Internet_Security_Research_Gro... reply throwaway81523 39 minutes agorootparentIt's fake because it provides information that can be used for evil purposes: attribution to an individual has nothing to do with it. It's fake if it really is 100% anonymous. Example: Count Jackboot (your favorite evil politician, Trump or Biden or whoever) is running for office. He wants to know voter opinion on topic X so he can lie about it. He commissions a reputable polling firm to ask people about X, and give him only the aggregated results. The polling firm contacts you, asks your opinion about X, and promises you that your opinion can't be linked back to you. You'll be helping the Jackboot campaign completely anonymously. You believe the anonymity promise, but that's irrelevant, you hopefully don't want to help the Jackboot campaign at all! Saying everything is private because Jackboot only gets anonymous information is a self-serving rationalization by the advertisers and data collectors. The only way to be private is give no information whatsoever. reply thangalin 2 hours agoprev1. Visit about:config 2. Set dom.private-attribution.submission.enabled to false I've added the configuration value to the following page: https://wiki.mozilla.org/Privacy/Privacy_Task_Force/firefox_... I'm not affiliated with Mozilla, but I do understand how wikis work. ;-) reply Topgamer7 1 hour agoparentUnfortunately about config no longer exists for mobile devices reply krono 1 hour agorootparentThere is chrome://geckoview/content/config.xhtml but many options shown there are nonfunctional. The relevant option is listed but I'm not sure if setting it to false has any effect. Edit: Just found out that on that link above, you can set general.aboutConfig.enable to true to enable about.config. It looks like the xml page and the about.config one are the same, as the modifications I made are synced. reply neitsab 41 minutes agorootparentThank you so much for that! I was missing the ability to configure a very important option for me in Stable (layout.css.prefers-color-scheme.content-override), but couldn't keep using Nightly because of its instability... You're a lifesaver! reply mikae1 26 minutes agorootparentprevUse Mull[1]. It has sane defaults and about:config is accessible. [1] https://f-droid.org/packages/us.spotco.fennec_dos/ reply BenjiWiebe 1 hour agorootparentprevabout:config works on Android Firefox nightly. Just checked. reply GeoAtreides 1 hour agorootparenthm, should I use the stable version, but be tracked, or should I use Nightly, and risk crashes and catastrophic loss of data choices, choices reply janice1999 1 hour agorootparentI believe the unbranded Firefox build on F-Droid (called Fennec) enabled it even on stable releases. reply anticensor 1 hour agorootparentprevor use an unbranded fork that has about:config enabled reply exe34 1 hour agorootparentamazing, the one app that has access to everything that matters in your digital life, and you'd be willing to use a non-standard source? reply tooltower 1 hour agorootparentprevI just found this setting in mobile, but I don't know if it's the same feature: Settings > Data Collection > Marketing reply morsch 1 hour agorootparentI don't think so. I already had the setting visible in the UI disabled, but the thing in about:config was still on. reply billfor 1 hour agoparentprevLooks like the windows build for 127 didn't have it but it's there in 128. Updating to 128.0 adds the preference (defaulted to true) and also the new \"Website Advertising Preferences\", which seems to control the same preference. I would just uncheck the box as it's right there on the Security page. reply johnnyanmac 1 hour agorootparentThanks, I just updated to 128, and found the setting under Settings -> Security -> Website Advertising Preferences. I wish I could be surprised that this was opt-out by default, but when you know how Mozilla is funded it all clicks. reply lkdfjlkdfjlg 1 hour agoparentprev> I'm not affiliated with Mozilla, but I do understand how wikis work. ;-) You don't seem to understand why this is problematic though, so I'll explain it to you: enabling tracking when you know that one of your selling points to your users is respect for privacy is a huge breach of trust. reply intelVISA 1 hour agorootparentTo be fair, I don't think that has been an active Mozilla mission for close to a decade now. They mostly exist as a Google owned shell now... reply tredre3 1 hour agorootparent> To be fair, I don't think that has been an active Mozilla mission for close to a decade now. Ok, maybe they should update firefox's home page, then? Very first lines: > Get the browser that protects what's important > No shady privacy policies or back doors for advertisers. Just a lightning fast browser that doesn’t sell you out. reply johnnyanmac 1 hour agorootparentGP did say \"active mission\". I'm sure Google had \"Do no evil\" on their site for maybe 4 years after they in fact started being evil. reply lkdfjlkdfjlg 1 hour agorootparentprevMaybe it hasn't been an active Mozilla mission for a decade in practice, but they did paid it lip service many times in the past decade, so still counts as breach of trust. reply tedunangst 1 hour agorootparentprevWhat do you expect thangalin to do with this understanding? reply lkdfjlkdfjlg 17 minutes agorootparentBe less pretentious. reply david_draco 2 hours agoprevIt is obvious why Firefox does this though; they have no income otherwise like Google does. Firefox users somehow think that using is \"supporting\" Mozilla/Firefox, but it is not, and they would not pay for the browser, or pay a subscription free. Privacy-friendly ads are a reasonable way for Mozilla to survive long-term -- if they are indeed privacy-friendly. Ultimately you probably either need a clean-room NGO that ensures the data cannot be de-anonymized, or accept that ad impact counting is BS anyway and only measure profit increases across A/B ad phases. reply dsr_ 2 hours agoparentYou can't reasonably claim \"they would not pay for the browser, or pay a subscription [fee]\" when it is not even possible for a user to donate to the Firefox project specifically. reply photonbeam 2 hours agorootparentIts rather annoying that theres no way to support firefox without sending money to a Mozilla ceo who could use it for some silly side project reply ta754774675 1 hour agorootparentTime to go back to lynx geez. The internet caught cancer. reply SkyPuncher 1 hour agorootparentprevWhat browser has a subscription user base that could support Mozilla? reply SoftTalker 2 hours agoparentprevI would pay for a browser that was 100% ad- and tracking-free. I pay for an email account. I pay for YouTube. I pay for several streaming media services. I get that people are used to browsers being free, but no reason that can't change. reply SamuelAdams 2 hours agorootparentYou are very rare. Most people are not like you. Apple tried this with iOS 2 and 3. Minor versions cost users roughly 5-10 USD per update. Therefore many users did not install the latest OS on their devices. The cost, although small, was a barrier for many people. Apple quickly pivoted and now all software updates are free of charge to all supported devices. If Mozilla starts charging for Firefox, I predict either people stick with the oldest version that is free, or stop using Firefox and use a fork that maintains its free (in cost) license. Or maybe only 2% of users convert to a paid version of Firefox. reply mikeocool 1 hour agorootparentI don’t disagree with your point — however apple only charged for the early iOS updates on the iPod touch. And they only did it to comply with the Sarbanes-Oxley Act — which required that if you upgraded a device not on a subscription, you had to charge. They stopped doing it after they lobbied congress to change the law. https://www.macworld.com/article/189247/ipodtouch-3.html reply jbaber 1 hour agorootparentprevHonsetly, pay-what-you-want-if-you-can would work well. Just make a button appear once in a while (that you can permanently turn off in about:config). If firefox asked me once a month \"Enjoying the entire internet? Is it worth $1 to you?\" I'd press the button often. reply aaomidi 1 hour agorootparentprevOS updates are not comparable. Apple charging for updates is idiotic because as a user I don’t have a choice to go use a different OS. reply aetch 1 hour agorootparentYou do have a choice, just keep using the OS version you have. Just like using the an older Firefox version. Adding security updates and new features costs time and money. reply evah 36 minutes agorootparentIf Apple delivers a defective device to the customer, I see no reason why they shouldn't be fixing it using the money the customer originally paid. A security vulnerability may eventually leave a device completely unusable. reply GTP 54 minutes agorootparentprevThe point is that with browsers there's also the option of using an entire different one, not just keeping an older version of the same browser. reply redserk 1 hour agorootparentprevApple seems to have made the economics work out well with devices alone. You don’t even have to touch their services revenue to consider ongoing iOS development a significantly successful return on their investment. reply HighGoldstein 1 hour agorootparentprevThere are a couple of problems with this argument. One is that with a device (especially a premium one) the cost of support for a reasonable lifetime is considered baked into the price. The other is that security updates imply a security issue, meaning the company sold you an insecure, i.e. defective device in the first place. reply fancy_pantser 2 hours agorootparentprevWhat stops you? Are you not satisfied with the ones on offer? Is there a compatability issue that's a show-stopper? reply nemomarx 2 hours agorootparentWhat paid browsers are on offer with good technology? The only ones I'm aware of are still chromium based or I think mac only, so that's a pretty bad feature set. reply dtech 2 hours agoparentprevThe thing is, you cannot pay for Firefox even if you wanted to*, so the assertion that people wouldn't pay is unproven (but has good circumstancial evidence). I'd still prefer they make a paid version without this crap. * Donations to Mozilla go to a non profit which is separated from Firefox development and has questionable effectiveness in general reply poikroequ 1 hour agorootparentThe reason many companies don't offer a paid option to remove tracking is it can be seen as an admission by the company that they know tracking is wrong to some extent. So these companies would rather just force it on everyone and pretend like there's nothing wrong with it. reply kredd 1 hour agorootparentprevAfter all these \"I would pay for Firefox if I could\" comments, it would be fun for Mozilla to start a Gofundme like page, where if it hits $300M (or whatever amount they're getting from Google per year) they'll make it an option, otherwise they'll go back to trying to find another revenue source. It's very hard to believe that an average user would ever pay for a browser, when alternatives like Chrome and Safari exist. It's the same as paid email services, in my opinion. Like sure, there will be some segment of users who'll do it, and they'll probably get $10-20M/year if it offers some features free email services don't. But hitting that $100M through donations on a yearly basis would be hard when there are free equivalent alternatives. reply wizee 1 hour agorootparentMozilla is like Wikipedia, where the vast majority of the funds they receive go to causes unrelated to development or maintenance of their core product (web browser or encyclopedia). reply dralley 1 hour agorootparentHN is interminably ignorant about the mechanics of this for a website theoretically so heavy on entrepreneurs. Mozilla Foundation is a 501c3 nonprofit. Of course their donations go towards causes. \"product\" is the word you use for an artifact provided or service rendered that makes you money. Spending tax-exempt dollars on developing products that make you money is illegal for reasons that really aren't that difficult to understand if you think about it even a little bit. reply lolinder 1 hour agorootparentMozilla chose to structure themselves in a way that was confusing and has led to their interminable distractions and side projects. They apparently were under the impression early on that Mozilla Corporation would somehow actually make money from Firefox which would then fund the Mozilla Foundation's other projects. I doubt that they anticipated that nearly all of the money coming into Firefox would come from their primary competition and they'd wish they could allocate funds in the other direction, but here we are. If you know so much more than the rest of us: Is there some important reason why they haven't fixed their structure already? The for-profit supposedly reinvests all its profits into the non-profit, so I'm unclear what purpose the distinction is serving at this juncture. reply lolinder 1 hour agorootparentprevFor example, acquiring ad companies. reply dralley 1 hour agorootparentMozilla Foundation did not acquire an ad company, and none of their dollars can be legally used to acquire and ad company. Once again, HNers fail to understand the difference between Mozilla Corporation and Mozilla Foundation, and conflate all their criticisms. reply lolinder 1 hour agorootparentI'm well aware of the convoluted corporate structure and that convoluted structure is one of my primary criticisms of Mozilla. reply lolinder 1 hour agorootparentprevYou're attacking two strawmen: 1. The average user doesn't have to pay for the browser in a donation model, you just need enough users to feel passionately enough about it to fund it sufficiently to develop it. 2. No one is arguing that Mozilla should replace their revenue from Google overnight with donations. We're just asking that Mozilla give us the option to pay for Firefox already. Another user (trying to demonstrate to me that donations would never be enough [0]) figured that if we assume a similar rate of donations as Thunderbird gets then Firefox would bring in $70m/year just in donations. That is a heck of a lot of money. That funds 140 developers even at inflated Bay Area salaries, 280 developers if you're willing to branch out of the Bay and offer closer to $200k/year on average as a base salary (still an insanely high average rate in most of the country and the world). Even if you took a full 50% for general/administrative and overhead, that sum would still pay for 70 bay-area or 140 rest-of-the-world developers. If Mozilla really does need more developers than that for Firefox specifically, then fine, they can keep accepting money from Google—no one is saying they should only be funded by donations. But that they don't even make it an option is frankly bizarre. [0] https://news.ycombinator.com/item?id=40901664 reply mulmen 1 hour agoparentprevI pay for Orion through Kagi. I would gladly contribute to the salaries of developers to maintain the Firefox project. reply iroddis 1 hour agorootparentI also pay for Orion, but can’t use it much until the multi-container support is working well. Right now Firefox is the only browser that does this right. Upvoting you in the hopes that more people in this thread will put their money where their stated principles are and help support a privacy focused browser with clear funding sources so that Orion doesn’t go the way of Opera. reply kstrauser 54 minutes agorootparentI also used Firefox’s containers a lot. In my case, I often need to log into multiple AWS simultaneously, or at least bounce between them quickly enough to be a major hassle if I had to log out of one to log into the other. Now I use Safari’s profiles to do that. What’s your Firefox use case that Orion doesn’t handle? (Sincere question; that wasn’t meant as “it works for me so stop complaining!” snark.) reply bloopernova 1 hour agorootparentprevOrion isn't available for Linux. reply mulmen 1 hour agorootparentTrue, not yet. It is still proof that people are willing to pay for a browser that aligns with their priorities. reply hot_gril 1 hour agoparentprevBesides money, market share must also be important for Firefox. Otherwise the web becomes Chromium-only. reply zaphod420 58 minutes agoparentprevI paid for an Orion Plus lifetime license (browser made by kagi). I'd happily pay for Firefox too if it was an option. reply JadeNB 2 hours agoparentprev> Privacy-friendly ads are a reasonable way for Mozilla to survive long-term -- if they are indeed privacy-friendly. At some level, trusting that privacy-friendly advertising through Firefox actually respects privacy is going to have to involve trusting Mozilla. Mozilla seems to have gone out of its way over the years to erode user trust, and this is just one more step down that road. As the author says, if Firefox is even sneakier about this than Chrome, what scope is there for trust? reply mulmen 1 hour agorootparentI don't think Mozilla is going to pull a Google and deliberately choose to become evil. Mozilla simply doesn't have (or want?) the resources to hire competent product people (if such a thing even exists) to manage features and marketing. This is the problem with running software as a company instead of an open project where the product is the end rather than a means to profit. reply CrendKing 1 hour agoprevThis gives me impression like what happens to the nuclear weapon proliferation. At beginning, it is an arms race, between US and USSR, between users and advertisers. Either side thinks they can't survive without vanquishing the other. Eventually they realize it is stupid to continue, and reach a point to both step back. I think Mozilla is at the point where they realize it is no longer beneficial to continue the race against advertisers. It is time to collaborate. This way both users, advertisers, and maybe Mozilla themselves can all benefit from stepping back one foot. I personally support this move. Morally speaking, content creators I consume deserve income from my visit, as long as my privacy is preserved. Seems a good compromise if it works. reply galdosdi 48 minutes agoparent> Either side thinks they can't survive without vanquishing the other. Except, it actually is not a two way street. It's purely one way. Without users, the content sellers can't survive. But if users stop consuming ads and eliminate content revenue streams, the users will be just fine. So what if TikTok goes out of business or something? All the failure of online advertising would mean would be regressing to a time when the internet was not very commercialized yet, which was an amazing awesome time when we had pretty much all the positives of today with few of the negatives. They need us, but we don't need them. Big Content is a parasite. reply johnnyanmac 1 hour agoparentprev>Morally speaking, content creators I consume deserve income from my visit, as long as my privacy is preserved. sad for the content creators (I do actievely try to donate and subscribe to quality content when apt), but I simply don't tryst my privacy being preserved any longer. So opt out of this setting and keep Adblock extension on. The well has long been poisoned for me. But I'm also in the minority and it seems there's still enough adrev going that I'm barely an atom in the market. reply Falkon1313 12 minutes agoparentprevI might consider it when advertisers stop using malware and spyware in their ads. There's absolutely no reason that an ad would need to run a script, contact a third-party system, or track anything about the viewer. So far though, they show no intentions of doing non-hostile advertising. Instead they're constantly striving to make it even worse. So I'll keep the adblocking as it remains a reasonable and necessary defense measure. reply lolinder 41 minutes agoparentprev> Morally speaking, content creators I consume deserve income from my visit, as long as my privacy is preserved. For me, tracking is not my primary concern with ads: I use an ad blocker as an accessibility tool to allow me to even exist on the internet at all. I have ADHD. Nearly all content on the internet is flanked by ads that make it impossible for me to actually read or watch it—they're intentionally distracting enough to draw the eye of a neurotypical person, and it's hopeless for me. I dread a world where even Mozilla embraces advertising and the false idea that the only thing to solve is privacy. Ads are a problem for many, many reasons, and we need to find alternative answers for funding. reply elashri 1 hour agoparentprev> content creators I consume deserve income from my visit, as long as my privacy is preserved I doubt that most people in these discussions wouldn't agree with that point. The problem lies in the details. Advertisers don't take anything less that complete personalized targeting. We are not in the 2000s era of buying ad space on related websites/forums anymore. The problem is there are misalignment between targeted ads and privacy. And I didn't find all the proposal for anonymity successful, it is always possible to de-anonmize the data. reply david_draco 2 hours agoprevThe thread does not explain whether the behaviour of Firefox was actually changed when they added the check box, or if it is a new option to opt-out of something that could not be opted-out before. reply nullindividual 2 hours agoparentIt added in the 'privacy preserving feature' that did not exist before, automatically enabled. > Firefox now supports the experimental Privacy Preserving Attribution API, which provides an alternative to user tracking for ad attribution. This experiment is only enabled via origin trial and can be disabled in the new Website Advertising Preferences section in the Privacy and Security settings. https://www.mozilla.org/en-US/firefox/128.0/releasenotes/ This is the page users saw when they updated to 128. No mention of this setting. https://www.mozilla.org/en-US/firefox/128.0/whatsnew/?oldver... reply Vinnl 2 hours agoparentprevIt's new: > Firefox now supports the experimental Privacy Preserving Attribution API, which provides an alternative to user tracking for ad attribution. This experiment is only enabled via origin trial and can be disabled in the new Website Advertising Preferences section in the Privacy and Security settings. https://www.mozilla.org/en-US/firefox/128.0/releasenotes/ reply ZeroGravitas 2 hours agoparentprevThe poster of the tweet also says later: > I recommend turning it off, or switching to a more privacy-conscious browser such as Google Chrome. Which seems an interesting comment from someone so extremely anti-advertising. reply lukan 1 hour agorootparentI read it as satire. Even chrome is better than firefox. \"Now to be clear, the disclosure Chrome provides to users is not adequate. Their wording of the \"Ad Privacy\" feature popup is highly disingenuous and the process to disable once notification is given is too complex and must be performed on a per-profile basis. But at least they do it\" reply NuSkooler 1 hour agorootparentprevThe suggestion to switch to Chrome is pretty rich. Firefox is more privacy conscious Chrome even with this enabled. reply Porygon 2 hours agoparentprevI think this is the check box in question: https://pomf2.lain.la/f/8v4aq9sg.png It is located at \"Settings\" -> \"Privacy & Security\" -> \"Website Advertising Preferences\" and is checked by default. reply instagib 1 hour agorootparentI followed the top level comment and added Boolean dom.private-attribution.submission.enabled False It disabled the checkbox after updating Firefox. I set it to true to test and it enabled the checkbox also. reply nullindividual 3 hours agoprevhttps://news.ycombinator.com/item?id=40952330 [131 comments, 8 hours ago] reply Animats 2 hours agoprevIt's new. That misfeature was checked for me, even though I had \"strict privacy protection\" set, the level that comes with a warning that it might break some websites. And even though I had \"Do not track\" checked. On two different computers. This version of Firefox, 128.0, was auto-installed by Ubuntu update. Has someone filed a bug report on Firefox yet? reply notamy 3 hours agoprevWas uncertain how to word the title better, open to suggestions. HN's title character limit made it hard. This is about \"privacy-preserving attribution\" https://support.mozilla.org/en-US/kb/privacy-preserving-attr... reply brunoqc 1 hour agoprevAnyone knows how to disable this on Android? reply hoppyhoppy2 50 minutes agoparentmenu > Settings > Data Collection reply brunoqc 40 minutes agorootparentIt doesn't seem to mention the new \"privacy-aware\" tracking. I got studies, something about \"adjust\" and some telemetry thing. Also they were already all off so it must not be one of them, I think. reply metadat 1 hour agoprevWhat incentivized Mozilla to do this? How does this get them more money? (I.e., who's paying M for this?) reply lolinder 41 minutes agoparentThey just bought an ad company. reply ta57834774774 1 hour agoparentprevThey have been Goog's pet for a long time. Only thing staving off antitrust. reply mikeiz404 47 minutes agoprevThis is the browser setting's \"learn more\" link: https://support.mozilla.org/en-US/kb/privacy-preserving-attr... reply idle76 58 minutes agoprevWierd, I just updated to v128 (been offline a couple of days) on android, and the Settings -> Data Collection -> Marketing wad already off. I already had the usage and studied off, so maybe that's why reply CTOSian 49 minutes agoparentI am on 128.0 on android and no such option on settings, but that 'dom.privacy...' flag was 'true' reply ChrisArchitect 1 hour agoprev[dupe] More discussion: https://news.ycombinator.com/item?id=40952330 reply mouse_ 2 hours agoprevMeanwhile, Ladybird, the first new browser engine since the 90's to 100% the Acid3 JavaScript test, just secured $1 million in funding from the founder of GitHub. We, as responsible web users, need to do whatever it takes to break up Google's web oligopoly. The open web is at stake. reply pjmlp 2 hours agoparentIt starts by developers stopping by shipping Electron junk (want a Web app with native APIs?, target the system browser with a daemon), or using Chrome only APIs. reply jacknews 1 hour agoparentprevAnd I see servo (the ex-mozilla engine rewrite in rust) is getting some news lately too. reply tatersolid 2 hours agoparentprevI read the Ladybird FAQ for their rationale, but building a new browser in 2024 using an unsafe language is such a facepalm it’s hard to take the entire project seriously. reply pjmlp 1 hour agorootparentWhile I enjoy the sentiment, it isn't as if Rust is doing any favours regarding Firefox adoption. reply intelVISA 1 hour agorootparentWe've added tracking but at least it's safe(tm). reply TacticalCoder 1 hour agorootparentprevYup I don't understand the downvotes. I don't code in Rust but I also don't feel insecure about it: I wish more projects were written in Rust (or something similar). There are several research, already published here many times, which show that something insane like 75%+ of all the security exploits would be rendered cold dead in their tracks had Rust been used. I don't know how anyone, even a C/C++/VisualBasic/PHP coder, could not like that. reply ninjin 37 minutes agorootparentI suspect the downvotes are for a very simple reason: \"Why not Rust?\" comments are contributing next to nothing to the conversation. At this point, comments like this are tiresome and predictable. What would be interesting are detailed separate posts such as what you mention about security exploits addressed and of course the stream of wonderful software that people are writing in Rust (and other languages as well for that matter). Bringing it up in relationship to Ladybird, which is an amazing accomplishment already, is incredibly petty and off-putting. The poster can do better and the community deserves better. reply jacknews 1 hour agorootparentprev'unsafe language' sounds like something out of '1984' or 'Animal Farm'; a totalitarian political euphemism, attempting to demonize all 'others'. The reality is that no language is actually 'safe', and 'safety' itself is a complex trade-off between enforced restrictions, flexibility, and other factors, just like in life. reply nottorp 1 hour agorootparentDevelopers gotta have religion. It's not about fear of death like mainstream churches, it's about fear of buffer overflows. reply convolvatron 1 hour agorootparentprev'the rationale that they are actively evaluating other develoment platforms? \"However, now that Ladybird has forked and become its own independent project, all constraints previously imposed by SerenityOS are no longer in effect. We are actively evaluating a number of alternatives and will be adding a mature successor language to the project in the near future. This process is already quite far along, and prototypes exist in multiple languages.\" reply aaomidi 1 hour agorootparentThey need to commit to not using C/C++ for the majority of the project. If not, then yes it can’t be taken seriously. reply solardev 1 hour agoparentprevWhy can't they just fork Blink like everyone else? A new rendering engine is an unnecessary duplication of effort and ripe for security issues. I don't think the open web is really as dependent on the browser anymore anyway. We already have multiple choices today, but the web is still mostly controlled by a huge big companies and nations. Doesn't feel particularly open. A new browser engine won't really solve that. Meanwhile a million dollars could go towards some network R&D or popularizing Freenet instead. The world doesn't need a dozen HTML renderers. If we actually want a free and open web, that's a infrastructure and content censorship/algorithmic promotion issue, not a layout engine or JS engine problem. reply ta46873676 1 hour agoprevOK I'll step up, senior dev of 30 or so years, and sick to death of the likes of Google ad-men trying to ruin things - how do I help? That said, keeping up with changes in JS/ecma seems hard. Whats the answer? reply TheDong 1 hour agoparent> how do I help? Ads aren't very important really, if you want to help, I recommend donating all your money to those in need and volunteering at a local homeless shelter, soup kitchen, or anything of the like. reply 1oooqooq 15 minutes agoprevfun thing is, only google/bing/meta/amz/anyone who sell ads in large scale/etc can profit from this. the best Mozilla can hope to gain is to get some scraps from google. disgusting. btw this wasn't discussed in any of the public Mozilla forums we monitor. reply jacknews 1 hour agoprevThe fact they didn't loudly announce this 'feature' has seriously undermined their trustworthiness, for me at least. What's the expression; 'So much depends on reputation. Guard it with your life'. Mozilla seem to be just throwing it away. reply jopsen 36 minutes agoparentWho would rather trust? Apple, Microsoft, Google? Realistically, these are the organizations that can afford to develop a high performance browser engine. All the chrome and firefox forks probably don't have the devs or infrastructure to fork blink/gecko and keep up with security and features. It's easy to be excited about Ladybird -- and maybe it will work, MAYBE. It's fair to argue that we've let the web evolve into such an advanced platform that building a secure high performance browser is a HUGE moat. People are so fast to criticize Mozilla. Maybe, this isn't all bad. reply jacknews 0 minutes agorootparentwhataboutism? Sure I trust Mozilla more than Google/Microsoft. But less so, now. reply nullc 44 minutes agoprevDo I understand the feature correctly? Your browser tracks your activity and submits it, non-anonymously to a mozilla operated server where it is vulnerable to lawful or lawless interception or compromise by hackers, and from there they sell anonymized (hopefully) data to advertisers? reply 93po 52 minutes agoprevStill waiting for a browser that doesn't send a million things to fingerprint me with. Random websites don't need to know the battery level of my device. It shouldn't even be able to know my window size or resolution. It's beyond me that we should provide any information to send back other than our IP address and the resource we want to access. Anything more than that should be allowed on a case-by-case basis, but 99% of websites don't need more. reply deng 1 hour agoprevAs usual when people criticize Mozilla, this thread is way over the top. I agree it's not good that this is on by default. But saying that Chrome is better because it at least asks is disingenuous. Chrome simply presents you with the \"Enhanced Ad Privacy\" window and a button \"Got it\" or \"Settings\". That's clearly a dark pattern and technically not \"asking\" at all. The Topics API which you enable by clicking \"Got it\" is, at least from what I read, clearly worse than what Mozilla has implemented. Calling \"differential privacy\" a fake is simply untrue. It is not easy to implement, but if done properly, it's absolutely not fake. I agree though that Mozilla has, as usual, dropped the ball here in how they have introduced this technology. They are obviously desperate, and they know if they would ask, probably the vast majority of people would not agree. Also as usual, they will probably roll back this setting once the outcry is large enough, and they have once again lost trust and gained absolutely nothing. It's also clear that with the tiny market share Firefox has nowadays, thinking they could introduce a new ad technology is simply hubris. reply zihotki 26 minutes agoparentThey are not desperate, they are transforming into ads company which started after they bought adtech company reply lern_too_spel 2 hours agoprev [–] This is similar to the PPA/PCM that Safari added in 2021. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "Firefox has introduced ad tracking by default, igniting debates on privacy and browser funding models.",
      "Critics argue that essential communication tools should not depend on ad technology, and Mozilla's reliance on Google funding is seen as problematic.",
      "Users can disable the tracking feature in settings, but the move has damaged trust in Mozilla, raising broader concerns about sustainable funding for privacy-centric technology."
    ],
    "points": 171,
    "commentCount": 123,
    "retryCount": 0,
    "time": 1720883302
  },
  {
    "id": 40952330,
    "title": "Ad-tech setting 'Privacy-Preserving Attribution' is opt-out in Firefox 128",
    "originLink": "https://gladtech.social/@cuchaz/112775302929069283",
    "originBody": "Create accountLogin Recent searches No recent searches Search options Not available on gladtech.social. gladtech.social is part of the decentralized social network powered by Mastodon. Administered by: Server stats: Learn more gladtech.social: About · Profiles directory · Privacy policy Mastodon: About · Get the app · Keyboard shortcuts · View source code · v4.2.10 SearchLive feeds Login to follow profiles or hashtags, favorite, share and reply to posts. You can also interact from your account on a different server. Create accountLogin About",
    "commentLink": "https://news.ycombinator.com/item?id=40952330",
    "commentBody": "Ad-tech setting 'Privacy-Preserving Attribution' is opt-out in Firefox 128 (gladtech.social)170 points by rapnie 12 hours agohidepastfavorite152 comments ajb 9 hours agoRight, so there is a genuine moral case for Mozilla doing this. It depends why you hate ads: A: the main problem with ads is tracking and privacy invasion B: the main problem with ads is manipulation and seizure of my attention If you only care about A, you might like this approach, as in principle if it works and becomes standard, then the pressure from the ad industry to track everything will be easier to resist, as 1) they will have less incentive, and 2) the argument that tracking is essential is undermined, so it may be possible eventually to ban it. Of course, that assumes that you trust that this is better for your privacy than tracking. After all, it does feel a bit like tracking... If it's done properly then your individual data is not sprayed to a thousand dodgy ad brokers, but only to one company who tells advertisers not about you personally, but just whether their ad is working. The question is, does it work? Are they doing it properly? Do they have the correct incentives to keep doing it properly in the long term? Can advertisers just undermine it by giving everyone's ads a different ID? Also, your threat model may include that this aggregator company is hacked, or that the government secretly forces it to share the data with them. Nevertheless, I think that Mozilla probably genuinely think this is better for privacy. And there is a case that there is. A big issue however its that at present the constituency for Firefox includes people who care about B. This doesn't undermine that directly, but it does mean that Mozilla have an incentive not to care about it. reply macNchz 5 hours agoparentFar above and beyond issues of privacy and attention, my primary concern with internet ads today is malicious content. There is simply too high a volume of deceptive—or outright malware-laden—ads to feel comfortable allowing them to render in my browser. It’s not even an edge case hiding in dark corners of the internet, search ads at the top of Google results are consistently used for phishing and other fraud. The incentives are all wrong for ad networks to address the issue. As long as websites use third-party networks which themselves let anyone sign up and buy ads, it will continue. Doing too much vetting of ads or KYC on advertisers cuts into the bottom line. I don’t think all that much about privacy issues with ad measurement, because all of my browsers, those of the friends and family I advise, and those of the companies whose security policies I am responsible for, will block ads for the foreseeable future. reply 1vuio0pswjnm7 9 hours agoparentprevC. the main problem _is_ ads Mozilla assumes ads are required. Ad blockers assume ads are not required. Ad blockers are probably more popular than Firefox. The original web did not have targeted ads and did not require ads to exist. It has never faced an existential threat due to \"not enough data collection and targeted advertising\". Ad blockers are correct. reply amelius 6 hours agorootparentD. the main problem with ads is that they stimulate more unnecessary consumption Ads are the wrong monetization model for the web. Instead of paying for information directly, you are now paying for information by buying something physical (most ads are about physical items). In this resource-limited world, this is clearly ridiculous. reply heresie-dabord 33 minutes agorootparent> D. the main problem with ads is that they stimulate more unnecessary consumption The main problem with ads is... I do not want them. My computer, my electricity, my network, my ISP contract, my privacy, my life and my decision. This is a fork of Firefox that I support: https://librewolf.net/ reply nullindividual 3 hours agorootparentprev> D. the main problem with ads is that they stimulate more unnecessary consumption Yes! If our culture(s) weren't so damned full of pure consumerism, forgetting the first 'R' in Reduce, Reuse, Recycle, we'd potentially be living in a healthier ecosystem. And honestly, having gone from living with a hoarder to living with a tad more than essentials where I can see the damned floor in my house, it's such a f-ing relief. reply BadHumans 1 hour agorootparentNot all ads are for physical goods. reply johnnyanmac 57 minutes agorootparenthonestly the worst, most persistant ads are for games. The pump millions into it because they can utilize whales to make billions. Which means more ads. reply immibis 9 hours agorootparentprevMozilla is an ad company by its revenue sources. reply whamlastxmas 8 hours agorootparentBy this logic, every TV station and website whose main revenue is ads are also ad companies, which I don’t think is accurate. reply h0l0cube 6 hours agorootparentI agree that TV stations and websites don't have to be purely vessels for advertising, but in today's climate of slim margins – due to an expectation that everything is 'free' – and sliding ethics, advertising lends more and more bias to operations. And that bias has been there as long as there has been advertiser funded operations. https://en.wiktionary.org/wiki/news_hole We could just return to a culture of paying for things (that are conspicuously not tracked) and this whole imposition of advertising in our daily consumption can go away. But the vast majority of people simply don't care. reply michael9423 5 hours agorootparentI think ads should not be allowed for anything at all in the digital world, including TV. In a world without ads, products would be known due to reputation. Ads are an assault on the limited resource of attention. People say how can new products reach people, but the solution to that problem isn't ads but platforms/websites that catalog new products and allow for easy discovery. Ads actually make the problem of finding new products worse because you only see a fraction of new products if you consume ads, primarily those from companies that already have lots of money to invest into ads. reply h0l0cube 21 minutes agorootparentIt’s a nice dream, but enough people have to be willing to pay for things in preference to tolerating ads to not pay. Streaming services and some online news services provide this option, so it’s available. But asking for everything to be free and ad free is wishful thinking, at least outside government funded media (which is still paid by taxes) reply lolinder 2 hours agorootparentprevGoogle income aside, Mozilla literally owns an ad company: https://blog.mozilla.org/en/mozilla/mozilla-anonym-raising-t... reply Gud 6 hours agorootparentprevIf their revenue is from advertising, they’re in the advertising business. There are other business models out there. reply JTyQZSnP3cQGa8B 6 hours agorootparentprevThe CEO of the biggest French TV channel once said \"We’re selling eyeballs to advertisers,\" so yes, it’s accurate. reply harrisi 8 hours agorootparentprevWhy not? What else would you call them? reply michaelt 6 hours agorootparentWe'd call them TV stations. I mean, we call social media companies social media companies despite them being ad funded. We call newspapers newspapers despite them being ad funded. We call sports teams sports teams despite them being ad funded. reply alexvoda 6 hours agorootparentAnd we have also learned to use taxonomies and hierarchies and sets. Therefore the intersection between the set tv stations and the set ad companies is the set ad-funded tv stations. Ad-funded tv stations are a subcategory of the larger ad companies category. reply michaelt 6 hours agorootparentprevWhy stop there? If Mozilla produce software, funded by search engines, funded by ads, funded by laundry detergent manufacturers, are they a laundry detergent company? reply shrimp_emoji 6 hours agorootparentprevMozilla is an ad company under the new leadership, hence the assumption. And exactly: I reject C. I reject the monetization of the web. It didn't need to happen; we just wanted it to happen in our greed. We bit the apple. Money is the root of all evil. It ruins everything. reply Barrin92 5 hours agorootparentprevConsumers vote with their wallets and will virtually always take a free product with ads over anything else, so taking a dogmatic stance against ads is just going to be financial suicide. Ad blockers simply live on borrowed time until they're banned or made unusable. In which case if a competitor springs up, at some point they need money, which means they'll need to serve ads. Like say Brave, which blocks ads and then ... serves you their own ads. The original web is irrelevant because it didn't sustain large economies. It's like comparing radio hobbyists to the central TV station, the economics change when you have to actually pay employees and serve a mass market. reply lolinder 2 hours agorootparent> The original web is irrelevant because it didn't sustain large economies. It's like comparing radio hobbyists to the central TV station, the economics change when you have to actually pay employees and serve a mass market. A large number of us wish that we could go back to a web that didn't exist primarily to sustain large economies. Back to a web that existed for academics and enthusiasts, a web where modern HN discussions would have been ranked in the lower half of quality instead of the top 5%. That web existed as late as 2010 (already as a bit of an enclave, but a thriving one). It was killed by ad-funded social media and some of us are still bitter about it. reply ajb 4 hours agorootparentprevI'm a bit puzzled by this comment. By saying you have an option \"C\" you are indicating that there is some other factor, intrinsic to ads, that you object to. Which is a valid position - others have mentioned some- but, you don't say what that factor is. My curiosity is piqued! Can you elucidate? reply Trompair 3 hours agoparentprevC: the main problem with ads is they serve as one of the most prevalent infection vectors for malware. The tracking and seizure of attention are of lesser concern to me personally. reply ring-c 8 hours agoparentprevC: Ad use my network and my hardware reply BadHumans 1 hour agorootparentI never understood this argument. You agree to see ads when you use a service someone provides you for free. You are within your right to block ads but they also have every right to show them. reply leni536 1 hour agorootparentThey have every right to not show their main content. Some content providers do that, but it's weirdly unpopular. reply MyFedora 9 hours agoparentprev> if it works and becomes standard We're talking about Firefox here. Advertisers care about browsers that most people use, and Firefox isn't one of them. reply danieljacksonno 6 hours agorootparentthis standard is also pushed by Google and Chrome, AFAIK reply robin_reala 6 hours agorootparentChrome has a different standard called Topics. reply client4 11 hours agoprevIn a few years, when Ladybird is stable and largely feature complete, Mozilla will act incredibly confused as to why they are losing market share. https://ladybird.org/ reply defrost 11 hours agoparentServo independantly restarted last year (ex-Mozilla project from way back, Rust based web rendering engine). Already a good way through CSS and WPT test suites: https://servo.org/about/ That's two seperate lines of not-Chrome not-Mozilla dev in the pipeline. reply OJFord 9 hours agorootparentAre there any (obviously as yet quite broken) browser projects using Servo? reply defrost 9 hours agorootparentThese pre-built nightly snapshots allow developers to try Servo and report issues without building Servo locally. https://servo.org/download/ Please don’t log into your bank with Servo just yet! So .. just their own test shell compiled about engine as yet. (AFAIK) There a few longish tech talks on youtube, the initial focus was on bringing documentation up to date in order to draw third party dev's in. Now there's more focus on more coverage of test suite. It's in a decent sweet spot for certain types to get on board; new funding, new energy, room to change the guide rails and a milestone to work towards. reply OJFord 8 hours agorootparentWell as I understand it the goal now (or arguably 'still') outside Mozilla is just the engine, not the browser that uses it (which would have been Firefox)? So there still needs to be some browser project like the Ladybird mentioned, or a Brave, or whatever that decides to use Servo instead of WebKit, is my understanding. (A bit of a tangent since not a browser, but I assume Tauri will be keen to use it.) reply amelius 5 hours agoparentprevBy that time, large companies have concocted a method to make popular websites inaccessible without using the proper ad-tech API, just like DRM. reply tgv 11 hours agoparentprevI'm afraid it'll take very, very much time and effort for Ladybird to be as fast as Firefox. Web pages are terrible resource hogs. An inefficient browser will not be popular. reply dsissitka 10 hours agorootparent> One of the long term goals is to match the performance of other production JavaScript engines like JavaScriptCore and V8 when they run without a JIT compiler. https://youtu.be/n4YBMjlGWRc?t=77 I imagine a lot people won't accept that. Especially not as we're shipping more and more JavaScript. reply mort96 9 hours agorootparentPlus JavaScript execution speed is just one small part of web performance. reply KingOfCoders 11 hours agoparentprev30+ years Firefox user, eager for Ladybird to be usable on Windows. reply guilamu 11 hours agorootparentWon't happen anytime soon : \" Will Ladybird work on Windows? We don't have anyone actively working on Windows support, and there are considerable changes required to make it work well outside a Unix-like environment. We would like to do Windows eventually, but it's not a priority at the moment. \" reply RedShift1 11 hours agorootparentTargeting the smallest market first seems like a good recipe for a project to die early reply dhx 7 hours agorootparentThe Linux ecosystem is by far the largest market for enthusiasts volunteering their time to develop a new browser. It makes sense this community would only care about building a browser for their own needs (Linux support only). Additionally and unlike Mozilla, this volunteer community is also very unlikely to care about non-enthusiasts who may complain the browser doesn't support Encrypted Media Extensions (EME), Web Environment Integrity (WEI) or whatever anti-features ad-tech companies are trying to force onto mainstream users. This volunteer community is also unlikely to care too much about whether web sites containing 10MB of obfuscated JavaScript that was developed and tested solely against Chromium-based browsers works well. I think you'd find that the community would rather spend time working on projects such as yt-dlp to just re-implement front-ends for horribly broken websites, or would simply prefer to use non-broken alternative websites. Linux is also the easiest kernel to develop against too for reasons that include _much_ better sandboxing features being available, better debugging tools and availability of source code to learn from and debug with. Contrast to Windows with undocumented or poorly documented kernel and other system library APIs, lack of source code (particularly examples of APIs being used in other software), and having to do more work to opt-in to security features that are enabled by default on a Linux system. reply PhilipRoman 11 hours agorootparentprev\"smallest market\" is relative, the Linux market for suckless tools for example is likely 10x bigger than the Windows one. For privacy focused alternative browsers I'd say its somewhere close to 50/50. reply danielbln 9 hours agorootparentAlso, UNIX like system includes Mac, yeah? reply alexvoda 5 hours agorootparentAnd WSL. reply spookie 6 hours agorootparentprevYes reply sulandor 11 hours agorootparentprevit's not a commercial product reply prox 10 hours agorootparentWhich is not the point. If you want to have success you need to copy the Blender/Godot model. Year by year they make great versions for all platforms, and they do well. A new browser should support both and rally the people to work on it, again like the Blender Foundation has done. reply master-lincoln 5 hours agorootparentThey already had success by their definition: having a fun project and learning stuff while working on it. Not everyone is a greedy capitalist reply prox 4 hours agorootparentOf course thats a great metric, but to better exemplify, taking Blender again, with their pool of money (mostly donated by enthusiasts) they could really accelerate the development, hiring the best contributors on a case by case basis, which solidified the code base and continuity. What I am saying is there is a great middle ground, with a good team, where money and enthusiasm go hand in hand. reply KingOfCoders 11 hours agorootparentprevI'm sure this will change. Or I need PowerPoint and Affinity on Linux. reply asicsp 11 hours agorootparentHow about dual-booting or virtual machines? Wonder if WSL supports GUIs? reply moogly 8 hours agorootparent> Wonder if WSL supports GUIs? Yes, it's called WSLg. Uses Wayland, so many apps are a bit messed up. I think there's a way to install X11. Last time I tried it over a year ago it was a bit rough. reply KingOfCoders 10 hours agorootparentprevUsing WSL. Tried dual boot for some time, found it too cumbersome. reply timeon 10 hours agorootparentprevI have Affinity on Unix environment (macOS). You can probably use PowerPoint there as well. reply KingOfCoders 10 hours agorootparentAfter 20+ years I stopped owning Apple (Quadra, G4 Cube (favorite one), MacBooks, iMac Pro), no plan to go back to a golden cage. reply soraminazuki 7 hours agorootparentprevIf you're using Windows, you have far bigger problems to worry about than this. reply commandersaki 11 hours agorootparentprev30 years? Pretty sure the options were Netscape and Mosaic. reply KingOfCoders 11 hours agorootparentYes, Mosaic (VAX, Sun), Netscape (Dec Alpha), Firefox (Linux, OSX, Windows). reply OJFord 9 hours agorootparentFirefox was released 20 years ago, in 2004. 30 years ago in 1994 you could have been using Linux (v0.99/1.0/1.1), Apple System 7 (not OSX), and Windows (3.2) - but not with Firefox. You might have used Lynx or NCSA Mosaic, or by the end of 1994 Mosaic Netscape (later Netscape Navigator) or beta Opera. reply barrkel 9 hours agorootparentprevDraw a trend line through Windows releases and it doesn't look good. I can't see myself upgrading to 11 any time soon and I'm seriously considering just switching to Linux. It's all I've used on work laptops since 2013 and it's just fine. I still despise Gtk 3 or whatever it is that is killing menus and sticking controls into the title bar etc. but it's fine. You don't need to use Gtk apps much these days anyway. reply bityard 6 hours agorootparentKDE is nice this time of year reply CalRobert 11 hours agoparentprevSeems likely that nothing will work on ladybird or Firefox due to attestation (WEI) reply neverrroot 10 hours agorootparentAre you being sarcastic? WEI is pretty dead reply CalRobert 6 hours agorootparentNo, I fully expect it to come back and be implemented. reply kasabali 7 hours agorootparentprevOnly for a few years until they come up with same shit but with a different name. Take a look at the presumably \"dead\" Palladium initiative and consider how much of it we're actually been subjected to now in our computing. reply hooverd 1 hour agorootparentThe people pushing it weren't ostracized nearly hard enough. reply hughesjj 2 hours agorootparentprevOr heck, SOPA reply moogly 8 hours agoparentprevI'm not seeing anything about plugins/extensions on that page. As we all know, a browser is academic and unusable until it has extension support. reply Phelinofist 11 hours agoparentprevIs there a doc describing the goals of Ladybird? The page just mentions speed & security, but is privacy also one of them? reply PufPufPuf 10 hours agorootparentFrom what they've shown so far, the main goal is \"no politics\". This prevented a PR that changed some pronouns in the docs talking about a generic user from \"he\" to \"they\", because that's politics. I wouldn't be surprised if user privacy is also politics. reply shaoonb 10 hours agorootparentUsing \"he\" as default is just as much of a political stance as using \"they\" as default. The fact that whoever rejected this PR thinks that pronouns are \"political\" gives me a pretty good guess as to their overall political leanings. reply OJFord 8 hours agorootparentTo me that depends on the context it's used in - if it's like: > When a user visits a web page, he expects [...] then yes, I'd even just say that's wrong, no opinion or politics about it. However if it's: > So once Alice has published the website, and Bob visits it in his browser, he expects [...] and the PR is suggesting that actually we don't know how fictional Bob identifies... Then personally I just think that's tedious, the pronouns are helpful to disambiguate Alice & Bob in shorthand anyway, and that is bringing 'political' (ish? Societal?) views into it. reply ajdude 3 hours agorootparentprevDo you happen to have a link to that PR? This was the closest one I could find https://github.com/LadybirdBrowser/ladybird/pull/366 reply BaudouinVH 11 hours agoparentprevThe following Mastodon toot is about https://servo.org that could become another alternative. Quote : \"Servo is faaaar from ready for general use yet, but it's picking up development speed. Definitely an option to keep an eye on for the future. \" reply devwastaken 4 hours agoparentprevYou don't need to rely on alpha vaporware. You can use a Firefox or chromium fork that patches out the changes and still have a quality battle tested browser. reply logicprog 2 hours agorootparentOr alternative browsers based on WebKit reply lynndotpy 5 hours agoparentprevIt's hard to put faith in a project that's partially AI-generated and doesn't disclose it. That picture of the laptop is the most blatant part. Is that just one contributor phoning it in for the landing page, or does that culture run deeper through the Ladybird project? reply logicprog 2 hours agorootparentI've followed Kling's videos for years, both the ones working on serenity OS and the ones working on Ladybird, and followed the general arc of those projects and even contributed once a few year' back, and they actually seem to take the quality of their work very seriously and enjoy producing good high-quality code. I think it's just that none of them had experience with website design and the one guy who stepped up to do it happens to be one of those people that thinks AI generated stuff is fine. reply 256_ 9 hours agoparentprevI can't help but see these \"let's create a Web browser from scratch\" projects as massive wastes of time. You can't build a sane implementation of an insane standard. Modern websites and the standards they rely on are overcomplicated. The problem lies with the standards, and the way they are used. The browser can't control that. I could never work on such a project without quickly losing motivation. Also, that link says \"The main community hub is our Discord server.\" That doesn't inspire confidence in anything. reply Klonoar 15 minutes agorootparentPlenty of notable open source projects are on Discord. They have no issues inspiring confidence. (E.g, Dolphin emulator, who seem to have largely moved away from IRC) reply ghostwords 27 minutes agoprevDismantle systems of online surveillance. Limit online advertising to contextual ads, and to the same attribution methods advertisers have access to with traditional (TV, print and billboard) ads. Is your browser a user agent or an advertiser agent? reply Kwpolska 11 hours agoprevIt is an attempt to replace more invasive tracking techniques. The AdTech industry is unlikely to give up on knowing which ads were \"successful\". A privacy-friendly solution developed by Firefox is miles better than something invented by Google, the AdTech company masquerading as a browser vendor. reply JoshTriplett 10 hours agoparentGiving more tracking misfeatures to advertisers does not cause them to give up the old tracking mechanisms, just add more tools to their toolbox. The only way to get them to stop using the old mechanisms is to block those mechanisms. If you can't block those mechanisms then you're just giving more ways for advertisers to track people, not taking away the old ones. And if you can block those mechanisms, then just block them, you're done, stop there. reply mindslight 7 hours agorootparentPersonally I don't see why we should treat browser/web vulnerabilities that are being abused for tracking any different than every other security vulnerability... apart from the discussion being warped by Doubleclick pumping out new browser vulnerabilities, baking them into web standards, and marketing them as \"features\" to be rapidly adopted. reply buildfocus 10 hours agorootparentprevThis is not a tracking mechanism - it's quite convincingly private to _all_ participants in the protocol (only you, as the browser client, have the full data). Websites specifically receive only aggregate data from their complete user base, not any individual info. Whether it's worthwhile and/or will help reduce industry tracking practices is a good & separate question, but it's not reasonable to describe this as anything akin to a attack on privacy. reply JoshTriplett 10 hours agorootparentUser data is valuable in aggregate as well, and keeping it from advertisers in every form is a component of privacy. It's an attack surface with no redeeming value for end users. There is no value gained by compromising with advertisers. reply arp242 9 hours agorootparentYou can't just claim it's an \"attack surface\" without anything to back that up. That's just engaging in boring black/white thinking. And of course the data is useful. That's why this is being done. That in itself is not a argument against it. reply JoshTriplett 9 hours agorootparentI'm quite happy to continue saying \"advertising is bad, tracking is bad, anything that serves no purpose other than to help them is bad for users\". Calling that \"black and white thinking\" is not an argument that advertising is good, or tracking is good, or that this API proposal is good. What is the reason this is good for users? It's not \"this is better than other tracking\", because this does nothing to take away other tracking so it's not an either-or. Other tracking won't go away until it's blocked. There is no requirement to provide a replacement. \"We've should eliminate toxic waste being dumped in the water!\" \"What do you propose to replace the toxic wastewith? Why don't we provide a less toxic waste? Maybe if we offer the option of dumping a less toxic waste, that'll incentivize factories to dump that instead of the more toxic waste?\" reply arp242 8 hours agorootparent> Calling that \"black and white thinking\" is not an argument that advertising is good I never said this proposal is any good. I don't know if is as I just looked at the general overview, and that's not really enough to make a judgement one way or the other. As a general point I do think it's a problem with solving. What I am saying your case for \"it's bad\" is entirely without substance and seems to be based on axiomatic black/white thinking. reply Kbelicius 8 hours agorootparentprev> I'm quite happy to continue saying \"advertising is bad, tracking is bad, anything that serves no purpose other than to help them is bad for users\". Calling that \"black and white thinking\" is not an argument that advertising is good, or tracking is good, or that this API proposal is good. GP was very clear in what he considered \"black and white thinking\" and you very clearly avoided addressing what GP wrote. > What is the reason this is good for users? Did GP said it was good, no. So why are you even asking this question? reply PoignardAzur 7 hours agorootparentprev> There is no value gained by compromising with advertisers. I mean, there's the part where advertisers pay for a huge chunk of the web? Maybe you're ideologically opposed to that and think websites should only have crowdsourcing-type revenue sources, but in the meantime website owners need to pay the bills. reply thoroughburro 6 hours agorootparent“Advertisers are responsible for the 90% of the web which is inhuman, outraged, clickbait garbage” isn’t the endorsement you think. reply wbl 10 hours agorootparentprevThird party cookies cannot be turned off without this replacement and for other things due to competition law and web ecosystem issues. Ad supported content is worth something and eliminating that business model overnight would be bad as a bunch of things would be less accessible. reply JoshTriplett 10 hours agorootparent> Third party cookies cannot be turned off without this replacement and for other things due to competition law and web ecosystem issues. \"competition law\" is a problem for Chrome; as a browser run by a massive advertising company, interfering with other advertising companies raises antitrust concerns. It is not a problem Firefox needs to care about. (Also, it's not a problem that a browser with a tiny fraction of market share needs to worry about.) \"web ecosystem issues\" is a fascinating euphemism. Let's cause more \"issues\" for advertisers. > Ad supported content is worth something and eliminating that business model overnight would be bad as a bunch of things would be less accessible. It's not going to go away overnight; it will take a long lingering time to die, and that time gets longer every time someone hesitates to kill it. In the meantime, as it becomes less effective, other models will become more effective. reply Kwpolska 9 hours agorootparentIf Firefox delivers a feature the AdTech industry would be OK with, then Chrome can adopt it and kill third-party cookies. Firefox already can block such cookies, but some legitimate sites may break (but developers don't care about Firefox). If Chrome blocks them by default, those sites will have to adjust. reply JoshTriplett 9 hours agorootparentChrome is already delivering a (different) feature that they propose as a replacement for third-party cookies. That's not a reason for Firefox to adopt that API. reply KingOfCoders 11 hours agoparentprevIt is an attempt by Firefox to earn money from the AdTech industry. I use an adblocker, I have no need for \"Privacy preserving ads\". reply frabcus 11 hours agorootparentHow will it help them earn money? Are Google threatening them with withdrawal of search revenue unless they add the feature? Something else? reply Yeri 11 hours agorootparentFirefox is in the ads business now: https://blog.mozilla.org/en/mozilla/mozilla-anonym-raising-t... reply frabcus 6 hours agorootparentInteresting thanks! Anonyms technology is presumably what the original article was about? Any idea what a transaction will look like. Who will pay them for this and how? I guess ad networks would pay so they can still attribute and do retain their customers better? I wonder what the fee is per reply rwmj 11 hours agorootparentprevIf only they'd saved the Google money into a trust rather than spaffing it on senior management. reply KingOfCoders 10 hours agorootparent>$6.000.000.000 reply whamlastxmas 8 hours agorootparentprevFirefox is controlled opposition. Firefox will hire leadership that is most capable of maximizing revenue from Google, and Google sends their money to Firefox based on now nicely that CEO cooperates. Therefore it’s probably not a direct threat but a “do we understand each other?” situation reply MenhirMike 11 hours agoparentprev> A privacy-friendly solution developed by Firefox It's actually developed together with Meta/Facebook: https://blog.mozilla.org/en/mozilla/privacy-preserving-attri... reply justinclift 11 hours agoparentprevSo you're pretty much saying Mozilla have given up fighting and joined the dark side yeah? reply barbariangrunge 11 hours agoparentprevCan anyone comment on how effectively privacy is preserved with this approach? reply throwawa14223 11 minutes agorootparentIf I understand it is not privacy preserving with regard to you and the aggregator but it is preserving with regard to you and the ad agency. reply bn-l 11 hours agoprev> But, and I swear I'm not even joking a little bit here, Mozilla goes on to say that advertisers might be happier if Firefox itself just tracked you directly and sent activity reports back to them. Could this advertiser be Google who also pays their salary? reply amelius 6 hours agoparentI don't get why the EU isn't paying their salary, frankly. reply bravetraveler 2 hours agoprevI don't negotiate with advertisers, the most privacy-respecting option is to block/starve them. reply frabcus 11 hours agoprevI'd love a detailed description of the politics and economics of adding this. Why are Firefox doing it, how does it relate to similar features in other browsers? If it leads to direct revenue, how? If there are relationship reasons for doing it, what are the forces at play? Just the actual background, not opinions on whether it is good or bad before understanding that background. reply justinclift 11 hours agoparentBearing in mind that Mozilla literally bought an advertising company recently too. reply TekMol 11 hours agoprevThe gist of it seems to be that Mozilla and ISRG now proxy the tracking data and give aggregated reports to advertisers. And that they handle the data in a way so that neither Mozilla nor ISRG alone can access the unaggregated data. From reading around their documents ... https://blog.mozilla.org/en/mozilla/privacy-preserving-attribution-for-advertising/ https://github.com/mozilla/explainers/tree/main/ppa-experiment https://datatracker.ietf.org/doc/draft-ietf-ppm-dap/ ... it sounds a bit like they route the data through something like an onion network. So nodes in the network do not know what data they are routing and the advertisers get aggregated data without knowing who the users were: ... our DAP service, which is a Multiparty Compute (MPC) system based on Prio ... And the \"Multiparty\" seems to be these two: Our DAP deployment is jointly run by Mozilla and ISRG. Privacy is lost if the two organizations collude I think this is actually an interesting approach. I don't know whether two parties are enough and whether the specific algorithm used is good. But if it works as advertised (haha), it would result in measuring ad effectiveness without compromising user privacy. Which might be a good thing. reply rawling 11 hours agoparentIs it not the \"ISRG\" referenced in your second link? reply TekMol 11 hours agorootparentYes, updated the text. (In the first version, I asked who is involved in proxying the data) reply KingOfCoders 11 hours agoprevThey think they can make money by pushing this and owning (parts of) the infrastructure to transfer the reports of PPA. Not long and we'll see the same game as Chrome + ad blockers. reply tgv 11 hours agoprevThat is very disappointing, indeed. \"[A]nnounced with very little fanfare\" is an understatement. It is mentioned as an afterthought on the page with new features, without the screen-shots that are shown for the other configurable features, and couched in soothing language: they call it \"privacy preserving\". It's almost “But Mr Dent, the plans have been available in the local planning office for the last nine months.” (https://www.planetclaire.tv/quotes/hitchhikers/the-hitchhike...) You do this when you want to hide something. This does not inspire trust. Edit: instead, if they believe this to be a way out of the ongoing data thievery that is the ad industry, they could have announced this openly and boldly. \"You want privacy, they want ads, here's a middle-ground.\" It could have been scrutinized by members of the community beforehand. Now, it'll just tarnish the reputation of Firefox and go down with it. reply reify 11 hours agoprevhttps://gitlab.torproject.org/tpo/applications/tor-browser/-... reply entuno 11 hours agoprevDisappointing behaviour by Mozilla, but not hugely surprising. It's also rather suspicious that the setting to disable this seems to be somewhat hidden. If I go to settings and search for \"advertising\" then I get: > Sorry! There are no results in Settings for \"advertising\". But if I browse to it manually in then the setting is there, in the section named \"Website Advertising Preferences\". And the search definitely includes section titles, because if I search for \"collection\" then it shows that section of the Privacy & Security settings, with a highlight on the text on the title. reply logicprog 1 hour agoprevOnce again, I have to stick my ore in here and chill for LibreWolf :D I think it might really be worth checking out for people like me that prefer the user interface and functionality of Firefox over chromium and don't want to contribute to the blink engine monopoly, but also often doesn't approve of what Mozilla is doing upstream and wants someone to shield them from it. It's my daily drive for browser for basically everything, and 99% of the time, even on stuff that you would think wouldn't work, it works just fine. I keep chromium in my back pocket just in case, but I've only had to pull chromium out like twice in the past year, once for something that required the USB-HID protocol and once for iCloud. And yes, for those of you on distributions, you might say that your distribution maintainers will just patch out or customize out, and the nefarious changes that Mozilla makes upstream. But the thing you have to remember is that distribution maintainers are handling, by a whole lot of other things, tens of thousands of other packages, and an entire operating system, and its upkeep, go through them. So they will often just not patch out or patch things out inconsistently or not really pay attention. I think it's much better to rely on a project whose whole purpose and explicit mission is making Mozilla more privacy-friendly and secure and who have a dedicated community of a few developers consistently working on it. Especially since distribution maintainers don't really make any specific mission statement promises with regards to specific packages, but something like LW does. It does a lot more than just this one thing. It's essentially equivalent to having a arkenfox config maintained for you and always applied to your browser and updated in lockstep with your browser, as well as a set of patches that they maintain to remove things like Pocket. reply singularity2001 6 hours agoprevTangential: How do some sites manage to open other sites in new tabs without confirmation in 2024? Popups need explicit confirmation since 200x?? reply matt3210 11 hours agoprevConsent is opt in, yes means yes reply from-nibly 7 hours agoprevYou are the product. Firefox is not the exception. Unless they wholly operate on donations from users, then the users are the product. reply thomassmith65 10 hours agoprevI assume Waterfox (http://waterfox.net) won't adopt this garbage feature. reply jampekka 9 hours agoprevThis is how users are turned into products. reply worksonmine 10 hours agoprevConfirmed the new setting was on for me. Mozilla is really starting to test my patience. Do I have to check the settings every time I update my browser now? reply tgvaughan 9 hours agoprevDoes anybody know whether this also applies to firefox android? reply jampekka 9 hours agoparentAt least to Beta. Not sure about the release version because it doesn't have about:flags. Probably it's infested with it too, and it's unclear how to disable it. I fucking hate this. reply throwawa14223 3 hours agoprevHas anyone here been the victim of this and can look up what IP addresses and domain names need to be added to blocklists? reply jankovicsandras 9 hours agoprevIs this illegal in GDPR countries (EU + Norway + ...) or am I reading GDPR wrong? \"If informed consent is used as the lawful basis for processing, consent must have been explicit for data collected and each purpose data is used for\" https://en.wikipedia.org/wiki/Gdpr reply Signez 9 hours agoparentThere is no personal data collected as it's one of the key feature of that process, so in my humble opinion it's out of the scope of GDPR. reply jankovicsandras 5 hours agorootparent\"Controller and processor Data controllers must clearly disclose any data collection, declare the lawful basis and purpose for data processing, and state how long data is being retained and if it is being shared with any third parties or outside of the EEA.\" I don't see the word \"personal\" in this sentence, only \"any data collection\". It's clearly not in the users' interest/benefit to activate this data collection, and not required for the normal functioning of the browser/websites. So activating this silently is minimum _very unethical_ and probably illegal, but I'm not a lawyer. reply lkdfjlkdfjlg 11 hours agoprevWasn't the a HN post a couple of weeks ago claiming that mozilla was now an advertising company? It seems almost that mozilla's leadership is intentionally trying to ruin it. reply KingOfCoders 11 hours agoparentSince decades, marketshare is down down down, >$6.000.000.000 squandered. reply hulitu 11 hours agorootparentThey did it themselves. Why did they have to become another Chome ? reply bogdan 9 hours agorootparentBecause it was repeatedly the easiest path to making more money. reply ranger_danger 11 hours agoprevNot taking sides here, but does anyone actually think that people will use strictly opt-in features at all? They went to the trouble to develop it so if you were them, why should it be so hidden as to have barely any users? I feel the same way about debugging telemetry... it's so valuable for developers and yet people want to see you hang if it's not (at best) a manual opt-in, but they don't care that it won't be used by anyone in that case. reply fallingsquirrel 11 hours agoparentCounterpoint: Syncthing's telemetry is strictly opt-in, and > 100,000 people have opted in. It's possible to collect useful data in a way that also respects your users. Many projects do so. Firefox just chooses not to. https://data.syncthing.net/ reply entuno 11 hours agoparentprevThere are plenty of opt-in features that people do use. Hell, the entire extension ecosystem is based around people choosing to opt in to various extra features. So if this is something that is so undesirable that no one would opt it to it, then maybe it's not a feature that should be included the the browser at all - especially when Mozilla's slogan is \"Internet for people, not profit\". reply chefandy 11 hours agorootparentI wonder if they snagged themselves one of those executives that justifies ignoring privacy by insisting people really do want targeted ads more than privacy... so you really really don't even need to inform them about it or give them a choice. reply pacifika 11 hours agoparentprevWhy should users opt in to a feature if they perceive it has no value to them? So what developers should be asking is how they can make it directly valuable to the user to enable the feature. User led not business led. Reliability ratings, notifying of fixed reported crashes, improvements in performance telemetry etc reply ranger_danger 11 hours agorootparent> Why should users opt in to a feature if they perceive it has no value to them There will be users that perceive any feature of the application to have no value. I think developers should not only consider the opinions and wishes of the user but also what's best for the application itself. reply chefandy 11 hours agoparentprevI enable debugging telemetry when I encounter bugs, but I'm not going to constantly send some company a shitload of my usage metadata just in case it's useful at some point. reply KingOfCoders 11 hours agoparentprev\"Not taking sides here\" but taking sides here. reply Hamuko 11 hours agoparentprevFirefox is an opt-in feature on all of my devices. reply matheusmoreira 11 hours agoparentprevWhether anyone will use it or not doesn't matter. They do things like this, they simply show everyone they can't be trusted. reply alserio 11 hours agoparentprevyou can just ask then reply Digit-Al 9 hours agoprev [–] I really don't see anything wrong with this, personally. It's a limited test with a small number of websites involved. Only aggregate data is received, so all they'll know is something like \"x number of people saw this ad and y number of people clicked on it\"; no other information about those users. Seems very innocuous to me. As to it being opt out instead of opt in: consider the fact that only a small number of websites are involved in the experiment; if it was opt in then it seems quite likely that there could potentially be no intersection between users who opt in and users who visit those specific websites, rendering the entire experiment pointless. [Edit: fixed spelling error] reply throwawa14223 6 minutes agoparent [–] Am I wrong that it is only aggregate to the advertisers and not aggregate to the ISRG? I feel pretty negative about the ISRG having my data to provide aggregate data from. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "Firefox 128 introduces an opt-out 'Privacy-Preserving Attribution' setting, aiming to address privacy concerns by reducing tracking incentives.",
      "Critics argue this move may compromise user trust, highlighting issues like manipulation, attention capture, and Mozilla's recent acquisition of an ad company.",
      "Alternatives such as LibreWolf and Ladybird are recommended for users seeking more privacy-focused browsers."
    ],
    "points": 170,
    "commentCount": 152,
    "retryCount": 0,
    "time": 1720853771
  },
  {
    "id": 40948971,
    "title": "Goldman Sachs: AI Is overhyped, expensive, and unreliable",
    "originLink": "https://www.404media.co/goldman-sachs-ai-is-overhyped-wildly-expensive-and-unreliable/",
    "originBody": "Investment giant Goldman Sachs published a research paper about the economic viability of generative AI which notes that there is “little to show for” the huge amount of spending on generative AI infrastructure and questions “whether this large spend will ever pay off in terms of AI benefits and returns.” The paper, called “Gen AI: too much spend, too little benefit?” is based on a series of interviews with Goldman Sachs economists and researchers, MIT professor Daron Acemoglu, and infrastructure experts. The paper ultimately questions whether generative AI will ever become the transformative technology that Silicon Valley and large portions of the stock market are currently betting on, but says investors may continue to get rich anyway. “Despite these concerns and constraints, we still see room for the AI theme to run, either because AI starts to deliver on its promise, or because bubbles take a long time to burst,” the paper notes. Goldman Sachs researchers also say that AI optimism is driving large growth in stocks like Nvidia and other S&P 500 companies (the largest companies in the stock market), but say that the stock price gains we’ve seen are based on the assumption that generative AI is going to lead to higher productivity (which necessarily means automation, layoffs, lower labor costs, and higher efficiency). These stock gains are already baked in, Goldman Sachs argues in the paper: “Although the productivity pick-up that AI promises could benefit equities via higher profit growth, we find that stocks often anticipate higher productivity growth before it materializes, raising the risk of overpaying. And using our new long-term return forecasting framework, we find that a very favorable AI scenario may be required for the S&P 500 to deliver above-average returns in the coming decade.” (Ed Zitron also has a thorough writeup of the Goldman Sachs report over at Where's Your Ed At.) It adds that “outside of the most bullish AI scenario that includes a material improvement to the structural growth/inflation mix and peak US corporate profitability, we forecast that S&P 500 returns would be below their post-1950 average. AI’s impact on corporate profitability will matter critically.” \"Despite its expensive price tag, the technology is nowhere near where it needs to be in order to be useful for even such basic tasks\" What this means in plain English is that one of the largest financial institutions in the world is seeing what people who are paying attention are seeing with their eyes: Companies are acting like generative AI is going to change the world and are acting as such, while the reality is that this is a technology that is currently deeply unreliable and may not change much of anything at all. Meanwhile, their stock prices are skyrocketing based on all of this hype and investment, which may not ultimately change much of anything at all. Acemoglu, the MIT professor, told Goldman that the industry is banking on the idea that largely scaling the amount of AI training data—which may not actually be possible given the massive amount of training data already ingested—is going to solve some of generative AI’s growing pains and problems. But there is no evidence that this will actually be the case: “What does a doubling of data really mean, and what can it achieve? Including twice as much data from Reddit into the next version of GPT may improve its ability to predict the next word when engaging in an informal conversation, but it won't necessarily improve a customer service representative’s ability to help a customer troubleshoot problems with their video service,” he said. “The quality of the data also matters, and it’s not clear where more high-quality data will come from and whether it will be easily and cheaply available to AI models.” He also posits that large language models themselves “may have limitations” and that the current architecture of today’s AI products may not get measurably better. Jim Covello, who is Goldman Sachs’ head of global equity research, meanwhile, said that he is skeptical about both the cost of generative AI and its “ultimate transformative potential.” “AI technology is exceptionally expensive, and to justify those costs, the technology must be able to solve complex problems, which it isn’t designed to do,” he said. “People generally substantially overestimate what the technology is capable of today. In our experience, even basic summarization tasks often yield illegible and nonsensical results. This is not a matter of just some tweaks being required here and there; despite its expensive price tag, the technology is nowhere near where it needs to be in order to be useful for even such basic tasks.” He added that Goldman Sachs has tested AI to “update historical data in our company models more quickly than doing so manually, but at six times the cost.” Covello then likens the “AI arms race” to “virtual reality, the metaverse, and blockchain,” which are “examples of technologies that saw substantial spend but have few—if any—real world applications today.” The Goldman Sachs report comes on the heels of a piece by David Cahn, partner at the venture capital firm Sequoia Capital, which is one of the largest investors in generative AI startups, titled “AI’s $600 Billion Question,” which attempts to analyze how much revenue the AI industry as a whole needs to make in order to simply pay for the processing power and infrastructure costs being spent on AI right now. To break even on what they’re spending on AI compute infrastructure, companies need to vastly scale their revenue, which Sequoia argues is not currently happening anywhere near the scale these companies need to break even. OpenAI’s annualized revenue has doubled from $1.6 billion in late 2023 to $3.4 billion, but Sequoia’s Cahn asks in his piece: “Outside of ChatGPT, how many AI products are consumers really using today? Consider how much value you get from Netflix for $15.49/month or Spotify for $11.99. Long term, AI companies will need to deliver significant value for consumers to continue opening their wallets.” This is all to say that journalists, artists, workers, and even people who use generative AI are not the only ones who are skeptical about the transformative potential of it. The very financial institutions that have funded and invested in the AI frenzy, and are responsible for billions of dollars in investment decisions are starting to wonder what this is all for. About the author Jason is a cofounder of 404 Media. He was previously the editor-in-chief of Motherboard. He loves the Freedom of Information Act and surfing. More from Jason Koebler",
    "commentLink": "https://news.ycombinator.com/item?id=40948971",
    "commentBody": "Goldman Sachs: AI Is overhyped, expensive, and unreliable (404media.co)129 points by mrzool 22 hours agohidepastfavorite135 comments wkat4242 19 hours agoWow. I did not imagine I would ever agree with Goldman Sachs on anything in my life. Now, I do think it has its uses, but it's once again way overhyped like all the hypes that came before it. As always there is a certain use to it but it's way overblown. reply robotnikman 19 hours agoparentI think it's great in specific use cases, like the meeting discussion overview in teams. The problems come when you think you can build one AI that can do anything and everything, or tackle problems with a wide scope, or start using the AI as an infallible source of information. It seems like a lot of people were expecting the latter to be true. reply kibwen 19 hours agorootparent> like the meeting discussion overview in teams Amusingly, Microsoft had an autosummarize feature in Word as far back as, like, 1997. The fact that this appears to be the rare feature that actually got removed from Word could be an indication that this use case might not be as compelling to users as one might think. reply wkat4242 16 hours agorootparentI think it was more because it was no good at that time and couldn't be relied upon. For all their flaws, summarisation is something LLMs do pretty well. I think the chat feature is also really helpful \"tell me more about this aspect\" kind of thing. That really helps make the summary more tailored to your needs. reply HenryBemis 19 hours agorootparentprevWait until the moment that they can monetize it/save a bunch of money. Then see them dropping 5000-10000 employees globally because 'shareholders are pressing us to...' Some banks that develop their own applications will start using it heavily when they will realize that their in-house trading platform can execute orders 0.0001 seconds faster (or something insanely small). I strongly believe that the only thing that will hold AI/LLMs back is regulation, and nothing else. reply brookst 19 hours agoparentprevIt’s overhyped like the internet was in 1995. reply Jensson 9 hours agorootparentInternet in 1995 had already solved all the problems required to revolutionize the world with cheap fiber optics fiber being laid everywhere, the world just had to catch up to it. The same isn't true for generative AI today, in order for it to revolutionize the world similarly to how the internet did it still need some more unknown pieces, which you are expecting to happen but it might not. The internet however was already completely solved when it was hyped, very different. reply brookst 4 hours agorootparentI mean I was there, and it feels similar. The internet was “solved” in the way that transformers “solve” AI. And there were so many unknown pieces for internet business and tech. The difference between internet and AI is one is mature and the other is nascent. That’s it. reply kibwen 19 hours agorootparentprevIt's overhyped like the time everyone told us cryptocurrencies in 2015 were like the internet in 1995. reply brookst 4 hours agorootparentCryptocurrency is a use case, AI is a technology. It’s odd to compare them. AI is more comparable to GUI or internet or cell service than it is to cryptocurrency. Blockchain, if you must, but I don’t think anyone could saw AI has to more relevance than blockchain. reply GaggiX 19 hours agorootparentprevHave they? AI compared to cryptocurrency has already been deployed and battle tested on billions of users, just think of BERT used on Google queries, all the models used for moderation and recommendation on social media (probably the whole reason why TikTok is so popular), translation, captioning etc... It really seems quite different. reply Jensson 9 hours agorootparentHow is that different from bitcoin being used as currency in many places all over the world? You used to be able to go and buy a pizza using a bitcoin transaction, then that regressed but at the time it looked like bitcoin might actually be the future. Turns out there were some problems that crypto currencies hadn't solved yet, and those still aren't solved. At the time people expected the tech to advance to solve all those things, but they failed to do so. Same thing applies to current day AI, in order to revolutionize the world like the hype expects they need more things that are completely unknown whether we are actually able to solve or not. Until those things are solved generative AI is just new search, translation and some funny pictures. Useful but not that world changing. Internet 1995 you could already connect and talk to people from all over the world in social networks, do banking, shopping etc, all the technical problems were already solved, people just hadn't caught up to it yet. Internet just needed social changes to happen, generative AI still requires technological breakthroughs. reply AbstractH24 4 hours agorootparentGenerative AI will take my spreadsheet and reorganize it based on a prompt I give it in natural language I’ve still yet to discover a use for crypto other than waiting for it to increase in value and sell to the next person reply GaggiX 9 hours agorootparentprevBitcoin has never reached the proliferation of AI, even though some have used it to pay for pizzas. It's 2024, and with AI you can already do: semantic search, moderation, translation, captioning, TTS, STT, context-aware grammar checker, LLM, audio/image classifier, smart editing tools etc... So yeah it's quite different compare to Bitcoin. reply Jensson 8 hours agorootparentAnd none of those changes the everyday life of most people. Bitcoin revolutionizing banking would, as would generative AI if some new breakthrough was made, but currently the biggest thing it does for most people is that it lets them cheat on homework etc. reply GaggiX 8 hours agorootparentThis is incredibly delusional ahaha Just look at how popular TikTok is (it wouldn't be without its powerful recommendation model), or the fact that the majority of the world's population has to rely on translation models and subtitles to translate the vast amount of English text online. Bitcoin is never going to \"revolutionize\" banking, it's really just a downgrade, no privacy, no fraud protection. reply player1234 7 hours agorootparentFound the CCP plant. This example makes AI a net negative. reply mewpmewp2 1 hour agorootparentTiktok may be a net negative, but it's clear how it has changed the World for youth who are addicted to AI suggestion algorithms. reply ls612 19 hours agorootparentprevExactly. I’ve thought since ChatGPT first took the world by storm that the directional promises of AI were completely correct but the hardware isn’t at all there yet. The dot com bubble crashed yet the Internet is more important than ever today. reply senectus1 16 hours agoparentprevI like to think that its nice to see that Goldman Sachs agree with me for a change. I came to this conclusion a while ago :-P reply hypeatei 19 hours agoparentprevAnd we're adding browser APIs for it! What could go wrong? reply meiraleal 19 hours agorootparentAre we? reply hypeatei 18 hours agorootparentYes, see: https://developer.chrome.com/docs/ai/built-in Also: https://news.ycombinator.com/item?id=40834600 reply mandmandam 19 hours agoparentprevHere's the thing though: You don't agree. Not really. Because Goldman Sachs have been using AI for well over a decade. They're saying one (popular right now) thing, and very much doing another (quietly). Their employees are capitalizing on this latest AI boom [1] to make their efficiency go way up, radically changing their processes... And a lot of people who were slow to grasp what AI can do for their workflow are being made redundant [2]. I can only imagine what those people laid off are thinking of this statement. 1 - https://www.wsj.com/articles/goldman-sachs-deploys-its-first... 2 - https://duckduckgo.com/?q=goldman+sachs+layoffs reply anal_reactor 11 hours agoparentprevI strongly believe that AI is the next big thing, but there's no point in discussing things, because there's no empirical evidence either way. There was no evidence that internet would take off, and there was no evidence that crypto wouldn't. reply OutOfHere 18 hours agoparentprevIt's 100% stock market manipulation; nothing more and nothing less. reply gnabgib 22 hours agoprevDiscussions (88 points, 12 days ago, 157 comments) https://news.ycombinator.com/item?id=40837081 (89 points, 1 week ago, 47 comments) https://news.ycombinator.com/item?id=40885632 reply tedsanders 20 hours agoprevGoldman is correct that AI is expensive and unreliable. But it's only overhyped if AI stays that way. The hype isn't coming from where AI is today, but where it will be in 2030 and 2040. What excites the true believers is the slope, not the intercept. reply xk_id 20 hours agoparent“Over an arbitrarily long timeline, assuming stable geopolitical conditions, technology will continue to improve, at an arbitrary rate”. What a groundbreaking insight, no wonder people are excited. reply tim333 8 hours agorootparentThat's not the hype you are looking for. It's more that by 2040 AI/robots will be considerably smarter than humans and we can kick back while they build us palaces and yachts. Not that that will definitely happen but it's not a business as usual scenario. reply brigadier132 19 hours agorootparentprevDid you even look at the Wright brothers plane? Why are people excited about it? It's an expensive deathtrap! reply hervature 17 hours agorootparentThe Wright flight was in December 1903 and planes were considered for a ban as weapons at the international level in 1911 [1]. Their capabilities and potential were obvious to everyone before the flight occurred. Imagining traveling through the air like a bird was obviously going to be a game changer to even a simpleton. The \"Attention Is All You Need\" has now been out for over 7 years and so a directly comparable timeline. What do we have? Better autocomplete? Worse search results? An overall efficiency increase of 5%? For people knowledgeable about the technical details, a path to AGI as a bigger LLM is about as probable to a path to AGI as using a big random forest. Arguing by example is stupid. Plenty of other examples have been proposed elsewhere in this thread. My only contribution is that I think LLMs are not going to yield anything more than incremental efficiency gains. If the only result after 7 years is that NVidia became a new $1T company and no one else then I think it should be becoming more obvious that this is a gold rush situation and not an iPhone situation. [1] - https://en.wikipedia.org/wiki/Aviation_in_World_War_I#cite_n... reply karmakurtisaani 11 hours agorootparentTangentially related, I've been rather disappointed with the advances coming from software since communication was solved at some point in the early 2000s. Software mainly seems to enable new business models, of which some tangentially improve our quality of life, others are actively harmful. Contrasting this to advances in technologies like solar panels or electric cars almost makes me wish I had chosen a different career path. reply ls612 15 hours agorootparentprevWhen the AI gets good enough to fly FPV drones with no datalink (which is sooner than you think) that will be the tipping point you are looking for. reply brigadier132 14 hours agorootparentprev> The Wright flight was in December 1903 and planes were considered for a ban as weapons at the international level in What a coincidence that we have people trying to ban AI at a domestic and international level > What do we have? Better autocomplete? Worse search results? It's ridiculous to minimize LLMs as better autocomplete. Also I get it, for most people remembering how things were more than a couple of years ago is hard. Can you even remember when you couldn't talk to computers? reply xk_id 18 hours agorootparentprevSo you are basing your outlook on extrapolating from a completely different engineering domain and a completely different socio-historical context? reply brigadier132 15 hours agorootparentyes and im right reply elevatedastalt 20 hours agorootparentprevThe slope has changed between 10 years and ago. You are free to disagree, but don't pretend like you don't understand what the AI crowd is saying. reply xk_id 20 hours agorootparentNever in the history of man has the slope of progress been linear. The AI crowd is entirely predicated on a platitude. reply Davidzheng 19 hours agorootparentAI right now is like covid in March or climate change in the 1980s. It's no longer a real intellectual debate on whether it will arrive (that is basically over by 2022) now it's an emotional battle where people are incapable of accepting that such a possibility can exist vs actually calculating reply Jensson 9 hours agorootparent> AI right now is like covid in March March 2021 yeah, well known all over the world, everyone talks about it but not much has really changed in a while. Some fear COVID might mutate into something deadlier, and some think generative AI will turn into something completely different. The quick pace was GPT-2 and GPT-3, those happened 2019 and 2020, today it is a crawl in comparison, GPT-3 was like covid in march-2020, at the time most were barely aware of it but today it is mainstream. The revolution you are talking about has already happened, people all over the world interact with their computer using natural language, so your prediction here is a bit late. reply AbstractH24 4 hours agorootparentAgreed, but might push that back to January 2020 By the second week of March I was well aware of COVID. By the third I was locked in my apartment. In January someone at the airport in Bangkok thought it was important to scan my body as I arrived, but I had no idea why reply Davidzheng 9 hours agorootparentprevDid we experience the same thing? March 2020 saw explosion in nyc and much of mainland US and widespread lockdowns. However many people were convinced the lockdowns would last two weeks at most with covid dropping to near zero (in hindsight completely ridiculous low probability thinking) reply Jensson 9 hours agorootparentYeah, that isn't generative AI today, the explosion already happened. At best you can call the explosion ChatGPT, but that was already 1.5 years ago. Not much unexpected will happen now unless a new breakthrough is made. Similarly covid in march 2021 were well understood and not much would happen unless it mutated into a much deadlier strain, which of course is unlikely to happen. Covid in march 2021 was still a massive thing, as is AI today, but just that it is a massive well understood thing not something that will completely change the world in the future. reply lame-robot-hoax 19 hours agorootparentprevDepending on your reference point, a sigmoid and an exponential curve look the same. reply Davidzheng 19 hours agorootparentTrue but i think some sigmoid curves have very low probability. Like COVID doing down right after it spread in NYC reply nullindividual 20 hours agorootparentprevThe slope changed between 1980 and 1990. The slope changed between 1990 and 2000. There's a pattern somewhere, here. reply thriftwy 19 hours agorootparentBetween 2000 and 2020 slope was lousy. Or, frame it the other way, the rest were catching up with the best. reply ChildOfChaos 6 hours agoparentprevExactly. It's amazing how many people seem to be missing this. I think a big problem is the amount of companies rushing to add AI to it's products when it's not really that good at the moment or not really that useful so there is somewhat of a backlash against it and people hearing the headlines about AI changing the world, seeing that AI has been added to a product they use, testing it and seeing it's useless and then wondering what all the fuss is about. I stay up to date with AI news, I follow it quite closely and I interact with chatbots somewhat, however I've yet to find it useful in any real use cases for me, since I don't code and the answers it gives for everything else are usually fairly generic nonsense, but it's amazing just that they can do that and the improvement is there. Image generation is pretty crazy too, video is getting there and audio for sure is going to be done, so I wonder what happens to the music industry? Instead of having Spotify you can generate your own music on a whim. As a music lover, that is going to be very odd. reply swatcoder 20 hours agoparentprevThat's not a very interesting opportunity for insitutional investors (hedge funds, banks, pensions, etc). They have the resources and confidence to move fast and cleverly and don't need to make big bets on far off possibilities. They have better things to do with their money in the meantime. The nearly $1T in investment hype since Q42022 was chasing the possibility that revolutionary commercialization of generative AI might arrive any day. That no longer seems likely. As you note, there seem to be at least a few more profound discoveries that need to be made and matured, and so that revolution is still a long way out (from the perspective of these investors), even if it looks meaningfully more possible than it did two years ago. reply Davidzheng 19 hours agorootparentWhat is far off for you? Is it five years? reply nabla9 19 hours agoparentprev>But it's only overhyped if AI stays that way. This is a fallacy that comes from not doing discounting. When you apply discounting with any realistic discount rate, current investments don't pay off if AI becomes something in 2040. Fro investors eventually is not a good enough. They move money into some other asset and wait until the time comes. reply bugbuddy 20 hours agoparentprevYour take reminds me how self driving car boosters have been saying for the last 20 years. I know Waymo is out there driving itself IF you don’t count the operators with remote controls connected over 5G. We could achieve AGI today if you don’t count the Indian worker sitting remotely typing a response to your chat messages. reply Davidzheng 19 hours agorootparentlol the cognitive dissonance of knowing your own argument is wrong and still not able to admit it reply bugbuddy 19 hours agorootparentPlease feel free to put your money where your mouth is and convert your portfolios into an AI frontier fund. reply Davidzheng 9 hours agorootparentYes. This argument is kind of pointless because it's just a simple bet. I would be very happy if the market was full of people like you--i would gladly take a huge bet on AI against it. But i guess the market is not fully in agreement with you in this matter reply smt88 19 hours agoparentprevThis is exactly the argument we were all having about crypto a few years ago. \"It's early days!\" they told us for 10 years. Now crypto is used primarily for crime and speculation, which is exactly what we'll be using GenAI for in a few years, especially as it poisons its own source of training data. reply kaonwarb 19 hours agorootparentKey difference from my perspective: I never did hear a convincing use case for large-scale use of crypto. It's really easy to make one for AI. reply anytime5704 19 hours agorootparentprevCrypto (currency) never served a real use case other than pump and dumps or dark web transactions. GenAI is used by people for all sorts of purposes. Anecdotally, it has largely replaced my own usage of traditional search engines. This strikes me as a false equivalence the same way comparisons to the dot com bubble do. reply rhdunn 10 hours agorootparentI'm using it primarily for code autocomplete, interactive story telling, and listening to online articles and stories. I don't think the current generation of AI will go away -- there are too many use cases for it. I think it will take some time as people and companies experiment with it, find what works and what doesn't. Then, some use cases will disappear but others will remain -- like how autocorrect came from previous generation of AI tech. --- With the code autocomplete it works best when there is repetitive logic such as doing the same thing for each variable in a class. With interactive story telling it can be good, but can often say things that don't make sense or not pick up on the subtext of what you are saying. It can still be entertaining. -- I still use other forms of entertainment. On the TTS side, I've been using it for reading articles and stories for a long time. The current gen models are very good quality wise, but need work on accuracy and artefacts in generation. reply smt88 16 hours agorootparentprevI have found hallucinations to be bad enough that I can't use it as a search engine. Even Google's AI search is awful. How do you deal with its likelihood of being convincingly wrong, even if it's small? reply anytime5704 3 hours agorootparentThere’s a time and a place for checking its work. Depending on the search, I either don’t care, or I use something like perplexity, which includes its references. reply _heimdall 20 hours agoparentprevThe hype follows what OpenAI's latest release does though, not what OpenAI may release in a decade or two. > What excites the true believers is the slope, not the intercept. The phrase \"true believers\" jumps out to me here. It sure makes AI (well, ML) sound more like a religion than a field of technological research. reply signal_space 20 hours agorootparentCult of the exponential, considers sigmoids heresy. reply elevatedastalt 20 hours agorootparentprev> The hype follows what OpenAI's latest release does though Yes, because what OpenAI releases today helps us improve the prediction of what it may release in a decade or so. reply _heimdall 17 hours agorootparentBut how can we actually quantify the accuracy of said predictions? Each release of an LLM is largely a black box. We don't know exactly how they work, what they learned, why they learned it, or what their limitations are. How exactly does that help us predict what could exist in a decade or two? And how do we align that predictive ability with the fact that so many people working in the industry would not have been able to predict where we are today a decade or two in the past? reply groby_b 19 hours agorootparentprevThat is only true if you believe research never has dead ends and progresses on a well-defined curve. It does neither. What OAI or others release next does, in the very best case, help set a floor. And if it's a blind alley to a local maximum, even that isn't super-helpful. reply metaphor 19 hours agoparentprev> The hype isn't coming from where AI is today, but where it will be in 2030 and 2040. Scott McNealy circa 2002[1][2] had some relevant food for thought: >> But two years ago we were selling at 10 times revenues when we were at $64. At 10 times revenues, to give you a 10-year payback, I have to pay you 100% of revenues for 10 straight years in dividends. That assumes I can get that by my shareholders. That assumes I have zero cost of goods sold, which is very hard for a computer company. That assumes zero expenses, which is really hard with 39,000 employees. That assumes I pay no taxes, which is very hard. And that assumes you pay no taxes on your dividends, which is kind of illegal. And that assumes with zero R&D for the next 10 years, I can maintain the current revenue run rate. Now, having done that, would any of you like to buy my stock at $64? Do you realize how ridiculous those basic assumptions are? You don't need any transparency. You don't need any footnotes. What were you thinking? >> I was thinking it was at $64, what do I do? I'm here to represent the shareholders. Do I stand up and say, \"Sell\"? I'd get sued if I said that. Do I stand up and say, \"Buy\"? Then they say you're [Enron Chairman] Ken Lay. So you just sit there and go, \"I'm going to be a bum for the next two years. I'm just going to keep my mouth shut, and I'm not going to predict anything.\" And that's what I did. [1] https://archive.is/EZ6vD#selection-2985.79-2985.928 [2] https://archive.is/EZ6vD#selection-3009.1-3009.399 reply wkat4242 19 hours agoparentprev> The hype isn't coming from where AI is today, but where it will be in 2030 and 2040. Yeah but the investors they're talking to aren't thinking about 2030-2040. They are thinking about 2025. If they don't get their 2000% profits they are imagining now they will declare it a FLOP!!!! and drop it. Doing harm to the overall development of a technology that does have its merits but is just propped up to a position it's not ready for by a long shot. With this kind of pressure a slow slope is not enough to keep investors happy. These guys are all hitting themselves in the head every day that they didn't buy bitcoins when they cost $0.01 and they are constantly telling themselves the next big thing will make them rich beyond their wildest dreams. They are ruining good tech this way. Metaverse was pretty decent too, it was just not for everyone yet (and it won't be for a loooong time), but it does have its niche uses. reply gexla 16 hours agoparentprevI didn't read the paper. But I opened it up in a tab, typed \"unreliable\" in the search box, and zero results. I'm wondering what words they actually used. I also didn't find \"overhyped? or \"expensive.\" I'm sure those words aren't unreasonable as a take-away from the paper. But they aren't in the paper. reply advael 19 hours agoparentprevThere has been no convincing argument by anyone that this will change in a way that will drive the value that the grifters in silicon valley are promising. It's tautological to say that people making bets are making expected value estimations about the future, and goldman sachs are doing just that reply freejazz 19 hours agoparentprevNo, the hype is coming from where people are saying it will be in 2030 and 2040. reply inopinatus 19 hours agoparentprev“All you need is irrational faith and you too can be unable to distinguish between hype and overhype!” reply meiraleal 19 hours agoparentprev- The hype isn't coming from where Bitcoin is today, but where it will be in 2030 and 2040. I'm among the AI true believers but the change above made me have second thoughts. reply ramon156 20 hours agoprevNews flash, stock related company says thing to influence market reply 93po 2 hours agoparentnews flash, 155 year old company (that has never meaningfully innovated anything and is run by a 62 year old man with zero experience outside finance) is resistant and pessimistic about technology reply frithsun 20 hours agoprevAI is enabling a new generation of exploits and hacks that will force tech to get a lot simpler, more human readable, and more privacy oriented. Everything is going to need to be refactored for a world where bad actors have truly unlimited time and attention to invest in identifying privacy and security vulnerabilities. reply meiraleal 19 hours agoparentThat would be a great change. reply AbstractH24 4 hours agoprevIt’s useful for turning unstructured data into structured data. And other similar repetitive tasks. Not chatting with and asking to write something open ended that even you don’t know how to do. reply jojola 19 hours agoprevReally hard to disagree with Goldman Sachs on that one. There's no reason to believe in all this hype unless you are one of the many that are surfing, and making money, with it. Besides all those chatbots there is not a lot of AI products that are really useful to the everyday user. reply EternalFury 20 hours agoprevIt’s healthy for GS to express a contrarian opinion. If GPT 5 doesn’t meet high expectations, another winter could soon arrive. reply duskwuff 20 hours agoparentIf the success of a single company -- indeed, a single product -- is really that pivotal, the industry is already on the brink of collapse. reply GaggiX 19 hours agoparentprevA winter where AI will continue to be used for semantic search, moderation, translation, captioning, TTS, STT, context-aware grammar checker, LLM, audio/image classifier. An AI winter where AI will be used everywhere. reply threetonesun 19 hours agorootparentIt's a difference between big, interesting, investment worthy AI that stands alone and small, uninteresting, practical business features that happen to use AI. The real bellwether for me will be whether or not Apple, after releasing it's AI features, ever talks about them as a main point in a WWDC again, or if it just gets pushed into \"here's something new photos can do (with AI)\" like it has for the last x many years. reply EternalFury 18 hours agorootparentprevPeople always conclude I hate AI. But it’s only a matter of ROI. The amount of capital investment must be matched with increased sales or reduced expenses. If reduced expenses come via job elimination, it doesn’t last long in a consumer society. Increased sales through new product categories are an option, but costs will need to come down and reliability will need to increase. Hence the current hopes. reply GaggiX 18 hours agorootparentI didn't infer anything about your opinion on AI, I just reported that the AI winter will, funny enough, still have AI everywhere, if it's going to happen. reply Jensson 9 hours agorootparentThe last AI winter also had AI everywhere, just not AI research everywhere. reply GaggiX 8 hours agorootparentWhat AIs are you referring to? \"If ... Else\" type of AI? reply Jensson 1 hour agorootparentThe statistical model type of AI/machine learning, things like decision trees, decision forests etc. Those has been everywhere for a long time now. reply bigfishrunning 19 hours agoparentprevAnother winter is inevitable, but how soon will it come? reply Davidzheng 19 hours agorootparentIt's not inevitable reply Jensson 9 hours agorootparentAI is fundamental research, it is inevitable that funding for AI research will dry up at some point in the future just like all such research. AI winter doesn't mean AI isn't used, it refers to AI research funding shrinking back to the same level as other fundamental research fields. reply bastien2 19 hours agoprevJust remember: the only reason they threw a trillion dollars at genAI is because they thought they could lay off their entire creative staff reply ChrisArchitect 18 hours agoprev[dupe] More discussion: https://news.ycombinator.com/item?id=40837081 https://news.ycombinator.com/item?id=40856329 reply pie420 20 hours agoprevGoldman Sachs has no reason to disseminate valuable market insights and analysis for free. All financial thinkpieces by investment firms should be disregarded as economic manipulation and propaganda reply sojournerc 20 hours agoparentThey do have an interest in accurate pricing in markets (as does any investor). Sometimes something can be both true and have a portion of self-interest. An organization of that size might know something you don't. Do you have information they don't? Can you show that LLM tools are worthy of the hype (investment in particular), are consistent, and reliable? That's certainly not my experience. It's worth saying the emperor has no clothes, especially if we want to avoid the bubble pop, which admittedly GS has an interest in. reply bostonsre 20 hours agoparentprevThey don't have an incentive to share their view of reality until after they make investments that would capitalize on their belief in that view of reality. After they take those positions though, it is in their best interest to inform everyone about what they view to be real. It would seem to be risky for them to try to take positions based on some false narrative on some aspect of the market because if someone else finds and publishes the true narrative, and that becomes the prevailing sentiment, they would be harmed. reply ohxh 20 hours agoparentprevOr maybe they do believe this, and entered into trades to express this sentiment. Now, they need the market to correct to what (they believe) is accurate, so they can take profits and free up their capital again. reply singleshot_ 20 hours agoparentprevIncomplete understanding of GS’s business and why someone might want to do business with them. reply impulser_ 19 hours agoparentprevI think you are vastly misunderstanding the role of investments banks. I think you might be confusing them with Hedge Funds. reply jjtheblunt 19 hours agoparentprevdo you mean something like generalized front running, where they see imminent happenings, get there first, and in this case dissuade competition? reply tim333 8 hours agoprevQuibbling a bit: >questions whether generative AI will ever become the transformative technology that Silicon Valley and large portions of the stock market are currently betting on... Who really cares if the AI is generative or not? AI is pretty much bound to be a transformative technology. >higher productivity (which necessarily means automation, layoffs, lower labor costs... Nah. It can also mean more stuff produced. And probably will. That said current asset prices may well be a bit inflated. And some startup could come up with a better algo for self improving AGI rendering the other companies not worth much. Bit like how Google came along and rendered the other search companies not worth much. reply crispyambulance 20 hours agoprevGoldman Sachs is overhyped, overpaid, and unreliable. reply DarkNova6 20 hours agoparentThose are not mutually exclusive. reply OldSchool 19 hours agoprevI would have thought GS would be wise enough to not bully AI now in its early days just in case it ends up taking over the world. reply gexla 18 hours agoprevI'm not going to read the whole paper, but I didn't find anything in the paper which used the words \"overhyped\" \"wildly expensive,\" or \"unreliable.\" Unreliable for what? There's so many different ways which people use these services. The way I use them, it's not unreliable. Because I don't use these services in a way which would be unreliable. Instead, it seems the broader conversation that the huge investment in AI might not pay off right away is very plausible. I pay $20 per month for one of these services. And that $20 service is maybe the most expensive to implement of anything I have paid $20 monthly by a long shot. And this is important for the audience of Goldman Sachs. Maybe it's not so important for the typical reader of Hacker News. Who reads Goldman Sachs papers? reply nipponese 20 hours agoprevActual report https://www.goldmansachs.com/intelligence/pages/gs-research/... Right in the preamble ..despite these concerns and constraints, we still see room for the AI theme to run, either because AI starts to deliver on its promise, or because bubbles take a long time to burst. reply knowaveragejoe 20 hours agoparentOverhyped? Sure. Expensive? Definitely. Unreliable? Sometimes. So many HNers love to be contrarian about anything resembling a hype train, and feel validated when a report like this comes out. I think the reality is that people moving forward with integrating AI into various aspects of their business and reaping the rewards aren't here making snarky \"I told you so\" comments, which seems to be every other comment on this and the other threads about the same article. reply sojournerc 20 hours agorootparentBut, I've yet to see the \"rewards\". I see tech companies laying off staff with the expectation of AI taking up the slack while every piece of software I use gets shitier. Show me the bottom line improvement, or better tools, better art, better music... It just doesn't live up to the hype. reply knowaveragejoe 19 hours agorootparentOthers can speak to the bottom line improvement. I'm sure for investors the \"return\" looks a lot different, but for my own uses various AI tools have been a major boon to my productivity. reply wredue 19 hours agorootparentSuch as? I tried copilot once and it was pretty keen on sneaking itty bitty bugs in to my programs very frequently. It is fine for certain boiler plates, but it wasn’t “great” for anything that templates weren’t already great for. reply Davidzheng 19 hours agorootparentThis is classic example of y intercept that the parents mean reply jncfhnb 20 hours agorootparentprevSometimes unreliable is a very funny qualifier. If something were consistently unreliable then unreliable wouldn’t be the right term. reply knowaveragejoe 19 hours agorootparentSometimes is the qualifier. What are you using it for and how are you using it? The barrier to consistent reliability with these tools is definitely above most organizations' grasp at this point, but that'll only improve with time. reply jncfhnb 17 hours agorootparentIf something is reliable, it is, by definition, consistently reliable. If something is unreliable, it is, by definition inconsistently unreliable. reply Davidzheng 19 hours agorootparentprevIt's not overhyped. It's correctly hyped because it's hard to estimate the upper bounds of the value AI can bring. reply monero-xmr 20 hours agorootparentprevHN is overwhelmingly pro-AI and on the LLM hype train. Silicon Valley and YC are largely only investing in that sector right now. Pretending there is some huge contrarian group here is absurd reply 93po 2 hours agorootparentthis is a really common problem online and HN too: some people think the general consensus is really biased one direction, and some people (reading the same material) think the opposite is true. i see people constantly ragging on LLMs as being infringing and bad and wanting to see them fail reply knowaveragejoe 20 hours agorootparentprevAre they? Virtually every comment on this thread, and the two others from the past week, have a pretty negative sentiment overall if not more or less saying \"I told you so\" reply rsynnott 19 hours agorootparentI mean, I think people are getting a little tired of the hype; not sure how many true believers there are at this point. reply Davidzheng 19 hours agorootparentLosing faith at an (arguable) lull of two years are not the believers we need anyway. Some of us were believers even ten years ago. reply rsynnott 5 hours agorootparentRoko’s Basilisk thanks you for your devotion. reply robjan 17 hours agorootparentprevYou're talking like AI is a religion reply phendrenad2 19 hours agoprevThey're early to this opinion, people will start to realize this en masse soon. Digital coding assistants are here to stay, but not at the $10/month price point. Maybe $10/year. reply shepherdjerred 17 hours agoparentI would easily pay $100/mo for copilot at its current level of function reply meiraleal 19 hours agoparentprevWhat's current AI platforms are missing is a friendly way to use their API individually. It should be easier for me to use my own ChatGPT or Claude account on another app/website. They could get a much bigger revenue with that too. But none of them are really willing to share the pie. reply djaouen 7 hours agoprevSay what you will about GS, they are correct here. reply OutOfHere 18 hours agoprevWhy do people not understand that such news come out of the woodwork when they have a big short position in the stock market. reply tim333 8 hours agoparentIt doesn't sound like they are short saying bubbles may go on for a considerable time etc. Goldman etc make most of their money from fees from clients, not from punting the market. reply honeybadger1 19 hours agoprevHNers are wrong about this just like they're wrong about Tesla, and the proof is outside on the road. reply unstatusthequo 20 hours agoprev [–] Also Goldman Sachs: uses quant algorithms for high speed trading at scale. What a joke. reply TeaBrain 20 hours agoparentThe existence of sophisticated algorithms does not necessarily imply the existence of machine learning/\"AI\". Despite Rentech executing all their trades via algorithmic models, Jim Simons once said that they didn't use any machine learning. reply johnthewise 20 hours agorootparenthttps://youtu.be/X72PZ_tQ4cA?si=Mj-w9iXFO4CCTCj0&t=78 I think he says the opposite. reply TeaBrain 15 hours agorootparentInteresting that he does say that in that somewhat recent interview. I forget where exactly I heard that he said that Rentech didn't use machine learning, but it may have been from an older interview. reply drewcoo 20 hours agoparentprev [–] There is no contradiction in that. I'd like to see the quants that are using ChatGPT for HFT. Apples and oranges. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Goldman Sachs released a research paper questioning the economic viability of generative AI, highlighting high infrastructure costs with minimal benefits.",
      "The report, featuring insights from economists and experts, suggests that generative AI may not become the transformative technology many expect, despite current stock market optimism.",
      "Comparisons are made to other overhyped technologies like virtual reality and blockchain, with skepticism about AI's cost and potential productivity gains."
    ],
    "commentSummary": [
      "Goldman Sachs has labeled AI as overhyped, expensive, and unreliable, sparking significant discussion among tech enthusiasts.",
      "Critics argue that while AI has specific useful applications, it is not yet capable of solving broad, complex problems or serving as an infallible source of information.",
      "The debate highlights a divide between those who see AI's current limitations and those who believe in its long-term potential, comparing it to past technological revolutions like the internet."
    ],
    "points": 129,
    "commentCount": 135,
    "retryCount": 0,
    "time": 1720815149
  },
  {
    "id": 40952650,
    "title": "For the Colonel, It Was Finger-Lickin’ Bad (1976)",
    "originLink": "https://kottke.org/16/08/for-the-colonel-it-was-fingerlickin-bad",
    "originBody": "KOTTKE DOT ORG KOTTKE DOT ORG MENU Member Login Home Membership Newsletter Goods Archive + Tags About/Contact dark mode light mode Advertise here with Carbon Ads Stay Connected Newsletter RSS Feed Threads Mastodon Bluesky Tumblr Facebook This site is made possible by member support. ❤ Big thanks to Arcustech for hosting the site and offering amazing tech support. When you buy through links on kottke.org, I may earn an affiliate commission. Thanks for supporting the site! kottke.org. home of fine hypertext products since 1998. 🍔 💀 📸 😭 🕳 🤠 🎬 🥔 posted Aug 26, 2016 by Jason Kottke · gift link “For the Colonel, It Was Finger-Lickin’ Bad” Here’s a gem from the archive of the NY Times. One day in September 1976, NY Times food critic Mimi Sheraton and Colonel Harland Sanders stopped into a Manhattan Kentucky Fried Chicken. The Colonel, then estranged from the company he founded, strolled into the kitchen after glad-handing some patrons and proceeded to tear into the quality of the food: Once in the kitchen, the colonel walked over to a vat full of frying chicken pieces and announced, ‘That’s much too black. It should be golden brown. You’re frying for 12 minutes — that’s six minutes too long. What’s more, your frying fat should have been changed a week ago. That’s the worst fried chicken I’ve ever seen. Let me see your mashed potatoes with gravy, and how do you make them?” When Mr. Singleton explained that he first mixed boiling water into the instant powdered potatoes, the colonel interrupted. “And then you have wallpaper paste,” he said. “Next suppose you add some of this brown gravy stuff and then you have sludge.” “There’s no way anyone can get me to swallow those potatoes,” he said after tasting some. “And this cole slaw. This cole slaw! They just won’t listen to me. It should he chopped, not shredded, and it should be made with Miracle Whip. Anything else turns gray. And there should be nothing in it but cabbage. No carrots!” Sanders sold his company to an investment group in 1964, which took the company public two years later and eventually sold to a company called Heublein. After selling, Sanders officially still worked for the company as an advisor but grew more and more dissatisfied with it, as evidenced by the story above. When the company HQ moved to Tennessee, the Colonel was quoted as saying: This ain’t no goddam Tennessee Fried Chicken, no matter what some slick, silk-suited son-of-a-bitch says. And he got sued by a KFC franchisee after he commented: My God, that gravy is horrible. They buy tap water for 15 to 20 cents a thousand gallons and then mix it with flour and starch and end up with pure wallpaper paste. And I know wallpaper paste, by God, because I’ve seen my mother make it. To the “wallpaper paste” they add some sludge and sell it for 65 or 75 cents a pint. There’s no nutrition in it and the ought not to be allowed to sell it. And another thing. That new crispy chicken is nothing in the world but a damn fried doughball stuck on some chicken. Colonel Sanders: serving up chicken and sick burns with equal spiciness. (via @mccanner) food Harland Sanders KFC Mimi Sheraton restaurants Share",
    "commentLink": "https://news.ycombinator.com/item?id=40952650",
    "commentBody": "For the Colonel, It Was Finger-Lickin’ Bad (1976) (kottke.org)122 points by sublinear 10 hours agohidepastfavorite107 comments UncleSlacky 8 hours agoIf you want the real thing you have to go to the restaurant named after his wife: https://en.wikipedia.org/wiki/Claudia_Sanders_Dinner_House reply e40 5 hours agoparentWent there as a kid. Can confirm it was insanely good. reply amiga386 8 hours agoprevSanders in 1963 on What's my Line? - https://www.youtube.com/watch?v=Wk4Eq8IcQMk And a video on the history of Sanders' fight with KFC corporate, and what exactly is in the seasoning? https://www.youtube.com/watch?v=7WJYOgzFydc My takeaway is that Sanders cared about serving quality fast food, and KFC corporate cared about reducing costs and doing brand necromancy. The upshot is you won't get KFC chicken at KFC, just a pale, cheap imitation of the original, meanwhile the original recipe is still kicking about out there in other chicken takeaways (assuming they cook the chicken properly!) reply saghm 5 hours agoparent> My takeaway is that Sanders cared about serving quality fast food, and KFC corporate cared about reducing costs and doing brand necromancy. The upshot is you won't get KFC chicken at KFC, just a pale, cheap imitation of the original Assuming that's the case, the anecdote in the article of him walking into the kitchen in a random KFC and berating the people for making the chicken the was they did seems a bit unfair; it's not like the random people working there had any choice in the corporate policies or autonomy to choose to make the chicken in a different way. I get that he was frustrated and that this wasn't the only way he tried to fight the changes, but it shouldn't be that hard to have a little empathy for the presumably minimum wage workers who are just there to earn a paycheck. reply eszed 2 hours agorootparentWent there with a reporter from the NY Times, and put on a show in the kitchen. One hopes he didn't make a habit of \"walking into the kitchen in a random KFC and berating the people\", but this specific incident was not that at all. reply ryandrake 3 hours agorootparentprev> it's not like the random people working there had any choice in the corporate policies or autonomy to choose to make the chicken in a different way. I'm torn on this one, and can see both sides. For any kind of mega-business, not just restaurant chains: If your goal is to correct corporate behavior, protest or \"make things difficult for corporate\" then there really aren't any good options. You could annoy the individual stores and/or frontline staff, but as you say they are usually powerless minimum-wage drones who can't change things, and even might probably be sympathetic to your cause. On the other hand, if enough people make the businesses unpleasant or do things at those business that end up costing corporate, there is a slim chance that corporate might make changes. A big problem with mega-business style capitalism is that key stakeholders like employees and the general public are powerless. You can only change a business's behavior if you're either 1. shareholders or 2. customers via boycott or 3. regulators. Unfortunately, the non-customer general public cannot vote a corporation out, nor can they walk into their local WalMart and \"complain to the owner of WalMart\" like they can for smaller local businesses. reply crawfishphase 2 hours agoprevSanders might sound A LOT like Gordon Ramsay in this article, but I doubt Gordon ever shot and killed a man over a turf war and beat up one of his legal clients. I think I remember hearing the Colonel beat down at least one of his bosses. He must have coated himself in restaurant-grade teflon as he seemed to get away with it.. Should have called it Gangster Fried Chicken. reply zamadatix 8 minutes agoparentSanders never killed anyone either. He was involved in a shootout where the other guy shot and killed the gas station manager, Sanders had shot said guy who lived and later went to jail as a result of killing the manager. reply COGlory 7 minutes agoparentprevSanders killed anyone. He was involved in a shootout at a gas station, and a gas station employee got shot by his shootout opponent and killed, which effectively won Sanders the turf war. reply dzink 5 hours agoprevThe first and second times I visited KFC in the US I was shocked there were no vegetables in their sandwiches. Other than the corn, and coleslaw, no lettuce or other ingredients in the sandwiches. In europe, and China and any other country I’ve seen KFC they have amazing Zinger sandwiches with lattice and sauces and grea flavor. We make it a point to never go to KFC in the US. reply infotainment 5 hours agoparentOne thing I find utterly depressing is how literally every American fast food chain has infinitely better quality and taste in their overseas locations. Evidently they save their absolute worst products for their home market. reply dzink 22 minutes agorootparentIn the US they aim to become a local monopoly through rock bottom prices, while abroad they are an upscale location with premium prices where people go to treat mostly their kids. McDonalds abroad I’ve seen had Hollywood theme or Elvis theme, etc. It’s a piece of US culture with the same big macs but broader menu. reply flenserboy 3 hours agorootparentprevThat's because the US is a profit farm for US corporations (& a tax farm for those making bank on US foreign policy) so they can subsidize the rest of the world. Food, medicine, you name it — bottom-of-the-barrel service & quality for the average person & below in the US at top-dollar prices, & the people in the middle & below classes think they're doing well because it's all they know. reply Ylpertnodi 2 hours agorootparent>That's because the US is a profit farm for US corporations....so they can subsidize the rest of the world. Do you mean the corporations subsidise the rest of their corporations (ie cheap usa kfc supports mmm-lovely jpn kfc etc), or the subsidies extend to 'the rest of the world' in general? reply ryandrake 19 minutes agorootparentI think what OP is saying is that corporations can charge USA people absurdly high prices for terrible quality stuff (reaping huge profits) because we allow it, whereas overseas, the same corporations have to accept lower margins and provide better product/service because non-Americans wont accept it. Not sure I 100% buy that, but it does sound kind of truthy. reply brookst 4 hours agorootparentprevI’d say they’re responding to different market demands. reply ricardobayes 2 hours agorootparentDefinitely. Dining out regularly, even at fast food places, is not necessarily a usual, or normal thing for lots of Europeans. Most \"middle-class\" people I know, like teachers or so, dine out probably a few times a year to celebrate an event. Groceries are cheap and fresh and people have less disposable income in Europe, generally. Fast food's biggest competitor is home cooking. It's also the reason why we prefer (or preferred) diesel vehicles here. reply opo 1 hour agorootparentThis likely varies by country in Europe and even what part of the country (rural vs urban). For example, a survey done in Germany found the following distribution of how often people went to a restaurant: Once a month: 44% A few times a month: 34% Once a week: 13% Several times a week: 7% https://www.statista.com/statistics/1085317/dining-out-habit... reply TehCorwiz 3 hours agorootparentprevYeah, the rest of the world has laws that demand minimum product quality. reply daseiner1 2 hours agorootparentI’d love to hear the inside baseball on how a committee determines “minimum product quality” for a fried chicken sandwich. reply ryandrake 16 minutes agorootparentThere is probably a valuable formula hidden in a safe at KFC showing how little chicken and how much breading/sawdust they can get away with using, to optimize profit. reply UberFly 1 hour agorootparentprevRight, or they force you to order lettuce and tomato to peel off or no sandwich for you. reply spywaregorilla 4 hours agorootparentprevwell the flip side take is that other countries put a premium on the american brands we consider trashy reply rdedev 4 hours agorootparentThis was a shock to me. During college back in India I had a friend who used to live in Canada. When I suggested going to McD for food, he was like no it's too trashy and the food is bad. I couldn't believe that cause the lines at McD were long. After coming to the US I understood what he meant. Burger king in India is almost gourmet compared to what you get in USA reply adamomada 45 minutes agorootparentAnd Burger King in Italy IS gourmet. You have to compete with your competitors, in North America it’s a corporate franchise fast food wasteland and the bar is set exceptionally low. reply CrazyStat 4 hours agorootparentprevIn Managua, Nicaragua ca. 2000 McDonalds was a sit down restaurant with waiters. reply JackMorgan 4 hours agorootparentIt was the same growing up in San Jose, CR. My friends would always meet up on Saturday nights at the Burger King next to the San Pedro mall. It was so fancy! American fast food wasn't cheap either, probably 2-3x more expensive than most other places you could eat in town. reply CoastalCoder 4 hours agorootparentprevWas it the same food as we get in the US, just with better service/setting? reply SoftTalker 2 hours agorootparentIn my experience, yes. A Big Mac is a Big Mac pretty much everywhere. The non-US stores do often have some menu items that are targeted to local tastes, that you would not see in a mainland US store. reply podunkPDX 1 hour agorootparentThe McDonalds in Rome near Termini had an amazing desert bar! reply SoftTalker 38 minutes agorootparentI wonder if the international franchises have more freedom to vary the menu? I'd guess the standard signature items like the Big Mac and fries are pretty much mandatory though. reply readthenotes1 2 hours agorootparentprevThe McDonald's I went to in Vienna Austria was horribler on every level. reply pseingatl 1 hour agoprevHere's the recipe for the Colonel's original herbs and spices seasoning: https://www.chicagotribune.com/2016/08/19/kfc-recipe-reveale... reply oe 43 minutes agoparentCould someone copy and paste it here for us lowly EU folks? reply detourdog 12 minutes agorootparenthttps://www.youtube.com/watch?v=VYSo8zTrTAs&list=PLgOb3zseg1... These Glen and Friends Cooking videos of North American cuisine development are top notch. reply nanoxide 34 minutes agorootparentprevhttps://archive.ph/jzgsP reply KingOfCoders 9 hours agoprevEvery founder I have met after selling their company. reply ahartmetz 9 hours agoparentUnderstandable (as in: I could see myself acting like that if I had a company to sell), but also, you sold it, what did you expect? Did you believe some legally not enforceable promises because you wanted the money and were willing to lie to yourself? reply KingOfCoders 9 hours agorootparentFrom my own experience - I didn't expect anything. But it makes you sad if what you have built and spent so much time on it is going down. reply fifilura 5 hours agorootparentprevThere are many ways to loose control of your company without selling it upfront. For example if you take money from an minority investor with the promise of an IPO (so they can make profit from their minority share). After the company goes public, control is much more iffy. You may for example not have your place in the board anymore even though you are majority owner. reply Etheryte 4 hours agorootparentThis makes no sense whatsoever. If you're the majority holder then by definition you hold more than 50% of the stock and can force whatever resolution you see fit. Perhaps you meant largest holder, but without a majority? reply SargeDebian 3 hours agorootparentOr multiple stock classes, where done have more voting rights than others. reply nradov 4 hours agorootparentprevYou're not making any sense. Unless the majority owner has a different share class with reduced voting rights then they can absolutely control the Board. reply fifilura 4 hours agorootparentYeah brainfart. What i meant to say that in a public company you are expected to choose. Either you have control of the board or you are the CEO having control of the operations. Not both. reply bitwize 3 hours agorootparentBill Gates was chairman and CEO of Microsoft for the longest. Have things changed since then? reply nradov 4 hours agorootparentprevThere is no such expectation. Some corporate governance experts prefer that the Chairman and CEO roles be split in order to prevent conflicts of interest and protect the rights of minority shareholders but there are many companies where a single person does both. reply jasonjayr 3 hours agoparentprevHowever he sold it; they apparently, to this day have rights to his likeness. I would hope that would give him some power to call them out if they are making a lesser product in his name, all the while passing it off as his 'original recipe' reply fallingknife 3 hours agorootparentI love how in the article they just let him go back and walk around the kitchen and bitch even though he had nothing to do with the company at that point. Who's going to say no when you walk into a KFC and you're Colonel fucking Sanders reply jasoneckert 5 hours agoprevFun fact: After selling out in the US, Colonel Sanders moved to Mississauga, Ontario, Canada to oversee the Canadian operations to ensure that the quality was of his liking. When I grew up in the 80s in Canada, KFC was incredible, as were all of their items. At age 12, I scored a job working there too. During that time was when they announced their crispy chicken variant in Canada and \"new taste\" - but what we saw were new flour bags (we had to throw out the original flour bags), new oil in the cookers, new processes (no more soaking the chicken for 15 min before frying), and gravy that was made from a soup packet. And while the chicken tasted the same, it was far more greasy and disgusting to handle compared to beforehand - and everyone noticed. I remember our manager telling us \"Well I guess we now have to make it US style. But our prepared cost went from 11 cents per piece to 8 cents per piece after all bills are paid.\" reply conception 6 hours agoprevHere’s the original seasoning mix - https://marionkay.com/product/chicken-seasoning-99-x/ reply dylan604 5 hours agoparent\"Ingredients: Monosodium Glutamate, White and Black Pepper, Fine Flake Salt, Sage, Coriander, and other natural spices\" So if the first listed ingredient represents the most abundant ingredient... Also, such a cop out that the FDA has allowed \"other natural spices\" to be a legit listing. Supposedly to protect corporate secrets blah blah. What if someone is allergic to one of those \"natural spices\". Either we're for accurate food labeling for the public's safety, or we're not. This in between state highly suggests we're not. reply mort96 4 hours agorootparentIt's so weird to see this completely legitimate critique of food labelling standards is stitched on to a nothing-comment about seasoning containing MSG reply dylan604 4 hours agorootparentHow is it any more of a nothing-comment than yours? MSG is fine, but it definitely isn't what I would have imagined being the main ingredient. A little MSG goes a long way, so if that's the main ingredient, how little is used of the actual herbs&spices? I never said anything negative about MSG. You read that into it on your own instead of just thinking about the rest of what was implied reply ryandrake 9 minutes agorootparentI think it should go without saying that allowing companies to vaguely say things like “our product contains a bunch of stuff, trust us, bro” goes against the spirit of transparency behind the FDA’s rules. reply mort96 2 hours agorootparentprevThe reason I called the MSG part a \"nothing comment\" is that it doesn't say anything, everything is left to implication. Nowhere did I say that you said anything negative about MSG, I was complaining about the total lack of substance. You just pointed out that MSG is the \"most abundant ingredient\" (meaning it makes up at least 17% in this case, I think). Anyway it doesn't matter. reply rootusrootus 4 hours agorootparentprev> So if the first listed ingredient represents the most abundant ingredient... It's seasoning, so it doesn't seem surprising at all that MSG would be pretty high on the list. Do you think that's bad? reply dylan604 4 hours agorootparentWhen some one says \"secret recipe of 11 herbs & spices\" my mind doesn't immediately jump to MSG as an herb or spice. Then to see that it is the primary ingredient definitely jumps out to me. Does it not to you? reply kemayo 3 hours agorootparentIt's in the same category as salt, which I'd kinda expect to be in the 11-things \"secret recipe\", but which isn't a herb or spice. reply tekla 3 hours agorootparentprevCall it celery powder then if it makes you feel better reply rootusrootus 2 hours agorootparentIsn't celery powder a 'natural' source of nitrates? I didn't think of it as an analog to MSG. reply jajko 5 hours agorootparentprevSame would go for coca cola recipe, and many others reply alsetmusic 4 hours agoprevI used to have a photo of a man who owned a KFC with The Colonel on my fridge. Shot in the 70s, from his appearance. The man was our customer when I had a retail gig and gave me the photo. I wish I still had it. Here’s to Bernard. reply mistrial9 3 hours agoparentA 1970s KFC franchise was a profitable thing to have .. lots of people who come from working class background had a chance to connect to \"big business\" and get a real economic lift. After seeing how some people genuinely struggle about money, it is hard for me to be wholly critical of the business, major flaws and all... reply elchief 4 hours agoprevKFC has been garbage for decades, at least in Canada. I hope Popeye's eats their lunch reply sublinear 4 hours agoparentNot sure if this is also true in Canada, but Popeyes now sells some of the worst quality fried chicken since 2020. They haven't even been anything close to \"Louisiana inspired\" in years. This is supposed to be their brand differentiator, but I haven't seen jambalaya or gumbo on their menu in over a decade. Most locations in my area haven't brought back the seafood since 4 years ago despite being on the menu (always out of stock). I don't see any of these legacy brands ever being on top again. Their most recent idea is selling some nasty soggy wings that are now routinely given away for free with any order. It's about as ironic as it gets that these wings didn't take them to new heights. reply Yhippa 2 hours agorootparentI feel like Popeyes had a random moment during the pandemic when the released their dark meat fried chicken sandwich which was initially good, but when the hype died down, they regressed to their normal fried chicken quality: bad. reply dylan604 3 hours agorootparentprev> Most locations in my area haven't brought back the seafood since 4 years ago despite being on the menu (always out of stock). I don't know, I'm kind of okay with that. Seeing seafood offered in very land locked locations has always been suspect to me. In a fast food place, I'd doubt it was actually anything other than imitation version anyways though so what's the point? reply astura 4 hours agorootparentprev>I haven't seen jambalaya or gumbo on their menu in over a decade. Probably because nobody wants it? I worked for Popeyes 25 years ago. We never had gumbo and people would order jambalaya like every other day, if that. Literally everything was more popular than jambalaya. reply sublinear 3 hours agorootparentIt was probably true back then for the same reason nobody wants wings from pizza hut today either :D All I was saying is that it was on the menu in the past and the decline in quality has been steady for a long time. It's as if it's built into their long term strategy for the business. reply gramie 5 hours agoprevI remember eating KFC in Japan, about 15 years ago, and it was markedly better than we get in Canada. reply paradox460 1 hour agoprevAmusing that he griped about Tennessee fried chicken when the original restaurant was in Utah reply sublinear 10 hours agoprev> This ain’t no goddam Tennessee Fried Chicken, no matter what some slick, silk-suited son-of-a-bitch says. reply davidhyde 9 hours agoprev> “They really gag me, that’s what I think of them” When asked about a new product line, what a quote! reply interpunct 7 hours agoprevI guess he would need an \"Extreme Makeover\" to go on \"Undercover Boss\". My dad was the financial controller for a large pizza chain in the '70s--they used to send him into the field to do spot checks, which was progressive IMO. reply dsr_ 7 hours agoparentIt was a well-documented technique of quality control across industries by 1940. reply interpunct 5 hours agorootparentAnd still meticulously adhered to, I'm sure. I guess you mean \"management by walking around\"? For perspective, we had TQM in the '90s, which consisted of orders to tell TQM consultants that we knew where the TQM manuals were at, if we couldn't otherwise avoid \"The TQM Bobs\". The corporate headquarters building my dad worked in was also considered progressive and employee friendly in the '70s--with natural lighting and office noise abatement (with white noise piped in, for example). reply nunez 5 hours agoprevI would have loved to see him write a \"I will fucking piledrive you\" style blog post on modern KFC. reply peanut_worm 8 hours agoprevMiracle whip in coleslaw? Not sure I can trust his judgement reply mywittyname 4 hours agoparentI also hate MW on sandwiches and whatnot because it's way too sweet. However, most coleslaw recipes add a ton of sugar anyway. So I don't think the end result is that far off. The primary reason that I suspect he used MW is that it contains a yeast inhibitor (potassium sorbate), which seems to keep the yeast from turning the coleslaw into sauerkrautslaw. The Colonel probably didn't understand this, but knew the results were much better. I've made coleslaw both ways (mayo and MW) and I do think the MW keeps the slaw crunchier. reply sublinear 4 hours agorootparent> MW keeps the slaw crunchier It contains less oil in it than mayo. Corn starch is used to achieve this. reply _sys49152 2 hours agoparentprevman was a goddamned genius. love the kfc coleslaw. im sure the miracle whip pairs well with the tarragon vinegar thats called for. heres a reddit link: https://www.reddit.com/r/food/comments/mq5ty/original_kfc_re... reply fifilura 5 hours agoparentprevFantastic rabbit-hole for me as an European! I had never heard of Miracle Whip (or Miracel whip\" (sic!) as sold in Germany) Maybe it is the secret ingredient to a perfect cole slaw? More sweet and more mustard. I would really like to try, but I need to find an import shop for this magic paste! reply sublinear 4 hours agorootparentI've made this before in a pinch and it's your usual homemade mayonnaise recipe except you deliberately add too much vinegar and as much sugar as your palate can take. If you want to get it even closer to what we have in the USA, use the lowest quality \"vegetable oil\" you can find. It's only purpose in my life is precisely that: coleslaw. reply astura 4 hours agorootparentprevFor some reason it has a terrible aftertaste that ruins whatever it's on. It's definitely not just \"mayo but with sugar and mustard.\" reply Mistletoe 4 hours agoprevThe real Colonel sounds amazing. Every time you guys exit your company to private equity, or sell it to some huge conglomerate, realize this is what will happen to it. > My God, that gravy is horrible. They buy tap water for 15 to 20 cents a thousand gallons and then mix it with flour and starch and end up with pure wallpaper paste. And I know wallpaper paste, by God, because I’ve seen my mother make it. reply PorterBHall 5 hours agoprevSeems like an early example of \"enshitification.\" reply the_third_wave 7 hours agoprevFrom the newspaper article: \"I'll never go to India, I don't like to see people sleeping in the streets\" The times, they are a'changeing and the Colonel would presumably \"never go to (insert west coast city)\" because of all the people sleeping (etc.) in the streets reply gumby 6 hours agoparentInteresting that it only happens on the west coast, eh? You’ve never seen a homeless person anywhere else? reply spacecadet 5 hours agorootparentYes, but there is significantly more of it happening, optically, on the west coast than anywhere else in the United States. What is San Diego's unofficial name? \"The Home of the Homeless\". Before you flame, I spent years touring the United States, living and photographing homeless people and communities. West Coasters should really stop getting defensive and take action, if it actually matters to you all. All that wealth... So greedily spent... All those people... laying in the streets... strung out on cra...\"KFC\"! Had to tie it back to avoid the rule crazies. ;) reply mulmen 1 hour agorootparent> West Coasters should really stop getting defensive and take action, if it actually matters to you all. As a lifetime resident of the northwest and a current resident of Seattle let me say, sincerely, fuck you too. Your characterization of west coast residents as uncaring and inactive is inconsistent with reality. There is no shortage of people working on homelessness and related issues. Not every person has to spend every waking moment on your pet issue for it to be taken seriously. Everyone has different talents and homelessness isn't the only issue we face. reply akira2501 38 minutes agorootparentprev> living and photographing homeless people and communities. For your own benefit, or for theirs? > if it actually matters to you all You were there for a while, apparently. What did you do about it? reply hasmolo 4 hours agorootparentprevimho it comes from western states not simply rounding them up. in atlanta, before the peach bowl the cops would descend on downtown, arrest all the homeless, bus them up to cherokee, and then the time it took them to return was greater than the length of the event. now that's a little less common and the yearly doctor conference has noticed the homeless and is complaining about it. i think homelessness is a consequence not of any one area but of the american way of doing things. we treat it as an incurable disease, like addiction, but that we don't care enough about to fix. reply spacecadet 4 hours agorootparentThats why I said \"optically\", I actually agree with the sentiment that homelessness is often much worse and unseen in other parts of the United States- but! CA could do a-lot more given its wealth and desire to be seen as \"thoughtful\". reply gumby 24 minutes agorootparent> CA could do a-lot more given its ... desire to be seen as \"thoughtful\". Uhh, what? The state that produced Nixon, Reagan, Prop 13; the capital of NIMBYism and the state that had more Trump voters than any other? Sure, California is wealthy and spends a lot on its citizens, especially the needy, but it also has strong countervailing pressure, more influential than you might think given the makeup of its legislature. A state is a big amorphous group and can't hardly have a \"desire\". reply LiquidPolymer 2 hours agoprevAs a kid in the early 70's my dad would bring home a bucket and it was an amazing treat. As an adult near 60, I cannot eat Kentucky Fried Chicken. The few times I've tried the crazy level of salt* is repulsive and I feel awful afterward. I don't know if this a change in my sense of food, or change in the KFC product. *My wife's family salts everything to hell and back. I think this is because their sense of taste is declining. So perhaps I've been gifted a sensitive palette that has not lost much with age. Its worth mentioning that my in-laws struggle with obesity, diabetes, and high-blood pressure. I'm thankfully afflicted with none of these things. reply dghughes 6 hours agoprev [–] I remember the day when KFC started \"boiling\" the chicken it went from crispy to soggy. Here in Canada currently there's big controversy KFC went halal. No more bacon! reply crazygringo 5 hours agoparentCan you explain what you mean by \"boiling\"? It's very much deep-fried. And I've never had it anything but crispy, unless you seal it up in a container for too long while it's still hot. reply gruez 5 hours agoparentprev>Here in Canada currently there's big controversy KFC went halal Source? Was it just the chicken or the entire restaurant? reply julesnp 4 hours agorootparentThe entire restaurant. https://www.cbc.ca/news/canada/kfc-halal-menu-boycott-1.7258... reply petre 4 hours agoparentprev [–] Vote with your Canadian dollars, eat from restaurants who serve pork. It turns out that my countrymen love porchetta. reply crazygringo 3 hours agorootparent [–] But when has KFC ever served pork or bacon? They're a chicken restaurant. Best I can guess is maybe they had a fried chicken sandwich that had strips of bacon too? But not really a huge loss -- bacon is definitely not any kind of classic topping for fried chicken, the way it is for burgers. I mean, I love bacon but I don't want it with fried chicken. Bacon adds crunch and chewiness to a burger; fried chicken is already crunchy and chewy. reply ejj28 3 hours agorootparentI used to work at a Canadian KFC and it's just like you guessed, occasionally we'd have a special sandwich for sale for a limited time that had strips of bacon on it. Normally we wouldn't have any pork products on the menu, and when we did have bacon I'm pretty sure it was microwaved. reply petre 3 hours agorootparentprev [–] Dunno, I never eat KFC food. I'd rather eat bacon than hormone infused chicken, fully grown in less than a month, pressure fried in a crust of MSG saturated dough, which is supposedly halal and Colonel Sanders actually hates. In fact I just went to the Italian store and bought almost half a kilo of porchetta because of what I read here. It'll keep us well fed for at least two days and has all the collagen my wife otherwise gets from awfully tasting expensive supplements. Praise the lard! reply gruez 2 hours agorootparent>I'd rather eat bacon than hormone infused chicken, fully grown in less than a month They're both hormone free. \"Under Federal law, hormones are only approved for use in beef cattle, swine**, and lamb production. There are no hormones approved for use in the production of poultry, goat, veal calves, mature sheep, or exotic, non-amenable species\" https://www.fsis.usda.gov/sites/default/files/media_file/202... >pressure fried in a crust of MSG saturated dough The typical preparation of bacon basically involves it frying in its own fat. I'm not sure how pressure frying is any more worse. Moreover bacon contains nitrates and nitrites, which is known to cause cancer, unlike msg https://en.wikipedia.org/wiki/Curing_(food_preservation)#Nit... >which is supposedly halal I think you're misunderstanding whether Halal means. Halal just means the food adheres to Islamic laws. It says nothing whether it's safe or healthy. Unless you're a practicing muslim (which seems unlikely), it shouldn't be part of your consideration one way or the other. reply lmz 1 hour agorootparent> Halal just means the food adheres to Islamic laws Also ritually slaughtered if that matters to you. reply crazygringo 1 hour agorootparentprev [–] You realize that KFC chicken is just the same chicken you buy at the supermarket? It's an urban legend that KFC somehow raises its own chicken that is somehow different, whether genetically, chemically, or speed of growth. I mean, if you prefer the taste of pork over chicken then great. But the idea there's anything uniquely bad about the chicken supplied to KFC is just factually untrue. Also, since you really like Italian food, you might be surprised to find that Parmesan cheese is chock-full of MSG. Which is a major reason why it's used so much in Italian cuisine to impart flavor. MSG isn't bad -- it's umami, just like NaCl is salt. reply petre 26 minutes agorootparent [–] We don't buy chicken at the supermarket. Our parents raise chickens. Fake supermarket parmesan? Probably. Parmigiano Reggiano DOP, not really. \"The only additive allowed is salt, which the cheese absorbs while being submerged for 20 days in brine tanks saturated to near-total salinity with Mediterranean sea salt.\" > Moreover bacon contains nitrates and nitrites, which is known to cause cancer, unlike msg We don't buy bacon treated with nitrates and nitrites. That's 95% of supermarket bacon. We mostly end up buying prosciutto, which is just dried, salted and nitrate free or use home made bacon, which is basically 100% pork fat in my country. The Italians also make it, it's called lardo and it's cured with herbs. We only cure it with salt and smoke it. I'm not much of a fan of 100% pork fat or lard, but it does make good fries. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "In 1976, Colonel Harland Sanders, the founder of KFC, criticized the food quality at a Manhattan KFC, highlighting issues with frying time, oil freshness, and overall taste.",
      "Sanders, who sold KFC in 1964 but remained an advisor, expressed dissatisfaction with the company's changes, even calling the gravy \"wallpaper paste,\" leading to a lawsuit from a franchisee.",
      "This incident underscores Sanders' commitment to quality and his outspoken nature, even after parting ways with the company."
    ],
    "commentSummary": [
      "Colonel Sanders criticized KFC for compromising on quality after he sold the company, claiming it no longer adhered to his original recipe.",
      "Sanders argued that KFC's focus on cost-cutting led to a decline in product quality, sparking debates on corporate practices and food standards.",
      "Discussions highlighted that KFC's quality varies globally, with many noting better experiences outside the US."
    ],
    "points": 122,
    "commentCount": 108,
    "retryCount": 0,
    "time": 1720859434
  },
  {
    "id": 40949021,
    "title": "Responsive bar charts in HTML and CSS",
    "originLink": "https://9elements.com/blog/responsive-bar-charts-in-html-and-css/",
    "originBody": "PUBLISHED ON JUL 4, 2024 BY BLOG ARTICLE BY MATHIAS SCHÄFER Responsive bar charts in HTML and CSS Building flexible data visualizations for international sites For our international clients, we have created dynamic charts and data visualizations for the web. Charts typically render shapes like lines and paths, rectangles and circles. They contain text for titles, axis labels, numerical values and legends. SVG is the good fit for this purpose. It embeds directly into HTML and pairs well with CSS. However, for dynamic data visualizations on the web, SVG poses a challenge. Responsive charts and the problems of SVG The websites we build feature responsive layouts and fluid typography. We employ CSS Flexbox and Grid together with media and container queries to fit in the content. There is not one single fixed presentation, but many possible presentations depending on the content and the reading environment. In contrast, SVG does not have layout techniques like Flexbox, Grid or even Normal Flow. In SVG, all shapes are absolutely positioned. Text does not wrap automatically. The shapes and text need to be laid out manually by the code that generates the SVG. SVG does scale continuously, as the name says – but for charts on the web, we usually do not want that. A small chart should not look like a downscaled big chart. Text would become unreadable, shapes would become tiny pixel mush – even with techniques that prevent the scaling of some graphical features. For charts on the web, we want quantitative and qualitative responsive scaling. A small and a large chart should be designed and laid out differently. A small chart should focus on clear, distinguishable marks that represent the data. A large chart should take advantage of the screen estate to show more items and details as well as provide context. For example, a line chart with multiple lines may switch to small multiples on smaller viewports or containers. We have typically implemented this responsiveness with client-side JavaScript logic. JavaScript is able to read the container size and measure text in order to compute all shape coordinates and sizes. This often involves decollision with force simulations. This approach has severe disadvantages. The cycle of forcing the browser to compute the style, reading sizes and setting positions leads to layout thrashing and slows down the chart rendering. When the container size changes, for example due to a browser resize or orientation change, the JavaScript needs to compute all SVG positions and sizes from scratch. Assuming this takes 50-100ms per chart, a page with 20 charts freezes the browser for 1-2 seconds. HTML, CSS and SVG hybrid Horizontal bar charts are simple yet effective, intuitive and accessible visualizations. They are versatile regarding the bar design, labeling, value placement and axes. And they can be highly flexible regarding the container size. We have a pretty solid implementation of a responsive bar chart. In narrow containers, the row label is shown on top of the bar. In wide containers, it is shown next to the bar. This chart is a hybrid of HTML, CSS and SVG. We wanted to use essential CSS layout methods like Flexbox instead of re-implementing layout algorithms in JavaScript. However, the synchronization with the SVG parts is still slow, complex client-side JavaScript code. Bar chart in plain HTML & CSS We were wondering: Can we achieve this with HTML and CSS alone, preferably without SVG and with less JavaScript logic? We fiddled around, but never finished this idea. Then we saw the beautiful responsive bar charts of State of JS, made with HTML & CSS only. On narrow viewports, they use a two-column grid: On wide viewports, this is a three-column grid with subgrids that inherit the column setup: These well-made charts encouraged us to try to migrate our bar charts to HTML & CSS. Grid setup For a start, we rebuild the basic structure: See the Pen Responsive bar chart with ticks by molily (@molily) on CodePen. In the narrow version, the each row (li element) is a two-column grid: css display: grid; grid-template-columns: minmax(0, 1fr) min-content; grid-template-areas: \"dimension value\" \"bar bar\"; position: relative; In the wide version, the wrapper (ol element) is a three-column grid: css display: grid; grid-template-areas: \"dimension bar value\"; grid-template-columns: fit-content(10rem) 1fr min-content; The row (li) is a subgrid that spans all columns: css display: grid; grid-template-columns: subgrid; grid-template-areas: none; grid-column: 1 / -1; Real-world requirements Our real bar chart, however, is much more complex and has the following requirements: Internationalization with bidirectional text: We're building charts for sites in six languages and two text directions: Left-to-right (LTR, like English and Russian) and right-to-left (RTL, like Arabic and Hebrew). Positive and negative values. Bars grow to both sides. Row labels may have an arbitrary length and should wrap and align nicely. Value labels should be positioned at the end of the bars, not inside them or in a separate column. Do not repeat the axis tick lines for each row if it's avoidable. This is the solution we came up with: Responsive bar chart in HTML & CSS This is version 2 which implements essential feedback from Vesa Piitiinen. See the Pen Bar chart by molily (@molily) on CodePen. Let's dive into the implementation. Responsive grid setup The HTML structure looks like this: html {tick value}{dimension label} { bar label }{ bar label again } {dimension label} {bar label again}{bar label}In the narrow version, the .bar-chart wrapper is a three-column grid: css display: grid; grid-template-columns: min-content 1fr min-content; grid-template-areas: \"dimension dimension dimension\" \"valuePaddingStart bar valuePaddingEnd\"; The ol element and li elements are subgrids: css display: grid; grid-column: 1 / -1; grid-template-columns: subgrid; In the wide version, the wrapper becomes a four-column grid: css grid-template-areas: \"dimension valuePaddingStart bar valuePaddingEnd\"; grid-template-columns: fit-content(10rem) min-content 1fr min-content; Each row remains a subgrid. Bidirectional text Internationalization is where HTML and CSS shine compared to SVG. Our JavaScript code that generates SVG charts is full or if (isLTR) {…} else {…} conditionals. In SVG, the origin of the coordinate system is always top left. X coordinates need to be calculated using those LTR/RTL switches. In HTML and CSS, we can simply use logical properties like inset-inline-start/-end, margin-inline-start/-end as well as padding-inline-start/-end to solve most left-to-right vs. right-to-left differences. When laying out the boxes in CSS, we can work with the text direction. For example, each bar is a Flexbox container with the value label nested inside. Then the label is positioned next to the bar: For positive values, we add a box with ::before plus content: '' with a padding-inline-start of 100%. For negative values, we add a box with ::after plus content: '' with a padding-inline-end: 100%. These boxes push the label out of the bar so it sits right next to it. We still need to handle positive and negative values differently, but by using Flexbox, logical properties and the current text direction, we don't need to handle left-to-right and right-to-left differently. The bar labels are also rendered into the columns named valuePaddingStart and valuePaddingEnd. These invisible placeholders ensure the columns have the correct width to accommodate the absolutely positioned value labels. So the labels appear twice in the DOM. The placeholders have aria-hidden=\"true\" and visibility: hidden though. Tick lines spanning the full height Our goal to put the axis tick lines in the DOM only once instead of repeating them for each row complicates the grid. The challenge is to constrain the tick lines in the bar column horizontally, but let them span the whole grid vertically. This is possible with grid-row: 1 / -1 given the grid has explicit rows. It does not work with an arbitrary number of implicitly-created rows. So we defined an outer grid that has two fixed rows. The ticks are then positioned in the first row, spanning two rows. css .ticks { grid-column: bar; grid-row: 1 / span 2; } The list of bars is then positioned in the second row and spans all columns of the parent grid. It creates a subgrid that inherits the grid configuration from the parent grid. css ol { display: grid; grid-row: 2; grid-column: 1 / -1; grid-template-columns: subgrid; } The subgrid may then create an arbitrary number of implicit rows. It remains nested in the second row of the outer grid. Minimal example on CodePen: See the Pen Grid: Span whole grid by molily (@molily) on CodePen. Accessibility considerations Accessibility of data visualizations is a top priority for us and our clients. In our SVG charts and HTML / SVG hybrids, we have assigned ARIA roles and accessible labels so graphical shapes have proper semantics and textual representation. In the accessibility tree, these charts appear either as lists (like ul or ol elements) or tables (like the table element) so users can read and navigate the chart in a familiar way. While we have made SVG charts accessible, it is simpler and more robust to use semantic HTML directly. The shown HTML & CSS bar chart uses plain ol and li elements with built-in ARIA roles. Screen readers and other assistive tools read out the labels and values. Edge with JAWS on Windows: Chrome with VoiceOver on MacOS: Recap Today's websites feature responsive layout and fluid typography. Data visualizations should adapt these design techniques. While responsive and accessible SVGs are possible, they require manual client-side JavaScript logic. HTML and CSS allow us to create charts using declarative layouts and bidirectional positioning without computing positions and preventing overlap manually. We've demonstrated this for a bar chart. We've also created HTML, CSS and SVG hybrids where each technology does what it is good at. Building your next data visualizations At 9elements, we have been visualizing data for our clients for more than 10 years. In 2013, we developed GED VIZ for the Bertelsmann Foundation, visualizing global economic relations. From 2014 on, we developed the front-end and the chart rendering of the OECD Data Portal. In 2015, we contributed to the World Economic Forum Inclusive Growth Report. The bar charts described in this article are part of a long-term work for an international organization in the public health sector. Let us discuss how we can help you to explore, present and visualize the data of your organization or business! Contact us. Acknowledgements Thanks to my colleagues Nils Binder, Julian Laubstein and Matthias von Schmettow for this collaboration. Thanks to Vesa Piittinen for substantial feedback and many valuable ideas on how to improve and simplify the HTML and CSS. Please have a look at Vesa's version of the bar chart which demonstrates more clever optimizations. Thanks to the data visualization designers Alice Thudt, Christian Laesser and Moritz Stefaner for their stellar work on the Data Design Language. Thanks to the Devographics team behind the “State of HTML/CSS/JS“ surveys for the inspiration. Thanks to our client for the opportunity to work on ambitious data visualizations. MORE ABOUT MATHIAS SCHÄFER Mathias is a software developer with a focus on front-end web development and JavaScript web applications. He joined 9elements in 2007. Mathias Schäfer on LinkedIn Mathias Schäfer on X, formerly Twitter Mathias Schäfer on Github Mathias Schäfer on Mastodon",
    "commentLink": "https://news.ycombinator.com/item?id=40949021",
    "commentBody": "Responsive bar charts in HTML and CSS (9elements.com)120 points by sippndipp 22 hours agohidepastfavorite24 comments sings 20 hours agoI’ve been using a similar technique to display our poll data[1] for several years, although without using grid. If you can measure the text because you know which font will be used, and store the widths of various characters, you can take a little more control over layout too, even while rendering server-side. I also started writing a simple responsive SVG charting library[2], but the author is right in that there are some fluid layouts that are just impossible to realise with SVG at the moment. [1]: https://poll.lowyinstitute.org/report/2024/global-powers-and... [2]: https://stephenhutchings.github.io/shown/ reply chrismorgan 13 hours agoparent> If you can measure the text because you know which font will be used If you are using text, you can’t know which font will be used. There are no universally-available fonts, so web fonts are the closest you get, but they could fail to load for various reasons, and it’s honestly more common than people allow for. You can also block web fonts (e.g. uBlock Origin has an entire button for it), or just turn off font selection altogether, in Firefox via Settings → Fonts → Advanced → untick Allow pages to choose their own fonts, instead of your selections above. I did this four years ago as an experiment and have never gone back because it made the web so much better (I did switch back for one week a couple of years ago to convince myself the web really was that awful without it). Not many people will do this deliberately, but it is a thing. reply inbx0 6 hours agorootparentYou could embed your web font inline to the stylesheet, as base64 encoded data url.@font-face { src: url(data:application/font-woff2;charset=utf-8;base64,...) format('woff2'); } Your page loading times will obviously take a hit, but that'll give you pretty close to 100% certainty that the font will be used, should you want that. reply sings 12 hours agorootparentprevKnowing which font is used with a hundred percent certainty, maybe not. This probably is something more people should be aware of. But the failure mode might be some slightly awkward line breaks, assuming you allow for the edge case of alternative fonts. And even when the calculation isn’t a perfect representation of the rendered reality, the approximate width is still more useful for layout purposes than going in blind. Combined with em-based sizing I’ve found this to be pretty robust. Just because you can’t make it perfect for all users doesn't mean you can’t do the work to improve it for many. reply felixfbecker 15 hours agoprev> In SVG, all shapes are absolutely positioned. Text does not wrap automatically. This is not really true — you can position elements inside the SVG coordinate system using percentages and you can mix absolute coordinates and percentages. This allows you to have elements grow and shrink in reaction to width and height without distortion. Wrapping text is possible with , simply let HTML/CSS do the text layout wherever you need text within the SVG. However it is still true that you usually want to do a bunch of calculations in JS based on the width to know how many chart ticks you want, how many labels, etc. But that is pretty easy to compute with the helpers from D3. reply patze 20 hours agoprevMaking charts (of any kind) accessible is a really hard endeavour. I watched the demo videos and if I’m allowed to make a suggestion... Add the context to the data points. A simple “50k” might not cut it for people navigating the plot with their fingers over a smartphone. Full disclosure: I worked with those peeps a decade ago and really love their work. reply rikroots 7 hours agoparent> Making charts (of any kind) accessible is a really hard endeavour. Agreed! I've done a lot of experimenting with my canvas library to try and make accessibility a \"first class citizen\" in graphical representations, including charts. Things (I think) I've got right is to make it easier for people to use keyboard navigation to interrogate a chart, and making sure that graphical text gets properly copied into the DOM in an ordered way, and updates when the graphical text changes. The main failure I face is getting screen readers to recognise that the graphical text exists/updates. For example, this chart demo[1] is responsive, interactive and keyboard accessible but, when listening to that page with VoiceOver on a Mac, there's a clear failure to get the current data point from the screen to the listener. I know screen reader tech is the wildest of Wild Wests when it comes to front end. My only hope is that there's an easy solution to the issue, like I've badly misunderstood how screen readers work and the solution will be obvious when I stumble across the key errors I'm making ... and correct them! [1] London crime charts - https://scrawl-v8.rikweb.org.uk/demo/modules-001.html reply xnx 21 hours agoprevActual live example of said charts: https://codepen.io/molily/pen/JjqgxVR reply nipperkinfeet 16 hours agoprevWhat in the world? Horrible website. Text is zoomed so large on my 4k screen. reply KingOfCoders 10 hours agoprevLovely! reply green-eclipse 21 hours agoprevWhy are all these responsive charts displayed as images? Are they not proud of their work? reply I_am_tiberius 18 hours agoparentI think responsive refers to data, not to mouse/user interactions. reply locallost 12 hours agoprevI like the work, it looks good and (probably) works well, but there are some assumptions here that are off: > When the container size changes, for example due to a browser resize or orientation change, the JavaScript needs to compute all SVG positions and sizes from scratch. Assuming this takes 50-100ms per chart, a page with 20 charts freezes the browser for 1-2 seconds. I was once calculating positions for some complex labels on a timeline like chart (e.g. you have two entries close to each other and you would align the labels left and right to fit, but if there more you needed to stack them etc) and it did not take 50-100ms even for hundreds of entries. My code was neither pretty nor very efficient. Their 5-10 data points especially would not take that long. But even then you can always calculate them one by one and not block page interaction. In addition people don't really resize their browsers that much anymore, if they ever did. The majority of traffic is mobile and there even the occasional phone rotation is not really that common, especially not for reading articles. reply esafak 22 hours agoprevAn article about responsive bar charts without them? (Firefox) I'm not hip enough to find this funny. reply contravariant 21 hours agoparentWe live in an era where people will unironically tell you your article written in html/css might be a bit difficult to include on their website. reply swores 20 hours agorootparentThere's never been an era where that wasn't difficult for the majority of humans. What's changed isn't that becoming harder, it's that hosting websites which don't need any understanding of html/css has become hugely easier and therefore done by far more people, including less tech-savvy people. Somehow you're seeing that as a bad thing, focussing on the fact that the number of websites that are set up with the operator using html directly to create each page has decreased from 100%, but that's not actually a negative it's just the side effect of the positive changes! (Sure there are some specific situations where a person who would be capable of using raw html chooses to use a CMS or something that does restrict them, but it only restricts them if they don't care enough to choose one of the millions of simple options for not having that restriction.) Try to notice that your glass is more than half full, and that it's never been easier for anyone to create a website with the choice of manually writing hrml or not! reply zamadatix 19 hours agorootparentEasy page generators are definitely a good default which have enabled more people to create content (and not just recently, think myspace era and before) but the point wasn't \"and it was better when you had to write HTML/CSS manually to have a page\" rather \"and these platform tools have started to elide the option to embed HTML/CSS when you want\". This doesn't mean wanting to throw out the easy to use interfaces for a traditional CMS and writing everything out it means easy embeddability. As an example think mediawiki and markdown. Each was designed to allow you to just make content, each has powerful GUI tools, and each was designed to allow direct embedding as needed. A lot of blog platforms have started to drop the latter bit for no gain in making the former bit more accessible than it would be otherwise. reply Sn0wCoder 21 hours agoparentprevThe very first chart is a ‘bar’ chart running horizontally, not sure what’s missing (Firefox 127.0.2 64-bit). There are code examples down below…. reply itishappy 21 hours agorootparentIt's an image. None of the charts are responsive charts in HTML and CSS. I didn't even notice the CodePen at first because it doesn't load automatically (instead displays a cropped and zoomed image). reply Sn0wCoder 20 hours agorootparentYou are correct they are images besides the CodePen, I did not inspect them before commenting (hope the images is at least the output of the code). Guessing it must have something to do with the companies hosting platform tools and authoring code to it but have no idea why. reply kylebenzle 21 hours agoparentprevAre you saying the interavtive code snippies are not what you want? This article is great and I've needed something like this in the past. What do you mean that there are \"without charts?\" reply nhggfu 21 hours agoprev [–] \"While responsive and accessible SVGs are possible, they require manual client-side JavaScript logic\" an oxymoron, given that many assistive devices don't run JS [eg text-browsers like lynx] reply arp242 21 hours agoparentAlmost all commonly used assistive devices support JavaScript, and have for years. Lynx is not a commonly used assistive device. reply n2e 21 hours agoparentprev [–] “given that many assistive devices don't run JS [eg text-browsers like lynx]” an oxymoron, given that many assistive devices don’t cooperate with any of the web or HMI accessibility guidelines at all [eg my wheelchair] reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The post discusses creating responsive bar charts using HTML and CSS, addressing the limitations of SVG in responsive design.",
      "It highlights a hybrid approach combining HTML, CSS, and SVG, but emphasizes the benefits of using only HTML and CSS for simplicity and robustness.",
      "Accessibility is a key focus, with the use of semantic elements and ARIA roles to ensure screen reader compatibility."
    ],
    "commentSummary": [
      "Discussion centers on creating responsive bar charts using HTML and CSS, with various techniques and challenges highlighted.",
      "Key points include the difficulty of ensuring font consistency, the complexity of making charts accessible, and the limitations of SVG for fluid layouts.",
      "Some users noted that the charts in the article are images, not actual responsive HTML/CSS charts, sparking debate on the article's accuracy."
    ],
    "points": 120,
    "commentCount": 24,
    "retryCount": 0,
    "time": 1720815460
  },
  {
    "id": 40949034,
    "title": "\"GitHub\" Is Starting to Feel Like Legacy Software",
    "originLink": "https://www.mistys-internet.website/blog/blog/2024/07/12/github-is-starting-to-feel-like-legacy-software/",
    "originBody": "\"GitHub\" Is Starting to Feel Like Legacy Software Jul 12th, 2024 12:58 pm I’ve used a lot of tools over the years, which means I’ve seen a lot of tools hit a plateau. That’s not always a problem; sometimes something is just “done” and won’t need any changes. Often, though, it’s a sign of what’s coming. Every now and then, something will pull back out of it and start improving again, but it’s often an early sign of long-term decline. I can’t always tell if something’s just coasting along or if it’s actually started to get worse; it’s easy to be the boiling frog. That changes for me when something that really matters to me breaks. To me, one of GitHub’s killer power user features is its blame view. git blame on the commandline is useful but hard to read; it’s not the interface I reach for every day. GitHub’s web UI is not only convenient, but the ease by which I can click through to older versions of the blame view on a line by line basis is uniquely powerful. It’s one of those features that anchors me to a product: I stopped using offline graphical git clients because it was just that much nicer. The other day though, I tried to use the blame view on a large file and ran into an issue I don’t remember seeing before: I just couldn’t find the line of code I was searching for. I threw various keywords from that line into the browser’s command+F search box, and nothing came up. I was stumped until a moment later, while I was idly scrolling the page while doing the search again, and it finally found the line I was looking for. I realized what must have happened. I’d heard rumblings that GitHub’s in the middle of shipping a frontend rewrite in React, and I realized this must be it. The problem wasn’t that the line I wanted wasn’t on the page—it’s that the whole document wasn’t being rendered at once, so my browser’s builtin search bar just couldn’t find it. On a hunch, I tried disabling JavaScript entirely in the browser, and suddenly it started working again. GitHub is able to send a fully server-side rendered version of the page, which actually works like it should, but doesn’t do so unless JavaScript is completely unavailable. I’m hardly anti-JavaScript, and I’m not anti-React either. Any tool’s perfectly fine when used in the right place. The problem: this isn’t the right place, and what is to me personally a key feature suddenly doesn’t work right all the time anymore. This isn’t the only GitHub feature that’s felt subtly worse in the past few years—the once-industry-leading status page no longer reports minor availability issues in an even vaguely timely manner; Actions runs randomly drop network connections to GitHub’s own APIs; hitting the merge button sometimes scrolls the page to the wrong position—but this is the first moment where it really hit me that GitHub’s probably not going to get better again from here. The corporate branding, the new “AI-powered developer platform” slogan, makes it clear that what I think of as “GitHub”—the traditional website, what are to me the core features—simply isn’t Microsoft’s priority at this point in time. I know many talented people at GitHub who care, but the company’s priorities just don’t seem to value what I value about the service. This isn’t an anti-AI statement so much as a recognition that the tool I still need to use every day is past its prime. Copilot isn’t navigating the website for me, replacing my need to the website as it exists today. I’ve had tools hit this phase of decline and turn it around, but I’m not optimistic. It’s still plenty usable now, and probably will be for some years to come, but I’ll want to know what other options I have now rather than when things get worse than this. And in the meantime, well… I still need to use GitHub everyday, but maybe it’s time to start exploring new platforms—and find a good local blame tool that works as well as the GitHub web interface used to. (Got a fave? Send it to me at misty@digipres.club / @cdrom.ca. Please!) Posted by Misty De Meo Jul 12th, 2024 12:58 pm Tweet « The Working Archivist's Guide to Enthusiast CD-ROM Archiving Tools",
    "commentLink": "https://news.ycombinator.com/item?id=40949034",
    "commentBody": "\"GitHub\" Is Starting to Feel Like Legacy Software (mistys-internet.website)116 points by zdw 22 hours agohidepastfavorite100 comments kylehotchkiss 22 hours agoIt's really nice to have reliable tools that you can learn once and continue using a decade to free your mind up to master other parts of the job. (like the CSS in JS flavor of the week) Same thing seems to apply to Visual Studio Code. Sure they have a big changelog, but the actual experience of using it feels very much like when it first released. reply rurp 22 hours agoparent> It's really nice to have reliable tools that you can learn once and continue using a decade to free your mind up to master other parts of the job. I strongly agree, and it's weird how much hostility I encounter from folks who think the opposite. There are an endless amount of interesting and/or useful things to learn in this industry; spending time relearning how to do a mundane task in an app I've been using for a decade is a waste. reply tivert 14 hours agorootparent> There are an endless amount of interesting and/or useful things to learn in this industry; spending time relearning how to do a mundane task in an app I've been using for a decade is a waste. It's not just \"in this industry,\" the problem is a societal one. It just goes to show you how unwise some people are. They like to \"learn new things,\" so proceed to relearn the same thing over and over. They got brainwashed by the \"new is better\" meme. reply onionisafruit 22 hours agoparentprevI agree, and I wish GitHub could continue to be one of those instead of what ms is turning it into. reply theamk 22 hours agoparentprevThat's what the the post's author wants, but apparently Microsoft is breaking github instead. I am sure Github will be around in the decade, but I am not sure at all it will still be everyone's default location to host source code. Which is a pity. reply shrimp_emoji 20 hours agorootparentA pity? Uh, no. Please, move the network effects to a platform not owned by Microsoft and their shitty AI venture so my code can follow suit, for the love of god, HURRY! reply bottlepalm 22 hours agoparentprevDevelopers, developers, developers! reply juped 22 hours agorootparentI also miss Ballmer reply k8sToGo 22 hours agorootparentor do you miss the cocaine? reply ssl-3 22 hours agorootparentI also miss cocaine reply wenc 13 hours agoparentprevVS Code is the emacs that emacs never was. It has great extensions, lets you work on remote machines as if they were local, and supports language servers, notebook interfaces, and all manners of code intelligence. reply beepbooptheory 5 hours agorootparenthttps://youtu.be/WO23WBji_Z0?si=ZBchB1nvqOe42snu reply scottbez1 22 hours agoprevThe thing that's really legacy in GitHub is the code review flow, which is likely one of the surface areas that corporate customers use the most. Actions and Codespaces have been huge and transformative, so it's disappointing that core functionality like code reviews hasn't seen the same level of improvement. Basic features that Phabricator had nearly a decade ago are still missing: - highlighting copied/moved code blocks - gutter indicators for code coverage - stacked diffs - not collapsing large changes by default (what a ridiculous default! \"A lot of code has changed here, meaning it's likely bugs may be here - I'll hide that for you to make your review easier!\") And I don't recall how well Phabricator supported it, but handling rebases sanely by carrying over comments and showing diffs from prior PR versions across rebases would be amazing. The number of times I have to re-review an entire PR in GitHub because it can't show what changed since the last review if the author rebases their branch... reply piotrkaminski 21 hours agoparentTo be fair, they did improve several things in the PR flow over the last decade but there's definitely a lot more that could be done. I just hope they keep their API team well-resourced so that code review tools like Graphite and Reviewable can keep working! (Disclaimer: I'm the founder of Reviewable.) reply atmavatar 19 hours agorootparentThe least they can do is support pull requests using fast-forward merges. See: https://github.com/orgs/community/discussions/4618 reply codeapprove 11 hours agoparentprevThis is 100% true. For some reason GitHub has put very little love into code review even though the Pull Request is probably their most important flow. The good news is that many people have built better code review interfaces on top of GitHub. My favorites: - CodeApprove (I created it, so yeah I like it) - Graphite - Reviewable - GitContext Check them out! You’d be surprised how much better they are and how quick they are to set up. reply habosa 22 hours agoprevGitHub’s most important product at this point is probably the API. It’s incredibly comprehensive so it’s easy for tools to augment or replace pretty much any part of the experience. So many teams with private repos only use GitHub for the basics (version control, access management) and then use third party compatible tools for the rest. Linear for issues, Buildkite for CI, Graphite for code review, etc. The API allowed me to build CodeApprove (codeapprove.com), which is the code review interface I always wanted GitHub to have. And teams that share my preferences can use it a la carte without losing the rock solid foundation of GitHub. I know it’s not great that we’re all locked into this platform that is owned by a megacorp that hasn’t always been friendly, but it’s at least a great platform. reply piotrkaminski 21 hours agoparentAgreed that their API is the most important thing, but IMHO they're not doing a particularly great job of it. You've got two separate API systems (REST and GraphQL) that overlap but with neither a superset of the other, two separate ID systems, no help in handling the wide range of incompatible API versions used by old GHE instances in the field, OAuth apps vs Apps that have different constraints on the APIs they're allowed to call and with no clear migration path between them, important bits of functionality used by their own UI that aren't available through the API at all, a convoluted API quota system, etc. Frankly, a lot of it feels like they were trying to deprecate and replace an older system with the shiny new thing, then realized halfway through that they couldn't and now we're stuck with two somewhat incompatible ones for the foreseeable future. They could really use a strong tech lead on this. reply doctor_eval 21 hours agorootparentIt feels this way with PATs as well, with legacy PATs still necessary for most of the things I use them for, despite being pushed into the newer ones. And the documentation is terrible. I had the exact same thought, the PAT transition feels like an unfinished feature as does the API transition. For me the API transition is even more bizarre since it’s almost trivial to wrap REST calls with GraphQL. What it feels like to me is a mounting level of serious technical debt which isn’t being addressed, and if that’s not a sign of trouble in a product like GitHub, I don’t know what is. reply Ayesh 13 hours agorootparentThe only reason holding me from using granular PATs is that they _must_ have an expiration date and the maximum is one year. reply doctor_eval 12 hours agorootparentYeah - I understand why they do this but I reckon they could have made them renewable without having to replace the tokens themselves. Also granular PATs still don’t work everywhere. reply zx8080 16 hours agorootparentprevWhat is PAT? reply abraham 14 hours agorootparenthttps://docs.github.com/en/authentication/keeping-your-accou... reply EstanislaoStan 14 hours agorootparentprevPersonal Access Token reply codingdave 22 hours agoprevThe complaint in the article is legit, and it sounds completely plausible that is it due to re-write to React, as avoiding rendering of off-screen content is fairly standard practice in React, as a performance optimization. But that really is about the management of the re-write, not the tech. Someone at github decided to not render off-screen content. That breaks expectations compared to how it worked before, and that is why it feels like the app is going downhill. The bigger point being that if anyone is doing a re-write, and trying to match the existing UX... that needs to be a deeper match than just getting the pixels in the right place. reply marcosdumay 22 hours agoparentIt's due to the people responsible for the tech having no idea how people want to use the tech. What means you can expect it continuing downhill until somebody higher up acts. reply rurp 22 hours agoprevI remember when Jira introduced this exact same text rendering regression. The Kanban board we used at work was hardly packed, it usually less than two dozen tickets on it. I hit ctrl+f to jump to the ticket number I new I wanted and... nothing. The browser couldn't find it! Turned out that the ticket was slightly below the view port and rendering 20 tickets at once is apparently too much for a modern ticketing web app. The amount of regressions in modern frameworks can be staggering at times. Personally I would be embarrassed to work on a well resourced project that could handle such basic tasks, but I'm sure the redesign that introduced this regression was done with all sorts of buzzy frameworks and patterns that look great on a resume. reply mewpmewp2 22 hours agoparentI feel like ctrl/cmd f has taken a whole hit in general throughout the whole web. It used to be this really reliable tool that got you quickest where you wanted to go to. reply gkoberger 22 hours agoprevI've heard this opinion echoed a lot (esp on Twitter), but I don't agree with it at all. I think Github is still an amazing piece of software, and Microsoft has invested in it more than almost any recent acquisition from any parent company. This feels like a quite dramatic framing for a bug that popped up in a very complex piece of software. reply Sohcahtoa82 22 hours agoparentIt's not a bug, though. Like many web sites these days, they don't load the entire page upon navigation as a deliberate design choice to save bandwidth and resources of the client browser. I've been bitten by this one before as well. Looking at the files tab of a pull request, I hit ctrl-f and started typing the name of a file I knew was part of the PR, and...nothing. No hits. Couldn't find it. It wasn't until I scrolled down a little bit that the JS on the page loaded more of the diff and THEN the file I was looking for appeared in the list and could then be clicked to scroll to that file. This kind of lazy loading saves system resources for both client and server, but reduces usability. reply yencabulator 20 hours agorootparent> as a deliberate design choice to save bandwidth and resources of the client browser. The complete source file is likely a tiny fraction of the size of the React gunk to show it. It's more they're trying to limit DOM size than download size, with a virtual view. And, unfortunately, HTML virtual views break things like find-in-page. reply itsdrewmiller 22 hours agorootparentprevThis pattern is so frustrating - especially when they capture the hotkey for find in page but not until js has fully loaded. Makes me wonder if exposing the browser find events to the page could help - handle the find event by ensuring the whole dom is loaded and searchable. reply smitty1e 13 hours agorootparentprevEnhancements are fine, but if one is so sophisticated that the basics are forgotten, then what good are the enhancements? It's not always the most scalable approach, but I like the DTTCPW \"dit-ka-pow\": the Dumbest Thing That Could Possibly Work. reply arp242 21 hours agoparentprevI feel \"Github is still an amazing piece of software\" and \"GitHub has annoying bugs it really shouldn't have\" are not really conflicting opinions; they can both be true at the same time. I use GitHub because it's still the best option, by quite some margin.[1] But I also think it has stupid bugs (that sometimes take ages to fix), some misdesigns, and at times complete misfeatures. For example if you have a light colour scheme the GitHub actions log is shown as fairly low-contrast dark. This is quite unreadable for me; there's a reason I use the light theme. So I need to open the 'raw log' and check it from there. Is this doable? Yes. Do I absolutely hate having to use GitHub Actions because it's just such an annoying workflow? Also yes. GitHub discussions is borderline useless. Try commenting on a larger thread; you'll struggle finding your own comment and seeing if anyone replied. Making a conversation UI that's more confusing and worse than Twitter is quite the achievement, but GitHub pulled it off. Some of this is just mystifying; these are not new features, and pretty low-hanging fruit. Just like how the back button behaviour was just broken on issues for like half year or longer until they finally fixed it. [1]: I haven't evaluated Gogs/gitea/Forgejo in a while though. Also the entire double fork thing is somewhat unfortunate. reply huevosabio 22 hours agoparentprev100% agree. Absolutely amazing platform. In fact, surprised it has remained high-quality even past acquisition and overwhelming adoption. I would expect scope creep to grow unboundedly. reply dmix 22 hours agoprevGitHub is moving to react for the web UI? Sad, Rails offers ways to deliver async updating views with websockets ala Elixir Liveview. I’ve pretty much abandoned Vue after discovering how much easier it is to maintain by keeping as much as possible in Rails. Not just myself but the whole team works quicker that way, backend people often know enough frontend but might not know the depths of Vue/React so it creates delays by distributing simple tasks. Plus the whole mass duplication of logic (permissions, models, etc). You’d need to some pretty serious “desktop style” (hate that phrase) UI complexity to demand React and I’m not fully seeing where GitHub needs that. Maybe they want to make GitHub like VSCode in the browser? reply dmd 22 hours agoparent> Maybe they want to make GitHub like VSCode in the browser? It already is. Hit the “.” key in any repo and it literally opens the repo in VS Code in your browser. reply arp242 21 hours agoprevBefore the Microsoft take-over GitHub was fairly stagnant for quite a few years. Remember not being able to meaningfully copy code because it would also copy the line numbers? Kind of ridiculous that took such a long time to fix. But for the last few years ... yeah, I feel it's just getting slowly more annoying. I sometimes genuinely wonder if the people working on GitHub actually use it themselves. I've tried reporting some really obvious and annoying bugs to GitHub, and they even never got fixed, took months to get fixed, or got fixed and then broke again a while later. I stopped bothering. GitLab is still tons worse, so there's that. As for React, unfortunately there is a contingent of frontend developers who seem to think that everything must be React, that React is the only possible answer, and everyone not using React is a decrepit developer and probably a decrepit human being and quite possibly a registered nonce, and will generally just forever bang on about React until the entire world has converted to React. Your frontend will be assimilated. React is not the only tech where this is a problem, but it is probably among the more frustrating because as a user I don't really care what language or database you use as I typically don't really interface with it, but I do interface directly with your frontend. reply shrimp_emoji 20 hours agoparentTIL React devs are GNOME devs reply arp242 19 hours agorootparentJust stop it with these zero-effort pointless zero-content off-topic swipes. This is explicitly not allowed here. reply londons_explore 22 hours agoprevPlease don't say this! They'll go and 'modernize' it, making it some nasty 10x slower and more bloated webUI, and move all the buttons and URLs about just for the sake of it. reply efilife 22 hours agoparentThis already happened to some extent. Repo files and the sidebar are loaded client side and are slow reply cosmojg 22 hours agoparentprevI'd argue that many of the problems articulated in the article are a result of GitHub's recentish \"modernization\" switch to React. reply marcosdumay 22 hours agoparentprevThe article is complaining exactly about 'modernization'. reply pkilgore 22 hours agoparentprevYou might want to read the article a bit closer: It's complaining about exactly that. reply mardifoufs 20 hours agoparentprevIt's a rails app right now, so I'd be surprised if anything else would be slower lol. Except if they really screw up the rewrite or something, but tech wise almost anything else would be faster. reply aidos 22 hours agoprevI find GitHub to broadly be ok. One exception I have is for code reviews, arguably its most important usecase. Once there’s a little back and forth going on and a few rounds of things being fixed it becomes really hard to find a list of the comments. Maybe I’m just missing it somewhere but spread in the conversation tab it’s impossible to find things. I’ve resorted to a couple of scripts that pull all the threads down into a file I can open in vim. Then I have a shortcut for jumping to the corresponding location in the code. Works well, but seems really unnecessary. reply lucasoshiro 22 hours agoprev> and find a good local blame tool that works as well as the GitHub web interface used to I don't exactly what you want, but if you want a better look of git blame, git show, git diff and so on I suggest delta. Delta formats the output of those git commands so you can see their output in prettier way. You can also use it as a human-readable diff replacement. More info: https://dandavison.github.io/delta/ However, even though git blame (and its graphical interfaces) is a quick and easy tool, it has some problems, and Git has better tools for code archaeology. Check this for more info: https://news.ycombinator.com/item?id=39877637 reply geoka9 21 hours agoparentAlso, if you are an Emacs user, its VC package is very powerful. For example, `C-x v g` to bring up the \"blame\" (vc-annotate) view, in which you can (for any commit) press `a` to see what the file looked like before that commit, `d` to see the diff of the commit, `l` to see the commit message... That's just the tip of the iceberg of things you can do with it. It supports Git, Mercurial, Bazaar, CVS, Subversion and more as a backend. https://www.emacswiki.org/emacs/VersionControl reply lucasoshiro 16 hours agorootparentThanks! I'll try it reply zbowling 22 hours agoprevAs someone who used GitHub for years (2009-2014), got acquired into a big tech company for 8 years, so I didn't get to use it much, and then got to go back to a startup that used it again, it feels like a totally different product. It's a starkly different offering from that perspective. GitHub Actions are a major part of that. Codespaces, a solution for Mac CI, that you automate so much, store secrets, drive not CI but CD straight from GitHub now, etc. It's evolved a ton. Then you have the other things they are doing as company with copilot and vscode (and formerly atom). Sure, code search is only moderately better, and maybe some of the core git features aren't getting as much razzle-dazzle in the UI, but it's evolved a ton. reply londons_explore 22 hours agoparentCode search is the one bit Id like to see improved. Just hire someone who worked for Google and copy their internal code search. (Preview available here: https://source.chromium.org/) Technical caveat: To work effectively, it needs to be able to compile (parse into an AST) the code, which for many GitHub projects is challenging. I'd bet they could easily get to 80% compilation rate though and then use dumb text search on the rest. reply zzo38computer 21 hours agoprevDisabling JavaScripts does not always work for all files, for some reason; I don't know why. Some files can be displayed even though JavaScripts are disabled, but some don't work. Some files will only partially work if JavaScripts are disabled. However, even if it doesn't work, the server will send the JSON as a part of the HTML file, which contains all of the relevant data, so I wrote my own script which is much shorter and much faster than those in GitHub, which also avoids needing extra requests to download the scripts. Also, it does support the git protocol so that can still be used, and there is also the API and fortunately it has good documentation, and that does work without JavaScripts (documentation should always be made to work without JavaScripts; if your web site has documentation and does not work without JavaScripts, please correct that, even if there are valid reasons why the other files might not work without JavaScripts (which is uncommon, because usually it is for no good reason)). Still, it isn't very good that they have to do that; they should allow to design to work without such a mess. (How they describe in this article, it does have the problems described there and more; this is because of the messy of WWW, but they could avoid using much of the newer stuff, and make it more compatible and improve accessibility and avoid many problems where they need to add considerations that should not even be necessary to consider if it was designed properly (since then the client would be able to handle it automatically according to the user preferences, without needing to be told by the server). reply krackers 21 hours agoparentCan you share your script? reply fosterfriends 22 hours agoprevThis feeling, plus their lack of stacking support, is what motivated Tomas, Merrill, and me to hack on graphite.dev for years reply msephton 7 hours agoprevI'm watching one important QOL feature request on the desktop app that has gone unaddressed for 6 years. They just don't care about their user's experience, just their own priorities and roadmap. It's like an abusive relationship. reply ergonaught 22 hours agoprevI had a moment recently where I was trying to understand \"why developers would create such an interface\" in an application I was using when I realized I was looking at, basically, a \"React-type interface\" in a place where that doesn't fit, and the \"why\" is perhaps that this is the kind of thing younger developers are used to seeing/implementing, so they probably don't even recognize when it's a bad approach. reply alberth 22 hours agoprev> “Feel Like Legacy Software” Is a bit ironic to state given their blog design is reminiscent of OS X ‘brushed metal’ theme from 24-years ago https://en.m.wikipedia.org/wiki/Aqua_(user_interface)#/media... reply ghostpepper 22 hours agoparentPlenty of software from that vintage works better than modern equivalents. UX is about more than how something looks visually - equally (or more) important is how it behaves, how the user interacts with it, and how it gets out of your way and lets you do your actual work (or doesn't) reply alberth 22 hours agorootparentI don't disagree. But it's the author who is implying that \"legacy = bad\" Note: I genuinely love Windows 2000 UI/UX (and wish it still existed today) reply peterhadlaw 22 hours agoparentprevI think it's a perfect little detail that captures this writer's persona. Their blog is powered by the (once famed, and very popular) Octopress [0]. Every developer was running their own blog off of it. My point is, it looks like the author found something that works... for over a decade and well. They wish GitHub would 'just work' 0: http://octopress.org/ reply empiko 22 hours agoparentprevI don't think that the design is that old. I was working as a web designed ~11 years back and a lot of these design ideas were \"in\" back then. The subtle dark and light 1px lines that create this embedded effect, some shadows around certain elements, not a completely trivial typography. This is a 2010-2014 type of design. reply ronsor 21 hours agoparentprevThe blog is literally built using unsupported software (Octopress). I don't think it's been updated since 2016. reply moogly 22 hours agoprevGitHub's notifications are the worst. Completely useless; nay, infuriating. That's the first thing they should improve IMO. Whilst I do get the _motivation_ behind collapsing large files in diffs, because you want to keep the number of DOM elements low, come on... you really _cannot_ hide the lion's share of changes in the PR and get away with it. At least make the collapsed files stand out more so you don't actually miss them when reviewing. Or, I guess you didn't want me to expand them, or you did all that collapsing effort for no reason. So what was the point again? Org gists were proposed like 10 years ago, but still not in place. Seems straight forward enough... reply ChrisMarshallNY 22 hours agoprevIt actually sounds like a bug. In the apps I'm writing, when someone reports something like that, I consider it a \"bug,\" and try to address it. > I’ve seen a lot of tools hit a plateau. I consider that the ideal state, for development platforms. Constant tweaking can cause really big problems (which actually sounds like what happened, here). I have had Xcode fart on me, in a big way. reply zx8080 16 hours agoprevSame issue (lazy loading) exists with Gitlab also. And also MR pages are so overloaded (with scripts?) so it loads and renders forever for any seriously large (yes, it happens with a large codebase) changeset. reply jamesgpearce 22 hours agoprevA while back I started on an experiment to build a local-first GitHub client: https://github.com/tinyplex/tinyhub It certainly _seems_ like the GitHub interface should feel more like an 'app', and less a 'site'. Or just me? reply creesch 22 hours agoparent> Or just me? I am just a data set of one, but I disagree. Github the website has worked pretty great for well over a decade at this point. As the author of the post says, it is more recently that some cracks have started to show. reply marcosdumay 21 hours agoparentprevIt's main use is for browsing documents. It's not so much that it isn't an app, but that many of its goals should be similar to browsers. reply Borg3 22 hours agoprevYeah, im not a big fun of GitHub (especially after Microsoft aquasition), but I also hate that stupid 'we need to grow', 'we need to expand' attitude. Sometimes, tool is done and there is really not much you can improve upon. Especially from UI stand point of view. If you start to force that improvement, usually you end up with over-engineered stuff that starts to be pain to use. Not always legacy stuff means it bad or should be avoided. Actually, most of the time its kinda other way around. It means its stable and just helps you get the job done. The balance between legacy and progress is not that obvious imo. reply klabb3 22 hours agoprevI feel like the post has a narrow focus. Github is solving a quite heterogeneous problem for a diverse audience. People use Github and Git in completely different ways; so they have a spacebar heating issue. Their blame view, which half the post is about, I've rarely ever used. Likewise, some teams rebase during code review, others handle review outside, etc etc. As for the Ctrl+F issue, I could see that rendering giant files with syntax highlighting can tank performance on older computers, and that they're trying to fix it for people who use Github (on web - remember), thinking that if you need full greppability, you might clone the repo locally. I don't know, maybe Github is screwing everything up behind the scenes, but this post doesn't make a compelling argument in my view. reply peterhadlaw 22 hours agoprevI haven't seen an octopress site in a hot minute. I miss octopress and the dev culture of writing that came with it, personal website. Shared template based on max writing speed and efficiency. reply fluoridation 22 hours agoprevDoes anyone know of any client-side blame front-ends for Git? I used to use the one from Git Extensions, but since an indeterminate amount of time ago, it just stopped working for me. reply _ikke_ 22 hours agoparentIf a TUI is an option, then `tig blame` is a good interface. https://github.com/jonas/tig reply cratermoon 22 hours agorootparent\"git gui blame\" is pretty good, too. and git-gui comes with git. reply onel 6 hours agoparentprevI'm a big fan of Sublime Merge https://www.sublimemerge.com/ reply lucasoshiro 22 hours agoparentprevI think most modern IDEs and text editors have at least one. The ones that I remember: - Annotate for JetBrains - GitLens for VSCode - Magit for Emacs reply Pathogen-David 22 hours agoparentprev> but since an indeterminate amount of time ago, it just stopped working for me. Updating both Git and Git Extensions has seemingly fixed it for me, but the issues I was having were sporadic so maybe I've just been lucky reply moogly 22 hours agoparentprevThe one in JetBrains' IDEs is pretty good. \"Annotate previous revision\" is very helpful to step through it in time. reply sideshowb 22 hours agoparentprevTortoisegit? reply 2o3895j23o45j 22 hours agoprevI am increasingly anti-react. It's infuriating that on a daily basis, multiple times per day all over the place, I'm dealing with UI lag problems. I'm tired of buttons moving between the time my finger starts to move and the time when the mouse click registers, or the finger tap on a touchscreen registers, because one module loaded ten seconds after all the rest. reply rafram 22 hours agoprev> The problem wasn’t that the line I wanted wasn’t on the page—it’s that the whole document wasn’t being rendered at once, so my browser’s builtin search bar just couldn’t find it. Ctrl/Cmd-F is intercepted on blame/code pages and opens a custom find UI, which honestly seems fine to me. Browser find-in-page isn't meant for code search and it's pretty slow. Maybe GitHub is switching to React (I don't know anything about that), but React doesn't mandate, or even natively support, culling offscreen content, so I'm not sure what the connection is there. reply xmprt 22 hours agoparentI hate that content is rendered so inefficiently today that the whole document can't be rendered at once and breaks the browser builtin search. It feels like such a regression in modern software. reply yongjik 22 hours agorootparentSlack does this and I absolutely hate it. It's impossible to search for a keyword in a long thread. The native app will simply hijack the search function, and even opening it on a browser won't work because it only renders the part that's on the screen. I'm sure someone felt really clever when they implemented it. reply _heimdall 22 hours agorootparentprevIt really is worth considering the web an OS or an app platform these days, given that so many sites are web apps rather than web pages. reply sideshowb 22 hours agorootparentprevIf only there was an established tool for scrolling up and down a document that didn't fit on screen Wait reply creesch 22 hours agoparentprev> so I'm not sure what the connection is there. I don't think they are blaming react specifically. It is more that for them, it dawned that it might due to a rewrite when they remembered reading the front-end was rewritten in react. reply buovjaga 22 hours agoprevFor a blame view, OpenGrok has an \"Annotate\" feature. reply dgeiser13 18 hours agoprevI wish all software felt like legacy software. reply noahlt 22 hours agoprevI honestly cannot tell whether this blog is trolling, but the comments here suggest they are not. Am I the only one who is satisfied running `git blame` in my terminal, using `less` as the pager and searching by pressing `/`? reply roryokane 11 hours agoparent`git blame` lets you identify the commit that changed the line you’re looking for, but doesn’t make it easy to then view further details. The author might prefer GitHub’s blame view because it has these features: • each hunk has a link to the commit that changed it, as opposed to needing to copy a line’s SHA and then run a new `git show …` command • each hunk has a link to view the `blame` as of that older commit, as opposed to needing to copy a line’s SHA and then run a new `git blame … path/to/same/file` command • the code is syntax highlighted by default, without you needing to configure your local Git install to use https://github.com/dandavison/delta These features lead to a better experience than `git blame`. Various IDEs, editor plugins, TUIs, and GUIs provide similar features. reply ilrwbwrkhv 22 hours agoprevGitlab has been much faster than Github for a while now. reply smallerfish 22 hours agoparentGitlab is genuinely better in many respects (aside from their rat's nest of settings menus). For example, if you merge the middle branch in a stack of branches, gitlab will retarget the higher branches. Github just orphans them. reply chrisweekly 22 hours agoparentprevInteresting. They felt subjectively comparable to me in 2021-22 when I last used gitlab. Do you have any benchmarks or other source? reply syndicatedjelly 22 hours agoprevGitHub is slowly prioritizing social features over developer tools and experience. The number of devs concerned with GitHub Stars these days is a little concerning. I feel like at least once a week, I see a post on HN asking for ways to boost their stars on their GitHub repos. reply kristopolous 22 hours agoparentThey can apparently be purchased like metrics on any other platform. People have used them in cons whereby they clone a legitimate project, add a backdoor, then buy the stars and SEO it to get people to clone the bad repo reply whalesalad 22 hours agoprev [–] Well, it is. It's a big ass antique rails app that tries to be fast with turbolinks. reply rubiquity 22 hours agoparent [–] It being a Rails app isn't a problem. A whole bunch of noise and features that have nothing to do with collaboration or managing the versioning of code is the problem. reply whalesalad 22 hours agorootparent [–] I like rails myself, not knocking that. Prob well past due for a SPA like experience though. So much of the UI is so antiquated. The fact that very basic things in a PR are hidden beneath a disclosure menu when there is more than enough room to put it in-line without the menu is frustrating. Like simply editing the description of a PR requires 2 clicks when it could easily be 1. Everthing feels bolted-on or shoehorned in. Like marking a PR as a draft ... it took me forever to find that the first time. reply rubiquity 22 hours agorootparent [–] There isn't any correlation between the quality of product design and how the bits arrive at your browser. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "GitHub is starting to feel outdated, with key features like the blame view experiencing issues due to a frontend rewrite in React.",
      "The rewrite caused problems with the browser's search function, which can be temporarily fixed by disabling JavaScript to reveal a server-side rendered page.",
      "Other features such as GitHub’s status page, Actions, and merge button have also degraded, and Microsoft's focus on AI over core features suggests these issues may persist."
    ],
    "commentSummary": [
      "Users are expressing frustration over GitHub's stagnant features and lack of significant improvements in core functionalities like code review flow.",
      "Some attribute these issues to Microsoft's influence and the switch to React, despite new features like Actions and Codespaces.",
      "There is a growing call for better user experience and functionality, with users suggesting alternatives and improvements."
    ],
    "points": 116,
    "commentCount": 100,
    "retryCount": 0,
    "time": 1720815561
  },
  {
    "id": 40952182,
    "title": "gpu.cpp: A lightweight library for portable low-level GPU computation",
    "originLink": "https://github.com/AnswerDotAI/gpu.cpp",
    "originBody": "gpu.cpp gpu.cpp is a lightweight library that makes portable GPU compute with C++ simple. It focuses on general purpose native GPU computation, leveraging the WebGPU specification as a portable low-level GPU interface. This means we can drop in GPU code in C++ projects and have it run on Nvidia, Intel, AMD, and other GPUs. The same C++ code can work on a wide variety of laptops, workstations, mobile devices or virtually any hardware with Vulkan, Metal, or DirectX support. Technical Objectives: Lightweight, Fast Iteration, and Low Boilerplate With gpu.cpp we want to enable a high-leverage library for individual developers and researchers to incorporate GPU computation into programs relying on nothing more than a standard C++ compiler as tooling. Our goals are: High power-to-weight ratio API: Provide the smallest API surface area that can cover the full range of GPU compute needs. Fast compile/run cycles: Ensure projects can build nearly instantaneously, compile/run cycles should be#include#include#include \"gpu.h\" using namespace gpu; // createContext, createTensor, createKernel, // createShader, dispatchKernel, wait, toCPU // Bindings, Tensor, Kernel, Context, Shape, kf32 static const char *kGelu = R\"( const GELU_SCALING_FACTOR: f32 = 0.7978845608028654; // sqrt(2.0 / PI) @group(0) @binding(0) var inp: array; @group(0) @binding(1) var out: array; @compute @workgroup_size({{workgroupSize}}) fn main( @builtin(global_invocation_id) GlobalInvocationID: vec3) { let i: u32 = GlobalInvocationID.x; if (i10.0); } } )\"; int main(int argc, char **argv) { Context ctx = createContext(); static constexpr size_t N = 10000; std::array inputArr, outputArr; for (int i = 0; i (i) / 10.0; // dummy input data } Tensor input = createTensor(ctx, Shape{N}, kf32, inputArr.data()); Tensor output = createTensor(ctx, Shape{N}, kf32); std::promise promise; std::future future = promise.get_future(); Kernel op = createKernel(ctx, createShader(kGelu, /* 1-D workgroup size */ 256, kf32), Bindings{input, output}, /* number of workgroups */ {cdiv(N, 256), 1, 1}); dispatchKernel(ctx, op, promise); wait(ctx, future); toCPU(ctx, output, outputArr.data(), sizeof(outputArr)); for (int i = 0; i < 16; ++i) { printf(\" gelu(%.2f) = %.2f\", inputArr[i], outputArr[i]); } return 0; } Here we see the GPU code is quoted in a domain specific language called WGSL (WebGPU Shading Language). In a larger project, you might store this code in a separate file to be loaded at runtime (see examples/shadertui for a demonstration of live WGSL code re-loading). The CPU code in main() sets up the host coordination for the GPU computation. We can think of the use of gpu.cpp library as a collection of GPU nouns and verbs. The \"nouns\" are GPU resources modeled by the type definitions of the library and the \"verbs\" actions on GPU resources, modeled by the functions of the library. The ahead-of-time resource acquisition functions are prefaced with create*, such as: createContext() - constructs a reference to the GPU device context (Context). createTensor() - acquires a contiguous buffer on the GPU (Tensor). createShader() - constructs WGSL code string to run on the GPU) (ShaderCode) createKernel() - constructs a handle to resources for the GPU computation (Kernel), which combines bindings to GPU buffers from createTensor() with the computation definition from createShader(). These resource acquisition functions are tied to resource types for interacting with the GPU: Context - a handle to the state of resources for interacting with the GPU device. Tensor - a buffer of data on the GPU. ShaderCode - the code for a shader program that can be dispatched to the GPU. This is a thin wrapper around a WGSL string but also includes the workgroup size the code is designed to run with. Kernel - a GPU program that can be dispatched to the GPU. This accepts a ShaderCode and a list of Tensor resources to bind for the dispatch computation. This takes an argument Bindings that is a list of Tensor instances and should map the bindings declared at the top of the WGSL code. In this example there's two bindings corresponding to the input buffer on the GPU and the ouptut buffer on the GPU. In this example, the GELU computation is performed only once and the program immediately exits so preparing resources and dispatch are side-by-side. Other examples in the examples/ directory illustrate how resource acquisition is prepared ahead of time and dispatch occurs in the hot path like a render, model inference, or simulation loop. Besides the create* resource acquisition functions, there are a few more \"verbs\" in the gpu.cpp library for handling dispatching execution to the GPU and data movement: dispatchKernel() - dispatches a Kernel to the GPU for computation. This is an asynchronous operation that returns immediately. wait() - blocks until the GPU computation is complete. This is a standard C++ future/promise pattern. toCPU() - moves data from the GPU to the CPU. This is a synchronous operation that blocks until the data is copied. toGPU() - moves data from the CPU to the GPU. This is a synchronous operation that blocks until the data is copied. In this particular example, toGPU() is not used because there's only one data movement from CPU to GPU in the program and that happens when the createTensor() function is called. This example is available in examples/hello_world/run.cpp. Other Examples: Matrix Multiplication, Physics Sim, and SDF Rendering You can explore the example projects in examples/ which illustrate how to use gpu.cpp as a library. After you have run make in the top-level directory which retrieves the prebuilt Dawn shared library, you can run each example by navigating to its directory and running make from the example's directory. An example of tiled matrix multiplication is in examples/matmul. This implements a WebGPU version of the first few kernels of Simon Boehm's How to Optimize a CUDA Matmul Kernel for cuBLAS-like Performance: a Worklog post. It is only weakly optimized (up to 1D blocktiling, kernel number 4) at ~ 1.2+ TFLOPs on a Macbook Pro M1 laptop, which has a theoretical peak of 10.4 TFLOPs. Contributions to optimize this further are welcome - kernels 5-9 of Simon's post would be a natural starting point. A parallel physics simulation of an ensemble of double pendulums simulated in parallel with different initial conditions on the GPU is shown in examples/physics. We also show some examples of signed distance function computations, rendered in the terminal as ascii. A 3D SDF of spheres is shown in examples/render and a shadertoy-like live-reloading example is in examples/shadertui. Interestingly, with a starting example, LLMs such as Claude 3.5 Sonnet can be quite capable at writing low-level WGSL code for you - the other shaders in the shadertui example are written by the LLM. Who is gpu.cpp for? gpu.cpp is aimed at enabling projects requiring portable on-device GPU computation with minimal implementation complexity and friction. Some example use cases are: Development of GPU algorithms to be run on personal computing devices Direct standalone implementations of neural network models Physics simulations and simulation environments Multimodal applications - audio and video processing Offline graphics rendering ML inference engines and runtimes Parallel compute intensive data processing applications Although gpu.cpp is meant for any general purpose GPU computation and not strictly AI, one area we're interested in is pushing the limits exploring the intersection of new algorithms for post-training and on-device compute. To date, AI research has primarily been built with CUDA as the priveledged first-class target. CUDA has been dominant at large scale training and inference but at the other end of the the spectrum in the world of GPU compute on personal devices, there exists far more heterogeneity in the hardware and software stack. GPU compute in this personal device ecosystem has been largely limited to a small group of experts such as game engine developers and engineers working directly on ML compilers or inference runtimes. Along with that, implementing against the Vulkan or even WebGPU API directly tends to be targeted mostly towards infrastrcture scale efforts - game engines, production ML inference engines, large software packages. We want to make it easier for a broader range of projects to harness the power of GPUs on personal devices. With a small amount of code, we can access the GPU at a low-level, focusing on directly implementing algorithms rather than the scaffolding and tech stack around the GPU. For example, in our AI research there's much to explore with the various forms of dynamic/conditional post-training computation - dynamic use of adapters, sparsity, model compression, realtime multimodal integrations etc. gpu.cpp lets us implement and drop-in any algorithm with fine-grained control of data movement and GPU code, and explore outside boundaries of what is supported by existing production-oriented inference runtimes. At the same time we can write code that is portable and immediately usable on a wide variety of and GPU vendors and compute form factors - workstations, laptops, mobile, or even emerging hardware platforms such as AR/VR and robotics. What gpu.cpp is not gpu.cpp is meant for developers with some familiarity with C++ and GPU programming. It is not a high-level numerical computing or machine learning framework or inference engine, though it can be used in support of such implementations. Second, in spite of the name, WebGPU has native implementations decoupled from the web and the browser. gpu.cpp leverages WebGPU as a portable native GPU API first and foremost, with the possibility of running in the browser being being a convenient additional benefit in the future. If you find it counerintuitive, as many do, that WebGPU is a native technology and not just for the web, watch Elie Michel's excellent talk \"WebGPU is Not Just About the Web\". Finally, the focus of gpu.cpp is general-purpose GPU computation rather than rendering/graphics on the GPU, although it can be useful for offline rendering or video processing use cases. We may explore directions with graphics in the future, but for now our focus is GPU compute. Limitations and Upcoming Features API Improvements - gpu.cpp is a work-in-progress and there are many features and improvements to come. At this early stage, we expect the API design to evolve as we identify improvements / needs from use cases. In particular, the handling of structured parameters and asynchronous dispatch will undergo refinement and maturation in the short-term. Browser Targets - In spite of using WebGPU we haven't tested builds targeting the browser yet though this is a short-term priority. Reusable Kernels and Shader Library - Currently the core library is strictly the operations and types for interfacing with the WebGPU API, with some specific use case example WGSL implementations in examples/. Over time, as kernel implementations mature we may migrate some of the reusable operations from specific examples into a small reusable kernel library. More Use Case Examples and Tests - Expect an iteration loop of use cases to design tweaks and improvements, which in turn make the use cases cleaner and easier to write. One short term use cases to flesh out the kernels from llm.c in WebGPU form. As these mature into a reusable kernel library, we hope to help realize the potential for WebGPU compute in AI. Troubleshooting If you run into issues building the project, please open an issue. Acknowledgements gpu.cpp makes use of: Dawn as the WebGPU implementation webgpu-dawn-binaries by @jspanchu to build a binary artifact of Dawn. webgpu-distribution by @eliemichel for cmake builds. Thanks also to fellow colleagues at Answer.AI team for their support, testing help, and feedback. Discord Community and Contributing Join our community in the #gpu-cpp channel on the AnswerDotAI Discord with this invite link. Feel free to get in touch via X @austinvhuang as well. Feedback, issues and pull requests are welcome.",
    "commentLink": "https://news.ycombinator.com/item?id=40952182",
    "commentBody": "gpu.cpp: A lightweight library for portable low-level GPU computation (github.com/answerdotai)111 points by bovem 12 hours agohidepastfavorite16 comments pavlov 6 hours agoLovely! I like how the API is in a single header file that you can read through and understand in one sitting. I've worked with OpenGL and Direct3D and Metal in the past, but the pure compute side of GPUs is mostly foreign to me. Learning CUDA always felt like a big time investment when I never had an obvious need at hand. So I'm definitely going to play with library and try to get up to speed. Thanks for publishing it. reply almostgotcaught 2 hours agoprevTIL you can run the WebGPU runtime without a browser. reply summarity 2 hours agoparentFor me that’s its most promising feature. At last a truly cross platform compute library (not this, WebGPU itself). With two complete and mature implementations no less (dawn and wgpu). reply binary132 2 hours agorootparentI do not think of dawn or wgpu as complete and mature, has something changed? reply moffkalast 1 hour agorootparentYeah does Firefox support it yet in stable, or are they still a solid year behind Chrome as usual? reply 01HNNWZ0MV43FF 3 hours agoprev> The only library dependency of gpu.cpp is a WebGPU implementation. Noo reply hpen 5 hours agoprevAny performance metrics vs Vulkan, metal, etc? reply mpreda 4 hours agoparentvs OpenCL, ROCm, CUDA? reply zamadatix 24 minutes agorootparentSince this library ends up acting as a layer on top of the listed specifications it'd be more applicable to see benchmarks comparing the performance to building on top of said specifications directly to get an idea of overhead. At that point you could layer existing generic comparisons for the specifications you listed (or anything else for that matter) instead of needing them all to be redone specifically with this in mind. reply uLogMicheal 2 hours agoprevThis is awesome! Was looking at creating similar, inspired by the miniaudio approach. Will likely contribute a dart wrapper soon. reply Arech 5 hours agoprevVery interesting... I wonder, how does code performance compares to raw Vulkan? reply byefruit 4 hours agoprevThis looks great. Is there an equivalent project in rust? reply LegNeato 3 hours agoparenthttps://github.com/charles-r-earp/krnl, and more broadly https://github.com/EmbarkStudios/rust-gpu. reply xaxaxb 3 hours agoprev [4 more] [flagged] abenga 2 hours agoparentBe the change you want to see in the world. Write the rust implementation yourself. reply yazzku 2 hours agoparentprevBecause C++ is better. reply ranger_danger 1 hour agoparentprev [–] I wish rust people spent their time writing software instead of going around telling other people to do the job for them. They only manage to get others annoyed with this attitude. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "**gpu.cpp** is a lightweight C++ library designed for portable GPU computation, leveraging the WebGPU specification to support various hardware and APIs like Vulkan, Metal, and DirectX.",
      "**Key Features** include minimal API surface, quick compile/run cycles, and essential functions for GPU resource management and computation, such as `createContext()`, `createTensor()`, and `dispatchKernel()`.",
      "**Target Audience** includes developers and researchers needing portable GPU computation for applications like neural network models, physics simulations, and audio/video processing, with ongoing improvements and community support."
    ],
    "commentSummary": [
      "gpu.cpp is a lightweight library designed for portable low-level GPU computation, with a single header file API, making it accessible for developers familiar with OpenGL, Direct3D, and Metal.",
      "The library relies on a WebGPU implementation, which is noted for its cross-platform capabilities and mature implementations like dawn and wgpu, though some users question their completeness.",
      "Discussions highlight interest in performance comparisons with other GPU computation frameworks like Vulkan, Metal, OpenCL, ROCm, and CUDA, emphasizing the need for benchmarks."
    ],
    "points": 111,
    "commentCount": 16,
    "retryCount": 0,
    "time": 1720851155
  },
  {
    "id": 40948566,
    "title": "Common Expression Language interpreter written in Rust",
    "originLink": "https://github.com/clarkmcc/cel-rust",
    "originBody": "Common Expression Language (Rust) The Common Expression Language (CEL) is a non-Turing complete language designed for simplicity, speed, safety, and portability. CEL's C-like syntax looks nearly identical to equivalent expressions in C++, Go, Java, and TypeScript. CEL is ideal for lightweight expression evaluation when a fully sandboxed scripting language is too resource intensive. // Check whether a resource name starts with a group name. resource.name.startsWith(\"/groups/\" + auth.claims.group) // Determine whether the request is in the permitted time window. request.time - resource.age < duration(\"24h\") // Check whether all resource names in a list match a given filter. auth.claims.email_verified && resources.all(r, r.startsWith(auth.claims.email)) Getting Started This project includes a CEL-parser and an interpreter which means that it can be used to evaluate CEL-expressions. The library aims to be very simple to use, while still being fast, safe, and customizable. fn main() { let program = Program::compile(\"add(2, 3) == 5\").unwrap(); let mut context = Context::default(); context.add_function(\"add\", |a: i64, b: i64| a + b); let value = program.execute(&context).unwrap(); assert_eq!(value, true.into()); } Examples Check out these other examples to learn how to use this library: Simple - A simple example of how to use the library. Variables - Passing variables and using them in your program. Functions - Defining and using custom functions in your program. Concurrent Execution - Executing the same program concurrently.",
    "commentLink": "https://news.ycombinator.com/item?id=40948566",
    "commentBody": "Common Expression Language interpreter written in Rust (github.com/clarkmcc)111 points by PaulHoule 23 hours agohidepastfavorite26 comments clarkmcc 11 hours agoAh, I was wondering why the project was getting a few more eyes today! Maintainer here, I took over this excellent project from Tom Forbes in April 2023. He did a phenomenal job writing the parser and laying the groundwork for an interpreter. One of the beautiful things right now about this project is its simplicity — it’s tiny compared to cel-go for example. I’m also a huge fan of our Axum-style functions[1] where you can register pretty much any closure as a custom function to be used in your CEL expressions. There’s still some mileage to go to support some of the more obscure aspects of the spec, but I feel like we’re getting close, and we have an excellent little cadre of contributors that have been extremely helpful in moving this forward. [1] https://github.com/clarkmcc/cel-rust/blob/master/example/src... reply zackbrown 20 hours agoprevThis is really cool. I've been building a GUI system in Rust that features an expression language[0] — and we ruled out using CEL a while ago because the canonical Go CEL interpreter would cost several megabytes of runtime footprint (e.g. in a WASM bundle.) Haven't measured the footprint of cel-rust yet, but I expect it's orders of magnitude smaller than cel-go. The Go runtime itself is the culprit with cel-go. This Rust implementation may let us port to CEL after all, while maintaining Pax'scanonical Go CEL interpreter would cost several megabytes of runtime footprint (e.g. in a WASM bundle.) At first blush Go seems incompatible with WASM in terms of maintaining Go's strengths—the two runtimes have different memory models and stack representations. I'm curious why one would choose Go if WASM were a target to begin with. reply 1oooqooq 16 hours agorootparentbecause very few people porting things to wasm understand the model more than they understand the abi they are compiling code to. asm is a dead art. reply brabel 12 hours agoparentprevI don't know why people don't just use Lua. It's extremely small, can be embedded only with the Lua modules you want it to include (e.g. you do not need to provide file system access, networking etc. making it similar to CEL, just better), can be compiled to every possible target being written in standard C, is very mature and is completely open source. reply jey 11 hours agorootparentThe problem is that Lua is Turing complete, so you can write programs that don't halt or which take an input-dependent duration to halt. Example: while true do print(\"meow?\") end In contrast, each CEL expression has a maximum depth which directly determines how long it takes to execute. (More precisely: by calculating the maximum costs up the expression tree from leaves to root.) reply summarity 7 hours agorootparentLua is a VM and instruction limits can easily be imposed to kill programs like this. (This does not apply to LuaJIT) reply kbolino 3 hours agorootparentThe point here is that you know (at maximum) how many instructions will be executed before running it, and so if it exceeds the limit, you can avoid running it altogether, instead of killing it midway through its execution, leaving things in an indeterminate/corrupt state. reply IshKebab 22 minutes agorootparentYou don't. Even non-Turing complete languages can run for an arbitrary length of time. In general Turing completeness is never a relevant property in the real world. reply tines 17 minutes agorootparentOne of the selling points of this language is claimed to be that it executes in constant time. reply IshKebab 21 minutes agorootparentprevHonestly, I've ruled it out for 1-based indexing. Why deal with that mistake when you don't have to? reply 01HNNWZ0MV43FF 11 hours agorootparentprevIt's not popular enough. I love Lua (except for 1-indexing) but I can't use a language if nobody else uses it reply pjmlp 6 hours agorootparentThe games industry isn't nobody, that is how Lua got its fame, being a common scripting language, before Unreal and Unity became the only two engines most people know about. reply samatman 1 hour agorootparentprevLua is in the running for most-used language, second only to, possibly, JavaScript. Few people write just Lua (although professionally speaking, that was me for several years) but many people write some Lua. It adds up. reply ginko 3 hours agorootparentprevLua has been around for ages. I've never heard of CEL before today. reply ay 20 hours agoprevVery interesting, and looks extremely flexible. I had needed a small interpretive environment that would be highly controlled, used in a proprietary configuration templating solution for parametrizing various values. I wrote https://github.com/ayourtch/aycalc - very rudimentary by default with just basic arithmetic, but easy to give different security guarantees, depending on the context - the references to functions and variables can be either separate from each other or share the space, also it’s easy to special-case the handling for both variables and functions. The entire source code for the library is just around 400 lines, so i thought it can be a different enough type of a beast to mention, in case someone finds it useful. reply verdverm 20 hours agoparentCEL is becoming a \"standard\" implementation found in a lot of places. I generally like it, but loathe how much it's been growing the Yaml Engineering (devops) space. We need better options like CUE and Starlark that can scale and CEL feels more like duct tape in a lot of places. You kind of need CEL-in-config when the values are dynamic and processed within another system. In time, I expect that if something like GitHub Actions supported CUE natively, we could remove the `when: \"CEL expression\"` even with the dynamic values from previous steps. Eventually CUE's evaluator will be smart enough to know when / how to order sub-values. - https://cel.dev - https://github.com/google/cel-spec reply incrudible 24 minutes agoprevFrom the CEL Github page: > The required components of a system that supports CEL are: > The textual representation of an expression as written by a developer. It is of similar syntax to expressions in C/C++/Java/JavaScript Ok > A binary representation of an expression. It is an abstract syntax tree (AST). > A compiler library that converts the textual representation to the binary representation. This can be done ahead of time (in the control plane) or just before evaluation (in the data plane). > A context containing one or more typed variables, often protobuf messages. Most use-cases will use attribute_context.proto > An evaluator library that takes the binary format in the context and produces a result, usually a Boolean. Why? All of these sound like implementation details to me, some of which I prefer not to have, such as the necessity for binary representation. reply ramon156 20 hours agoprevProbably a silly and useless idea, but what if there was an ORM that was designed around CEL? reply processunknown 20 hours agoparentNot a silly or useless idea at all. Ive seen a custom built ORM that parsed a CEL-like syntax into a SQL query using a query builder. It was pretty nifty. The use-case was to allow users to craft arbitrary queries on a data-driven application. reply dlahoda 20 hours agoprevthere is visual editor which exports cel https://github.com/react-querybuilder/react-querybuilder reply lalaithion 21 hours agoprevCEL is super cool, it's really great to have a quick and easy way to add a filter parameter to every list API on your server. This project should add an easy way to take a list/iterator of something that implements Serde Serialize, and filter based on a CEL expression. reply PaulHoule 21 hours agoparentIn my mind it has some similarity to https://en.wikipedia.org/wiki/Object_Constraint_Language insofar as there is an expression language inside of OCL. OMG uses OCL in many parts of standards that can use that functionality. reply tempodox 21 hours agoprevCool! I made something remotely similar, a library for complex arithmetic that dynamically evaluates user-defined expressions. Since it only has pre-defined functions, the compiler can do pervasive constant folding. reply junon 12 hours agoprevMan I almost built this exact thing about a year ago and while I would have used it just didn't have enough of a usecase to justify investing the time. This is awesome work. reply 1oooqooq 16 hours agoprev [–] is CEL just a buzzword/certification gate keeping for typed data?! reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Common Expression Language (CEL) is a non-Turing complete language designed for simplicity, speed, safety, and portability, with a C-like syntax similar to C++, Go, Java, and TypeScript.",
      "CEL is ideal for lightweight expression evaluation, especially when a fully sandboxed scripting language is too resource-intensive, and includes a parser and interpreter for evaluating expressions.",
      "Example use cases include checking resource names, determining time windows, and verifying email claims, with support for custom functions and concurrent execution."
    ],
    "commentSummary": [
      "A Common Expression Language (CEL) interpreter written in Rust has gained attention for its simplicity and efficiency compared to cel-go.",
      "The project, now maintained by clarkmcc, supports Axum-style functions and custom closures in CEL expressions, with ongoing progress despite some unsupported spec aspects.",
      "Users appreciate its small runtime footprint, especially for WebAssembly (WASM), and its constant execution time, making it a standard in various applications, though some prefer alternatives like CUE and Starlark."
    ],
    "points": 111,
    "commentCount": 26,
    "retryCount": 0,
    "time": 1720811971
  },
  {
    "id": 40949229,
    "title": "Things I know about Git commits",
    "originLink": "https://www.jvt.me/posts/2024/07/12/things-know-commits/",
    "originBody": "Written by Jamie Tanna on July 12, 2024 CC-BY-NC-SA-4.0 Apache-2.0 7 mins 89 things I know about Git commits This is an article that's been swimming around in my head for ~5 weeks now, and may become a \"living post\" that I keep updated over time. In no particular order, some things I've learned about Git commits and commit history, over the last 12 years. This is a mix of experience in companies with teams of 2-12 people, as well as in Open Source codebases with a vast range of contributors. Git has different uses - a collaboration tool, a backup tool, a documentation tool Git commit messages are excellent I've never met anyone who likes reading commit messages as much as me Finding out why a change was made is easier sorting through commits than it is through an issue/bug tracker It's better to have a commit that says \"Various fixes. DEV-123\" than it is to have \"Various fixes\" It's worse to have a commit that says \"Various fixes. DEV-123\" when the issue itself has no useful information Rebase-merging is my preference. Then squash-merge, then merge If you don't learn how to rebase, you're missing out on a good skill People who say \"just delete the repo\" when things go wrong really frustrate me Learn how to use git reflog, and it'll save you deleting a repo and work that was recoverable Learn how to use git reflog, and you'll be able to save yourself from mistakes that aren't that bad No amount of learning fancy tools and commands saves you from fucking up every once in a while My most recent botched rebase was last week, and I needed git reflog to help un-fuck it Learn how to undo a force push and then how to more safely force push (remember the =ref!!) Squashing is a waste of well-written atomic commits Squashing is better than 100 crap commits Squashing, and writing a good commit message at the time of merge is good Squashing, and then not re-editing the message is the worst Squashing, when you have 100 crap commits, and then not re-editing the message is a crime Squashing, and then not re-editing the message is worse than a merge commit from a branch with 100 crap commits Writing a well documented PR/MR description but not using that to inform the squash-merge message is a waste of time Writing commit messages have helped me pick up on missing test cases, missing documentation, or invalid thought processes, as it helps me rewrite why the changes are being made Using your git log as an indication for standup updates is valid I can't be bothered to sign my commits (unless I'm forced to) If I have to sign my commits, SSH key signing makes it almost not awful If you're moving files between repos, you need to keep the history intact, using git subtree Commits should be atomic - all the code and tests, and configuration changes should all be in there I spend a good chunk of time ensuring that each commit passes CI checks atomically Some people do horrific things, like split their implementation and test code from each other It's OK to put documentation in a separate commit - we don't have to have a whole end-to-end feature delivery in a single commit Repos that use squash-merges suck As a maintainer of Open Source projects, I like squash-merge, so I can rewrite contributors' commit messages Sometimes it's not worth coaching how to write a given commit message The people around you shape the way you write commits Do the work up front to make your history atomic It's much more painful to split a mega-commit into atomic commits after the fact Splitting work atomically can be good for improving your reward drive - you can get many more things done Atomic commits work really well with prefactoring Sometimes prefactor commits can go into separate PRs (especially if squash-merge is used) Writing commit messages can take longer than the implementation The commit message can be an order of magnitude larger than the number of lines changed in a commit If you end up writing a lot of \"and\"s or \"also\" in a commit message, you may be trying to do too many things Trawling through Git commit history in the past has helped unlock a number of cases where I could then understand the why without the original authors there to answer my questions Commit messages are a great point to reflect on not just what you've done, but why Why is more important than what - anyone can look at the diff and generally work out what changes were made, but the intent behind it is the special sauce If you only write what changed, you're annoying, and I dislike you A commit that explains what is better than a commit that just has \"fixes\" Chris Beams' article, How to Write a Git Commit Message is still an excellent post and a great place to start, just under 10 years later! Commits are a point-in-time explanation of the assumptions and the state of the world for the committer. Don't be too hard on them I don't want to read AI/LLM rewrites of your changes - either write it yourself, or call it Various fixes There needs to be a way to add an annotation (maybe using git notes) to a previous message to correct assumptions I won't write perfect commit messages up front - they'll sometimes be as much as rew! add support for SBOMs or sq, or use git commit --fixup I will, generally, break off very good atomic chunks of work into commits I will split atomic commits into multiple commits, sometimes Make sure you review your own code changes before you send it out for review to your collaborators Reviewing your commit messages should be as important as reviewing the code changes Getting all your contributors to invest the same amount of care into the commit history is a losing battle Trying to police commit history is going to be painful Trying to mandate reviews of commit messages as part of code review is going to be painful Trying to police commit history does lead to a greater level of documentation and consideration around changes int he codebase Making implicit assumptions explicit is really useful Introducing commitlint can be useful, but also frustrating It's nicer to have your collaborators want to write good commit messages than you having to force them Some people don't write, and that's alright Writing is a skill I'm not perfect at writing (commit messages) Sometimes I can't be arsed to write the perfect message Sometimes I write some really great commit messages, and impress myself Using a template for your Git commit messages is a good nudge to doing it right fixup commits and git rebase --autosquash has been one of the best Git tips I've learned I value working on a team with a diverse set of perspectives, skills and approaches to work But I also really value having a team who writes atomic commits with well-written commit messages Commit message writing is as useful as writing well-refined user stories/tickets git commit -m sq is probably my most-run command Using git add -p and git commit -p are hugely important for atomic commits Never use git add -u or git add . Learn when you can use git add -u or git add . I really need to look into tools like Graphite, git-branchless and other means to provide a stacked PR setup Using conventional commits with semantic-release or go-semantic-release can make a huge difference when wanting to release automagically and often Using conventional commits as a framework for your commits can be really useful Using conventional commits, as someone with ADHD, reduces the need for thinking at times and can allow you to focus more on what the changes are Using conventional commits helps you work out when you're trying to do too much in a commit I think through writing so commit messages help understand why I did the thing It can be better to write a good commit message than a piece of documentation, stored elsewhere It can be better to write a good commit message than a code comment Give people space to learn Give people space to fail Remember you weren't so great at one point in time Documentation is rad. Do more of it This post's permalink is https://www.jvt.me/posts/2024/07/12/things-know-commits/ and has the following summary: . Some of the things I've learned over a decade of Git usage, and working on writing good commit messages. The canonical URL for this post is https://www.jvt.me/posts/2024/07/12/things-know-commits/ . Written by Jamie Tanna on Fri, 12 Jul 2024 20:01:09+01:00, and last updated on Fri, 12 Jul 2024 20:00:52+01:00. Content for this article is shared under the terms of the Creative Commons Attribution Non Commercial Share Alike 4.0 International, and code is shared under the Apache License 2.0. #git. Has this content helped you? Did it solve that difficult-to-resolve issue you've been chasing for weeks? Or has it taught you something new you'll be able to re-use daily? Please consider supporting me so I can continue to create content like this! This post was filed under articles. Interactions with this post ← → Top",
    "commentLink": "https://news.ycombinator.com/item?id=40949229",
    "commentBody": "Things I know about Git commits (jvt.me)108 points by sea-gold 22 hours agohidepastfavorite83 comments anyfoo 15 hours ago`git reflog`, which the article prominently mentions, really should be the VERY FIRST thing to teach everyone who is learning git. If you don't know about it, it is well worth checking it out now. It is your one ticket to getting to whatever previous, good (or bad!) state you were at any point before[1]. The only thing it cannot help you recover is locally made changes that you never committed to anything at any point, and have since locally deleted. This is surprisingly rare, at least for me, as I just tend to `git stash` stuff I don't want anymore. With `git reflog` in your backpocket, you can rebase, merge, branch, delete branches, cherry-pick, rewrite history, whatever, all to your heart's content. `git reflog` keeps a journal of every single action, and tells you the HEAD before and after each action, which you can then simply checkout or `reset --hard` your branch to. I never even use any arguments with it, I literally just `git reflog`. [1] Unless garbage collection has deleted the commit you were pointing to, I guess, but I've never had to use `git reflog` that far in the future. reply idoubtit 10 hours agoparentOver the last decade, I think I've only used `reflog` twice. The only case I remember clearly was to help a co-worker that had created a local mess, starting with a `git pull` (meaning a merge) on the wrong branch then piling onto that. If you use Git in a terminal, a much simpler alternative to the reflog is having the commit hash in the prompt. Then comparing/reverting to a previous state just requires scrolling the terminal to find the commit id. It's much easier to read than the reflog. With the help of the shell prompt, the terminal can help with the questions \"what did I do to get there and how to get out?\". I also think that Git has become much more error-proof. It's been a long time since I've seen anyone lost in their own repository like I'd seen years ago. There are better GUIs. In the command line, modern Git often displays hints, has warnings for some dangerous commands, and sometimes explains how to rollback. reply lucasoshiro 2 hours agoparentprevEven though I find reflog very useful, I don't think it should be the first thing. What I think that should be one of the first things and most people don't know is written in \"What is Git?\" chapter from Pro Git: https://git-scm.com/book/en/v2/Getting-Started-What-is-Git%3... reply compressedgas 13 hours agoparentprevI'd also recommend turning off reflog expiration with: gc.reflogexpire=never gc.reflogexpireunreachable=never reply dabber 9 hours agorootparentDefaults for each option of anyone is curious: > gc.reflogexpire 90 days > gc.reflogexpireunreachable 30 days reply ezst 10 hours agoparentprev> `git reflog`, which the article prominently mentions, really should be the VERY FIRST thing to teach everyone who is learning git. Top that up with `log` showing all branches at all times (except if you ask otherwise) and you end up pretty much where mercurial is. Detached heads is such nonsense. Git missing phases and obsolescence (to denote if and how history eventually got rewritten) is another. reply vbezhenar 7 hours agoparentprevI'm creating local tmp1 tmp2 branches before I'm doing anything sketchy. Branches will keep the commit visible and I can always return to the previous state if necessary. Tags would be more appropriate, I guess, I just got used to branches more. reply JelteF 8 hours agoprevThe most effective way I've found to get other people to \"write\" good commit messages is by changing the \"Default commit message\" for squash merges on the GitHub repo to \"Pull request title and description\". [1] That fixes the \"Squashing, when you have 100 crap commits, and then not re-editing the message is a crime\" item, because suddenly not re-editing will give you a fairly useful message. This ofcourse assumes the PR description is useful, but I've found it much easier to convince people to write a decent PR description than to write decent commit messages. [1]: https://github.blog/changelog/2022-08-23-new-options-for-con... reply praash 7 hours agoparentI love giving examples and context in a PR description. Squash-merged PRs can become cumbersome snowballs, but the final commit message should elaborate in proportion. reply rudasn 6 hours agorootparentI also like to include before and after tables, either with stats (eg. perf) or with screenshot with UI changes. First I head about automatic squash messages, and if they include the whole title and description I should look into it a bit more! reply NotBoolean 7 hours agoparentprevThis is what we have resorted to in my team. It was just too difficult to get everyone to keep good commit hygiene and follow a best practice like conventional commits. reply ericrallen 6 hours agorootparentHaving been through the pain of getting teams to adopt conventional commits a few times, I found that integrating a wizard like commitizen helped folks who were annoyed by commit linting learn and get comfortable with the rules and format so there was less friction when their commit was rejected by the linter. It also really helps if you can wire up some continuous deployment to automate something tedious like properly incrementing the version number in the semantic version, updating the changelog, and deploy out a new `latest` or `next` tag to the package registry. Even the most reticent users are often inspired to follow conventional commits once they see the possibilities that open up. reply donatj 13 hours agoprev> People who say \"just delete the repo\" when things go wrong really frustrate me YES. Go be a baker or something! If you're going to be a developer you should generally speaking be willing to figure out what's wrong and how to fix it. Know how your tools work. Bad carpenter what have you... Also speaking of `git reflog` and such - I still stand by this article I wrote a few years back - https://donatstudios.com/yagni-git-gc 99% of people should just turn automatic `git gc` off. You don't need it. Turn it off and you never have to worry about losing work. It's there, you just have to find it. reply zvrba 10 hours agoparentGit is an abysmal tool for many (most?) uses of it, but that unfortunately has become \"standard\". The sheer awfulness of git is witnessed by the amount of posts about it and little consensus on \"best practices\" (e.g., rebase vs merge). So, I don't judge, but sympathize with people who just \"delete the repo and start from scratch\". Unintuitive, user-hostile tools call for heavy-handed solutions. IME, most people are willing to learn something when they're shown the value for invested effort. That \"delete the repo\" is standard answer for fixing f*up, tells more about the tool than the people using it. (I.e. it requires disproportionately big investment of time for little value.) reply DexesTTP 8 hours agorootparentI really don't agree with that. Git is a powerful tool with very few actual downsides, and the unwillingness of some developers to spend an hour learning how it works hurts them in the long-term. It's like sticking to the text editing feature of your IDE because you can't be bothered to learn how it works. Sure, you _technically_ can do that, but you're losing on everything that makes an IDE useful and probably losing actual days or weeks worth of work because of that. reply Rinzler89 4 hours agorootparent>the unwillingness of some developers to spend an hour learning how it works hurts them in the long-term And that's the problem. Because every developer has spent an hour learning how it works by themselves but then each of them in completely different ways, from different sources, on different projects and workflows, some more correct than others, because there's not one single perfect ground truth way of using git in every situation, but git offers one million ways of shooting yourself in the foot once you land on the job, even after you think you learned git in that one hour. And that IMHO is git's biggest problem: too powerful, too many features, too many ways of doing something, no sane defaults out of the box that everyone can just stick with and start working, too many config variables that you have to tinker with, etc. Case in point, just look at the endless debates in the comments here on what the correct git workflows are wand what the correct config variables are, nobody can agree on anything unanimously on what the right workflow of configs are everyone has their own diverging opinion. reply jiripospisil 4 hours agorootparentI think you've just described why Git is so popular and literally everywhere. reply Rinzler89 4 hours agorootparentSomething being popular doesn't mean it's universally good everywhere and loved by everyone. Windows and Teams are also popular, almost every company uses them, that doesn't make them good. Diesel ICE cars are also highly popular in Europe even though they're much worse for our air quality and health. Do you see the issue with using popularity as an argument? I've met many devs who hate git with a passion but they just have to use it because management said so and because evry other workplace now uses it, just like Teams and Windows. Not saying git is bad per se, just pointing out the crater of pitfalls it opens up. reply jiripospisil 3 hours agorootparentRight but the world is bigger than corporate and yet I don't see anyone choosing anything else for their pet project large or small either. If Git was such a pain to use, wouldn't a lot of open source projects use something else? I know OpenBSD uses CSV, SQLite uses Fossil.. I can't honestly think of anything else non-Git right now that I use (I'm sure I'm missing some). Years ago when private repositories were still a paid feature on GitHub, you could use Bitbucket, which had them for free, and offered Git and Mercurial. A few years later Bitbucket announced they were removing Mercurial support because \"Mercurial usage on Bitbucket is steadily declining, and the percentage of new Bitbucket users choosing Mercurial has fallen to less than 1%\". https://bitbucket.org/blog/sunsetting-mercurial-support-in-b... reply Rinzler89 3 hours agorootparent>I don't see anyone choosing anything else for their pet project large or small either. I also don't see anyone else choosing to breathing anything else than oxygen either. It's not like they have so many other options when the job market requires git and most coding tutorials also feature git and schools also use git, so the entire industry decided to use git despite other options existing. Again, that doesn't mean git is bad or that is loved by everyone or that it's the best. Betamax also lost to VHS despite being technically superior. A lot of victories are won by the lesser product given enough inertia and being at the right time and the right place. Kind of how Windows and SAP got entrenched in the 90s. People and orgs were buying into it because everyone else was also using it so your only choice was to use it too no matter your own opinions on it. What were you gonna do? Piss against the wind and torpedo your hiring prospects by pigeonholing is some other \"better\" tool that nobody else uses? I don't remember what VCS I used at my first job in the embedded industry but that one was hands down way better, easier and fool proof compared to git, with a nice GUI long before GIT GUI tools were even remotely good, it just didn't survive there long term because it costed a fuck tonne of money in licensing fees for the org. You can see where this is going, right? When it comes to bean counters, free beats paid every day regardless of most other arguments. reply zvrba 4 hours agorootparentprevAlso smoking was once both popular and everywhere. At least smokers enjoy it, I'm not sure anyone \"enjoys\" using git. reply tinyhitman 41 minutes agorootparentI'm sure I do. reply compressedgas 13 hours agoparentprevInstead of turning off the automatic git gc, turn off the reflog expiration with: gc.reflogexpire=never gc.reflogexpireunreachable=never reply cedws 10 hours agoparentprevI haven’t done that since my junior days. Do people really do this? I know Git’s UX is bad but wow. reply jimmaswell 13 hours agoparentprevI could spend hours of my workday on some uninteresting, irrelevant bullshit because the local repo is fucked, or just delete it and move on to solving actual problems and producing value. The choice is clear. reply Zambyte 13 hours agorootparentOr you could spend an hour or so learning to use git (or a git compatible vcs like Jujutsu) and then you never bork your repos, and instead spend your time solving actual problems and producing value. What kind of pastries do you like? reply jimmaswell 13 hours agorootparentI can't remember the last time I had to delete a repo, at most I do a hard reset to the head of the main branch. Still don't care and I'm doing well enough getting consistent maximum performance ratings at my job and building things in my own time. Sometimes a problem is not worth addressing, and part of being an effective software engineer on the whole is recognizing those times. reply matrss 4 hours agorootparentSo, contrary to your own point, you did indeed put in some time and learned one of the ways you can fix a \"fucked\" repo, because it is more convenient to just 'git reset --hard main' than to delete and re-clone. There are some simple next steps to go from there: - notice that you don't have to reset to the main branch, you can put in any branch name - it doesn't even have to be a branch name, it can be any kind of reference to a commit - 'git reflog' is useful to find out a reference to a commit you recently were on, that you can then give to reset - you don't have to use --hard, I personally like using --keep, and sometimes you might want to keep local changes so you don't supply those options at all - ... What you already did is what Zambyte suggested you should do. Of course there is diminishing returns at some point, but that point comes way after 'git commit -am $msg && git push'. reply jenadine 12 hours agorootparentprevBoth points are valid. But it shouldn't take hours to \"fix\" a repo in most cases. As you said: git reset. Anyway, I've been putting some of my repo in very bad states by doing git operation from different unix user that lacked the right permission to do all the writes leaving some corrupted objects and stuff which needs a bit more than just a reset to recover. Then it takes some time to recover, but that's because I also don't want to loose some hours of un-pushed work. Doing a delete of my repo could have been faster but I'd have lost some temporary branches and squashes and stuff that would also represent some work. But I think the original point is for people who are not even able to do a git reset. reply skeeter2020 8 hours agorootparentprev>> Or you could spend an hour or so learning to use git This is such nonsense. The git you can learn in an hour gets you exactly to the point where you end up in this \"now what?\" situation. Furthermore unless you're working through this more regularly than every 6+ months you're not going to remember how to fix it. reply Zambyte 4 hours agorootparent> Furthermore unless you're working through this more regularly than every 6+ months you're not going to remember how to fix it. Take notes. reply usr1106 12 hours agorootparentprevI have never deleted a repo because it's messed up. In the early days understanding what was wrong was sometimes challenging. But once I learned it it's just routine to clean it up. I do occasionally search stuff from my reflog or trial branches from years ago, no way I want to lose that opportunity. One needs to learn full branch names and how the abbreviations used in daily life really work. A local branch called origin/foo can be confusing, especially after being pushed. And of course one needs to understanding the concept of remote branches in the first place, they are not really remote, typical poor git terminology. For mass mess-ups in the work tree git status -sgrep ... oneliners are your friend. reply matheusmoreira 7 hours agorootparentprevOr you could actually learn git once and enjoy using a powerful tool without experiencing any of this friction. If you understand git you'll just know what to do when some weird thing happens. reply cxr 2 hours agoprev> There needs to be a way to add an annotation[...] to a previous message to correct assumptions You rarely want to do _just_ this, i.e. call out the bad commit message in question and nothing else; you're generally going to have something to fix because of that \"bad assumption\", too. You just branch off the old commit, do the needful (read: make the correction to the code), commit it, and then merge that into mainline. It seems like a lot of people fail to consider that a branch merged into mainline does not become immutable; at any point you can check it out and make further changes and then remerge those and do all of this as many times as necessary. If you really do need to only draw attention to a bad commit message in a past commit and correct the record but nothing else, then you can just \"annotate\" it with an ordinary merge commit— 1. Check out the old commit that you want to annotate (create a branch there if you want, but it's optional). 2. Merge the mainline into this HEAD/branch, making sure force a merge commit using --no-ff, and write your \"annotation\" in the commit message. 3. Merge that into the mainline branch. reply alganet 16 hours agoprevTo me, commit messages really shine on file histories and `git blame`. Opening an unknown file and having the option to see all it went through is powerful. The commit messages and history will tell you which files are related, which files change together, why they do, etc. It's a superpower that a team can cultivate. reply lucasoshiro 16 hours agoparentI strongly agree. Git isn't only code sharing that you commit and forget. It is a database of code that tracks what changed, why changed, who changed and when changed. Want to know why a piece of code exists? Find the commit that introduced it, read the message. If it's not enough, you can search the commit and find the discussion on GitHub/GitLab/etc. A new bug suddenly appeared? Use git bisect to find what was the commit that introduced it. Some months ago I posted some tips about using Git as a debugging tool, if you want to read more about it: https://news.ycombinator.com/item?id=39877637 reply PaulDavisThe1st 13 hours agorootparentSee also: git log -L:funcname:file funcname will be matched after wrapping it in a \"function declaration identifying\" regexp. Get the entire history of a given function (as long as it was in ). Not a daily driver, but sometimes unimaginably valuable. reply lucasoshiro 3 hours agorootparentThanks! I'll check it and I'll update my article with that! reply ufo 13 hours agorootparentprevCould you please give a concrete example of how to use funcname? I tried looking in the man page and didn't get it. reply PaulDavisThe1st 3 hours agorootparentI work in C++, so I might be looking for the history of Session::set_state (XMLNode const & node) I would write: git log -L:set_state:session_state.cc If I worked in C, and I had this in a file: void some_function_name (type1_t t1, type2_t t2) I would write git log -L:some_function_name:thefile.cc reply vbezhenar 7 hours agoparentprevI'm using git history may be one time a year. And even those times might be just of pure curiosity: who's that dumb who did that bug. Basically it's useless for me. So I don't care much about commits. Git for me is a collaboration tool and code backup tool, and that's about it. Nobody will look at my 1-year old commit and I'm not going to look at anyone's 1-year old commit. reply lucasoshiro 3 hours agorootparent> who's that dumb who did that bug You can find more than that, e.g. what was the context of the introduction of a code, find the discussion about that introduction, what was the situation of the project when the code was introduced, etc. But even if you only want to blame, it is enough to keep a good commit history. > Nobody will look at my 1-year old commit and I'm not going to look at anyone's 1-year old commit. If you commit history isn't good no one will. And no one will benefit of using git. And then you don't need to use git. > Git for me is a collaboration tool and code backup tool, and that's about it. If you don't see the point of having a commit history, there are other tools that fit it better. For code collaboration, JetBrains' Code With Me and VSCode's CodeTogether. For code backup, Google Drive or Dropbox. reply RadiozRadioz 6 hours agorootparentprevWell, it completely depends on your job and work style. Personally I'm a site reliability engineer, so when something breaks it's an invaluable step to look at the commit history to see what changed, when, how, why, and by whom. I'm searching through commit histories several times per hour. reply rudasn 6 hours agorootparentprevGood commit messages can be used in lieu of official documentation, and that makes them veey helpful. Whether or not relying on git commit messages for source of truth documentation is a good idea is debatable, but personally, when people do it I'm glad they did. reply lucasoshiro 16 hours agoprevIf you want more controversial things about squash, here are two that I posted here last week: 1. \"How squash merge can break Git repos with submodules\", about a situation where squash merges silently broke a repository: https://news.ycombinator.com/item?id=40846596 2. \"Git: please stop squash merging!\" giving some technical explanation about what squash merges actually are under the hood and how the misconceptions about Git leads to invalid \"pros\" about squash merging: https://news.ycombinator.com/item?id=40840572 reply fmorel 2 hours agoparentThe first is solvable with a PR check. You can do whatever you want with the submodule on your branch, but if you want it to merge to `main`, then your changes to the submodule have to merged to its `main`. We have a check that makes sure the referenced commits is on `main`, and not older than 10 PRs. reply jraph 11 hours agoprev> Make sure you review your own code changes The author says he reviews himself on a GitLab MR / GitHub PL, I rely on two things for this: - git add -p, which also helps me split stuff in several commits if needed. It bothers me that it doesn't work for new files. - git difftool dir-diff for changes with several commits I like that it would work on any git hosting, and that it works locally. And that I can just amend my commits if I see something. reply JNRowe 11 hours agoparent`git add -p` can also work for new files, it just requires you to call `git add --intent-to-add` first. After using -N/--intent-to-add you'll see that the file is registered in the status output, and -p will work exactly how like you expect. I often find myself spiking things then breaking them back down with -N and repeated `git commit -p` to form a reasonable history. The workflow seems to really suit my mind. However, it does require some testing vigilance if you're manually editing the hunks for clarity on top of simply splitting them up. reply ydant 5 hours agorootparentThanks for the hint about `--intent-to-add` / `-N`. There's constantly new things to learn about git. I use `git add -p` extensively, but never thought to check for an option like that. reply arcanemachiner 11 hours agoparentprev> - git add -p, which also helps me split stuff in several commits if needed. It bothers me that it doesn't work for new files. Lazygit is an amazing tool that can definitely do this (and many other useful things). reply metadat 16 hours agoprev> Do the work up front to make your history atomic Is this saying 1 feature / main \"idea\" per commit? Overall this post is gold, but also probably preaching to the choir. IME it's challenging to convert non-believers to the faction of Orthodox Git. For me, learning the ins and outs of Git felt like uncovering a part of myself which has always been there. Nothing new was created, only revealed. reply jcoder 16 hours agoparentNot the author, but when I use the phrase I mean each commit accomplishes a single important thing, but also that each commit is complete: it includes necessary tests for example. IMO every commit that lands on `main` must pass the test suite (this means intermediate commits should be squashed into that atomic commit). reply epage 15 hours agorootparentThe more I've been doing open source maintenance and contributions where there isn't as much context between the code author and reviewer, the more I've been pushing for a little more than this. - Add tests in a commit *before* the fix. They should pass, showing the behavior before your change. Then, the commit with your change will update the tests. The diff between these commits represents the change in behavior. This helps the author test their tests (I've written tests thinking they covered the relevant case but didn't), the reviewer to more precisely see the change in behavior and comment on it, and the wider community to understand what the PR description is about. - Where reasonable, find ways to split code changes out of feature / fix commits into refactor commits. Reading a diff top-down doesn't tell you anything; you need to jump around a lot to see how the parts interact. By splitting it up, you can more quickly understand each piece and the series of commits tells a story of how the feature of fix came to be. - Commits are atomic while PRs tell a story, as long as it doesn't get too big. Refactor are usually leading towards a goal and having them tied together with that goal helps to provide the context to understand it all. However, this has to be balanced with the fact that larger reviews mean more things are missed on each pass and its different things on each pass, causing a lot of \"20 rounds of feedback in and I just noticed this major problem\". As an example of these is a recent PR of mine against Cargo: https://github.com/rust-lang/cargo/pull/14239 In particular, the refactors leading up to the final change made it so the actual fix was a one line change. It also linked out to the prior refactors that I split out into separate PRs to keep this one smaller. reply OJFord 8 hours agorootparentI agree, but I usually explain (and do) this from the side of fixing a bug, but where the test suite is currently passing: first commit adds the failing test (shows that it would have caught the error), second commit makes it pass. Also agree with GP that each commit on master should be passing/deployable/etc., but I don't see why they can't be merge commits of a branch that wasn't like that. reply epage 6 hours agorootparentThat still interferes with `git bisect`. Make the test pass in history but then make it fail in your working directory and work to get it to pass before committing. reply simonw 13 hours agorootparentprevI absolutely love that testing suggestion - I'd never considered shipping a whole separate commit adding the OLD test first, but having a second commit that then updates that test to illustrate the change in behavior is such an obviously good idea. reply jenadine 12 hours agoprevIt frighten me how some contributor to my open source repo just don't care about their git history. They do 100 of \"crap\" commits (author's wording) and merge commits. Sometimes I'd write a comment like: \"can you please rebase your branch\" and even very senior software engineer are clueless and can't do it even if I give the exact git command i'd use. In the end it's simpler for me to do it myself. Some developers don't even use git and just use some UI on top. I wish GitHub would have an interface to do interactive rebase and edit commit messages. reply jonathanlydall 11 hours agoparentI consider myself very knowledgeable on using Git even though I almost exclusively use it through GUI tools, however my particular GUI wrapper of choice, TortoiseGit on Windows is incredibly powerful. I very regularly do rebasing, squashing, use worktrees extensively, etc. When I occasionally have to work on Linux or macOS for our Electron based cross platform product, I haven’t yet found a GUI tool I feel comes remotely close to TortoiseGit in terms of advanced features. So much so that if I need to do things like rebasing or dealing with slightly complicated merges, I push first to a branch and then do the Git work on my Windows machine. I could work out all the Git command line arguments for these advanced use cases, but for something like rebasing with some squashing or skipping, I can’t see how anything except a well designed GUI could be anything except a seriously clunky and much slower experience. My feeling is that (good) GUIs in general make a lot more sense for Git, instead of having to read the entire manual upfront to know the available tools, the GUI can show relevant options based on context and you don’t get bogged down having to type out/paste commit references and other things into your terminal. I occasionally use the built in Git tools on VS Code and Visual Studio 2022 (the latter of which only in the last couple of years didn’t completely suck), but generally only for simple pulls, commits and pushes, which is generally more convenient in the IDE than having to switch to TortoiseGit. Of course a tool is just a tool, if you don’t “get” how Git works, no tool will save you, but I do think TortoiseGit’s rebase UI helped me get to the point where Git “clicked” for me sooner. reply philwelch 10 hours agorootparent> I could work out all the Git command line arguments for these advanced use cases, but for something like rebasing with some squashing or skipping, I can’t see how anything except a well designed GUI could be anything except a seriously clunky and much slower experience. That’s just an interactive rebase; on the command line it opens a text editor. It’s pretty easy. reply gpderetta 5 hours agorootparentprevmagit. reply usr1106 12 hours agoprevI agree with most of the items on the list. But some are too short to understand what they mean or they are really \"It depends\". The most important point for me is that the same tool is used for 2 purposes: Keeping track of quick trial and error experiments so that you can return to the best one once you have made up your mind. And producing a readable story of self-contained atomic commits telling a story to the reviewer and yourself weeks or years later. I hate myself every time when I don't get the context switch right between those 2 modes of operation. Or just don't do the second phase because I first need fo fix a bug and will continue with the current branch later. reply MaryBeew 2 hours agoprevFighting to restore love and peace in my relationship was so frustrating until I saw a video of a lady's testimony talking about how are marriage was restored. It was a whole experience I never thought could have been possible. My partner and I are happily reunited in Love and harmony, All thanks to priest Mandla for the Help he rendered to me and my family. You can still save your marriage or relationship, email: ( supremacylovespell01 @ gmail. com ) reply yerdoingitwrong 15 hours agoprevThe most USELESS commit messages are \"Fixes #12345 (#23456)\". It tells the git log reviewer NOTHING. You shouldn't have to rely on github to understand your project. State the intent of the bug fix in the commit message. reply dakiol 12 hours agoparentDepends. If the number points to a very well written ticket, that’s fine. We work with Jira and Github and all the bugs/features are explained in Jira tickets. I’m not staying long enough in this company to see them changing from Jira to something else, so I don’t care about the Jira dependence to understand commits. reply jraph 12 hours agorootparentThe git log should still stand on its own. The ticketing platform could get migrated in a way ticket numbers are not kept or old tickets are a bit less convenient to find, or just temporarily down. When working on the code, it's also very useful to access the history without having to open each ticket. Being able to hover a line, having the message of the commit which last changed it, and be able to say \"ah ok, it was for this\" is quite useful. > I’m not staying long enough in this company to see them changing from Jira to something else, so I don’t care about the Jira dependence to understand commits Isn't this a terrible argument in favor of \"Fixes NNN\" commits? Someone who care about the longevity of a versioned thing should care. If you don't care anyway, nothing matters much. reply fmorel 2 hours agorootparent> Being able to hover a line, having the message of the commit which last changed it, and be able to say \"ah ok, it was for this\" is quite useful. Honestly, this just sounds like we need better integrations with JIRA into dev tools. I'm not going to rewrite tickets in commits / PRs. That ticket has its own history, linked tickets, acceptance criteria, epics, related bug tickets, etc. reply karmarepellent 10 hours agorootparentprevI don't know about the \"git log should stand on its own\". Chances are that when you finally migrate to another ticketing system and the ticket numbers in your commits \"break\" or become meaningless, you do not care much about that old history anyway. And even if someone is interested in an old commit that contains a ticket number for a ticketing system that does not exist anymore, the commit itself (the code changes, not the message) tell the story as well, just not in prose. reply terinjokes 8 hours agorootparentIt's pretty common with configuration or infrastructure as code that a blame brings me to a commit from 5-6 years previous, where I wish to understand why the authors at the time made a specific decision. That is more than enough time to change ticketing system, wiki platforms, or collaborative chat services. reply karmarepellent 7 hours agorootparentFair enough, I did not think about the IaC space when I wrote the comment. And I can absolutely see how a blame could take you to commits from a few years back. I was only thinking about the application code space where elaborate code comments are definitely more common. reply gpderetta 4 hours agorootparentprevHaving had to dig through git history across ticketing system migrations, I agree with grand parent. reply karmarepellent 10 hours agoparentprevWell written GitHub issues and PRs that you can reference in your git commit also don't save you from commenting code. I mean please do not reference an issue in a code comments, but describing to other people what a function in general does goes a long way to build understanding. For new contributors it's probably also more useful to see the latest snapshot of the code base and code comments than having to check a git log in order to find out why a function would be written in the way it is. reply ydant 5 hours agorootparentI'll absolutely reference an issue number in a comment (with some context comments as well) in cases where it makes sense. Sometimes you need to go back and read through a lot of context understanding why a block of code does what it does, and it's a lot more efficient for the rest of the time to have a quick reference to the full explanation rather than pages of comments giving that context. reply turboponyy 12 hours agoparentprevAgreed. For PR commits, appending the PR number (e.g. `#8`) to the short message is fine, but the short message should still contain a summary of changes. For issues, having something like \"Fixed #8\" in the commit message body is picked up by forges like GitHub. reply osigurdson 13 hours agoprev>> Squashing is better than 100 crap commits Unless your work is trivial, you will have crap commits. Have discipline but don't insist on working on trivial things just so your commits appear logical without rework. reply dougthesnails 12 hours agoparentOr learn to rebase those crap commits into coherent ones. reply turboponyy 12 hours agoparentprevYou realize that commits can be changed, right? reply osigurdson 4 hours agorootparentOf course. However, refactoring N crap commits into M atomic commits where M != 1 is non-trivial. My point is about atomic commits, not git. reply r-s 15 hours agoprev> 58. Trying to police commit history is going to be painful This has been my experience. Very painful reply at_a_remove 12 hours agoprevAh, the endless mysteries of git. People say, \"It's so simple once you understand the base model.\" And yet there are these inevitable buts. And somehow, people keep arguing over how to use it, what is the Correct Way. If it were that simple, the One True methodology would be obvious, yet the disagreements continue. Then I ask dumb questions about merging and have yet to hear how the magic happens, just very circular \"merging ... merges\" answers. One day I hope to have a positive and comprehensible experience involving git and perhaps even git and another programmer, but that might be too much to ask. reply g-b-r 11 hours agoprevPLEASE, recognize that \"atomic\" commits as the author defines them (basically whole features), will actually jumble up a large part of the diff, however much advanced your diff program is, making verifying the changes a lot harder..! If you like to see chains of commits each with a complete feature, just remember that merge commits are commits as well! Sure, rebase a feature branch before merging it, but then merge it with a merge commit ! (merge --no-ff) That way you'll have both your \"atomic commits\", and actually atomic commits, that is commits with changes that any diff program will be able to highlight correctly. Verification will be extremely easier, but you'll retain the ability to browse the history at a higher abstraction level (either use any decent graphical git browser, or simply --first-parent). reply philwelch 10 hours agoparentI prefer the exact opposite approach personally: no merge commits, only rebases and fast-forward merges to master. Of course I never merge more than a handful of commits to master at a time—almost always just one—and if it’s more than one, it’s specifically because the diff is more understandable broken apart that way. Though to be honest I am not sure what you mean by “jumbled” diffs. If you’re pushing a bunch of intermediary commits that don’t make sense on their own, don’t; that’s what interactive rebase is for. If you’re seeing jumbled diffs it might be specifically because you’re relying on merge commits instead of rebasing and fast-forwarding; if you consistently and diligently rebase that isn’t a problem. reply dougthesnails 12 hours agoprevBe still my heart. reply khana 14 hours agoprev [–] and the winner: Do the work up front to make your history atomic reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The article shares insights from 12 years of experience in various teams and Open Source projects about Git commits and commit history.",
      "Key points include the importance of commit messages, the preference for rebase-merging, and the utility of learning git reflog to recover from mistakes.",
      "Emphasizes the value of atomic commits, detailed commit messages, and the challenge of encouraging good commit practices among team members."
    ],
    "commentSummary": [
      "`git reflog` is crucial for recovering previous states, though it doesn't help with uncommitted, deleted changes.",
      "Clear and useful commit messages are essential, especially for squash merges, and tools like commitizen can help maintain commit hygiene.",
      "Atomic commits (one feature/idea per commit) are recommended for clarity, and reviewing code changes with tools like `git add -p` and `git difftool` is crucial."
    ],
    "points": 108,
    "commentCount": 83,
    "retryCount": 0,
    "time": 1720816945
  }
]
