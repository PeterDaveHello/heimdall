[
  {
    "id": 39810378,
    "title": "Monolith: CLI Tool for Bundling Web Pages into Single HTML File",
    "originLink": "https://github.com/Y2Z/monolith",
    "originBody": "_____ ______________ __________ ___________________ ___\\ / \\|||\\_/ __ \\_| __| ___ ___ |__|||||||| |\\ /| |__| _ |__| |____||| __|\\___/| \\||||___| |__________| \\_____________________| |___| |___| |___| A data hoarder’s dream come true: bundle any web page into a single HTML file. You can finally replace that gazillion of open tabs with a gazillion of .html files stored somewhere on your precious little drive. Unlike the conventional “Save page as”, monolith not only saves the target document, it embeds CSS, image, and JavaScript assets all at once, producing a single HTML5 document that is a joy to store and share. If compared to saving websites with wget -mpk, this tool embeds all assets as data URLs and therefore lets browsers render the saved page exactly the way it was on the Internet, even when no network connection is available. Installation Using Cargo (cross-platform) cargo install monolith Via Homebrew (macOS and GNU/Linux) brew install monolith Via Chocolatey (Windows) choco install monolith Via MacPorts (macOS) sudo port install monolith Using Snapcraft (GNU/Linux) snap install monolith Using Guix (GNU/Linux) guix install monolith Using AUR (Arch Linux) yay monolith Using aports (Alpine Linux) apk add monolith Using FreeBSD packages (FreeBSD) pkg install monolith Using FreeBSD ports (FreeBSD) cd /usr/ports/www/monolith/ make install clean Using pkgsrc (NetBSD, OpenBSD, Haiku, etc) cd /usr/pkgsrc/www/monolith make install clean Using containers docker build -t Y2Z/monolith . sudo install -b dist/run-in-container.sh /usr/local/bin/monolith From source Dependency: libssl git clone https://github.com/Y2Z/monolith.git cd monolith make install Using pre-built binaries (Windows, ARM-based devices, etc) Every release contains pre-built binaries for Windows, GNU/Linux, as well as platforms with non-standard CPU architecture. Usage monolith https://lyrics.github.io/db/P/Portishead/Dummy/Roads/ -o portishead-roads-lyrics.html cat index.htmlmonolith -aIiFfcMv -b https://original.site/ - > result.html Options -a: Exclude audio sources -b: Use custom base URL -B: Forbid retrieving assets from specified domain(s) -c: Exclude CSS -C: Read cookies from file -d: Allow retrieving assets only from specified domain(s) -e: Ignore network errors -E: Save document using custom encoding -f: Omit frames -F: Exclude web fonts -h: Print help information -i: Remove images -I: Isolate the document -j: Exclude JavaScript -k: Accept invalid X.509 (TLS) certificates -M: Don't add timestamp and URL information -n: Extract contents of NOSCRIPT elements -o: Write output to file (use “-” for STDOUT) -s: Be quiet -t: Adjust network request timeout -u: Provide custom User-Agent -v: Exclude videos Whitelisting and blacklisting domains Options -d and -B provide control over what domains can be used to retrieve assets from, e.g.: monolith -I -d example.com -d www.example.com https://example.com -o example-only.html monolith -I -B -d .googleusercontent.com -d googleanalytics.com -d .google.com https://example.com -o example-no-ads.html Dynamic content Monolith doesn't feature a JavaScript engine, hence websites that retrieve and display data after initial load may require usage of additional tools. For example, Chromium (Chrome) can be used to act as a pre-processor for such pages: chromium --headless --incognito --dump-dom https://github.commonolith - -I -b https://github.com -o github.html Proxies Please set https_proxy, http_proxy, and no_proxy environment variables. Contributing Please open an issue if something is wrong, that helps make this project better. Related projects Monolith Chrome Extension: https://github.com/rhysd/monolith-of-web Pagesaver: https://github.com/distributed-mind/pagesaver Personal WayBack Machine: https://github.com/popey/pwbm Hako: https://github.com/dmpop/hako Monk: https://github.com/monk-dev/monk License To the extent possible under law, the author(s) have dedicated all copyright related and neighboring rights to this software to the public domain worldwide. This software is distributed without any warranty. Keep in mind that monolith is not aware of your browser’s session",
    "commentLink": "https://news.ycombinator.com/item?id=39810378",
    "commentBody": "Monolith – CLI tool for saving complete web pages as a single HTML file (github.com/y2z)506 points by iscream26 13 hours agohidepastfavorite99 comments simonw 11 hours agoWell this is fun... from the README here I learned I can do this on macOS: /Applications/Google\\ Chrome.app/Contents/MacOS/Google\\ Chrome \\ --headless --incognito --dump-dom https://github.com > /tmp/github.html And get an HTML file for a page after the JavaScript has been executed. Wrote up a TIL about this with more details: https://til.simonwillison.net/chrome/headless My own https://shot-scraper.datasette.io/ tool (which uses headless Playwright Chromium under the hood) has a command for this too: shot-scraper html https://github.com/ > /tmp/github.html But it's neat that you can do it with just Google Chrome installed and nothing else. reply mkl 58 minutes agoparentCan shot-scraper load a bunch of content on an \"infinite scroll\" page before saving? I'm guessing Monolith can't as it has no JS. The most effective way I've found to work through the history of a big YouTube channel is to hold page-down for a while then save to a static \"Web Page, Complete\" HTML file, but it's a bit clunky. reply simpaticoder 7 hours agoparentprevYes, it's a neat thing. I use a node script[1] that wraps the chrome invocation to do CLI driven acceptance testing with a node script that loads the site acceptance tests[2]. I adopted the simple convention to remove all body elements on successful completion, and checking the output string, but I've also considered other methods like embedding a JSON string and parsing it back out. 1 - https://simpatico.io/acceptance.js 2 - https://simpatico.io/acceptance reply wodenokoto 5 hours agoparentprevCan Firefox do the same? reply dotancohen 1 hour agoparentprevThank you for shot-scraper! I've tested it in the past, but something severely missing from all screenshot tools, shot-scraper included, is a way to avoid screenshoting popups. For instance, newsletter or login popups, GDPR popups, etc. If shot-scraper has a reliable way of screenshoting websites while avoiding these popups, I would love to know. I'm on mobile so don't have access to my notes, but I'm pretty sure that a year ago when I tried there was no reliable way to screenshot e.g. the BBC news website without getting the popups. Again, thank you. reply DANmode 38 minutes agorootparentScreenshot the archive.org render? reply samstave 9 hours agoparentprevYay! I love Shot Scrapeer - I wish you had made it a decade ago! Thanks for shot scraper. Off the top of you head what would be the easiest command to have shotscraper barf a directory of shot-scraper HTMLs each day from my daily browsing history. This would be interesting if I have a browsing session for learning something and I am researching across a bunch of sites - roll it all up into a Digi-ography of the sites used in learning that topic? --- I've always been baffled that this isnt an inate functionality in any app/OS - its a damn computeer - I should have a great ability to recall what it displays and what you have been doing. Heck - we need our machines to write us a daily status report for what we did at the end of each day. Surely that would change productivity. If you were force to do a self-digital-confession and stare you ADHD and procrastination right in the face. reply simonw 7 hours agorootparentYeah, things like Archive Box are probably a better bet there. But... you could write a script that queries the SQLite database of your history, figures out the pages you visited then loops through and runs `shot-scraper html ... > ...html` against each one. I just wasted a few minutes trying to get Claude 3 Opus to write me a script - nearly got there but Firefox had my SQLite database locked and I lost interest. My conversation so far is at: https://gist.github.com/simonw/9f20a02f35f7a129b9850988117c0... reply eichin 6 hours agorootparentMy \"cheat\" for \"poke at chrome's sqlite database for current live state\" is that they're always locked but none of them are that big, just make a copy and query the copy. `current-chrome-url` runs in `real 0m0.057s` and does a `jq -r .profile.last_used ~/.config/google-chrome/Local\\ State` to get the profile, then copies `~/.config/google-chrome/\"$PROFILE\"` into a mktemp dir, then `sqlite3 -cmd 'select url from urls where id = (select url from visits where visit_time = (select max(visit_time) from visits));' $H .exit` on that copy. reply genewitch 7 hours agorootparentprevThis used to be fairly simple to do before https everywhere, just install squid (or whatever) and cron the cache folder to a zip file once a day or whatever. There's paid solutions that kinda do what you want, but they capture all text on your screen and OCR it to make it searchable, which at least lets you backtrack and has the added advantage that it will make pdfs, meme images, etc searchable, too. last i heard it was mac only but a few folks mentioned some windows software that does it too. as an aside i don't consider reading/learning nearly all day to be a net negative, even if ADD is to blame. (i haven't had the \"H\" since i was a child.) A status report wouldn't \"stare\" me in the face; in fact, it would be nice to have some language model take the daily report and over time suggest other things to read or possible contradictions to link to. reply jimmySixDOF 9 hours agorootparentprevlook at ArchiveBox from the comments below reply jaimex2 6 hours agoparentprevDoes shot-scraper have a work around for sites that detect headless chrome? ie. news.com.au , nowsecure.nl reply simonw 5 hours agorootparentNo, nothing like that. I wonder how that detection works? I tried this and it took a shot of a \"bot detected\" screen: shot-scraper https://news.com.au/ But when I used interactive mode I could take the screenshot - run this: shot-scraper -i https://news.com.au/ It opens a Chrome window. Then hit \"enter\" in the CLI tool to take the screenshot. reply simonw 5 hours agorootparentGot this to work! shot-scraper https://news.com.au/ \\ --init-script 'delete Object.getPrototypeOf(navigator).webdriver' \\ --user-agent 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:124.0) Gecko/20100101 Firefox/124.0' Code and screenshots and prototype --init-script feature here: https://github.com/simonw/shot-scraper/issues/147#issuecomme... reply nojs 4 hours agorootparentIf you want to go down the nowsecure.nl rabbithole (often used as a benchmark for passing bot detection) [1] is a good resource. It includes a few fixes that undetected-chromedriver doesn’t. 1. https://seleniumbase.io/help_docs/uc_mode/#uc-mode reply Exuma 6 hours agoparentprevMmmm…. This is clever reply k1ck4ss 2 minutes agoprevHow would I archive an on-prem hosted redmine solution (https://www.redmine.org/)? It is many, many years old and I want to abandon it for good but save everything and archive it. Is that possible with monolith? reply andai 9 hours agoprevI always ship single file pages whenever possible. My original reasoning for this was that you should be able to press view source and see everything. (It follows that pages should be reasonably small and readable.) An unexpected side effect is that they are self contained. You can download pages, drag them onto a browser to use them offline, or reupload them. I used to author the whole HTML file at once, but lately I am fond of TypeScript, and made a simple build system to let me write games in TS and have them built to one HTML file. (The sprites are base64 encoded.) On that note, it seems (there is a proposal) that browsers will eventually get support for TypeScript syntax, at which point I won't need a compiler / build step anymore. (Sadly they won't do type checking, but hey... baby steps!) reply _flux 50 minutes agoparent> On that note, it seems (there is a proposal) that browsers will eventually get support for TypeScript syntax, at which point I won't need a compiler / build step anymore. (Sadly they won't do type checking, but hey... baby steps!) Careful what you wish for. Assuming the browser is able to run original TS without processing and you want type checking, then that also seems to effecticely lock the typechecking abilities of TS to their current level. Even without type checking it would already hinder the ability to add new syntax or standard types to TS. Given TS is made for providing expressive typing over JS instead of constructively coming up with a type system with a language, there's still a lot of ground to cover, as can be seen by the improvements made in every TS release. reply slmjkdbtl 8 hours agoparentprevI used to only do single file HTML pages too, until I have a page that have multiple occurrences of the same image, it's wasteful to have the dataurl string every time that img occurs. Maybe I can save the dataurl string in JS once and assign it to those img in JS, but most of the time my page doesn't have any JS it feels bad to use JS just for this. reply csande17 7 hours agorootparentYou could use theelement in an inline SVG to duplicate the same bitmapto multiple parts of the page. reply noduerme 8 hours agoparentprevDoesn't this make the dead screen time rather long if you have to load all the game assets before you can even display a loader? (I guess you don't even have or need progress bars?) reply toastedwedge 9 hours agoparentprevIf I may ask, where can I read more about this? I wouldn't really know where to look for something like that, I'm afraid. Edit: wording. reply teaearlgraycold 8 hours agoparentprevThe proposal: https://github.com/tc39/proposal-type-annotations reply hasty_pudding 7 hours agoparentprevhow do you share CSS between pages? reply lopkeny12ko 11 hours agoprevHow does this compare to SingleFile? https://www.npmjs.com/package/single-file-cli reply gildas 10 hours agoparentAuthor of SingleFile here, one of the major differences is that monolith doesn't use a web browser to take page captures. As a result, it doesn't support JavaScript, for example. SingleFile, on the other hand, requires a Chromium-based browser to be installed. It should also produce smaller pages and is capable of generating ZIP or self-extracting ZIP files. However, it will take longer to capture a page. Note that since version 2, it is now possible to download executable files of the CLI tool [1]. [1] https://github.com/gildas-lormeau/single-file-cli/releases reply darkteflon 10 hours agorootparentSingleFile is amazing - use it tens of times every day across desktop and mobile. Can’t recall a single instance of it breaking. Thank you sincerely for your excellent work. reply gildas 9 hours agorootparentThanks a lot! Believe me, there have been a lot of bugs (+900 issues closed today) because it's hard to save a web page actually. You were lucky not to suffer ;) reply darkteflon 7 hours agorootparentI bet! The proof of that must surely be in how poor a job formats like .webarchive do of it. SingleFile just makes this one really complex, really important thing trivially easy, and in a portable format. For anyone curating a knowledge base it’s an absolute godsend. I didn’t see any donation instructions on your GitHub - I for one would certainly love to chip in if you could point me in the right direction? reply profsummergig 9 hours agorootparentprevHow do you use it on mobile? Is there an app for it? I don't see it on the Google Play store. reply gildas 8 hours agorootparentIt's officially available on Firefox for Android [1] and Safari [2] on mobile. You might also be able to use it with Kiwi Browser [3] on Android. [1] https://addons.mozilla.org/android/addon/single-file/ [2] https://apps.apple.com/app/singlefile-for-safari/id644432254... [3] https://play.google.com/store/apps/details?id=com.kiwibrowse... reply voltaireodactyl 3 hours agorootparentAnybox (on Mac and ios) also supports SingleFile, presenting as a WebDAV server for archives to be saved. It’s flawless and hugely convenient in my experience. reply alpacca-farm 8 hours agorootparentprevJust stumbled across Monolith and SingleFile recently and it's fascinating to see how these tools approach the challenge of web archiving in different ways. SingleFile seems to be a powerhouse, especially for those who rely heavily on JavaScript-laden pages. The ability to produce smaller pages and even generate ZIP files is pretty handy for content archiving and sharing. That said, Monolith's approach of not requiring a web browser could be a game changer for simpler projects or where installing a Chromium-based browser isn't viable. It strikes me as a more straightforward, lightweight solution, albeit with the clear trade-off of not supporting JavaScript. Has anyone run into situations where one tool clearly outperformed the other in real-world usage? I'm particularly curious about the impact on performance and convenience when choosing between these two, especially for mobile use. Also, kudos to the authors and contributors of these tools. The tech community benefits greatly from such innovations that help preserve and share knowledge. reply supriyo-biswas 5 hours agorootparentIs this a LLM generated comment? The structure of this response seems to be too close to the “while X, it’s also important to Y” construction that LLMs like to use. Anyway, to answer your question, lots of pages need JS to work correctly, so using Singlefile is the better option. reply samstave 9 hours agorootparentprevMay you please share what workflow is having you do this so much each day? What do? reply darkteflon 7 hours agorootparentSure! I use SingleFile to save a copy of every article / post / SO & forum discussion I find interesting or useful. I sort them into two buckets: work, and not-work. I’ve been doing this for 10+ years (before SingleFile I used things like .pdf, plain .html, .webarchive files - although these all have drawbacks). In the pre-LLM era, I would then interface with these almost exclusively through a search front-end. I use Houdahspot on Mac and easySearch on iOS. That lets me see everything interesting I’ve read on a particular subject just by typing it in (with the usual caveats that apply to basic keyword search - although in practice that alone has proven very effective). Because it’s just a folder of essentially zipped .html files, there’s no lock-in. Now that we’ve got LLMs, I plug those 10+ years of files straight into my RAG pipeline using llama-index. It’s quite nice :) reply felipefar 7 hours agorootparentSorry for the ignorance, but if the forum posts require login to access then you won't be able to use SingleFile, right? Also, how is the quality of the output generated compared to a .pdf? I'm used to print PDFs from chrome for articles that I want to save, but the layout can become awkward sometimes, and navigation bars can appear several times and hide portions of the text. I like this feature from chrome, but it's not consistently reliable. reply freedomben 5 hours agorootparentIf you use the browser extension, then pages requiring login are no problem because you are already logged in. The output compared to PDF is like night and day. It is high Fidelity versus low Fidelity. At this point now, I only use PDF if for some reason I need it reply n8henrie 7 hours agorootparentprev> requires a Chromium-based browser to be installed Not to try to correct the author here, but it supports geckobrowser as well (not just chromium-based), right? I'm currently trying to package for nixpkgs[0] and am using Firefox for the checkPhase. [0]: https://github.com/NixOS/nixpkgs/pull/283878 reply pbnjeh 3 hours agorootparentI was about to post a similar question: What does this mean for those using the Firefox versions of the extensions (SingleFile as well as the version that zips the result)? reply codazoda 7 hours agorootparentprevWhat does SingleFile do? The intro tells you how to run it, but not what it does. reply freedomben 5 hours agorootparentIt takes whatever is in the Dom of the page you are viewing, and sticks it into a single HTML file that can be served later and will reproduce with high Fidelity the source page. I use it to export an HTML file that I can stick in my logseq archive for later. So much better than just printing to a PDF! reply genewitch 7 hours agorootparentprevFor me it bridged the gap that warped into existence between the time when \"take screenshot\" existed on firefox and when webpages figured out some people did this to archive pages and started putting crap in to either mess with the layout or otherwise \"break\" the resulting file. It snapshots a web page to a single html file. At least that's what i use it for. I use it to both archive stuff and to have proof that some site published something. The next order up would be archivebox or whatever archive.org uses (the name escapes me) - which is a very heavy caching proxy that can save entire websites into a single directory in a way that wget/curl and all the other crawlers cannot. If you care that the exact layout and everything is perfect, right now i think singlefile is aces. reply eviks 5 hours agorootparentprevIt saves a web page into a single file reply mikae1 3 hours agorootparentprev> SingleFile, on the other hand, requires a Chromium-based browser to be installed. I'm using it as a a Firefox extension. Am I missing something? reply Capricorn2481 8 hours agorootparentprevOn the front page, Monolith says it embeds javascript. Are you saying it doesn't use this javascript to render the page before taking a snap shot? reply slmjkdbtl 8 hours agorootparentSounds like they fetch the JS code from the url and embed those code in the HTML, but doesn't have a JS engine to execute those JS. reply throwaway290 8 hours agorootparentprevIt probably means that if this JS fetched more JS it won't be included so if you render offline it will be broken. reply jchook 7 hours agoprevHm, very interesting, especially for bookmarking/archiving. I'm curious, why not use the MHTML standard for this? - AFAIK data URIs have practical length limits that vary per browser. MHTML would enable bundling larger files such as video. - MHTML would avoid transforming meaningful relative URLs into opaque data URIs in the HTML attributes. - MHTML is supported by most major browsers in some way (either natively in Chrome or with an extension in Safari, etc). - MIME defines a standard for putting pure binary data into document parts, so it could avoid the 33% size inflation from base64 encoding. That said, I do not know if the `binary` Content-Transfer-Encoding is widely supported. reply Hamuko 1 hour agoparent>MHTML is supported by most major browsers in some way Firefox? What about mobile versions of browsers? reply al_borland 10 hours agoprevI use read-it-later type services a lot, and save more than I read. On many occasions I've gone back to finally read things and find that the pages no longer exist. I'm thinking moving to some kind of offline archival version would be a better option. reply nelsonfigueroa 10 hours agoparentI've used ArchiveBox in the past and it's been great for this purpose: https://github.com/ArchiveBox/ArchiveBox reply saganus 9 hours agorootparentThis is great, thanks! reply mateo1 9 hours agoparentprevI used to have almost 10k bookmarks that I was keeping from circa 2010 to 2017. Only to realize the majority of them were now useless. Some kind of tool like this is way overdue to become commonspread. reply samstave 9 hours agorootparent(They missed a chance to have a link to a download of the mtnl file of the github page haha) Archive.org and wayback machine should ask for people to submit snaps of pages using this tool directly into the archive - especially during world events. This would allow digtal archeologists to grok the sentiment of the world during that era... (aside: when I interviewed at twitter they asked me what I thought twitter was, and I said I thought it was a global sentiment engine...) But kudos to the world for having us now in the AI birth onto the global internet, as a wayback machine, coupled with AIs and LLMs and this tool - will allow one to ask questions about history in ways that will be very interesting. -- \"What was the general media coverage of [topic] in [decade] with respect to how we currently look at it - and are they articles covering [SUBJECT] in this topic for that time period. etc... reply toomuchtodo 8 hours agorootparenthttps://github.com/palewire/savepagenow https://github.com/jjjake/internetarchive The Internet Archive cannot trust arbitrary content previously archived, so it is more optimal to have whatever archival tools or operations you’re performing to make a request to Wayback to take a snapshot at the same time. If you’re bookmarking something, archive it too! reply samstave 6 hours agorootparentYes, that a better version of what I meant... reply arp242 9 hours agoparentprevI have a lot of old unsorted bookmarks of \"I want to look in to this, but don't have time now\". Newer stuff is more organized, but I exported the old stuff and haven't looked at them in about five years. Last week I started organizing them a bit, and it's shocking how much is a 404. Even from major newspapers and such. I have no idea why anyone would take down old content (outside of some specific and rare reasons). Some are also on neither internet archive or archive.today. reply al_borland 7 hours agorootparentI assume when it happens at big sites it’s from a major site design that doesn’t care to keep backward compatibility with old links. reply genewitch 6 hours agorootparentHow many programmer-hours are required to have a separate page that translates between URI schemes? Your comment, to me, implies that the 404 links' content still exists but is not at a canonical URI anymore. I'm assuming converting stuff like /2018/08/foo.html to /newscheme/fetch?foo or whatever isn't that difficult? This whole thing is one of the reasons i haven't ever set up a blog or even a website that has dynamic content, because i can't be assed to decide on a URI scheme that will \"just work\" with any future engine. Someone has to have written converters, right? I know you can import some blogs to wordpress (and vice versa, export WP to other engines...) reply Martinussen 7 hours agoparentprevhttps://omnivore.app basically entirely filled that void in my life. 100% recommend. reply jcul 2 hours agorootparentHow does it compare to pocket? reply avinassh 4 hours agorootparentprevdoes it archive / save web pages? I am using Omnivore too and I did not find this option. reply amcpu 10 hours agoparentprevI use a locally hosted YaCy instance with cached results to work around this scenario. Much of the content I am interested in is kept locally, so it’s good enough. When I have a bunch of “read later” tabs that pile up, I copy all their URLs into the crawler form with “Store to Web Cache” checked and it accomplishes what I described. Just another option to consider. reply andai 9 hours agoprevDoes anyone know how an entire website can be restored from Wayback Machine? A beloved website of mine had its database deleted. Everything's on Internet Archive, but I think I'd have to (1) scrape it manually (they don't seem to let you download an entire site?), (2) write some python magic to fix the css URLs etc so the site can be reuploaded (and maybe add .html to the URLs? Or just make everything a folder with index.html...) It seems like a fairly common use case but I barely found functional scrapers, let alone anything designed to restore the original content in a useful form. reply gildas 9 hours agoparentIt's documented here: https://wiki.archiveteam.org/index.php?title=Restoring reply belthesar 9 hours agoparentprevI bet the ArchiveTeam might be able to help you out with this. They were quite helpful when I wanted to make sure a site was preserved, and might be able to help you as well, or at least point you in the right direction. https://wiki.archiveteam.org/ reply joeyhage 11 hours agoprevIt would be awesome to see support for following links to a specified depth, similar to [Httrack](https://www.httrack.com/) reply codetrotter 11 hours agoparentI made a basic crawler using Firefox, thirtyfour https://docs.rs/thirtyfour/latest/thirtyfour/ and squid Basically, I took a start URL for the crawl, and my program would load the page in Firefox using thirtyfour, and then extract all links from the page and use some basic rules for keeping track of which ones to visit and in which order. I had Squid proxy configured to save all traffic that passed through it. It worked ok-ish. I only really stopped that project because of a hardware malfunction. The main annoyance that I didn’t get around to solving was being more smart about not trying to load non-html content that was already loaded anyway as part of the page. Because the way I extracted links from the page I also extracted URLs of JS, CSS etc that were referenced. reply gildas 9 hours agoparentprevYou can have a look at the last 2 examples here [1]. [1] https://github.com/gildas-lormeau/single-file-cli?tab=readme... reply arp242 11 hours agoprevI wrote something very similar a few years ago – https://github.com/arp242/singlepage I mostly use it for a few Go programs where I generate HTML; I can \"just\" use links to external stylesheets and JavaScript because that's more convenient to work with, and then process it to produce a single HTML file. reply fagrobot 2 hours agoprevhttps://github.com/gildas-lormeau/SingleFile reply max_ 1 hour agoprevIt still blows my mind that browsers don't provide features this out of the box. reply Alifatisk 52 minutes agoparentI think they do? Have you tried hitting cmd+s or ctrl+s? You can save webpages like that. But I don’t know if they can compress everything into a single html file though. reply vanderZwan 44 minutes agorootparentLast time I tried that it saved a static version of the current DOM, instead of the page source. I'm assuming that the reasoning behind that is that most people want to save a snapshot of what they are currently seeing, and that this is the easiest way to have somewhat reliable results for that. reply max_ 47 minutes agorootparentprevAlot of the CSS & JavaScript is usually broken with ctrl+s. A great option used to be the mhtml format chrome. (It had to be enabled in chrome flags) But mhtml seemed to be removed from chrome since recently. reply publius_0xf3 3 hours agoprevAwesome tool. A note to the devs: the latest version on winget is v2.7.0, which is several months behind the latest version. reply toomuchtodo 11 hours agoprevRelated: CLI tool for saving web pages as a single file - https://news.ycombinator.com/item?id=20774322 - August 2019 (209 comments) reply victorbjorklund 1 hour agoprevThis is great. I have wished for something like this. reply lagt_t 11 hours agoprevI remember IE5 was able to do this lol. It fell out of vogue for some reason, glad to see the concept is still alive. reply berkes 11 hours agoparentFirefox can still do it. reply Hamuko 3 hours agorootparentCan it? I'm only having Firefox save a bunch of files. reply thrdbndndn 8 hours agorootparentprevChrome can too reply phrz 11 hours agoparentprevSafari does this with .webarchive files reply russellbeattie 9 hours agoprevIf anyone is interested, I wrote a long blog post where I analyzed all the various ways of saving HTML pages into a single file, starting back in the 90s. It'll answer a lot of questions asked in this thread (MHTML, SingleFile, web archive, etc.) https://www.russellbeattie.com/notes/posts/the-decades-long-... reply rnewme 6 hours agoparentCool post. You should make hn entry reply stringtoint 10 hours agoprevNice! Reminds me of the time I was working on a browser extension to do this. reply dosourcenotcode 7 hours agoprevA cool tool to be sure. However I feel this tool is a crutch for the stupid way browsers handle web pages and shouldn't be necessary in a sane world. Instead of the bullshit browsers do where they save a page as \"blah.html\" file + \"blah_files\" folder they should instead wrap both in folder that can then later be moved/copied as one unit and still benefit from it's subcomponents being easily accessed / picked apart as desired. reply genewitch 6 hours agoparent\"save as [single] html\" or whatever hasn't worked reliably in over a decade. I wrote a snapshotter that i could post in a slack alternative \"!screenshot \" and it would respond (eventually) with an inline jpeg and a .png link of that URL. As i mentioned upthread, this worked for a couple of years (2017-2020 or so) and then it became unreliable on some sites as well. as an example, old.reddit.com hellthread pages would only render blank white after the first couple dozen comments. I haven't had the heart to try it with singlefile, but now that there's at least 3 tools that claim to do this correctly, i might try again. This tool, singlefile (which i already use but haven't tested on reddit yet) and archivebox. 4 tools, if you count the WARC stuff from archive.org reply causality0 10 hours agoprevHow's this better than the MHTML functionality built into my browser? reply gildas 10 hours agoparentYou can find a comparison of file formats here: https://github.com/gildas-lormeau/SingleFile?tab=readme-ov-f... reply dohello1 10 hours agoprevand I thought my code pages were long haha reply sunshine202022 6 hours agoprevfun reply ethanpil 10 hours agoprevNice. My next step: Figure out how to make a web extension 1 click button. Tab to Monolith to Joplin with a tag. reply gildas 10 hours agoparentYou could download SingleFile [1], configure a WebDAV server in the options page (cf. \"Destination\" section), and set up Joplin to synchronize with the server. [1] https://github.com/gildas-lormeau/SingleFile reply AdieuToLogic 8 hours agoprev [–] Or perhaps wget[0] as described here[1] and documented here[2] could do the trick. 0 - https://www.gnu.org/software/wget/ 1 - https://tinkerlog.dev/journal/downloading-a-webpage-and-all-... 2 - https://www.gnu.org/software/wget/manual/wget.html reply mattsan 8 hours agoparent [–] This is addressed in the README and a comparison is given reply AdieuToLogic 7 hours agorootparent [–] > This is addressed in the README and a comparison is given The only mention of wget in the README reads thusly: If compared to saving websites with wget -mpk, this tool embeds all assets as data URLs and therefore lets browsers render the saved page exactly the way it was on the Internet, even when no network connection is available. This is not the only way to invoke wget in order to download a web page along with its assets. Should the introduction article I referenced above be deemed insufficient, consider this[0] as well. 0 - https://simpleit.rocks/linux/how-to-download-a-website-with-... reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Monolith is a tool enabling users to combine a web page into a single HTML file with embedded CSS, images, and JavaScript assets, offering customization and domain control.",
      "It lacks support for dynamic content but can be complemented by tools like Chromium, while users can engage in the project and explore related initiatives.",
      "The software is distributed without guarantees and is dedicated to the public domain globally."
    ],
    "commentSummary": [
      "Users are discussing tools like Monolith, SingleFile, and ArchiveBox for saving and archiving web pages, showing interest in features such as bypassing bot detection and optimizing web pages.",
      "The significance of archiving web content during crucial events is emphasized, noting challenges like broken links and browser limitations.",
      "Users commend the convenience and advantages of these tools for preserving and organizing online content."
    ],
    "points": 506,
    "commentCount": 99,
    "retryCount": 0,
    "time": 1711313286
  },
  {
    "id": 39808921,
    "title": "Aegis v3.0: Enhanced 2FA App for Android",
    "originLink": "https://github.com/beemdevelopment/Aegis/releases/tag/v3.0",
    "originBody": "beemdevelopment / Aegis Public Notifications Fork 332 Star 7.6k Code Issues 69 Pull requests 1 Actions Projects Wiki Security Insights Releases v3.0 v3.0 Latest Latest Compare alexbakker released this · 6 commits to master since this release v3.0 f9f37d3 New features Material 3 (and Material You) Automatic assignment of icons to entries Ability to select all entries in one go Support for importing 2FAS schema v4 backups Sort entries based on the last time they were used Some clarifications related to importing and backup permission errors Preparations for the ability to assign a single entry to multiple groups Performance improvements when scrolling through an entry list with lots of icons A new look for the third-party licenses list Fixed bugs Directly importing from Authy using root would fail Minor glitches related to animation duration scale settings Various stability improvements Material 3 screenshots Assets 3 47 3 48 45 26 112 people reacted",
    "commentLink": "https://news.ycombinator.com/item?id=39808921",
    "commentBody": "Aegis v3.0 – a free, secure and open source 2FA app for Android (github.com/beemdevelopment)308 points by microflash 16 hours agohidepastfavorite132 comments freedomben 13 hours agoI adore Aegis, and view it as one of the most important apps on my phone. If you use Aegis on Android and use a Gnome-based Linux distro, I highly recommend complementing with Gnome Authenticator[1][2][3][4]. flatpak install flathub com.belmoussaoui.Authenticator Gnome Authenticator is still a little early and buggy (mainly performance issues when you have lots of tokens), but it can import and export Aegis format (and a few others). It's been downright luxurious having my seeds on my phone and my laptop and desktop. [1] https://gitlab.gnome.org/World/Authenticator [2] https://flathub.org/apps/com.belmoussaoui.Authenticator [3] I think (I hope) that Gnome Authenticator will be distributed as part of Gnome at some point in the future, but it isn't yet [4] It's also super easy to build and run from source using Gnome Builder[5]. Just open Builder and clone the source from gitlab, and click the \"Build\" button and it will do its thing [5] https://wiki.gnome.org/Newcomers/BuildProject reply pigpang 4 hours agoparentThere is no way I will install authentificator as flatpak. reply sliken 3 hours agorootparentJust curious, why? reply lxgr 9 hours agoparentprev> It's been downright luxurious having my seeds on my phone and my laptop and desktop. The same is possible for my iOS tool of choice (called \"OTP auth\"). It can also synchronize to iCloud (passphrase encrypted) and make use of that on macOS. I've resisted the temptation of that comfort so far (and of just putting the TOTP seeds into Bitwarden or 1Password), because it does seem a lot like collapsing what's now definitely two or maybe three factors into two or sometimes only one. reply shortsunblack 2 hours agoparentprevWhich is idiotic, as having your seeds on your desktop no longer makes it two factor authentication, rendering the use of phrase factually incorrect. reply reddalo 1 hour agorootparentIf you login on some services on your phone, you have the same problem. 2FA protects you mainly from password leaks, not from people phisically accessing your devices. reply izacus 1 hour agorootparentprevThat is neither true nor idiotic. reply billforsternz 4 hours agoprevI'm a veteran developer, but really more of a \"normal\" than the type of developer who is commenting on this story. I admit I find this stuff really, really, really confusing. I hate dealing with any of this stuff, I don't do it voluntarily for the same reasons I don't (for example) use pretty good privacy for email (I just use web gmail like a regular person). Anyway, I do (involuntarily) use 2FA for two services, and managed to set myself up with Google Authenticator on my Android phone. Both services that onboarded me for this explained it really poorly, but at least got me hooked up and I now routinely (and reluctantly) login to those services this way. Reading this I suddenly realised, whoaaa, if I lose my phone do I lose access to those (important) services? Well no, I hope not at least, when I look at the Authenticator app it has the green \"your codes are being saved to your google account\" cloud icon. That's kind of reassuring. I suppose. I'm not really sure what my point is, other than online security is an ever more important issue, it's a swamp and even many technical people who might know everything there is to know about some arcane corner of the technology universe don't necessarily properly understand it. Although I suspect most would not be prepared to admit it like I just did. Actual normal people (like my wife for example) have absolutely no chance of getting on top of the details and navigating their way to a best practice solution. I hope Google (or Apple) don't either give up on this or go full evil, that would be really bad. I think I will check out whether my two services can give me recovery codes. I am confident I can manage vital username/password combinations and recovery codes, that's the level of sophistication (or not) I'm comfortable with in this space. reply BlackFly 46 minutes agoparentFor my personal threat model, most 2FA flows decrease my security. Historically, the loss of my phone is a more likelier event than a compromise of my credentials and the damage to me and the businesses holding my accounts from loss of my credentials is much less than the denial of service from loss of access. This flips with some access from my employer and my bank, but I don't associate any of my personal devices with my employer's account in any way. It is a shame how the industry seems to think that security is some single dimension along which things are more or less secure. Denial of service for personal accounts is often times more damning and common than account compromise. 2FA makes me less secure in some cases. reply daqhris 1 hour agoparentprevThe current state of technology seems frightening indeed. This 2FA is a miracle. It is free and independent of the big tech companies. I'd put it on the same level of importance as Mozilla products. In the future, we will see more proof-of-personality applications for security reasons. But recovery codes won't be going out of fashion any time soon. Unless, of course, AI-enabled developers are gifted with long-term memory in the next few years. reply GoblinSlayer 2 hours agoparentprevBut can you access your google account if you lose your phone? That account belongs to google, not to you. reply cosmojg 11 hours agoprevBitwarden and KeePassXC also provide free, secure, and open-source 2FA in addition to password management. I keep my TOTP secret keys separate from my passwords simply by storing them in separate vaults. I don't know why anyone would use anything else (although I'd love for someone to comment and tell me). reply Sander_Marechal 51 minutes agoparentThe main downside for me with using Bitwarden for 2FA TOTP is that I can't add more than one TOTP to a password. At work we use Active Directory so I can access a lot of sites with the same account, but all those sites have a different TOTP. I can only store one in Bitwarden and I am forced to store the rest on my phone in a regular authenticator app. I don't want to create duplicate accounts in Bitwarden just for the TOTP. reply Xaiph_Rahci 7 hours agoparentprevBecause doing this reduces the 2FA into 1FA (i.e. there is no longer a possession factor). reply bobbylarrybobby 5 hours agorootparentYes, if your vault is hacked, your 2fa will become 1fa, but: - 2fa is still good for stopping someone who steals your password but not your whole vault - 2fa blocks people from guessing your password (through brute force etc) So there is still quite a bit of benefit. reply aryonoco 5 hours agorootparentprevThere are different ways to avoid this. Nearly all of my 2FA are in Bitwarden, because it's just so damn convenient. But my Bitwarden itself uses YubiKey as 2FA. Since I adopted this setup last year, it's been the best if both worlds for me. reply 911e 5 hours agorootparentprevThe goal is to protect your data from brute force not from yourself, it’s perfectly reasonable to have 2fa in your password manager, saying it’s 1fa is just fud reply aryonoco 5 hours agorootparentIt's not fud. 2FA traditionally means relying on one thing you know (i.e. a password) plus one thing you have, or one thing you are (biometrics). Every single one of my passwords is unique and randomly generated and at least 32 characters, none of them are getting brute forced unless there is a sudden gigantic leap in quantum computing. And if that happens, the world has bigger problems than my passwords. Having a separate identity factor, something that I own, is not to save me from myself. It's to save me if someone steals my phone or laptop and is able to get into it. Now we all face different threat models and if your threat model doesn't call for having a totally separate identity factor, great! There's nothing wrong with that. But we don't all face your threat model, and some of us do indeed need a second identity factor that's not stored in the same place as the password. reply polygamous_bat 5 hours agorootparent> Every single one of my passwords is unique and randomly generated and at least 32 characters, none of them are getting brute forced unless there is a sudden gigantic leap in quantum computing. One of the threat models that I consider is there being a bug in the particular RNG/encryption algorithm implementation used to get that encrypted password. In that case, my password can possibly be brute forced much faster than purely random guessing. reply Alpha3031 5 hours agorootparentprev... The posession factor is the encrypted file that stores your secrets. It is in fact the same factor that Aegis uses, because it also uses an encrypted file to store your secrets. I'm not sure what you're expecting Aegis to do that is different from storing TOTP secrets in an encrypted file. reply jrm4 14 hours agoprevSince we're here: Anyone else dealing with the stupid thing where your organization won't let you have your generating token thing and instead force you into e.g. Duo? I have only one, and its frustrating. I know it's probably breakable with rooted Android or something but haven't had much time to look into it (or fight it) reply explosion-s 14 hours agoparentDuo lets you use a physical security key, I then use bitwarden to store that as a passkey. Not quite a full replacement but good enough for me (you can also self host bitwarden [0]) [0] Vaultwarden reply genpfault 8 hours agoparentprevAegis will happily slurp the secrets out from Duo on a rooted device. reply devsda 6 hours agorootparentSome organizations disable totp entirely. Instead, everytime there's a login attempt a pop-up notification with \"allow\" & \"deny\" options is pushed to the registered device. reply lxgr 9 hours agoparentprevTOTPs are inherently less secure than many of these proprietary solutions (they don't offer control over where people store them and whether they create backups of them, for one thing), so I do somewhat understand companies preferring those. reply wofo 15 hours agoprevAegis should really be more well-known IMO. I installed it on an old phone that didn't have enough storage for Google Authenticator and was really pleased with the app. The fact that it's a community project is also a nice bonus. reply mrd3v0 14 hours agoparent> The fact that it's a community project is also a nice bonus. It is more than a bonus. This is the only kind of project you know that tomorrow, the day after or a year from now there wouldn't have profit incentives or a pending IPO to completely abuse your experience as a user and extract as much profit as possible. reply rkagerer 10 hours agoprevWhat's the backup story like? Can you do an encrypted backup on demand (protected with a password you supply)? Is there any desktop app such backup can be opened/read with (or even eg. read with something like sqlite db browser)? Can the app be configured to save an encrypted copy to eg. Dropbox whenever changes are made? Is it recommended to install from Play store, or the APK off GitHub? reply logicprog 10 hours agoparentI use Aegis as my main app for 2FA so I can answer these questions: > Can you do an encrypted backup on demand (protected with a password you supply)? Yes! > Is there any desktop app such backup can be opened/read with (or even eg. read with something like sqlite db browser)? It's just plain JSON once decrypted, so it's always readable; I do know the GNOME Circle app \"Authenticator\" can natively import Aegis backups as well, since it's what I use on my desktop machine, but I don't know what other apps exist. > Can the app be configured to save an encrypted copy to eg. Dropbox whenever changes are made? It does have some facilities for automatic and cloud backups judging from the settings page, but I've never tried them > Is it recommended to install from Play store, or the APK off GitHub? If you do the latter you'd lose automatic updates. I used F-Droid. reply greenmartian 5 hours agorootparent>If you do the latter you'd lose automatic updates. Obtainium[1] will give you automatic updates from most sources, including Github/Gitlab/Codeberg and F-Droid repos. Especially relevant to this discussion, since Aegis 3.0 hasn't hit F-Droid yet, as at the writing of this comment. [1] https://github.com/ImranR98/Obtainium reply SushiHippie 15 hours agoprevI really like it that more and more apps start using Material 3/You. Apples UI design was never my cup of tea, but I love the consistency of UI design in most iOS apps, compared to the wild UI inconsistencies on Android. reply mrd3v0 15 hours agoparentI'd take a more diverse UI experience on Android any day over a more polished yet heavily opinionated experience at iOS. That being said, I feel like the main complaint about Android apps design is the fact that a lot of apps are just horrible half-assed implementations of old Material UI slapped together on a drag and drop editor like the Android Studio widget system. Offering an incentive for people to build anything and make money off data collection and ads without the corporate tyranny of Apple results in just that. So apps that are on FOSS repositories such as F-Droid are usually much cleaner to use, despite their UI/UX being just as diverse. reply SushiHippie 12 hours agorootparentAgreed I've never owned an iPhone, so I'm kind of used to the android experience and don't mind that much, but I'm just happy that it gets more and more consistent, at least the apps I use. Though I only use FOSS apps so I can't speak for the playstore apps. reply e12e 8 hours agoprevThose interested in this, might also be interested in Ente auth: https://github.com/ente-io/ente/tree/main/auth reply Fervicus 7 hours agoparentHappy Ente user on ios. reply ParetoOptimal 14 hours agoprevAegis is good and I enjoy using it. I hope others don't follow Microsoft Authenticators footsteps in creating their own Authenticator, saying others are insecure, and not allowing Authenticators like Aegis. reply Narushia 13 hours agoprevI'm currently a happy user of 2FAS[1], any idea how Aegis compares to it? A quick search suggests that Aegis doesn't support multiple devices and is not available on desktop. [1]: https://2fas.com/ reply brandensilva 13 hours agoparentLet's hope they add a desktop app. I'm on that screen more than my phone. I'm not one to care about having my phone on me all the time. reply occam65 12 hours agoprevI've been using Aegis for a number of years, and have found nothing I don't like about it. It's a perfectly functional app, and I'm looking forward to trying out the new update! reply Semaphor 4 hours agoparentIt’s why I was a bit scared when reading the title (though the screenshot makes it look like I’ll be fine), I’ve had two open source apps make their UI/UX vastly worse recently with major updates (granted, I’m assuming some people like the changes), one was Gajim (XMPP messenger) on Desktop, and another Breathly (guided breathing) on mobile. reply bonki 10 hours agoparentprevI totally agree, it's probably the only app I ever used that I would consider flawless. reply sunng 14 hours agoprevI have being using andOTP for years but the development seems to halt, also it's no longer available from f-droid. The feature that backup with gpg encryption is broken. I hope it's possible to import my otps from andotp into aegis. Also the backup encryption with gpg (openkeychain) is welcomed. reply CorrectHorseBat 12 hours agoparentYes it's possible to import from andOTP reply tremarley 12 hours agoprevAre there any good 2FA applications for Desktop? Using the phone to authenticate every login seems very inefficient. Some of us do not like using the phone. reply alisonatwork 4 hours agoparentI use KeePass. It's a little bit cryptic to set up the OTP - you have to create an advanced field called TimeOtp-Secret-Base32 with the seed in it - but after that you just Ctrl-T to get the latest code. reply mhitza 12 hours agoparentprevI use Bitwarden for passwords and 2FA (browser plugin, Android app for mobile). Definitely not recommended by anyone security focused, but these 2fa are forced onto me by different platforms and not something I chose/care. There should be desktop authenticator software. If I can have one on Linux I'm sure all the other desktop OSs have at least 1. reply usr1106 2 hours agoparentprevI use my own 15 lines of Python code calling pyotp. At least for desktop means preferably CLI. reply Zizizizz 11 hours agoparentprevhttps://github.com/tadfisher/pass-otp reply jonotime 11 hours agoparentprevI use keepass on phone and desktop reply nicoco 15 hours agoprevGreat app that does the job! The kind I don't mind installing on my phone. I use it for nextcloud, github and my microsoft account (it was really buried in the settings but it is possible to avoid using MS auth something app). reply TrailMixRaisin 14 hours agoprevI use this app and are very happy. For me the selling point was the possibility to backup my profile and therefore all the configured keys. reply nogajun 9 hours agoprevFreeOTP, supported by Redhat, is another open source 2FA application. I use it. FreeOTP: https://freeotp.github.io/ reply noman-land 14 hours agoprevAegis is really great. So nice not to use proprietary authenticators. And it can do import and export. Does anyone know the history of this project? It seems legit but an authenticator is a pretty sensitive application so making sure this app is trustworthy is a little more important than for other apps. reply nzeid 14 hours agoprevI happened upon this app recently when I was frantically searching for a Google replacement. Couldn't believe something this polished was lurking. I used another open source app several years ago but it got discontinued (FreeTOTP or something). reply Zuiii 7 hours agoprevTruly open-source, available on f-droid, works on everything including low-end android hardware with everything except microsoft (because microsoft). What's not to like. reply abhinavk 6 hours agoprevWhat do you guys prefer to use on iOS? reply alibert 2 hours agoparentI use OTP Auth, it doesn’t get updated much but still maintained. I consider it « feature complete ». https://apps.apple.com/fr/app/otp-auth/id659877384 reply Ringz 1 hour agoparentprevOTP Auth. In use for years. Backups, iCloud Sync, Widgets, Folders. Perfekt. reply halJordan 5 hours agoparentprevRaivo is a source-available ios app. It has exports, icloud sync and can generate a new QR code. Keychain is also not bad. reply microflash 26 minutes agorootparentRaivo was acquired by Mobime[1] and since then the status of project has been up in the air and so is its privacy policy. [1]: https://twitter.com/RaivoOTP/status/1683372954002808833 reply KTibow 7 hours agoprevWhile we're talking about places to use 2FA, if you have a watch it might be a good idea to put your 2FA codes there for redundancy. reply korm 12 hours agoprevHere's a utility to convert exported Aegis JSON to a Keepass 2 or KeepassXC database if anyone's interested https://github.com/GeKorm/atk (binaries in the releases page) reply graynk 7 hours agoparentThanks for forking and supporting my initial clumsy tool, glad to see someone else found it useful :) reply korm 51 minutes agorootparentNot only did your tool help me with Aegis & Keepass, but it introduced me to Go, so thank you 2x for it! reply jonotime 11 hours agoprevThis looks very nice. Had I not just moved all my 2FA to keepass, I would give it a go. My setup: mac desktop, linux desktops, android with syncthing to tie it all together. reply kristjank 14 hours agoprevI used it until I switched to KeePassXC for all of my secret management means, but it's still a great app to fall back to, and allows for simple information exchange when moving to another app. reply RandomGuy456 14 hours agoparentHi! I use both plus Syncthing to automatically backup my vault to the pc. Great combo! reply lern_too_spel 12 hours agoprevJust use Bitwarden. The UI is clunkier, but the UX is better. After it fills in the username and password, it puts the OTP in the clipboard, so you can just paste and go without opening an app and manually copying it into the login form. reply belthesar 12 hours agoparentI'm hard opposed to storing my second factor codes alongside my first factors. Part of the reason why I use 2FA is because if my password store is compromised, the accounts in it that are compromised do not contain all of the credentials necessary to log into the accounts protected by 2FA. I also do not store my emergency removal codes in the same secret store as my passwords for this reason. reply tremarley 12 hours agoparentprevUsing a password manager as your authenticator seems very risky to me. You should use separate services. If your password manager is breached, at least the infiltrator cannot pass 2FA. reply lern_too_spel 12 hours agorootparentIt is no different from Passkeys in that regard, yet we're fine with that. If you want extra security, you would have your password manager and your OTP generator on different devices, but only a small fraction of people do that. Storing your OTP generator secrets and your passwords in the same app provides a reasonable trade-off between security and convenience for most people. reply Timber-6539 6 hours agoprevLong ago, I used Google Authenticator to store 2FA tokens without giving it much thought. When I lost the app data to a phone reset, I also lost my 2FA tokens. Got lucky I didn't have many tokens saved at the time and was able to restore all the important accounts despite losing the tokens. Even though it was my fault for not reading the T&C of the Google Authenticator app, I cursed Google for creating an inferior product on an OS they controlled. What was the use of requiring login with a Google account on the Android device if you are not going to persist this kind of data. Then I moved to Authy which syncs and stores your tokens online to their cloud, allaying all the fears I had from previous experience. Incidentally another phone reset happened. Now Authy allows you to access your tokens \"locally\" to any device that can install their app or browser extension. Using more than one \"device\" locally gives you data redundancy. I cannot just trust a browser extension with my 2FA tokens (yikes), so at the time I only had my Android device with the tokens locally. When this \"trusted device\" (read app data) was lost I had to request support for a reset to gain back my data from Authy. That process takes 48 hours after initiating the reset. (The app data counts as a device, not the other way around; this is the crux of my problem with 2FA application design.) As soon as I got my tokens back I moved to Aegis and never looked back. I can export backups, save them encrypted on any location and import them anytime without fear of losing app data aka device. reply borplk 8 hours agoprevDoes it support folders for separating entries? I like to separate work from personal entries. reply microflash 8 hours agoparentIt supports Groups to organize entries. reply panick21_ 14 hours agoprevI use one that has to be activated with the Yubikey over NFC. Pretty slick. reply sebastiennight 14 hours agoprevLast year Google Authenticator started syncing secrets to the cloud[0] which means that those secrets can now be accessed in new ways outside of the user's control[1], which resulted in a huge breach at a startup called Retool[2]. From then on I started moving my company's team and contractors (as well as family and friends) off of Google Auth and onto Aegis. The app is clean, easy to use, open source, has all the options we could dream off. (and its privacy policy isn't tens-of-pages-long like some other apps, where privacy seemed to be part of the marketing strategy but not the product itself) I've been a very happy user. [0]: https://news.ycombinator.com/item?id=35690398 [1]: https://news.ycombinator.com/item?id=35708869 [2]: https://news.ycombinator.com/item?id=37500895 reply tptacek 6 hours agoparentPoint of order: Retool got hit by an SMS phishing attack, and while they made a big deal out of Google syncing TOTP seeds, the real moral of their story is to stop using TOTP altogether; it's not phishing-resistant. Rather than cutting a whole team from one TOTP app to another, it would probably be a better idea to shift the whole team to an IdP that forces FIDO2. TOTP is obsolete. reply Elbrus 11 hours agoparentprev> I started moving my company's team and contractors (as well as family and friends) ... onto Aegis An important question on this, if you don't mind: If the phone, where Aegis was installed, is dead/lost/stolen, which options are available to make sure that access to the accounts linked to that phone wouldn't be lost either? reply thombles 10 hours agorootparentEvery service I've used with TOTP codes (12 at current count) has given me some sort of randomised backup token at the same time to use if I lose my 2FA app. I store those somewhere separate. I'm not going to argue that this is user-friendly but AFAIK there's no reason you're obliged to use cloud backups today. reply lxgr 9 hours agorootparentThis is actually a pattern I really don't like: Why do I mostly get these thrown at me for TOTP, but not other 2FA methods? What am I supposed to do with these \"backup codes\"? Store them all in my password manager? At that point, I might as well store the TOTP seed there and rely on its multifactor authentication – which is probably fair for many use cases, but suffers from the problem outlined by GP. I think sites should treat TOTPs effectively equivalent to Passkeys, i.e. as maybe synced, maybe backed up, but maybe neither – and then the user needs an alternative login method, just like for all 2FA methods. reply brewdad 8 hours agorootparentPersonally, I use Bitwarden for passwords and store my 2FA seeds in a Keepass database on my PC and backed up to my cloud. It isn't perfect by any means but at least if my Bitwarden gets compromised, my 2FA tokens are safe and vice versa. If I lose control of both, welp, it's gonna be a bad time I guess. reply sowbug 7 hours agorootparentprevChoosing 2FA segregates users into two buckets. Most people are satisfied with the risk of allowing password reset emails and social engineering attacks. They don't pick 2FA. The rest are generally more sophisticated users, and are willing to risk loss of the entire account if they lose their credentials. That's the price for an overall increase in account security. From this perspective, it makes sense to provide backup codes as another tool in the DIY account-management toolbox. These buckets oversimplify the situation, but they help explain why backup codes are offered as last-ditch authentication for 2FA. reply ploxiln 7 hours agorootparentprevI write the backup codes in a small paper notebook, kept in a drawer at home. I've had to use it maybe once in the past decade. It's very unlikely that I'll lose both my phone and the notebook at the same time. It's extremely unlikely that anyone who breaks into my house and finds this notebook, can do anything with it. reply microflash 10 hours agorootparentprevYou can optionally backup your encrypted data using Android's built-in backup utility tied to your Google account. It can, then, automatically restore codes when you sign in on a new device. reply lxgr 9 hours agorootparentDoes that back up TOTP seeds in Google Authenticator? I thought apps had to opt in to this type of backup and would assume that Authenticator doesn't, but Google has changed the Android backup mechanism so many times, I lost track. reply microflash 7 hours agorootparentIt backs up TOTP seeds. You need to enable this manually. As a part of Android Device backup, Google does backup certain things automatically[1] but non-Google apps require explicit opt-in. [1]: https://support.google.com/googleone/answer/9149304?hl=en&co... reply lxgr 5 hours agorootparentInteresting, thank you! I would have expected Google Authenticator to somehow tangle the encryption keys used to the device it’s running on, but apparently it doesn’t if this works. reply bonki 10 hours agorootparentprevIt has android cloud as well as automatic local backups. I do automatic local backups and use Syncthing to sync them off my phone. Works a charm! reply vraylle 7 hours agorootparentprevIt can put encrypted backups on almost any cloud service, pCloud in my case. reply Zuiii 7 hours agorootparentprevAegis can export an encrypted backup file that can be imported on another phone. reply teekert 13 hours agoparentprevHow do you make sure normies you support don't loose their TOTP private keys? Do you ask them to back them up? On iOS I use MS authenticator with backup in iCloud, I'm just too scared of losing the keys. I advise muggles to do the same. reply crossroadsguy 7 hours agorootparentThis is the fear I went back to Authy from Raivo (iirc they didn’t sync back then; besides I have never trusted iCloud sync) and Tofu. I lost access to two accounts that I would have wanted to avoid. By the way, a few days back Twitter simply decided to stop accepting my 2FA keys. Luckily I was logged-in in another browser as well where I could disable 2FA. And no it didn’t accept backup code either. So that’s another risk with 2FAs. What I am trying to say is — 2FA synced or not synced are problematic either way. I am also wary of passkeys! What happens if my Apple (for example) account is disabled or I am locked out of it? reply snibsnib 12 hours agorootparentprevIt does support android cloud backups, but I usually have an encrypted export saved somewhere else. reply warkdarrior 13 hours agoparentprevSyncing to the cloud is an opt-in setting in Google Auth. reply cmiles74 13 hours agorootparentIt was not in my case. I found out about this feature when I saw the green cloud icon and pressed it to find out what it meant. At that time I was made aware the my data was saved in my Google account. reply sangnoir 11 hours agorootparentIt's likely your company's administrator forced this default behavior. Cloud sync was an opt-in on my account too, fwiw. It would have been a huge story had it been opt-out. reply lxgr 9 hours agorootparentprevIt's opt-in, yes, but via a dark pattern of \"hey, cool new thing, say yes quickly?\" as far as I remember it. I remember getting tripped up by this, because I also have work credentials in there that by policy I'm not supposed to store in a synchronizing TOTP client, and Google Authenticator didn't even allow reviewing the TOTP seeds for the longest time, so this seemed like quite the departure from their previous security stance. reply ckcheng 13 hours agoparentprevThat sounded scary, but after reading into the Retool breach (thanks for pointing it out), it doesn't sound like Google Authenticator is completely to blame. Retool points out the \"attacker was able to navigate through multiple layers of security\" [0], i.e.: 1. \"through a SMS-based phishing attack\" on \"Several employees\" 2. \"one employee logged into the [SMS phishing] link\", \"logging into the fake portal\" 3. \"attacker called the [phished] employee\" \"and deepfaked our [IT team] employee’s actual voice\" 4. \"the [phished] employee grew more and more suspicious, but unfortunately did provide the attacker one ... (MFA) code\" (over the call) 5. \"The additional OTP token shared over the call was critical, because it allowed the attacker to add their own personal device to the employee’s Okta account, which allowed them to produce their own Okta MFA from that point forward.\" 6. \"This enabled them to have an active GSuite session on that device.\" With \"Google Authenticator synchronization feature that syncs MFA codes to the cloud\", \"if your Google account is compromised, so now are your MFA codes\". By #5, I'm thinking GA sync is about as blameworthy as Okta for allowing a device to be added with just a single additional OTP token shared over a phone call? Here's a different perspective (tptacek) [1]: >> We use OTPs extensively at Retool: it’s how we authenticate into [Google, Okta, internal VPN and Retool] > They should stop using OTPs. OTPs are obsolete. For the past decade, the industry has been migrating from OTPs to phishing-proof authenticators: U2F, then WebAuthn, and now Passkeys†. The entire motivation for these new 2FA schemes is that OTPs are susceptible to phishing, and it is practically impossible to prevent phishing attacks with real user populations > TOTP is dead. SMS is whatever \"past dead\" is. Whatever your system of record is for authentication (Okta, Google, what have you), it needs to require phishing-resistant authentication. > My only concern is the present tense in this post about OTPs, and the diagnosis of the problem this post reached. The problem here isn't software custody of secrets. It's authenticators that only authenticate one way, from the user to the service. [0] https://retool.com/blog/mfa-isnt-mfa [1] https://news.ycombinator.com/item?id=37503551 reply lxgr 9 hours agorootparent> it doesn't sound like Google Authenticator is completely to blame. I don't think anyone would seriously claim that, but I think it's fair to call it an unfortunate additional hole in the swiss cheese. reply ckcheng 4 hours agorootparentI only started reading into the Retool case because there was the claim above that sounded serious and scary: > Google Authenticator started syncing secrets to the cloud[0] which means that those secrets can now be accessed in new ways outside of the user's control[1], which resulted in a huge breach at a startup called Retool[2]. reply yoavm 14 hours agoprev [–] I love Aegis but I can't help but think that it's sad we ended up in this place with regards to 2FA. When all these temporary codes started they were sent over SMS, which was insecure but at least all I needed to do was to pick up my phone. Nowadays I open Aegis and I have > 20 services there, and trying to look for my code between all the running numbers is a pain. It would have been so much more comfortable if we flipped this around a little - the website would present a QR code, you would open the phone and scan the code, the phone would make a request signed with your key to a URL, and the website would authenticate you because by making this signed request you proved that \"something you have\" part is done. It feels like when the 2FA thing started no one considered that sooner or later all services will require it, and the UX will be terrible. reply OJFord 11 hours agoparentI almost^ missed a train recently because I tried to book my ticket and for the first time ever (and actually not since either) Amex wanted to send me a verification code. They support only SMS & email, but you can't change it for the current one and it was set to SMS, and I don't have email on my phone anyway & was at the station. Anyway - SMS didn't arrive. Had the same thing recently with them from a bank, it's the network blocking them, suspected spam or whatever. There's plenty of other reasons not to use SMS 2FA, but it might suddenly not work one day right when you need it, and totally out of your control, is perhaps the most universally compelling? reply yoavm 3 hours agorootparentI'm totally not advocating for 2FA over SMS. It's also just not secure enough. What if the website presented a QR you can scan with Aegis and then Aegis would make a request with your one time code? You could still type it manually - there would be an input and a QR code next to it. reply sliken 3 hours agorootparentKinda surprised banks aren't more current with security, after all even NIST recognizes the problem with 2FA over SMS: https://www.nist.gov/blogs/cybersecurity-insights/questionsa... reply lxgr 9 hours agorootparentprevFortunately, SMS-OTP isn't considered SCA/PSD2 compliant anymore (by itself, since it's only a single factor – and the card number doesn't count) by the regulator in the EU (not sure about the UK), so hopefully we'll be seeing less of that going forward. reply qingcharles 9 hours agorootparentprevHad this crap happen to me the other day with a banking app. Luckily I eventually managed to find a way to get them to call my phone with the code instead. There are so many problems with SMS. reply chippiewill 10 hours agoparentprevI used to quite like the hack that LastPass authenticator had to make it easier. If you ever encountered a TOTP form in your browser that the LastPass browser extension recognised it would send a push notification to the aunthenticator app on your phone which if approved would send the TOTP code to your browser and submit the form on your behalf. Unfortunately it only ever worked on the handful of websites Lastpass had implemented bespoke support for, but it was magic when it worked. It would be nice to have a universal standard for push notification 2FA. reply lxgr 9 hours agoparentprev> the website would present a QR code, you would open the phone and scan the code, the phone would make a request signed with your key to a URL, and the website would authenticate you because by making this signed request you proved that \"something you have\" part is done. WebAuthN essentially gives you that behavior, with the addition of making it MITM-resistant (which TOTP isn't). It even works cross-platform these days (I think both devices need Bluetooth as a proof-of-proximity, to make sure an adversary isn't relaying you a QR code). Unfortunately both iOS and Android absolutely insist on syncing these credentials to the cloud, but both now have APIs that would allow a third party to provide a local-only backend. reply eightnoteight 8 hours agoparentprev> Nowadays I open Aegis and I have > 20 services there, and trying to look for my code between all the running numbers is a pain. exactly :( I wish passkeys get rolled out quickly across all sites, most people use just 2 or 3 trusted devices 99% of the time. for those edge cases where you are working on an untrusted device, the passkey on your trusted mobile can help with authentication via Bluetooth or some QR code etc,... reply freedomben 13 hours agoparentprevI agree this kind of sucks (I have about 40 tokens on there), but it's relatively well mitigated with the search functionality and typing the first few characters of the service. This works for all that I've tried except the root MFA token from AWS, and I could easily fix that by exporting and changing the name and re-importing if I wanted to. This has two things about it that make me actively not want it: 1. Does not work offline (requires an internet connection to work). The current design for TOTP is super flexible as they only require time syncronization, which doesn't require an internet connection. 2. It means I have to install an app for each service, which I absoulutely do not want to do. I would prefer to only use native apps for things that actually need to be native. PWAs and web UIs are strongly preferred for me. A comprehensive and robust way to manage permissions would mitigate my dislike for native apps somewhat, but this is getting harder and harder (though praise be unto GrapheneOS for their efforts!) From an engineering perspective, it also feels like unnecesary bloat/complexity and coupling. reply yoavm 13 hours agorootparentI agree regarding the offline ability, though literally all the things I'm using 2FA for are online, as they are about logging-in to services. As for the 2nd point - I definitely don't think it has to be a separate app for each service. Why would it? Imagine an app that holds a private key, the website showing a QR code, you scan it with the app, the app sends the public key to the service using a URL provided in the QR code, and the service stores your public key. From now on, every time you want to login you're asked to scan a QR code, which makes the app send a signed request to the a URL encoded in it. The service gets the request and proceeds with the login. One app, all services. reply cam_l 7 hours agoparentprevAt least with aegis I can create groups. So I can separate email, work stuff, etc and only see a few tokens in each group. Sure the ui isn't great. And it takes a couple extra clicks to the groups. But given the stupid shit that sites do like disabling paste for two factor codes or passwords, i just would never trust them to not fuck up a more streamlined solution. I like a bit more manual control sometimes, at the expense of convenience. And it also runs and is backed up completely offline, which is nice. reply LinAGKar 14 hours agoparentprevSome can do that in their own app, e.g. Steam Guard. Too bad that's not a standard. But the FIDO2/webauthn stuff may be similar. reply oefrha 7 hours agorootparentIf you do that in your own app, you might as well just show a QR code which the user scans and opens up the app for approval. Code as backup. Having to type in an alphanumeric code (talking about Steam Guard, not standard TOTP) when the app is already involved is quite outdated. I believe Steam Guard already added the scan QR code flow a while ago? reply lxgr 9 hours agorootparentprevYou raise a good point: It seems like this should be possible to integrate into WebAuthN somehow, but currently it isn't. Passkeys could be coupled with Web Push somehow, for a \"confirm action x\" type of experience pushed to your (networked) authenticator even when you're not at the website of the relying party that owns it. reply bonki 10 hours agorootparentprevSteam uses \"standard\" TOTP but displays the code in a non-standard format. You used to be able to extract the secret and use e.g. KeePass to provide the code in the Steam Guard format. reply gsich 9 hours agorootparentYou still can. reply panick21_ 14 hours agorootparentprevI don't want that to be the standard. I don't want 20 different apps for each bank and each provider. reply LinAGKar 12 hours agorootparentNo, but a standard so that any app could implement that functionality and have it work with any service that supports it reply bonki 10 hours agoparentprevI use Aegis only as backup because the workflow is cumbersome as you say (the app itself is flawless in my book), my primary TOTP authenticator is KeePass. I use the exact same KeePass setup on Linux, Windows and MacOS and copying a TOTP token is only a hotkey away. My KeePass also acts as SSH agent which also works with Cygwin, MSYS and WSL. On Android I use KeePassDX which also supports TOTP. I almost never ever open Aegis and when I do it's mostly to check that I can remember the password. I sync my KeePass databases across all devices and OSes with Syncthing. reply pnw 10 hours agorootparentAre you using the original Keepass or KeepassXC? I need to get off Authy now they've dropped their desktop apps. reply bonki 10 hours agorootparentThe original, never liked KeePassXC and it doesn't support plugins. I also used Authy until I migrated TOTP to KeePass and Aegis years ago :) reply noman-land 13 hours agoparentprevCouple things no one's mentioned yet. In Aegis you can add icons to token slots, and manually sort them (alphabetically). This, plus searching, helps a lot in finding tokens quickly. They have pre-existing icons for most of the common sites. reply eipi10_hn 13 hours agoparentprev> Nowadays I open Aegis and I have > 20 services there, and trying to look for my code between all the running numbers is a pain. I just click search and type 2-3 characters and most of the time I can see what I need right away. I'm using way over 20 services with 2FA and that's really the least of my concern. And I actually don't use search that much since Aegis also has a feature of sorting by the usage so whatever I'm using regularly are already at the top for me. reply kmlx 14 hours agoparentprev> the UX will be terrible. if you run safari and store all your passwords using icloud “passwords”, safari will automatically prefill the 2fa code. i assume this is the case for other browsers as well? reply fcsp 14 hours agoparentprevThat's pretty close to webauthn, which works very nicely with yubikeys if the service supports it. 2fa? Please tap yubi button - done. reply khimaros 14 hours agoparentprevyou might enjoy Bitwarden (self hosted with vaultwarden) which copies the TOTP to clipboard after logging in to a site. reply yoavm 13 hours agorootparentI'm using Bitwarden, for passwords, but I always felt uncomfortable with the idea of having my 2FA on my laptop too. It feels a little silly - if Bitwarden has both my password and my 2FA code, it's enough to hack my Bitwarden and all this \"multi-factor authentication\" isn't very \"multi\" anymore... reply sunaookami 13 hours agorootparentYou shouldn't lose any security when your vault itself is protected with 2FA. reply OJFord 11 hours agorootparentBut then it's barely better than 2FA vault & 1FA app. ('Barely' because it's like a bit of depth, and breadth into a few specific attacks revolving around the app's poor handling of your password.) reply lxgr 9 hours agorootparentprevTheoretically that's true, but practically, Bitwarden (and any other browser-based extension capable of autofill) runs in a much less secure environment than e.g. Aegis or Google Authenticator on a smartphone, and people often keep it unlocked (or at least not requiring 2FA for every password access). reply Eduard 8 hours agoparentprev [–] phone making a request means it cannot be airgapped. working airgapped / offline is a great quality speaking in favor of TOTP reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Aegis Public Notifications Fork by beemdevelopment has launched version 3.0 introducing Material 3 design, auto icon assignment, batch selection, 2FAS schema v4 backups import, and last use-based sorting.",
      "The update brings bug fixes, stability enhancements, and improved performance for scrolling through long lists of entries with numerous icons.",
      "The release aims to offer a more user-friendly experience with added features and optimizations."
    ],
    "commentSummary": [
      "The discussion focuses on the security and implementation of two-factor authentication (2FA) apps such as Aegis, Authy, FreeOTP, and Google Authenticator, highlighting concerns about seed storage across multiple devices and the effectiveness of 2FA.",
      "Various authentication methods like push notifications, WebAuthN, and U2F are debated for their security and convenience, suggesting standardized approaches emphasizing open-source projects like Aegis.",
      "Experiences with password managers, along with the Retool security breach due to SMS phishing attacks, underscore the importance of secure authentication methods and data redundancy."
    ],
    "points": 308,
    "commentCount": 132,
    "retryCount": 0,
    "time": 1711303240
  },
  {
    "id": 39806139,
    "title": "Secure and Minimalistic TinySSH Server with NaCl Encryption",
    "originLink": "https://github.com/janmojzis/tinyssh",
    "originBody": "Introduction tinysshd is a minimalistic SSH server which implements only a subset of SSHv2 features. tinysshd supports only secure cryptography (minimum 128-bit security, protected against cache-timing attacks) tinysshd doesn't implement older crypto (such as RSA, DSA, HMAC-MD5, HMAC-SHA1, 3DES, RC4, ...) tinysshd doesn't implement unsafe features (such as password or hostbased authentication) tinysshd doesn't have features such: SSH1 protocol, compression, port forwarding, agent forwarding, X11 forwarding ... tinysshd doesn't use dynamic memory allocation (no allocation failures, etc.) Crypto primitives State-of-the-art crypto: ssh-ed25519, curve25519-sha256, chacha20-poly1305@openssh.com Older standard: ecdsa-sha2-nistp256, ecdh-sha2-nistp256, aes256-ctr, hmac-sha2-256 removed in version 20190101 Postquantum crypto: sntrup761x25519-sha512@openssh.com, chacha20-poly1305@openssh.com Project timelime experimental: 2014.01.01 - 2014.12.31 (experimentation) alpha(updated): 2015.01.01 - 2017.12.31 (not ready for production use, ready for testing) beta(updated): 2018.01.01 - ????.??.?? (ready for production use) stable: expected ????.??.?? - (ready for production use - including post-quantum crypto) Current release (20240101) has 63899 words of code beta release How-to run TCPSERVER tcpserver -HRDl0 0.0.0.0 22 /usr/sbin/tinysshd -v /etc/tinyssh/sshkeydir & BUSYBOX busybox tcpsvd 0 22 tinysshd -v /etc/tinyssh/sshkeydir & INETD /etc/inetd.conf: ssh stream tcp nowait root /usr/sbin/tinysshd tinysshd -l -v /etc/tinyssh/sshkeydir SYSTEMD tinysshd.socket: [Unit] Description=TinySSH server socket ConditionPathExists=!/etc/tinyssh/disable_tinysshd [Socket] ListenStream=22 Accept=yes [Install] WantedBy=sockets.target tinysshd@.service: [Unit] Description=Tiny SSH server After=network.target auditd.service [Service] ExecStartPre=-/usr/sbin/tinysshd-makekey -q /etc/tinyssh/sshkeydir EnvironmentFile=-/etc/default/tinysshd ExecStart=/usr/sbin/tinysshd ${TINYSSHDOPTS} -- /etc/tinyssh/sshkeydir KillMode=process SuccessExitStatus=111 StandardInput=socket StandardError=journal [Install] WantedBy=multi-user.target",
    "commentLink": "https://news.ycombinator.com/item?id=39806139",
    "commentBody": "TinySSH is a small SSH server using NaCl, TweetNaCl (github.com/janmojzis)289 points by ThreeHopsAhead 23 hours agohidepastfavorite106 comments bretthoerner 20 hours agotinyssh is great. One use case for it that people may not know about: using it during Linux boot so you can remotely unlock encrypted drives. I have a headless NAS server that uses dm-crypt/LUKS under ZFS. When I update my kernel/ZFS I remotely reboot the server, wait a few seconds, and then ssh into a tinyssh powered encryption key prompt to unlock the drives. (I am immediately booted from ssh, as tinyssh exits.) I can then ssh again a few seconds later and I'm hitting openssh on a fully booted machine that wasn't able to open the drives without my intervention. https://github.com/grazzolini/mkinitcpio-tinyssh reply jethro_tell 20 hours agoparentI use normal opensshd for this. No reason to support two ssh daemons when you can do it with one. The difference in size on your init image is minimal and you probably aren't even trying to optimize for space there. If you don't know the size of your rd off the top of your head then it almost certainly doesn't matter. reply streb-lo 18 hours agorootparentProbably not more popular because (for reasons I do not know) the mkinitcpio hooks Arch Linux provides are only for tinyssh and dropbear: https://wiki.archlinux.org/title/dm-crypt/Specialties#Remote... reply bretthoerner 20 hours agorootparentprevAll fair, I guess I just landed on mkinitcpio-tinyssh first and it was my introduction to the idea, and only took a few seconds to setup. I'll switch to openssh if I ever have issues, but this has been working fine for many years, so I'm no rush. reply jethro_tell 14 hours agorootparentMakes sense. Probably more work to go off the beaten path then to maintain two configs reply forty 20 hours agoparentprevQuestion: when remotely unlock the boot disk via ssh, how do you make sure the boot has not been compromised and that you are not just sending the password to the bad guys? At some point I wanted to do something with utrablue [1], to work over network rather than Bluetooth, but then it was in go and I got lazy suddenly :) [1] https://github.com/ANSSI-FR/ultrablue reply bretthoerner 20 hours agorootparent> how do you make sure the boot has not been compromised and that you are not just sending the password to the bad guys? In my case, I can't. This is a NAS in my house and this is mostly to prevent me from having to go to another room and plug in a monitor and keyboard. (Also, I've done this from across the country after a power outage.) The threat vectors I'm protecting against are I guess mostly theft of the entire machine, or forgetting to wipe the drives when I eventually toss them out. Mostly, it's just fun practice because I'm a nerd and every drive should be encrypted. For my use-case, the auto-unlock-by-polling-a-specific-LAN-IP linked in this thread would probably be fine, for example. reply jethro_tell 14 hours agorootparentThis is mostly me but the case that's the most common is that a disk can't be wiped because its dead. Gotta do that before hand. reply aftbit 13 hours agorootparentWell you can always drill holes in the platter, or hit them with a strong magnet, or just separate them and toss them in the trash. Unless you're fighting the NSA, you can probably get away with enough physical destruction to make recovery challenging. reply crote 15 hours agorootparentprevIn theory this should be a perfect match for TPM-powered secure booting: your machine starts every time with a clean TPM, which validates the BIOS into it, which validates the bootloader into it, which validates the kernel into it, which derives an RSA private key from the resulting TPM state. If an attacker compromises it, it'll present the wrong host key and you get a big fat error message. reply teddyh 12 hours agorootparentprevMandos tries to mitigate this problem by periodically checking that the rebooting server is still up. Exactly how this is done is configurable, but by default it uses ssh-keyscan to check if the server is up and using the SSH server key from its normal encrypted root file system. If a server is ever down too long (configurable, default 5 minutes), the Mandos server will “disable” that server in its server list, and not provide the password to that server anymore. The idea is that you should configure the timeout to be long enough to allow for a normal kernel panic and reboot, but hopefully short enough that it would be hard for anyone to compromise the server in that time. It’s not a perfect solution, but it’s the best anyone has come up with as far as I know. (Disclosure: I am a co-author of Mandos.) reply mbreese 15 hours agorootparentprevIsn’t there a host SSH key involved? The server itself may have been physically breached, and if so you can’t trust anything. But, if your host key matches, you should be confident that at least you’re logging into the correct machine (there was no IP takeover). reply akerl_ 8 hours agorootparentIf the server was breached, what stops somebody from copying the host key to their new system. reply rand846633 17 hours agorootparentprevWhat are the best options to solve this problem? It’s a hard problem imho for most threat models. If the booting machine has been compromised and i use my usb connected keyboard to enter the full disk encryption key I would run into the exact same issues, no? reply fbdab103 15 hours agorootparentIf you have physical access, seems way easier to put a keylogger directly into the keyboard. reply jsmith99 18 hours agorootparentprevThis is what a TPM is designed to prevent, but they’re not very popular in Linux community. reply pmorici 17 hours agorootparentprevTPM and checking your physical security boundary hasn't been breached. reply toast0 15 hours agorootparentprevI have disk encryption on a remote server on cheap low-asssurance hosting, and I do it so I don't have to worry about what happens when the disks are recycled. If I could (easily) automate entering the key on boot, I would (but I'm running FreeBSD, so Linux solutions don't help). But I don't need to reboot often and the hardware is fairly reliable, so eh. This is for my personal hosting which if someone wants to take over, I guess I'd be more curious than upset. reply slug 16 hours agoparentprevFor debian/ubuntu users, there's also dropbear-initramfs package with same functionality (works with any fs luks/ext4/lvm/zfs/etc). https://packages.debian.org/bookworm/dropbear-initramfs https://packages.ubuntu.com/jammy/dropbear-initramfs reply teddyh 12 hours agorootparentNote: Mandos is also in Debian and Ubuntu. (Obligatory disclaimer: I am a co-author of Mandos) reply justin_oaks 14 hours agorootparentprevI've used this for several years now. It works well and is relatively easy to set up. reply orev 15 hours agoparentprevThis is more or less the RedHat based solution to do this using openssh: https://github.com/gsauthof/dracut-sshd https://copr.fedorainfracloud.org/coprs/gsauthof/dracut-sshd... reply gymbeaux 20 hours agoparentprevUsually I use DropBear for this. Do you know if one is necessarily better than the other? DropBear I think is what RHEL docs recommend for remote boot disk decryption. reply bretthoerner 20 hours agorootparentAh, I've never used DropBear. I don't know how one could be better than another for my simple use case, honestly. reply babuskov 17 hours agoparentprevI thought that everyone has switched to Clevis + Tang for that? https://access.redhat.com/documentation/en-us/red_hat_enterp... It's fully automated and supposed to be much more secure. Has anyone got experience with it? reply traceroute66 17 hours agorootparent> I thought that everyone has switched to Clevis + Tang for that? Clevis+Tang is good. There's also Keylime which takes a different approach to the same[1]. [1] https://keylime.dev/ reply ahepp 10 hours agorootparentprevI've seen a bit about Clevis. Is there a major difference between using this, and systemd-cryptenroll? reply soraminazuki 16 hours agorootparentprevIIUC whether that is secure depends on your threat model. For example, how good is automated unlocking compared to unencrypted drives in a homelab setup? reply teddyh 15 hours agoparentprevThere’s a non-interactive solution to rebooting safely with encrypted disks: MandosReboot your server while you sleep! Disclosure: I am a co-author of Mandos. reply jethro_tell 14 hours agorootparentThis is really cool. I'm going to give this a try! reply 1vuio0pswjnm7 6 hours agoparentprev\"tinyssh is great.\" Agreed. A static tinysshd works well for the small userlands I create. reply krab 20 hours agoparentprevA tool based on Dropbear that does exactly this, automatically. https://github.com/ViktorStiskala/cryptsetup-ssh-unlocker reply teddyh 12 hours agorootparentThe documentation for Cryptsetup SSH unlocker states “To further limit the attack possibility, you should use monitoring and possibly disable SSH unlocker in the case of unexpected behavior.” Mandos has a built-in feature to deal with this, enabled by default. (Again, disclosure: I am the co-author of Mandos.) reply kureikain 1 hour agoprevI think for these use case a SSH server in Go would be way simpler such as https://github.com/gliderlabs/ssh reply throw0101c 22 hours agoprevCurrently Slashdotted: * https://web.archive.org/web/20240324101238/https://tinyssh.o... reply cchance 19 hours agoprevCool but something i saw that was weird, this may be the first repo i've ever seen to advertise ... words of code, i've always seen secure repos advertise their \"in only X LOC\" seeing words of code as a metric was funny reply creshal 19 hours agoparentIt's unusual, but IMO makes sense, as it encapsulates complexity better than LOC do, because the latter are more sensitive to formatting preferences etc. Books are also measured in words too (also for category thresholds, e.g. between a novella and a full novel), so there's precedent too. reply fbdab103 15 hours agoparentprevThe Wren language[0] uses semicolons as its size metric: Wren is small. The VM implementation is under 4,000 semicolons. You can skim the whole thing in an afternoon. It’s small, but not dense. It is readable and lovingly-commented. [0] https://wren.io/ reply justin_oaks 14 hours agorootparentSemicolons seems an odd metric since complexity is often introduced by if-else branches, while loops, and function declarations. Each of those doesn't involve semicolons. reply oxryly1 18 hours agoparentprevMade it seem like it was written in Forth. reply seniorivn 23 hours agoprevhttps://github.com/janmojzis/tinyssh reply AceJohnny2 8 hours agoprev> State-of-the-art crypto: ssh-ed25519, curve25519-sha256, chacha20-poly1305@openssh.com > Older standard: ecdsa-sha2-nistp256, ecdh-sha2-nistp256, aes256-ctr, hmac-sha2-256 removed in version 20190101 Bah! I've soured on ed25519 because two of the tools I depend on have lackluster support. We have one tool that leverages Macbook TouchID as a hardware keystore, and it doesn't support ed25519, only ecdsa (I don't know whether this is a TouchID or a tool limitation, I suspect tool). The other is that recent versions of Gerrit, which leverages Apache SSH, will crash the SSH connection when presented with some ed25519 certificates, which is funny since Gerrit does not support certificates! I really wish ed25519 was more widely and better supported, or that TinySSH supported ECDSA. reply Hendrikto 20 hours agoprevShame that it does not support ed25519-sk. Apart from that, it looks very promising. reply CarRamrod 22 hours agoprev>easy auditable - TinySSH has less than 100000 words of code Is approximately one hundred thousand words really easily auditable? reply latexr 22 hours agoparent> approximately one hundred thousand words According to the index page, the current release is closer to half of that at 62989 words. Which is (allegedly, don’t trust the number too much) as long as 2001: A Space Odyssey. https://www.readinglength.com/book/isbn-0451452739 Not a long book, but in the context of code I wouldn’t consider that easily auditable. reply tptacek 19 hours agoparentprevSeems like the wrong question. Rather: if it's easily auditable, who's audited it so far? reply loeg 15 hours agorootparentI think they're both pretty reasonable questions! reply nemoniac 13 hours agoparentprevWhat is a \"word of code\"? I've seen \"lines of code\" but what's a \"word\" in C? reply teddyh 12 hours agorootparentMaybe M-x count-words? reply edf13 21 hours agoparentprevNot really, especially when they have: > TinySSH has its own crypto library reply rthnbgrredf 21 hours agoparentprevWell, at least it fits into GPT-4 Turbo context. I think we are not far away from a fully automated audit that can at least check for 99% of common bugs and security issues. reply k8sToGo 23 hours agoprevWhat is the difference between this and dropbear ssh? reply chasil 23 hours agoparentThis server is very restricted compared to dropbear. -passwords are not allowed, only keys -only a single AEAD cipher is supported, and a single elliptic curve for key exchange -root cannot be locked out with this server -the key restrictions available in OpenSSH are not supported -the server does not use dynamic memory, and has a better security record than dropbear reply themoonisachees 23 hours agoparentprevI don't think there's much of one. The concept in itself isn't really useful, I've never seen someone go \"you know the real problem with sshd? It's too bloated\". It is slightly useful to put in smaller devices that don't have much space but I think it still relies on top many linux facilities to be an appropriate fix for that too. Still, cool project. reply dhon_ 23 hours agorootparentI suspect the average openssh-server user just uses it for remote terminal access and copying files. Reducing the attack surface by dropping features and outdated standards is certainly valuable. reply maxcoder4 23 hours agorootparentprevDropbear ssh is very useful when you have a full disk encryption on a remote server and want to be able to decrypt it after a reboot. reply Nextgrid 22 hours agorootparentThere's technically no reason OpenSSHd can't also be used in this context. Maybe 2 decades ago there was a legitimate performance/disk space reason which is why Dropbear was preferred for this use case (and the convention remains to do this day), but nowadays the couple megabytes of difference in your initrd between using Dropbear and OpenSSH won't matter. reply m45t3r 20 hours agorootparent> There's technically no reason OpenSSHd can't also be used in this context. For initrd you generally prefer static binaries. Not saying that OpenSSHd doesn't build statically, but having less code and dependencies makes it easier to statically compile. But yes, technically there is no reason to not use OpenSSHd, but in practice having a smaller and more self contained binary helps considering that you would want the bare minimum during initrd. reply akerl_ 20 hours agorootparentWhat dependencies does openssh have? reply mkj 19 hours agorootparentOpenSSL reply 1oooqooq 16 hours agorootparentif you don't care for v1 and odd cyphers, --without-openssl reply ThreeHopsAhead 23 hours agoparentprevdropbear has the goal of being small and light on ressources while still providing featurefull ssh support. tinyssh is small because it only implements a tiny subset of SSH that is needed for secure basic SSH connections. It only includes few crypto primitives excluding even RSA. There is considerable overlap in the two and you can reach something similar to tinyssh by compiling dropbear with only few select features, but tinyssh aims to be as secure and attack surface minimized as possible out of the box. Another notable difference: > no dynamic memory allocation - TinySSH has all memory statically allocated (less than 1MB) reply Columbo818 23 hours agoparentprevDropbear claims to be RFC-compliant, but isnt. Proof here: https://www.cvedetails.com/cve/CVE-2021-36369/ TinySSH doesnt claim to be compliant, and isnt. Does less in exchange for a reduced attack surface. reply mkj 19 hours agorootparentThat CVE is a UI confusion issue in the client, I'm not sure exactly what bit the reporter thought was non-RFC compliant. reply bjoli 23 hours agoparentprevIt is smaller and supports less features. Dropbear does password auth and x11 agent forwarding iirc. reply traceroute66 17 hours agoprevI don't see anyone sensible replacing OpenSSH with anything else for two fundamental reasons: 1. OpenSSH has more eyes on it and more deployments than almost any other piece of non-OS/kernel software on the planet. By this stage in its life, it is very mature. Look at the vulnerability database, OpenSSH has not had a serious REMOTE vulnerability for a long time, all the recent vulnerabilities require the attacker to have some form of pre-existing host access (https://www.openssh.com/security.html). 2. OpenSSH comes from the house of OpenBSD. Those guys are serious about writing secure code and have a well-established track record. These days you can also compile OpenSSH against LibreSSL instead of OpenSSL. Instead of replacing OpenSSH, most people would be better off spending their time switching OpenSSH to key-based-auth only and then making a few simple configuration changes to further harden OpenSSH. Starting with the config ideas proposed by Mozilla[1] and adding in options such as the built-in rate-limiting config options (PerSourceMaxStartups, PerSourceNetBlockSize and friends). [1] https://infosec.mozilla.org/guidelines/openssh reply wibblewobble125 12 hours agoparentBad reasons to trust code: * “Many __informal__ eyes have probably looked at it” * Lack of recent __number__ of (known) vulnerabilities * “Serious guys” (appeal to authority) I think you’re using short-hand, but perhaps the short-hand should be different. E.g. * A list of audits by date, independent organization, is provided __here__ which is evidence of review * The vulnerability acknowledgement, correction and release process is prompt, accurate and detailed, which is documented __here__ * XYZ coding, testing, fuzzing, proving, bounty, integration with other systems, documentation, defaults etc. practices are used in the interest in hardening the code, limiting moving parts, attack radius, etc. reply traceroute66 10 hours agorootparent> I think you’re using short-hand Yes I was using short-hand. Because you're the only one here trying to make the stupid argument that OpenSSH code is somehow not trustworthy. Frankly, if you don't trust OpenSSH code for the reasons you suggest, then you should not be trusting any Operating System, whether BSD, Linux, Mac or Windows. As I said, OpenSSH is used extensively, INCLUDING in security-critical environments, the sort of security-critical environments that you can be sure have done their homework, even if they don't publish it. The simple fact of the matter is this: Given the widespread global deployment of OpenSSH for DECADES now, if there were shortcomings in the code, you would have heard of it because we would be seeing BILLIONS of compromised endpoints. Fact is, there aren't, unless you haven't bothered to update your system in the last decade. So you can talk about fuzzing or whatever until you are blue in the face, but widespread global deployment is hard to beat, because that's REAL WORLD, failed attempts at finding zero-day exploits and all ! reply keepamovin 19 hours agoprevI like this person's work. Check it out: https://github.com/janmojzis - tinyssh - TinySSH is a small server with less than 100,000 words of code. Language: C. Stars: 1.1k. Forks: 65. - acmeshell - Shell-style client for LetsEncrypt. Language: Python. Stars: 31. Forks: 6. - dq - Recursive DNS/DNSCurve server and command-line tool to debug DNS/DNSCurve. Language: C. Stars: 23. Forks: 1. - pstree - Unix process tree viewer. Language: C. Stars: 14. Forks: 2. - ntpserver - Pure python NTP server. Language: Python. Stars: 11. Forks: 3. - httpfile - Httpfile is an HTTP server derived from publicfile-0.52. A collection of tiny, standard net utils and servers. Gives the impression the person does it to craft something, and to understand. Inspiring and impressive! reply Panino 16 hours agoparent> dq - Recursive DNS/DNSCurve server and command-line tool to debug DNS/DNSCurve I use dqcache, the DNSCurve-aware recursive resolver from the dq package, and love it. reply 1oooqooq 16 hours agoparentprevsome of those look like spoofs? reply keepamovin 6 hours agorootparentHow? I guess perhaps to you they would. But why don’t you check it out rather than writing a silly comment? reply numpad0 21 hours agoprevtinysshd doesn't implement unsafe features (such as password or hostbased authentication) Isn't password support useful for shared devices, like printers and routers? How would one enroll his personal keys on something like a car? reply Retr0id 20 hours agoparentThe device can generate a fresh keypair and show you the private key via a QR code or some other output mechanism. Then you can log in and enroll your real keys. reply rstat1 3 hours agoparentprevYou wouldn't, because you wouldn't be SSH'ing in to a car. I would hope such things wouldn't even be possible. reply vzaliva 17 hours agoprevIt looks like a good choice for access to small embedded devices. Except the missing port forwarding feature. reply rand846633 22 hours agoprevCool project! There are many use cases where this is a good tool! Allways great to have another alternative to some other great tools. reply yodon 19 hours agoprevIs NaCl still a thing? Genuine question as I've not heard it mentioned in years. [edit added] For those, like me, who thought this was using Google's NaCl (sandboxed C++), it's actually using Daniel Bernstein's NaCl (cryptography library). reply jvanderbot 20 hours agoprevThe license is \"CC0 1.0 Universal\" In light of this post outlining a bug in early CC licenses: https://doctorow.medium.com/a-bug-in-early-creative-commons-... Discussed here: https://news.ycombinator.com/item?id=39610509 Does this need updating? EDIT: based on some discussion, it does need updating, but not for the reason I thought. I filed a suggestion here: https://github.com/janmojzis/tinyssh/issues/85 reply fanf2 19 hours agoparentNot for that reason. The copyleft trolls that Doctorow wrote about are using a termination clause in attribution-required CC licences. (Remember, there are lots of different CC licences with varying requirements on licensees.) CC0 doesn’t impose requirements on licensees nor does it have a termination clause, so it isn’t affected by these trolls. However, CC0 is not good as a software license. It is explicitly restricted to being a copyright license. If there are patents covering the software, CC0 does not give you permission to exercise the patented invention. It’s better to use 0BSD or MIT-0 instead, which grant permission to use the software without weird exceptions. reply skissane 14 hours agorootparent> It’s better to use 0BSD or MIT-0 instead, which grant permission to use the software without weird exceptions. 0BSD and MIT-0 are zero attribution ultra-permissive copyright licenses, aka public domain-equivalent copyright licenses. CC0 is a public domain declaration with a fallback copyright license for jurisdictions (such as Germany) which don't recognise public domain declarations. There is a big technical difference between the two, in some jurisdictions (such as the US) – CC0 puts something in the public domain, MIT-0/0BSD technically doesn't. A real difference in theory, maybe not much in practice. If the author really cares about the public domain part, something like Unlicense is a better option than MIT-0/0BSD – an actual public domain dedication, without the patent/trademarkconcerns which exist regarding CC-0. If they want to make the maximum possible number of people happy, they could even use disjunctive licensing, e.g. CC-0 OR Unlicense OR MIT-0 reply jvanderbot 19 hours agorootparentprevI copied your comment to https://github.com/janmojzis/tinyssh/issues/85 If author/maintainer doesn't frequent HN, perhaps discussion there might get some action. reply fanf2 18 hours agorootparentYou can find a more authoritative but longer explanation in an OSI FAQ about CC0 https://opensource.org/faq#cc-zero and the related discussion http://lists.opensource.org/pipermail/license-review_lists.o... reply makeworld 19 hours agorootparentprev0BSD and MIT-0 don't mention patents explicitly though, few licenses do. I only know of two: https://blueoakcouncil.org/list reply fanf2 18 hours agorootparentThey don’t mention copyright explicitly either. reply gkbrk 20 hours agoparentprevIs that necessarily a bug? If you use Disney content without a license they won't give you a 30-day period to keep mis-using it. Same with using Oracle software. Why should people who create CC content provide such a grace period? reply jasomill 19 hours agorootparentFor the reason given by Doctorow in the linked article: If you put a CC license on your work, its explicit message is, “I want you to re-use this.” Not “I am a pedantic asshole with a fetish for well-formed attribution strings.” The point of CC is not to teach the world to write attribution strings: it is to facilitate sharing and re-use. If you are a good-faith user of CC licenses, then your response to an incorrect attribution string should be a request to correct it, not a threat to sue for $150,000 in statutory damages. reply jvanderbot 19 hours agorootparentprevValidity of the post aside, there are real-world examples of this license being abused by third parties. Given the cost is very low to just change the license, I think it might be worth considering. reply skissane 14 hours agorootparentTo be clear, the issue we are talking about here does not exist for the CC0 license TinySSH is using. CC0 lacks a termination clause, it wouldn't make sense for it to have one. It only exists for other CC licenses, like the (earlier versions of) CC-BY(-NC/-SA). CC0 has other issues – some people (e.g. Red Hat Legal) are concerned about its language explicitly excluding patent and trademark rights, and think that is legally inferior to other public domain declarations (such as The Unlicense) which don't mention that topic at all. In a declaration/license in which patents and trademarks go unmentioned, if the original author sues you on those grounds, you can try to argue that by releasing the software they gave you an implied patent/trademark license – that argument may or may not win in Court, but at least it has a chance. With language in the declaration/license explicitly excluding patents and trademarks (like CC0 has), that argument is likely dead-on-arrival. reply saagarjha 19 hours agorootparentprevBecause people who create CC content typically do so because they want it to be available to good-faith uses, which may not always follow the exact requirements of the license by accident reply underdeserver 23 hours agoprevnext [9 more] [flagged] latexr 22 hours agoparentIn this day and age, it’s probably best to understand that blindly suggesting something should be in Rust is indistinguishable from zealotry. Or from mockery of the people who do it. reply bazzargh 14 hours agoparentprevA better question to ask would have been, why settle for just memory safety - does a formally verified sshd exist? That kind of thing seems to be implemented more in OCaml and F#, like Project Everest, which has formally verified implementations of primitives (HACL) TLS, QUIC, and Signal https://project-everest.github.io/ ... ssh is notably missing? I had a dig and found that ssh had in fact been done 9 years ago, tho it doesn't seem to have made it to a distribution: it's an offshoot of the CryptoVerif project[1] (which is, maybe unsurprisingly, under the umbrella of the same Prosecco team at Inria who worked on Project Everest). In 2015 Bruno Blanchet and David Cadé wrote a paper \"From Computationally-Proved Protocol Specifications to Implementations and Application to SSH\"[2] which describes using CryptoVerif to generate an implementation of SSH from the spec; the code is in the CryptoVerif tarball, but someone's helpfully put that up on github if you want a look[3] The eye opening bits in the paper (given the claims of tinyssh to be small at < 100k words): \"We have verified that our client and server correctly interoperate with OpenSSH...in order to give an idea on the amount of code this work represents, the CryptoVerif specification amounts to 331 lines of code, and we generate from it 531 lines of OCaml, split among multiple files. The manually written code representing the primitives and the authentication and connection protocols amount to 1124 lines.\" The bad news would be that it's not very performant: 30MB/s compared to 90MB/s for openssh on the same hardware, in the paper. Since HACL is from the same stable and was designed to perform well as well as be verified, it might be worth dusting off and seeing if this could be made more usable. The implementation probably has value even if just as a test oracle. [1]: https://bblanche.gitlabpages.inria.fr/CryptoVerif/ [2]: https://bblanche.gitlabpages.inria.fr/publications/CadeBlanc... [3]: https://github.com/mgrabovsky/cryptoverif/tree/master/implem... the official implementation appears to be here now https://gitlab.inria.fr/bblanche/CryptoVerif/-/tree/master/c... reply jddj 23 hours agoparentprevCan you justify this further in the context of this specific offering which appears not to manage dynamic memory? reply mronetwo 22 hours agorootparentYou don't need further justification if you just agree that using Rust makes everything automatically better... somehow... and you need to agree with it without evidence. reply hughw 18 hours agorootparentI admit I am susceptible to this thinking though. reply pquki4 21 hours agoparentprevThis kind of comment had become a joke these days, especially when you say that under a project written in C. Nobody is going to even think about that until Rust has as good portability as C. reply FergusArgyll 21 hours agoparentprevI feel like someone has to say this: What about LISP?! reply yjftsjthsd-h 18 hours agoparentprevExcellent point; which production-grade rust ssh server would you recommend? reply sylware 22 hours agoprevI am pleased to see another \"small\"-is-beautifull alternative of a critical network protocol, and in plain in simple C (I am sorry for the fan boys of absurdely complex computer languages...). There will be plenty of compiler generated holes, and other security issues, but keep your head above the water and fix all of them, you are going for the long run there. We also have drop-bear, which is in between openssh and tinycc if I recall properly. I have to admit... I may deploy tinyssh for my everyday work (I rarely code directly on my workstation, usually I am \"away\" and do ssh to it via 4g internet IPv6/ssh). Now a bit of whining (come on, we are on HN), microsoft github is always a bad idea, should move to a fully noscript/basic (x)html friendly git repository (aka not gitlab based for instance, yet). reply pquki4 21 hours agoparentYou are fighting a losing battle if you don't want to see GitHub links on HN. reply serf 20 hours agorootparentit's still worthwhile to point out the issues for people who have recently jumped into the occupation/hobby and haven't yet had the time to meditate on why Microsoft having the keys to the worlds' software kingdom might be A Very Bad THing down the line. reply jonathaneunice 20 hours agoparentprevIf neither GitHub nor GitLab, what are you recommending? There are a few other non-DIY hosted options, but it's hard for me to translate your \"fully noscript/basic (x)html friendly\" spec into an actionable list of options. Or is this a \"host it yourself\" / DIY plea? reply gkbrk 20 hours agorootparentSoucehut works with noscript. No need to host or DIY anything. Codeberg has a message that says \"This website requires JavaScript.\" but I was able to use it without JS to browse around and look at code properly. reply aAaaArrRgH 14 hours agoprevGiven that it's a small SSH server, I wonder how feasible it would be to rewrite it in a memory-safe language. C doesn't feel like the most security-conscious (and, quite frankly, legible) language in this day and age. reply itpcc 23 hours agoprev [–] Poor, poor site. They got HN hug of death. reply Brian_K_White 15 hours agoparent [–] I can only assume it's idiots downvoting this, who think you are bashing the site instead of sympathising with it. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Tinysshd is a sleek SSH server focusing on secure cryptography with a minimum of 128-bit security, excluding older cryptographic algorithms and insecure elements like password authentication.",
      "It incorporates state-of-the-art cryptographic primitives and post-quantum cryptography and follows a development roadmap from experimental to stable releases, with the present beta release labeled as 20240101.",
      "Instructions for deploying the server are available for diverse methods like busybox, tcpsvd, inetd, and systemd."
    ],
    "commentSummary": [
      "Users are discussing the use of TinySSH, a small SSH server for encrypted drives in the Linux boot process, weighing convenience and security against potential risks.",
      "Topics include securing encrypted disks with TPM, Mandos, and server size debates, code complexity, and security measures.",
      "Discussions cover trust in OpenSSH, license issues, termination clauses in Creative Commons licenses, and SSH implementations in OCaml and F#, along with technical talks on using CryptoVerif, programming languages like Rust and LISP, and hosting repositories on platforms like Sourcehut."
    ],
    "points": 290,
    "commentCount": 106,
    "retryCount": 0,
    "time": 1711274695
  },
  {
    "id": 39807912,
    "title": "Glossarie: Reading eBooks for Language Learning",
    "originLink": "https://glossarie.app/",
    "originBody": "Hi HN, For over two years I&#x27;ve been working on an App to learn languages (currently French, Italian and Spanish), together with my partner, a language teacher. I think it is finally ready to share with this community!The idea is to introduce vocabulary and grammar whilst you read eBooks in your own language. I&#x27;ve found that it is easier to remember vocabulary &#x27;in context&#x27; and with regular repetition. Plus you don&#x27;t have to carve out dedicated time for language learning. Other apps require you to build a habit around various exercises or ‘games’, whereas lots of people already read books.From testing with early users so far it&#x27;s proving effective for building a basic understanding of a language and quickly getting to the point where you can read and broadly understand text in the target language. It’s even better in combination with other apps that help with listening&#x2F;speaking like Pimsleur.There were lots of technical challenges making this. It turned out to be (reassuringly) hard to get accuracy to an acceptable level, requiring a rabbit-hole into machine translation. There was a lot of testing required to optimise the engine that chooses the translations to show and to reduce the friction when reading books. And the backend to support uploading books is a beast in itself. I’d love to share details if there is interest.Roadmap- Accuracy - 100% accuracy is the target, but at present there can be errors. Feedback from users will be important here so that accuracy issues can be generalised and solved at scale. Errors can be reported within the app - please do so if you spot anything!- Dynamic difficulty - rather than have a progression of difficulty levels I’d prefer to introduce vocabulary and grammar automatically in response to user progress, balancing against the friction of seeing unfamiliar words. There’s a lot ‘under the hood’ to manage this today, but plenty of room to improve.- More practice features - to reinforce vocabulary&#x2F;grammar and support writing, listening and speaking.- Better eBook support - improving the formatting of eBooks within the app and providing more methods for finding good books to read.Use of AI- LLMs provided a step change in accuracy and have enabled a feature that explains translations and grammar to the user- vastly improving the utility versus a year ago.- I believe apps like this, which use AI to enhance or scale functionality rather than simply acting as a wrapper over APIs, will be the major beneficiaries as LLMs improve.Take a look, and let me know your thoughts or questions!",
    "commentLink": "https://news.ycombinator.com/item?id=39807912",
    "commentBody": "Glossarie – a new, immersive way to learn a language (glossarie.app)259 points by jonathanb88 18 hours agohidepastfavorite119 comments Hi HN, For over two years I've been working on an App to learn languages (currently French, Italian and Spanish), together with my partner, a language teacher. I think it is finally ready to share with this community! The idea is to introduce vocabulary and grammar whilst you read eBooks in your own language. I've found that it is easier to remember vocabulary 'in context' and with regular repetition. Plus you don't have to carve out dedicated time for language learning. Other apps require you to build a habit around various exercises or ‘games’, whereas lots of people already read books. From testing with early users so far it's proving effective for building a basic understanding of a language and quickly getting to the point where you can read and broadly understand text in the target language. It’s even better in combination with other apps that help with listening/speaking like Pimsleur. There were lots of technical challenges making this. It turned out to be (reassuringly) hard to get accuracy to an acceptable level, requiring a rabbit-hole into machine translation. There was a lot of testing required to optimise the engine that chooses the translations to show and to reduce the friction when reading books. And the backend to support uploading books is a beast in itself. I’d love to share details if there is interest. Roadmap - Accuracy - 100% accuracy is the target, but at present there can be errors. Feedback from users will be important here so that accuracy issues can be generalised and solved at scale. Errors can be reported within the app - please do so if you spot anything! - Dynamic difficulty - rather than have a progression of difficulty levels I’d prefer to introduce vocabulary and grammar automatically in response to user progress, balancing against the friction of seeing unfamiliar words. There’s a lot ‘under the hood’ to manage this today, but plenty of room to improve. - More practice features - to reinforce vocabulary/grammar and support writing, listening and speaking. - Better eBook support - improving the formatting of eBooks within the app and providing more methods for finding good books to read. Use of AI - LLMs provided a step change in accuracy and have enabled a feature that explains translations and grammar to the user - vastly improving the utility versus a year ago. - I believe apps like this, which use AI to enhance or scale functionality rather than simply acting as a wrapper over APIs, will be the major beneficiaries as LLMs improve. Take a look, and let me know your thoughts or questions! ximeng 15 hours agoShout out for an app called Language Transfer that I just came across via Reddit (https://www.languagetransfer.org/). It teaches languages speaking first through a simple audio course. Developed by one guy, completely free and without ads. From what I’ve seen so far has a very clear focus on quickly getting up to speed with a different angle than other courses. It talks about how to build vocabulary by looking at general patterns for shared vocabulary between languages. reply Onawa 8 hours agoparentThat dude needs more money to support development, because his method is amazing. The other amazing method I love and used to learn Spanish was \"Fluent Forever\". Stick to the book there though, the author tried to grow his empire too big and the apps aren't that great in my opinion. reply rgovostes 4 hours agorootparentI used the Fluent Forever app to acquire about 1,000 Italian words in one year. Agree that it wouldn't win awards for polish (maybe Polish), but it fairly faithfully implements the method described in the book, which is roughly: make your own flash cards, using memorable imagery, from curated word lists designed for forming a wide range of sentences, and then study these using spaced repetition. You also get the ear training audio cues. You can do it all yourself with Anki, indeed the author of the method started that way, but I am happy to not have had to pay the Anki usability tax. Even if you don't use the app/method, the book (of the same title) is a quick and useful read about tricks used to optimize language acquisition. reply jcul 2 hours agoparentprevLooks interesting and I'm going to try it out. Curious about what differentiates it from something like pimsleur courses, other than price of course. reply multiplepointer 2 hours agoparentprevAlso found this app on forums and I wonder why it's not popular. It's amazing, simple and effective. And absolutely free reply SamBam 10 hours agoprevI have to admit I don't quite understand this. Does it just replace words on a word-by-word basis? But the ordering of words in English is different from the ordering of words in other languages. How is this not going to teach you terrible grammar? Looking at one of the screenshots, for instance, it translated \"they had met their dead father\" as \"they had recontré leur mort father.\" But in French \"mort\" would come after \"père.\" reply jonathanb88 8 minutes agoparentThanks for the feedback. This is something that will improve over time - as it gets better at identifying longer phrases I can implement rules so it won't omit a neighbouring noun if the phrase contains a verb. reply totetsu 9 hours agoparentprevAnd those differences that are not marked in English but are in French are the only hard part about learning it.. reply deliriumchn 10 minutes agoprevI could say that its interesting, but its instead region restricted (for android at least) and now available in Europe. reply yaj54 16 hours agoprevI hate browser plugins - but this needs to be a browser plugin. Then I would use it while reading HN. ;-) I would suggest tackling dynamic difficulty and algorithmic selection of what words to learn, when, and how often, and then let improving LLMs handle accuracy improvements. reply Player6225 15 hours agoparentAFAIK, I think the most popular version of this idea is https://readlang.com reply igeligel_dev 14 hours agoparentprevhttps://jointoucan.com/ - hope you enjoy it reply xendipity 12 hours agorootparentJust tried Toucan and it can't be disabled on localhost, a major pain for using it during work as an engineer. For those that haven't used toucan, it's an extension that translates words/phrases inline on a page with various levels of replacement frequency and complexity based on your proficiency with the language. reply spidersouris 13 hours agorootparentprevNot available on Firefox. reply Mazzen 4 hours agorootparentBummer. I was about to give it a try reply allarm 3 hours agorootparentprevI wasted 10 minutes trying to find the pricing page. There’s no way I’m going to use an app that deliberately hides the pricing information. reply jonathanb88 14 hours agoparentprevDefinitely an idea on the roadmap. I know most people do most of their reading in a browser and not eBooks. I'll see how easy it is. I get palpitations thinking about developing on another new platform. Java and Swift were a challenge enough to learn! reply netsharc 9 hours agoparentprevRusty eel-gathering hovercraft reply OJFord 15 hours agoparentprevThat would be amazing, not everyone reads eBooks (whether because they don't read books, or just prefer physical) but everyone whose a potential user anyway does browse websites. Also because while I absolutely love the idea for seamless Hinglish style integration (as opposed to say a side bar which just told you what some words would be in a different language) it does mean that I'm no longer really reading the book, I'm reading the content but not the author. I don't personally read anything that I'd want to alter like that, but I can imagine for others it might limit its use to 'trashy novella read while travelling' or something. Tldr the idea is brilliant, but for me too it needs to not be for eBooks. reply OJFord 10 hours agorootparentWho's* (can't edit) reply opdahl 1 hour agoprevThis looks really cool! Too bad you don’t have Korean though. But I’m already a LingQ user, how is Glossarie different? reply igeligel_dev 14 hours agoprevThis looks nice and similar to toucan [1]. I have built something similar but a bit different for the web [2]. Happy, to share the code in a non-so-permissive license with you if you plan to build out web support. [1] https://jointoucan.com/ [2] https://github.com/igeligel/tooltipr-extension reply archsurface 14 hours agoparentCurious to hear the reasoning behind using the US flag for English, given there's a country called England. reply anotherevan 13 hours agorootparenthttps://jakubmarian.com/wp-content/uploads/2015/10/english-t... reply tomstoms 13 hours agorootparentprevAnd you would prefer the English flag then - not the flag of the UK? reply Mystery-Machine 54 minutes agorootparentprevAs far as I know English is different from English in many ways. Colour Organisation Trousers I don't know. Search online. reply shakesbeard 12 hours agorootparentprevhttps://www.flagsarenotlanguages.com/ reply thalesmello 13 hours agorootparentprevThat's based on relevance. Same goes for: Portuguese -> Brazil flag; Spanish -> Mexico flag; German -> Austria flag; Italian -> Switzerland flag ;) reply aae42 13 hours agorootparentprevIt's the 3rd most populous country in the world, and it's filled with English speakers? reply bzmrgonz 4 hours agorootparentIt's just marketing guys, think of it as political pandering, more population, more patriotic population(emotional attachment), etc. I'm sure we all agree that America is more linguistically and geographically \"challenged\"(market potential) than the Brits yeah?.... sooo market where the market is. As for Brazil, that's a sheer population decision. They tried to shove Spain's version of spanish audio translation into the latin american portion of the new world (the thinking I guess was since america loves the british accent in movies... but it flopped.... now Hollywood/movie industry recruits voices from Mexico and other latam countries. reply heyest 13 hours agorootparentprevIf those are the metrics we should be using the Indian flag. reply mahatofu 5 hours agorootparentThe most import comment ^ reply jonathanb88 14 hours agoparentprevThanks, I'll take a look! reply anonzzzies 1 hour agoprevPortugals Portuguese please (no Brazil, almost all the apps that teach Portuguese, teach the Brazil version, which makes you sound like a joke here; thanks Duolingo!). reply mft_ 16 hours agoprevLove the idea; can I kindly ask if you're expanding to include German? reply jonathanb88 16 hours agoparentThanks! At some point I may add it, but the difference in grammatical structure might limit how well it works. I'll try to start testing it soon. reply jgtor 15 hours agoparentprevI actually built 'Tembo - Bilingual Stories' while learning German myself, so hate to plug it on this thread, but we've got lots of German content, maybe you'll find something that interests you? https://www.tembo.app reply kebsup 15 hours agoparentprevHi, if it would be sufficient for you to read websites, I'm building https://vokabeln.io, though the concept is a bit different, focusing more on flashcards and spaced-repetition. reply barrenko 16 hours agoparentprevseconded reply stonedge 16 hours agorootparentThirded reply Jeaye 16 hours agoprevThanks for building and sharing this! As a feature request, which would likely be the decider for me using this, would you please consider an integration into koreader? As far as I know, koreader is the #1 open source app for ereaders. If anyone using their ereader can use this, you can expand your userbase outside of those who just read on mobile. I don't think I'm alone in never wanting to read an actual book with my phone. At any rate, best of luck and great work getting this shipped! reply alwayslikethis 10 hours agoparentNot quite the same thing, but Vocabsieve (disclosure:my project) can read your KOReader lookup history to generate Anki cards with context and audio. I also feel like reading in your target language is a far better use of your time. Vocabulary is not the only thing you need to learn, you also need to internalize collocations and grammatical structures, which is best done through actual reading. https://github.com/FreeLanguageTools/vocabsieve/ reply petemir 16 hours agoparentprev> I don't think I'm alone in never wanting to read an actual book with my phone Although a koreader integration would be great, there are tablets with both Android and iOS, as well as eReaders with the former. reply BlueFalconHD 7 hours agoparentprevNever used koreader, does it have plugin support? If so something like this wouldn't be too complex to integrate. reply Jeaye 6 hours agorootparentIt does have plugin support: https://koreader.rocks/user_guide/#plugins reply RomanPushkin 3 hours agoprevCan you add one Russian book with translations into Spanish? I wanna give it a try. Maybe a couple of chapters would work. This book is my favourite since I was a kid: \"magician in town yuri tomin\" (Russian: \"шёл по городу волшебник юрий томин\"). reply whycome 15 hours agoprevUX things. Let me click anywhere to close the popup. Having to target a small button when reading means I have to stretch my thumb to reach it as it can be far from the word I clicked. Don’t make me have to think about the targeting. Eg when reading I never want to think about HOW to turn a page or where I need to swipe. Make the scroll bar stay visible or at least make it big enough to easily grab! Not sure why the reflow is causing a horizontal scroll. Can you retain chapters from the original epub? Text size options. Hide the bottom logo and percentage when reading if I want. Its possible to convert html to epub but it would be better if you handled web pages natively. This is a really great language app! reply jonathanb88 14 hours agoparentThanks for the feedback, very helpful! I'll look to fix the first tomorrow, and add the rest to my list. reply dr_kiszonka 3 hours agoprevI tried that app and it looks good. It was a bit hard for me to decide what book to choose. Question: do users later get quizzed on the words they looked up? reply kazinator 16 hours agoprevRather, with this, you can learn to speak English like a Frenchman. For any English word that came from French, use the French cognate, and pronounce it in the French way. Or at least the latter. reply erichi 2 hours agoprevWould be nice to have an option to send an epub to Kindle. reply foundry27 16 hours agoprevThis looks really cool. I’d have loved to give it a try as someone interested in improving my French, but I wasn’t able to download the app since it’s pinned to the latest version of iOS only. Are you using APIs that are unavailable on iOS 16 and under, or is it a matter of testing? My understanding was that about 25% of iPhone users aren’t on iOS 17 (myself included!) so it’s a fairly large demographic reply jonathanb88 16 hours agoparentI think it is SwiftData that meant I had to limit to iOS 17. I will double check because it might be a limitation I can solve easily enough. reply internetter 16 hours agoparentprevObviously, I do not know about this app, but as an iOS developer, my apps are pinned to iOS 17 as well. SwiftUI is way too infant to not have the latest features at your disposal, in fact, I'd describe it as 'unusable' before iOS 16. reply dmattia 16 hours agoparentprev+1, my device is unfortunately too old to upgrade to iOS 17 and I wouldn't imagine this app would use too many new features reply phiresky 14 hours agoprevNeat! I had a very similar idea recently: https://seamlang.app/ (the sign up button doesn't work yet) The main difference seems to be that I start with text in the foreign language, and then translate the difficult vocabulary back to the known language (English). That way you always ensure you have the correct grammar of your goal language even if you don't know most of the vocabulary yet. This can be a bit confusing at first because you have mostly English text with Spanish word order, but just trying it a bit it works pretty well. It also makes the difficulty an easier problem because the grammer stays the same. I haven't gotten around to finishing it yet, especially judging which vocabulary to translate and ensuring each translated word still makes sense in context isn't easy. reply jonathanb88 12 hours agoprevMany thanks for all of the positive feedback today, lots of good ideas for me to get working on; what a great community! Side-note: A few eBooks are causing errors on the backend that don't appear to be DRM-related. I will prioritise getting this fixed. reply davidshepherd7 14 hours agoprevIn case it's useful to anyone: another implementation of this idea is Weeve https://shop.weeve.ie I bought one of their books (a study in scarlet) but it wasn't great. Lots of mistranslation, especially later on in the book. The general idea seemed to work well though, with better implementation I think it could really help my french. I'll give this one a try, being able to add my own books is particularly exciting. reply juancn 9 hours agoprevHow would you deal with things like Japanese and English, where the source and target languages are awfully apart? French, Italian and Spanish all share the same root and English borrows a lot of words from the three of them (plus the alphabet and the indo-european origin). reply troydavis 17 hours agoprevI'd use this if it had a book I wanted to read. A while back, I tried Prismatext (https://prismatext.com/). It only offered old classics that had come out of copyright (ie, Project Gutenberg) and a handful of poorly-reviewed modern novels. If you can license a modern book that someone would actually choose to read on their own, I'd pay for it. Bonus if I can sort/browse the available books by Goodreads (or similar) score. Prismatext makes it tedious to discover that readers didn't care for their modern books. reply jonathanb88 17 hours agoparentThanks for the feedback. One of the features allows users to upload an epub to use in the app. Although I realise that a better method is needed, as it has become harder to find legitimate places to buy epub formatted books. reply troydavis 14 hours agorootparentYou're welcome for the feedback. I saw the epub import and thought it was novel, but as you said, I don't know of anywhere to buy modern fiction in epub. If you know of legal sellers and linked to them from the site, that would probably be enough for me as a customer. That said, I'd gladly pay you/the site to handle that for me (by paying more than the book's retail price). Hopefully the translation would also be better than anything I imported. (Two sibling replies linked to sites that sell technical non-fiction. That is a very hard way to learn :-) ) reply throwaway81523 16 hours agorootparentprevI've bought a few epubs (English only) from https://www.humblebundle.com/books and https://www.fanatical.com/en/bundle/books . I'm still skeptical of this proposition though: to learn a language by listening to a TTS? Better to go talk to people. The language starts to solidify after the sounds and non-verbal cues start matching up. reply mdaniel 16 hours agorootparentprevDo you require the epub to be drm stripped first? reply jonathanb88 15 hours agorootparentThe app doesn't do anything to remove DRM, so it'll only work with DRM-free files. reply mertbio 16 hours agoprev> Other apps require you to build a habit around various exercises or ‘games’, whereas lots of people already read books. Shameless plug: I’ve identified the same problem and built an app that shows a new word every minute on the Menu Bar so I can learn a new word while working: https://apps.apple.com/us/app/wunderbar-learn-language/id647... reply IOT_Apprentice 3 hours agoprevWhen will you have Farsi, Japanese & Mandarin? reply toddmorey 15 hours agoprev\"I believe apps like this, which use AI to enhance or scale functionality rather than simply acting as a wrapper over APIs, will be the major beneficiaries as LLMs improve.\" Can you elaborate a bit more? Are you training your own model? Or do you mean this is a task that uniquely needs AI to solve and couldn't be accomplished with traditional APIs? reply jonathanb88 15 hours agoparentSome apps rely upon ongoing LLM API calls for their core functionality. Some require a lot of human editorial work up front. i.e. either high variable cost or high fixed cost economics. This app lies in a sweet spot where no ongoing API calls are required, everything is pre-calculated (at moderate expense!), but LLMs can scale some of the more 'human' work like explaining translations or checking accuracy. Albeit with the quirks and inconsistencies inherent with the current generation of models. reply jddj 15 hours agoparentprevI think they were differentiating themselves from their competitors by hinting that they put more work into this than just coming up with a UI and prompt for gpt4. reply vaughnegut 16 hours agoprevLooks great! I'd like to try it but the play store link has this error: > We're sorry, the requested URL was not found on this server. reply jonathanb88 15 hours agoparentA few people have raised this. I'll take a look. Unfortunately the App isn't available in all locations. reply gardenhedge 14 hours agorootparentGot the same. Was about to sign up for a month to check it out reply trelliscoded 14 hours agoprevThe LLM based explanations were the key thing about this app for me. It’s hard for me to fit foreign vocabularies into context for long-term recall without etymologies and comparisons to common roots in Latin, and I’ve already had several ah-ha moments due to those explanations. Thanks. reply jonathanb88 14 hours agoparentThanks for the feedback! Whilst there are some, often amusing, quirks with the LLM based explanations, I agree that the utility of the app is much higher with them. reply dabreegster 16 hours agoprevThis is super cool, thank you for building it! Two small UX ideas: - a scrollbar and search for the Online Library would be helpful - switching difficulty levels in the middle of reading could be helpful. Or if you keep that on a separate page, returning automatically to the last open position. (I was floating between beginner levels to find the right amount of challenge) reply jonathanb88 15 hours agoparentThanks for the feedback! I'll look at adding those. reply gleenn 16 hours agoprevThis such an awesome and unique new way to learn a language. I use both Pimsleurs and DuoLingo but it's always kind of a chore. Will definitely give this a shot. Really refreshing take on learning too, everyone basically has variants of flash cards which gets tedious. And it's free! Thank you! reply jasonjmcghee 16 hours agoparentIt's definitely a cool project, but this same concept has been around quite a while. Many chrome extensions do this. https://chrome.google.com/webstore/detail/readlang-web-reade... If OPs project is using LLMs, it could definitely be much higher quality when it's swapping out more than a few words. reply AnonHP 16 hours agoprevWhat’s the planned business model? Neither the website nor the app page mention “free”. There is no pricing link or an FAQ page on the website about the business model. Clarity in this area would be helpful. Until then, I wouldn’t want to spend too much time on it. Thanks. reply jonathanb88 16 hours agoparentRight now I see this as an MVP to get feedback and see if there is interest. I have no plans to charge for any of the current features. reply petemir 16 hours agoparentprevI second this! Obviously I wouldn't expect it to be free because of the different technologies (either current or planned) involved, but the lack of clarity in all the descriptions makes me doubt investing time/effort on it. reply tidojo 14 hours agoprevI’ve been wanting to get back my faded ability to read French after having neglected this skill for far too long. This is a pleasant way to ease me into this, I am loving this app! reply jonathanb88 14 hours agoparentThanks! reply parentheses 17 hours agoprevI'd love to see this on top of audible. Game changer for language learning! reply jgtor 15 hours agoparentThen can I shamelessly plug Tembo - Bilingual Stories (iOS/Android) to you. We offer audiobooks for some of our titles, so you can listen and learn simultaneously. It's a big undertaking to editing them all in-house, but we have audiobooks available for about 30 stories across our courses. https://www.tembo.app reply jonathanb88 16 hours agoparentprevLong term audio is something I'd look at, either audio books or podcasts. AR is another long term ambition. reply Player6225 15 hours agoprevOh great timing! I was just starting to play with building a toy project (https://github.com/bpevs/multireader) for practicing Spanish while reading books with my e-reader, but frankly, I'm just building it because I haven't found an app that works for me, and I'd rather spend time actually actually learning language... Just playing with your app for a bit, and it's pretty cool! had a few questions though: 1. Wondering about the decision of using English books and translating pieces into other languages vs starting with (for example) a Spanish book, and translating the other way? Also, would something like this be a future thought of plan? Because currently I'm trying to read more popular books in my target language, rather than English books (right now, my toy app is just highlight arbitrary text -> send to azure translate). I tried to upload my book into your app in Spanish, but I guess it only works rn if the source is in English? Basically, a mode for even more immersion would be killer (Ala either full-target-languge mode or upload target language books). 2. The practice mode is pretty cool! I like this format of \"complete the sentence\". It looks like it's not based on book content at all, right? Would be cool to practice based on what I'm reading. 3. I'm reading on an e-reader, so I'd reeeeally like a no-animation/no-scroll mode. On an e-reader, the paginated page refreshing can help to reduce ghosting. Even better if there could be an e-reader mode that can flash the screen to further reduce ghosting issues on those devices. reply jonathanb88 15 hours agoparentThanks for the feedback! 1. I would love to get it to work all the way from a few translations in the target language to a full translation, with a sliding scale in between. 2. It's not connected to the book content. One idea I have is an optional quiz at the end of a session to reinforce new vocabulary/grammar seen. 3. I'll see if I can remove the animation when using the page ahead/back buttons on Android. reply KomoD 15 hours agoprevGoogle Play button on the site leads to a 404. reply whycome 15 hours agoprevCan you clarify what the “upload” ability does? Is it putting my epub on your server? Does it remain there? reply jonathanb88 15 hours agoparentIt gets automatically deleted after 24 hours. reply unhammer 16 hours agoprevIs there a waiting list for Android? :) reply david_allison 16 hours agoparentFrom the site, there's a Play Store link: https://play.google.com/store/apps/details?id=com.glossarie.... reply unhammer 1 hour agorootparenthm, maybe region-restricted? I get a \"not found\" reply vaughnegut 15 hours agorootparentprevThis leads to a not found page, was the app taken down? reply david_allison 14 hours agorootparentWorks for me reply scandox 10 hours agorootparentNot working for me in Europe reply bomewish 15 hours agoprevWasn’t this idea an ACX post? I am absolutely sure Scott wrote that someone should create this. reply harshitpdoshi 17 hours agoprevBummer, it’s not available in my country. reply sinuhe69 15 hours agoparentNot in my App Store, too. I wonder why not release it worldwide? reply jonathanb88 14 hours agorootparentBecause I'm making public domain books available within the app (from Project Gutenburg), I have to limit the territories it is launched in, as some countries may have copyright limitations. I will expand the app into more countries once I'm happy none of the books have copyright restrictions there. Feel free to message me with the country you reside in and I'll take a look soon. reply Zenul_Abidin 16 hours agoprevSounds great. Any plans for a visionOS app? I think this would be the perfect use case for it. reply jonathanb88 12 hours agoparentSomething I'd love to look at longer-term. I think an overlay onto the real world that slowly immersed you in a new language would be a really powerful way to learn. reply tremarley 15 hours agoprevIt only works with iOS 17.0+ Could you make it work with any versions lower than 17.0? reply jonathanb88 14 hours agoparentI'll take a look. I think there were a couple of features that required 17.0+, but I may be able to solve with an earlier version. reply artemonster 2 hours agoprevcant find it on my iphone appstore reply Alexito 16 hours agoprevThis is a beautiful idea and easy app to use, thanks for the work! reply jonathanb88 16 hours agoparentThank you! reply exe34 15 hours agoprevI definitely don't want to learn another language by reading English. I've seen comments about people who learnt a language using Google translate from English, and they end up sounding like Google translate. I prefer to learn by reading in the target language and translating to English as I go along. reply JulianWasTaken 15 hours agoparentYeah I was gonna provide essentially the same feedback (so I'll just tack on here). I definitely didn't see what I expected when opening a book for the first time -- I can already read or watch content in Italian. What I do today is pause (or stop reading) when I encounter a word I don't know. What I expected when picking a level was definitely to see all Italian, though in retrospect I can imagine it's near impossible to do that without lots of paraphrasing. But to me personally (much as I think this space needs more things, and that you OP are awesome for sharing it) that I'd not personally use something which wasn't entirely in my target language, as I find the way I've learned languages best so far to be similar to my current workflow, and over time I have to look up fewer and fewer words. reply jonathanb88 15 hours agorootparentI agree - the dream would be to bridge from beginner level vocabulary all the way to a full translation in the target language. The limitation now is getting consistently high accuracy for whole sentences - but something I'll keep working on as the underlying technologies improve. reply revskill 16 hours agoprevWhich languages do you support ? reply jonathanb88 15 hours agoparentEnglish eBooks, with French, Italian and Spanish as the target languages to learn. I will also start looking at integrating German. reply darthrupert 16 hours agoprevThank you for the effort. I tried the practice a bit, and the explanations (generated by ai I guess?) were very nice. I met a bit of an unfair situation in one question. The sentence started with \"They\" and the options were Ils and Elles. However, the sentence in English didn't hint towards a gender, and I failed the 1d2 and got what felt like a sarcastic explanation. reply jonathanb88 15 hours agoparentThanks for the feedback! Admittedly the practice feature needs a bit more work. Helpful to know the issues you are experiencing. reply visarga 16 hours agoprev [–] This \"new\" idea I implemented for myself 15 years ago. reply kazinator 15 hours agoparent [–] An idea generally becomes new to the world when it is published. If you claim that you privately had the idea 15 years ago, it's possible; you just need credible, and credibly dated evidence. reply visarga 1 hour agorootparent [–] I used it to teach myself Japanese. I would replace words in an English text with the Japanese translations, including kanji, furigana and English. I was working on a way to develop a curriculum of vocabulary. I prepared a couple of books this way and read them, then abandoned the approach. But it was pretty fun. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "An app created by an individual and their language teacher partner enables language learning through reading eBooks in the user's native language, emphasizing vocabulary and grammar in context.",
      "The app has proven effective in establishing a foundational comprehension of a language and is constantly enhanced for precision and user satisfaction.",
      "Upcoming developments involve implementing dynamic difficulty levels, enhancing practice features, improving eBook support, and integrating AI for advanced functionality, with user feedback playing a crucial role in refining the app."
    ],
    "commentSummary": [
      "Reddit users are sharing insights on language learning apps and techniques, such as Glossarie for vocabulary and grammar via reading native language eBooks.",
      "Recommendations include Language Transfer for audio courses and browser plugins to facilitate learning while reading online.",
      "Discussions cover the use of flags for languages, spaced-repetition learning, and apps teaching languages through cognates and pronunciation; developers are seeking feedback to enhance features and translations amid some user concerns about tool quality and fairness."
    ],
    "points": 259,
    "commentCount": 119,
    "retryCount": 0,
    "time": 1711294996
  },
  {
    "id": 39812969,
    "title": "Unlocking Efficiency: Embracing async/await in Rust",
    "originLink": "https://notgull.net/why-not-threads/",
    "originBody": "Why choose async/await over threads? John Nunley · March 24, 2024 async rust smol A common refrain is that threads can do everything that async/await can, but simpler. So why would anyone choose async/await? This is a common question that I’ve seen a lot in the Rust community. Frankly, I completely understand where it’s coming from. Rust is a low-level language that doesn’t hide the complexity of coroutines from you. This is in opposition to languages like Go, where async happens by default, without the programmer needing to even consider it. Smart programmers try to avoid complexity. So, they see the extra complexity in async/await and question why it is needed. This question is especially pertinent when considering that a reasonable alternative exists in OS threads. Let’s take a mind-journey through async and see how it stacks up. Background Blitz Rust is a low-level language. Normally, code is linear; one thing runs after another. It looks like this: fn main() { foo(); bar(); baz(); } Nice and simple, right? However, sometimes you will want to run many things at once. The canonical example for this is a web server. Consider the following written in linear code: fn main() -> io::Result { let socket = TcpListener::bind(\"0.0.0.0:80\")?; loop { let (client, _) = socket.accept()?; handle_client(client)?; } } Imagine if handle_client takes a few milliseconds, and two clients try to connect to your webserver at the same time. You’ll run into a serious problem! Client #1 connects to the webserver, and is accepted by the accept() function. It starts running handle_client(). Client #2 connects to the webserver. However, since accept() is not currently running, we have to wait for handle_client() for Client #1 to finish running. After waiting a few milliseconds, we get back to accept(). Client #2 can connect. Now imagine that instead of two clients, there are two million simultaneous clients. At the end of the queue, you’ll have to wait several minutes before the web server can help you. It becomes un-scalable very quickly. Obviously, the embryonic web tried to solve this problem. The original solution was to introduce threading. By saving the value of some registers and the program’s stack into memory, the operating system can stop a program, run another program in its place, then resume running that program later. Essentially, it allows for multiple routines (or “threads”, or “processes”) to run on the same CPU. Using threads, we can rewrite the above code as follows: fn main() -> io::Result { let socket = TcpListener::bind(\"0.0.0.0:80\")?; loop { let (client, _) = socket.accept()?; thread::spawn(move || handle_client(client)); } } Now, the client is being handled by a separate thread than the one handling waiting for new connections. Great! This avoids the problem by allowing concurrent thread access. Client #1 is accepted by the server. The server spawns a thread that calls handle_client. Client #2 tries to connect to the server. Eventually, handle_client blocks on something. The OS saves the thread handling Client #1 and brings back the main thread. The main thread accepts Client #2. It spawns a separate thread to handle Client #2. With only a few microseconds of delay, Client #1 and Client #2 are run in parallel. Threads work especially well when you consider that production-grade web servers have dozens of CPU cores. It’s not just that the OS can give the illusion that all of these threads run at the same time; it’s that the OS can actually make them all run at once. Eventually, for reasons I’ll elaborate later, programmers wanted to bring this concurrency out of the OS space and into the user space. There are many different models for userspace concurrency. There is event-driven programming, actors, and coroutines. The one Rust settled on is async/await. To oversimplify, you compile the program as a grab-bag of state machines that can all be run independently of another. Rust itself provides a mechanism for creating state machines; the mechanism of async and await. The above program in terms of async/await would look like this, written using smol: #[apply(smol_macros::main!)] async fn main(ex: &smol::Executor) -> io::Result { let socket = TcpListener::bind(\"0.0.0.0:80\").await?; loop { let (client, _) = socket.accept().await?; ex.spawn(async move { handle_client(client).await; }).detach(); } } The main function is preceded with the async keyword. This means that it is not a traditional function, but one that returns a state machine. Roughly, the function’s contents correspond to that state machine. await includes another state machine as a part of the currently running state machine. For accept(), it means that the state machine will include it as a step. Eventually, one of the inner functions will yield, or give up control. For example, when accept() waits for a new connection. At this point the entire state machine will yield its execution to the higher-level executor. For us, that is smol::Executor. Once execution is yielded, the Executor will replace the current state machine with another one that is running concurrently, spawned through the spawn function. We pass an async block to the spawn function. This block represents an entire new state machine, independent of the one created by the main function. All this state machine does is run the handle_client function. Once main yields, one of the clients is selected to run in its place. Once that client yields, the cycle repeats. You can now handle millions of simultaneous clients. Of course, user-space concurrency like this introduces an uptick in complexity. When you’re using threads, you don’t have to deal with executors and tasks and state machines and all. If you’re a reasonable person, you might be asking “why do we need to do all of this? Threads work well; for 99% of programs, we don’t need to involve any kind of user-space concurrency. Introducing new complexity is technical debt, and technical debt costs us time and money. “So why wouldn’t we use threads?” Timeout Trouble Perhaps one of Rust’s biggest strengths is composability. It provides a set of abstractions that can be nested, built upon, put together, and expanded upon. I recall that the thing that made me stick with Rust is the Iterator trait. It blew my mind that you could make something an Iterator, apply a handful of different combinators, then pass the resulting Iterator into any function that took an Iterator. It continues to impress me how powerful it is. Let’s say you want to receive a list of integers from another thread, only take the ones that are immediately available, discard any integers that aren’t even, add one to all of them, then push them onto a new list. That would be fifty lines and a helper function in some other languages. In Rust it can be done in five: let (send, recv) = mpsc::channel(); my_list.extend( recv.try_iter() .filter(|x| x & 1 == 0) .map(|x| x + 1) ); The best thing about async/await is that it lets you apply this composability to I/O-bound functions. Let’s say you have a new client requirement; you want to add a timeout to your above function. Assume that our handle_client above function looks like this: async fn handle_client(client: TcpStream) -> io::Result { let mut data = vec![]; client.read_to_end(&mut data).await?; let response = do_something_with_data(data).await? client.write_all(&response).await?; Ok(()) } If we want to add, say, a three-second timeout, we can combine two combinators to do that: The race function takes two futures and runs them at the same time. The Timer future waits for some time before returning. Here is what the final code looks like: async fn handle_client(client: TcpStream) -> io::Result { // Future that handles the actual connection. let driver = async move { let mut data = vec![]; client.read_to_end(&mut data).await?; let response = do_something_with_data(data).await? client.write_all(&response).await?; Ok(()) }; // Future that handles waiting for a timeout. let timeout = async { Timer::after(Duration::from_secs(3)).await; // We just hit a timeout! Return an error. Err(io::ErrorKind::TimedOut.into()) }; // Run both in parallel. driver.race(timeout).await } I find this to be a very easy process. All you have to do is wrap your existing code in an async block and race it against another future. An added bonus of this approach is that it works with any kind of stream. Here, we use a TcpStream. However we can easily replace it with anything that implements impl AsyncRead + AsyncWrite. It could be a GZIP stream on top of the normal stream, or a Unix socket, or a file. async just slides into whatever pattern you need from it. Thematic Threads What if we wanted to implement this in our threaded example above? fn handle_client(client: TcpStream) -> io::Result { let mut data = vec![]; client.read_to_end(&mut data)?; let response = do_something_with_data(data)? client.write_all(&response)?; Ok(()) } Well, it’s not easy. Generally, you can’t interrupt the read or write system calls in blocking code, without doing something catastrophic like closing the file descriptor (which can’t be done in Rust). Thankfully, TcpStream has two functions set_read_timeout and set_write_timeout that can be used to set the timeouts for reading and writing, respectively. However, we can’t just use it naively. Imagine a client that sends one byte every 2.9 seconds, just to reset the timeout. So we have to program a little defensively here. Due to the power of Rust combinators, we can write our own type wrapping around the TcpStream to program the timeout. // Deadline-aware wrapper around `TcpStream. struct DeadlineStream { tcp: TcpStream, deadline: Instant } impl DeadlineStream { /// Create a new `DeadlineStream` that expires after some time. fn new(tcp: TcpStream, timeout: Duration) -> Self { Self { tcp, deadline: Instant::now() + timeout, } } } impl io::Read for DeadlineStream { fn read(&mut self, buf: &mut [u8]) -> io::Result { // Set the deadline. let time_left = self.deadline.saturating_duration_since(Instant::now()); self.tcp.set_read_timeout(Some(time_left))?; // Read from the stream. self.tcp.read(buf) } } impl io::Write for DeadlineStream { fn write(&mut self, buf: &[u8]) -> io::Result { // Set the deadline. let time_left = self.deadline.saturating_duration_since(Instant::now()); self.tcp.set_write_timeout(Some(time_left))?; // Read from the stream. self.tcp.write(buf) } } // Create the wrapper. let client = DeadlineStream::new(client, Duration::from_secs(3)); let mut data = vec![]; client.read_to_end(&mut data)?; let response = do_something_with_data(data)? client.write_all(&response)?; Ok(()) On one hand, it could be argued that this is elegant. We used Rust’s capabilities to solve the problem with a relatively simple combinator. I’m sure it would work well enough. On the other hand, it’s definitely hacky. We’ve locked ourselves into using TcpStream. There’s no trait in Rust to abstract over using the set_read_timeout and set_write_timeout types. So it would take a lot of additional work to make it use any kind of writer. It involves an extra system call for setting the timeout. I imagine this type is much more unwieldy to use for the kinds of actual logic that web servers demand. If I saw this code in production, I would ask the author why they avoided using async/await to solve this problem. This is the phenomenon I was describing in my post “Why you might actually want async in your project”. Quite frequently I encounter a pattern where synchronous code can’t be used without contortion, so I have to rewrite it in async. Async Success Stories There’s a reason why the HTTP ecosystem has adopted async/await as its primary runtime mechanism, even for clients. You can take any function that makes an HTTP call, and make it fit whatever hole or use case you want it to. tower is probably the best example of this phenomenon I can think of, and it’s really the thing that made me realize how powerful async/await can be. If you implement your service as an async function, you get timeouts, rate limiting, load balancing, hedging and back-pressure handling. All of that for free. It doesn’t matter what runtime you used, or what you’re actually doing in your service. You can throw tower at it to make it more robust. macroquad is a miniature Rust game engine that aims to make game development as easy as possible. Its main function uses async/await in order to run its engine. This is because async/await is really the best way in Rust to express a linear function that needs to be stopped in order to wait for something else. In practice, this can be extremely powerful. Imagine simultaneously polling a network connection to your game server and your GUI framework, on the same thread. The possibilities are endless. Improving Async’s Image I don’t think the issue is that some people think threads are better than async. I think the issue is that the benefits of async aren’t widely broadcast. This leads some people to be misinformed about the benefits of async. If this is an educational problem, I think it’s worth taking a look at the educational material. Here’s what the Rust Async Book says when comparing async/await to operating system threads. OS threads don’t require any changes to the programming model, which makes it very easy to express concurrency. However, synchronizing between threads can be difficult, and the performance overhead is large. Thread pools can mitigate some of these costs, but not enough to support massive IO-bound workloads. - Rust Async Book, various authors I think this is a consistent problem throughout the async community. When someone asks the question of “why do we want to use this over OS threads”, people have a tendency to kind of wave their hand and say “async has less overhead. Other than that, everything’s the same.” This is the reason why web server authors switched to async/await. It’s how they solved the C10k problem. But, it’s not going to be the reason why everyone else switches to async/await. Performance gains are fickle and can disappear in the wrong circumstances. There are plenty of cases where a threaded workflow can be faster than an equivalent async workflow (mostly, in the case of CPU bound tasks). I think that we, as a community, have over-emphasized the ephemeral performance benefits of async Rust while downplaying its semantic benefits. In the worst case, it leads to people shrugging off async/await as “a weird thing that you resort to for niche use cases”. It should be seen as a powerful programming model that lets you succinctly express patterns that can’t be expressed in synchronous Rust without dozens of threads and channels. I also think there’s a tendency to try to make async Rust “just like sync Rust” in a way that encourages negative comparison. By “tendency”, I mean that it’s the stated roadmap for the Rust project, saying that “that writing async Rust code should be as easy as writing sync code, apart from the occasional async and await keyword.”. I reject this framing because it’s fundamentally impossible. It’s like trying to host a pizza party on a ski slope. Sure, you can probably get 99% of the way there, especially if you’re really talented. But there are differences that the average bear will notice, no matter how good you are. We shouldn’t be trying to force our model into unfriendly idioms to appease programmers who refuse to adopt another type of pattern. We should be trying to highlight the strengths of Rust’s async/await ecosystem; its composability and its power. We should be trying to make it so async/await is the default choice whenever a programmer reaches for concurrency. Rather than trying to make sync Rust and async Rust the same, we should embrace the differences. In short, we shouldn’t be using technical reasons to argue for a semantic model. We should be using semantic reasons. Share: Twitter, Facebook This website's source code is hosted via Codeberg",
    "commentLink": "https://news.ycombinator.com/item?id=39812969",
    "commentBody": "Why choose async/await over threads? (notgull.net)198 points by thunderbong 5 hours agohidepastfavorite120 comments newpavlov 4 hours agoI think a better question is \"why choose async/await over fibers?\". Yes, I know that Rust had green threads in the pre-1.0 days and it was intentionally removed, but there are different approaches for implementing fiber-based concurrency, including those which do not require a fat runtime built-in into the language. If I understand the article correctly, it mostly lauds the ability to drop futures at any moment. Yes, you can not do a similar thing with threads for obvious reasons (well, technically, you can, but it's extremely unsafe). But this ability comes at a HUGE cost. Not only you can not use stack-based arrays with completion-based executors like io-uring and execute sub-tasks on different executor threads, but it also introduces certain subtle footguns and reliability issues (e.g. see [0]), which become very unpleasant surprises after writing sync Rust. My opinion is that cancellation of tasks fundamentally should be cooperative and uncooperative cancellation is more of a misfeature, which is convenient at the surface level, but has deep issues underneath. Also, praising composability of async/await sounds... strange. Its viral nature makes it anything but composable (with the current version of Rust without a proper effect system). For example, try to use async closure with map methods from std. What about using the standard io::Read/Write traits? [0]: https://smallcultfollowing.com/babysteps/blog/2022/06/13/asy... reply dwaite 2 hours agoparentFor rust, fibers (as a user-space, cooperative concurrency abstraction) would mandate a lot of design choices, such as whether stacks should be implemented using spaghetti stacks or require some sort of process-level memory mapping library, or even if they were just limited to a fixed size stack. All three of these approaches would cause issues when interacting with code in another language with a different ABI. It can get really complicated, for example, when C code gets called from one fiber and wants to then resume another. One of the benefits of async/await is the 'await' keyword itself. The explicit wait-points give you the ability to actually reason about the interactions of a concurrent program. Yielding fibers are a bit like the 'goto' of the concurrency world - whenever you call a method, you don't know if as a side effect it may cause your processing to pause, and if when it continues the state of the world has changed. The need to be defensive when interfacing with the outside world means fibers tend to be better for tasks which run in isolation and communicate by completion. Green threads, fibers and coroutines all share the same set of problems here, but really user space cooperative concurrency is just shuffling papers on a desk in terms of solving the hard parts of concurrency. Rust async/await leaves things more explicit, but as a result doesn't hide certain side effects other mechanisms do. reply pron 19 minutes agorootparent> you don't know if as a side effect it may cause your processing to pause, and if when it continues the state of the world has changed This may be true in JS (or Haskell), but not in Rust, where you already have multithreading (and unrestricted side-effects), and so other code may always be interleaved. Furthermore, the difference is merely in the default. With threads, the default is that interleaving may happen anywhere unless excluded, while with async/await it's the other way around. The threading approach is more composable, maintainable, and safer, because the important property is that of non -interleaving, which threads state explicitly. Any subroutine specifies where it does not tolerate contention regardless of other subroutine it calls. In the async/await model, adding a yield point to a previously non-yielding subroutine requires examining the assumptions of its callers. reply newpavlov 1 hour agorootparentprevIn my opinion, by default fibers should use \"full\" stacks, i.e. a reasonable amount of unpopulated memory pages (e.g. 2 MiB) with guard page. Effectively, the same stack which we use for threads. It should eliminate all issues about interfacing with external code. But it obviously has performance implications, especially for very small tasks. Further, on top of this we can then develop spawning tasks which would use parent's stack. It would require certain language development to allow computing maximum stack usage bound of functions. Obviously, such computation would mean that programmers have to take additional restrictions on their code (such as disallowing recursion, alloca, and calling external functions without attributing stack usage), but compilers already routinely compute stack usage of functions, so for pure Rust code it should be doable. >It can get really complicated, for example, when C code gets called from one fiber and wants to then resume another. It's a weird example. How would a C library know about fiber runtime used in Rust? >Yielding fibers are a bit like the 'goto' of the concurrency world - whenever you call a method, you don't know if as a side effect it may cause your processing to pause, and if when it continues the state of the world has changed. I find this argument funny. Why don't you have the same issue with preemptive multitasking? We live with exactly this \"issue\" in the threading world just fine. Even worse, we can not even rely on \"critical sections\", thread's execution can be preempted at ANY moment. As for `await` keyword, in almost all cases I find it nothing more than a visual noise. It does not provide any practically useful information for programmer. How often did you wonder when writing threading-based code about whether function does any IO or not? reply vlovich123 1 hour agorootparent> It’s a weird example. How would a C library know about the fiber runtime used in Rust. Well, if you green thread were launched on an arbitrary free OS thread (work stealing), then for example your TLS variables would be very wrong when you resume execution. Does it break all FFI? No. But it can cause issues for some FFI in a way that async/await cannot. > I find this argument funny. Why don't you have the same issue with preemptive multitasking? We live with exactly this \"issue\" in the threading world just fine. Even worse, we can not even rely on \"critical sections\", thread's execution can be preempted at ANY moment. It’s not about critical sections as much. Since the author referenced go to, I think the point is that it gets harder to reason about control flow within your own code. Whether or not that’s true is debatable since there’s not really any implementation of green threads for Rust. It does seem to work well enough for Go but it has a required dedicated keyword to create that green thread to ease readability. > As for `await` keyword, in almost all cases I find it nothing more than a visual noise. It does not provide any practically useful information for programmer. How often did you wonder when writing threading-based code about whether function does any IO or not? Agree to disagree. It provides very clear demarcation of which lines are possible suspension points which is important when trying to figure out where “non interruptible” operations need to be written for things to work as intended. reply immibis 10 minutes agorootparentObviously you would not use operating system TLS variables when your code does not correspond the operating system threads. They're just globals, anyway - why are we on Hacker News discussing the best kind of globals? Avoid them and things will go better. reply lmm 3 hours agoparentprevHow do fibers solve your cancellation problem? Aren't they more or less equivalent? (I find fiber-based code hard to follow because you're effectively forced to reason operationally. Keeping track of in-progress threads in your head is much harder than keeping track of to-be-completed values, at least for me) reply newpavlov 3 hours agorootparentWith fibers you send cancellation signal to a task, then on next IO operation (or more generally yield) it will get cancellation error code with ability to get true result of IO operation, if there is any. Note that it does not mean that the task will sleep until the IO operation gets completed, cancellation signal causes any ongoing IO to \"complete\" immediately if it's possible (e.g. IIRC disk IO can not be cancelled). It then becomes responsibility of the task to handle this signal. It may either finish immediately (e.g. by bubbling the \"cancellation\" error), finish some critical section before that and do some cleanup IO, or it may even outright ignore the signal. With futures you just drop the task's future (i.e. its persistent stack) maybe with some synchronous cleanup and that's it, you don't give the task a chance to say a word in its cancellation. Hypothetical async Drop could help here (though you would have to rely on async drop guards extensively instead of processing \"cancellation errors\"), but adding it to Rust is far from easy and AFAIK there are certain fundamental issues with it. With io-uring sending cancellation signals is quite straightforward (though you need to account for different possibilities, such as task being currently executed on a separate executor thread, or its CQE being already in completion queue), but with epoll, unfortunately, it's... less pleasant. reply lmm 2 hours agorootparent> Hypothetical async Drop could help here (though you would have to rely on async drop guards extensively instead of processing \"cancellation errors\"), but adding it to Rust is far from easy and AFAIK there are certain fundamental issues with it. Wouldn't fiber cancellation be equivalent and have equivalent implementation difficulties? You say you just send a signal to the task, but in practice picking up and running the task to trigger its cancellation error handling is going to look the same as running a future's async drop, isn't it? reply newpavlov 2 hours agorootparentFirstly, Rust does not have async Drop and it's unlikely to be added in the foreseeable future. Secondly, cancellation signals is a more general technique than async Drop, i.e. you can implement the latter on top of the former, but not the other way around. For example, with async Drop you can not ignore cancellation event (unless you copy code of your whole task into Drop impl). Some may say that it's a good thing, but it's just an obvious example of cancellation signals being more powerful than hypothetical async Drop. As for implementation difficulties, I don't think so. For async Drop you need to mess with some fundamental parts of the Rust language (since Futures are \"just types\"), while fiber-based concurrency, in a certain sense, is transparent for compiler and implementation complexity is moved to executors. If you are asking about how it would look in user code, then, yes, they would be somewhat similar. With cancellation signals you would call something like `let res = task_handle.cancell_join();`, while with async Drop you would use `drop(task_future)`. Note that the former also allows to get result from a cancelled task, another example of greater flexibility. reply bheadmaster 3 hours agorootparentprev> Keeping track of in-progress threads in your head is much harder than keeping track of to-be-completed values, at least for me I think that's true for everybody. Our minds barely handle state for sequential code - the explosion of complexity of multiple state-modifying threads is almost impossible to follow. There are ways to convert \"keeping track of in-progress threads\" to \"keeping track or to-be-completed\" values - in particular, Go uses channels as a communication mechanism which explicitly does the latter, while abstracting away the former. reply lloeki 3 hours agoparentprev> I think a better question is \"why choose async/await over fibers? > there are different approaches for implementing fiber-based concurrency, including those which do not require a fat runtime built-in into the language This keynote below is Ruby so the thread/GVL situation is different than for Rust, but is that the kind of thing you mean? https://m.youtube.com/watch?v=qKQcUDEo-ZI I think it makes a good case that async/await is infectious and awkward, and fibers (at least as implemented in Ruby) is quite simply a better paradigm. reply dwaite 1 hour agorootparent> I think it makes a good case that async/await is infectious and awkward, and fibers (at least as implemented in Ruby) is quite simply a better paradigm. As someone who has done fibers development in Ruby, I disagree. CRuby has the disadvantage of a global interpreter lock. This means that parallelism can only be achieved via multiple processes. This is not the case in Rust, where have access to do true parallelism in a single process. Second, this talk is not arguing for use of fibers as much as it is arguing as using fibers to rig up a bespoke green threads-like system for a specific web application server, and advocating for ruby runtime features to make the code burden on them of doing this lighter. Ruby has a global interpreter lock, so even though it uses native threads only one of them can be executing ruby code at a time. Fibers have native stacks, so they have all the resource requirements of a thread sans context switching - but the limitations from the GIL actually mean you aren't _saving_ context switching by structuring your code to use fibers in typical (non \"hello world\" web server) usage. reply newpavlov 3 hours agorootparentprevI only scrolled the video, but it sounds similar, yes. Though, implementation details would probably vary significantly, since Rust is a lower-level language. reply ngrilly 1 hour agoparentprevCould you share an example of a fiber implementation not relying a fat runtime built in the language? reply newpavlov 1 hour agorootparenthttps://github.com/Xudong-Huang/may The project has some serious restrictions and unsound footguns (e.g. around TLS), but otherwise it's usable enough. There are also a number of C/C++ libraries, but I can not comment on those. reply anonymoushn 21 minutes agorootparentprevFor example https://github.com/creationix/libco The required thing is mostly just to dump your registers on the stack and jump. reply logicchains 2 hours agoparentprevThe sole advantage of async/await over fibers is the possibility to achieve ultra-low-latency via the compiler converting the async/await into a state machine. This is important for Rust as a systems language, but if you don't need ultra-low-latency then something with a CSP model built on fibers, like Goroutines or the new Java coroutines, is much easier to reason about. reply pron 31 minutes agorootparent> The key requirement for ultra-low-latency software is minimising/eliminating dynamic memory allocation First, that is only true in languages -- like C++ or Rust -- where dynamic memory allocation (and deallocation) is relatively costly. In a language like Java, the cost of heap allocation is comparable to stack allocation (it's a pointer bump). Second, in the most common case of writing high throughput servers, the performance comes from Little's law and depends on having a large number of threads/coroutines. That means that all the data required for the concurrent tasks cannot fit in the CPU cache, and so switching in a task incurs a cache-miss, and so cannot be too low-latency. The only use-cases where avoiding memory allocation could be useful and achieving very low latency is possible are when the number of threads/coroutines is very small, e.g. generators. The questions, then, are which use-case you pick to guide the design, servers or generators, and what the costs of memory management are in your language. reply newpavlov 2 hours agorootparentprevFibers and async/await are backed by the same OS APIs, they can achieve more or less the same latency. The main advantage of async/await (or to be more precise stackless coroutines) is that they require less memory for task stacks since tasks can reuse executor's stack for non-persistent part of their stack (i.e. stack variables which do not cross yield points). It has very little to do with latency. At most you can argue that executor's stack stays in CPU cache, which reduces amount of cache misses a bit. Stackless coroutines also make it easier to use parent's stack for its children stacks. But IMO it's only because compilers currently do not have tools to communicate maximum stack usage bound of functions to programming languages. reply nominatronic 1 hour agorootparent> Fibers and async/await are backed by the same OS APIs async/await doesn't require any OS APIs, or even an OS at all. You can write async rust that runs on a microcontroller and poll a future directly from an interrupt handler. And there's a huge advantage to doing so, too: you can write out sequences of operations in a straightforward procedural form, and let the compiler do the work of turning that into a state machine with a minimal state representation, rather than doing that manually. reply newpavlov 1 hour agorootparentSigh... It gets tiring to hear about embedded from async/await advocates as if it's a unique advantage of the model. Fibers and similar mechanisms are used routinely in embedded world as demonstrated by various RTOSes. Fibers are built on yielding execution to someone else, which is implemented trivially on embedded targets. Arguably, in a certain sense, fibers can be even better suited for embedded since they allow preemption of task by interrupts at any moment with interrupts being processed by another task, while with async/await you have to put event into queue and continue execution of previously executed future. reply logicchains 1 hour agorootparentprev>Fibers and async/await are backed by the same OS APIs, they can achieve more or less the same latency. The key requirement for ultra-low-latency software is minimising/eliminating dynamic memory allocation, and stackless coroutines allow avoiding memory allocation. For managed coroutines on the other hand (e.g. goroutines, Java coroutines) as far as I'm aware it's impossible to have an implementation that doesn't do any dynamic memory allocation, or at least there aren't any such implementations in practice. reply newpavlov 1 hour agorootparentYes, it's what I wrote about in the last paragraph. If you can compute maximum stack size of a function, then you can avoid dynamic allocation with fibers as well (you also could provide stack size manually, but it would break horribly if the provided number is wrong). You are right that such implementations do not exist right now, but I think it's technically feasible, as demonstrated by tools such as https://github.com/japaric/cargo-call-stack The main stumbling block here is FFI, historically shared libraries do not have any annotations about stack usage, so functions with bounded stack usage would not be able to use even libc. reply Animats 2 hours agoprevAsync/await with one thread is simple and well-understood. That's the Javascript model. Threads let you get all those CPUs working on the problem, and Rust helps you manage the locking. Plus, you can have threads at different priorities, which may be necessary if you're compute-bound. Multi-threaded async/await gets ugly. If you have serious compute-bound sections, the model tends to break down, because you're effectively blocking a thread that you share with others. Compute-bound multi-threaded does not work as well in Rust as it should. Problems include: - Futex congestion collapse. This tends to be a problem with some storage allocators. Many threads are hitting the same locks. In particular, growing a buffer can get very expensive in allocators where the recopying takes place with the entire storage allocator locked. I've mentioned before that Wine's library allocator, in a .DLL that's emulating a Microsoft library, is badly prone to this problem. Performance drops by two orders of magnitude with all the CPU time going into spinlocks. Microsoft's own implementation does not have this problem. - Starvation of unfair mutexes. Both the standard Mutex and crossbeam-channel channels are unfair. If you have multiple threads locking a resource, doing something, unlocking the resource, and repeating that cycle, one thread will win repeatedly and the others will get locked out.[1] If you need fair mutexes, there's \"parking-lot\". But you don't get the poisoning safety on thread panic that the standard mutexes give you. If you're not I/O bound, this gets much more complicated. [1] https://users.rust-lang.org/t/mutex-starvation/89080 reply jmspring 1 hour agoparentYou focus on rust rather than generalizing... If you are IO bound, consider threads. This is almost the same as async / await. What was missing above, and the problem with how most compute education is these days, if you are compute bound you need to think about processes. If you were dealing with python concurrent.futures, you would need to consider processpooexecutor vs. threadpoolexecutor. Threadpoolexecutor gives you the same as the above. With multiprocessor executor, you will have multiple processes executing independently but you have to copy a memory space. Which people don't consider. In python DS work - multiprocessor workloads need to determine memory space considerations. It's kinda f'd up how JS doesn't have engineers think about their workloads. reply danbruc 0 minutes agorootparent[...] if you are compute bound you need to think about processes. How would that help? Running several processes instead of several threads will not speed anything up [1] and might actually slow you down because of additional inter-process communication overhead. [1] Unless we are talking about running processes across multiple machines to make use of additional processors. reply baq 30 minutes agorootparentprevBackend JS just spins up another container and/or lambda and if it's too slow and requires multiple CPUs in a single deployment, oh well, too bad. reply exfalso 2 hours agoparentprevYes, 100%. I've mostly only dealt with IO-bound computations, but the contention issues arise there as well. What's the point of having a million coroutines when the IO throughput is bounded again? How will coroutines save me when I immediately exhaust my size 10 DB connection pool? It won't, it just makes debugging and working around the issues harder and difficult to reason about. reply _flux 1 hour agoparentprevJemalloc can use separate arenas for different threads which I imagine mostly solves the futex congestion issue. Perhaps it introduces new ones? reply bsaul 16 minutes agoprev\"Obviously, the embryonic web tried to solve this problem. The original solution was to introduce threading\" Are we calling 1970's mainframes \"the embryonic web\" now ? reply mike_hearn 1 minute agoparentThat sentence is super confusing. The web has never supported multi-threading. reply immibis 7 minutes agoparentprevWell, apparently they did consist of a lot of tangled threads... reply exfalso 2 hours agoprevIt's interesting to see an almost marketing-like campaign to save face for async/await. It is very clear from my experience that it was not only a technical mistake, it also cost the community dearly. Instead of focusing on language features that are actually useful, the Rust effort has been sidetracked by this mess. I'm still very hopeful for the language though, and it is the best thing we've got at the moment. I'm just worried that this whole fight will drag on forever. P.S the AsyncWrite/AsyncRead example looks reasonable, but in fact you can do the same thing with threads/fds as long as you restrict yourself to *nix. reply guappa 5 minutes agoparentIf you think that threads are faster than poll(), I would like to know in what use case that happens, because I have never once encountered this in my life. reply junon 1 hour agoparentprevI've used async in firmware before. It was a lifesaver. The generalizations you make are unfounded and are clearly biased toward a certain workload. reply ykonstant 1 hour agoparentprev> Instead of focusing on language features that are actually useful, the Rust effort has been sidetracked by this mess. I don't know if you are correct or not (I am not very familiar with Rust) but empirically 9/10 Rust discussions I see nowadays on HN/reddit do revolve around async. It kinda sucks for me because I don't care about async at all, and I am interested in reading stuff about Rust. reply logicchains 2 hours agoparentprevIt's not a technical mistake, it's a brilliant solution for when you need ultra-low-latency async code. The mistake is pushing it for the vast majority of use-cases where this isn't needed. reply avodonosov 1 hour agoprevIssues with the article: 1. Only one example is given (web server), solved incorrectly for threads. I will elaborate below. 2. The question is framed as if people specifically want OS threads instead of async/await . But programmers want threads conceptually, semantically. Write sequential logic and don't use strange annotations like \"async\". In other words, if async/await is so good, why not make all functions in the language implicitly async, and instead of \"await\" just write normal function calls? Then you will suddenly be programming in threads. OS threads are expensive due to statically allocated stack, and we don't want that. We want cheap threads, that can be run in millions on a single CPU. But without the clumsy \"async/await\" words. (The `wait` word remains in it's classic sense: when you wait for an event, for another thread to complete, etc - a blocking operation of waiting. But we don't want it for function invocations). Back to #1 - the web server example. When timeout is implemented in async/await variant of the solution, using the `driver.race(timeout).await`, what happens to the client socket after the `race` signals the timeout error? Does socket remain open, remains connected to the client - essentially leaked? The timeout solution for threaded version may look almost the same, as it looks for async/await: `threaded_race(client_thread, timeout).wait`. This threaded_race function uses a timer to track a timeout in parallel with the thread, and when the timeout is reached it calls `client_thread.interrupt()` - the Java way. (The `Thread.interrupt()`, if thread is not blocked, simply sets a flag; and if the thread is blocked in an IO call, this call throws an InterruptedException. That's a checked exception, so compiler forces programmer to wrap the `client.read_to_end(&mut data)` into try / catch or declare the exception in the `handle_client`. So programmer will not forget to close the client socket). reply f_devd 39 minutes agoparent> When timeout is implemented in async/await variant of the solution, using the `driver.race(timeout).await`, what happens to the client socket after the `race` signals the timeout error? Any internal race() values will be `Drop`ed and driver itself will remain (although rust will complain you are not handling the Result if you type it 'as is'), if a new socket was created local to the future it will be cleaned up. The niceness of futures (in Rust) is that all the behavior around it can be defined, while \"all functions are blocking.\" as you state in a sibling comment, Rust allows you to specify when to defer execution to the next task in the task queue, meaning it will poll tasks arbitrarily quickly with an explicitly held state (the Future struct). This makes it both very fast (compared to threads which need to sleep() in order to defer) and easy to reason about. Java's Thread.interrupt is also just a sleep loop, which is fine for most applications to be fair. Rust is a system language, you can't have that in embedded systems, and it's not desirable for kernels or low-latency applications. reply fragmede 1 hour agoparentprevThere's also writing your code with poll() and select(), which is its own thing. reply littlestymaar 1 hour agoparentprev> But programmers want threads conceptually, semantically. Write sequential logic and don't use strange annotations like \"async\". In other words, if async/await is so good, why not make all functions in the language implicitly async, and in stead of \"await\" just use normal function calls? Then you will suddenly be programming in threads. Some programmers do, but many want exactly the opposite as well. Most of the time I don't care if it's an OS blocking syscall or a non-blocking one, but I do care about understanding the control flow of the program I'm reading and see where there's waiting time and how to make them run concurrently. In fact, I'd kill to have a blocking/block keyword pair whenever I'm working with blocking functions, because they can surreptitiously slow down everything without you paying attention (I can't count how many pieces of software I've seen with blocking syscalls in the UI thread, leading to frustratingly slow apps!). reply avodonosov 1 hour agorootparentBut all functions are blocking. fn foo() {bar(1, 2);} fn bar(a, b) {return a + b;} Here bar is a blocking function. reply littlestymaar 45 minutes agorootparentNo they aren't, and that's exactly my point. Most functions aren't doing any syscall at all, and as such they aren't either blocking or non-blocking. Now because of path dependency and because we've been using blocking functions like regular functions, we're accustomed to think that blocking is “normal”, but that's actually a source of bugs as I mentioned before. In reality, async functions are more “normal” than regular functions: they don't do anything fancy, they just return a value when you call them, and what they return is a future/promise. In fact you don't even need to use any async anotation for a function to be async in Rust, this is an async function: fn toto() -> impl Future> { unimplemented!(); } The async keyword exists simply so that the compiler knows it has to desugar the await inside the function into a state machine. But since Rust has async blocks it doesn't even need async on functions at all, the information you need comes from the type of the return value, that is a future. Blocking functions, on the contrary, are utterly bizarre. In fact, you cannot make one yourself, you must either call another blocking function[1] or do a system call on your own using inline assembly. Blocking functions are the anomaly, but many people miss that because they've lived with them long enough to accept them as normal. [1] because blockingness is contagious, unlike asynchronousness which must be propagated manually, yes ironically people criticizing async/await get this one backward too reply immibis 2 minutes agorootparentbar blocks waiting for the CPU to add the numbers. adontz 4 hours agoprevThere are a lot of moments not covered. For example: - async/await runs in context of one thread, so there is no need for locks or synchronization. Unless one runs async/await in multiple threads to actually utilize CPU cores, then locks and synchronization are necessary again. This complexity may be hidden in some external code. For example instead of synchronizing access to a single database connection it is much easier to open one database connection per async task. However such approach may affect performance, especially with sqlite and postgres. - error propagation in async/await is not obvious. Especially when one tries to group up async tasks. Happy eyeballs are a classic example. - since network I/O was mentioned, backpressure should also be mentioned. CPython implementation of async/await notoriously lacks network backpressure causing some problems. reply graphenus 3 hours agoparentAsync/await just like threads is a concurrency mechanism and also always requires locks when accessing the shared memory. Where does your statement come from? reply conradludgate 3 hours agorootparentIf you perform single threaded async in Rust, you can drop down to the cheap single threaded RefCell rather than the expensive multithreaded Mutex/RwLock reply romanovcode 3 hours agorootparentprev> Where does your statement come from? This is how async/await works in Node (which is single-threaded) so most developers think this is how it works in every technology. reply bheadmaster 3 hours agorootparentEven in Node, if you perform asynchronous operations on a shared resource, you need synchronization mechanisms to prevent interleaving of async functions. There has been more than one occasion when I \"fixed\" a system in NodeJS just by wrapping some complex async function up in a mutex. reply nurple 2 hours agorootparentThis lacks quite a bit of nuance. In node you are guaranteed that synchronous code between two awaits will run to completion before another task(that could access your state) from the event loop gets a turn; with multi-threaded concurrency you could be preempted between any two machine instructions. So while you _do_ have to serialize access to shared IO resources, you do _not_ have to serialize access to memory(just add the connection to the hashset, no locks). What you usually see with JS for concurrency of shared IO resources in practice is that they are \"owned\" by the closure of a flow of async execution and rarely available to other flows. This architecture often obviates the need to lock on the shared resource at all as the natural serialization orchestrated by the string of state machines already naturally accomplishes this. This pattern was even quite common in the CPS style before async/await. For example, one of the first things an app needs do before talking to a DB is to get a connection which is often retrieved by pulling from a pool; acquiring the reservation requires no lock, and by virtue of the connection being exclusively closed over in the async query code, it also needs no locking. When the query is done, the connection can be replaced to the pool sans locking. The place where I found synchronization most useful was in acquiring resources that are unavailable. Interestingly, an async flow waiting on a signal for a shared resource resembles a channel in golang in how it shifts the state and execution to the other flow when a pooled resource is available. All this to say, yeah I'm one of the huge fans of node that finds rust's take on default concurrency painfully over complicated. I really wish there was an event-loop async/await that was able to eschew most of the sync, send, lifetime insanity. While I am very comfortable with locks-required multithreaded concurrency as well, I honestly find little use for it and would much prefer to scale by process than thread to preserve the simplicity of single-threaded IO-bound concurrency. reply bheadmaster 56 minutes agorootparent> So while you _do_ have to serialize access to shared IO resources, you do _not_ have to serialize access to memory Yes, in Node you don't get the usual data races like in C++, but data-structure races can be just as dangerous. E.g. modifying the same array/object from two different async coroutines was a common source of bugs in the systems I've referred to. Of course, you can always rely on your code being synchronous and thus not needing a lock, but if you're doing anything asynchronous and you want a guarantee that your data will not be mutated from another async function, you need a lock, just like in ordinary threads. One thing I deeply dislike about Node is how it convinces programmers that async/await is special, different from threading, and doesn't need any synchronisation mechanisms because of some Node-specific implementation details. This is fundamentally wrong and teaches wrong practices when it comes to concurrency. reply dehrmann 3 hours agoparentprevasync can be scarier for locks since a block of code might depend on having exclusive access, and since there wasn't an await, it got it. Once you add an await in the middle, the code breaks. Threading at least makes you codify what actually needs exclusive access. async also signs you up for managing your own thread scheduling. If you have a lot of IO and short CPU-bound code, this can be OK. If you have (or occasionally have) CPU-bound code, you'll find yourself playing scheduler. reply cageface 2 hours agorootparentYeah once your app gets to be sufficiently complex you will find yourself needing mutexes after all. Async/await makes the easy parts of concurrency easy but the hard parts are still hard. reply bsder 3 hours agoparentprevI have lots of issues with async/await, but this is my primary beef with async/await: Remember the Gang of Four book \"Design Patterns\"? It was basically a cookbook on how to work around the deficiencies of (mostly) C++. Yet everybody applied those patterns inside languages that didn't have those deficiencies. Rust can run multiple threads just fine--it's not Javascript. As such, it didn't have to use async/await. It could have tried any of a bunch of different solutions. Rust is a systems language, after all. However, async/await was necessary in order to shove Rust down the throats of the Javascript programmers who didn't know anything else. Quoting without.boats: https://without.boats/blog/why-async-rust/ > I drove at async/await with the diligent fervor of the assumption that Rust’s survival depended on this feature. Whether async/await was even a good fit for Rust technically was of no consequence. Javascript programmers were used to async/await so Rust was going to have async/await so Rust could be jammed down the throats of the Javascript network services programmers--technical consequences be damned. reply tsimionescu 2 hours agorootparentAsync/await was invented for C#, another multithreaded language. It was not designed to work around a lack of true parallelism. It is instead designed to make it easier to interact with async IO without having to resort to manually managed thread pools. It basically codifies at the language level a very common pattern for writing concurrent code. It is true though that async/await has a significant advantage compared to fibers that is related to single threaded code: it makes it very easy to add good concurrency support on a single thread, especially in languages which support both. In C#, it was particularly useful for executing concurrent operations from the single GUI thread of WPF or WinForms, or from parts of the app which interact with COM. This used the SingleThreadedExecutor, which schedules tasks on the current thread, so it's safe to run GUI updates or COM interactions from a Task, while also using any other async/await code, since tasks inherit their executor. reply guappa 2 minutes agorootparentprevThreads are much much slower than async/await. reply yxhuvud 2 hours agorootparentprevI generally don't agree with the direction withoutboats went with asynchricity but you are reading in a whole lot more into that sentence than is really there. It is very clear (based on his writing, in this and other articles) that he went with the solution because he thinks it is the right one, on a technical level. I don't agree, but making it sound like it was about marketing the language to JavaScript people is just wrong. reply seabrookmx 3 hours agorootparentprevThreads have a cost. Context switching between them at the kernel level has a cost. There are some workloads that gain performance by multiplexing requests on a thread. Java virtual threads, golang goroutines, and dotnet async/await (which is multi threaded like Rust+tokio) all moved this way for _performance_ reasons not for ergonomic or political ones. It's also worth pointing out that async/await was not originally a JavaScript thing. It's in many languages now but was first introduced in C#. So by your logic Rust introduced it so it could be \"jammed down the throats\" of all the dotnet devs.. reply lelanthran 2 hours agorootparent> So by your logic Rust introduced it so it could be \"jammed down the throats\" of all the dotnet devs.. You're missing his point. His point is that the most popular language, which has the most number of programmers forced the hand of Rust devs. His point is not that the first language had this feature, it's that the most programmers used this feature, and that was due to the most popular programming language having this feature. reply tsimionescu 2 hours agorootparentThat Rust needed async/await to be palatable to JS devs would only be a problem if we think async/await is not needed in Rust, because it is only useful to work around limitations of JS (single-threaded execution, in this case). If instead async/await is a good feature in its own right (even if not critical), then JS forcing Rust's hand would be at best an annoyance. And the idea that async/await was only added to JS to work around its limitations is simply wrong. So the OP is overall wrong: async/await is not an example of someone taking something that only makes sense in one language and using it another language for familiarity. reply lelanthran 1 hour agorootparent> So the OP is overall wrong: async/await is not an example of someone taking something that only makes sense in one language and using it another language for familiarity. I don't really understand the counter argument here. My reading of the argument[1] is that \"Popularity amongst developers forced Rust devs hands in adding async\". If this is the argument, then a counter argument of \"It never (or only) made sense in the popular language (either)\" is a non-sequitor. IOW, if it wasn't added due to technical reasons (which is the original argument, IIRC), then explaining technical reasons for/against isn't a counter argument. [1] i.e. Maybe I am reading it wrong? reply bsder 1 hour agorootparentYou are not reading it wrong, and your statements are accurate. My broader point is that the possibility of there being a \"technically better\" construct was simply not in scope for Rust. In order for Rust to capture Javascript programmers, async/await was the only construct that could possibly be considered. And, to be fair, it worked. Rust's growth has been almost completely on the back of network services programming. reply bsder 1 hour agorootparentprev> all moved this way for _performance_ reasons They did NOT. Async performance is quite often (I would even go so far as to say \"generally\") worse than single threaded performance in both latency AND throughput under most loads that programmers ever see. Most of the complications of async are much like C#: 1) Async allows a more ergonomic way to deal with a prima donna GUI that must be the main thread and that you must not block. This has nothing to do with \"performance\"--it is a limitation of the GUI toolkit/Javascript VM/etc.. 2) Async adds unavoidable latency overhead and everybody hits this issue. 3) Async nominally allows throughput scaling. Most programmers never gain enough throughput to offset the lost latency performance. reply OtomotO 3 hours agorootparentprevI would damn this, if Async/Await wasn't a good enough (TM) solution for certain problems where Threads are NOT good enough. Remember: there is a reason why Async/Await was created B E F O R E JavaScript was used for more than sprinkling a few fancy effects on some otherwise static webpages reply Karrot_Kream 2 hours agorootparentprevasync/await is just a different concurrency paradigm with different strengths and weaknesses than threads. Rust has support for threaded concurrency as well though the ecosystem for it is a lot less mature. reply junon 1 hour agoparentprev> async/await runs in context of one thread, Not in Rust. reply paulsutter 14 minutes agoprevIf you are doing anything complex you really want to use a polling loop. That’s how video games, avionics systems, industrial equipment, etc is programmed reply guappa 3 minutes agoparentThat's what async/await abstracts internally. reply jayd16 2 hours agoprevAnother discussion where people don't get async/await, can't fathom why you would want a concurrency mechanism on a single thread and assume no one needs it. UI programming, communication with the GPU, and cross runtime communication are good examples but I'm sure there are more. Threads, green or otherwise, don't work for those cases but async/await does. reply gudzpoz 2 hours agoprevI think the author is confusing two things here: 1. User-space threads / Green threads 2. Structured concurrency The former one is an advantage of async/await, but is not unique to it (see Go or Java Loom for examples that involves no function coloring problem). And the latter one can be implemented with both OS threads and green threads (see Structured concurrency JEPS for Java [1]). [1] https://openjdk.org/jeps/462 reply ggm 3 hours agoprevAs one who struggled with thread safe storage since threads started, the amount of code we carry around with us which looks ok, but turns out not to be viable in threads is remarkably high. I bump into this in C, Python3 quite a lot: you have to work harder to do anything which is not synchronous, no matter how you arrive in that place. For long lived work, it is not impossible there is no advantage overall to forking a lot of heavyweight processes which operate fast as a single execution state. If you have the cores, the CPU and the memory. Context switching delay is very possibly not your main problem. For example, I have typically 24 1 hour capture/log files, each 300m+ lines big and I need to ungzip them, parse/grep out some stuff, calculate some aggregates and then recombine. Its a natural pipeline of 3 or 4 processes. The effort to code this into a single language and uplift it to run threads inside, where it's basically 24 forked pipe sequences begs questions: What exactly is going to get faster, when I am probably resource limited by the gunzip process at the front? You think you can code faster than a grep DFA to select inputs? How sure are you that memory structure is faster than awk? Really sure? I tested some. radix tries are nice and have that log(n) length thing, but AWK was as fast. (IP address \"hashing\" lookup costs) (hint: more than one interpreted language simply forks gzip for you, to unzip an input stream) If you can go to C, then it's possible forking heavyweight processes in C, with good IPC buffer selections or mmap is \"good enough\" reply pjmlp 2 hours agoparentAs someone that was big into threading back in the 2000, with the experience gathered through times, I think with modern hardware resources we are better off with OS IPC, as it offers much better safety guarantees, specially in C like languages. reply KaiserPro 1 hour agoprevI think its partly down to what you grew up with. If you're of the JS (or python 3) generation then you're _probably_ more comfortable with async. However there are things where async seems to be more of a semantic fit for what youre doing. for example FastAPI is all async, and it makes sense. I started using it, because it scaled better than anything else. They have don't a nice job of making the interface as painless as possible. It almost doesn't feel like surprise goto. I do a lot of stream processing, so for me threads is a better fit. It scales well enough, and should I need to either go to multiprocessing (not great) or duplicate to a new stand-alone process, its fairly simple (Keeping everything message based also helps.) async/threads is _almost_ like shop bought coke vs sodastream. They are mostly the same, but have slightly different semantics. reply ardies 32 minutes agoprevJust tried changing my python code to use AA instead of the 2 threads I use and it just complicated the code more. I had to create a thread anyway to deal with the tkinter event loop issue. And then I still have to check inside the functions if the task is still active. But the worst part is converting all the stack of functions to to be async/await. reply anacrolix 2 hours agoprevasync/await is syntax. Most of what people associate with it is actually the benefit of virtual threading. Asynchrony is how virtual threading is achieved in user space. Rust has direct native threading by default and can't do virtual threading without a runtime. Async/await makes the use of a runtime explicit. It could be possible to have both virtual and OS threading without special syntax but it would require a marker trait, algebraic effects or monads. Without those things you either need to choose user or OS level threads by default. If user level threads are the default, you need a runtime by default. (Even if that runtime is only for concurrency.). Go made that the default. Again, you don't need async/await to have virtual threading. reply tsimionescu 1 hour agoparentAsync/await is syntax, but it's not equivalent to virtual threads. It is special syntax for a certain pattern of writing concurrent (and possibly parallel) code: one in which you launch a concurrent operation specifically to get back one result. When you start an OS thread or a virtual thread, that thread can do anything. When you launch a task, it can only do one thing: return one result of the type you asked for. Async/await is perfect for operations that map well onto this structure. For example, most IO reads and writes fit well into this model - you trigger the IO operation to get back a specific result, then do some other work while it's being prepared, and when you need the result, you block until it's available. Other common concurrent operations don't map well on this at all - for example, if you want to monitor a resource to do something every time its state changes, then this is not well modeled at all as a single operation with a unique result, and a virtual or real thread would be a much better option. Also, having both virtual and OS threads doesn't need any special syntax. You just need two different functions for creating a thread - StartOSThread(func) and StartVirtualThread(func), and a similar split for other functions that directly interact with threads (join, cancel, etc), and a function for telling whether you are currently running in a virtual thread. Everything else stays the same. This is what Java is doing with Project Loom, I'm not speaking just in principle. The huge difficulty with virtual threads is implementing all blocking operations (IO, synchronization primitives, waits, etc) such that they use async OS primitives and yield to the virtual thread scheduler, instead of actually blocking the OS thread running the virtual thread. reply OtomotO 3 hours agoprev> \"Smart programmers try to avoid complexity. So, they see the extra complexity in async/await and question why it is needed. This question is especially pertinent when considering that a reasonable alternative exists in OS threads.\" Hm, yes, but is that complexity really avoided if it's in the language/runtime? Sure, it's not in your code and it will probably have way more extensive testing and maybe also people that thought about the problem for months and you're just trying to implement Business feature #31514 and not reinventing the wheel, but as someone who has been bitten by specific implementations like the one in Quarkus (not a language, granted), I must say: Complexity is there to stay (and occasionally bite you), even if it's hidden! reply delusional 3 hours agoparentI tend to agree, but an important note is that I've never seen an async/await system that didn't ALSO interact with OS threads. Async await is not an alternative to OS threads but rather an additional layer. reply OtomotO 2 hours agorootparentI have heard about them in embedded systems where there are no OS threads, because there is no OS, but there is Async tasks and a scheduler reply IshKebab 2 hours agorootparentprevYou mean like the async/await systems in the two most popular languages in the world - JavaScript and Python? Async/await is definitely an alternative to user space threads, especially for IO bound tasks. reply grumpyprole 2 hours agoparentprev> Hm, yes, but is that complexity really avoided if it's in the language/runtime? Complexity can be abstracted away and this is still a worthwhile goal. Otherwise, we'd all still be studying Intel CPU documentation. reply OtomotO 2 hours agorootparentIt can and it should, but I disagree with the sentiment that when it's abstracted \"away\" it's not there anymore. You just don't see it. Abstraction doesn't change any fact about how the system actually works. reply stevefan1999 3 hours agoprevBecause you want to avoid context switching cost. In fact, async/await/promise/future/Task is also closely related to fiber as opposed to thread at least in Windows, but fiber is provided from an OS level while async/promise/future/Task is provided by the language itself. You can switch to a different fiber without doing a context switch, just like how you use a state machine to switch to different job using async/promise/future/Task reply mrkeen 2 hours agoprev> A common refrain is that threads can do everything that async/await can, but simpler. > OS threads don’t require any changes to the programming model, which makes it very easy to express concurrency. I think these claims need a bit of justification, or else why write the article? reply hsjsbeebue 4 hours agoprevWhat is the language agnostic answer to the same question? I imagine something to do with memory usage or avoiding thread or thread pool starvation issues. Maybe performance too? reply toast0 3 hours agoparentI don't have a lot of experience with async/await at high numbers of tasks, but I've run Erlang with millions of processes. It's a lot easier to run millions of Erlang processes on one machine than to run a million OS threads. I suspect async tasks would be similar; an OS thread needs its own stack, and that's going to use at least a page of memory, but often much more. Otoh, an async task or green thread might be able to use less. If you're running real OS threads, I think task switching is going to be real context switches, which might mean spectre mitigations clear your cpu caches, but task switching can avoid that. You may end up with more system calls with OS threads, because your runtime might be able to aggregate things a bit (blocking reads become kqueue/epoll/select, but maybe that's actually a wash, because you do still need a read call when the FD is ready, and real blocking only makes a single call) reply lmm 3 hours agoparentprevIMO the biggest reason to avoid threads is simply that it's ~impossible to write safe code using threads (e.g. without race conditions). Arguably with Rust's ownership system that's less true there than in other languages. reply treflop 2 hours agorootparentAsynchronous code has race conditions and synchronization issues too. I pray for all the code written by people who think they didn’t need to learn about synchronization because they wrote asynchronous code. And unfortunately I’ve come across and had to fix asynchronous code with race conditions. You cannot escape learning about synchronization. Writing race-condition-free code is not hard. What is actually hard is writing fast lock-free routines, but that’s more a parallelism problem that affects both threaded and asynchronous code. And most people will never need to reach that level of code optimization for their work. reply lelanthran 2 hours agorootparentprev> IMO the biggest reason to avoid threads is simply that it's ~impossible to write safe code using threads (e.g. without race conditions). Javascript has race conditions too, even with no threads involved. reply logicchains 2 hours agorootparentprevYou can write safe code using threads if you enforce that the only way threads can communicate is by sending messages to each other (via copying, not pointers). This is what Erlang does. reply kaba0 3 hours agorootparentprevAsync-await is about concurrency, not parallelism. It can work in both a single-threaded and multi-threaded context, the latter exposing all the typical failure modes of multi-threaded code. Also, Rust’s ownership model only prevents data races, that’s only the tip of the iceberg of race conditions, and I don’t think that any general model makes it possible to statically determine that any given multithreaded code is safe. Nonetheless, that’s the only way to speed up most kind of code, so possibly the benefits outweigh the cost in many cases. reply feverzsj 3 hours agoprevRust async/await is as bad as C++ coroutines. They lack one of the most important language counterparts: async drop/destructor. reply OtomotO 3 hours agoparentThat's the main gripe imo: Missing features that are not optional for certain problems where the solution would otherwise make perfect sense. I get the pragmatism to \"better ship something that is 80% finished now, than wait for it to be 100% finished in some years\", but with Rust's async/await it was released in 2018 and the more time passes, the more it looks like some sharp edges are here to stay. reply ikekkdcjkfke 3 hours agoprevWhat i would hope for is implicit async/await so i don't have to write it out / wrap all the time reply t-writescode 47 minutes agoparentIn Kotlin, only the outer-most wrapping 'suspend' function swap needs to be wrapped; and when you're writing web servers and such, that's handled in the server middleware and so all your code can be `suspend fun blahblah()` reply yodon 3 hours agoparentprev>implicit async/await I keep suspecting C# will be the place where we see this, but probably not for another couple years. reply asabla 1 hour agorootparentWe might not be that far away already. There is this issue[1] on Github, where Microsoft and the community discuss some significant changes. There is still a lot of questions unanswered, but initial tests look promising. Ref: https://github.com/dotnet/runtime/issues/94620 reply groestl 3 hours agoparentprevThat's eerily similar to what (green) threads are. reply hinkley 2 hours agorootparentI’ve always thought async await is just a less clunky implementation of green threads. Not as a feature parity replacement, but trying to fill the same niche. reply globular-toast 1 hour agoprev> I recall that the thing that made me stick with Rust is the Iterator trait. It blew my mind that you could make something an Iterator, apply a handful of different combinators, then pass the resulting Iterator into any function that took an Iterator. What languages did the author use before? Is this any different from the interface pattern seen in many other languages? reply darthrupert 2 hours agoprevMy possibly unpopular opinion is that async/await is a mistake as a first class programming construct. That functionality should 100% be in libraries for two reasons: 1) that way it won't infect the actual language in any way and 2) it will be more difficult to use so people will only reach for it if they really need it. Sync code and threads is the way to go for 99% of the cases where concurrency is needed. Rust handles most of the footguns of that combination anyway. reply nurple 2 hours agoparentCouldn't disagree more. In my experience, single-threaded event-loop driven async/await should be used for every possible concurrency need, with the complexity of multi-threaded concurrency being reserved for the rare cases it's needed. As auto-scaled services and FaaS began to become popular, I've found most any need for multithreaded programming almost wholly unnecessary. reply zokier 52 minutes agorootparentThe problem is that in Rust the main way of doing async/await (Tokio) is multithreaded by default. So you get all the problems of multithreading, and additionally all the problems with async/await. The discourse would be very different if the default choice of executor would have been thread-local. reply spacechild1 25 minutes agorootparentprevNot everything is a web backend... CPU bound workloads may be rare in your domain, but I would be careful with generalizations. reply t-writescode 45 minutes agorootparentprevI'm actually more of a fan of the actor model than the single-threaded event-loop. Individual blobs of guaranteed synchronicity with message passing. reply nottorp 53 minutes agoprevNo offense but you don't serve 1 million clients with 1 thread per client. So what this ends up as is describing the mechanism that Rust uses to hide the state machine you'd do by hand in an older language. reply plugin-baby 3 hours agoprevis the difference between threads and async/await more than syntax? or language-specific? reply jokethrowaway 1 hour agoparentAwait-async is implemented with a runtime which uses a thread pool or a single thread and allocates work when needed on any thread and waits for IO to yield a result. With threads you just fully control what blocking code is running on a single thread. If you are just running computations (or reading files, as filesystem api are not async) it's simpler to just use threads. reply romanovcode 3 hours agoparentprevYes, they are different in most cases. In general await job is to pass the process to 3rd party e.g. database or http and wait for callback whereas thread job is to launch multiple CPU operations in parallel. reply coolThingsFirst 3 hours agoprevHow is async/await implemented under the hood? reply lmm 3 hours agoparentGenerally via continuations. An async function is transformed to continuation-passing style, await calls with the current continuation, and then you have a runtime that at its simplest is just, like, a queue of tasks and has special-cased primitives for doing things like async I/O where you suspend, and it just pulls tasks off the queue and runs them, and when one task suspends it stores the continuation and runs the next one. reply politelemon 2 hours agorootparentDoes that runtime run the tasks across multiple cores? reply amaranth 56 minutes agorootparentIn Rust the answer is \"it depends\". Since the runtime is not provided by the language you can have implementations that are a single thread, thread-per-task, a thread pool, or whatever other setup you can think of. Tokio at least offers a single threaded version and a thread pool version. reply LtWorf 3 hours agoparentprevpoll() reply andrewstuart 4 hours agoprev [–] >> A common refrain is that threads can do everything that async/await can, but simpler. Who says that? Threads and async/await are different things, and it doesn’t make sense to say one can do what the other does. And threads definitely are not simpler than AA reply OtomotO 3 hours agoparent [–] Threads are easier to spawn and that's where all the fuzz comes from, I argue. Especially in Rust, Async isn't as easy as in other languages with a runtime and it does indeed have some caveats (e.g. cancellation), but all the real fuzz comes from not understanding that they are different strategies for some similar but not the same problems. It makes little to no sense to use Async/Await for number crunching/CPU-compute intensive tasks, for example. One can use Threads for some IO waiting though, but it's definitely not the best solution for that particular problem. To me this whole discussion has two facets: 1) How can Async/Await be more ergonomic in Rust 2) How can we teach people that Async/Await is a different solution with different tradeoffs to Threads - there is a reason why Async/Await was created AFTER WE ALREADY HAD INVENTED THREADS! reply nurple 2 hours agorootparentMy problem with rust's async/await is that it's _not_ a different strategy as the continuation tasks _are_ run multithreaded, so it's technically both strategies. IMO one of the biggest selling points of single-threaded async/await was how much complexity falls away compared to managing preemptive synchronization in the multi-threaded case. I can see why there's so much controversy over async/await in rust. If I had to take both the syntax and cognitive hit of using async/await _and_ multi-threading, I would also angrily call for its removal. reply dboreham 3 hours agorootparentprev [–] Async wasn't invented after threads. It was primarily popularized in a system that has been designed such that it couldn't use threads (the web browser). Everything else is post-justifcation for why it's better. It isn't better. reply OtomotO 2 hours agorootparent [–] Async/Await as a syntax thing wasn't, but Async/Await as \"don't just blindly use Threads for scaling to a myriad of incoming HTTP requests\" was. I remember the Apache webserver story and that one has little to nothing to do with Webbrowsers or JavaScript ;) reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The article compares async/await and threads in Rust programming, noting async/await complexity but necessity for handling concurrent tasks, especially in web servers.",
      "Emphasizes async/await benefits in Rust like efficiency, composability, and streamlining complex operations, challenging the belief that threads are superior.",
      "The author stresses the need for improved awareness on async advantages, positioning async/await as a valuable tool for enhancing performance in Rust concurrency."
    ],
    "commentSummary": [
      "The text debates using async/await versus threads or fibers for concurrency in Rust programming, praising async/await for explicit wait-points and future dropping capabilities.",
      "It highlights the challenges of user space cooperative concurrency, fiber design, and calling C code from a fiber, emphasizing trade-offs and complexities in different programming contexts.",
      "Topics covered include cancellation signals, low-latency needs, microcontroller applications, and comparing async/await with fibers across programming languages."
    ],
    "points": 198,
    "commentCount": 120,
    "retryCount": 0,
    "time": 1711342584
  },
  {
    "id": 39807967,
    "title": "Teen's $800k Trading: Ethical Questions Arise",
    "originLink": "http://www.kentlaw.edu/faculty/rwarner/classes/legalaspects_ukraine/securities/case_studies/ledbed.htm",
    "originBody": "Case Study: What Happens When a Fifteen Year Old Pumps and Dumps, Obtaining a Net Profit of Nearly $800,000? The SEC Settles for $285,000. Jonathan Lebed and two of his friends came in fourth place in a stock-picking contest sponsored by CNBC when he was just twelve years old. They had managed to turn an imaginary $10,000 into $240,000 during the first round of the competition. From there, Lebed was able to convince his parents to invest money in the stock market on his behalf. His parents first opened an Ameritrade account for him, and upon his mother’s decision to close that account, his father opened an E*Trade account for him. During two years of trading, Lebed made a minimum of 27 trades, and netted nearly $800,000. The SEC later investigated these 27 trades and eventually declared eleven of them as illegal trades and forced to Lebed to pay approximately $272,000 in fines and over $12,000 in interest. In particular, these eleven trades occurred during the five and a half month period between August 23, 1999, and February 4, 2000. While Lebed never admitted or denied the findings by the SEC, he consented to entry of the findings and the imposition of sanctions.[1] The SEC findings asserted that Lebed would purchase large blocks of thinly traded microcap stocks, post between 200 and 300 messages on the Yahoo! Finance message boards, hyping the stocks he had just purchased, and then sell all of the stocks he had purchased, usually within twenty-four hours of said purchase. The messages that Lebed posted would predict marked increases in the stocks value, i.e. he would post comments that this stock would be the “next stock to gain 1,000%” or that a stock currently trading at just over a dollar would be trading at more than $20 per share “very soon.” In response to Lebed’s messages, trading volume of these stocks would soar on average from 60,000 shares per day to over a million shares on the days he posted his messages. For the eleven trades that the SEC declared its findings, Lebed realized a net profit of $272,826. The individual gross profits ranged from over $11,000 per trade to nearly $74,000. Lebed professes that he did nothing wrong – he did nothing different from Wall Street analysts. He states that he learned how people react to the stock market and acted on that knowledge. According to Lebed’s friends, even Lebed’s teachers would follow his advice regarding the stock market. While one of Lebed’s friends lost a significant amount of money on one of the trades Lebed pushed, he states “[i]n the stock market, you go in knowing you can lose. We were just doing what Jon was doing, but not doing as good a job at it.”[2] Depending on who you talk to, Lebed was either viewed as a person who knowingly abused the system and broke the law, or someone who acted no differently than Wall Street analysts acted and was able to use the system to his advantage, actually performing no wrong-doing. Lebed’s own father stated on 60 Minutes that he was proud of his son and that he hadn’t done anything wrong. This belief can also be evinced by the fact that after Lebed’s first meeting with the SEC, while his mother closed the Ameritrade account, Lebed’s father opened an E*Trade account so that Lebed could continue to trade stocks. The SEC, however, stated that Lebed violated Section 17(a) of the Securities Act and Section 10(b) and Rule 10b-5 of the Exchange Act. These Acts “prohibit acts, transactions, practices or course of business that operate as a fraud or deceit in connection with the offer, purchase or sale of securities, including misrepresentation and omissions of material fact.”[3] The SEC therefore ordered Lebed pay his fine and to “cease and desist” from causing any further violation of Section 17(a). Even the news articles that covered Lebed’s actions appear split on whether he had done anything wrong. While an article written by Peter Carbonara for Money Magazine[4] appears to favor the SEC decision (albeit wondering why the SEC let Lebed keep the remaining $500,000 of profit), Michael Lewis, writing for New York Times Magazine,[5] acknowledges that there were victims who suffered from Lebed’s actions, yet asserts that the SEC only settled because they believed they would not be able to win in court, and that its evidence was not as strong as it had alleged. [1] In Re: Lebed, No. 3-10291, 2000 SEC LEXIS 1964, at *1 (Sept. 20, 2000). [2] Michael Lewis, He Wanted to Get Rich. He Wanted to Tune Out his School-Kid Life. And Neither His Parents Nor the S.E.C. was in a Position to Stop Him: Jonathan Lebed’s Extracurricular Activities, New York Times Magazine, February 25, 2001. [3] Id. at *6. [4] March 2001, available at http://www62.homepage.villanova.edu/john.matthews/conkid.html [5] February 25, 2001, available at http://www62.homepage.villanova.edu/john.matthews/conkid.html",
    "commentLink": "https://news.ycombinator.com/item?id=39807967",
    "commentBody": "What Happens When a Fifteen Year Old Pumps and Dumps with a Net Profit of $800k? (2002) (kentlaw.edu)170 points by melenaboija 18 hours agohidepastfavorite156 comments ch33zer 16 hours agoThere was a recent decision out of Texas that pump and dumps are legal if you do them without directly selling stock to the victims. So I guess that according to Texas what this guy did is totally fine! [PDF] https://assets.bwbx.io/documents/users/iqjWHBFdfxIU/rmMOgPQg... or coverage from Matt Levine: https://www.bloomberg.com/opinion/articles/2024-03-21/pump-a... reply bradleybuda 13 hours agoparentThe judge in that case simply does not know what they are talking about. This will be swiftly overturned on appeal. reply giarc 15 hours agoparentprevWouldn't that mean that you can have a friend do the pump, and you do the dump? Sounds like the guy in the original article did both and therefore wouldn't be saved by that Texas ruling. reply kasey_junk 14 hours agorootparentThe Texas ruling doesn’t (as far as the coverage I’ve read) require that level of separation. Rather just selling to “the market” is enough to insulate you from fraud charges. reply malfist 10 hours agorootparentDo people actually direct sell stock? At least retail investors reply pcthrowaway 6 hours agorootparentMaybe I'm wrong but I think people can use OTC trades for this (though OTC trading desks typically wouldn't share counterparty information IIUC) reply kasey_junk 7 hours agorootparentprevNo. Even those that hold directly via a transfer agent don’t directly sell. If that court case holds there is no such thing as securities fraud. reply DoodahMan 9 hours agoparentprevi'll admit i was a little shocked when the case was dismissed. for anyone not familiar with Atlas Trading this page gives a bit of an overview: https://bullishbears.com/atlas-trading-lawsuit/ reply hilux 17 hours agoprevInteresting. A little googling turns up that 25 years later, he's still in the pump-and-dump business! Do what you love, I guess. reply ProjectArcturis 16 hours agoparentIf you made half a million bucks doing something in high school, it'd be pretty hard to go the college route and hope to eventually get a job grinding $100k a year. reply adventured 15 hours agorootparentIf you're good at a thing, you can easily make a lot more than that on Wall Street. $500k is just your bonus. reply cmcaleer 17 hours agoparentprevI found this Twitter account last posted to in 2014. https://twitter.com/lebedbiz I checked a good few of the stocks he listed. I figured he would have struck a lucky score at one point but he seemed to have an uncanny ability to always pick losing stocks. I was hoping for a happy ending like how many of the people I played Runescape with who hosted dicing games, speculated and flipped in-game assets became successful quants irl, but he sure did seem to just pick a gimmick and roll with it, never evolving or changing. reply jldugger 15 hours agorootparentNot sure how it came to be that every article publishes this kids name, but I'm starting to find a tragic arc to this guy's life. Dead twitter account, no LinkedIn account, and apparently was booked for narcotics possession in 2016. All I found was an arrest record[1]. Maybe he was just on some bad meds and the police caught him before things got worse, maybe they made the whole thing up, but many citations from his Wikipedia page(!) 404 now, and I have my theories as to why. [1]: https://www.northjersey.com/story/news/passaic/wayne/2016/03... reply seadan83 15 hours agorootparentprevNot a surprise it is loser stocks. The pump and dumper wants cheap, low volume stocks, preferably those with a plausible turnaround story. The pump and dumper does not disclose holdings or the prospect for them to acquire or sell. Hard to then know if the shilling was unsuccessful or not. reply Solvitieg 16 hours agorootparentprevIs it safe to assume they transitioned to become an anonymous crypto trader around this time? reply bayouborne 11 hours agorootparentprevSep 3, 2012 I Am Legend. reply shnock 16 hours agorootparentprevReally? Merching in RS3 always seemed to be more of a pump and dump than anything that overlapped with real life quantitative investing reply Frummy 15 hours agorootparentIf theyre working as quants now, they were probably around before RS3 as well. I remember daytrading the grand exchange personally. There were plenty of profitable patterns that were basically guaranteed to work. I can see how someone who could program at the time could get much higher volumes by automating multiple accounts. The problem was finding profitable new trades at different levels of wealth, what worked at one volume would have to be replaced eventually. I never got into clans and such. I got scammed a few times especially memorable was a scam with a sort of party of social proof around the scammer and they would rinse repeat in every world, some things were so organised that you got really surprised by the intricacy of the scams. reply mewpmewp2 9 hours agorootparentRuneScape really taught about scams, huh. reply cmcaleer 9 hours agorootparentprevIf you were one of the rubes falling for Smoking Mils sure. But there was always plenty of money to be made from placing stink orders for high volume items though, to name one very basic strategy. When you have a big enough quantity for an in-demand, relatively unvolatile asset like prayer pots you can essentially just have orders in where you get paid by the spread from bigger orders and be the main market maker. There wasn't exactly a CLOB but it was pretty easy to figure out the spread by buying 100 then selling it back for max/min price. Other players would treat the GE like an NPC where they'd dump items in mass at 1gp just to get it all sold - knowing and taking advantage of this was (and prob still is) free money. There were some very fun strategies where like, you would submit a question for the QnA proposing some use for $item around high alch price (e.g. 'the dark bow used to be such an iconic weapon but now it's useless and feels pathetic selling for 70k. with the new boss coming out, could we see a new use for these? :((') and when it got read out and if they liked the idea, speculators would buy these in bulk and especially if they weren't a very liquid item you would cash in, with minimal downside if you don't get read. Jmods caught on to this though, lol. I think the natural scepticism it teaches makes for a good quant too. I remember seeing a Twitter thread like, 'If I have a coin and show you the two sides, then flip 19 heads in a row, what price should you accept to bet that it would be heads again?' with the options all being well >50%. The comments were all like, 'Well based on Laplace's rule of succession...', '...based on a beta distribution with a uniform prior...', etc., all theoretically correct stuff if we live in a mathematically perfect world. The Runescape-brained would give you the real-life most likely answer: 'That coin is rigged, dude. I'm going to find better value elsewhere.' reply mauvehaus 16 hours agoprevHe can afford college? In seriousness though: are analysts allowed to have an interest in stocks they're publishing analysis for? It seems like a fine line between creating an obvious conflict of interest (see: TFA perhaps on a more subtle level) and wanting them to have skin in the game so they're incentivized to provide useful analysis (rather than YOLO: stock goes up/down and I don't care because it's other people's money on the line). In general, it seems like a hard problem to compensate an analyst without either creating one of the above scenarios or generally setting up a scenario where they'd be better off trading for themselves on the basis of their analysis rather than providing a service to other investors. reply _sword 14 hours agoparentAt virtually all banks, equities analysts are banned from trading in their coverage. Banned as in, if you, your spouse, your dependents, etc. have an interest you didn't proactively disclose and dispense with you're fired on the spot. FINRA regulation states that registered equities analysts (i.e. the ones working at banks) at a minimum cannot trade against their ratings [0]. At most / all banks there are further restrictions that ban trading in coverage. [0] https://www.finra.org/rules-guidance/rulebooks/finra-rules/2... reply ghaff 16 hours agoparentprevFor both stock analysts and other types of analysts like IT industry analysts, it depends on the firm as far as I know and the rules may be more or less strict depending upon how involved they, or their fund, are with the given company. Generally, rules lean towards avoiding both conflicts of interest and the appearance of conflicts of interest. reply seadan83 15 hours agoparentprevThe difference, an analyst states if they have a long or short holding and if they intend to buy or sell in the future. The scammer does not, that is the key difference. reply dataflow 15 hours agoprev> The messages that Lebed posted would predict marked increases in the stocks value [...] In response to Lebed’s messages, trading volume of these stocks would soar I'm so confused reading the comments here. How in the world is anyone defending this? reply gnfargbl 13 hours agoparentI'm equally confused, but in the other direction! He was acting without any special knowledge, so the only thing he was taking advantage of was the gullibility of the general public. That's usually legal, no? reply autoexec 10 hours agorootparent> the only thing he was taking advantage of was the gullibility of the general public. Like every conman, snake oil salesman, scammer, and fraudster. Why treat him any differently? He should be tarred, feathered, and run out of town. reply robertlagrant 10 hours agorootparent> He should be tarred, feathered, and run out of town. I can't imagine what this could possibly mean. What should actually happen to him for posting messages on message boards? reply autoexec 10 hours agorootparent> What should actually happen to him for posting messages on message boards? If the problem was \"posting messages on message boards\" we'd all be just as guilty, but obviously that's not the case. That's like saying that someone who stabbed a person to death got punished for \"using cutlery\". People who intentionally hurt others for profit should be treated as such, no matter if they're mugging their victims or scamming them. He's no better than a Nigerian prince who uses email or the \"pig butchers\" who use SMS/MMS/social media. reply robertlagrant 8 hours agorootparent> People who intentionally hurt others for profit should be treated as such, no matter if they're mugging their victims or scamming them. Mugging and scamming are definitely not the same. Being robbed is not the same as giving your money away voluntarily. But I also don't think muggers should be \"tarred and feathered and run out of town\" either. They should be tried for theft, because the money/goods weren't handed over voluntarily. > He's no better than a Nigerian prince who uses email I'm not saying he's better or worse than anyone. I'm asking what tarring and feathering should actually mean. reply autoexec 8 hours agorootparent> Being robbed is not the same as giving your money away voluntarily. \"2 a : to deprive of something due, expected, or desired\" https://www.merriam-webster.com/dictionary/robbed You can argue semantics, but if I convince you that the beans I have for sale are magic and you voluntarily pay me for them only to find out that they aren't magic, it'd be fair to say you've been robbed. > But I also don't think muggers should be \"tarred and feathered and run out of town\" either. They should be tried for theft The specifics don't matter all that much. I'd also prefer that justice be handled in courtrooms, but what actually matters is that victims are made whole where possible and criminals who prey on others are punished. Society has always had ways to deal with parasites that hurt people by scamming them. reply robertlagrant 7 hours agorootparent> You can argue semantics Yes - absolutely. Semantics is about meaning, and meaning is central to discussion. > if I convince you that the beans I have for sale are magic and you voluntarily pay me for them only to find out that they aren't magic, it'd be fair to say you've been robbed Colloquially yes, but not really robbed. Possibly a victim of false advertising. But to establish that you'd have to prove they aren't magic, first. The problem would be that if you think beans can be magic, or stock picks can be guaranteed, then things will go wrong. No one can protect you from yourself as well as you can. > Society has always had ways to deal with parasites that hurt people by scamming them. This is an odd and unnecessary thing to say, though. Just like the tarring and feathering. reply dehrmann 9 hours agorootparentprevIt's interesting how the ethics of this change depending on how you frame it. reply matwood 11 hours agorootparentprevTalking your own book is basically CNBC in a nutshell. reply darby_eight 15 hours agoparentprevIt's not hard to mock peoples' trust of the market. reply AnthonyMouse 15 hours agoparentprev> How in the world is anyone defending this? Suppose you're an ordinary trader who likes high risk bets. You find a stock you like, you buy some. Now you go on message boards telling everyone you think the stock is going to the moon straight away. This is your sincere belief. Soon the stock goes up 300%. You thought it was going to go up 1000%, that's what you said before, but hey, it still went up 300%, that's still a tidy profit, so you sell. Whether this is fraud or not basically hinges on whether you were being intentionally dishonest, right? But now how do you distinguish between malice and stupidity? reply BrandonMarc 12 hours agorootparentBrings to mind this quote from the film The Big Short: \"Tell me the difference between stupid and illegal and I'll have my wife's brother arrested.\" (based on the book of the same name by Michael Lewis, a name I've seen elsewhere in these comments: go figure) reply dataflow 14 hours agorootparentprev> Whether this is fraud or not basically hinges on whether you were being intentionally dishonest, right? No. Do you mean legally? or morally? Legally the offenses include things beyond what might strictly be \"fraud\" (see \"fraud or deceit\" as one example, but there are more [1]). IANAL, but failing to mention that your posts are with the hope of raising the price because you have a vested interest in that... seems like a material omission, if nothing else. Morally (and I'd expect legally based on the text, but IANAL) what matters is what he expected to happen as a result of his posts on the message board. If he knew people would likely buy the stock in response to his posts, and he intended to take advantage of that influence to make money off those sales, that's the end of the story. Whether he sincerely believed the stock would actually rise regardless is beside the point. > But now how do you distinguish between malice and stupidity? I'll assume you mean good faith because stupidity isn't in the picture here. Someone who's genuinely predicting the stock market correctly isn't stupid. Do you sincerely believe this person didn't expect his messages to raise the stock price, or that he didn't intend to take advantage of such a rise to make money off the stock sales? I sure don't. [1] https://www.law.cornell.edu/uscode/text/15/77q reply AnthonyMouse 14 hours agorootparent> failing to mention that your posts are with the hope of raising the price because you have a vested interest in that... seems like a material omission, if nothing else. And if you run around telling people that you just bought some of the stock you think is going to the moon, is that supposed to make them less inclined to buy it, or do they now just think you're putting your money where your mouth is? > what matters is what he expected to happen as a result of his posts on the message board. Which is the problem. How do you prove what's inside of someone's head? Especially if they are malicious and know what not to say out loud. > Someone who's genuinely predicting the stock market correctly isn't stupid. If you think the stock is going up 1000% and it only goes up 300%, you were wrong, and your pick was wrong, and the people waiting for it to go up 1000% before selling are the people who lost money when it went back down. But as far as I know, being initially wrong and then changing your mind before you lose your money isn't illegal. reply joncrocks 13 hours agorootparentThe rabbit hole is deeper than you think. https://en.wikipedia.org/wiki/Mens_rea FWIW, there are other examples of where securities laws where intent (which is difficult to prove) is needed in order to demonstrate a law has been broken. e.g. Spoofing. reply AnthonyMouse 12 hours agorootparentIntent is often not that difficult to prove. It isn't murder if you were just sitting in the bell tower innocently cleaning your sniper rifle and it accidentally went off and put a round through the head of your sworn enemy, but you're entitled to reasonable doubt, not preposterous fabrications. The problem here is that the innocent behavior and the prohibited one are basically identical and neither of them is particularly implausible. reply dataflow 13 hours agorootparentprevI don't have the energy to dissect all this, but you're completely conflating legality, enforcement, and provability. We have juries for a reason. Most would a priori find it hard to believe that someone who found a winning strategy would repeatedly post it on random message boards with altruistic intent. reply AnthonyMouse 13 hours agorootparentJuries need some basis on which to make a decision. The law is ridiculous if it only turns on whether you had a lawyer ahead of time to tell you what not to say while operating the identical scam. It's not clear why telling people you think a stock is going to go up after you've already bought it would require altruism, and more than that you would benefit by building a reputation as someone who makes accurate predictions if you turn out to be right, independent of whether anyone acts on your statement by buying the stock. reply robertlagrant 10 hours agorootparentprev> Most would a priori find it hard to believe that someone who found a winning strategy would repeatedly post it on random message boards with altruistic intent. That doesn't make it right, though, at least in my mind. There's accountability on both sides in these cases. People reading message boards and treating the contents as financial advice are not taking sufficient care, particularly when they click past / agree to the statement on their trading platform that says that their money is at risk. Their money was at risk, and they did it anyway. My impression is this would come down to the jury's bias in either direction, when it should be pretty clear cut: you buy random stuff hoping to get rich, be it stocks picks from an Instagrammer, or NFTs, or whatever, and it is you choosing how to gamble your money. If I were in a casino and a guy was holding a sign saying I should put all my money on number 8 on Roulette, it wouldn't matter if he were the casino owner, and stood to gain, or was just a random guy. It's up to me to decide what to do with the advice. reply toss1 13 hours agorootparentprevyup, especially if they are not including the standard: \"Disclosure: I have positions in this security.\" This could be interpreted as (and/or variously written to indicate) \"this person is just talking up his/her book\" or \"they like it so much they have skin in the game\". Plus the factor of how much noise there is to any signal in the social media & message boards, how much attention can one reliably actually get for your shilling? Wrong vs provably wrong are often different reply j33zusjuice 8 hours agorootparentprevI think you misunderstood. The guy bought the stock, posted on some forum that he got the stock and it was going to the moon, and then he saw a 300% increase in whatever period they said. At the time they posted, they believed there would be a 1000% increase in price. But now they’re seeing 300%, and thinking they should get out now instead of trying to chase the high point, 300% is still a good profit. reply dehrmann 16 hours agoprev> Lebed professes that he did nothing wrong – he did nothing different from Wall Street analysts Except analysts usually can't trade stocks. Short sellers can. In theory, libel law makes that safe, but when it's a 15-year-old playing the penny stocks, the company won't notice or have the legal firepower to chase down a /r/wallstreetbets user. reply op00to 15 hours agoparentanalysts can trade stocks, though they are not supposed to trade counter to their recommendations. reply dehrmann 13 hours agorootparentWhen I googled it, it was a little murky on if it's allowed, and sometimes it was the analyst's employer that banned it. What is legal is different parts of the same ~bank not being as independent as they should be, so the kid's argument that the big boys do the same thing wasn't entirely wrong. reply JoshGG 17 hours agoprevMichael Lewis wrote about this story in his book “Next”. Also in the New York Times magazine. The articles and book have some pretty interesting discussion and opinion about this case. https://www.umass.edu/cyber/readings/marcusarnold.htm reply midnight2 10 hours agoprevCrypto is basically this writ large. So many kids being taught that scamming is how to succeed reply TapWaterBandit 10 hours agoparentNot really, way more kids are getting caught in the crypto scams than are making money doing them. reply lazyasciiart 9 hours agorootparentThe ones who got caught are still taking away the lesson that scammers win. reply ehaveman 14 hours agoprevfrom the lewis article: > the kid had bought stock and then, ''using multiple fictitious names,'' posted hundreds of messages on Yahoo Finance message boards recommending that stock to others if that's not illegal it really should be. reply littlestymaar 13 hours agoparentIf A16Z are allowed to buy thousands of their own crypto-hype essay in order to get it listed on NYT best-seller and drive shitcoin prices up, I don't see why random netizens could not do the same kind of things… Of course we could ban both. reply wil421 17 hours agoprevHow is this any different than meme stocks on WallStreetBets? reply ghaff 17 hours agoparentThis was a long time ago in a galaxy far far away. Also the article mentions these were thinly-traded stocks. I assume the argument would be that today this sort of thing is happening because it's Tuesday. i.e. it's normalized and if you get burned, it's almost someone's civic responsibility to part you from your money. reply gmd63 17 hours agorootparentScammers who think it's their \"civic responsibility\" to dupe their fellow man with intentional fraud should think two or three times before committing to that logic. If they want to value someone's worth to the human team based on their gullibility to professional liars, it's not reaching very far for other like-minded logicians to value the scammers' worth based on their ability to navigate any other man-made stressor, like their ability to dodge a bullet. reply ghaff 16 hours agorootparentMy last crack should probably have had a /s. I don't actually believe that. reply gmd63 16 hours agorootparentI didn't take you as believing it, just as framing what others might think. I've seen the sentiment too and it frustrates me. reply fmajid 16 hours agorootparentIt's known as Canada Bill Jones' Motto: \"It is morally wrong to allow suckers to keep their money.\" reply pc86 15 hours agorootparentprevAlmost as frustrating as people who posit the penalty for lying/fraud/hucksterism should be death. reply gmd63 15 hours agorootparentI didn't suggest that. Just illustrating how the incredibly moronic logic of \"weeding out people who don't meet my standards of fitness is a civic duty\" can be easily extended to non-financial areas of life. reply underlipton 16 hours agorootparentprevThe banksters are aware. Hence, doormen, gated communities, and commuting by helicopter. reply tacocataco 14 hours agorootparentprevGood luck getting law enforcement to care about you getting your stuff stolen. reply hyperhello 17 hours agoparentprevHe makes the case that this is no different from anything anyone does at all. The SEC is essentially a spam filter. reply williamDafoe 16 hours agorootparentNo, Lewis argues that SEC is a protection racket! They make a bunch of rules and regulations with the input of Wall Street so that Wall Street can pump and dump legally!! Congress will not touch this radioactive protection racket because they are doing insider trading on legal changes which should also be illegal... reply heartbl33d 17 hours agoparentprevOnly difference is he was doing this in the 90's. Today you will find telegrams pumping/dumping and manipulating the price of small stocks, crypto, etc. reply ghaff 17 hours agorootparentI was doing some market research for a penny stock company in 2000 and made my way over to a stock discussion board devoted to the company at one point. The amount of scaminess, cheerleading, and ignorance (feigned or otherwise) was unreal. reply lebean 17 hours agoparentprevThe difference is merely the essential part of meme stocks that make them meme stocks and not just a pump-and-dump scheme executed by some guy. Other than that, though, totally the same thing. reply gmd63 17 hours agorootparentI'm betting that large players manipulate the market through \"meme stocks\" just like advertisers astroturf reddit. reply seadan83 15 hours agorootparentI'm on a phone, apology for lack of citation. I ran across an article that found it was mostly memers bidding the price up against each other. The short squeeze aspect did come un, but not as often and not to the extent that is commonly claimed. reply bink 9 hours agoparentprevNot defending the sub but don't they have a market cap minimum for posts? reply hiddencost 17 hours agoparentprevBoth are morally outrageous and are or should be criminal? reply decafninja 12 hours agoprev15 (or let’s say, 21yo) me would use $800k very differently than current 40yo me. 21yo me would make a beeline for the local Porsche dealership. 40yo me would not. reply Broge 11 hours agoparentPower reply dehrmann 9 hours agoparentprevWhy porche when you can lambo /s But really, you don't get to have $800k by buying Porsches. reply listenallyall 5 hours agorootparentLots of Porsches tend to increase in value, some older models, spectacularly so. reply Broge 11 hours agoparentprevP reply d--b 15 hours agoprevHe was too early to the crypto party… reply xyst 15 hours agoprevRookie mistake. Always got to add in the fine print: \"Not financial advice\" and disclose current positions. You don't even need to say the exact positions (ie, long call option, X shares, ...). This is how the \"stock analysts\" on TV get away with it. When those credits role at the end of the segment, it will say something to this effect. Or TV show will disclose it at the bottom of the screen. It's a grift. reply daft_pink 14 hours agoprevI’m really curious if he is unable to trade when he gets older. reply Brian_K_White 10 hours agoprevSome people say with a straight face: \"Hey if someone is stupid enough to believe something I say, it's not my fault, even if I say something I know is false.\" And enough other people like being able to cash in at that same well that they avoid working towards making it officially illegal with significant penalties. It's that John Oliver piece on timeshares all over the place in essentially every arena. reply stevenjgarner 15 hours agoprevkentlaw.edu doesn't use SSL? reply mistrial9 15 hours agoparentthis site also ? http://whois.educause.edu reply fmajid 16 hours agoprev> Depending on who you talk to, Lebed was either viewed as a person who knowingly abused the system and broke the law, or someone who acted no differently than Wall Street analysts acted Both of these statements can be true at the same time. reply hunter2_ 15 hours agoparent> Depending on who you talk to, Lebed was either viewed as a person who knowingly abused the system and broke the law, or someone ... actually performing no wrong-doing. Funny how selectively quoting a bit differently makes that appear far less so. reply backtoyoujim 12 hours agorootparenthttps://www.youtube.com/watch?v=XIh44OEyQgc Cramer admits to doing basically an insider version of \"flood finance.yahoo.com with bs\" as his job reply pierat 15 hours agorootparentprevIt all comes down to selective enforcement by the not-actual-real-judges at the SEC. You have to go through their kangaroo court with an ALJ, and arger likely getting a judgement against you, can you actually go to a real court. And this is combined with different rules for different income classes. Kid from middle income family gets smacked down hard. Whereas billionaire employees doing work on wall street either get low scrutiny, or pay 2% of what they made (Read: cost of doing business). The \"rights\" of the rich are not for the rest of us. reply sdwr 14 hours agorootparentIf you read the article, it looks like he got punished pretty lightly - had to pay back some of the gains, got to keep others, and had no further repercussions. reply neomantra 12 hours agorootparentprevThe Supreme Court is considering if Administrative Law Judges (ALJ [1]) are unconstitutional. SEC v. Jarkesy. Fun fact: adherence to law is not required in an ALJ court. If one loses in ALJ, then one may proceed to appeal in a regular court where law must be considered. [1] https://www.law.cornell.edu/wex/administrative_law_judge_(al... reply robertlagrant 10 hours agorootparentprev> Whereas billionaire employees doing work on wall street What's a billionaire employee? reply Brian_K_White 10 hours agorootparentSlightly wrong or ambiguous parsing, but, any employee of a billionaire also works, at least as well as any other sloppy natural communication. reply robertlagrant 8 hours agorootparentWould the janitor of a company that has a share owned by a billionaire count? Surely a billionaire in this case is just a share owner. reply Brian_K_White 3 hours agorootparentQuestion does not seem to apply. What point are you trying to make or what mystery baffles you about the concept of an employee of a billionaire? reply paulryanrogers 17 hours agoprev> Lebed professes that he did nothing wrong – he did nothing different from Wall Street analysts. He states that he learned how people react to the stock market and acted on that knowledge. So you trick people by lying to them, and it's ok because you take their money indirectly via the stock market. And because some analysts also do this (often suffering legal consequences), then it's definitely not wrong? Shameful. And the father getting the kid an E-Trade account after the mother closed his other account? Sounds like the father should be sanctioned too. reply lucianbr 17 hours agoparent> The 2007–2008 financial crisis, or Global Economic Crisis (GEC), was the most severe worldwide economic crisis since the Great Depression. Predatory lending in the form of subprime mortgages targeting low-income homebuyers,[1] excessive risk-taking by global financial institutions,[2] a continuous buildup of toxic assets within banks, and the bursting of the United States housing bubble culminated in a \"perfect storm\", which led to the Great Recession. (from wikipedia) > In total, 47 bankers served jail time as a result of the crisis, over half of which were from Iceland One example of countless that could be found, that legal consequences are the exception, not the norm. reply vincnetas 16 hours agorootparentWell done Iceland. I would like to see ratio of jailed bankers to total bankers in country. Because now it looks like GES was mostly caused by Icelandic bankers :) reply lucianbr 15 hours agorootparentI recently read a comment by an icelander saying that they're basically just the same as the rest of the world, incapable of punishing bankers and rich people. Just that on this occasion, the banking sector being so small, something like 90% of it failed, so they really had no choice but to jail some people. Definitely hearsay, but it really saddened me. reply ghaff 15 hours agorootparentThat sounds about right. It's a small homogeneous country and the situation really was dire. Much more so than in many other countries, including the US, in general. The US had some big failures but the system largely held together (with the help of the government of course). reply wslh 16 hours agoparentprevRephrasing the Arthur C. Clarke statement “Any sufficiently advanced technology is indistinguishable from magic.” I would say \"Any sufficiently advanced scam is indistinguishable from genuine trading advice.\". reply wouldbecouldbe 16 hours agoparentprevHe was just ahead of his time, this is an honest days work for a crypto trader. reply dehrmann 16 hours agoparentprev> So you trick people by lying to them, and it's ok because you take their money indirectly via the stock market No one was arguing that it was indirect; the people who lost money took advice from an anonymous user on a forum on tiny companies they'd never heard of. The morality question is still interesting because it hinges on whether or not people knew they were playing a game with deception. reply d0gsg0w00f 15 hours agorootparentThese situations always remind me of a quote from Clark Howard who was a radio host of personal finance shows. \"It's almost impossible to get scammed if you're not a greedy person\" reply lazyasciiart 9 hours agorootparentIt's not though. Greed isn't involved for all the well meaning naifs who are just wiring cash to bail out their wayward grandson after he called them on facebook, or received mail indistinguishable from a bill and sent back a check. reply d0gsg0w00f 8 hours agorootparentSure. The quote doesn't sit well with that situation but it works for the topic of discussion. reply dehrmann 13 hours agorootparentprevMad props to Clark Howard. He's like the Mr. Rogers of personal finance. also: Bulls make money, bears make money, pigs get slaughtered. reply IncreasePosts 13 hours agorootparentprev\"Can't con an honest john\" reply bagels 16 hours agoparentprevDo you have examples of lies? If he just said this stock will go up, it seems he wasn't even wrong. reply EGreg 17 hours agoparentprevThe entire trading industry is a zero-sum game bro. For someone to win someone else has to lose. As is the advertising industry and surveillance capitalism industry that powers the “free web”. (Only at the edges is new value created. Most of it exists to have large institutionals take money from the small newbs, you’re the fish at the poker table.) And more broadly, capitalism exploits people. This is like blaming Andrew Tate for exploiting women but throwing up your hands when Amazon Nike and Apple were running sweatshops and warehouses at scale, exploiting people in general. Technology just made it easier to extract the rents (eg tax the uber drivers and passengers to pay the shareholder class of Uber). And generative AI just automates that further. Thanks for all your creativity, judgment and input, humans. You can be fired now thanks! Hey, if you look at the man behind the curtain, you’ll see all your favorite stuff needs it to even function. For example for Sam Altman’s ventures we have: Kenya’s cheap brains: https://time.com/6247678/openai-chatgpt-kenya-workers/ Kenya’s cheap eyeballs: https://www.bbc.com/news/world-africa-66383325.amp Don’t shoot the messenger reply Thorrez 17 hours agorootparent>The entire trading industry is a zero-sum game bro. For someone to win someone else has to lose. Traders add liquidity. That liquidity allows me to easily buy and sell stocks without waiting long periods or worrying about getting ripped off by the spread. And since the economy isn't zero-sum, it's possible for everyone to end up better than before. reply peteradio 16 hours agorootparentDoes trading stocks somehow cause an increase production of actual stuff? If not, I don't see how everyone ends up better off. reply AnthonyMouse 14 hours agorootparent> Does trading stocks somehow cause an increase production of actual stuff? Sure it does. If the price of a stock changes, it becomes easier/harder for that company to raise money relative to other companies. If it's a good company then making their shares go up can increase production of actual stuff because they can afford to build more factories etc. If it's an inefficient company and its shares go down then the people who sold it can go invest that money in something else. There is a net increase in production if the company who gets the money is the one more efficient at using it to make stuff than the alternative, i.e. share price becomes \"more accurate\" as a measure of the company's market efficiency. reply hallway_monitor 15 hours agorootparentprevPeople are saying that pump and dump schemes are unethical even if they're not always illegal. I would assert that investing for short-term gain is always unethical and the only way the stock market really benefits retail investors is when people invest and hold for long-term gains. reply jokethrowaway 14 hours agorootparentprevYou have to buy stocks or you'll money will go to the government thanks to inflation. Without the government the stock market would be less crowded. reply dragonwriter 16 hours agorootparentprev> The entire trading industry is a zero-sum game bro. It isn’t in utility terms, which are the only terms that matter for whether something is actually zero sum. Under certain assumptions, it is in (e.g.) dollar (or other specific commodity) terms, but the whole reason markets work at all is that the no specific commodity (including any fiat currency at any point in time) has a consistent relationship to utility across market participants. reply peteradio 16 hours agorootparentHow well does that utility argument play out when companies see fit to grant 0 dividend? reply AnthonyMouse 14 hours agorootparentIf you buy shares for $100 and they increase to $200 without paying you a dividend, you've still made $100. If you prefer to have $100 in shares and $100 in cash rather than $200 in shares, you can sell half of your shares. reply dragonwriter 14 hours agorootparentprevDividends or not are irrelevant, what matters is that the marginal utility of market value measured in monetary terms (or in market equivalent in any other commodity, but money is the most common one for people to mistake for a direct measure of utility) differs between market participants, and for the same participant (relative to others, not just by, e.g., inflation that applies in the same way to all market participants) at different points in time. reply gmd63 16 hours agorootparentprevIncorrect. Capitalism allows economies to accelerate the birth of new companies that benefit people. When it's policed and regulated poorly, yes, scammers can proliferate. So don't vote for politicians who lie without apology, and don't let people you don't trust manage your money or provide you with goods or services. Predatory people will exist with or without capitalism, only Capitalism is fine-grained democracy where you vote literally every day with your dollar. I should also add, don't do any scamming or predatory behavior yourself. Because the most important vote you make for how society is run is with your own actions. reply Supermancho 12 hours agorootparent> only Capitalism is fine-grained democracy where you vote literally every day with your dollar. I'm not sure where you get that from. Every economic model has trading, wherein the participants make trades (goods for goods). Capitalism is not democratic. Communism isn't democratic for a similar reason. The pareto principle (and natural hierarchies) appears and equality goes out the window for the traders. reply gmd63 11 hours agorootparentIt certainly is more democratic than any alternatives I'm aware of. If you don't want someone to accumulate more economic influence, do not pay them. The government will not jail you for doing so. If business is controlled by a democratically elected government, and 49% of the country voted against those officials, there is no way for them to start their own alternative business to articulate those preferences in the market. reply Supermancho 10 hours agorootparent> It certainly is more democratic than any alternatives I'm aware of It's no different at all, to any other economic system. > If business is controlled by a democratically elected government, and 49% of the country voted against those officials, there is no way for them to start their own alternative business to articulate those preferences in the market. If you have 1 dollar and I have one hundred thousand, the same problem exists. This has nothing to do with capitalism or democracy, per se. reply gmd63 10 hours agorootparentIt is not the same. You can build your own business and the guy with one hundred thousand dollars does not have any ability to prevent you from doing that under the law. The hierarchy under capitalism is very different. reply everforward 17 hours agorootparentprev> The entire trading industry is a zero-sum game bro. It's not, and even a cursory understanding of what zero-sum games are and how the stock market works would tell you that. How does the market grow if all trades are zero-sum? How do recessions happen if all trades are zero-sum? Some portion of trades are zero-sum, maybe even most of them. Stock markets don't function if every trade was zero-sum. Pump and dump schemes are bad because they distort information in the marketplace. The tradeoff is supposed to be that these major firms perform price discovery, and in exchange for providing accurate prices to the market, they are allowed to arbitrage off the difference between the current price and the \"true\" price. Pump and dump schemes don't do any price discovery, they do the opposite. They create a fake new price, arbitrage off that difference, and then leave the followers holding the bag. reply ItsMonkk 16 hours agorootparentThe trading industry is separate from the investing industry. The investing industry is positive sum - but you must hold for the long-run, or only make decisions if new information is discovered to be an investor. Vanguard and other passive firms are a part of the investing industry. As the GP says, the trading industry is zero-sum. reply mhluongo 16 hours agorootparentNot a fan of over-financialization, but... if trading is propagating market signals to people who otherwise wouldn't have them, it should have some positive externalities? reply __MatrixMan__ 16 hours agorootparentprev> How does the market grow if all trades are zero-sum? The same way a cancer does: by repurposing adjacent systems in service of propagating itself, often to the determent of the function of those systems. In the case of a financial market this means people spending their time watching lines squiggle around on a screen instead of finding real problems to solve. Referring to the increase of stock prices as an indicator that markets are positive sum is a little like referring to an increase in hit points as an indicator that D&D is positive sum It's not useful to use a system's own abstractions to measure its effectiveness. reply seadan83 15 hours agorootparentOutside money flows into the market, this allows it to be positive sum. For the DnD analogy, it is as if you had a cleric. Eg: IPO sells at $2, next seller buys at $4, next at $6, etc. Everyond makes money in that scenario. Eventually there can be losses, do all the losses counter balance these profits? No, because on net everything went up. If there were always trades that had losses to balance the gains, only then it would be zero sum. What's more, people convert types of holdings all the time, and it is the big players that move things. When a mutual fund allocates 1% more to stocks, or 1% less and putd that to bonds, those are very big net new or net negative money flows. So, it's not just paychecks being added in, there are lots of sources of dynamism to make the stock market system anything but closed. There is net new money flowing into it all the time. reply __MatrixMan__ 15 hours agorootparentMoney gets created when people take out loans which creates debt which is then traded in the markets. That's not money flowing into the markets, that's just the markets doing their own internal bookkeeping (much like D&D uses HP for internal bookkeeping about character health). If you want to convincingly argue that markets are positive sum you have to demonstrate that markets produce more than they consume in terms of something besides money. E.g. when some of the factory workers quit to go be day traders, the factory ends up producing more while consuming the same or less due to the positive effects of the investments enabled by the increased trading activity. This might be the case in some cases, and it won't be the case in others, and I don't know how to sum it in aggregate. Presumably somebody does. Whether it comes out positive or negative, asset prices are an implementation detail, not the thing we're measuring. reply AnthonyMouse 14 hours agorootparentIn general there are two ways for companies to make money. 1) They make something that otherwise wouldn't have been made, or make it more efficiently than the competition and then capture volume with lower prices. 2) Rent seeking; they extract higher profits from the same activity, e.g. by buying their competitors and raising prices. Traders do the work of finding the companies that can do one of these things and allocating resources to them so they can do them. When it's the first one, the societal gains are quite significant. When it's the second one, well, there are supposed to be laws against that and if it's happening then we need better laws or better enforcement. reply __MatrixMan__ 13 hours agorootparentI'd argue for a third category where the new thing addresses a problem that the company themselves is making more problematic (e.g. TurboTax) or where \"more efficiently\" involves cases where costs to the company are exchanged for costs to society (e.g. petroleum companies, cambridge analytica). I think this category represents the majority of economic activity today, which is why I'm skeptical of markets being zero sum. The result is the same though: find was to get more of (1) and less of (2) and (3). reply AnthonyMouse 13 hours agorootparentThe third thing is just the second thing. TurboTax lobbying to avoid free tax filing is obviously rent-seeking. It might even be more accurate to call this category rent seeking through regulatory capture. Fossil fuels would have been trounced by nuclear except that they lobbied to require nuclear to internalize all of its costs (and then some) but not their own, extracting undue rents by handicapping the competition. The closest you get to a third category is Cambridge Analytica, in which case the third category would be criminal enterprises. But these rarely remain listed on the stock market; you won't find shares of the Guadalajara Cartel at your brokerage. It's the companies in the second category that do, because they're doing things that should be illegal but aren't. reply __MatrixMan__ 11 hours agorootparentI was going for extreme examples to make the point, but look at any modern advertiser: their products are used to target segments of society such that we can be encouraged to either turn against one another or to make some kind of decision which is bad for us and good for whoever is doing the advertising. The perspective you're describing doesn't seem to have a way to classify this activity as unhelpful without going so far as making it illegal. Without that ability, I don't see how we can ever expect the market's notion of value to find itself in line with society's values. The best we can hope for is a random walk within the bounds of \"illegal\". reply AnthonyMouse 2 hours agorootparentThere are only a finite number of possibilities here. If you're stuck with a machine you don't like that shows you harmful advertising because it has no viable competitors, the problem is with the law, because it's propping up that institution or otherwise failing to enforce anti-trust laws. If that machine exists, even though there are plenty of viable competitors that don't do that, and then you freely choose that one on purpose, it's not obvious that the company is doing anything wrong. You apparently like that kind of advertising. The exception would be for some kind of information asymmetry, like the advertiser is lying to you and you don't find out until it's too late. In which case we're back to needing a law, in that case against false advertising. Now I can tell why you're choosing this one, because it's a sticky one: What happens when they're lying about politics? We can't have the government enforcing a law against \"lying\" about political issues, because the very thing that makes them political is that people disagree and don't find the other side's evidence to be credible. We can't have whoever is in power at any given time censoring the opposition. The solution we use for this is to put the determination in the hands of the public. The marketplace of ideas. If someone is full of crap, you prove them wrong in front of everybody, and then they lose credibility. This isn't perfect but it works pretty well in the absence of the aforementioned anti-trust problem. Because then instead of a handful of megasites that people get siloed into and fed content like livestock, you'd have a million blogs that intersect like the web. There would be no walls between sites, no way to prevent someone you're trying to mislead from hearing from the other side just because you're in with the site's operators, because none of the sites would be big enough to exercise that kind of power when the average person is getting their information from hundreds of separate sources instead of only one or two. Sometimes companies have bad incentives. Those either get curbed by laws, or by free market competition, or... what's your alternative? reply everforward 11 hours agorootparentprev> If you want to convincingly argue that markets are positive sum you have to demonstrate that markets produce more than they consume in terms of something besides money. E.g. when some of the factory workers quit to go be day traders, the factory ends up producing more while consuming the same or less due to the positive effects of the investments enabled by the increased trading activity. Price discovery would be the function of the markets that allows this. If the markets find out that iron ore production is going to crash because of a flood or something, they create demand for futures of that iron production, raising the price of iron and encouraging stability in the supply of iron via more companies mining it. These futures then provide stability of supply and price (somewhat) to the factory. Conversely, they provide stability of demand and price to the mine, who can sell their production ahead of time. I would not be surprised if most factories do employ investors, even if indirectly via a third-party supplier. Knowing that the price of iron was going to go up would be tremendously useful, and they would be able to produce their goods cheaper than everyone else if they stocked up on materials at lower prices. They're traded in dollars, but the same would be true in a barter system. Liquidity would be incredibly important in any non-monetary system; people would pay a pretty penny to be able to exchange payment in goods they don't want to something they did want. > Money gets created when people take out loans which creates debt which is then traded in the markets. That's not money flowing into the markets, that's just the markets doing their own internal bookkeeping (much like D&D uses HP for internal bookkeeping about character health). That is meant to reflect the new value being created by all of the entities in the market. If you don't add currency to account for growth, the currency increases in value equal to demand for it, making the currency itself a very attractive investment. Those loans have interest rates, so the borrowers need to be doing something with the cash to generate value in excess of the interest rate, which in turn creates value on the market. Loans backed by actual currency are dramatically more expensive to service because of opportunity costs. The borrower's interest rate would be higher than what the loan offerer thought they could get in return for safer investments. Current loan rates for practically everything are far below that. The factory benefits by having access to extra cash for cheap. They don't have to stockpile cash for a decade to open a new factory, they can borrow money and do it now at an interest rate that still allows them to keep most of the value. reply seadan83 14 hours agorootparentprevI'm responding to only: \"> How does the market grow if all trades are zero-sum?\" I'm not trying to say that markets are necessarily a net positive. But we should also not be myopic about it either. There is more to it. Firstly, money being made solely by trading is rent seeking. It's maybe even the definition of rent seeking (?) Personally I'd be in favor of tax rates in the 40% to 50% region for rent seeking (long term and short term capital gains tax IMO should both be doubled). Perhaps we agree a bit there. > Money gets created when people take out loans which creates debt which is then traded in the markets This is still net new money into the market, ergo, the market is not at all zero sum. There is new money flowing into it. That _negates_ the statement that there must be one person losing for every person winning in the stock market, it is not zero sum. Everyone can actually be making money because there is new money being injected (and the opposite can be true when money is net leaving because interest rates are high and everyone is putting their money to cash or bonds instead). Same thing happens in a Ponzi scheme, everyone can actually be making money while there is new money flowing into the system. In the example you stated, if a person borrows money against their house, they are moving money away from real estate to the market. Their wealth is net-zero in that scenario, but relative to the market - the market saw a net increase. Thus, the market is not a closed system, it is therefore not by necessity zero-sum - it's possible to have more winners than losers when trading (and/or more losers than winners too) > If you want to convincingly argue that markets are positive sum you have to demonstrate that markets produce more than they consume in terms of something besides money. E.g. when half of the factory quits to go be day traders, the factory ends up producing more while consuming the same or less. Yeah, this is what happens with IPO. Oatly is an example, they had a supply crunch, they could not produce enough milk for store shelves. With their IPO money they funded the construction of additional factories to increase their production capacity. I think people forget this, the IPO of a stock is a huge injection of money to a company, as-is whenever the company issues more stock. Amazon is another example, instead of using cash to pay employees, they used stock; which freed up cash to go to other places. Though, post-IPO, shares being traded around is arguably all just rent seeking. A company can still issue more shares too though. This very thing saved both AMC and GME; both of those companies would have gone under if they were not able to raise money by issuing stock. On the other side, this situation is not always fully pure. Plenty of companies are run by MBAs that give themselves too many shares & their sole goal is to go public so they can offload their holdings rather than grow their company. Still though, the primary reason for stocks to exist is that companies can acquire additional funding without taking out loans, they get that funding by trading ownership. That's still a thing even if there is a lot of other corruption & rent seeking relating to it. Thus, we should not be myopic it, there is more to it. reply gmd63 16 hours agorootparentprevCancer is an exploitation of the politics present in human cellular biology. The DNA of what makes us human mammals is molecular capitalism in this analogy. You can dismantle that, but then we would be floating amoebas in the primordial ooze. What you can do is identify when cells aren't behaving in good faith and punish them appropriately (through chemo etc) and find ways to prevent that behavior from surfacing again (diet / avoiding carcinogens / unknown future solutions). reply __MatrixMan__ 16 hours agorootparentAgreed, I'm not for dismantling it wholesale, just for being a bit more targeted with our punishments. That is, I don't think that \"the markets are up\" is not an adequate indicator of \"behaving in good faith\". You have to look more closely at the actual outcomes. Whatever is being invested in, does it make life better or worse for the rest of us, etc. reply gmd63 14 hours agorootparentI agree we need more keen punishments, and am annoyed by market value being some coarse indicator for economic health. It's been more morally pleasing to me to see criminal companies and \"currencies\" collapse in the past few years than it has for me to have seen the general market go up, as long term health depends on excising fraud. reply EGreg 11 hours agorootparentprevThe stock market grows because companies create value for real consumers. Companies serving the people. But the stock market and its derivative markets are just people speculating on the price of the stock. So the vast majority of the value is a zero sum game. Think of it like the \"use value\" of a house, vs a housing speculative bubble on top. They can pump, they can dump. reply Solvency 16 hours agorootparentprevIt's almost like a financial system that is so incredibly prone and vulnerable to manipulation shouldn't form the backbone of our entire economy. Oh wait. This is by design so the elite can retain all money and power. reply amne 16 hours agorootparentprevit's not, but only because someone keeps printing more money. reply voisin 17 hours agorootparentprevI concur - we as a society have agreed implicitly (since I think around the 1980s) that it’s everyone for themselves so take whatever the hell you can for yourself, consequences be damned. If you’re lucky, you’ll be dead before the world burns and society devolves into chaos. reply seadan83 15 hours agorootparentThere are places in the world that is an apt description. The US is closer to that than previously (as judged by effective tax rates on millionaires comparing the 1950s to today), but nonetheless, the US us not \"there\" yet and still has a ways to go reply nullstyle 17 hours agorootparentprevIn the immortal words of Booker T: “don’t hate the playa, hate the game” reply falcor84 17 hours agoprevTL;DR: he argues that he did the same as everyone else on Wall street, so the SEC fines him $285,000, letting him keep more than half a mil of net profits. The system works as intended reply arkades 17 hours agoparentThe SEC only found wrongdoing in eleven trades, whose profit netted approx $272K. They did not penalize him for trades not found in violation of the law. Clearly, too long, didn’t read. reply pierat 16 hours agorootparentAgain, not a real court. And no jury, and basically a fake judge. reply Solvency 16 hours agoprev [–] Why is this page double line spaced? It's outrageous on mobile. reply chatmasta 16 hours agoparentThe page is from 2002, when \"mobile\" was pronounced \"mob-eel\" and the closest thing to a smartphone was an early version of the PalmPilot. reply sillysaurusx 16 hours agorootparentOur friends pronounced it \"carrying around our computer towers and setting up folding tables to play Planetside and Stepmania all night\". I think it’s a regional thing. reply Solvency 8 hours agorootparentprevI had plenty of my own webpages in 2002 and never had this weird double spacing. Ever. reply Bud 15 hours agorootparentprev2002 was actually quite late in the Palm era. The iconic Palm V, for instance, dates back to 1999. And BlackBerry had a mature smartphone by 2002. Nothing compared to an iPhone of course, but quite capable. reply kube-system 12 hours agoparentprevIt was generated in MS Word 2000. Double spacing of Word docs is common at educational institutions. reply lolinder 15 hours agoparentprev [–] Reader mode? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Jonathan Lebed, a teenager, earned around $800,000 by trading stocks, but SEC cited eleven trades as illegal for market manipulation.",
      "Lebed justified his actions, likening them to those of Wall Street analysts, leading to a $285,000 settlement with the SEC.",
      "Debate exists on the morality of his trading methods and the SEC's response to the case, sparking ethical concerns in the trading community."
    ],
    "commentSummary": [
      "The discussion delves into unethical practices in financial markets like pump and dump schemes, stock manipulation, and scams, emphasizing the ethical dilemmas of deceiving for profit.",
      "It distinguishes between legitimate analysts and scammers, exploring the challenges in proving criminal liability for intentional market manipulation.",
      "The conversation also touches on the societal impacts of market dynamics and modern advertising's influence while advocating for improved laws and enforcement to benefit society."
    ],
    "points": 170,
    "commentCount": 156,
    "retryCount": 0,
    "time": 1711295526
  },
  {
    "id": 39813240,
    "title": "Exploring Windows 98 Icons by Alex Meub",
    "originLink": "https://win98icons.alexmeub.com/",
    "originBody": "Windows 98 Icon Viewer Recycle Bin Computer Documents Download All Icons Windows 98 Icon Viewer Made by Alex Meub",
    "commentLink": "https://news.ycombinator.com/item?id=39813240",
    "commentBody": "Windows 98 Icons (alexmeub.com)158 points by keepamovin 3 hours agohidepastfavorite69 comments srvmshr 2 hours agoGetting reminded of late 90s & early 2000s design choices, the glassy feel of Bluecurve icon themes of Redhat Linux looked way ahead of their competition [1] Most people didn't have access to Classic Macs OS 8.5~9, which probably had beautiful icon sets too (Aqua?). As a highschooler, this looked so futuristic & colorful as compared to 2000/XP desktop themes & icons. EDIT: Adding as postscript: Is there anything similar which would come close to Bluecurve today? TIA if there is anything in reader's minds. 1. https://upload.wikimedia.org/wikipedia/commons/7/71/GNOME_2.... 2. https://upload.wikimedia.org/wikipedia/commons/b/b8/Fedora-y... reply lqet 1 hour agoparent> As a highschooler, this looked so futuristic & colorful as compared to 2000/XP desktop themes & icons. Exactly my experience. The first time I installed Linux when I was around 15 I was surprised how modern, \"sharp\" and professional everything looked compared to Windows XP. (The default Windows XP design was really the beginning of a downward curve for Windows designs. Windows 98 looked like a tool, Windows XP looked like a toy, which is a shame because Windows XP was a vastly superior (and in my experience ridiculously stable) OS. I always used the \"classic\" Win 98 design). reply brtkdotse 37 minutes agorootparentWindows 2k was the pinnacle of Windows. Basically same OS as XP, but with a spruced up Win 98 design. reply srvmshr 27 minutes agorootparentWindows 2000 startup screen (as well as the ill-received Windows Me) was very elegant for its time. I think the Windows 7 GUI emulation in 8.1 was also pretty good. reply akx 29 minutes agorootparentprevHeh, I used to get the best of both worlds with a patched uxtheme.dll... Took a bit of digging, but yeah, SlanXP was a theme I used a lot. https://images-wixmp-ed30a86b8c4ca887773594c2.wixmp.com/i/4b... reply mdasen 2 hours agoparentprevBluecurve was amazing. For me, nothing in the Linux world has equaled it since. It was so nice and polished with thoughtful colors and design. It was quite similar in colors to Mac OS 8 & 9 (the platinum gray, the purple-ish blue). reply srvmshr 1 hour agorootparentI sort of agree with that. That was the peak Linux GUI we saw (at least so far). Sadly after the Gnome GTK2 vs GTK3 forks, none of the child products carried the same finesse as Gnome 2.4/Bluecurve. They were sharp, colorful and vibrant. Windows icons seemed more 'classic' versus the KDE-inspired Bluecurve, Clearlooks which were more playful. reply mjhagen 2 hours agorootparentprevReminds me a bit of BeOS as well. reply pndy 2 hours agorootparentIt's the icons angle and that thin black border around images, I believe reply prmoustache 1 hour agorootparentprevYeah I loved the beos icons too reply lloeki 53 minutes agoparentprev> Classic Macs OS 8.5~9 [...] Aqua? Aqua is (was†) Mac OS X, Mac OS 8~9 was Platinum. † While the current design continues to be called \"Aqua\" I personally consider the last iteration of Aqua to be Mavericks. While Cheetah, Puma, and Jaguar had the filling lines, Panther and Tiger had brushed metal, and the brief touchdown with skeuomorphism in Lion and Mountain Lion, they overall consistently carried the colourful glossy-watery \"lick-it-candy\" theme throughout - including icons - all the way, until Yosemite happens... Then it gets all silky with frosted glass, desaturated colours, and muted gradients, which just does not have the Aqua feel to me. https://www.mailvita.com/kb/list-of-all-versions-of-mac-os-f... reply srvmshr 47 minutes agorootparent> Then it gets all silky with frosted glass, desaturated colours, and muted gradients, which just does not have the Aqua feel to me The JonnyIvification of Apple interfaces starting with Mavericks & iOS7. Although it has grown on us, the flat designs were very jarring back then. Our brains were so used to pill shaped buttons & then everything felt flat reply pndy 2 hours agoparentprevBluecurve was fitting with the overall candy-like interfaces style at that time and yet, still giving a nice and professional, serious looking GUI for Redhat and later for short time Fedora. I even skinned my XP with theme that was most close to the original - sadly, on Windows it was always a hard task and in few spots GUI was falling back into 9x widgets, not mention the usual problem with icons and other resource files. reply alex_duf 1 hour agoprevI find it so interesting how many of the symbol used are with objects that have or that will disappear from our lives. A camcorder, a floppy disk, a CD-ROM, a film (for a camera), maybe even the fountain pen. All of which I can recognise, but all of which will probably leave my daughters baffled if I were to ask them what it was... reply usrusr 18 minutes agoparentA grammophone icon for music wouldn't exactly be cryptic to me and I'm not sure I ever heard one. But you said baffled when asked to describe, not confused by the icon. I think we have precedent in the iconography of specialist occupations that have already existed in preindustrial times. The various brewing implements that adorn the labels of many breweries, the geometry implements that now only live on in freemasonry.. They all have names, but those names are lost to everybody but those who enjoy accumulating knowledge of old-timey things. Baffled, yes. But the world already is full of that kind of echoes from the past, it's not like someone is confused by them. Those who might consider a box of 3.5\" a box of old save icons grew up with those save icons, it's the world they know. reply nonrandomstring 1 hour agoparentprev> I find it so interesting how many of the symbol used are with objects that have or that will disappear from our lives. It is interesting how iconography links to timeless design. Function and appearance have a way of coexisting in good design so the \"idea\" of something gets into semiotics. Disk storage is a timeless concept. It's absent now but will no doubt return in some new guise on the next wave of materials science and storage tech. The pen is timeless too. Perhaps the weird one is the classic phone handset. My daughter who never saw one except maybe on a cartoon, instantly knew what to do with an old 1970s rotary I was hacking. The design \"reaches out\" and suggests its usage. Partly that's our monkey brains that sees tools and uses in everything. reply stavros 17 minutes agorootparentI'm surprised your daughter knew what to do. I made a rotary mobile phone and showed it off in an exhibition, and kids were universally baffled. They tried to press the numbers in the wheel, then they tried to turn it without picking up the handset (why would they pick up first, everyone knows you dial the number and then you confirm the call), etc. How did your daughter immediately grasp those non-obvious, hidden concepts? reply nonrandomstring 4 minutes agorootparentThink I remember you posting this before when we had a discussion about modding old phones. I'm convinced she saw it in a cartoon like Dora or Fireman Sam. But didn't hesitate to pick up the handset and put it to her ear. reply stavros 2 minutes agorootparentAhh interesting, I hadn't thought about cartoons, thanks. I probably did post this before, I find it interesting how one thing can be entirely obvious to one generation, but completely opaque to another. taco-hands 1 hour agoparentprevTHAT telephone modem icon! reply atum47 2 minutes agoprevAre they free to use? I have some in my website but i don't even remember where i got them. I just started using it hoping that nobody would sue me. reply imathew 45 minutes agoprevIn around 1997 I installed I think Red Hat 4.x because I'd seen screenshots of the Enlightenment window manager and wanted that beauty on my desktop. Now all I want is the Windows 98 look and feel back, when icons and user controls were clear and legible and consistent and you could drag a window from anywhere on a title bar. reply myfonj 41 minutes agoprevIf you yearn to see those icons with crisp zoomed pixels without blurry aliasing, try this makeshift viewer of the source sprite image (https://win98icons.alexmeub.com/css_sprites.png): data:text/html;verbatim, document.body.style.setProperty('--dpr',devicePixelRatio);It forces it to map to physical device display pixels and use nearest-neighbour scaling; the last number there sets it to display each source pixel as 5×5 square in your browser's window. reply qwerty456127 2 hours agoprevI've seen this and am amazed, also obviously grateful for the job, nevertheless I dream (I don't demand, this is just a dream) of a way better library including also all the icons from all the prominent apps and libraries of all the Windows eras since Windows 3.x. I mean also MS Office and Visual Studio to say the least. Ideally every single one should be accompanied with upscaled versions up to 512x512px. This all probably is very questionable from the legal point of view but I have courage to hope nobody is going to care busting us as long as we only use these in small personal projects just a handful of people sees. I even hope there will one day be a Dall-E-like neural network which would take a prompt and produce icons in Windows 3.x/98 style in any resolution you might need (i.e. up to whatever it takes to make a big icon on a Retina display). Long live vapourwave! :-) reply etaioinshrdlu 2 hours agoparentI think a fine-tune of SD or SDXL would be perfectly able to produce such images. I would also like to see this. reply p0w3n3d 2 hours agoprevBefore I tried to make my own, I had been convinced that an icon is just a resized large image. For me up to this moment, it's amazing, how hard is to create an icon or pixel art. How every pixel counts, how the creator needs to manage the shades, gradients, and sometimes two pixels will show for example the white and the pupil of the eye. reply Nition 48 minutes agoparentDid you ever see Mark Ferrari's animated pixel art landscapes from the 1990s? There's a working archive of them here: http://www.effectgames.com/demos/canvascycle/ They're real masterpieces. 256 colour pixel art, with some colours palette cycling to create animation by using the right pixels in the right place. But then he also changes the palette to create different times of day. Some of them have rain or snow and the particles appear to move smoothly over the various parts of the background. Carefully switching the palette index not just for each bit of movement but also using a different section of the palette each time a snowflake transitions to pass over a different-coloured background layer. There's a pool with a shimmering reflection! There's an animated aurora borealis! There's underwater scenes, oceans, fires, all animated via fixed palette cycling only! And none of it looks forced, it all looks beautifully natural. You can open up the \"Options\" tab to see the palette and hover over the colours to see where they're used. reply stavros 14 minutes agorootparentAhh, I remember this site, thanks for linking. These are magnificent. reply wanderingstan 2 hours agoparentprevImagine going back to Susan Kare's shoes in 1983, having to design icons in black and white in only 32x32 pixels. Made even harder by the fact that people had never encountered icons on a screen before. https://archive.org/details/paint.net-macintosh-icons reply Someone 1 hour agorootparentOn her own site: http://kare.com/apple-icons/ reply keepamovin 1 hour agorootparentprevYes. her designs are truly iconic. So charming. A good icon seems like a good invention, something that has always existed, speaks to you directly and seems to have been recalled rather than created for the first time. I think the nostalgia over some retro OSes is warranted. reply timeon 2 hours agoparentprevThis might sound contradictory, but constraints are boosters for creativity. reply KineticLensman 50 minutes agorootparentYeah! I had to fix a shed roof during one of the lockdowns when DIY stores were closed and amazed myself with what I managed to achieve with just the scraps i had lying around reply yitchelle 46 minutes agorootparentWhen I was younger, a senior told me that If all you have is a hammer to solve the problem, all problems will start to look like a nail. reply larschdk 1 hour agoprevSuch an effective design. I immediately recognize even subtle difference between icons at the lowest sizes. Meanwhile, today I get lost on my Android phone and in most desktop applications with low-contrast and monochrome icons. reply gwervc 57 minutes agoparentFlat and monochrome icons have lost so much expressivity they are basically useless to express what they stand for. A lot of time I had to think for a moment while having the name of a menu written to make the connection between the image and the feature. reply jwmoz 8 minutes agoprevIconic design. Have used mac since around 2012 but would happily use this Windows again. reply jim180 2 hours agoprevThere is a repo[0] with Win98 icons (no complete, I think) in higher resolution. [0] https://www.opencode.net/nestoris/Win98SE reply hlandau 2 hours agoprevStill nice icons. Love the clarity and detailed shading of them at the pixel level, which is not something we see today with \"flat\" vector design. It is kind of amusing how the printer icons when I come to think about it are actually visually referencing 10BASE2/10BASE5. reply Sharlin 50 minutes agoparentFunnily the original Win95 versions of these were flat shaded without gradients, designed with 16-color mode in mind. IIRC these 256-color shaded variants were added in a service pack? Or the Plus pack? And then came with Win98 by default. reply Dwedit 2 hours agoprevIn Windows 95, even if you authored an icon containing more than 16 colors, it would still be displayed as a 16-color icon. (With a strange exception that there were 4 additional colors that could appear in icons, including another gray, a greenish gray, sky blue, and a nearly white color) Then finally Windows 98 updates the shell to actually support full-color icons, and this was a big deal. I think Microsoft Plus for Windows 95 also added this feature as well. reply Lammy 2 hours agoparentYou can do it for free with [HKEY_CURRENT_USER\\Control Panel\\Desktop\\WindowsMetrics] \"Shell Icon BPP\"=\"16\" \"24\" and \"32\" should also work as values if you have a capable adapter+display combination. reply nercury 37 minutes agoprevThat disappearing scrollbar is weirdly out of place. reply Kwpolska 2 hours agoprevSome of those icons are from 2000/Me, such as the round recycle bin with blue arrows, or the computer and network icons with cyan screens. reply zokier 1 hour agoprevThis icon seems oddly out of place among all the others, and I don't recall it from the era.. Does anyone know where it was used? https://win98icons.alexmeub.com/icons/png/computer_2_cool-3.... reply dfox 1 hour agoparentThat is one of the 16bpp high-resolution icons that came with Plus for 95 and IIRC by default with NT4. So the most probable place where you would see that is NT4 desktop (and indeed it looked somewhat out of place). reply wrzlbrmpft 50 minutes agoparentprevWindows 95 Plus? https://oldwindowsicons.tumblr.com/post/655069899824185344/w... reply victorbohr 31 minutes agoprevI'm missing the icons from pifmgr.dll in this list. Always used those to make my shortcuts look cool. reply anilakar 2 hours agoprevSeeing these in 48 * 48 feels uncanny. 32 pixels was the norm back then. reply infotainment 2 hours agoparentI remember feeling like such a power user when I discovered I could set the icons to 48x48 back then. reply xanathar 3 hours agoprevFrom a cursory (icony?) view, it seems to be missing the ones from moricons.dll, but memory is foggy and it might be that moricons.dll had been dropped by win98. It's been 26 years, wouldn't be surprised to find my memory is wrong. reply hnlmorg 3 hours agoparentI’m pretty certain Microsoft wouldn’t have dropped it in Win98. In fact I wouldn’t be surprised if that file is still shipped. Thanks for the trip down memory lane too. It’s been years since I’ve last thought about moricons.dll. Possibly even decades. For those who haven’t heard of it: https://retrocomputing.stackexchange.com/questions/6705/what... Edit: looks like it is still available https://en.m.wikipedia.org/wiki/Moricons.dll reply thegabriele 2 hours agorootparentYep, I still open moreicons.dll everytime I need a fancy icon for a script shortcut reply duskwuff 3 hours agoparentprevMORICONS.DLL is still present in current versions of Windows! But (AFAIK) none of the icons were ever updated after Windows 3.1 to conform to newer design standards, so it's reasonable to leave them out of a set of \"Windows 98\" icons. reply Dwedit 2 hours agorootparentThere are a few MSIE web icons in there too that would not have been in Windows 3.1. reply hooby 2 hours agoprevJust out of curiosity - why are there so many duplicates? Like 4 nearly identical IE icons. 6 very similar PC+Keyboard icons. 6 very similar trashes? reply seba_dos1 2 hours agoparentMost of them are just variations that would be used with different display color modes, as 24-bit color displays weren't ubiquitous yet back then. There are also some icons there from later versions than 98. reply anilakar 2 hours agoparentprevThey are from earlier releases. Microsoft likes to keep old features around forever for compatibility, so 98 has icons from 95 and 95 OSR2. reply seba_dos1 1 hour agorootparentTo the contrary, this set includes mostly icons from 98, but there are also some from 2000/Me, plus also some icons with \"cool\" in their filename that I don't know where they're from. I don't think there are icons from earlier releases there. [edit] Apparently the \"cool\" ones are from 98 beta builds (1351-1569) that didn't end up being ever used in actual release. reply nsjames 1 hour agoprevI can't believe how good it feels to navigate this UI. I'm not even sure if it's just nostalgic. The UX was damn good. reply LeoPanthera 1 hour agoprevPersonally, I always slightly preferred the 16-color Windows 95 icons. Something about them made them a little cleaner, clearer, and quicker to identify to me. reply dheera 2 hours agoprevIt's funny how that shade of teal green went super out of fashion for many years but it's now a \"hot\" color again on Android material design apps. reply p0w3n3d 2 hours agoparentthe same with fashion all around reply Dalewyn 3 hours agoprevI miss when icons were still useful descriptors. reply taspeotis 2 hours agoprevRemember when they followed up with Visual Studio 2012? reply ideasphere 2 hours agoprev‘Tree’ has always been my go-to. How about you? reply akx 3 hours agoprevRelated: I've written a thing that can automate extracting various resources (icons, cursors, bitmaps, sounds) from vintage Windowses: https://github.com/akx/res-extract ... originally because someone needed the Excel 95 icon as a Slack emoji, naturally. reply HenryBemis 1 hour agoprevThank you!!! reply mouzogu 1 hour agoprev [–] the lack of font and icon smoothing in old windows was always off-putting apple and linux fonts/icons always looked much nicer to me reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The Windows 98 Icon Viewer, developed by Alex Meub, enables users to browse and download all Windows 98 icons, such as those for the Recycle Bin, Computer, and Documents.",
      "It offers a comprehensive collection of icons associated with Windows 98 for users to explore and download.",
      "This tool provides a nostalgic journey for users interested in the iconic visuals of the Windows 98 operating system."
    ],
    "commentSummary": [
      "The discussion delves into nostalgia for iconic design elements in older operating systems like Windows 98, Classic Mac OS, and Redhat Linux's Bluecurve theme, focusing on icon design evolution and timeless symbols.",
      "Participants appreciate Susan Kare's early pixel art designs for creativity and detail, discussing the technical aspects and nostalgia of icon design in vintage operating systems.",
      "Mark Ferrari's animated pixel art landscapes from the 1990s are also admired, highlighting the complexities of creating pixel art in that era."
    ],
    "points": 158,
    "commentCount": 69,
    "retryCount": 0,
    "time": 1711347758
  },
  {
    "id": 39811604,
    "title": "Author Develops Format Dialog for Windows NT",
    "originLink": "https://twitter.com/davepl1968/status/1772042158046146792",
    "originBody": "I wrote this Format dialog back on a rainy Thursday morning at Microsoft in late 1994, I think it was.We were porting the bajillion lines of code from the Windows95 user interface over to NT, and Format was just one of those areas where WindowsNT was different enough from… pic.twitter.com/PbrhQe0n3K— Dave W Plummer (@davepl1968) March 24, 2024",
    "commentLink": "https://news.ycombinator.com/item?id=39811604",
    "commentBody": "The Format Dialog in Windows NT (twitter.com/davepl1968)154 points by develatio 10 hours agohidepastfavorite63 comments bc_programming 6 hours agoI was able to confirm in the Windows NT4 source code that he originally wrote some of the code for the format dialog on 2-13-95. That much is true. (late 1994, early 1995, close enough) >\"I also had to decide how much 'cluster slack' would be too much, and that wound up constraining the format size of a FAT volume to 32GB.\" NT4 didn't support FAT32, and NT4 actually was actually able to be 4GB rather than 2GB for a FAT volume because NT4 allowed 64K clusters, so actually exceeded what most systems were able to do at the time. Formatting as FAT in NT4 had no cluster check or option. The cluster size used was decided based on the size of the volume. Furthermore, the The 32GB limitation for FAT32 volumes was originally in the internal format functions, not the dialog itself. On Windows 2000 (Which does support FAT32) you can try to format a drive bigger than 32GB as FAT32, but the formatting will fail, as it is hard-coded at the end of the format to fail trying to format FAT32 volumes larger than 32GB. The dialog itself isn't what presents this limitation and it is shared by the command line format.com which uses the same functions. Not sure why he seems to always exaggerate his own involvement. He's got people believing that he wrote the Zip folder code that Microsoft literally licensed from Info-Zip because he had to touch it to get it integrated. I guess exaggeration is what \"influencers\" do, and that's what he is at least trying to be now. reply rsweeney21 5 hours agoparentI owned zip folders for a while during the windows Vista days. I could have sworn the code was originally purchased, not written by someone at Microsoft. Honestly, it looked like it had been run through an obfuscator. I assumed that the original author had done that to ensure it was difficult for Microsoft to make changes/improvements. I still have nightmares about that code. :) reply aargh_aargh 5 minutes agorootparentI may be misremembering, but Dave has spoken on his Dave's Garage Youtube channel about running a separate software business selling small utilities and later selling some of them to Microsoft. This might have been one of them. A quick search confirms that he also claims that here: https://old.reddit.com/r/IAmA/comments/kfpjhg/i_am_dave_plum... reply shp0ngle 5 hours agorootparentprevIt was purchased. reply chungy 3 hours agoparentprevMemory is a fickle thing, but the format dialog enforcing a FAT32 limit is probably Dave's biggest failing when it comes to telling old stories. I don't know if FAT32 was in development in late 1994, it's possible, but it sure didn't ship in Windows NT 4, nor the original Windows 95. Even when it did land in Windows 95 OSR2, the format command happily accepted partitions up to 128GiB; but okay, Windows 95 isn't NT. Windows 2000's internal formatting functions appear to be the real reason FAT32 is limited to 32GiB on new formats. The GUI, format command, and diskpart are all equally incapable of creating a >32GiB file system. Why? Who knows, it's not like drives of that size or larger didn't already exist at the time. If you use, say, mkdosfs on Linux, the VFAT driver in Windows 2000+ will take volumes up to 2TiB, you can even install Windows 2000 on such large volumes. reply chungy 3 hours agorootparentAs an aside, and connected to the failing memory thing, Raymond Chen has repeated multiple times that Space Cadet Pinball couldn't get working on a 64-bit build and never shipped with any version of Windows, and that's why it was gone in Vista. Windows XP Professional x64 Edition has a working 64-bit native build of Pinball. Well, it was a game from 1995 and wouldn't fit in with Vista's style revamp. Even the old Minesweeper and Solitaire games became Direct3D accelerated in Vista. Maybe the effort to do the same overhaul to Pinball was discarded, but the idea that a 64-bit native build couldn't get working is absurd when the previous Windows version included the very thing. reply jborean93 2 hours agorootparentIn his defence he has clarified this was for the Alpha AXP 64bit build that had the problem https://devblogs.microsoft.com/oldnewthing/20220106-00/?p=10.... reply Kwpolska 2 hours agorootparentprevThere were issues with Pinball on \"Windows XP 64-bit Edition\", or the Itanium version. A great video explaining this is https://youtube.com/watch?v=3EPTfOTC4Jw reply Kwpolska 2 hours agoparentprevDave's contributions are quite mediocre, yet he makes himself sound like a programming god... He built this crude format dialog, the awfully slow and limited ZIP integration, the old ugly task manager, and Windows XP activation. Not exactly the Windows features to be proud of. reply dmw_ng 6 minutes agorootparentI didn't read heroics in the above tweet and I'm more than a little confused about the sour grapes here. Is it too much to forgive inaccuracies in a 29 year old memory for such a low stakes issue, especially when there is a huge benefit to giving a personality to some ancient OS history? It wouldn't matter if he claimed his 5.25\" drive shot lightning bolts the first time he tried to compile that dialog, the stories are still great and it's personally enough for me having grown up with these systems to hear them told (not much different from listening to a pseudo-senile grandparent exaggerate war stories for entertainment's sake) reply Maxious 6 hours agoparentprev\"vzip150.zip\" credited to plummer@visualzip.com doesn't mention anything about info-zip https://www.sac.sk/files.php?d=7&l=V Where did the covette come from then? https://www.tomshardware.com/software/windows/dev-shows-off-... reply Lammy 5 hours agorootparentIt used InnerMedia DynaZIP. Source: Extracted VZIP150's InstallShield `data.z`, saw `DZIP32.DLL`, and `DUNZIP32.DLL`, and confirmed with `strings` on `Vzip.dll`. http://dynazip.com/ confirms “DynaZip technology is used by Microsoft Corp. and for many years it has been directly incorporated into the Desktop and Server versions of the Windows Operating Systems. DynaZip technology implements the compression engine behind the Windows Zip Folder user interface which allows users to view, extract-from and create ZIP files that are managed as compressed folders.” https://web.archive.org/web/19961130210204/http://www.innerm... Version 3 https://web.archive.org/web/20000226025901/http://www.dynazi... Version 4 reply EMIRELADERO 4 hours agoparentprevFor reference: https://github.com/lianthony/NT4.0/blob/b4a8d373d8a082db6758... reply josephcsible 2 hours agorootparentWarning: don't look if you ever want to contribute to Wine or ReactOS reply EMIRELADERO 1 hour agorootparentThat's just a myth. ReactOS routinely uses IDA to inspect actual Windows binary logic. reply jeroenhd 3 hours agoparentprevI think he's misremembering about the format part because the UI does have some arbitrary cluster sizes for NTFS: https://github.com/lianthony/NT4.0/blob/b4a8d373d8a082db6758... That code still seemed to be around in (some versions of) XP: https://github.com/tongzx/nt5src/blob/daad8a087a4e75422ec96b... As for the ZIP support, I can't find the source code for ZIP folders specifically. There's this excerpt from another company (Schlumberger Technology Corp.): https://github.com/tongzx/nt5src/blob/daad8a087a4e75422ec96b... which was added in 1996 if the comments are to be believed. reply wannacboatmovie 6 hours agoparentprevI thought that sounded dubious. The 32GB limit is also enforced in diskpart.exe (for the Windows-uninitiated, a console tool for disk manipulation) which would mean it's decoupled from any UI code. In other words, the UI is not what is enforcing the limit OS-wide. reply Dalewyn 5 hours agoparentprevIt would be nice if you can cite credentials, if possible; not everyone has/had access to Windows NT source code and Dave for his part provided his credentials. Not that I am discounting your claim, but from a cursory glance through your comment history you're practically a nobody with a seeming vendetta against someone who credibly had a part in much of Windows NT's innards. reply bc_programming 2 hours agorootparentFair enough. As others have noted the source code was leaked; I've got NT4, NT5, XP, XPSP1 and Server 2003 codebases to review from that, which I downloaded a good while ago. I've been a software developer primarily targeting Windows for 20 years, 10 of those professionally. I was a Microsoft MVP for half of those (2012-2016 inclusive). The reference to the date is found in the header of nt4\\private\\windows\\shell\\shelldll\\unicode\\format.c The 32GB limit for FAT32 doesn't appear to be handled by the shell code or the dialog, so when I said it was handled internally to the formatting code, I couldn't actually find that. It's just somewhere loweer than the UI. What I did do howeever was boot up Windows 2000, the first version that did have FAT32, and there's no limitation in the format dialog itself. It goes through the entire process and then gives a \"Volume too big\" error after it has gone through the process, an error shared by not just the format dialog, but diskpart, disk management, and format, which certainly suggests that that is happening at some lower level. The dialog in question does just call into other functions to perform the actual format, but I wasn't able to find the actual source files for it. >Dave for his part provided his credentials. Now he understandably does leave out a bit of info about his history which you could arguably say is part of my \"vendetta\" that you observed. The reason for the proportion of comments is less a vendetta and more that I get kind of worked up every time stuff of his is posted and people applaud how great he is. He's no Dave Cutler but he certainly talks like he was. Now, as to the tidbit. Dave Plummer ran a scam company that was sued by Washington State in 2006, \"SoftwareOnline.com, Inc. \". He actually left Microsoft specifically to run this company. Court documents can be seen here: https://www.atg.wa.gov/news/news-releases/attorney-general-s... You can find David W. Plummer listed in the court complaint. The short of it is that it was an online software scam company that tricked people into downloading fake Anti-virus and security software using online ads, and then the software delivered additional adware and nagware onto users machines. That is why it may appear I have a vendetta. I don't trust a word he says and especially when what he says directly contradicts other sources. Mistakes in memory are one thing, but some of them are rather beyond the sort of thing I think is reasonable. Paired with his history, I'm convinced he's actually lying intentionally because he's trying to build a \"following\" and \"Dave's Garage\" is just his latest scam. That's why he keeps \"coming forward\" in posts and his youtube as writing this or doing that. What bothers me is that it's working, because most people don't seem to even question it, even where there are rather severe contradictions. reply mappu 5 hours agorootparentprevNT 4 and Windows 2000 source code were leaked on Feb 12 2004. The second google result is a copy that Microsoft are hosting on their own website: https://github.com/lianthony/NT4.0 reply Dalewyn 4 hours agorootparentIn that case, parent commenter or someone sufficiently motivated should cite file and line number to corroborate the claim. (I'm not going to do it because I'm drunk and I probably have better things to do than illicitly look through Microsoft source code.) reply ikari_pl 5 hours agorootparentprevI do remember there was a leak of Windows 2000 source code years ago. I downloaded it only to search for swear words. The most offensive thing I've ever found was things like \"workaround because some idiots call\" reply Solvency 5 hours agoparentprevDude this guy is desperately trying to avoid fading into oblivion and is clinging to a little ancient format window to remain relevant in public eye. Have some mercy and let him have his 32gb moment. reply exe34 2 hours agorootparentMemory is an interesting thing. I once claimed to somebody I was seeing that I had done/said something humorous, fully believing that it was my own experience. It was only when he mentioned hearing that joke from a famous standup comedian that it really hit me - I think what really happened was that I watched a clip a long time ago, and thought hey that's the kind of thing I'd have done - and then proceeded to record the memory as mine, even going as far as attaching it to a specific shop from the same chain in the story. reply m_sahaf 2 hours agorootparentOnce famous incident of this effect is reported by Oliver Sacks, where he confidently narrated a story that presumably happened to him during the war until his brother corrected him: https://www.wired.com/2015/08/fully-immersive-mind-oliver-sa... reply accrual 9 hours agoprevDave Plummer is such a cool guy. I love how casually he writes about building one of the most ubiquitous kernels of all time. > He created the Task Manager for Windows, the Space Cadet Pinball ports to Windows NT, Zip file support for Windows, HyperCache[4] for the Amiga and many other software products. He has been issued six patents in the software engineering space. https://en.wikipedia.org/wiki/Dave_Plummer reply jonny_eh 5 hours agoparentThis guy claims Plummer is full of it: https://news.ycombinator.com/item?id=39812561 reply akx 3 hours agorootparentAnd downthread from there, that guy's claim of \"literally licensed from Info-Zip\" is debunked. https://news.ycombinator.com/item?id=39812860 reply Kwpolska 1 hour agorootparentThe OP did get the original authors wrong, but it's still a third-party implementation, and Dave's only contribution is the integration with Windows Explorer. reply amanzi 9 hours agoparentprevHis YouTube channel is great too! https://www.youtube.com/@DavesGarage reply hoistbypetard 7 hours agorootparentYes! It always makes me smile when the algorithm tells me to watch something from there. reply cap10morgan 6 hours agoprevThis highlights a common pitfall: If you “solve” a problem with a “temporary” solution, you lower the priority of the better solution below every unsolved problem. And there are always enough of those to ensure no one ever revisits the temporary solutions. reply Sander_Marechal 57 minutes agoparent\"There is nothing more permanent than a temporary solution that works\" I have no idea where I heard that, but I use it often at work to ensure we don't ship temporary solutions but do it right the first time. reply omegabravo 50 minutes agoparentprevI disagree, I consider that an absolute win of efficient engineering. Develop the feature enough that it lasts 30 years without needing to be fixed. reply sverhagen 11 minutes agorootparentIt has to be good enough. Was it? You could argue it was, because it survived that long. But it wasn't a standalone product, rather part of Windows, which was a (commercially) successful product because it was good or great enough. Some parts were better, some were worse. While not completely and utterly broken, I think the suggestion here is that the Format dialog fell in the \"worse\" camp. So, I'm not saying that it was okay because there were more important things to fix (that was true too, probably), but I'm saying that it was okay because there were enough equally important things that were done better/well enough. reply jonny_eh 5 hours agoparentprevI've noticed this in my career. The lesson is to not \"ship\" something until you're proud of it. Sometimes that's easier said than done though. reply ranger207 6 hours agoprev> I got out a piece of paper and wrote down all the options and choices you could make with respect to formatting a disk, like filesystem, label, cluster size, compression, encryption, and so on. > ... > It wasn't elegant, but it would do until the elegant UI arrived. This doesn't follow IMO. It presents all the options you need in a simple interface; what would go into a more \"elegant\" interface? reply jameshart 5 hours agoparentAre you... a programmer by any chance? reply TillE 5 hours agorootparentTo improve things, we need to talk about common user scenarios. Problem is, normal users will approximately never have to format a disk, and it's a perfectly adequate interface for nerds. If this were the 90s and everyone was still using floppy disks, yeah it would be worth making it slightly more user-friendly. reply Dalewyn 4 hours agorootparentThis thing literally is from the 90s when everyone was still using floppy disks. reply omegabravo 1 hour agorootparentThis interface was perfectly acceptable in the 90s, and it's perfectly acceptable today. Dropping a big graphic with a large circle button labelled format would have been worse. I say this as someone who writes my fair share of aesthetic user interfaces. My takeaway is different. If your \"temporary code\" is sufficient to solve the problem, then it may last 30 years. If it was an actual problem, it would have been fixed by now. reply seoulmetro 3 hours agorootparentprevI'm a programmer and I can't even stand the formatting on Linux distros. Windows was usable even as a kid. reply exe34 2 hours agorootparentWhat's wrong with mkfs.[select-middle-click] /dev/sdc1? reply ZoomZoomZoom 7 minutes agorootparentThe fact that you need to invoke lsblk/blkid/mount first to be sure what you're about to erase forever, then possibly sync and unmount, and only then format. Keeping the same target device at every step is completely on user and the price of a typo is huge. I love (and usually prefer) the Unix way, but it's as dangerous as it's powerful and the amount of the required prior knowledge is incomparable. reply Lammy 9 hours agoprev> the dialog is still my temporary one from that Thursday morning, so be careful about checking in \"temporary\" solutions! Not only still there but inspired others, like the HP USB Disk Storage Format Tool that I always used to bypass the 32GB FAT32 limit: https://www.majorgeeks.com/files/details/hp_usb_disk_storage... reply accrual 9 hours agoparentIt's a useful tool! I used it to create a 128GB FAT32 flash drive for sharing my games collection between retro PCs. reply wlesieutre 9 hours agoprevIt’s perfect, I hope they don’t replace it with an “elegant UI” reply zamadatix 8 hours agoparentIt's \"replaced\" already under Settings -> Storage -> Disks & volumes. When doing it through File Explorer the old one gets called because File Explorer is still (largely) a legacy style app. I actually quite like the new UI as far as the replacement of the format dialog but the portions that replace the Disk Manager functionality are a bit lacking because it doesn't have the graphical representation of the disk layouts. Default view: https://i.imgur.com/56yZ8gZ.png Advanced view section: https://i.imgur.com/fKb3R8c.png Formatting an existing partition: https://i.imgur.com/DmbX3FQ.png It's not perfect but it does largely retain the same layout while matching with the OS theme and adds a few options that used to be multi paged wizards in the disk manager. Minor things like \"format\" is a bit more direct than \"start\" and \"cancel\" a bit more apt than \"close\" sneak in as well. I'm sure it being 33% wider will be the end of the world to someone though. One thing I think I'm a bit split on is it still expects you to enter new partition sizes in MB. It does at least add localized number separators but it seems a bit silly to worry about how many millions of megabytes given the numbers will only grow with time for here. reply thombles 9 hours agoparentprevFormat needs an update. We are getting the update ready for you . . . reply Nextgrid 9 hours agorootparentDon't forget to leave a review and send feedback (network-attached /dev/null). reply nine_k 8 hours agoprevI'd say that there are no temporary and permanent solutions, but there are bad and good solutions. A good solution, even if implemented quickly, will stick, because it's good (enough). A bad solution can also stick, if it's the only one available / viable for some time, and then everything else has to be backwards-compatible to it. reply chilling 8 hours agoprevI worked at one of the biggest and oldest banks in Europe, and our production was often filled with these 'temporary solutions.' They were typically created as quick fixes for major issues resulting from previous temporary fixes. reply userbinator 7 hours agoprevI remember seeing a patch floating around that removed the 32GB limit; can't find it now but this was back when XP was the most popular OS. reply asveikau 7 hours agoparentThe mkfs or newfs tools on any Linux or *BSD don't have the limit. The Windows kernel also does fine on larger volumes formatted on a free OS. I do this all the time for SD cards. (Because who wants exfat?) reply chungy 3 hours agorootparentWho doesn't want exFAT? It's all around better. reply asveikau 3 hours agorootparentIt's not as universally supported and it's patent encumbered. reply chungy 2 hours agorootparentIf you have some random set-top box or camera that doesn't support exFAT, fair enough, but the situations are becoming few and far between. Windows, Mac, Linux all have native support for it. No (practical) volume size or file size limits is a huge win for exFAT over FAT32. reply giorgosts 4 hours agorootparentprevdoes fat32 support larger files than 4GB, common for video files shared with USB or SD cards? reply asveikau 3 hours agorootparentNo. A lot of software will split files for this use case. reply jaydeegee 6 hours agoprevAbout 14years ago in my first week at a new job I cobbled some code together to query our ticketing system (BMC Remedy) as a sort of quasi queue monitor. It's had a few cosmetic iterations but its still running the same backend code. reply xerox13ster 4 hours agoparentThis name has been popping into my head a lot recently. I hated it a decade ago at Walmart and it was a right PITA, but I would absolutely KILL to be using BMC Remedy at this point. I hate Salesforce for tech work and support tickets. It is the worst experience possible. Remedy was Remedy and it was obscure and weird and hard to use at times. But I swear to god Salesforce is going to cost me my job becuase it makes it impossible to do my job well. Why can't I just open one ticket in one browser tab without 15 other tickets popping into a second tab bar? Why does it NEEEEEED to hide whatever other tab I was working on in a drop down, why can't I sort, filter, rearrange, or resize anything? IT'S BRAIN BREAKING. Anyway thanks for reading this rant. The idea of running any sort of query or script in Remedy sounds like water in the desert to me now. I'd settle for using service-now to track things. Anything but Salesforce dammit. reply Dalewyn 9 hours agoprev [–] As the old saying goes: There is nothing as permanent as a temporary solution. reply cm2187 7 hours agoparent [–] I think that was about taxes! reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The author developed a Format dialog for Microsoft in late 1994 while transitioning code from Windows95 to WindowsNT."
    ],
    "commentSummary": [
      "The discussion encompasses a wide range of topics, including operating systems, file systems, software development, and engineering, touching on the history and constraints of Windows NT and FAT32.",
      "Conversations involve debates on the reputation of figures like Dave Plummer, temporary programming solutions, and user-friendly interfaces, where users express their preferences and issues with formatting techniques and tools.",
      "Anecdotes about past tech industry encounters and challenges with ticketing platforms such as Salesforce are also shared during the discussion."
    ],
    "points": 154,
    "commentCount": 64,
    "retryCount": 0,
    "time": 1711324323
  },
  {
    "id": 39811155,
    "title": "Study: Language Models' Abilities Develop Predictably",
    "originLink": "https://www.quantamagazine.org/how-quickly-do-large-language-models-learn-unexpected-skills-20240213/",
    "originBody": "How Quickly Do Large Language Models Learn Unexpected Skills? Read Later Share Copied! Comments Read Later Read Later artificial intelligence How Quickly Do Large Language Models Learn Unexpected Skills? By Stephen Ornes February 13, 2024 A new study suggests that so-called emergent abilities actually develop gradually and predictably, depending on how you measure them. Read Later Kristina Armitage/Quanta Magazine By Stephen Ornes Contributing Writer February 13, 2024 View PDF/Print Mode Abstractions blogartificial intelligencecomputer sciencemachine learningnatural language processingneural networksAll topics Introduction Two years ago, in a project called the Beyond the Imitation Game benchmark, or BIG-bench, 450 researchers compiled a list of 204 tasks designed to test the capabilities of large language models, which power chatbots like ChatGPT. On most tasks, performance improved predictably and smoothly as the models scaled up — the larger the model, the better it got. But with other tasks, the jump in ability wasn’t smooth. The performance remained near zero for a while, then performance jumped. Other studies found similar leaps in ability. The authors described this as “breakthrough” behavior; other researchers have likened it to a phase transition in physics, like when liquid water freezes into ice. In a paper published in August 2022, researchers noted that these behaviors are not only surprising but unpredictable, and that they should inform the evolving conversations around AI safety, potential and risk. They called the abilities “emergent,” a word that describes collective behaviors that only appear once a system reaches a high level of complexity. But things may not be so simple. A new paper by a trio of researchers at Stanford University posits that the sudden appearance of these abilities is just a consequence of the way researchers measure the LLM’s performance. The abilities, they argue, are neither unpredictable nor sudden. “The transition is much more predictable than people give it credit for,” said Sanmi Koyejo, a computer scientist at Stanford and the paper’s senior author. “Strong claims of emergence have as much to do with the way we choose to measure as they do with what the models are doing.” Abstractions navigates promising ideas in science and mathematics. Journey with us and join the conversation. See all Abstractions blog We’re only now seeing and studying this behavior because of how large these models have become. Large language models train by analyzing enormous datasets of text — words from online sources including books, web searches and Wikipedia — and finding links between words that often appear together. The size is measured in terms of parameters, roughly analogous to all the ways that words can be connected. The more parameters, the more connections an LLM can find. GPT-2 had 1.5 billion parameters, while GPT-3.5, the LLM that powers ChatGPT, uses 350 billion. GPT-4, which debuted in March 2023 and now underlies Microsoft Copilot, reportedly uses 1.75 trillion. That rapid growth has brought an astonishing surge in performance and efficacy, and no one is disputing that large enough LLMs can complete tasks that smaller models can’t, including ones for which they weren’t trained. The trio at Stanford who cast emergence as a “mirage” recognize that LLMs become more effective as they scale up; in fact, the added complexity of larger models should make it possible to get better at more difficult and diverse problems. But they argue that whether this improvement looks smooth and predictable or jagged and sharp results from the choice of metric — or even a paucity of test examples — rather than the model’s inner workings. Merrill Sherman/Quanta Magazine Three-digit addition offers an example. In the 2022 BIG-bench study, researchers reported that with fewer parameters, both GPT-3 and another LLM named LAMDA failed to accurately complete addition problems. However, when GPT-3 trained using 13 billion parameters, its ability changed as if with the flip of a switch. Suddenly, it could add — and LAMDA could, too, at 68 billion parameters. This suggests that the ability to add emerges at a certain threshold. But the Stanford researchers point out that the LLMs were judged only on accuracy: Either they could do it perfectly, or they couldn’t. So even if an LLM predicted most of the digits correctly, it failed. That didn’t seem right. If you’re calculating 100 plus 278, then 376 seems like a much more accurate answer than, say, −9.34. So instead, Koyejo and his collaborators tested the same task using a metric that awards partial credit. “We can ask: How well does it predict the first digit? Then the second? Then the third?” he said. Koyejo credits the idea for the new work to his graduate student Rylan Schaeffer, who he said noticed that an LLM’s performance seems to change with how its ability is measured. Together with Brando Miranda, another Stanford graduate student, they chose new metrics showing that as parameters increased, the LLMs predicted an increasingly correct sequence of digits in addition problems. This suggests that the ability to add isn’t emergent — meaning that it undergoes a sudden, unpredictable jump — but gradual and predictable. They find that with a different measuring stick, emergence vanishes. Share this article Copied! Newsletter Get Quanta Magazine delivered to your inbox Subscribe now Recent newsletters Brando Miranda (left), Sanmi Koyejo and Rylan Schaeffer (not pictured) have suggested that the “emergent” abilities of large language models are both predictable and gradual. Kris Brewer; Ananya Navale Introduction But other scientists point out that the work doesn’t fully dispel the notion of emergence. For example, the trio’s paper doesn’t explain how to predict when metrics, or which ones, will show abrupt improvement in an LLM, said Tianshi Li, a computer scientist at Northeastern University. “So in that sense, these abilities are still unpredictable,” she said. Others, such as Jason Wei, a computer scientist now at OpenAI who has compiled a list of emergent abilities and was an author on the BIG-bench paper, have argued that the earlier reports of emergence were sound because for abilities like arithmetic, the right answer really is all that matters. “There’s definitely an interesting conversation to be had here,” said Alex Tamkin, a research scientist at the AI startup Anthropic. The new paper deftly breaks down multistep tasks to recognize the contributions of individual components, he said. “But this is not the full story. We can’t say that all of these jumps are a mirage. I still think the literature shows that even when you have one-step predictions or use continuous metrics, you still have discontinuities, and as you increase the size of your model, you can still see it getting better in a jump-like fashion.” Related: New Theory Suggests Chatbots Can Understand Text The Unpredictable Abilities Emerging From Large AI Models To Teach Computers Math, Researchers Merge AI Approaches Will Transformers Take Over Artificial Intelligence? And even if emergence in today’s LLMs can be explained away by different measuring tools, it’s likely that won’t be the case for tomorrow’s larger, more complicated LLMs. “When we grow LLMs to the next level, inevitably they will borrow knowledge from other tasks and other models,” said Xia “Ben” Hu, a computer scientist at Rice University. This evolving consideration of emergence isn’t just an abstract question for researchers to consider. For Tamkin, it speaks directly to ongoing efforts to predict how LLMs will behave. “These technologies are so broad and so applicable,” he said. “I would hope that the community uses this as a jumping-off point as a continued emphasis on how important it is to build a science of prediction for these things. How do we not get surprised by the next generation of models?” By Stephen Ornes Contributing Writer February 13, 2024 View PDF/Print Mode Abstractions blogartificial intelligencecomputer sciencemachine learningnatural language processingneural networksAll topics Share this article Copied! Newsletter Get Quanta Magazine delivered to your inbox Subscribe now Recent newsletters The Quanta Newsletter Get highlights of the most important news delivered to your email inbox Email Subscribe Recent newsletters Comment on this article Quanta Magazine moderates comments to facilitate an informed, substantive, civil conversation. Abusive, profane, self-promotional, misleading, incoherent or off-topic comments will be rejected. Moderators are staffed during regular business hours (New York time) and can only accept comments written in English. Show comments Next article To See Black Holes in Stunning Detail, She Uses ‘Echoes’ Like a Bat",
    "commentLink": "https://news.ycombinator.com/item?id=39811155",
    "commentBody": "“Emergent” abilities in LLMs actually develop gradually and predictably – study (quantamagazine.org)152 points by Anon84 11 hours agohidepastfavorite78 comments Satam 1 minute agoSo the models might be getting smarter without discontinuous jumps. And it could just be that we're measuring them in a way that gives no credit for partial answers, thus we end up missing the signs that they've been getting continuously sharper all along. This sounds kind of in line with what I gather Sam Altman's thinking is. They feel they can predict a model's reasoning abilities quite well based just on the size of the training compute and data. reply a_wild_dandan 1 hour agoprevThere are several issues with the study: 1. Replacing pass/fail accuracy with smoother alternatives (e.g token edit distance) could be a terrible proxy for skill, depending on the task. 2. Even by the authors' metrics, they _still_ find a few potentially emergent abilities. 3. Hindsight is 20-20. Yes, we can revisit the data and fiddle until we find transforms that erase emergence from aptitude plots. The fact is, folk used commonplace test accuracy measurements, and the results were unpredictable and surprising. That's the true notable phenomenon. I think there's value in the paper. Just...don't take its conclusions too far. reply Gisbitus 56 minutes agoparentJust like it's mentioned later in the article: it doesn't really matter if you get an addition mostly right. You either get it right or you don't. I still appreciate their effort though, because even after altering the grading system, there were still some emergent abilities. reply IanCal 22 minutes agorootparent> it doesn't really matter if you get an addition mostly right. This is definitely not true in the real world. Approximate solutions are often good enough to answer a question. reply RandomLensman 16 minutes agorootparentBut then it isn't \"addition\" but approximating \"addition\". reply jonplackett 34 minutes agoprev> But with other tasks, the jump in ability wasn’t smooth. The performance remained near zero for a while, then performance jumped. Second paragraph completely contradicts the title given. reply esafak 8 hours agoprevThe paper: Are Emergent Abilities of Large Language Models a Mirage? https://arxiv.org/abs/2304.15004 reply tambourine_man 7 hours agoprev> They find that with a different measuring stick, emergence vanishes. Isn’t that the case for most/all emergent behavior? If you change the scale and watch individual water molecules, you’d see them snapping into a crystal structure one by one instead of a sudden emergent block of ice. reply godelski 7 hours agoparentNot quite. The problem is that the definition is especially poorly defined in ML. I elaborate more here[0]. You are describing emergence, but not what was claimed when LLMs were said to have emergent abilities (the distinction is explained in the article fwiw) [0] https://news.ycombinator.com/item?id=39812315 reply n2d4 3 hours agoparentprevBut knowing the molecule structures at 50C and 75C tells you very little about the freezing point. Different example: If you measure the number of cases of a given virus, it will either spread across the entire globe (R0 > 1, eg. COVID-19) or fail to spread widely (R0implying an upper bound on any dependent phenomenon the implication sounds good, but is easily rejected with a counter example: a good student can surpass his master. A mediorcre student with multiple masters can surpass all of them (tangent: this is why the medival master-journeyman system was so efficient, I think). Or to make the argument more abstract: Your implication seems to assume, that transferlearning does not exist. reply FeepingCreature 2 hours agoparentprevYou're assuming that humans reliably pursue correctness rather than plausibility. The whole scientific system specifically being designed to suppress plausible-sounding-but-incorrect claims says otherwise. reply happytiger 32 minutes agorootparentThat’s an astute observation. reply sigsergv 7 hours agoprevIt's really interesting question how can we measure emergent abilities like arithmetic operations. We cannot test every operation on every possible combinations of numbers. Instead we must make sure somehow that LLM performing arithmetic operations using corresponding rules and axioms. reply sfink 3 hours agoparentWhy would it do that? Rules and axioms scale (slowly) with the number of layers. The model can heuristically approximate more easily and more incrementally. reply lewhoo 2 hours agorootparentBut in this case why would you prefer approximation over answer ? reply eropple 6 hours agoparentprev> Instead we must make sure somehow that LLM performing arithmetic operations using corresponding rules and axioms. It isn't. It's stringing together likely tokens that approximate (often very effectively!) what its data corpus has done in the past. And, relatedly, the best way I've found GPT4 to solve a word problem is to tell it to write some Python code to spit out an answer; the actual computation part is an easier thing to figure out when it's just running code. reply scarface_74 3 hours agorootparentA very simple example “list the Presidents in the order they were born”. It gets the order wrong unless you tell it to “use Python” https://chat.openai.com/share/4a673ea0-67d3-4256-b57d-dc1cf8... reply chmod775 1 hour agorootparentMy favorite example is telling it to reverse some longer text character-by-character. Trivial for any human to perform perfectly, but all models I've tested struggle with it and make mistakes all over. It's really hard for them because they lack hidden state to perform algorithms - or what you would call thought in a human. Instead (essentially) for each step they have to re-consider the entire thing and their past output, figure out what they already did, and what they have to do next. On the other hand they'll spit out python code that'll get you the reversed text just fine. It's also one of their greatest shortcomings when it comes to coding: They lack the ability to do any meaningful symbolical execution. https://chat.openai.com/share/9faaae97-e20f-454e-b245-3e4c19... reply exe34 2 hours agorootparentprevDoes python involve calling \"get_us_presidents()\"? reply maxcoder4 2 hours agorootparentYou can check the code it generated in the long OP provided (this button is not very visible so I understand if you missed it). reply p1esk 10 hours agoprevThis is a good paper. Though emergence doesn’t necessarily require a sudden jump in metrics, or unpredictability. New abilities can emerge gradually. reply n2d4 4 hours agoparentWhen we talk about \"emergence\" in machine learning, we talk about those metrics with a sudden jump, as explained in the paper that introduced the term: https://arxiv.org/abs/2206.07682 reply exe34 2 hours agorootparentJust because somebody overloaded a word doesn't mean we can't correct people who use it wrong. The correct phrase should be \"sudden/unexpected emergence\". Otherwise the phrase \"gradual emergence\" would be an oxymoron. reply ekez 5 hours agoprevThe metric the authors use confuses me. Edit distance seems like a strange way to test if the model understands arithmetic ([1], Figure 3). I think `1+3=3` would be equally as correct as `1+1=9`? Why not consider how far off the model is `abs(actual-expected)`? I wonder if there is an inflection point with that metric. https://arxiv.org/abs/2206.07682 reply n2d4 4 hours agoparentIt depends on how you do arithmetic. If you're a human and you do column addition, 12345+35791=58136 is just as big of a mistake as 48146 (the actual result is 48136). It's just one mistaken column in both. Binary half-adders work the same way. We don't really know how LLMs do arithmetic. Maybe token edit distance would be interesting, but either way it doesn't really change the claim of the paper. Unrelated: The link is incorrect, the one you're referring to is here: https://arxiv.org/pdf/2304.15004.pdf reply hatenberg 19 minutes agoprevPhase changes are a natural pattern in complex systems. See state transition in fluids. Depending on how you measure, they look instant or gradual reply dataking 9 hours agoprevhttps://archive.is/Z0EOB reply ambicapter 7 hours agoprevThis is very insightful. Another one of those \"obvious in retrospect\" ideas. We tend to think of addition as only correct or not but has anyone ever claimed that LLMs have a \"discrete\" output (might be using the wrong terminology)? It would then make sense that you need to measure performance in a continuous, not discrete way. Otherwise you'll end up with a sort of \"aliasing\"-type error. reply woopsn 3 hours agoparentNot quite in retrospect -- the earlier paper on emergence addresses this > It is also important to consider the evaluation metrics used to measure emergent abilities. For instance, using exact string match as the evaluation metric for long-sequence targets may disguise compounding incremental improvements as emergence. Similar logic may apply for multi-step or arithmetic reasoning problems, where models are only scored on whether they get the final answer to a multi-step problem correct, without any credit given to partially correct solutions. - Page 7, https://arxiv.org/pdf/2206.07682.pdf They say that cross entropy loss in these cases goes down with model size incrementally, well before \"emergent\" capabilities appear. If so the model is improving (in a sense) even though these capabilities aren't observable below some critical size. reply Valakas_ 12 minutes agoprevSmall pet-peeve with title: Everything is math/predictable. We just may or may not be able to predict it with current knowledge. reply knallfrosch 7 hours agoprevToo bad the article only mentions addition – which shows gradual improvement. Would have loved to see the claim of emergence supported by an example as well. And more importantly, if the measure is boolean, completely wrong or completely right, then, well, I do expect to see a \"sudden\" jump in ability. reply n2d4 4 hours agoparentThe original emergence paper has multiple \"emergent\" metrics. [1] Though, the paper this article is about [2] mentions that among the \"emergent\" tasks they tested, 92% were measuring accuracy on either exact-string-match or multiple choice, so probably just bad metrics. [1] https://arxiv.org/pdf/2206.07682.pdf [2] https://arxiv.org/pdf/2304.15004.pdf reply wpietri 7 hours agoprevI think the graph is especially interesting. If the progress of LLMs is less radical when measured correctly, that could be another factor driving a bubble and its later popping. reply abound 7 hours agoparentThe updated \"partial credit\" metric proposed by the researchers suggests that LLMs will continue to improve as parameter count increases (assuming you have the data to make use of XX trillion parameters). It's just showing that it isn't as \"step-wise\" as other rankings may have indicated. reply anon291 6 hours agorootparentIt still is stepwise for the kinds of use cases being envisioned. For example, it's useful to know your 1 trillion parameter self driving car model is actually 90 percent of the way there, but it's also useful to know that the threshold you need to meet to implement the technology will likely be met at some point when your parameter count increases. reply Lockal 3 hours agoprevStudy: scientists prefer metrics which create an illusion that every time-consuming process is gradual and predictable reply maxcoder4 1 hour agoparentIf a process can be measured by a linearly growing metric I'd say it's predictible. And of course scientists try to understand complex processes and make them more predictible. That distinguishes science from magic. You said that like it's a bad thing reply 29athrowaway 3 hours agoprevTransformers have in-context meta learning. If you give them examples they can perform new tasks. When you run AutoGPT, the outcome is not always predictable. reply godelski 7 hours agoprevEmergence is a weird topic. If we go with the physicist's common definition [0,1] we'd need to differentiate into weak and strong. As for weak emergence[2] that is de facto. As for strong emergence[3] well... do we have an example of the phenomena anywhere? To be clear, ML people have a different definition which I do not think is entirely useful. Let's look at the wording used in [4] Scaling up language models has been shown to predictably improve performance and sample efficiency on a wide range of downstream tasks. This paper instead discusses an unpredictable phenomenon that we refer to as emergent abilities of large language models. We consider an ability to be emergent if it is not present in smaller models but is present in larger models. Thus, emergent abilities cannot be predicted simply by extrapolating the performance of smaller models. The existence of such emergence raises the question of whether additional scaling could potentially further expand the range of capabilities of language models Emergence is when quantitative changes in a system result in qualitative changes in behavior. An ability is emergent if it is not present in smaller models but is present in larger models. I'm not sure anything here is actually meaningful. We have very little knowledge about the interpretation of ML models, so it seems to be jumping the gun to say that phenomena cannot be predicted. The aspects are highly coupled with things like training techniques, optimization objectives (or loss functions), optimization methods, not just architectural designs. We've also after more research found smaller capable of performing things that large models can do. So the goal post moves and makes the condition of emergent abilities. [RTFA] So how do we define emergent abilities? Everything static except for the number of parameters? Does this make sense? Changing the number of parameters is an abstracted way of changing the optimization technique or training method. Clearly these are also different by nature of different batch sizes and/or a LR scheduler. So it is hard to be consistent. Not to mention that we are not great at predicting much of what networks can do in general. There's plenty of people working on the subject (though this is a small proportion of total ML researchers, even if we exclude those that are highly application focused). So are we gonna call something emergent if it is just something we don't know about? And who gets to be the one to decide? I've seen abilities called emergent that are clearly a result of training methods like KL Divergence but surprising to individuals who do not have a rigorous statistics and/or metric theory background. It just all seems to be jumping the gun. It's a new field and a new science. I think it is okay if we're willing to admit that our understanding just isn't great yet. There's no need to embellish or over attribute. These models are without a doubt powerful and useful, but at the same time I feel we are happy to greatly exaggerate all aspects about them. And for the life of me, I can't figure out why critiques are interpreted as dismissive. You can call an LLM a stochastic parrot and still think it is a great achievement and useful tool. Moreso, criticism is essential. Criticism gives us direction in research. Hype gives us motivation. But the two have to be in balance. Motivation without direction is an undirected Monte Carlo search (can still work, but we can do better than a drunken man). Overly criticizing (turning into dismissing, such as calling LLMs useless) is just pulling wool over one's eyes. It is the equivalent of lights being turned off and deciding to sit down and give up. When the lights get turned off you should find how to turn them back on. And we have more clues than a pure random process. After all, aren't we trying to make these better? I know I am. It is why I got interested in researching these things in the first place. I just wish we could have moderate hype rather than this ridiculously excessive one (which I think operates in a feedback loop with excessive criticism as it drives polarization). [0] Sabine Hossenfelder (I know HN loves her): https://www.youtube.com/watch?v=bJE6-VTdbjw [1] Sean Carrol: https://www.youtube.com/watch?v=0_PdLja-eGQ Yes, Sabine and Sean are both controversial characters but they are well known. [2] Weak emergence is where some larger phenomena forms out of smaller phenomena and results in something that the smaller thing can't do. The result requires the interactions of parts of a system. Temperature is a common example because it is generally easier to discuss the aggregated value of all the particles' \"jigglyness\" rather than each individual particle. The resultant property can be derived from the individual components. Conway's game of life is another common example. But in game theory we'd call a coalition an emergent property since utility is higher in the collective than the sum of each individual. Clearly this is true for ANNs since they are composed of neurons and a single neuron cannot perform these tasks. [3] Strong convergence is about behavior that CANNOT be derived from the individual constituents. [4] https://arxiv.org/abs/2206.07682 reply renewiltord 6 hours agoprevOkay, neat. Using a binary metric means that you observe sudden transitions from success to failure. Using a more granular metric means you observe smoother improvement. Logical and meaningful. It does make sense to evaluate things like 3-digit-sum by “how close textually?” And “how close numerically?” and the phase change is an artifact of the actual question being “is fully correct?” reply m3kw9 5 hours agoprevI think it only feels like it is emergent just like how some people thinks LLMs are conscious. It’s emergent because you don’t really understand what it was capable of? Or is it you have a confirmation bias because you don’t say its an emergent flaw when LLMs get some stuff so crazy wrong reply mellosouls 9 hours agoprevOP Should link to the original article which is listed in the first line: https://www.quantamagazine.org/how-quickly-do-large-language... reply stavros 8 hours agoparentHow does this work? Why is the whole story just copied over? reply animaomnium 8 hours agorootparentWired has a deal with Quanta to republish some of their stories. There are other stories from Quanta in Wired; I no longer read Wired, but I believe that this is how I first learned of Quanta. reply stavros 8 hours agorootparentAhh, thanks, I didn't realize this was a thing. reply mellosouls 8 hours agorootparentprevI don't know but would assume it's marketing-related, the story seen as meeting the demographic requirements of both publications, with an established mutually-beneficial relationship satisfied in the transaction, eg. maybe cheaper content for Wired, bumped readership figures for Quanta. reply empath-nirvana 9 hours agoprevThat is a terrible headline. It implies that the _abilities_ are a mirage, but it's actually the \"emergent\" part that might be a mirage -- which is to say it's not an unpredictable \"phase transition\" but gradual and predictable improvement in ability as the models scale. reply unclenoriega 8 hours agoparentFWIW, I read the headline as saying the abilities of LLMs are not emergent. reply anon291 6 hours agorootparentEmergent and breakthrough are not the same quality. Something can be emergent and develop gradually reply andsoitis 5 hours agorootparent> Something can be emergent and develop gradually Emergent can be defined as the sudden appearance of novel behavior. reply CamperBob2 5 hours agoparentprevNot only that, but what's the point in evaluating a language model on its ability to do arithmetic? It's like criticizing a talking dog because the C++ code it wrote was full of buffer-overflow bugs. In reality, if you ask a good LLM to solve a math-related problem, it will write and run a Python program to return the answer. Sometimes this even works. Sometimes it returns garbage. And sometimes it realizes that its own answer isn't realistic and tries a different approach. Sometimes people claim this isn't a valid manifestation of intelligence. Completely pointless study, unworthy of Wired or HN. reply necovek 4 hours agorootparentThis is a study of \"emergent\" properties of LLMs — whether something unexpected shows up (i.e. imagine that talking dog suddenly becoming great at pointer arithmetic and never using-after-free). It was noticed that LLMs can do some arithmetic, but we are yet uncertain how much and how it happens exactly. reply naasking 5 hours agorootparentprevArithmetic is treated as a proxy for general reasoning abilities. reply CamperBob2 4 hours agorootparentArithmetic is treated as a proxy for general reasoning abilities. Which is stupid, because it's not. A pocket calculator can perform arithmetic, as can a Python program, or for that matter a Japanese cormorant, who counts the fish it helps you catch. None of those are considered capable of \"reasoning\" on their own. Meanwhile, GPT4 will cheerfully write a program to carry out the required arithmetic operations (and then some.) A study that doesn't acknowledge that is worthless at best. reply Jensson 1 hour agorootparent> Meanwhile, GPT4 will cheerfully write a program to carry out the required arithmetic operations (and then some.) A study that doesn't acknowledge that is worthless at best. We don't want to see if the LLM can be used as a tool to do arithmetics, but whether it can learn complex data relationships like arithmetics. Arithmetics is a stepping stone, not a goal so that the model can solve it by invoking a calculator isn't relevant. The problems we want it to solve doesn't have tools like calculators so it doesn't help getting us there. reply taneq 8 hours agoparentprevAnd “emergent” in this context usually means “behaviour resulting from the interaction of a large number of individually simple units”, not “suddenly appearing.” reply skeledrew 6 hours agoprev [–] There seems to be some sort of flaw or misalignment in the hypothesis. By my reading, it seems this trio is saying LLMs do arithmetic by predicting the digits, and at intermediate stages there are actually partial predictions, and so there isn't a huge jump, which would then be classifiable as emergent. But I'm thinking that in order for an LLM to generally predict arithmetic results in the way they are designed, there would be an infinite memory requirement as it'd have to \"store\" an infinite set of completions. So the key may be to find sets of operands that GPT4/Claude Opus are unable to solve, even with hints similar to what a teacher may give a student learning the topic. Until such a set is found, I'd say this capability continues to meet the \"emergent\" definition, as the only other explanation - that I can think of - is that the models \"discovered\" how to do arithmetic in a general way. reply og_kalu 5 hours agoparent [–] >that I can think of - is that the models \"discovered\" how to do arithmetic in a general way. I mean..yeah. Maybe i've simply misunderstood but It seems like you're presenting this as an outlandish idea but something like this is what Neural Networks regularly end up doing especially in the limit (as loss trends down to zero). https://cprimozic.net/blog/reverse-engineering-a-small-neura... https://www.alignmentforum.org/posts/N6WM6hs7RQMKDhYjB/a-mec... Nobody is going to find any such set because time and time again, neural networks show us complete memorization is actually more difficult than just learning the distribution (even if imperfectly). reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "A recent study by Stanford researchers challenges the idea that large language models develop unforeseen skills suddenly, suggesting that these abilities may not emerge unpredictably but are influenced by how they are evaluated.",
      "Despite some scientists maintaining that the concept of emergence persists, the study underscores the significance of comprehending the behavior of language models as they expand and evolve."
    ],
    "commentSummary": [
      "The text explores emergent abilities in large language models, focusing on debates and critiques around evaluating these abilities in machine learning.",
      "It discusses challenges in defining emergence, limitations of models without hidden states, and predicting AI systems' capabilities, emphasizing the significance of constructive criticism for research progress.",
      "Additionally, it touches on using arithmetic to measure intelligence and the effectiveness of AI models in solving arithmetic problems, shedding light on the evolving landscape of AI capabilities."
    ],
    "points": 152,
    "commentCount": 78,
    "retryCount": 0,
    "time": 1711319793
  },
  {
    "id": 39808664,
    "title": "China Phases Out Intel and AMD Chips in Gov't Computers",
    "originLink": "https://www.channelnewsasia.com/business/china-blocks-use-intel-and-amd-chips-government-computers-report-4218101",
    "originBody": "Advertisement Business China blocks use of Intel and AMD chips in government computers: Report A central processing unit (CPU) semiconductor chip is displayed among flags of China and US. (Illustration: REUTERS/Florence Lo) 24 Mar 2024 05:37PM Bookmark Bookmark Share WhatsApp Telegram Facebook Twitter Email LinkedIn China has introduced guidelines to phase out US microprocessors from Intel and AMD from government personal computers and servers, the Financial Times reported on Sunday (Mar 24). The procurement guidance also seeks to sideline Microsoft's Windows operating system and foreign-made database software in favour of domestic options, the report said. Iklan Government agencies above the township level have been told to include criteria requiring \"safe and reliable\" processors and operating systems when making purchases, the newspaper said. China's industry ministry in late December issued a statement with three separate lists of CPUs, operating systems and centralised database deemed \"safe and reliable\" for three years after the publication date, all from Chinese companies, Reuters checks showed. The State Council Information Office, which handles media queries for the council, China's Cabinet, did not immediately respond to a faxed request for comment. Intel and AMD did not immediately respond to Reuters' request for comment. The US has been aiming to boost domestic semiconductor output and reduce reliance on China and Taiwan with the Biden administration's 2022 CHIPS and Science Act. Iklan It is designed to bolster US semiconductors and contains financial aid for domestic production with subsidies for production of advanced chips. Source: Reuters/gr Related Topics China microchips Intel United States Advertisement Also worth reading Content is loading... Advertisement Expand to read the full story Get bite-sized news via a new cards interface. Give it a try. Click here to return to FAST Tap here to return to FAST FAST",
    "commentLink": "https://news.ycombinator.com/item?id=39808664",
    "commentBody": "China blocks use of Intel and AMD chips in government computers: Report (channelnewsasia.com)137 points by fnordpiglet 16 hours agohidepastfavorite129 comments bee_rider 11 hours agoI think we’d consider it absolutely nuts in the US if anybody on our side suggested using Loongsons in government computers. So, this seems pretty reasonable to me. reply google234123 3 hours agoparentAbsolutely nuts that we allow them to do any business where it isn’t reciprocated - social networks/ internet companies reply underlogic 11 hours agoprevI'm surprised they didn't do it sooner. What can you expect the response to be from things like ME? If we can ban tiktok with a straight face they reasonably could ban all US made processors under the secure but with anti competitive cherry logic. reply tw04 11 hours agoparentThey already banned Facebook with a straight face a long time ago. And insist that western companies give joint ownership of subsidiaries doing business in China to a Chinese company. The US is hardly the aggressor on this one. They’ve simply been taking advantage of American greed for several decades and it’s finally catching up to them. reply pokepim 11 hours agorootparentI think it’s not a facebook ban per se but more like set of regulations (like no nsfw content and so on) that you need to meet to operate a business in the country. Facebook couldn’t care to meet those rules so they aren’t allowed to operate. Apple, Tesla and thousands other american businesses can still operate. reply bdd8f1df777b 10 hours agorootparentIt was that in the beginning. Then Zuckerberg had a change of heart and was willing to cooperate with the Chinese authorities on censorship and came to Beijing multiple times to express his newfound friendliness. Yet the Chinese government still shunned him. Not sure why. reply aragonite 10 hours agorootparentprev> They already banned Facebook with a straight face a long time ago Zuckerberg himself said in his 2019 speech that they don't operate in China because \"we could never come to agreement on what it would take for us to operate there.\" American tech companies absolutely can operate in China if they agree to abide by local censorship rules (like Bing is doing for China, like Google was going to do with Dragonfly, or like YouTube is doing for India). Facebook also has a substantial ad business in China [1] > And insist that western companies give joint ownership of subsidiaries doing business in China to a Chinese company. According to the Department of Commerce (https://www.export.gov/apex/article2?id=China-Establishing-a...), > A large majority of new foreign investments in China are WFOEs [wholly foreign owned enterprises], rather than JVs. As Chinese legal entities, WFOEs experience greater independence than ROs, are allowed exclusive control over carrying out business activities while abiding by Chinese law and are granted intellectual and technological rights. Also (https://arc-group.com/china-company-setup/): > WFOE refers to a limited liability company that is 100% invested, owned by foreign investors, and independently operated. Almost 60% of foreign-owned companies are WFOEs, making it the most adopted business type. Famous multinational companies such as Apple, Amazon, Oracle, and General Electric are all examples of WFOEs. [1] > Facebook sells more than $5 billion a year worth of ad space to Chinese businesses and government agencies looking to promote their messages abroad, analysts estimate. That makes China Facebook's biggest country for revenue after the United States, which delivered $24.1 billion in advertising sales in 2018 (https://www.cnbc.com/amp/2020/01/07/facebook-makes-a-new-ad-...) reply a_random_canuck 10 hours agorootparent> while abiding by Chinese law This includes Chinese national security law which is far more invasive than the US law. The party can order a company to do anything they want, or make any physical places they want available to them, and everything is kept under permanent gag order. reply aragonite 10 hours agorootparent> The U.S. government can also demand user information from online providers through National Security Letters, which can both require providers to turn over user information and gag them from speaking about it > By using NSLs, the FBI can directly order companies to turn over information about their customers and then gag the companies from telling anyone that they did so. Because the process is secret, and because even the companies can’t tell if specific NSLs violate the law, the process is ripe for abuse. https://www.eff.org/issues/national-security-letters reply jml7c5 8 hours agorootparentIt's worth noting a very, very large difference: the US cannot compel companies to build things. They can secretly obtain some information, but they can't secretly require backdoors, tracking tools, etc. reply st380752143 8 hours agorootparentprevHowever, you know this thing called NSL at least. In China, this performed because only someone wants. reply adolph 9 hours agorootparentprev>> while abiding by Chinese law > This includes Chinese national security law which is far more invasive than the US law Does anyone have links and translations or shall we all just conjecture in a fact free way? All the above might be true or might not, but there isn’t a way to evaluate the claims. reply nradov 5 hours agorootparentprevIt is disingenuous for you to refer to \"Chinese law\". The reality is that there is no rule of law or independent judiciary in China. The law is whatever order CCP officials issue. reply shrimp_emoji 9 hours agoparentprevI wanna ban ME from my house. :D reply ChrisArchitect 12 hours agoprev[dupe] More discussion: https://news.ycombinator.com/item?id=39805362 reply chasil 11 hours agoprev>foreign-made database software China is also beating the living daylights out of everyone else on TPC benchmarks. It seems like western software firms don't care anymore. https://www.tpc.org/tpcc/results/tpcc_results5.asp?print=fal... https://www.tpc.org/tpch/results/tpch_perf_results5.asp?resu... https://m.slashdot.org/story/361786 reply starspangled 8 hours agoparentNo, not about TPC-C. Hardware firms neither. The last hurrah for it was the fight between Itanium and PowerPC and then everybody stopped caring. It devolved into a benchmark where you'd get another % performance for every cache miss you saved from the SCSI / block IO request stack. It was totally IOPs bound, so they loaded up these huge machines with literally tens of thousands of hard disks that cost millions to run, and come out with some number that really wasn't very relevant. Actually I heard toward the end even the smaller 4 socket machines were driving tens of thousands of HDDs, so who knows what the 16/32 socket ones required. I think PCI attached NAND promised to (and did) vastly reduce the cost of the hardware, because the test is pretty well IOPs-bound, but they were still very expensive, and interest had already been waning, so it didn't turn that around. reply juancn 10 hours agoprevWhen facing an advanced adversary such as China, embargoes can backfire badly. Short term, you may annoy them for while, but you better use that time wisely, long term, your embargo may be the forcing function that makes them even more advanced. Granted, there may not be other politically palatable options available, but it's nonetheless risky. The Loongsons or Kirins may be not a huge threat now, but think of Japan in the 20th century, where it turned from barely competent, almost rural country to a tech powerhouse in a few decades. I read this tit for tat behavior as proof that the US embargoes may end up just making their own companies less competitive. reply nradov 5 hours agoparentJapan eventually ran into structural limits and growth has been stalled for many years. Out of the 100 largest companies in the world, only a couple are Japanese. The country is still doing well enough but has lost the ability to produce disruptive innovations. Expect the same thing to happen to China. If they can avoid another violent revolution, that is. reply mrtksn 4 hours agorootparentWhy does it matter to have x number of companies in top 100 or something? People repeat the same lines for Europe too but both in Japan and EU people live longer, healthier lives and the life quality in general isn’t any lower than the US or China. What if giant corporations isn’t the metric to measure success? reply nradov 4 hours agorootparentFeel free to use other metrics if it makes you happy. The point is that there is no historical basis to expect that technology embargoes against China will make them stronger over the long term. They are facing severe demographic and economic challenges, and will be hard pressed just to maintain their current levels let alone achieve major increases. reply mrtksn 4 hours agorootparentI don't know what will happen to China but both China and US have high hitters in the TOP100 lists and both have lower life quality than EU and Japan. What if giant corporations aren't predictor of anything but problems? Giant corporations don't actually innovate much, the thing about them is that they can extract money from the society without doing much innovation. They are bean counters. Having the biggest bean counters isn't as cool as a TOP100 lists imply. If you read closely, you will notice that TOP100 lists are curated by some bean counting metrics, not innovation or anything like that. TOP100 by capital is where innovation goes to die. The best they can do is to buy startups and try to stay relevant when actual inventors invents stuff. reply Dalewyn 4 hours agoparentprev>The Loongsons or Kirins may be not a huge threat now, Anyone who thinks CPUs allegedly on par with 10th gen Intel (or even a 14600K?!) and practical implementations of 7nm nodes are not huge threats is, sincerely, fucking delusional. And assuming the claims are hot air and they're more on par with 6th gen Intel or thereabouts, anyone who thinks that's not a huge threat is, sincerely, still fucking delusional. I think a lot of people, including those who really should know better, aren't realizing just how much processing horsepower even 6th gen Intel really has. Of course, if those people also don't consider China a hostile in the first place, then I will grant their logic would make sense. reply wejick 14 hours agoprevThis will boost more CPU designed and produced in China and probably will be good for consumers in general. I noted there're Longsoon MIPS based, many ARM vendors from china and I remember there's also an AMD based x86 cpu. More players more competition, pick your poison. reply bilbo0s 14 hours agoparentThe cynic in me thinks this isn't necessarily good for consumers so much as it is just different criminals who get keys to your back door. Maybe I'm just too negative? reply bobthepanda 10 hours agorootparentThe ship of Chinese consumers getting spied on sailed a long, long time ago. Pinduoduo got banned by Android as malware (in the US their app is Temu). As far as I know that is exceedingly rare reply bilbo0s 9 hours agorootparentWay more worried about me being spied on than Chinese users being spied on. And the way this is shaping up, I will be able to be spied on by the US or by China. There will be no option for me to buy hardware free of spying. reply brokenmachine 6 hours agorootparent>There will be no option for me to buy hardware free of spying. So, just like now? reply alephnerd 14 hours agorootparentprevThe kinda of CPUs being banned aren't the type that would be used in consumer usecases. reply mlindner 12 hours agoparentprevThis isn't really good for consumers at all. reply papichulo2023 14 hours agoprevThe real problem is not the CPUs but the mobo chipsets. The actual root. reply TacticalCoder 13 hours agoparent> The real problem is not the CPUs but the mobo chipsets. The actual root. But that actual root (complete with a networking stack etc.) backdoor (sorry: that amazing feature allowing updates we-swear-its-not-a-backdoor) is running the Minix OS on an... Intel chip? So if they ban Intel chips for government use, technically they also ban all these backdoored (sorry, \"upgradable\") mobos too no!? reply userbinator 11 hours agoparentprevThere were a few non-US companies making chipsets for PCs back in the late 90s/early 2000s --- VIA, SiS, ALi, UMC, Winbond, etc. I'm sure China has the resources to make its own. reply eric-hu 12 hours agoparentprevPardon my ignorance, but does “actual root” imply some kind of connection between a Linux root user and the motherboard chipset? reply jpgvm 12 hours agorootparentProbably not. They are probably referring to root of trust. Though they could also just be referring to \"root\" as a colloquialism to compromise the system but we are going to go with root of trust for sake of discussion. The motherboard (and associated UEFI/BIOS bits) is responsible for bootstrapping the Secure Boot process, it holds the keys that are used to verify the boot loader etc, which then chain-loads into a signed initramfs and kernel which then would normally decrypt and authenticate the filesystem. This chain of trust only works forwards. If you pwn any link in that chain then everything forward of it is now untrusted. Thus compromising the actual root, i.e the UEFI/BIOS firmware is the ultimate hack. Especially if it can be done persistently and without detection. Now for a normal desktop this doesn't really matter but on something like an EV (basically computer on wheels in Tesla's case atleast) then it really matters. High value military computers wouldn't be much different in this respect. reply yencabulator 7 hours agorootparentprev\"root\" has come to mean \"unrestricted access\", e.g. \"to root\" = \"to gain unrestricted access\". In this case, it most likely refers to a network connected hardware subsystem that can pause the main CPU and access its state freely. reply tianqi 14 hours agoprevSo what are the alternatives? Based on current information it seems that there are no domestic product in China that are comparable to these mainstream CPUs. reply neurostimulant 13 hours agoparentThey went hard with ARM, while developing RISC on the side. So perhaps something like this: https://www.tomshardware.com/pc-components/motherboards/firs... reply dist-epoch 14 hours agoparentprevCurrent Chinese CPUs have half the performance of current Intel/AMD ones. That's pretty good, especially for office work. reply bdd8f1df777b 10 hours agoparentprevOffice computers do not need high end CPUs. In fact, I know that several places have so old computers (with Intel chips) that they are probably slower than modern day Chinese processors. reply blackoil 13 hours agoparentprevHuawei ARM chips were pretty good. Now that they have 7nm out, a ARM Linux PC seems feasible. reply alephnerd 11 hours agorootparentIt needs to be fabricated by TSMC. Domestic fabrication capabilities haven't caught up yet and are currently at the same level as Taiwan, SK, and the US around 2014. reply blackoil 7 hours agorootparent7nm chips are fabricated in China by SMIC. It is used in latest Huawei 5G phones. They may also be able to make 5nm in near future. Beyond that I don't know. reply throwaway2990 11 hours agorootparentprevThe Huawei ARM chips are progression. They are not good. They are several years behind in performance. reply riku_iki 11 hours agorootparentIs several years behind that large gap? I think most apps/infra are fine to run on 10yo hardware reply throwaway2990 10 hours agorootparent6 or 7. Fine for consumers. reply coliveira 12 hours agoparentprevFrom observation of the Chinese government, I'm pretty sure they already have an alternative to these Intel CPUs. They're just creating the legal framework to require their use. reply logicchains 14 hours agoparentprevhttps://www.tomshardware.com/news/loongson-launches-3a6000-c... they have this, which is apparently comparable to an Intel Core i3, which is probably good enough for most government work. reply not_your_vase 13 hours agorootparentWell, the hardware, yes. But you are expected to compile your own software... Windows doesn't exist on Longsoon (AFAIK), and no notable distro have ready made image, nor package repository for this arch (not in the west at least. Are there such in China?). You can make it work with not a lot of effort of course, but I wonder if this is really what government folks do in their lunch break. reply yencabulator 7 hours agorootparentWikipedia says Loongson is used as the MIPS target of both Debian's and Go's continuous integration systems. https://en.wikipedia.org/wiki/Loongson#Operating_systems reply yorwba 13 hours agorootparentprevUOS reportedly supports Loongson as of 2019 https://cntechpost.com/2019/12/16/china-made-operating-syste... Though I found this post from last month https://bbs.chinauos.com/phone/zh/post/17586 where someone complains that UOS isn't updating to Loongson ABI2.0 quickly enough. reply xcv123 12 hours agorootparentprevThe Chinese government uses Kylin https://en.wikipedia.org/wiki/Kylin_(operating_system) https://en.wikipedia.org/wiki/Ubuntu_Kylin reply retrocryptid 14 hours agoparentprevI was sort of expecting at least one RISC V CPU to be on the list, but I guess we're still several years out from competing w/ Intel or ARM designs. And there's significant investment in MIPS toolchains, so maybe RV64 isn't quite ready for prime time in China? reply topspin 10 hours agorootparentAlibaba Group is promising server grade RISC-V this year: https://news.ycombinator.com/item?id=39776337 reply retrocryptid 6 hours agorootparentWhich is why it's weird the CCP is offering no regulatory love to a CPU developed and fabbed in mainland China. reply topspin 4 hours agorootparentI don't assume the left and right hands are coordinated: Alibaba only announced that a few days ago, and the CPU isn't actually available yet. reply beefnugs 12 hours agoprevIt makes it tough for us trying to figure out if it is because: - They are projecting that they know they are shipping backdoors into motherboard all over the world, so they are scared of the same - or else its just paranoia Regardless the simplest way to protect ourselves is major shift into whitelist only network allowance reply squarefoot 11 hours agoparentIt is probably safe to assume, until being proven wrong, that every manufacturer adds backdoors controlled by their own government, and consequently governments are suspicious of \"foreign\" hardware. If that's the case, the real question should be: as a normal citizen, am I more comfortable with a backdoor installed by my own government or any other one that shares information with them, or I would better use something controlled by a foreign country that doesn't share that information and quite likely also doesn't give a damn about who I am and what I do? reply eek2121 10 hours agorootparentGotta disagree. When you know enough about hardware engineering you'll understand that there are very talented folks around the world and if you released a vulnerable piece of hardware, SOMEONE SOMEWHERE will notice. You could argue that Spectre, etc. are part of that, but I don't think that is the case. This is just retaliation because of all the Taiwan crap, but more importantly, it does not matter as much as many folks think. reply brokenmachine 6 hours agorootparent>if you released a vulnerable piece of hardware, SOMEONE SOMEWHERE will notice But is that someone somewhere only part of the NSA or its Chinese equivalent? It makes me think of the TriangleDB iPhone exploit. I don't think anyone could have realistically known that those MMIO opcodes were there, without being part of the actual CPU design team. https://securelist.com/operation-triangulation-the-last-hard... >The exploit targets Apple A12–A16 Bionic SoCs, targeting unknown MMIO blocks of registers that are located at the following addresses: 0x206040000, 0x206140000, and 0x206150000. >The prompted me to try something. I checked different device tree files for different devices and different firmware files: no luck. I checked publicly available source code: no luck. I checked the kernel images, kernel extensions, iboot, and coprocessor firmware in search of a direct reference to these addresses: nothing. >How could it be that that the exploit used MMIOs that were not used by the firmware? How did the attackers find out about them? What peripheral device(s) do these MMIO addresses belong to I believe it's very easy nowadays to release undetectably vulnerable devices. reply s4mw1se 9 hours agorootparentprevIt happens a ton, and the hardware and software supply chains are more fragile than people realize. One book I was reading “Secrets of a Cybersecurity architect” by Brook S.E Shoenfield stated that security starts at the physical port and all hardware must go through integrity checks before it can be used. If my memory servers me right he was one of NSAs first cyber warfare and security architects. I also have retired family that was SVP of Engineering at a major telecom and he said this is a practice they had to implement as well because backbone routers would go missing for three days during shipping and just show up with no excuses for the delay. The book is worth a glance, i never finished but it’s pretty insightful. Additionally, on podcast i will never remember, China circumvents the banned entities list by stealthy acquiring approved chip manufactures and not rebranding, and new ownership adds backdoor into the chips. reply dehrmann 11 hours agoparentprevCould just be tit for tat over the Huawei ban. reply maskedinvader 11 hours agoparentprevnit: lets stop using whitelist and switch to allowlist reply a_random_canuck 10 hours agorootparentOr let’s not change anything. Because it has nothing to do with race. reply yencabulator 7 hours agorootparentprevAnd nit means lice eggs which, referring to a parasite, is clearly impolite, repulsive and unclean thoughtcrime. reply xcv123 23 minutes agorootparentprevNothing to do with race. That is delusion and paranoia. Seeing something that isn't there. You can take psychiatric meds to fix that. reply what 7 hours agorootparentprevWhy? reply exe34 11 hours agorootparentprevYes and Spanish should change their word for black, because it triggers me. reply neverokay 14 hours agoprevSo what are all the chips being illegally brought into China being used for? This is like banning the internet for Teenagers. It’s not gonna work. reply lolinder 12 hours agoparentYou missed this part: > in government computers Seems like a pretty achievable target. The US does this same thing at even larger scales for materials used in federal construction projects—the contractors are responsible for ensuring that (almost?) all materials are sourced from the US and are required to provide documentation to prove it. If the US can do it for everything in a building down to the bolts and screws, I have little doubt that China can figure it out for CPUs. reply phs318u 10 hours agorootparent> You missed this part: > > in government computers That's the rub. In a country where the ruling party has a significant stake in most core 'private' enterprises, the line between government and non-government is very blurry. Presumably someone in the Chinese government has an opinion on where this line is, or should be, drawn. reply rat9988 9 hours agorootparentThe line is not blurry at all even in china. reply bilbo0s 14 hours agoparentprevYou've misunderstood their most likely impact target. Their goal is not to prevent regular people from using those chips. This rule does nothing to stop ordinary Chinese from using anything in fact. Their goal is to route billions, over time hundreds of billions actually, to their own chipmakers while those chipmakers develop their capabilities with MIPS and ARM architectures. reply fnordpiglet 12 hours agorootparentThe issue will be if it is channeled into effective investments or into graft. Pork is definitely an issue in the west. In China it’s next level. The other issue is that hundreds of billions is peanuts compared to free market and global governmental investment. It’s akin to the US government saying they’re going to develop their own operating systems and architectures via subcontractors. Most people would assume it’s doomed instantly. I sort of expect this is window dressing and isn’t practically meaningful. Telling an entire mega-enterprise like a major nations bureaucracy to somehow get all their software and processes onto a bespoke architecture and operating system is absurd. The software ecosystem doesn’t exist, and the existing software used today obviously doesn’t run on the new stack. If they actually enforced this processes would need to degenerate to paper trails for a decade or more as everything is either cross compiled and debugged or rewritten from scratch. That seems absurdly unlikely. reply woooooo 12 hours agorootparentXiaomi and Huawei already distribute billions of devices not using Intel. You could take their phones, right now, plug in a keyboard and monitor and call it a workstation. reply fnordpiglet 11 hours agorootparentThis is government computer use. I assume their ERP software stacks don’t run on a cell phone. reply woooooo 11 hours agorootparentServer portions probably run on Linux, though, which the phone also runs. And most business software written in the last 25 years assumes a web browser for a client. Yes, there's a lot of software outside that box but quite a lot in it, we're not talking about the soviets starting from first principles. reply tredre3 12 hours agorootparentprev> It’s akin to the US government saying they’re going to develop their own operating systems and architectures via subcontractors. Most people would assume it’s doomed instantly. > In China it’s next level. And yet, somehow, China gets things done and the USA doesn't. I honestly don't know how much corruption goes on and how much grifting there is in those processes, but their infrastructure, manufacturing, and overall tech are literally next level compared to America. I have no worries they'll achieve their goal of CPU independence. reply alephnerd 12 hours agorootparent> China gets things done and the USA doesn't. Because graft and corruption works - look at the rise of fabrication in China despite just about every single vendor other than SMIC or YMTC collapsing due to misappropriation of funds and embezzlement (eg. Tsinghua Unigroup) The American military procurement system is extremely ossified by regulations made in the aftermath of the 1980s-90s procurement corruption scandals [0] Ironically, the US holds its lawmakers to a higher standard around graft than those in peer countries like France (Chirac, de Villipen, Mitterand), Israel (Netanyahu, Deri), Germany (Von der Leyden, Scholz), Canada (Bombardier, LaValle), or the UK (Johnson, Sunak), let alone countries like China. This has lead to compliance overload which severely narrows down the potential pool of vendors. For example, on the cybersecurity side - no startup even attempts to become FedRAMP certified until they are at Series E or above, as it's a multi-year process that swamps your platform and development team, and costs around $7-10M ime. Yet FedRAMP is nowhere near as painful as other procurement and compliance systems that other parts of the Defense industry need to face. [0] - https://www.politico.com/news/magazine/2023/06/09/america-we... reply fnordpiglet 11 hours agorootparentprevChina is largely a success story for foreign investment. Its prowess is almost entirely borrowed/extorted/stolen with little to no domestic expertise. I don’t mean this out of spite, but the best and brightest of China have left and leave. Very few repatriate and with the current climate that’s not reversing it’s accelerating. Most of the high tech in China is actually western, Japanese, Korean, or Taiwanese expertise and built staffed with Chinese labor. It’s certainly true their infrastructure as far as rail goes and other mass investment is top notch, but it’s not very productive. It’s also incomparable to compare China, which has only recently developed its infrastructure, to nations who have been building their modern infrastructure for over a hundred years. Legacy can weigh anyone down in so far as adoption. But last time I checked you don’t find self driving cars tooling the streets of China, all their top fabrication equipment is imported, and all of their official statistics of growth etc are almost entirely fabricated to the extent it’s unknowable how progressed they are other than it’s not as much as they claim. The flight of foreign investment out of China is going to be sobering as it accelerates faster and faster. The Silk Road projects are bankrupt, the real estate bubble is finally popping, etc. My real worry is the upcoming instability leads to military adventurism. reply blackhaz 13 hours agorootparentprevEffectively forcing their gov't to decouple from the West. Good move. Even if their chips are 30% as effective, it will still work. Lots of pain the rear, but it will work. reply jdsully 13 hours agorootparentThe USSR had the same strategy. It didn't work back then because the goal posts moved faster than they could march toward them. reply addicted 13 hours agorootparentExactly. This also further deepens Chinese dependence on the government which means the government’s on the hook for more things. And the way the Chinese government is doing it they’re on the hook on both sides, both on the supply and demand side. reply alephnerd 12 hours agorootparentThis is something the Xi Administration is fine with. His administration is largely traditionalists within the CCP, who are apathetic to market capitalism due to a couple very high profile corruption scandals in the 2000s and early 2010s. reply anticodon 12 hours agorootparentprevMarket size of USSR was tiny compared to Chinese market size. Also, USSR was a socialist country with plan economy, China is capitalist economy. It's incorrect to compare China to USSR. reply logicchains 14 hours agoparentprevProbably for training LLMs. reply alephnerd 14 hours agorootparentNuclear simulations - not LLMs. I made a pretty in-depth comment chain about this a couple weeks ago [0] and used to be a researcher in the space (both from a policymaking and technical standpoint) [0] - https://news.ycombinator.com/item?id=39515697 reply hodder 10 hours agoprevAre GPUs not listed simply because there is no workable alternative at the moment? Presumably they are going to attempt that next if/when anyone can make a workable alternative to Nvidia chips? reply h0l0cube 13 hours agoprevHot wars between large economies obviously destroy a lot of wealth, but I’d never considered how bad a protracted cold war could get, especially lacking the asymmetry of the first one reply fnordpiglet 12 hours agoparentThe first Cold War was pretty destructive and extremely protracted. In some ways it’s still happening with the same players just different political ideologies. reply h0l0cube 9 hours agorootparentIt can’t be said that the world’s biggest economy was dependent in any way on the other players last time. Taking China out of the supply chain seems almost quixotic at this stage. Globalization and free trade has enabled so much growth, and also has made the world safer by ensuring codependency of the world’s biggest powers. There’s no way to unwind that without consequences reply miked85 10 hours agoprevNo real reason to use Intel or AMD since they already stole all the tech. reply fsflover 15 hours agoprevThey have good reasons for that: https://en.wikipedia.org/wiki/Intel_Management_Engine and https://en.wikipedia.org/wiki/AMD_Platform_Security_Processo... reply spacephysics 14 hours agoparentIt’s the same reason why NSA contracts with intel demanded IME be removed reply autoexec 11 hours agoparentprevI'm surprised that other countries are cool with the it to be honest. Every major nation should be making their own chips. reply partiallypro 11 hours agoprevI'm curious how long it will be before they try to do this with Apple silicon and Nvidia. reply alephnerd 11 hours agoparentNvidia can't sell in China due to Department of Commerce sanctions, and Apple Silicon was always targeted at consumer applications. This is basically targeting AMD EPYC and Intel Xeon product lines, both of which already can't be sold to the Chinese government due to existing DoD and DoC sanctions. Basically, it's just PR. reply notjulianjaynes 11 hours agorootparent>Nvidia can't sell in China due to Department of Commerce sanctions I believe they can still do business there, just not sell cards above a (likely somewhat arbitrary) level of performance. There is a Chinese market RTX4090 they recently released for example. https://www.tomshardware.com/news/nvidia-reportedly-creating... reply alephnerd 11 hours agorootparentYep! And it's on the chopping block [0]: \"That's not productive,\" Raimondo said. \"I am telling you if you redesign a chip around a particular cutline that enables them to do AI, I am going to control it the very next day.\" [0] Also, it's moreso about the buyers and use case. If it can be used for cutting edge simulations related applications, it will absolutely get controlled. [0] - https://www.reuters.com/technology/us-talks-with-nvidia-abou... reply Dalewyn 10 hours agorootparentLinked article, second paragraph, emphasis mine: >U.S. Commerce Secretary Gina Raimondo, speaking in an interview with Reuters on Monday, said Nvidia \"can, will and should sell AI chips to China because most AI chips will be for commercial applications.\" China doesn't need to do anything to win this New Cold War, we (the US) will lose of our own accord. reply alephnerd 9 hours agorootparentAnd after that paragraph: > \"What we cannot allow them to ship is the most sophisticated, highest-processing power AI chips, which would enable China to train their frontier models,\" she added The Lovelace line is on the newer end, though the RTX4090 is more commercial driven in usecases. reply Dalewyn 9 hours agorootparentAnd they've complied: https://tech.slashdot.org/story/23/12/29/0111242/nvidia-slow... They aren't selling the \"most sophisticated, highest-processing power\" chips, but they are (and rightfully so given the law) selling sophisticated, high-processing power chips. When we're beating around the bush refusing to not sell to China while they go and do the deed wholesale on their side, it's a foregone conclusion who is winning this New Cold War and whether it's worth waging it (for us) at all. Also, it's been well over 24 hours since ComSec gave that \"the very next day\" statement. reply mlindner 12 hours agoprevAnd use what instead? Some non-x86 low performance processor? reply mepian 12 hours agoparentThe latest Loongsons are more than good enough for office work, at least. For HPC, China already built some of the fastest supercomputers in the world with Sunway processors. reply userbinator 11 hours agoparentprevThey have this: https://en.wikipedia.org/wiki/Zhaoxin reply kotaKat 11 hours agorootparentI have one! https://valid.x86.fr/xt39rz It's not great. If you're an information worker from 15 years ago, then sure, this thing has a lot of power to spare for... Office. You can buy one too, for only $79: https://www.newegg.com/p/1VK-01Z8-00004?Item=9SIAZN1FNJ9468 reply userbinator 8 hours agorootparentThere's apparently a newer version now, faster but obviously still not competitive with leading-edge Western tech: https://www.techpowerup.com/320504/zhaoxin-kx-7000-8-core-cp... But based on the discussions here of people using decade-old machines as daily drivers (including me), for undemanding workloads they may be just fine --- software inefficiency is a big factor, after all. It'd be amusing if this situation causes Chinese software to become more efficient as a result. reply xcv123 11 hours agoparentprevhttps://www.techpowerup.com/316189/chinese-loongson-3a6000-c... Even if the Chinese CPU has half the performance of Intel, it will be more productive than an i7 burdened with Windows 11 and corporate trashware. The Chinese government runs Linux. https://en.wikipedia.org/wiki/Kylin_(operating_system) https://en.wikipedia.org/wiki/Ubuntu_Kylin reply kkfx 14 hours agoprevIt's logic, since they are complex things running proprietary software BUT I can't really name alternatives that not share the same issues AND perform enough... reply prettywoman 14 hours agoprevIt's kind of difficult use software for RISC-V processors and even more dificult using *nix. RISC-V is a good option though but only for servers I think reply nntwozz 12 hours agoprevI guess they're fine with Apple Silicon then? reply mcphage 9 hours agoparentDoes the Chinese government use Apple Silicon for their machines? reply downrightmike 15 hours agoprev [–] Doesn't really matter, ARM and Nvidia chips are the future. reply zamadatix 15 hours agoparentI think the point is more the \"deemed safe\" list being completely domestic than the idea of whether said companies are the future of chips or not. reply qwytw 12 hours agoparentprevSo since you can't use Nvidia's chips more or less for anything outside of gaming/ML you're saying that Nvidia will enter the (relatively) very low margin CPU market? Why would they do that? reply bushbaba 14 hours agoparentprev [–] How is arm / nvidia any safer? reply amelius 14 hours agorootparent [14 more] [flagged] usui 14 hours agorootparent> CEO of nVidia is a Chinese guy. Perhaps the Chinese view them as safer because of this. Are you kidding? Please fact-check strong assertions like that. Jensen Huang is Taiwan-born and so is cousin-related AMD CEO Lisa Su, also Taiwanese, which even if the hypothesis of Taiwan-connected people being safer (due to more Chinese relationships) was true, then there would be no need to issue a curtailment on AMD chips in government computers. reply JumpCrisscross 13 hours agorootparentTaiwanese American. reply A4ET8a8uTh0 13 hours agorootparentprevIt is going to sound a little weird, but some people have trouble understanding complexities of ethnicity, country of origin, its intersection with local history, religion and so on ( my wife struggled hard with this ). Some people see Huang as Chinese, the same way they may see me as European. It might be technically true ( and it does betray some perspective a person may have on the subject ), but it happens to ignore some reality on the subject ( as in, how does Huang see himself ). I guess what I am saying is that it is an easy ( if a little lazy ) way to label people. reply aragonite 10 hours agorootparentThere's the anthropological concept of \"Chinese\" and then there's the identitarian notion. This is analogous to biological versus identitarian notions of gender. reply A4ET8a8uTh0 10 hours agorootparentInteresting comparison, but I don't want to get into that particular minefield today. Would say it is possible to be an American today without having appropriate documentation declaring that? Likewise, is it possible to be not an American ( not uphold its values ) and be properly documented? reply aragonite 10 hours agorootparent\"American\" is not an anthropological category, though. Consider how odd \"ethnically American\" sounds to the year. Compare that with \"ethically Chinese\", \"ethnically Indian\", \"ethnically Korean\". \"American\" is a geopolitical concept defined in terms of a political entity, the United States (just like \"Israeli\" is a geopolitical concept defined in terms of the country Israel). This is just a consequence of the fact that America is not a nation state (In this respect compare e.g. the -stan countries of Central Asia, each of which is named after a dominant ethnicity. \"Tajik\" is not defined in terms of \"Tajikistan\", rather the reverse is true). reply Dalewyn 8 hours agorootparentNative Americans are ethnically American. reply defrost 8 hours agorootparentIs \"ethnically American\" even a single homogeneous thing though? How many languages are spoken, how much territory used to be part of the New Spainish lands for how many hundred years, how many diaspora's fed into modern America? \"Native American\" itself is a broad spectrum, many past language and cutural groups, many degrees of living on and off reservation lands, many intertwinings with former slaves, ranchers, et al. reply A4ET8a8uTh0 9 hours agorootparentprevWhile I follow the argument, wouldn't by necessity, an ethnic Americans begin to exist the moment the next generation is born ? tldr: Anthropological quality has to start somewhere; not completely unlike myths. reply aragonite 9 hours agorootparentI wouldn't say ethnic-group formation happens as soon as the next generation (after the founding generation), but I agree with the larger point. I wouldn't be surprised if in a couple hundred years no one would bat an eye at talk of \"ethnically American\" or \"Norwegian of American descent\". My understanding is that that's more or less what happened with the French. reply amelius 11 hours agorootparentprevNo, I just didn't read past the word Chinese on Wikipedia, but it referred to language, not nationality. My bad. reply A4ET8a8uTh0 10 hours agorootparentOccam's razor. I withdraw my original diatribe then. edit: On my end, apologies for assuming. reply orochimaaru 13 hours agorootparentprev [–] Just because they are ethnically Chinese doesn’t mean they are Chinese citizens. Please research before making that claim. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "China plans to eliminate US Intel and AMD microprocessors from government systems, aiming to switch to domestic CPUs, operating systems, and databases.",
      "The Chinese industry ministry considers local technology \"safe and reliable\" for three years, intending to replace Microsoft's Windows and foreign database software.",
      "The US responds by enhancing semiconductor production through initiatives like the CHIPS and Science Act, while Intel and AMD have not commented on the situation."
    ],
    "commentSummary": [
      "China's ban on Intel and AMD chips for government computers triggers debates on business reciprocity between China and the US, citing concerns over spying risks and promoting domestic CPU development.",
      "Discussions highlight global competitiveness impacted by China's tech growth, supply chain vulnerabilities, and potential ramifications of an extended cold war.",
      "Nvidia's AI chip sales to China and speculation about China's tech progress and stability draw attention amid concerns about economic and political implications."
    ],
    "points": 137,
    "commentCount": 129,
    "retryCount": 0,
    "time": 1711301267
  },
  {
    "id": 39808616,
    "title": "Using Left Arrow Operator in C++ for Method Invocation",
    "originLink": "https://www.atnnn.com/p/operator-larrow/",
    "originBody": "Étienne Laurin Articles About C++ left arrow operator Posted on July 29, 2016 Sometimes you have a pointer to a class, and you want to invoke a method. You can use the -> operator for that. So what do you do when you have a pointer to a method, and want to invoke it on a class? Use thetemplate struct larrow { larrow(T* a_) : a(a_) { } T* a; }; templateR operator it) { return (it.a->*f)(); } template larrow operator-(T& a) { return larrow(&a); } struct C { void f() { std::cout << \"foo\"; } }; int main() { C x; (&C::f)<-x; } © 2010-2023 Etienne Laurin",
    "commentLink": "https://news.ycombinator.com/item?id=39808616",
    "commentBody": "C++ left arrow operator (2016) (atnnn.com)135 points by layer8 16 hours agohidepastfavorite90 comments WalterBright 9 hours agoThe C++ pointer-to-member is a fairly confusing concept. What it actually is is a pair - a pointer to the instance of the struct, and a pointer to a function in that struct. D calls these delegates, and generalizes it to being a pair consisting of a \"context pointer\" and a \"pointer to a function\". The neato thing is these do not have to be struct or class member functions. They can be nested functions, where the the \"context pointer\" is a pointer to the stack frame of the caller. I.e., a pointer to the closure. Hence, these become lambdas. Lamdas and pointers-to-nested functions are completely interchangeable with pointers to members. The caller does not know the difference. In fact, lambdas are far, far more commonly used in D than pointers-to-members. reply Kranar 6 hours agoparentThis isn't correct. C++ pointer to member's do not carry any instance information. They are more complex than plain pointers because they will still do proper virtual dispatching in the presence of multiple inheritance which requires more machinery than a plain function pointer, but this has absolutely nothing to do with keeping a pointer to an instance/this. reply WalterBright 4 hours agorootparentYou are correct, what I wrote is unclear. See my other posts in this thread. reply AdieuToLogic 5 hours agorootparentprev>> The C++ pointer-to-member is a fairly confusing concept. What it actually is is a pair - a pointer to the instance of the struct, and a pointer to a function in that struct. > This isn't correct. C++ pointer to member's do not carry any instance information. Mr. Bright's description is correct from the perspective of pointer-to-member's use, not when declared nor instantiated. Given Mr. Bright's role in creating Zortech C, Zortech C++, amongst other compilers, and having used the two specifically mentioned personally, I believe he has a proven understanding of C/C++ compiler implementations. Since one can interpret Mr. Bright's statement in the form of usage or definition, the former seems most applicable when viewed in the context of the remaining text. https://en.wikipedia.org/wiki/Digital_Mars reply Kranar 5 hours agorootparentNo it's just incorrect and no amount of pandering to his authority will change that. People make mistakes and this is one of them, it's not the end of the world. Avoid the temptation to believe false information just because you happen to worship someone, it reflects poorly on your ability to critically judge information. reply nurettin 4 hours agorootparentprevHe's the eccentric D guy who used to argue on IRC 20 years ago about how D is superior because the C++ standard doesn't require any C++ standard library to be present. If he says something weird, just nod and continue what you were doing. reply WalterBright 2 hours agorootparentI've never been on internet relay chat. reply p0w3n3d 21 minutes agoparentprevI like how delegates are easy in Python. del1 = someobject.some_method_without_parentheses del2 = someotherobject.other_method del2() del1() And the other way del1 = class1.method del2 = class2.other_method del1(obj_of_class1) del2(obj_of_class2) reply boguscoder 7 hours agoparentprevC++ pointers-to-members have no standard structure and are implementation defined. But in no implementations they have pointers to instances inside them. If that was the case Why would you need an instance (again) pointer/reference to use pointer-to-member? In most implementations they just encode an offset reply WalterBright 6 hours agorootparentThe pointer to instance part is the `this` pointer. > C++ pointers-to-members have no standard structure and are implementation defined. A missed opportunity, though every C++ compiler I've examined did it the same way. reply amluto 6 hours agorootparent> The pointer to instance part is the `this` pointer. What do you mean? The this pointer isn’t computed until you try to call the pointer-to-member-function. reply AdieuToLogic 5 hours agorootparent>> The pointer to instance part is the `this` pointer. > What do you mean? The this pointer isn’t computed until you try to call the pointer-to-member-function. Think of it from a compiler writer's perspective. The implicit parameter when using a pointer-to-member function is the function pointer itself. The `this` (instance) pointer must be passed explicitly when invoking it (along with whatever other parameters the function requires). Ergo: >> The pointer to instance part is the `this` pointer. reply lallysingh 5 hours agorootparentPointer to what, exactly? It doesn't make sense for it to be an object instance pointer, because you can use the same pointer to member on several different objects instances. If one instance's pointer was embedded in the pointer-to-member, it wouldn't work for other instances. I have no idea what the format of a pointer-to-member is. It sure looks like a small closure. reply WalterBright 5 hours agorootparentprevSee https://news.ycombinator.com/item?id=39812834 for a more complete explanation. reply boguscoder 6 hours agorootparentprevBut again, ‘this’ pointer is not in any way stored inside pointer-to-member. Caller has to provide one reply vintagedave 5 hours agoparentprevDelphi captures the pointer-to-instance and pointer-to-function concept too. It uses them for event handlers: delegating behaviour to other classes. To do that, method pointers are often a 'fat pointer': the pair of object instance and function. type TMethodPtr = procedure(x : Integer) of object; // 'of object' means an OO method pointer, not procedural var p : TMethodPtr = foo.bar; // Captures both foo and bar p(4); // Calls foo.bar(4) Fun: you can use them in C++ too in C++Builder's dialect via the rather ugly syntax, void(__closure * myClosure)(int); // myClosure can point to a method taking an int, returning void myClosure = pObj->func; // Assigng myClosure; this captures both pObj and the address of func myClosure(4); // Call it: this calls pObj->func(4) Syntax aside, they're neat because they match type safety by the method signature not by the type of the object on which they're called (which lets yu use them to delegate to any classes, not just ones inheriting from specific ancestors.) This bypasses contravariance constraints too. More info with a discussion on C++: https://www.codeproject.com/Articles/44874/C-Delegates-and-B... reply titzer 6 hours agoparentprevC# calls them delegates too, as does Virgil. Virgil uses the fat pointer trick so to avoid any heap allocations. In Virgil, class C { def m() -> int { return 33; } } var x = C.new(); // allocate a new C var y: void -> int = x.m; // delegate bound to x and m var z: C -> int = C.m; // C.m method is first-class, takes an object var t = z(x); // equivalent to x.m(); I never understood why C++ and other languages had such ugly syntax for such obvious concepts. reply Kranar 6 hours agorootparentI can't speak to Virgil, never used it, but C++ features try their best to adhere to the principle that you don't pay for what you don't use. It's not perfect but pointer to member functions in C++ use the simplest implementation that is possible for that given functionality. In C# delegates can not be unbound from instance variables, they must be bound at the point of creation. In C++ that is not the case which makes C++ pointer to members much more flexible. It looks like in Virgil you can have both bound and unbound pointers. You are welcome to correct me but I suspect it's implemented in such a way that you are always paying the cost of having a bound function pointer even if you only ever use it as an unbound function pointer. This would violate the principle I mentioned. In C++ if you don't mind always paying that cost, you are welcome to use std::function and it will work just as it does in the example you gave: std::function reply titzer 6 hours agorootparentIn Virgil every first-class function is a represented as a fat pointer, i.e. two words[1], for both bound and unbound delegates. It's simple to understand, the syntax is clean, and it always works the way you expect it to. It doesn't create garbage and basically means you use two registers instead of one. The unbound version even does a proper virtual dispatch for you. I understand C++'s intention, but it's doing no one any favors here and makes it really hard to build proper abstractions. Pointers to members are clunky, hard to use, and easy to screw up. AFAIU it's possible that a pointer to a member to be a single simple function pointer, but not guaranteed, and you shouldn't rely on it. It seems like a very bad tradeoff. std::function is considerably more heavyweight (and according to this https://stackoverflow.com/questions/13503511/sizeof-of-stdfu... might be arbitrarily large); i.e. it's slow in practice and people avoid it. [1] For programs less than 4GB combined code and heap, the two pointers can be packed into a single 64-bit word (though the compiler doesn't do this currently). reply Kranar 5 hours agorootparent>The unbound version even does a proper virtual dispatch for you--in C++ you can break class invariants by skipping a virtual dispatch by using a member pointer. This is not true, the C++ version does the proper virtual dispatch. >I understand C++'s intention, but it's doing no one any favors here and makes it really hard to build proper abstractions. std::function is a perfectly fine abstraction built on-top of the lower level primitives if you don't care about always paying a performance penalty. As someone who writes performance sensitive code, I do care so I try to avoid that penalty. >std::function is considerably more heavyweight (and according to this https://stackoverflow.com/questions/13503511/sizeof-of-stdfu... might be arbitrarily large); Of course std::function can allocate arbitrarily large amounts of memory, so can Virgil's implementation. The sizeof(std::function) is always fixed, but because it can capture arbitrarily functions which themselves can carry arbitrarily large state, then so too must std::function also have the potential to allocate arbitrarily large amounts of memory. In C++ people avoid std::function because as you said it's slow, and people tend to not use C++ for programs that can be slow, this goes back to the principle of not having to pay for what you don't use. In other languages you don't get that choice, you basically are required to pay the worst case cost even if you never use it. reply WalterBright 5 hours agorootparentprev> In C++ that is not the case which makes C++ pointer to members much more flexible. Here's an article describing how to achieve the same effect in D: https://www.digitalmars.com/articles/b68.html reply mgaunard 5 hours agorootparentAdding a trampoline means making the calls slower. Even if the lambda were to inline the call, it still would be a completely different location in the executable image. reply WalterBright 4 hours agorootparentIt's almost never used. The delegate version is almost universally used. > Even if the lambda were to inline the call, it still would be a completely different location in the executable image. Not sure what you mean. Inlined code is not in a different location, it's right where one is executing! Also, optimizers are pretty darned good these days. reply titzer 6 hours agorootparentprevAlso, FWIW, Virgil doesn't have \"pointer to member\" for fields; it does have a first-class getter for fields, but not a setter: class C { var f: int; } var x = C.new(); var g: C -> int = C.f; // C.f is a getter method for f var z: int = g(x); reply psyclobe 9 hours agoprevC++ is a never ending series of complexities that lead to incredible use cases reply a1o 11 hours agoprevLove code made for kicks. Also the other articles on the blog are super different one from the other! reply spacechild1 11 hours agoprevThis is pretty evil :-D reply 1letterunixname 11 hours agoparentBut is it as evil as? delete this; https://isocpp.org/wiki/faq/freestore-mgmt#delete-this reply spacechild1 10 hours agorootparentIt certainly has its use cases, e.g. classes with intrusive reference count, where the \"release\" method would destroy the object once the refcount has reached 0. reply amluto 6 hours agorootparentIs that really a good use case? Suppose you have: struct foo { atomic refcount; // other stuff }; You can pass around a pointer, COM-style, and call ->Release as needed. But you can be a lot less error-prone by passing around a smart pointer that understands the intrusive refcount and handles releasing. At which point you get something like 'delete ptr' in a destructor, not 'delete this' in a Release function. reply Const-me 18 minutes agorootparent> 'delete ptr' in a destructor, not 'delete this' in a Release function That approach requires client and server to be written in the same language, use the same memory allocator, and same compiler. For COM objects, often all of these are false. For example, C#, PowerShell or VBScript code can call IUnknown.Release() method of C++ implemented COM objects, which will cause C++ code to deallocate the memory. However, these higher level languages can’t directly delete C++ objects: they know nothing about C++ runtime, or C runtime. reply spacechild1 50 minutes agorootparentprevIn practice, people would (hopefully) wrap the whole thing in a smart pointer anyway, instead of manually calling addRef() and release(). But for COM style interfaces you need the release() method because you only want to expose pure virtual interfaces. reply amluto 6 hours agorootparentprevIn a language with affine types (i.e. real move semantics, like Rust but not like C++), destroying this is safe. OTOH deleting, as in freeing memory under the assumption that it was allocated in a particular manner, is only safe if you can somehow enforce that it was allocated that way. reply mgaunard 5 hours agorootparentwhich you can by making the constructor private. reply wheybags 7 hours agorootparentprevI'm a big fan of: this->~MyClass(); new (this) MyClass(); I have unironically used this reply amluto 6 hours agorootparentI hope you are confident that this doesn’t point to an instance of a derived class. reply classified 3 hours agoprevShocking, and very creative. I like it. reply Sharlin 10 hours agoprevSee also the long right arrow, or \"goes to\" operator --> (https://stackoverflow.com/questions/1642028/what-is-the-oper...) reply alejohausner 10 hours agoparentThere’s also the “comes from” operator: while (x10) (hmmm… the HN text formatter is fighting me. Those are all minus signs) reply layer8 8 hours agorootparentHN doesn't do that. It is your browser or (more likely) the standard text input of your OS. reply WalterBright 2 hours agorootparentprevI like the ...___... operator when your code is sinking. reply wruza 10 hours agorootparentprevPrepend two spaces to turn a paragraph into a code block which leaves the text as is. Not code -- Code -- (“ Code --”) reply jeroenhd 9 hours agoparentprevSee also: the tadpole operator (https://devblogs.microsoft.com/oldnewthing/20150525-00/?p=45...) Returns x+1 or x-1 depending on the direction the tadpole swims (-~x to increment x and or ~-x to decrement x). For a short moment I believed this was a real thing, because my programming font has a special ligature for ~- and -~. reply omoikane 9 hours agorootparentIt's actually useful in a code golf context, because \"x * -~y\" is two characters shorter than \"x * (y + 1)\". reply layer8 7 hours agorootparentprevNote that \"increment\" and \"decrement\" normally refer to changing the value of a variable (what ++ and -- do). It would have been funny if C had defined -~= and ~-= as the increment and decrement operators. reply pharrington 4 hours agoprevyes, C++, the original comedy programming language. reply travisd 8 hours agoprevThanks! I hate it! reply dzogchen 11 hours agoprevOperator overloading is one of those things that needs to be done with good taste. reply Sharlin 10 hours agoparentOne of the most magnificent abuses of operator overloading ever: multi-dimensional analog literals (http://www.eelis.net/C++/analogliterals.xhtml) reply Q6T46nT668w6i3m 10 hours agorootparentI don’t hate this! The author is, despite their sarcasm, correct—sometimes integers are insufficiently expressive. It’s the C++ equivalent of geometry in the Wolfram Language. reply layer8 8 hours agorootparentprevThis could potentially be extended to UML diagrams to express program logic (or at least to generate SVG versions). reply gumby 10 hours agorootparentprevTruly marvellous! reply kibwen 8 hours agoparentprevExtreme take, but the older I get the more I start thinking that the very concept of operators was a mistake, overloadable or otherwise, especially in a systems language. Even for something as simple as addition there's just too many actual operations that I might want to invoke (wrap on overflow, saturate on overflow, crash on overflow, raise exception on overflow, flag on overflow so I can raise later, return boolean to indicate overflow, return tuple of wrapped result and overflowed remainder on overflow). reply wwalexander 8 hours agorootparentPerhaps operators could only be overloaded, without any defined by the language itself. Then projects could define useful operators for their specific case to make things more terse/infix/etc. while still requiring awareness of their implementation. reply knome 8 hours agorootparentNo. Haskell made me decide that operators should not be definable by users. It is truly a miserable affair to read terse point free code riddled with operators from a half dozen libraries ( or one overly clever library ). Cleverness is a register that is easy to overflow, and too many don't have to good taste to avoid doing so. Give me names every time. reply j16sdiz 7 hours agorootparentIt depends on what you tries to do. Doing linear algebra without any operator overloading is just unreadable reply layer8 7 hours agorootparentI think GP meant users should not be able to define new operators (as is possible and fairly common in Haskell), but may still be allowed to overload the existing operators. reply wwalexander 6 hours agorootparentprevThat’s a good point. I suppose I was imagining something like internal-only operators that could be defined within the scope of a single module but not exported; though I suppose with any sufficiently large module with enough orthogonal pieces you’d still run into arcane and overly-terse operators. reply hollerith 7 hours agorootparentprevDo you consider them as bad as user-definable Lisp or Scheme macros? reply lmm 6 hours agorootparentprevDon't make \"operators\" a special case - make them just functions following the normal rules. Scala gets close to this (though sadly it still has some operator precedence rules). To define a function called \"plus\", you do \"def plus\" (or \"fun plus\" or whatever your syntax is). To define a function called \"+\" you do \"def +\". It's so much nicer and more consistent. Yes, some library authors abuse it to write functions or classes with dumb names, but you can write dumb names with letters as well. reply golergka 8 hours agorootparentprevAnd each library will end up with its own semantics. reply whiterknight 7 hours agorootparentprevUnfortunately life is complex and all of those semantics are useful. You have to read and write documentation and follow well established conventions (like math). reply kibwen 6 hours agorootparentBut that's the problem, even the mathematical operations in programming languages don't follow the established conventions of mathematics. And even mathematics overloads operators all the time to mean subtly different things. And even if it didn't, mathematics notation was optimized over centuries for terseness, not for readability, so optimizing programming languages for familiarity with mathematics is optimizing for the wrong thing anyway. We can do better than standard mathematic notation, and that \"better\" probably looks like \"just use properly-named methods or functions that communicate their intent in plain English, not in forbidding moon-runes\". reply whiterknight 4 hours agorootparentThat’s my point. symbols and names are merely suggestive mnemonic devices. They can’t ever specify actual behaviors. Accept it and supplement with English. That’s what math does. You have to read the context. > optimized to tersness That is a form of readability. If you think there is a conflict you need to explain mathematicians wouldn’t care about readability. reply phaedrus 7 hours agorootparentprevThis is especially well illustrated by the > (and > \"hello\" >> serviceB. I really loved that (my classmates not so much) reply lioeters 8 hours agorootparentReminds me of the ChucK operator. SinOsc b => Gain g => BiQuad f => dac; https://chuck.stanford.edu/doc/language/oper.html#chuck reply puffybuf 8 hours agorootparentprevI did something similar as a student, making my own exception class with std::ostream: throw exceptionC()string to_string(Arg &&arg, Args &&...args) { stringstream buf; buf (args)), ...); return buf.str(); } reply rockwotj 8 hours agorootparentprevGoogletest does this: http://google.github.io/googletest/primer.html#assertions reply hawski 11 hours agorootparentprevThere could be a couple of examples in Boost like Boost Spirit [0]. Qt had some like putting stuff in containers with bitshift operator. There was a GUI toolkit that used + operator to put widgets on a window. [0] https://www.boost.org/doc/libs/1_78_0/libs/spirit/doc/html/s... reply duped 10 hours agorootparentprevI've seen operator* overloaded to return RAII guards, including ones for RCU, and operator() overloaded for so many things that should just be lambdas it's hilarious reply DrFalkyn 10 hours agorootparentYeah the () overload was for functors, the old school way of doing closures before lambdas. reply duped 9 hours agorootparentIt was certainly how I learned to do it, but lambdas have been around for over a decade and I still see people writing functors. The only use case I've seen where it made sense is for things like coroutines. reply ksherlock 7 hours agorootparentFor std::visit() (std::variant.visit in c++26 I see) which is the visitor pattern, you can use a functor with multiple visit types or roll your own overload() template to merge multiple lambdas into one class. https://en.cppreference.com/w/cpp/utility/variant/visit https://en.cppreference.com/w/cpp/utility/variant/visit2 reply duped 5 hours agorootparentBut why would I want to do this instead of just having multiple lambdas that capture the same values by reference, or using shared_ptr and synchronization? I can see lifetimes of the data being an issue, but you shouldn't be calling std::visit in a way where that could cause a problem without synchronization anyway. reply barosl 10 hours agorootparentprev> to overload the bit shift operators for stream I/O MFC extends this idea to network programming, allowing the use of shift operators to send and receive data. reply wolfspider 6 hours agorootparentMFC also has CComPtrBase which uses & to represent pointer lifetimes to COM objects such as while(pEnum->Next(1, &pFilter, &cFetched) == S_OK). Especially fun when debugging DirectShow filtergraphs someone made in the UI completely. There is more of an explanation here: https://devblogs.microsoft.com/oldnewthing/20221010-00/?p=10... reply rwbt 10 hours agorootparentprevThe soon to be standard C++26 Reflection library? reply garaetjjte 8 hours agorootparentprevoperator/ for concatenating path components in std::filesystem reply 1letterunixname 10 hours agorootparentprevContinuation passing monads form the basis of a perfectly valid and usable software architecture and programming pattern. In the case of ostream and operator If you object to iostream on religious or stylistic grounds, No I object to it on usability grounds. It took more than 30 years for the committee to admit it but C++ now has typesafe std::print/std::format finally, after admonishing programmers for using `printf` in C. reply kccqzy 6 hours agorootparentprevWhich part of operator<< involves continuations, let alone continuation passing monads? reply hawski 10 hours agorootparentprevBut it is achievable without operator overloading, isn't it? There is also a reason why format strings seem to be more popular among the crowd. reply gumby 10 hours agorootparentStreams have numerous design problems (e.g. representation is managed by the stream, not the thing you’re printing) making the shift operator syntax the least of the problems. reply Quekid5 9 hours agoparentprevThat was a very cheeky comment, and fully agreed. I'm on the permissive side (Haskell and Lenseful Haskell at that!), but make it consistent and principled and it's usually fine. reply rryugciuf 11 hours agoprevOh my reply 1letterunixname 11 hours agoprev [–] Haskell, F#, and Scala enter the chat to lend a hand with custom operators. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Étienne Laurin explains utilizing the left arrow operator in C++ to call a method on a class with a pointer to the method.'- The discussion includes creating a template struct and overloading the left arrow operator to implement this feature.'- This post delves into a unique approach in C++ programming, showcasing advanced techniques for invoking methods through pointers.\""
    ],
    "commentSummary": [
      "The debate focuses on pointer-to-member in C++ and its ambiguity regarding instance information, comparing it to delegates and method pointers in C++ and Virgil, with Virgil's method deemed more user-friendly.",
      "Challenges of utilizing member pointers in C++, the intricacies of operator overloading, and comparisons with other programming languages are deliberated, along with programming concepts and best practices shared for clarity.",
      "The discussion delves into the complexities of operator overloading, offering numerous examples and insights into best practices for clearer understanding."
    ],
    "points": 135,
    "commentCount": 89,
    "retryCount": 0,
    "time": 1711300921
  },
  {
    "id": 39808357,
    "title": "Resolving Diacritical Marks Issue in File Names",
    "originLink": "https://unravelweb.dev/2024/02/12/the-u-u-conundrum/",
    "originBody": "The ü/ü Conundrum I implemented search and filtering for entities on our product at epilot. The users were heavily using the feature, however, a unique issue surfaced: difficulties in filtering file names containing diacritical marks like umlauts (Äpfel, über, schön, etc.). Intrigued, I delved into the logic to investigate. Initially, everything seemed in order. For instance, a file named “blöb” was saved precisely as “blöb” — no bizarre encoding alterations were apparent. Can you spot any difference between “blöb” and “blöb”? This time instead of typing the word in the search bar, I copied it from the uploaded and saved filename, and and voilà, the search functioned perfectly! 🤯. This definitely meant the file name was subtly altered upon upload, hinting at a background encoding transformation. So, as anyone would, I employed the power of encodeURIComponent to see for myself what really was different. There it was! The different types of encoding forms that led to this. Now you might be wondering (I was wondering too!): As if worrying about encoding wasn’t enough, now we’ve got to stress over its different types too 🤦♀ and wait, its 2024, and we are still grappling with Unicode character encoding problems? The culprits in this case were these two contrasting encoding types: %C3%BC and %CC%88, leading to the search malfunction. This is a well-known issue with NFD/NFC encoding forms. There is a detailed explanation here. Now imagine explaining this technical jargon that to a user! 😃 To fix it, I aligned the encoding type of the searched filename with that of the saved filename. On MacOS, the encoding was NFD (for ö: 111 + 776), whereas on Linux, it switched to NFC (ö: 246). The solution? – Post-upload, before saving the file as an entity, I applied .normalize() to the filename, ensuring it was uniformly searchable. – Additionally, as a cleanup step — to write a migration script to normalize the names of all existing files with non-ASCII characters. A simple string normalise proved effective in this case. ✅ Read more about String.prototype.normalize() here. There is also a specific normalization option to pass to ensure canonically equivalent normalization — for example: string.normalize(\"NFC\") Here’s a visual representation of the problem with the solution output: https://drive.google.com/file/d/1Jkp8ye47iRfDoeTaMQkkskY_BTHegj_D/view Hope it helps! Published February 12, 2024By Nishu Goel Categorized as Javascript Tagged browsers, encoding, files, javascript, programming",
    "commentLink": "https://news.ycombinator.com/item?id=39808357",
    "commentBody": "The ü/ü Conundrum (unravelweb.dev)133 points by firstSpeaker 15 hours agohidepastfavorite191 comments re 14 hours ago> Can you spot any difference between “blöb” and “blöb”? It's tricky to try to determine this because normalization can end up getting applied unexpectedly (for instance, on Mac, Firefox appears to normalize copied text as NFC while Chrome does not), but by downloading the page with cURL and checking the raw bytes I can confirm that there is no difference between those two words :) Something in the author's editing or publishing pipeline is applying normalization and not giving her the end result that she was going for. 00009000: 0a3c 7020 6964 3d22 3066 3939 223e 4361 .Ca 00009010: 6e20 796f 7520 7370 6f74 2061 6e79 2064 n you spot any d 00009020: 6966 6665 7265 6e63 6520 6265 7477 6565 ifference betwee 00009030: 6e20 e280 9c62 6cc3 b662 e280 9d20 616e n ...bl..b... an 00009040: 6420 e280 9c62 6cc3 b662 e280 9d3f 3c2f d ...bl..b...? I don't see every institution come up with a fix anytime soon, but having it clear that they're breaking the law is such a huge step. It's excellent, but also sad that it takes legislation to motivate companies to fix their crappy legacy systems, and they will likely fight tooth and nail rather than comply. reply jojobas 10 hours agorootparentprev> Does it mean Z̸̰̈́̓a̸͖̰͗́l̸̻͊g̸͙͂͝ǒ̷̬́̐ can finally have a bank account? I wonder if this also means one can require a European bank have a name on file in Kanju, Thai script or some other not-so-well-known in Europe alphabet. reply makeitdouble 8 hours agorootparentA bank can specially request it to be the name on a passport or domestic ID card. That's one way to make sure that the name falls within some parameters, though that can be tough on the customer in some conditions. reply jojobas 7 hours agorootparentI guess every country has a technical document on what's allowed in names, but then say EU banks have to cater for full superset of EU rules. As far as the passports go, ICAO 9303-3 allows for latin characters, additional latin characters, such as Þ and ß, and \"diacritics\", so something not too crazy, i.e. Z̷̪͘a̵͈͘l̷̹̃g̷̣̈́ő̶͍ would still be plausible. reply benhurmarcel 49 minutes agoparentprevWhen my child was born, one of the requirements I had to choose his name was that it shouldn't have any accent (or character that's not in the 26 universal letters basically). reply zokier 11 hours agoparentprev> * Especially in France, clerks try to emulate ü with the diacritics used for the trema e, ë. This makes it virtually impossible to find me in a system again In Unicode umlaut and diaeresis are both represented by same codepoint, U+0308 COMBINING DIAERESIS. https://en.wikipedia.org/wiki/Umlaut_(diacritic) reply lmm 11 hours agoparentprev> Give up and ask people to consistenly use a ascii-only name? > Officially change my name? Yes. That's the only one that's going to actually work. You can go on about how these systems ought to work until until the cows come home, and I'm sure plenty of people on HN will, but if you actually want to get on with your life and avoid problems, legally change your name to one that's short and ascii-only. reply taejo 1 hour agorootparentAnd then never move to Japan (or any other country where names are expected not to have Latin letters in them) reply zokier 11 hours agoparentprevGermans have of course a standard for this > a normative subset of Unicode Latin characters, sequences of base characters and diacritic signs, and special characters for use in names of persons, legal entities, products, addresses etc https://en.wikipedia.org/wiki/DIN_91379 reply em-bee 11 hours agorootparentand it's used in the passport too. so names with umlaut show up in both forms and it is possible to match either form reply Chilko 10 hours agoparentprev> Officially change my name? My German last name also contains an ü, so when we emigrated to an English-speaking country and obtained dual-citizenship we used 'ue' for that passport and I now use 'ue' on a day-to-day basis. This also means I have two slightly different legal surnames depending by which passport I go. reply hodgesrm 10 hours agorootparentAt least German transliteration is 1-to-1. Slavic names among others often have multiple transliterations available. The Russian name Валерий can be rendered for example as Valery, Valeriy, or Valeri. It's very confusing for documents that require the person's name. [0] https://en.wikipedia.org/wiki/Wikipedia:Romanization_of_Russ... reply krab 4 hours agorootparentThat's the English transliteration. Don't forget that other Slavic languages also transcribe according to their own rules. For example in Czech, Валерий would be transliterated as Valerij because \"j\" is pronounced in Czech as English \"y\" in \"you\". reply illiac786 3 hours agorootparentprevThere's an ISO standard for this. Can't find it but I am 100% sure for russian for example. reply bobthepanda 7 hours agorootparentprevAlso don't forget Chinese, which due to different romanizations or different dialects being used for the romanization, can result in different outputs depending on whether a person is from PRC, ROC, Macao, Hong Kong, or Singapore. reply fsckboy 7 hours agorootparentprevjust out of curiosity, can you port the ue back to Germany (or wherever) or will they automatically transform it to ü? (could you change your name in a German speaking country to Mueller et al?) reply ulucs 11 hours agoparentprevCan ü be printed on a passport rather than a u? I have a ş and a ç so I have been successfully substituting s and c for them in a somewhat consistent manner. reply mschuster91 10 hours agorootparentOn the human-readable zone (\"VIZ\" in ICAO 9303) yes, see part 3 section 3.1 [1]. The MRZ however, not - it is limited to Latin alphanumeric only, see section 4.3. How to transliterate non-Latin characters is left to the discretion of the issuing government, and that has been a consistent source of annoyances for people who have identity cards issued by different governments (e.g. dual-nationals of Western European and Turkish, Arabic or Cyrillic-using Slavic countries). [1] https://www.icao.int/publications/Documents/9303_p3_cons_en.... reply samatman 11 hours agoparentprevThe only solution is going to be a lot of patience, unfortunately. Everyone should be storing strings as UTF-8, and any time strings are being compared they should undergo some form of normalization. Doesn't matter which, as long as it's consistent. There's no reason to store string data in any other format, and any comparison code which isn't normalizing is buggy. But thanks to institutional inertia, it will be a very long time before everything works that way. reply lmm 11 hours agorootparent> Everyone should be storing strings as UTF-8, and any time strings are being compared they should undergo some form of normalization. Doesn't matter which, as long as it's consistent. There's no reason to store string data in any other format, and any comparison code which isn't normalizing is buggy. This will result in misprinting Japanese names (or misprinting Chinese names depending on the rest of your system). reply earthboundkid 11 hours agorootparentCan we please talk about Unicode without the myth of Han Unification being bad somehow? The problem here is exactly the lack of unification in Roman alphabets! reply lmm 10 hours agorootparent> Can we please talk about Unicode without the myth of Han Unification being bad somehow? It's not a myth, as anyone living in Japan knows, and the \"just use Unicode, all you need is Unicode\" dogma is really harmful; a lot of \"international\" software has become significantly worse for Japanese users since it took hold. > The problem here is exactly the lack of unification in Roman alphabets! Problems caused by failing to unify characters that look the same do not mean it was a good idea to unify characters that look different! reply jhanschoo 3 hours agorootparent> \"just use Unicode, all you need is Unicode\" dogma is really harmful; a lot of \"international\" software has become significantly worse for Japanese users since it took hold. The alternative would be that the software used Shift_JIS with a Japanese font. If the software used a Japanese font for Japanese it wouldn't need metadata anyway. There really isn't a problem with Han unification as long as you always switch to a font appropriate for your language; you don't need to configure metadata. If you don't you are always going to run into missing codepoint problems. In cases where the system or user configures the font, properly using Unicode is still easier than configuring alternate encodings for multiple languages. reply lmm 3 hours agorootparent> The alternative would be that the software used Shift_JIS with a Japanese font. As far as I know all Shift_JIS fonts are Japanese; you would have to be wilfully perverse to make one that wasn't. > If the software used a Japanese font for Japanese it wouldn't need metadata anyway. If it just uses the system default font for that encoding, as almost all software does, then it will also behave correctly. > There really isn't a problem with Han unification as long as you always switch to a font appropriate for your language Right. But approximately no software does that, because if you don't do it then your software will work fine everywhere other than Japan, and even in Japan it will kind-of-sort-of work to the point that a non-native probably won't notice a problem. > In cases where the system or user configures the font, properly using Unicode is still easier than configuring alternate encodings for multiple languages. I'm not convinced it is. Configuring your software to use the right font on a Unicode system is, as far as I can see, at least as hard as configuring your software to use the right encoding on a non-Unicode system. It just fails less obviously when you don't, particularly outside Japan. reply eviks 6 hours agorootparentprevWould it still be harmful if language tag were used? reply lmm 6 hours agorootparentIf the tag mechanism was used consistently and handled by all software, no. But in practice the only way that would happen is if the tag mechanism was required for many languages. Unicode is, in practice, a system that works the same way for ~every human language except Japanese, which makes it much worse than the previous \"byte stream + encoding\" system where any program written to support anything more than just US English would naturally work correctly for every other language, including Japanese. reply renhanxue 10 hours agorootparentprevBoth problems are missing the point: you cannot handle Unicode correctly without locale information (which needs to be carried alongside as metadata outside of the string itself). To a Swede or a Finn, o and ö are different letters, as distinct as a and b (ö sorts at the very end at the alphabet). A search function that mixes them up would be very frustrating. On the other hand, to an American, a search function that doesn't find \"coöperation\" when you search for \"cooperation\" is also very frustrating. Back in Sweden, v and w are basically the same letter, especially when it comes to people's last names, and should probably be treated the same. Further south, if you try to lowercase an I and the text is in Turkish (or in certain other Turkic languages), you want a dotless i (ı), not a regular lowercase i. This is extremely spooky if you try to do case insensitive equality comparisons and aren't paying attention, because if you do it wrong and end up with a regular lowercase i, you've lost information and uppercasing again will not restore the original string. There are tons and tons of problems like this in European languages. The root cause is exactly the same as the Han unification gripes: Unicode without locale information is not enough to handle natural languages in the way users expect. reply eviks 6 hours agorootparent> which needs to be carried alongside as metadata outside of the string itself Why not as data tagged with the appropriate language? https://www.unicode.org/faq/languagetagging.html reply RedNifre 11 hours agorootparentprevHow? reply lmm 10 hours agorootparentUnicode reuses codepoints for characters that the committee decided were in some sense \"the same\", including Japanese and Chinese characters that are written differently from each other (different numbers of strokes etc.). This is a minor irritation for everyday text, but can be quite upsetting when it's someone's name that's getting printed wrong. reply userbinator 12 hours agoparentprevEveryone's name should just be a GUID. /s reply BuyMyBitcoins 11 hours agorootparentFalsehoods Programmers Believe About Names, #41 - People have GUIDs. https://www.kalzumeus.com/2010/06/17/falsehoods-programmers-... reply noodlesUK 14 hours agoprevOne thing that is very unintuitive with normalization is that MacOS is much more aggressive with normalizing Unicode than Windows or Linux distros. Even if you copy and paste non-normalized text into a text box in safari on Mac, it will be normalized before it gets posted to the server. This leads to strange issues with string matching. reply ttepasse 13 hours agoparentUnfun normalisation fact: You can’t have a file named \"ss\" and a file named \"ß\" in the same folder in Mac OS. reply bawolff 11 hours agorootparentThat's less a normal form issue and more a case-insensitivity issue. You also can't have a file named \"a\" and one named \"A\" in the same folder. reply samatman 11 hours agorootparentThat would be true if the test strings were \"SS\" and \"ß\", because although \"ẞ\" is a valid capitalization of \"ß\", it's officially a newcomer. It's more of a hybrid issue: it appears that APFS uses uppercasing for case-insensitive comparison, and also uppercases \"ß\" to \"SS\", not \"ẞ\". This is the default casing, Unicode also defines a \"tailored casing\" which doesn't have this property. So it isn't per se normalization, but it's not not normalization either. In any case (heh) it's a weird thing that probably shouldn't happen. Worth noting that APFS doesn't normalize file names, but normalization happens higher up in the toolchain, this has made some things better and others worse. reply eropple 11 hours agorootparentprevThis shows up in other places, too. One of my Slacks has a textji of `groß`, because I enjoy making our German speakers' teeth grind, but you sure can just type `:gross:` to get it. reply thaumasiotes 7 hours agorootparent> a textji This is a weird formation; \"ji\" means text. It's half of the half of \"emoji\" that means text: 絵文字, 絵 [e, \"picture\"] 文字 [moji, \"character\", from 文 \"text\" + 字 \"character\"]. https://satwcomic.com/half-human-half-scandinavian reply eropple 6 hours agorootparentYes, but \"reactji\" is also weird and yet people use it for Slack reactions. It's fine. reply yxhuvud 12 hours agorootparentprevSo what happens if someone puts those two in a git repo and a Mac user checks out the folder? reply staplung 12 hours agorootparentgit clone https://github.com/ghurley/encodingtest Cloning into 'encodingtest'... remote: Enumerating objects: 9, done. remote: Counting objects: 100% (9/9), done. remote: Compressing objects: 100% (5/5), done. remote: Total 9 (delta 1), reused 0 (delta 0), pack-reused 0 Receiving objects: 100% (9/9), done. Resolving deltas: 100% (1/1), done. warning: the following paths have collided (e.g. case-sensitive paths on a case-insensitive filesystem) and only one from the same colliding group is in the working tree: 'ss' 'ß' reply Twisol 12 hours agorootparentI have this issue on occasion with older mixed C/C++ codebases that use `.c` for C files and `.C` for C++ files. Maddening. reply Athas 11 hours agorootparentI never understood the popularity of the '.C' extension for C++ files. I have my own preference (.cpp), but it's essentially arbitrary compared to most other common alternatives (.cxx, .c++). The '.C' extension is the only one that just seems worse (this case sensitivity issue, and just general confusion given how similar '.c' looks to '.C'). But even more than that, I just don't get how C++ turns into 'C' at all. It seems actively misleading. reply keketi 10 hours agorootparentC++ is Incremented C which is Big C which is Capital C reply Athas 10 hours agorootparentBut C is already capital C! Even .d would have been a better extension. reply imtringued 2 hours agorootparentHe is clearly taking about the capital version of capital C. reply basil-rash 9 hours agorootparentprevYou can always reformat as APFS (Case Sensitive) reply tetromino_ 12 hours agorootparentprevEEXIST reply nradov 8 hours agorootparentprevThere are people with the surname \"Con\" and it's impossible to create a file with that name in MS Windows. https://learn.microsoft.com/en-us/windows/win32/fileio/namin... reply codesnik 13 hours agoparentprevI was really surprised when realized that at least in hpfs cyrillics is normalized too. For example, no russian ever thinks that Й is a И with some diacritics. It's a different letter on it's own right. But mac normalizes it into two codepoints. reply asveikau 13 hours agorootparentI dislike explaining string compares to monolingual English speakers who are programmers. Similar to this phenomenon of Й/И is people who think ñ and n should compare equally, or ç and c, or that the lowercase of I is always i (or that case conversion is locale-independent). In something like a code review, people will think you're insane for pointing out that this type of assumption might not hold. Actually, come to think of it, explaining localization bugs at all is a tough task in general. reply yxhuvud 12 hours agorootparentOr that sort order is locale independent. Swedish is a good example here as åäö are sorted at the end, and where until 2006 w was sorted as v. And then it changed and w is now considered a letter of its own. reply iforgotpassword 12 hours agorootparentprevWell, I do like this behavior for search though. I don't want to install a new keyboard layout just to be able to search for a Spanish word. reply NeoTar 12 hours agorootparentMy brother recently asked for help in determining who a footballer (soccer player) was from a photo. Like in many sports, the jerseys have the players name on the rear, and this player’s was in Cyrillic - Шунин (Anton Shunin) - and my brother had tried searching for Wyhnh without success. Anyway, my point is that perhaps ideally (and maybe search engines do this) the results should be determined by the locale of the searcher. So someone in the English speaking world can find Łódź by searching for Lodz, but a Pole may need to type Łódź. My brother could find Shunin by typing Wyhnh, but a Russian could not… reply nradov 8 hours agorootparentEssentially you are asking for search engines to recognize \"Volapuk\" encoding. https://en.wikipedia.org/wiki/Informal_romanizations_of_Cyri... reply david-gpu 12 hours agorootparentprevIs the convenience of a few foreigners searching for something more important than the convenience of the many native speakers searching for the same? Maybe we should start modifying the search behavior of English words to make them more convenient for non-native speakers as well. We could start by making \"bed aidia\" match \"bad idea\", since both sound similar to my foreign ears. reply MrJohz 12 hours agorootparentIn fairness, for search, allowing multiple ways of typing the same thing is probably the best choice: you can prioritise true matches, where the user has typed the correct form of the letter, but also allow for more visual based matches. (Correcting common typos is also very convenient even for native speakers of a language — and of course a phonetic search that actually produced good results would be wonderful, albeit I suspect practically very difficult given just how many ways of writing a given pronunciation there might be!) reply makeitdouble 12 hours agorootparentprevSearch probably needs both modes. A literal and a fuzzy one. reply dmckeon 10 hours agorootparentFor similar sounding names, this fuzzy match is pretty effective. https://www.archives.gov/research/census/soundex reply nradov 8 hours agorootparentIn terms of phonetic matching algorithms, Soundex is considered badly outdated. Most MDM products use more advanced alternatives. reply makeitdouble 12 hours agorootparentprevThe general reaction I've see until now was \"meh, we have to make compromises (don't make me rewrite this for people I'll probably never meet)\" Diacritics exacerbate this so much as they can be shared between two language yet have different rules/handling. French typically has a decent amount and they're meaningful but traditionally ignores them for comparison (in the dictionary for instance). That makes it more difficult for a dev to have an intuitive feeling of where it matters and where it doesn't. reply bawolff 11 hours agorootparentprevNormalization isn't based on what language the text is. NFC just means never use combining characters if possible, and NFD means always use combining characters if possible. It has nothing to do with whether something is a \"real\" letter in a specific language or not. The whether or not something is a \"real\" letter vs a letter with a modifier, more comes into play in the unicode collation algorithm, which is a separate thing. reply anamexis 13 hours agorootparentprevWell, there's no expectation in unicode that something viewed as a letter in its own right should use a single codepoint. reply sorenjan 13 hours agoparentprevI sometimes see texts where ä is rendered as a¨, i.e. with the dots next to the a instead of above it even though it's a completely different letter and not a version of a. I managed to track the issue down to MacOS' normalization, but it has happened on big national newspapers' websites and similar. I haven't seen it in a while, maybe Firefox on Windows renders it better or maybe various publishing tools have fixed it. It looks really unprofessional which is a bit strange since I thought Apple prides themselves on their typography. reply aidos 12 hours agorootparentI have never see that on all my years on a Mac (though admittedly I’m not dealing in languages where I encounter it often). I’m assuming there’s an issue with the gpos table in the font you’re using so the dots aren’t negative shifted into position as they should be? reply iforgotpassword 12 hours agorootparentprevI have that in gnome terminal. The dots always end up on the letter after, not before. At least makes it easy to spot filenames in decomposed form so I can fix them. reply dathinab 10 hours agoparentprevWhile this (probably) still applies to Apple UI elements when they switched to APFS they stopped doing Unicode normalization on filesystem level. So now on macOS you can have a very mixed bag with some programs normalizing, some not (it's a bug) and many expecting normalized file names. So it's kinda like other Linux now except a lot of dev assuming normalization is happening (and in some cases still is when the string passes through certain APIs). Worse due to normalization now being somewhat application/framework dependent and often going beyond basic Unicode normalization it can lead to quite not so funny bugs. But luckily most users will never run into any of this bugs even if the use characters which might need normalization. reply yxhuvud 12 hours agoparentprevOn the other hand, stuff written on macs are a lot more likely to require normalization in the first place. reply creshal 13 hours agoparentprevMacOS creates so many normalization problems in mixed environments that it's not even funny any more. No common server-side CMS etc. can deal with it, so the more Macs you add to an organization, the more problems you get with inconsistent normalization in your content. (And indeed, CMSes shouldn't have to second-guess users' intentions - diacretics and umlauts are pronounced differently and I should be able to encode that difference, e.g. to better cue TTS.) And, of course, the Apple fanboys will just shrug and suggest you also convert the rest of the organization to Apple devices, after all, if Apple made a choice, it can't be wrong. reply fauigerzigerk 13 hours agorootparentI'm not sure I understand. On the one hand you seem to be saying that users should be able to choose which normalisation form to use (not sure why). On the other hand you're unhappy about macOS sending NFD. If it's a user choice then CMSs have to be able to deal with all normalisation forms anyway and shouldn't care one bit whether macOS sends NFD or NFC. Mac users could of course complain about their choice not being honoured by macOS but that's of no concern to CMSs. reply creshal 11 hours agorootparent> On the other hand you're unhappy about macOS sending NFD. Because MacOS always uses it, regardless of the user's intention, so it decomposes umlauts into diaereses (despite them having different meanings and pronunciations) and mangles cyrillic, and probably more problems I haven't yet run into. reply kps 11 hours agorootparentUnicode doesn't have ‘umlauts’, and (with a few unfortunate exceptions) doesn't care about meanings and pronunciations. From the Unicode perspective, what you're talking about is the difference between Unicode Normalization Form C: U+00FC LATIN SMALL LETTER U WITH DIAERESIS and Unicode Normalization Form D: U+0075 LATIN SMALL LETTER U U+0308 COMBINING DIAERESIS Unicode calls these two forms ‘canonically equivalent’. reply cozzyd 8 hours agorootparentprevFor maximum pain, they should start populating folders with .DS_STÖRE reply eviks 6 hours agorootparentBut store decomposed form on Tuesdays! reply zh3 13 hours agorootparentprevSuspect you're getting downvoted because of the last sentence. However, I do sympathise with MacOS tending to mangle standard (even plain ASCII) text in a way that adds to the workload for users of other OS's. reply creshal 11 hours agorootparentIt adds to the workload of everyone, including the Apple users. The latter ones are just in denial about it. reply josephcsible 12 hours agoprevIMO, it was a mistake for Unicode to provide multiple ways to represent 100% identical-looking characters. After all, ASCII doesn't have separate \"c\"s for \"hard c\" and \"soft c\". reply renhanxue 10 hours agoparentThe problem in the linked article barely scratches the surface of the issue. You _cannot_ compare Unicode strings for equality (or sort them) without locale information. A simple example: to a Swedish or Finnish speaker, o and ö completely different letters, as distinct as a is from b, and ö sorts at the very end at the alphabet. A user that searches for ö will definitely not expect words with o to appear. However, to an American, a user that searches for \"cooperation\" when your text data happens to include writings by people who write like in The New Yorker, would probably to expect to find \"coöperation\". This rabbit hole goes very, very deep. In Dutch, the digraph IJ is a single letter. In Swedish, V and W are considered the same letter for most purposes (watch out, people who are using the MySQL default utf8_swedish_ci collation). The Turkish dotless i (ı) in its lowercase form uppercases to a normal I, which then does _not_ lowercase back to a dotless i if you're just lowercasing naively without locale info. In Danish, the digraph aa is an alternate way of writing å (which sorts near the end of the alphabet). Hungarian has a whole bunch of bizarre di- and trigraphs IIRC. Try looking up the standard Unicode algorithm for doing case insensitive equality comparison by the way; it's one heck of a thing. People somehow think that issues like these are only an issue with Han unification or something, but it's all over European languages as well. Comparing strings for equality is a deeply political issue. reply fhars 12 hours agoparentprevUnicode was never designed for ease of use or efficiency of encoding, but for ease of adoption. And that meant that it had to support lossless round trips from any legacy format to Unicode and back to the legacy format, because otherwise no decision maker would have allowed to start a transition to Unicode for important systems. So now we are saddled with an encoding that has to be bug compatible with any encoding ever designed before. reply striking 12 hours agoparentprevIf you take a peek at an extended ASCII table (like the one at https://www.ascii-code.com/), you'll notice that 0xC5 specifies a precomposed capital A with ring above. It predates Unicode. Accepting that that's the case, and acknowledging that forward compatibility from ASCII to Unicode is a good thing (so we don't have any more encodings, we're just extending the most popular one), and understanding that you're going to have the ring-above diacritic in Unicode anyway... you kind of just end up with both representations. reply arp242 11 hours agorootparentEverything can just be pre-composed; Unicode doesn't need composing characters. There's history here, with Unicode originally having just 65k characters, and hindsight is always 20/20, but I do wish there was a move towards deprecating all of this in favour of always using pre-composed. Also: what you linked isn't \"ASCII\" and \"extended ASCII\" doesn't really mean anything. ASCII is a 7-bit character set with 128 characters, and there are dozens, if not hundreds, of 8-bit character sets with 256 characters. Both CP-1252 and ISO-8859-1 saw wide use for Latin alphabet text, but others saw wide use for text in other scripts. So if you give me a document and tell me \"this is extended ASCII\" then I still don't know how to read it and will have to trail-and-error it. I don't think Unicode after U+007F is compatible with any specific character set? To be honest I never checked, and I don't see in what case that would be convenient. UTF-8 is only compatible with ASCII, not any specific \"extended ASCII\". reply adrian_b 11 hours agorootparentIn my opinion, only the reverse could be true, i.e. that Unicode does not need pre-composed characters because everything can be written with composing characters. The pre-composed characters are necessary only for backwards compatibility. It is completely unrealistic to expect that Unicode will ever provide all the pre-composed characters that have ever been used in the past or which will ever be desired in the future. There are pre-composed characters that do not exist in Unicode because they have been very seldom used. Some of them may even be unused in any language right now, but they have been used in some languages in the past, e.g. in the 19th century, but then they have been replaced by orthographic reforms. Nevertheless, when you digitize and OCR some old book, you may want to keep its text as it was written originally, so you want the missing composed characters. Another case that I have encountered where I needed composed characters not existing in Unicode was when choosing a more consistent transliteration for languages that do not use the Latin alphabet. Many such languages use quite bad transliteration systems, precisely because whoever designed them has attempted to use only whatever restricted character set was available at that time. By choosing appropriate composing characters it is possible to design improved transliterations. reply arp242 9 hours agorootparent> It is completely unrealistic to expect that Unicode will ever provide all the pre-composed characters that have ever been used in the past or which will ever be desired in the future. I agree it's unlikely this will ever happen, but as far as I know there aren't really any serious technical barriers, and from purely a technical point of view it could be done if there was a desire to do so. There are plenty of rarely used codepoints in Unicode already, and while adding more is certainly an inconvenience, the status quo is also inconvenient, which is why we have one of those \"wow, I just discovered Unicode normalisation!\" (and variants thereof) posts on the front-page here every few months. Your last paragraph can be summarize as \"it makes it easier to innovate with new diacritics\". This is actually an interesting point – in the past anyone could \"just\" write a new character and it may or may not get any uptake, just as anyone can \"just\" coin a new word. I've bemoaned this inability to innovate before. That is not inherent to Unicode but computerized alphabets in general, and I that composing characters alleviates at least some of that is probably the best reason I've heard for favouring compose characters. I'm actually also okay with just using composing characters and deprecating the pre-composed forms. Overall I feel that pre-composed is probably better, partly because that's what most text currently uses and partly because it's simpler, but that's the lesser issue – the more important one that it would be nice to move towards \"one obviously canonical\" form that everything uses. reply adrian_b 9 hours agorootparentThere is also another reason that makes the composing characters very convenient right now. Many of the existing typefaces, even some that are quite expensive, do not contain all the pre-composed characters defined by Unicode, especially when those characters have been added in more recent Unicode versions or when they are used only in languages that are not Western European. The missing characters can be synthesized with composing characters. The alternatives, which are to use a font editor to add characters to the typeface or to buy another more complete and more expensive version of the typeface, are not acceptable or even possible for most users. Therefore the fact that Unicode has defined composing characters is quite useful in such cases. reply arp242 8 hours agorootparentEvery avenue opens inconveniences for someone, but I'd rather choose the relatively rare inconvenience of font designers over the relatively common inconvenience of every piece of software ever written. Especially because this can be automated in font design tools, or even font formats itself. reply kps 11 hours agorootparentprev> I don't think Unicode after U+007F is compatible with any specific character set? The ‘early’ Unicode alphabetic code blocks came from ISO 8859 encodings¹, e.g. the Unicode Cyrillic block follows ISO 8859-5, the Greek and Coptic block follows ISO 8859-7, etc. ¹ https://en.wikipedia.org/wiki/ISO/IEC_8859 reply zokier 11 hours agorootparentprevFor roundtripping e.g. https://en.wikipedia.org/wiki/VSCII you do need both composing characters and precomposed characters. reply bandrami 11 hours agorootparentprev> Unicode doesn't need composing characters But it does, IIRC, for both Bengali and Telugu. reply arp242 11 hours agorootparentOnly because they chose to do it like that. It doesn't need to. reply kbolino 8 hours agorootparentConsidering that Unicode did not invent combining diacritics, it follows that simple compatibility with existing encodings demanded it. Now that Unicode's goals have expanded beyond simply representing what already exists, precomposed characters would be too limiting. reply pavel_lishin 11 hours agoparentprevIt might not be ludicrous to suggest that the English letter \"a\" and the Russian letter \"а\" should be a single entity, if you don't think about it very hard. But the English letter \"c\" and the Russian letter \"с\" are completely different characters, even if at a glance they look the same - they make completely different sounds, and are different letters. It would be ludicrous to suggest that they should share a single symbol. reply josephcsible 10 hours agorootparentIf they're always supposed to look the same, then Unicode should encode them the same, even if they mean different things in different contexts. reply pavel_lishin 10 hours agorootparentTwo counterpoints: 1. Unicode isn't a method of storing pixel or graphic representations of writing systems; it's meant to store text, regardless of how similar certain characters look. 2. What do you do about screen readers & the like? If it encounters something that looks like a little half-moon glyph that's in the middle of a sentence about foreign alphabets that reads \"Por ejemplo, la letra 'c'\", should it pronounce it as the English \"see\" or as Russian \"ess\"? reply patal 1 hour agorootparent1. \"graphic representation of writing systems\" and \"text\" mean the same thing to me. Do you mean text as spoken? 2. I think the pronunciation should not be encoded into the text representation on a general scale. You would need different encodings for \"though\" and \"through\" in english alone. Your example leaves the meaning open, even if being read as text. If I was the editor, and the distinction was important, I'd change it to \"For example, the cyrillic letter 'c'\". I understand that Unicode provides different code points for same-looking characters, mostly because of history, where these characters came from different code sheets in language-specific encodings. reply Izkata 9 hours agorootparentprevFor 8 minutes of this (among other translation mistakes), you've reminded me of Peggy Hill's understanding of Spanish in the cartoon King of the Hill - https://www.youtube.com/watch?v=g62A1vkSxB0 (IIRC, she learned the language entirely from books so has no idea of the correct pronunciation and thinks she's fluent) reply Joker_vD 9 hours agorootparentprevWhat about Latin \"k\" and Cyrillic \"к\"? Do they look the same in your font of choice? Should they? reply josephcsible 7 hours agorootparentThose look different, so I have no issue with them being different code points. reply eviks 6 hours agorootparentBut they don't \"fundamentally\" look different, it's font dependent(there are fonts where they look the same), just like the same Latin k will look different depending on a font, so you need a better rule to make your own simple Unicode reply allarm 2 hours agorootparentprevThe capitalized \"K\" and \"К\" look exactly the same though. reply bawolff 11 hours agoparentprevMaybe, but then you can no longer round trip with other encodings, which seems worse to me. reply jesprenj 14 hours agoprevShould you really change filenames of users' files and depend on the fact that they are valid utf8? Wouldn't it be better to keep the original filename and use that most of the time sans the searches and indexing? Why don't you normalize latin alphabets filenames for indexing even further -- allow searching for \"Führer\" with queries like \"Fuehrer\" and \"Fuhrer\"? reply zeroCalories 13 hours agoparentI generally agree that you shouldn't change the file name, but in reality I bet OP stored it as another column in a database. For more aggressive normalization like that, I think it makes more sense to implement something like a spell checker that suggests similar files. reply layer8 15 hours agoprevThe more general solution is specified here: https://unicode.org/reports/tr10/#Searching reply bawolff 11 hours agoparentCollation and normal forms are totally different things with different purposes and goals. Edit: reread the article. My comment is silly. UCA is the correct solution to the author's problem. reply chuckadams 14 hours agoprevClearly the author already knows this, but it highlights the importance of always normalizing your input, and consistently using the same form instead of relying on the OS defaults. reply makeitdouble 12 hours agoparentThe larger point is probably that search and comparison are inherently hard as what humans understand as equivalent isn't the same for the machine. Next stop will be upper case and lower case. Then different transcriptions of the same words in CJK. reply mckn1ght 13 hours agoparentprevAlso, never trust user input. File names are user inputs. You can execute XSS attacks via filenames on an unsecured site. reply blablabla123 13 hours agoprevAs a German macOS user with US keyboard I run into a related issue every now and then. What's nice about macOS is I can easily combine Umlaute but also other common letters from European languages without any extra configuration. But some (Web) Applications stumble upon it, while entering because it's like: 1. ¨ (Option-u) 2. ü (u pressed) reply kps 11 hours agoparentEarly on, Netscape effectively exposed Windows keyboard events directly to Javascript, and browsers on other platforms were forced to try to emulate Windows events, which is necessarily imperfect given different underlying input systems. “These features were never formally specified and the current browser implementations vary in significant ways. The large amount of legacy content, including script libraries, that relies upon detecting the user agent and acting accordingly means that any attempt to formalize these legacy attributes and events would risk breaking as much content as it would fix or enable. Additionally, these attributes are not suitable for international usage, nor do they address accessibility concerns.” The current method is much better designed to avoid such problems, and has been supported by all major browsers for quite a while now (the laggard Safari arriving 7 years from this Tuesday). https://www.w3.org/TR/uievents reply _nalply 14 hours agoprevSometimes it makes sense to reduce to Unicode confusables. For example the Greek letter Big Alpha looks like uppercase A. Or some characters look very similar like the slash and the fraction slash. Yes, Unicode has separate scalar values for them. There are Open Source tools to handle confusables. This is in addition to the search specified by Unicode. reply wanderingstan 14 hours agoparentI wrote such a library for Python here: https://github.com/wanderingstan/Confusables My use case was to thwart spammers in our company’s channels, but I suppose it could be used to also normalize accent encoding issues. Basically converts a phrase into a regular expression matching confusables. E.g. \"ℍ℮1೦\" would match \"Hello\" reply _nalply 13 hours agorootparentInteresting. What would you think about this approach: reduce each character to a standard form which is the same for all characters in the same confusable group? Then match all search input to this standard form. This means \"ℍ℮1l೦\" is converted to \"Hello\" before searching, for example. reply wanderingstan 13 hours agorootparentIt’s been a long time since I wrote this, but I think the issue with that approach is the possibility of one character being confusable with more than one letter. I.e. there may not be a single correct form to reduce to. reply wyldfire 14 hours agoparentprev> For example the Greek letter Big Alpha looks like uppercase A. If they're truly drawn the same (are they?) then why have a distinct encoding? reply schoen 14 hours agorootparentOne argument would be that you can apply functions to change their case. For example in Python >>> \"Ᾰ̓ΡΕΤΉ\".lower() 'ᾰ̓ρετή' >>> \"AWESOME\".lower() 'awesome' The Greek Α has lowercase form α, whereas the Roman A has lowercase form a. Another argument would be that you want a distinct encoding in order to be able to sort properly. Suppose we used the same codepoint (U+0050) for everything that looked like P. Then Greek Ρόδος would sort before Greek Δήλος because Roman P is numerically prior to Greek Δ in Unicode, even though Ρ comes later than Δ in the Greek alphabet. reply mmoskal 13 hours agorootparentApparently this works very well, except for a single letter, Turkish I. Turkish has two version of 'i' and Unicode folks decided to use the Latin 'i' for lowercase dotted i, and Latin 'I' for uppercase dot-less I (and have two new code points for upper-case dotted I and lower-case dot-less I). Now, 'I'.lower() depends on your locale. A cause for a number of security exploits and lots of pain in regular expression engines. edit: Well, apparently 'I'.lower() doesn't depend on locale (so it's incorrect for Turkish languages); in JS you have to do 'I'.toLocaleLowerCase('tr-TR'). Regexps don't support it in neither. reply ninkendo 13 hours agorootparentprevTo me, it depends on what you think Unicode’s priorities should be. Let’s consider the opposite approach, that any letters that render the same should collapse to the same code point. What about Cherokee letter “go” (Ꭺ) versus the Latin A? What if they’re not precisely the same? Should lowercase l and capital I have the same encoding? What about the Roman numeral for 1 versus the letter I? Doesn’t it depend on the font too? How exactly do you draw the line? If Unicode sets out to say “no two letters that render the same shall ever have different encodings”, all it takes is one counterexample to break software. And I don’t think we’d ever get everyone to agree on whether certain letters should be distinct or not. Look at Han unification (and how poorly it was received) for examples of this. To me it’s much more sane to say that some written languages have visual overlap in their glyphs, and that’s to be expected, and if you want to prevent two similar looking strings from being confused with one another, you’re going to have to deploy an algorithm to de-dupe them. (Unicode even has an official list of this called “confusables”, devoted to helping you solve this.) reply layer8 13 hours agorootparentprevThey can be drawn the same, but when combining fonts (one latin, one greek), they might not. Or, put differently, you don’t want to require the latin and greek glyphs to be designed by the same font designer so that “A” is consistent with both. There are more reasons: – As a basic principle, Unicode uses separate encodings when the lower/upper case mappings differ. (The one exception, as far as I know, being the Turkish “I”.) – Unicode was designed for round-trip compatibility with legacy encodings (which weren’t legacy yet at the time). To that effect, a given script would often be added as whole, in a contiguous block, to simplify transcoding. – Unifying characters in that way would cause additional complications when sorting. reply andrewaylett 12 hours agorootparentprevIn some cases, because they have distinct encodings in a pre-Unicode character set. Unicode wants to be able to represent any legacy encoding in a lossless manner. ISO8859-7 encodes Α and A to different code-points, and ISO8859-5 has А at yet another code point, so Unicode needs to give them different encodings too. And, indeed, they are different letters -- as sibling comments point out, if you want to lowercase them then you wind up with α, a, and а, and that's not going to work very well if the capitals have the same encoding. reply michaelt 13 hours agorootparentprevUnicode's \"Han Unification\" https://en.wikipedia.org/wiki/Han_unification aimed to create a unified character set for the characters which are (approximately) identical between Chinese, Japanese, Korean and Vietnamese. It turns out this is complex and controversial enough that the wikipedia page is pretty gigantic. reply adzm 14 hours agorootparentprev> If they're truly drawn the same (are they?) then why have a distinct encoding? They may be drawn the same or similar in some typefaces but not all. reply crote 8 hours agorootparentprevBecause some characters which look the same need to be treated differently depending on context. A 'toLowercase' function would convert Α->α, but A->a. That would be impossible if both variants had the same encoding. reply mgaunard 13 hours agorootparentprevBecause graphemes and glyphs are different things. reply hanche 13 hours agorootparentprevYou may be amused to learn about these, then: U+2012 FIGURE DASH, U+2013 EN DASH and U+2212 MINUS SIGN all look exactly the same, as far as I can tell. But they have different semantics. reply layer8 13 hours agorootparentThey don’t necessarily look the same. The distinction is typographic, and only indirectly semantic. Figure dash is defined to have the same width as a digit (for use in tabular output). Minus sign is defined to have the same width and vertical position as the plus sign. They may all three differ for typographic reasons. reply hanche 12 hours agorootparentAh, good point. But typography is supposed to support the semantics, so at least I was not totally wrong. reply ahazred8ta 13 hours agorootparentprevIn Hawaiʻi, there's a constant struggle between the proper ʻokina, left single quote, and apostrophe. reply samatman 10 hours agorootparentprevThe basic answer here is that Unicode exists to encode characters, or really, scripts and their characters. Not typefaces or fonts. Consider broadcasting of text in Morse code. The Morse for the Cyrillic letter В is International Morse W. In the early years of Unicode, conversion from disparate encodings to Unicode was an urgent priority. Insofar as possible, they wanted to preserve the collation properties of those encodings, so the characters were in the same order as the original encoding whenever they could be. But it's more that Unicode encodes scripts, which have characters, it doesn't encode shapes. With 10,000 caveats to go with that, Unicode is messy and will preserve every mistake until the end of time. But encoding Α and A and А as three different letters, that they did on purpose, because they are three different letters, because they're a part of three different scripts. reply schoen 3 hours agorootparentIt occurs to me (after mentioning collation order, in a different part of this thread, as one reason that we would want to distinguish scripts) that it might be unclear even for collation purposes when scripts are or are not distinct, especially for Cyrillic, Latin, and Arabic scripts which are used to write many different languages which have often added their own extensions. I guess the official answer is \"attempt to distinguish everything that any language is known to distinguish, and then use locales to implement different collation orders by language\", or something like that? But it's still not totally obvious how one could make a principled decision about, say, whether the encoding of Persian and Urdu writing (obviously including their extensions) should be unified with the encoding of Arabic writing. One could argue that Nastaliq is like a \"font\"... or not... reply Havoc 13 hours agoprevFor those intrigued by this sort of thing check tech talk “plain text” by Dylan Beattie Absolute gem. His other talks are entertaining too reply hanche 11 hours agoparentHe seems to have done that talk several times. I watched the 2022 one. Time well spent! reply userbinator 13 hours agoprevits[sic] 2024, and we are still grappling with Unicode character encoding problems More like \"because it's 2024.\" This wouldn't be a problem before the complexity of Unicode became prevalent. reply bornfreddy 13 hours agoparentYou mean this wouldn't be a problem if we used the myriad different encodings like we did before Unicode, because we would probably not be able to even save the files anyway? So true. reply userbinator 12 hours agorootparentBefore Unicode, most systems were effectively \"byte-transparent\" and encoding only a top-level concern. Those working in one language would use the appropriate encoding (likely CP1252 for most Latin languages) and there wouldn't be confusion about different bytes for same-looking characters. reply deathanatos 11 hours agorootparentA single user system, perhaps. I've worked on a system that … well, didn't predate Unicode, but was sort of near the leading edge of it and was multi-system. The database columns containing text were all byte arrays. And because the client (a Windows tool, but honestly Linux isn't any better off here) just took a LPCSTR or whatever, it they bytes were just in whatever locale the client was. But that was recorded nowhere, and of course, all the rows were in different locales. I think that would be far more common, today, if Unicode had never come along. reply bawolff 11 hours agorootparentprevMy understanding is way back in the day, people would use ascii backspace to combine an ascii letter with an ascii accent character. reply kps 10 hours agorootparentASCII 1967 (and the equivalent ECMA-6) suggested this, and that the characters ,\"'`~ could be shaped to look like a cedilla, diaeresis, acute accent, grave accent, and raised tilde respectively for that purpose. But I've never once seen or heard of that method used. ASCII also allowed the characters @[\\]^{|}~ to be replaced by others in ‘national character allocations’, and this was commonly used in the 7-bit ASCII era. In the 8-bit days, for alphabetic scripts, typically the range 0xA0–0xFF would represent a block of characters (e.g. an ISO 8859¹ range) selected by convention or explicitly by ISO 2022². (There were also pre-standard similar methods like DEC NRCS and IBM's EBCDIC code pages.) ¹ https://en.wikipedia.org/wiki/ISO/IEC_8859 ¹ https://en.wikipedia.org/wiki/ISO/IEC_2022 reply bawolff 9 hours agorootparentGoogling i saw people link to http://git.savannah.gnu.org/cgit/bash.git/tree/doc/bash.0 as an example of overstriking (albeit for bold not accents). The telnet rfc also makes reference to it. I also see lots of references in the context of APL. I suppose in the 60s/70s it would be in the era of teletypewriters where maybe over striking would more naturally be a thing. I also found references to less supporting this sort of thing, but seems to be about bold and underline, not accents. reply kps 6 hours agorootparentnroff did do overstriking for underlining and bold. I don't remember if it did so for accents, but in any case it was for printer output and not plain text itself. APL did use overstriking extensively, and there were video terminals that knew how to compose overstruck APL characters. reply n2d4 13 hours agoparentprevYou make it sound like non-English languages were invented in 2024 reply bawolff 11 hours agoparentprevCombining characters go back to the 90s. The unicode normal forms were defined in the 90s. None of this is new at this point. reply mschuster91 10 hours agoparentprev> This wouldn't be a problem before the complexity of Unicode became prevalent. It was a problem even before then. It worked fine as long as you had countries that were composed of one dominant ethnicity that sharted upon how minorities and immigrants lived (they were just forced to use a transliterated name, which could be one hell of a lot of fun for multi-national or adopted people) - and even that wasn't enough to prevent issues. In Germany, for example, someone had to go up to the highest public-service courts in the late 70s [1] to have his name changed from Götz to Goetz because he was pissed off that computers were unable to store the ö and so he'd liked to change his name rather than keep getting mis-named, but German bureaucracy does not like name changes outside of marriage and adoption. [1] https://www.schweizer.eu//aktuelles/urteile/7304-bverwg-vom-... reply anewhnaccount2 3 hours agoprevReminded of this classic diveintomark post http://web.archive.org/web/20080209154953/http://diveintomar... reply mawise 14 hours agoprevI ran into this building search for a family tree project. I found out that Rails provides `ActiveSupport::Inflector.transliterate()` which I could use for normalization. reply CoastalCoder 12 hours agoprevIsn't ü/ü-encoding a solved problem on Unix systems?reply jph 14 hours agoprevNormalizing can help with search. For example for Ruby I maintain this gem: https://rubygems.org/gems/sixarm_ruby_unaccent reply noname120 13 hours agoparentWow the code[1] looks horrific! Why not just do this: string → NFD → strip diacritics → NFC? See [2] for more. [1] https://github.com/SixArm/sixarm_ruby_unaccent/blob/eb674a78... [2] https://stackoverflow.com/a/74029319/3634271 reply jph 10 hours agorootparentSure does look horrific. :-) That's because it's the same code from 2008, long before Ruby had the Unicode handlers. In fact it's the same code as for many other programming languages, all the way back to Perl in the mid-1990s. I didn't create it; I merely ported it from Perl to Ruby. More important, the normalization does more than just diacritics. For example, it converts superscript 2 to ASCII 2. A better naming convention probably would have been \"string normalize\" or \"searchable string\" or some such, but the naming convention in 2012 was based on Perl. reply kazinator 14 hours agoprevOh that Mötley Ünicöde. reply lxgr 14 hours agoparentI'm aware of the \"metal umlaut\" meme, but as a German native speaker, I can't not read these in my head in a way that sounds much less Metal than probably intended :) reply 082349872349872 14 hours agorootparent> \"When we finally went to Germany, the crowds were chanting, ‘Mutley Cruh! Mutley Cruh!’ We couldn’t figure out why the fuck they were doing that.\" —VNW reply ooterness 13 hours agorootparentprevThe best metal umlauts are placed on a consonant (e.g., Spın̈al Tap). This makes it completely clear when it's there for aesthetics and not pronunciation. reply Symbiote 11 hours agorootparentprevYears ago, an American metalhead was added to a group chat before she came to visit. She was called Daniela, but she'd written it \"Däniëlä\". When my Swedish friend met her in person, havin seen her name in the group chat, he said something like \"Hej, Dayne-ee-lair right? How was the flight?\". reply yxhuvud 12 hours agorootparentprevYes, those umlauts made it sound more like a fake french accent. reply ginko 13 hours agorootparentprevI will always pronounce the umlaut in Motörhead. Lemmy brought that on himself. reply 082349872349872 14 hours agoparentprevIt can encode Spın̈al Tap, so it's all good. reply chuckadams 14 hours agorootparentOh sweet summer child, i̶̯͖̩̦̯͉͈͎͛̇͗̌͆̓̉̿̇̚͜͝͠ͅt̶̥̳͙̺̀͊͐͘ ̷̧͉̲̩̩̠̥̀̍̔͝c̸̢̛̙̦͙̠̱̖̠͆̆̄̈́͋͘ą̴̩̪̻̭̐́̒n̶̡̛̛̳̗̦͚̙̖͓̝̻̓̔̎̎̅̒͊ͅ ̵̰̞̰̺̠̲̯̤̠̹̯̩͚̥̗͌̓e̴̪̯̠͙̩̝͓̎́̋̈́̂̓̏̈͗͛̓̀̾͗͘n̶͕̗̣͙̺̰̠͐́͆̀́̌͑̔̊̚ĉ̴̗͔̼̦̟̰͐̌̂̅͋̄̄͘̕̚o̵̧͙̤͔̻̞̝̯̱̰̤̻̠̝̎͐̈́̈̐͆͑̃̀̏̂͝͠͝d̸͕̼̀̐̚ế̴̢̢̡̳͇̪̤͇͉̳̟̈̈̈́̎̀̋͆͊̃̓͛̈́͘ ̷̞̞̜̖͇̱̞͔̈́͋̈́̃̎̇̈͜͝ͅs̷̢̡͚͉͚̬̙̼̾̅̀̊̈́̏̇͘͜ö̸̥̠̲̞̪̦͚̞̝̦́̃̈́́̊͐̾̏̂͂̓̋͋̚͠ ̶̞̺̯̖͓̞͇̳͈̗͖̗̫̍̌̋̈͗̉͝͠m̶̳̥͔͔͚̈́̕̕̚͘͜͠u̵͚̓͗̔̐̽̍ċ̷̨̢̡̛̭͓̪͕̗̝̟͓̩͇͒̽͒͑̃́̇͌̊͊̄̈́͘͜h̶̳̮̟̃͂͛̑̚̚ ̵̢͉̣̲͇͕̈̈̍̕͘ͅm̴̱͙̜͔̋̐̅͗̋̈̀̌͛̈͘̕͠o̷̧̡̮̜͎͙̖̞͈̘̩̙͓̿̆̀̋͜r̶͙̗̯͎̎͛̌̈́̂̓̈̑̅̓͊̒̊̑̈ę̷͕͉̲̟̽̄͒̍͑̀̿̔̒̃̅̿́͘͝ͅ.̷̡̧̻̘̝̞̹̯̞͚̱̼͓̠͇̌̅͂.̷̧̫͙̮̞̳̼̤̪̖̦̟͕̏̐͑̾̈́̀̅͌̓.̵̧̛̛̖̥͔͍̲̲͉̺̩̪̭̋́̓̌͂̽̋̃̎͋͆͝͠ͅ reply 082349872349872 14 hours agorootparentTIL about https://esolangs.org/wiki/Zalgo#Number_to_String reply ComputerGuru 11 hours agoprevZFS can be configured to force the use of a particular normalized Unicode form for all filenames. Amazing filesystem. reply WalterBright 6 hours agoprev> Can you spot any difference between “blöb” and “blöb”? That's where Unicode lost its way and went into a ditch. Identical glyphs should always have the same code point (or sequence of code points). Imagine all the coding time spent trying to deal with this nonsense. reply euroderf 58 minutes agoparentA fine sentiment, but (FWIW) it goes into a ditch when dealing with CJK. reply earthboundkid 11 hours agoprevThis isn’t an encoding problem. It’s a search problem. reply ulrischa 12 hours agoprevIt is really so awful that we have to deal with encoding issues in 2024. reply NotYourLawyer 13 hours agoprevASCII should be enough for anyone. reply euroderf 55 minutes agoparentFilling the upper 128 characters with box-drawing characters was all well & fine, but you'd think IBM might've given some thought instead to defining a character set that would have maximum applicability for the set of all (Roman alphabet -descended) Western languages. (Plus pinyin.) reply zzo38computer 13 hours agoparentprevASCII is good for a lot of stuff, but not for everything. Sometimes, other character sets/encodings will be better, but which one is better depends on the circumstances. (Unicode does have many problems, though. My opinion is that Unicode is no good.) reply hanche 13 hours agoparentprevAnd who needs more than 640 kilobytes of memory anyhow? reply mckn1ght 13 hours agorootparentDon’t forget butterflies in case you need to edit some text. reply juujian 13 hours agoprevI ran into encoding problems so many times, I just use ASCII aggressively now. There is still kanji, Hanzi, etc. but at least for Western alphabets, not worth the hassle. reply zzo38computer 13 hours agoparentI also just use ASCII when possible; it is the most likely to work and to be portable. For some purposes, other character sets/encodings are better, but which ones are better depends on the specific case (not only what language of text but also the use of the text in the computer, etc). reply arp242 11 hours agoparentprevThis works fine as a personal choice, but doesn't really work if you're writing something other random people interact with. Even for just English it doesn't work all that well because it lacks things like the Euro which is fairly common (certainly in Europe), there are names with diacritics (including \"native\" names, e.g. in Ireland it's common), there are too many loanwords with diacritics, and ASCII has a somewhat limited set of punctuation. There are some languages where this can sort of work (e.g. Indonesian can be fairly reliably written in just ASCII), although even there you will run in to some of these issue. It certainly doesn't work for English, and even less for other Latin-based European languages. reply layer8 13 hours agoparentprevThe article isn’t about non-Unicode encodings. reply juujian 13 hours agorootparentMeant to write ASCII reply keybored 13 hours agoprevI try to avoid Unicode in filenames (I’m on Linux). It seems that a lot of normal users might have the same intuition as well? I get the sense that a lot will instinctually transcode to ASCII, like they do for URLs. reply zzo38computer 13 hours agoparentI also try to avoid non-ASCII characters in file names (and I am also on Linux). I also like to avoid spaces and most punctuations in file names (if I need word separation I can use underscores or hyphens). reply skissane 11 hours agorootparentSometimes I wish they had disallowed spaces in file names. Historically, many systems were very restrictive in what characters are allowed in file names. In part in reaction to that, Unix went to the other extreme, allowing any byte except NUL and slash. I think that was a mistake - allowing C0 control characters in file names (bytes 0x01 thru 0x1F) serves no useful use case, it just creates the potential for bugs and security vulnerabilities. I wish they’d blocked them. POSIX debated banning C0 controls, although appears to have settled on just a recommendation (not a mandate) that implementations disallow newline: https://www.austingroupbugs.net/view.php?id=251 reply samatman 10 hours agorootparentI firmly agree that control characters, including tab and newline, should have been shown the door decades ago. All they do is make problems. But spaces in filenames are really just an inconvenience at most for heavy terminal users, and are a natural thing to use for basically everyone else. All my markdown files are word-word-word.md, but all my WYSIWIG documents are \"Word word word.doc\". The hassle of constantly explaining to angry civilians \"why won't it let me write this file\" would be worse than the hassle of having to quote or backslash-escape the occasional path in the shell. reply skissane 10 hours agorootparentSpaces in file names are the cause of countless bugs in shell scripts, even C code which uses APIs like system() or popen(). Yes, solutions exist to those issues, but many people forget, and they add complexity which otherwise might not be necessary. For non-technical WYSIWYG users, there is a simple solution: auto-replace space with underscore when user enters a filename containing it; you could even convert the underscore back to a space on display. Some GUIs already do stuff like this anyway - e.g. macOS exchanging slash and colon in its GUI layer (primarily for backward compatibility with Classic MacOS where slash not colon was the path separator.) reply eviks 5 hours agorootparentIf you have the power of a wish, why do you wish to make the world worse without such a common thing like spaces instead of wishing for the legacy APIs to have a better solution? reply raffy 13 hours agoprev [–] I created a bunch of Unicode tools during development of ENSIP-15 for ENS (Ethereum Name Service) ENSIP-15 Specification: https://docs.ens.domains/ensip/15 ENS Normalization Tool: https://adraffy.github.io/ens-normalize.js/test/resolver.htm... Browser Tests: https://adraffy.github.io/ens-normalize.js/test/report-nf.ht... 0-dependancy JS Unicode 15.1 NFC/NFD Implementation [10KB] https://github.com/adraffy/ens-normalize.js/blob/main/dist/n... Unicode Character Browser: https://adraffy.github.io/ens-normalize.js/test/chars.html Unicode Emoji Browser: https://adraffy.github.io/ens-normalize.js/test/emoji.html Unicode Confusables: https://adraffy.github.io/ens-normalize.js/test/confused.htm... reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The author faced a search filtering problem with file names having diacritical marks (e.g., umlauts) on their product at epilot due to encoding transformations, which they resolved by matching the encoding type of the search file names with the saved ones using .normalize().",
      "To address the issue comprehensively, a migration script was developed to standardize the names of all current files containing non-ASCII characters."
    ],
    "commentSummary": [
      "The article addresses text normalization challenges with non-Latin characters on various platforms, focusing on names in official documents and legacy systems.",
      "It emphasizes problems related to Unicode handling, transliterations, file names, encoding, and sorting across different languages.",
      "Discussion includes the importance of consistent encoding, pronunciation representation, and utilizing composing characters in Unicode to improve search results, prevent compatibility problems, and enhance text manipulation in programming."
    ],
    "points": 133,
    "commentCount": 191,
    "retryCount": 0,
    "time": 1711299041
  }
]
