[
  {
    "id": 39718558,
    "title": "Ollama Adds Support for AMD Graphics Cards",
    "originLink": "https://ollama.com/blog/amd-preview",
    "originBody": "Ollama now supports AMD graphics cards March 14, 2024 Ollama now supports AMD graphics cards in preview on Windows and Linux. All the features of Ollama can now be accelerated by AMD graphics cards on Ollama for Linux and Windows. Supported graphics cards Family Supported cards and accelerators AMD Radeon RX 7900 XTX 7900 XT 7900 GRE 7800 XT 7700 XT 7600 XT 7600 6950 XT 6900 XTX 6900XT 6800 XT 6800 Vega 64 Vega 56 AMD Radeon PRO W7900 W7800 W7700 W7600 W7500 W6900X W6800X Duo W6800X W6800 V620 V420 V340 V320 Vega II Duo Vega II VII SSG AMD Instinct MI300X MI300A MI300 MI250X MI250 MI210 MI200 MI100 MI60 MI50 Support for more AMD graphics cards is coming soon. Get started To get started with Ollama with support for AMD graphics cards, download Ollama for Linux or Windows.",
    "commentLink": "https://news.ycombinator.com/item?id=39718558",
    "commentBody": "Ollama now supports AMD graphics cards (ollama.com)590 points by tosh 19 hours agohidepastfavorite206 comments Zambyte 18 hours agoIt's pretty funny to see this blog post, when I have been running Ollama on my AMD RX 6650 for weeks :D They have shipped ROCm containers since 0.1.27 (21 days ago). This blog post seems to be published along with the latest release, 0.1.29. I wonder what they actually changed in this release with regards to AMD support. Also: see this issue[0] that I made where I worked through running Ollama on an AMD card that they don't \"officially\" support yet. It's just a matter of setting an environment variable. [0] https://github.com/ollama/ollama/issues/2870 Edit: I did notice one change, now the starcoder2[1] model works now. Before that would crash[2]. [1] https://ollama.com/library/starcoder2 [2] https://github.com/ollama/ollama/issues/2953 reply mchiang 16 hours agoparentWhile the PRs went in slightly earlier, much of the time was spent on testing the integrations, and working with AMD directly to resolve issues. There were issues that we resolved prior to cutting the release, and many reported by the community as well. reply Zambyte 14 hours agorootparentThank you for clarifying and thanks for the great work you do! reply cjbprime 8 hours agoparentprevMaybe they wanted to wait for bug reports for 21 days before publishing a popular blog post about it..? reply layoric 3 hours agoparentprevWhat kind of performance do you see with an RX 6650? reply throwaway5959 18 hours agoparentprevI mean, it was 21 days ago. What’s the difference? reply yjftsjthsd-h 18 hours agorootparent2 versions, apparently reply freedomben 17 hours agoprevI'm thrilled to see support for RX 6800/6800 XT / 6900 XT. I bought one of those for an outrageous amount during the post-covid shortage in hopes that I could use it for ML stuff, and thus far it hasn't been very successful, which is a shame because it's a beast of a card! Many thanks to ollama project and llama.cpp! reply mey 17 hours agoparentSad to see that the cut off is just after 6700 XT which is what is in my desktop. They indicate more devices are coming, hopefully that includes some of the more modern all in one chips with RDNA 2/3 from AMD as well. reply mey 17 hours agorootparentIt appears that the cut off lines up with HIP SDK support from AMD, https://rocm.docs.amd.com/projects/install-on-windows/en/lat... reply bavell 8 hours agorootparentprevI've been using my 6750XT for more than a year now on all sorts of AI projects. Takes a little research and a few env vars but no need to wait for \"official\" support most of the time. reply throawayonthe 15 hours agorootparentprevI’ve already been using ollama with my 6700xt just fine, you just have to set some env variable to make rocm work “unoficially” The linked page says they will support more soon, so i’m guessing this will just be integrated reply Rising6378 3 hours agorootparentwhich env variable did you set to what? currently I'm struggeling setting it up reply eclectic29 16 hours agoprevI'm not sure why Ollama garners so much attention. It has limited value - used for only experimenting with models + cannot support more than 1 model at a time. It's not meant for production deployments. Granted that it makes the experimentation process super easy but for something that relies on llama.cpp completely and whose main value proposition is easy model management I'm not sure it deserves the brouhaha people are giving it. Edit: what do you do after the initial experimentation? you need to deploy these models eventually to production. I'm not even talking about giving credit to llama.cpp, just mentioning that this product is gaining disproportionate attention and kudos compared to the value it delivers. Not denying that it's a great product. reply kergonath 13 hours agoparent> It's not meant for production deployments. I am probably not the demographics you expect. I don’t do “production” in that sense, but I have ollama running quite often when I am working, as I use it for RAG and as a fancy knowledge extraction engine. It is incredibly useful: - I can test a lot of models by just pulling them (very useful as progress is very fast), - using their command line is trivial, - the fact that it keeps running in the background means that it starts once every few days and stays out of the way, - it integrates nicely with langchain (and a host of other libraries), which means that it is easy to set up some sophisticated process and abstract away the LLM itself. > what do you do after the initial experimentation? I just keep using it. And for now, I keep tweaking my scripts but I expect them to stabilise at some point, because I use these models to do some real work, and this work is not monkeying about with LLMs. > I'm not even talking about giving credit to llama.cpp, just mentioning that this product is gaining disproportionate attention and kudos compared to the value it delivers. For me, there is nothing that comes close in terms of integration and convenience. The value it delivers is great, because it enables me to do some useful work without wasting time worrying about lower-level architecture details. Again, I am probably not in the demographics you have in mind (I am not a CS person and my programming is usually limited to HPC), but ollama is very useful to me. Its reputation is completely deserved, as far as I am concerned. reply rkwz 7 hours agorootparent> I use it for RAG and as a fancy knowledge extraction engine Curious, can you share more details about your usecase? reply kergonath 17 minutes agorootparentThe use case is exploratory literature review in a specific scientific field. I have a setup that takes pdfs and does some OCR and layout detection with Amazon, and then bunch them with some internal reports. Then, I have a pipeline to write summaries of each document and another one to slice them into chunks, get embeddings and set up a vector store for a RAG chat bot. At the moment it’s using Mixtral and the command line. But I like being able to swap LLMs to experiments with different models and quantisation without hassle, and I more or less plan to set this up on a remote server to free some resources on my workstation so the web UI could come in handy. Running this locally is a must for confidentiality reasons. I’d like to get rid of Textract as well, but unfortunately I haven’t found a solution that’s even close. Tesseract in particular was very disappointing. reply idncsk 1 hour agorootparentprevTry ollama webui(now open-webui). Sry on my phone now => no links reply cbhl 16 hours agoparentprevIn my opinion, pre-built binaries and an easy-to-use front-end are things that should exist and are valid as a separate project unto themselves (see, e.g., HandBrake vs ffmpeg). Using the name of the authors or the project you're building on can also read like an endorsement, which is not _necessarily_ desirable for the original authors (it can lead to ollama bugs being reported against llama.cpp instead of to the ollama devs and other forms of support request toil). Consider the third clause of BSD 3-Clause for an example used in other projects (although llama.cpp is licensed under MIT). reply crooked-v 16 hours agoparentprev> Granted that it makes the experimentation process super easy That's the answer to your question. It may have less space than a Zune, but the average person doesn't care about technically superior alternatives that are much harder to use. reply thejohnconway 12 hours agorootparent*Nomad And lame. reply nerdix 16 hours agoparentprevThe answer to your question is: ollama run mixtral That's it. You're running a local LLM. I have no clue how to run llama.cpp I got Stable Diffusion running and I wish there was something like ollama for it. It was painful. reply viraptor 14 hours agorootparentOn a mac, https://drawthings.ai is the ollama of Stable Diffusion. reply ghurtado 14 hours agorootparentprevFor me, ComfyUI made the process of installing and playing with SD about as simple as a Windows installer. reply jameshart 13 hours agorootparentprevThe README is pretty clear, albeit it talks about a lot of optional steps you don’t need, but it’s essentially gonna be something like: git clone https://github.com/ggerganov/llama.cpp.git cd llama.cpp make wget https://huggingface.co/TheBloke/Mixtral-8x7B-v0.1-GGUF/resolve/main/mixtral-8x7b-v0.1.Q4_K_M.gguf?download=true ./main -m ./mixtral-8x7b-v0.1.Q4_K_M.gguf -n 128 reply verdverm 13 hours agorootparentThis shows the value ollama provides I only need to know the model name and then run a single command reply hnfong 9 hours agorootparentThe first 3 steps GP provided are literally just the steps for installation. The \"value\" you mentioned is just a packaged installer (or, in the case of Linux, apparently a `curlsh` -- and I'd much prefer the git clone version). On multiple occasions I've been modifying llama.cpp code directly and recompiling for my own purposes. If you're using ollama on the command line, I'd say having the option to easily do that is much more useful than saving a couple commands upon installation. reply verdverm 1 hour agorootparentWhen I get to the point of modification, I will go with Python. This is where the AI ecosystem is largely at I stopped using C++ when Go came out, no interest in ever having to write it again. reply jameshart 13 hours agorootparentprevIt should be fairly obvious that one can find alternative models and use them in the above command too. Look, I’m not arguing that a prebuilt binary that handles model downloading has no value over a source build and manually pulling down gguf files. I just want to dispel some of the mystery. Local LLM execution doesn’t require some mysterious voodoo that can only be done by installing and running a server runtime. It’s just something you can do by running code that loads a model file into memory and feeds tokens to it. More programmers should be looking at llama.cpp language bindings than at Ollama’s implementation of the openAI api. reply roenxi 10 hours agorootparentThere are 5 commands in that README two comments up, 4 can reasonably fail (I'll give cd high marks for reliability). `make` especially is a minefield and usually involves a half-hour of searching the internet and figuring out which dependencies are a problem today. And that is all assuming someone is comfortable with compiled languages. I'd hazard most devs these days are from JS land and don't know how to debug make. Finding the correct model weights is also a challenge in my experience, there are a lot of alternatives and it is often difficult to figure out what the differences are and whether they matter. The README is clear that I'm probably about to lose an hour debugging if I follow it. It might be one of those rare cases where it works first time but that is the exception not the rule. reply jameshart 3 minutes agorootparentYour mileage may vary. It runs first time for me on an Apple Silicon Mac. verdverm 13 hours agorootparentprevI'd rather focus on building on top of of LLMs than going lower level Ollama makes that super easy. I tried llama.cpp first and hit build issues. Ollama worked out of the box reply jameshart 13 hours agorootparentSure. Just be aware that there’s a lot of expressive difference between building on top of an HTTP API vs on top of a direct interface to the token sampler and model state. reply verdverm 13 hours agorootparentI'm aware, I don't need that amount of sophistication yet. Python seems to be the way to go deeper though. Is there a good reason I should be aware of to pick llama.cpp over python? reply jameshart 12 hours agorootparentPython’s as good a choice as any for the application layer. You’re either going to be using PyTorch or llama-cpp-python to get the CUDA stuff working - both rely on native compiled C/C++ code to access GPUs and manage memory at the scale needed for LLMs. I’m not actually up to speed on the current state of the game there but my understanding is that llama.cpp’s less generic approach has allowed it to focus on specifically optimizing performance of llama-style LLMs. reply verdverm 12 hours agorootparentI've seen more of the model fiddling, like logits restrictions and layer dropping, implemented in python, which is why I ask Most of AI has centralized around Python, I see more of my code moving that way, like how I'm using LlamaIndex as my primary interface now, which supports ollama and many more model loaders / APIs reply read_if_gay_ 5 hours agorootparentprev>Hacker News reply eclectic29 13 hours agorootparentprevAnd what will you do after trying it? Sure, you saved a few mins in trying out a model or models. What next? reply verdverm 13 hours agorootparentI focus on building the application rather than figuring out someone else preferred method for how I should work? I use Docker Compose locally, Kubernetes in the cloud I run in hot-reload locally, I build for production I often nuke my database locally, but I run it HA in production It is very rare to use the same technology locally (or the same way) as in production reply ramblerman 7 hours agorootparentprevRelax. Not everything in this world was built exactly for you. You almost seem to have a problem with this. reply imtringued 1 hour agorootparentprevThere is no \"next\", there is a whole world of people running LLMs locally on their computer and they are far more likely to switch between models on a whim every few days. reply ies7 12 hours agorootparentprevFor us this may like a walk in the park. For non technical people there is a possibility their os don't have git, wget and c++ compiler (especially in windows) This is just like dropbox case years ago. reply vidarh 13 hours agorootparentprevLast time I tried llama.cpp I got errors when running make that were way too time consuming to bother tracking down. It's probably a simple build if everything is how it wants it, but it wasn't in my machine, while running ollama was. reply cjbprime 8 hours agorootparentprevThis will likely build a version without GPU acceleration, I think? reply imtringued 1 hour agorootparentprevThe average user isn't going to compile llama.cpp. They will either download a fully integrated application that contains llama.cpp and is able to read gguf files directly, like kobold.cpp or they are going to use any arbitrary front end like Silly Tavern which needs to connect to an inference server via an API and ollama is one of the easier inference servers to install and use. reply kergonath 13 hours agorootparentprevCompared to “ollama pull mixtral”? And then actually using the thing is easier as well. reply icelain 6 hours agorootparentprevCheck out EasyDiffusion. reply reustle 16 hours agoparentprevEven for just running a model locally, Ollama provided a much simpler \"one click install\" earlier than most tools. That in itself is worth the support. reply Aka456 14 hours agorootparentKoboldcpp is also very, very good, plug and play, very complet web UI, nice little api with sse text streaming, vulkan accelerated, have an AMD fork... reply davidhariri 16 hours agoparentprevAs it turns out, making it faster and better to manage things tends to get people’s attention. I think it’s well deserved. reply Abishek_Muthian 4 hours agoparentprevFor me all the projects which enable running & fine-tuning LLMs locally like llama.cpp, ollama, open-webui, unsolth etc. play a very important part in democratizing AI. > what do you do after the initial experimentation? you need to deploy these models eventually to production I built GaitAnalyzer[1], to analyze my gait laptop; I had deployed it briefly in production when I had enough credits to foot the AWS GPU bills. Ollama made it very simple to deploy the application, Anyone who has used docker before can now run GaitAnalyzer in their computer. [1] https://github.com/abishekmuthian/gaitanalyzer reply ecnahc515 13 hours agoparentprevOllama is the Docker of LLMs. Ollama made it _very_ easy to run LLMs locally. This is surprisingly not as easy as it seems, and incredibly useful. reply brucethemoose2 14 hours agoparentprevInterestingly, Ollama is not popular at all in the \"localllama\" community (which also extends to related discords and repos). And I think thats because of capabilities... Ollama is somewhat restrictive compared to other frontends. I have a littany of reasons I personally wouldn't run it over exui or koboldcpp, both for performance and output quality. This is a necessity of being stable and one-click though. reply Karrot_Kream 15 hours agoparentprevI mean, it takes something difficult like an LLM and makes it easy to run. It's bound to get attention. If you've tried to get other models like BERT based models to run you'll realize just how big the usability gains are running ollama than anything else in the space. If the question you're asking is why so many folks are focused on experimentation instead of productionizing these models, then I see where you're coming from. There's the question of how much LLMs are actually being used in prod scenarios right now as opposed to just excited people chucking things at them; that maybe LLMs are more just fun playthings than tools for production. But in my experience as HN has gotten bigger, the number of posters talking about productionizing anything has really gone down. I suspect the userbase has become more broadly \"interested in software\" rather than \"ships production facing code\" and the enthusiasm in these comments reflects those interests. FWIW we use some LLMs in production and we do not use ollama at all. Our prod story is very different than what folks are talking about here and I'd love to have a thread that focuses more on language model prod deployments. reply jart 14 hours agorootparentWell you would be one of the few hundred people on the planet doing that. With local LLMs we're just trying to create a way for everyone else to use AI that doesn't require sharing all their data with them. First thing everyone asks for of course is how to turn the open source local llms into their own online service. reply Karrot_Kream 14 hours agorootparentOllama's purpose and usefulness is clear. I don't think anyone is disputing that nor the large usability gains ollama has driven. At least I'm not. As far as being one of the few hundred on the planet, well yeah that's why I'm on HN. There's tons of publications and subreddits and fora for generic tech conversation. I come here because I want to talk about the unknowns. reply kergonath 12 hours agorootparent> I come here because I want to talk about the unknowns. Your knowns an are unknowns to some people and vice versa. This is a great strength of HN; on a whole lot of subject you’ll find people ranging from enthusiastic to expert. There are probably subreddits or discord servers tailored to narrow niches and that’s cool, but HN is not that. They are complementary, if anything. In contrast, HN is much more interesting and with a much better S/N ratio than generic tech subreddits, it’s not even comparable. reply Karrot_Kream 12 hours agorootparentI've been using this site since 2008. This is my second account from 2009. HN very much used to be a tech niche site. I realize that for newer users to HN, the appeal is like r/programming or r/technology but with a more text oriented interface or higher SNR or whatever but this is a shift in audience and there are folks like I on this site who still want to use it for niche content. There are still threads where people do discuss gory details, even if the topics aren't technical. A lot of the mapping articles on the site bring out folks with deep knowledge about mapping stacks. Alternate energy threads do it too. It can be like that for LLMs also, but the user base has to want this site to be more than just Yet Another Tech News Aggregator thread. For me as of late I've come to realize that the current audience wants YATNE more than they want deep discussion here and so I modulate my time here accordingly. The LLM threads bring me here because experts like jart chime in. reply kergonath 12 hours agorootparent> I've been using this site since 2008. This is my second account from 2009. I did not really like HN back in the day because it felt too startup-y, but maybe I got a wrong impression. I much preferred Ars Technica and their forum (now, Ars is much less compelling). > For me as of late I've come to realize that the current audience wants YATNE more than they want deep discussion here and so I modulate my time here accordingly. I think it depends on the stories. Different subjects have different demographics, and I have almost completely stopped reading physics stuff because it is way too Reddit-like (and full of confident people asserting embarrassingly wrong facts). I can see how you could feel about fields closer to your interests being dumbed down by over-confident non-specialists. There are still good, highly technical discussions, but it is true that the home page is a bit limited and inefficient to find them. reply eclectic29 13 hours agorootparentprevFew hundred on the planet? Are you kidding me? We're asking enterprises to run LLMs on-premise (I'm intentionally discounting the cloud scenario where the traffic rates are much higher). That's way more than a hundred and sorry to break it to you that Ollama is just not going to cut it. reply Karrot_Kream 13 hours agorootparentNo need to be angry about this. Tech folks should be discussing this collectively and collaboratively. There's space for everything from local models running on smartphones all the way up to OpenAI style industrialized models. Back when social networks were first coming out, I used to read lots of comments about deploying and running distributed systems. I remember reading early incident reports about hotspotting and consistent hashing and TTL problems and such. We need to foster more of that kind of conversation for LMs. Sadly right now Xitter seems to be the best place for that. reply eclectic29 13 hours agorootparentNot angry. Having a discussion :-). It just amazes me how the HN crowd is more than happy with just trying out a model on their machine and calling it a day and not seeing the real picture ahead. Let ignore perf concerns for a moment. Let's say I want to run it on a shared server in the enterprise network so that any application can make use of it. Each application might want to use a model of their choosing. Ollama will unload/load/unload models as each new request arrives. Not sure if folks here are realizing this :-) reply theshackleford 11 hours agorootparent> Not sure if folks here are realizing this :-) I’m not sure you’re capable of understanding that your needs and requirements are just that, yours. reply evilduck 15 hours agoparentprevI am 100% uninterested in your production deployment of rent seeking behavior for tools and models I can run myself. Ollama empowers me to do more of that easier. That’s why it’s popular. reply jameshart 13 hours agorootparentOP’s point is more that Ollama isn’t what’s doing the empowering. Llama.cpp is. reply elwebmaster 14 hours agoparentprevYou are not making any sense. I am running ollama and Open WebUI (which takes care of auth) in production. reply vikramkr 16 hours agoparentprevIt's nice for personal use which is what I think it was built for, has some nice frontend options too. The tooling around it is nice, and there are projects building in rag etc. I don't think people are intending to deploy days services through these tools reply andrewstuart 16 hours agoparentprevSounds like you are dismissing Ollama as a \"toy\". Refer: https://paulgraham.com/startupideas.html reply airocker 15 hours agoparentprevMore than one is easy: put it behind a load balancer. Put one ollama in one container or one port. reply eclectic29 13 hours agorootparentFWIW Ollama has no concurrency support even though llama.cpp's server component (the thing that Ollama actually uses) supports it. Besides, you can't have more than 1 model running. Unloading and loading models is not free. Again, there's a lot more and really much of the real optimization work is not in Ollama; it's in llama.cpp which is completely ignored in this equation. reply airocker 13 hours agorootparentThanks! Great to know. I did not know llama.cpp could do this. It should be pretty straight forward to support, not sure why they would not do it. reply sdesol 9 hours agorootparentI'm pretty sure their primary focus right now is to gain as much mindshare as possible and they seem to be doing a great job of it. If you look at the following GitHub metrics: https://devboard.gitsense.com/ggerganov?r=ggerganov%2Fllama.... https://devboard.gitsense.com/ollama?r=ollama%2Follama&nb=tr... The number of people engaging with ollama is twice that of llama.cpp. And there hasn't been a dip in people engaging with Ollama in the past 6 months. However, what I do find interesting with regards to these two projects is the number of merged pull requests. If you click on the \"Groups\" tab and look at \"Hooray\", you can see llama.cpp had 72 contributors with one or more merged pull requests vs 25 for Ollama. For Ollama, people are certainly more interested in commenting and raising issues. Compare this to llama.cpp, where the number of people contributing code changes is double that of Ollama. I know llama.cpp is VC funded and if they don't focus on make using llama.cpp as easy to use as Ollama, they may find themselves doing all the hard stuff with Ollama reaping all the benefits. Full Disclosure: The tool that I used is mine. reply Zambyte 14 hours agorootparentprevThat is still one model per instance of Ollama, right? reply airocker 13 hours agorootparentyes, not sure you can do better than that. You cannot still have one instance of LLM in (GPU) memory answer two queries at one time. reply eclectic29 13 hours agorootparentOf course, you can support concurrent requests. But Ollama doesn't support it and it's not meant for this purpose and that's perfectly ok. That's not the point though. For fast/perf scenarios, you're better off with vllm. reply airocker 13 hours agorootparentThanks! This is great to know. reply sofixa 19 hours agoprevThis is great news. The more projects do this, the less of a moat CUDA is, and the less of a competitive advantage Nvidia has. reply anonymous-panda 18 hours agoparentWhat does performance look like? reply sevagh 18 hours agorootparentSpoiler alert: not good enough to break CUDA's moat reply qeternity 18 hours agorootparentThis is not CUDA's moat. That is on the R&D/training side. Inference side is partly about performance, but mostly about cost per token. And given that there has been a ton of standardization around LLaMA architectures, AMD/ROCm can target this much more easily, and still take a nice chunk of the inference market for non-SOTA models. reply imtringued 1 hour agorootparentprevHypotheticals don't matter. The average user won't have the most expensive GPU and when it comes to VRAM AMD is half as expensive so they lead in this area. reply bornfreddy 18 hours agorootparentprevNot sure why you're downvoted, but as far as I've heard AMD cards can't beat 4090 - yet. Still, I think AMD will catch or overtake NVidia in hardware soon, but software is a bigger problem. Hopefully the opensource strategy will pay off for them. reply arein3 17 hours agorootparentReally hope so, maybe this time will catch and last Usually when corps open source stuff to get adoption, they stuff the adopters after they gain enough market share and the cycle repeats again reply nerdix 16 hours agorootparentprevA RTX 4090 is about twice the price of and 50%-ish faster than AMD's most expensive consumer card so I'm not sure anyone really expects it to ever surpass a 4090. A 7900 XTX beating a RTX 4080 at inference is probably a more realistic goal though I'm not sure how they compare right now. reply Zambyte 13 hours agorootparentThe 4080 is $1k for 16gb of VRAM, and the 7900 is $1k for 24gb of VRAM. Unless you're constantly hammering it with requests, the extra speed you may get with CUDA on a 4080 is basically irrelevant when you can run much better models at a reasonable speed. reply Zambyte 18 hours agorootparentprevI get 35tps on Mistral:7b-Instruct-Q6_K with my 6650 XT. reply ixaxaar 18 hours agoparentprevHey I did, and sorry for the self promo, Please check out https://github.com/geniusrise - tool for running llms and other stuff, behaves like docker compose, works with whatever is supported by underlying engines: Huggingface - MPS, cuda VLLM - cuda, ROCm llama.cpp, whisper.cpp - cuda, mps, rocm Also coming up integration with spark (TorchDistributor), kafka and airflow. reply jjice 15 hours agoprevJust downloaded this and gave it a go. I have no experience with running any local models, but this just worked out of the box on my 7600 on Ubuntu 22. This is fantastic. reply KronisLV 18 hours agoprevFeels like all of this local LLM stuff is definitely pushing people in the direction of getting new hardware, since nothing like RX 570/580 or other older cards sees support. On one hand, the hardware nowadays is better and more powerful, but on the other, the initial version of CUDA came out in 2007 and ROCm in 2016. You'd think that compute on GPUs wouldn't require the latest cards. reply mysteria 18 hours agoparentThe Ollama backend llama.cpp definitely supports those older cards with the OpenCL and Vulkan backends, though performance is worse than ROCm or CUDA. In their Vulkan thread for instance I see people getting it working with Polaris and even Hawaii cards. https://github.com/ggerganov/llama.cpp/pull/2059 Personally I just run it on CPU and several tokens/s is good enough for my purposes. reply bradley13 18 hours agoparentprevNo new hardware needed. I was shocked that Mixtral runs well on my laptop, which has a so-so mobile GPU. Mixtral isn't hugely fast, but definitely good enough! reply hugozap 18 hours agoparentprevI'm a happy user of Mistral on my Mac Air M1. reply isoprophlex 18 hours agorootparentHow many gbs of RAM do you have in your M1 machine? reply hugozap 18 hours agorootparent8gb reply isoprophlex 18 hours agorootparentThanks, wow, amazing that you can already run a small model with so little ram. I need to buy a new laptop, guess more than 16 gb on a macbook isn't really needed reply evilduck 15 hours agorootparentI use several LLM models locally for chat UIs and IDE autocompletions like copilot (continue.dev). Between Teams, Chrome, VS Code, Outlook, and now LLMs my RAM usage sits around 20-22GB. 16GB will be a bottleneck to utility. reply SparkyMcUnicorn 17 hours agorootparentprevI would advise getting as much RAM as you possibly can. You can't upgrade later, so get as much as you can afford. Mine is 64GB, and my memory pressure goes into the red when running a quantized 70B model with a dozen Chrome tabs open. reply TylerE 16 hours agorootparentprevI've run LLMs and some of the various image models on my M1 Studio 32GB without issue. Not as fast as my old 3080 card, but considering the Mac all in has about a 5th the power draw, it's a lot closer than I expected. I'm not sure of the exact details but there is clearly some secret sauce that allows it to leverage the onboard NN hardware. reply dartos 17 hours agorootparentprevMistral is _very_ small when quantized. I’d still go with 16gbs reply jonplackett 18 hours agorootparentprevIs it easy to set this up? reply hugozap 18 hours agorootparentIt is, it doesn't require any setup. After installation: > ollama run mistral:latest reply LoganDark 18 hours agorootparentprevSuper easy. You can just head down to https://lmstudio.ai and pick up an app that lets you play around. It's not particularly advanced, but it works pretty well. It's mostly optimized for M-series silicon, but it also technically works on Windows, and isn't too difficult to trick into working on Linux either. reply glial 18 hours agorootparentAlso, https://jan.ai is open source and worth trying out too. reply LoganDark 18 hours agorootparentLooks super cool, though it seems to be missing a good chunk of features, like the ability to change the prompt format. (Just installed it myself to check out all the options.) All the other missing stuff I can see though is stuff that LM Studio doesn't have either (such as a notebook mode). If it has a good chat mode then that's good enough for most! reply jmorgan 16 hours agoparentprevThe compatibility matrix is quite complex for both AMD and NVIDIA graphics cards, and completely agree: there is a lot of work to do, but the hope is to gracefully fall back to older cards.. they still speed up inference quite a bit when they do work! reply XCSme 13 hours agoparentprevMy 1080ti runs ok even this 47B model: https://ollama.com/library/dolphin-mixtral reply superkuh 16 hours agoparentprevllama.cpp added first class support for the RX 580 by implementing the vulkan backend. There are some issues on older kernel amdgpu code where a llm process VRAM is never reloaded if it gets kicked out to GTT (in 5.x kernels) but overall it's much faster than the clBLAST opencl implementation. reply rahimnathwani 17 hours agoprevHere is the commit that added ROCm support to llama.cpp back in August: https://github.com/ggerganov/llama.cpp/commit/6bbc598a632560... reply 65a 8 hours agoparentYep, and it deserves the credit! He who writes the cuda kernel (or translates it) controls the spice. I had wrapped this and had it working in Ollama months ago as well: https://github.com/ollama/ollama/pull/814. I don't use Ollama anymore, but I really like the way they handle device memory allocation dynamically, I think they were the first to do this well. reply rahimnathwani 7 hours agorootparentI'm curious about both: - what's special about the memory allocation, and how might it help me? - what are you now using instead of ollama? reply 65a 7 hours agorootparentOllama does a nice job of looking at how much VRAM the card has and tuning the number of gpu layers offloaded. Before that, I mainly just had to guess. It's still a heuristic, but I thought that was neat. I'm mainly just using llama.cpp as a native library now, mainly for the direct access to more of llama's data structures, and because I have a sort of unique sampler setup. reply reilly3000 18 hours agoprevI’m curious as to how they pulled this off. OpenCL isn’t that common in the wild relative to Cuda. Hopefully it can become robust and widespread soon enough. I personally succumbed to the pressure and spent a relative fortune on a 4090 but wish I had some choice in the matter. reply Apofis 18 hours agoparentI'm surprised they didn't speak about the implementation at all. Anyone got more intel? reply harwoodr 18 hours agorootparentROCm: https://github.com/ollama/ollama/commit/6c5ccb11f993ccc88c47... reply skipants 17 hours agorootparentAnother giveaway that it's ROCm is that it doesn't support the 5700 series... I'm really salty because I \"upgraded\" to a 5700XT from a Nvidia GTX 1070 and can't do AI on the GPU anymore, purely because the software is unsupported. But, as a dev, I suppose I should feel some empathy that there's probably some really difficult problem causing 5700XT to be unsupported by ROCm. reply JonChesterfield 16 hours agorootparentI wrote a bunch of openmp code on a 5700XT a couple of years ago, if you're building from source it'll probably run fine reply refulgentis 18 hours agorootparentprevThey're open source and based on llama.cpp so nothings secret. My money, looking at nothing, would be on one of the two Vulkan backends added in Jan/Feb. I continue to be flummoxed by a mostly-programmer-forum treating ollama like a magical new commercial entity breaking new ground. It's a CLI wrapper around llama.cpp so you don't have to figure out how to compile it reply washadjeffmad 17 hours agorootparentI tried it recently and couldn't figure out why it existed. It's just a very feature limited app that doesn't require you to know anything or be able to read a model card to \"do AI\". And that more or less answered it. reply dartos 17 hours agorootparentIt’s because most devs nowadays are new devs and probably aren’t very familiar with native compilation. So compiling the correct version of llama.cpp for their hardware is confusing. Compound that with everyone’s relative inexperience with configuring any given model and you have prime grounds for a simple tool to exist. That’s what ollama and their Modelfiles accomplish. reply tracerbulletx 16 hours agorootparentIt's just because it's convenient. I wrote a rich text editor front end for llama.cpp and I originally wrote a quick go web server with streaming using the go bindings, but now I just use ollama because it's just simpler and the workflow for pulling down models with their registry and packaging new ones in containers is simpler. Also most people who want to play around with local models aren't developers at all. reply imtringued 1 hour agorootparentprevI'm not sure why you are assuming that ollama users are developers when there are at least 30 different applications that have direct API integration with ollama. reply mypalmike 16 hours agorootparentprevEh, I've been building native code for decades and hit quite a few roadblocks trying to get llama.cpp building with cuda support on my Ubuntu box. Library version issues and such. Ended up down a rabbit hole related to codenames for the various Nvidia architectures... It's a project on hold for now. Weirdly, the Python bindings built without issue with pip. reply refulgentis 17 hours agorootparentprevEdited it out of my original comment because I didn't want to seem ranty/angry/like I have some personal vendatta, as opposed to just being extremely puzzled, but it legit took me months to realize it wasn't a GUI because of how it's discussed on HN, i.e. as key to democratizing, as a large, unique, entity, etc. Hadn't thought about it recently. After seeing it again here, and being gobsmacked by the # of genuine, earnest, comments assuming there's extensive independent development of large pieces going on in it, I'm going with: - \"The puzzled feeling you have is simply because llama.cpp is a challenge on the best of days, you need to know a lot to get to fully accelerated on ye average MacBook. and technical users don't want a GUI for an LLM, they want a way to call an API, so that's why there isn't content extalling the virtues of GPT4All*. So TL;DR you're old and have been on computer too much :P\" but I legit don't know and still can't figure it out. * picked them because they're the most recent example of a genuinely democratizing tool that goes far beyond llama.cpp and also makes large contributions back to llama.cpp, ex. GPT4All landed 1 of the 2 vulkan backends reply j33zusjuice 18 hours agorootparentprevAhhhh, I see what you did there. reply karmakaze 17 hours agoparentprevIt would serve Nvidia right if their insistence on only running CUDA workloads on their hardware results in adoption of ROCm/OpenCL. reply aseipp 17 hours agorootparentYou can use OpenCL just fine on Nvidia, but CUDA is just a superior compute programming model overall (both in features and design.) Pretty much every vendor offers something superior to OpenCL (HIP, OneAPI, etc), because it simply isn't very nice to use. reply karmakaze 15 hours agorootparentI suppose that's about right. The implementors are busy building on a path to profit and much less concerned about any sort-of lock-in or open standards--that comes much later in the cycle. reply KeplerBoy 17 hours agorootparentprevOpenCL is fine on Nvidia Hardware. Of course it's a second class citizen next to CUDA, but then again everything is a second class citizen on AMD hardware. reply shmerl 17 hours agoparentprevMay be Vulkan compute? But yeah, interesting how. reply moffkalast 18 hours agoparentprevOpenCL is as dead as OpenGL and the inference implementations that exist are very unperformant. The only real options are CUDA, ROCm, Vulkan and CPU. And Vulkan is a proper pain too, takes forver to build compute shaders and has to do so for each model. It only makes sense on Intel Arc since there's nothing else there. reply zozbot234 17 hours agorootparentSYCL is a fairly direct successor to the OpenCL model and is not quite dead, Intel seems to be betting on it more than others. reply mpreda 17 hours agorootparentprevROCm includes OpenCL. And it's a very performant OpenCL implementation. reply taminka 17 hours agorootparentprevwhy though? except for apple, most vendors still actively support it and newer versions of OpenCL are released… reply programmarchy 18 hours agoparentprevApple killed off OpenCL for their platforms when they created Metal which was disappointing. Sounds like ROCm will keep it alive but the fragmentation sucks. Gotta support CUDA, OpenCL, and Metal now to be cross-platform. reply jart 17 hours agorootparentWhat is OpenCL? AMD GPUs support CUDA. It's called HIP. You just need a bunch of #define statements like this: #ifndef __HIP__ #include#include#else #include#include#define cudaSuccess hipSuccess #define cudaStream_t hipStream_t #define cudaGetLastError hipGetLastError #endif Then your CUDA code works on AMD. reply jiggawatts 16 hours agorootparentCan you explain why nobody knows this trick, for some values of “nobody”? reply jart 16 hours agorootparentNo idea. My best guess is their background is in graphics and games rather than machine learning. When CUDA is all you've ever known, you try just a little harder to find a way to keep using it elsewhere. reply wmf 15 hours agorootparentprevPeople know; it just hasn't been reliable. reply jart 14 hours agorootparentWhat's not reliable about it? On Linux hipcc is about as easy to use as gcc. On Windows it's a little janky because hipcc is a perl script and there's no perl interpreter I'll admit. I'm otherwise happy with it though. It'd be nice if they had a shell script installer like NVIDIA, so I could use an OS that isn't a 2 year old Ubuntu. I own 2 XTX cards but I'm actually switching back to NVIDIA on my main workstation for that reason alone. GPUs shouldn't be choosing winners in the OS world. The lack of a profiler is also a source of frustration. I think the smart thing to do is to develop on NVIDIA and then distribute to AMD. I hope things change though and I plan to continue doing everything I can do to support AMD since I badly want to see more balance in this space. reply wmf 14 hours agorootparentThe compilation toolchain may be reliable but then you get kernel panics at runtime. reply jart 14 hours agorootparentI've heard geohot is upset about that. I haven't tortured any of my AMD cards enough to run into that issue yet. Do you know how to make it happen? reply imtringued 51 minutes agorootparentLast time I used AMD GPUs for GPGPU all it took was running hashcat to make the desktop rendering unstable. I'm sure leaving it run overnight would've gotten me a system crash. reply programmarchy 9 hours agorootparentprevOpenCL is a Khronos open spec for GPU compute, and what you’d use on Apple platforms before Metal compute shaders and CoreML were released. If you wanted to run early ML models on Apple hardware, it was an option. There was an OpenCL backend for torch, for example. reply Symmetry 18 hours agoprevGiven the price of top line NVidia cards, if they can be had at all, there's got to be a lot of effort going on behind the scenes to improve AMD support in various places. reply jeff-davis 9 hours agoparentWhat are the barriers to doing so? reply simon83 18 hours agoprevDoes anyone know how the AMD consumer GPU support on Linux has been implemented? Must use something else than ROCm I assume? Because ROCm only supports the 7900 XTX on Linux[1], while on Windows[2] support is from RX 6600 and upwards. [1]: https://rocblas.readthedocs.io/en/rocm-6.0.0/about/compatibi... [2]: https://rocblas.readthedocs.io/en/rocm-6.0.0/about/compatibi... reply Symmetry 18 hours agoparentThe newest release, 6.0.2, supports a number of other cards[1] and in general people are able to get a lot more cards to work than are officially supported. My 7900 XT worked on 6.0.0 for instance. [1]https://rocm.docs.amd.com/projects/install-on-linux/en/lates... reply bravetraveler 18 hours agoparentprevI wouldn't read too much into support. It's more in terms of business/warranty/promises than what can actually do things I've had a 6900XT since launch and this is the first I'm hearing \"unsupported\", having played with ROCM plenty over the years with Fedora Linux. I think, at most, it's taken a couple key environment variables reply HarHarVeryFunny 17 hours agorootparentHow hard would it be for AMD just to document the levels of support of different cards the way NVIDIA does with their \"compute capability\" numbers ?! I'm not sure what is worse from AMD - the ML software support they provide for their cards, or the utterly crap documentation. How about one page documenting AMD's software stack compared to NVIDIA, one page documenting what ML frameworks support AMD cards, and another documenting \"compute capability\" type numbers to define the capabilities of different cards. reply londons_explore 17 hours agorootparentAnd almost looks like they're deliberately trying to not win any market share. It's as if the CEO is mates with NVidias CEO and has an unwritten agreement not to try too hard to topple the applecart... Oh wait... They're cousins! reply SushiHippie 18 hours agoprevCan anyone (maybe ollama contributors) explain to me the relationship between llama.cpp and ollama? I always thought that ollama basically just was a wrapper (i.e. not much changes to inference code, and only built on top) around llama.cpp, but this makes it seem like it is more than that? reply Zambyte 18 hours agoparentllama.cpp also supports AMD cards: https://github.com/ggerganov/llama.cpp?tab=readme-ov-file#hi... reply airocker 17 hours agoprevI heard \"Nvidia for LLM today is similar to how Sun Microsystems was for the web\" reply api 16 hours agoparent... for a very brief period of time until Linux servers and other options caught up. reply airocker 13 hours agorootparentOS was possibly much more complicated to write at that time than CUDA is to write today. And competition is too strong. It might be more briefer than even Sun. reply OtomotO 14 hours agoprevHm, fooocus manages to run, but for Ollama I get: >time=2024-03-16T00:11:07.993+01:00 level=WARN source=amd_linux.go:50 msg=\"ollama >recommends running the https://www.amd.com/en/support/linux-drivers: amdgpu version file >missing: /sys/module/amdgpu/version stat /sys/module/amdgpu/version: no such file or >directory\" >time=2024-03-16T00:11:07.993+01:00 level=INFO source=amd_linux.go:85 msg=\"detected amdgpu >versions [gfx1031]\" >time=2024-03-16T00:11:07.996+01:00 level=WARN source=amd_linux.go:339 msg=\"amdgpu >detected, but no compatible rocm library found. Either install rocm v6, or follow manual >install instructions at https://github.com/ollama/ollama/blob/main/docs/linux.md#man...\" >time=2024-03-16T00:11:07.996+01:00 level=WARN source=amd_linux.go:96 msg=\"unable to verify >rocm library, will use cpu: no suitable rocm found, falling back to CPU\" >time=2024-03-16T00:11:07.996+01:00 level=INFO source=routes.go:1105 msg=\"no GPU detected\" Need to check how to install rocm on arch again... have done it once, a few moons back, but alas... reply jmorgan 13 hours agoparentAh, this is probably from missing ROCm libraries. The dynamic libraries are available as one of the release assets (warning: it's about 4GB expanded) https://github.com/ollama/ollama/releases/tag/v0.1.29 – dropping them in the same directory as the `ollama` binary should work. reply skenderbeu 48 minutes agoprevllama.cpp now supports AMD graphics. Ollama just wraps around it reply deadalus 18 hours agoprevI wish AMD did well in the Stable Diffusion front because AMD is never greedy on VRAM. The 4060Ti 16GB(minimum required for Stable Diffusion in 2024) starts at $450. AMD with ROCm is decent on Linux but pretty bad on Windows. reply Adverblessly 18 hours agoparentI run A1111, ComfyUI and kohya-ss on an AMD (6900XT which has 16GB, the minimum required for Stable Diffusion in 2024 ;)), though on Linux. Is it a Windows specific Issue for you? Edit to add: Though apparently I still don't run ollama on AMD since it seems to disagree with my setup. reply wastewastewaste 4 hours agoparentprevYou don't need 16gb, literally majority of people don't have that and use 8gb and up. especially with forge reply choilive 18 hours agoparentprevThey bump up VRAM because they can't compete on raw compute. reply wongarsu 17 hours agorootparentOr rather Nvidia is purposefully restricting VRAM to avoid gaming cards canibalizing their supremely profitable professional/server cards. AMD has no relevant server cards, so they have no reason to hold back on VRAM in consumer cards reply karolist 5 hours agorootparentNvidia released consumer RTX 3090 with 24GB VRAM in Sep 2020, AMDs flagship release in that same month was 6900 XT with 16GB VRAM. Who is being restrictive here exactly? reply risho 18 hours agorootparentprevit doesn't matter how much compute you have if you don't have enough vram to run the model. reply Zambyte 17 hours agorootparentExactly. My friend was telling me that I was making a mistake for getting a 7900 XTX to run language models, when the fact of the matter is the cheapest NVIDIA card with 24 GB of VRAM is over 50% more expensive than the 7900 XTX. Running a high quality model at like 80 tps is way more important to me than running a way lower quality model at like 120 tps. reply api 16 hours agorootparentprevThey lag on software a lot more than the lag on silicon. reply VadimPR 6 hours agoprevOllama runs really, really slow on my MBP for Mistral - as in just a few tokens a second and it takes a long while before it starts giving a result. Anyone else run into this? I've seen that it seems to be related to the amount of system memory available when ollama is started (??) however LM Studio does not have such issues. reply tarruda 17 hours agoprevDoes this work with integrated Radeon graphics? If so it might be worth getting one of those Ryzen mini PCs to use as a local LLM server. reply stebalien 17 hours agoparentNot yet: https://github.com/ollama/ollama/issues/2637 reply rcarmo 17 hours agoprevCurious to see if this will work on APUs. Have a 7840HS to test, will give it a go ASAP. reply latchkey 18 hours agoprevIf anyone wants to run some benchmarks on MI300x, ping me. reply kodarna 18 hours agoprevI wonder why they aren't supporting RX 6750 XT and lower yet, are there architectural differences between these and RX 6800+? reply Zambyte 18 hours agoparentThey don't support it, but it works if you set an environment variable. https://github.com/ollama/ollama/issues/2870#issuecomment-19... reply slavik81 18 hours agoparentprevThose are Navi 22/23/24 GPUs while the RX 6800+ GPUs are Navi 21. They have different ISAs... however, the ISAs are identical in all but name. LLVM has recently introduced a unified ISA for all RDNA 2 GPUs (gfx10.3-generic), so the need for the environment variable workaround mentioned in the other comment should eventually disappear. reply haolez 14 hours agoprevIs there an equivalent to ollama or gpt4all for Android? I'd like to host my model somewhere and talk to it via app. reply verdverm 13 hours agoparentAt that point, it sounds like your after an API endpoint for any model. Lots of solutions out there, depends more on your hosting reply userbinator 11 hours agoprevThanks, Ollama. (Sorry, could not resist.) reply treprinum 14 hours agoprevAny info on the performance? How does it compare to 4080/4090? reply h4x0rr 13 hours agoprevNo support for my rx 5700xt :( reply ekianjo 7 hours agoprevDoes it also support AMD APUs? reply renewiltord 18 hours agoprevWow, that's a huge feature. Thank you, guys. By the way, does anyone have a preferred case where they can put 4 AMD 7900XTX? There's a lot of motherboards and CPUs that support 128 lanes. It's the physical arrangement that I have trouble with. reply segmondy 13 hours agoparentYou don't need 128 lanes. 8x PCIe3 is more than enough, so for 4 cards that's 32. Most CPUs have about 40lanes. If you are not doing much that would be more than sufficient. Buy a PCIe riser. Go to amazon and search for it, a 16x to 16x PCIe riser. They go for about $25-$30 often about 20-30cm. If you want really long one, you can get one from China a 60cm for about the same price, you just have to wait for 3 weeks. That's what I did. Stuffing all those in a case is often difficult, so you have to go open rig. Either have the cables running out your computer and figuring out a way to support the cards while keeping them cool or just buy a small $20-$30 open rig frame. reply Dylan16807 7 hours agorootparentMost CPUs have about 20 lanes (plus a link to the chipset). On the one hand, they will be gen 4 or 5, so they're the equivalent of 40-80 gen 3 lanes. On the other hand, you can only split them up if you have a motherboard that supports bifurcation. If you buy the wrong model, you're stuck dedicating the equivalent of 64 gen 3 lanes to a single card. Edit: Actually, looking into it further, current Intel desktop processors will only run their lanes as 16(+4) or 8+8(+4). You can kind of make 4 cards work by using chipset-fed slots, but that sucks. You could also get a PCIe switch but those are very expensive. AMD will do 4+4+4+4(+4) on the right boards. reply renewiltord 13 hours agorootparentprevAh ha, that's the part I was curious about. I was wondering if I could keep everything cool open rig. I'm waiting for the stuff to arrive: risers, board, CPU, GPUs. And I've been putting it off because I wasn't sure how about the case. All right then, open rig frame. Thank you! reply nottorp 18 hours agoparentprevUsed crypto mining parts not available any more? reply duskwuff 17 hours agorootparentCrypto mining didn't require significant bandwidth to the card. Mining-oriented motherboards typically only provisioned a single lane of PCIe to each card, and often used anemic host CPUs (like Celeron embedded parts). reply cjbprime 8 hours agorootparentDoes LLM inference require significant bandwidth to the card? You have to get the model into VRAM, but that's a fixed startup cost, not a per-output-token cost. reply renewiltord 17 hours agorootparentprevExactly. They'd use PCIe x1 to PCIe x16 risers with power adapters. These require high-bandwidth. reply nottorp 15 hours agorootparentOh. Shows I wasn't into that. I did once work with a crypto case, but yes, it was one motherboard with a lot of wifis and we still didn't need the pcie lanes. reply observationist 16 hours agoprev [–] There's a thing somewhat conspicuous in its absence - why isn't llama.cpp more directly credited and thanked for providing the base technology powering this tool? All the other cool \"run local\" software seems to have the appropriate level of credit. You can find llama.cpp references in the code, being set up in a kind of \"as is\" fashion such that it might be OK as far as MIT licensing goes, but it seems kind of petty to have no shout out or thank you anywhere in the repository or blog or ollama website. GPT4All - https://gpt4all.io/index.html LM Studio - https://lmstudio.ai/ Both of these projects credit and attribute appropriately, Ollama seems to bend over backwards so they don't have to? reply jart 16 hours agoparentollama has made a lot of nice contributions of their own. It's a good look to give a hat tip to the great work llama.cpp is also doing, but they're strictly speaking not required to do that in their advertising any more than llama.cpp is required to give credit to Google Brain, and I think that's because llama.cpp has pulled off tricks in the execution that Brain never could have accomplished, just as ollama has had great success focusing on things that wouldn't make sense for llama.cpp. Besides everyone who wants to know what's up can read the source code, research papers, etc. then make their own judgements about who's who. It's all in the open. reply galaxyLogic 10 hours agorootparentNewton only gave credits to \"Gods\". He said he stands on shoulders of Gods or something like that. But he never mentioned which Gods in particular, did he? reply aragilar 8 hours agorootparentGiants, not Gods, and it's debated whether it was actually an insult or not: https://en.wikipedia.org/wiki/Isaac_Newton#Personality reply refulgentis 12 hours agorootparentprevLike what? What contributions has ollama made to llama.cpp? It's not a big deal or problem. It just hasnt. And they could have very, very, easily, there's a server, sitting right there. They chose not to. That's fine. But it's a choice. The rest I chalk it up to inexperience and being busy. reply kleiba 6 hours agorootparentOP never said \"contributions to llama.cpp\", the comment just said just \"contributions\" which I read as \"own developments\". reply varjag 6 hours agorootparentprevWhere the name does come from? reply dheera 15 hours agoparentprev [10 more] [flagged] mkarrmann 13 hours agorootparentI've recently taken to calling it llama.cpp plus ollama reply hnfong 10 hours agorootparentIt's the \"GNU/Linux\" thing all over again.... reply dkjaudyeqooe 13 hours agorootparentprevTrust me, they do. Programmers curse him out on a daily basis. reply observationist 15 hours agorootparentprev [–] Honestly, they probably should keep a little Bjarne shrine on their desk and light a little votive candle every time they release to production. reply ronsor 15 hours agorootparentRealistically that's the only way to reduce the amount of bugs in your C++ code. reply jon_richards 14 hours agorootparentDo you want machine spirits? Because that's how you get machine spirits. reply reissbaker 13 hours agorootparentMy brother in Stroustrup, we are trying to run llama.cpp, we are definitionally attempting to summon machine spirits. reply octopoc 14 hours agorootparentprevlol, although you made me think--the history of computers has involved patching layer after layer of sediment on top of each other until the how things work 10 layers deep is forgotten. Imagine when local LLMs are five layers deep. It'll be like having machine spirits. You know how you have to yell to get the LLM to do what you want sometimes? That's what the future version of sudo will be like. reply roenxi 11 hours agorootparentprev [–] Who are these heathens who are going to ignore the basic language design work done by Kernighan & Ritchie? And whoever invented ++. Stroustrup didn't invent the C, the ++ or the notion of ++ing things in general. His contribution here is clearly quite small. I thought the original argument for focusing on ollama alone was quite good. Once you get into dependencies it is turtles all the way down. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Ollama now offers preview support for AMD graphics cards on Windows and Linux starting March 14, 2024.",
      "Supported AMD cards are from Radeon, Radeon PRO families, and Instinct accelerators, enabling users to accelerate all Ollama features.",
      "Additional AMD graphics card models are anticipated to be included in the future to enhance the Ollama experience."
    ],
    "commentSummary": [
      "The conversation delves into Ollama software's compatibility and performance on AMD graphics cards, with users sharing experiences and challenges.",
      "Users debate Ollama's utility for model experimentation versus constraints in production deployments, highlighting convenience over llama.cpp and the preference for Python in building language models.",
      "Frustrations with AMD GPU support and inadequate documentation are expressed, along with discussions on benchmarking and GPU setup limitations, showcasing the intricacies of leveraging AMD GPUs for machine learning."
    ],
    "points": 590,
    "commentCount": 206,
    "retryCount": 0,
    "time": 1710524840
  },
  {
    "id": 39720007,
    "title": "Nix: The Superior Docker Image Builder",
    "originLink": "https://xeiaso.net/talks/2024/nix-docker-build/",
    "originBody": "Nix is a better Docker image builder than Docker's image builder Fri Mar 15 2024 Douglas Adams Quotes Live: douglas-adams-quotes.fly.dev Source code: github:Xe/douglas-adams-quotes gomod2nix $50 of Fly.io Credits Coupon code go-fly-nix. Only valid for new accounts that have not used a DevRel coupon code before. Slides and Video Slides: Google Drive Script: Google Drive Video: Coming Soon!A full copy of the talk will be available later today. The video may take longer. Conference wifi is horrible. The Talk The title slide of the talk. It features a hot air balloon breaking into a shipping container with a crowbar. Art by Annie Rugyt. Hi, I'm Xe Iaso and today I'm gonna get to talk with you about one of my favourite tools: Nix. Nix is many things, but my super hot take is that it's a much better Docker image builder than Docker's image builder. As many of you know, Nix is a tool that makes it easy to build packages based on the instructions you give it using its little domain-specific language. For reasons which are an exercise to the listener, this language is also called Nix. A Nix package can be just about anything, but usually you'll see Nix being used to build software packages, your own custom python tooling, OS hard drive image, or container images. If you've never used it before, Nix is gonna seem a bit weird. It's going to feel like you're doing a lot of work up front, and that's because at some level it is. You're doing work today that you would have done in a few months anyways. I'll get into more detail about this as the talk goes on. As I said, I'm Xe Iaso. I'm the Senior Technophilosopher at Fly.io where I do developer relations. My friends and loved ones can attest that I have a slight tendency to write on my blog. I've been using Nix and NixOS across all of my personal and professional projects for the last four years. I live in Ottawa with my husband. It's the morning and I know we're all waiting for that precious bean juice to kick in. Let's get that blood pumping with a little exercise. If you've read my blog before, can you raise your hand? (Ad-lib on the number of hands raised) Okay, that's good. Raise your hand if this is your first introduction to Nix or NixOS. (Ad-lib again) How about if you're a Nix or NixOS expert? Raise your hand if you'd call yourself a Nix or NixOS expert. (Ad-lib again) Finally, Raise your hand if you got into Nix or NixOS because of my blog. (Ad-lib again) Alright thanks, you can lower your hands now. This talk is a bit more introductory. There's a mixed audience here of people that are gonna be more hardcore Nix users and people that have probably never heard of Nix before. I want this talk to be a bridge so that those of you who are brand new to Nix can understand what it's about and why you should care. For those of you who have ascended the mortal plane with NixOS powers, maybe this can help you realize where we're needed most. Today I'm gonna cover what Nix is, why it's better than Docker at making Docker images, and some neat second-order properties of Nix that makes it so much more efficient in the long run. The holy trinity of Nix, showing that Nix the language, the package manager, and the OS are different facets of the same thing. Nix is just a package manager, right? Well, it's a bit more. It's a package manager, a language, and an operating system. It's kind of a weird balance because they all have the name \"Nix\", but you can use this handy diagram to split the differences. You use Nix the language to make Nix the package manager build packages. Those packages can be anything from software to entire NixOS images. This is compounded by the difficulty of adopting Nix at work if you have anything but a brand new startup or homelab that's willing to burn down everything and start anew with Nix. Nix is really different than what most developers are used to, which makes it difficult to cram into existing battle-worn CI/CD pipelines. This is not sustainable. I'm afraid that if there's not a bridge like this, Nix will wilt and die because of the lack of adoption. I want to show you how to take advantage of Nix today somewhere that it's desperately needed: building and deploying container images. The docker logo on a sky background. To say that Docker won would be an understatement. My career started just about the same time that Docker left public beta. Docker and containerization has been adopted so widely that I'd say that Docker containers have become the de-facto universal package format of the Internet. Modern platforms like Fly.io, Railway, and Render could let people run arbitrary VM images or Linux programs in tarball slugs, but they use Docker images because that works out better for everyone. The docker logo with a badly photoshopped muscle-bound beefy arm on a sky background. This gives people a lot of infrastructure superpowers and the advantages make the thing sell itself. It's popular for a reason. It solves real-world problems that previously required complicated cross-team coordination. No more arguing with your sysadmin or SRE team over upgrading your local fork of Ubuntu to chase the dragon with package dependencies! However, there's just one fatal flaw: Docker builds are not deterministic. Not even slightly. Sure, the average docker file you find on the internet will build 99.99% of the time, but that last 0.01% is where the real issues come into play. Speaking as a former wielder of the SRE shouting pager, that last 0.01% of problems ends up coming into play at 4am. Always 4am, never while you are at work.Ask me how I know. One of the biggest problems that doesn't sound like a problem at first is that Docker builds have access to the public Internet. This is needed to download packages from the Ubuntu repositories, but that also means that it's hard to go back and recreate the exact state of the Ubuntu repositories when you inevitably need to recreate an image at a future date. Remember, Ubuntu 18.04 is going out of support this year! You're going to have a flag day finding out what depends on that version of Ubuntu when things break and not any sooner. Even more fun, adding packages to a docker image the naïve way means that you get wasted space. If you run apt-get upgrade at the beginning of your docker build, you can end up replacing files in the container image. Those extra files end up being some \"wasted space\" shadow copies that will add up over time, especially with AWS charging you per millibyte of disk space and network transfer or whatever.What if we had the ability to know all of the dependencies that are needed ahead of time and then just use those? What if your builds didn't need an internet connection at all? The Nix/NixOS logo on a purple and black gradient background. This is the real advantage of Nix when compared to docker builds. Nix lets you know exactly what you're depending on ahead of time and then can break that into the fewest docker layers possible. This means that pushing updates to your programs only means that the minimal number of changes are actually made. You don't need to wait for apt or npm to install your dependencies yet again just to change a single line of code in your service. I think one of the best ways to adopt it is to use it to build docker images. This helps you bridge the gap so that you can experiment with new tools without breaking too much of your existing workflows. As an example, let's say I have a Go program that gives you quotes from Douglas Adams. I want to deploy it to a platform that only takes Docker images, like Fly.io, Railway, or Google Cloud Functions. In order to do this, I'd need to do a few things: First, I'd need to build the program into a package with Nix and make sure it works. Then I'd need to turn that into a docker image, load it into my docker daemon, and push it to their registry. Finally I can deploy my application and everyone can benefit from the wisdom of days gone past. Here's what that package definition looks like in my project's Nix flake. Let's break this down into parts. bin = pkgs.buildGoModule { pname = \"douglas-adams-quotes\"; inherit version; src = ./.; vendorHash = null; }; This project is in a Go module, so pkgs.buildGoModule tells Nix to use the Go module template. That template will set everything up for us: mainly the Go compiler, a C compiler for CGo code, and downloading any external dependencies for you. Here are the arguments to the buildGoModule function: a package name, the version, the path to the source code, and the hash of the external dependencies. The name of the package is \"Douglas Adams Quotes\" in kebab case, the version is automagically generated from the git commit of the service, the source code is in the current working directory, and I don't need anything beyond Go's standard library. If you need external dependencies, you can specify the hash of all the dependencies here or use gomod2nix to automate this (it's linked in the description at the end of the talk). # nix build .#bin Now that we have a package definition, you can build it with nix build dot hash bin. That makes Nix build the bin package in your flake and put the result in dot slash result. Next comes building that into a Docker image with the dockerTools family of helpers. dockerTools lets you take that Nix package you just made and put it and all its dependencies into a Docker image so you can deploy it. An onion and an onion with an X over it. An onion is a visual metaphor for layered Docker images. There's two basic ways to use it, making a layered image and a non-layered image. A non-layered image is the simplest way to use Nix to build a docker image. It takes the program, its dependencies, any additional things like TLS root certificates and puts it all into a folder to be exposed as a single-layer docker image. This works, but it doesn't really let us take advantage of the benefits of Nix. Making any change to a non-layered image means you have to push all of the things that haven't changed. Nix knows what all your dependencies are, so it should be able to take advantage of that when building a container image. Why should you have to upload new copies of glibc and the python interpreter over and over? An onion pointing to a bunch of folders with Nix packages in its layers. Nix also lets you make a layered image. A layered image puts every dependency into its own image layer so you only upload the parts of your image that have actually changed. Made an update to the webp library to fix a trivial bounds checking vulnerability because nobody writes those libraries in memory-safe languages? The only thing that'd need to be uploaded is that single webp library layer. The reason why this works is that there's a dirty secret deep into Docker that nobody can really take advantage of: Docker has a content-aware store baked into the heart of it, but because docker build isn't made with it in mind, nothing is really able to take advantage of it. Except Nix! A layered image means that every package is in its own layer, so glibc only needs to get uploaded once......until we find yet another trivial memory safety vulnerability in glibc that's been ignored for my entire time on this planet and need to have a fire day rebuilding everything to cope. Here's what a layered docker image build for that Douglas Adams quotes service would look like: docker = pkgs.dockerTools.buildLayeredImage { name = \"registry.fly.io/douglas-adams-quotes\"; tag = \"latest\"; config.Cmd = \"${bin}/bin/douglas-adams-quotes\"; }; Again, let's break it down. You start by saying that you want to build a layered image by calling the dockerTools.buildLayeredImage function with the image name and tag, just like you would with docker build. Now comes the fun part: the rest of the container image. config.Cmd = \"${bin}/bin/douglas-adams-quotes\"; Just tell Nix that the container should run the built version of the Douglas Adams quotes server and bam, everything'll be copied over for you. Glibc will make it over as well as whatever detritus you need to make Glibc happy these days. If you need to add something like the CA certificate root, you can specify it with the contents argument. You can use this to add any package from nixpkgs into your image. My website uses this to add Typst, Deno, and Dhall tools to the container. docker = pkgs.dockerTools.buildLayeredImage { name = \"registry.fly.io/douglas-adams-quotes\"; tag = \"latest\"; contents = with pkgs; [ cacert ]; # <-- config.Cmd = \"${bin}/bin/douglas-adams-quotes\"; }; Then you type in nix build .#docker and whack enter. A shiny new image will show up in ./result. nix build .#docker Load it using docker load < ./result and it'll be ready for deployment. docker load < ./result TODO: embed video Opening the image in dive, we see that every layer adds another package from nixpkgs until you get to the end where it all gets tied together and any contents are symlinked to the root of the image. A successful slide with a lot of cheery imagery. And that's it! All that's left is to deploy it to the cloud and find out if you just broke production. It should be fine, right? The really cool part is that this will work for the cases where you have single images exposed from a code repository, but that content-aware hackery doesn't end at making just one of your services faster to upload. A diagram showing several programs sharing the same layers. If you have multiple services in the same repository, they'll share docker layers between each other. For free. Without any extra configuration. I don't think you can even dream of doing this with Docker without making a bunch of common base images that have a bunch of tools and bloat that some of your services will never make use of. As a practical example, I have a repo I call \"x\". It's full of a decade's worth of side projects, experiments, and tools that help me explore various bits of technology. It's also a monorepo for a bunch of other projects: A diagram showing several programs sharing the same layers. This is a lot of stuff and I don't expect anyone to read that, so I made the text small enough to discourage it. Most of it is deployed across like three platforms too, but I've been slowly converging on one common deployment backbone by shoving everything into Docker images. A diagram showing several programs sharing the same layers. Pushing updates to any one of these services also pushes parts of the updates to most of the other ones. This saves me a lot of time and money across my plethora of projects. Take that, Managed NAT Gateway! Oh no, I think I sense it, you do too right? It's the pedantry alert! Yes in theory I could take advantage of Docker caching to build the images just as efficiently as Nix, but then my build steps would have to look like this: A giant depressing mess of wires. Sure, you can do it, but you'd end up with unmaintainable balls of mud that would have you install shared libraries into their own layers and then you risk invoking the wrath of general protection fault. Not only would you have to turn the network stack back on during builds (there goes reproducibility!), I'd have to rejigger search paths, compiler flags, CGO-related goat sacrifices and more. It'd just be a mess. docker = pkgs.dockerTools.buildLayeredImage { name = \"registry.fly.io/douglas-adams-quotes\"; tag = \"latest\"; contents = with pkgs; [ cacert ]; config.Cmd = \"${bin}/bin/douglas-adams-quotes\"; }; Look at this though, it's just so much simpler. It takes the package and shoves it into a container for you so you don't need to care about the details. It's so much more beautiful in comparison. Above all though, the biggest advantage Nix gives you is the ability to travel back in time and build software exactly as it was in the past. This lets you recreate a docker image exactly at a later point in the future when facts and circumstances demand because that one on-prem customer had apparently never updated their software and was experiencing a weird bug. This means that in theory, when you write package builds today, you're taking from that time you would have spent in the future to recreate it. You don't just build your software though, you crystallize a point in time that describes the entire state of the world including your software to get the resulting packages and docker images. I've been working on a project called XeDN for a few years. Here's how easy it is to build a version from 14 months ago: nix build github:Xe/x/567fdc2#xedn-docker That's it. That's the entire command. I say that I want to build the GitHub repo Xe/x at an arbitrary commit hash and get the xedn-docker target. I can then load it into my docker daemon and then I have the exact same bytes I had back then, Go 1.19 and all. This party trick isn't as easy to pull off with vanilla docker builds unless you pay a lot for storage. An even cooler part of that is that most of the code didn't even need to be rebuilt thanks to the fact that I upload all of my builds into a Nix cache. A Nix cache lets you put the output of Nix commands into a safe place so that they don't need to be run again in the future. This means that developer laptops don't all need to build new versions of nokogiri every time it's bumped ever so slightly. It'll already be built for you with the power of the cloud. I have that uploaded into a cache through Garnix, which I use to do CI on all of my flakes projects. Garnix is effortless. Turn it on and then wait for it to report build status on every commit. It's super great because I don't have to think about it. A terrible picture of my homelab. I even have all of my homelab machine configurations built with Garnix so that when they update every evening, they just pull the newest versions of their config from the Garnix cache instead of building it themselves. Around 7pm or so I hear them reboot after the day of a kernel upgrade. It's really great. Not to mention never having to ever wait for my custom variant of Iosevka to build on my MacBook or shellbox. In conclusion: Nix is a better docker image builder than docker's image builder. Nix makes you specify the results, not the steps you take to get there. Building Docker images with Nix makes adopting Nix easy if you already use Docker. Nix makes docker images that share layers between parts of your monorepo. Nix lets you avoid building code that was built in the past thanks to binary caches. And you end up with normal, ordinary container images that you can deploy anywhere. Even platforms like AWS, Google Cloud, or Fly.io. A slide listing everyone I have to thank for the talk. Before I get all of this wrapped up, I want to thank everyone on this list for their input, feedback, and more to help this talk shine. Thank you so much! A conclusion slide showing information about me and the link to this page. And thank you for watching! I've been Xe Iaso and I'm gonna linger around afterwards for questions. If I don't get to you and you really want a question answered, please email dockerimage@xeserv.us. I promise I'll get back to you as soon as possible. If you want to work with me to make developer relations better, my employer Fly.io is hiring. Catch up with me if you want stickers! I have some extra information linked at the QR code on screen. This includes the source code for the Douglas Adams quotes server so you can clone it on your laptop and play around with it. Be well, all. Share Facts and circumstances may have changed since publication. Please contact me before jumping to conclusions if something seems wrong or unclear. Tags: View slides",
    "commentLink": "https://news.ycombinator.com/item?id=39720007",
    "commentBody": "Nix is a better Docker image builder than Docker's image builder (xeiaso.net)346 points by tosh 17 hours agohidepastfavorite195 comments kstenerud 7 hours agoI've tried again and again to like Nix, but at this point I have to throw in the towel. I have 2 systems running Nix, and I'm afraid to touch them. I've already broken both of them enough that I had to reinstall from scratch in the past (yes yes - it's supposed to be impossible I know), and now I've forgotten most of it. In theory, Nix is idempotent and deterministic, but the problem is \"deterministic in what way?\" Unless you intimately understand what every dependent part is doing, you're going to get strange results and absolutely bizarre and unhelpful errors (or far more likely: nothing at all, with no feedback). Nix feels more like alchemy than science. Like trying to get random Lisp packages to play nice together. Documentation is just plain AWFUL (as in: complete and technically accurate, but maddeningly obtuse), and tutorials only get you part of the way. The moment you step off the 80% path, you're in for a world of hurt, because the underlying components are just not built to support anything else. Sure, you can always \"build your own\", but this requires years of experiential knowledge and layers upon layers of frustration that I just don't want to deal with anymore (which is also why I left Gentoo all those years ago). And woe unto you if you want to use a more modern version than the distribution supports! The strength of Docker is the chaos itself. You can easily build pretty much anything, without needing much more than a cursory understanding of the shell and your distro's package manager. Or you can mix and match whatever the hell you want! When things break, it's MUCH easier to diagnose and fix the problems because all of the tooling has been around for decades, which makes it mature enough to handle edge cases (and breakage is almost ALWAYS about edge cases). Nix is more like Emacs: It can do absolutely anything if you have the patience for it and the deep, arcane knowledge to keep it from exploding in a brilliant flash of octarine. You either go full-in and drink the kool aid, or you keep it at arm's length - smiling and nodding as you back slowly towards the door whenever an enthusiast speaks. reply janjongboom 6 hours agoparentI've gone down the same path. I love deterministic builds, and I think Docker's biggest fault is that to the average developer a Dockerfile _looks_ deterministic - and it even is for a while (build a container twice in a row on the same machine => same output), but then packages get updated in the package manager, base images get updated w/ the same tag, and when you rebuild a month later you get something completely different. Do that times 40 (the number of containers my team manages) and now fixing containers is a significant part of your job. So in theory Nix would be perfect. But it's not, because it's so different. Get a tool from a vendor => won't work on Nix. Get an error => impossible to quickly find a solution on the web. Anyway, out of that frustration I've funded https://www.stablebuild.com. Deterministic builds w/ Docker, but with containers built on Ubuntu, Debian or Alpine. Currently consists of an immutable Docker Hub pull-through cache, full daily copies of the Ubuntu/Debian/Alpine package registries, full daily copies of most popular PPAs, daily copies of the PyPI index (we do a lot of ML), and arbitrary immutable file/URL cache. So far it's been the best of both worlds in my day job: easy to write, easy to debug, wide software compatibility, and we have seen 0 issues due to non-determinism in containers that we moved over to StableBuild in my day job. reply TeeWEE 4 hours agorootparentJust pin the dependencies and your mostly fine right? reply janjongboom 3 hours agorootparentYeah, but it's impossible to properly pin w/o running your own mirrors. Anything you install via apt is unpinnable, as old versions get removed when a new version is released; pinning multi-arch Docker base images is impossible because you can only pin on a tag which is not immutable (pinning on hashes is architecture dependent); Docker base images might get deleted (e.g. nvidia-cuda base images); pinning Python dependencies, even with a tool like Poetry is impossible, because people delete packages / versions from PyPI (e.g. jaxlib 0.4.1 this week); GitHub repos get deleted; the list goes on. So you need to mirror every dependency. reply ktosobcy 2 hours agorootparentprevI don't have any experience with Nix but regarding stable builds of Docker: we provide Java application, have all dependencies as fixed versions so when doing a release, if someone is not doing anything fishy (re-releasing particular version, which is bad-bad-bad) you will get exactly same binaries on top of the same image (again, considering you are not using `:latest` or somesuch)... reply janjongboom 2 hours agorootparentUntil someone overwrites or deletes the Docker base image (regularly happens), or when you depend on some packages installed through apt - as you'll get the latest version (impossible to pin those). reply ktosobcy 1 hour agorootparent> Until someone overwrites or deletes the Docker base image (regularly happens) Any source of that claim? > or when you depend on some packages installed through apt - as you'll get the latest version (impossible to pin those). Well... please re-read my previous comment - we do Java thing so we use any JDK base image and then we slap our distribution on top of it (which are mostly fixed-version jars). Of course if you are after perfection and require additional packages then you can install it via dpgk or somesuch but... do you really need that? What about security implications? reply stefanha 2 hours agorootparentprevAnother option for reproducible container images is https://github.com/reproducible-containers although you may need to cache package downloads yourself, depending on the distro you choose. reply janjongboom 2 hours agorootparentYeah, very similar approach. We did this before, see e.g. https://www.stablebuild.com/blog/create-a-historic-ubuntu-pa... - but then figured everyone needs exactly the same packages cached, so why not set up a generic service for that. reply stefanha 48 minutes agorootparentFor Debian, Ubuntu, and Arch Linux there are official snapshots available so you don't need to cache package downloads yourself. For example, https://snapshot.debian.org/. reply keybits 4 hours agorootparentprevThe pricing page for StableBuild says Free … Number of Users 1 Number of Users 15GB Is that a mistake or if not can you explain please? https://www.stablebuild.com/pricing reply janjongboom 3 hours agorootparentAh, yes, on mobile it shows the wrong pricing table... Copying here while I get it fixed: Free => Access to all functionality, 1 user, 15GB traffic/month, 1GB of storage for files/URLs. $0 Pro => Unlimited users, 500GB traffic included (overage fees apply), 1TB of storage included. $199/mo Enterprise => Unlimited users, 2,000GB traffic included (overage fees apply), 3TB of storage included, SAML/SSO. $499/mo reply IshKebab 4 hours agorootparentprevBut also Nix solves more problems than Docker. For example if you need to use different versions of software for different projects. Nix lets you pick and choose the software that is visible in your current environment without having to build a new Docker image for every combination, which leads to a combinatorial explosion of images and is not practical. But I also agree with all the flaws of Nix people are pointing out here. reply mtmk 9 minutes agoparentprevI recently faced a similar hurdle with Nix, particularly when trying to run a .NET 8 AOT application. What initially seemed like it would be a simple setup spiraled into a plethora of issues, ultimately forcing me to back down. I found myself having to abandon the AOT method in favor of a more straightforward solution. To give credit where it's due, .NET AOT is relatively new and, as far as I know, still has several kinks that need ironing out. Nonetheless, I agree that, at least based on my experience, you need a solid understanding of the ins and outs before you can be reasonably productive using Nix. reply orbital-decay 5 hours agoparentprev>Documentation is just plain AWFUL (as in: complete and technically accurate, but maddeningly obtuse) Documentation is often just plain erroneous, especially for the new CLI and flakes, not even edge cases. I remember spending some time trying to understand why nix develop doesn't work like described and how to make it work like it should. I feel like nobody ever actually used it for its intended purpose. Turns out that by default it doesn't just drop you into the build-time environment like the docs claim (hermetically sealed with stdenv scripts available), it's not sealed by default and the commandline options have confusing naming, you need to fish out the knowledge from the sources to make it work. Plenty of little things like this. >In theory, Nix is idempotent and deterministic I surely wish they talked more about edge cases that break reproducibility. Things like floating point code being sensitive to the order of operations with state potentially leaking from OS preemption, and all that. Which might be obvious, but not saying obvious things explicitly is how you get people shoot themselves in the foot. reply mananaysiempre 2 hours agorootparent> Things like floating point code being sensitive to the order of operations with state potentially leaking from OS preemption, and all that. That’s profoundly cursed and also something that doesn’t happen, to my knowledge. Unless the kernel programmer screwed up, an x86-64 FPU is perfectly virtualizable (and I expect an AArch64 FPU too, I just haven’t tried). So it doesn’t matter where preemtion happens. (What did happen with x87 is that it likes to compute things in more precision than you requested, depending on how it’s configured—normally determined by the OS ABI. Yet variable spills usually happened in the declared precision, so you got different results depending on the particulars of the compiler’s register allocator. But that’s still a far cry from depending on preemption of all things, and anyway don’t use x87. Floating-point computation does depend on associativity, in that nearestfp(nearestfp(a+b)+c) is not the same as nearestfp(a+nearestfp(b+c)), but the sane default state is that the compiler will reproduce the source code as written, without reassociating things behind your back.) reply orbital-decay 43 minutes agorootparentThat's doesn't happen in a single thread, but e.g. asynchronous multithreaded code can spit values in arbitrary order, and depending on what you do you can end up with a different result (floating point is just an example). Generally, you can't guarantee 100% reproducibility for uncooperative code because there's too much hardware state that can't be isolated even in a VM. Sure, 99% software doesn't depend on it or do cursed stuff like microarchitecture probing during building, and you won't care until you try to package some automated tests for a game physics engine or something like that. What can happen, inevitably happens. We don't need to be looking for such contrived examples actually, nixpkgs track the packages that fail to reproduce for much more trivial reasons. There aren't many of them, but they exist: https://github.com/NixOS/nixpkgs/issues?q=is%3Aopen+is%3Aiss... reply fuzzy2 4 hours agoparentprevIt’s really not that bad. However, with a standard NixOS setup, you still have a tremendous amount of non-reproducible state, both inside user accounts and in the system. I’m running a “Erase your darlings” setup, it mostly gets rid of non-reproducible state outside my user account. It’s a bit of a pain, but then what isn’t on NixOS. https://grahamc.com/blog/erase-your-darlings/ Inside my user account, I don’t bother. I don’t like Home Manager. reply SkyMarshal 3 hours agorootparentA nice upgrade to that is to put root in a tmpfs RAM filesystem instead of ZFS: https://elis.nu/blog/2020/05/nixos-tmpfs-as-root/ That way it doesn't even need to bother with resetting to ZFS snapshots, instead it just wipes root on shutdown and reconstructs it in RAM on reboot. Then, optionally, with some extra work you can put /home in tmpfs too: https://elis.nu/blog/2020/06/nixos-tmpfs-as-home/ That setup uses Home Manager, so maybe it's not for you, but worth mentioning if we're talking about making all state declarative and reproducible. You have to use the Impermanence module and set up some soft links to permanent home folders on different drive or partition. But for making all state on the system reproducible and declarative, this is the best way afaik. reply weatherlight 3 hours agoparentprevI use both Docker and NixOs at work. I've never had any of the problems you seemed to have above. Docker is fine, performance wise it's not great on Macs. I love nix because it's trivial to get something to install and behave the same across different machines. Nix Doc are horrible but I've found that ChatGPT4 is awesome at troubleshooting Nix issues. I feel like 90% of the time I run into Nix issues, it's because I decided to do something \"Not the Nix way.\" reply fransje26 1 hour agoparentprev> Documentation is just plain AWFUL (as in: complete and technically accurate, but maddeningly obtuse) That has been the case for as long as I can remember. I gave up on Nix about 5 years ago because of it, and apparently not much has changed on that front since then.. reply aredox 3 hours agoparentprevMaybe it won't be your cup of tea given your reference to Emacs, but there's guix if you want to try a saner alternative to nix. reply laerus 4 hours agoparentprevGive a try to Fedora Atomic (immutable). At this point I have pretty much played around and used every distro package maneger there is and I have broken all of them in one way or another even without doing something exotic (pacman I am looking at you). My Fedora Kinoite is still going strong even with adding/removing different layers, daily updates, and a rebase from Silverblue. Imho rpm-ostree will obsolete Nix. reply plagiarist 1 hour agorootparentHow do you alter layering without a restart? Just have an immutable base and do other rpm-ostrees in containers? Is that what flatpak is up to? reply paulddraper 5 hours agoparentprev> The strength of Docker is the chaos itself. That depends whether you are okay with chaos. It appears that you are, so it is suitable tool for you. Choose the right tool for the right job. --- Docker is a poor choice for people who are interested in deterministic/reproducible builds. reply devjab 4 hours agorootparentI’m not sure exactly why this is being downvoted. It seems pretty fair to want your container builds to not fail because of the “chaos” with docker images and how they change quite a lot. This isn’t about the freedom to build how you want, it’s about securing your build pipelines so that they don’t break at 4am because docker only builds 99% of the time. I’ll use docker, I like docker, but I can see the point of how it’s not necessarily advantageous if stability is your main goal. reply benreesman 3 hours agoprevNix and NixOS are in something like the state git was in before GitHub: the fundamental idea is based on more serious computer science than the status quo (SVN, Docker), the plumbing still has some issues but isn’t worse, and the porcelain and docs are just not there for mainstream adoption. I think that might have changed with the release of flox: https://flox.dev, it’s basically seamless (and that’s not surprising coming from DE Shaw). Nix doesn’t really make sense without flakes and nix-command, those things are documented as experimental and defaulted off. The documentation story is getting better, but it’s not there. nixlang is pretty cool once you learn it well, but it’s never going to be an acceptable barrier to entry at the mainstream. It’s not really the package manager it’s advertised as, nix-env -iA foo is basically never what you want. It’s completely unsurprising that it’s still a secret weapon of companies with an appetite for bleeding-edge shit that requires in-house expertise. flox addresses all of this for the “try it out and immediately have a better time” barrier. Nix/NixOS or something like it is going to send Docker to the same dustbin Subversion is in now, but that’s not going to happen until it has the GitHub moment, and then it’ll happen all at once. Most of the complaints with Nix in this thread are technically false, but eminently understandable and more importantly (repeat after me Nix folks): it’s never the users fault. I’m aware that I’m part of a shrinking cohort who ever knew a world without git/GitHub, so I know this probably sounds crazy to a large part of the readership, but listen to Linus explaining to a room full of people who passed the early Google hiring bar why they should care about a tool they feel is too complicated for them: https://youtu.be/MjIPv8a0hU8?si=QC0UnHXRdMpp2tI4 reply rfoo 3 hours agoparentI believe for a developer tool to success, for the most common thing to do there has to be at least three ways an engineer may misuse your tool and still get it \"done\" (by leaving non-obvious tech debts behind). This is true for git, but not so true yet for Nix, so I'm not sure a GitHub-like moment helps. reply benreesman 2 hours agorootparentIn a full-metal-jacket NixOS setting it’s bloody hard to bash (no pun intended) your way through to the next screen by leaving behind tech debt (Python comes to mind, I made the mistake of trying to use Nix to manage Python once, never again). But anywhere else you just brew/apt/rpm install whatever and nix develop —impure, which is easier than most intermediate git stuff and plenty of beginner stuff. git and Nix are almost the same data structure if you start poking around in .git or /nix/store, I might not understand what you mean without examples. But all my guesses about what you might mean are addressed well by flox. reply jossephus01 9 hours agoprevMy experience with building Docker images for Java applications using Nix wasn't very pleasant though. After the deprecation of gradle2nix, there doesn't seem to be a clear alternative method for building Docker images for Gradle-based Java applications. I challenged a friend to create the smallest possible Docker image for a simple Spring Boot application some time ago. While I was using Nix, the resulting image was twice the size of the image built without Nix. You can check out the code for yourself here: https://github.com/jossephus/Docker_challenge/blob/main/flak... . reply tadfisher 8 hours agoparentThat's because you're including two JDKs, zulu and the one that gradle includes via its jdk argument. Look for gradleGen in nixpkgs to see what I mean. And sorry for gradle2nix, I'm working on an improvement that's less of a hack. reply jossephus01 7 hours agorootparentThanks tadfisher, I will check it out. This is by no means meant to be a dunk on gradle2nix. Love your work on android-nixpkgs and I will be looking for the alternative. Thanks. reply rapnie 6 hours agorootparentprev> And sorry for gradle2nix, I'm working on an improvement that's less of a hack. Don't be. Thanks for your work. Excited to learn about the improvement. Can you tell more about what you have in mind? reply takeda 8 hours agoparentprevI haven't used java in over a decade so won't be able to help much with that, but for example I was able to get my application to fit in just 70MB container including python and all dependencies + busybox and tini It looked something like this: https://gist.github.com/takeda/17b6b645ad4758d5aaf472b84447b... So what I did was: - link everything with musl - compile python and disable all packages that I didn't use in my application - trim boto3/botocore, to remove all stuff I did not use, that sucker on it's own is over 100MB The thing is what you need to understand is that the packages are primarily targeting the NixOS operating system, where in normal situation you have plenty of disk space, and you rather want all features to be available (because why not?). So you end up with bunch of dependencies, that you don't need. Alpine image for example was designed to be for docker, so the goal with all packages is to disable extra bells and whistles. This is why your result is bigger. To build a small image you will need to use override and disable all that unnecessary shit. Look at zulu for example: https://github.com/NixOS/nixpkgs/blob/master/pkgs/developmen... you add alsa, fontconfig (probably comes with entire X11), freetype, xorg (oh, nvm fontconfig, it's added explicitly), cups, gtk, cairo and ffmpeg) Notice how your friend carefully extracts and places only needed files in the container, while you just bundle the entire zulu package with all of its dependencies in your project. Edit: tadfisher seems to be more familiar with it than me, so I would start with that advice and modify code so it only includes a single jdk. Then things that I mentioned could cut the size of jdk further. Edit2: noticed another comment from tadfisher about openjdk_headless, so things might be even simpler than I thought. reply chrisandchris 2 hours agorootparentI've never used Nix, but this looks like hell'a of an unreadable config file (compared to docker)? How do you manage these files? reply elbear 2 hours agorootparentWhat do you mean by manage? I agree with your assertion regarding the language though. I think nix-lang makes it harder to get into Nix. reply jossephus01 7 hours agorootparentprevYou are correct. I havent done any trimming. Thanks for the suggestions and the gist. Thanks reply takeda 7 hours agorootparentI found this discussion and contains code fragments and links that might help. https://discourse.nixos.org/t/how-to-create-a-docker-image-w... reply max-privatevoid 4 minutes agoparentprevI decided to participate in your challenge and cleaned up your Nix code a little bit. It seems like the main task of the challenge is building a really small JRE. I've switched to using a headless OpenJDK build from Nixpkgs as a baseline instead of Zulu, to remove all the unnecessary dependencies on GUI libraries. Then I've used pkgs.jre_minimal to produce a custom minimal JRE with jlink. The image size now comes out to 161MB, which is slightly larger than the demo_jlink image. This is because it actually includes all the modules required to run the application, resulting in a ~90MB JRE. The jdeps invocation in Dockerfile_jlink fails to detect all the modules, so that JRE is only built with java.base. Building my minimal JRE with only java.base brings the JRE size down to about 50MB, the resulting (broken) container image is 117MB according to Podman. I've also removed the erroneous copyToRoot from your call to dockerTools.buildImage, which resulted in copying the app into the image a second time while the use of string context in config.Cmd would have already sufficed. I've also switched to dockerTools.buildLayeredImage, which puts each individual store path into its own image layer, which is great for space scalability due to dependency sharing between multiple container images, but won't have an impact for this single-image experiment. This is mostly a JRE size optimization challenge. The full list of dependencies and their respective size is as follows: /nix/store/v27dxnsw0cb7f4l1i3s44knc7y9sw688-zlib-1.3 125.6K /nix/store/j6n6ky7pidajcc3aaisd5qpni1w1rmya-xgcc-12.3.0-libgcc 139.1K /nix/store/l0ydz31lwa97zickpsxj2vmprcigh1m4-gcc-12.3.0-libgcc 139.1K /nix/store/a3n1vq6fxkpk5jv4wmqa1kpd3jzqhml9-libidn2-2.3.4 350.4K /nix/store/s5ka5vdlp4izan3nfny194yzqw3y4d1z-lcms2-2.15 445.3K /nix/store/a5l3w6hiprvsz7c46jv938iij41v57k6-libjpeg-turbo-2.1.5.1 1.6M /nix/store/r9h133c9m8f6jnlsqzwf89zg9w0w78s8-bash-5.2-p15 1.6M /nix/store/3dfyf6lyg6rvlslvik5116pnjbv57sn0-libunistring-1.1 1.8M /nix/store/a3zlvnswi1p8cg7i9w4lpnvaankc7dxx-gcc-12.3.0-lib 7.5M /nix/store/657b81mfpbdz09m4sk4r9i1c86pm0i8f-app-1.0.0 19.0M /nix/store/1zy01hjzwvvia6h9dq5xar88v77fgh9x-glibc-2.38-44 28.8M /nix/store/b1fhkmscb0vff63xl8ypp4nsc7sd96np-openjdk-headless-minimal-jre-21+35 91.4M There's not much else that can be done here. glibc is the next largest dependency at ~30MB. This large size seems to be because Nixpkgs configures glibc to be built with support for many locales and character encodings. I don't know if it would be possible or practical to split these files out into separate derivations or outputs and make them optional that way. If you're using multiple images built by dockerTools.buildLayeredImage, glibc (and everything else) will be shared across all of them anyway (given you're using roughly the same Nixpkgs commit). https://github.com/max-privatevoid/hackernews-docker-challen... reply tadfisher 7 hours agoparentprevOh, and openjdk_headless skips the GTK and X dependencies that you won't need for Spring. reply okr 7 hours agorootparentThat's interesting. We have some applications, that produce PDFs, which use fonts, which usually requires a non-headless (headfull?) jdk. At AWS i wonder, what the default alpine jdk contains. And how much space could be saved, if people were more aware, that they can use a headless one. reply hawk_ 4 hours agorootparent> headfull? Wonder if there is a good term for this. I have been jokingly referring to this as 'headed' and headless as 'beheaded'. reply okr 1 hour agorootparentAxed! :) reply markelliot 5 hours agoparentprevHard to beat jib (https://github.com/GoogleContainerTools/jib/tree/master/jib-...) for minimal Java OCI containers. reply yjftsjthsd-h 8 hours agoparentprev> While I was using Nix, the resulting image was twice the size of the image built without Nix. I would be very interested to know where the difference is; is nix including things it doesn't need to? Is the non-nix build not including things it should? reply jossephus01 7 hours agorootparentI have included the result of running dive on the resulting image. You can check it out on https://github.com/jossephus/Docker_challenge/wiki. As stated above, I havent done any trimming on the resulting image, so There's too many stuff in the image. reply wbl 7 hours agoparentprevDon't you just stick the JAR in? reply xlii 14 hours agoprevI spent last 2-3 days trying to get Docker images built on Darwin and I feel that this article is a universe making fun of me. Nix is absolutely the best tool for what I want to achieve but it has those dark forsaken corners that just suck your soul out dry. I love it but sometimes it feels like being a Morty on Rick’s adventure to the compilerland. reply takeda 9 hours agoparentThe big problem is how docker was designed. It is essentially a jail that is supposed to contain a Linux binary. Things are straight forward on Linux. You build your binary, place in a docker container and you are done. The nix code will also be straight forward. If you can build your code, then creating a container is just one more operation away. Unfortunately docker requires Linux binary and you are on Mac. So the docker desktop actually runs a Linux VM and performs all operations on it, abstracting this away from you. Nix doesn't do that and you have two options: 1. Do cross compilation, the problem is that for this to work you need to be able to cross compile down to glibc, the problem is that while this will work for most community used dependencies you might get some package where the author didn't put effort making sure it cross compile. To make things worse the Hydra that populates standard caches that nix uses, doesn't do cross compile builds, so you will run into lengthy processes that might potentially end with a failure. 2. You can have a Linux builder, that you add to your Mac and configure to send build jobs for x86_64-linux to that builder. Now you could have a physical box, create a VM, or even have a NixOS docker container (after all docker ion Mac runs inside of the VM). The #1 seems like the proper way, while #2 is more of a practical way. I think you are running into issues, because you're likely trying #1, and that requires a lot of experience not only with Nix, but also with cross compiling. I wish Nix's Hydra would also build Darwin to Linux cross compilation as that would not only provide caches, but also help making sure the cross compilation doesn't break, but that would also increase costs for them. I think you should try the #2 solution. Edit: looks like there might have been an official solution to this problem: https://ryantm.github.io/nixpkgs/builders/special/darwin-bui... I haven't used it yet. reply indiv0 3 hours agorootparentHydra not populating with cross compile builds is the bane of my existence. I'm using `clang` from `pkgs.pkgsCross.musl64.llvmPackages_latest.stdenv` to cross-compile Rust binaries from ARM macos to `x86_64-unknown-linux-musl`. It _works_, but every time I update my `flake.nix` it rebuilds *the entire LLVM toolchain*. On an M2 air, that takes something like 4 hours. It's incredibly frustrating and makes me wary of updating my dependencies or my flake file. The alternative is to switch to dockerized builds but: 1) That adds a fairly heavyweight requirement to the build process 2) All the headache of writing dockerfiles with careful cache layering 3) Most importantly, feels like admitting defeat. reply endgame 11 hours agoparentprevhttps://github.com/gytis-ivaskevicius/high-quality-nix-conte... This sort of twenty-minute adventure? reply deathanatos 9 hours agoparentprevmacOS is a definitely rougher. I use colima there, and it does alright. There are one or two bugs with it, but I think those are primarily around volumes. But it does alright with building Docker images. The rougher part is the speed of it; it's a one-two punch between the hardware & the fact that Docker has to emulate a Linux VM. reply bsder 11 hours agoparentprevIn Docker, the dark corners have dust. In Nix, the dark corners have a grue. reply xedrac 11 hours agorootparent100% this. Nix may seem better, until something goes wrong and you have to waste your weekend digging into it's depths. reply MadnessASAP 9 hours agorootparentOn the flip side, once you have fixed the problem it has a very strong tendency to stay fixed. More importantly, the fix does not typically require me to remember that fix months later. If something does break, rollbacks are free and an integral part of Nix. reply renewiltord 12 hours agoparentprevI use Orbstack and it works flawlessly to do this. Really good tool. I use Docker to cross-compile for {aarch64,amd64} x {linux,darwin} since not all the cross-compiling is super robust across our stacks (I'm using a specific glibc for one Linux part, etc.). Just a bunch of docker on my Darwin aarch64 and it compiles everything. Good experience. reply e40 12 hours agorootparentI installed Orbstack and found that I didn't really need it, so I removed the directory in /Applications. Wow, for weeks and weeks I found remnants of it in a lot of places. Very disappointing that it left so much cruft around. They should have an uninstaller. It left a really bad taste and I'm unlikely to try it again. Before someone asks. I've been using macOS for a long time. I've never seen remnants like this from a program. Sure, there are often directories left in ~/Library/Application Support/, but this was more than that. Unfortunately, I didn't write down the details, but I ran across the bits in at least 3-4 places. reply kdrag0n 4 hours agorootparentDev here — I've been meaning to update the Homebrew cask to be more complete on zap, but there's a good reason that all of these are needed: - ~/.orbstack - Docker context that points to OrbStack (for CLI) - \"source ~/.orbstack/shell/init.zsh\" in .zprofile/bash_profile (to add CLI tools to PATH) - ~/.ssh/config (for convenient SSH to OrbStack's Linux machines) - Symlinks to CLI tools in ~/.local/bin, ~/bin, or /usr/local/bin depending on what's available (to add CLI tools to existing shells on first install — only one of these is used, not all) - Standard macOS paths (~/Library/{Application Support, Preferences, Caches, HTTPStorages, Saved Application State, WebKit}) - Keychain items (for secure storage) - ~/OrbStack (empty dir for mounting shared files) - /Library/PrivilegedHelperTools (to create symlinks for compatibility) Not sure what the best solution is for people who don't use Homebrew to uninstall it. I've never liked separate uninstaller apps, and it's not possible to detect removal from /Applications when the app isn't running. reply xlii 3 hours agorootparentIMO documenting this (and uninstall section in GUI with link) would be enough for me. Used that and never felt neglected by devs. And cough since we’re at with - did you consider Nixpkgs distribution? I’m slowly moving deeper and deeper into ecosystem and use Home Manager for utilities that I use often (and use nix shell/nix run for one offs). Some packages are strictly GUI and while they aren’t handled flawlessly (self-updaters) it’s nice to have them on a single list. Yet based on your list it’s a definitely a nixventure… reply cqqxo4zV46cp 12 hours agorootparentprevI’ve found this to be the norm for ‘Docker Desktop alternatives’. Not to say that Orbstack isn’t uniquely messy. reply xlii 12 hours agorootparentprevI’m also on Orbstack mostly for performance. But unfortunately cross compiling quickly broke when I started doing mild customization (and one reasons I’m doing this is a complex setup that’s very sensitive to version changes). In the end solution was to “simply” get darwin.linux-builder up but that pulled a lot of weight behind it. It works, but it’s not the first time I spent my time on nix-ventures. reply tuananh 12 hours agoprevas platform engineer, i want to like nix. but it's not easy for everyone else. and the dx is still pretty bad IMO. for example, i prefer devbox DX just because i can add pkg like this `devbox add python@3.11`. also, looking at 120 lines flake.nix. it's not exactly \"easier\" https://github.com/Xe/douglas-adams-quotes/blob/main/flake.n... reply Cyph0n 11 hours agoparentThat's kind of an unfair comparison. The flake you linked: 1. Defines a Go binary (i.e., how to build it) 2. Defines a Docker image that uses said Go binary as an entry point 3. Defines a NixOS module that creates a systemd service that runs the Go binary (only relevant on NixOS) 4. Defines a NixOS test for the module that ensures that the NixOS module actually creates a systemd service that runs the Go binary as expected. The NixOS test framework is actually quite impressive - tests run in a QEMU VM that also runs NixOS :) Note that only (1) and (2) are relevant to the linked article (+ some of the surrounding boilerplate). reply hamandcheese 11 hours agorootparentI agree that it's not a fair comparison, but I will add that this is a big barrier to newcomers. Everyone experienced with nix builds their own ivory tower of a nix flake (myself included), so it's hard to find actually good examples of how to do basic things without wading through a bunch of other bullshit. reply rapnie 10 hours agorootparentNewcomer here. Could anyone tell if std [0] is a good way to bring more sanity into flake design, esp. in avoiding ivory towery custom approaches? Using devenv.sh is another option, but I liked emphasis on creating a common mental picture of the architecture and focus on SLDC that std provides. [0] https://std.divnix.com reply hamandcheese 10 hours agorootparentI haven't used std, but I would like to point you at what I think is the ideal way to organize a lot of nix: readTree from the virus lounge[0]. It doesn't add kitschy terms like \"Cell\" and \"growOn\", it's just a way to standardize attribute names according to where things live in the filesystem. So in their repo, /foo/bar/baz.nix has the attribute path depot.foo.bar.baz I will say that to understand how it works you need to have a solid grasp of the nix language. But once established I think it's a pattern that noobs could learn in a very quick and superficial way. [0]: https://cs.tvl.fyi/depot/-/blob/nix/readTree/README.md reply rapnie 9 hours agorootparentThank you! I already bumped into the virus lounge, with their TVIX project [0] that I found quite interesting. [0] https://code.tvl.fyi/about/tvix reply MadnessASAP 9 hours agorootparentprevI thought I was weird for my bespoke ivory tower monorepo flake.nix, glad to hear I'm not the only one. It has been a tremendous help in managing my homelab. reply Cyph0n 10 hours agorootparentprevI'm a beginner myself and have been trying my best to keep things simple. But I do agree that the complexity creep is quite tempting with Nix. Not sure why though.. reply hamandcheese 10 hours agorootparentI think nix has a fair amount of gravity - once you've started, assuming like it, you will quickly want to use it for everything. I don't think most people's flakes are more complex than the alternative (which would be, I don't know, a bunch of different script, maybe some ansible playbooks?) but it is a bit daunting when all that complexity is wrangled into a single abstraction. reply mkleczek 8 hours agorootparentprevDoes Nix make it difficult to properly modularise nix files (flakes or not)? Because it certainly looks like the things you listed are separate/orthogonal and should be in separate modules/files. Having many years of Java experience this is the reason why I stick to Maven (not moving to Gradle) - it is opinionated and strongly encourages fine grained modularisation. reply rfoo 3 hours agorootparentThe thing I hate most about a Java codebase is 100 separate .java files with 30-50 lines each and somehow it indeed worked but I have no idea where to look at if I want to find out how is something implemented. reply mkleczek 1 hour agorootparentIndeed - that’s very often the case - the right balance between too many and too big is not easy to find. Having said that - Java is in general IDE targeted language and once you have an IDE - many small files is not an issue anymore. reply yjftsjthsd-h 8 hours agorootparentprev> Because it certainly looks like the things you listed are separate/orthogonal and should be in separate modules/files. Nix absolutely allows that, to the point where I'm surprised that the linked example doesn't separate them. Most of my flakes have a flake.nix that's just a thin wrapper around a block that looks like devShell = import ./shell.nix { inherit pkgs; }; defaultPackage = import ./default.nix { inherit pkgs; }; ... reply mkleczek 8 hours agorootparentAnd what's the story with generic libraries that can later be used in your nix files to produce desired output? I am aware of flake-parts that are supposed to offer that but the ecosystem of flake-parts is on the smaller side of things... reply lkjdsklf 5 hours agorootparentMaybe I'm not understanding your question, but this is what the inputs of flakes are for. You can pull in arbitrary code from pretty much anywhere as an input reply mkleczek 4 hours agorootparentThe question is if there actually _is_ a rich ecosystem of such libraries - similar to the rich ecosystem of Maven plugins. reply georgyo 4 hours agorootparentI think something got lost along the way. Nix does not replace maven, you call maven from nix. https://ryantm.github.io/nixpkgs/languages-frameworks/maven/ nix is a build tool a similae way that docker is a build tool. You define build scripts that call the tools you already use. The major difference is that a docker file gives you no way tl be sure you can reproduce that build in the future. A flake on the other hand gives you a high degree of trust. reply mkleczek 3 hours agorootparentNothing got lost: both Nix and Maven are dependency management tools. Both are also build tools. The difference is that Maven was created as a Java build tool (and it stayed that way in general). What we have today is that there is a multitude of dependency managers and - what's worse - all of them are _also_ build tools targeted at specific language. Nix has a unique position because it is not language specific. Where it is lacking is missing standards and reusable libraries that would simplify common tasks. I am comparing to Maven because I am looking for multi-platform Maven alternative. There is Bazel but its dependency management is non-existent. There is Buck2 which is great in theory but the lack of ecosystem makes it a non-starter. Nix is the only contender in the space that offers almost everything and has a chance to become a de-facto standard of software delivery thanks to this. What's missing though is... easy to use canned solutions similar to maven plugins. EDIT: grammar reply georgyo 2 hours agorootparentIn the link I referenced, it shows you how to use maven plugins in nix. Nix has composable, reusable, and shareable functions. Nix is a full, be awkward, programming language. You'll find functions and flakes for nearly everything you might want to do. An example of one that is more plugin like is sops-nix. Though have never used maven or maven plugins, I may be missing your overall point. reply mkleczek 2 hours agorootparentIn Java/Maven ecosystem a lot of things is simple because there is a huge ecosystem of easy to integrate libraries/plugins. Want a packaged spring boot application? There is a plugin for that. Want to package it in a container image? Just add a plugin dependency. Want to build an RPM or deb package? Add another two plugin dependencies. All various artifacts are going to be uploaded to a repository and made available as dependencies. Missing a specific plugin? You can easily implement it in Java as a module and use it inside your project (and expose it as a standalone artifact as well). I can’t find anything similar in Nix ecosystem. Having a language allowing for this is not the same as having solutions already available. reply elbear 2 hours agorootparentI was reading this thread and now I finally understood what you mean by plugins. Plugins kind of exist in Nix, except they're not called that. They are functions available in some specific module. For example, `dockerTools` provides different functions like creating an image and other things. There is a module of fetchers, functions that retrieve source files from GitHub and other sources. But I don't think there are many language-specific functions, like the ones you are describing. I can't think of any, except for the ones that build a Nix package from a language-specific package. reply mkleczek 1 hour agorootparentThere are attempts like https://flake.parts/ or https://github.com/nix-community/flakelight Their aim is to create an ecosystem of reusable Nix libraries. But it is tiny. reply Cyph0n 8 hours agorootparentprevYes, to the point where it can become more confusing than helpful. The fact that Nix handles merging data structures for you makes it easy to fall into over modularization. reply viraptor 11 hours agoparentprevThose 120 lines are not exactly representative. For example if I was writing this for a single service, I wouldn't bother making a new module and would inline it instead. Then, you've got many lines which would map 1:1 to an inlined systemd service description, so you're not getting rid of those whatever the system you choose. There's also a fancy way to declare multiple systems with an override. This example is a \"let's do a trivial thing the way you'd do a big serious thing\". If you wanted to treat it as a one-off, I'm sure you could cut it down to 40 lines or so. reply whydoineedthis 10 hours agorootparentSo I can write a Dockerfile using 17 verbs (13, actually), and it's understandable language to a 4th grader... or I can write 120 lines of complete abstract nix code that means nothing to someone doing software for 20 years. Hrmmmm...this is such a TOUGH decision. reply m463 10 hours agorootparentI would like to mention one pet peeve of mine wrt docker... cramming everything into one RUN line to save space in a layer. I really wish instead of: RUN foo && \\ bar && \\ bletch You could do: LAYER RUN foo RUN bar RUN bletch LAYER or something similar. maybe even during development you could do: docker build --ignore-layer . then at the end: docker build . reply sre2 4 hours agorootparentHeredocs (described in the docs[1] under \"shell form\") was introduced 3 years ago[1]. The flag '--squash' has been in podman for quite a while[2]. It might be time for you to upgrade your tooling. [0]: https://docs.docker.com/reference/dockerfile/#shell-form [1]: https://www.docker.com/blog/introduction-to-heredocs-in-dock... [2]: https://docs.podman.io/en/latest/markdown/podman-build.1.htm... reply viraptor 9 hours agorootparentprevCramming things into one layer can save gigabytes in size. Even if it saves a 100MB, it adds up between multiple CI runs and in deployment latencies. Docker should address it one day, but until then, we just have to do it in real world scenarios. reply a_t48 9 hours agorootparentParent comment agrees with you, is just asking for more ergonomic syntax reply viraptor 9 hours agorootparentI get it, just providing more context. Should've phrased it nicer. reply yjftsjthsd-h 8 hours agorootparentprevAn alternative solution... for some definition of the term... is to write a \"naive\" Dockerfile like RUN foo RUN bar RUN baz and then just build it with something like... I think kaniko did this last I looked?... that smashes the whole thing into a single layer. Obviously that has other tradeoffs (no reuse of layers) but depending on your usecase it can be a good trade to make (why yes, I did cut my teeth in an environment where very few images shared a base layer, why do you ask?). reply IshKebab 4 hours agorootparentDocker itself can do that now too. reply viraptor 9 hours agorootparentprevI addressed the 120 lines already and they're not completely abstract. You seem uncomfortable with an alternative approach and that's fine. But this is not a good intentions argument. reply xena 10 hours agorootparentprevI've used this app for like four different talks over the years, I could clean it up, but then I break code samples in my talks. reply kaba0 1 hour agorootparentprevDockerfile’s are absolutely not something a 4th grader would understand. It looks familiar to you, because you have already learnt it. They are definitely not trivial to understand before that, and the same is also true for Nix. reply klntsky 12 hours agoparentprevThese 120 lines do quite a lot more, don't they? reply zer00eyz 9 hours agoparentprevits 120 lines of code to deal with a binary and its systemd setup. That binary + config file are effectively as close as we're going to get to a \"flat pack\" on linux. Im not sure what is the forest and where are the trees but this example shows exactly what we have lost sight of. reply operator-name 16 hours agoprevThis is great if you've already adopted Nix, and I'd love for nothing than more declarative package management solutions like Nix or Guix to take off. If you're already using Docker but want to gradually adopt Nix, there is an alternative approach outlined by this talk: https://youtu.be/l17oRkhgqHE. Instead of migrating both the configuration AND container building to Nix straight away, you can keep the Dockerfile to build the nix configuration. The biggest downside is that you don't take advantage of layers at all, but the upside is that you can gradually adapt your Dockerfiles, and reuse any Docker infrastructure or automation you already use. reply FiberBundle 12 hours agoparent[1] also uses this approach. [1] https://mitchellh.com/writing/nix-with-dockerfiles reply AtlasBarfed 9 hours agoparentprevSo one of the pillars of the article is that docker builds aren't reproducible, but Nix is. But... is a lot of that irreproducibiity (apologies for that word) because there's no guarantee one of the docker layers will be available? And... does Nix have some guarantee to the end of the universe that package versions will stay in the repository? reply operator-name 4 hours agorootparentI'd give this article a read, as it can explain it more clearly than I can: https://serokell.io/blog/what-is-nix But to briefly answer your specific questions: Docker files are commonly not reproducible because they contain arbitary stateful commands like `apt-get update`, `curl`, etc. For a layer with these kinds of commands to be reproducible you would need a mechanism to version and verify the result. Nix provides such a mechanism, and a community package repository with versioned dependancies between packages. These are defined in a domain specific language called Nix (text files) and kept into a git repository. This should be familiar if you've used a package manager with lock files before. You can guarentee the package version will stay in the repository by pinning your build to an exact commit hash in the repository. reply whazor 14 hours agoprevThis blog post is missing the reasoning on why shared docker layers are useful. It is because of caching. The more images are sharing the same layers the better, as it allows you to cache more stuff. Better caching means faster startup of containers. Why is docker bad at this? In order to enjoy the caching benefit, each time you build a docker image you want it to output as much existing layers as possible. So running apt-get install python3 today should result in the exact same layer as yesterday, if there are no new updates. But this requires the all the files to be exactly the same, including the metadata like creation time. As docker layers are cached by hashing the files. Now, Nix already does storing dependencies by hash. So the layers will always be the same with the same version and same configuration. reply hamandcheese 10 hours agoparentI would rephrase this as: The Dockerfile format imposes a hierarchical relationship between layers. This quickly becomes very annoying, since dependencies usually form dependency graphs, not dependency trees. Alternative tools, like nix (probably bazel too), are not bound in the same way. They can achieve fine grained caching by mapping their dependency graph to docker layers, which is something that can not be expressed with a Dockerfile. reply cpuguy83 7 hours agorootparentSteps in a stage are hierarchical. The final result need not be. You can build a bunch of things then merge the results in a final stage without any hierarchy (this is \"COPY --link\" in a Dockerfile). reply georgyo 4 hours agorootparentThat requires some very explicit and non-obvious effort to do. It's quite painful to do this properly in Docker. reply m1keil 13 hours agoparentprevIn Docker, if the layers are cached, the layer with apt-get won't be automatically invalidated (unless --no-cache or any changes to the upper layers). reply whazor 7 hours agorootparentI am thinking more about pipelines that run daily. Also, this ‘docker cache’ effectively means not running the step. So you might miss important security updates. Via Nix you can ensure that your dependencies are updated. And no updates means same hash. When said caching, I meant on the nodes that run the containers. With Nix you can also update only one layer, while keeping the other layers the same. reply raffraffraff 12 hours agorootparentprevBut that's what I would expect to happen. I don't see a problem. reply eichin 12 hours agorootparentWon't get invalidated even if what \"apt-get install python3\" does changes - the cache is only based on the syntax of the RUN string plus the previous layer hash, IIRC. (COPY actually invalidates if the file being copied changes, so maybe there's a way to fetch a hash of the repo and stash it where copy will notice, or something, but then it seems you need external tooling to do that bit?) reply m1keil 12 hours agorootparentprevI didn't claim there is a problem. The original comment made it sound as if docker will expire a cached layer because the (potential) result of apt-get is different, which isn't the case. reply verdverm 1 hour agoprevI've been using Dagger, it's awesome. It's the second take by the creators of Docker. It accomplishes most of what nix does for this problem Write your pipelines and more in languages you already use. It unlocks the advanced features of BuildKit, like the layer graph and advanced caching. nit: this post makes a point about deterministic builds and then uses \"latest\" for source image tags, which is not deterministic. I've always appreciated Kelsey's comment that \"latest\" is not a version reply jrockway 11 hours agoprevI spent a half a day or so relatively recently trying to build our CI base image with Nix (at the recommendation of our infra team), but it was huge, and some stuff didn't work because of linking issues. One issue that really bugged me was to build multi-arch images, it actually wants to execute stuff as the other architecture, and only supports using qemu with hardware virtualization for that. My build machine (and workstation) is a VM, so I don't have that. I do have binfmt-misc, though, so if you just happened to fork and exec the arm64 \"mkdir\" to \"mkdir /tmp\", it would have worked. Of course, this implementation is a travesty when docker layers are just tar files, and you can make the directory like this: echo \"tmp uid=0 gid=0 time=0 mode=0755 type=dir\"bsdtar -cf - @- (As an aside, I'm sure this exact layer already exists somewhere. So users probably don't even have to download it.) Every time I try nix, I feel like it's just a few months away from being something I'd use regularly. nixpkgs has a lot of packages, everything you could ever want. They all install OK onto my workstation. But \"I need bash, python, build-essential, and Bazel\" doesn't seem like something they're targeting the docker image builder at. I guess people just want to put their go binary in a docker image and ... you don't need nix for that. Pull distroless, stick your application in a tar file, and there's your container. (I personally use `rules_oci` with Bazel... but that's all it does behind the scenes. It just has some smarts about knowing how to build different binaries for different architectures and assembling and image index yaml file to push to your registry.) reply viraptor 11 hours agoparent> to build multi-arch images, it actually wants to execute stuff as the other architecture You should be able to cross compile binaries for other architectures without actually running them. As long as the package's build files support it of course. > and only supports using qemu with hardware virtualization for that That doesn't sound right. You can use qemu for architectures that be only software emulated too. The minimal example is discussed here: https://discourse.nixos.org/t/how-do-i-get-a-shell-nix-with-... I don't want to say it should be as simple as using pkgCross (https://nix.dev/tutorials/cross-compilation.html), but... are some specific issues with the usual process that you're running into? reply operator-name 4 hours agoparentprevWhat did your final Nix and Docker file look like, and did you have to use `buildFHSEnv` at all to support the odd 3rd party binaries? I think Nix really needs some articles outlining how to play well and smoothly transition from an existing system piece by piece. reply l0b0 9 hours agoparentprev> I spent a half a day or so relatively recently trying to build our CI base image with Nix (at the recommendation of our infra team), but it was huge, and some stuff didn't work because of linking issues. You must be talking about the official Nix Docker image[1], which indeed is huge. I've been using it for years for a handful of projects, but if the size is an issue you can use the method mentioned in the article and build a very minimal image with only the stuff you specify. [1] https://hub.docker.com/r/nixos/nix/tags reply bfrog 10 hours agoparentprevHmm? Cross compiling to docker images is exactly what I used nix for. I even had musl being used, it was the smallest image I could build with any tool and built the images quickly and consistently in ci with caching working well. I never saw went being used so im a bit confused where that came into play for you reply torcete 2 hours agoprevCoincidentally, Two days ago I was trying to adapt a flake to include a docker derivation. I came across with xelaso's page and inspired by the example provided (and after a few tries) I manage to compose a docker image. That was very cool! BTW: Thanks Xelaso. reply denysvitali 10 hours agoprevUnfortunately the result of a Nix Docker image is an image that is 100+ MB for no particular reason :( reply l0b0 9 hours agoparentWhere on Earth are you getting that result from? [1] gives an 11 MB image. [1] https://nix.dev/tutorials/nixos/building-and-running-docker-... reply denysvitali 6 hours agorootparentFrom experience - but I'm more than happy to be proven wrong as I would love to build all the Docker images with Nix. What you linked is the equivalent of: FROM scratch COPY hello-world /hello-world Of course that's small (hello-world is statically linked). Try to add coreutils (or any other small package) and you'll see what I mean. In my experience the size of a Docker image built with some nix packages is greater than the Debian counterpart. I don't know why though. reply georgyo 4 hours agorootparentNo, that dockerfile is not equivalent because the hello-world is not statically built in the nix version. However I'll give you that it could be smaller in more complex examples. For example glibcLocales is for all locales which is quite chunky but your application only needs one locale. reply TeeMassive 9 hours agoparentprevThere are ways to properly build a nix container image so this kind of things doesn't happen. You'll find plenty of projects on GitHub dedicated to only that. reply j-bos 15 hours agoprevI like the article but had a hard time following the specifics of the configs and commamds. Feels like it's more meant for people already familiar with nix, or sufficiently interested to study up while reading reply earthling8118 11 hours agoparentThat's very interesting, because as someone familiar with nix my take was that this was information I considered to be aimed at people who weren't familiar. reply woile 4 hours agoprevI've been using nix to build docker containers (from a Mac). I would like to skip docker as well, but I wouldn't know how. On the server, I use docker swarm, with traefik as load balancer, in a very small machine, which I can later grow. It works pretty well for me. Nix on the CI has never fail for anything but mistakes of my own. reply febed 15 hours agoprevI didn’t fully grok how this works - what is the base image for the generated image? Also wouldn’t the image size be large if the glibc is copied over again reply aidenn0 14 hours agoparent> what is the base image for the generated image? Default is none (i.e. like \"FROM scratch\" in a Dockerfile); you can specify a baseImage if needed, but I haven't had to yet. It works by copying parts of the nix store into the image as needed, but see also below. > wouldn’t the image size be large if the glibc is copied over again The original Nix docker-tools buildImage did suffer from poor reuse of common dependencies. Docker already has a way to reuse parts of images (e.g. if you build 7 images where the first N lines of a Dockerfile are the same, the 7 images will use a shared store for the results of running the first N lines). There are several backends for Docker storage that accomplish this in various ways (e.g. FS overlays, tricks with ZFS/btrfs snapshots). Nix docker-tools now has a \"buildLayeredImage\" that uses this ability of Docker to share much of the storage for the dependencies, so if you build several images that all rely on glibc, you only pay the cost of storing glibc in docker once. reply febed 14 hours agorootparentThanks, that made the article clearer for me reply conradludgate 5 hours agoprevI wanted to love nix. It seems like something I would like. I tried to compile rust using nix on my mac. Didn't work, known bug. I reinstalled my desktop to use nixos. I got lost between flakes, nixpkgs, homemanager. I managed to get vscode installed but when I added the nix extension (declared in nix) it would refuse to run vscode... It's just not a good experience so I reinstalled arch reply madjam002 16 hours agoprevDoes anyone here have any experience using https://github.com/pdtpartners/nix-snapshotter ? I build a lot of Docker images using Nix, and while yes it’s generally more pleasant than using Dockerfiles, the 128 layer limit is really annoying and easy to hit when you start building images with Nix. The workaround of grouping store paths makes poor use of storage and bandwidth. reply hinshun 14 hours agoparentAuthor of nix-snapshotter here. Yes, one of the main downsides of Docker images using Nix is the 128 layer limit. It means we have to use a heuristic to combine packages into the same layer and losing Nix’s package granularity. When building containers with Nix packages already on a Nix binary cache you also have to transform the Nix packages into layer tarballs effectively doubling the storage requirements. Nix-snapshotter brings native understanding of Nix packages to the container ecosystem so the runtime prepares the container root filesystem directly from the Nix store. This means docker pull == Nix substitution and also at Nix package granularity. It goes a bit further with Kubernetes integration that you can read about in the repo. Let me know if you have any other questions! reply k8svet 5 hours agorootparentWhat's the state of deployment for something like nix-snapshotter nowadays (with the realization that the answer depends on which of N k8s install methods might be in use)? I assume it's mostly in the field of ... \"you're making a semi-large investment on this enough that you're doing semi-custom kubernetes deployments with custom containerd?\" Or maybe the thought is nix-snapshotter users are running k8s/kubelet with nixos anyway so its not a big deal to swapout/add containerd config? reply mikepurvis 16 hours agoparentprevI haven't tried it yet as I need to produce containers that can work on public cloud k8s, but it definitely looks like the way to go. All the existing methods for grouping store paths into layers are finnicky, brittle, and non-optimal. reply mikepurvis 16 hours agoprevNo discussion about Nix-built containers is complete without mentioning nix2container: https://github.com/nlewo/nix2container It is truly magical for handling large, multi-layered containers. Instead of building the container archives themselves and storing them in the nix store, it builds a JSON manifest that is consumed by a lightly patched version of skopeo that streams the layers directly to either your local container engine or the registry. This means you never rebuild or reupload a container layer that is unchanged. Disclosure: I contributed a change in nix2container that allows cheaply pulling non-Nix layers into the build, just using the content hashes from their registry manifests. reply Nullabillity 12 hours agoparentNixpkgs' streamDockerImage does something similar, instead of storing a multi-layer tarball it produces a script that cats them all together on demand, ready to feed into `docker load` or whatever. reply takeda 8 hours agorootparentIn that case streamDockerImage produces script that you run then you pipe output to skopeo or docker. nix2container wraps all of that and it automatically runs it in behind the scenes when you call nix run. The whole image generation is much more efficient as well. In the standard streamDockerImage you get a script that generates a docker layers and the image. In nix2container all layers are stored in nix cache so subsequent run doesn't regenerate them. I believe that was the main goal behind this solution. Another benefit (and I think it is even better than the caching) is that it also allows you manually specify what dependencies go to each layer. The automatic way that the code offers, is nice in theory, but because docker has a limit of 128 layers, in practice it starts nicely then the last layer is a clump of remaining dependencies that didn't fit in previous layers. With nix2container I managed for example to create the first layer which contains just Python and all of its dependencies, then the next layer are my application dependencies (Python packages) the last layer is my application. With this approach a simple bugfix in the application only replaces the last layer that's the size of a few kb. Only a change of Python version will rebuild the whole thing. reply Thaxll 9 hours agoprevI keep reading about Nix and I still don't understand what it does better than Docker, all the example in the post are trivial to do in a Dockerfile so where is the added value? Docker build are deterministic and easily reproductible, you use a tagged image, that's it, it set in stone. The 0.01% of Dockerfile that don't work, what does it even means, what does not work? The other thing is about that buildGoModule module so now you need somehow a third party tool to use or build Go in a Docker image, when using a Dockerfile you just use regular Go commands such as go build and you know exactly what is going on and what args you use to build the binary. As for the thing about using Ubuntu 18 which is out of date and not finding it, most orgs have docker image cache especially since docker hub closed access to large downloads, but more importantly there is a reason that's its not there anymore, it's not secure to use it, it's like wanting to use the JVM 6, you should not use something that is out of date security wise. reply clhodapp 9 hours agoparentMost Docker builds are not remotely deterministic or reproducible, as most of them pull in floating versions of their dependencies. This means that the same Dockerfile is likely to produce different results today than it did yesterday. reply ok_dad 9 hours agoparentprevAren’t Nix builds actually deterministic in that they’ll build the same each time? Docker doesn’t have that, you’re just using prebuilt images everywhere. Determinism has a computer science definition, it’s not “build once run anywhere,” it’s more like “builds the exact same binary each time.” reply cpuguy83 7 hours agorootparentDon't conflate using \"apt-get\" in a Dockerfile with what \"docker build\" does. reply takeda 7 hours agorootparentYou can absolutely build a reproducible image in Dockerfile if you have discipline and follow specific patterns of doing. But you can achieve the same result if you use similar techniques with a bash script. reply georgyo 4 hours agorootparentYou _can_ if you have _disipline_. That sounds like a foot gun the longer a project goes on and as more people touch the code. Just create a snapshot of the OS repo, so apt/dnf/opkg/ etc will all reproduce the same results. Make sure _any_ scripts you call don't make web requests. If they do you have the validate the checksums of everything downloaded. Have no way to be sure that npm/pip/cargo's package build scripts are not actually pulling down arbitrary content at build time. reply clhodapp 7 hours agorootparentprevDocker doesn't give you the proper tooling to not have to use e.g. apt-get in your Dockerfiles. For that reason, one might as well conflate them. reply vergessenmir 4 hours agorootparentI'm not sure that this is a Docker problem but you do have a point. I've used docker from the very beginning and it always surprised me that users opted to use package managers over downloading the dependencies and then using ADD in the docker file. Using this approach you get something reproducible. Using apt-get in a docker file is an antipattern reply zmgsabst 3 hours agorootparentWhy? — I agree that it’s not reproducible, but so what? We have 2-3 service updates a day from a dozen engineers working asynchronously — and we allow non-critical packages to float their versions. I’d say that successfully applies a fix/security patch/etc far, far more often than it breaks things. Presumably we’re trying to minimize developer effort or maximize system reliability — and in my experience, having fresh packages does both. So what’s the harm, precisely? reply cpuguy83 6 hours agorootparentprevDocker doesn't give you the tooling to build a package and opts for you to bring the toolchain of your choice. Docker executes your toolchain, and does not prescribe one to you except for how it is executed. Nix is the toolchain, which of course has its advantages. reply ok_dad 7 hours agorootparentprevUh, I never even mentioned apt. Docker and nix are, likewise, very different. In not super familiar with either but I do know docker isn’t reproducible by design whereas nix is. I’m not sure nix is always deterministic, though i know docker (and apt) certainly aren’t, nor are they reproducible by design. reply jhanoncomm 9 hours agorootparentprevIs this about timestamps or is there more to it? reply takeda 7 hours agorootparentThe Nix idea is to start building with a known state of the system and list every dependency explicitly (nothing is implicit, or downloaded over net during build). This is achieved by building inside of a chroot, with blocked network access etc. Only the dependencies that are explicitly listed in the derivation are available. reply MadnessASAP 8 hours agorootparentprevThe timestamps thing is part of ensuring that archives will have the correct hash. Nix ensures that the inputs to a build, that being the compiler, environment, dependencies, file system, are exactly the same. The idea being then that the compiler will produce an identical output. Hash's are used throughout the process to ensure this is actually the case, they are also used to identify specific outputs. reply janjongboom 6 hours agoparentprevThat's the interesting bit about Dockerfiles. They look _looks_ deterministic, and they even are for a while while you're looking at it as a developer. I've done a detailed writeup of how it's not deterministic in https://docs.stablebuild.com/why-stablebuild reply otabdeveloper4 4 hours agoparentprev> I still don't understand what it does better than Docker It doesn't break as you scale. If you don't need that, then keep using Docker. (Personally, for me \"scale\" starts at \"3 PC's in the home\", so I eventually switched all of them to NixOS. I don't have time to babysit these computers.) > Docker build are deterministic and easily reproductible No, they definitely aren't. You don't really want to go down this rabbit hole, because at the end you realize Nix is still the simplest and most mature solution. reply TeeMassive 9 hours agoparentprevDocker builds are not deterministic, I don't get where you get that idea. I can't count the hours lost because the last guy who left one year ago built the image using duck tape and sed commands everywhere. The image is set in stone, but so is a zip file, there's nothing special here. Building an image using nix solves many problems regarding not only reproducible environments that can be tested outside a container but also fully horizontal dependency management where each dependency gets a layer that's not stacked on one another like a typical apt/npm/cargo/pip command. And I don't have to reverse engineer the world just to see what files changed in the filesystem since everything has its place and has a systematic BOM. reply jhanoncomm 9 hours agorootparentSo is it right, to make docker reproducible it needs to either build dependencies from source from say a git hash or use other package managers that are reproducible or rely on base images that are reproducible. And that all relies on discipline. Just like using a dynamically typed programming language can in theory have no type errors at run time, if you are careful enough. reply yjftsjthsd-h 8 hours agorootparentRight; you could write a Dockerfile that went something like FROM base-image@e70197813aa3b7c86586e6ecbbf0e18d2643dfc8a788aac79e8c906b9e2b0785 RUN pkg install foo=1.2.3 bar=2.3.4 RUN git clone https://some/source.git && cd source && git checkout f8b02f5809843d97553a1df02997a5896ba3c1c6 RUN gcc --reproducible-flags source/foo.c -o foo but that's (IME) really rare; you're more likely to find `FROM debian:10` (which isn't too likely to change but is not pinned) and `RUN git clone -b v1.2.3 repo.git` (which is probably fixed but could change)... And then there's the Dockerfiles that just `RUN git clone repo.git` and run with whatever happened to be in the latest commit at the moment... reply janjongboom 6 hours agorootparentAnd that assumes that `foo` and `bar` are not overwritten or deleted in your package repository, and that the git repository remains available. reply nullify88 5 hours agorootparentprevMaintaining something like that is a pain unless you have tooling like Renovate to inform and update the digests and versions. reply cpuguy83 7 hours agorootparentprevIt is likely just as rare for someone to use nix for this, though. reply yjftsjthsd-h 7 hours agorootparentPossible; I don't have a feel for the relative likelihoods. I think the thing nix has going for it is that you can write a nix package definition without having to actually hardcode anything in and nix itself will give you the defaults to make ex. compilers be deterministic/reproducible, and automate handling flake.lock so you don't have to actually pay attention to the pins yourself. Or put differently; you can make either one reproducible, but nix is designed to help you do that while docker really doesn't care. reply clhodapp 7 hours agorootparentprevIt's actually how nix works by default. When you pull in a dependency, you are actually pulling in a full description of how to build it. And it pulls in full descriptions of how to build its dependencies and so on. The only reason nix isn't dog slow is that it has really strong caching so it doesn't have to build everything from source. reply MadnessASAP 7 hours agorootparentprevIf you're using Nix, that is what you are ultimately producing, it's buried under significant amounts of boilerplate and sensible defaults. Ultimately the output of Nix (called a derivation) reads a lot like a pile of references, build instructions, and checksums. reply isbvhodnvemrwvn 4 hours agorootparentI think their point was that number of people who use nix is a rounding error, perhaps due to poor user experience. reply MadnessASAP 7 hours agorootparentprevYou can also use a hammer to put a screw in the wall. Dockerfiles being at their core a set of instructions for producing a container image could of course be used to make a reproducible image. Although you'd have to be painfully verbose to ensure that you got the exact same output. You would actually likely need 2 files, the first being the build environment that the second actually get built in. Or you could use Nix that is actually intended to do this and provides the necessary framework for reproducibility. reply inopinatus 4 hours agorootparentfun fact: there actually is a class of impact driver[1] that couples longitudinal force to rotation to prevent cam-out on screws like the Phillips head when high torque is required [1] https://en.wikipedia.org/wiki/Impact_driver#Manual_impact_dr... reply tetris11 16 hours agoprevGuix is also pretty good at this, only lacking up to date packages that one would want to build an image with reply Zambyte 11 hours agoparentIt is very easy to overload package versions locally for your needs, and it's quite easy to push that upstream to Guix so that others may benefit as well :) reply tetris11 4 hours agorootparentI have patches on Guix over a year old :-) The problem isn't contributions, it's reviews reply djaouen 8 hours agorootparentprevCan you explain (or point to an explanation) of exactly how to do that? The Guix versions of Erlang and Elixir are way out of date, and I would like to push a fix. reply Zambyte 8 hours agorootparentSee here[0] for pushing a fix, here[1] for the anatomy of a package definition (often you only need to bump the version number and update the hash, but compilers may be a bit more involved). It may be useful to define package variants[2], which is what I do for some packages locally. You can also see this page[3] for using ad-hoc package variants using command line flags. Hope this helps :) [0] https://guix.gnu.org/manual/devel/en/html_node/Contributing.... [1] https://guix.gnu.org/manual/devel/en/html_node/Defining-Pack... [2] https://guix.gnu.org/manual/devel/en/html_node/Defining-Pack... [3] https://guix.gnu.org/manual/devel/en/html_node/Package-Trans... reply djaouen 7 hours agorootparentThanks for the info, I will look into these links and see if I can push an update to Erlang and Elixir in the coming days. :) reply djaouen 15 hours agoparentprevThis is my one gripe with Guix. Alas, we (apparently) can't have it all! reply andrewstuart 6 hours agoprevI like nspawn over docker because it doesn't use the layered file system thing. Instead, it's just a simple root directory placed somewhere and you run the container on that. Much more straightforward. reply djaouen 16 hours agoprevI just wanted to chime in here and say that Guix also has a nice and easy-to-use Docker option with \"guix pack -f docker\" [1]. Guix also has the advantage of using an already-used language (Guile/Scheme) rather than its own bespoke one. :) [1] https://guix.gnu.org/manual/en/html_node/Invoking-guix-pack.... reply asmor 6 hours agoprevLittle known (possibly unintended) feature, but you can put the `toplevel` attribute of a nixosSystem into docker image `contents`, which lets you use NixOS modules to set things up. Just be sure to import the minimal preset, because those images get large. Unfortunately booting the entire system with /init is largely broken, especially without --privileged. This would be an amazing feature if it didn't require so much extra tinkering. reply paulddraper 8 hours agoprevI really like Bazel (rules_oci) for building containers. It's the way building containers \"\"should\"\" be. Here's my base image, here's my files, here's my command, write the into an image. reply kmarc 5 hours agoparentAs many bazel rules, rules_oci's predecessor (rules_docker) was an unmaintained spagetti of hell, now we are pushed to rules_oci, and it's rpmtree recommended way of installing rpms which then in turn don't support post install script... All this bazel-is-our-savior complex burns down when we want to build a tiny bit complicated thing with it. And we unfortunately do try to do that (our devbox image), which with full caching takes long minutes to an hour, and it's a freaking thousands pine mess instead of using a lined dockerfioe with pinned versions. I absolutely hate bazel and its broken unmaintained Google-abandoned rulesets and I wish we either used either Docker or Buck2 for everything. reply paulddraper 5 hours agorootparent> unmaintained spagetti of hell, rules_docker was fundamentally flawed -- and overreaching in scope -- in ways that rules_oci is not. > broken unmaintained Google-abandoned rulesets rules_oci last commit was 12 hours ago, and it's been actively maintained for years. (This criticism is even weirder from someone pushing Buck2. Like, it's a great tool. But, apparently it warranted a complete rewrite too, eh?) > instead of using a lined dockerfioe with pinned versions You'll pin every transitive version? > All this bazel-is-our-savior complex burns down when we want to build a tiny bit complicated thing with it. Let me rephrase. I would not use Bazel to build images from rpm or deb packages. But....would you use Nix for installing rpm or deb packages????? I believe you've lost the thread. reply takeda 6 hours agoparentprevThat's pretty much how building on nix works, except you don't need base image or your application file, you specify what command to run from which package and it will be placed in the container with all runtime dependencies automatically. Of course you can customize the container further if needed. reply paulddraper 5 hours agorootparentYes, similar idea. With the objective of reproducible software. (Also, you don't need a base image for rules_oci either; most people choose to start with one.) reply Nonoyesnoyes 15 hours agoprevThe article lost me somewhere. Long intro and than just assuming too much. That's just possible due to nix being more granular? Is that right? Can I really build nix from 5 years ago? No src gone? No cache server gone? Nothing? I mean yeah Ubuntu as a base is shitty. reply xena 14 hours agoparentThe text is written to be spoken, it works better when I present it. I'll have the video edited next week, that may flow better for you. reply Nonoyesnoyes 3 hours agorootparentI will check it out then. Would you answer my assumption? Is this easier with nix because it's more granular? Like if I have a cacerts alone I can select it? And can I really build stuff from 5 years ago? reply chaxor 11 hours agoparentprevI don't like Ubuntu or Debian as a base image for docker, but it's typically my go-to if I need glibc stuff or browser emulation. Is there a better alternative like Alpine but with glibc that isn't Debian? reply imp0cat 2 hours agorootparentI think Debian is a solid choice, mostly everything is either present or available. Image sizes can get out of hand tho. reply debuggerpk 16 hours agoprevhorrible, horrible font on the website! reply mmh0000 15 hours agoparentWhat a weird thing to complain about. Just override the font if you dislike it that much. That's the great thing about the interwebs, it's all just text, you can format it however you want. Assuming you're using something based off chrome: https://chromewebstore.google.com/detail/font-changer/obgkji... Or Firefox: https://addons.mozilla.org/en-US/firefox/addon/refont/ reply eichin 12 hours agorootparentIt's actually a smooth readable (but tall-and-skinny) font on my personal laptop, and a pixelated mess on my work laptop; I've never figured out why... Oh, huh, they're using a custom font: https://xeiaso.net/blog/iaso-fonts/ so that should actually be consistent. (After the weekend I'll poke at it on the other laptop and see what's up.) What I'm getting at is that they might (unintentionally) be complaining about a rendering bug and not the actual font... reply xena 10 hours agorootparentWhat browser? reply eichin 4 hours agorootparentgoogle chrome in both cases. reply mrd3v0 11 hours agorootparentprevWhat a neat reply I am going to use whenever valid criticism about he readability or accessibility of my websites arises. \"Just use an extension that changes the website bro!\" reply cstrahan 11 hours agoparentprevThe font is a custom build of Iosevka, which is almost certainly inspired by the commercial font Pragmata Pro (https://fsd.it/shop/fonts/pragmatapro/). When Pragmata Pro was first released a little over 10 years ago, it sold for around $400 (I know this because I and many, many others bought a copy back then). As another commenter points out, you may have some rendering issue. Alternatively, you may just not like the font. Can't please everyone. reply ildjarn 4 hours agoprevWhat the post misses is that lots of packages are not available on Nix, but everything is available on Docker automatically. If what you need is available, however, then it can be so much better. reply georgyo 4 hours agoparentWhat is automatic about docker? Do you mean other people have already put in the work? Or do you mean that it's more trivial to pip/npm/cargo install stuff? reply solatic 7 hours agoprev [–] Having the author do this for a service written in Go is a mistake. Your first address for containerizing Go services should be ko: https://ko.build/ , and similar solutions like Jib in the Java ecosystem: https://github.com/GoogleContainerTools/jib . No need to require everyone to install something heavy like Nix, no need for privileged containers in CI to connect to a Docker daemon so that actual commands can be executed to determine filesystem contents, just the absolute bare minimum of a manifest defining a base layer + the compiled artifacts copied into the tarball at the correct positions. More languages should support this kind of model - when you see that pnpm's recipe (https://pnpm.io/docker), ultimately, is to pick a pre-existing node base image, then copy artifacts in and set some manifest settings, there's really no technical reason why something like \"pnpm build-container-image\", without a dependency on a Docker daemon, hasn't been implemented yet. Using nix, or Dockerfile, or similar systems are, today, fundamentally additional complications to support building containerized systems that are not pure Go or pure Java etc. So we should stop recommending them as the default. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Xe Iaso, a Senior Technophilosopher at Fly.io, recommends using Nix as a Docker image builder over Docker's builder for enhanced efficiency and unique features.",
      "Nix excels in dependency management, reducing layer modifications, and enhancing reproducibility when creating Docker images, advocating for its use in building and deploying container images.",
      "The article details the process of building layered Docker images with Nix, deploying them to the cloud, and promoting layer sharing among services to minimize redundancy, catering to both new and seasoned users for cost-effective and efficient solutions."
    ],
    "commentSummary": [
      "The discussion delves into using Nix and Docker for building container images, focusing on reproducibility, determinism, container size optimization, and package management.",
      "Participants share their experiences and recommendations, acknowledging the complexity of software development and the importance of consistent and secure software environments.",
      "Alternative tools like StableBuild, flox, Orbstack, and nix-snapshotter are suggested to enhance the efficiency of building container images and managing dependencies."
    ],
    "points": 346,
    "commentCount": 195,
    "retryCount": 0,
    "time": 1710532607
  },
  {
    "id": 39717558,
    "title": "FTC and DOJ push for McDonald's ice cream machine repair exemptions",
    "originLink": "https://arstechnica.com/tech-policy/2024/03/ftc-and-doj-want-to-free-mcdonalds-ice-cream-machines-from-dmca-repair-rules/",
    "originBody": "I scream, you scream, we all scream for 1201(c)3 exemptions — US government agencies demand fixable ice cream machines McFlurries are a notable part of petition for commercial and industrial repairs. Kevin Purdy - 3/15/2024, 4:26 PM Enlarge / Taylor's C709 Soft Serve Freezer isn't so much mechanically complicated as it is a software and diagnostic trap for anyone without authorized access. iFixit/YouTube reader comments 69 Many devices have been made difficult or financially nonviable to repair, whether by design or because of a lack of parts, manuals, or specialty tools. Machines that make ice cream, however, seem to have a special place in the hearts of lawmakers. Those machines are often broken and locked down for only the most profitable repairs. The Federal Trade Commission and the antitrust division of the Department of Justice have asked the US Copyright Office (PDF) to exempt \"commercial soft serve machines\" from the anti-circumvention rules of Section 1201 of the Digital Millennium Copyright Act (DMCA). The governing bodies also submitted proprietary diagnostic kits, programmable logic controllers, and enterprise IT devices for DMCA exemptions. \"In each case, an exemption would give users more choices for third-party and self-repair and would likely lead to cost savings and a better return on investment in commercial and industrial equipment,\" the joint comment states. Those markets would also see greater competition in the repair market, and companies would be prevented from using DMCA laws to enforce monopolies on repair, according to the comment. Further Reading iFixit tears down a McDonald’s ice cream machine, demands DMCA exemption for it The joint comment builds upon a petition filed by repair vendor and advocate iFixit and interest group Public Knowledge, which advocated for broad reforms while keeping a relatable, ingestible example at its center. McDonald's soft serve ice cream machines, which are famously frequently broken, are supplied by industrial vendor Taylor. Taylor's C709 Soft Serve Freezer requires lengthy, finicky warm-up and cleaning cycles, produces obtuse error codes, and, perhaps not coincidentally, costs $350 per 15 minutes of service for a Taylor technician to fix. iFixit tore down such a machine, confirming the lengthy process between plugging in and soft serving. After one company built a Raspberry Pi-powered device, the Kytch, that could provide better diagnostics and insights, Taylor moved to ban franchisees from installing the device, then offered up its own competing product. Kytch has sued Taylor for $900 million in a case that is still pending. Advertisement Beyond ice cream, the petitions to the Copyright Office would provide more broad exemptions for industrial and commercial repairs that require some kind of workaround, decryption, or other software tinkering. Going past technological protection measures (TPMs) was made illegal by the 1998 DMCA, which was put in place largely because of the concerns of media firms facing what they considered rampant piracy. Every three years, the Copyright Office allows for petitions to exempt certain exceptions to DMCA violations (and renew prior exemptions). Repair advocates have won exemptions for farm equipment repair, video game consoles, cars, and certain medical gear. The exemption is often granted for device fixing if a repair person can work past its locks, but not for the distribution of tools that would make such a repair far easier. The esoteric nature of such \"release valve\" offerings has led groups like the EFF to push for the DMCA's abolishment. DMCA exemptions occur on a parallel track to state right-to-repair bills and broader federal action. President Biden issued an executive order that included a push for repair reforms. The FTC has issued studies that call out unnecessary repair restrictions and has taken action against firms like Harley-Davidson, Westinghouse, and grill maker Weber for tying warranties to an authorized repair service. Disclosure: Kevin Purdy previously worked for iFixit. He has no financial ties to the company. reader comments 69 Kevin Purdy Kevin is a senior technology reporter at Ars Technica, covering a variety of technology topics and reviewing products. He started his writing career as a newspaper reporter, covering business, crime, and other topics. He has written about technology and computing for more than 15 years. Advertisement Channel Ars Technica SITREP: F-16 replacement search a signal of F-35 fail? Footage courtesy of Dvids, Boeing, and The United States Navy. SITREP: F-16 replacement search a signal of F-35 fail? Sitrep: Boeing 707 The F-35's next tech upgrade US Navy Gets an Italian Accent SITREP: DOD Resets Ballistic Missile Interceptor program SITREP: DOD's New Long-Range Air-to-Air Missile Aims to \"Outstick\" China Army's New Pistol Has Had Some Misfires Army's Next (Vertical) Lift En Route SITREP: President Trump's Missile Defense Strategy Hybrid Options for US's Next Top Fighter The Air Force’s Senior Citizen Chopper Can’t Retire Yet Ars Live #23: The History and Future of Tech Law Police re-creation of body camera evidence - Pueblo, COArs Technica Visual Labs body camera software with the Dos Palos PDArs Technica He knew his rights; he got tased anyway More videos ← Previous story Next story → Related Stories Today on Ars",
    "commentLink": "https://news.ycombinator.com/item?id=39717558",
    "commentBody": "FTC and DOJ want to free McDonald's ice cream machines from DMCA repair rules (arstechnica.com)263 points by rbanffy 20 hours agohidepastfavorite171 comments glenstein 20 hours agoI understand individual actors following incentives wherever they lead. I wouldn't say condone, but understand. What I don't understand is how it came to pass that McDonalds, whom I would assume are canny and cutthroat in their own right, would find themselves maneuvered into a position like this. Is there no equivalent phenomenon when it comes to their grills, deep fryers, drive-thru comms systems, self-order kiosks, etc? reply LinuxBender 19 hours agoparentHere [1] is a video on this topic by Johnny Harris from a couple years ago. It's been a while since I watched it but IIRC there is a conflicting relationship between specifically McDonalds the corporation and Taylor which benefits both of them but screws over the individual franchise owners. Here is an interactive map [2] that shows where all the broken machines are. [1] - https://www.youtube.com/watch?v=SrDEtSlqJC4 [video][30 mins] There is a sponsor ad from 06:49 to 08:47 [2] - https://mcbroken.com/ reply lotsofpulp 19 hours agorootparentAll franchise brands do this once they have sufficient power relative to the franchisees. Hotel brands force hotel owners to use only certain vendors, taking a kickback from the vendor. Same with restaurants/etc. I can give a specific example. Roughly 8 years ago, IHG hotels required all of their branded hotels to only sell Coca Cola products. The only reason this would happen is if Coke paid IHG. It has nothing to do with a person’s hotel stay. It just restricts hotel owners and hotel guests from being able to sell and buy their preferred products, such as Red Bull or Starbucks or Pepsi/Tropicana, for the benefit of Coke and IHG. reply TheGRS 15 hours agorootparentIf I'm following OP's question, it was basically that all of this makes sense from a \"people are doing what benefits them the most\" POV. But some time went by and it morphed into being overall bad for the McDonald's brand, and that's the puzzling part of it. You would think Micky D's would have power to step in and say \"enough is a enough, this is hurting our image, quit fucking around and fix the machines or we find another vendor.\" I would agree Micky D's corporate probably does have sway here, so this problem probably just isn't interesting or important enough for them to do anything about it. Maybe ice cream is such a small part of their sales they don't see the point in raising a stink about it. Or maybe the vendor just happened to Game of Thrones their way into a little niche of power that Ronald can't do anything about. And for the size that they are, I find that pretty interesting and puzzling as well! reply autoexec 14 hours agorootparent> If I'm following OP's question, it was basically that all of this makes sense from a \"people are doing what benefits them the most\" POV. I wouldn't go so far as to say that it makes sense, but it's no surprise that things break down when a bunch of people who should be working together each act selfishly. As you'd expect nobody really wins, but it's the little guy that gets screwed the worst. Customers don't get ice cream, employees deal with frustrated customers, franchise owners lose time and money, the shitty vendor gets worked around, and the corporation's image takes a hit for being the ringleader of this circus. reply lesuorac 13 hours agorootparentprevI'm really surprised kickbacks haven't become illegal. It seems that a lot of really bad behavior can be traced to that especially in the medical field. It also pretty much just hurts the consumer as well since competition (or lack thereof) is based on who gives the seller the most money. reply lotsofpulp 12 hours agorootparentBecause there is plausible deniability that it is not a kickback. You call it a vendor quality assurance fee or something. reply tacticalturtle 19 hours agorootparentprevIs there such a thing as a franchisee Union? A quick search suggests no for McDonalds. It seems like there should be some trade group responsible for collectively bargaining with the main corporation, as a check against kickbacks. reply kennethrc 19 hours agorootparentprevPossibly apocryphal, but I'd heard the reason most Marriott (and their associated brands) have Pepsi vs. Coke is 'cause Marriott Sr. asked Coca-Cola for a loan some years ago and they refused him. As the joke goes, \"... is Pepsi okay?\" reply bombcar 18 hours agorootparentMost \"Pepsi places\" do have Coke on hand if they have a bar - a rum and Pepsi just isn't right. reply smallmancontrov 16 hours agorootparentDo they? Pepsi has been making a big push into \"rich people places\" (air ports, ski resorts, conference venues, that sort of thing) and the usual result is not that I ask for a Diet Coke and one appears, it's that I ask and one fails to appear, and continues to fail to appear at every place I visit on the entire trip/vacation and I simply must go without. Which is fine, it's just a flavor, but this is noticeable and repeatable and annoying and almost exactly the opposite of the situation where a bar has a strategic reserve of the good stuff. reply bombcar 15 hours agorootparentThe ones I've seen do - and pretty expensive hotels, too. If you're running a conference and ask them to get some coke from the bar, they usually have been able to get it (it's also possible they have a secret \"guest happiness\" stash) - and I've seen them pour it at the bar - Pepsi is from the spigot machine, Coke from a can. reply smallmancontrov 13 hours agorootparentRules are different if you are at a bar and paying bar prices for bar products, I get that, but it's not what I'm looking for. I could also go out of my way to buy from an unaffiliated store and schlep the stuff, but I'm just not that much of an addict. My point is that I don't think it's reasonable to be dismissive of Pepsi's \"accomplishment\" here -- they have arranged a substantial enough barrier to corral the behavior of someone with a significant (though not mountain-scaling) preference to the contrary. Significant enough that my fallback is usually water. Like out the toilet. Is that an actual preference or just spite-driven? Nope, sprite is a Coke product, we don't serve that here! But seriously, it's a real preference, it holds for the generics too. Most generic colas seem to be based on the Pepsi taste. Ugh. RC isn't, so RC Cola is fine, but too rare to ever be helpful. Caring about this is a curse! reply handsclean 16 hours agorootparentprevI think they’d just make it with Pepsi and call it a “rum and coke” anyway. “Coke” is the generic word for all soda/pop, including Pepsi, in much of the US and world, and I think “rum and coke” also qualifies as a name in its own right. reply rfrey 14 hours agorootparent>“Coke” is the generic word for all soda/pop, including Pepsi, in much of the US and world I don't think this is even true for most of the USA, much less the world. I've never heard anyone refer to anything except a cola as \"coke\" although I heard that was the vernacular in some subset of the US... some southern US states? reply galangalalgol 14 hours agorootparentIt isn't that niche. Probably in second behind \"soda\" pop being in third place. Some areas have other names like \"cold drink\" that while seeming generic actually only refer to carbonated sweet beverages. Edit: forgot map https://www.reddit.com/media?url=https%3A%2F%2Fexternal-prev... reply actionfromafar 14 hours agorootparentprevA Fanta is a Coke. reply autoexec 13 hours agorootparentnot that there's anything wrong with that reply nradov 19 hours agorootparentprevBeyond vendor kickbacks, hotel brands do have legitimate reasons to ensure a consistent guest experience regardless of which franchisee owns a particular property. Frequent travelers can be surprisingly picky about little things like soft drinks and they don't like surprises. reply lotsofpulp 19 hours agorootparentThen they would have required offering specific products. It would not require prohibiting other products. Edit: to respond to tomnipotent: I don’t see why a distributor is relevant. There are myriad to choose from. Hotel owners/operators can buy from the local Coke/Pepsi/whatever distributor, or Sam’s/Costco/grocery store, or Restaurant Depot/US Foods, etc. Edit 2: note that IHG is not buying the beverages to sell in the franchised hotel, the IHG franchisees are. IHG does not suffer any costs from making an exclusive deal with Coke, or not making an exclusive deal with Coke. reply bombcar 18 hours agorootparentIt's definitely possible, and I've seen it done, but obviously for most businesses that have to answer the \"coke or Pepsi\" question pick one. Nothing is really preventing having both in the same soda machine, but it's rare enough that there must be a reason, likely exclusivity deals and discounts. Gas stations will usually have both, for example. Fast food places won't. reply tomnipotent 19 hours agorootparentprev> It would not require prohibiting other products. You're forgetting about distribution and margin. Your distributor is unlikely to offer both choices, and keeping the original product means selling less of the new so you're getting less favorable volume deals with both. For the hotel, it makes sense to concentrate that sales volume into a single contract for better discounts. reply barbazoo 19 hours agorootparentprevJust imagine for a second if your hotel had Pepsi instead of Coca Cola. *shudder*. Not a world I want to live in /s reply Someone1234 19 hours agoparentprevMcDonald's are the bad actor here. There is two \"McDonald's\" the corporation, that requires a specific machine and by extension a specific company to repair it exclusively, and then there is a ton of \"McDonald's\" franchises who are the victims (paying the literal price). So McDonald's corporate didn't get out-maneuvered, they just found a new way to extract money from their franchises, who tried to fight back (and kind of lost the first battle). Sounds like the FTC/DOJ are stepping in to give the franchises more ammo to fight back with. reply autoexec 13 hours agorootparent> and then there is a ton of \"McDonald's\" franchises who are the victims (paying the literal price). Everybody loses in this situation. the biggest victims here are the consumers who can't get the products that they want, and have had to resort to developing/using apps to track down working machines, followed by the employees who have to deal with the constantly angry/frustrated customers, followed by the franchise owners who are losing money and disappointing their customers. The franchise owners are acting in self-defense by working around the vendor which means the vendor is losing money on service calls, and the corporation loses because (quite rightly) their image suffers for causing this mess in the first place and allowing it to continue. reply cogman10 19 hours agoparentprev> grills, deep fryers, drive-thru comms systems, self-order kiosks, etc? The key difference between all those things is there's almost no moving parts involved. Grills are simply resistive heaters and a thermostat that will outlast everyone. Deep fryers are exactly the same, but with a tub to put in oil. Drive through comms systems are simply speakers/telephone systems with the only moving part being the speaker itself. (The rest is software). And The kiosks are basically just giant touch screens with a small computer running everything. Ice cream is different. It's a thick highly spoil-able product being pushed through tubes which need to be clean. It involves a refrigerant and often a mixing device to keep the ice cream from freezing solid. Mcdonalds isn't the only one with icecream machines that constantly break down. If you notice, everyone has this problem. It's simply a hard product to serve in a sanitary way using machinery. reply jdietrich 16 hours agorootparent>Deep fryers are exactly the same, but with a tub to put in oil. Not really true. The fryers used by McDonalds and the vast majority of QSRs are digitally controlled have a fairly sophisticated recirculating filter system to keep the shortening in good condition. The state of this system is much less critical from a food safety standpoint, but it can be extremely hazardous to employees for obvious reasons. The pressure fryers used by fried chicken restaurants are potentially even more hazardous. To my knowledge, none of these systems have weird DMCA-protected firmware to prevent unauthorised maintenance. reply cogman10 15 hours agorootparentFair point. I do think there's a little bit more difficulty in the ice cream machine that doesn't exist with oil. I went and watched some videos on fry machine maintenance (lol) and one thing that doesn't translate to the ice cream machine is how easy it is to empty the oil, throw in polish, refill, empty, refill. You can't really have an automated process which drains out the ice-cream and then sterilizes it. > To my knowledge, none of these systems have weird DMCA-protected firmware to prevent unauthorised maintenance. Absolutely agree. The point of my post was more why a fry machine might have easier (and hence cheaper) maintenance than an ice cream machine. reply 7thaccount 19 hours agorootparentprevNegative. Yes, others break down, but at a MUCH lower rate. It's a scam. reply cogman10 17 hours agorootparentOh sorry, I wasn't trying to say it wasn't a scam. Just explaining why ice cream machines would be harder to maintain/install than something like a grill and why there might be limited options on who to go with. reply 7thaccount 17 hours agorootparentAh gotcha and totally agree. reply NotYourLawyer 14 hours agorootparentprevAll of the things you just mentioned break down regularly. A restaurant is a hot, dirty, greasy environment. reply ehutch79 19 hours agoparentprevShort answer; the franchisee pays for the repairs, and allegedly McD's corporate gets a kickback. Supposedly, wendy's, burgerking, etc, have a model that is identical except for the opaque error system. Info from the internet at large, so big grain of salt. reply jerf 20 hours agoparentprevThis isn't an answer, so much as a sketch of my guess at answer, but check out \"path dependence\": https://en.wikipedia.org/wiki/Path_dependence If they go down the right path right at the beginning, a bit of payoff in the right places in McDonalds (which, remember, can be just taking the right people out to a really nice dinner every so often), and it could be that one equipment supplier managed to go down a path that no other current supplier could ever hope to replicate because it really depends on some early decisions, and those suppliers no longer have any opportunity to make \"early\" decisions. reply mherkender 19 hours agoparentprevGiven the poor technical choices made regularly in old corporations, I'm guessing they had no idea what they got themselves into. That or it's their franchisee's problem and they don't have any incentive to fix it. reply karaterobot 19 hours agorootparentI think of McDonald's as being pretty savvy, having dominated for decades in a competitive domain with large volume and low margins. In violation of Hanlon's Razor, I'd bet the reason is more malice rather than stupidity. Or rather, the specific form of malice more precisely called short-sighted greed. reply bombcar 18 hours agorootparentMcDonald's probably has paperwork that shows \"ice cream machine broke\" doesn't really cost them anything in sales or return visits, so why work on fixing it? If it was a critical product, they'd have multiple machines per store, just like the friers (often way more than is needed). reply toomuchtodo 19 hours agorootparentprev> That or it's their franchisee's problem and they don't have any incentive to fix it. https://news.ycombinator.com/item?id=29326999 reply anonymouse008 17 hours agoprevThat's curious - we use the Taylor machines with almost '0 downtime' besides maintenance (scheduled and rarely interferes with customers). The tech who installed our machines says it's all a skill issue with anyone who can't keep these machines up. According to him, the issues are rare / negligent maintenance and very old product left entirely too long in the machine. He will have one from our machines, but never from some other establishments. reply jolux 15 hours agoparentThe specific one used at McDonald's, the C602? https://en.wikipedia.org/wiki/McDonald%27s_ice_cream_machine The C602 is supposedly exclusive to McDonald's, which if you watch the video posted elsewhere in this thread, explains why their machines are less reliable than those of other chains. reply anonymouse008 10 hours agorootparentYou’re right there, we have different ones but same principles of milk and sugar beverages (ratio strongly matters in operation / performance) reply rkagerer 15 hours agoparentprevOut of curiosity, who's \"we\"? reply sirspacey 13 hours agorootparentI suspect Taylor’s social media listening company. This looks pretty similar to the boilerplate response they put on Reddit, where they get torn to shreds by McD managers who are Reddit users and disagree. Doubt we’ll see that response here. reply DANmode 2 hours agorootparentOkay, good to be speculative in situations such as this, but after skimming that user's page for even 3 seconds, one can see this was a lazy, really off-base accusation - unless this user sold their HN account. Is that the accusation? If so, better to email the site admins, rather than alert the fraudster, or distact from discussion. reply anonymouse008 12 hours agorootparentprevLols no // caffeine dealers reply bell-cot 19 hours agoprevBetter Idea: A Lemon Law for commercial equipment. Doesn't matter if Taylor uses copyright, DMCA, patents, contract law, replace-every-10-hours proprietary filters, brittle plastic gears, or what - companies that inflict unreliable & high-maintenance crap on the economy need to suffer sustained fiscal agony. reply pwg 19 hours agoparent> Better Idea: A Lemon Law for commercial equipment. Even better idea: Amend the DMCA to permanently add an exemption for bypassing DRM for repair purposes. Of course it likely won't happen because those most impacted by the adjustment of the statute would buy a sufficient number of congress critters to block passage of the amendment. reply madeofpalk 18 hours agorootparentThat is actually what the FTC and DOJ is proposing here, when reading their actual ask rather than blogspam poorly rehashing it. > This proceeding concerns possible temporary exemptions to the DMCA’s prohibition against circumvention of technological protection measures (“TPMs”) that control access to copyrighted works, including functional software that facilitates the repair and monitoring of consumer and industrial product McDonalds soft serve are mentioned only as an example of what would benefit > As described by Public Knowledge and iFixit, four “index” examples of commercial and industrial device categories would benefit from the proposed expansion: commercial soft serve machines, proprietary diagnostic kits, programmable logic controllers, and enterprise IT > The proposed expansion would benefit competition in areas beyond those outlined by Public Knowledge and iFixit. For example, increasingly sophisticated agricultural equipment often employs onboard computers that control error identification and repair, limiting options for farmers in need of quick repairs. https://www.ftc.gov/system/files/ftc_gov/pdf/ATR-FTC-JointCo... reply mikepurvis 16 hours agorootparentNice to see PLCs called out here— they're ground zero for traditional factory automation, and are typically the easy route to anything that needs to be low volume and programmable while also being capital-s Safe. But anything smarter than a tape-following AGV also has a computer on it running a bunch of high level perception and control software, so there's a natural friction point there, particularly if the PLC can only be flashed from proprietary Windows-based software. reply bsza 18 hours agorootparentprevIt will probably cost you a lot of money to figure out how to bypass the DRM though, assuming that’s even possible at all. That’s money you should never have to pay in the first place. reply donmcronald 17 hours agorootparent> It will probably cost you a lot of money to figure out how to bypass the DRM though This is the reason I think we've been \"allowed\" to get some right to repair laws passed. The legislators are a decade behind and, now that tech is good enough to make bypassing impossible, the lobbyists are letting the government pass legislation that gives you the right to do something that's technically impossible. The legislators are like a younger sibling playing with an unplugged Nintendo controller. And, even if someone can figure out a bypass, the barrier to getting there is so high that they'll most likely charge a huge premium too. reply teeray 17 hours agorootparentprevSounds like we also need a DRM FOIA. reply extraduder_ire 12 hours agorootparentprevTo what extent though? If you remove the blu-ray drive from a ps3, it will not play any games, even digital ones. The only way to fix it is to install a noBD custom firmware or use custom firmware and recovery tools to \"remarry\" another drive to the console. I don't think either of those things should be illegal, but they currently are. Having an exception may result in designs changing to make such repairs easier, however. reply Simulacra 18 hours agorootparentprevBe thankful for your government, it's the best that money can buy. reply stouset 17 hours agoparentprev> brittle plastic gears FWIW, in most cases this is done intentionally not as a cost-saving measure but to improve repairability. It's cheaper and easier to have a 5¢ nylon gear fail when subjected to out-of-spec loads than to have catastrophic failure of multiple structural parts that happen to be the weakest link in the kinetic chain from the motor. This does also cause them to fail through regular use, but having to do minor maintenance is a fundamental aspect of operating machines that are designed to be repaired and not simply thrown away. reply Atotalnoob 17 hours agorootparentDo all of them need to be plastic, or can just one? Typically what I have seen is one gear is plastic and the rest are metal reply mikepurvis 17 hours agorootparentAlso depends what the larger policy is around that one designed-to-fail part. Is it a toolless swap, with the store manager having a baggy of them in a cabinet somewhere? Or is even the sacrificial component only able to be swapped out by a special technician who must be specially summoned with some amount of cost and lead-time? reply simcop2387 16 hours agorootparentprevThat'll depend a lot on the plastics and forces involved. Sometimes metal against plastic like that will increase the wear of the plastic parts because of the differences in hardness of the materials. reply adolph 16 hours agorootparentprev> when subjected to out-of-spec loads A clutch similar to a drill could be used to protect a drive chain. reply nickff 14 hours agorootparentClutches are much larger, more expensive, and liable to fail themselves if not regularly actuated. Most commercial equipment is quite well-designed by thoughtful people; a non-expert is unlikely to come up with a better design at a moment's thought. reply kube-system 19 hours agoparentprevThat doesn't really make any sense in the context of a franchise. AFAIK McDonalds franchises are required to buy those specific Taylor machines. reply bell-cot 19 hours agorootparentWhy doesn't it make sense? Taylor makes crap, Taylor sells crap to \"Smallville Burgers, LLC\", Taylor pays agonizing penalties & fines. Doesn't matter if the victim bought the Taylor equipment because McD's contract required it, or because their daughter is a Swiftie. reply kube-system 19 hours agorootparentTaylor doesn't think there's anything wrong with their machine, the maintenance is just onerous. The result of applying any Lemon Law here would be that Taylor buys the machine back. Leaving the franchisee without an ice cream machine, and potentially out of a franchise. Given that an ice cream machine costs thousands, and MCD franchises cost millions, it is a no brainer to just deal with the machine. reply TulliusCicero 19 hours agorootparentIf Taylor just buys everything back, they no longer have a business, no? Wouldn't that be a good incentive structure for change? reply kube-system 19 hours agorootparentTaylor wouldn't be buying anything back because no franchisee wants to torpedo their investment over an ice cream machine. reply bell-cot 18 hours agorootparentAssume a loophole-free Commercial Equipment Lemon Law would disallow any \"Oops, sorry, your contract with McD. requires you to suffer while we profit!\" crap. Or perhaps McD. would also be liable for agonizing fines? reply kube-system 18 hours agorootparentThen you're not talking about a lemon law, you're talking about something else. I think what you actually want are laws that regulate franchise agreements. That's the actual problem here. reply JumpCrisscross 19 hours agorootparentprev> If Taylor just buys everything back, they no longer have a business, no? No. They have a monopoly. If they’re forced to buy them back, they’ll incorporate that cost into the next round that the franchises are required to purchase. reply bell-cot 18 hours agorootparentIf Taylor buy $X machines back for $X plus agonizing financial penalties, then figures they'll recover the losses selling $2X machines...$4X machines - then the SEC will be expecting \"Taylor may not be a viable or sustainable business\" disclosures in Taylor's next quarterly filing. If there is one - major creditors and investors could move first. reply JumpCrisscross 18 hours agorootparent> then figures they'll recover the losses selling $2X machines...$4X machines Yes, this is what having a captive market means. The problem is the franchise agreement. Shoehorning this into a lemon-law framework doesn’t work. reply indymike 17 hours agorootparentprevForcing a buyback of defective equipment does not preclude the purchase of a new, non-defective unit. Without the lemon law, the buyer would toss the old unit out without compensation and buy a new one. This way, the buyer will get money back from the defective unit and be able to get a new unit with it. reply wtallis 17 hours agorootparentThe McDonald's franchise agreement is what precludes the purchase of a non-defective unit. reply indymike 17 hours agorootparentSounds like Taylor will be set up to loose a lot of money if they don't fix the lemon. Even if it's just swapping out a lemon for a new lemon, Taylor will have an incentive to fix this. reply kube-system 16 hours agorootparentThere's a lot of miscommunication on this topic. These \"broken\" machines aren't actually \"broken\". They require extensive cleaning cycles and some operations require manufacturer servicing. It's just they way they work, it's documented, and there are service contracts in place to do the servicing. When the kid at the drive through says the ice cream machine is \"broken\", what is actually meant is that it is currently not producing ice cream, and they don't want to argue with the person who wants ice cream. But the reason isn't that it is malfunctioning. Taylor isn't losing money \"fixing\" these machines, the maintenance they do on them is on contract, and is quite lucrative. reply bsza 17 hours agorootparentprev> Leaving the franchisee without an ice cream machine, and potentially out of a franchise Their ice cream machines are broken so often that it’s already become a meme. They might as well not have them at this point. reply kube-system 17 hours agorootparentRight, we know they're broken all of the time. However, the franchisees must have the machines to comply with their franchise agreement which grants them the ability to sell any McDonalds food. reply devNoise 18 hours agorootparentprevThe machine it sells to McDonald franchisee are crap. They make other ice cream machines for other franchises and they don't seem to have the same repair issues. reply 7thaccount 19 hours agorootparentprevIt does because McDonald's isn't the one that suffers. It's the franchise owners having revenue extracted by them from the ice cream company and McDonald's corporate. Taylor makes perfectly good machines for other restaurants iirc. reply bombcar 18 hours agorootparentFrom what I recall (and I may be wrong) the McDs versions are \"idiot proofed\" in ways that the other restaurant machines aren't. The complaint is you can't de-idiot proof it, even if you know what you're doing (and some McDs franchises are owned by people who run other restaurants with the other machines). It's quite possible that McDs requires these idiot-proof ones because the risk of \"someone got sick from the ice cream gonorreah machine\" is too high. reply ransom1538 18 hours agorootparentThis. They breakdown on purpose if not cleaned. Being dairy and all, I think it's a good plan. reply kube-system 19 hours agorootparentprevA Lemon Law wouldn't let franchisees use different machines. A Lemon Law would (might) require Taylor to buy them back, which wouldn't be desirable for a franchisee. reply bsder 17 hours agorootparentprevThe McDonald's machines ARE unique and supposedly superior. They can sterilize the feed liquid without removing it from the machine. Nominally, this is far more idiot proof than other machines which are dependent upon underpaid fast food staff cleaning the machine correctly every single night. The problem is that Taylor didn't allocate enough development funding because the sales volumes are low and consequently they produced an unreliable machine. Even worse, Taylor gets to double dip by being the only people allowed to repair the unreliable machine. McDonald's only cares in that the machine doesn't give customers food poisoning--for them, mission accomplished. Taylor only sees positive cashflow from repair and consequently has zero incentive to improve the machine--mission accomplished. The fact that the franchisees get shafted isn't on the radar. reply Reason077 17 hours agorootparentprev> ”McDonalds franchises are required to buy those specific Taylor machines.” This is the weird part. Why doesn’t McDonald’s step in and certify an additional equipment supplier so that franchisees have a choice? This would put competitive pressure on Taylor to improve their machines, since franchisees are likely to avoid equipment that is unreliable or has high maintenance costs. McDonald’s customers would be happier too. It reflects poorly on the McDonald’s brand that there is this constant guessing game (or having to check in the app) of whether shakes and flurries etc are available at any given time/location. reply entropicdrifter 17 hours agorootparentMcDonald's corporate doesn't give a shit about the health of their franchises, they're getting kickbacks every time they renew their contract with Taylor. Taylor makes non-garbage ice cream machines that work just fine at, e.g. Wendy's. McDonald's just has the brand-recognition and clout to get away with a shitty corrupt practice for the moment, so that's what they're doing. For money. reply raverbashing 17 hours agorootparentprevBecause the only thing McD cares about is getting theirs \"Ice Cream machine out of operation because of technical issue?\" Not their problem reply HeyLaughingBoy 18 hours agorootparentprevThe problem exists outside the arena of franchisor/franchisee relationships. The downthread idea of a DMCA exemption for repair reasons is a very good one. reply kube-system 18 hours agorootparentYeah, a DMCA exemption for repair is a great idea. It has absolutely nothing to do with lemon law, however. reply nightfly 18 hours agoparentprev> brittle plastic gears That fail protecting the rest of the mechanisms reply mattmaroon 18 hours agoparentprevTaylor isn’t selling lemons. Their machines are among the best. They simply (like any soft serve machine) can’t withstand improper use and they’re being run mainly by teenagers who don’t give a shit. The lack of serviceability is the problem here. McDonald’s has for a few years been allowing franchisees to use Carpigiani (the other big brand in commercial ice cream machines) but they’re not better machines and while they are much more open in regards to servicing, parts must come from Italy and wait times are longer. Most of the franchisees are still opting for Taylor even though they have a choice. reply ToucanLoucan 18 hours agorootparentThe average age of a McDonalds employee is 23. https://www.zippia.com/mcdonalds-crew-member-jobs/demographi... I'd also like to add my own experience with jobs that tell me constantly to go faster and faster: that you have to do everything so fast requires a certain level of durability in your equipment and tools. If you are moving fast, you have less leeway to be careful with things. reply Kerrick 17 hours agorootparentThe average age of a workplace being run by one 60-year-old manager employing five 16-year-olds is also 23. reply ToucanLoucan 17 hours agorootparentThen maybe they should hire fewer 16 year olds. reply nickff 13 hours agorootparentHaving a job while in one's teenage years can be a huge benefit to many people, in many ways. Learning the basics of how to participate in a workplace, experiencing self-worth as a result of providing others with things they want, and the opportunity to build a resume which will come in useful. It can be hard to understand these things if one is from a wealthy background where teenage years are filled with extra-curricular activities in anticipation of 'higher education'. reply ToucanLoucan 13 hours agorootparentI grew up the son of a single mother who has been working more or less constantly in some capacity since the age of 15, so kindly take your presumptive comment and stick it somewhere anatomically inappropriate. I'm not commenting on whether or not 16 year olds should have jobs: I'm saying it's pretty rich to (according to the comments I replied to anyway, the actual demographics are much more varied) hire a bunch of children to work at your restaurant, and then complain that they break things. Like, I dunno man, maybe if your business needs to hire so much labor that is JUST over the line of legally allowed to work (or as recent news shows, isn't) and pay them as little as you can legally do so in order to function, maybe your business sucks or, more likely, you suck at running your business and you should close it. reply margalabargala 17 hours agorootparentprevIf the mean age is 23, then the median age is likely much lower. reply mattmaroon 17 hours agorootparentprevOk 23 isn’t that far from teenager and I agree employers are at least partially (maybe largely) at fault, but that just backs up that Taylor is not selling lemons. reply dimitrios1 19 hours agoparentprevBest Idea: companies that inflict unreliable & high-maintenance crap on the market naturally go out of business because people freely choose to no longer purchase their unreliable and high-maintenance crap. reply robocat 18 hours agorootparentPerfectly spherical economic thinking. How many times do you see friends buying unreliable rubbish? I was talking with an acquaintance the other day and they were predictably complaining about the high maintenance costs of their VW. Another friend was spending huge amounts every year on their very good looking Jeep. Modern goods are fractally complex: individuals can't be expected to become experts in all the disciplines required to judge the quality of a good. Even specialists can't be expected to judge correctly. Government interference in purchase decisions is a big hammer. It is hard to imagine policies that would actually help. Plenty of the \"solutions\" suggested here cause other harms or are gameable or otherwise unworkable. reply Y_Y 18 hours agorootparentprevSo you're proposing to change the people. I can't imagine that's easier than changing the law, even if I agree with you that the free market should solve the problem. reply dimitrios1 18 hours agorootparentFrom what I understand (could be wrong): McDonalds forces their franchises to buy these Taylor ice cream machines, and despite the high cost and frequency oof repairs, McDonalds franchisees continue to own and operate McDonalds meaning that despite the troubles, it is still worth it. If the ice cream machine caused a big of an issue, McDonalds would see less franchisees, and discern from their research that the machine was an issue, and change brands. Again, it starts with the customer. If people really are that pissed off McDonalds ice cream machines are seemingly always broken, then they should stop going to McDonalds, and then the feedback loop will start from there. It's not like there isn't ample other options (Wendys, Burger King, etc). That tells me that this \"problem\" is really overblown, and while McDonalds customers really would prefer to have the option to reliably order ice cream, it does not deter them significantly enough from consuming McDonalds, and doing something like simply purchasing their ice cream from a different location. If McDonalds wants to recoup that income, then they will, again, prioritize that initiative, and work on improving the ice cream machine, but again, that juice does not seem worth the squeeze. reply sahila 18 hours agorootparentYou’re making it out to be far simpler than reality. McDonald’s is more than just an ice cream place, they sell Big Macs, fries, a clean place with wifi, and cheap coffee. It would have to be the sum of those worse off than a competitor. A bad ice cream machine could just mean sub optimal profits / experience. reply dimitrios1 18 hours agorootparentI don't know many, if any, fast food places in America that don't have wifi, coffee, fries, and try to keep their restaurant clean (Calling McDonalds clean is debatable). Taco Bell? Even they have Nacho Fries and coffee. reply sahila 13 hours agorootparentIs your belief what makes a McDonald's a McDonald's just its ice cream machine, an d that's its unique selling point compared to its peers? reply xkcd-sucks 15 hours agorootparentprevFor every \"free market\" there is an equal and opposite \"can remain irrational longer than you can wait\" it seems reply CatWChainsaw 13 hours agoparentprevWhat if we went full medieval with our disincentives and used physical agony instead? Anyone who benefited from this scheme gets bathed in McDonalds coffee, right in their overheated kettles? reply ncr100 19 hours agoprevIs the DMCA worth keeping? There have been many anti-consumer news stories citing the DMCA as the linch pin. reply anonymousab 18 hours agoparentThere have been initiatives and proposals from various legislators and interested parties to revamp the DMCA. Without fail, they all seem to universally make things worse for the rights of the consumer and small creators while vastly increasing corporate power. A few token concessions to normal people that inevitably have large loopholes or wording that make them actually meaningless. So it may be the case that, while it is really shitty in a lot of ways, there is currently no way to open up that Pandora's box and change the DMCA without things getting much worse. The incentives and influences in place for legislators today are simply too perverse. reply kube-system 19 hours agoparentprevMost websites with user submitted content would not be able to exist without DMCA safe harbor provisions. The status quo before the whole \"DMCA takedown request\" process became a thing was that websites were liable for any user submitted content that was in violation. If the DMCA was repealed today, and tomorrow someone posted copyright content here on HN, then HN would be liable for copyright infringement. A repeal of the DMCA would essentially be a death sentence for social media. reply zer00eyz 19 hours agorootparent>> A repeal of the DMCA would essentially be a death sentence for social media. How is this a bad thing? If we leave an exemption for search I would consider this a positive. reply kube-system 19 hours agorootparentDid you like posting that comment? Would you like to continue posting comments on the internet? You probably wouldn't be doing so without Safe Harbor. reply zer00eyz 19 hours agorootparentDo you like that you dont own your content any more? Do you like that it gets sold to open AI to make the next ML power draw? We have the compute power and the networking to decentralize all of it. That means ownership and with that comes personal responsibility. It's not a trivial problem but it's a solvable one today. reply kube-system 19 hours agorootparent> Do you like that you dont own your content any more? I don't know what you're talking about, I hold a copyright to all of my content, (except the content I produce at work) even if I decide to license a bunch of it. > Do you like that it gets sold to open AI to make the next ML power draw? What about the DMCA enabled that? > We have the compute power and the networking to decentralize all of it. That means ownership and with that comes personal responsibility. It's not a trivial problem but it's a solvable one today. I think people still want to have conversations using services on the internet. Even email isn't decentralized enough that it would alleviate concerns about third party content. To decentralize enough so that third-party content isn't a legal concern without DMCA, you'd essentially be asking everyone in the world to run federated services with a maximum server size of one user. I don't think we have the technology to make that actually work for the average user. reply zer00eyz 18 hours agorootparent>> What about the DMCA enabled that? NO safe harbor means... no reddit, no Facebook, no YouTube... content aggregation in gated monoliths goes away. Data aggregation, data brokering gets a lot harder. And where are you going to mass license content at that point? > run federated services with a maximum server size of one user. Family exist, friends exist. Killing the DMCA, then the sea of copyright law suits is a path to copyright reform. reply kube-system 18 hours agorootparentI guess? I personally still like the idea of a public internet. reply zer00eyz 18 hours agorootparentAs do I, but the corporate internet is sort of gross... reply ta1243 17 hours agorootparentprevWe had usenet for 2 decades before the dmca reply kube-system 16 hours agorootparent...because the internet was still tiny and wasn't getting any legal attention. The passage of the DMCA was a response to the situation that was going on once it started to get industry attention. reply thrtythreeforty 11 hours agoparentprevParts of the DMCA are alright, but I think section 1201 should be outright repealed. Everything it intends to cover, can already be covered under either \"copyright infringement\" or \"theft of service.\" On top of that, I'd like to see certain forms copyright infringement for the sake of repairing/maintaining equipment, be made legal (so you can legally hack a tractor to get it to run, without relying on a fair use defense). Of course this is a much more complex discussion than just repealing 1201. reply LordKeren 19 hours agoparentprevIt could definitely use a strong revision / overhaul. But it is also the reason many sites can exist without being sued in to the ground over hosting copyright infringing material. DRM itself seems like the most pointed issue with the DMCA, but any change that reduces the efficacy or DRM is going to be fought by every large company in America reply dexwiz 17 hours agoprevThe whole Taylor repair scenario reeks of some sort of corporate kickback scheme. Taylor makes a deal with corporate that forces franchises to buy a subpar machine. Every time the machine goes down Taylor gets a repair order and corporate extracts power by potentially revoking the franchise if the owner seeks other means to repair it. Talking to McDonalds workers one main complaint was that every item on the menu required a special machine. McDonald’s more closely resembles a factory than a kitchen. reply gamepsys 17 hours agoparent> every item on the menu required a special machine * One of McDonald's well known qualities is how consistent their products are. This is largely because every McDonald uses the same ingredients, the same equipment, and the same process. * Assembly line production is a key part of how McDonald's maintains speed. Not surprised to see that the equipment resembles factory equipment. * All of this is highly visible to the customer. You see inside the kitchen when you place an order inside the restaurant. reply shuntress 19 hours agoprevIs there some reason I'm not understanding why McDonald's can't or won't just change vendors? Why is there no competition in the commercial-grade ice cream machine market? reply AaronM 19 hours agoparentMcdonalds the corporation benefits from the argeement while the Mcdonalds franchises suffer from the same agreement. reply shuntress 19 hours agorootparentAh, so this is just a backdoor fee in the franchise agreement? You (the franchisee) must use this specific machine that breaks all the time and for repairs to that machine you must use this specific vendor with whom we (the franchisor) have a \"special\" deal. reply MostlyStable 17 hours agorootparentwhich is part of why I'm so confused by the scheme. Sure, as other commenters have pointed out, it's the franchisees that are bearing this cost and not the \"corporation\" (which is allegedly getting a kickback), but I have to imagine that, at the margin and in the long term, the reputation that McDonalds has of never having working ice cream machines causes some harm (if caused zero harm, then the flip side is that there is zero value to them having the machines/product), so in the long term there are fewer franchises and therefore less money to the corporation. Rather than this complicated kick back scheme....why not just charge the franchisees more money? It's Mcdonald's gets the same amount of money, the franchisees pay the same amount of money, but you have ice cream machines with a greater uptime and therefore a better brand image which leads to greater long term growth. It just seems stupid and short sighted. I realize that humans do stupid and short sighted things all the time, but this one just seems really obvious. This almost feels like it has to be a cause of corruption where the the corporation isn't actually getting something but rather some executive got a direct kickback in order to slip this in, even though it's in the best interests of neither the franchisees nor the corporation. reply TheGRS 15 hours agorootparentI'm with you on this part of it. I don't see what the real benefit is to McD if it hurts their brand long-term, which seems to be happening in regard to their ice cream products. We are not talking about highly sophisticated machines or repairs, its more like an organic situation that's made visible because there are so many McDonald's franchises and they all have the same temperamental machines. I would guess that the real reason is that within the bureaucracy of Mickey D's there is little incentive to get this problem fixed. Leadership either isn't aware or doesn't see the value to fix it. I'm sure some people are definitely benefitting a lot from this, but not enough people higher up are affected by it to care. reply CSSer 18 hours agorootparentprevAlmost. There’s also an alternative machine available, but it’s not a serious competitor. reply 20after4 19 hours agorootparentprevYes this exactly. reply bogwog 19 hours agoparentprev> Why is there no competition in the commercial-grade ice cream machine market? Maybe patents? That's what they're for, after all. reply avs733 19 hours agoparentprev> Why is there no competition in the commercial-grade ice cream machine market? There is plenty of competition in the commercial-grade ice cream machine market. There is no competition in the commercial-grade ice cream machine for mcdonalds franchisee market reply bee_rider 18 hours agoprevWow, the apocryphal story I thought was just that the machines were a pain to clean, so they were always “broken.” I had no idea there was a whole corporate kickback scheme behind it all. reply reilly3000 18 hours agoprevI was a bit shocked to learn about a $900M lawsuit over a Raspberry Pi device. At first I thought it was Taylor suing them out of existence, but it’s actually Kytch suing McDonald’s https://www.farm-equipment.com/blogs/6-opinions-columns/post... reply anonymousab 18 hours agoprevSince this is a case of McDonald's corporate and Taylor joining hands to exploit their franchisees, I'm not sure how this will help. If they get it have the legal right to repair the machines, it's kinda pointless if McDonald's corporate merely needs to send down an order such that all franchises must not repair their own ice cream machines, as they would for any other kickback-driven franchise requirement. reply lxe 17 hours agoprevWe should add more specific exceptions to 1201 of DMCA to prevent rampant abuse and this unrepairability nonsense. reply scarface_74 19 hours agoprevCorporate likes the status quo since they get a kickback from the manufacturer for each repair The more I read about franchises, the more I’m convinced that most people are just “buying a job” and not starting a business. https://www.mashed.com/178309/how-much-mcdonalds-franchise-o... The rate of return from starting a McDonald’s franchise is less than the average return of just throwing your money into an index fund and most owners work ungodly hours reply bombcar 18 hours agoparentThe key is they're buying a job they can buy more of later; if you get a McDs off the ground (you need about $500k in cash and assets - like land, you can borrow the rest) you will have a much easier time expanding. Most McDs franchises run a few of them. reply 20after4 19 hours agoparentprevJust being a manager in the fast food industry is a really bad job, also. Overworked and underpaid. I wouldn't want anything to do with any of it. reply whartung 19 hours agoprevThe McDs ice cream experience is just flat out the worst. It's like ice cream lottery. \"Boy, a bit of ice cream might be nice!\" And it's a total crapshoot. Drive up with no idea whether it's going to happen or not. You want to just write it off, give up, \"McDs doesn't sell ice cream\". But then you go another day, and there it is on the menu, teasing you! Lies. Deception. So aggravating. reply DANmode 2 hours agoparentYou know who doesn't lie about ice cream availability? Ice cream parlours! reply pierat 19 hours agoprevThe real answer here is REMOVING the DMCA on criminalizing reverse engineering and DRM. If its your property, you should legally speaking tell any OTHER company to rightly fuck off about restrictions on your property. Intellectual property rights (copyright, patent, trademark) should NOT have any ground in restricting property rights of things directly owned. Instead, we see shit like Iphones where they use the term \"SELL\", and then continual and ongoing restrictions by the FORMER OWNER. If a company wants to maintain control, they should RENT it, with an explicit, simple to read rental agreement. reply kstrauser 19 hours agoparentAs the owner of an iPhone, I can legally do whatever I want with it. The restrictions are on developers who sign agreements with Apple. Nothing stops me legally from jailbreaking my own phone. reply notherhacker 16 hours agorootparentNo this is false. If this law wasn't in place you could pay a professional service to jailbreak and modify your iphone for you. reply pierat 19 hours agorootparentprevWhich legally speaking, hogwash. This link covers 15 years of DMCA abuse, which you say doesn't apply to you. https://www.eff.org/pages/unintended-consequences-fifteen-ye... Your ignorance of this terrible law is not immunity of this law. Apple could as easily destroy your device with no recourse, or sue YOU personally for \"defeat of technical prevention\". reply kstrauser 18 hours agorootparentI didn't see any cases there of someone getting hassled for modding their own gear. Which one should I be looking at that backs up your contention? reply prophesi 18 hours agorootparentApple will be fighting against it every 3 years when DMCA exemptions have to be renewed. https://www.eff.org/deeplinks/2009/02/apple-says-jailbreakin... edit: after digging into this, my above statement is false as it no longer needs to be renewed every 3 years reply kstrauser 18 hours agorootparentThat's from 2009. Did they file comments claiming individual-level jailbreaking is illegal in 2012, 2015, 2018, 2021, or this year? I'm not an Apple employee or direct stockholder. I imagine my 401(k) has some in it, but I don't really know. I'm not claiming they're angels. What I am claiming is that Apple's never told me I'm not allowed to do whatever I want with the things I've bought from them. They might not support me in my ventures, and I'm sure they'd prefer I not, but I'm not aware of any case where Apple tried to legally limit a regular end customer's usage. reply prophesi 18 hours agorootparentWell, they're the ones who spearheaded the attempt to make it illegal. They're not able to raise complaints for its renewal because, well, it _wasn't_ renewed in 2012, but is now covered under the Unlocking Consumer Choice and Wireless Competition Act. Unfortunately we can't deduce the reasons why it wasn't renewed in 2012, but I imagine Apple was doing the best in their power to convince the Librarian to make that decision. reply pierat 17 hours agorootparentprevYour goalposts are moving. You might want to see to that. reply kstrauser 17 hours agorootparentHardly. I said it was legal for me to jailbreak my own phone if I want to, and have yet to see evidence to the contrary. reply prophesi 17 hours agorootparentYou asked if there were any cases of someone getting hassled for modding their own gear (and outside of phones, there are plenty of examples of that happening). If we're going back to phones, then it would have been illegal to jailbreak for a few months in 2014 after their anti-circumvention exemption expired and before the aforementioned Unlocking Consumer Choice act took effect. Though jailbreaking is still just one aspect of how DMCA is being used to restrict what you can do with the hardware you've purchased. reply the8472 18 hours agorootparentprevhttps://copyright.gov/1201/2015/comments-032715/class%2021/J... reply jmyeet 19 hours agoprevI see the expected comments about addressing symptoms here. Yes, intellectual property lock-in is bad. Confusion over why McDonald's put up with this (as in, in an idealized view of a free market, that problem would resolve itself). The problem is capitalism. And it's working as intended. IP, by definition, is just an enclosure [1]. McDonald's the corporation has an adversarial relationship with its franchisees and you see this in every franchise. The head corporation will simply try to extract the maximum possible wealth from their franchisees, even to their detriment. Profits tend to decrease over time. To keep profits growing, it inevitably leads to cutting quality to lower costs and predating on your customers, franchisees and employees. [1]: https://en.wikipedia.org/wiki/Enclosure reply criddell 19 hours agoparentThey may have an adversarial relationship with franchisees, but the McDonald's corporation owns and runs 2800 restaurants themselves. You would think that's enough self-inflicted pain that they would do something. reply shuntress 19 hours agoparentprevOk so, this is good for McDonalds actually? reply jmyeet 19 hours agorootparentIn the short-term financial sense, yes. McDonald's gave an exclusive contract for a particular ice cream machine. They then locked down those machines so only first-party repairers can fix them. It's a means of extracting wealth from the franchisees. The ice creame machine maker makes money on these service call outs so they have no incentive to make them reliable (the opposite, actually). And I'm sure McDonald's gets a piece of those service call outs. Now does this do long-term damage to the McDonald's brand? Maybe. But nobody actually cares about that. reply LordKeren 19 hours agoparentprevI’m not sure changing capitalism is a very viable solution to malfunctioning ice cream machines reply matheusmoreira 19 hours agoparentprevThe problem is not capitalism, it's a very specific part of it: intellectual property. The notion that you can own ideas is fundamentally broken and nonsensical, it's logically reducible to attempting to claim ownership over numbers. The economies and laws that arose out of it are fundamentally exploitative. Corporations put a little software on a machine and suddenly they have complete control over what you can and can't do with it, any attempt to resist is felony contempt of business model. reply jimbob45 20 hours agoprevRevoke patents for machines that aren’t demonstrably reparable. For that matter, suspend copyright for works that aren’t purchasable. reply mattashii 20 hours agoparent> For that matter, suspend copyright for works that aren’t purchasable. When do you start (or stop) counting something as \"purchasable\"? Does it mean I can copy the tapes of movies the day before their first screenings, but not the day after? Or if my local shop doesn't have a show on disk? reply fkyoureadthedoc 19 hours agorootparentYou do realize that the proposal would work much the same as any other law? First a little more rigor would go into it than an off the cuff HN comment. Second, if you find something ambiguous then go to court and find out. reply mattashii 18 hours agorootparentThat is exactly why I asked: I can't think of a definition that would be both intuitive and that would not rule out reasonable and valid sales strategies for some forms of art. reply notherhacker 16 hours agoprevThey shouldn't get an exception, the law is painful as designed and consumers have to deal with it so should they. The underlying laws should be removed. IP laws like this are unenforceable at a global level and don't help/protect the consumers. All they do is create artificial walled gardens. There is no reason that its illegal to jailbreak an iPhone, or repair a tractor, or hack your bmw seat warmer. This is going to continue to lead us to a corporate hellscape. China has a more free market in this regard until the party shows up with a gun to tell you to do something for political reasons, but meanwhile they are able to leverage a more free market. reply runlevel1 15 hours agoparentIn the US, IP law flows from this line of the Constitution: > [Congress shall have power] to promote the Progress of Science and useful Arts, by securing for limited Times to Authors and Inventors the exclusive Right to their respective Writings and Discoveries. Are laws like this really helping promote the progress of science and useful arts? Because it sure does seem like it's doing the opposite. reply kevincox 16 hours agoparentprevI completely agree. The DCMA Anti-Circumvention rules are just a hack to make it easier to enforce copyright protections. But why should we make that easier? Why do we have a law making it illegal to do something that may be used for illegal purposes? It's a dumb law, let's get rid of it. reply aow83u45oaw8u 19 hours agoprev [–] Why should McDonalds be individually saved from a shitty contract that they signed? How about we fix the crappy DMCA anti-circumvention clause that has been causing problems for tons of people since 1999? Oh, because this will provide press coverage and notoriety for somebody who's probably up for reelection soon. reply dylan604 19 hours agoparentSometimes a contract is a bad contract, but it's not known how bad at first reading. There should absolutely be a mechanism for nullifying in whole or in part contracts that are this way. Especially if the contract forces you to purchase equipment that does not work. After all, a contract is just a piece of paper than can easily be torn up. You're trying to make it some magical binding spell type of thing instead reply treyd 19 hours agorootparentThere is a mechanism, a judge can decide to unilaterally annul a contract and order the parties involve to form a new one that they mediate the negotiation and terms of. reply dylan604 18 hours agorootparentincluding if there's an arbitration clause? reply LoganDark 19 hours agorootparentprev> After all, a contract is just a piece of paper than can easily be torn up. The piece of paper just represents the contract, other forms of contracts exist (such as digital, verbal). I agree there should be an easier way to get out of shitty ones. But make it too easy and you make contracts too weak. It's a balancing act. reply connicpu 17 hours agoparentprev [–] McDonald's the corporation is not being hurt by this contract, in fact they are heavily profiting from it. Independently owned individual franchises are the ones suffering from this machine that corporate forces them to use. reply s1artibartfast 17 hours agorootparent [–] I don't understand what the public problem is here. People are free not to operate a franchise. Why should I, John Q. Public, care if some restaurant owner enters into a shitty contract with McDonald corporation or a ice cream machine vendor. Ice cream isn't exactly a human right. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Government agencies are supporting exemptions to Section 1201 of the Digital Millennium Copyright Act to facilitate the repair of commercial soft serve machines, including those in McDonald's, addressing concerns about repair difficulties and monopolies on repair services.",
      "Advocates are urging for wider exemptions for industrial and commercial repairs involving software modifications, aligning with a broader movement for right-to-repair laws and federal initiatives endorsing repair freedoms.",
      "President Biden has endorsed repair reforms through an executive order, while the FTC has acted against companies linking warranties to authorized repair services, signaling a significant push for repair rights."
    ],
    "commentSummary": [
      "Challenges exist with McDonald's exclusive ice cream machines, leading to tension between the corporation and franchisees, with potential legal and financial consequences.",
      "Topics include monopolies, repair challenges, financial impacts on franchisees, and effects on brand image.",
      "Discussions also touch on DMCA concerns, consumer rights, and intellectual property laws, revealing the intricate issues faced in the fast-food sector."
    ],
    "points": 263,
    "commentCount": 171,
    "retryCount": 0,
    "time": 1710520207
  },
  {
    "id": 39720388,
    "title": "Exploring Theoretical Computer Science at CMU",
    "originLink": "https://www.cs251.com",
    "originBody": "Great Ideas in Theoretical Computer Science Great Ideas in Theoretical Computer Science Great Ideas in Theoretical Computer Science Great Ideas in Theoretical Computer Science Great Ideas in Theoretical Computer Science Welcome to CS251 at CMU! This course is about the rigorous study of computation, which is a fundamental component of our universe, the societies we live in, the new technologies we discover, as well as the minds we use to understand these things. Therefore, having the right language and tools to study computation is important. In this course, we explore some of the central results and questions regarding the nature of computation. PART 1 Formalizing Computation MODULE 1 MODULE 1 Introduction Welcome to CS251! In this module, our main goal is to explain at a high-level what theoretical computer science is about and set the right context for the material covered in the future. In the first part of the course, we want to build up formally/mathematically, the important notions related to computation and algorithms. We start this journey here by discussing how to formally represent data and how to formally define the concept of a computational problem. Contents: Introduction Mathematical Reasoning and Proofs What Is Information MODULE 2 MODULE 2 Finite Automata The goal of this module is to introduce you to a simple (and restricted) model of computation known as deterministic finite automata (DFA). This model is interesting to study in its own right, and has very nice applications, however, our main motivation to study this model is to use it as a stepping stone towards formally defining the notion of an algorithm in its full generality. Treating deterministic finite automata as a warm-up, we would like you to get comfortable with how one formally defines a model of computation, and then proves interesting theorems related to the model. Along the way, you will start getting comfortable with using a bit more sophisticated mathematical notation than you might be used to. You will see how mathematical notation helps us express ideas and concepts accurately, succinctly and clearly. Contents: Deterministic Finite Automata 1 Deterministic Finite Automata 2 MODULE 3 MODULE 3 Formalizing Computation In this module, our main goal is to introduce the definition of a Turing machine, which is the standard mathematical model for any kind of computational device. As such, this definition is very foundational. As we discuss in lecture, the physical Church-Turing thesis asserts that any kind of physical device or phenomenon, when viewed as a computational process mapping input data to output data, can be simulated by some Turing machine. Thus, rigorously studying Turing machines does not just give us insights about what our laptops can or cannot do, but also tells us what the universe can and cannot do computationally. This module kicks things off with examples of computable problems. In the next module, we will start exploring the limitations of computation. Contents: Turing Machines Universality of Computation MODULE 4 MODULE 4 Limits of Computation In this module, we prove that most problems are undecidable, and give some explicit examples of undecidable problems. The two key techniques we use are diagonalization and reductions. These are two of the most fundamental concepts in mathematics and computer science. Contents: Limits of Computation, Part 1 Limits of Computation, Part 2 Limits of Computation, Part 3 MODULE 5 MODULE 5 Limits of Human Reasoning The late 19th to early 20th century was an important time in mathematics. With various problems arising with the usual way of doing mathematics and proving things, it became clear that there was a need to put mathematical reasoning on a secure foundation. In other words, there was a need to mathematically formalize mathematical reasoning itself. As mathematicians took on the task of formalizing mathematics, two things started to become clear. First, a complete formalization of mathematics was not going to be possible. Second, formalization of mathematics involves formalizing what we informally understand as “algorithm” or “computation”. This is because one of the defining features of mathematical reasoning is that it is a computation. In this module we will make this connection explicit and see how the language of theoretical computer science can be effectively used to answer important questions in the foundations of mathematics. Contents: Limits of Human Reasoning PART 2 Computational Complexity MODULE 6 MODULE 6 Time Complexity So far, we have formally defined what a computational/decision problem is, what an algorithm is, and saw that most (decision) problems are undecidable. We also saw some explicit and interesting examples of undecidable problems. Nevertheless, it turns out that many problems that we care about are actually decidable. So the next natural thing to study is the computational complexity of problems. If a problem is decidable, but the most efficient algorithm solving it takes vigintillion computational steps even for reasonably sized inputs, then practically speaking, that problem is still undecidable. In a sense, computational complexity is the study of practical computability. Even though computational complexity can be with respect to various resources like time, memory, randomness, and so on, we will be focusing on arguably the most important one: time complexity. In this module, we will set the right context and language to study time complexity. Contents: To be added (under construction). MODULE 7 MODULE 7 Graph Theory In the study of computational complexity of languages and computational problems, graphs play a very fundamental role. This is because an enormous number of computational problems that arise in computer science can be abstracted away as problems on graphs, which model pairwise relations between objects. This is great for various reasons. For one, this kind of abstraction removes unnecessary distractions about the problem and allows us to focus on its essence. Second, there is a huge literature on graph theory, so we can use this arsenal to better understand the computational complexity of graph problems. Applications of graphs are too many and diverse to list here, but we’ll name a few to give you an idea: communication networks, finding shortest routes in various settings, finding matchings between two sets of objects, social network analysis, kidney exchange protocols, linguistics, topology of atoms, and compiler optimization. This module introduces basic graph theoretic concepts as well as some of the fundamental graph algorithms. Contents: To be added (under construction). MODULE 8 MODULE 8 P vs NP In this module, we introduce the complexity class NP and discuss the most important open problem in computer science: the P vs NP problem. The class NP contains many natural and well-studied languages that we would love to decide in polynomial time. In particular, if we could decide the languages in NP efficiently, this would lead to amazing applications. For instance, in mathematics, proofs to theorems with reasonable length proofs would be found automatically by computers. In artificial intelligence, many machine learning tasks we struggle with would be easy to solve (like vision recognition, speech recognition, language translation and comprehension, etc). Many optimization tasks would become efficiently solvable, which would affect the economy in a major way. Another main impact would happen in privacy and security. We would say “bye” to public-key cryptography which is being used heavily on the internet today. (We will learn about public-key cryptography in a later module.) These are just a few examples; there are many more. Our goal in this module is to present the formal definition of NP, and discuss how it relates to P. We also discuss the notion of NP-completeness (which is intimately related to the question of whether NP equals P) and give several examples of NP-complete languages. Contents: To be added (under construction). MODULE 9 MODULE 9 Randomized Algorithms Randomness is an essential concept and tool in modeling and analyzing nature. Therefore, it should not be surprising that it also plays a foundational role in computer science. For many problems, solutions that make use of randomness are the simplest, most efficient and most elegant solutions. And in many settings, one can prove that randomness is absolutely required to achieve a solution. (We mention some concrete examples in lecture.) One of the primary applications of randomness to computer science is randomized algorithms. A randomized algorithm is an algorithm that has access to a randomness source like a random number generator, and a randomized algorithm is allowed to err with a very small probability of error. There are computational problems that we know how to solve efficiently using a randomized algorithms, however, we do not know how to solve those problems efficiently with a deterministic algorithm (i.e. an algorithm that does not make use of randomness). In fact, one of the most important open problems in computer science asks whether every efficient randomized algorithm has a deterministic counterpart solving the same problem. In this module, we start by reviewing probability theory, and then introduce the concept of randomized algorithms. Contents: To be added (under construction). MODULE 10 MODULE 10 Cryptography The quest for secure communication in the presence of adversaries is an ancient one. From Caesar shift to the sophisticated Enigma machines used by Germans during World War 2, there have been a variety of interesting cryptographic protocols used in history. But it wasn’t until the computer science revolution in the mid 20th century when the field of cryptography really started to flourish. In fact, it is fair to say that the study of computational complexity completely revolutionized cryptography. The key idea is to observe that any adversary would be computationally bounded just like anyone else. And we can exploit the computational hardness of certain problems to design beautiful cryptographic protocols for many different tasks. In this module, we will first review the mathematical background needed (modular arithmetic), and then present some of the fundamental cryptographic protocols to achieve secure communication. Contents: To be added (under construction). PART 3 Highlights of Theoretical Computer Science MODULE 11 MODULE 11 Extra Topics In this module, we present a selection of highlights from theoretical computer science. Contents: To be added (under construction). This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License.",
    "commentLink": "https://news.ycombinator.com/item?id=39720388",
    "commentBody": "Great ideas in theoretical computer science (cs251.com)240 points by __rito__ 16 hours agohidepastfavorite73 comments diebeforei485 13 hours agoThis class is great because it introduces a lot of concepts but most importantly really sharpens your problem-solving skills. > The method we tend to use in 251 is throwing you deep into the water without a paddle (i.e., giving you very basic definitions for a new topic you haven't seen and expecting you to solve things that you might be given in a course dedicated to that topic in say the third week). We do this over and over, pretty much starting from scratch on a new topic every week. As a result, it can be very frustrating—which, of course, is the intention. If you're constantly trying to solve things that are just outside of your reach, you develop better strategies for thinking about problems you're given. reply VoidWhisperer 6 hours agoparentI'm not sure I necessarily agree with this strategy. In my CS undergrad, I had an algorithms class where the professor did this and expected the students to be able to do homework assignments each week that were worth a significant portion of our grade with basically no explanation on how to solve these extremely complicated optimization problems. When comparing our assignments to that of another student taking the same class with a different professor, theirs were much, much easier. I dont think this teaching methodology made me gain any more from the course, if anything, it made me gain less because I constantly had the stress of trying to do these ridiculous problems on top of all of my other work every week reply cloogshicer 6 hours agoparentprevHonestly, this just sounds like a terrible way to teach. reply _zoltan_ 1 hour agorootparentI disagree. Doing my math degree I had constantly sat in classes where I had no idea what I was being taught. Doing my own research helped me understand the topic a lot better. reply fho 3 hours agorootparentprevHonestly, for a long time this was the standard way of learning in academia. Lecturers gave lectures and everything else was on the students to find out (from books). Right now we reached the other end of the pendulum swing, where everything is hand-fed to the students. reply inhumantsar 5 hours agorootparentprevthis is my favourite way to learn reply atoav 4 hours agorootparentIf the problems are interesting, why not. If they are not and they are purely abstract without purpose, I might not feel motivated at all. reply tzs 12 hours agoprevTheoretical computer science can be fun. But it can also be really annoying! Around the time most schools were starting the 2014-15 school year I saw a post on Reddit in either a math or CS group from someone asking for how to solve a particular theoretical CS problem. The poster didn't say where the problem was from or why they needed a solution, but others quickly figured out it was from someone trying to cheat on the first homework set from COMS 331 (Theory of Computing) at Iowa State University. No one helped and the poster deleted their post. I thought it was an interesting problem and gave it a try, expecting it to be a short but interesting diversion. I didn't expect it to take too long because although I was not a CS major (I was a math major) I did take all of the undergraduate theoretical courses at my school and I got decent grades in them. That had been ~35 years earlier so I had forgotten a lot but the problem was from the first COMS 331 homework set so shouldn't require anything past the first week or so of that class, which should all be fairly basic stuff I would still remember. I spent a couple days on it and got absolutely nowhere. Several times since then I've remembered it, thought about it for a few hours or a day and have continued to completely fail. If anyone is curious, here is the problem: Define a 2-coloring of {0, 1}∗ to be a function χ : {0, 1}∗ → {red, blue}. (For example, if χ(1101) = red, we say that 1101 is red in the coloring χ.) Prove: For every 2-coloring χ of {0, 1}∗ and every (infinite) binary sequence s ∈ {0, 1}∞, there is a sequence w₀,w₁,w₂,··· of strings wₙ ∈ {0, 1}∗ such that (i) s = w₀w₁w₂ ···, and (ii) w₁, w₂, w₃, · · · are all the same color. (The string w₀ may or may not be this color.) reply suyjuris 5 hours agoparentLet us say that an index i is bad, if every finite subsequence of s starting at i is red (i.e. for every j ≥ i we have χ(s_i ... s_j) = red). Two cases: Case 1: there are infinitely many bad indices. Here we go to the first bad index then the second, and so on. The colour of w₀ does not matter, and since subsequent words start at a bad index, they will all be red. Case 2: there are finitely many bad indices. Then there is some k which is larger than all bad indices. We start by going to k (again, the colour of w₀ does not matter). Since k is not bad, there must be some blue word starting at k. We take that one and move to a larger index. Again, that index is not bad. We repeat this process to find our sequence. reply akoboldfrying 2 hours agorootparentNice proof, similar to the one I posted just now but simpler -- you realised that we only need one special category (\"bad\" rather than \"hard-red\" + \"hard-blue\"). Gonna leave mine up though :) reply georgecmu 7 hours agoparentprevI don't feel like doing a formal proof, but it looks like it should be a direct consequence of the pumping lemma: \"Informally, it says that all sufficiently long strings in a regular language may be pumped—that is, have a middle section of the string repeated an arbitrary number of times—to produce a new string that is also part of the language.\" [1] Needless to say, this exercise would be trivial if you just covered the pumping lemma and its applications in class, and next to impossible if you never heard of it. [1] https://en.wikipedia.org/wiki/Pumping_lemma_for_regular_lang... PS. I took 15-251 back when it was 15-299: a brand new class without a regular number assignment. Honestly, I would have enjoyed it a lot more now than I did back then. But several assignments still stand out for me, in particular \"Building from scratch\" [2]. Trying to get some of that feeling now, working through Turing Complete[3] with my daughter. [2] https://www.cs.cmu.edu/afs/cs/academic/class/15299/handouts/... [3] https://turingcomplete.game/ reply jhanschoo 7 hours agorootparentThe problem is that 2-colorable is equivalent not to a regular language but an arbitrary language. reply akoboldfrying 3 hours agoparentprevThe following proof is much tighter than the meandering process I followed to get to it. Let me know if the meandering would be interesting! Call a position i in s \"hard-red\" (respectively, \"hard-blue\") if every positive-length substring of s beginning at i is red (respectively, blue); otherwise call i \"soft\". A position is \"hard\" if it is hard-red or hard-blue. There are 3 possible cases: 1. s has no hard positions. 2. s has a positive but finite number of hard positions, with the last being i. 3. s has an infinite number of hard positions. Case 1 Every position is soft, meaning that if we start a substring of s at that position and continue to grow it by appending digits from s, eventually (after a finite number of steps) the colour of the substring will change. So we can set w₀ arbitrarily to the first digit of s, then grow w₁ from position 2 of s until it has the same colour as w₀. Then we can grow w₂ from the next digit in s until it has the same colour, and so on. This results in all words having the same colour, including the first. Case 2 Set w₀ to the first i-1 digits of s, and w₁ to the i-th digit of s. All positions > i are soft, meaning that, as for case 1, we can repeatedly grow substrings by appending digits from s until the substring turns the same colour as w₁. Case 3 Since s has an infinite number of hard positions, it must have an infinite number of hard-red positions, an infinite number of hard-blue positions, or both. Suppose w.l.o.g. that it has an infinite number of hard-red positions (it may or may not also have an infinite number of hard-blue positions). Define p(k) to be the k-th hard-red position in s. Set w₀ to the first p(1)-1 digits of s, and for k >= 1 set word w_k to the substring of s beginning at p(k) and ending at p(k+1)-1. w₁, w₂, w₃, ... all begin at hard-red positions, so are all red. □ reply danbruc 9 hours agoparentprevMy first idea is to think of this as the following game. Initially we consider all words uncolored and I start by picking an initial set of red words that can color any infinite binary sequence in red. Then you get to take away one of my words and make it blue. After that I get to pick replacement words from the remaining uncolored words to fix my set. Rinse and repeat. I start with { 0, 1 } which will obviously do. You take - I guess without loss of generality - my 0. I pick 00 and 01 as replacements and end up with { 00, 01, 1 } which will work again. You take away my 01, I replace it with 010 and 011 and { 00, 010, 011, 1 } will still work, I guess, but it is certainly becoming less obvious. In general I will pick x0 and x1 as replacements if you take x away which will allow me to still color x, I just have to choose between x0 and x1 depending on the following bit. If I am not mistaken, you will have to take away an infinite set of words from me in order to prevent me from being able to color any infinite binary sequence in red, if you take only finitely many, I will be left with a set of red words that still works. My guess is now that if you take away that infinite set in order to stop me, you will accidentally assemble a set of blue words that will be able to color any infinite binary sequence blue. Unfortunately I do not have the time right now to properly think this through. I am also not too confident because I do not clearly see how being allowed to have the first word in the wrong color comes into play. On the other hand, maybe, maybe you need my 1 for sequences starting with one or something like that. EDIT: Just had an additional thought, this might actually work. If you take my 0, then you can not also take my 1 later on or you will end up with { 0, 1 }. If you take 01, then you can not also take 10 later. So you always have to take one of my two latest replacements and leave the other one to me forever. We will build complementary sets, you have 0, I have 1, you have 01, I have 10, you maybe 011, I 010, ... Now it seems quite plausible that this will work out and we end up with two complementary set that both can color all infinite binary sequences in [mostly] a single color. reply scubbo 12 hours agoparentprevInteresting problem! I've never formally studied computer science, and my math degree is far behind me, so I probably won't be the one to help you solve this, but I'm commenting to at least remind myself to check back here. Some thoughts that immediately spring to mind (apologies if you've already thought of these - just getting the brain flowing!): * We only need consider colorings for which 0 and 1 are differently coloured. If 0 and 1 are the same colour, then we're trivially done - let w1, w2, w3, etc. all have length 1 * this smells like it'll be some sort of combination of induction and contradiction? E.g. \"assume this property is true for w1w2w3...wn-1, and that wlog they are red. Assume there is no wn that can continue the sequence of red substrings - thus, all strings starting from the end of wn are blue. If that's the case _and_ if we can convert the string [w1...wn-1] into blue substrings, then we're done. So show that there is some contradiction proving that impossible (maybe using the observation I made above that we only care about cases where 1 and 0 are different colours)\" reply owlbite 12 hours agorootparentI suspect that if you have a sequence w_0, ... w_n with the property but all possible remaining words in your sequence following w_n are the wrong color, then you can use set w_0' = concat(w_0, ... w_n) and declare victory. reply ironSkillet 12 hours agoparentprevThe cardinality of the set if possible infinite binaries sequences is uncountable. My gut tells me there must be a way to assume the negation and construct a bijection between the naturals, leading to a contradiction. reply melagonster 7 hours agoparentprevI know that I have no chance in real computer science department. but is this question for freshman? reply rramadass 3 hours agorootparent> I know that I have no chance in real computer science department. Don't say/believe that and limit yourself. You just need to find the right books and slowly educate yourself (never mind what others say). I have spent a lot of time collecting and reading books much of which i still don't \"grok fully\" but what i do understand is intellectually very stimulating and gives me an edge over the competition (when needed in the industry). reply melagonster 1 hour agorootparentthanks for your friendly response haha. reply sudosysgen 7 hours agoparentprevCan this be solved with a proof by contradiction? Let's assume that this is impossible for our sequence. There must then be at least one (sub)word of the wrong color. For this to be fatal, it must be impossible for any word containing it to be of the correct color while having the rest of the sequence (after and before that word) continue to be the same color, no matter how large we make this word to the right, because if this wasn't true, we could then simply use this bigger word and there would be a contradiction. If this is true for more than one word, the problem can be solved by simply inverting the chosen color and using two larger words containing both subwords, which will always be of the same, originally wrong color, which leads to a contradiction. Otherwise, if this is only true for one subword, and therefore the initial sequence and the rest of the sequence around cannot be made to be worded correctly while the subword is contained in a word of the correct color, but that the rest of the sequence can be covered with words of the same color, we can simply include this problematic subword inside w₀ In either of the three cases 0, 1 or 2+ \"toxic subwords\", it is always possible to find words of the same color covering the sequence. Therefore, there can be no sequence for which it is impossible to find a suitable w₀w₁w₂ ··, and the original proposition is proven. Please tell me if you find any issue with this approach! reply rramadass 14 hours agoprevNice! If folks would like to learn these ideas by hand via programming, i highly recommend Tom Stuart's Understanding Computation From Simple Machines to Impossible Programs - https://computationbook.com/ reply NlightNFotis 14 hours agoparentMy favourite computing book. Very highly recommended. reply 3abiton 14 hours agorootparentDo you have any other favorite recommendations? reply NlightNFotis 12 hours agorootparentYeah, I’ve also enjoyed greatly the following: * Computer Science, an interdisciplinary perspective by Robert Sedgewick * Code by Charles Petzold * Good Math by Mark C. Chu-Carroll reply __rito__ 8 hours agoparentprevWow, thanks for sharing! reply rramadass 3 hours agorootparentYou might also like Algorithmics The Spirit of Computing by the great David Harel (https://en.wikipedia.org/wiki/David_Harel). The book is accessible to the intelligent general reader (though not a popular science book) and covers its domain rather comprehensively. reply sporkland 1 hour agoprevInspired by the title my two top ideas from theoretical compsci: 1. Move to front lists are at most 2x the optimal list order search time (and are often much better than any static list order). [1]. A similar conjecture exists for Splay trees but is unproven called the dynamic optimality conjecture. 2. Randomized algorithms (e.g quicksort) often have the same worst case time as a non-randomized one but often are much faster in practice than their non-randomized variants. 1: https://www.dgp.toronto.edu/public_user/JamesStewart/378note... reply soganess 15 hours agoprevI don't know where, but there is another version of this class that includes \"The Probabilistic Method\"[1]. While not exactly the same thing, I can't imagine doing modern existential (vs constructive) proofs of topological solution space obstructions without that style of thinking. [1]: https://math.bme.hu/~gabor/oktatas/SztoM/AlonSpencer.ProbMet... Bonus points for those interested -- I think this paper(sadly, not mine) has a concise collection / history of obstruction proofs in its background section (1.2/1.3 and since it is a paper all of them are cited!): https://arxiv.org/abs/2309.09913 reply pushedx 14 hours agoparentYou may be referring to the version of this course that was taught by Luis von Ahn (the creator of reCAPTCHA and the founder of Duo Lingo), when he was a professor at CMU. https://www.cs.cmu.edu/afs/cs.cmu.edu/academic/class/15251-s... He liked to call the course \"SOME Theoretical Ideas FOR Computer Science\", and it was known to be a very popular (and difficult) course. reply soganess 14 hours agorootparentI didn't know LVA taught at CMU, rad! I did some digging and found the class: https://people.eecs.berkeley.edu/~venkatg/teaching/15252-sp2... I enjoy that it is called \"More Great Ideas in Theoretical Computer Science\", like a sequel (...and 'some' being the prequel). I'm now eagerly awaiting the 'return of the jedi' installment. reply pushedx 14 hours agorootparentAh, great! Seems like there was a tradition of having fun with the course title. reply dogcomplex 13 hours agoparentprevDamn. Impressed to see something this fundamental that never made it into my computer science postgrad education (and judging by the depths and difficulty of it, might not for a while!) Seems like a powerful technique that scales to cover quite a lot of other problems. reply Karrot_Kream 12 hours agorootparentWow I'm really surprised this didn't enter your post grad! We covered this briefly in undergrad but had a much meatier grad level course on fundamentals of computational with this included and it destroyed pretty much every student in the class lol. Our professor ended the final at 4 hours to \"spare you the misery\". reply __rito__ 15 hours agoprevA more complete version: https://youtube.com/playlist?list=PLm3J0oaFux3aafQm568blS9bl... reply riedel 12 hours agoparentThanks. To me the first part linked above is rather \"foundation of computer science\". I remember this to be content of my first classes in CS ( turing machines, lambda calculus, finite automata, ... (Actually the course in 1999 in Uni Karlsruhe was unfortunately also was the last time I implemented things using Haskell). This full list I guess goes beyond and looks interesting. reply projectileboy 13 hours agoprevIf you dig this you might like https://cs121.boazbarak.org/, which also has a link to Barak’s textbook, available as a PDF. reply Simon_ORourke 5 hours agoprevAhhh, I remember my old college course on time complexity, which was effectively the professor picking on random kids and saying something like - \"I have some 4 dimensional matrix multiplication algorithm which then uses a bloom filters to match on random ids... what is the time complexity of this?\", and some poor kid would stutter something like, \"aaaaahhhh.... O(nlogn)\", only to have the professor to shout \"No, wrong!\" and go pick on someone else. reply abdullahkhalids 16 hours agoprevWhat would versions of this course look like for other fields? Great Ideas in Theoretical Physics? Great Ideas in Experimental Physics? Great Ideas in Economics? etc I did teach a course once called, Inventing the Information Age, in which we discussed all the inventions and ideas (starting from writing) all the way to modern computing infrastructure needed for a civilization to replicate our information age. This was not a single-field course, because the ideas/inventions were in language, physics, mathematics, and computer science. That made it more fun. reply josh-sematic 14 hours agoparentSean Carrol has a series he’s working on called “The Biggest Ideas in the Universe” which is on physics. Only the first one is out now (Time, Space, and Motion), but the second is due in May I think (on Quanta and Fields). Its intended audience is anyone with a high school education. It introduces basic ideas in calculus and then does use equations to motivate understanding, but not at the level you’d need if you were actually studying physics. I have a bachelor’s in physics but still deepened my understanding of General Relativity from the first one. Highly looking forward to part 2. https://www.preposterousuniverse.com/biggestideas/ reply osti 15 hours agoparentprevThe CMU class is for freshmen, so students without advanced background can still comprehend most of it. But for physics I feel that without at least some good physics and math background, you can't really understand and appreciate the \"great ideas\". reply abdullahkhalids 14 hours agorootparentIf the students know the core concepts and mechanics of Calculus and Linear Algebra, which is entirely possible for high school students in certain parts of the world, it's possible to teach a lot of core physics ideas with simple models. In fact, in my undergrad, the Physics lab and theory courses were flipped - you look the lab course before the theory course. In the lab course you used simple maths and simple experimental setups to probe phenomena in mechanics, thermodynamics, electromagnetism, optics etc. Doing experiments/observations in astronomy, quantum mechanics are also possible at that level. reply trenchgun 5 hours agoparentprevYour course sounds awesome. Do you have the materials somewhere available? Would love to read more about it. reply shanusmagnus 12 hours agoparentprevThat sounds awesome and very creative. Do you have a link to an old class site, or a syllabus, or anything that would give a flavor? reply brcmthrowaway 12 hours agoprevComputation is a fundamental propery of the universe? How is that? How do flora and fauna compute? reply rramadass 3 hours agoparentYou might find your answers in; 1) The Nature of Code by Daniel Shiffman - https://natureofcode.com/ 2) The Computational Beauty of Nature by Gary William Flake - https://mitpress.mit.edu/9780262561273/the-computational-bea... reply ozim 12 hours agoparentprevYou have to have right amounts of inputs to get outputs. Plants I grow are complex function of CO2, water, energy and bunch of other things things. But I do know if I don’t water enough or too much, plant function will return not good results. reply woopsn 8 hours agoparentprevWith ribosomes, hormones, spores, senses, brains etc., and by processing our genetic \"code\" into these things. reply graycat 11 hours agoparentprevUh, let's take an example: Mars gets a force from the gravity of the earth. That force has magnitude and direction so is a vector. Mars also gets such a vector from each of the Sun, Jupiter, Andromeda, dark matter, .... Then to determine the direction Mars will go, have to add all those vectors with the current value of the Mars momentum vector. Soooo. the universe adds vectors, that is, does computation. Done? reply bo1024 14 hours agoprevInteresting positioning as \"great ideas\" -- it seems like the topics are a pretty standard undergrad Theory of Computer Science course. reply shusaku 6 hours agoparentThat’s my impression too, maybe the name just gets undergrads more excited. reply lupire 13 hours agoparentprevI wouldn't expect the theory of a field to be built upon Bad Ideas! It is a strange course though. It is grad level (2xx at Harvard) but the syllabus is survey topics from a range of undergrad courses in CS. I would guess it's a smattering of advanced topics in those areas, but the description doesn't sell it as having all those topics as prereqs. Maybes it's meant as an extension school / professional master's class for people who do not have CS degrees? Refugees from math, physics, and autodidacts? reply yankjenets 13 hours agorootparentIt's a second semester freshman course for CMU CS undergrads. reply pvg 15 hours agoprevA previous thread from 2017 https://news.ycombinator.com/item?id=15146905 reply Simon_ORourke 5 hours agoprev> Garbage collection This is a huge Yes now. Every language that's become successful since 1999 has GC and is designed for all normal users to use it to manage all memory. In five years, Rust or D might make that last sentence untrue, but even if that happens, GC will still be in the yes category. Sorry, I had to guffaw when I read that last part about D. A dumpster fire of a language for garbage collection. reply Avtomatk 12 hours agoprevAnd where is Category Theory and Lambda Calculus? reply bsdpufferfish 9 hours agoparentlambda calculus is one way to describe computable functions. this course chose to focus on Turing machines. category theory is historically not important to theory of computation, and it has questionable use if you aren't familiar with abstract algebra reply assbuttbuttass 13 hours agoprevI TA'd for this course back in undergrad. Despite being in the CS department, it's structured like a math class, and all of the homework assignments are proof-based. I don't think I wrote code for that course a single time, except for pseudo-code for a Turing machine reply a-dub 8 hours agoprev\"string theory\" i found this stuff to be a lot of fun! reply djaouen 15 hours agoprevI have nothing to add other than saying that Physicists and Philosophers often make better programmers than Computer Scientists. :) reply bo1024 14 hours agoparentNot really on the topic of this link at all. But I wouldn't be surprised, since you're likely comparing the few physicists and philosophers who switch fields against the average computer scientist. reply DrDroop 14 hours agoparentprevSometimes you find the truth at the bottom of HN. It hurts, I spend my early adulthood trying to learning most of the topics on this page thinking it would make me a great software engineer. These are good ideas, but I wouldn't call them great, they are often very dull and don't make one a great thinker. reply thejazzman 16 hours agoprevThe most painful class I ever took. I know, I'm not a real nerd, or something. I'm okay with it. reply russfink 15 hours agoparentInteresting to read your experiences. Looking over this, it seems to take a highly theoretical approach and work up, rather than a practical/applied approach and work down. I was surprised to see automata covered so early, which is more in the vein of computability theory than that of introductory concepts. reply leetrout 15 hours agoparentprevThere are literally dozens of us making great careers solving problems with computers without CS degrees or any particular interest in the lowest levels of computing nor the mathematical backings / abstractions. Horses for courses. reply rramadass 14 hours agoparentprevYou might find this motivating : https://news.ycombinator.com/item?id=39721301 reply __rito__ 15 hours agoparentprevRelevant: https://news.ycombinator.com/item?id=15153838 reply zitterbewegung 15 hours agoprevI think there should be a companion class on anti patterns or bad ideas in computer science. It’s much harder to see a horrible idea and then you have to argue why not to do something. Something like variable names being too long or short. How to figure out what to do with unrealistic timelines. reply freework 11 hours agoprev [–] \"Theoretical computer science\" is garbage. Of all the scientific fields, computer science is the one that is mostly closely related to fields where theory is most worthless. If you have a computer science theory, then write some damn code to prove it. Then share your code. Others can then run it and replicate those results. At the \"theory\" phase, the idea is worthless. reply bollu 11 hours agoparentYou are very wrong. Many of the key results of theoretical CS is to prove impossibility. There is no code to be written when you show that something is not possible. - Halting problem: impossibility of TM to determine if other TMs halt. - crypto: impossibility for a Turing machine to break a cryposystem in polytime - sorting lower bounds: impossibility to sort objects given only a less_than operator on them in time less than O(n log n) and so on. There is no code to be written for these, because they are mathematical theorems. > computer science is about computers as much as astronomy is about telescopes ~Dikjstra. reply rramadass 3 hours agoparentprevYou are either trolling or are absolutely clueless. However in the event that you really want to learn this via programming see this book - https://news.ycombinator.com/item?id=39721301 reply ChrisClark 11 hours agoparentprev [–] Wow, just... wow. Theoretical computer science is literally more solid and real than any code you could write. It's literally the mathematical foundation of how your favorite language works. It is all precise mathematical proofs, not just 'thoughts'. reply hnfong 4 hours agorootparent [–] What are mathematical proofs but mere 'thoughts'? (I'm only being half rhetorical. I've been thinking about this deeply lately.) reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "CS251 at CMU provides a course on theoretical computer science, emphasizing the formal examination of computation, including areas like finite automata, Turing machines, computational complexity, and cryptography.",
      "Students learn to formalize computation, explore graph theory, understand the P vs. NP problem, delve into randomized algorithms, and grasp the basics of computation limits.",
      "The course equips students with the necessary language and tools to comprehend computation's essence and its diverse applications across different disciplines."
    ],
    "commentSummary": [
      "CS251, a theoretical computer science class, enhances problem-solving skills through weekly exposure to new concepts and challenges.",
      "Recent cheating incident at Iowa State University's related course involved demonstrating properties of binary sequences, emphasizing understanding set theory and problem-solving strategies with colorings and game analogies.",
      "The significance of theoretical computer science lies in proving impossibilities, drawing parallels with physics and economics, underscoring the essence of foundational mathematics in comprehending complex computational concepts."
    ],
    "points": 240,
    "commentCount": 73,
    "retryCount": 0,
    "time": 1710534645
  },
  {
    "id": 39723704,
    "title": "Brain Waves Flush Out Waste During Sleep in Mice",
    "originLink": "https://jamanetwork.com/journals/jama/fullarticle/2816616",
    "originBody": "[Skip to Navigation] Our website uses cookies to enhance your experience. By continuing to use our site, or clicking \"Continue,\" you are agreeing to our Cookie PolicyContinue JAMA HomeNew OnlineCurrent IssueFor Authors JOURNALS JAMA JAMA Network Open JAMA Cardiology JAMA Dermatology JAMA Health Forum JAMA Internal Medicine JAMA Neurology JAMA Oncology JAMA Ophthalmology JAMA Otolaryngology–Head & Neck Surgery JAMA Pediatrics JAMA Psychiatry JAMA Surgery Archives of Neurology & Psychiatry (1919-1959) PODCASTS Clinical Reviews Editors' Summary Medical News Author Interviews More JN Learning / CMESubscribeJobsInstitutions / LibrariansReprints & Permissions Terms of UsePrivacy PolicyAccessibility Statement 2024 American Medical Association. All Rights Reserved Search All JAMA JAMA Network Open JAMA Cardiology JAMA Dermatology JAMA Forum Archive JAMA Health Forum JAMA Internal Medicine JAMA Neurology JAMA Oncology JAMA Ophthalmology JAMA Otolaryngology–Head & Neck Surgery JAMA Pediatrics JAMA Psychiatry JAMA Surgery Archives of Neurology & Psychiatry Sign In Individual Sign In Sign inCreate an Account Access through your institution Sign In Purchase Options: Subscribe to the JAMA journal Featured Clinical Reviews Screening for Atrial Fibrillation: US Preventive Services Task Force Recommendation Statement JAMA Recommendation Statement January 25, 2022 Evaluating the Patient With a Pulmonary Nodule: A Review JAMA Review January 18, 2022 Select Your Interests Customize your JAMA Network experience by selecting one or more topics from the list below. Academic Medicine Acid Base, Electrolytes, Fluids Allergy and Clinical Immunology American Indian or Alaska Natives Anesthesiology Anticoagulation Art and Images in Psychiatry Artificial Intelligence Assisted Reproduction Bleeding and Transfusion Cardiology Caring for the Critically Ill Patient Challenges in Clinical Electrocardiography Climate and Health Climate Change Clinical Challenge Clinical Decision Support Clinical Implications of Basic Neuroscience Clinical Pharmacy and Pharmacology Complementary and Alternative Medicine Consensus Statements Coronavirus (COVID-19) Critical Care Medicine Cultural Competency Dental Medicine Dermatology Diabetes and Endocrinology Diagnostic Test Interpretation Drug Development Electronic Health Records Emergency Medicine End of Life, Hospice, Palliative Care Environmental Health Equity, Diversity, and Inclusion Ethics Facial Plastic Surgery Gastroenterology and Hepatology Genetics and Genomics Genomics and Precision Health Geriatrics Global Health Guide to Statistics and Methods Guidelines Hair Disorders Health Care Delivery Models Health Care Economics, Insurance, Payment Health Care Quality Health Care Reform Health Care Safety Health Care Workforce Health Disparities Health Inequities Health Policy Health Systems Science Hematology History of Medicine Humanities Hypertension Images in Neurology Implementation Science Infectious Diseases Innovations in Health Care Delivery JAMA Infographic Law and Medicine Leading Change Less is More LGBTQIA Medicine Lifestyle Behaviors Medical Coding Medical Devices and Equipment Medical Education Medical Education and Training Medical Journals and Publishing Melanoma Mobile Health and Telemedicine Narrative Medicine Nephrology Neurology Neuroscience and Psychiatry Notable Notes Nursing Nutrition Nutrition, Obesity, Exercise Obesity Obstetrics and Gynecology Occupational Health Oncology Ophthalmology Orthopedics Otolaryngology Pain Medicine Palliative Care Pathology and Laboratory Medicine Patient Care Patient Information Pediatrics Performance Improvement Performance Measures Perioperative Care and Consultation Pharmacoeconomics Pharmacoepidemiology Pharmacogenetics Pharmacy and Clinical Pharmacology Physical Medicine and Rehabilitation Physical Therapy Physician Leadership Poetry Population Health Primary Care Professional Well-being Professionalism Psychiatry and Behavioral Health Public Health Pulmonary Medicine Radiology Regulatory Agencies Reproductive Health Research, Methods, Statistics Resuscitation Rheumatology Risk Management Scientific Discovery and the Future of Medicine Shared Decision Making and Communication Sleep Medicine Sports Medicine Stem Cell Transplantation Substance Use and Addiction Medicine Surgery Surgical Innovation Surgical Pearls Teachable Moment Technology and Finance The Art of JAMA The Arts and Medicine The Rational Clinical Examination Tobacco and e-Cigarettes Toxicology Translational Medicine Trauma and Injury Treatment Adherence Ultrasonography Urology Users' Guide to the Medical Literature Vaccination Venous Thromboembolism Veterans Health Violence Women's Health Workflow and Process Wound Care, Infection, Healing Save Preferences Privacy PolicyTerms of Use X Facebook LinkedIn More New Online Views 1,180 Citations 0 View Metrics Download PDF Share X Facebook Email LinkedIn Cite This Permissions Comment Medical News in Brief March 15, 2024 Brain Waves Appear to Wash Out Waste During Sleep Samantha Anderer Article Information JAMA. Published online March 15, 2024. doi:10.1001/jama.2024.2077 visual abstract icon Visual Abstract editorial comment icon Editorial Comment related articles icon Related Articles author interview icon Interviews multimedia icon Multimedia audio icon Listen to this article Many important cellular functions occur during sleep, including the brain’s removal of waste and toxins. However, researchers previously lacked an understanding of the mechanisms behind this complex process. Now, research involving mice recently published in Nature suggests that brain waves are responsible for the self-cleansing. Researchers observed neurons in the brains of sleeping mice. As neurons fired electrical signals, they acted as coordinated pumps, producing rhythmic waves that appeared to flush cerebrospinal fluid through brain tissue, washing out waste. After the researchers silenced certain areas of the brain to stop the waves, cerebrospinal fluid lost the ability to flow, leaving detritus in place. First author Li-Feng Jiang-Xie, PhD, of the Washington University School of Medicine, noted in a press release that this knowledge has potential implications for neurological conditions such as Alzheimer disease and Parkinson disease, in which the accumulation of metabolic waste and “junk proteins” can lead to neurodegeneration. Back to top Article Information Published Online: March 15, 2024. doi:10.1001/jama.2024.2077 Comment See More About Sleep Medicine Download PDF Cite This Citation Anderer S. Brain Waves Appear to Wash Out Waste During Sleep. JAMA. Published online March 15, 2024. doi:10.1001/jama.2024.2077 Manage citations: Ris (Zotero) EndNote BibTex Medlars ProCite RefWorks Reference Manager Mendeley © 2024 Permissions Comment Artificial Intelligence Resource Center Others Also Liked X JAMA Content Home New Online Current Issue Podcasts Coronavirus (COVID-19) Q&A Clinical Reviews Editors' Summary Medical News Author Interviews JAMAevidence: JAMA Guide to Statistics and Methods JAMAevidence: The Rational Clinical Examination JAMAevidence: Users' Guides to the Medical Literature Journal Information For Authors JAMA Editors Editors & Publishers RSS Contact Us JN Learning / CME Store Apps Jobs Institutions Reprints & Permissions Subscribe Go JAMA Network Publications JAMA JAMA Network Open JAMA Cardiology JAMA Dermatology JAMA Health Forum JAMA Internal Medicine JAMA Neurology JAMA Oncology JAMA Ophthalmology JAMA Otolaryngology–Head & Neck Surgery JAMA Pediatrics JAMA Psychiatry JAMA Surgery Archives of Neurology & Psychiatry (1919-1959) Sites Art and Images in Psychiatry Artificial Intelligence (AI) Resource Center Best of the JAMA Network Caring for the Critically Ill Patient Clinical Crosswords from JAMA Coronavirus Resource Center Evidence-Based Medicine: An Oral History Fishbein Fellowship Genomics and Precision Health Hypertension JAMA Forum Archive JAMA Network Audio JAMA Network Video JAMA Network Conferences JAMA Surgery Guide to Statistics and Methods Medical News Mpox (Monkeypox) Research Ethics Topics and Collections Visual Abstracts War and Health Women's Health Featured Articles Antiretroviral Drugs for HIV Treatment and Prevention in Adults - 2022 IAS-USA Recommendations CONSERVE 2021 Guidelines for Reporting Trials Modified for the COVID-19 Pandemic Creation and Adoption of Large Language Models in Medicine Global Burden of Cancer, 2010-2019 Global Burden of Long COVID Global Burden of Melanoma Global Burden of Skin Diseases, 1990-2017 Global Disparities in Parkinson Disease Guidelines for Reporting Outcomes in Trial Protocols: The SPIRIT-Outcomes 2022 Extension Mass Violence and the Complex Spectrum of Mental Illness and Mental Functioning Neuropsychiatry Sequelae of COVID-19 A New Framework for Dementia Nomenclature Organization and Performance of US Health Systems Pharmacy Benefit Managers: History, Business Practices, Economics, and Policy Promoting EDI in Genetics Research PTSD and Cardiovascular Disease Red Blood Cell Transfusion: 2023 AABB International Guidelines Reimagining Children’s Rights in the US Spirituality in Serious Illness and Health The US Medicaid Program: Coverage, Financing, Reforms, and Implications for Health Equity USPSTF Recommendation Statements Screening for Colorectal Cancer Screening for Hypertension Screening for Lung Cancer Screening for Prediabetes and Type 2 Diabetes In Children Screening for Prediabetes and Type 2 Diabetes In Adults Statins for Primary Prevention of Cardiovascular Disease Vitamin and Mineral Supplements for Primary Prevention of of Cardiovascular Disease and Cancer Blogs AMA Style Insider Information For Authors For Institutions & Librarians For Advertisers For Subscription Agents For Employers & Job Seekers For the Media Equity, Diversity, Inclusion Online Commenting Policy Public Access and Open Access Policy Statement on Potentially Offensive Content JAMA Network Products AMA Manual of Style JAMAevidence Peer Review Congress JN Learning Home State CME Clinical Challenge CME Atrial Fibrillation Course Women's Health Course CME / MOC Reporting Preferences About CME & MOC Help Subscriptions & Renewals Manage Emails Update My Address Support Center My Account JAMA CAREER CENTER Physician Job Listings Get the latest from JAMA Sign Up Privacy PolicyTerms of Use © 2024 American Medical Association. All rights reserved, including those for text and data mining, AI training, and similar technologies. Terms of Use| Privacy Policy| Accessibility Statement| Cookie Settings . × Access your subscriptions Sign inpersonal account Access through your institution Add or change institution Free access to newly published articles Create a free personal account To register for email alerts, access free PDF, and more Purchase access Subscribe to journal Get full journal access for 1 year Buy article Get unlimited access and a printable PDF ($40.00)— Sign in or create a free account Rent article Rent this article from DeepDyve Access your subscriptions Sign inpersonal account Access through your institution Add or change institution Free access to newly published articles Create a free personal account To register for email alerts, access free PDF, and more Purchase access Subscribe to journal Get full journal access for 1 year Buy article Get unlimited access and a printable PDF ($40.00)— Sign in or create a free account Rent article Rent this article from DeepDyve Sign in to access free PDF Sign inpersonal account Access through your institution Add or change institution Free access to newly published articles Create a free personal account To register for email alerts, access free PDF, and more Save your search Sign inpersonal account Free access to newly published articles Create a free personal account To register for email alerts, access free PDF, and more Purchase access Subscribe now Customize your interests Sign inpersonal account Free access to newly published articles Create a free personal account To register for email alerts, access free PDF, and more Create a personal account or sign in to: Register for email alerts with links to free full-text articles Access PDFs of free articles Manage your interests Save searches and receive search alerts Privacy Policy Make a comment Sign inpersonal account Free access to newly published articles Create a free personal account To register for email alerts, access free PDF, and more Create a personal account or sign in to: Register for email alerts with links to free full-text articles Access PDFs of free articles Manage your interests Save searches and receive search alerts Privacy Policy",
    "commentLink": "https://news.ycombinator.com/item?id=39723704",
    "commentBody": "Brain waves appear to wash out waste during sleep in mice (jamanetwork.com)217 points by hdevarajan 7 hours agohidepastfavorite104 comments robg 1 hour agoIf you have any doubts about sleep quantity and quality, worth reading about the glymphatic nervous system, which is so newly discovered likely you didn’t learn about it in school. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4636982/ In short it explains, mechanistically, why poor sleep affects daily cognition, mental health, and age-related declines. Robust scientific theories explain more of the evidence. The glymphatic nervous system explains why sleep is so key to surviving and thriving. Maiken Nedergaard will end up winning the Nobel for its discovery. reply achow 5 hours agoprevMore details at Washington Univ' site: https://medicine.wustl.edu/news/neurons-help-flush-waste-out... Could not find any free access to the paper, even though the Univ' says \"This work was supported by the National Institutes of Health (NIH); the BJC Investigators Program at Washington University; and the Neuroscience Innovation Foundation.\" These kind of grant funded research should be accessible at the least from the Univ' websites. reply UniverseHacker 19 minutes agoparentAll NIH funded research is freely open access by law. In this case it seems the authors didn’t pay the (crazy expensive) Nature open access fee. They will have to upload a free PDF to PubMed manually, but have up to a year to do so, and don’t seem to have done it yet. reply JPLeRouzic 3 hours agoparentprevMore discussion about this paper on ALZ forum: https://www.alzforum.org/news/research-news/do-sleep-rhythms... reply VagabundoP 4 hours agoparentprevChecked and it was published through Nature: Jiang-Xie, LF., Drieu, A., Bhasiin, K. et al. Neuronal dynamics direct cerebrospinal fluid perfusion and brain clearance. Nature 627, 157–164 (2024). https://doi.org/10.1038/s41586-024-07108-6 Every Uni should have access through your library system. reply gapan 4 hours agorootparentI think the point was that every member of the public should have access to publicly funded research at least through the university's website, rather than at least everyone currently in universities should have access. reply baubino 3 hours agorootparentI agree but I also know (as a university employee) that the reason they don’t do this is because then they would have to pay for and manage the hosting of hundreds of thousands of articles. From the university’s perspective, managing and hosting articles is exactly what publishers do, so universities cede that work to publishers. What is needed is a public database of all publicly-funded research. reply achow 3 hours agorootparentHow hard or expensive is it to embed a PDF link in the Univ news article. It maybe less harder and expensive than the stock image that they have used. Stanford maintains an extensive catalog of research papers: https://searchworks.stanford.edu/view/in00000064276 reply jimberlage 2 hours agorootparentprevAt first I was like, “oh, that’s a reasonable perspective,” and then I thought about it more and it kinda isn’t? When I buy a keyboard, the manufacturer doesn’t run a shipping company, so they use $x from the purchase price to subcontract shipping. (They tell me it’s separate at billing, but they don’t make me ring up DSL myself either.) When I hire an electrician, they buy materials from Home Depot, so they use $x from the purchase price for materials. (They sometimes break down the bill into materials and labor, but they don’t make me drive to Home Depot myself and buy every part.) When the public hires academics to do research, they have administrative overhead and have to hire a publisher, so they use $x from the grant for administrative overhead and… Abdicate responsibility for the $y needed for publishing and pretend the public didn’t intend for part of the grant money to go to that?? reply civilized 1 hour agorootparentprevHosting is a long-solved problem. High-quality repositories have provided free, universally accessible document hosting to the academic world for decades. The arXiv is a nonprofit organization run by Cornell that hosts preprints, and I wouldn't be surprised if it holds more stuff than all the for-profit publishers combined at this point. The barriers to uploading on the arXiv are very low - all you need is to be at a recognized academic institution or endorsed by someone who works for one. Resources like arXiv have made official publishing largely superfluous for spreading knowledge in the parts of academia that understand and care about the value of spreading knowledge for free. Analogous archives exist for (at least) biology, medicine, psychology, and economics. https://info.arxiv.org/about/index.html reply fragmede 3 hours agorootparentprevsome sort of scientific hub, one might say reply mock-possum 3 hours agorootparentprevSounds like a good cost for the government to subsidize. reply VagabundoP 35 minutes agorootparentprevI agree publicly funded research should not be siloed in anyway and should be in the public domain. reply samstave 3 hours agorootparentprevAaronSwartz+1* reply lloeki 3 hours agorootparentSwartz. Schwartz is an actor. reply achow 4 hours agorootparentprevYes, it was published through Nature. But what stops Univ' site to have original research paper available for download? Nature magazine is just a distribution channel (they did not sponsor the research so should not have exclusive rights on it). Left college long time ago; and do not have access to any library system. reply bayindirh 3 hours agorootparentCopyright Transfer Agreement. The moment you send the paper to Nature, you can only publish the draft you sent in. Unless you pay for publish, editorial costs are paid by selling access to that paper. If you want to publish in an Open Access manner, you need to pay the costs. Or if your country wants to license it for Open Access, your country pays the costs. reply pca006132 1 hour agorootparentIf I understand correctly, for most publishers you are allowed to host a copy on your personal web as well as the university library catalog? reply bayindirh 1 hour agorootparentAFAIK, this comes with distribution limitations like to your department and students in the university. There's an exception for arXiv generally for publishing intermediate revisions, but not the final one. Elsevier has a nice summary at [0]. At the bottom there's \"Publicly share the final published article\". It's \"YES\" for Open Access and \"NO\" for subscription (classical) model. There's a rather detailed \"Scholarly Share\" which allows sharing with researchers, but not to any researcher, as I aforementioned on top. [0]: https://www.elsevier.com/about/policies-and-standards/copyri... reply pca006132 17 minutes agorootparentNever knew this before! Thanks for sharing. reply achow 3 hours agorootparentprevThe final draft that was sent is good enough, maybe all that one needs. reply bayindirh 3 hours agorootparentSometimes yes, sometimes no. In my case, the edited versions of (my) paper had more experiments, refined images and was more complete. The paper was not bad to begin with, but the edited/revised version painted a more complete picture which shouldn't be possible without a set of external reviewers. reply SturgeonsLaw 4 hours agorootparentprevIf you email the authors they might be happy to send you a copy - scientists tend to be in favour of the free exchange of information, there's no restriction on them sharing their paper if they choose to, and they don't get royalties from journal subscriptions so they have no vested interest in paywalling their research reply bayindirh 3 hours agorootparentNot all of them. I sent a couple of such mails, to only get back the freely available summary PDFs back. Even with the same checksum with the ones on the web. reply cj 2 hours agorootparentYep. My dog was part of a study at UC Davis. All I was able to get was an abstract. reply 0xEF 1 hour agorootparentprevTrue, but I'd say it's still worth the email asking. I used to work in the mental health field requested paywalled research papers on cognitive and developmental disabilities pretty regularly, and about 80% of the people I contacted sent the paper to me without much question. The more we all do it, the more expected the practice becomes and with any luck, these money-grubbing journals will take note and start offering copies at a minimal fee or time-limited access. reply bayindirh 1 hour agorootparentOf course. I'm a big proponent of Open Access. OTOH, Elsevier seems to give 50 days of Open Access via share links. I didn't know that. I was away from academic arena for some time. Looks like I'll be returning soon-ish. reply tombert 15 minutes agoprevI was diagnosed with sleep apnea about six months ago. Once I started getting treatment for it (losing weight, cutting out caffeine, and an oral appliance) my life started getting dramatically better. I had never taken sleep very seriously prior to that and I feel extremely foolish for it. Life is 100x better if you get an appropriate amount of sleep. Everything becomes a little easier. reply greggsy 5 hours agoprevIs anyone more knowledgeable able to interpret the paper beyond ‘brain waves wash toxins’? Are they actually responsible for the observed activity, or do they merely trigger some ‘flushing’ mechanism inherent to the cells? Or, are the waves a result of that process? reply swores 5 hours agoparentI can't find free access to the full paper, but here's the abstract (afraid I'm not knowledgeable enough to add to or explain any of it): \"The accumulation of metabolic waste is a leading cause of numerous neurological disorders, yet we still have only limited knowledge of how the brain performs self-cleansing. Here we demonstrate that neural networks synchronize individual action potentials to create large-amplitude, rhythmic and self-perpetuating ionic waves in the interstitial fluid of the brain. These waves are a plausible mechanism to explain the correlated potentiation of the glymphatic flow1,2 through the brain parenchyma. Chemogenetic flattening of these high-energy ionic waves largely impeded cerebrospinal fluid infiltration into and clearance of molecules from the brain parenchyma. Notably, synthesized waves generated through transcranial optogenetic stimulation substantially potentiated cerebrospinal fluid-to-interstitial fluid perfusion. Our study demonstrates that neurons serve as master organizers for brain clearance. This fundamental principle introduces a new theoretical framework for the functioning of macroscopic brain waves.\" https://pubmed.ncbi.nlm.nih.gov/38418877/ reply SturgeonsLaw 3 hours agorootparent> Here we demonstrate that neural networks synchronize individual action potentials to create large-amplitude, rhythmic and self-perpetuating ionic waves in the interstitial fluid of the brain This line is the money shot. An action potential is the variable electrical charge of a neuron, and they maintain that charge by containing a certain concentration of ions relative to the surrounding cerebrospinal fluid. This paper proposes that neurons synchonise their charge state, which forces ions to flow in or out of the neurons in bulk, the movement of these ions causing the cerebrospinal fluid to move around, clearing out the accumulated debris. reply niemandhier 1 hour agorootparentI’d expect that it’s probably not the ion movement but the global electric field, maybe even mechanical effects in the axons. If you look at the Goldstein-Katz equations you see that the conductivities play an as important role as the concentrations. Most of the voltage change is driven by changes in conductivities and not concentration changes, the ion movements across the membrane should be negligible. Off course you have the free ions outside the cells than diffuse in a field gradient. reply salawat 4 hours agorootparentprev>Chemogenetic flattening of these high-energy ionic waves largely impeded cerebrospinal fluid infiltration into and clearance of molecules from the brain parenchyma. Sounds like unsync'd brain waves, (being awake) impedes cerebrospinal fluid flow through the brain. When asleep, neurons sync their action potentials, acting like a ionized fluid pump, moving old dirty cerebrospinal fluid out, and allowing new, clean in. Interestingly, when they synthesize that type of ionization using other means, they saw the same increase in fluid movement. This is my best attempt at translating. Hopefully within 1-2 football fields. The author leaves choice of American or the rest of the world to the reader to decide. reply Staple_Diet 58 minutes agorootparentAs someone in this field (research field not football field) I'd say that is an excellent layman explanation. reply samstave 3 hours agorootparentprevI wonder if this state could be ultrasonically induced to have a \"sleep helmet\" which can calm and align rhythmic waves? and induce an artificial flush or \"power nap\" -- Well, thats basically the trope of every Cybernetic design, with the connectors comming out the back of head/spine... Like a continual wash which keeps you in this suspended hyper-aware, but calm, rested and focused mental state. This is why cyborgs have such incredible reflexes. reply 0xFEE1DEAD 1 hour agorootparentHave you seen the video on hn a couple of weeks ago about the scientist using ultrasound to halt the progression of Alzheimer’s and to treat addiction? Most fascinating video I’ve seen in a long time. Edit: If you haven’t seen it yet, I highly recommend checking it out https://youtu.be/7BGtVJ3lBdE reply eschatology 3 hours agorootparentprevI was thinking if we could directly tap and pump fresh cerebrospinal fluid and eliminate sleep reply MrDrDr 5 hours agoparentprevThe paper shows a correlation between the observed ‘brain waves’ and the ‘toxin removal’. From the abstract: ‘These waves are a plausible mechanism to explain the correlated potentiation of the glymphatic flow through the brain parenchyma’. I think the main objective of this paper is to justify more funding to explore this phenomenon, to establish causation (beyond correlation). reply malux85 5 hours agoparentprevWouldn’t it be cool if we could periodically trigger waves like this to refresh the brain and maybe require less (or no) sleep! I have no expertise in this area I’m just a dreamer :) reply loopz 4 hours agorootparentThat's been in use for thousands of years already: yoga, pranayama and meditation. There are courses one can take where one learns this, like Art of Living and any other that follows the same traditions. Yes, there's research on that, and new studies should absolutely gain from this study. Not entirely sure you'll observe the same effect, that depends on meditator, but you can fall asleep during practice. I have over a decade experience with it, and have also participated on a study on breathing exercises and epigenetic effects from that versus blind control. reply mattlondon 3 hours agorootparentI think the point of replacing sleep is to save time. Replacing hours of sleep with hours of yogo or mediation seems like you are not gaining anything. reply frereubu 2 hours agorootparentI think the point would be if you could replace ~8 hours of sleep with ~1-2 hours of meditation. I very much doubt it's a like-for-like replacement, but it might go some way to reducing the need for full sleep, e.g. 2 hours meditating + 2 hours sleeping = 8 hours sleeping. reply hahajk 1 hour agorootparentIs the idea that yoga/meditation are more efficient than sleep at redistributing the cerebral fluid? That would be very surprising. reply ArthurAardvark 3 hours agorootparentprevIt's hilarious to me that these practices are laughed off, trivialized through memes and hyper-objectivity (science can't evaluate their value in a practical manner -> \"Lulz hippie-dippie nonsense for ditzy/ungrounded women!!!!\" -- Western dolt). Its insane considering their benefits...that are of course tragically perfect for those who would never participate Of course that's a huge generality, I'd say it may be 1 out of 5 people like that in the US, but its the fact that it is not constrained to any particular demographic/background. reply dotnet00 2 hours agorootparentI've seen actual psychiatrists suggesting meditation or yoga as additional ways to help with certain issues (eg anxiety) along with medicine, and the benefits of say, just conscious, controlled breathing as is involved in both are obvious to anyone who has a temper. I wouldn't be surprised if the idea that it's completely useless because it can't be scientifically observed in a traditional sense is highly correlated with people who think that mental health issues aren't real in general. reply zadler 3 hours agorootparentprevWhich exercises would you say trigger movement of cerebrospinal dluid in the brain? reply DonHopkins 3 hours agorootparenthttps://www.youtube.com/watch?v=PM5_dgKDsrc reply vasco 5 hours agorootparentprevFor companies it'd be a dream. 1/4 more awake time is 1/4 more time to buy stuff and watch content and ads. Or work longer hours. Gonna go back to sleep now. reply Dylan16807 3 hours agorootparentSounds like a win-win. Let's say my discretionary spending is 30% of my income, it goes up by a third, and I need to work 10% longer hours. Okay, so I adjust to a 44 hour work week and I get an extra 30+ hours of free time? Great! reply kyleyeats 4 hours agorootparentprevDefinitely get some sleep. Your post suggests you're getting less than five hours a night. reply samstave 3 hours agorootparentprevthe immediate use doesnt have to buy more shit. it could be used in endurance/focused events. high stress relief, traumatic brain injury - like immediate induce afte cte event in foot ball head trauma in car accident ODs etc... - When I was a baby, and I would get ill, my mom would calm me and put me to sleep - and I would sleep for an extended period and my mom would say that I would wake up fully well. If this could be induced in infants going through trauma's such as a surgery, where inducing natural brain-flush instead of pumping a tiny body full of \"medicine\" this might be great for their long term mental development outside of having medicines in them when growing extremely fast. reply d1sxeyes 5 hours agorootparentprevDreamers have no problems here :) reply barkingcat 5 hours agorootparentprevthe trigger could be like a sequence of flashes or certain pulsing of sound frequencies, it'd be like hacking the nervous system via unsanitized inputs reply foota 5 hours agorootparentHave you read Snow Crash? reply gitaarik 5 hours agorootparentprevMeditation? reply huppeldepup 5 hours agoprevThere is an mri greyscale clip that shows this process I saw a couple of years ago but could never find it back. It showed spinal fluid “washing” the brain. If anyone knows where I can find it, please link. reply speedylight 5 hours agoparentThere’s this one I just found - [https://player.vimeo.com/video/370150539?background=1](https... reply huppeldepup 4 hours agorootparentThat one is the default google answer, but not the one I saw. Mine had a spine attached and there was no highlighting, the fluid movements were distinguishable in the scans. Thank you for trying. reply xeonmc 4 hours agorootparentthis? https://www.youtube.com/watch?v=r8cgZn3aW3o or this? https://www.youtube.com/watch?v=zPEOviBqr_c EDIT: also https://www.facebook.com/watch/?v=785729915751081 reply pkoird 2 hours agoprevKinda tangential but I've noticed that my most satisfying and restorative sleep is always followed by tons of eye goop the following morning. Maybe a kind of brain-waste? reply Hendrikto 1 hour agoparentThis has probably way more to do with how much REM sleep you get, which is said to be the most restorative sleep. reply Havoc 50 minutes agoprevI thought this was the prevailing theory anyway? Is this just confirming that? reply ineedaj0b 5 hours agoprevWe need to figure out what activity or thought creates waves most similar to the cleaning function. Maybe it’s dreaming? Dreaming by my experience is tedious and not restful… I have 4-6 dreams every night and never feel like I’m truly asleep. Shouldn’t be hard to figure out with an EKG and a decent graph to see what your aiming for. reply pjerem 4 hours agoparentIt’s not dreaming that is tedious. My understanding is that the brain always dream but that you only remember your dreams if you wake up during them. So, remembering 4-6 dreams would mean that you wake up (probably unconsciously) multiple times per night. It’s probably what is tiring rather than the dream itself. Disclaimer : I have no source and I may be wrong. It’s my understanding. reply ineedaj0b 3 hours agorootparentI don’t feel physically tired in the mornings. I ran in high school 8-10 miles a day and would collapse, waking up dream free. Now I wake up refreshed but I’m sick of watching the same rehashed combinations of dreams. I can do lucid dreaming, but I’d just rather not dream at all reply deprecative 23 minutes agorootparentYou always dream. That's the thing. Unless something is seriously wrong you dream every night. You might not want to remember them but you'll dream all the same. reply drowsspa 2 hours agoparentprevI almost never remember my dreams. When I do, it means I had a really bad night's sleep and I don't feel rested. 4 to 6 dreams every night sounds exhausting. You should probably check it out with a doctor, it seems you're subconsciously waking up too often during the night. reply bloqs 3 hours agoparentprevModafinil does exactly this. reply smolder 3 hours agorootparentHuh? It cleans the brain? Care to elaborate or provide a source? reply ineedaj0b 3 hours agorootparentIt certainly keeps you alert. Removes the ‘sleepy/tired’ feeling. But it might not clean the brain. It might change the comfort level of ‘clean’ you brain finds acceptable before commanding the janitor out of his closet. reply dbtc 4 hours agoparentprevmeditation reply v7engine 4 hours agoprevcould someone eli5 this for me pls. reply samstave 3 hours agoparentThink of it as whilst awake all the experience and sensory input, nutrients(or lack) you experience throughout the day result in basically the brain doing all its activites and various parts, functions and syncopated waves/rthyms etc get out of phase. Much like you occasionally reboot your machine to flush and re-align memory etc... When you sleep, your sensory input systems are handed a surrogate (dreams) whist the rest of your brain gets back into phase with itself, and the idea is that this allows for an 'alignment' of sorts within the brain which allows for the cerberal fluids to more porously flow through the brain and carry away ionic and other molecules which freely float through the brain. As the rain washes the paths and the streets after a windy, dusty day... To allow for a fresh path where the previous wake-cycles experiences in molocules can be more properly absorbed into the brain and your neurons can 'take in' what happened all wake cycle. reply theaussiestew 5 hours agoprevThat's a really interesting scientific finding. From a personal experience, once I closed my eyes and actually observed this phenomenon. I could observe many spherical waves of light each pulsating, and not only that but I could feel the effect of each wave viscerally. It definitely felt healing. I wasn't sure what this was, was it the vibes of meditators around the world? Maybe this research finding is what I was observing. reply quesera 46 minutes agoparentReflexively, that doesn't make sense. I cannot imagine perceiving ionic CSF flow with my eyes. But thinking about it more, maybe there is something to it? I'm wholly unqualified to speculate here, but I know computers and computers are exactly the same as everything in the real world, so let's go! ... - When restful and with my eyes closed, I definitely \"see\" waves move across my field of vision. My eyes see \"black\" (or dark gray) with slightly lighter edges that move and act like a wavefront in a fluid. There's distinct flow, and (minor) swirling and interference. I've seen these since I was very young, and I have no idea if it's a common thing or if it has a name. It must. I've mentioned it to a very small number of other non-experts but no one has ever recognized it. (I also sometimes see tiny colored tiles that light up in moving masses -- again rarely mentioned but never recognized, probably an unrelated phenomenon...) - I do not think these are simply phosphenes, because there is no external proximate stimuli. Though I suppose we all live in environments saturated with EMF, most of the time, and a proper test would require being in a very remote area (and that's probably not enough). Though if I can \"see\" the presence of EMF, I'm totally making some phone calls to Charles Xavier's people. - I've theorized that this was fluid in my eye (eyelid is too thin) moving around and refracting what little light is getting through in a dark room. But it also happens when I am completely still (though nothing in the body is ever completely still), and it also happens in complete darkness. - The \"shape\" of the waves does not resemble anything that could be related to blood vessels in the eyelid. And again, complete darkness. - So, is it possible that it's actually this ionic flow of CSF through the vision-sensing part of the brain? Not actually light coming through my eyeballs at all, but fluidlike electrical variations in the parts of the brain itself which are sensitive to the electrical signals normally coming from the optic nerves? I have no idea. But it's an fun new angle to consider in the idle moments while I watch the waves flow before I fall asleep. (That said, please feel free to correct my wild speculation if there's an obvious explanation that has not intersected my completely-unrelated fields of study! :) reply dynisor 4 hours agoparentprevAre you perhaps describing phosphenes? reply russellbeattie 5 hours agoprevSo, if we could induce those brain waves we wouldn't need sleep? Maybe in the future our AirPods will have a \"rejuvenate\" feature which plays tones that cause sleep like brain waves to go through and clean out our neurons. Probably not... I think our brains are like LLMs. We train them on sensory input we perceive over time, building up a data model that is unique to us. The models may generate similar results, but each person's brains are wired as differently as the raw binary data stored in ChatGPT vs Llama or Gemini. Thus the cleaning mechanism is probably unique per person as well. But I don't really have a clue. It's just a logical inference. reply rpozarickij 2 hours agoparentI'd think that the brain isn't the only part of the human body that needs sleep/rest. Becoming immobile for such a long amount of time must have a much larger systemic purpose given that this is how our bodies evolved. reply onlyrealcuzzo 1 hour agorootparentYour spine contracts during the day and releases while you sleep at night. reply Jolter 3 hours agoparentprevI have an idea: if you induced those brain waves, you would technically be asleep. So you would not actually be magically given more awake time. reply samstave 3 hours agoparentprevinduction may cause a feeling of restedness, but unless the actual fluid pumps out the waste, you'd likely get diminishing, perhaps even damaging results if you over use it... like and \"sleep spell\" it has a cooldown that is needed for actual biological pumping time reply luxuryballs 2 hours agoprevAfter learning this and reading about lion’s mane reversing plaque induced dementia in rats I started taking it as “brain washing detergent” before bed. It gives me some wildly vivid dreams and I always wake up more refreshed. reply slowmotiony 1 hour agoparentHow much? reply alexnewman 3 hours agoprevImagine I said this in a nice way. Haven't we known this for a while? Specifically during deep non rem sleep. For the lazy, what's new here? reply throwaway81523 5 hours agoprevIs this news? Lookup glynphatic system. I thought it was known that it flushes out waste products during sleep. reply tunnuz 5 hours agoparentSame, but the way I read it is that now we know the “how”. reply DANmode 5 hours agoparentprevOnly commenter mentioning the glymphatic system, and spelled it wrong. Gonna say it's still news! reply pulse7 5 hours agoprevSo garbage collection is not such a bad idea after all... reply lwansbrough 5 hours agoparent1/3 of compute time spent on garbage collection is pretty inefficient! reply flancian 3 hours agorootparentWhat if it's garbage collection and NN training/backprop time? Then it starts sounding like a great deal to me at least. reply pjerem 4 hours agorootparentprevWell if it gives you an uptime of 80-110 years, that’s pretty useful. reply lelanthran 4 hours agorootparentprevOnly in f it's exclusively for GC. If it's opportunistic GC, then it's simply running when nothing else is. IOW, we may need the sleep for a different reason, and that is when the load is low enough to allow the GC to run. Assume that we can trigger this while awake. This doesn't mean that we don't need to sleep for some other reason. reply steve1977 4 hours agorootparentprevMaybe it's comforting to think that the time is not only spent with garbage collection. That's one part of sleep, but by far not the only one. reply dennis_jeeves2 1 hour agorootparentprevIt didn't get the memo on big 'O'. reply DANmode 5 hours agorootparentprevHave you seen this volume of garbage?! reply aendruk 5 hours agoparentprevWhere sleeping is an acceptable cost. reply forinti 3 hours agoparentprevGC with a time window. Maybe there's a paper there. reply art3m 1 hour agoprevI read about it few years ago in Matthew Walker's book \"Why we sleep\". reply koenraad 49 minutes agoprevI immediately start to think about the effect of smartphone related waves in the room while sleeping. reply im3w1l 8 minutes agoparentBrain waves are approximately in the 1hz to 100hz range. While smartphone waves are much higher frequency. AC however is 50/60hz depending on country. Maybe I'm overly cautious, but I have a thick wire that runs right next to the natural spot for my pillow I very intentionally put the pillow on the other side of the bed. reply niemandhier 1 hour agoprev [–] Notably, synthesized waves generated through transcranial optogenetic stimulation substantially potentiated cerebrospinal fluid-to-interstitial fluid perfusion. No wonder Musk is interested in this area of research. This line indicates that with an optigenetic implant you can substitute sleep with something more efficient. reply Staple_Diet 1 hour agoparent [–] I'm sorry but your comment is incredibly I'll informed. Not only because you think Musk is any where near the leader in the implant space, but more so that you don't understand optogenetics. The device referenced here was transcranial (i.e., non-invasive) but regardless optogenetics requires tagging specific neurons with an opsin (usually via viral vector). I'm sorry for being curt but this is the 4th or 5th neuroscience post in the last 72h that HN have blasted with shit takes. If I wanted misunderstood science I'd revive my Reddit account. reply niemandhier 24 minutes agorootparentI am very aware how optogenetics works, I wasted some part of my live getting channelrhodopsins into cells to trigger neurons. Yes you are right one would probably use transient gene therapy tho make a modification like that, but transcranial methods are afaik for human adults less suitable since the structures lying above the stuff you usually want to stimulate are simply too thick, so a hypothetical sleep replacement machine would probably need to be an implant. That is unless you can use self-refocusing lasers or holograms for stimulation, but I do not see how one would do this unless you replace most of the skull with glass. The point with Musk was a joke more or less. Though it’s true that he is probably the only person both morally ambiguous enough to even try to build a dystopian sleep replacement machine and capable of getting funding for it. Maybe a Reddit account is a good idea. reply deprecative 25 minutes agorootparentprev [–] HN is great but it's only illusory that it has smarter people than Reddit. We're all just people rising to our incompetence to the limit the system allows. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Recent research indicates that brain waves are involved in removing waste and toxins during sleep, potentially impacting neurological conditions like Alzheimer's and Parkinson's disease."
    ],
    "commentSummary": [
      "NIH-funded research highlights the role of brain waves in clearing waste via the glymphatic system during sleep.",
      "Emphasis is placed on the accessibility of publicly funded research to avoid high publishing costs, covering Open Access journals and brain health benefits like inducing brain cleansing waves through sleep and meditation.",
      "Ongoing discussions involve the efficiency of brain waste removal during sleep, potential uses of brain wave manipulation, and Elon Musk's interest, alongside user intelligence on online platforms."
    ],
    "points": 217,
    "commentCount": 104,
    "retryCount": 0,
    "time": 1710568867
  },
  {
    "id": 39718672,
    "title": "Boeing whistleblower's death prompts skepticism over suicide ruling",
    "originLink": "https://futurism.com/the-byte/boeing-whistleblower-warning-not-suicide",
    "originBody": "Ticket to Ride Boeing Whistleblower Before Death: \"If Anything Happens, It's Not Suicide\" byNoor Al-Sibai \"I know that he did not commit suicide.\" Yesterday, 11:24 AM EDT Getty / Futurism \"I know that he did not commit suicide.\" Curious Causes As Boeing continues to be in the news for its repeatedly malfunctioning planes, the fallout from one ex-employee's death continues — and new reports complicate the coroner's initial suicide ruling. In an interview with Charleston, South Carolina's ABC4 News, a friend of Boeing whistleblower John Barnett, whose body was found dead in a car parked in a hotel lot amid his testimony against his former employer last weekend, said that he warned her that something might happen to him. \"I said, 'Aren't you scared?'\" the woman, who gave only her first name Jennifer, told the local broadcaster. \"And he said, 'No, I ain't scared, but if anything happens to me, it's not suicide.'\" Jennifer said that Barnett's words were spoken ahead of his deposition against Boeing. At the time, he'd mentioned that the company had retaliated against him for raising safety concerns before — which was, indeed, the subject of his Occupational Safety and Health Administration (OSHA) complaint that led to his now-unfinished deposition. The woman, who lives in the whistleblower's home state of Louisiana where he'd moved in recent years to take care of his aging mother, said that that cryptic warning has come back to haunt her since Barnett's death, which a coroner in Charleston says was self-inflicted. \"I know that he did not commit suicide,\" Jennifer told ABC4. \"There's no way. He loved life too much. He loved his family too much. He loved his brothers too much to put them through what they're going through right now.\" Hearsay She's not alone in that sentiment either, it seems. In a statement to Futurism, Barnett's attorneys said that they also \"didn't see any indication\" that the whistleblower may have been planning to take his own life, and that he'd seemed in \"good spirits\" as his deposition was coming to a close. Not everyone close to the longtime Boeing quality control manager agrees with that sentiment, however. In an interview with the Seattles Times, Barnett's niece, Katelyn Gillespie, said that her \"fun uncle\" had become \"stressed and depressed\" in recent months as his ex-employer was in the news amid the same kinds of safety concerns he'd raised and ultimately resigned over. \"He battled a lot due to the Boeing stuff,\" the whistleblower's niece said. \"It took a major toll on him.\" Despite the Charleston County coroner's preliminary autopsy report on the cause of death being a self-inflicted gunshot wound and that local police found, per the city's Live 5 News, \"some sort of note,\" authorities said they are still actively investigating the case while awaiting an official coroner's ruling. As in other suspicious whistleblower suicides, it's almost impossible for anyone to know exactly what happened when Barnett died — but with this new claim from someone close to Barnett, things just got a lot more complicated. More on Boeing: Pilot Lost Control of Boeing Jet Because Gauges “Went Blank,\" Causing Nosedive Share This Article",
    "commentLink": "https://news.ycombinator.com/item?id=39718672",
    "commentBody": "[flagged] Boeing whistleblower before death: \"If anything happens, it's not suicide\" (futurism.com)193 points by Geekette 19 hours agohidepastfavorite30 comments dang 11 hours agoComments moved to https://news.ycombinator.com/item?id=39712618. reply Jtsummers 18 hours agoprev [–] https://hn.algolia.com/?dateRange=all&page=0&prefix=true&que... Several previous submissions and two previous discussions on this same topic (one with 14 comments and one with 70 comments). All in the last few hours. https://news.ycombinator.com/item?id=39712618 - another with 351 comments from about 11 hours ago reply theultdev 18 hours agoparentMaybe this one will stick this time. I see people are flagging it heavily. I vouched it, because it's a worthy discussion to have, and it should stay up so people can see it. The other two were knocked off the top 20+ pages within a few minutes, you'd only notice it if you got lucky. @dang, can we have an intervention here? Maybe merge the two other dupes together and allow the discussion to be had? People clearly want to talk about it. reply ChrisArchitect 18 hours agorootparentWhat are you talking about? The big one from earlier isn't flagged? This is a [dupe]. People saw it. People talked about it. And are still. Direct to the discussion: https://news.ycombinator.com/item?id=39712618 reply theultdev 16 hours agorootparentI was talking about page rank, it disappeared from the first 20+ pages of HN. If you didn't see it within the 20 minutes it was there, you would never find it unless you had that handy direct link of yours. And this one is currently flagged. It was flagged when I vouched for it (that's how I was able to vouch for it) reply dang 11 hours agorootparenthttps://news.ycombinator.com/item?id=39712618 was on HN's front page for 4 hours, to judge by our logs. reply edgyquant 18 hours agorootparentprevI don’t understand why anyone would flag it. Yea it’s definitely worth having and worse case scenario it’s proven to be a suicide. But on the off chance it wasn’t it deserves to be looked into and companies shouldn’t be allowed to silence these things reply js2 17 hours agorootparentI flagged this one because I'm not sure what kind of meaningful discussion we're supposed to have about it, nor do I currently see one occurring here. It is not even remotely on-topic for this site: https://news.ycombinator.com/newsguidelines.html reply samatman 18 hours agorootparentprev> Off-Topic: Most stories about politics, or crime, or sports, or celebrities, unless they're evidence of some interesting new phenomenon. This is not evidence of some interesting new phenomenon. This is rank hearsay, and it doesn't belong here. reply zer00eyz 18 hours agorootparentprev> I don’t understand why anyone would flag it. The mans family said he had PTSD and Anxiety. The gun he killed himself with was his own gun. Considering those factors and occam's razor it feels disrespectful to have this sort of speculative conversation. If this were his wife, kids, mother I would be inclined to say news worthy, but random friend as the only person who heard this story feels a whole lot like someone looking for their own 15 minutes of fame. Note: I did not flag it and have a comment on one of the other threads... reply theultdev 18 hours agorootparentThat's an opinion you can have, but people deserve to find it, read it, and also read others and to offer counterpoints to your POV. reply kevinventullo 11 hours agorootparentprevYour first two sentences are compelling facts. Where did you learn about them? reply tylerritchie 18 hours agorootparentprev>I don’t understand why anyone would flag it. I do, I didn't, but I understand why it happens. my HN account is 14 years old, I read comments frequently, comment rarely, upvote occasionally, and flag very very rarely. I don't downvote because can't, I don't have enough karma yet. so, even if people who can't yet downvote don't know the full effects of flagging it's _literally_ their only option to indicate their belief something doesn't belong on the front page. reply tjoff 14 hours agorootparentYou can't downvote submissions regardless of karma. reply tylerritchie 14 hours agorootparentright, I could've been clearer. there's an implicit analogue between comments and submissions. i suspect low karma users use the only tools at their disposal for both reply theultdev 18 hours agorootparentprevI'm in the thinking certain entities have a farm of \"users\" with 500+ karma that can mass user-flag articles they don't want to be talked about. But who would do something like that and have the resources to do that? (He was not testifying against Boeing for anything to do with their planes. He already did that and has been public about it since 2019. This deposition was for the appeal for his own civil defamation case against Boeing, which he orginially lost. So Boeing was not threatened by his testimony in anyway. Even if they lost the case or settled, it would be a drop in the bucket compared to the harm he has already caused them and other legal issues they are already having... reply layer8 17 hours agorootparentAccording to dang: “The purpose of flagging is to indicate that a story does not belong on HN.” [0] It’s valid to think that a topic that is essentially just gossip and speculation does not belong on HN. See also the “Off-Topic” section in the guidelines [1]. [0] https://news.ycombinator.com/item?id=12173836 [1] https://news.ycombinator.com/newsguidelines.html reply burnished 17 hours agorootparentThat makes sense, thank you. I can imagine that a more substantive piece would not be flagged. reply burnished 17 hours agorootparentprevA sibling comment convinced me, this was probably flagged correctly. Not that the title statement is not important, but that this piece specifically is an especially shallow article and not up to standards. reply runeofdoom 17 hours agorootparentprevTo the best of my knowledge, his civil case revolves around his claim that Boeing retaliated against him for his whistleblowing. \"Company that cut corners on safety found to have retaliated against whistleblower\" certainly seems like it would have a pretty significant impact on PR and stock price. reply Zircom 17 hours agorootparentprev>This should only be flagged if it was found that this article was fraudulent and incorrect and he didn’t say that in my opinion. Okay then it should be flagged by your reasoning, did you read the article? Because there's no proof in it that he ever said it. A woman who refused to identify herself past a first name claims he said it. reply js2 16 hours agorootparentprevSubmissions cannot be downvoted. They can only be upvoted or flagged. Flagging is the tool we have for off-topic submissions. Comments can be upvoted, downvoted, or flagged. There, voting may be used to express agreement and disagreement, but flagging should only be used for comments that violate the site's guidelines. reply Mistletoe 11 hours agorootparentOkay I'm not interested in participating in this community anymore. It's an incredibly stupid way to handle anything and reeks of censorship and mob rule by a few that hit the flag button every time a story disturbs their world view. reply reaperman 18 hours agorootparentprevRank speculation about newsworthy deaths never go well on HN and historically have caused more pain for family and friends, and contributed to large amounts of misinformation being spread. I'm speaking specifically of Bob Lee, and also Tony Hsieh. You say \"Maybe this one will stick this time\" but the main posting about this received over 360 comments, which is a lot for HN. It's not like the whole thing has just been nuked with no chance for productive discussion. But I'm not seeing much productive discussion. I don't mind these being de-ranked for now. If it turns out to be an eBay stalking scandal type story, then we'll have a lot to talk about. If it turns out that even being a successful whistleblowing can lead to life experience so awful that you want to kill yourself halfway through the depositions, then maybe we can talk about what can we as a society can provide whistleblowers to help them navigate these incredibly difficult times (financially, emotionally, socially, and spiritually). reply theultdev 18 hours agorootparentWhile it did receive 360 comments, you cannot find it on any of the pages, going back 20+ pages. While you can find many much much older articles that are far more dead just a couple pages back. You wouldn't get the news at all about this tidbit of information if you didn't check HN in the right few minutes. I for one get a lot of my news from HN, as it's a news aggregator and a lot of other mediums are more censored. People are clearly interested in talking about it and have opinions about it and find it newsworthy given the TIMING and possible MOTIVE. YOU even had an OPINION you inserted in, but you don't feel others should be able to see this or put their POV or COUNTERPOINTS in? reply Jtsummers 17 hours agorootparentJust an FYI, there are some guidelines and a FAQ linked at the bottom of this page. A useful one: > Please don't use uppercase for emphasis. If you want to emphasize a word or phrase, put *asterisks* around it and it will get italicized. In other words, don't SHOUT EVERYTHING. Just like when speaking with people in person, shouting is generally a rude thing to do. If you want to emphasize something, just use asterisks around it: *emphasize* becomes emphasize (double up the *'s like ** or use \\* if you want to include an asterisk literally). reply reaperman 17 hours agorootparentprevThe reason we're talking about this is because the Emmy award-winning network executive/producer/writer/reporter Anne Emerson made it her biggest story on ABC4 today[0]. The YouTube video has almost a million views, which is a lot more than these HN threads will have. It's the top post on r/interestingasfuck today[1], tied for 2nd most popular this week. It's referenced in the top post of r/wallstreetbets today[2] (bullet point #2). I inserted nuanced opinion to help steer discussion away from emotional knee-jerk responses with no substance, to something a bit more productive. I haven't seen a lot of that in these threads, so I'm happy they're getting de-ranked. I don't think it's fair to characterize me the way you are. 0: https://abcnews4.com/news/local/if-anything-happens-its-not-... 1: https://www.reddit.com/r/interestingasfuck/top/?t=day 2: https://www.reddit.com/r/wallstreetbets/top/?t=day reply MilStdJunkie 17 hours agoparentprev [–] A single Boeing subsidiary - under 300m revenue - had a Social Media budget of 7m/year. That's two percent of revenue. Now imagine what capital-B-Boeing's social media budget is, and think about what a primitive -if awesome! - platform YN is. Something to keep in mind. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Boeing whistleblower John Barnett's death is under scrutiny following doubts raised by his friend and attorneys regarding the initial suicide ruling by the coroner.",
      "Despite some evidence indicating suicide, close associates point to Barnett's prior warnings and behavior as potential indicators of foul play.",
      "The ongoing investigation is adding layers of complexity to the case, raising questions about the circumstances surrounding Barnett's death."
    ],
    "commentSummary": [
      "A Boeing whistleblower hinted that their death was not a suicide before passing away, sparking significant discussion.",
      "The conversation on Hacker News faced numerous flags, with users debating the relevance and appropriateness of discussing the topic on the platform.",
      "Debates on censorship, speculation, and the essence of whistleblowing were central to the discussions around the whistleblower's statement."
    ],
    "points": 193,
    "commentCount": 30,
    "retryCount": 0,
    "time": 1710525359
  },
  {
    "id": 39717268,
    "title": "Introducing Pretzel: Browser-Based Data Exploration Tool with Privacy in Mind",
    "originLink": "https://github.com/pretzelai/pretzelai",
    "originBody": "Hey HN! We’ve built Pretzel, an open-source data exploration and visualization tool that runs fully in the browser and can handle large files (200 MB CSV on my 8gb MacBook air is snappy). It’s also reactive - so if, for example, you change a filter, all the data transform blocks after it re-evaluate automatically. You can try it here: https:&#x2F;&#x2F;pretzelai.github.io&#x2F; (static hosted webpage) or see a demo video here: https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=73wNEun_L7wYou can play with the demo CSV that’s pre-loaded (GitHub data of text-editor adjacent projects) or upload your own CSV&#x2F;XLSX file. The tool runs fully in-browser—you can disconnect from the internet once the website loads—so feel free to use sensitive data if you like.Here’s how it works: You upload a CSV file and then, explore your data as a series of successive data transforms and plots. For example, you might: (1) Remove some columns; (2) Apply some filters (remove nulls, remove outliers, restrict time range etc); (3) Do a pivot (i.e, a group-by but fancier); (4) Plot a chart; (5) Download the chart and the the transformed data. See screenshot: https:&#x2F;&#x2F;imgur.com&#x2F;a&#x2F;qO4yURIIn the UI, each transform step appears as a “Block”. You can always see the result of the full transform in a table on the right. The transform blocks are editable - for instance in the example above, you can go to step 2, change some filters and the reactivity will take care of re-computing all the cells that follow, including the charts.We wanted Pretzel to run locally in the browser and be extremely performant on large files. So, we parse CSVs with the fastest CSV parser (uDSV: https:&#x2F;&#x2F;github.com&#x2F;leeoniya&#x2F;uDSV) and use DuckDB-Wasm (https:&#x2F;&#x2F;github.com&#x2F;duckdb&#x2F;duckdb-wasm) to do all the heavy lifting of processing the data. We also wanted to allow for chained data transformations where each new block operates on the result of the previous block. For this, we’re using PRQL (https:&#x2F;&#x2F;prql-lang.org&#x2F;) since it maps 1-1 with chained data transform blocks - each block maps to a chunk of PRQL which when combined, describes the full data transform chain. (PRQL doesn’t support DuckDB’s Pivot statement though so we had to make some CTE based hacks).There’s also an AI block: This is the only (optional) feature that requires an internet connection but we’re working on adding local model support via Ollama. For now, you can use your own OpenAI API key or use an AI server we provide (GPT4 proxy; it’s loaded with a few credits), specify a transform in plain english and get back the SQL for the transform which you can edit.Our roadmap includes allowing API calls to create new columns; support for an SQL block with nice autocomplete features, and a Python block (using Pyodide to run Python in the browser) on the results of the data transforms, much like a jupyter notebook.There’s two of us and we’ve only spent about a week coding this and fixing major bugs so there are still some bugs to iron out. We’d love for you to try this and to get your feedback!",
    "commentLink": "https://news.ycombinator.com/item?id=39717268",
    "commentBody": "Open-source, browser-local data exploration using DuckDB-WASM and PRQL (github.com/pretzelai)180 points by prasoonds 21 hours agohidepastfavorite63 comments Hey HN! We’ve built Pretzel, an open-source data exploration and visualization tool that runs fully in the browser and can handle large files (200 MB CSV on my 8gb MacBook air is snappy). It’s also reactive - so if, for example, you change a filter, all the data transform blocks after it re-evaluate automatically. You can try it here: https://pretzelai.github.io/ (static hosted webpage) or see a demo video here: https://www.youtube.com/watch?v=73wNEun_L7w You can play with the demo CSV that’s pre-loaded (GitHub data of text-editor adjacent projects) or upload your own CSV/XLSX file. The tool runs fully in-browser—you can disconnect from the internet once the website loads—so feel free to use sensitive data if you like. Here’s how it works: You upload a CSV file and then, explore your data as a series of successive data transforms and plots. For example, you might: (1) Remove some columns; (2) Apply some filters (remove nulls, remove outliers, restrict time range etc); (3) Do a pivot (i.e, a group-by but fancier); (4) Plot a chart; (5) Download the chart and the the transformed data. See screenshot: https://imgur.com/a/qO4yURI In the UI, each transform step appears as a “Block”. You can always see the result of the full transform in a table on the right. The transform blocks are editable - for instance in the example above, you can go to step 2, change some filters and the reactivity will take care of re-computing all the cells that follow, including the charts. We wanted Pretzel to run locally in the browser and be extremely performant on large files. So, we parse CSVs with the fastest CSV parser (uDSV: https://github.com/leeoniya/uDSV) and use DuckDB-Wasm (https://github.com/duckdb/duckdb-wasm) to do all the heavy lifting of processing the data. We also wanted to allow for chained data transformations where each new block operates on the result of the previous block. For this, we’re using PRQL (https://prql-lang.org/) since it maps 1-1 with chained data transform blocks - each block maps to a chunk of PRQL which when combined, describes the full data transform chain. (PRQL doesn’t support DuckDB’s Pivot statement though so we had to make some CTE based hacks). There’s also an AI block: This is the only (optional) feature that requires an internet connection but we’re working on adding local model support via Ollama. For now, you can use your own OpenAI API key or use an AI server we provide (GPT4 proxy; it’s loaded with a few credits), specify a transform in plain english and get back the SQL for the transform which you can edit. Our roadmap includes allowing API calls to create new columns; support for an SQL block with nice autocomplete features, and a Python block (using Pyodide to run Python in the browser) on the results of the data transforms, much like a jupyter notebook. There’s two of us and we’ve only spent about a week coding this and fixing major bugs so there are still some bugs to iron out. We’d love for you to try this and to get your feedback! MarcoConverta 18 hours agoJust tried it out - thanks for sharing. We're a sales tech startup and I've had to look through large CSVs with prospecting information in the past - anything over 10 MB crashes my browser. I use a Mac so I don't have Excel. This looks great for simple data manipulations. I tried out a large CSV I had, and it loaded without a problem. Quick bug report: The filter interface seems to be slow for me though with the large file. Also, one feature that would be really helpful would be connecting this to a database and also some way to share my workflow/analysis. reply prasoonds 18 hours agoparentHey Marco, I'll take a look - filters theoretically should be fast, when you create a new filter, it simply reads does a `select * from table limit 1` to get column names I wasn't sure whether you could query DBs directly from the browser but looks like you can! (https://github.com/alexanderguy/pgress) - will add it to roadmap! reply prasoonds 15 hours agorootparentReplying as I couldn't edit: Alright, did a bit of digging and the flamegraph points to a problem with the Table component. I'd hoped that BlueprintJS tables would be performant enough for our usecase but apparently not! We'll try to move to canvas based rendering ASAP - that should fix any lags in the filter UI reply DannyPage 19 hours agoprevI was recently searching for something like this - the ability to write PRQL queries on mobile is my ideal usecase. As you note, PRQL is perfect for putting together queries with drop-downs, so I'm a huge fan of the UI that you've put together. Feature request: Exposing the PRQL would be great, and using tabs to switch between the query and results would make it possible to work on mobile. Lastly, bookmarking those queries for later use would be great too! reply prasoonds 19 hours agoparentYes! I'm heartened to see people realize the the benefits of PRQL and chained transforms over vanilla SQL (with some caveats!). We'll definitely add a PRQL/SQL block to the UI soon (you will be able to toggle to select which one you'd like). On that point - PRQL doesn't natively support PIVOT statements. As a hack, we made our own flavor of PRQL with a PIVOT statement and we parse that to SQL CTEs to make it work for now. I will be submitting a pull request to PRQL to add support to get around this. We're working on saving queries to local storage as well as sharing via downloadable config files (we're thinking a PRQL file with some sugar that when uploaded, recreates the entire transform) reply maximilianroos 17 hours agorootparent[PRQL dev here] Love what you've done and thanks for building on PRQL! We're very happy to add something like a PIVOT statement. In the long term, we'd have to think about how to make it work deeply in PRQL, since the column names are suddenly runtime dependent. In the short term, we should definitely make PRQL work for your case — it's important that there's an escape hatch for things that aren't natively supported by PRQL yet. (We have s-strings but I'm guessing they don't cover this specific case?) reply prasoonds 16 hours agorootparentHey Max, love what you all doing with PRQL! It's a wonderful tool. That makes sense. I ran into some old issues about Pivot but it seems it's not been implemented yet. And yes, s-strings wouldn't work - the DuckDB pivot statement looks like this: PIVOT ⟨dataset⟩ ON ⟨columns⟩ USING ⟨values⟩ GROUP BY ⟨rows⟩ So, we'd have to pass the `dataset` so far into the pivot statement. This is where CTEs come in handy but happy to hear if there's a better solution :) reply d1sxeyes 17 hours agorootparentprevFor folks looking for a workaround in the meantime, you can add an AI block and it will give you the returned SQL to edit. Not ideal because it’s just wasting credits, but it worked for me. reply maxloh 17 hours agorootparentprevIt would be good to have EdgeQL too. reply prasoonds 17 hours agorootparentOh this looks really interesting! I hadn't heard of EdgeDB/EdgeQL - I'll have to do a deeper read but at first brush, it looks like EdgeQL can be compiled down to SQL based on this HN comment: https://news.ycombinator.com/item?id=30296669 I'll take a look and if it's simply a matter of changing from the PRQL compiler to EdgeQL compiler, then, we should be able to have land in main in short order. It's not the highest priority thing right now though. reply dndn1 17 hours agoprevLooks great and thanks for sharing! You mentioned that you went to some length to implement pivots. How far do you think you will take the pivot feature and UI? For financial type usecases it's pretty much a requirement to be able to map many fields on X/Y, be able to collapse them in the the browser, and also show/control aggregations. Asking because many apps stop at the simplest level of pivot features and go to great lengths in other areas like more advanced visualization. For mapping many fields though, I think a good pivot table UI is like a secret weapon. I get that this isn't what you would focus on so much in week 1. Nice work + love the local-first and direct UI. reply prasoonds 16 hours agoparentRight - I used to work in GS and they had this really great internal table display tool where you could simply drag a column from the top to the left side and it would Pivot + Collapse all fields on it (plus allow multi level pivots). Then, you could look at the Pivot table OR do a drill-down to see the root-cause why a value was so high. I really liked that interface and haven't really found anything near as useable yet. Perspective JS (a free library by JP Morgan, you can try it here: https://perspective.finos.org/block/) has some really cool functionality in this direction but it has its one data processing engine, query language, rendering engine etc so we couldn't have used that for this project. But, this is an interesting thought - I definitely would want to see this functionality in Pretzel - the only question is how to prioritize this. If you know of any performant table libraries that support collapsible pivots out-of-the-box, I'd love to integrate that. Alternatively, we'll write our own! reply tobilg 13 hours agorootparentPerspective.js is what I use for https://sql-workbench.com grid and charting functionality. You can just do the data manipulations in DuckDB WASM and pipe the Arrow data to Perspective... reply dndn1 14 hours agorootparentprevOne that I'm watching is the Table mark in Graphic Walker [1]. There is a license thing about logos to note if you do use it in Pretzel [2]. On GW [1], Create Dataset -> Public Datasets -> Student Performance then change the Mark Type to Table you can play around. It hits the things that I mentioned pretty good! You might have similar issues as you have with Perspective though. I make calculang [3] and I'm getting ready to plug into a lot of things. A good pivot table generalizes well for my needs (mainly quick feedback during DX). Perspective is on the list but so is GW and Pretzel. Perspective might suit my needs perfectly. But I like DuckDB-WASM approach anyway so I hope you continue and I hope you nail it one way or another! :) [1] https://graphic-walker.kanaries.net [2] https://github.com/Kanaries/graphic-walker/issues/330 [3] https://calculang.dev reply prasoonds 12 hours agorootparentWow, Graphic Walker looks incredible! AND they vega-lite - I'm a BIG altair fan - this look like fun - I'll take a look! reply antonycourtney 14 hours agorootparentprevVery impressive project and vision! Love the demo! I am also ex-GS and worked on what I am fairly sure is the table display tool you're describing. I tried to carry the essential aspects of that work (multi-level pivots, with drill-down to the leaf level, and all interactive events and analytics supported by db queries) to Tad (https://www.tadviewer.com/, https://github.com/antonycourtney/tad), another open source project powered by DuckDb. An embeddable version of Tad, powered by DuckDb WASM, is used as the results viewer in the MotherDuck Web UI (https://app.motherduck.com/). If you're interested in embedding Tad in Pretzel, or leveraging pieces of it in your work, or collaborating on other aspects of DuckDb WASM powered UIs, please get in touch! reply prasoonds 14 hours agorootparentYes! I think it was TDS viewer or something like it - I loved using it so thank you for building it :) Tad viewer looks perfect for embedding in Pretzel! Is it easy to embed it in web apps? I’m on mobile right now and did a quick search and didn’t find anything. I’ll definitely be in touch! reply antonycourtney 9 hours agorootparentYes, Tad should be pretty easy to embed -- in the github repo there's a React component (TadViewerPane), and a fairly modular API for adding a new data source. I'm happy to work with you on this, this should be a fun exercise! reply ThreeMoonsAreUp 16 hours agoprevI enjoyed your demo video and played around with a CSV of my own. This is a cool project, but I have a (perhaps stupid) question: Why should I use this tool over Excel or Google Sheets? I don't mean this question to be a challenge. Just trying to understand which sorts of tasks would I be better off using Pretzel as opposed to a traditional spreadsheet tool. reply prasoonds 16 hours agoparentHey, absolutely - that's a perfectly fair question. I can give you my personal reasons: - I don't have excel on a Mac so I need to use Google Sheets. Google Sheets crashes on any file over 30-40 MB (at least on my macbook air) - I think this type of chained data transform is a much better (more code-like?) way of doing transforms. You can see exactly how you got to your end-result. Vs, for example, in Google Sheets, you're modifying data in-place and your transform history is only accessible via Undo and Redo. This is far more reproducible short of using a python script. (I've worked as a data scientist for a while and I suppose that influences how I think about chained data transforms vs the in-place Excel way too!) But this is just for now - the long term vision is: Being able to easily switch between visual no-code blocks, SQL and Python in the same browser-local \"notebook\"/\"workbook\" that's easily shareable. I think it doesn't take much convincing that such a tool would be more powerful than Excel for certain workflows! reply dns_snek 19 hours agoprevThat looks great, I love all these new use cases for WASM that are starting to pop up. Are you planning on supporting parquet files any time soon? I'd love an easy way to just drop a parquet file on there and easily visualize time-series data with it (e.g. sensor readings). Small issue I noticed: When you chart time-series data, the timestamp axis is just represented as an integer rather than a meaningfully formatted timestamp. reply prasoonds 19 hours agoparentThanks for the feedback! > Are you planning on supporting parquet files any time soon? Yes. This is an oversight on our part - duckdb natively supports Parquet and Arrow files but we just haven't gotten around to adding upload support for those yet. It's a small change - landing in main tomorrow! On the bug, yes, we've been trying to parse timestamps but it's just been very finicky. I'll try to fix this! Thanks for the report. reply funcDropShadow 4 hours agoprevIsn't doing data science locally in a browser like tieing one arm to your back and trying to tie your shoe laces? Why would you do it? reply thawab 17 hours agoprevThis is really promising, please look into embedded analytics. We are struggling with building our embedding for SAAS. Now we are doing it with cube and superset. All the alternatives are expensive or hard to implement.Being able to offload the analytics to the browser is a plus. reply prasoonds 17 hours agoparentThanks! This is interesting - just to make sure I'm understanding you correctly - you're trying to build analytics into your SaaS tool to offer it to your users, correct? And you're using Superset to build the analytics inside your SaaS app? I'll definitely look into it. For the moment, I know a team trying to solve this exact problem (they're not open-source/free but I think they're pretty inexpensive) - happy to connect you with them if you like :) reply mritchie712 17 hours agoparentprevWhat database are you using? reply thawab 17 hours agorootparentnow we use postgres, testing duckdb/md for analytics. reply sudiptabiswas22 8 hours agoprevI think no one really enjoys working with sheet or excel. Apart from the inability to handle large data, have such a high/unintuitive learning curve for most data analysis operations. I can see this tool become a ‘metabase’ for everyday usage of data analysis. reply timhh 18 hours agoprevNeat. I did a similar thing for analysing the output of commands that can produce JSON (SLURM in this case). Worked pretty well but I immediately ran into PRQL's lack of support for DuckDB's struct types. reply prasoonds 18 hours agoparentYes, there's definitely some downsides (which honestly, we didn't realize when we started to build this). For us, it was the lack of a PIVOT statement so I had to make a modified dialect of PRQL that supports PIVOT, then split on the final PRQL into PIVOT and non-PIVOT chunks, and then convert it to chained SQL CTEs. Annoying work for sure. As a result, I'll be adding support for PIVOTs very soon, either in the main repo or just in a temporary fork. reply crowcroft 19 hours agoprevOh man this is cool. I don't really have much feedback tbh I just think this awesome. I can see myself using this as a handy little utility when data sets get a bit too big for Google Sheets. In terms of it being a product, things I can think of that would be useful to me. - Importing/exporting data from Google Sheets/Google Drive. - Scheduling queries to run and export etc. Not sure if that really aligns to this specific project being that it runs locally though. Edit: Having a few standard dimension tables available to join would be great too. I often want to join time from a date dimension table when I'm making time series data for example. reply prasoonds 19 hours agoparentThat's great feedback, thanks! This tool definitely comes from a place of personal need - beyond just handling large files, I've also never really gelled well with the Excel/Google Sheet model of changing data in place as if you were editing text. I'm a Data Scientist and always preferred the chained data transforms you see in things like dplyr (https://dplyr.tidyverse.org/) or Polars (https://pola.rs/) and I feel this tool maps very closely to the chained model. Also, thank you for the feature requests! Those would all be very useful - we'll put them on the roadmap. reply Dikin 17 hours agorootparentHow ary you reply auntienomen 16 hours agoprevThis is quite nice. Any plans to add simpler ML tools? A sklearn plugin could be as valueable as LLM access. reply prasoonds 16 hours agoparentThank you! Yes, one of the items in the Roadmap is support for Pyodide (https://github.com/pyodide/pyodide) for running in-browser python on the results of each of the code blocks! This should allow most ML libs to be usable in-browser! This is pretty high-up on our priority list. reply antman 17 hours agoprevCan we bookmark or localstorage our etl? reply prasoonds 17 hours agoparentWe're going to build this over the next week - it's a high priority item for us. Once done, you should be able to do two things: - Save existing workdflows to localStorage (a little drawer on the left side of the screen) - Export workflows as a JSON file so you can share workflows with other by simply sending them the JSON file however you like reply derhuerst 13 hours agorootparentI think it would be great to be able to embed the workflow in the URL, so that I can share pre-made workflows with people who can then apply it to their data. reply tobilg 17 hours agoparentprevHave a look at https://sql-workbench.com, it supports query sharing via URL, as well as sharing visualizations. Let me know if you have questions! reply xnx 19 hours agoprevVery cool. Would love to be able to save/share my work (could probably even be in the url string if the CSV were online). reply ramonverse 19 hours agoparentRamon from Pretzel here, thank you for your feedback! We definitely have this on the roadmap: 1. Allow to download/upload workflows as files (maybe JSON, or PRQL) 2. Allow to share private URLs by attaching the workflow as a URL parameter We are building a roadmap upvoting site, we'll add it to the README as soon as it's ready reply gnishemcknight 18 hours agoprevNice to see DuckDB-Wasm get some love Curious from the pure performance point of view whether you have performed any benchmarking against any of the alternatives for this sort of in-browser data work? reply prasoonds 18 hours agoparentThat's an interesting point! No we haven't really - for now, I asked a bunch of friends to send me large CSVs - I loaded them on Google Sheets to see how it performs (usually terribly - crashes my browser tab or it's unbearably slow). Then I try the CSV in Pretzel with some filters and pivots and if I don't feel any annoyance with speed, it's a success in my book for now. We'll definitely need to figure out (a) what's the right thing to measure, and (b) an automated testing/CI to check for regressions and do perf testing reply bbkane 15 hours agoprevWow! Is there any way this could support SQLite databases? reply chrisjc 15 hours agoparentI haven't taken a close enough look at it yet, but duckdb supports Sqlite databases, so in theory yes. However, using Sqlite requires a duckdb extension and i'm not sure if works for wasm, and if so enabled this project in particular. https://duckdb.org/docs/guides/import/query_sqlite But I believe that most of these duckdb web-IDEs will revolve around a couple of generalized used cases: 1. OLAP/analytical type workloads/queries. Sqlite is really more for OLTP/transactional workloads. 2. Querying datasets that are available on, targeted for blob/object storage like S3. Parquet, CSV, line-delimited JSON, etc reply prasoonds 15 hours agoparentprevDuckDB supports SQLite through extensions (https://duckdb.org/docs/extensions/sqlite.html) So, it should be pretty straightforward to let folks drop a SQLite file instead of a CSV file! I haven't been able to get extensions to work on WASM so far though but I will definitely take a look to see if we can make this work! reply fillipvt 19 hours agoprevthis is great, thanks for sharing I'm currently using a self hosted instance of lightdash connected to a dbt project and I can see this being really efficient for data exploration for business users quite interesting! reply prasoonds 18 hours agoparentThank you. We're definitely looking to get to a point where you don't need to jump to a jupyter notebook for simple analyses, especially things which are hard to do in SQL (a basic linear regression model for eg). Happy to chat at prasoon [at] withpretzel [dot] com if you need any integration help! reply tobilg 17 hours agoprevThe DuckDB WASM space is really heating up! I released https://sql-workbench.com a few weeks ago, which can be used to query and visualize Parquet, CSV, JSON and Arrow data. There’s also a accompanying tutorial blog post at https://tobilg.com/using-duckdb-wasm-for-in-browser-data-eng... reply chrisjc 17 hours agoparentI just like to thank you (again) for how much you've made a lot of what is required to get duckdb up and started (esp on AWS) much easier. Interesting to watch the similarities (duckdb wasm) and differences (python vs js) between your SQL IDEs in the browser. Exciting stuff. reply tobilg 17 hours agorootparentThank you! It’s really great that the in-browser data analysis ecosystem is really firing up… Exited to see what others build reply prasoonds 16 hours agoparentprevOh hey! I remember seeing your HN post several weeks ago, I think - it's a great tool, thanks for building this! It was definitely an inspiration for us :) reply tobilg 16 hours agorootparentThanks and congrats on the launch! reply hitradostava 16 hours agoprevHey congrats on the Show HN. Local, browser based data exploration works for a lot of uses cases and is so much faster thancloud based tools. We've implemented something similar at https://addmaple.com/ - but with a graphical interface designed for rapid exploratory data analysis of large datasets. Memory per tab can be an issue for really big files (1gb+) but we're exploring a transform to CBOR which allows us to free up JS memory, i.e. when parsing CBOR we can leave row level data as Uint8Array and it doesn't increase the JS memory overhead. reply prasoonds 15 hours agoparentThanks! Maple looks really cool - really interesting demo video, too! This is quite interesting - we've not explored really large files so far and being honest, we haven't thought that far either. Didn't know about CBOR! I will have to look deeper into how this can save on memory. I was wondering though, since WASM memory is limited to 4GB, if I have sufficiently large memory on my compute device, at least one tab should be able to handle 1gb+ files too, correct? reply hitradostava 15 hours agorootparentThanks :-) I've not done much profiling on DuckDB and what the overhead is - i.e. after the data is parsed how much memory is used. Would be really interesting to push it to the limit - or to explore not loading the entire file in, but only reading the relevant parts, but again that probably requires a conversion first, e.g. to parquet or some other column based storage format. reply prasoonds 15 hours agorootparentthat sounds like a fun project :) maybe I'll take a stab at it! reply logical_person 17 hours agoprev [–] 200mb of data is not a large file, and chromium tabs have a memory limit of something ridiculously low so actual large 20-100gb datasets render this useless. reply yawnxyz 17 hours agoparentSometimes you need a scooter, sometimes you need a truck. I think a snappy interface for <1gb datasets is really neat and super useful for certain kinds of data reply nojvek 49 minutes agorootparentYou can now use the file api to work with terabyte sized files if your disk can handle it. https://developer.chrome.com/docs/capabilities/web-apis/file... Browser is pretty powerful nowadays. reply prasoonds 16 hours agorootparentprevThis echoes my thoughts exactly. Right now, we're actually more limited by the JS UI so a couple 100 MBs is the most you can do in a browser otherwise the UI becomes really slow. There's a lot of room for improvement - we're using React and that's causing a bunch of un-needed re-renders right now that we don't need. We probably need to create our own DAG based task management system and use Canvas to render everything - with all that, workflows on much larger files will hopefully become usable. reply prasoonds 16 hours agoparentprevThis is certainly true - I'm not saying \"large file\" in the colloquial sense of the \"big data\" but rather as in - a file you might want to open in Excel/Google Sheets. I've worked actual large datasets before - upwards of 500GB - pretty often before an I really wouldn't think about using my laptop for a such a thing! We are thinking of making data connectors to major DBs though so you should be able to do a similar style visual analysis while keeping the compute on your DB. reply slaymaker1907 16 hours agoparentprev [–] I looked up the limit and as of 2021, tabs seem to have been limited to 16GB which is moderate in size for an in-memory dataset. However, I know WASM has a hard limit of 4GB without Memory64. Data size is all relative. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Pretzel is an open-source data exploration and visualization tool that operates in the browser, capable of managing large files and responsive.",
      "Users can upload CSV/XLSX files, apply filters, conduct data transforms, and generate plots using the tool, which ensures local browser operation for privacy protection.",
      "The creators are actively enhancing Pretzel with new functionalities and embracing user feedback while incorporating advanced technologies for swift processing and offering an optional AI component."
    ],
    "commentSummary": [
      "Pretzel is an open-source tool for exploring and visualizing large CSV files in the browser, utilizing DuckDB-WASM and PRQL for data processing.",
      "Users appreciate its interactive features like autocomplete, AI block, and support for SQL and Python, suggesting enhancements in filter performance.",
      "Discussions revolve around implementing PIVOT statements, collaborating with tools like Perspective.js and Tad for advanced visualization, and the advantages of DuckDB-WASM for data analysis in the browser."
    ],
    "points": 180,
    "commentCount": 63,
    "retryCount": 0,
    "time": 1710518561
  },
  {
    "id": 39717615,
    "title": "Optimizing Chess Moves for Storage and Speed",
    "originLink": "https://mbuffett.com/posts/compressing-chess-moves/",
    "originBody": "Compressing Chess Moves for Fun and Profit Chess notation has come a long way since descriptive notation, now we have nice and decipherable Standard Algebraic Notation, like Qxf7 (queen takes on f7) or Nf3 (knight takes on f3). This is a great text format, but a massive waste of space if you’re trying to store a lot of these. Qxf7 takes 4 bytes, or 32 bits. Let’s do some rough back-of-the-envelope math of how much information is actually being transmitted though. This move affects one of 6 pieces (3 bits), this move is also a capture (1 bit), and it specifies a destination squuare (64 possibilities == 6 bits). Add those up, you get 10 bits. Far from the 32 bits that the textual representation needs. Why do I care? I run a site that stores a ton of chess lines, something like 100 million in total. Assume an average of 6 moves for each line, that’s 600 million moves. The database is growing large enough that querying it is IO-constrained. I want to speed up the reads from this database when I’m fetching thousands of lines. A first pass First some general numbers: Encoding a file or rank (a-h or 1-8) takes 3 bits (8 possibilities) Encoding a piece (k, q, r, b, n, p) takes 3 bits (6 possibilities) Encoding a square takes 6 bits (64 possibilities) So let’s see the pieces we’re working with for encoding a SAN. So let’s go with the most naive approach. Which piece was it? 3 bits Is it a capture? 1 bit Do we have to disabiguate it (ie Ngf3)? Maximum of 2 bits + 6 bits (this is explained more further down) Where did it go? 6 bits Is it a promotion, and to which piece? 7 bits Is it a check? 1 bit Is it a checkmate? 1 bit Is it a castle? Short or long? 2 bits This gives us a total of 3+1+2+6+6+7+1+1+2 = 29 bits, or about 3.5 bytes per move. That’s not great though. A lot of moves actually take up less bits than that in text format. Getting smarter The first few bits In the first pass, we just encoded the piece that moved using 3 bits, but that leaves 2 unused permutations available, since there are only 6 pieces in chess. Luckily, there are two very common moves that fit neatly into this hole, those being short castles (O-O) and long castles (O-O-O). So we can encode the first 3 bits like so: match first_bytes { FirstBytes::Pawn => bits.extend(&[false, false, false]), FirstBytes::Knight => bits.extend(&[false, false, true]), FirstBytes::Bishop => bits.extend(&[false, true, false]), FirstBytes::Rook => bits.extend(&[false, true, true]), FirstBytes::Queen => bits.extend(&[true, false, false]), FirstBytes::King => bits.extend(&[true, false, true]), FirstBytes::ShortCastling => bits.extend(&[true, true, false]), FirstBytes::LongCastling => bits.extend(&[true, true, true]), } The destination square In all cases except castling, you need to know the square that the piece is moving to, to reproduce the SAN. So we’ll skip this entirely when castling, but otherwise always include 6 bits for the destination square. Just kidding, pawn moves! There’s another exception to the destination square rule that might not seem so obvious. Let’s take the move exf6. We know it’s a pawn move from the e-file, so we don’t really need to encode the file it’s capturing using 6 bits. After all, you’ll never see exa6. So in these cases instead of 6 bits for the destination square, we only need 4 (one for the direction, and one for the rank). But we can get even more clever here. Take hxg6 for example. You know as soon as you see hx, that the file is going to be the g-file. So we don’t even need the extra bit to encode direction, we can just encode the file once, and the rank once. So here’s the setup: Pawn capture from b-g files: 3 bits for the file you’re capturing from, 1 bit for the direction of the capture, and 3 bits for the rank Total: 10 bits for movement Pawn capture from a and h files: 3 bits for the file you’re capturing from, 3 bits for rank Total: 6 bits for movement “Special” moves It’s a bit of a waste to encode for each move, whether it’s a promotion, check, checkmate, capture, etc. After all, the vast majority of chess moves in a game are not any of these. So we’ll devote one bit to determining whether a move is “special”. It means we have to use an extra bit for moves that are promotions/checks/captures, but it also means that we save a whole lot of bits for “regular” moves. Promotions Promotions are nice, because even though there are 6 chess pieces, there are only 4 valid pieces that you can promote to (after all, you can’t promote to a King or Pawn). So we only need 2 bits instead of 3, to encode the promotion piece. if let Some(promotion) = promotion { bits.push(true); bits.extend(match promotion { PromotionPiece::Queen => &[false, false], PromotionPiece::Rook => &[false, true], PromotionPiece::Bishop => &[true, false], PromotionPiece::Knight => &[true, true], }); } else { bits.push(false); } Disambiguation Disambiguation is a bit thorny. You have to encode a surprising amount of information, to be able to decode the SAN exactly as you received it. Take Ngf3 for example. Besides the usual stuff you also need to encode the disambiguation (file=g). There are 3 different disambiguation possibilities (rank, file, or whole square), so we need 2 bits. One goes to waste but disambiguated moves aren’t that common and I can’t think of a nice way to take advantage of that last permutation. So we always use 2 bits, then either 3 bits for the rank, 3 bits for the file, or 6 bits for a whole square (very rare). if let Some(disambiguate) = disambiguate { bits.push(true); match disambiguate { Disambiguation::File(file) => { bits.extend(&[false, true]); bits.extend(square_component_to_bits(file)); } Disambiguation::Rank(rank) => { bits.extend(&[true, false]); bits.extend(square_component_to_bits(rank)); } Disambiguation::Square(square) => { bits.extend(&[true, true]); bits.extend(square_component_to_bits(square.0)); bits.extend(square_component_to_bits(square.1)); } } } else { bits.push(false); } How does this do? Move Original bits Encoded bits Savings e4 16 10 37.5% exd5 32 12 62.5% Nf3+ 24 10 58.33% Qxa5+ 40 16 60% cxd8=Q# 56 16 71.43% Not bad! We’re saving anywhere from 37.5% to 71.43% of the bits. PGNs You may be thinking something like this: “Isn’t is sorta cheating to measure these in bits? Since you can only address one byte at a time, needing 10 bits for a move is virtually the same as 16 bits” Well yes, that’s true and means that we get less savings when storing individual SANs. But they don’t account for the majority of what I’m storing, which are PGNs. PGNs are a way to store lines or games, and they look something like this: 1.e4 d5 2.exd5 Nf6 3.d4 Bg4 4.Be2 Bxe2 5.Nxe2 Qxd5 6.O-O Nc6 7.c3 O-O-O 8.Qb3 Qh5 9.Nf4 Qh4 10.Qxf7 Ng4 11.h3 Nf6 12.Be3 e6 13.Qxe6+ Kb8 14.Nd2 Bd6 15.g3 Qh6 16.Qf5 Ne7 17.Qa5 b6 18.Qa6 Bxf4 19.Bxf4 Qxh3 20.Nf3 Ned5 21.Be5 Qh5 22.Kg2 Qg6 23.Rae1 Nh5 24.Nh4 Nhf4+ 25.Bxf4 Nxf4+ 26.Kh2 b5 27.Qxg6 hxg6 28.gxf4 Rxh4+ 29.Kg3 Rdh8 30.Re5 Rh3+ 31.Kg4 R8h4+ 32.Kg5 Rh5+ 33.Kxg6 Rh6+ 34.Kf7 Rh7 35.Rxb5+ Kc8 36.Rg5 g6+ 37.Kxg6 R3h6+ 38.Kf5 Rf7+ 39.Kg4 Rhf6 40.f5 Kd7 41.Re1 Kd6 42.Re8 Kd7 43.Ra8 Rh6 44.Rxa7 Rfh7 45.Kf4 Rh4+ 46.Rg4 Rh2 47.f3 Rxb2 48.f6 Rf7 49.Rg7 Ke6 50.Rxf7 Kxf7 51.Rxc7+ Kxf6 52.a4 Ra2 53.Rc4 Ra3 54.d5 Ke7 55.Ke5 Kd7 56.f4 Ra2 57.Kd4 Rd2+ 58.Kc5 Ra2 59.Kb5 Kd6 60.Rd4 Rb2+ 61.Ka6 Ra2 62.a5 Kc5 63.d6 Ra3 64.d7 Kc6 65.d8=Q Rxa5+ 66.Kxa5 Kc5 67.Qc7# This PGN is 759 bytes. There’s a ton of wasted space though. One byte between each move (the space). Then at least 3 bytes between full moves ( 2.). This is sort of crazy, and if we combine our SAN encoding, we can compress this to be way smaller. If we encode the whole PGN using our SAN encoding, with no space between moves because we know once we’ve reached the end of a move, we can compress this specific 759-byte PGN down to 195 bytes, for a savings of 74%. Impact This hasn’t been deployed yet, I’m working on a ton of other performance improvements. But I anticipate this along with EPD compression (which I may write another at article on) will reduce the size of the database by about 70%. We’re almost entierly read-constrained, which should mean a 3x speedup for the most expensive queries we run. Speed Another consideration here is the speed of doing this encoding/decoding; will it just cancel out the gains from having a much smaller database? Turns out, computers are really fast at this stuff, and conversion to and from this encoding is a rounding error. I’m using Rust + the bitvec library. Encoding and then decoding 1000 moves takes about 600,000ns, or 0.6ms. I haven’t taken a performance pass at all either, and there’s a few places I know are very inefficient. I’m guessing I’m not even within 10x of optimal, but it should be good enough.",
    "commentLink": "https://news.ycombinator.com/item?id=39717615",
    "commentBody": "Compressing chess moves for fun and profit (mbuffett.com)167 points by thunderbong 20 hours agohidepastfavorite130 comments tromp 20 hours agoThe optimum compression seems to be an index into the list of all legal moves. But one can do better on average, by assuming that moves are not uniformly distributed. Let an engine assign each legal move m some probability p of being played. The worse the move, the lower the probability we assign to it. Then a more optimal code for move m is -log p bits, corresponding to an entropy (expected surprise) of sum -p log p bits for the distribution over legal moves. Related: accurately estimating the number of legal positions of chess [1]. [1] https://github.com/tromp/ChessPositionRanking reply perihelions 19 hours agoparentThough, that engine evaluation is going to be absurdly expensive, when you're compressing/decompressing billions of games. I think everyone's optimizing the wrong thing! The size of a useful, real-world chess database would be dominated by its search indexes—indexes that allow fast, random-access lookup of every position that occurs in every game. (Or even better: fuzzy searching for nearby, \"similar\" positions). This is the difficult, open-ended algorithmic problem. Disk space is cheap! (Out of curiosity, does anyone here actually know how to solve the \"fuzzy search of chess positions\" problem?) reply alfalfasprout 18 hours agorootparentAgreed. Ultimately, the most common reason for needing to store a bunch of games is you want a chess database so you can look up a given position. So probably some kind of tree structure so you can look up games by sequence of moves since opening. And something like zobrist hashing for looking up a given position (regardless of sequence). Some of these ultra-small encodings would be counterproductive for this purpose. reply perihelions 18 hours agorootparent- \"And something like zobrist hashing for looking up a given position\" Which gets particularly good if you combine it with some hack like a Bloom filter. There's a trick, I don't remember if it has a name, where you can effect a time/space tradeoff by partitioning a search set and building a separate Bloom filter for each partition. A lookup is a constant-time query of each Bloom filter, plus a linear scan of each partition that signaled a match. (This is a reasonable thing to do, because (0) the size of chess databases can get extremely large and (1) the queries are human UX interactions that need to be responsive). reply alfalfasprout 18 hours agorootparentI mean, chess engines use transposition tables to store encountered positions. In this case you can use the zobrist hash as the index to a hash table and it's an O(1) lookup for a given position. Typically that's going to be the best way to index into full games. Since you can point to an entry via a game tree search (eg; each node of the tree is a position that you can use as an index to a big transposition table). You can probably use a bloom filter to check if a given position has ever been encountered (eg; as a layer on top of the transposition table) though. For something like storing 10 billion positions w/ a 10^-6 false positive probability (w/ reasonable number of hashes) you're looking at 30+GiB bloom filters. reply perihelions 17 hours agorootparentRight! The hash table you're describing is O(1) lookup, but its memory size is a constant multiple larger than the Bloom filter. A Bloom filter doesn't tell you anything about where the needle is–only whether the needle exists; so, if used for lookup, it reduces to O(n) brute-force search. In between the two, you can create an array of M partitions of the haystack, each with their own Bloom filter: this structure has something like O(M + n/M) lookup, but with the smaller memory size of the Bloom filter. When I implemented this, I deliberately avoided constructing the hash table you're describing, because (to my recollection) it was larger than my remaining disk space! :) reply koolba 17 hours agorootparentprev> Out of curiosity, does anyone here actually know how to solve the \"fuzzy search of chess positions\" problem? A classification based indexing would help here. The data points of the index would be not just the position of the pieces, but their relative as well. Things like the squares under attack, pinned weaker pieces, and maybe some evaluation of control of the board (again maybe via some attacking based metric). This seems like the kind of thing that could be done by analyzing a larger dataset of games to create a set of functional categorizations. Once the data points themselves have been identified, actually indexing them is the easy part. Solving this more generally for the “find me games like X…” is much trickier without locking down that X to something concrete. reply perihelions 17 hours agorootparentFor example: \"this position, but a few of the pieces can be in different places\". (Maybe penalizing pawn moves with a higher weight—a pawn move is in practice a \"larger change\" to a position than a piece move). This version is easy to formalize: it's just querying a bitstring x \\in {0,1}^N, a bitboard vector representation of a chess position, and asking if there's any other needles within a short Hamming distance. I haven't a clue how to solve that (or if it's solvable at all). reply kevindamm 9 hours agorootparentI wonder if shingle hashes might help a lot here... https://en.wikipedia.org/wiki/MinHash reply lifthrasiir 10 hours agorootparentprev> Though, that engine evaluation is going to be absurdly expensive, when you're compressing/decompressing billions of games. It only has to be accurate enough to be beneficial though. Even a knowledge about two sets of more frequent and less frequent moves would be much better than the baseline. reply stevage 13 hours agorootparentprevYeah unfortunately the blog post didn't really explain the use case well enough. There has to be a reason they want to waste space storing the redundant indicators of check and checkmate for instance. reply tucnak 6 hours agorootparentprevI had built a demo like that a few months ago, it relied on pg_tgrm GIST indexes of the position's FEN string encoding and only a few bits of metadata per position. You could push it by \"learning\" some representation and perhaps relying on bloom filters or summat clever like that maybe... reply ant6n 19 hours agorootparentprevYou just need to enumerate for all possible states, the possible transitions and evaluate the probabilities for them. How many could there possibly be? Store in a small database and look em up on the fly. Easy peasy. reply anonymous-panda 18 hours agorootparentI know you’re joking, but it wouldn’t really be that hard in terms of a lookup for played positions which is what OP asked for - you could have Bitboards for the relevant info so finding a state would be a refined search starting with the busy/occupied Bitboard. Since you’re only storing games that have been played rather than all positions, the size of the index shouldn’t be outrageously large compared with other things already being stored. reply perihelions 19 hours agorootparentprev- \"and evaluate the probabilities for them\" I don't know any good, fast heuristics for predicting this (likelihood of a human choosing a specific chess move). Do you? Chess engine evaluation is computationally quite heavy. reply svnt 18 hours agorootparentParent was making a joke. The best upper bound found for the number of chess positions is 4.8x10^44. https://tromp.github.io/chess/chess.html reply ant6n 16 hours agorootparentprevYou don’t need to make perfect predictions. There could actually be a lookup for a few common board states (say a couple million, to cover the first couple moves). Beyond that just use a simple scoring function, for example the value of your vs opponent pieces, whether any of your pieces (notably king) are in danger, whether u won, etc. This means u get better scores for the more likely winning moves, for capturing valuable pieces and for avoiding the loss of your own. U could also play two or so turns of minimax, or perhaps use a neural network to evaluate the various reachable board states. So for a given state, enumerate possible transitions, score the resulting states, and map to some sort of probability distribution, then use some prefix code (think Huffman tree[+]) based on the distribution of probabilities to encode the transition. It’s perhaps not super fast, and not super accurate but if you can weed out the 50% of dumb moves, that already saves a bit. [+] an easier and better approach is to map the probabilities into an interval between 0 and 1, and keep using fractional bits to „zoom in“ until one of the subintervals is uniquely defined, then recurse on that. Some of the common compression algorithms uses that (but I don’t remember the specifics, those intro courses were a long time ago). reply crystaln 18 hours agorootparentprevRealizing this is a joke, however you only move one piece at a time and there are limited valid moves. Standard chess notation depends on this. reply jstanley 16 hours agoparentprevIf you turn an arbitrary byte string into a list of indexes into legal chess moves, then you can encode arbitrary data as chess games! (My implementation: https://incoherency.co.uk/chess-steg/ and explanation at https://incoherency.co.uk/blog/stories/chess-steg.html ) reply stevage 13 hours agorootparentThat's really cute. I'm surprised stalemate detection is harder than checkmate detection. Aren't they the same except checkmate has the extra step of detecting check? reply bubblyworld 7 hours agorootparentNot quite - stalemate is no legal move whatsoever for any of your pieces, and checkmate is king is in check and no legal move stops check. reply stevage 6 hours agorootparentBut given that a legal move by definition ends with the king not in check, the distinction often kind of goes away. In implementation terms, I think you basically have to just look at every possible move of every piece and see if any of them end without being in check. Stalemate can and almost always does involve pieces that can't move because they would create check. reply bubblyworld 5 hours agorootparentYeah, that's true, I see what you're saying. I guess knowing the king is already in check allows for optimisations, since you have to either move the king, capture the attacker or block the line of attack, and nothing else is legal. And if you're double-checked you have to move the king, no matter what. As opposed to stalemate where you really do have to check the moves of every piece. reply bicijay 19 hours agoparentprevSometimes i wonder, how i got so far im my software developer career... reply vecter 19 hours agorootparent\"Software\", like \"science\", is a broad term. Just like there are many kinds of scientists and you could be, say, a biologist or a physicist, there are many entirely disparate fields of software engineering. At the end of the day, software itself is just a means to an ends: solving a problem. Solving some of these problems like the one that GP discussed have more to do with math than programming. The programming is mostly the implementation of the math. So if you're not versed in that math, of course you wouldn't understand the code that's being written to implement it. Building machine learning systems is vastly different from building operating systems which is vastly different from embedded systems which is vastly different from networking which is vastly different what most of us do (myself included), which is building CRUD apps. We're just solving different problems. Of course there are common themes to writing good code, just like there are common themes to performing science, but the code and solutions will look almost nothing alike. reply Operyl 19 hours agorootparentprevBecause there are so many fields of software development, and you don’t need to be good in all of them. Don’t put yourself down for not understanding, you can always put some time into algorithms again if you need to :). reply acomar 17 hours agorootparentprevmost of the optimizations discussed are actually kind of obvious if you know how chess notation works in the first place. like the reason the starting move \"Nf3\" makes any sense is because only 1 knight of all 4 on the board can possibly move to f3 on the first move. what the OP is doing is trying to take that kind of information density and trying to represent it, first by assigning bits to each part of a possible move and then by paring it down where the space of legal moves excludes entire categories of possibilities. there's another article that goes much further into how much optimization is possible here: https://lichess.org/@/lichess/blog/developer-update-275-impr... reply porphyra 19 hours agoparentprevIndeed, compression is just the dual problem of prediction, so the better you are at predicting the next move, the better you can compress. reply andoando 19 hours agoparentprevIm confused, how does probability help here when you need to encode the possibility of all legal moves? reply perihelions 19 hours agorootparentYou use variable-length encodings—shorter encodings for more likely moves, longer encodings for ones that happen infrequently. This is the thing the parent comment was referring to, https://en.wikipedia.org/wiki/Huffman_coding reply jprete 19 hours agorootparentprevUsing the probability distribution of moves at any given position, so that more likely moves take less space, will result in saving space on average. This is the same as Huffman coding, just applied to a different domain. For that matter, it might also resemble LLM compression of input text, although LLM compression is intentionally lossy and the above schemes are not. reply neon5077 18 hours agorootparentprevLook up Huffman codes on Wikipedia. In short, you put the most likely or most common values at the front of the dictionary. Values 0-255 take up one byte, 256-65535 take two bytes, etc. The lower your index, the fewer bits are needed to represent the corresponding state. reply vitus 18 hours agorootparentYou're missing an important detail of Huffman codes: they need to be prefix-free so you can actually tell when one value ends and another begins. A simplified example: suppose you have four letters in your alphabet that you want to represent in binary. You can't just represent these as A=0, B=1, C=10, and D=01, since it's impossible to tell whether \"01011\" is meant to represent ABABB or DDA or ACBB. (Hamming codes on the other hand are intended for error correction, where you want maximally distant code words to minimize the chance that bitflips cause decoding errors.) reply neon5077 17 hours agorootparentI don't really think that's too relevant to the core concept of using fewer bits for the most common codes. That's really a problem inherent to binary streams in general, not just Huffman encoding. reply vitus 15 hours agorootparentIt was a minor point re: this claim: > Values 0-255 take up one byte, 256-65535 take two bytes If you wanted to encode more than 256 values, then at best you'd be able to specify values 0-254 taking one byte, e.g. if you used 0xff as a special prefix to represent \"use another byte for all other values\", but that particular encoding means that you'd only be able to encode another 256 values with a second byte. reply angry_albatross 18 hours agorootparentprevI think you mean Huffman codes, not Hamming codes. reply neon5077 17 hours agorootparentYup. reply sedatk 19 hours agoparentprevSo, Huffman encoding but for chess moves? reply vardump 19 hours agorootparentKinda, but with a different dynamically determined dictionary for each successive position. reply transitionnel 19 hours agorootparentThis is the kind of discussion that will get us micro-cybernetics in that ~1 year window. reply groby_b 18 hours agoparentprevLichess covered that :) https://lichess.org/@/lichess/blog/developer-update-275-impr... reply joshka 15 hours agorootparentThat's neat, so it looks like ln2(216) =~ 7.7 bits to encode any valid move, and then compresses to even better! reply groby_b 14 hours agorootparentYep. There's a paper somewhere claiming that the lower bound is ~5.1 bits/move, based on an exhaustive analysis of existing games. https://www.chessprogramming.org/Encoding_Moves sort-of mentions it (search for 5.11), but I can't for the life of me find the actual analysis. reply dmurray 19 hours agoparentprevWith a different engine for weaker players, whose moves would generally need a longer representation to encode. For most use cases where you're encoding a ton of PGNs, you also encode some information about the playing strength of the players, so you get that for free. reply cjbprime 9 hours agorootparentThis probably isn't necessary. Anyone past beginner is still sometimes playing one of the best moves for each move, and when they don't it is probably not bad in a predictable way. reply adrianmonk 15 hours agoparentprev> Let an engine assign each legal move m some probability p of being played. This assumes you're storing moves of players who are good at chess! reply cjbprime 9 hours agorootparentNo, it just saves more space when the players are good. (Note the engine assigns each legal move a probability.) reply adrianmonk 6 hours agorootparentThey said one can do better on average this way. That conclusion does rest on an assumption. Anyway, I wrote it in the form of a joke, but I was trying to suggest that for optimal compression, you may need to model how a player actually plays, not how they should ideally do it. reply programjames 14 hours agoparentprevYep, make an LLM but for chess moves. reply sterlind 11 hours agoparentprevwhat you're describing is arithmetic coding, correct? reply Someone 20 hours agoprevFTA: “But they don’t account for the majority of what I’m storing, which are PGNs.” If they’re storing chess games, why do they try to compress individual moves? If you compress games, you can get much better compression. For example, in any board position, deterministically order all legal moves (they’re already ignoring some illegal moves in their setup, so I think ignoring all is within the rules of the game), and write down the number of the move made. At game start, that will be an integer between 1 and 20 (16 different pawn moves, 4 different knight moves). Black’s reply similarly will be an integer between 1 and 20. For white’s second move, the maximum will depend on the first move. To compress an entire game, use arithmetic encoding (https://en.wikipedia.org/wiki/Arithmetic_coding). If there are, on average, n legal moves per position, that will use 2log n bits per move. https://www.chessprogramming.org/Chess_Position: “The maximum number of moves per chess position seems 218”, so that will give you less than 8 bits per move. In real life, it probably will be less than 6. The only reason I see why this wouldn’t be an option is performance. Decoding would require move generation and thus be more complex. To improve on that, make a deterministic predictor of how likely moves are and use that to improve on the arithmetic encoder. That probably isn’t worth it, though because of the increased decoding complexity. reply marcusbuffett 18 hours agoparentI did end up doing something like this: https://mbuffett.com/posts/compressing-chess-moves-even-furt... You’re spot on about the performance reason I didn’t want to do this originally, but I did some testing and turns out move generation in the library I use is Blazing Fast and wouldn’t be a bottleneck reply lifthrasiir 9 hours agorootparentYou don't seem to have a public repository for the code described in your posts (presumably a part of Chessbook?), but you might be using a textbook arithmetic coding algorithm which maintains both lower and upper bounds and does a renormalization by comparing topmost bits. If it's the case rANS would be much simpler to write and more efficient [1]. If your AC code is already well-optimized, there is not much reason to switch though. [1] Fabian Giesen's sample implementation is already as good as is, and also contains a good alias table implementation if you want multi-symbol inputs: https://github.com/rygorous/ryg_rans/blob/master/rans_byte.h reply joshka 15 hours agorootparentprevI just added your blog to my RSS reader and noticed that your blog title in the RSS is a bit weird (in case you were unaware): Posts on A blog https://mbuffett.com/posts/ Recent content in Posts on A blog reply crdrost 18 hours agorootparentprevIt's also that PGN allows you to encode illegal moves, which is important if your dataset contains over-the-board games. That you only got to about 10-12 bits per move is actually kind of sad in a way, because it means you're not doing substantially better than the approach where you record a move as just (delete-piece-at ____, recreate-that-piece-at ____) 12-bit pairs, where castles are implicitly only recorded by moving the king further than usual and underpromotion has to be explicitly recorded with an extra 12-bit move that is otherwise semantically impossible. reply lifthrasiir 10 hours agorootparentSuch games should be infrequent enough that you don't need to optimize for them. A single unused code (or for arithmetic coding, a single symbol with a very low but non-zero frequency) can be used as an escape for a full non-optimized move format. reply VikingCoder 12 hours agoparentprev> At game start, that will be an integer between 1 and 20 (16 different pawn moves, 4 different knight moves). And offer draw. And resign. Just to complete that thought. reply ChrisArchitect 20 hours agoprevRelated follow-up from same author: Compressing Chess Moves Even Further, To 3.7 Bits Per Move https://mbuffett.com/posts/compressing-chess-moves-even-furt... reply stevage 13 hours agoparentAwesome. Amusingly they didn't actually mention the final move size anywhere except the title. reply nmilo 20 hours agoprevNot sure the author's use case the naive solution seems like just using 6 bits for the piece's original position, 6 bits for the destination, and 2 bits to designate promotion type, for 14 total bits. Everything else can be found from the board state, and I can't imagine a scenario where you have a move, want to convert to algebraic notation, and also don't know what the board is. reply jprete 18 hours agoparentI had very similar thoughts but I think one can do much better, even. Each move only needs four bits to identify the piece because each side only has 16 pieces at maximum. The game replayer would need to keep track of each pawn's starting file though (including after promotions). Variable-length encodings of the move options help a lot. Pawns only need two bits because there are never more than four legal moves for a pawn - except for promotions, but just handle those specially for those exact cases (if a pawn moves to the last rank then encode the promotion with two bits, otherwise encode the next move). Knights and kings each need three bits for the move - encode each king-castling option as a move to the appropriate rear diagonal (normally off the board so not otherwise legal). Bishops and rooks need four bits (rank and direction). Queens only need five (three bits for rank, two for direction). This way you can get down to between six and nine bits per move. reply amluto 18 hours agorootparent> The game replayer would need to keep track of each pawn's starting file though (including after promotions). That seems unnecessary. Don’t index the pieces based on their type and where they started — index them based on their current location. So the piece the lexicographically first (rank, file) is piece 0, and 4 bits trivially identifies a piece once you know the color. reply jprete 18 hours agorootparentTrue, I hadn't thought of that. reply andruby 19 hours agoparentprevI was thinking the same thing. Often both squares will be close to each other, so I'm sure that a generic compression (eg zstd) can even reduce it even further by taking advantage of that proximity. reply groby_b 18 hours agoparentprevThe problem is that the board state requirement makes searching for specific moves hard. But if you allow board state, you can do less than 14. 3 bits for the type of piece, 6 for the destination, 2 extra bits. (Promotion if destination is last/first line, disambiguation for pawns in en passant) You're still at about twice the theoretical minimum, I think. I vaguely recall an upper bound of 6 bits? reply Karellen 20 hours agoprevIt's kinda complex. I think my initial stab would be to encoding the source position (6 bits) and end position (6 bits) for a constant 12 bits per move. You don't need to store whether it's a capture, you can figure that out from the game state. You don't need to store disambiguations, there are none in that format. You don't need to store check/mate, you can figure that out from the game state. The only wrinkles (that I can tell) are castles and promotions. But you can get around this by the fact that kings and pawns have limited movement, so their legal destination squares are highly constrained. So you could signal a promotion by encoding the destination with opposite rank, and using the file to encode which piece. Promoting to a rook on c8 gets its destination encoded as a1 - \"a\" for a rook, and \"1\" to indicate a promotion. Similarly, you could encode a castle by encoding the king's move with opposite destination rank. Castling the king to f1 gets encoded as f8, and to g1 as g8. reply tzs 15 hours agoparent> Similarly, you could encode a castle by encoding the king's move with opposite destination rank. Castling the king to f1 gets encoded as f8, and to g1 as g8. You are overcomplicating this. For castling just record it as the King's source and destination. E.g., White kingside castling is e1g1, White castling queenside is e1c1, Black castling kingside is e8g8, and white castling queenside is e8c8. All king moves other than castling move the king at most one file, so when you see a king on e1 and the move is e1g1 or e1c2 which is a move of two files you can infer that it must be castling. For promotion, I suggest splitting it into two cases: promotion to a queen and promotion to something else. I saw a post once on Reddit from someone who analyzed promotions from all games in the Lichess database, and 98.7% were to queens, so we'll make that case the simplest. I suggest that pawns that promote to a queen are simply recorded as moving to the promotion square. It is implicit that they promote to a queen. For example a pawn at b7 that moves straight forward and promotes to a queen would be recorded as b7b8. A pawn at b7 that captures on a8 and promotes to a queen would be recorded as b7a8. For pawns that promote to a rook, record the move as a move to a square one rank back from the promotion square. For example b7b8 promoting to rook would be recorded as b7b7, and b7a8 promoting to a rook would be recorded as b7a7. Similarly for promotions to bishops. Record the destination two ranks back. So b7b8 promoting to bishop would be b7b6. Similar for knights but three ranks back, so b7a5 for a pawn at b7 that captures on a8 and promotes to a knight. reply Karellen 14 hours agorootparent> All king moves other than castling move the king at most one file, so when you see a king on e1 and the move is e1g1 or e1c2 which is a move of two files you can infer that it must be castling. Yeah. For some reason I had a brain fart and thought that the two castles moved the king 1 and 2 files, instead of 2 and 3 files, and that made me think you needed to disambiguate a 1 file castle with a 1 file move. Which is clearly dumb. I blame insufficient coffee. reply kuboble 4 hours agorootparentBoth O-O and O-O-O castles move the king by two squares. It gets more interesting if you want to also store fisher random chess, because the casting can involve a king move by any number of swuares or not moving at all. But even in this case the casting could be unambiguously recorded as a move by 2 squares(even if the king doesn't move at all) reply porkbrain 19 hours agoparentprevAs for promotion: in a valid chess position you could promote a single pawn at three files simultaneously. One by moving forward and then left and right capture. Additionally, you could have two pawns promoting on the same square, again with capture. And you can have a combination of both. Encode a white's move exd8=N with white pawns on e7 and c7 and three black queens on f8, d8 and b8. reply Karellen 19 hours agorootparent> you could promote a single pawn at three files simultaneously. One by moving forward and then left and right capture. Ooh, didn't think of that, thanks. Still there's enough constraint in the legal destination squares to work around that. There's at least half the board that's inaccessible to a pawn about to promote, which should be enough to encode any of the 3 legal destination squares and 5 possible promotion targets. Edit: maybe keep the destination file as-is, and use the destination rank to encode the promotion piece? > Additionally, you could have two pawns promoting on the same square The source square should disambiguate between those though, right? reply nmilo 19 hours agorootparentprevYou can encode a promotion as the destination file plus rank 1-4 for the 4 different kinds, since pawns can't move backwards reply fsniper 13 hours agorootparentprevPromotions could be compressed to Source position -> 6 bits destination position as ( forward left / forward / forward right -> 2 bits Target piece info Queen/Bishop/Knight/Rook 2 bits ) reply sdenton4 19 hours agoprevUse StockFish to predict the next move, and only store the diff between the actual move and the prediction. Or, better, get a ranked list of all possible moves from stockfish, and use a variable-length integer to encode the position in the list. Then the best move takes ~1 bit, and worse moves take more bits. (And we can do fun things like compute how bad a player is by how big their game is after compression.) reply Retr0id 19 hours agoparentThe more advanced your predictor is, the slower your compressor gets. OP has 600 Million moves to encode, how long does it take to ask Stockfish for its opinion on 600M board states? (and then again, every time you want to decompress) (not a rhetorical question btw, I know little about chess engines) I suspect the sweet spot here would be to use a much worse chess engine for the predictions, giving faster compression/decompression at the expense of the compression ratio. reply marcusbuffett 18 hours agorootparentYour intuition here is right on the money. I wrote a subsequent post with pretty much that exact approach: https://mbuffett.com/posts/compressing-chess-moves-even-furt... reply gus_massa 19 hours agoparentprevI almost agree, but as other comment says, it's probaby better to use the StocFish points to generate a probability and then use Huffman encoding. reply Retr0id 19 hours agorootparentYou'd probably want to use something like CABAC rather than Huffman, since it deals better with dynamic probabilities. https://en.wikipedia.org/wiki/Context-adaptive_binary_arithm... reply gus_massa 16 hours agorootparentIt looks like a very interesting comparison. I still not sure how to tranform StockFish points to probabilities. (Perhaps proportional to exp(-difference/temperature), where temperature is a magical number or perhaps it's like something/elo??? There are too many parameters to tweak.) reply Retr0id 16 hours agorootparentGiven OP's dataset, you could probably figure out how to map them, experimentally (e.g. by making a histogram) reply gus_massa 13 hours agorootparentI was imagining downloading the lichess database and trying different strategies to compress it, It's too much work for me, but I'd love to read a blog post if somsome does that. reply V-2 14 hours agoparentprev> Use StockFish to predict the next move, and only store the diff between the actual move and the prediction. This ties the algorithm down to one specific version of Stockfish, and configured identically (stuff like the hashtable size etc.), because all such factors will have an impact on Stockfish's evaluations. One factor changes, and you can't decompress the backup. reply teo_zero 3 hours agoprevIt depends what the goal is. If i's to display moves to humans, then this approach seems correct (not sure if optimal). It doesn't require implementing a full chess engine just to code/decode a move, as others have suggested. If the goal is to replay the moves, instead, you need to keep track of the position anyway. In this case I would suggest a simple scheme: piece-destination. Each player only has 16 pieces at most at any moment in a game, so 4 bits will suffice. Each piece in a given square can only move to a fixed number of destination squares. A Queen can move to 27 other squares (7 horizontally, 7 vertically, 13 diagonally when in one of the four central square). All other pieces have less freedom: Kings have 8 natural moves, plus 2 castles; Pawns have 2 captures, 2 normal moves forward, and 4 possible move-and-promote; and so on. So the 27 for a Queen is the upper limit, and that needs 5 bits. In total 9 bits per move. Unfortunately that's more than one byte, so you still need some bit-wise processing... reply sshine 20 hours agoprevRelated, compressing bingo cards: https://github.com/diku-dk/openbanko Among other things, contains: Theoretically optimal compression of excessive amounts of bingo cards GPU-accelerated massively parallel bingo simulation using Futhark I suspect the research was triggered by a late professor of Computer Science who unironically calculated how many (Danish) bingo cards there are: https://sprutskalle.dk/blog/wp-content/uploads/bankoplader.p... -- to see the mathematical rigour played out on such a casual question is nothing but inspirational. reply sfeng 20 hours agoprevI would love to see this benchmarked against just running LZW compression on a list of the moves. I think the repeated nature of the moves means the entropy is actually rather low. It should be possible to come up with a compression dictionary which can save a lot more space than just a tight packing like this. reply vardump 19 hours agoparentLZW would do very badly on a list of binary chess moves. Huffman (and even more so arithmetic encoding or ANS (asymmetric numeral systems [0]) would be significantly better, if you're careful how you encode data. [0]: https://en.wikipedia.org/wiki/Asymmetric_numeral_systems reply kondro 20 hours agoparentprevOr a trained zstd dictionary. reply sedatk 19 hours agoparentprev> the repeated nature of the moves On a single game? I don’t think the goal is to pack all plays together and unpack them every time. Every game must be individually accessible as I understand. reply billforsternz 10 hours agoprevI went through this process many years ago and documented it here https://triplehappy.wordpress.com/2015/10/26/chess-move-comp... Basically I demonstrate how you can compress to much less than one byte per move, but settle instead on a one byte per move scheme that is also very performant, something you'd have to sacrifice for optimal compression. I used this one byte representation to good effect in my chess GUI Tarrasch https://triplehappy.com reply lifthrasiir 9 hours agoparentThere was also a lengthy off-topic discussion about the compressed board representation this year [1]. [1] https://news.ycombinator.com/item?id=39065595 reply devit 9 hours agoprevIf you actually want good compression, this is definitely not the proper way to go. The proper way to go is to compute all available moves on a given position, assign them a probability distribution and then perform arithmetic coding using it. If you want simplicity, assign an uniform distribution. For optimal compression, use an engine such as Stockfish to evaluate how strong the moves are, and then apply a statistical model that converts move strength to weights to make a probability distribution for; also use an opening books for the openings and tablebases for the endgames. My wild guess is that it will probably result in something like 3-4 bits per move on average. If you instead want a simple encoding, then encode in 4 bits the piece that moved based on any order on the chessboard squares, then in 6 bits the destination square for 10 bits per move. reply aj7 14 hours agoprevPosition to position. 12 bits. Plus occasionally, an extra 12 bits for promotion. Promotion found by impossible position prefix. Whether it’s a capture, check, castle, what piece, etc. all interpreted by a reader program. What am I missing here? reply Dave_Rosenthal 13 hours agoparent+1 You're not missing anything. And there are plenty of very simple ways to just use a few \"special destinations\" to encode any move including possible promotions in a constant 12 bits per move and be very fast to encode/decode. The proposal in the article is just silly. But really the whole thing is silly. 100 million games is a lot of chess, but at 1KB per \"inefficient\" PGN you are talking a whopping 100GB--big deal. (At 12 bits and, say, 80 half-moves on average, you are talking ~12GB.) Plus the article says that he is \"IO-constrained\" but shrinking game size isn't going to help with random lookups--a PGN is already below the page size. If you really did care about best compression you would simply assign all legal moves an index and use a heuristic (chess engine) to figure out which moves were likely (i.e. good). In chess, it's not uncommon that good players are usually picking between only a few reasonable candidate moves. I wouldn't be surprised if good compression of human games yielded something like 2-3 bits per move. reply robrenaud 17 hours agoprevOnly tangentially related, but I think the mid/late game is a much more interesting/useful/hard avenue for chess tools. Chess.com's opening explorer is pretty good, which it seems like this is kind of trying to compete against. What I struggle with is the post game review, where I make a dubious move but I don't actually understand the reason why it's bad. Sure, the engine shows better moves and the refutation to my move, but I often don't know why. The automatically generated explanation is usually pretty poor. I wish I had even a poor copy of Danya giving me commentary. Perhaps I should just lean in and try to implement something like this, but focus it more on coaching than commentary. https://arxiv.org/pdf/2212.08195.pdf reply V-2 14 hours agoparentI think that AI, overhyped as it may be, will truly get to shine when it comes to this aspect of chess - basically, tutoring. reply jaryd 15 hours agoparentprevYou could check to see if one of his games has that position using https://fenfinder.com (site I built). Of course it's a long shot given the huge possibility-space of unique positions, but hey, one can hope :). reply V-2 14 hours agoprevPGN is a highly redundant format, but it has the inherent advantage of being human readable. The problem is interesting, but I think it falls on the side of \"fun\" more than \"profit\". Storage is cheap, and PGN files are still small. An average PGN is still below 1 kilobyte. So one movie in BlueRay quality = about 20 million games. That's a lot. The practical problem is not storage, it's computation. Basically, querying the game database quickly. Compression gets in the way of that. For example, I've just played a game, now I want to go through the opening and fetch all games from the database that went through the same initial moves/positions (that's not the same thing, as a game may arrive at the same position through a different order of moves; AKA transposition). Let's say, all the way until move 15 or 20, because it will only be at that point that a decent game finally becomes unique by deviating from all the recorded games in the database (AKA a novelty was played). Or I want to find all games where an endgame of a Queen and a pawn against a lonely Queen occurred. There is actually a query language for that, named (surprise, surprise) Chess Query Language: https://www.chessprogramming.org/Chess_Query_Language I feel that whatever a superior alternative to PGN might be, its strength would likely be better queryability rather than higher storage efficiency as such. reply marcusbuffett 12 hours agoparentThe problem I’m facing with storing roughly 600 million shorter PGNs is that the database is 100GB or so, and I’m grabbing thousands of them sort of at random. This makes the query IO bound, even though the finding the pages they’re on is virtually instant with the indexes. So a smaller database means less pages read when I do these large reads, ideally. I also have other ideas on ordering the database in a smarter way, but hoping this part helps. reply V-2 3 hours agorootparentSure, I see your point. Obviously a wasteful format is also getting in the way of queryability. My point is that the main goal should be to improve for queryability, which inherently requires some optimizing for storage, but that's secondary. As opposed to optimizing exlusively for data size. Because in the former case it may still be best to accept some compromise (in the form of redundancy/simplicity) to hit the sweet spot. Especially in the context of many comments that seem to have taken an extremely \"code golf\"-like approach towards the problem. reply dhsysusbsjsi 15 hours agoprevWhat I haven’t seen discussed here much is optimising for developer maintenance. I think the author’s solution is great for this; it’s easy to understand and each move has a bit pattern. Easy to debug. Somebody taking over in the future will understand this compression. If on the other hand you can squeeze another 10% storage from Huffman encoded inverse tree lookup tables that only neckbeards understand, you’re limiting your pool of people able to do maintenance on this system in the future when the author is long gone. reply stevage 13 hours agoparentHopefully you just use a library for the Huffman tables and there's not much to maintain. reply ks2048 8 hours agoprevFunny, I was just working on this same thing last week. I ended up using 16 bits per move. As other comments here note, you could do it in fewer bits, but everything is a trade-off: with speed, size, and what the goal of your compression is. For me, fixed-size, two bytes per move seems good. Also, I don't bother with storing capture, or check, because those you can infer from the game state. Again, depends on how you be using the result - I just wanted to be able to re-play the game state. reply stevage 13 hours agoprevLichess's equivalent blog post, using an even fancier technique. https://lichess.org/@/lichess/blog/developer-update-275-impr... reply andy_threos_io 8 hours agoprevWith a better coding you don't need more than 8 bits for a move (less for some). The implementation should store a state machine for the game, a struct for each pieces, and also the board with index to the pieces (occupation). There is only 16 pieces for each player, the two players move one another. The Naive would be that you index the pieces (4 bits) and store the movement after that, Pawns 2 bits for movement (2 forward, 2 diagonal) King 4 bits for movement (normal move 1 + 3 bits, castle 1 + 1 bits) Knight 3 bits for movement (8 possible move) Rook 4 bits for movement ( 1 bit orientation horizontal/vertical 3 bit new position ) Bishop 4 bits for movement ( 1 bit orientation left/right diagonal 3 bit new position) Queen 5 bits for movement ( 2 bit orientation horizontal/vertical/ left/right diagonal 3 bits for new position) This way you need Pawn 4 (index) + 2 (movement) = 6 bits King 4 (index) + 4 (movement) = 8 bits Knight 4 (index) + 3 (movement) = 7 bits Rook, Bishop 4 (index) + 4 (movement) = 8 bits Queen 4 (index) + 5 (movement) = 9 bits But you can encode the pieces index in another way, that the Queen got 3 bit index and other pieces got longer index (Pawn 5 bits) index bits + movement 000 + 5 movement Queen -> 8 bits 00100 + 3 movement King Normal -> 8 bits 00101 + 1 movement King castle -> 6 bits 0011x + 3 movement Knight (2) -> 8 bits 01xxx + 2 movement Pawn (8) -> 7 bits 10x + 4 movement Bishop (2) -> 7 bits 11x + 4 movement Rook (2) -> 7 bits Also other bit allocations are possible, I don't know if it's worth it to make the Pawns index 1 bit longer, but in this way the max 8 bits required for a movement. (You need the state machine for the \"decompression\", also have to track that the black and white Pawns are moving in the opposite direction. ) There can be further compression, if you check the possible move for the piece. ex. the first possible Knight movement is only 1 place, so no need to store the movement. But this need more calculation. reply evertedsphere 20 hours agoprevSome thoughts I had while reading this, probably not very original: the fact that chess moves are not random and usually adhere to opening theory to some degree means you could get some value out of using something like a trie of some small finite depth, trained on a database of games, to compress common opening lines much better, at the cost of making things like 1. a4 a5 2. h4 h5 that aren't in the trie much costlier to represent. A practical way to actually do this would likely look like applying arithmetic coding to all of the possible length-N prefixes of the game. The mid- or lategame are also far from random and could probably be compressed in small \"chunks\" via a coder that would (effectively) learn to predict patterns of e.g. capture being followed by a recapture and cause such chunks to require fewer bits to represent. I'm not very knowledgeable about compression algorithms, though; I'm sure others will be able to provide corrections or references. reply andruby 19 hours agoparentThat's exactly what the author seems to have done and describes in their subsequent post: https://mbuffett.com/posts/compressing-chess-moves-even-furt... (published yesterday) reply Spark_Ed 19 hours agoparentprevYou can reference specific move numbers from specific master games to compress. But you still need to compress how you store those master games. What you're describing for mid/late game might make more sense like this: you use a fixed version of stockfish to suggest moves (depth and version should be locked to static values so it reproduces the same engine move each time). If it's within the first 8 suggestions, you can flag that it's an engine move with 4 bits. Decompression time is significantly larger, but it maximizes the space for cpu trade-off. reply andjd 18 hours agoprevI would be curious how this compares to a more-or-less off-the-shelf text compression algorithm like gzip. My guess is that over the entire database, this would be more efficient than the OP's ad-hoc implementation or any alternative mentioned here. reply ska 16 hours agoparentUnlikely. Gzip and the like will do well with getting rid of the inherent redundancy of ascii coding but it's a general algorithm and can't take advantage of known structure. reply pimlottc 18 hours agoparentprevThat’s my first thought as well. Plain old gzip should do pretty well and provides a baseline to beat. reply adrianmonk 15 hours agoprev> I run a site that stores a ton of chess lines, something like 100 million in total. Because there are so many, one approach would be to sort the list of lines lexicographically to group similar ones together, then compress the result with a sliding window compression algorithm. The sliding window compression will avoid storing repeated parts a second time, and the first part of the lines will be repeated a lot because of the sorting. There may also be some other repeated sequences. This assumes that it's OK to sort the lines, though, which might or might not be true. reply bee_rider 18 hours agoprevThinking about this has made me realize I really don’t know anything about compression other than which arguments to pass to tar to get it to compress, haha. How would a typical compression algorithm do, losslessly compressing the whole game? Can you lossily compress the game? If so, you would end up, after decompressing, with a game that had some ambiguous moves, right? But, only one is valid all the way through. Why not lossily compress and then step through each game, checking if each move is valid? Is that even still considered lossy? Hypothetically you could end up with multiple valid games I guess… but they are just games, haha, who cares if a few are wrong? reply ska 16 hours agoparentIt's not lossy if you can always recover the correct data. Rather than use a general algorithm, your compressor would have to reason: I'd have to spend a bit to disambiguate x & y, but only x is a valid board state, so I won't bother and the decompressor also knows I won't bother so it will all work out. This sort of implicit coding can work, but it is fragile. reply zzo38computer 13 hours agoprevI had my own idea compressing chess moves, using one byte per move, because I think the number of legal moves can never exceed 256. Further compression may be possible due to the number of legal moves may be much less, so some of the previously numbers may be used for common sequences of moves. (It would also be possible to use some of the unused numbers to indicate e.g. resigning, agreement of a draw, and unfinished games.) reply zitterbewegung 19 hours agoprevRight now I think you could compress chess games for fun and profit . Since chess databases have come along higher level chess players can look pretty far ahead. We are reaching the point where if you study your opponent you can pregame it but studying what openings they make and prepping for openings. But now we have come to the point if you can memorize large portions of chess databases you know optimal play given the database because if anyone has played the game and gets any advantage out of it people will play the line. It would be interesting to take the chess databases see their compression ratio and then how long would it take to lose against swordfish. reply velcrovan 20 hours agoprevWas discussed recent-ish-ly (Sep 2023) — this was positions, not moves: https://news.ycombinator.com/item?id=37525348 Site linked there seems to be offline, so here's the archive copy: https://web.archive.org/web/20230928072950/https://www.ezzer... reply infogulch 18 hours agoprevHas anyone tried columnar compression of full game states instead of bit packing moves? 64 squares * (Empty(BlackWhite) * (KingQueenBishopKnightRookPawn)) + (BlackWhite) * (Can castle queensideCan castle kingside) It could be encoded as 64 4-bit int columns + 4 bool columns = 260 bits, most of which don't change between moves. Normal columnar encoding and compression strategies could probably reduce that to a few bits per board state. reply marcusbuffett 18 hours agoparentI have actually, alongside my work of compressing moves. My compression scheme averages about 150 bits per position, or about 35% of the standard text notation in EPD format. The thing I optimized for is that there’s very often repeated blank spaces or repeated pawns on the same rank. Also instead of storing castling status separately, it’s stored as a separate piece on the appropriate rook. These take advantage of the fact that there’s 6 pieces, so 3 bits to encode them leaves two options remaining. One is PawnRepeating and the other is RookCastleAvailable, in my scheme. There’s probably improvements to be made. I’ll write a post on it when it’s finalized. reply infogulch 17 hours agorootparentUsing the extra available piece indexes for \"rook that can castle\" and \"pawn that moved two squares\" is a great idea. So e.g. moving the king would not only change the squares where the king moved from and to, but would also convert both of the rooks from \"rook that can castle\" to \"rook that can't castle\". Same for a pawn that moved two squares and can be captured by en passant. I guess you also need some way to encode which player's turn it is. Though maybe you could eliminate that by flipping the board on black's move and always encoding from the perspective of the last player's move? I'm curious about whether a naive columnar encoding scheme could beat a more complex encoding scheme, after compression. Not columnar in the sense of ranks and files, but columnar in the sense of data science storage formats (e.g. parquet), where each 'column' is the state of a specific square across different board states. Given 64 such columns, a game state is a single row. The hypothesis being that if you're encoding all the game states of a large number of games (say 1000), after RLE and compression etc you would see a net compression better than more complex encoding schemes. Given a big block of game states like this, then a single 'game' would be a list of offsets into the game states. This would probably also compress very nicely in columnar format. Now I want to try it... reply srameshc 20 hours agoprev> If you're into chess, I've made a repertoire builder embeds the link chessbook.com, It's a great way to learn and practice chess. reply stouset 11 hours agoprevI feel like zstd trained on a randomized sampling of games from the database would come very close to (or better than) this with nearly zero effort. reply bluedino 19 hours agoprevAnyone know of a discussion of say, 8 bit chess computers and their techniques and tricks? reply omoikane 18 hours agoparentIs this what you are looking for? https://news.ycombinator.com/item?id=36431917 - Video Chess disassembled and commented (2023-06-22) https://nanochess.org/video_chess.html Video Chess for Atari 2600 worked with just 128 bytes of memory. reply davguerrero 20 hours agoprevFor optimal compression, look into Huffman coding: https://en.wikipedia.org/wiki/Huffman_coding reply marcusbuffett 18 hours agoparentI mention Huffman in my follow-up post: https://mbuffett.com/posts/compressing-chess-moves-even-furt... , but chose arithmetic encoding instead. reply andruby 19 hours agoparentprevI think Huffman would be good, but I don't think it would be optimal (as in, the smallest possible amount of bits) reply canucker2016 17 hours agorootparentHuffman requires minimum one bit IIRC whereas Arithmetic coding or ANS ( https://en.wikipedia.org/wiki/Asymmetric_numeral_systems ) can use fewer if there's a skewed distribution. Arithmetic coding has been avoided in practical situations due to patents, but many of those have expired ( see the patent section of https://en.wikipedia.org/wiki/Arithmetic_coding or maybe not, lawyers tend to advise not even reading about patent specifics ) reply ska 16 hours agorootparentThe core tech patents have expired. They did have some interesting ones around hardware implementations, iirc. Overall the chilling effect was interesting - I found that many people doing academic work around compression didn't really know much about it, didn't teach it, just because it was a mess. The core idea is elegant, and easily implemented. reply modeless 19 hours agoprevI wonder how this compares to using zstd with a dictionary. reply DrNosferatu 13 hours agoprev [–] Just Huffman code the text. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Utilizing a compact encoding system for chess moves can lead to substantial space savings and quicker query performance in databases.",
      "The efficient encoding and decoding process not only reduces the size of the database but also enhances computational efficiency, offering a practical approach to streamline chess move storage."
    ],
    "commentSummary": [
      "The debate focuses on optimizing data storage and compression methods for chess moves by implementing various techniques like Huffman encoding, Zobrist hashing, and bloom filters to reduce bits per move and enhance encoding efficiency.",
      "Strategies include using AI, Stockfish predictions, and compression algorithms such as trie and arithmetic coding for effective storage and querying of chess data, including moves, promotions, and castling.",
      "Varied compression schemes are explored for analyzing, studying, and training in chess, emphasizing efficient compression with minimal data loss to enhance overall performance."
    ],
    "points": 167,
    "commentCount": 130,
    "retryCount": 0,
    "time": 1710520531
  },
  {
    "id": 39721158,
    "title": "US Launches First Major Offshore Wind Farm, Signaling Industry Growth",
    "originLink": "https://apnews.com/article/orsted-offshore-wind-new-york-south-fork-climate-cbb9360388d91be1368dd91ba35aa384",
    "originBody": "1 of 2FILE - The first operating South Fork Wind farm turbine, Thursday, Dec. 7, 2023, stands east of Montauk Point, N.Y. South Fork Wind, America’s first commercial-scale offshore wind farm, is officially open. (AP Photo/Julia Nikhinson, File) Read More 2 of 2FILE - The first operating South Fork Wind farm turbine, Dec. 7, 2023, stands east of Montauk Point, N.Y. South Fork Wind, America’s first commercial-scale offshore wind farm, is officially open. (AP Photo/Julia Nikhinson, File) Read More By JENNIFER McDERMOTT Updated 11:17 PM UTC, March 14, 2024 Share Share Copy Link copied Email Facebook X Reddit LinkedIn Pinterest Flipboard Print America’s first commercial-scale offshore wind farm is officially open, a long-awaited moment that helps pave the way for a succession of large wind farms. Danish wind energy developer Ørsted and the utility Eversource built a 12-turbine wind farm called South Fork Wind 35 miles (56 kilometers) east of Montauk Point, New York. New York Gov. Kathy Hochul went to Long Island Thursday to announce that the turbines are delivering clean power to the local electric grid, flipping a massive light switch to “turn on the future.” Interior Secretary Deb Haaland was also on hand. Achieving commercial scale is a turning point for the industry, but what’s next? Experts say the nation needs a major buildout of this type of clean electricity to address climate change. Offshore wind is central to both national and state plans to transition to a carbon-free electricity system. The Biden administration has approved six commercial-scale offshore wind energy projects, and auctioned lease areas for offshore wind for the first time off the Pacific and Gulf of Mexico coasts. New York picked two more projects last month to power more than 1 million homes. This is just the beginning, Hochul said. She said the completion of South Fork shows that New York will aggressively pursue climate change solutions to save future generations from a world that otherwise could be dangerous. South Fork can generate 132 megawatts of offshore wind energy to power more than 70,000 homes. READ MORE New York City won’t offer ‘right to shelter’ to some immigrants after 30 days in deal with advocates Feds pick New England’s offshore wind development area, drawing cheers and questions alike Marriages in the US are back to pre-pandemic levels, CDC says “It’s great to be first, we want to make sure we’re not the last. That’s why we’re showing other states how it can be done, why we’re moving forward, on to other projects,” Hochul told The Associated Press in an exclusive interview before the announcement. “This is the date and the time that people will look back in the history of our nation and say, ‘This is when it changed,’” Hochul added. South Fork will generate more than four times the power of a five-turbine pilot project developed earlier off the coast of Rhode Island, and unlike that subsidized test project, was developed after Orsted and Eversource were chosen in a competitive bidding process to supply power to Long Island. The Long Island Power Authority first approved this project in 2017. The blades for the 12 Siemens Gamesa turbines reach speeds of more than 200 miles per hour (350 kilometers per hour). Ørsted CEO Mads Nipper called the opening a major milestone that proves large offshore wind farms can be built, both in the United States and in other countries with little or no offshore wind energy currently. With South Fork finished, Ørsted and Eversource are turning their attention to the work they will do offshore beginning this spring for a wind farm more than five times its size. Revolution Wind will be Rhode Island and Connecticut’s first commercial-scale offshore wind farm, capable of powering more than 350,000 homes next year. The site where the cable will connect in Rhode Island is already under construction. In New York, the state said last month it would negotiate a contract with Ørsted and Eversource for an even larger wind farm, Sunrise Wind, to power 600,000 homes. The Norwegian company Equinor was picked for its Empire Wind 1 project to power more than 500,000 New York homes. Both aim to start providing power in 2026. After years of planning and development, 2024 is a year of action— building projects that will deliver sizeable amounts of clean power to the grid, said David Hardy, group executive vice president and CEO Americas at Ørsted. Ørsted, formerly DONG Energy, for Danish Oil and Natural Gas, started aggressively building wind farms off the coast of Denmark, the U.K. and Germany in 2008. The company sold off the North Sea oil and gas assets on which it had built its identity to focus on clean energy, becoming Ørsted. It’s now one of the biggest wind power developers. The first U.S. offshore wind farm was supposed to be a project off the coast of Massachusetts known as Cape Wind. A Massachusetts developer proposed the project in 2001. It failed after years of local opposition and litigation. Turbines began spinning off Rhode Island’s Block Island as a pilot project in 2016. But with just five of them, it’s not a commercial-scale wind farm. Last year brought challenges for the nascent U.S. offshore wind industry, as Ørsted and other developers canceled projects in the Northeast that they said were no longer financially feasible. High inflation, supply chain disruptions and the rising cost of capital and building materials were making projects more expensive as developers were trying to get the first large U.S. offshore wind farms opened. Industry leaders expect 2024 to be a better year, as interest rates come down and states ask for more offshore wind to meet their climate goals. The nation’s second large offshore wind farm, Vineyard Wind, is expected to open later this year off the coast of Massachusetts, too. The first five turbines are providing power for about 30,000 homes and businesses in Massachusetts. When all 62 turbines are spinning, they’ll generate enough electricity for 400,000 homes and businesses. Avangrid and Copenhagen Infrastructure Partners are the joint owners of that project. The Biden administration wants enough offshore wind energy to power 10 million homes by 2030. Interior Secretary Haaland said that “America’s clean energy transition is not a dream for a distant future— it’s happening right here and right now.” ___ The Associated Press’ climate and environmental coverage receives financial support from multiple private foundations. AP is solely responsible for all content. Find AP’s standards for working with philanthropies, a list of supporters and funded coverage areas at AP.org. JENNIFER McDERMOTT McDermott is a reporter on the Associated Press climate and environment team. She focuses on the transition to clean energy. twitter mailto",
    "commentLink": "https://news.ycombinator.com/item?id=39721158",
    "commentBody": "The United States has its first large offshore wind farm, with more to come (apnews.com)164 points by geox 15 hours agohidepastfavorite86 comments Etheryte 14 hours agoFor those surprised by this being the first, keep in mind that offshore wind farms are considerably more expensive than onshore wind farms, solar, etc. Many countries that have invested heavily into offshore have done so either because land is a premium or the geography makes in difficult to build on land. If you have land to spare, onshore wind farms can often be cheaper to both build and maintain. reply davis 13 hours agoparentIt's also partially self inflicted due to the Jones Act which means boats to do the work need to be made by the US. A special boat that has legs that makes installation possible is required to do the work but we can't just use foreign boats which are already made and designed for this kind of thing. https://www.wired.com/story/us-energy-offshore-wind-jones-ac... reply thinkcontext 13 hours agorootparentI remember for a previous wind project in US waters they were using a foreign vessel in construction. That made it necessary for it to go to port in Canada adding hundreds of miles and a ton of logistic problems. reply ianai 1 hour agorootparentYes, and so much could be addressed and fixed if the US Congress could commit to some acts of representative democracy. Laws could be written, laws could be changed, and many or all the seeming impasses could be legislated away-through acts of representative democracy. Something, something, people should politely write their representatives and otherwise fulfill their civic duties. reply jeffbee 11 hours agorootparentprevYep. You could have seen the towers of Vineyard Wind 1 in the port of Halifax recently. reply wcoenen 13 hours agorootparentprevThere are a bunch of different types of special vessels needed to build and maintain offshore wind: https://www.dco.uscg.mil/OCSNCOE/ORE-Support-Vessels/Types/ reply avar 13 hours agoparentprevMany countries that have invested heavily into offshore have done so either because land is a premium or the geography makes in difficult to buil on land. The Netherlands is investing heavily in offshore wind, and last I looked it's not those reasons, but NIMBYs. The cost per kW for offshore wind is higher, and even in a country as dense as The Netherlands the footprint of a windmill is relatively trivial, and can be combined with farming etc. But people don't like how they look and sound, so we're paying a premium to have wind installed offshore. reply gamepsys 12 hours agorootparentI think this is all a way of saying The Netherlands land is at a premium. Views and noise pollution are valued higher than cheaper electricity. The United States has over 200 times more land than The Netherlands, and only 20 times the population. The US has plenty of windy places with adequate infrastructure where selling electricity is valued higher than view/noise because no one lives within 30km of the windmill. reply dmix 11 hours agorootparent> I think this is all a way of saying The Netherlands land is at a premium. Artificially at a premium because of local government (and by proxy their upper midde backers). Or basically always why houses are expensive absent major population booms or extreme land shortages. So basically we can just call it \"land is a premium\" because people don't seem to care these days even though hypothetically people knocking on your door offering millions is very persuasive even to NIMBYs but that option is so out of reach for developers they don't even try to rally support (unless it's a mega project with people richer than the local powerbrokers). reply gamepsys 10 hours agorootparentThe land is expensive. The people that own the land have capital. The people that own the land leverage their position of authority to influence local regulations. This is all just part of the premium. reply sofixa 7 hours agorootparentprevI mean, if there are countries where one can legitimately say land is scarce, Netherlands is definitely one of them. Compare maps of the Netherlands today with 100 years ago and you can see they've done tons of land reclamation and draining projects. And it wasn't for fun, it was because they needed the land for agriculture and human settlement. reply Vinnl 5 hours agorootparentAnd population density is way higher than most places in the US. reply looofooo0 4 hours agorootparentSo they use less Land for housing! reply Vinnl 3 hours agorootparentHaha, per person, yes, but even then I'm fairly sure that a way larger percentage of land is used for housing than it is in most areas in the US. As in, you really can't travel in a straight line for very long without encountering man-made structures anywhere in the Netherlands. reply CPLX 11 hours agorootparentprevSeems ironic that the last generation of windmills is an iconic image of the country. reply andsoitis 11 hours agorootparentprev> Views and noise pollution are valued higher than cheaper electricity. Nothing like walking on the beach to relax and seeing wind farms or oil rigs off the coast. reply usrusr 1 hour agorootparentOr ships passing by on the horizon, or aircraft painting contrails in the sky. Really terrible to have a sight line to evidence of human existence. And all those fields, vineyards, orchards and meadows humans have created in place of native forests! Unbearable! reply mechagodzilla 11 hours agorootparentprevI love walking on the beach, and I can’t imagine seeing a tiny structure miles off in the distance somehow detracting from the experience. reply dukeyukey 10 hours agorootparentprevI loved for years on the coast with wind farms in the distance, I quite liked it. Like living in a solar punk town. reply saagarjha 4 hours agorootparentprevI went to college on a beach town. The oil rigs in the distance was a fixture of the landscape and generally referred to positively (even by those who were no fan of the oil and gas industry.) I can only assume wind farms would be even cooler! reply exodust 5 hours agorootparentprevInteresting the official Ørsted promo video about South Fork Wind is 3 minutes long, and contains no more than 8 seconds of footage showing the actual turbines. 8 seconds! Don't blink. The main focus is slow-motion shots of smiling people and kids. They really don't want any focus on how they look. https://www.youtube.com/watch?v=q5R_5Tzd5wQ reply chii 8 hours agorootparentprev> but NIMBYs a high rise is one thing, but having windfarms near residential areas is probably not great. they are quite loud. > But people don't like how they look and sound and they have the right to object. They should not have to pay a sacrifice without compensation. This is why that premium exists - it spreads the cost of that sacrifice (to whoever is going to consume the power generated). reply autoexec 12 hours agorootparentprevThere's been research done showing that people's view on how windmills look is most strongly influenced by how they feel about wind energy in general. I wonder if people in the Netherlands have disproportionately negative views of wind energy. As for noise, it probably varies a lot depending on how close you are, and the type of turbines in use so maybe it does bother some people (although not everyone feels that way https://www.theguardian.com/environment/2015/jun/26/minimal-...) but I know there's also a lot of fear around \"wind farm syndrome\" (https://en.wikipedia.org/wiki/Wind_turbine_syndrome) which I'd place in the same category as people worried about wifi and 5G signals (https://en.wikipedia.org/wiki/Electromagnetic_hypersensitivi...) making them sick and hopefully that can be overcome with education. reply Wohlf 12 hours agorootparentI wonder if it's more proximity that's the issue. In larger countries a wind farm would most likely just be an eyesore on the horizon or nearby hills, but in the Netherlands it could effectively be in someone's backyard, blocking the visible sky and casting large shadows. reply ben_w 3 hours agorootparentI went to university in Aberystwyth 20 years ago, and there were wind farms visible in the distance. As people were still complaining about them being eyesores back then, on one occasion I just scanned over the horizon, seeing what I could see — Those turbines were only noticed because they were unusual, they were greatly outnumbered by the unsightly clutter of pylons, the roads[0], and even particularly ugly farm buildings. [0] despite this being rural Wales where even the major road from there to anywhere else, the A44, was slightly narrower than an American residential street: https://maps.app.goo.gl/YitFrEFRt576Yz7K9?g_st=ic reply Fnoord 12 hours agorootparentprevGive the people who have to live near these machines a benefit from it, such as a monthly discount on electricity? Has this been tried? reply Propelloni 1 hour agorootparentSo called Bürgerbeteiligungsgesellschaften (citizen participation companies) are quite common in rural Germany. Basically a citizen of a concerned community can opt to become a shareholder of such a company and gets a yearly kickback from the proceeds. It's not quite that simple but that's the basic idea. The other form you find even more often is the so called Energiegenossenschaft (energy cooperative) which is organized like any other cooperative. The federal system of the FRoG promotes these local cooperatives over capital companies in certain areas, such as utilities. AFAIK, there is no model where you reap benefits without doing something first, ie. join a cooperative or buy shares. reply mattlondon 1 hour agorootparentprevIt is not just where people live. People like to walk and hike and camp and holiday places, and sometimes they don't like it if their view is \"ruined\". reply rootusrootus 12 hours agorootparentprevSort of like the people who are willing to have a cell phone tower put on their property. Ugly, but the rent more than makes up for that. reply pyuser583 11 hours agorootparentOr like oil - “black gold.” reply makomk 3 hours agorootparentprevAt least here in the UK (which has the second most offshore wind generation in the world behind China) it's both - onshore wind got stuck in NIMBY planning hell and there's not really enough land in the right places to make it an easy option. Also, it looked for a while like with sufficient investment offshore wind could actually wind up being cheaper due to higher capacity factor, larger turbines, etc. Frustratingly, a lot of the British press has been pretending that the NIMBY issues didn't exist and the current government just made them up for ideological reasons or because their super-rich donors didn't like wind turbines, along with presenting onshore wind as the solution to all our problems and essentially ignoring the offshore wind and pretending the government has done nothing on renewables. (This seems to happen with literally everything - whatever the current governemnt didn't do or cancelled is spun as a magic solution and any problems it was having before are written out of existence.) reply dukeyukey 12 hours agorootparentprev\"people don't like how they look and sound\" is just another effect of density. I doubt Dutch people are inherently more NIMBY than Americans, it's just there's nowhere to hide infrastructure in the Netherlands (unless offshore). reply KptMarchewa 11 hours agorootparentprevCost per kW might be higher, but offshore has higher capacity factor which is very important. reply raverbashing 6 hours agorootparentprevSea is much more windy than land as well reply jillesvangurp 3 hours agoparentprevThey are more expensive but still cost effective (i.e. cheaper than burning fossil fuels or nuclear power). And winds tend to be more predictable and stable on the ocean. So, the capacity factor is better. And you are right that off shore wind is a lot less controversial. More importantly, it's way less hassle and risk. There are no NIMBY's blocking it. There are no complex land ownership issues to resolve. Magic wands to make all that go away sadly don't exist. So, there's very little land where you can just decide to plop down a few hundred wind mills without getting bogged down in years of bickering with the locals, sorting out all sorts of complex issues, etc. Also, out on the ocean there are no roads to navigate so moving big/heavy things around is a lot easier to do. All you need is ships. Transporting a blade that is over a 100m long over land is hard and expensive. The infrastructure just isn't ready for that. Off shore wind mills can be bigger than onshore wind mills for this reason. Floating offshore wind is now also becoming a thing. This will unlock areas further away from the coast. Mass producing, mostly pre-assembled wind mills and shipping them out is a scalable business. Companies have been doing this for a few years now and it's working. They are getting good at it. They are making incremental improvements and learning ways to improve things. This learning effect is very real. There's no shortage of suitable spots to place these things. It's more a matter of how many GW do you need and how expensive is that going to be? reply codewench 11 hours agoparentprevAlso previous attempts[0] have been bogged down in endless red tape by groups against the creation of such projects. [0] see for example the Cape Wind project: https://en.m.wikipedia.org/wiki/Cape_Wind reply spiderice 14 hours agoparentprevWhy do it here then? We have a ridiculous amount of land where we could put solar farms, no? reply philipkglass 14 hours agorootparentThere are a couple of cases for offshore wind in the US: - To supply power to densely populated coastal urban areas, where there isn't a lot of free land for onshore farms nearby. - To provide some after-dark generation in areas where daytime solar power already surpasses the limits of demand. The second case gets weaker if there is room for onshore wind power in the same region, or if storage prices keep falling so that solar electricity can be used after dark. The second case gets a little stronger if a region's highest seasonal electricity demand occurs in winter instead of summer. reply xen2xen1 12 hours agorootparentOffshore wind farms can also slow down hurricanes. Studies have shown that a large wind farm could reduce a storm by a whole category level. Not a small thing. reply bilsbie 12 hours agorootparentWow I’ve heard of that. Do you have further reading I could check out? That’s actually worth more than the energy generation. reply theultdev 12 hours agorootparentprevnext [10 more] [flagged] Dylan16807 7 hours agorootparentHow far do you think nearly-horizontal wind is going to carry a huge piece of metal? Unless you think it's not going to sink pretty quickly after it lands? reply theultdev 7 hours agorootparentI don't know, do you know how far it can carry them? It can carry roofs of trailer homes over miles after ripping them off, I believe it can carry wind turbine blade multiple miles. Have you never heard of massive schools of fish, including sharks, being sucked up and carried miles away onto land? I think you underestimate the power of a hurricane. Regardless, it's a reasonable concern to have, is it not? It should be proven to be safe, not the other way around. Don't assume something isn't hazardous. reply Dylan16807 7 hours agorootparent> it can carry roofs of trailer homes over miles after ripping them off Yeah, across land with nothing to sink into. > Have you never heard of massive schools of fish, including sharks, being sucked up and carried miles away onto land? From a hurricane, not a tornado? I have not heard of that. reply theultdev 7 hours agorootparent> Yeah, across land. You do know hurricanes lose power when they hit land right? > From a hurricane, not a tornado? Tornadoes do. Hurricanes do as well. Why do you think there would be any difference? Hurricanes are generally more powerful. reply Dylan16807 7 hours agorootparentCite something. > Why do you think there would be any difference? The direction the air currents go. reply theultdev 6 hours agorootparent> Cite something. I was asking. I don't know what a hurricane will do to a wind turbine blade. You seem to be very sure, so I assume you have something to cite, could you link it? reply Dylan16807 6 hours agorootparentCite your claim about fish being sucked up and carried for miles. I don't know how I'm supposed to cite a negative. (I can easily find citations for tornadoes/waterspouts. I don't see any for hurricanes.) reply bretpiatt 11 hours agorootparentprevTLDR: This is a real thing, turbines have built in protection, and there innovating on the whole system to really do this. It's a potential game changer vs. trying to build seawalls or levees. Read more, \"Taming hurricanes with arrays of offshore wind turbines\", https://www.nature.com/articles/nclimate2120 ...and..., \"Wind Turbines in Extreme Weather: Solutions for Hurricane Resiliency\", https://www.energy.gov/eere/articles/wind-turbines-extreme-w... reply theultdev 10 hours agorootparentI'm aware of the current built in protections we deploy for land installations. Those prototypes for hurricane prone sea installations do look promising though, especially the downwind blades. Looking forward to seeing successful test data for these blades, it's certainly not going to be easy to withstand these forces. reply thinkcontext 13 hours agorootparentprevTo add some more points the others have made: - You can make the turbines substantially bigger offshore than onshore - There's better wind offshore - Both the previous ensure there is a higher capacity factor - There's a lot more flexibility in connecting to the grid. Grid accessibility is a huge challenge in siting onshore projects. You either need to be near an access point or conquer the immense, years long hurdles of constructing new transmission lines - The farms can have a large number of turbines which makes capital and labor more efficient reply Retric 13 hours agorootparent> higher capacity factor It's mostly the other bits. Modern US wind farms built between 2014-2020 have a 39% capacity factor, there's only so much room to improve upon it. That said different areas have different wind patterns and electricity's value depends on the time of day not just total kWh / year. reply WJW 14 hours agorootparentprevBecause offshore wind can be even cheaper than onshore solar, basically. It all depends on local grid conditions of course. reply pixl97 14 hours agorootparentprevBecause NY state isn't going to be the most efficient place to put solar farms. And running distribution long distances is also expensive and inefficient. reply rowls66 13 hours agorootparentNY state has plenty of room for onshore wind, just not in the places where most of the people are. reply KptMarchewa 11 hours agorootparentprevAnd shut down the country at 4PM in the winter. reply jeffbee 11 hours agorootparentprevIt's really an economic equation that you have to optimize for our moronic system of law. Do you want to go to war against Sea NIMBYs or Land NIMBYs? How much energy can you generate with a windmill weighed against how much it will cost to litigate against people who claim you are giving whales cancer. How much energy can you generate with solar panels and how long will it take to prevail over the people who say you are wrecking the habitat of the critically endangered San Benito Burrowing Amoeba. reply whazor 7 hours agoparentprevThe latest wind turbines at sea don’t require subsidies anymore. reply mc32 12 hours agoparentprevOne of the reasons they insist on offshore is that there they can transport very large blades via ship --but building in water is way more expensive and maintenance is costlier. In the continental US very large blades are difficult to transport (twists and turns on the way there) and there aren't planes capable of carrying such large bales to destination. I think there may be a start up trying to get one into certification. reply Arrath 10 hours agorootparentThere's a reason a good number of wind farms were build along the Columbia River Gorge, beyond it being a very windy place. Floating the turbine components upriver via barge shortened the last stretch of delivery that would have to be via trucks and eased the logistics considerably. reply superultra 11 hours agoprevBecause these are so reliant upon political capital, the biggest impediment to these things is public opinion. The problem here is that the bar is unreasonably high. If the solution doesn't solve climate change, it's useless. The bar isn't the status quo, which is going to kill off humans. But our solutions will never work like that. Short of not existing, the human race at the scale at which we exist will always have an impact. It's about decreasing and minimizing the impact at incremental, evolutionary levels, rather than \"solving\" climate change. Point being, this is good, but we'd be better off if we could just build on-shore wind farms. But...NIMBY, and an impossible bar. reply bobthepanda 10 hours agoparentAt least the offshore wind farms aren’t super far from population centers. There is a permitting issue for new long distance transmission. reply glial 10 hours agoparentprevThis is all true, but we need to start somewhere. reply tromp 4 hours agoprevCuriously, at a whopping 115-metre-long, the blades on the Siemens Gamesa wind turbine exceed by 10m the maximum length that can be transported by the Radia Windrunner blade-transport plane [2]. [1] https://www.designboom.com/technology/siemens-gamesa-longest...'. [2] https://news.ycombinator.com/item?id=39690182 reply hoerensagen 1 hour agoparentYes but it's an offshore turbine, so it can be transported by see. I think the plane is targeting onshore turbines. reply usrusr 1 hour agorootparentIt's an off-shore turbine only because the blades are too big for ground transport. Really feels like missing an opportunity not aiming higher with that purpose-built plane. reply evilos 4 hours agoprevStarted construction in Feb 2022, with an estimated cost of 16.3 cents per kWh. https://www.nortonrosefulbright.com/en/knowledge/publication... reply KoftaBob 1 hour agoprevKeep in mind this is the first large *offshore* wind farm in the US. When it comes to onshore wind power, the US is 2nd only to China in wind power production, and adding a ton of wind power capacity per year. https://en.wikipedia.org/wiki/Wind_power_by_country reply ta1243 1 hour agoparent> the US is 2nd only to China in wind power production Only because you could the \"US\" as a single entity without adjusting for area, population, or total generation. The US 29th in terms of electricity produced by wind, China is 32nd. reply ggm 14 hours agoprevNot wishing to entirely trivialise this, Ørstet energy used to be Dong energy and I cannot stop thinking the US investment market wouldn't buy in until a name change. Opposition to wind is growing from existing energy supply sources, because their income is under threat. Corporate power companies have zero social conscience. It's a huge problem in Australia where they combine with opposition political parties and amp up local opposition to \"ugly\" windmills. Wind power at sea is harder, and so more expensive than on land but has some advantages alongside all the marine disadvantages. Coastal wind is often very predictable. For every whale fan concerned they impede migration there's a lobster fishery looking at increased yield from spawning ground improvement. reply mike-the-mikado 13 hours agoparentI'm not sure on the point your making (phallic or Chinese sounding), but I hadn't realised that DONG energy was originally Dansk Olie og Naturgas A/S (DONG), meaning Danish Oil and Natural Gas. reply ggm 13 hours agorootparentIt's both. Anti CCP and the phallic probably combined to make it a harder sell to the US market. reply porkbeer 13 hours agorootparentor lean into it with Big Dong Energy marketing. reply exodust 5 hours agorootparentprevThey sold off their oil and gas business in 2017 for a billion dollars before the name change [wikipedia]. Obviously the 'ONG' in their name no longer represented their activities. The idea their former name would be offensive to Americans is highly unlikely! reply lostlogin 13 hours agoparentprevIt’s a bit depressing. Seeing how Germany and Austria cram solar panels into little patches of land all over the place is in stark contrast to New Zealand where I live. If they can make it work, surely we can? We are better placed for sun and the wind blows. We are mostly hydro already, I suppose that’s a plus. reply zizee 12 hours agorootparentThat's a pretty negative perspective. 87% of electricity in NZ is from renewable sources. NZ is a world leader, and should be lauded. What is the basis for your perspective that NZ is failing to \"make it work\"? NZ is so far ahead of Germany it's not funny (Austria is similar to NZ with 87%). https://en.m.wikipedia.org/wiki/Renewable_energy_in_New_Zeal... https://en.m.wikipedia.org/wiki/Renewable_energy_in_Germany reply lostlogin 11 hours agorootparent> What is the basis for your perspective that NZ is failing to \"make it work\"? Regarding solar! Not about renewables as a whole. Any perception that NZ is green is primarily due to other factors than environmental concern. Hydro was a good choice and it’s green, not because of it being green. We burn coal to make milk powder and trash our rivers to fuel the dairy industry. Recycling efforts are abysmal and there is a regressive attitude to greening transport. The high level of green energy is great, I just grow frustrated at the lack of progress. reply ta1243 58 minutes agorootparentHow would solar help with \"milk powder\"? Or recycling? Or transport? reply standeven 12 hours agorootparentprevMaybe it's similar to BC, Canada. They're mostly hydro powered, so they already have clean energy that's fairly cheap. As a result, the economics for solar never made much sense. However, with crazy-cheap panels and low hydro reservoirs due to climate change, solar now makes sense and it's appearing on more and more roof tops. reply jncfhnb 9 hours agorootparentRooftop is not very cost effective. It works for individuals because of awkward rules about what they owe for society maintaining their access to the larger grid. It’s much more expensive than a dedicated utility scale solar plant overall reply dukeyukey 12 hours agorootparentprevBC being extremely mountainous and relatively cloudy can't help either. reply danielovichdk 13 hours agoprev [–] A tragedy for a country that in many aspects are looked upon as a technological beacon. But the profits are low in green energy which is why the backing from governments are essential. The private sector will not save you on this one. The first....wow reply jncfhnb 9 hours agoparentNot correct. In the United States, utility scale solar and on shore wind has a lower average LCOE than anything else, including gas, without considering subsidies. Gas is still cheaper than an offshore but it’s close. The subsidies are to develop it faster And in more circumstances reply atonse 12 hours agoparentprevI would think it’s the opposite… once you invest in the equipment, then “generation” is essentially free isn’t it? As opposed to fossil fuels where you have to keep drilling and mining. reply jncfhnb 10 hours agorootparentThe buck stops at selling the energy to net a profit, not producing it reply standeven 12 hours agoparentprevI think that used to be the case, but more and more often I'm seeing distributed wind and solar/storage reported as the cheapest way to add power generation. reply KoftaBob 1 hour agoparentprev [–] Keep in mind this is the first large *offshore* wind farm in the US. When it comes to onshore wind power, the US is 2nd only to China in wind power production, and adding a ton of wind power capacity per year. https://en.wikipedia.org/wiki/Wind_power_by_country reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The first commercial-scale offshore wind farm in the US, South Fork Wind, is now operational off Montauk Point, NY, marking a significant achievement for the industry in combating climate change.",
      "Several other large offshore wind farm projects are underway, such as Revolution Wind in Rhode Island, Connecticut, Sunrise Wind in New York, and Empire Wind 1 project, indicating substantial industry growth.",
      "Despite obstacles like inflation and supply chain disruptions, the industry is flourishing, supported by the Biden administration's goal to provide offshore wind energy to 10 million households by 2030."
    ],
    "commentSummary": [
      "The discussion explores offshore wind farm development globally, contrasting advantages like higher capacity and easier installation with challenges like increased costs and potential environmental issues.",
      "It mentions the possibility of offshore wind farms reducing hurricane impact and contrasts renewable energy efforts across nations.",
      "The conversation delves into the difficulties of siting renewable projects, the economic viability of solar power, and the importance of maximizing renewable energy usage to decrease dependence on traditional sources."
    ],
    "points": 164,
    "commentCount": 86,
    "retryCount": 0,
    "time": 1710540031
  },
  {
    "id": 39715161,
    "title": "Boeing Whistleblower Warns: \"Not Suicide if Anything Happens\"",
    "originLink": "https://twitter.com/WallStreetSilv/status/1768517997285482626",
    "originBody": "BREAKING: Boeing whistleblower said this before his death to his friend Jennifer.\"If anything happens to me\"\"It&#39;s not suicide\"🚨🚨🚨 pic.twitter.com/mZr9Qk7CwF— Wall Street Silver (@WallStreetSilv) March 15, 2024",
    "commentLink": "https://news.ycombinator.com/item?id=39715161",
    "commentBody": "[dupe] Boeing Whistleblower: \"If Anything Happens to Me, It's Not Suicide\" (twitter.com/wallstreetsilv)134 points by hammock 23 hours agohidepastfavorite70 comments reaperman 23 hours agoDuplicate: https://news.ycombinator.com/item?id=39712618 (300 comments) theultdev 23 hours agoparentnext [4 more] Probably a dupe because I can't find this submission or the one you linked on any page of HN now. edit: just found this one on page 6 now. edit2: both are completely gone now off any page. edit3: now it's claimed as a [dupe] and the active discussion is dead. Wow. reply reaperman 21 hours agorootparentWell, enough people found it to get >330 comments, which is a lot. reply theultdev 20 hours agorootparentIt's strange neither are on the first 20 pages of HN. Many older Boeing articles with less points and less comments are on the first 10 pages. It dropped to page 8 within seconds, I haven't seen that before. reply lamontcg 19 hours agorootparentprevIIRC, the algorithm on HN deliberately punishes submissions which have more comments than upvotes. It uses this as a flamebait-detector. reply lr4444lr 23 hours agoprevI find it hard to believe that there many people in this world worth the kind of professional assassination that could avoid all of the widespread surveillance apparatus (consumer surveillance cameras, leaving behind no genetic data) and the advances in post-mortem medical examination, but maybe I'm just naïve. reply theultdev 23 hours agoparentWell, the cameras could go off for an hour and both guards could be asleep for the same hour. -- edit: on a meta note, why did this submission flash on the homepage, and now I can't find it? edit2: I can't find the dupe submission either: https://news.ycombinator.com/item?id=39712618 edit3: I found this submission now on page 6, it was just on the first page and gaining comments and votes fast until just now. edit4: both submissions are completely gone now off any page. @dang? reply Simulacra 23 hours agorootparentThat only happens at the Metropolitan detention center, in the Jeffrey Epstein memorial wing. reply joshstrange 23 hours agorootparentprevWell yeah, if you want to make up outlandish hypotheticals with no basis in reality /s I think we fall into a false sense of \"How could this happen and no one get caught doing it\". I know I've had that thought for decades now but murders keep happening and people keep getting away with it. I find myself thinking \"Wow, I guess that was easier than I thought it'd be (thinking back 1-4 years), but _now_ it would be harder\", the issue is I've had that same thought almost every year so clearly my guess of how hard it would be is way off. reply theultdev 23 hours agorootparentThat wasn't a hypothetical. I was referencing a current event. edit: missed the /s, stupid brain reply wtfwhateven 23 hours agorootparentthey were being sarcastic, hence the /s :P reply joshstrange 23 hours agorootparentprevI know, that's why I had the \"/s\" I know you're referencing Epstein reply theultdev 23 hours agorootparentMy brain completely skipped over that, woops. reply joshstrange 23 hours agorootparentNo problem, easy mistake. Joking is the only way I can deal with this stuff. reply theultdev 23 hours agorootparentI feel ya. Side note, for my sanity, did this article disappear off the first page for you as well? It's on page 6 for me now. edit: it's completely gone now off any page. reply joshstrange 21 hours agorootparentYeah, it did. This type of stuff gets flagged pretty hard so I’m not surprised. reply theultdev 20 hours agorootparentI never saw a [flagged] pop up but that's possible I suppose. The other submission also wasn't flagged, even as of now, but it's gone on the pages. There's much older and deader Boeing articles that are still on pages 3-10. reply aaomidi 23 hours agorootparentprevI've obviously never bought a hitman off of the dark web. However, it seems like these services aren't going anywhere. I'm not sure if they're all a scam or what. The interesting case with Boeing is, its not only that the company would want this person gone, but rather the entire US government too. Boeing is \"too big to fail.\" reply bitcharmer 19 hours agorootparentprev> edit4: both submissions are completely gone now off any page. Front page isn't what we vote for, it's manually curated by dang and other moderators. For actual popular stuff go to /active. reply bwestergard 23 hours agoparentprevIt seems we hear fairly often about mercenary - as opposed to ideological - killings that are revealed to be such only long after the fact. It would seem gratuitous to assume that all such killings are eventually revealed. For example, the U.S. and German governments have taken the position that the Russian federation conspired to have a Chechen leader killed in Berlin, Germany[1]. He was ultimately caught, and surveillance footage seems to have played a role, but if you look into the details of the case, it seems likely he was an experienced contract killer who could have gotten away with many other lower-profile killings. 1: https://en.wikipedia.org/wiki/Zelimkhan_Khangoshvili reply acover 23 hours agoparentprevHow many murders are actually solved? reply smallmancontrov 23 hours agoparentprevWould YOU want to pursue and present evidence that Boeing just offed a guy for pursuing and presenting evidence? reply lr4444lr 21 hours agorootparentA chance to be famous for bringing justice? Absolutely! reply Simulacra 23 hours agorootparentprevWould you want to be the policing agency that did not, only to be later found that it was the case? Authority should investigate and go where the investigation takes them. Have someone look at the camera systems, determine if it has been hacked, look at the footage and compare it - due diligence is not a bad thing. reply theultdev 23 hours agorootparentYou may not want to be the policing agency or agent that did look into it. But yes, authorities should investigate and should go where the investigation takes them. Boeing and friends are too big to actually look into, similar to the island being an event that was too big to actually look into. reply smallmancontrov 23 hours agorootparentprevThat's not what I asked. Of course I want them to look into it, but I am not the one who has to weigh my own life against the possibility of holding a big company accountable. Would you choose to risk it? reply hugh-avherald 21 hours agoparentprevA professional hit can cost as little as a few thousand dollars. The persons who do it for that kind of fee aren't the sharpest tools in the shed. reply iancmceachern 23 hours agoparentprevWell, he was definitely one of them. reply kylebenzle 21 hours agoparentprevLOL, you are indeed, most murders go unsolved and a hit can cost as little as $10k. 99% it is a cartel member and if they do get caught worst case they get sent back to mexico. reply Simulacra 23 hours agoparentprevWhat kind of \"security\" are we talking about? Hotel exterior cameras that are cloud controlled by (probably) a large commercial provider? Considering what is possible with hacking that, and that this is an enormously powerful company, I think it's entirely possible that a hit man could have been involved. That said, I think suicide is the most likely cause, but we should avoid failures of imagination and consider the possibility.[0] 0. https://en.m.wikipedia.org/wiki/Failure_of_imagination reply theultdev 23 hours agorootparentConversely, there's also these relevant wikipedia articles: https://en.wikipedia.org/wiki/Motive_(law) https://en.wikipedia.org/wiki/Naivety You can't just take things as face value when people can lie, deceive, plot, and other more horrific things. Especially when there's motive and a wall of coincidences. reply guardiangod 23 hours agoprevI don’t understand the motivation behind Boeing ordering the hit. The guy already let out all the dirty laundry and nothing he said can further damage Boeings reputation. So why do it? I’d think it’s one of Boeings stockholders who had a judge against him speaking out. reply reactordev 23 hours agoparentLet's break it down. He was about to under-go day 3 of whistleblower testimony. He had already given 2 days worth and they have found instances worth following up with criminal investigations. Guy's in a hotel (probably under his name) and is found in his car - shot to death in what appears to be a suicide. Then this, saying he stated that \"If anything happens to me...\", then Boeing \"conveniently\" overwriting the door bolt safety footage, etc has all the hallmarks of it being a hit. To protect Boeing from criminal investigation and costing the company potentially billions in fines and lost sales. I'm in the camp that they did, in fact, hire someone or someone within their organization did to protect their interests in this case. Whether it's from corporate or from just the group of folks that might/would/will be indicted from the whistleblowers testimony. You don't land on 21-black 3 times in a row and say it's luck. reply alphabettsy 20 hours agorootparentHe was not going into “whistleblower” testimony. He was suing Boeing for defamation And his claims were that Boeing retaliated against him for being a whistleblower. It might seem like the same thing, but it’s not. And the instances promoting criminal investigation are related to the 737 probes, not the 787 that he was a whistleblower about. There no evidence his civil case was the impetus for those criminal probes. reply reactordev 20 hours agorootparentOk, “private testimony against his company”… Also to say that Foo in Green is in no way connected to Foo in Blue is a farce. 737,777,787, they all share a safety process and protocol. To say he was talking about oranges when the probes are about apples when it’s all about the Grocery Store is absurd. It’s connected. May not be directly connected, but it’s connected. It’s bad management, bad practice, sus circumstances, a coverup on the left and a dead body on the right. reply sxg 23 hours agoparentprevWe don't know that all the laundry has been let out. It's also possible that there could be criminal charges against some of Boeing's leadership if this guy were to testify against them or if he had more info. reply Simulacra 23 hours agorootparentI'm not a lawyer, but if he was dead before the deposition, it doesn't really count. In order to have any weight, he would've had to been under oath, in a deposition, some type of very official interview and documentation. It's like when a mob witness goes missing. If they can't testify, they can't testify. reply dragonwriter 21 hours agoparentprevWhy would someone under criminal investigation kill a key witness after they had talked but before they could testify? Not only are the reasons fairly obvious, but the history of that being exactly what happens is pretty compelling evidence that, independently of arguments over its rationality, that is very much a thing people in that situation do. reply kylebenzle 21 hours agorootparentMaybe they threatened him at first not to talk, he did, then they killed him. reply cmiles74 23 hours agoparentprevThey were scheduled for an additional day of deposition. Apparently there was more \"dirty laundry\" to talk about. reply baja_blast 21 hours agorootparentAnd not to mention a potential criminal trail from the DOJ. He would be a key witness against individuals for a criminal case that bubbles up to the top brass. reply iancmceachern 23 hours agoparentprevMaybe there was more (unaired dirty laundry) reply kylebenzle 21 hours agoparentprevThere would be revolution in the streets tomorrow if the average person understood where money comes from and how it works. Once a group of people is wired into the free money pipeline they will do ANYTHING to make sure their supply is not cut off. The USA has been printing and giving away free money for 30+ years now. The recipients of those funds have proven again and again, they will and do commit heinous crimes to maintain their position. 1) The legislature - Law makers have been captured through campaign contributions. In MOST cases a law maker has no say in how they vote, they are told by their campaign managers. 2) The judicial - Similarly judges tend to run for their seat every 4 years. In family court any divorce attorney worth their salt is donating the maximum amount to every judge every time they run. Then when they are in front of the judge in court just a thumbs up and a nod is enough to remind the judge who REALLY got them elected. 3) Corporations are DONE paying taxes and have been for a long time. Successful corporations have figured out how to siphon money from not only their customers but their employees and the federal government as well. 4) The money supply and system in general is fucked beyond repair and anyone willing or able to point that out is going to be on the \"kill list\". reply LiquidSky 23 hours agoparentprevSends a pretty clear message to anyone else thinking of speaking up. reply Traubenfuchs 23 hours agoparentprevA late message to potential future whistleblowers. You think crazy Jennifer made this up for her 5 minutes of fame? reply baja_blast 21 hours agoparentprevSure he may have already aired out what Boeing was doing, but he was only talking about general actions. But given that DOJ is looking into a criminal investigation that means he can name names, and testify against individuals which would bubble up the chain since lower level management just follows upper management’s direction. Also there could have been key information unsurfaced that he may not even know the importance of that the legal team would have spotted. I mean why would someone who has given up everything to blow the whistle and devoted the past 7 years trying to get something done about it all a sudden kill himself in the middle of a trail and before a potential DOJ criminal investigation? The man had a mission, why would he do Boeing a favor and kill himself so that he could not testify at a criminal hearing? Imagine this in a criminal trail he would testify against his direct managers who would then just say “hey, I was told by my boss to tell him to do x” and then it bubbles up the chain to the executives. reply lupusreal 23 hours agoparentprevBoeing is presently in the middle of a coverup; they don't want anybody coming forward with information that Boeing management knew about the \"opening a plug door\" process shortcut. reply theodorejb 23 hours agoprevI wonder if the police are afraid to investigate it thoroughly. reply bell-cot 22 hours agoparentIn much of modern America - 's/are afraid to/can be bothered, and have the resources to/'. reply axus 23 hours agoparentprevOnly if it's a major local employer reply risho 23 hours agoprevI don't really have an opinion on this either way, but I would just like to point out that it is in fact possible to say publicly (or privately for that matter) that you will not kill yourself and then kill yourself. There is no fundamental law of nature that says if you say something that are cosmically stuck to that statement. reply aaomidi 23 hours agoparentWith all due respect, \"no shit?\" There however is less reason to believe this was suicide given the circumstances. reply risho 23 hours agorootparentthen maybe consider using real evidence instead of something that isn't evidence of anything. reply aaomidi 22 hours agorootparentThis isn't a court and the bar for real evidence isn't going to be met here. Assassinations, by definition, are done to keep the actual person behind them hidden. What we do have is a series of problematic behavior Boeing is doing, that fit an MO. Also, it's clear that *you do* have an opinion on this. reply risho 22 hours agorootparenti literally know absolutely nothing about boeing or this guy. i have 0 opinion on this case at all. what i have an opinion about is people using someone claiming they won't commit suicide as evidence that they were murdered when they die. see john mcafee. it was a major headline when he died that he said he wouldn't commit suicide and then he died therefore conspiracy. except in reality he is the exact sort of unhinged maniac that would commit suicide. i wouldn't be surprised if there was ACTUAL evidence of a conspiracy, but him saying him saying he won't kill himself aint it. reply VincentEvans 23 hours agoprevThis seems like a screen capture from Fox? Anyone has the link to the original video on Fox? reply Traubenfuchs 23 hours agoprevAs much as this was meant to silence him, this was meant as a message to anyone else considering whistleblowing. Whistleblowing rapidly loses attractively if you know that you will get putined. Why pay someone off? Why be scared? Just kill anyone you don't like. They do it because they can. They are companies with country-sized budgets. This is corporation sponsored terrorism against the population. Will there be consequences? I doubt it. reply epolanski 23 hours agoparentI don't think we have that many informations to label c-suite in Boeing as killers with such ease. Those are serious allegations. I would rather wait for an inquiry/coroner/etc to shed more light on what's happened. reply pizzafeelsright 23 hours agorootparentIt is never so direct. Management gets a contact who is a fixer, hired though an attorney, hiring a contractor who does investigations who happen to have contacts who will do field work research. Field researcher gets name, hotel, car, follows from deposition and when the schedule becomes predictable an interaction or two occurs. Maybe an offer to stay silent followed up with a threat or action that will further persuade a favorable outcome for the hiring agency. Upper management has zero exposure and everyone down the line was vague enough to be removed from direct action with the trigger being unaffiliated party. Want to punish a neighbor? Pay a homeless guy to hire a homeless guy to throw a rock through a window. Who gets arrested? Not the home owner. Homeless guys can't be trusted for their witness. No records. Window is still broken. reply spacephysics 23 hours agorootparentprevI’d more readily believe it’s a shareholder than anyone directly at Boeing. But who knows reply epolanski 23 hours agorootparentWhat would be the incentive? Seeing his portfolio shrink and the company go through even more problems than it has? reply smallmancontrov 23 hours agorootparentModest 401ks invested in index funds yielding a tiny amount of supplemental income that mostly gets reinvested for retirement are not a universal truth. Some people invest for a living. If you have the net worth for it, it's a nice way to live. Other people strive for the same using concentration and leverage. So yeah, there are investors out there who care a lot. reply epolanski 23 hours agorootparentBut no investor here benefits from throwing shades on Boeing killing whistleblowers. reply smallmancontrov 23 hours agorootparentWhat? Why on Earth would you think that? \"We are all pretty sure Boeing did it\" doesn't meet any actionable legal standard. Deposing this guy and the followups is a critical part of the accountability process that will no longer happen or, in the case of followups, be significantly impeded by the presence of a credible threat looming over their heads. Shareholders (and management) absolutely benefit. reply epolanski 17 hours agorootparentBecause getting the company you have shares in dragged in even more mud at such a time with a whistleblower dying mysteriously does really your shares no good. reply spacephysics 22 hours agorootparentprevOnce you start to look at hedge funds with very large option positions, the losses can be many thousands of times larger than just “stock lost 5%, means I lost 5%” It’s usually something like “if stock drops 2.5% at this number, we’re forced to buy the inverse option to hedge our bets and we loose X amount” (over simplification) Just look at the GME stuff and how much money was lost due to retail trading meme behavior reply kylebenzle 21 hours agorootparentIm surprised they didn't murder Roaring Kitty (Keith Gill) probably wh he has gone into hiding. He probably got the same offer he couldn't refuse, \"shut the fuck up or else.\" reply lupusreal 23 hours agorootparentprevWe already know they're killers, the second MAX crash showed that if the first didn't. Now it's just a question of how direct they're willing to get. reply readthenotes1 23 hours agorootparentprevWe already know that they are killers. Just ask the people who died on their planes, for which they did their best to cover up Boeing's responsibility. reply johnea 20 hours agoprev [–] Some non-twitverse links to this story: https://www.newsweek.com/john-barnett-boeing-whistleblower-p... https://abcnews4.com/news/local/if-anything-happens-its-not-... reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "A Boeing whistleblower voiced safety concerns, stating he didn't think his possible death would be due to suicide."
    ],
    "commentSummary": [
      "The Hacker News discussion centered around a Boeing whistleblower's alarming statement and a mysterious article removal, raising concerns about safety threats, criminal investigations, and internal corruption at the company.",
      "Speculation on motives, power dynamics, and financial impacts of corporate decisions, along with worries about whistleblower safety and associated risks, were major talking points.",
      "Comparisons to the GameStop situation were made, highlighting the potential repercussions for hedge funds and whistleblowers."
    ],
    "points": 134,
    "commentCount": 70,
    "retryCount": 0,
    "time": 1710508829
  },
  {
    "id": 39717838,
    "title": "Evolution of Computer Science: 1999 vs. 2015",
    "originLink": "http://danluu.com/butler-lampson-1999/",
    "originBody": "In 1999, Butler Lampson gave a talk about the past and future of “computer systems research”. Here are his opinions from 1999 on \"what worked\". Yes Maybe No Virtual memory Parallelism Capabilities Address spaces RISC Fancy type systems Packet nets Garbage collection Functional programming Objects / subtypes Reuse Formal methods RDB and SQL Software engineering Transactions RPC Bitmaps and GUIs Distributed computing Web Security Algorithms Basically everything that was a Yes in 1999 is still important today. Looking at the Maybe category, we have: Parallelism This is, unfortunately, still a Maybe. Between the end of Dennard scaling and the continued demand for compute, chips now expose plenty of the parallelism to the programmer. Concurrency has gotten much easier to deal with, but really extracting anything close to the full performance available isn't much easier than it was in 1999. In 2009, Erik Meijer and Butler Lampson talked about this, and Lampson's comment was that when they came up with threading, locks, and conditional variables at PARC, they thought they were creating something that programmers could use to take advantage of parallelism, but that they now have decades of evidence that they were wrong. Lampson further remarks that to do parallel programming, what you need to do is put all your parallelism into a little box and then have a wizard go write the code in that box. Not much has changed since 2009. Also, note that I'm using the same criteria to judge all of these. Whenever you say something doesn't work, someone will drop in say that, no wait, here's a PhD that demonstrates that someone has once done this thing, or here are nine programs that demonstrate that Idris is, in fact, widely used in large scale production systems. I take Lampson's view, which is that if the vast majority of programmers are literally incapable of using a certain class of technologies, that class of technologies has probably not succeeded. On recent advancements in parallelism, Intel recently added features that make it easier to take advantage of trivial parallelism by co-scheduling multiple applications on the same machine without interference, but outside of a couple big companies, no one's really taking advantage of this yet. They also added hardware support for STM recently, but it's still not clear how much STM helps with usability when designing large scale systems. RISC If this was a Maybe in 1999 it's certainly a No now. In the 80s and 90s a lot of folks, probably the majority of folks, believed RISC was going to take over the world and x86 was doomed. In 1991, Apple, IBM, and Motorola got together to create PowerPC (PPC) chips that were going to demolish Intel in the consumer market. They opened the Somerset facility for chip design, and collected a lot of their best folks for what was going to be a world changing effort. At the upper end of the market, DEC's Alpha chips were getting twice the performance of Intel's, and their threat to the workstation market was serious enough that Microsoft ported Windows NT to the Alpha. DEC started a project to do dynamic translation from x86 to Alpha; at the time the project started, the projected performance of x86 basically running in emulation on Alpha was substantially better than native x86 on Intel chips. In 1995, Intel released the Pentium Pro. At the time, it had better workstation integer performance than anything else out there, including much more expensive chips targeted at workstations, and its floating point performance was within a factor of 2 of high-end chips. That immediately destroyed the viability of the mainstream Apple/IBM/Moto PPC chips, and in 1998 IBM pulled out of the Somerset venture1 and everyone gave up on really trying to produce desktop class PPC chips. Apple continued to sell PPC chips for a while, but they had to cook up bogus benchmarks to make the chips look even remotely competitive. By the time DEC finished their dynamic translation efforts, x86 in translation was barely faster than native x86 in floating point code, and substantially slower in integer code. While that was a very impressive technical feat, it wasn't enough to convince people to switch from x86 to Alpha, which killed DEC's attempts to move into the low-end workstation and high-end PC market. In 1999, high-end workstations were still mostly RISC machines, and supercomputers were a mix of custom chips, RISC chips, and x86 chips. Today, Intel dominates the workstation market with x86, and the supercomputer market has also moved towards x86. Other than POWER, RISC ISAs were mostly wiped out (like PA-RISC) or managed to survive by moving to the low-margin embedded market (like MIPS), which wasn't profitable enough for Intel to pursue with any vigor. You can see a kind of instruction set arbitrage that MIPS and ARM have been able to take advantage of because of this. Cavium and ARM will sell you a network card that offloads a lot of processing to the NIC, which have a bunch of cheap MIPS and ARM processors, respectively, on board. The low-end processors aren't inherently better at processing packets than Intel CPUS; they're just priced low enough that Intel won't compete on price because they don't want to cannibalize their higher margin chips with sales of lower margin chips. MIPS and ARM have no such concerns because MIPS flunked out of the high-end processor market and ARM has yet to get there. If the best thing you can say about RISC chips is that they manage to exist in areas where the profit margins are too low for Intel to care, that's not exactly great evidence of a RISC victory. That Intel ceded the low end of the market might seem ironic considering Intel's origins, but they've always been aggressive about moving upmarket (they did the same thing when they transitioned from DRAM to SRAM to flash, ceding the barely profitable DRAM market to their competitors). If there's any threat to x86, it's ARM, and it's their business model that's a threat, not their ISA. And as for their ISA, ARM's biggest inroads into mobile and personal computing came with ARMv7 and earlier ISAs, which aren't really more RISC-like than x862. In the area in which they dominated, their \"modern\" RISC-y ISA, ARMv8, is hopeless and will continue to be hopeless for years, and they'll continue to dominate with their non-RISC ISAs. In retrospect, the reason RISC chips looked so good in the 80s was that you could fit a complete high-performance RISC microprocessor onto a single chip, which wasn't true of x86 chips at the time. But as we got more transistors, this mattered less. It's possible to nitpick RISC being a no by saying that modern processors translate x86 ops into RISC micro-ops internally, but if you listened to talk at the time, people thought that having an external RISC ISA would be so much lower overhead that RISC would win, which has clearly not happened. Moreover, modern chips also do micro-op fusion in order to fuse operations into decidedly un-RISC-y operations. A clean RISC ISA is a beautiful thing. I sometimes re-read Dick Sites's explanation of the Alpha design just to admire it, but it turns out beauty isn't really critical for the commercial success of an ISA. Garbage collection This is a huge Yes now. Every language that's become successful since 1999 has GC and is designed for all normal users to use it to manage all memory. In five years, Rust or D might make that last sentence untrue, but even if that happens, GC will still be in the yes category. Reuse Yes, I think, although I'm not 100% sure what Lampson was referring to here. Lampson said that reuse was a maybe because it sometimes works (for UNIX filters, OS, DB, browser) but was also flaky (for OLE/COM). There are now widely used substitutes for OLE; service oriented architectures also seem to fit his definition of re-use. Looking at the No category, we have: Capabilities Yes. Widely used on mobile operating systems. Fancy type systems It depends on what qualifies as a fancy type system, but if “fancy” means something at least as fancy as Scala or Haskell, this is a No. That's even true if you relax the standard to an ML-like type system. Boy, would I love to be able to do everyday programming in an ML (F# seems particularly nice to me), but we're pretty far from that. In 1999 C, and C++ were mainstream, along with maybe Visual Basic and Pascal, with Java on the rise. And maybe Perl, but at the time most people thought of it as a scripting language, not something you'd use for \"real\" development. PHP, Python, Ruby, and JavaScript all existed, but were mostly used in small niches. Back then, Tcl was one of the most widely used scripting languages, and it wasn't exactly widely used. Now, PHP, Python, Ruby, and JavaScript are not only more mainstream than Tcl, but more mainstream than C and C++. C# is probably the only other language in the same league as those languages in terms of popularity, and Go looks like the only language that's growing fast enough to catch up in the foreseeable future. Since 1999, we have a bunch of dynamic languages, and a few languages with type systems that are specifically designed not to be fancy. Maybe I'll get to use F# for non-hobby projects in another 16 years, but things don't look promising. Functional programming I'd lean towards Maybe on this one, although this is arguably a No. Functional languages are still quite niche, but functional programming ideas are now mainstream, at least for the HN/reddit/twitter crowd. You might say that I'm being too generous to functional programming here because I have a soft spot for immutability. That's fair. In 1982, James Morris wrote: Functional languages are unnatural to use; but so are knives and forks, diplomatic protocols, double-entry bookkeeping, and a host of other things modern civilization has found useful. Any discipline is unnatural, in that it takes a while to master, and can break down in extreme situations. That is no reason to reject a particular discipline. The important question is whether functional programming in unnatural the way Haiku is unnatural or the way Karate is unnatural. Haiku is a rigid form poetry in which each poem must have precisely three lines and seventeen syllables. As with poetry, writing a purely functional program often gives one a feeling of great aesthetic pleasure. It is often very enlightening to read or write such a program. These are undoubted benefits, but real programmers are more results-oriented and are not interested in laboring over a program that already works. They will not accept a language discipline unless it can be used to write programs to solve problems the first time -- just as Karate is occasionally used to deal with real problems as they present themselves. A person who has learned the discipline of Karate finds it directly applicable even in bar-room brawls where no one else knows Karate. Can the same be said of the functional programmer in today's computing environments? No. Many people would make the same case today. I don't agree, but that's a matter of opinion, not a matter of fact. Formal methods Maybe? Formal methods have had high impact in a few areas. Model checking is omnipresent in chip design. Microsoft's driver verification tool has probably had more impact than all formal chip design tools combined, clang now has a fair amount of static analysis built in, and so on and so forth. But, formal methods are still quite niche, and the vast majority of developers don't apply formal methods. Software engineering No. In 1995, David Parnas had a talk at ICSE (the premier software engineering conference) about the fact that even the ICSE papers that won their “most influential paper award” (including two of Parnas's papers) had very little impact on industry. Basically all of Parnas's criticisms are still true today. One of his suggestions, that there should be distinct conferences for researchers and for practitioners has been taken up, but there's not much cross-pollination between academic conferences like ICSE and FSE and practitioner-focused conferences like StrangeLoop and PyCon. RPC Yes. In fact RPCs are now so widely used that I've seen multiple RPCs considered harmful talks. Distributed systems Yes. These are so ubiquitous that startups with zero distributed systems expertise regularly use distributed systems provided by Amazon or Microsoft, and it's totally fine. The systems aren't perfect and there are some infamous downtime incidents, but if you compare the bit error rate of random storage from 1999 to something like EBS or Azure Blob Storage, distributed systems don't look so bad. Security Maybe? As with formal methods, a handful of projects with very high real world impact get a lot of mileage out of security research. But security still isn't a first class concern for most programmers. Conclusion What's worked in computer systems research? Topic 1999 2015 Virtual memory Yes Yes Address spaces Yes Yes Packet nets Yes Yes Objects / subtypes Yes Yes RDB and SQL Yes Yes Transactions Yes Yes Bitmaps and GUIs Yes Yes Web Yes Yes Algorithms Yes Yes Parallelism Maybe Maybe RISC Maybe No Garbage collection Maybe Yes Reuse Maybe Yes Capabilities No Yes Fancy type systems No No Functional programming No Maybe Formal methods No Maybe Software engineering No No RPC No Yes Distributed computing No Yes Security No Maybe Not only is every Yes from 1999 still Yes today, seven of the Maybes and Nos were upgraded, and only one was downgraded. And on top of that, there are a lot of topics like neural networks that weren't even worth adding to the list as a No that are an unambiguous Yes today. In 1999, I was taking the SATs and applying to colleges. Today, I'm not really all that far into my career, and the landscape has changed substantially; many previously impractical academic topics are now widely used in industry. I probably have twice again as much time until the end of my career and things are changing faster now than they were in 1999. After reviewing Lampson's 1999 talk, I'm much more optimistic about research areas that haven't yielded much real-world impact (yet), like capability based computing and fancy type systems. It seems basically impossible to predict what areas will become valuable over the next thirty years. Correction This post originally had Capabilities as a No in 2015. In retrospect, I think that was a mistake and it should have been a Yes due to use on mobile. Thanks to Seth Holloway, Leah Hanson, Ian Whitlock, Lindsey Kuper, Chris Ball, Steven McCarthy, Joe Wilder, David Wragg and Alex Clemmer for comments/discussion. I know a fair number of folks who were relocated to Somerset from the east coast by IBM because they later ended up working at a company I worked at. It's interesting to me that software companies don't have the same kind of power over employees, and can't just insist that employees move to a new facility they're creating in some arbitrary location. [return] I once worked for a company that implemented both x86 and ARM decoders (I'm guessing it was the first company to do so for desktop class chips), and we found that our ARM decoder was physically larger and more complex than our x86 decoder. From talking to other people who've also implemented both ARM and x86 frontends, this doesn't seem to be unusual for high performance implementations. [return]",
    "commentLink": "https://news.ycombinator.com/item?id=39717838",
    "commentBody": "What's worked in Computer Science: 1999 vs. 2015 (2015) (danluu.com)128 points by not_a_boat 20 hours agohidepastfavorite113 comments stevefan1999 19 hours agoI would like to suggest that the classical taxonomy of RISC/CISC dichotomy is basically non-existent nowadays -- namely because both sides have influenced each other. It is well known that CISC has taken a lot of inspirations from RISC designs (such as having a lot more registers in x64), and RISC designs also taken some inspirations from CISC (such as having SIMD/vectorization units). In other words, the line between RISC and CISC has been very fine lately. Also, at the end of the day, they all turned into μops. If I remember Jim Keller correctly, ARM and x86, they are basically the same in the back nowadays, its just the frontend and their decoding units are different. And that's why he strongly suggested AMD to also adapt Zen design with ARM ISA during his tenure there, oh I think it is called K12. I think there is another blurry line between superscalar and VLIW architecture, too. reply abainbridge 19 hours agoparentYep. RISC was interesting when gate budgets for CPU pipelines were seriously limited. It was interesting because before RISC the industry had been merrily spending the gate budget increase on adding lots of use-specific instructions. The RISC people pointed out that if you removed support for all the fancy instructions you had enough gate budget for the ALU to be nicely pipelined, and then you could wind up the clock rate greatly and this was worth much more than the fancy instructions. For decades now we've had enough gate budget to have nicely pipelined designs with complex instruction sets, so that's what everyone does. RISC solves a problem that no longer exists. reply thesz 18 hours agorootparentYou are not quite right about pipelined design being faster. At least, not without substantial effort. https://en.wikipedia.org/wiki/R2000_microprocessor \"The R2000 is a 32-bit microprocessor chip set developed by MIPS Computer Systems that implemented the MIPS I instruction set architecture (ISA)...\" \"The R2000 was available in 8.3, 12.5 and 15 MHz grades...\" https://en.wikipedia.org/wiki/I386 \"The Intel 386, originally released as 80386 and later renamed i386, is a 32-bit microprocessor introduced in 1985...\" \"Max. CPU clock rate: 12.5 MHz to 40 MHz\" As you can see, 80386 was released a year earlier than R2000 and was about 1.5 times faster than MIPS implementation from the start. The critical path is, usually, in addition/subtraction, which should be complete in one cycle in both 80386 and in R2000. To pipeline addition you need a superpipelined CPU, one that has several stages for computation. Even seemingly simple computation of condition codes can make clock cycle 10% longer (SPARC vs MIPS) if your CPU is just simply pipelined. BTW, some Pentiums did computed 32-bit addition in two cycles, all in name of higher clock frequencies. reply abainbridge 16 hours agorootparentInteresting. An R2000 did run programs faster than a 80386, right? This was a few years before my time. From a quick google now, it looks like the R2000 was about 3x better than the 80386 at Dhrystone MIPS/MHz. I guess an accurate comparison of how the R2000 and 80386 spent they gate budget and what they got in return would involve a lot of detail. I remember my compsci professor giving us the computer architecture course in about 1997, and he dispaired at how all the clever RISC stuff in the Patterson and Hennessey seemed irrelevant when Intel could just throw money at the implementation (and fab, I guess) and produce competitive chips despite their (allegedly) inferior architecture. reply thesz 15 hours agorootparentMy point is that you cannot get design much faster in terms of clock frequency by just pipelining. Pipeline unrolls state machine and overlaps different executions of the state machines. But the bottleneck, which is addition, is there in all designs and you need additional effort to break it. (also MIPS has [i]ntelocked [p]ipeline [s]tages - that \"IPS\" in MIPS; I implemented it, I know - exception in execution should inform other stages about failure) By the 1997 Intel has already bought Elbrus II design team, lead by Pentkovski [1]. That Pentkovski guy made Elbrus 2 a superscalar CPU with a stack machine front-end. E.g., Elbrus 2 executed stack operations in a superscalar fashion. You can entertain yourself by figuring out how complex or simple can that be. [1] https://en.wikipedia.org/wiki/Vladimir_Pentkovski So at the time your professor complained about Intel's inferior architecture being faster, that inferior architecture implementation has a translation unit inside it to translate x86 opcodes into superscalar-ready uops. reply abainbridge 2 hours agorootparentI think the Wikipedia page [1] agrees with your main point. I said pipelining allowed you to increase the clock rate, which isn't the best thing to say. The wiki page says, \"instruction pipelining is a technique for implementing instruction-level parallelism within a single processor. Pipelining attempts to keep every part of the processor busy with some instruction by dividing incoming instructions into a series of sequential steps (the eponymous \"pipeline\") performed by different processor units with different parts of instructions processed in parallel.\" And, \"This arrangement lets the CPU complete an instruction on each clock cycle. It is common for even-numbered stages to operate on one edge of the square-wave clock, while odd-numbered stages operate on the other edge. This allows more CPU throughput than a multicycle computer at a given clock rate, but may increase latency due to the added overhead of the pipelining process itself.\" [1] https://en.wikipedia.org/wiki/Instruction_pipelining reply peterfirefly 13 hours agorootparentprevAddition was not the bottle neck for the 386. It had a FO4 delay of 80+ per clock. An adder is much faster. Maybe you meant that it was (one, just one!, of many of) the bottle neck(s) in an optimized implementation? reply thesz 13 hours agorootparent> Addition was not the bottle neck for the 386. It is a bottleneck for MIPS, SPARC, Alpha and not for 386. How so? reply peterfirefly 13 hours agorootparentThe 386 wastes so many FO4 gate delays on other things. I thought I made that extremely clear? reply peterfirefly 13 hours agorootparentprevAnd the R2000 was implemented in 2µm and the (early) 386 was implemented in 1.5µm. Double-metal for both. Didn't bother to look up die size. reply smcin 15 hours agorootparentprevIt's not apples-to-apples to compare raw clock rates between semiconductor processes; Intel's 386 was intially fabbed on 1.5μ then shrunk to 1.0μ process (Intel CHMOS III and IV), whereas MIPS R2000 was 2.0μ, fabless and relied on Sierra, Toshiba, then in 1987 LSI, IDT and other licensees [0][1]. Back in the 1980s/90s/2000s, Intel was consistently a process generation or two ahead of competitors. That was one of their main sources of advantage. Just imagine if MIPS had been able to fab on Intel process. [0]: https://www.righto.com/2023/10/intel-386-die-versions.html [1]: https://en.wikipedia.org/wiki/R2000_microprocessor reply thesz 15 hours agorootparentAnd you are also confirm that in order to have higher clock frequency you need more than just pipelining. Thank you. I also think that 1.5x difference in clock speeds cannot be directly attributed to the difference between node size (lambda): difference in lambdas 1.3(3)=2.0/1.5 at the introduction of the 80386 and R2000 is noticeably less than 1.47=12.5/8.5. reply smcin 13 hours agorootparentSmaller transistors are faster, but the relationship between clock frequency and 1/feature size isn't necessarily linear like you're assuming. https://cs.stackexchange.com/questions/27875/moores-law-and-... reply thesz 13 hours agorootparentMy assumption is that speedup is less than lambda's ratio. reply bananabiscuit 17 hours agorootparentprevIs there something about RISC that is still makes it better than CISC when it comes to per-watt performance? Seems like nobody has any success making an x86 processor that's as power efficient as ARM or RISC. reply simne 14 hours agorootparent> Is there something about RISC that is still makes it better than CISC when it comes to per-watt performance? CLASSIC CISC was micro-coded (for example, IBM S/360 have feature, you could make your custom microcode for compatibility with your inherited equipment, like IBM-1401 machines or IBM-7XXX series, or for other purposes), and RISC was with pipeline from birth. Second thing, as I understand, many CISC existed as multiple chips board or even as multiple boards, so have great losses on wires, but RISC appear in 1990s as one die immediately (only external cache added as additional IC), but I could mistake on this. > nobody has any success making an x86 processor that's as power efficient as ARM or RISC Rumors said, Intel Atom (essentially CMOS version of Pentium first generations) was very good in mobiles, but ARM far succeed it on software support of huge number of power saving features (modern ARM SOC allows to turn off near any part of chip any time and OS support this), and because of lack of software support, smartphones with Intel have poor time on battery. More or less official info said, that Intel made bad power conversion circuit, so Atom consumes too much in mode between deep sleep and full speed, but I don't believe them, as this is too obvious mistake for hardware developer. reply card_zero 17 hours agorootparentprevHave there been recent attempts? Maybe it's just, like, speciation, by this point in time. reply mcbishop 6 hours agorootparentprevA great discussion on this: Lex Fridman's interview of David Patterson. reply simne 12 hours agoparentprev> classical taxonomy of RISC/CISC dichotomy is basically non-existent nowadays After digest information about IBM 360, I decided, we lost CISCs. One of most important feature of 360 was customizable microcode, which you could load on system boot and got effectively different hardware (like with FPGA emulators of Amiga's). It was widely used to emulate old hardware, like IBM 1401 or IBM 7xxx series. But I have not seen this feature in 390 documentation, so looks like their 360 emulation become just software (and with achievements of semiconductors in 1990s it looks like adequate, to switch to software emulation). I must admit, ARM marketed feature of customized microcode, to add new instructions (they have standardized place in instruction set, named \"custom coprocessor instructions\", so if you have enough money, you could make special ARM with your additional instructions), but it is nothing if compare to 360. reply jcranmer 16 hours agoparentprevRISC/CISC is near the top of the list of \"things emphasized in education that bear little relevance in practice\". RISC isn't so much a single coherent design idea as a collection of ideas, some of which have won out (more register files), and some of which haven't (avoid instructions that take multiple clock cycles). The architectures from the days the \"debate\" was more relevant that have had the most success are the ones which most thoroughly blurred the lines between classical RISC and CISC--namely, Arm and x86. > I think there is another blurry line between superscalar and VLIW architecture, too. No, the line is pretty damn sharp. The core idea behind VLIW is that having hardware doing dynamic scheduling (as superscalar does) is silly and the compiler should be responsible for statically scheduling all instructions. The only blur here is that both VLIW and superscalar envision having multiple execution units that can be simultaneously scheduled with work, but who is responsible for doing that scheduling is pretty distinct. reply titzer 17 hours agoparentprevI agree that the line is pretty thin, but would draw it as: fixed-width versus variable-width. I think the M? line of Apple CPUs, with extremely-wide parallel decode, has been a game changer. The performance per watt is really off the charts. That's partly due to integrated RAM and all, but mostly due to microarchitectural changes, which I believe to be a massive step function in superscalar bandwidth (wider decode, huge ROB, huge numbers of ports). It seems like the power-hungry decode stage has been tamed, and I think this is because of fixed-width instructions in arm. reply gumby 14 hours agoparentprev> RISC designs also taken some inspirations from CISC (such as having SIMD/vectorization units) I think that one went the other way: for example the PlayStation 2 used a MIPS chip with 256-bit SIMD instructions (the TMPR 5900) as well as a dedicated GPU (the so-called “emotion engine”) reply CalChris 18 hours agoparentprevA 20,000 gate minimal RISC-V RV32E controller CPU isn't going to use μops. In 2024, RISC-V has turned that 2015 No into an unqualified Yes even if the microarchitecture of more complex OOO RISC-V systems resemble the microarchitectures of similarly complex x86 and ARM CPUs. reply wmf 18 hours agorootparentTo the extent that RISC-V is successful it's due to openness/freeness; RISC has nothing to do with it. An open \"CISC-V\" community would have been just as successful (and people wouldn't gripe about instruction fusion). reply pclmulqdq 15 hours agorootparentprevI have made a few of those RISC-V CPUs. The minute the \"M\" instruction set shows up (with division), a macro/micro-op split becomes worth it if you want to minimize gate count or maximize speed. reply silvestrov 19 hours agoprevI'd say that \"pure Functional programming\" has become a no. But \"Functional programming approach\" has been subsumed into existing programming languages, e.g. records in Java. You get most of the benefit of FP while keeping all of the other good stuf from an imperative language. reply tubthumper8 16 hours agoparentDefinitely agree that mainstream languages are adopting functional features, but records aren't a functional feature. Records are basic data modeling and something that has been around since the beginning of programming languages, whether procedural or functional. It's one of the bare minimums of having a type system, and Java didn't have this due to the misguided belief that \"everything is an object\". I think records being added is more of a symptom of that belief weakening. There are functional features that have made it over to Java which are the things related to functions, i.e. lambdas and such which are a welcome addition. reply gardenhedge 15 hours agorootparentJava records are object-oriented construct reply tubthumper8 7 hours agorootparentMaybe. It depends what the definition of \"object-oriented\" is, which depends what the definition of \"object\" is. Neither of which has any semblance of agreement among programmers. Many people would define an object as something that has externally visible behavior and internally hidden _data_. Objects could never be compared to each other for equality, because one could not access the internal data, only its public behavior. However, Java records can be compared for equality which objects would not. Java records also cannot be extended from. Inheritance is the only uniquely OOP feature, which doesn't apply here either. So I'm not sure that Java records are an object-oriented construct. First, the precise definition of OO should be established, and then we'll see reply ildjarn 4 hours agorootparentprevOOP is about bundling state and behaviour into units that hide their internal mechanisms by some kind of interface. Records can have no state - compared to regular classes - so they are an anti-OOP feature. reply unregistereddev 19 hours agoparentprevI'd agree with this and add that pattern matching and functional interfaces are additional examples of tools from the FP toolbox being subsumed into existing programming languages. Pure functional programming seems tempting. In small projects it can produce beautiful, readable code. In large projects I've only seen it result in messes, and I still haven't decided whether that's due to limitations of functional purity, due to the team (and myself) misusing features that we don't understand, or both. reply ildjarn 18 hours agoparentprevWhat mainstream languages have a good suite of FP features though? If you try to write mostly pure code in Java I’m afraid you’re in for a bad time, despite the (big!) improvements of records and lambdas. Minimum viable FP starts at OCaml, F#, Scala and Closure, yet none of these are mainstream. reply anon291 9 hours agorootparentHold on... Purity and functional programming are different things. Functional programs can be pure but not necessarily. Functional programming means functions are first class citizens and can be constructed on the fly. Modern python, c++, rust, even java now do this. Purity is a nice to have (and arguably rusts borrow system enforces a kind of purity). reply ildjarn 4 hours agorootparent> Functional programming means functions are first class citizens and can be constructed on the fly. FP is much more than this one language feature. reply anon291 20 minutes agorootparentYes, the field of FP is much more, but the core of FP is that. A language doesn't become non-FP simply because it has some imperative abilities, or logic programming builtin, etc. reply AS37 17 hours agorootparentprevI code in C# and use a ton of LINQ when writing business logic. It's FP-ish enough to avoid logic mistakes. The mediator design pattern, which is kind of bringing another FP paradigm to the OO world, also features heavily. reply ska 17 hours agorootparentI think this is the point really, not that mainstream, \"general purpose\" languages support FP well, but that FP ideas and aspects have been adapted into many of them. reply ildjarn 16 hours agorootparentprevC# LINQ, being a round-about implementation of do-notation, is a pretty advanced FP feature, and beyond most mainstream languages. C# is certainly a step up over Java etc. However, most developers wouldn’t understand, say, a result monad implemented via LINQ, so you’re still fighting the ecosystem somewhat. reply paulddraper 7 hours agorootparentprevJava streams. java.time Valhalla even reply macintux 18 hours agoparentprevHybrid languages are sub-optimal in a lot of ways. One of the joys of functional programming are the guarantees that imperative languages can't offer (primarily immutability). reply Terr_ 17 hours agorootparentThe moment you have any database or filesystem or remote-service, the hybridization starts anyway. Sure, technically a single computer's local RAM is being managed with strict guidelines, but that correlates less and less to overall application state and behavior these days. reply marcosdumay 17 hours agorootparentThe promise of FRP was to solve this problem. It makes \"state change\" a first class object, and organizes your code around it. The problem is that nobody made an usable FRP system yet. It's not obvious why it's so hard, and everything feels like it should be easy. But everybody just keep failing. reply macintux 10 hours agorootparentprevTrue, but you can isolate side effects. Knowing that a small collection of functions is not referentially transparent is better than having to deal with any part of the program potentially changing your state. reply dehrmann 8 hours agorootparentprev> Hybrid languages are sub-optimal in a lot of ways. The real world is messy and stateful. reply jprival 17 hours agorootparentprev“Functional style” doesn’t offer the same guarantees, sure, but I wouldn’t underestimate the structural and readability benefits it can offer for certain kinds of tasks. reply ipython 20 hours agoprevFascinating about how ARM has made tremendous strides in the “high end” market since 2015. I would argue that I would categorize RISC as a “yes” with its absolute dominance on mobile and now moving into data center not to mention all the embedded use cases. reply t43562 19 hours agoparentI think the battle between RISC and CISC is overshadowed by the battle between non-x86 and x86. On average are new designs more RISC than CISC? I don't know but that's different from numbers of chips of a certain type sold. RISC showed what you could do with a clean sheet and the battle has really been about whether we can afford the cost of changing to a new instruction set just because it's better by a bit. reply zer00eyz 19 hours agorootparent>> by the battle between non-x86 and x86. 68000 -> PowerPC -> x86 -> arm Apple has been through 4 architectures in 30 years. Given enough money and tyranny (as in \"were doing this\" kind of leadership, vs committee) its apparently not that terrible. reply peterfirefly 13 hours agorootparentAdd the transition to \"32-bit clean\" on 68K (and some trouble with caches) + 32-to-64-bit transitions on PPC and x86. reply simne 14 hours agorootparentprevAnd rumors said, they even considered IBM RS-6000, but pushed to make simplified PowerPC, and IBM had not very good times, so agreed. reply wmf 18 hours agoparentprevI still have to agree with Dan. \"If there's any threat to x86, it's ARM, and it's their business model that's a threat, not their ISA.\" reply titzer 17 hours agorootparentI would have believed this before the M1, but now I think fixed-width instructions are great for parallel decode, so there's a real threat now which is attributable to RISC. reply pclmulqdq 15 hours agorootparentFixed-width decode only one of the ideas of \"RISC,\" but not the main one. I would argue that modern ARM is almost as much of a CISC-y abomination as was x86 in 1999. ARM has a lot of instructions that do multiple things and are very non-RISC. POWER is probably the most RISC-y architecture in use at the high end right now, but it still looks like CISC to the people who originally came up with that idea. reply bdw5204 18 hours agoparentprevAt the moment, x86 is mainly hanging on in legacy spaces because of backward compatibility. Everything new is RISC including Apple's newer Macs. Windows computers still run on x86 because people buy Windows to run legacy code. Playstation and Xbox run on x86 because that makes porting games from Windows more convenient. My view, even when I was studying RISC V in grad school around 6 years ago, was that RISC is clearly superior technically and this would only become more obvious with Moore's Law dying. I think subsequent events are only confirming this view. The strongest evidence for it is that nobody would even think about making a new CISC architecture that isn't x86. reply bluescrn 17 hours agorootparent> At the moment, x86 is mainly hanging on in legacy spaces because of backward compatibility. x86 platforms also tend to let you run your own code, and are associated with 'proper computers/proper operating systems' where you have full access to your own device. The vast majority of non-x86 devices are of the 'locked down and dumbed-down' variety. Content consumption devices built around monopolistic App Stores and touch-centric UIs. They tend to be entirely non-upgradable and, increasingly, actively repair-resistant, too. The market for a 'real computer' may be shrinking, but it's premature to call them 'legacy devices'. (It'd be nice if serious ARM-based PCs became more of an option though, not just little devices like the Pi, or glued-in-battery Apple products, but fully-upgradeable replacements for a high-end x86 workstation or gaming PC) reply marcosdumay 16 hours agorootparent> not just little devices like the Pi What you are dismissing is by-the-book disruptive competition. reply oconnor663 17 hours agoprev> Lampson further remarks that to do parallel programming, what you need to do is put all your parallelism into a little box and then have a wizard go write the code in that box. Not much has changed since 2009. I would like to see a reevaluation of this take with respect to Rust. When we're talking about Rust's safety features, we usually focus on memory safety, because that has the biggest impact on security. But there are lots of memory safe languages, and I think it's actually Rust's thread safety features that are the most unique. (Lots of shared type system machinery between both sets of features.) reply sampo 19 hours agoprev> Fancy type systems We are taking steps to this direction. By adding optional typing to dynamic languages Python and JavaScript/TypeScript. And then type checker tools and local programming style guides are making using these maybe less optional, and more mandatory. reply t43562 19 hours agoparentIMO the day python types become mandatory there will be a fork. It would be such a total betrayal of it's reason for existing that we would have to invent another untyped or duck-typed language again. reply kfajdsl 18 hours agorootparentFor me the primary purpose of TypeScript is to give my editor information about interfaces so I don’t have to do the menial back and forth of accidentally making a typo in a function name, object param, etc. I find myself using less type hints in Python because the story for REPL driven development is so much better than JavaScript, and that takes most of the pain away. reply sampo 19 hours agorootparentprevI mean, your company or your project may require you to write type hints and use a type checker tool. Not that Python the language makes typing mandatory. reply chasd00 19 hours agorootparentprevnagware like Typescript have turned type bolt-ons into a religion almost. It has its place but really gets in the way in others. Hence, the reason untyped languages exist in the first place. reply slaymaker1907 17 hours agoparentprevSum types have definitely been making a big splash. I'd also look at how C++ has added concepts and Go has added generics. Local type inference is also commonplace too, even Java has it. reply peterfirefly 13 hours agorootparentEven C23 has it. reply acchow 17 hours agoparentprevThose would be considered basic type systems. For \"fancy\", you gotta dive deeper into the lambda cube (tho generics I guess in 1999 would be fancy and one step in). reply Ar-Curunir 9 hours agorootparentThe borrow-checker is pretty fancy. reply tptacek 18 hours agoprevGood 'jcranmer on the RISC-CISC thing: https://news.ycombinator.com/item?id=39481940 reply jayd16 19 hours agoprevI guess from some sense, all web apps and all GUI apps using the GPU are using parallelism. That's almost everything. Those are both leveraging shared-nothing type architectures so you can think in a single thread and apply it to concurrent work loads for parallelism. Does that not count? What would satisfy this category? reply dehrmann 8 hours agoparentI think the dream of parallelism in 1999 was that CPUs aren't going to get much faster in terms of clock cycles, so how will algorithms run on more data. This is a bit dumbed-down, but things like sorting in parallel. It turns out that local data didn't quite scale like that, we got more ram and SSDs, and that coordination usually makes small-scale parallel algorithms prohibitively expensive. Where parallelism does work today is some flavor of SIMD like vector instructions, parallel matrix multiplication on a GPU, or map-reduce. reply keybored 18 hours agoprevYes Maybe Firefox Reader Mode CSS reply MattGrommes 17 hours agoparentAlso, Reading Mode in the sidebar of Chrome. The days of square monitors and wall-to-wall text have been over for a while. reply devjab 20 hours agoprevThe phone networks and basic infrastructure for the internet in many countries wouldn’t work without Erlang. Which is functional… but maybe that doesn’t count? reply caditinpiscinam 19 hours agoparentOn the other hand, the banking infrastructure in many countries wouldn't work without COBOL, scientific research wouldn't work without MATLAB, and healthcare wouldn't work without MUMPS. Many languages and paradigms, good and bad, end up entrenched within an industry. reply jnxx 16 hours agorootparent> COBOL > MATLAB > MUMPS One of each uglier than the one before. But: Is there something we can learn from these examples? Are there good reasons for these languages being adopted? And are the Racket designers with their approach of \"a language to define interoperable DSLs\" up to something? reply caditinpiscinam 14 hours agorootparentI think programming languages are more like spoken languages than we give them credit for. Their design is more intentional, but the processes by which they spread, compete, and evolve is similarly difficult to pin down. reply simne 14 hours agorootparentprevYou forget FORTRAN (was very popular for scientific computing, and nearest neighbor with COBOL in S/360 environment) :))) reply simne 14 hours agoprev> That Intel ceded the low end of the market might seem ironic considering Intel's origins, but they've always been aggressive about moving upmarket Will see. Intel after 2023 is not like before 2023, when it lost its dominance to AMD and partially to Nvidia and ARM (Apple-M2). Now lowest end CPUs become RISC-V and ARM pushing to become one of leaders. x86 is strong, but who knows, how long it could dominate under pressure. And if we count not desktops but all personal devices, ARM is already won (on just smartphones, now more ARM CPUs than people on Earth, and Data Centers share of ARM CPUs growing). Only one final thing I could copypaste from \"X-files\" - \"truth is out of there\". reply simne 10 hours agoparentAfter digest information about IBM 360, I decided, we lost CISCs. One of most important feature of 360 was customizable microcode, which you could load on system boot and got effectively different hardware (like with FPGA emulators of Amiga's). It was widely used to emulate old hardware, like IBM 1401 or IBM 7xxx series. But I have not seen this feature in 390 documentation, so looks like their 360 emulation become just software (and with achievements of semiconductors in 1990s it looks like adequate, to switch to software emulation). This was not only feature of 360th, for example Xerox Alto, also have documented feature to alter microcode when need (as I hear, they have special framework to work with it, just like we now work with Assembler when need), and I hear rumors that something similar was shipped with DEC mini-computers, but for micro-computers, this feature practically disappeared. Even when we have \"microcode update\" feature in many modern CPUs, but it is usually undocumented feature, to which nobody have access outside CPU manufacturers (only could upload encrypted binary, supplied by manufacturer to fix bugs). As I said, in 360th, this was documented standard feature, you could use if need. I must admit, ARM marketed feature of customized microcode, to add new instructions (they have standardized place in instruction set, named \"custom coprocessor instructions\", so if you have enough money, you could make special ARM with your additional instructions, you could even order some additions on die), but it is nothing if compare to 360. And I could tell from FPGA cooking, not all things implemented in FPGAs as hardware logic pipelines. When speed accepting, many people using in FPGA practically microcode engines (very common thing is 1-bit CPU, which is very similar to CISC microcode engine, and programmed in Assembler, very similar to early 8-bits; some people prefer full-featured CPU, like 8048 or even more). reply dang 10 hours agoprevRelated: What's worked in Computer Science (2015) - https://news.ycombinator.com/item?id=15796515 - Nov 2017 (62 comments) What's Worked in Computer Science - https://news.ycombinator.com/item?id=10623600 - Nov 2015 (63 comments) reply skybrian 20 hours agoprevFancy type systems have come a long way since then. (TypeScript, for example.) reply pyuser583 19 hours agoparentTS really stands out as amazing. It’s realized a lot of the promise of strong typing. reply malcolmgreaves 18 hours agorootparentI agree that TypeScript is quite amazing. And that it's indeed improved the programs created that run in the browser or on `node`. However, I wanted to point out something that I think often gets overlooked when folks talk about TypeScript. I'd argue that TS has realized it for the web programming masses, who usually don't have any computer science education and thus are unaware of computing fundamentals such as types. TS hasn't realized the promise of strong typing only because it has been quite late to the game, so to speak. I do think I understand where you're coming from. Web programmers are the largest group of programming-related professionals. Bringing CS wins to this large group absolutely does bring the largest gains and impact to industry. There have been many programming languages created decades before TypeScript that have realized the promise of strong typing. Unfortunately, I think this road took too long. Collectively, people have known about these ideas since John Backus's 1977 Turing Award Lecture [1] -- \"Can Programming Be Liberated from the von Neumann Style? A Functional Style and Its Algebra of Programs\" Anyways, I digress. Hopefully this was insightful! Have a great day :) [1] https://dl.acm.org/doi/pdf/10.1145/359576.359579 reply anthk 15 hours agorootparenthttps://hypercubed.github.io/joy/joy.html reply ildjarn 18 hours agorootparentprevTS has a powerful type system that behaves in strange and counter intuitive ways, particular for people coming from JS. It doesn’t even have a spec, really. I would prefer something less powerful but more predictable, personally. reply cgh 18 hours agorootparentprevHow so? It looks a lot like a less-rich Java, whose type system has existed for literally decades. In fact, the generics syntax and type erasure was directly influenced by Java. reply skybrian 13 hours agorootparentI haven’t used Java in many years, but TypeScript’s structural types give it a different flavor than I remember from Java. Unlike in Java, most of the types I define aren’t classes or interfaces. It’s just data that has an expected shape. Check out the design of Zod, for example. I have a bunch of Zod types for the validation of incoming JSON messages. This automatically generates the corresponding TypeScript types through the magic of type inference. It’s quite easy to create discriminated unions, which are just structs where one field has a fixed value. reply ildjarn 18 hours agorootparentprevTypescript has far more powerful type constructs than Java. reply za3faran 16 hours agorootparentSuch as? reply ildjarn 16 hours agorootparenthttps://github.com/codemix/ts-sql reply acchow 16 hours agorootparentprevTho Typescript does have structural types, mapped types, union types, optional types, type aliases..... Bunch of useful type features. reply coev 18 hours agorootparentprevMore like indirectly influenced by Java by way of C# (Anders Hejlsberg designed both C# and TS) reply pionar 18 hours agorootparentprevI disagree. The language was probably more inspired by C#, which is itself inspired by Java. Type erasure is just part of transpiling to JS. reply blt 20 hours agoprevThe link to \"Microsoft's driver verification tool\", which \"has probably had more impact than all formal chip design tools combined\", is broken. Does anyone know which tool this meant? reply eimrine 19 hours agoparentThe document is still searchable: https://www.microsoft.com/en-us/research/wp-content/uploads/... reply nequo 19 hours agoparentprevThat link is archived here: https://web.archive.org/web/20160416132849/http://research.m... reply hosh 19 hours agoprevDo GPUs and TPUs for AI count towards parallelism? reply sophacles 18 hours agoparentI don't think so, they still tend to fall into the \"Lampson further remarks that to do parallel programming, what you need to do is put all your parallelism into a little box and then have a wizard go write the code in that box.\" statment, for the most part. reply jnxx 16 hours agorootparentWhat could be a different case is Clojure. But I am not sure, and the performance characteristics of Clojure make it well-suited for server-style concurrency, but not so much for high-performance parallelism. reply aleign 19 hours agoprevWhat is \"capabilities\" supposed to mean? reply jayd16 19 hours agoparentProbably Capability based security https://en.wikipedia.org/wiki/Capability-based_security reply narrator 19 hours agorootparentFunny how in the 90s security was an enterprise feature and now better security and cryptography than we had in the 90s is on every $15 android phone. reply aleign 19 hours agorootparentprevThank you :) reply jnxx 16 hours agoparentprev> What is \"capabilities\" supposed to mean? \"Is this app allowed to read your contacts?\" reply Legend2440 19 hours agoprevNeural networks: 1999 - No 2015 - Not really 2024 - Yes? reply Calavar 18 hours agoparentI'd say 2005 to 2010 is probably more probably more appropriate for not really. By 2015, neural networks were already a really hot research topic. Just off the top of my head, the seq2seq paper was published in 2014 and U-Net and ResNet were published in 2015. reply ska 17 hours agorootparentThis is eliding most of the history of neural networks as a hot reasearch topics. Since the late 50's early 1960s they've had several resurgences in interest, e.g. 80s RNNs, 90s stuff around several centers. Hell people were doing interesting things with them commercially in the 90's. The late 90s and early 00's had a lot of other interest (kernel methods, SVMs) but NN folks kept plugging, and the hardware to hit the next level was just around the corner. The resurgence you're noting are papers with a 10 year tail before them (hell, most of the deep concepts were initiated decades before but lacked both the data sources and efficient hardware to really work them out). This stuff has a long and deeply connected history. reply IshKebab 19 hours agoprevInteresting how much has changed since 2015. Apple's M chips have made RISC is a clear yes now. Rust has a pretty fancy type system. Not Haskell-fancy but still. Functional programming is pretty much an expected feature now. reply aleph_minus_one 19 hours agoparent> Apple's M chips have made RISC is a clear yes now. ARM is not a pure RISC architecture (even though its instruction set is somewhat inspired by those ideas behind RISC that stood the test of time). reply IshKebab 19 hours agorootparentWhat counts as \"pure RISC\"? reply simne 13 hours agorootparentGood question. Now things are complicated. When all beginning, CISC CPUs was not one chip but board or even cabinet (like IBM S/360), and they featured customizable microcode, so you could even load different microcode on different CPUs in system. As I know, very powerful feature of S/360 series was to supply custom microcode to make machine compatible with older IBM hardware, like 1401 series or 7xxx series (or to make your own architecture if you wish and if you have enough money). I have not hear about customizable microcode in RISC (to be honest, I hear, it exists in ARM, but it is rare used option, if compare with IBM 360). reply SV_BubbleTime 18 hours agorootparentprevAsk the Scottish. Elsewhere in the comment section is describe the question isn’t RISC/CISC, it’s x86/Nonx86. And far more interesting since those lines are still fairly well established. reply Divver 19 hours agoprevI wish this was updated for 2024 His RISC being “No” most would agree is somewhat incorrect though it seemed correct back in 2015. But it kinda goes to show how these yes/maybe/no things can evolve unexpectedly over time and not be to taken as gospel that will stand the test of time. reply sys_64738 19 hours agoprev [–] Formal Methods was for people who didn't want to write any code. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "In 1999, Butler Lampson presented a talk on the computer systems research landscape, emphasizing virtual memory, parallelism, and capabilities.",
      "Lampson discussed the challenges in programming for parallelism and the shift from RISC technology to obsolescence.",
      "The talk touched on Intel's dominance over DEC's Alpha chips, the emergence of languages like PHP and Python, and computer systems research moving towards widespread acceptance."
    ],
    "commentSummary": [
      "The discussion delves into the evolution and convergence of RISC (Reduced Instruction Set Computing) and CISC (Complex Instruction Set Computing) architectures in computer design.",
      "It highlights the significant role of RISC in modern designs such as ARM, emphasizing the impact of Moore's Law and custom coprocessor instructions on computer architecture.",
      "Moreover, it explores the adoption of functional programming in mainstream languages, the development of TypeScript compared to Java, and advancements in security, cryptography, and neural networks affecting modern processor design and programming languages."
    ],
    "points": 128,
    "commentCount": 113,
    "retryCount": 0,
    "time": 1710521596
  },
  {
    "id": 39718389,
    "title": "OpenVPN Vulnerable to VPN Fingerprinting",
    "originLink": "https://arxiv.org/abs/2403.03998",
    "originBody": "Computer Science > Cryptography and Security arXiv:2403.03998 (cs) [Submitted on 6 Mar 2024] Title:OpenVPN is Open to VPN Fingerprinting Authors:Diwen Xue, Reethika Ramesh, Arham Jain, Michalis Kallitsis, J. Alex Halderman, Jedidiah R. Crandall, Roya Ensafi Download PDF HTML (experimental) Abstract:VPN adoption has seen steady growth over the past decade due to increased public awareness of privacy and surveillance threats. In response, certain governments are attempting to restrict VPN access by identifying connections using \"dual use\" DPI technology. To investigate the potential for VPN blocking, we develop mechanisms for accurately fingerprinting connections using OpenVPN, the most popular protocol for commercial VPN services. We identify three fingerprints based on protocol features such as byte pattern, packet size, and server response. Playing the role of an attacker who controls the network, we design a two-phase framework that performs passive fingerprinting and active probing in sequence. We evaluate our framework in partnership with a million-user ISP and find that we identify over 85% of OpenVPN flows with only negligible false positives, suggesting that OpenVPN-based services can be effectively blocked with little collateral damage. Although some commercial VPNs implement countermeasures to avoid detection, our framework successfully identified connections to 34 out of 41 \"obfuscated\" VPN configurations. We discuss the implications of the VPN fingerprintability for different threat models and propose short-term defenses. In the longer term, we urge commercial VPN providers to be more transparent about their obfuscation approaches and to adopt more principled detection countermeasures, such as those developed in censorship circumvention research. Comments: In: USENIX Security Symposium 2022 (USENIX Security '22) Subjects: Cryptography and Security (cs.CR) Cite as: arXiv:2403.03998 [cs.CR](or arXiv:2403.03998v1 [cs.CR] for this version)https://doi.org/10.48550/arXiv.2403.03998 Focus to learn more arXiv-issued DOI via DataCite Journal reference: 31st USENIX Security Symposium (USENIX Security 22). 2022 Submission history From: Diwen Xue [view email] [v1] Wed, 6 Mar 2024 19:15:02 UTC (25,370 KB) Full-text links: Access Paper: Download PDF HTML (experimental) TeX Source Other Formats view license Current browse context: cs.CRnewrecent2403 Change to browse by: cs References & Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer (What is the Explorer?) Litmaps Toggle Litmaps (What is Litmaps?) scite.ai Toggle scite Smart Citations (What are Smart Citations?) Code, Data, Media Code, Data and Media Associated with this Article Links to Code Toggle CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub Toggle DagsHub (What is DagsHub?) GotitPub Toggle Gotit.pub (What is GotitPub?) Links to Code Toggle Papers with Code (What is Papers with Code?) ScienceCast Toggle ScienceCast (What is ScienceCast?) Demos Demos Replicate Toggle Replicate (What is Replicate?) Spaces Toggle Hugging Face Spaces (What is Spaces?) Spaces Toggle TXYZ.AI (What is TXYZ.AI?) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower (What are Influence Flowers?) Connected Papers Toggle Connected Papers (What is Connected Papers?) Core recommender toggle CORE Recommender (What is CORE?) About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs. Which authors of this paper are endorsers?Disable MathJax (What is MathJax?)",
    "commentLink": "https://news.ycombinator.com/item?id=39718389",
    "commentBody": "OpenVPN Is Open to VPN Fingerprinting (arxiv.org)127 points by PaulHoule 19 hours agohidepastfavorite53 comments tamimio 15 hours agoI remember years ago someone made a site that detects if you are using a vpn based on some packets latency, and it was pretty accurate! Unfortunately, I don’t know what’s the website now. reply madars 15 hours agoparentMight be http://witch.valdikss.org.ru/ , e.g., using number of flows and MTU (and maybe other techniques) reply ogurechny 9 hours agorootparentBy the way, the author has a small forum with lots of updates on developments (and bugs) in large scale DPI systems. https://ntc.party What governments use today for censorship and spying will tomorrow be a one-click solution for small businesses and script kiddies. reply ameyv 4 hours agorootparentprevKaspersky is flagging this website as HEUR:Exploit.Script.Generic;Trojan;High;Heuristic Analysis;http://witch.valdikss.org.ru; Expert analysis Pls be careful. reply username135 11 hours agorootparentprevPretty neat. Doesnt pick up Proton, which I guess is good. reply tamimio 15 hours agorootparentprev> Might be http://witch.valdikss.org.ru/ I think so! It looks slightly different than how I remember it but same elements in there, thanks for sharing it. reply Izkata 13 hours agorootparent> No OpenVPN detected. Looks like Mullvad is clear. Also in case anyone was curious about the name and image, I haven't thought about this series in forever: https://en.wikipedia.org/wiki/W.I.T.C.H._(TV_series) reply sunsay 10 hours agorootparent>OpenVPN detected. Not for me. It detected Mulvad reply wraptile 9 hours agorootparentProtonVPN passes too which is funny because I'm solving so many captchas I could power an entire captcha solving saas all by myself. So I'm not entirely convinced by this method. reply Zenul_Abidin 4 hours agorootparentThat's because most sites serve you a captcha based on your IP address and ASN. reply nichch 9 hours agorootparentprevMullvad uses both OpenVPN and Wireguard reply dishsoap 7 hours agorootparentAnd the site seems to tend to detect mullvad's wireguard as 'openvpn' while mullvad's openvpn is undetected by it. Perhaps that's what the parent is referring to. reply Izkata 7 hours agorootparentThat would make sense because it didn't detect Mullvad's openvpn for me. reply anjel 6 hours agorootparentprevwindscribe/wireguard not detected reply topato 8 hours agorootparentprevAll Vypr obfuscation options pass. I'm surprised more people don't use VyprVPN, but then again, it's professional options are limited. I.e. surfshark's ability to choose specific data centers and IPs.But I bought 5 years of Vypr for like 50 bucks 3 years ago, so I'm pretty invested Edit: I forgot I use VyprVPN because of it's verified no-log, does anyone else know of a better service with verified history of denying log access / having active subpoena dead man's switch notices? reply Zenul_Abidin 4 hours agorootparentExpressVPN but they sold out to shady folks a few years ago. reply fastily 6 hours agoparentprevMaybe it was doileak.com? It’s now at https://www.top10vpn.com/tools/do-i-leak/ reply guardiangod 15 hours agoprevAll those firewalls with Application Control IPS (Checkpoint, Palo Alto Network, Fortinet etc.) can already block OpenVPN connections, so this is no surprise that you can fingerprint them. reply nickburns 15 hours agoparentyou have the cart before the horse here. modern IPS uses, and has been using, more or less the same methodology the researchers mention in their abstract (full disclosure: i read no further): \"[. . .] fingerprints based on protocol features such as byte pattern, packet size, and server response.\" this technique has been around for a very long time and is no way novel. applying it to OpenVPN traffic specifically isn't either. reply lilsoso 15 hours agoparentprevThey can determine a connection to their network is through an OpenVPN server even if that server has a clean/normal IP address? Is there some otherwise basic tell that the host is running a VPN server? Could Palo Alto Network also identify say a different VPN server, such as Wireguard? reply jcrawfordor 13 hours agorootparentThis paper is about detecting the connection between the VPN client and the VPN server---the first segment of the connection you describe. It's unsurprising that OpenVPN can be fingerprinted, as like most VPN protocols, it was not designed with contravention of fingerprinting as a goal. The counter-censorship or network policy bypass application of VPNs is a relatively modern concept and the modifications made to meet it tend to be haphazard. OpenVPN predates this kind of application being a design goal. To address your direct question, whether or not a service can detect that you are reaching it with a VPN service in the middle, the answer is a soft maybe. There are several heuristic methods, but they will not be entirely reliable and using them will risk false positives. Most service operators probably wouldn't go beyond filtering of known VPN services, which is of course widely implemented. One reliable method is active probing of the traffic source, which is sometimes done, but it comes with some hazards for the service operator and is often easy to defeat. reply ranger_danger 14 hours agorootparentprevMy understanding after talking to OpenVPN developers is that a known header is exchanged PRIOR to starting a TLS session, thus making it extremely easy to detect (and block). reply nickburns 14 hours agorootparentprevliteral bytes. this is one of the primary methods modern IDS/IPS engines, like Snort and Suricata for example, use to fingerprint traffic types and otherwise indicators of compromise. OpenVPN traffic, even encrypted, can look unique enough somewhere in the 'stream' (to borrow the IDS/IPS term) to be reliably idenitfied. reply lilsoso 14 hours agorootparentThanks, I didn't know that. So if you have a VPN server at home and you bounce through it from a foreign location to a corporate job then perhaps the employer could identify the connection is a relay. I'm talking about the part of the connection outgoing from the VPN, not the incoming traffic to the VPN, to be clear. I know for example that China can do deep packet inspection and that there are a number of projects to attempt to thwart this technique. But you seem to be saying that the part after the VPN can be identified? reply elwebmaster 8 hours agorootparentNo, the article is about you connecting to your home from the corporate network over OpenVPN. The case you are describing, while possible, is highly unlikely to be detected unless you are using a public VPN. Most of the time your employer just cares to check a box saying employees are working from the US and has no incentive to go the extra mile to active traffic monitoring and deep packet inspection. Hell, some are so incompetent, a CTO once said employees can work offshore as long as they are using Remote Desktop to a VM in the US because then they are “telecommuting”, but they can’t connect over the corporate VPN. reply nickburns 12 hours agorootparentprevI'm talking about the part of the connection outgoing from the VPN your understanding is correct—that the 'segment' between VPN server and final destination/employer's public-facing infrastucture is no longer traversing a VPN tunnel and therefore could not be fingerprinted as VPN traffic. if using a public VPN service provider, it would be identified, however (quite easily and at very low technical cost mind you), based on source address, as public VPN service provider netblocks are well-documented. see, for example: https://github.com/X4BNet/lists_vpn (first search engine result for me querying \"vpn ip list\") reply riedel 12 hours agoparentprevI guess the great firewall of china is a good benchmark too. It also scales quite good I guess. reply clwg 4 hours agoprevThis was the case over a decade ago as well, at the time obfsproxy[0] was one of the more reliable methods for establishing and maintaining openvpn connections in the more censorship heavy countries. [0]https://blog.torproject.org/obfsproxy-next-step-censorship-a... reply dfawcus 12 hours agoprevHow do these fingerprinting schemes work when the handshake protocol is not merely obfuscated, but actually encrypted? i.e. if one uses the tls-crypt option? As I understand it, that encrypts the handshake protocol such that simple data value matching will not work, and one would have to either use length and/or timing matches. reply gruez 8 hours agoparentThey can do what the gfw does and block any encrypted traffic that it doesn't recognize. https://www.usenix.org/conference/usenixsecurity23/presentat... reply niceice 9 hours agoprevSo what's the good VPN? reply some_furry 8 hours agoparentWireGuard reply thayne 7 hours agorootparentI would be surprised if wireguard was fingerprint resistant. If you want to avoid a Mitm from detecting that you are using a VPN your best bet is probably to use some kind of tunnel that looks like regular https traffic. which means it uses TLS either with TCP or QUIC on port 443. reply xrisk 5 hours agorootparentObfs4 is what you need reply Timber-6539 8 hours agoparentprevAny provider with shadowsocks configs. Or if not, if you have hardware lying around you could host a tunnel with gluetun. reply thwarted 16 hours agoprevThis required research and publication on arxiv? OpenVPN is meant for access control to/between private networks, not for skirting public access controls put in place on your immediate, local upstream. The default config even encourages the use of the defined ports. reply dc-programmer 13 hours agoparentIt seems like other VPN vendors are slapping obfuscation on top of OpenVPN and advertising their service as unobservable. This paper contests that claim reply grubbs 15 hours agoparentprevDefault config with port 1194 is super common with \"anonymous\" VPN providers. It can very well be fingerprinted. But I hope the data in transit would be secure. Maybe not from NSA. reply nimbius 16 hours agoparentprevcorrect. it sorta depends on what OpenVPNs goals are... the boilerplate of the corporate face insists its for your businesses and their connectivity, so you could argue that confidentiality doesnt really include clandestine or obfuscated traffic presence at all. However, you could also argue for OpenVPN (and several others) that as a security tool they should at least consider Goguen and Meseguer type noninterference as a conformant operation model by reducing the awareness of the traffic. reply gsich 13 hours agoparentprev>not for skirting public access controls put in place on your immediate, local Of course it's also meant for that. reply mianos 15 hours agoprevA simple search \"OpenVPN traffic detection\" leads you to many pages on how this is not a thing OpenVPN tries to do and how to detect it. This whole paper is no more notable than a stack overflow question and answer, maybe less than something on quora. reply mianos 13 hours agoparenthere is one of many for those that seem to be voting me down: https://github.com/corelight/zeek-spicy-openvpn I can't say this would be much more of a weekend project with spicy. reply jerhewet 16 hours agoprevJesus wept. https://www.doileak.com/classic.html reply LegitShady 15 hours agoparentthis site says I have third party cookies enabled when firefox says I do not. reply nickburns 15 hours agorootparentthen you have 3P cookies enabled. reply iLoveOncall 15 hours agoprev [–] Fingerprinting? This is just clickbait. Identifying that the murder weapon was a knife isn't remotely the same as getting the fingerprint of the killer. reply riehwvfbk 14 hours agoparentContext matters. In this case \"fingerprinting\" refers to fingerprinting of the protocol by a DPI system, and the problem the author is concerned with is the ability to use a VPN at all. reply iLoveOncall 13 hours agorootparentWords matter too. It's simply not fingerprinting. reply autoexec 13 hours agorootparentIt really is fingerprinting. It’s just showing the fingerprint of the service and not the user. This is a very normal usage of the word for people interested in identifying specific types of traffic. Lots of things different things have detectable fingerprints that are useful within certain contexts. reply JamesSwift 13 hours agoparentprevFingerprinting doesnt mean \"uniquely identified an individual\", it just means \"uniquely identifies some aspect of the target\". You could fingerprint 'firefox' amongst all browsers, or in this case they are fingerprinting OpenVPN amongst all traffic. reply sfmike 6 hours agoparentprevit is. what if we know that only one person had said knife and then this \"fingerprint\" would be 100% empirical support. So yes sometimes just noting a VPN was used with said foreknowledge is enough to correlate and prove something. reply rileymat2 15 hours agoparentprev [–] The term fingerprinting is in really common usage for this, “browser fingerprinting” reply nickburns 15 hours agorootparent [–] you guys... https://en.wikipedia.org/wiki/Fingerprint_(computing) reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The paper explores how OpenVPN is susceptible to VPN fingerprinting, allowing for the blocking of VPN connections based on the developed framework's accurate identification of OpenVPN connections.",
      "Over 85% of OpenVPN flows can be distinguished with few false positives, including those using \"obfuscated\" VPN setups, emphasizing the need for short-term defenses and transparency from VPN providers regarding their obfuscation methods.",
      "Presented at the USENIX Security Symposium 2022, this research sheds light on the importance of addressing vulnerabilities in VPN technologies to enhance user privacy and security."
    ],
    "commentSummary": [
      "The article delves into OpenVPN's vulnerability to VPN fingerprinting, as users share their experiences detecting various VPN services on a website.",
      "It explores the limitations and preferences of different VPN services, methods to detect and block OpenVPN traffic, and concerns surrounding privacy and security.",
      "Discussions include techniques to evade detection, alternative VPN choices, and debates on the precision of the term \"fingerprinting\" in recognizing internet traffic."
    ],
    "points": 127,
    "commentCount": 53,
    "retryCount": 0,
    "time": 1710524049
  },
  {
    "id": 39716494,
    "title": "Introduction to Reverse Engineering: x86 Assembly & C Code",
    "originLink": "https://0x44.cc/reversing/2021/07/21/reversing-x86-and-c-code-for-beginners.html",
    "originBody": "Reversing for dummies - x86 assembly and C code (Beginner/ADHD friendly) 21 Jul 2021 • Reverse Engineering Context Before I got into reverse engineering, executables always seemed like black magic to me. I always wondered how stuff worked under the hood, and how binary code is represented inside .exe files, and how hard it is to modify this ‘compiled code’ without access to the original source code. But one of the main intimidating hurdles always seemed to be the assembly language, it’s the thing that scares most people away from trying to learn about this field. That’s the main reason why I thought of writing this straight-to-the-point article that only contains the essential stuff that you encounter the most when reversing, albeit missing crucial details for the sake of brevity, and assumes the reader has a reflex of finding answers online, looking up definitions, and more importantly, coming up with examples/ideas/projects to practice on. The goal is to hopefully guide an aspiring reverse engineer and arouse motivation towards learning more about this seemingly elusive passion. Note: This article assumes the reader has elementary knowledge regarding the hexadecimal numeral system, as well as the C programming language, and is based on a 32-bit Windows executable case study - results might differ across different OSes/architectures. Introduction Compilation After writing code using a compiled language, a compilation takes place (duh), in order to generate the output binary file (an example of such is an .exe file). Compilers are sophisticated programs which do this task. They make sure the syntax of your ugly code is correct, before compiling and optimizing the resulting machine code by minimizing its size and improving its performance, whenever applicable. Binary code As we were saying, the resulting output file contains binary code, which can only be ‘understood’ by a CPU, it’s essentially a succession of varying-length instructions to be executed in order - here’s what some of them look like: CPU-readable instruction data (in hex) Human-readable interpretation 55 push ebp 8B EC mov ebp, esp 83 EC 08 sub esp, 8 33 C5 xor eax, ebp 83 7D 0C 01 cmp dword ptr [ebp+0Ch], 1 These instructions are predominantly arithmetical, and they manipulate CPU registers/flags as well as volatile memory, as they’re executed. CPU registers A CPU register is almost like a temporary integer variable - there’s a small fixed number of them, and they exist because they’re quick to access, unlike memory-based variables, and they help the CPU keep track of its data (results, operands, counts, etc.) during execution. It’s important to note the presence of a special register called the FLAGS register (EFLAGS on 32-bit), which houses a bunch of flags (boolean indicators), which hold information about the state of the CPU, which include details about the last arithmetic operation (zero: ZF, overflow: OF, parity: PF, sign: SF, etc.). CPU registers visualized while debugging a 32-bit process on x64dbg, a debugging tool. Some of these registers can also be spotted on the assembly excerpt mentioned previously, namely: EAX, ESP (stack pointer) and EBP (base pointer). Memory access As the CPU executes stuff, it needs to access and interact with memory, that’s when the role of the stack and the heap comes. These are (without getting into too much detail) the 2 main ways of ‘keeping track of variable data’ during the execution of a program: 🥞 Stack The simpler and faster of the two - it’s a linear contiguous LIFO (last in = first out) data structure with a push/pop mechanism, it serves to remember function-scoped variables, arguments, and keeps track of calls (ever heard of a stack trace?) ⛰ Heap The heap, however, is pretty unordered, and is for more complicated data structures, it’s typically used for dynamic allocations, where the size of the buffer isn’t initially known, and/or if it’s too big, and/or needs to be modified later. Assembly instructions As I’ve mentioned earlier, assembly instructions have a varying ‘byte-size’, and a varying number of arguments. Arguments can also be either immediate (‘hardcoded’), or they can be registers, depending on the instruction: 55 push ebp ; size: 1 byte, argument: register 6A 01 push 1 ; size: 2 bytes, argument: immediate Let’s quickly run through a very small set of some of the common ones we’ll get to see - feel free to do your own research for more detail: Stack operations push value ; pushes a value into the stack (decrements ESP by 4, the size of one stack ‘unit’). pop register ; pops a value to a register (increments ESP by 4). Data transfer mov destination, source ; moves copies a value from/to a register. mov destination, [expression] ; copies a value from a memory address resolved from a ‘register expression’ (single register or arithmetic expression involving one or more registers) into a register. Flow control jmp destination ; jumps into a code location (sets EIP (instruction pointer)). jz/je destination ; jumps into a code location if ZF (the zero flag) is set. jnz/jne destination ; jumps into a code location if ZF is not set. Operations cmp operand1, operand2 ; compares the 2 operands and sets ZF if they’re equal. add operand1, operand2 ; operand1 += operand2; sub operand1, operand2 ; operand1 -= operand2; Function transitions call function ; calls a function (pushes current EIP, then jumps to the function). retn ; returns to caller function (pops back the previous EIP). Note: You might notice the words ‘equal’ and ‘zero’ being used interchangeably in x86 terminology - that’s because comparison instructions internally perform a subtraction, which means if the 2 operands are equal, ZF is set. Assembly patterns Now that we have a rough idea of the main elements used during the execution of a program, let’s get familiarized with the patterns of instructions that you can encounter reverse engineering your average everyday 32-bit PE binary. Function prologue A function prologue is some initial code embedded in the beginning of most functions, it serves to set up a new stack frame for said function. It typically looks like this (X being a number): 55 push ebp ; preserve caller function's base pointer in stack 8B EC mov ebp, esp ; caller function's stack pointer becomes base pointer (new stack frame) 83 EC XX sub esp, X ; adjust the stack pointer by X bytes to reserve space for local variables Function epilogue The epilogue is simply the opposite of the prologue - it undoes its steps to restore the stack frame of the caller function, before it returns to it: 8B E5 mov esp, ebp ; restore caller function's stack pointer (current base pointer) 5D pop ebp ; restore base pointer from the stack C3 retn ; return to caller function Now at this point, you might be wondering - how do functions talk to each other? How exactly do you send/access arguments when calling a function, and how do you receive the return value? That’s precisely why we have calling conventions. Calling conventions: __cdecl A calling convention is basically a protocol used to communicate with functions, there’s a few variations of them, but they share the same principle. We will be looking at the __cdecl (C declaration) convention, which is the standard one when compiling C code. In __cdecl (32-bit), function arguments are passed on the stack (pushed in reverse order), while the return value is returned in the EAX register (assuming it’s not a float). This means that a func(1, 2, 3); call will generate the following: 6A 03 push 3 6A 02 push 2 6A 01 push 1 E8 XX XX XX XX call func Putting everything together Assuming func() simply does an addition on the arguments and returns the result, it would probably look like this: int __cdecl func(int, int, int): prologue: 55 push ebp ; save base pointer 8B EC mov ebp, esp ; new stack frame body: 8B 45 08 mov eax, [ebp+8] ; load first argument to EAX (return value) 03 45 0C add eax, [ebp+0Ch] ; add 2nd argument 03 45 10 add eax, [ebp+10h] ; add 3rd argument epilogue: 5D pop ebp ; restore base pointer C3 retn ; return to caller Now if you’ve been paying attention and you’re still confused, you might be asking yourself one of these 2 questions: 1) Why do we have to adjust EBP by 8 to get to the first argument? If you check the definition of the call instruction we mentioned earlier, you’ll realize that, internally, it actually pushes EIP to the stack. And if you also check the definition for push, you’ll realize that it decrements ESP (which is copied to EBP after the prologue) by 4 bytes. In addition, the prologue’s first instruction is also a push, so we end up with 2 decrements of 4, hence the need to add 8. 2) What happened to the prologue and epilogue, why are they seemingly ‘truncated’? It’s simply because we haven’t had a use for the stack during the execution of our function - if you’ve noticed, we haven’t modified ESP at all, which means we also don’t need to restore it. If conditions To demo the flow control assembly instructions, I’d like to add one more example to show how an if condition was compiled to assembly. Assume we have the following function: void print_equal(int a, int b) { if (a == b) { printf(\"equal\"); } else { printf(\"nah\"); } } After compiling it, here’s the disassembly that I got with the help of IDA: void __cdecl print_equal(int, int): 10000000 55 push ebp 10000001 8B EC mov ebp, esp 10000003 8B 45 08 mov eax, [ebp+8] ; load 1st argument 10000006 3B 45 0C cmp eax, [ebp+0Ch] ; compare it with 2nd ┌┅ 10000009 75 0F jnz short loc_1000001A ; jump if not equal ┊ 1000000B 68 94 67 00 10 push offset aEqual ; \"equal\" ┊ 10000010 E8 DB F8 FF FF call _printf ┊ 10000015 83 C4 04 add esp, 4 ┌─┊─ 10000018 EB 0D jmp short loc_10000027 │ ┊ │ └ loc_1000001A: │ 1000001A 68 9C 67 00 10 push offset aNah ; \"nah\" │ 1000001F E8 CC F8 FF FF call _printf │ 10000024 83 C4 04 add esp, 4 │ └── loc_10000027: 10000027 5D pop ebp 10000028 C3 retn Give yourself a minute and try to make sense of this disassembly output (for simplicity’s sake, I’ve changed the real addresses and made the function start from 10000000 instead). In case you’re wondering about the add esp, 4 part, it’s simply there to adjust ESP back to its initial value (same effect as a pop, except without modifying any register), since we had to push the printf string argument. Basic data structures Now let’s move on and talk about how data is stored (integers and strings especially). Endianness Endianness is the order of the sequence of bytes representing a value in computer memory. There’s 2 types - big-endian and little-endian: For reference, x86 family processors (the ones on pretty much any computer you can find) always use little-endian. To give you a live example of this concept, I’ve compiled a Visual Studio C++ console app, where I declared an int variable with the value 1337 assigned to it, then I printed the variable’s address using printf(), on the main function. Then I ran the program attached to the debugger in order to check the printed variable’s address on the memory hex view, and here’s the result I obtained: To elaborate more on this - int variables are 4 bytes long (32 bits) (in case you didn’t know), so this means that if the variable starts from the address D2FCB8 it would end right before D2FCBC (+4). To go from human readable value to memory bytes, follow these steps: decimal: 1337 -> hex: 539 -> bytes: 00 00 05 39 -> little-endian: 39 05 00 00 Signed integers This part is interesting yet relatively simple. What you should know here is that integer signing (positive/negative) is typically done on computers with the help of a concept called two’s complement. The gist of it is that the lowest/first half of an integer is reserved for positive numbers, while the highest/last half is for negative numbers, here’s what this looks like in hex, for a 32-bit signed int (highlighted = hex, in parenthesis = decimal): Positives (1/2): 00000000 (0) -> 7FFFFFFF (2,147,483,647 or INT_MAX) Negatives (2/2): 80000000 (-2,147,483,648 or INT_MIN) -> FFFFFFFF (-1) If you’ve noticed, we’re always ascending in value. Whether we go up in hex or decimal. And that’s the crucial point of this concept - arithmetical operation do not have to do anything special to handle signing, they can simply treat all values as unsigned/positive, and the result would still be interpreted correctly (as long as we don’t go beyond INT_MAX or INT_MIN), and that’s because integers will also ‘rollover’ on overflow/underflow by design, kinda like an analog odometer. Protip: The Windows calculator is a very helpful tool - you can set it to programmer mode and set the size to DWORD (4 bytes), then enter negative decimal values and visualize them in hex and binary, and have fun performing operations on them. Strings In C, strings are stored as char arrays, therefore, there’s nothing special to note here, except for something called null termination. If you ever wondered how strlen() is able to know the size of a string, it’s very simple - strings have a character that indicates their end, and that’s the null byte/character - 00 or '\\0'. If you declare a string constant in C code, and hover over it in Visual Studio, for instance, it will tell you the size of the generated array, and as you can see, for this reason, it’s one element more than the ‘visible’ string size. Note: The endianness concept is not applicable on arrays, only on single variables. Therefore, the order of characters in memory would be normal here - low to high. Making sense of call and jmp instructions Now that you know all of this, you’re likely able to start making sense of some machine code, and emulate a CPU with your brain, to some extent, so to speak. Let’s take the print_equal() example, but let’s only focus on the printf() call instructions this time. void print_equal(int, int): ... 10000010 E8 DB F8 FF FF call _printf ... 1000001F E8 CC F8 FF FF call _printf You might be wondering to yourself - wait a second, if these are the same instructions, then why are their bytes different? That’s because, call (and jmp) instructions (usually) take an offset (relative address) as an argument, not an absolute address. An offset is basically the difference between the current location, and the destination, which also means that it can be either negative or positive. As you can see, the opcode of a call instruction that takes a 32-bit offset, is E8, and is followed by said offset - which makes the full instruction: E8 XX XX XX XX. Pull out your calculator, why’d you close it so early?! and calculate the difference between the offset of both instructions (don’t forget the endianness). You’ll notice that (the absolute value of) this difference is the same as the one between the instruction addresses (1000001F - 10000010 = F): Another small detail that we should add, is the fact that the CPU only executes an instruction after fully ‘reading’ it, which means that by the time the CPU starts ‘executing’, EIP (the instruction pointer) is already pointing at the next instruction to be executed. That’s why these offsets are actually accounting for this behaviour, which means that in order to get the real address of the target function, we have to also add the size of the call instruction: 5. Now let’s apply all these steps in order to resolve printf()’s address from the first instruction on the example: 10000010 E8 DB F8 FF FF call _printf 1) Extract the offset from the instruction: E8 (DB F8 FF FF) -> FFFFF8DB (-1829) 2) Add it to the instruction address: 10000010 + FFFFF8DB = 0FFFF8EB 3) And finally, add the instruction size: 0FFFF8EB + 5 = 0FFFF8F0 (&printf) The exact same principle applies to the jmp instruction: ... ┌─── 10000018 EB 0D jmp short loc_10000027 ... └── loc_10000027: 10000027 5D pop ebp ... The only difference in this example is that EB XX is a short version jmp instruction - which means it only takes an 8-bit (1 byte) offset. Therefore: 10000018 + 0D + 2 = 10000027 Conclusion That’s it! You should now have enough information (and hopefully, motivation) to start your journey reverse engineering executables. Start by writing dummy C code, compiling it, and debugging it while single-stepping through the disassembly instructions (Visual Studio allows you to do this, by the way). Compiler Explorer is also an extremely helpful website which compiles C code to assembly for you in real time using multiple compilers (select the x86 msvc compiler for Windows 32-bit). After that, you can try your luck with closed-source native binaries, by the help of disassemblers such as Ghidra and IDA, and debuggers such as x64dbg. Note: If you’ve noticed inaccurate information, or room for improvement regarding this article, and would like to improve it, feel free to submit a pull request on GitHub. Thanks for reading! (edited) If you've enjoyed this article, feel free to Comments Load Comments Related Posts Reverse engineering a car key fob signal (Part 1) • 13 Mar 2024 Google Assistant YouTube command for smart TVs • 02 Mar 2021 Unlocking IAM's Nokia G-240W-A router (Part 1) • 08 Oct 2019",
    "commentLink": "https://news.ycombinator.com/item?id=39716494",
    "commentBody": "Reversing for dummies – x86 assembly and C code (Beginner/ADHD friendly) (0x44.cc)115 points by wglb 22 hours agohidepastfavorite36 comments haswell 21 hours agoMaybe it’s just my pattern matching brain and current interests noticing this more, but there seems to be an uptick in content that bridges the knowledge gap between assembly and higher level languages, and that’s been awesome for someone diving in at this later phase of my career. As a self taught developer, I tried to learn “bottom up”, i.e. starting from low level concepts in my early teens. Everything I tried to learn bounced off of my brain, and what really got me going was picking up Python, and later Ruby (not rails) in the early 2000s. This eventually grew into a successful career, and I spent most of my professional life not really knowing much more than the general gist of lower level stuff. More recently, I decided to go back to trying to learn this and it’s like discovering a whole new world. 20 years of coding gave me great intuitions for higher level code, but left all of these unanswered questions about how it works at a lower level. And it’s making me fall in love with code all over again, because now it all makes so much sense, and answers these questions I sometimes forgot that I had. Taking this a step further and going down to the level of circuits is even more mind expanding, and has been a joy. If you’ve been coding for a long time and this all seems hard to imagine breaking into, all I can say is: try again! You might be surprised at how much easier it will be now. reply Semitangent 21 hours agoparentIf you have not encountered it yet, I highly recommend Ben Eater's 8-bit computer series (https://eater.net/8bit/). I don't remember any other piece of material which made me grok the deep underpinnings of CPU instructions as this. There was an almost surreal moment where my brain went \"ooooooh, this is how a CPU instructions is just a complex network of switches with on/off states\". Ben Eater's other material is also great, he's just a good teacher. Building on that, the \"NAND2Tetris\" starts, as the name suggests, with basic logic gates and packages these low level concepts Matryoshka-style into recursive boxes with the end goal of programming a Tetris clone. See https://www.nand2tetris.org/ reply haswell 20 hours agorootparentI’m deep down the Ben Eater rabbit hole and enjoying it thoroughly. His “Hacking a weird TV censoring device” [0] video is an incredibly fun watch, and what made me an instant subscriber. Currently working through his 8-bit series and planning to build one at home. - [0] https://youtu.be/a6EWIh2D1NQ reply ngcc_hk 19 hours agorootparentOh … I bought one kit for my hotel q for 2 weeks. The rule relaxed and hence no time for that. Should get it out to do after dust for 2+ years reply danielvaughn 20 hours agoparentprevI've noticed the same, and I'm equally happy about it. I was also self-taught. I began teaching myself web dev around 2009 and have been doing it ever since. A few times here and there, I tried to learn the lower level stuff, but it's very daunting outside of an academic setting. In the past year, however, I've really tried to change my attitude. My 2024 resolution is to gain a baseline level of competency down to the assembly layer at least. reply begueradj 21 hours agoparentprevThat's a long brave path you had. From my experience, once you understand the \"bottom/low level\", you will go back to the \"up/high level\". I studied and used at university Assembly language and C (and C++) and I am not willing to write an e-commerce application with ASM 80x86 reply haswell 21 hours agorootparentA big part of my interest is because I want to tinker with electronics projects, and I’m curious about embedded work. I love that there’s an ecosystem of chips and boards that make this easy, but I realized I’d love to be able to crack open e.g. a broken Commodore 64 and know enough to troubleshoot and repair it. Or use smaller microcontrollers for use cases that don’t really require an entire OS. Beyond this, I think a lot of human thinking is inherently limited by the higher level abstractions we impose on the world, and so while I’d never attempt to write an e-commerce app in assembly, I’ve grown to appreciate the value of understanding the whole system and the wider set of options and deeper intuitions available to me as a result. reply begueradj 20 hours agorootparentElectronics ... I understand you in this case. reply mikejulietbravo 21 hours agoprevnot a criticism here, just a genuine question. What about this makes it friendly for ADHD? Is it a formatting/style thing? Information ordering? reply always2slow 21 hours agoparentDiagnosed with ADHD here, there's nothing ADHD friendly about this imo. I'm REALLY into reverse engineering content and I just bounced right off the GIANT WALL of text at the beginning. It's presented in a highly linear conversational fashion but the data doesn't appear to need to be linear. I absorb things in a non-linear fashion (I jump around, sometimes I even read/skim things backwards and out of order.) So it could easily be made ADHD friendly with clear delineation between ideas. A really simple way to do this would be surrounding definition blocks with a slightly lighter background color box and sticking strictly to necessary data, not using conversational language. Also the color scheme sucks (or rather is non-functional at least to me.). The blue text words really stick out, but the white bold text is what's important. However it also matches with the headers since they seem to be the same font and color, it's really distracting for some reason. Under assembly instructions specifically: like all I see is \"value, register, destination, source, destination, expression, destination, destination, destination,...\" literally the least important information is highlighted. The sidebar on the left is distracting too and pulls my eye constantly. Equating Beginner with ADHD is also mildly offensive and defeats the point if this is targeted at people suffering from ADHD. People with ADHD dive head first in the deep end or not at all, no exceptions. If it's labelled beginner/for-dummies, I'm skipping it or at best skimming it (like i did with this). reply bigfatfrock 21 hours agoparentprevADD/ADHD diagnosed for >30 years now and I was actually curious if some new ADHD friendly web format had been discovered after my years in web engineering, turns out the answer is no. reply volemo 21 hours agorootparentLike most accessibility issues, this cannot be solved with a format, only with care on the author’s behalf. reply malwrar 21 hours agoparentprevAt least for me: short sections with emoji, varied formatting, etc provide anchors for me when I loss focus and come back. reply chefandy 21 hours agoparentprevThe chunking and labeling of concise, strictly topical content and illustrative examples rather than having paragraph-after-paragraph of exposition is a huge factor for my flavor of ADHD, though we're obviously not a monolith. For contrast, lots of folks love the expository format of the Python docs which explains a lot about why things work rather than simply how they work, but that setup is absolutely murder for me. My brain is screaming at me after skimming the first two paragraphs to see if I'm even looking at the right doc, and I'm hitting google or stack overflow 3 seconds later. It's not a good strategy-- I definitely consider it a shortcoming-- but other doc systems work much better for me. For example: C# regex docs: https://learn.microsoft.com/en-us/dotnet/api/system.text.reg... Java regex docs: https://docs.oracle.com/javase/7/docs/api/java/util/regex/Pa... JS regex docs: https://developer.mozilla.org/en-US/docs/Web/JavaScript/Refe... PHP preg_replace docs: https://www.php.net/manual/en/function.preg-replace.php Python regex docs: https://docs.python.org/3/howto/regex.html Which one of these is not like the others? The Python docs are brilliant for people that click with that style, and as we can see from this well-shared and very controversial piece, (http://cryto.net/~joepie91/blog/2013/02/19/the-python-docume...) suck for others. Edit: As usual when the Python docs usability is questioned, cue the \"well I've got ADHD and I just LOVE the Python docs\" (said in the voice of the 'Well I LOVE SPAM' guy from the Monty Python skit). Sure, but you aren't some sort of standard by which others are measured. reply jdnier 21 hours agorootparentThanks for pointing this out. I had never really thought much about it. I 100% agree with you! The docs are totally why Python is so comfortable for me. reply chefandy 20 hours agorootparentI know so many people that so love them, and I do find the content useful sometimes... but I wish they could be modified a bit to have the deep dive after the really concise reference sheet style doc. But almost any time I've seen them discussed, even intimating that they're not ideal in their current state is an open invitation to summoning the online equivalent to a pitchfork-wielding crowd implying you're incompetent, lazy, and stupid for having a different usage style. I'm sure there are a lot of people involved who want nothing more than for them to be as useful as possible to everyone who needs them, but those toxic dynamics emerge so quickly in critique. It would obviously be a huge mistake to simply remove that info, but I think the format could be enhanced to work well with other usage styles. reply __turbobrew__ 21 hours agorootparentprevI despise python documentation for this reason. When I want to reference the docs on how to do something I have to cut through huge paragraphs of unorganized and unstructured text to just find out how to do something. It doesn’t help that different builtin modules use different documentation layouts and formatting either. reply chefandy 20 hours agorootparentYeah I can see why a lot of people love it, but it's fundamentally anathema to the way I use docs. Considering how common other reference formats are, I can't imagine it's rare among developers, but I'm not sure. I'd love to do a usability study. Modern accessibility guidelines implore taking neurological differences into account with web content, but if it's not easily measurable or even a bit subjective, it's not going to get budgeted for in most cases. I imagine without expert advice, even gauging whether or not you've got the right approach is difficult. reply neonsunset 21 hours agorootparentprev(C# documentation tends to have both types, the API description itself, and expository articles that introduce you to it e.g. https://learn.microsoft.com/en-us/dotnet/standard/base-types... ) reply chefandy 20 hours agorootparentAs much as they annoy me, I think MS doesn't get enough credit for their dev docs. MDN has some more expository docs in addition to their great reference pages, too. They're super useful, but having them combined can be a real struggle for folks that have difficulty harnessing their attention. reply neonsunset 19 hours agorootparentGiven the context of conversation, just noting that I too have ADHD and writing in C# has been definitely more doable than dynamically typed languages (or languages that require a lot of code to express something LINQ allows me to do in 1 or 2 lines, like Go). And as long as you know the type for the feature you need, you can usually just write e.g. \"Regex\" and press `.` and the VS Code or any other IDE of your choice will show you the list of supported methods - most of them are really straightforward to use. It works really well and knowing I don't have to perform much of work to ensure what amounts to type safety manually, or write additional tests, has been really helpful. More on C# Regex, extra props for source-generated Regexes also generating documentation to explain the exact pattern and how it is matched similar to how websites like regexr do it. reply jfarina 21 hours agoparentprevIDK. I think sometimes people lean into the idiosyncrasies of their own ADHD. I have ADHD, but I get sucked into writing code because it lets me focus in a way that dishes and laundry do not. reply wnevets 21 hours agoparentprev> What about this makes it friendly for ADHD? Maybe because each section is short and straight to the point? reply para_parolu 21 hours agorootparentIsn’t it just friendly for any human? reply haswell 21 hours agorootparentAnother way to look at this is that a lot of content is not well organized. For some people, this is an inconvenience. For some people, this makes the content impenetrable. In a learning context, most people benefit from better organization and concision. Some people benefit significantly more than others, and they’re called out here. reply wnevets 21 hours agorootparentprevSome people are definitely turned off by a terse writing style. If nobody enjoyed long and verbose writing why is there so much of it? For the record I do think putting ADHD friendly in the title is a little silly. reply lubesGordi 21 hours agoprevI was just skimming this and I thought there were easy ways of decompiling into C source as well with generated function names like a, b, c or whatever. Could one just dump that sort of minimized C code into an llm and have it generate variable and function names maybe? I'd be curious to see if that worked. reply chc4 20 hours agoparentIDA and Ghidra, which they mention at the bottom, along with Binary Ninja which they didn't, have decompilers that will output C source-ish from compiled assembly. Variables and functions without symbols in the binary are just given unique non-meaningful names. There's a lot of AI-powered extensions for them now trying to automate the reverse engineering process of giving them meaningful names, but IMO they are completely worthless and of negative utility - they're either so high level they just reiterate what the assembly is doing but in English, or are wrong and make up explanations for nearly every name that are incorrect and take more effort to correct than they save. reply yagkasha 21 hours agoparentprevI found this[0]. [0]: https://thejunkland.com/blog/using-llms-to-reverse-javascrip... reply arendtio 21 hours agorootparentSome software might even get more readable by using minification and this tool afterward :-D At least it would bring some normalization to variable and function names. reply queuebert 21 hours agoparentprevI had the same thought. Couple it with a source fuzzer to generate a large training dataset of valid source, and you could make a very nice disassembler. reply npace12 20 hours agoprevIf you are interested in reversing but don't know much about it, including the tools, I made an small plugin for `radare2` that you can ask it questions, it runs the commands and explains you what you need. [1] The functionality has also been merged into the bigger `r2ai` tool and supports anthropic and local LLMs as well. [2] [1] https://github.com/dnakov/r2d2 [2] https://github.com/radareorg/r2ai reply svilen_dobrev 17 hours agoprevhere something from the few things left from Fravia's stuff: https://web.archive.org/web/20190917072736/http://www.woodma... reply AnarchismIsCool 21 hours agoprevI don't really appreciate this form of pandering honestly. Edit: ADHD in software isn't about being able to read blog posts, we don't need blog post labels. It's about surviving our mindless bullshit one-size-fits-all corporate culture that demands perfect organization of pointless tasks and butts in seats at all times....just like our mindless bullshit one-size-fits-all school system. reply margaretdouglas 20 hours agoprevI made it about 20% before having a panic attack... formatting seems adversarial reply rdelpret 21 hours agoprev [–] As someone with adhd I do. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The article offers a beginner-friendly guide to reverse engineering, focusing on x86 assembly and C code, explaining CPU registers, memory access, assembly instructions, function communication, and disassembly.",
      "It discusses endianness, data storage, and converting human-readable values to memory bytes while highlighting the significance of understanding call and jmp instructions in machine code.",
      "Real-world examples like reverse engineering a car key fob signal, utilizing Google Assistant for smart TVs, and unlocking a Nokia router are provided, along with recommended tools and tips for starting the reverse engineering process."
    ],
    "commentSummary": [
      "The article discusses resources available for self-taught developers to transition from assembly language to higher-level languages, stressing the significance of grasping low-level coding concepts.",
      "It explores the challenges and advantages of learning low-level programming, information accessibility for individuals with ADHD, thoughts on documentation styles such as Python, the ease of coding in C#, and tools for source code decompilation.",
      "Commentaries cover tactics for reversing JavaScript and share personal anecdotes from professionals in the industry."
    ],
    "points": 115,
    "commentCount": 36,
    "retryCount": 0,
    "time": 1710514710
  },
  {
    "id": 39720187,
    "title": "Choosing the Right Programming Language: A Comparison of Zig, Rust, Go, and C",
    "originLink": "https://notes.eatonphil.com/2024-03-15-zig-rust-and-other-languages.html",
    "originBody": "Home Notes Popular RSS Subscribe March 15, 2024 Zig, Rust, and other languages zigrustgoc Having worked a bit in Zig, Rust, Go and now C, I think there are a few common topics worth having a fresh conversation on: automatic memory management, the standard library, and explicit allocation. Zig is not a mature language. But it has made enough useful choices for a number of companies to invest in it and run it in production. The useful choices make Zig worth talking about. Go and Rust are mature languages. But they have both made questionable choices that seem worth talking about. All of these languages are developed by highly intelligent folks I personally look up to. And your choice to use any one of these is certainly fine, whichever it is. The positive and negative choices particular languages made, though, are worth talking about as we consider what a systems programming language 10 years from now would look like. Or how these languages themselves might evolve in the next 10 years. My perspective is mostly building distributed databases. So the points that I bring up may have no relevance to the kind of work you do, and that's alright. Moreover, I'm already aware most of these opinions are not shared by the language maintainers, and that's ok too. I am not writing to convince anyone. Automatic memory management One of my bigger issues with Zig is that it doesn't support RAII. You can defer cleanup to the end of a block; and this is half of the problem. But only RAII will allow for smart pointers and automatic (not manual) reference counting. RAII is an excellent option to default to, but in Zig you aren't allowed to. In contrast, even C \"supports\" automatic cleanup (via compiler extensions). But most of the time, arenas are fine. Postgres is written in C and memory is almost entirely managed through nested arenas (called \"memory contexts\") that get cleaned up when some subset of a task finishes, recursively. Zig has builtin support for arenas, which is great. Standard library It seems regrettable that some languages have been shipping smaller standard libraries. Smaller standard libraries seem to encourage users of the language to install more transitively-unvetted third-party libraries, which increases build time and build flakiness, and which increases bitrot over time as unnecessary breaking changes occur. People have been making jokes about node_modules for a decade now, but this problem is just as bad in Rust codebases I've seen. And to a degree it happens in Java and Go as well, though their larger standard libraries allow you to get further without dependencies. Zig has a good standard library, which may be Go and Java tier in a few years. But one goal of their package manager seemed to be to allow the standard library to be broken up; made smaller. For example, JSON support moving out of the standard library into a package. I don't know if that is actually the planned direction. I hope not. Having a large standard library doesn't mean that the programmer shouldn't be able to swap out implementations easily as needed. But all that is required is for the standard library to define an interface along with the standard library implementation. The small size of the standard library doesn't just affect developers using the language, it even encourages developers of the language itself to depend on libraries owned by individuals. Take a look at the transitive dependencies of an official Node.js package like node-gyp. Is it really the ideal outcome of a small standard library to encourage dependence in official libraries on libraries owned by individuals, like env-paths, that haven't been modified in 3 years? 68 lines of code. Is it not safer at this point to vendor that code? i.e. copy the env-paths code into node-gyp. Similarly, if you go looking for compression support in Rust, there's none in the standard library. But you may notice the flate2-rs repo under the official rust-lang GitHub namespace. If you look at its transitive dependencies: flate2-rs depends on (an individual's) miniz_oxide which depends on (an individual's) adler that hasn't been updated in 4 years. 300 lines of code including tests. Why not vendor this code? It's the habits a small standard library builds that seem to encourage everyone not to. I don't mean these necessarily constitute a supply-chain risk. I'm not talking about left-pad. But the pattern is sort of clear. Even official packages may end up depending on external party packages, because the commitment to a small standard library meant omitting stuff like compression, checksums, and common OS paths. It's a tradeoff and maybe makes the job of the standard library maintainer easier. But I don't think this is the ideal situation. Dependencies are useful but should be kept to a reasonable minimum. Hopefully languages end up more like Go than like Rust in this regard. Explicit allocation When folk discuss the Zig standard library's pattern of requiring an allocator argument for every method that allocates, they often talk about the benefit of swapping out allocators or the benefit of being able to handle OOM failures. Both of these seem pretty niche to me. For example, in Zig tests you are encouraged to pass around a debug allocator that tells you about memory leaks. But this doesn't seem too different from compiling a C project with a debug allocator or compiling with different sanitizers on and running tests against the binary produced. In both cases you mostly deal with allocators at a global level depending on the environment you're running the code in (production or tests). The real benefit of explicit allocations to me is much more trivial. You basically can't code a method in Zig without acknowledging allocations. This is particularly useful for hotpath code. Take an iterator for example. It has a new() method, a next() method, and a done() method. In most languages, it's basically impossible at the syntax or compiler-level to know if you are allocating in the next() method. You may know because you know the behavior of all the code in next() by heart. But that won't happen all the time. Zig is practically alone in that if you write the next() method and and don't pass an allocator to any method in the next() body, nothing in that next() method will allocate. In any other language it might not be until you run a profiler that you notice an allocation that should have been done once in new() accidentally ended up in next() instead. On the other hand, for all the same reasons, writing Zig is kind of a pain because everything takes an allocator! Explicit allocation is not intrinsic to Zig, the language. It is a convention that is prevalent in the standard library. There is still a global allocator and any user of Zig could decide to use the global allocator. At which point you've got implicit allocation. So explicit allocation as a convention isn't a perfect solution. But it, by default, gives you a level of awareness of allocations you just can't get from typical Go or Rust or C code, depending on the project's practices. Perhaps it's possible to switch off the Go, Rust and C standard library and use one where all functions that allocate do require an allocator. But explicitly passing allocators is still sort of a visual hack. I think the ideal situation in the future will be that every language supports annotating blocks of code as must-not-allocate or something along those lines. Either the compiler will enforce this and fail if you seem to allocate in a block marked must-not-allocate, or it will panic during runtime so you can catch this in tests. This would be useful beyond static programming languages. It would be as interesting to annotate blocks in JavaScript or Python as must-not-allocate too. Otherwise the current state of things is that you'd normally configure this sort of thing at the global level. Saying \"there must not be any allocations in this entire program\" just doesn't seem as useful in general as being able to say \"there must not be any allocations in this one block\". Optional, not required, allocator arguments Rust has nascent support for passing an allocator to methods that allocate. But it's optional. From what I understand, C++ STL is like this too. These are both super useful for programming extensions. And it's one of the reasons I think Zig makes a ton of sense for Postgres extensions specifically. Because it was only and always ever built for running in an environment with someone else's allocator. Praise for Zig, Rust, and Go tooling All three of these have really great first-party tooling including build system, package management, test runners and formatters. The idea that the language should provide a great environment to code in (end-to-end) makes things simpler and nicer for programmers. Meandering non-conclusion Use the language you want to use. Zig and Rust are both nice alternatives to writing vanilla C. On the other hand, I've been pleasantly surprised writing Postgres C. How high level it is. It's almost a separate language since you're often dealing with user-facing constructs, like Postgres's Datum objects which represent what you might think of as a cell in a Postgres database. And you can use all the same functions provided for Postgres SQL for working with Datums, but from C. I've also been able work a bit on Postgres extensions in Rust with pgrx lately, which I hope to write about soon. And when I saw pgzx for writing Postgres extensions in Zig I was excited to spend some time with that too. Wrote a post on my wishlist for Zig and Rust. Focused on automatic memory management, the standard library, and explicit allocation.https://t.co/dvynizU9V2 pic.twitter.com/iTXp5QVxj0 — Phil Eaton (@eatonphil) March 15, 2024 Feedback As always, please email or tweet me with questions, corrections, or ideas! Subscribe Enter your email if you'd like to be kept in the loop about future articles! You can expect 2 to 4 messages per month depending on how motivated I'm feeling. :) Cheers, Phil Subscribe",
    "commentLink": "https://news.ycombinator.com/item?id=39720187",
    "commentBody": "Zig, Rust, and Other Languages (eatonphil.com)114 points by tim_sw 17 hours agohidepastfavorite86 comments coldtea 11 hours ago>Zig is not a mature language. But it has made enough useful choices for a number of companies to invest in it and run it in production. The useful choices make Zig worth talking about.Go and Rust are mature languages. But they have both made questionable choices that seem worth talking about. Well, Zig's native string type support, or lack thereof, is also a questionable choice. It's not like Zig did all the right choices and only Go and Rust made questionable ones. reply floobertoober 11 hours agoparentThe more I've used the language, I'm starting to think that this decision makes sense in the context of Zig. Common string operations involve allocation, non-constant time access, etc. I can definitely understand how strange that sounds without the rest of the language around it as context, though. reply tialaramex 3 hours agorootparentRust's String requires allocation, but str (typically seen as the fat pointer type &str) does not. \"The cat sat on the mat\".split('a') is an iterator over sub-strings, no more allocation needed here than for 105u32.leading_zeros() A lot of the Rust string API lives in str, not String. reply slimsag 11 hours agoparentprevIn fact, Zig has pretty much the same support for strings as both Rust and Go. The primary difference which people seem to complain about, that \"[]const u8\" is not written as \"string\", can be solved by writing `const string = []const u8;` at the top of your program if you like. Rust: `&str`, 'some bytes, and a length', 'Constructing a non-UTF-8 string slice is not immediate undefined behavior, but any function called on a string slice may assume that it is valid UTF-8, which means that a non-UTF-8 string slice can lead to undefined behavior down the road.' - also `String` and `OsString`. edit: apparently Rust enforces that &str cannot be created with invalid UTF8, at the additional performance cost of runtime checks, so it has stronger guarantees than other languages like Go/Java/Zig/etc (as usual) Go: `string`, 'in effect a read-only slice of bytes', 'a string holds arbitrary bytes. It is not required to hold Unicode text, UTF-8 text, or any other predefined format. As far as the content of a string is concerned, it is exactly equivalent to a slice of bytes.' Zig: `[]const u8`, 'some bytes, and a length', 'Zig has no concept of strings', 'by convention parameters that are \"strings\" are expected to be UTF-8 encoded slices of u8.', 'Generally, you can use UTF-8 and not worry about whether something is a string.' - and the stdlib has functions for working with unicode and strings. reply josephg 11 hours agorootparent> 'Constructing a non-UTF-8 string slice is not immediate undefined behavior Rust’s string constructors (for both str and String) check that the data contains strictly valid UTF8. You can opt out of this check in performance sensitive code - but doing so is considered unsafe. In safe rust it’s impossible to construct a str / String which contains invalid utf8. Eg: https://doc.rust-lang.org/std/str/fn.from_utf8.html reply slimsag 11 hours agorootparentgood to know! I just copied that quote directly from the Rust docs[0] which were not clear it is only possible in unsafe code. [0] https://doc.rust-lang.org/std/primitive.str.html#invariant reply pgwhalen 11 hours agorootparentprevI am definitely not a rust expert, but as far as I can tell this is just not true? The quote you copied is from the documentation is in the context of creating a string in an unsafe manner. If you're going to include unsafe behavior, you may as well say that nothing in rust means anything at all. reply duped 10 hours agorootparentprev> also `String` and `OsString` Because these are different types with different semantics. Why should there be one string type? Languages have to interface with the real world, where string data could be anything, but programs still need to be written for the common cases... hence, &str and String. reply bobbylarrybobby 9 hours agorootparentprevString is utf8, OsString is not (necessarily). Having two separate types mean you can guarantee utf8 when you want to, but don't have to when you can't, such as when dealing with “strings” that an OS produces, which are basically just sequences of bytes (perhaps with zero bytes disallowed). When you have one string type that has to do both jobs, you lose out on the guarantees of the more restrictive type. reply thayne 9 hours agorootparentprev> and the stdlib has functions for working with unicode and strings. Last time I looked I don't think it did. It looks like there is a unicode namespace now, but it is very primative. It doesn't have a simple way to iterate over codepoints, much less graphemes. Nor does it have anything to inspect the category a codepoint belongs to. reply slimsag 9 hours agorootparent`std.unicode` supports validation, codepoint iteration, conversion from UTF8other encodings, etc. It doesn't have graphemes (neither does Go or Rust), or other advanced unicode support. Instead that is provided by ziglyph[0] [0] https://codeberg.org/dude_the_builder/ziglyph reply Ygg2 11 hours agorootparentprev> Rust: `&str`, 'some bytes, and a length', 'Constructing a non-UTF-8 string slice is not immediate undefined behavior If you're constructing invalid `str` you already have severely fucked up. And dog is eating your HDD. reply DinaCoder98 11 hours agorootparent> If you're constructing invalid `str` you already have severely fucked up. Presumably it'd be better to know this immediately on initialization of the variable rather than at some point in the program that actually expects valid utf8—god forbid you transmit or persist the data before this happens. reply duped 10 hours agorootparentThat's why str::from_utf8 returns Result. And if you know the string is UTF-8 encoded and don't want to pay the cost, there's `from_utf8_unchecked`, which is marked unsafe. reply DinaCoder98 9 hours agorootparentAh, I thought you were referring to the benefits of such a system as compared to zig and blithely passing the responsibility of input validation on to the programmer. Rust of course demonstrates the benefit of checking validity at initialization. reply pyrolistical 11 hours agoparentprevWhat advantages would there be with a native string type over []const u8. (For those not fluent in zig, a string is typed as a slice constant bytes, where a slice is a pointer + length) reply Ygg2 11 hours agorootparentAn UTF8 String would have invariant of always having valid UTF8, omitting need for further UTF8 validation. If you're working on bytes string in Zig the status of them is unknown, so you have to always validate even if it's just Utf8. In Rust you can omit Utf8 validation for bytes from a String. reply slimsag 11 hours agorootparentIn Zig strings are assumed to be UTF8 unless documented otherwise, so any function operating on strings is not wasting CPU cycles doing any validation. The person who creates the string is expected to ensure it is valid UTF8 if needed. reply tialaramex 54 minutes agorootparentThis is the \"Just don't write bugs\" that we laugh at for C and C++. reply binary132 9 hours agorootparentprevConstructing a runtime-invariant-checked utf8 string is not \"omitting utf8 validation\". reply wavemode 11 hours agoparentprevWell, Zig does have Unicode support in its standard library. A \"String\" type wrapping that functionality would be slightly safer but, I don't see it as some huge wart of the language. You could always just define your own. reply bsder 11 hours agoparentprevEverybody says they want a built-in String type. And then everybody proceeds to write their own String library anyway. \"The only winning move is not to play.\" reply pornel 11 hours agorootparentRust has a nice solution of having a &str slice (equivalent of C++ std::string_view) as the lowest common denominator and a deref operator that can easily coerce various string types to the slice. This allows Rust to have all kinds of string flavors (fixed-len or NUL-terminated or growable, on stack or heap or in ROM, with SSO, with CoW, interned or refcounted, atomically or not, and so on), but they all coerce to the basic &str, so they have the same basic methods, and are compatible with most functions that just want a string without caring how it's allocated. You can use your own weird string type if you want, but usually you're not forced to convert or copy it to use it with standard library functions or 3rd party dependencies. reply josephg 11 hours agorootparentYeah - although the cost of this is that it makes rust harder to learn. (Try answering the common beginner question: “How is &String different from &str?”) In rust Strings are usually passed in to functions as a &str, and returned from functions as String. It makes sense once you’re used to it, but I think philosophically Rust is much closer to C++ than C. Rust is missing the elegant minimalism of C or Zig. reply iknowstuff 10 hours agorootparentSimple is a lie: https://fasterthanli.me/articles/i-want-off-mr-golangs-wild-... reply pepa65 9 hours agorootparentOP is not talking about Golang. reply iknowstuff 9 hours agorootparentThe same critique applies reply pepa65 4 hours agorootparentSo Zig is not simple but Rust is? The article you ref is not talking about Zig at all by the way. reply pornel 10 hours agorootparentprevC also has &str and &String, but they're \"char* that crashes if you free() it\" and \"char* that leaks if you don't free() it\". Ownership is hard to learn, and it is a barrier to learning Rust. However, don't confuse it with C++. Rust has two+ string types on purpose, not because of carrying 1970's legacy. reply duped 10 hours agorootparentprev> Try answering the common beginner question: “How is &String different from &str?” This has nothing to do with strings though - how is &Vec different from &[T] or &Box? You need to understand ownership, references, and unsized types. reply josephg 10 hours agorootparent> This has nothing to do with strings Sort of. It certainly affects strings, and you can't really use strings without understanding the difference. My point is that its yet another thing in the big bucket of junk you need to learn in order to use rust effectively. Rust is the only language I know of that has separate types for owned strings and a borrowed strings. Its a powerful concept - and its fast and efficient for the computer. But its not \"free\". The cost is that it makes it harder to learn rust. While we're on the topic - it also really doesn't help that the names of these types in the standard library are super idiosyncratic. String is to Vec as &str is to &[u8]. You just have to memorise that. How about Path and PathBuf? One of them is like String, and one is like &str. I swear I have to look it up every single time. I think the investment is worthwhile - I adore rust. But as I said, rust feels more like a better C++ than a better C. All the pain is front loaded, with the assumption that you'll eventually write enough software in rust to make the investment worthwhile. Simpler languages like Python or Go get out of your way immediately and let you be productive. I prefer rust, but there's definitely something to be said for that attitude. reply bobbylarrybobby 9 hours agorootparentAgree that the naming is pretty bad, but ultimately there are like… three or four of these owned/deref pairs you actually have to remember in day to day usage? I do wish they had gone with a consistent naming scheme, but in the grand scheme of things this is one of the more minor things that Rust got wrong. reply duped 9 hours agorootparentprevI think I would sum this up with saying that strings are not simple, and there is a delicate balance between making simple things simple and complex things possible without shooting yourself in the foot - both as a user, and as a library designer. Since Rust prefers to have verbosity and correctness over ease, it makes sense to have multiple types to reflect the different requirements of the underlying data. I sympathize with some of the std library verbosity when it comes to strings (and paths) but I don't think it's really a big criticism of why Rust's learning curve is steep. You have the same issues in C++, and doubly so in C because it doesn't help you at all and when you screw up the program crashes. When you use strings in an unmanaged language and need to manipulate them, you do need to understand the underlying consequences of how those strings are defined and stored. Comparing to managed languages like Python and Go is a bit unfair, because they're working in a different domain. Python is a great example of what happens when you try and hide complexity too far from the programmer - the python3 ecosystem fracture took millions of dollars and over a decade to settle - because of the inherent complexity in string representations. > Rust is the only language I know of that has separate types for owned strings and a borrowed strings. C++ and Objective-C (kinda) have different types for owned and referenced strings. C++ in particular has the same requirements as Rust, where referenced strings need to be able to refer to literals and it's not acceptable to make the default allocate. reply neonsunset 2 hours agorootparentprevYup, I can’t imagine switching to a language that doesn’t have something like that, since in C# you have a similar feature in the form of Span and ReadOnlySpan, to which all manners of sources can be coerced: heap-allocated arrays, stack-allocated buffers, inline arrays, strings, native memory, etc. reply DinaCoder98 10 hours agorootparentprev> And then everybody proceeds to write their own String library anyway. Is this true? It was (is!) certainly true for C, but C has an especially emaciated expectation for string processing primitives. Any runtime developed after like 1995 that I can think of has fixed this by providing a sane string implementation people generally agree upon. reply slimsag 10 hours agorootparentIf you care about Unicode for real, yes. Rust and Go both don't have a builtin package for grapheme iteration - and many naively (and incorrectly) think that a 'Go unicode rune == \"character\"' in Go. I assume the same happens with Rust `char` type. If you care about unicode-aware string sorting (you should), rather than the naive string sorting the Go and Rust standard libraries provide out of the box... then you probably want a proper Unicode library. I think the only language that gets Unicode 'right' out of the box is Swift, as it actually provides grapheme iterators, Locale awareness, etc. - but it comes at the cost of the language being tied to the (ever-moving) Unicode standard. reply Measter 4 minutes agorootparent> I think the only language that gets Unicode 'right' out of the box is Swift, as it actually provides grapheme iterators, Locale awareness, etc. - but it comes at the cost of the language being tied to the (ever-moving) Unicode standard. I think this might (at least partially) be why Rust's stdlib doesn't have this. If it did, then support for it would be tied to Rust's release schedule and which version of Rust you're using. Granted it is every six weeks, and it's usually trivial to update, but that's still a connection that could be an issue. By having this be in a separate library it means that it can update as and when it needs to, and it's not inherently connected to a specific release of Rust. reply DinaCoder98 3 hours agorootparentprevI was pessimistic about grapheme-based orientation towards text, deleted it to research more, and I've come to the conclusion that this is simply not a consensus opinion. Can you give me an example where grapheme-based sorting makes a critical difference from codepoint-oriented sorting on a normalized text? Full unicode composition certainly seems to provide a reasonable solution with western languages, CJK characters, and romanization of CJK characters, but that leaves a hell of a lot of scripts that I don't know about. I mean unicode is incredibly complex, but it doesn't even seem like there's a consensus outside of swift's string implementation of what a grapheme even is. (Granted, this might support the above concept that people can't even agree on what a string is, but unicode code points seems like a reasonable baseline to expect from a modern language. That said, rust doesn't even include unicode normalization in the standard library, although the common crate for it seems like a reasonable solution.) reply tialaramex 2 hours agorootparent> it doesn't even seem like there's a consensus outside of swift's string implementation of what a grapheme even is. Linguistically it's easy, graphemes are the squiggles people actually draw, as distinct from how a machine encodes them. Of course since people aren't a single individual with just one consistent opinion that does mean there's room for nuance - maybe some people think this is two separate squiggles. reply bsder 10 hours agorootparentprevNope. It's not at all fixed because nobody can ever agree on what a \"String\" is and what performance guarantees the underlying data structure should provide. Let's just assume a String is UTF-8 to make things \"simple\". Is a String mutable or not? Should mutable and immutable Strings have the same underlying structure? If mutable, is a String extensible or not? Can a String be sliced into another String? Can those slices be shared? Should you walk across codepoints or characters (which could be multiple codepoints due to combining)? If you want to insert a codepoint in the middle of a String, what are the performance guarantees? I can go on and on ... \"String\" really has to be a library as there are simply far too many permutations once you step away from \"Shove ASCII to tty\". reply DinaCoder98 10 hours agorootparentWell sure, people may colloquially refer to a lot of things as \"strings\"—hell, you could refer to all sequences as strings if you just wanted to argue with people—but the idea of trying to encapsulate this all in the standard library in a single implementation seems confusing semantically and of questionable value. It seems a lot easier to work with a reasonable interpretation of a string with its associated tradeoffs—which again is implied by most standard libraries. That said, I personally would balk at willing adopting any runtime that didn't enable iterating over a sequence of unicode code points, whether they be stored as utf8 or some 16-bit form, from a string of bytes in 2024 unless I were guaranteed to avoid having to deal with text processing of free-form human input. reply MBCook 10 hours agorootparentprevI’ve used Java my entire career. I have seen a custom string type nor a post advocating it in Java. reply dleslie 10 hours agorootparentNever seen one for C#, either. reply neonsunset 2 hours agorootparentApologies for the plug haha https://github.com/U8String/U8String With that said, implementing it is not easy, especially in terms of ensuring it is as fast as built-in string or faster (because the project goal is to be faster than built-in string and Go implementation, and match or outperform Rust's String where applicable). I can't imagine something like that being possible in e.g. Java or most other high-level languages - they simply do not expose appropriate low-level primitives when you need a \"proper\" string type. reply alchemio 13 hours agoprevJust a correction: most std C functions don’t allocate. strdup does but it was only recently adopted into the standard, it was previously an extension. Similarly zig’s stdlib shouldn’t allocate behind your back, except for thread spawn where it does: https://github.com/ziglang/zig/blob/5cd7fef17faa2a40c8da23f0... Generally speaking, it’s as mentioned just a convention. A zig library might not allow its users to pass allocators for example. In C++, stl containers can take an allocator as a template parameter. Recent C++ versions also provide several polymorphic allocators in the stdlib. You can also override the global allocator or a specific class’ allocator (override placement new). reply AndyKelley 13 hours agoparentFor spawning a thread you're literally asking for the stack of the thread to be memory mapped. I fail to see how this is \"behind your back\". You also linked specifically to the POSIX threads implementation of thread spawning, which is by definition supposed to play nicely with the libc posix threads API, which expects you to use the libc allocator in combination with POSIX threads API, so that's what it does. You might as well accuse the mmap() function in the zig standard library of allocating behind your back. reply alchemio 11 hours agorootparentIt’s something Zig touts when compared to other languages(1). The idea is that in the end it’s a convention that an allocator needs to be passed to indicate that the function allocates, which not even the stdlib adheres religiously to. I’m fine with it since I do believe a library writer should know best what works with their library. 1. https://ziglang.org/learn/why_zig_rust_d_cpp/#no-hidden-allo... reply MindSpunk 12 hours agorootparentprevThe behind your back part is probably referring to the Args payload bouncing through a heap allocation. It isn't explicit on the signature it's making an allocation. The function has no choice though unless you leave it up to the user to keep the payload allocation live until the thread terminates. reply alchemio 11 hours agorootparentExactly reply fsckboy 11 hours agorootparentprevhe was pointing out that it does, and he was not applying to it the label \"behind your back, that's why he said \"except for\". the wording makes perfect sense. reply rootlocus 11 hours agorootparent> Similarly zig’s stdlib shouldn’t allocate behind your back, except for thread spawn where it does shouldn't X, exceptor for Y where it does [X] It makes perfect sense that GP was saying \"thread spawn allocates behind your back\". reply jcelerier 11 hours agoparentprev> strdup does but it was only recently adopted into the standard, it was previously an extension. Does it really matter when people were already writing code with strdup when the zig and rust creators were in middle school ? It was already there in BSD 4.3 (1986) apparently. reply uecker 7 hours agorootparentGNU obstacks also already exist for as long as I can remember. Everything old is new again... reply alchemio 11 hours agorootparentprevIt really doesn’t matter and that’s my point. reply duped 10 hours agoprev> People have been making jokes about node_modules for a decade now, but this problem is just as bad in Rust codebases I've seen. Just w.r.t the issue of size (not of scale) - cargo caches sources in ~/.cargo so they're not shared by every project on the system. Additionally, rustc uses dead code elimination by design so there's not nearly as bad an issue as in JS where it's not possible to tree shake out every unused class or method. Most of the bloat of target directories are stale incremental build artifacts, because cargo does not do any automatic cleanup. reply ViewTrick1002 1 hour agoparentAutomatic GC of the cargo cache is available on nightly. Really looking forward to it landing on stable. For large projects I've seen the cache grow to hundreds of GB, and not noticing it until the compilation fails because the disk is full... https://blog.rust-lang.org/2023/12/11/cargo-cache-cleaning.h... reply khaledh 13 hours agoprevNim is also a strong player as a systems programming language. In terms of memory management, it's configurable, and by default you get ARC (no GC). I've written a hobby kernel (if you can call it that) in Nim[1] as well as Zig[2], and I found Nim to be much more ergonomic and approachable. The fact that Zig requires weaving an allocator through most calls that may allocate gets in the way of what I'm trying to do. I'd rather focus on core logic and have ref counting take care of dropping memory when appropriate. One thing I wish Nim had though is true sum types with payloads. I think there's an RFC for that, but it's a shame it's not in the language yet. [1] https://github.com/khaledh/axiom [2] https://github.com/khaledh/axiom-zig reply throwawaymaths 12 hours agoparent> The fact that Zig requires weaving an allocator through most calls It does no such thing. Why didn't you just make a global allocator? Alternatively, you can have your allocator be attached to your objects. reply loctal 12 hours agoparentprevGleam is another strong candidate too reply lokelow 12 hours agorootparentGleam runs on the Erlang VM, and I believe it also compiles to WASM. That's pretty different than the other languages here. Cool language though, I am a big fan of the BEAM reply rwbt 10 hours agoprevThere's also Odin[0] too. I experimented them all and Odin is pretty nice. Nim is also good too but a lot more features. But - I concluded that language matters a lot less compared to APIs. Yes, the language should have enough good features to let the programmers express themselves, but overall well designed APIs matter a lot more than language. For example -tossing most of the C stdlib and following a consistent coding style (similar to one described here -[1]), with using Arenas for memory allocation, I can be just as productive in C. [0] - https://odin-lang.org [1] - https://nullprogram.com/blog/2023/10/08/ reply thayne 8 hours agoprevWRT the standard library. There are tradeoffs. Yes, a big standard library that has everything you need is great, as long as it is well designed, well maintained, and has some mechanism to prevent the whole thing bloating every executable. But executing well on that is difficult for a number of reasons: - the release cycle of the standard library is (usually) tied to the compuler, which often means it can't evolve quickly. - backwards compatibility is a much bigger deal for the standard library. Which means if you have a big library you will eventually have a big pile of deprecated APIs you will probably never be able to actually remove. It also feeds into the next point - new development in stdlib can be hindered by the need to get it right the first time. - the creators of the language probably aren't experts in every area. Which means in order to include say, a good compression library you either need to recruit someone who is an expert to write and maintain the package in your stdlib, or link to a library in another language, and the latter is still really an external depency. - a large stdlib is a large maintenance burden Python has a large standard library, but it has parts that are deprecated and/or largely abandoned, including packages for technologies that are no longer widely used. And its http packages are rarely used directly because third party packages like requests or urllib3 are better. Go's standard library that is praised here isn't perfect either. The log and flag packages are often insufficient, and the implementation of net.IP is suboptimal [1]. I think probably the best balance is to start with a fairly small standard library, and when community libraries for key functionality become popular and stable, then pull them in to the standard library or otherwise make them official. And maybe have official libraries that are external to the stdlib and maybe have less rigid backwards compatibility garantees that can move faster than the language itself. [1]: https://tailscale.com/blog/netaddr-new-ip-type-for-go reply klabb3 7 hours agoparent> The log […] There’s log/slog now. Is that sufficient? > and flag packages are often insufficient […] Yes. Flag is really primitive and awkward to use. On the other hand, in a typical “imperative shell” style CLI, sharing flags between packages is not exactly crucial, which is the main reason to put it in std (interop). I’ve never really suffered from importing a 3p flag parser in my own code. > and the implementation of net.IP is suboptimal [1]. Fixed for a long time. See net/netip (although I am annoyed it’s not universally available in all net functions). It’s based on the implementation from your link. reply Arnavion 10 hours agoprev>Zig is practically alone in that if you write the next() method and and don't pass an allocator to any method in the next() body, nothing in that next() method will allocate. It's not quite as simple as that. This can allocate just fine: fn next(self: *Self) { self.array_list.append(5); } ... where `array_list` is of type `std.ArrayList(u32)`. `std.ArrayList(T)` takes an allocator at construction time (`array_list = std.ArrayList(u32).init(allocator);`) and stores it as a field, so its methods don't need to take an allocator parameter. (And yes, there is a version of `std.ArrayList(T)` called `std.ArrayListUnmanaged(T)` that does *not* take an allocator in its constructor and does take an allocator in all its methods that need to allocate.) reply AndyKelley 4 hours agoparentThat would not compile because the append operation can fail, and errors cannot be ignored. reply slily 12 hours agoprev> People have been making jokes about node_modules for a decade now, but this problem is just as bad in Rust codebases I've seen I agree... but I think this is more indicative of a cultural problem than a language design/standard library scoping problem. A compact standard library is more resistant to scope creep. I don't want every language to end up like C++, and I think keeping the scope of the standard library small helps avoid that. On the other hand, popular third-party libraries that provide common functions have too many third-(fourth-?)party dependencies. Keeping dependency trees small should be a priority for such tools, but convenience trumps all right now so it isn't valued as it should be. reply coldtea 11 hours agoparent>A compact standard library is more resistant to scope creep I don't see \"scope creep\" as a problem. I see \"Lack of standard library support for X brought 5 competing libraries for the same X functionality, no de facto standard, and projects using incompatible implementations and versions, maintained in different states of disarray\" as a problem. Never had a problem with Python standard library having \"scope creep\" for example. Where I do have problem is where there are several competing libraries and no single well supported one you can always depend on for some - which happens for things not in the standard library. reply no_wizard 12 hours agoparentprevone way to approach this with compromise is to keep the shipped standard library small, but have officially maintained packages from the language foundation for exceedingly common functionality. That gives an avenue for exploration, and tight controls and quality control on packages, without bloating the standard library. reply MBCook 10 hours agorootparentWhat’s so bad about “bloating the standard library”? Java has an incredible amount of stuff and it’s incredibly useful. The compiler should be able to eliminate the unused parts during compilation, it’s not like adding JSON to the standard library is going to make a “hello world” program bigger. reply shit_game 9 hours agorootparent>The compiler should be able to eliminate the unused parts during compilation, it’s not like adding JSON to the standard library is going to make a “hello world” program bigger. I agree. I would argue against keeping standard libraries terse as an appropriate goal for a language because terse (or alternatively, inadequate) standard libraries are exactly what necessitate third party libraries for common functionality, which in turn lead to this dependency hell that so many people dislike so much. Not to say that a standard library should do absolutely everything, but the more capable a language is out of the box, the fewer third party libraries a language ecosystem will come to rely on, meaning that methodologies will be more consistent in more code and dependency management will be simpler. reply theLiminator 9 hours agorootparentprevMostly because it holds the language back due to backwards compatibility concerns. reply theLiminator 9 hours agorootparentprevI wish languages took this approach, not sure why they largely don't. reply duped 10 hours agoparentprevIf cargo did literally any garbage collection then it wouldn't be a problem. Most of the nonsense in a target/ directory are stale incremental build artifacts. reply menaerus 3 hours agorootparentThe problem as I understand it is not about the stale dependency artifacts in xyz directory but with the (transitive) (hell) dependency management itself. cargo makes it incredibly easy to pull in the shitload of dependencies of whom you don't have a slightest idea. reply pornel 10 hours agoparentprevThe problem is that a typical built-in standard library can't change or remove any APIs, even the ones that turned out to be mistakes or became obsolete, without breaking users. Eventually all the fancy standard parts of it get easier, faster, cleaner 3rd party replacements, and everyone is saddled with a dilemma of using the meh stdlib or a 3rd party dependency anyway. OTOH functionality shipped as external libraries can be versioned, upgraded, and replaced by users individually, on their own schedule. The obsolete cruft can be pulled in by old projects that still need it, without burdening new projects with the same old APIs (but they still can interoperate, because multiple versions of the same library can coexist!) For very long-lived languages (which Rust has ambition to be) a one-size-fits-all built-in big standard library is a big risk. The bigger it is, the less obvious and less timeless the APIs are. A basic Vec is easy to get right on the first try, but networking is a moving target, fashions for file formats come and go, threads/green threads/callbacks/promises/await come and go, parsers can be designed in many differently-bad ways, and OSes change — we don't have non-binary files any more, but ossified languages do! WASM/WASI has upturned even Rust's barebones standard library, and Fuchsia would even more. This risk of maintaining mistakes forever eventually makes maintainers of the standard library overly cautious, and design very defensive opaque non-committal APIs. IMHO languages should not have any single built-in unversioned library at all. Pull in what you use, in a version that makes sense this year, without the baggage that every API has to remain unchanged forever, and still be sensible and efficient half a century later, even on an OS that doesn't exist yet on hardware that hasn't been invented yet. reply Thaxll 12 hours agoprevZig does not have a good package manager, does it? reply eyegor 11 hours agoparentIt has one now, but I haven't played around enough to call it \"good\" or \"bad\". It feels more like a hack than something like cargo/pip/nuget/npm/etc, but it's a lot better than the normal c/cpp experience. build.zig.zon discussion: https://zig.news/edyu/zig-package-manager-wtf-is-zon-2-0110-... Example howto: https://ziggit.dev/t/how-to-package-a-zig-source-module-and-... Actual docs: https://ziglang.org/download/0.11.0/release-notes.html#Packa... reply papichulo2023 3 hours agorootparentAre you experienced with vcpkg or just repeating old facts? Considering you are merging c and cpp seems the earlier. I would take vcpkg over npm or pip any day. reply AndyKelley 11 hours agoparentprevI think the upcoming 0.12 release will be a good time to try it out. It was a major focus during this release cycle. reply otabdeveloper4 4 hours agoparentprevYou shouldn't need one. Use Nix instead, they did 99% of the work for you already. reply bsder 12 hours agoparentprevI don't believe Zig wants to have \"package manager\" at all. It has the ability to pull in a package/module. However, I believe that having a \"package manager\" is explicitly an anti-goal. I'm in agreement. It's become pretty clear that having any form of \"package repository\" without also allocating the necessary manpower to curate that repository causes disasters. reply wavemode 11 hours agorootparent> I don't believe Zig wants to have \"package manager\" at all. ? it's literally one of the project's main milestones: https://github.com/ziglang/zig/projects?type=classic reply bsder 10 hours agorootparentWe're probably talking past one another due to my imprecision: Zig can manage packages. It can pull in a Zig package using a reference to a Git repository on GitHub, for example. It can pin that to a hash and cache it. It can specify dependencies. This is, strictly speaking, a \"package manager\" but is not what people normally think of when you mention that. Zig does not have blessed mechanisms that pull from a central repository of packages. Having a central repository is normally what people think of when they talk about a \"package manager\" (node, cargo, deno, etc.). Yes, those managers could conceivably not use the central repository and specify everything explicitly, but nobody ever uses them like that. reply papichulo2023 3 hours agorootparentSo same as Golang? Still package manager though. reply baby 11 hours agorootparentprevThat sounds like a horrible decision reply eviks 7 hours agoprev [–] > 3 years? 68 lines of code. Is it not safer at this point to vendor that code? Why, though? There is no bitrot or build flakiness with unnecessary changes if no changes happen, so what's the actual argument for this? Why is it missing from the article? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The author shares their experiences with programming in Zig, Rust, Go, and C, emphasizing topics like automatic memory management and explicit allocation.",
      "A comparison of the strengths and weaknesses of each language is provided, discussing issues such as small standard libraries leading to reliance on third-party libraries.",
      "The author praises the tooling in Zig, Rust, and Go, underlining the importance of selecting the appropriate language for a specific task while expressing enthusiasm for exploring Postgres extensions in Zig and Rust."
    ],
    "commentSummary": [
      "The article delves into a comparison of the string handling approaches in Zig, Go, and Rust, highlighting Rust's strict UTF-8 validation, Zig's appeal for investments due to its concise structure, and Go's flexibility in handling byte slices.",
      "Rust offers distinct types for UTF-8 and non-UTF-8 strings, posing challenges compared to other languages, especially concerning Unicode standards implementation.",
      "It discusses the ongoing debate on prioritizing language over libraries, the pros and cons of extensive standard libraries, and the delicate balance in maintaining a compact standard library in Rust, while also exploring Zig's package management and code vendoring concept."
    ],
    "points": 114,
    "commentCount": 86,
    "retryCount": 0,
    "time": 1710533516
  },
  {
    "id": 39720508,
    "title": "GhostRace: Securing Against Speculative Race Conditions",
    "originLink": "https://www.vusec.net/projects/ghostrace/",
    "originBody": "GhostRace Exploiting and Mitigating Speculative Race Conditions GhostRace: CVE-2024-2193 Race conditions arise when multiple threads attempt to access a shared resource without proper synchronization, often leading to vulnerabilities such as concurrent use-after-free. To mitigate their occurrence, operating systems rely on synchronization primitives such as mutexes, spinlocks, etc. In this work, we present GhostRace, the first security analysis of these primitives on speculatively executed code paths. Our key finding is that all the common synchronization primitives implemented using conditional branches (Figure 1) can be microarchitecturally bypassed on speculative paths using a Spectre-v1 attack, turning all architecturally race-free critical regions into Speculative Race Conditions (SRCs), allowing attackers to leak information from the target software. Figure 1: Top part: The core implementation of the mutex_lock synchronization primitive in the Linux x86-64 kernel, with the conditional branch that can be abused to craft SRCs in red. Bottom part: The branch ultimately checks the outcome of the lock cmpxchgq instruction, which does not serialize the execution. Our GhostRace Paper (PDF) is accepted for publication at the 33rd USENIX Security Symposium 2024. This is a joint project with the Systems Security Research Group at IBM Research Europe. Speculative Synchronization Primitives Our analysis shows all the other common write-side synchronization primitives in the Linux kernel are ultimately implemented through a conditional branch and are therefore vulnerable to speculative race conditions. To experimentally confirm this intuition, we tested all such synchronization primitives under speculative execution after mistraining the vulnerable branch. In all cases, we confirmed transient execution of the guarded critical region despite another victim thread already architecturally executing in the region. To determine the transient window size, we measured the maximum number of speculative load instructions we could speculatively execute inside the critical region (Figure 2). Figure 2: Speculative window size for different write-side synchronization mechanisms, i.e., the number of speculative loads that leave an observable microarchitectural trace. SCUAF Gadget Scanner To investigate the severity of SRCs, we concentrate on Speculative Concurrent Use-After-Free (SCUAF) and statically scan the Linux kernel with Coccinelle (Figure 3), discovering 1,283 potentially exploitable gadgets. Figure 3: Simplified Cocci scripts (left Free and right Use) scanning for SCUAF gadgets in the Linux kernel. IPI Storming: CVE-2024-26602 To win an SRC, we need to interrupt the execution of the victim process at the right point (i.e., when the dangling pointer is created), and keep the victim there forever so that the attacker can perform the SCUAF attack. In order to achieve this, we created a new exploitation technique called Inter-Process Interrupt (IPI) Storming, which consists of infinitely flooding the victim process’s CPU core with IPIs once interrupted so that it never finishes handling the incoming interrupts, resulting in creating an unbounded exploitation window that allows the attacker to execute an arbitrary number of SCUAF invocations to mount an end-to-end attack within a single race window. In Figure 4 we show how the increasing number of storming SMTs widens the UAF exploitation window. Figure 4: Size of the UAF exploitation window vs. number of IPI storming cores targeting the victim core. Our test CPU contains 16 cores and 24 SMTs. SCUAF Information Disclosure Attacks Furthermore, we show that SCUAF information disclosure attacks (Figure 5) on the kernel are feasible and can match the reliability of typical Spectre attacks, with our proof of concept leaking kernel memory at 12 KB/s. Figure 5: Speculative information disclosure attack exploiting a speculative race condition. Steps 1-4 and 8-10 run in user mode, issuing syscalls to trigger the relevant kernel code. The other steps run in kernel mode. Our gadget scanner identified the nfc_hci_msg_tx_work function as a SCUAF gadget in the Linux kernel. Code You can find a minimalistic PoC exemplifying the concept of SRC in a step-by-step single-threaded fashion, Coccinelle SCUAF-scanning scripts, and 1200+ SCUAF gadgets found in the Linux kernel at https://github.com/vusec/ghostrace Affected Hardware & Software While we have explicitly focused on x86 and Linux in the paper, SRCs also affect other hardware and software targets. Hardware: We have confirmed that all the major hardware vendors are affected by SRCs since, regardless of the particular compare-and-exchange instruction implementation, the conditional branch that follows is subject to branch (mis)prediction. In other words, all the microarchitectures affected by Spectre-v1 are also affected by SRCs. Software: Any target relying on conditional branches to determine whether to enter critical regions—a common design pattern that extends well beyond Linux—is vulnerable to SRCs. In summary, any software, e.g., operating system, hypervisor, etc., implementing synchronization primitives through conditional branches without any serializing instruction on that path and running on any microarchitecture (e.g., x86, ARM, RISC-V, etc.), which allows conditional branches to be speculatively executed, is vulnerable to SRCs. As in other speculative execution attacks, this allows leaking data from the target software. Mitigation To address the new attack surface, we also propose a generic SRC mitigation to serialize all the affected synchronization primitives on Linux (i.e., adding an lfence instruction after the lock cmpxchq in Figure 1). Our mitigation requires minimal kernel changes (i.e., 2 LoC) and incurs only ≈5% geomean performance overhead on LMBench. Disclosure We disclosed Speculative Race Conditions to the major hardware vendors (Intel, AMD, ARM, and IBM) and the Linux kernel in late 2023. Hardware vendors have further notified other affected software (OS/hypervisors) vendors, and all parties have acknowledged the reported issue (CVE-2024-2193). Specifically, AMD responded with an explicit impact statement (i.e., “existing [Spectre-v1] mitigations apply”), pointing to the attacks relying on conditional branch mis-speculation, like Spectre-v1. The Linux kernel developers have no immediate plans to implement our proposed serialization of synchronization primitives due to performance concerns. However, they confirmed the IPI storming issue (CVE-2024-26602) and implemented an IPI rate-limiting feature to address the CPU saturation issue by adding a synchronization mutex on the path of sys_membarrier and avoiding its concurrent execution on multiple cores. Unfortunately, as our experiments show (Figure 4), hindering IPI storming primitives (i.e., 0 storming cores) is insufficient to close the attack surface completely. Acknowledgments We would like to thank the anonymous reviewers for their feedback, Andrew Cooper for his early comments on the paper, Julia Lawall for the Coccinelle clarifications, and Alessandro Sorniotti for the early discussions about the project. This work was partially supported by Intel Corporation through the “Allocamelus” project, by the Dutch Research Council (NWO) through project “INTERSECT”, and by the European Union’s Horizon Europe program under grant agreement No. 101120962 (“Rescale”).",
    "commentLink": "https://news.ycombinator.com/item?id=39720508",
    "commentBody": "GhostRace: Exploiting and mitigating speculative race conditions (vusec.net)111 points by PaulHoule 16 hours agohidepastfavorite16 comments Veserv 13 hours agoThe paper seems to make no mention of the memory barriers in play, merely talking about the atomic operation and the conditional branch. Despite that, they indicate they produced a proof-of-concept exploit leaking data. Traditionally, you would use a acquire, L-LS, barrier on entry to a critical section to order any memory accesses in the critical section after the load of the atomic operation completes. Without that, the critical section may speculatively execute on your core while somebody else holds the lock and those changes would be confirmed if the owner releases before your delayed acquiring load completes. For this paper to be true, either their synchronization primitives do not have acquire barriers, which is wrong to start with, or the acquire barrier must not be stalling later memory accesses which is how I have seen it portrayed. How would that even work correctly on normal code? To preserve the L-L ordering implicit in the L-LS ordering, you need to guarantee that any loads after the acquiring load will see all stores explicitly ordered before the releasing store. You would need to guarantee that any speculative load must at least see the value the memory would have immediately preceding the release barrier on the other core even if you happened to load that value arbitrarily long before the release barrier on the other core. I guess you could track every single speculative access after a L-* barrier and their dependency chain and then invalidate and re-compute as you see stores until you resolve all pertinent operations prior to your L-* barrier (such as the acquiring load in this case)? If the chip people are actually doing that, that seems... wild. reply eigenform 7 hours agoparentSee \"Speculative Lock Elision\" from MICRO34 (in 2001!): https://pages.cs.wisc.edu/~rajwar/papers/micro01.pdf reply loeg 11 hours agoparentprevIt seems like the chips are speculating past a 'lock cmpxchgq', which corresponds to a seq-cst access (IIRC). > or the acquire barrier must not be stalling later memory accesses That is my impression. But I am really confused about this stuff in general. reply trebligdivad 12 hours agoparentprevIs that lot true on x86? I thought it's (fairly) strict memory ordering meant you didn't have acquire type thing? reply Veserv 11 hours agorootparentThe concept of ordering is independent of architecture. What operations need to be ordered relative to other operations to be correct does not change. But, your architecture may implicitly guarantee certain orderings so you do not need to explicitly guarantee those orderings. If I remember correctly, x86 basically gives you everything except S-L automatically. That means a traditional acquire, L-LS, and traditional release, LS-S, barrier should be \"free\" and require no explicit instructions. I forgot about that, so it makes sense why they would not mention the presence of explicit barriers. But, that does not change the fact that you need that ordering to guarantee correctness, it is just built-in and automatic. The oddity here is that speculative loads across a L-LS ordering requires some pretty deep black magic to be going on at the hardware level. I have no idea how they could safely reorder stores across a L-S ordering and still maintain \"as-if\" the store happens after the load, and even safely reordering loads across a L-L ordering and maintaining the \"as-if\" seems pretty arcane. The fact that the paper is claiming their PoC works and thus the reordering must be happening is wild. Either the processor is not correctly implementing the declared memory ordering semantics, or they are doing some real reordering magic. reply BeeOnRope 6 hours agorootparentThis kind of memory order speculatiom is basically required on x86 since the strong semantics would otherwise prevent many useful reorderings (especially L-L which is absolutely critical). The basic way it works is that pretty much any reordered is allowed, speculatively, even if it violates the explicit or implicit barriers, but then the operations are tracked in a memory order buffer (MOB) until retirement which can detect whether the reordering was detectable and if so flush the pipeline (so-called \"memory order nuke\", visible in performance counters). reply Voultapher 14 hours agoprevSpectre-v1 really is the ghost that keeps haunting us. All the mitigations I'm aware of work by containing the domain, for example inter-process boundaries together with the MMU to limit the leaked surface. How are we developers supposed to reason about code where most conditions break the invariants we encoded? The demonstrated exploit strategy is pretty cool. reply WJW 14 hours agoprev> Our mitigation requires minimal kernel changes (i.e., 2 LoC) and incurs only ≈5% geomean performance overhead on LMBench. 5 percent is not nothing but seems like a worthwhile investment here. Really cool exploit strategy btw. reply loeg 11 hours agoparent5% is a pretty big regression. reply dataflow 14 hours agoprev [–] What does \"speculative race condition\" mean? reply sweetjuly 11 hours agoparentThis is described pretty well in the paper, but briefly: the \"race condition\" which they exploit happens because the CPU speculatively (and incorrectly) thinks it acquired a lock. Multiple threads being in a critical section can be exploitable and lead to arbitrary read/write and malicious code execution (one case demonstrated in the paper is some random NFC function). The researchers abuse the fact that the critical section can be entered speculatively to gain speculative malicious code execution, which they then use to construct a gadget which leaks arbitrary kernel memory into side channels. reply dataflow 10 hours agorootparentThank you! reply sweatypalmer 13 hours agoparentprev [–] I encourage you to research the rabbit hole that is speculative executive and 'spectre' attacks. reply dataflow 11 hours agorootparentI know what speculative execution is and have some understanding of spectre already. What I don't get is what it means to exploit a speculative race condition. reply saagarjha 5 hours agorootparentSo the thing with speculative attacks is that you can use an invariant that is broken speculatively to leak things. In this case they use a speculative race condition to gain speculative code execution, much like you can use a normal race condition to gain real code execution. Arbitrary speculative code execution can be used to leak data which can be picked up via side channels. reply Jerrrry 11 hours agorootparentprev [–] \"Speculative Execution\" is gonna be my boat name. I'll tell people it's a punny triple entendre, leading people to believe I am a wall street hacking guru hit-man. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The paper discusses the susceptibility of synchronization primitives to speculative race conditions in the Linux kernel on x86 hardware.",
      "GhostRace is introduced as a security analysis illustrating how speculative execution can evade standard synchronization methods, causing information leakage.",
      "A mitigation approach is suggested to tackle this problem efficiently, highlighting the risk of information disclosure and introducing a novel exploitation method termed IPI Storming."
    ],
    "commentSummary": [
      "The discussion focuses on a paper about GhostRace, addressing speculative race conditions, their risks like data leaks, and unauthorized code execution, along with proposed mitigation methods involving minimal kernel changes and a 5% performance overhead.",
      "It explores memory barriers, ordering guarantees, and speculative loads across various architectures, citing prior research on speculative lock elision and challenges in maintaining correctness amidst reordering.",
      "The conversation also considers broader implications of speculative attacks, security risks tied to exploiting speculative race conditions, and the nature of speculative execution as a security vulnerability."
    ],
    "points": 111,
    "commentCount": 16,
    "retryCount": 0,
    "time": 1710535421
  }
]
