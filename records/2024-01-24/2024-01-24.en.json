[
  {
    "id": 39102021,
    "title": "Whistleblower Reveals Defects in Boeing 737 MAX 9 Production Line",
    "originLink": "https://viewfromthewing.com/boeing-whistleblower-production-line-has-enormous-volume-of-defects-bolts-on-max-9-werent-installed/",
    "originBody": "Boeing Whistleblower: Production Line Has “Enormous Volume Of Defects” Bolts On MAX 9 Weren’t Installed by Gary Leff on January 22, 2024 A reader at respected airline industry site Leeham News offered a comment that suggests they have access to Boeing’s internal quality control systems, and shares details of what they saw regarding the Boeing 737 MAX 9 flown by Alaska Airlines that had a door plug detach inflight, causing rapid decompression of the aircraft. The takeaway appears to be that outsourced plane components have so many problems when they show up at the production line that Boeing’s quality control staff can’t keep up with them all. Current Boeing employee here – I will save you waiting two years for the NTSB report to come out and give it to you for free: the reason the door blew off is stated in black and white in Boeings own records. It is also very, very stupid and speaks volumes about the quality culture at certain portions of the business. …With that out of the way… why did the left hand (LH) mid-exit door plug blow off of the 737-9 registered as N704AL? Simple- as has been covered in a number of articles and videos across aviation channels, there are 4 bolts that prevent the mid-exit door plug from sliding up off of the door stop fittings that take the actual pressurization loads in flight, and these 4 bolts were not installed when Boeing delivered the airplane, our own records reflect this. …As a result, this check job that should find minimal defects has in the past 365 calendar days recorded 392 nonconforming findings on 737 mid fuselage door installations (so both actual doors for the high density configs, and plugs like the one that blew out). That is a hideously high and very alarming number, and if our quality system on 737 was healthy, it would have stopped the line and driven the issue back to supplier after the first few instances. …Now, on the incident aircraft this check job was completed on 31 August 2023, and did turn up discrepancies, but on the RH side door, not the LH that actually failed. I could blame the team for missing certain details, but given the enormous volume of defects they were already finding and fixing, it was inevitable something would slip through- and on the incident aircraft something did. I know what you are thinking at this point, but grab some popcorn because there is a plot twist coming up. The next day on 1 September 2023 a different team (remember 737s flow through the factory quite quickly, 24 hours completely changes who is working on the plane) wrote up a finding for damaged and improperly installed rivets on the LH mid-exit door of the incident aircraft. …Because there are so many problems with the Spirit build in the 737, Spirit has teams on site in Renton performing warranty work for all of their shoddy quality, and this SAT promptly gets shunted into their queue as a warranty item. Lots of bickering ensues in the SAT messages, and it takes a bit for Spirit to get to the work package. Once they have finished, they send it back to a Boeing QA for final acceptance, but then Malicious Stupid Happens! The Boeing QA writes another record in CMES (again, the correct venue) stating (with pictures) that Spirit has not actually reworked the discrepant rivets, they *just painted over the defects*. In Boeing production speak, this is a “process failure”. For an A&P mechanic at an airline, this would be called “federal crime”. …finally we get to the damning entry which reads something along the lines of “coordinating with the doors team to determine if the door will have to be removed entirely, or just opened. If it is removed then a Removal will have to be written.” Note: a Removal is a type of record in CMES that requires formal sign off from QA that the airplane been restored to drawing requirements. If you have been paying attention to this situation closely, you may be able to spot the critical error: regardless of whether the door is simply opened or removed entirely, the 4 retaining bolts that keep it from sliding off of the door stops have to be pulled out. A removal should be written in either case for QA to verify install, but as it turns out, someone (exactly who will be a fun question for investigators) decides that the door only needs to be opened, and no formal Removal is generated in CMES (the reason for which is unclear, and a major process failure). Therefore, in the official build records of the airplane, a pressure seal that cannot be accessed without opening the door (and thereby removing retaining bolts) is documented as being replaced, but the door is never officially opened and thus no QA inspection is required. The commenter concludes, “Where are the bolts? Probably sitting forgotten and unlabeled (because there is no formal record number to label them with) on a work-in-progress bench, unless someone already tossed them in the scrap bin to tidy up.” The information was first flagged by aviation watchdog JonNYC. ..unvetted (every other synonym for that as well,) commenter here:https://t.co/R3KdqDHION And now I'm seeing a couple other usually credible, usually careful folks positing same idea. As always– more so than usual– no idea myself. — JonNYC (@xJonNYC) January 22, 2024 Boeing outsources a lot of the production of components for its aircraft because it’s cheaper as part of an overall shift in strategy that dates to CEO Harry Stonecipher who had been CEO of McDonnell Douglas, When people say I changed the culture of Boeing, that was the intent, so that it’s run like a business rather than a great engineering firm. One of the major suppliers is Spirit AeroSystems, which used to be part of Boeing and was spun out and sold to private equity in 2005. That’s whose work is at issue here. This story suggests a one-off mistake with this particular part on this particular aircraft, though also that production issues are common. That doesn’t square with a theory that bolts could have come loose from flying a poorly-designed aircraft, or that Boeing 737-900ERs are being inspected too. Those have the same door plug because the MAX 9 is built on the same airframe. It may or may not square with finding loose door plugs on other Boeing 737 MAX 9s. So this story is far from ‘official’ but it seems knowledgeable from someone who suggests they’re a whistleblower inside of Boeing. A story in Politico this morning suggests that Boeing’s new team of lobbyists has their work cut out for them. It certainly appears so, but perhaps work needs to start at the board and C-suite level. More From View from the Wing More Loose Bolts: Boeing 737 MAX 9 Problems Widespread As United Finds 5 Faulty Planes During inspections following the Alaska Airlines loss of a a decommissioned exit door during flight, United Airlines has found at least 5 Boeing 737 MAX 9 aircraft with \"loose bolts and other parts.\" January 8, 2024 In \"Airlines\" Boeing’s Response To 737 MAX Crisis: More Inspections, Team Reviews, But Is It Enough? Boeing Commercial Airplanes CEO Stan Deal has outlined to employees the steps the airframe manufacturer plans to take to improve quality assurance in light of the Alaska Airlines Boeing 737 MAX 9 near-disaster that currently has that plane type grounded by regulators. January 15, 2024 In \"General\" FAA Grounds Boeing 737 MAX 9s After Kid Almost Sucked Out Of Alaska Airlines Jet Alaska Airlines quickly completed inspections on at least a quarter of the aircraft, failing to find any issues similar to the 10 week old plane on which “a kid in that row [where the aircraft’s fuselage ruptured] had his shirt was sucked off him and out of the plane and… January 6, 2024 In \"Airlines\" 50 Comments",
    "commentLink": "https://news.ycombinator.com/item?id=39102021",
    "commentBody": "Boeing whistleblower: MAX 9 production line has \"enormous volume of defects\" (viewfromthewing.com)535 points by bookofjoe 22 hours agohidepastfavorite419 comments andy_ppp 4 minutes agoIf I was the government I'd break up the arms contractor and plane manufacturer as soon as possible and make it possible for everyone in the plane organisation to talk about problems. The culture of secrecy around these issues is a big disaster. I only want to fly on Airbus and possibly Embraer planes from now on. reply gymbeaux 0 minutes agoparentBombardier is probably adequate as well. Tough to avoid those if you’re flying United Express or American Eagle. reply picadores 22 hours agoprevShoutout to a anonymous Boeing engineer sitting in some for a gruesome permanent crisis-meetings, where mostly pr- and mbas talk about what has to be done, but not what has to change, especially not about how engineering and QA should have a blocking-veto right for any decision and the ones actually talking about plans to change. reply commandlinefan 18 hours agoparent> what has to be done Add story points to your JIRA tickets and make sure they're either 1, 2, 3 or 5 points, but if they're more than 5 points, they have to be split into multiple 5-point stories, and points don't really mean anything but they also mean exactly how many days you're going to spend on them. Oh, yeah, and meet your commitments! We'll tell you what your commitments were. reply psunavy03 12 hours agorootparentThere's a circle of hell for senior managers who swear they're being \"agile\" and then butt into a team's estimations and expect totals of story points so they can \"increase velocity.\" Shouldn't have ever called it velocity in the first place; it's just a fucking WIP limit. reply Shocka1 14 hours agorootparentprevThanks. Now I'm going to have nightmare fuel about one of my experiences as a young software engineer at a well known Japanese auto manufacturer in a Research and Development position. They had a mess of a software project which involved endless meetings on Jira issues, which became more about using Jira than actually solving the issues and making a quality product. This was all done on impossible timelines with outsourced teams that seemed to change every couple months. We also can't forget about editing every Powerpoint presentation N to infinite times, which mostly involved moving a box or changing a color. Any lowly Boeing engineers reading this - my condolences... Truly. reply marcosdumay 10 hours agorootparent> outsourced teams that seemed to change every couple months But, of course. The productivity of the team they hired last month was just horrible, they didn't manage to finish anything! Of course they were fired. reply laborcontract 7 hours agorootparentprevIt’s just as bad at Toyota too, trust me. reply LASR 9 hours agorootparentprevWhere I used to work, we had a “tech lead” who used to demand ICs to come up with story points. When the actual engineers were blank about how much something costs, they had to sit through a 90 minute meeting of group-costing. Naturally the team delivered very little despite being quite skilled. But luckily the blame fell on the lead and not on the ICs as it usually does. reply baseballdork 16 hours agorootparentprev> points don't really mean anything but they also mean exactly how many days you're going to spend on them Work in defense, can confirm. Add \"increment planning\" where we plan the sprints out 3 months. Also, voting on points? Naw, someone yell out a number and that's that. reply commandlinefan 16 hours agorootparent> plan the sprints out 3 months In an \"agile\" way, of course. You can be agile, you just have to say weeks in advance when you're going to be agile so we can plan around it. reply mulmen 11 hours agorootparentI want to take “agile” proponents to a track and make them run Fibonacci multiples of 100m over and over with the only break being to ask how far they want to run next. Who can sprint forever? It doesn’t make any sense! When do we rest? When do we train? What is this metaphor!? reply psunavy03 10 hours agorootparentNice straw man you've got there. You're really beating the crap out of that thing. \"Sprint\" is just a term for something you can also call an \"iteration.\" No one is expecting you to actually go full-throttle 24/7. reply Amigo5862 9 hours agorootparentThen why not call it an iteration, or a round, or a week, or a month, or a cycle, or literally anything other than \"sprint\"? Oh, right, because you're supposed to sprint it because that's maximally profitable (in the short-term, anyway). reply docmars 7 hours agorootparentWhat ever would our beloved boards of directors do if they found out we weren't working fast? ;) reply aporetics 10 hours agorootparentprevEh, that is exactly the omega-point that end-stage capitalism is converging on. reply psunavy03 9 hours agorootparentI love it when people use the phrases \"end-stage capitalism\" or \"late-stage capitalism,\" because it's basically a flashing neon sign saying \"you can ignore me, because I have absolutely zero credibility.\" Makes it easy to move on. reply techbrovanguard 8 hours agorootparentSpeak for yourself, buddy. reply balex 4 hours agorootparentObviously he did. I guess you wanted to say \"I disagree\" but phrase it more aggressively? reply jsight 18 hours agorootparentprevI'm seeing that same pattern now. How did that weirdo version of scrumbut become so popular? reply commandlinefan 17 hours agorootparent> that weirdo version It's actually exactly the same as they way they \"managed\" projects before Scrum (and before XP, which Scrum itself is a bastardized version of): \"tell me everything you're going to do, and then tell me how long it's going to take, and I'll provide value by asking you every day if you're done with it yet. See, I'm justifying why I make more money than you do!\" reply berniedurfee 13 hours agorootparentI remember being on a team that spent a little over 2 weeks doing detailed project planning and estimating. We said the project would take 8 months. The leadership team came back and said upon further review, the project would take 3 months. I was so relieved to hear we could get 8 months of work done in just 3! That’s why executives get paid the big bucks! reply laurels-marts 12 hours agorootparentI’ve seen teams do 2 weeks of detailed project planning and estimating where the entire project could have been done in 2 weeks by a good team if they just set their nose to grindstone and did the work. It’s unbelievable how useless some of these planning and estimating sessions are. reply paleotrope 9 hours agorootparentDealing with that right now. I have multiple PMs and directors involved in requesting another team to literally do five minutes of work because we didn't insist they put it in the current iteration and they put it several iterations away. reply jsight 11 hours agorootparentprevIn a lot of places that was the real waterfall method, and also how waterfall earned its illustrious reputation. reply berniedurfee 10 hours agorootparentWaterfail FTW reply genman 10 hours agorootparentprevI'm now curious. So for how long did it actually take? reply jrs235 6 hours agorootparentProbably got cancelled after 2 months after they realized it was going to take 6 more months. reply jsight 14 hours agorootparentprevI want to believe that you are joking, but I'm afraid you are completely serious and also correct. I'm seeing an increasing drift with both this pattern and \"safE\" toward exactly what you describe. Don't forget your tps reports. reply psunavy03 12 hours agorootparentIt works better when you realize a lot of corporate senior leadership doesn't actually want to empower their employees. They only want to say they want to empower their employees. And then go back to being the Big Important Manager in the Big Important Office making Big Important Decisions. reply jsight 11 hours agorootparentThat's true. Sometimes it is flatly stated too. You see, this is how scrum really works as without this, how would we be able to control development timelines? Also mumbling compliance means it just has to be this way for unspecified reasons! reply docmars 7 hours agorootparentprevScrum is the equivalent of top-level management needing a baby rattle or else they'll start crying for metrics because they're lost without them. Trusting their engineers and seeing their progress for themselves, rather than an abstract bastardization of that progress, would destroy their er... productivity. reply bertil 21 hours agoparentprevIf you want to be the change you want to see in the world, shout in the ether that this engineer will have a job at your organization. That’s what is keeping so many of them silent: a mortgage, school fees, maybe a H1B. reply barelyauser 21 hours agorootparentA man with nothing to lose is the most dangerous. reply SteveNuts 19 hours agorootparentBy far the most effective leadership I’ve worked for are the ones that already have a very comfortable nest egg or are independently wealthy. They’re the ones that aren’t afraid whatsoever to rock the boat or do what they think is right, and are actually able to drive change. reply anonymouskimmer 12 hours agorootparent> They’re the ones that aren’t afraid whatsoever to rock the boat or do what they think is right, and are actually able to drive change. I think a great deal of their ability to drive change is because they know the right things to say to the right people. I.e. they have some diplomatic skills. Because while they might be able to afford to lose their job, they certainly won't drive any change if they actually do lose their job. reply raxxorraxor 1 hour agorootparentI have never seen an engineering project fail because of poor \"relationship skills\" - definitions pending. Diplomacy helps, but mostly it is about being open about problems and about pushing that info up the chain of responsibility. If you need diplomacy for this point, you already have a severe problem here. Otherwise the head will also fumble. I have been in expensive meetings talking a lot about parking space organization. This always happens if there is nothing interesting related to the business to talk about. Stuff gets off-topic quickly. That said, change and disruption has a positive connotation in the business of today. But it isn't always positive. reply gonzo41 19 hours agorootparentprevJust imagine how people would work if everyone had some basics assured! reply docmars 7 hours agorootparentNot that I think assuring basics is a bad thing, quite the opposite — but I think we would see a lot more rude co-workers who are willing to take risks with their working relationships. That is, specifically those with basics assured and still early in their career and/or poor relationship skills to begin with. Those issues already surface in any situation, but it's safe to assume that many hold their tongues because they're afraid of losing their income stream. reply rvba 18 hours agorootparentprevI can imagine that some would be incredibly lazy and some efficient. Question is which group would be bigger. reply Teever 18 hours agorootparentNo the question is which group will have a bigger influence. Lazy people tend to self sort into irrelevance. reply huytersd 17 hours agorootparentprevWhat a silly designation to assign to one of the cushiest, indoor, high paying jobs in our society. reply bertil 12 hours agorootparentCompared to the reality of the situation, I agree that it’s garish, but psychologically, people in cushy situations tend to over-dramatize their setbacks. If they _think_ they have nothing to lose, they will act foolishly just the same as if they really do. If a billionaire slams his toe on the leg of a chair, it still hurts, and being sorry for his real pain isn’t denying his otherwise staggering privilege. It’s an excellent example to use to have him empathize with others. If you want to make them act in society’s interest, you can point out that paying taxes or enforcing environmental regulations won’t prevent them from being rich beyond words. An engineer worried about being fired experiences genuine distress: you can empathize with that there and then because it’s real. Once they have a job, ask them to be more considerate when designing processes or buildings so that everyone can work comfortably in the shade or without having to fear how they are going to feed their family by the end of the week. reply throwawaysleep 20 hours agorootparentprev> shout in the ether that this engineer will have a job at your organization. This. Or crowdfund a mega million dollar bounty for evidence of certain types of malfeasance. The market seems to generally punish whistleblowers, and understandably so. I have never worked for a company that wasn't violating some contract, regulation, or law. In my first job we were forging stuff for PCI compliance. The next job stored medical records in a Git repo. So most people have something to lose if there is a whistleblower personality around, as much as we might admire them when they reveal the secrets of others. I wouldn't want to work with one. I am not a manager and even as a low level individual contributor I cut regulatory and security corners to keep bosses happy. You would get far more whistleblowers if it didn't essentially mean wrecking your livelihood, even if you are willing to sacrifice your career. reply bertil 19 hours agorootparentFar for me to challenge the idea of a fund and bounties. But that’s a lot of money—more than I can personally fork and more than I’d be comfortable advocating to collect. Again, no criticism of your ambition; I’m just a regular guy trying to do his thing. I do believe that good engineers like their work and drive a lot of self-esteem from it being valued—more than living off a stipend because they shot their career in the foot. It’s well-paid enough they rarely think they need to rely on the generosity of strangers, and that’s not a bad mindset to keep. In circumstances like these, people talk about loyalty, notably to their employer; I don’t think loyalty is a bad value, but in that case, you want to advocate for loyalty to the craft and the corps, not the organization profiting from putting people in danger. Engineering values, like the brass ring some get in Canada, are things that you want to support not by saying “we’ll support you if that blows in your face” but “We want people like you to speak up during our meetings.” Giving engineers a guarantee that their craft, their talent and their willingness to ask tough questions is what you value, as a team lead, is something I occasionally can do (not now, though: changing team) and something I’m happy to compensate at fair market value, because it’s objectively a good thing for me too — not because that would put me at odds with my organization, but I’m willing to sacrifice that for some ideal misaligned with profit. We need to work on getting eggheads at Boeing understand that their cost-cutting was a bad idea (SkyScanner letting you filter-out plane time is beautifully aggressive that way), but in the meantime, we need to tell people facing tough decisions that engineering is full of hard trade-offs but that security is not one of them, and not one that should need charity. reply Cheer2171 17 hours agorootparentprevOr require the government to give a proportion of the fine to the whistleblower, like the SEC has. We find out about a lot of shady finance through that program. reply specialist 19 hours agorootparentprev> crowdfund a mega million dollar bounty for evidence of certain types of malfeasance Yes and: Make whistleblowing-for-hire official. IIRC, there's a law practice whose sole activity is suing prescription management practices. Basically forensic accounting meets administrative law. I want more of that. Further: Winner-takes-all seems to be some kind of natural law. And by extension, regulatory capture. Civil society needs countervailing mechanisms that exist outside or separated somehow. I most like ensuring pro-competition and anti-monopoly policies. But monopolies will always exist, eg natural monopolies like utility districts and police, so we also need transparency and accountability hacks like you're suggesting. reply sparrowInHand 17 hours agorootparentHow about, if you can proof your company engages in monopoly practices, you get as a reward, parts of the company to start your own? reply sparrowInHand 21 hours agorootparentprevShould call them what they are, behavioural hacking to introduce compliant, seddated behaviour, without a longterm grasp of the damage the hacks inflict. Obvious solution? More law-ducttape on the imperfect ape. reply firstplacelast 20 hours agorootparentprevThis is what I'm hoping for the future, so many will be so poor we won't have mortgages or kids, nothing to lose. But I doubt it. reply iancmceachern 12 hours agoparentprevAnd these meetings mysteriously have a bunch of sticky notes everywhere, they're purpose is unknown, they're unintelligible, but load bearing for the project. Load bearing sticky notes. reply dclowd9901 18 hours agoparentprevIt's not like there haven't been tons of books and studying done on the topic. We have an _almost_ ideal production methodology that ensures absolute quality (the Toyota Production System). I take issue with its just-in-time supply chain, but the quality factor is worth emulating, and basically any company who has has guaranteed themselves best quality outcome. So this is willfully producing a less than superior product, which, when it comes to airplanes, is basically criminally negligent since it will result in a crashing plane that kills people. Not sure how execs are so capable of dodging culpability, but I'd say the real issue is our justice system holds no one rich to account. reply toss1 12 hours agoparentprevHere's the change that needs to be made (more accurately, reversed): >>Boeing outsources a lot of the production of components for its aircraft because it’s cheaper as part of an overall shift in strategy that dates to CEO Harry Stonecipher who had been CEO of McDonnell Douglas, >>>> \"When people say I changed the culture of Boeing, that was the intent, so that it’s run like a business rather than a great engineering firm.\" IOW, the McDonnel Douglas, now Boeing, CEO deliberately wrecked the engineering firm to make finance primary. In my book, that merits being tarred, feathered, drawn, and quartered. He's putting millions of lives at risk, risking the defense capabilities of the US, and already killed hundreds. All to line the pockets of himself and his cronies. The only question is whether the engineering organization is too far lost to reconstitute. reply RONROC 11 hours agoparentprevunreasonably based. there's almost no other take than this, and if there is one, it must address this. reply JoshTko 19 hours agoparentprevCan we just agree to not glorify and denigrate people based on title or role? It's a bit silly. reply sgarland 19 hours agorootparentIf you’re flying at 500 MPH and 30,000 feet, what department do you want the manufacturer to have paid attention to the most: engineering, or sales? reply bitcharmer 19 hours agorootparentprevBut it's that specific caste that ran Boeing into the ground. Let's call a spade a spade. reply tiahura 19 hours agoparentprevAn MBA designed MCAS? An MBA designed the door plug installation process? Maybe it’s the Boeing engineers that need some scrutiny? reply mihaaly 10 minutes agorootparentSounds like you suggest that an MBA have no influence on the production and output of the organization, the performance, not at all, they just stand there watching helplessly while others do something some to them uncontrolable way. Can they all be fired and the wealth they leach redistributed then please? :) reply jrockway 12 hours agorootparentprevI think MCAS was one of those organizational failures. I could see myself implementing something like that. I imagine the conversation would go a little like this: \"We need to replicate the pitch performance of the 737-800. These engines pitch the plane up when accelerating and it messes up go-arounds.\" \"What if we have the computer just pitch down to compensate? We know the angle of attack and the engine thrust output.\"Meanwhile in another conference room: \"United is telling us that Airbus is giving them an A321 for 10% less than us. What can we cut?\" \"No regulation requires us to have 3 angle of attack sensors. We could get by with 1 and stick it on the Minimum Equipment List if it fails.\" \"That sounds great.\" Then the 1 angle of attack sensor fails the system that was designed for 3. Was it the engineer who agreed to hack 737-800 handling emulation onto the 737 MAX that failed here? Was it the engineer that agreed that the AoA sensors didn't need to be redundant, perhaps before MCAS was even invented? These complex failures are rarely the result of one individual failing. Everyone did their job; the business saved United 10% (I actually have no idea who their launch customer was), the engineer saved thousands of pilots from being pulled off the line for re-training. But combined, it was a tragedy. Ultimately, it's the organization and its processes that failed, not an individual. As a manager, you own the organization and the processes. reply Prickle 11 hours agorootparentMinor correction about the Max8 that makes it worse. Turns out the Max8 does have two angle of attack (AoA) sensors. But the MCAS system only read from AoA1, and never validated it against AoA2. It was recieving info from AoA2, but never used it! reply 0x457 9 hours agorootparentMakes sense. Two sensors are about as useful as 2-node Consul cluster. reply marcosdumay 9 hours agorootparentprevWhat do you do with 2 sensors? You can do something with 3 or more, but 2 are completely useless. reply Prickle 6 hours agorootparentWhy would it be useless? Boeing's update for the Max8 MCAS (to make the FAA happy), was to make the system validate the two values. If there is a mismatch, the system outputs an error and MCAS will disable itself. In fact, that is literally one of the requirements set by the FAA to begin flying the Max8 again > FAA Airworthiness Directive approved design changes for each MAX aircraft, requiring input from two AoA sensors for MCAS activation, elimination of the system's ability to repeatedly activate, and allowing pilots to override the system if necessary. reply CamperBob2 9 hours agorootparentprevYou can signal an immediate, specific warning if they disagree. reply tiahura 11 hours agorootparentprevStick pushers aren't anything new, and the sad irony is that except for the two tragedies, MCAS worked so well that in the thousands of flight hours the Max had, no pilot had even noticed it. It absolutely was the fault of the engineers who failed to realize what would happen with an AOA fault. I'd love to be corrected, I don't believe a single email from the engineers to management telling them would would happen was ever dug up. And, there's never been an engineer whistleblower saying \"I warned them.\" Has a single PE lost their license over the Max? reply Prickle 11 hours agorootparent> that in the thousands of flight hours the Max had, no pilot had even noticed it. This is false. At the time of the incident (>1month) The FFA and NTSB found 23 reports of uncommanded pitch down attitudes during takeoff. Quick Google gave me this. https://www.sbs.com.au/news/article/it-pitched-nose-down-us-... reply mcmcmc 19 hours agorootparentprevEngineering failures don’t just happen in some contextless organizational vacuum reply tiahura 18 hours agorootparentEngineers can't fail, it's always management's fault. reply gilbetron 18 hours agorootparentEngineering needs to be embedded in a system that can handle the fact that engineers fail and still produce reliable and effective outputs. The primary purpose of management is making sure such as system exists and functions. Managers are the engineers of the entire meta-system, basically. reply enasterosophes 12 hours agorootparentprevIf management don't want to take responsibility, they could take a pay cut and give the money to the engineers. reply swexbe 18 hours agorootparentprevUnironically, yes? Management is where the buck stops. reply lamontcg 15 hours agorootparentprevYes, in fact the Captain is always responsible for literally everything that happens on the ship. reply g-b-r 18 hours agorootparentprev> Engineers can't fail, it's always management's fault. or, what can engineers possibly know about engineering: https://www.airliners.net/forum/viewtopic.php?t=213075 reply lou1306 18 hours agorootparentprevEngineers can fail, and when they don't QA may, but it's management that chose to believe otherwise reply scrps 17 hours agorootparentprevNormally I would agree if an engineer directly knew what they were doing was bad and didn't say no but in the case of MCAS: pilots had no idea it was there. Yes, it also wasn't properly redundant and that is bad but had the pilots known it had existed in the first place and understood it was operating off a single data source they would have acted accordingly and a lot of people would still be alive. Door plug I can't speak to. Hiding MCAS to avoid recertification was well above the paygrade of an engineer at a bench poking a circuit. Also, if you want a short version of how this stuff goes very wrong even when engineers do say something, the challenger disaster[1] movie is great. Chase it with the NASA documents[2] for the long version. [1]: https://www.youtube.com/watch?v=bvv2-7iOD_8 [2]: https://www.nasa.gov/history/rogersrep/genindex.htm Edit: typo reply aredox 18 hours agorootparentprevThe three dimensions of every project are always: Cost Quality Time MBA push for cost, then time (to market), above quality. reply iancmceachern 12 hours agorootparentprevRead about the Space Shuttle Disasters. There are several good books on them. reply baq 19 hours agorootparentprevMaybe the QA engineers were fired because they were doing their job but the MBAs optimised for the wrong metric? reply salawat 5 hours agorootparentDing ding ding. reply dclowd9901 18 hours agorootparentprevCurious who believe makes so-called \"executive\" decisions in an organization... If you believe it should be engineers, we're fully in agreement, but then why do we have managers... reply delfinom 18 hours agorootparentprevThey don't put Boeing engineers in management or executive positions these days. Heck, they fired the last CEO over the 737 crashes but he was literally only on the job for 3 years and had no involvement with the 737 design. The problem? He was an engineer and he was already stirring up shit inside. So they threw him under the bus and replaced with him another useless goon that spouts PR bullshit and has no technical experience other than milking companies dry. reply fantyoon 22 hours agoprevIts worth noting that this article is based entirely on an anonymous comment made below another article. They also link a Tweet alleging that \"[..] other usually credible, usually careful folks positing same idea.\", but no links are provided to that. reply tyingq 21 hours agoparentAlbeit a long comment which seems to have correct insider jargon, plausible numbers and scenarios, etc. Agreed it's getting more credence than I would expect, but it wasn't a lazy offhand comment. reply ufmace 13 hours agoparentprevThat's true, but where else would you expect to get such information? No real whistleblower is going to publish their name, rank, and serial number etc. The corporate PR department sure isn't ever going to tell you such things. Investigative journalism and reporters who can be trusted to verify sources and preserve their anonymity is all but dead these days. The Government might publish something that isn't a total white-wash in 5 years. Or they might not. Who can tell? reply mihaaly 19 hours agoparentprevI will never believe such until Boeing management announce it in an international press conference at their HQ! ;) reply nunez 11 hours agoparentprevjonnyc is a hardcore airline enthusiast (i think they might actually be ex-American Airlines) and is extremely active on flyertalk. i trust them when they said that several sources confirm this anon comment. reply fallingmeat 21 hours agoparentprevCame here to upvote this idea. Many of you probably relate to the under-informed or less experienced junior engineer who gets dramatic at times. Did this person see something they thought was a non-conformance but actually standard operating? reply bertil 12 hours agorootparentI’ve been that junior engineer more than once (and not always junior), and I’ve had to deal with it more than once. The proof is in the pudding, and the pudding is that there are at least four distinct significant mechanical issues; three have led to major emergencies on board. The second element is that this particular junior engineer has seen enough to know not to share his name. Naivety goes both ways, and the rhetorical engineer tends to leave identifiable information if they don’t think getting caught is a problem. This one does. He’s asked internally, and the answer wasn’t reassuring. I’ve been on the receiving end of “What do you think we should do about this?” and hard trade-offs; if the manager knows what they are doing and they are not pushing bad stories, the junior engineer learns something. The last element is that this is not new: very senior engineers complaining about penny-pushing MBAs ruining the company and “that won’t stop until someone gets hurt” are as old as the merger with McDonnel-Douglas. Would I trust the technical details and the recommendation of that particular engineer? Lord No: I know nothing about airplanes. But I know what a dysfunctional operation sounds like, and that sounds like the bassoon—but there’s the strings, the brass, and the percussions playing the same tune. reply happytoexplain 12 hours agorootparentprevWhether this whistle-blower is well-informed or not is an interesting question, but it is overshadowed by the fact that we even have to ask. The fact that the public still doesn't know what happened or is happening is unacceptable, to put it lightly. Boeing has been given a chance - now they should be raided top-to-bottom by 3rd party agents, whether that be federal or private, and be placed under existential threat until we are satisfied with the investigation and resulting consequences. reply bertil 12 hours agorootparentThat is very true, with the caveat that FAA and NHTSA aren’t the fastest at putting out a report —by design, thankfully: I want that thing vetted. It’s been three weeks. That would be better if we knew much more in the next week or two. If not, we shouldn’t be too patient either. reply stringsandchars 21 hours agoprevMuch as I'd like to see Boeing investigated for what look like systemic failures in their construction processes, I don't think a totally anonymous comment on some blog article on the internet really qualifies anyone as \"a Boeing whistleblower\". reply ricardobeat 20 hours agoparentThat’s understandable caution, but would you expect a whistleblower to not be completely anonymous? This is why we still need serious journalism by the way - a reporter would personally contact and verify the credentials of the whistleblower while protecting their identity, with their career on the line. Random news blogs or independent reporters will never replace this. reply stringsandchars 20 hours agorootparent> would you expect a whistleblower to not be completely anonymous? In my view, a \"whistleblower\" is someone who gives their information confidentially (not anonymously) to a responsible investigating authority. Not someone who writes a comment on the internet that you or I could write ourselves, with a little research sprinkled with credible-sounding 'insider chit-chat'. reply civilized 20 hours agorootparentOrdinarily I would agree with you, but the case of Boeing includes some extraordinary context. This company's incompetence killed 346 people in 2018-2019, and the postmortem revealed an unethically cozy relationship with their regulator, the FAA. Now, five years later, we are seeing more catastrophic failures that will eventually claim more lives, and once again the FAA is coming under scrutiny for being too lax in its oversight of Boeing. It seems to me that the FAA lacks some kind of either technical or organizational competence to regulate Boeing. Screws must be turned publicly until Boeing's financier-led culture is replaced with the original engineering-led culture. Anonymous insider comments are one reasonable way to do this, although it might be better to work as an anonymous source to a well-known journalistic outlet. reply willk 10 hours agorootparent> This company's incompetence killed 346 people It wasn't incompetence, it was greed. They said \"no new training for pilots\" in order to keep sales volume up and to ensure no new training they kept the MCAS system a secret from pilots. reply jakeinspace 19 hours agorootparentprevIt is very in-vogue to hate on Boeing on the internet. Deservedly, I might add, but it does mean that it wouldn’t be at all surprising for some random non-employee to make up some detailed post as a whistleblower. reply g-b-r 18 hours agorootparentRight it's \"in vogue\" \"to hate\" rather than being concrete facts reply g-b-r 20 hours agorootparentprevFor now it seems credible and can be at least kept in mind. Or maybe we could wait another 20 years: https://www.airliners.net/forum/viewtopic.php?t=213075 reply dm319 8 hours agorootparentWell that is a prescient piece. reply g-b-r 7 hours agorootparentYeah, and you can't miss its comments reply ricardobeat 19 hours agorootparentprevSee the second part of the comment. The anonymity is not the problem. reply Sebb767 20 hours agorootparentprev> That’s understandable caution, but would you expect a whistleblower to not be completely anonymous? On the other hand, wouldn't that also sound like something someone completely unqualified could easily make up and it would just fit our own bias enough to avoid thinking about it too critically? Complaining about Boeings quality, especially in the 737 Max department, is just an easy and cheap shot. reply lamontcg 15 hours agorootparentIt is \"easy\" because it fits perfectly with all the publicly accessible information that we have about the relationship between Boeing and Spirit Aerosystems. We know it was spun off out of Boeing in order to cut costs. We're also a forum full of engineers who have been inside companies that have done those kinds of things before in other industries and seen how it turns into a shit show. Yeah, this article is filling in the details that we fully expect to see around how the sausage is actually being made, to that extent it fits our preconceived biases. Biases can be entirely accurate. And even if this is a work of fiction around this particular incident, the reality may not be far enough off to matter. There's enough smoke that all the biased people rushing to assume there's a fire are likely more correct than the skeptics demanding to see the flames enveloping the building before rushing to any judgement. reply Sebb767 14 hours agorootparentI don't claim that the comment is wrong (I have no knowledge of Boeings internals and it actually fits my biases, too), but I still wouldn't qualify it as evidence. There's solid evidence that there's a metaphorical fire; trusting an anonymous internet comment actually weakens/poisons the pile of evidence. reply ricardobeat 19 hours agorootparentprevHence the rest of the comment. Journalistic integrity includes verifying your sources. reply dahart 17 hours agorootparentprevIsn’t speculating wildly on the internet that an internet comment is fake just about the cheapest shot there is? The comment does include many verifiable details… reply Sebb767 14 hours agorootparentI'm not making the accusations against Boeing, so I'm not the one who needs to provide evidence. I've also also given clear points on why I assume the comment to be fake and I did so with my known username attached - so no, it's not just a cheap shot :-) Also note that I never claimed that the comment is wrong (I have no private knowledge about Boeings processes), just that the comment could easily be fake. reply dahart 13 hours agorootparentI didn’t suggest you took a cheap shot at Boeing. FUD without evidence is always a cheap shot though. reply jrockway 12 hours agorootparentprev> with their career on the line Accessing the SAT system to re-read all the tickets to the reporter sounds like the easiest possible way to get fired. Obviously someone is going to review every access and be very skeptical of anyone viewing that data after the news broke. I guess there is always \"use your coworker's computer when they go to the bathroom\", though. Lock your screen! reply ajross 20 hours agorootparentprev> would you expect a whistleblower to not be completely anonymous? [...] This is why we still need serious journalism Exactly. I'd expect a whistleblower wanting to be taken seriously to be talking to a real journalist with editors and a publication with a history of getting things right, not a single-author mid-tier aviation blog. The temptation to spin things \"just a little\" and to fail to verify identity and authority, for a site like this, is just too high. reply g-b-r 18 hours agorootparentThis is how it can go: https://www.kansas.com/news/business/aviation/article2620683... reply voakbasda 17 hours agorootparentExactly. The problem is that whistleblower protections come from the government. The courts cannot be trusted to always have your back. Hell, I would go further and outright assume that they will blatantly favor corporate interests. I do not blame someone for not wanting to risk the exposure. reply g-b-r 17 hours agorootparentIn this case I'd stress that the FAA itself cannot be assumed to \"have your back\", given their coziness with Boeing (although I can't know if the whistleblowers were truthful) reply ajross 16 hours agorootparentprevTo be clear: mainstream major media publications use anonymous sources all the time. The point here is that the whistleblower be talking to the Times or the Post[1] and not \"View from the Wing\". No one is demanding they come out in public to tell us who they are. [1] And let's also be clear: journalists from both those organizations are surely on the phones full time right now trying to find dirt on Boeing quality processes. That would be a huge story if they could land it. reply g-b-r 7 hours agorootparentYeah you're not wrong that he should try to talk to a major publication (if the story is true) reply ufmace 13 hours agorootparentprevThe mid-tier blog (probably?) doesn't depend on revenue from it for their living. Professional journalists these days often do, and are teetering on the edge of solvency as-is. And so they're a lot more vulnerable to their corporate contacts telling them, don't publish that story or slip us the name of the whistleblower, or else you'll be blackballed from our PR department. Your job will be toast, since we're your only real source, and there's 50 other journalists and publications in line to replace you. reply ajross 11 hours agorootparent> The mid-tier blog (probably?) doesn't depend on revenue from it for their living. The page is absolutely filled with ads. Clearly it's a revenue source, not a labor of love. Also your ideas about the media industry somehow being compliantly beholden to the interests of Boeing (a company less than a tenth the size of Google or Apple) seems a bit silly. This would be a huge scoop and a big win for any paper that got the story. That they don't have it speaks broadly to this blog's story being spun. reply ufmace 6 hours agorootparentThe \"View from the Wing\" site that is the main link here has a relatively modest number of ads once I turn off my ad blocker. Not that much compared to many \"proper\" news sources. They do seem a little click-baity, and are loaded with tracking scripts, but I doubt they're making enough money for even one person to make a living. I wouldn't trust them that much, but they're not the actual source anyways, they're just republishing a comment from another page. The linked page that the comment that is the source of all this is actually on is \"Leeham News and Analysis\", which looks like a professional-quality aerospace news site. I don't see any ads or tracker scripts on them at all. The author and the bulk of the commenters sound pretty professional and knowledgeable. It looks like they sell subscriptions at substantial cost, which explains a lot. That's enough for me to presume they're much more trustworthy about the aviation industry than any legacy media source. When was the last time the legacy media did a real investigative report about something that wasn't super hip or somebody had a grudge against already? You may not necessarily agree, but I think it's at least reasonable that an actual whistleblower like this wouldn't bother with them. Or maybe they already did, and got ignored, because they thought the article would cost too much to put together and not get enough clicks to bother. On Leeham's site, the author did ask the commenter to contact them - perhaps something will come from that, or maybe not. reply magicalhippo 19 hours agoparentprevWhile I agree, Blancolirio over at YouTube went through a recent lawsuit[1] which had some non-anonymous sources claiming quite serious incidents at Spirit. [1]: https://www.youtube.com/watch?v=RSGujNq4bVM reply fendy3002 19 hours agoparentprevBecause recently there's news that aircrafts from boeing has loose bolts, the claim isn't too far fetched off. reply Grimblewald 20 hours agoparentprevWhile I agree in principle, as a society we don't have a great track record of supporting or protecting whistle-blower, so hard to verify shit like this is what we have to put up with in response. reply bertil 12 hours agoparentprevWhat would make such a person an official one? reply fghorow 12 hours agoprevWTAF? \"\"\" Once they have finished, they send it back to a Boeing QA for final acceptance, but then Malicious Stupid Happens! The Boeing QA writes another record in CMES (again, the correct venue) stating (with pictures) that Spirit has not actually reworked the discrepant rivets, they just painted over the defects. In Boeing production speak, this is a “process failure”. For an A&P mechanic at an airline, this would be called “federal crime”. \"\"\" reply bambax 20 hours agoprev> this check job that should find minimal defects has in the past 365 calendar days recorded 392 nonconforming findings on 737 mid fuselage door installations. That is a hideously high and very alarming number, and if our quality system on 737 was healthy, it would have stopped the line and driven the issue back to supplier after the first few instances. If this (anonymous) quote is true, it may mean Boeing thinks production quality can be achieved via Quality Control alone. Yet nothing could be further from the truth. reply rcbdev 19 hours agoparentThis is the same thing as developing software against a huge test suite and not thinking about code quality / coupling / sane architecture anymore. Incidentally, this is lived reality for developers on the DB team at Oracle. reply marcosdumay 9 hours agorootparentAnd, on the Oracle case, it shows. But getting failures when you need to test a clob for nullity, calculate the size or a raw binay, or get null when you were expecting an empty string (or the other way around) is usually not life-threatening. reply tiahura 19 hours agoparentprevSounds like a defective design. Just like MCAS. Hopefully the MBAs get control over the mess the engineers have created. reply wvh 16 hours agoprevIf this happens at Boeing, should we double-check other aviation companies? Is this part of a systemic trend to hollow out engineering industries for maximum profit extraction? Back to my \"trying to buy a toaster\" problem... I just found out all commercially available toasters, no matter what price, are made in China. I don't subscribe to the \"everything from China is bad\" philosophy, but I feel this is pretty absurd. You simply can't buy a toaster that is manufactured in Europe or America anymore. Even companies such as Bosch that still produce the majority of their products in Europe (as far as I know) seem to have thrown in the towel. Maybe we need to have an engineer revolution instead of silently being part of this hollowing out by that top percentage of business and political \"elite\". reply scrlk 15 hours agoparent> You simply can't buy a toaster that is manufactured in Europe or America anymore. Dualit Classic toasters are still made in the UK: https://www.dualit.com/collections/classic-toasters Other Dualit toasters are made in China. reply mglz 17 hours agoprevAt which point did the MBAs convince us they are suited for more than bookkeeping? Medicine, engineering, architecture and many other things have been slaved to economics and this is presented as the natural order of things. Why? reply acdha 12 hours agoparentLook at who gets them and their social status - the worst MBAs I’ve dealt with came from money, went to expensive schools, but … weren’t exactly going to knuckle down for med school or becoming a PE. The MBA was a way to add plausible deniability to the idea that the question of whose opinion matters usually comes down to class. (One notable exception: Scots-Irish guy from a very modest background - unsurprisingly, he worked harder than anyone else I’ve met and took care of the staff) reply peoplenotbots 21 hours agoprevIm depressed to know several areospace family members who have joked the commercial airline industry can be an open door soon reply RaoulP 18 hours agoparentWhat do you mean by open door? In the sense of the \"revolving door\" through which people move between politics and industry? reply mynameisash 18 hours agorootparentProbably a double entendre of that and the newly surfaced issues about the door blowing off[0]. [0] https://www.theguardian.com/world/2024/jan/09/alaska-airline... reply whitej125 19 hours agoprevAll the focus is on Boeing (as it is a household name) and they have final signoff... but Spirit Aerosystems (the fuselage manufacturer, not the airline) is also a big, public company that doesn't seem to be sharing any of the blame here. This is ironic because Spirit also manufactures parts for Airbus, etc. https://www.spiritaero.com/company/programs/ reply thallium205 19 hours agoparentIt's Boeing's job to stop the line but they didn't as the whistleblower mentioned: \"As a result, this check job that should find minimal defects has in the past 365 calendar days recorded 392 nonconforming findings on 737 mid fuselage door installations (so both actual doors for the high density configs, and plugs like the one that blew out). That is a hideously high and very alarming number, and if our quality system on 737 was healthy, it would have stopped the line and driven the issue back to supplier after the first few instances.\" reply cameldrv 15 hours agoparentprevI've heard Spirit has had a lot of problems, but likewise Boeing itself is not immune to this. A few years back the Boeing South Carolina plant was having a very consistent problem with foreign objects (wrenches, parts, etc) being left behind in assembled planes, to rattle around and damage who knows what. My understanding as well is that while Spirit makes the fuselage of the 737MAX, final assembly is at Boeing in Renton Washington, and the plug door would have been removed for final assembly and then reinstalled in Renton, so Boeing itself would be responsible, assuming, as appears likely, that the plane left the factory without the door bolts installed. reply hencq 18 hours agoparentprevYeah, assuming this article is true, I wonder if Airbus is finding similar quality issues with Spirit as a supplier. reply p_l 18 hours agorootparentAirbus is, supposedly, very much \"boots on the ground\" in cooperation with suppliers. Somehow nobody hears of issues they have with Spirit, but at the same time nobody hears of Spirit being squeezed dry by Airbus - only by Boeing. reply Symbiote 17 hours agorootparentAlso Airbus mostly use the Spirit factories in the UK (from the link above), which Spirit acquired from a British company. Plenty of room for differences between those factories and the former-Boeing one in the USA. reply p_l 17 hours agorootparentIncluding in terms of financial stability! reply Zetobal 9 hours agoparentprevIf you look something up look it up in detail. Especially which parts of the planes get build by them for Boeing and Airbus AND Spirit is a spinoff of Boeing... it's their bad bank. reply ironmagma 13 hours agoprev> The FAA has issued a new Airworthiness Directive (AD) that will allow most MAX 9s to return to service. Contrast this few-week stint with what happened with NASA after Challenger and Columbia. Everything came to a grinding halt while questions were asked about culture and entire operating principles. And then here, we just have an airworthiness directive followed by business as usual. This is pathetic. reply nomel 13 hours agoparent> there are 4 bolts that prevent the mid-exit door plug from sliding up off of the door stop fittings that take the actual pressurization loads in flight, and these 4 bolts were not installed when Boeing delivered the airplane, our own records reflect this. Yeah. Wtf. reply 0cf8612b2e1e 12 hours agoparentprevEffectively zero economic activity is changed depending upon the outcome of engineering challenges of a NASA vehicle. In contrast, grounding hundreds of planes for an indeterminate amount of time has severe implications for many companies. As far as I know, there are no claimed underlying engineering faults to the design. Ask the uncomfortable questions, but if all of the planes can be reviewed to confirm basic airworthiness, at some point you have to let them fly again. reply voisin 12 hours agorootparent> but if all of the planes can be reviewed to confirm basic airworthiness, at some point you have to let them fly again. Doesn’t this review and confirmation take place before they even go into service? So if they missed these bolts on the last review, what’s to say they won’t miss them or something equally as important on the next review? “Oh think of the economy” is the worst excuse I can imagine to let these back into service. reply ironmagma 12 hours agorootparentprevThe shuttle program was a significant source of economic activity in a number of different areas of the country. [1][2] Just because we really want to have planes flying, isn't a good reason to let them fly if they're not fit to do so. This is known in aviation as \"get-there-itis\". You really don't have to fly. [1] https://www.theguardian.com/science/2011/jul/07/space-shuttl... [2] https://spaceflightnow.com/shuttle/sts135/110706preview/inde... reply Denvercoder9 9 hours agorootparentVery little of that economic activity was directly linked to the Shuttle flying. If we had moved the Shuttle through the fabrication, preparation and refurbishment cycles without actually launching it, the economic impact would be more or less the same. That's not true of airline flights, which generate most of their economic value by actually moving people and/or stuff. reply ironmagma 9 hours agorootparentBut just as with pilots and get-there-itis, the \"need\" to fly the planes is secondary. The primary concern is making sure the higher order system is operating correctly, which we know it isn't. Flying planes will be counterproductive if it leads to a bad crash, setting the industry back even further than grounding the planes and planning ahead would in the first place. If your goal is to move people and stuff as much as possible, the strategy should be to make sure you can do that safely. We can and will abandon air travel as soon as it loses its safety record. reply dingnuts 12 hours agoparentprevI would like Boeing to get through this and be able to produce new aircraft, though. NASA hasn't produced a new human-capable spacecraft since these disasters, and it has been a long, long time. If NASA was a private enterprise, it would've simply gone out of business. Instead, it stopped being an exciting organization willing to push the boundaries of space travel, which is inherently risky, and became a risk-averse bureaucracy mostly wasting taxpayer money until they eventually got/get the fire lit under their ass by organizations like SpaceX showing that it is still possible to progress in space exploration in ways other than launching robots reply ironmagma 12 hours agorootparentNASA worked through its issues. When the shuttle program ended, it wasn’t directly because of either of the disasters. Both of those were followed by extensive reworking of the system behind the missions. Also, Boeing is not a fully private enterprise and receives significant federal funding. reply willk 12 hours agoparentprevThink of the shareholders though /s In all seriousness though, what can be done? It appears that Boeing has to much power in their relationship with the FAA. reply ironmagma 8 hours agorootparentReduce the power. It's much more easily said than done obviously, but things like exemptions just should not be done. If the FAA makes a rule, it needs to be adhered to. The FAA needs a spine. Whatever we need to do in terms of reform to achieve that is up for debate, but we aren't even having that conversation yet. reply aredox 21 hours agoprevThe questionable engineering of the 737 Max https://youtu.be/hhT4M0UjJcg?si=2-nj6h8Ni5HvKh26 reply JCM9 21 hours agoprevIf the cited level of incompetence checks out as true then that borders on criminally negligent and some Boeing exec probably needs to go to jail. reply rybosworld 17 hours agoparentCorporations tend to structure themselves so that those at the very top are never culpable beyond losing their job. And when you've made millions of dollars over the years, losing your job is at most an inconvenience. reply jacquesm 20 hours agoparentprevI'd happily bet that that will not happen. reply readthenotes1 20 hours agoparentprevHow many went to jail for killing hundreds of people with the max before and covering it up? reply AnimalMuppet 20 hours agorootparentWhat, specifically, are you referring to when you say \"covering it up\"? reply Zigurd 19 hours agorootparentOne example: David Calhoun told the newspaper that pilots from Indonesia and Ethiopia “don’t have anywhere near the experience that they have here in the US”. He added the planemaker made a “fatal mistake” by assuming those flying the aircraft would immediately counteract software failures, which played a role in both accidents. reply alkonaut 20 minutes agorootparentMinimizing the damage (legal and reputational) is of course the job of any executive. What that looks like will vary between business and legal cultures, but that's what it looks like in the US. The way the US law works (as I understand it from popular culture and news) is a good idea to deflect liability as much as possible as any admittance of liability could end up costing. reply HideousKojima 19 hours agorootparentprevThat doesn't sound like \"covering up,\" that sounds like saying \"the flight crews should have been skilled enough to compensate for our awful software.\" Still terrible, mind you, but I'm not seeing a \"cover up.\" reply acdha 12 hours agorootparentThat’s exactly what a coverup looks like. At that time, they knew they had significant airworthiness issues AND that they had deliberately removed mention of the MCAS system from the training guides in support of their marketing pitch that it’d save the airlines a ton of training costs. Those pilots did what they were trained to do but he lied about it trying to cover up the problems which Boeing had known about years earlier because that would make his company liable. reply albert180 12 hours agorootparentprevThe awful software they intentionally made not redundant so they wouldn't have to retrain and inform the pilots about it, as it would have then be classified as Security Critical (which it obviously was)? Also first blaming bad maintenance then poor training, when they knew they fucked up? reply PedroBatista 18 hours agorootparentprevThey already knew what was the problem before and during those crashes. But never admitted anything and deflected any problem until the very last second when it was already so obvious. reply spaniard89277 22 hours agoprevI wonder what's the difference in Airbus. reply marsRoverDev 21 hours agoparentHave worked at both, Airbus pays worse but has a much better engineering culture. Also arguably, because of the location(s) the quality of the engineers is quite high despite the pay as they can afford a pretty good lifestyle. Airbus also functions very much like a quasi governmental institution in many parts, so there's less interest in squeezing everything to death to save money. Finally, Airbus generally has a KISS mindset, and are very conservative w.r.t change in engineering practice and tooling. When I was there we spent way, way, way, way, way more time testing than writing software - and the software was written in a way that any software engineer could walk off the street and understand it. Oh, and quite low levels of outsourcing in critical software - they save that for things that don't have people's lives on the line. reply ahartmetz 21 hours agorootparent> the software was written in a way that any software engineer could walk off the street and understand it Amazing if true. One of the highest achievements in software. reply eddiewithzato 21 hours agorootparentprevDoesn’t Boeing use infosys? Yea I’d take airbus reply Zigurd 20 hours agorootparentHCL was mentioned as a Boeing software outsourcing shop. Not the only India software shop used by Boeing. I recall reading a news story about outsourcing being linked to Boeing sales in India, but there's a pile of news stories about Boeing outsourcing to India, so it's hard to find where and how it started. More recently, Boeing laid off 2000 people in the US and moved those function largely to Tata's BPO. This follows the pattern of how IBM was hollowed-out. reply leoh 10 hours agorootparentprevBoeing writes a lot of code in Banagalore iiuc and has an office there right around the corner from Infosys. reply TheCondor 19 hours agorootparentprevWhat are some of the differences in engineering culture? It’s kind of an opaque term that can mean a lot of different things. reply aredox 21 hours agoparentprevAircraft Certification: Comparison of U.S. and European Processes for Approving New Designs of Commercial Transport Airplanes https://www.gao.gov/products/gao-22-104480 See also this comment and answers: https://news.ycombinator.com/item?id=38973101 reply pjc50 19 hours agoparentprevAirbus are trying to compete in the US market, while Boeing rely on protectionism? reply ranger207 12 hours agoparentprevTheir corporate culture hasn't been corrupted by MBAs chasing profit over product yet reply MilStdJunkie 12 hours agoparentprevGermans. Germans everywhere. reply Glawen 4 hours agorootparentAs in understand, German played a minor role in the design in Airbus, for example the flight controls were designed in Toulouse by french engineer. However they have more responsibility in designing the production system and assembly line. The assembly line in Hamburg is quite unique. reply apexalpha 19 hours agoprevThis is a bad title. There's no whistleblower at all, at least not someone official who has come forward and asked for whistleblower protection. All we have here is an article written on the basis of a single comment on a single website by a anonymous user. reply alkonaut 14 minutes agoparentHopefully the anonymous commenter HAS also made a proper whistleblowing process where the problems are reported confidentially to the relevant authorities. To then comment on it anonymously in public is not in itself blowing the whistle, but it might make sense to do both at the same time. reply g-b-r 18 hours agoparentprevYou don't need to ask whistleblower protection to be a whistleblower reply apexalpha 18 hours agorootparentAt this point we have no clue if the person who wrote the comments: - Even works at Boeing - Works in that department - Is truthful about anything they claim - Doesn't have an agenda It's just an anonymous internet comment. Surely some scepticism is warranted. reply g-b-r 17 hours agorootparentSurely you can't assume it's true, but the same things could be said of Hacker News commenters, if it weren't forbidden reply exar0815 21 hours agoprev…As a result, this check job that should find minimal defects has in the past 365 calendar days recorded 392 nonconforming findings on 737 mid fuselage door installations >The Boeing QA writes another record in CMES (again, the correct venue) stating (with pictures) that Spirit has not actually reworked the discrepant rivets, they just painted over the defects. In Boeing production speak, this is a “process failure”. For an A&P mechanic at an airline, this would be called “federal crime”. Holy Shit. Whoever is directly and indirectly responsible needs to go to jail. Thats a level of malice which I don't remember eading very often in any kind of news. reply stef25 20 hours agoparentPainting over the defects was just a figure of speech right ? reply AnimalMuppet 20 hours agorootparentIf I understand the quote correctly, no. It's literally what they did. reply op00to 20 hours agorootparentprevNo, structural paint. reply whalesalad 22 hours agoprevA good friend of mine was in QA at Boeing. He turned things away constantly, to a point where he was let go for being too much of a pain in the ass. They’ve been a dumpster fire for a long time. reply jacquesm 20 hours agoparentWow. Getting fired for actually doing your job. That's a great case of 'shooting the messenger'. reply dboreham 19 hours agorootparentIn theory this is not supposed to happen because the QA organization reports directly to the top of the org chart. This structure was developed after some high profile quality failure episodes in the 1960s (Minuteman?). reply 0x457 8 hours agorootparentprevNeed further information. I've met QAs in software that raise the dumbest \"issues\" I've ever seen. reply jacquesm 4 hours agorootparentThis doesn't look like a 'dumb' issue to me, no matter who you've met. reply MilStdJunkie 12 hours agoparentprevYeah, calling attention to escapes is a good way to be on the short list for the next round of layoffs. So is having a LTD, or being old, or taking maternity/paternity, or taking more than a week of PTO at a time . . but none as sure-fire as flagging nonsense in parts lists. reply chmod600 12 hours agoprevMaybe there should be a rule along the lines of \"if there are bolts laying around someplace, and there is no record for them, start re-inspecting until you're damn sure where they came from\". reply graton 12 hours agoprevAs I suspected the four bolts were not installed at the time of the Alaska Airlines incident, based on this article. From what I had read and watched it seemed impossible for the door to slide upwards if any of the four bolts were installed. And if installed correctly with a castellated nut and cotter pin, seems highly unlikely the bolt could fall out, let alone four bolts. The NTSB stated that the door did slide upward and also stated they could not find the bolts and were unsure if the bolts were there at all. reply monkeynotes 19 hours agoprevI don't understand why the plug doesn't use pressure as a failsafe. You have high pressure one side, low the other side, and just like normal airline doors the interior pressure makes it impossible to open the door. reply mips_r4300i 17 hours agoparentThat is exactly how it works actually. There are a series of ears around the door, and the pressurization of the cabin forces the door out and against the airplane frame mating ears. The more it pressurizes, the greater the force jamming the door shut. However, since it's a door, it has to be able to open. The door is constructed such that if you slide it upwards, the ears clear each other and the door can come out. There are bolts to physically block the door plug from ever sliding upwards. If the bolts were correctly installed, or ever installed at all, the door would be perfectly safe. reply monkeynotes 16 hours agorootparentI understand the vulnerability of the slide up thing, but I guess what I didn't explain properly is why doesn't it open by swinging inside instead of upwards? I don't know if I've ever seen a passenger door that slides up, but I haven't flown on all aircraft types. reply cameldrv 15 hours agorootparentAdding a hinge there would be extra weight, but probably more importantly, the plug door is convertible to an emergency exit, so you want the hardware to be similar to the emergency exit door. For those, the door comes completely off and you throw it out of the plane when you use it. If the door hinged inward, it would take up a lot of space in the cabin during an evacuation and slow down people trying to get out. Ultimately it's extremely difficult to make a plane where you can just not install a bunch of bolts and everything still keeps working fine. You have to have a high quality manufacturing operation to make a reliable plane. reply hedgehog 12 hours agorootparentprevEmergency exit doors should open outward, fall outward, etc, otherwise people rushing to get out could pile up and get stuck. True in buildings as well as vehicles. Some Airbus models have a similar upward hinge on at least some exits, I think on many the door fully detaches and you're supposed to chuck it out of the way. reply g-b-r 18 hours agoparentprevYou'd probably still need bolt somewhere... (rivets actually?) reply rurban 12 hours agoprevOriginal source: https://leehamnews.com/2024/01/15/unplanned-removal-installa... reply yread 12 hours agoprevIf fdr and even tdracer at pprune https://www.pprune.org/11581522-post1250.html think it's plausible that doesn't sound good for the future production rate of 737 MAXs reply ChrisMarshallNY 12 hours agoprevAs it so happens, Wiley's Non Sequitur for today, dovetails nicely: https://www.gocomics.com/nonsequitur/2024/01/23 reply bluelightning2k 19 hours agoprevAirplane QA should be done by governments after the manufacturer's own QA. Significant findings at this stage should cause HEAVY fines. These fines should pay for the additional QA. This would not even increase the overall cost - as the minor increase in COGS is offset by 1 accident prevented per decade. reply thfuran 18 hours agoparentI'm not sure that approach to regulation would entail a minor increase in costs. Thorough design review and testing of a complex system is far from cheap and would require a lot of in house expertise and facilities. It would massively increase the scope of agencies like the FDA and FAA. reply poncho_romero 16 hours agorootparentWhy shouldn’t regulatory agencies have expertise in the industries they are intended to regulate? Government by corporation or consultant is not democratic. Without internal knowledge, government will always play the fool because it won’t know any better. reply g-b-r 18 hours agorootparentprevDidn't they use to do it? reply g-b-r 18 hours agoparentprevIt would require the manufacturer not owning the government... reply umanwizard 19 hours agoparentprev> the minor increase in COGS is offset by 1 accident prevented per decade. How do you know? What’s the math on this? reply andyish 18 hours agoprevI wonder what's happening at Airbus right now? If they're doubling down and empowering on their engineering divisions or if they too focused on cost cutting from engineering and QA and thanking their lucky stars it's not them. reply intunderflow 18 hours agoparentThey're popping the champagne at becoming the world's undisputed number one aircraft supplier. Boeing has crippled themselves in this two-horse race, and both airlines and consumers don't want to fly Boeing aircraft over Airbus. (Airlines because groundings are extraordinarily expensive, Consumers because they care about their safety). The only saving grace is that the A320 order book is backed up until almost 2030, but I would not be surprised if Airbus try to permanently ramp up production now. reply martythemaniak 18 hours agoparentprevThe best thing they could possibly do is just keep their heads down and crank out planes and they'll crush Boeing in all categories smaller than the 787. The 220, the 321 neo, etc are great, just keep on making them. reply ahmedfromtunis 19 hours agoprevMinor, minor point: why would an employee of an american company write the dates using the DD/MM/YYYY format instead of the american MM/DD/YYYY? Again, I know this is a very minor point, put it was distracting. reply ceh123 19 hours agoparentImportant correction, it’s not DD/MM/YYYY, but DD Month YYYY. (At least what I saw in the article) This format is common in heavily regulated industries and frequently a regulatory requirement since it’s fully unambiguous. I (American) worked in clinical research/pharma for a bit and still write my dates like 23Jan2024. reply daveslash 14 hours agorootparentI like dd MMM YYYY just fine, especially in documentation. But truthfully, I much prefer YYYY-MM-dd. I know that by using dd MMM YYYY you avoid the ambiguity of dd/MM vs MM/dd, and avoiding the ambiguity is important, but I think most folks know that when the year comes first, then what follows is the month. If someone sees 2024-03-05, I don't think most people wonder Is that March 5th, of May 3rd of 2024? And I like YYYY-MM-dd because it's (a) In alignment with ISO 8601 and (b) sorts so much easier as plaintext. In documentation, I'll use dd MMM YYYY for signatures and document dates, but for things like big tabular data in the body of a document or in spreadsheets, or logging, or CSV exports, I'll default to YYYY-MM-dd reply kube-system 18 hours agorootparentprev> DD Month YYYY Is also the standard international format for aeronautical publications. reply p_l 18 hours agoparentprevAviation, with the annoying exception of feet as unit, largely eradicates such problematic \"local standards\" from professional usage. At some point you find yourself simply using the \"standard\" ways even when you don't have to. Yours sincerely, guilty of using ICAO phonetic alphabet with poor random office workers over the phone. reply marcosdumay 9 hours agorootparentPeople just find the phonetic alphabet weird, but everybody gets it right every time. It's this good. Nobody suffers any setback from hearing it. reply p_l 1 hour agorootparentSometimes they are really confused. And some idioms involved in it don't translate easily if you're not conversing in English, for example X-Ray or Yankee. Still most of it works out so less confusing than if I tried to jury rig something (traditional polish way was to use First Names that use given letter as the first letter, but that doesn't cover X or Y well, either) reply xxpor 8 hours agorootparentprevdon't forget pressure reply p_l 1 hour agorootparentmmHg is, due to historical reasons, pretty well distributed, and hPa is also rather widely used. inHg appears to have way less penetration than feets for altitude or feet/minute for vertical speed (way inferior to meter/second IMHO). reply rightbyte 18 hours agoparentprevNot using a mixed endian date encoding when noone forces you to, is suspicious? Like the scene in that Tarantino movie with nazis, where the British spy give himself away by ordering three whiskey with the wrong fingers ... reply Traubenfuchs 15 hours agorootparenthttps://www.youtube.com/watch?v=r-lQWk79VAE&t=92s reply kube-system 18 hours agoparentprevUS customary units and formats are often used in US homes, but other units and formats are frequently used, particularly in industries that do business internationally. Even the US military actually uses DD/month/YY reply bitcharmer 19 hours agoparentprevMaybe because engineers don't like nonsense standards? reply newsclues 21 hours agoprevIt’s is called Boeing but this failure is really McDonnell Douglas. Being back engineering leadership supremacy. reply HKH2 21 hours agoparentMeritocracy? reply newsclues 20 hours agorootparentDifferent groups define the goals different. Sales/financial: make as much money as possible Engineering: make the best product possible Old Boeing was successful with the later group in charge, new Boeing (post MD merger) is the former. reply bertil 21 hours agorootparentprevMerit makes sense when you want to promote hard work, talent, and creativity—and we need all that. Engineering meritocracy is how we build bridges, go to space, and dig under mountains. Engineering supremacy in that context means that if engineering says no, it’s definite. What Boeing needs is QA supremacy: nobody wants them to be particularly creative, original, or unusually hard-working when doing audits; they need to be thorough and systematic. We need to make sure that their voice isn’t challenged. reply HKH2 20 hours agorootparentMaybe your view of merit is too narrow, or mine is too broad. QA supremacy makes sense, but it relies on the merits of QA (that QA are competing to be the best) to avoid false positives/negatives rather than rubber stamping, no? reply orwin 21 hours agorootparentprevI'm pushing back a bit on that? QA needs to be challenged, like any part of a company. They also need to have the last word on any decision on stuff that has a potential to ruin lives. reply bertil 19 hours agorootparentThat’s part of them being thorough. reply tiahura 19 hours agoparentprevWhat are you talking about? The last CEO was an engineer. MCAS was designed by engineers. The clearly over complicated door plug system was designed by engineers. It looks like the engineers are the problem and the MBAs get to clean it up. reply toast0 17 hours agorootparent> The clearly over complicated door plug system was designed by engineers. It may be a little early for armchair engineering; we should probably wait for a failure analysis report. But the door plug doesn't seem overly complicated. Put the plug in, bolt it on, tada. Where's the complication? The bolts are supposed to have safety wire to keep them from loosening by vibration. reply g-b-r 18 hours agorootparentprev[1] might explain it, if it were so. But the fault would be of the MBAs... Anyhow it was a management decision to not tell about the MCAS changes (not to mention what led to requiring it), management kind of does affect the engineering choices, and is it really over complicated to require 4 rivets? https://www.airliners.net/forum/viewtopic.php?t=213075 reply tiahura 17 hours agorootparentThe central thesis from your link: American engineers and technical designers are being laid off by the hundreds while Russian engineers are quietly hired at the Boeing Design Center in Moscow. There's a reason people don't let the inmates run the asylum. reply g-b-r 17 hours agorootparentIf they consider engineers inmates of an asylum, the reason is they're (bad, or with second aims) managers reply delfinom 18 hours agorootparentprevMCAS was designed by engineers in response to a design problem. The MBAs elected to cover it up from the manual because that would hurt their sales goals of not needing pilot training. Yes, sales/marketing and management leeches have a lot of control over documentation that leaves the company. The best part is, it was the leeches that decided to make a fucking indicator light that told you the sensors for MCAS were unhappy, a PAID FUCKING ADDON. The last CEO was an engineer who got thrown under the bus for the 737 crashes while not being the CEO when it was designed. He was pissing too many leeches off internally so they threw him under the bus. I know two long time employees of Boeing that left after the last CEO was removed because roughly paraphrasing \"Wow, this company is fucked, he was actually trying to fix the company\" reply gmokki 13 hours agorootparentYes. And the design problems was created by MBAs that decided that they want: 1) to keep the new engines that change how the plane flies 2) a convincing lie that passes minimal scrutiny that the plane flies the same - to avoid training pilots to a perfectly good new plane 3) after engineering came with the MCAS solution MBAs drove the cost down and removed redundancy - after all the whole feature was a secret so no one would demand redundancy reply bigbillheck 18 hours agoprevA lot of people here are blaming 'management culture' and saying 'this wouldn't have happened if Boeing was still ran by engineers' and 'it's the merger that killed them' but Boeing has had a long history of quality control issues, see forex https://www.washingtonpost.com/archive/politics/1989/02/19/b... https://www.latimes.com/archives/la-xpm-1989-02-26-mn-824-st... reply rcbdev 19 hours agoprev> \"When people say I changed the culture of Boeing, that was the intent, so that it’s run like a business rather than a great engineering firm.\" - CEO Harry Stonecipher Amazing how that one worked out. reply throw0101d 18 hours agoparentFrom a 2004 profile: > \"When people say I changed the culture of Boeing, that was the intent, so that it's run like a business rather than a great engineering firm,\" he said. \"It is a great engineering firm, but people invest in a company because they want to make money.\" * https://www.chicagotribune.com/chi-0402290256feb29-story.htm... The Boeing headquarters was moved from Seattle to Chicago around 2001: * https://hbr.org/2001/10/inside-boeings-big-move > In a 2019 article, Jerry Useem criticized Boeing's move to Chicago, suggesting that by \"isolating\" the Boeing management from its engineering and manufacturing staff, the company discounted its former engineering-led corporate culture in favor of a management style run by MBAs instead of engineers.[7] * https://en.wikipedia.org/wiki/Boeing_Building#Criticism In 2022 it moved again, from Chicago to the DC-area (Virginia): * https://www.cnbc.com/2022/05/05/boeing-to-move-headquarters-... Both moves seem quite symbolic to me. reply theyinwhy 19 hours agoparentprevLove the missing \"great\" in front of \"business\". reply samgranieri 18 hours agoparentprevMcDonnell Douglas bought Boeing with Boeing's money and IMHO ruined it. If it were up to me I'd just put the engineers that made amazing flying machines back in charge and phase out McDonnell Douglas's culture. reply dopidopHN 18 hours agorootparent( naïve comment ) Is that possible ? Tribal knowledge is probably lost, and those great employees have probably moved on or retired. reply _fat_santa 18 hours agoparentprevI hope Boeing's stock tanking will be a strong indicator to the bean counters at Boeing and other engineering firms that you engineering chops do bring value to the business. reply cyanydeez 18 hours agorootparentwe're watching stock prices soaring after employee cuts. So, you know, expect cuts to resolve that one. reply kingTug 17 hours agorootparentThe stock market is so fucking fake. What a dumb system we have created. reply epups 17 hours agoparentprevIt was a great engineering firm, and he turned it into a bad engineering firm. reply 169 more comments... GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The detachment of a mid-exit door plug on an Alaska Airlines Boeing 737 MAX 9 was caused by the failure to install four bolts during production, according to a Boeing employee.",
      "There have been a high number of nonconforming findings on 737 mid fuselage door installations in the past year, potentially indicating production issues.",
      "The problems may be linked to outsourced plane components that have defects upon arrival at the production line, suggesting a potential quality control and culture issue within certain parts of the business. The extent of the problem and whether it is isolated or systemic remains unclear."
    ],
    "commentSummary": [
      "Discussions surrounding concerns with Boeing's manufacturing practices, engineering culture, and decisions leading to safety issues with their planes.",
      "Criticism of management, outsourcing practices, and prioritization of profit over safety.",
      "Whistleblower credibility, protection, and accountability, as well as discussions on regulatory agencies, sales/marketing influence, and the need for change in project management practices."
    ],
    "points": 535,
    "commentCount": 419,
    "retryCount": 0,
    "time": 1706009334
  },
  {
    "id": 39101828,
    "title": "Templating YAML vs Generating JSON: A Better Approach for Config Management",
    "originLink": "https://leebriggs.co.uk/blog/2019/02/07/why-are-we-templating-yaml",
    "originBody": "lbr. home projects blog contact #kubernetes #configuration mgmt #jsonnet #helm #kr8 Why the fuck are we templating yaml? Published Feb 7, 2019 by Lee Briggs I was at cfgmgmtcamp 2019 in Ghent, and did a talk which I think was well received about the need for some Kubernetes configuration management as well as the solution we built for it at $work, kr8. I made a statement during the talk which ignited some fairly fierce discussion both online, and at the conference: \"If you're starting to template yaml, ask yourself the question: why am I not *generating* json?\" - @briggsl spitting straight fire at #cfgmgmtcamp — 🌈eric sorenson 🌊 (@ahpook) February 5, 2019 To put this into my own words: At some point, we decided it was okay for us to template yaml. When did this happen? How is this acceptable? After some conversation, I figured it was probably best to back up my claims in some way. This blog post is going to try to do that. The configuration problem Once the applications and infrastructure you’re going to manage grows past a certain size, you inevitably end up in some form of configuration complexity hell. If you’re only deploying 1 or maybe 2 things, you can write a yaml configuration file and be done with it. However once you grow beyond that, you need to figure out how to manage this complexity. It’s incredibly likely that the reason you have multiple configuration files is because the $thing that uses that config is slightly different from its companions. Examples of this include: Applications deployed in different environments, like dev, stg and prod Applications deployed in different regions, like Europe or North American Obviously, not all the configuration is different here, but it’s likely the configuration differs enough that you want to be able to differentiate between the two. This configuration complexity has been well known for Operators (System Administrators, DevOps engineers, whatever you want to call them) for some years now. An entire discpline grew up around this in Configuration Management, and each tool solved this problem in their own way, but ultimately, they used YAML to get the job done. My favourite method has always been hiera which comes bundled with Puppet. Having the ability to hierarchically look up the variables of specific config needs is incredibly powerful and flexible, and has generally meant you don’t actually need to do any templating of yaml at all, except perhaps for embedding Puppet facts into the yaml. Did we go backwards? Then, as our industries’ needs moved above the operating system and into cloud computing, we had a whole new data plane to configure. The tooling to configure this changed, and tools like CloudFormation and Helm appeared. These tools are excellent configuration tools, but I firmly believe we (as an industry) got something really, really wrong when we designed them. To examine that, let’s take a look at example of a helm chart taking a custom parameter Helm Charts Helm charts can take external parameters defined by an values.yaml file which you specify when rendering the chart. A simple example might look like this: Let’s say my external parameter is simple - it’s a string. It’d look a bit like this: image: \"{{ .Values.image }}\" That’s not so bad right? You just specify a value for image in your values.yaml and you’re on your way. The real problem starts to get highlighted when you want to do more complicated and complex things. In this particular example, you’re doing okay because you know you have to specify an image for a Kubernetes deployment. However, what if you’re working with something like an optional field? Well, then it gets a little more unwieldy: {{- with .resourceGroup }} resourceGroup: {{ . }} {{- end }} Optional values just make things ugly in templating languages, and you can’t just leave the value blank, so you have to resort to ugly loops and conditionals that are probably going to bite you later. Let’s say you need to go a step further, and you need to push an array or map into the config. With helm, you’d do something like this. {{- with .Values.podAnnotations }} annotations: {{ toYaml .indent 8 }} {{- end }} Firstly, let’s ignore the madness of having a templating function toYaml to convert yaml to yaml and focus more on the whitespace issue here. YAML has strict requirements and whitespace implementation rules. The following, for example, is not valid or complete yaml: something: nothing hello: goodbye Generally, if you’re handwriting something, this isn’t necessarily a problem because you just hit backspace twice and it’s fixed. However, if you’re generating YAML using a templating system, you can’t do that - and if you’re operating above 5 or 10 configuration files, you probably want to be generating your config rather than writing it. So, in the above example, you want to embed the values of .Values.podAnnotations under the annotations field, which is indented already. So you’re having to not only indent your values, but indent them correctly. What makes this even more confusing is that the go parser doesn’t actually know anything about YAML at all, so if you try to keep the syntax clean and indent the templates like this: {{- with .Values.podAnnotations }} annotations: {{ toYaml .indent 6 }} {{- end }} You actually can’t do that, because the templating system gets confused. This is a singular example of the complexity and difficulty you end up facing when generating config data in YAML, but when you really start to do more complex work, it really starts to become obvious that this isn’t the way to go. Needless to say, this isn’t what I want to spend my time doing. If fiddling around with whitespace requirements in a templating system doing something it’s not really designed for is what suits you, then I’m not going to stop you. I also don’t want to spend my time writing configuration in JSON without comments and accidentally missing commas all over the shop. We (as an industry) decided a long time ago that shit wasn’t going to work and that’s why YAML exists. So what should we do instead? That’s where jsonnet comes in. JSON, Jsonnet & YAML Before we actually talk about Jsonnet, it’s worth reminding people of a very important (but oft forgotten point). YAML is a superset of JSON and converting between the two is trivial. Many applications and programming languages will parse JSON and YAML natively, and many can convert between the two very simple. For example, in Python: python -c 'import json, sys, yaml ; y=yaml.safe_load(sys.stdin.read()) ; print(json.dumps(y))' So with that in mind, let’s talk about Jsonnet. Welcome to the church of Jsonnet Jsonnet is a relatively new, little known (outside the Kubernetes community?) language that calls itself a data templating language. It’s definitely a good exercise to read and consume the Jsonnet design rationale page to get an idea why it exists, but if I was going to define in a nutshell what its purpose is - it’s to generate JSON config. So, how does it help, exactly? Well, let’s take our earlier example - we want to generate some JSON config specifying a parameter (ie, the image string). We can do that very very easily with Jsonnet using external variables. Firstly, let’s define some Jsonnet: { image: std.extVar('image'), } Then, we can generate it using the Jsonnet command line tool, passing in the external variable as we need to: jsonnet image.jsonnet -V image=\"my-image\" { \"image\": \"my-image\" } Easy! Optional fields Before, I noted that if you wanted to define an optional field, with YAML templating you had to define if statements for everything. With Jsonnet, you’re just defining code! // define a variable - yes, jsonnet also has comments local rg = null; { image: std.extVar('image'), // if the variable is null, this will be blank [if rg != null then 'resourceGroup']: rg, } The output here, because our variable is null, means that we never actually populate resourceGroup. If you specify a value, it will appear: jsonnet image.jsonnet -V image=\"my-image\" { \"image\": \"my-image\" } Maps and parameters Okay, now let’s look at our previous annotation example. We want to define some pod annotations, which takes a YAML map as its input. You want this map to be configurable by specifying external data, and obviously doing that on the command line sucks (you’d be very unlikely to specify this with Helm on the command line, for example) so generally you’d use Jsonnet imports to this. I’m going to specify this config as a variable and then load that variable into the annotation: local annotations = { 'nginx.ingress.kubernetes.io/app-root': '/', 'nginx.ingress.kubernetes.io/enable-cors': true, }; { metadata: { // annotations are nested under the metadata of a pod annotations: annotations, }, } This might just be my bias towards Jsonnet talking, but this is so dramatically easier than faffing about with indentation that I can’t even begin to describe it. Additional goodies The final thing I wanted to quickly explore, which is something that I feel can’t really be done with Helm and other yaml templating tools, is the concept of manipulating existing objects in config. Let’s take our example above with the annotations, and look at the result file: { \"metadata\": { \"annotations\": { \"nginx.ingress.kubernetes.io/app-root\": \"/\", \"nginx.ingress.kubernetes.io/enable-cors\": true } } } Now, let’s say for example I wanted to append a set of annotations to this annotations map. In any templating system, I’d probably have to rewrite the whole map. Jsonnet makes this trivial. I can simply use the + operator to add something to this. Here’s a (poor) example: local annotations = { 'nginx.ingress.kubernetes.io/app-root': '/', 'nginx.ingress.kubernetes.io/enable-cors': true, }; { metadata: { annotations: annotations, }, } + { // this adds another JSON object metadata+: { // I'm using the + operator, so we'll append to the existing metadata annotations+: { // same as above something: 'nothing', }, }, } The end result is this: { \"metadata\": { \"annotations\": { \"nginx.ingress.kubernetes.io/app-root\": \"/\", \"nginx.ingress.kubernetes.io/enable-cors\": true, \"something\": \"nothing\" } } } Obviously, in this case, it’s more code to this, but as your example get more complex, it becomes extremely useful to be able to manipulate objects this way. Kr8 We use all of these methods in kr8 to make creating and manipulating configuration for multiple Kubernetes clusters easy and simple. I highly recommend you check it out if any of the concepts you’ve found here have found you nodding your head. ***** © 2021, Ritij JainPudhina Fresh theme for Jekyll.",
    "commentLink": "https://news.ycombinator.com/item?id=39101828",
    "commentBody": "Why are we templating YAML? (2019) (leebriggs.co.uk)417 points by olestr 23 hours agohidepastfavorite594 comments bilalq 17 hours agoI'm completely done with configs written in YAML. Easily the worst part of Github Actions, even worse than the reliability. When I see some cool tool require a YAML file for config, I immediately get hit with a wave of apprehension. These same feelings extend to other proprietary config languages like HCL for Terraform, ASL for AWS Step Functions, etc. It's fine that you want a declarative API, but let me generate my declaration programatically. Config declared in and generated by code has been a superior experience. It's one of the things that AWS CDK got absolutely right. My config and declarative definition of my cloud infra is all written in a typesafe language with great IDE support without the need for random plugins that some rando wrote and never updated since 2 years ago. reply jrockway 16 hours agoparentAt this point, I even prefer plain JSON to YAML. What pushed me over the edge is that \"deno fmt\" comes with a JSON formatter, but not a YAML formatter. It's a single binary that runs in milliseconds. For YAML auto-formatting you basically have to use Prettier, and Prettier depends on half of NPM and takes a good 2 seconds to startup and run. So, I literally moved every YAML file in our repository at work that could be JSON to JSON and I think everyone has been much happier. Or, at least I have been, and nobody has complained to me about it. Various editors also support a $schema tag in the JSON. I added this feature to our product (which has a flow that invokes your editor on a JSON file), and it works great. You can just press tab and make a config file without reading the docs. Truly wonderful. YAML has this too with the YAML language server, but you need your tab key to indent stuff, so the ergonomics are pretty un-fun. JSON isn't perfect, but at least the text \"no\" is true. reply matsemann 15 hours agorootparentAt work we're currently expanding to another country. Which means that many services now need a country label etc., which is fun when you're adding \"no\" to all our existing services. Luckily it's quick to catch, but man... why? reply throwaway894345 15 hours agorootparentYeah, I'm pretty sure there are exactly two substantive problems with JSON for (static) configuration file use cases, which are comments and multiline strings (especially with sane handling of indentation). YAML fixes these, but it adds so much complexity in the process including such a predictable footgun of unquoted strings (the no/false problem is particularly glaring/absurd, but it's also easy to forget to quote other boolean values or numbers in a long list of other strings). reply MathMonkeyMan 10 hours agorootparentSolutions abound, but one option is to use either Javascript (config.js): // comments! ({ no_quotes: [1, 2, (() => /* code! */)()], ... }) Or, let the whole thing be a function. Then your config can have parameters, maybe mapped from environment variables or something. ({foo, bar, ...kwargs}) => ({ datacenter: foo === 'old' ? 'useast1' : 'uswest', ... }) Can do as Crockford says, and write in the JSON subset of Javascript, but with comments, and convert it to JSON by running it through a JS minifier. I think you need parens around the object, though, or else it looks like a code block (boooo...): ({ // This is very much like JSON \"foo\": [1, 2, \"bar\"] }) Python also has JSON-like syntax, so you could use config.py: { 'foo': [1, 2, 'bar'] } That would require a wrapper script. Or, you can have the self-contained convention: import json import sys # Yay, comments! json.dump({ # more comments! 'foo': [1, 2, 'bar'] }, sys.stdout) reply johnmaguire 12 hours agorootparentprevCan I add \"trailing commas are invalid\" to the list? reply throwaway894345 11 hours agorootparentPlease do. reply bobbylarrybobby 7 hours agorootparentprevjson5 is pretty good, if you can use it reply bilalq 14 hours agorootparentprevI prefer JSON to YAML as well. The lack of comments is a problem though. But I feel like this is a false dichotomy. Both kind of suck for this need, but I can accept that JSON is at least reasonable to work with if you need language agnostic config. reply hintymad 15 hours agoparentprevAn often-heard benefit for using YAML is that JSON does not have comment. What I don't understand is why we would switch to a whole new language. Just add a filter before loading the configuration, which can't be harder than switching to YAML, right? Another reason for YAML is that it is easier to read. That I don't understand either. The endless pain of dealing with configuration does seem come from saving a few seconds of parsing off braces and brackets, but from not being about easily figure out what goes wrong, especially when what's wrong is a missing space or tab embedded in hundreds of lines of configurations. reply m463 10 hours agorootparentI like json a lot. That said, I think json would benefit from only two things: 1) comments 2) allow extra commas, like [\"a\", \"b\", \"c\",] or {\"a\":\"b\", \"c\":\"d\", } or more properly: { \"a\":\"b\", \"c\":\"d\", }, EDIT: and json5 does both, plus a few more niceties. (hmm. too much?) reply int_19h 13 hours agorootparentprevhttps://json5.org/ reply p10_user 14 hours agorootparentprevJust make another named list key called \"comment\". Problem solved. reply int_19h 13 hours agorootparentThis is not always an option when JSON is propagated as is, nor does it allow for comments on specific object properties. reply mekster 6 hours agorootparentprevNot sure why people just don't settle with TOML. reply weebull 16 hours agoparentprevGitHub actions would suck whatever you \"configured\" them in, because you are trying to describe a program in a data structure. Ansible makes the same mistake, as do countless other tools. reply tomjakubowski 16 hours agorootparent\"because you are trying to describe a program in a data structure\" (cries in lisp) reply kazinator 16 hours agorootparentThe best interpretation of weebull's comment is not that describing a program in a data structure is \"bad\" per se, but that doing that in a configuration language (or requiring configuration constructs to be programming constructs) might not be a hot idea. Even Lisp software that uses Lisp for configuration does not necessarily allow programming in that configuration notation. reply jrockway 16 hours agorootparentprevYeah, I think describing a program in a data structure is fine. I honestly prefer it to any syntax that a \"real\" programming language has brought me. It's so consistent and you can really focus on what you care about. What is unhappy about Github Actions and similar is that your programming language has like 2 keywords; \"download a container\" and \"run a shell script\". I would have preferred starting with \"func\", \"handle this error\", and \"retry this operation if the error is type Foo\" ;) Since this article is about helm, I'll point out that Go templates are very lispy. I often have things in them that look like {{ and (foo bar) (bar baz) }} and it only gets crazier as you add more parentheses ;) reply throwaway894345 15 hours agorootparentThe problem I have with GitHub Actions is that I usually want to metaprogram them. I have a monorepo and I want a particular action to run for each \"project\" subdirectory. I've written a program that generates GitHub Actions YAML files, but all of the ways to make sure the generator was run before each commit are fairly unsatisfying. The problem I have with infra-as-code tools is that what I really want is a pretty simple representation for \"the state of the world\" that some reconciliation can use, and then I want to generate that stuff in a typesafe, expression-based language like TypeScript or Python (Dhall exists, but its Haskell-like syntax and conventions are too steep a learning curve to get mainstream adoption). Instead we get CloudFormation and Terraform which shoehorn programming language constructs into a configuration language (which isn't strictly an objection to code-as-data generally) or things like Helm which uses text templates to generate a \"state of the world\" description or these CDKs which all seem to depend on a full JavaScript engine for reasons that don't make sense to me (why do I need JavaScript to generate configuration?). reply btown 16 hours agorootparentprevI often wonder if the only reason we haven't used lisp more as a society, and certainly in the devops world, is because our brains find it easier to parse nested indentation than nested parentheses. But in doing so, we've thrown out the other important part of lisp, which is that you can use the same syntax for data that you do for control flow. And so we're stuck in this world where a \"modern-looking\" program is seen as a thing that must be evaluated to make sense, not a data structure in and of itself. https://www.reddit.com/r/lisp/comments/1pyg07/why_not_use_in... is a fascinating 10 year old discussion. And of course, there's Smalltalk, which guided others to a treasure it could not possess. But most younger programmers have never even had these conversations. reply kazinator 16 hours agorootparentThe vast majority of Lisp code is assiduously written with nested indentation! So that can't be it. Non-lisp languages have parentheses, brackets and braces, using indentation to clarify the structure. Nobody can reasonably work with minified Javascript, without reformatting it first to span multiple lines, with indentation. Lisp has great support for indentation; reformatting Lisp nicely, though not entirely trivial, is easier than other languages. Oh, have you seen parinfer? It's an editing mode that infers indentation from nesting, and nesting from indentation (both directions) in real-time. It also infers closing parentheses. You can just delete lines and it reshuffles the closers. The github.io site has animations: https://shaunlebron.github.io/parinfer/ reply lolinder 16 hours agorootparentprev> you are trying to describe a program in a data structure This describes 100% of software development, though! Programming is just designing data structures that represent some computation. Each language lends itself better to some computations than to others (and some, like YAML, are terrible for describing any kind of computation at all), but they're all just data structures describing programs. The problem isn't that GitHub Actions tries to describe a program in a data structure, the problem is that the language that they chose to represent those programs (YAML and the meta language on top) is ill-suited to the task. reply latchkey 16 hours agorootparentprev> Ansible makes the same mistake, as do countless other tools. My favorite example of this is chown/chmod taking 4-5 lines, in yaml. Sure you can do it a bunch of different ways, sure it allows for repeatable commands. But, it just sucks. reply hintymad 12 hours agorootparentprevThe same reason I don't like AWS' Step Functions. The spec in JSON is horrible. On the other hand, Step Functions is pretty scalable and reliable and can take practically unlimited throughput. It's a good story for how a product can succeed by getting the primitives right and by removing just the key obstacle for users. Now that Step Functions has gained momentum, they can construct higher-level APIs and SDKs to translate user spec to the low-level JSON/YAML payload. reply Footkerchief 15 hours agoparentprevIn the case of GitHub Actions, it's made more painful by the lack of support for YAML anchors, which would provide a bare minimum of composability. https://github.com/actions/runner/issues/1182 reply SergeAx 23 minutes agoparentprev> Config declared in and generated by code has been a superior experience. And here we are at the point in time when people are plainly forgotten about compiled programming languages. reply didip 11 hours agoparentprevFor real, I want a real language (Lua/JS/Lisp) for configuration but without 3rd party imports so that it's secure and predictable. reply treflop 16 hours agoparentprevIf configs had well-adopted schema support, it wouldn't be so bad. reply bilalq 14 hours agorootparentEven then, it gets messy. From a tooling standpoint, how will I load your schema? How will my editor respect it? How do I run a validator against it? I know XML kind of solves some of these problems, but it has its own thorns and despite what anyone says, it is not easy to work with. XSD, XSLT, etc. So much complexity that needs to be managed in a different way in every runtime. And then type safety goes out at the boundary where it connects to your code. reply treflop 14 hours agorootparentThat's how it used to be for your suggestion too. We're living in a dream state now where the creators of IDEs like Visual Studio (Code) or IntelliJ actively implement common languages and frameworks. It used to be 'find a half-baked community plugin so JSON works.' If someone made a standard schema and people used it, I can assure you the magic you are expecting from your tooling would suddenly pop in just like how JSON support appeared one day. But they can't do nothin' if there is no community support for it. XSD and XSLT are complicated because XML is complicated. reply throwaway894345 15 hours agoparentprev> These same feelings extend to other proprietary config languages like HCL for Terraform, ASL for AWS Step Functions, etc. It's fine that you want a declarative API, but let me generate my declaration programatically. Yeah, I've had the same sort of opinion since the bad old AWS CloudFormation days. I wrote an experimental CloudFormation generator 4 years ago where all of the resources and Python type hints were generated from a JSON file that AWS published and it worked really well (https://github.com/weberc2/nimbus/blob/master/examples/src/n...). > Config declared in and generated by code has been a superior experience. It's one of the things that AWS CDK got absolutely right. Is that how CDK works? I've only dabbled with it, but it was pretty far from the \"generate cloudformation\" experience that I had built; I guess I never \"saw the light\" for CDK. It felt like trading YAML/templating problems for inheritance/magic problems. I'd really like to hear from more people who have used AWS CDK, Terraform's CDK, and/or Pulumi. reply bilalq 14 hours agorootparentIt's an annoyingly OOP model with mutations and side-effects, but if you look past that, it's pretty nice. The core idea is you create an instance of a CDK \"App\" object. You create new instances of \"Stack\" objects that take an \"App\" instance as a context parameter. From there, resources are grouped into logical chunks called \"Constructs\" which take either a stack or another construct as their parent context param. The only things you should ever inherit from are the base Constructs for Stack, Stage, and Construct. Don't use inheritance anywhere else and you'll be okay. The code then looks something like this (writing this straight in the comment box, probably has errors): // Entrypoint of CDK project like bin/app.ts or whatever import * as cdk from 'aws-cdk-lib' import { MyStack } from '../lib/my-stack.ts' const app = new cdk.App() const stack = new MyStack(app, 'StackNameHere', someProps) // lib/my-stack.ts // Imports go here export class MyStack extends cdk.Stack { constructor(scope: Construct, id: string, props: MyStackProps) { super(scope, id, props) const bucket = new s3.Bucket(this, 'MyBucket', { bucketName: 'example-bucket', }) const lambda = new NodejsFunction(this, 'MyLambdaFn', { functionName: 'My-Lambda-Fn', entryFile: 'my-handler.ts', memorySize: 1024, runtime: Runtime.NodeJS_20X }) bucket.grantRead(lambda), tracing: Tracing.Active }) } The best part is the way CI/CD is managed. CDK supports self-mutating pipelines where the pipeline itself is a stack in your CDK app. After the pipeline is created, it will update itself as part of the pipeline before promoting other changes to the rest of your environments. The equivalent CloudFormation for the above example would be ridiculously long. And that's putting aside all the complexity it would take for you to add on asset bundling for code deployed to things like Lambda. TL;DR: Infrastructure-as-code-as-code reply throwaway894345 11 hours agorootparent> It's an annoyingly OOP model with mutations and side-effects, but if you look past that, it's pretty nice I think I was getting hung up on the mutations and side-effects of it all. Thanks for putting words to that. I'll have to give it another try sometime. Have you used Terraform's CDK by chance? I assume it's heavily inspired from AWS's CDK, but my company has since moved to GCP/Terraform. reply bilalq 8 hours agorootparentThe mutations and side-effects only last until synthesis. You can imagine a CDK app as a pure function that runs a bunch of mutations on an App object and then serializes the state of that object in the end to static assets that can be deployed. The internals of it all are messy, but at a conceptual level, it's easy to think about. CDKTF is really promising, IMO. When I last looked, it was still pretty new, but it's maturing, I think. One downside compared to regular AWS CDK is that the higher level constructs from the official AWS CDK can't be used in CDKTF. There is an adapter that exists, but it's one more layer between you and knowing what's going on: https://github.com/hashicorp/cdktf-aws-cdk reply Draiken 22 hours agoprevI agree that YAML templating is kind of insane, but I will never understand why we don't stop using fake languages and simply use a real language. If you need complex logic, use a programming language and generate the YAML/JSON/whatever with it. There you go. Fixed it for you. Ruby, Python, or any other language really (I only favor scripting ones because they're generally easier to run), will give you all of that without some weird pseudo-language like Jsonnet or Go templates. Write the freaking code already and you'll get bitten way less by obscure weird issues that these template engines have. Seriously, use any real programing language and it'll be WAY better. reply kimbernator 18 hours agoparentI once took a job that involved managing Ansible playbooks for an absolutely massive number of servers that would run them semi-regularly for things like bootstrapping and patching. I had used Chef before for a similar task, and I loved it because it's just ruby and I could easily define any logic I wanted while using loops and proper variables. I understand that Ansible was designed for non-programmers, but there is no worse hell for someone who is actually familiar with basic programming than being confined to the hyper-verbose nonsense that is Jinja templating of Ansible playbooks when you need to have a lot of conditional tasks and loops. reply jimkoen 16 hours agorootparentI agree. And to make matters worse, the DSL on YAML has grown so large in features, it may as well be a programming language now. reply dabber 11 hours agorootparenthttps://yamlscript.org/ was posted here a while back: https://news.ycombinator.com/item?id=38726370 I thought I remembered more comments on that thread, but I guess nothing more than what's there needs to be said. reply dharmab 12 hours agorootparentprevIt technically is. Long ago as a junior sysadmin I created turing complete nightmares in Jinja. reply aarmenaa 16 hours agorootparentprevChef vs Ansible was the first example that popped into my mind. I had a very love/hate relationship with Chef when I used it, but writing cookbooks was definitely one of the good parts. reply linuxftw 17 hours agorootparentprevAnsible has a great module/plugin system. It's trivial to handle complex tasks or computations in a custom module or action. reply jimkoen 16 hours agorootparentSo why is there this massive ecosystem around not writing modules then? RedHat invented automation controller just so they didn't have to implement proper error handling with Ansible. reply linuxftw 12 hours agorootparentThe 'not writing modules' approach is for people that aren't comfortable writing code. I think most capable users for non-trivial things should write custom modules a lot of the time. reply duped 18 hours agoparentprevI think language embedding is kind of a lost architecture in modern stacks. It used to be if you had a sufficiently complex application you'd code the guts in C/C++/Java/Whatever and then if you needed to script it, you'd embed something like a LISP/Lua/whatever on top. But today, you have plenty of off-the-shelf JSON/TOML/YAML parsers you can just import into your app and a function called readConfig in place of where an embedded interpreter might be more appropriate. It's just easier for developers to add complexity to a config format rather than provide a full language embedding and provide bindings into the application. So people have forgotten how to do it (or even that they can do it - I don't think it occurs to people anymore) reply xwowsersx 20 hours agoparentprevPulumi is enticing because it allows you to write in your preferred language and abandon HCL, but it is strictly worse in my opinion. IaC should be declarative in my opinion. That allows for greater predictability, reproducibility and maintainability. In general, I think wanting to use Python or Ruby or whatever language you're going to use with Pulumi is not a good basis for choosing the tool. There are many graveyards filled with places that tried to start writing logic into their IaC back in the Chef/Puppet era and made a huge mess that was impossible to upgrade or maintain (recall that Chef is more imperative/procedural, whereas in Puppet you describe the desired end state). The Chef/Pulumi approach can work, but it requires one person who is draconian about style and maintenance. Otherwise, it turns into a pile of garbage very quick. Terraform/Puppet's model is a lot more maintainable for longer terms with bigger teams. It's just a better default for discouraging patterns that necessitate an outsized investment to maintain. Yes HCL can be annoying and it feels freeing to use Python/TS/whatever, but pure declarative code prevents a lot of spaghetti. reply flanked-evergl 19 hours agorootparentPulumi is declarative. The procedural code (Python, Go, etc) generates the declaration of the desired state, which Pulumi then effects on the providers. HCL is also not pure declarative code either. It can invoke non-declarative functions and can do loops based on environment variables, so in that sense there is really no difference between Pulumi and Terraform. The only real difference is that HCL is a terrible language compared to say Python. I'm actually fairly sure HCL is Turing complete, it has loops and variables. But even if it is not all the way turing complete it's pretty close. reply xwowsersx 14 hours agorootparentPulumi may be declarative, but you use imperative languages to define your end state. The language you're actually writing your Pulumi in is what's most relevant to the point I'm making about maintainability. HCL isn't turing comlete, but even if it was, the point is that doing the types of things you can do in Python or other \"real\" languages is a major pain in HCL which effectively discourages you from doing that. I'm arguing that is actually a good thing for maintainability. reply lamontcg 16 hours agorootparentprev> recall that Chef is more imperative/procedural, whereas in Puppet you describe the desired end state Chef's resources and resource collection and notifications scheme is entirely declarative. And after watching users beat their heads against Chef for a decade the thing that users really like is using declarative resources that other people wrote. The thing that they hate doing is trying to think declaratively themselves and write their own declarative resources or use the resource collection properly. People really want the glue code that they need to write to be imperative and simple. The biggest issue that Chef had was the \"two-pass parsing\" design (build the entire resource collection, then execute the entire resource collection) along with the way that the resource collection and attributes were two enormous global variables which were mutable across the entire collection of recipe code which was being run, and then the design encouraged you to do that. And recipes were kind of a shit design since they weren't really like procedures or methods in a real programming language, but more like this gigantic concatenated 'main context' script. Local variables didn't bleed through so you got some isolation but attributes and the resource collection flowing through all of them as god-object global variables was horrible. Along with some people getting a bit too clever with Ruby and Chef internals. I had dreams of freezing the entire node attribute tree after attribute file processing before executing resources to force the whole model into something more like a functional programming style of \"here's all your immutable description of your data fed into your functional code of how to configure your system\" but that would have been so much worse than Python 2.7-vs-3.0 and blown up the world. Just looking at imperative-vs-declarative is way too simplistic of an analysis of what went wrong with Chef. reply blandflakes 19 hours agorootparentprevThe fact that HCL has poor/nonexistent multi-language parsing support makes building tooling around terraform really annoying. I shouldn't have to install Python or a Go library to read my HCL. reply i_play_stax 18 hours agorootparentprevThe limitations of HCL are actually a good thing! I have never seen Pulumi or CDKTF stuff work well. At some point are you simply writing a script and abandoning the advantages of a declarative approach reply xwowsersx 14 hours agorootparentRight. That's what I'm arguing. reply jen20 19 hours agorootparentprevThe existence of the YAML language for Pulumi and the CDK for TF both confound this explanation, it’s just not grounded in reality. reply lamontcg 16 hours agoparentprev> I agree that YAML templating is kind of insane, but I will never understand why we don't stop using fake languages and simply use a real language. The problem is language nerds write languages for other language nerds. They all want it to be whatever the current sexiness is in language design and want it to be self-hosting and be able to write fast multithreaded webservers in it and then it becomes conceptually complicated. What we need is like a \"Logo\" for systems engineers / devops which is a simple toy language that can be described entirely in a book the size of the original K&R C book. It probably needs to be dynamically typed, have control structures that you can learn in a weekend, not have any threading or concurrency, not be object oriented or have inheritance and be functional/modular in design. And have a very easy to use FFI model so it can call out to / be called from other languages and frameworks. The problem is that language nerds can't control themselves and would add stuff that would grow the language to be more complex, and then they'd use that in core libraries and style guides so that newbies would have to learn it all. I myself would tend towards adding \"each/map\" kinds of functions on arrays/hashmaps instead of just using for loops and having first class functions and closures, which might be mistakes. There's that immutable FP language for configuration which already exists (i can't google this morning yet) which is exactly the kind of language which will never gain any traction because >95% of the people using templated YAML don't want to learn to program that way. reply trealira 10 hours agorootparentWhat we need is like a \"Logo\" for systems engineers / devops which is a simple toy language that can be described entirely in a book the size of the original K&R C book. It probably needs to be dynamically typed, have control structures that you can learn in a weekend, not have any threading or concurrency, not be object oriented or have inheritance and be functional/modular in design. And have a very easy to use FFI model so it can call out to / be called from other languages and frameworks. I think Scheme would work, as long as you ban all uses of call/cc and user-defined macros. It's simple and dynamically typed, and doesn't have built-in classes or hash maps. Only problem is that it seems like most programmers dislike Lisp syntax, or at least aren't used to it. There's also Awk, although it's oriented towards text, and doesn't have modules (the whole program has to be in one file). It probably wouldn't be that hard to make this language yourself. Read the book Crafting Interpreters, which guides you through making a toy language called Lox. It's close to the toy language you describe. reply kazinator 9 hours agorootparentIf you combine Awk with the C preprocessor, you have a way for an Awk program to load modules, relative to where that file is located. There is such a combination project: cppawk. https://www.kylheku.com/cgit/cppawk/about/ reply trealira 9 hours agorootparentThanks for the link! It seems interesting. reply int_19h 13 hours agorootparentprev> What we need is like a \"Logo\" for systems engineers / devops which is a simple toy language that can be described entirely in a book the size of the original K&R C book. I would argue that Tcl is exactly that. It's hard to make things any simpler than \"everything is a string, and then you get a bunch of commands to treat strings as code or data\". The entire language definition boils down to 12 simple rules (\"dodekalogue\"); everything else is just commands from the standard library. Simple Tcl code looks pretty much exactly like a typical (pre-XML, pre-JSON, pre-YAML) config file, and then you have conditionals, loops, variables etc added seamlessly on top of that, all described in very simple terms. reply hnlmorg 15 hours agorootparentprevThere’s plenty to choose from that support embedding: Python, Perl, Lua. Heck, even EMCAScript (JavaScript, VBA, etc). As another commenter rightfully stated, this used to be the norm. I wouldn’t say LOGO is the right example though. It’s basically a LISP and is tailored for geometry (of course you can do a heck of a lot more with it but its strength is in geometry). reply lamontcg 15 hours agorootparentYou're really missing the point. Logo was super simple and we learned it in elementary school as children, that's all that I'm talking about. And those other languages have accreted way too many features to be simple enough. reply hnlmorg 10 hours agorootparent> You're really missing the point. I got your point. I think it is you who is missing mine: > You're really missing the point. Logo was super simple and we learned it in elementary school as children You wouldn't have learned conditionals and other such things though. That stuff wasn't as easy to learn in LOGO because LOGO is basically a LISP. eg IFELSE :num = 1 [print [Number is 1]] [print [Number is 0]] vs if { $num == 1 } then { print \"number is 1\" } else { print \"number is 0\" } or if num == 1: print \"number is 1\" else: print \"number is 0\" I'm not saying these modern languages don't have their baggage. But LOGO wasn't exactly a walk in the park for anything outside of it's main domain either. Your memory of LOGO here is rose tinted. > And those other languages have accreted way too many features to be simple enough. I agree (though less so with Lua) but you don't need to use those features. Sure, my preference would be \"less is more\" and thus my personal opinion of modern Python isn't particularly high. And Perl is rather old fashioned these days (though I think modern Perl gets more criticism than it deserves). But the fact is we don't need to reinvent the wheel here. Visual Basic could make raw DLL calls meaning you had unfettered access to Win32 APIs (et al) but that doesn't mean every VBScript out there was making DLL calls left right and centre. Heck, if you really want to distil things down then there's nothing even stopping someone implementing a \"PythonScript\" type language which is a subset of Python. I just don't buy \"simplicity of the language\" as the reason languages aren't often embedded these days. I think it's the opposite problem: \"simplicity of the implementation\". It's far easier to load a JSON or YAML document into a C(++|#|Objective|whatever) struct than it is it to add API hooks for an embedded scripting language. And that's precisely why software written in dynamic languages do often expose their language runtime for configuration. Eg Ruby in Puppet and Chef, half of PHP applications having config written in PHP, XMPP servers written in Haskell, etc. In those kinds of languages, it is easy to read config from source files (sometimes even importing via `eval`) so there often isn't any need to stick config in JSON documents. reply lamontcg 10 hours agorootparentI'm deeply uninterested in continuing to have this discussion with you. reply ParetoOptimal 6 hours agorootparentprevWhat are your thoughts on: - https://dhall-lang.org/ - https://toml.io/en/ reply pseudonom- 13 hours agorootparentprevDhall is the FP config language you're thinking of, I think. reply anon291 15 hours agorootparentprevI mean... Nix satisfies every single one of what you mentioned and people say its too complicated. It's literally just the JSON data structure with lambdas, which really is basic knowledge for any computer scientist, and yet people complain about it. It's fairly straightforward to 'embed' and as a bonus it generates json anyway (you can use the Nix command line to generate JSON). Me personally, I use it as my templating system (independent of nixpkgs) and it works great. It's a real language, but also restrictive enough that you don't do anything stupid (no IO really, and the IO it does have is declarative, functional and pure -- via hashing). In Nix's favor: 1. Can be described in a one page flier. An in-depth exhaustive explanation of the language's features is a few pages (https://nixos.org/manual/nix/stable/language/) 2. dynamically typed 3. Turing complete and based on the lambda calculus so has access to the full suite of functional control structures. Also has basic if/then/else statements for the most common cases and for intuition. 4. no threading, no concurrency, no real IO 5. definitely not object-oriented and no inheritance 6. It is functional in design and has an extremely thin set of builtins 7. FFI model is either embed libnix directly (this does not require embedding the nix store stuff, which is a completely separate modular system), or use the command line to generate json (nix-instantiate --eval --json). Note: do not confuse nixpkgs and NixOS with the nix language. The former is a system to build linux packages and entire linux distributions that use the latter as a configuration language. The nix language is completely independent and can be used for whatever. reply lamontcg 15 hours agorootparentTried to use Nix as a homebrew replacement and failed to get it installed correctly with it blowing up with crazy error messages that I couldn't google. I didn't even get to the point of assessing the language. It really seems like the right kind of idea, but it doesn't seem particularly stable or easy enough to get to that initial payoff. If there's a nice language under there it is crippled by the fact that the average user is going to have a hard time getting to it. reply oever 13 hours agorootparentprevI agree with the point's in Nix's favor except for 2. dynamically typed. Defining structs as part of the language would be nice. In fact, type checking is done ad-hoc now by passing data through type checking functions. reply mrloba 19 hours agoparentprevI agree, and I just want to highlight what you said about generating a config file. It's extremely useful to constrain the config itself to something that can go in a json file or whatever. It makes the config simpler, easier to consume, and easier to document. But when it comes to _writing_ the config file, we should all use a programming language, and preferably a statically typed language that can check for errors and give nice auto complete and inline documentation. I think aws cdk is a good example of this. Writing plain cloudformation is a pain. CDK solves this not by extending cloudformation with programming capabilities, but by generating the cloudformation for you. And the cloudformation is still a fairly simple, stable input for aws to consume. reply c0l0 20 hours agoparentprevRelevant article (2012): http://mikehadlow.blogspot.com/2012/05/configuration-complex... reply angarg12 18 hours agoparentprevI'm very happy using Typescript to templatize JSON. You can define a template as a class, compose them if needed, and when you are done, just write an object to a file. reply mark_and_sweep 21 hours agoparentprevAgreed, and I almost feel silly for pointing this out, but for writing JSON (JavaScript Object Notation), I'd recommend using JavaScript... reply jeroenhd 21 hours agorootparentFor JSON I'd stick with Typescript to be honest. You end up executing Javascript and producing Javascript-native objects, but the typing in Typescript to ensure the objects you produce are actually valid will save a lot of debugging. reply Draiken 20 hours agorootparentprevJS is actually not that great for this IMO. You probably need an NPM package to even deal with YAML because JS has a shitty standard library. Sticking to a scripting language with a strong standard library is way better. Any unix system can get Ruby/Python and read/write YAML/JSON immediately without caring too much about versions. Of course in today's upside down world most developers seem to only know JS, so it would at least be \"familiar\". Still a bad choice in my view. The way this industry is going, give it a few years and we'll have React-Kubernetes for generating templates. And I wish I was joking. reply mark_and_sweep 16 hours agorootparentParent is talking specifically about writing JSON, not YAML. reply Draiken 14 hours agorootparentYeah, but the article is about YAML and my original comment was about configuration in multiple formats. So, to clarify, for JSON JS is definitely not the worse option. For me though, even for JSON, you have much better options. reply speleding 21 hours agoparentprevCompletely agree, my wish is that anything that risks getting complex uses a Ruby-based DSL. For example, I like using Capistrano, which is wrapper around rake, which is a Ruby based DSL. That means that if things get tricky I can just drop down to using a programming language. Split stuff into logical parts that I load where needed and, for example, I can do something like YAML.load(..file..).dig('attribute name') or JSON.load from somewhere else. Yes, you risk someone building spaghetti that way, but the flip side is that a good devops can build something much easier to maintain than dozens of YAML and JSON files, and you get all the power from your IDE and linters that are already available for the programming language, so silly syntax errors are caught without needing to run anything. reply valty 10 hours agoparentprevThe problem with imperative languages in configs is that they become harder to read. Webpack configs always devolve into this. We need better tooling to allow tracing a how final configuration values are being generated. And a _live programming_ environment so we can see the final generated configuration in one view. reply martsa1 20 hours agoparentprevThis. It's why things like Cloud Development Kit and Pulumi are quite interesting to me. reply jayd16 17 hours agoparentprevBecause the security surface of \"any language\" is tricky and most (all?) popular languages do not have nice data literal syntax better than JSON and YAML. reply otabdeveloper4 20 hours agoparentprev> I heard you liked configuration languages, so I made this configuration language for your configuration language generation scripts. It supports templates, of course. reply andix 21 hours agoparentprevHelm would probably benefit from something like JSX for YAML/JSON. Just being able to script a chart instead of this templating hell. reply karlicoss 18 hours agoparentprevI argued that point in my article some time ago https://beepb00p.xyz/configs-suck.html also HN discussion at the time news.ycombinator.com/item?id=22787332 reply krapp 21 hours agoparentprevYou shouldn't need the full complexity and power of a Turing complete programming language to do config. The point of config is to describe a state, it's just data. You don't need an application within an application to describe state. Inevitably, the path of just using a programming language for config leads to your config becoming more and more complex until it inevitably needs its own config, etc. You wind up with a sprawling, Byzantine mess. reply Draiken 21 hours agorootparentThe complexity is already there. If you only need static state like you say, then YAML/JSON/whatever is fine. But that's not what happens as software grows. You need data that is different depending on environments, clouds, teams, etc. This complexity will still exist if you use YAML, it'll just be a ridiculous mess where you can break your scripts because you have an extra space in the YAML or added an incorrect `True` somewhere. Complexity growth is inevitable. What is definitely avoidable is shoving concepts that in fact describe a \"business\" rule (maybe operational rule is a better name?) in unreadable templates. Rules like: a deployment needs add these things when in production, or change those when in staging, etc exist whether they are hidden behind shitty Go templates or they are structured inside of a class/struct, a method with a descriptive name, etc. The only downside is that you need to understand some basics of programming. But for me that's not a downside at all, since it's a much more useful skill than only knowing how to stitch Go templates together. reply sigwinch28 21 hours agorootparentprev> your config becoming more and more complex until it inevitably needs its own config, etc. You wind up with a sprawling, Byzantine mess. We're already there with Helm. People write YAML because it's \"just data\". Then they want to package it up so they put it in a helm chart. Then they add variable substitution so that the name of resources can be configured by the chart user. Then they want to do some control flow or repetitiveness, so they use ifs and loops in templates. Then it needs configuring, so they add a values.yaml configuration file to configure the YAML templating engine's behaviour. Then it gets complicated so they define helper functions in the templating language, which are saved in another template file. So we have a YAML program being configured by a YAML configuration file, with functions written in a limited templating language. But that's sometimes not enough, so sometimes variables are also defined in the values.yaml and referenced elsewhere in the values.yaml with templating. This then gets passed to the templating system, which then evaluates that template-within-a-template, to produce YAML. reply btown 16 hours agorootparentAt the end of the day, Helm's issues stem from two competing interests: (1) I want to write something where I can visualize exactly what will be sent to Kubernetes, and visually compare it to the wealth of YAML-based documentation and tutorials out there (2) I have a set of resources/runners/cronjobs that each require similar, but not identical, setups and environments, so I need looping control flow and/or best-in-class template inclusion utilities -- People who have been working in k8s for years can dispense with (1), and thus can use various abstractions for generating YAML/JSON that don't require the user to think about {toYamlindent 8}. But for a team that's still skilling up on k8s, Helm is a very reasonable choice of technology in that it lets you preserve (1) even if (2) is very far from a best-in-class level. reply matharmin 21 hours agorootparentprevI have a recent example of rolling out IPv6 in AWS: 1. Create a new VPC, get an auto-assigned /56 prefix from AWS. 2. Create subnets within the VPC. Each subnet needs an explicitly-specified /64 prefix. (Maybe it can be auto-assigned by AWS, but you may still want to follow a specific pattern for your subnets). 3. Add those subnet prefixis to security / Firewall rules. You can do this with a sufficiently-advanced config language - perhaps it has a built-in function to generate subnets from a given prefix. But in my experience, using a general-purpose programming language makes it really easy to do this kind of automation. For reference, I did this using Pulumi with TypeScript, which works really well for this. reply avianlyric 19 hours agorootparentprevThat kind of ignores the entire pipeline involved in computing the correct config. Nobody wants to be manually writing config for dozens of services in multiple environments. The number of configurations you need to create is multiplicative, take the number of applications, multiply by number of environments, multiply by number of complete deploys (i.e. multiple customers running multiple envs) and very quickly end up with an unmanageable number of unique configurations. At that point you need a something at least approaching Turing completeness to correctly compute all the unique configs. Whether you decide to achieve that by embedding that computation into your application, or into a separate system that produces pure static config, is kind of academic. The complexity exists either way, and tools are needed to make it manageable. reply pid-1 19 hours agorootparentprevThat's not my experience after using AWS CDK since 2020 in the same company. Most of our code is plain boring declarative stuff. However, tooling is lightyears ahead of YAML (we have types, methods, etc...), we can encapsulate best practices and distribute as libs and, finally, escape hatches are possible when declarative code won't cut. reply worldsayshi 21 hours agorootparentprevWe need turing completeness in the strangest of places. We can often limit these places to a smaller part of the code. But it's really hard to know beforehand where those places will occur. Whenever we think we have found a clear separation we invent a config language. And then we realize that we need scripting so we invent a templating language. Then everybody looses their minds and invents 5 more config languages that surely will make us not need the templating language. Let's just call it code and use clever types to separate turing and non-turing completeness? reply kevincox 21 hours agorootparentprevA really good solution here is to use a full programming language but run the config generator on every CI run and show the diff in review. This way you have a real language to make conditions as necessary but also can see the concrete results easily. Unfortunately few review tools handle this well. Checked-in snapshot tests are the closest approximation that I have seen. reply gtirloni 21 hours agorootparentprev> You don't need an application within an application to describe state. As shown in the article, you apparently do. reply IggleSniggle 21 hours agorootparentprevIt happens because config is dual purpose: its state, but it's also the text-UI for your program. It spirals out of control because people want the best of it being \"just text\" and being a nice clean UI. reply myaccountonhn 18 hours agorootparentprevI agree, I think a language like dhall (https://dhall-lang.org/) strikes a good balance. reply SOLAR_FIELDS 21 hours agorootparentprevYeah, YAML is good at declarative things. It’s when you start using it imperatively eg CI/CD is when it really starts to get ugly. reply ForHackernews 12 hours agoparentprevThrowing in a plug for https://dhall-lang.org/ > Dhall is a programmable configuration language that you can think of as: JSON + functions + types + imports reply asimpletune 18 hours agoparentprevThis is how config actually works in Scala. reply TheFuzzball 22 hours agoprevI just knew this would be about Kubernetes when I saw the title. The Kubernetes API is fairly straightforward, and has a well-defined (JSON) schema, people should be spending a bulk of their time learning k8s understanding how to use the API, but instead they spend it working out how to use a Helm chart. I don't think Jsonnet, Ksonnet, Nu, or CUE ever gained that much traction. I'm convinced most people just use Kustomize, because it's fairly straightforward and built in to kubectl. I'd like a tool that: - Gives definition writers type checking against the k8s schemas - validation, version deprecations, etc. - Gives users a single artefact that can be inspected easily and will fail (ACID) if deployed against a cluster that doesn't support any objects/versions. - Is built into the default toolchain --- I feel like writing a Bun or Deno TypeScript script that exports a function with arguments and returns a list of definitions would work well, esp. with `deno compile`, etc. but that violates the third point. reply ryandv 17 hours agoparent> The Kubernetes API is fairly straightforward, and has a well-defined (JSON) schema, people should be spending a bulk of their time learning k8s understanding how to use the API, but instead they spend it working out how to use a Helm chart. This is a general pattern in software. Instead of learning the primitives and fundamentals that your system is built on, which would be too hard, instead learn a bunch of abstractions over top of it. Sure, now you are insulated from the lower-level details of the system, but now you have to deal with a massive stack of abstractions that makes diagnosis and debugging difficult once something goes wrong. Now it's much harder to ascertain what exactly is happening in your system, since the details of what is actually going on have been abstracted away from you by design. Further, you are now dependent on that abstraction layer and must support and accommodate whatever updates may be released by the vendor, in addition to whatever else is lurking in your dependency graph. reply baq 22 hours agoparentprevprobably doesn't meet the 2nd requirement, most definitely doesn't meet the third, but: https://cdk8s.io/docs/latest/ reply TheFuzzball 22 hours agorootparentThe second requirement is actually probably the most important - if someone that just set up ArgoCD, Flux, or has their own GitOps pipeline, how much of a headache does using a new compile step present? Lots of things are simple in isolation: want to use Cue? Just get your definitions and install the compiler and call it and boom, there are your k8s defs! Ok, but how do I integrate all of that into my existing toolchain? How do I pass config? Etc, etc. The best, fastest tool won't win. The tool that has the most frictionless user story will. reply shepherdjerred 18 hours agorootparentI was able to get CDK8s working easily by simply committing the built template along with my TypeScript. Then, I just pointed ArgoCD to my repo. reply how_gauche 17 hours agorootparentWe do the same thing but commit to a second git repo that we treat like the \"k8s yaml release database\". reply ithkuil 21 hours agoparentprevI love the idea of keeping it simple and I do try to use kustomize or even plain yaml as installation method as much as possible. But in practice when managing large systems you inevitably end up benefiting from templating reply worldsayshi 21 hours agorootparentI've begun thinking that if you start thinking about templating you might be better off building an operator. Operators aren't as well understood and documented. But in my mind an operator is just a pod or deployment that creates on demand resources using the k8s api. reply ryandv 21 hours agorootparentThe purpose of an Operator is to realize the resources desired/requested in a (custom) resource manifest, often as YAML or JSON. You give the apiserver a document describing what resources you need. The Operator actually does the work of provisioning those resources in the \"real world\" and (should) update the status field on the API object to indicate if those resources are ready. reply ithkuil 18 hours agorootparentprevoh yeah; operators are great and sometimes they are necessary. On the other hand, most operators I've seen are just k8s manifest templates implemented in Go. I often end up preferring using Jsonnet to deal with that instead of doing the same stuff in Go. Jsonnet is much more close to the underlying datamodel (the k8s manifest Json/Yaml document) and comes with some useful functionality out of the box, such \"overlays\". It has downsides too! It's untyped, debugging tools are lacking, people are unfamiliar with it and don't care to learn it. So I totally get why one would entertain the possibility of writing your \"templates\" using a better language. However, an operator is often too much freedom. It's not just using Go or Rust or Typescript to \"generate\" some Json manifests, but it also contains the code to interact with the API server, setup watches, and reactions etc. I often wish there was a better way to separate those two concerns I'm a fan of metacontroller [1], which is a tool that allows you to write operators without actually writing a lot of imperative code that interacts with the k8s API, but instead just provide a general JSON->JSON transformer, which you could write in any langue (Go, Python, Rust, Javascript, .... and also Jsonnet if you want). I recently implemented something similar but much tailored to just \"installing\" stuff, called Kubit. An OCI artifact contains some abitrary tarball (generally containing some template sources) and a reference to a docker image containing an \"engine\" and runs the engine with your provided tarball + some parameters passed in a CRD. The OCI artifact could contain a helm chart and the template engine could contain the helm binary, or the template engine could be kubecfg and the OCI artifact could contain a bunch of jsonnet files. Or you could write your own stuff in python or typescript. The kubit operator then just runs your code, gathers the output and applies with with kubectl apply-set. 1. https://metacontroller.github.io/metacontroller/intro.html 2. https://github.com/kubecfg/kubit reply ryandv 17 hours agorootparent> On the other hand, most operators I've seen are just k8s manifest templates implemented in Go. > I'm a fan of metacontroller [1], which is a tool that allows you to write operators without actually writing a lot of imperative code that interacts with the k8s API, but instead just provide a general JSON->JSON transformer, That seems... surprising, to me. It's not clear to me how a JSON->JSON transformer (which is essentially a pure function on UTF-8 strings to UTF-8 strings, i.e. an operation without side effect) can actually modify the state of the world to bring your requested resources to life. If the only thing the Operator is being used for is pure computation, then I agree it's overkill. An example use case for an Operator would be a Pod running on the cluster that is able to receive YAML documents/resource objects describing what kind of x509 certificate is desired, fulfill an ACME certificate order, and populate a Secret resource on the cluster containing the x509 certificate requested. It's not strictly JSON to JSON, from \"certificate\" custom resource to Secret resource - there's a bunch of side-effecting that needs to take place to, for instance, respond to DNS01 or HTTP01 challenges by actually creating a publicly accessible artifact somewhere. That's what Operators are for. reply ithkuil 17 hours agorootparentMetacontroller is actually quite easy to learn. It comes with good examples too. Including a re-implementation of the Stateful Set controller, all done with iterations of an otherwise pure computation. The trick is obviously that the state lives in the k8s api server, from which the inputs of the subsequent invocation of your pure function come. reply worldsayshi 15 hours agorootparentprev> an operator is often too much freedom While that is true I'm a bit afraid that we might be overselling the concept of limiting freedom past a certain point. Limiting freedom has the upside of giving us some guarantees that makes a solution easier to reason about. But once we step out of dumb-yaml I don't see that making additional intermediate trade-offs is worth it. And there are apparently some downsides to introducing additional layers as well. The main downside of limiting freedom seems to be the chaos of having so many different ways to do things. Imagine what could happen if we agreed that there are two ways of doing things; write yaml without templates or write an operator. Then maybe we could focus efforts on the problem of writing maintainable operators. Things should be either dumb data or the kitchen sink I think. reply ithkuil 14 hours agorootparentI'm not against having actual controllers with powerful logic. But often is possible to separate the custom logic from the bulk of the parameterized boilerplate. reply doctorpangloss 16 hours agorootparentprevHelm is a low budget operator. reply ryandv 16 hours agorootparentNo... no, no, no. No kidding; Operators are indeed poorly understood. They are not just glorified XSLT for YAML/JSON. https://kubernetes.io/docs/concepts/extend-kubernetes/operat... reply DrBazza 16 hours agoparentprevWe're using jsonnet for our systems and they have absolutely nothing to do with k8s. I'm not sure it's true to say it has ever gained much traction. It's just a niche case for complex configuration, and isn't the most publicised tool. It does precisely what we need with zero fuss, cross platform and cross _language_ (we've embedded it in C++, .NET, and JVM executables). We can use the resulting json config with a vast array of tools that simply don't exist for the alternatives such toml/yaml/hocon/ini whatever. In fact we tried to get HOCON working for non-JVM languages but there was always some edge case. reply Havoc 22 hours agoparentprevHow would one use the json api without ending up writing a bunch of custom code? reply TheFuzzball 22 hours agorootparentI think custom code is to be expected, and making it maintainable is what's important. > everything should be made as simple as possible, but no simpler. Helm et al made it simpler than it was, IMO. reply Havoc 21 hours agorootparentEveryone hand rolling code does not seem like an improvement over tools like helm even if it’s yaml reply TheFuzzball 21 hours agorootparentNo, obviously not, and that's not what I've suggested. reply rad_gruchalski 21 hours agorootparentprevHelm is another can of hot garbage. Impossible to vendor without hitting name collisions, can configure only what’s templated. Jsonnet is the way to go with generated helm manifests transformed later. Kustomize with its post-renderer hooks is another can of even hotter garbage. reply aeyes 21 hours agorootparent> Impossible to vendor without hitting name collisions What problem exactly are you facing? I can change the name of the chart itself in chart.yaml and if the name of the resources collide I change them with nameOverride/fullnameOverride in the values. All charts have these because they are autogenerated by `helm create`. I vendor all charts and never had this problem. reply rad_gruchalski 20 hours agorootparentYou just made a copy of a chart. You modified your chart. What I’m missing is helm having some notion of an org in the chart name, like docker does: repo/name:tag, helm only has name and version. Hence you modify your chart.yaml and it should be preferable without having to modify anything. This is really problematic when a chart pulls dependencies in. reply fsniper 21 hours agoparentprevFor Helm the value is it is not a configuration managemet solution but a package manager. The rest are just methods of writing json/yaml. I understand the \"hate\" against yaml, But I don't think it's deserving it that much. Perhaps timoni will take over with it's usage of cue. At least it's a package management solution. reply liveoneggs 21 hours agoparentprevk8s make me miss xml reply valty 21 hours agoprevIt's funny how little developers think about how to do configuration right. It's just a bunch of keys and values, stored in some file, or generated by some code. But its actually the whole ball game. It's what programming is. Everything is configuration. Every function parameter is a kind of configuration. And all the configuration in external files inevitably ends up as a function parameter in some way. The problem is the plain-text representation of code. Declarative configuration files seem nice because you can see everything in one place. If you do your configuration programmatically, it is hard to find the correct place to change something. If our code ran in real-time to show us a representation of the final configuration, and we could trace how each final configuration value was generated, then it wouldn't be a problem. But no systems are designed with this capability, even though it is quite trivial to do. Configuration is always an after-thought. Now extend this concept to all of programming. Imagine being able to see every piece of code that depends upon a single configuration value, and any transformations of it. Also, most configuration is probably better placed into a central database because it is relational/graph-like. Different configuration values relate to one another. So we should be looking at configuration in a database/graph editor. Once you unchain yourself from plain-text, things start to become a lot simpler...of course the language capabilities I mentioned above still need to become a thing. reply Syntonicles 15 hours agoparentThis is something I'm trying really hard to do with a client. They have a bunch of 1500+ line \"config\" files for products, which are then used to make technical drawings and production files. The configs attempt to use naming scheme to group related variables together. I want to migrate to an actual nested data-structure using (maybe) JSON - and these engineers absolutely will not write code, so config-as-code is a no-go, in addition to the disadvantage you mentioned. My next thought was that there should be a better way to show the configuration, and allow that configuration to be modified. I was thinking maybe some sort of visual UI which where the user can navigate a representation of the final product, select a part and modify a parameter that way. Is that along the lines of your suggestion? If not will you please expand a little? Configuration is the absolute core of this application. reply valty 10 hours agorootparentSounds like you need an SQL database. You could use SQLite. Then provide a GUI to modify that database. You could add a bunch of constraints in the database too to ensure the config is correct. Usually when there is plain-text files though, it's because they want it that way. It's easier to edit a text file sometimes than rows in a database. Cut/copy/paste/duplicate files and text. Simple textual version control. reply Syntonicles 10 hours agorootparentSure, I agree - I'm proposing JSON as an intermediate step toward a well-defined data-model since the thousands of copied config files have evolved over time, so the data-model is a smear of backward-compatibility hacks. What I was trying to do is get you to explain what you mean by this: > If our code ran in real-time to show us a representation of the final configuration, and we could trace how each final configuration value was generated, then it wouldn't be a problem. [...] But no systems are designed with this capability, even though it is quite trivial to do. Configuration is always an after-thought. reply valty 7 hours agorootparentThis is only relevant if you allow code to define config. If you use conditionals and loops to create config, and then view the final json, it quickly becomes annoying when you know the thing you want to change in the final json, but have to trace backwards through the code to figure out where to change it. So programmatic configs only work if you have this \"value tracing\" capability. Which nothing really does. reply ManBeardPc 22 hours agoprevWorse yet, in some places (CI/CD) YAML becomes nearly a programming language. A very verbose, unintuitive, badly specified and vendor-specific one as well. reply marginalia_nu 19 hours agoparentIt's pretty much repeating the mistake of early 2010s Java, where the entire application frequently was glued together by enormous ball of XML that configured all the dependency injection. It had the familiar properties of (despite DTDs and XML validation) often blowing up late, and providing error messages that were difficult to interpret. At the time a lot of the frustration was aimed at XML, but the mid 2020s YAML hell shows us that the problem was never the markup language. reply epistasis 16 hours agorootparentYou have a loosely coupled bundle of modules that you need to glue together with some configuration language. So you decide to use X. Now you have two problems. reply dygd 22 hours agoparentprevSpot on. We use ytt[0], \"a slightly modified version of the Starlark programming language which is a dialect of Python\". Burying logic somewhere in a yaml template is one thing I dislike with passion. [0] https://tanzu.vmware.com/developer/guides/ytt-gs/ reply jsight 17 hours agorootparentTBH, ytt is the only yaml templating approach that I actually like. The downside is that it is easy to do dumb things and put a lot of loops in your yaml. The positive is that it is pretty easy to use it like an actual templating language with business logic in starlark files that look almost just like Python. In practice this works pretty well. The syntax is still fairly clumsy, but I like it more than helm. reply sspiff 22 hours agoparentprevIn some places working with Kubernetes, people unironically use the term \"YAML engineer\". reply donalhunt 21 hours agorootparentI've seen memes where SREs complain they have just become YAML engineers. :( reply ManBeardPc 20 hours agorootparentI've been there. Not YAML specifically, but basically just configuration (XML, JSON, properties, ...) for some proprietary systems without any good documentation or support available. \"It's easy, just do/insert X\", half a year and dozens of meetings and experts later, it was indeed not just X. Meanwhile I could've build everything myself from scratch or with common open-source solutions. reply switch007 21 hours agorootparentprevI mean...building a data centre / PaaS with YAML is pretty cool We used to have to shove servers in to racks ! Kids these days :D reply sspiff 17 hours agorootparentI *loved* shoving servers in racks! reply bpicolo 15 hours agorootparentI dream of a day there's a physical component of my job, not just the staring at a screen bit. reply jsight 17 hours agorootparentprevyamlops is a real thing :) reply indymike 22 hours agoparentprevYAML is the Bradford Pear of serialization formats. It looks good at first, but as your project ages, and the YAML grows it collapses under the weight of it's own branches. reply Y_Y 21 hours agorootparentI had to look up that tree. Invasive, offensive odour, cynaide-rich fruit. That's a a good insult! reply indymike 16 hours agorootparentYou should see what they look like after a 25kph breeze. Which isn't too far off from what templated YAML generates after someone commits a bad template. reply DonHopkins 21 hours agorootparentprevYAML is also just as bad as the Linden tree. https://www.youtube.com/watch?v=aoqlYGuZGVM reply deathanatos 17 hours agoparentprevYeah … for CI files (like Github workflows & such), one of the best things I think I've done is just to immediately exec out to a script or program. That is, most of our CI steps look like this: run: 'exec ci/some-program' … and that's it. It really aids being able to run the (failing) CI step offline, too, since it's a single script. Stuff like Ansible is another matter altogether. That really is programming in YAML, and it hurts. reply zelphirkalt 22 hours agoparentprevIn such places one frequently has to remind oneself and others to not start programming in that configuration language, if avoidable, to not create tons of headache and pain. reply MrBuddyCasino 22 hours agoparentprevEven worse, every generation repeats this mistake. I‘m not sure S-Expressions are the answer, but Terraform HCL should never have been invented. reply SOLAR_FIELDS 21 hours agorootparentI was just telling a colleague today that HCL is great until you need to do a loop. A lot of parallels to this YAML discussion reply quchen 19 hours agorootparentMy favorite pattern in HCL is the if-loop. Since there is no »only do this resource if P« in Terraform, the solution is »run this loop not at all or once«. reply kevincox 21 hours agorootparentprevI'll take HCL over YAML templating any day. At least it is working with real data structures not bashing strings together. That being said, yes, it is also an awful language. reply XorNot 22 hours agoparentprevThis criticism doesn't pass the sniff test though: your average Haskeller loves to extoll the virtues of using Haskell to implement a DSL for some system which is ultimately just doing the same thing in practice (because they're still not going to write documentation for it, but hey, how hard can it be to figure out it's just...) YAML becomes a programming language because vendors need a DSL for their system, and they need to present it in a form which every other language can mostly handle the AST for, which means it's easiest if it just lives atop a data transfer format. reply hiAndrewQuinn 22 hours agorootparentHey now. Your average Haskeller would simply recommend you replace YAML with Dhall. https://dhall-lang.org/ reply worldsayshi 21 hours agorootparentWhy not \"just\" use an embedded DSL? reply ManBeardPc 20 hours agorootparentprevI don't know what this has to do with Haskell. I understand that they need a DSL for their system. I just don't agree that it is a good idea to use some general purpose serialization format. In the end they always evolve to a nearly full programming language with conditions and loops. Using a full programming language makes much more sense IMHO, for example like Zig build files or how we use Python to build neural networks. That way I can actually use existing tools to do what I need. reply baq 22 hours agorootparentprevmaybe yaml should standardise hygienic macros. and a repl. reply TeMPOraL 19 hours agorootparentThe lengths people go to avoid using s-expressions never ceases to amaze me. We're talking countless centuries and great many minds pushed to brink of madness, just to keep the configs looking like Python or JavaScript. reply baq 18 hours agorootparentI'd say it's even worse: it's a collective hallucination that complex configs are not code. reply jrockway 16 hours agoprevYeah, I'm very sad that helm won. We do OSS k8s stuff at work, and 100% of users have asked for us to make a helm chart. So we had to. It is miserable to work on; your editor can't help you because the files are named like \"foo.yaml\" but they aren't YAML. You have to make sure you pipe all your data through \"indent 4\" so that things are lined up correctly in the YAML. What depresses me the most is that you have to re-expose every Kubernetes feature in your own way. Someone wants to add deployment.spec.template.spec.fooBars? Now you have to add deploymentFooBars to your values.yaml file and plumb it in. For every. single. feature. It's truly \"worse is better\" gone wrong. I have definitely done some terrible things like \"sed -e s/$FOO/foo/g\" to implement templating... and that's probably how Helm started. The result is a mess. I personally grew up on Kustomize before it was in kubectl, and was always exceedingly happy with it. (OK, it has a lot of quirks. But at least it saves you time because it actually understands the semantics of the objects you are creating.) I like Jsonnet a lot better. As part of our k8s app, we ship an Envoy deployment to do all of our crazy traffic routing (basically... maintaining backwards compatibility with old releases). Envoy configs are... verbose..., but Jsonnet makes it really easy to work on. (The code in question: https://github.com/pachyderm/pachyderm/blob/master/etc/gener...) I'm seriously considering transpiling jsonnet to the Go template language and just implementing everything with Jsonnet. At least that is slightly maintainable, and nobody will ever know because \"helm install\" will Just Work ;) But yeah, I think Helm will be the death of Kubernetes. Some competing computer allocator container runner thingie will have some decent language for configuration, and it will just take over overnight. Mark my words! reply thinkmassive 14 hours agoparent> I have definitely done some terrible things like \"sed -e s/$FOO/foo/g\" to implement templating Next time you reach for this, check out envsubst for a slightly improved solution that’s somewhat standard (at least common). On the topic of templating or modifying helm charts using jsonnet, you might find Tanka helpful: https://tanka.dev/helm reply dewbrite 7 hours agoparentprev> But yeah, I think Helm will be the death of Kubernetes. Some competing computer allocator container runner thingie will have some decent language for configuration, and it will just take over overnight. Mark my words! I want to believe this. Everywhere I've worked we're still rawdogging tf/hcl and helm though, because change is scary. At least I get some relief in my personal projects. :') reply neallindsay 21 hours agoprevMy personal philosophy is that string interpolation should not be used to generate machine-readable code, and template languages are just fancy string interpolation. We've all seen the consequences of SQL injection and cross-site scripting. That's the kind of thing that will keep happening as long as we keep putting arbitrary text into interpreters. Yes, this means I don't think we should use template files to make HTML at all. Alternatives to using template languages for HTML include Haml (for Ruby) and Pug (for JavaScript). These languages have defined ways to specify entire trees of tags, attributes, and text nodes. If you don't like Python-style significant indentation, JavaScript has JSX. The HTML-looking parts of JSX compile down to a bunch of `createElement` expressions that create a web document tree. That tree can then be output as HTML if necessary. Haml, Pug, and JSX are not template languages even though they can output HTML. Likewise, `JSON.stringify(myObj)` is not a template language for JSON. Generating machine-readable code should be done with a tool that understands and leverages the known structure of the target language when possible. reply lioeters 16 hours agoparent> Haml, Pug, and JSX are not template languages even though they can output HTML. That's nonsense, unless we go by your idiosyncratic definition of what a template language is (\"fancy string interpolation\"). > Haml (HTML Abstraction Markup Language) is a templating system that is designed to avoid writing inline code in a web document and make the HTML cleaner. > Pug – robust, elegant, feature rich template engine for Node.js > JSX is an XML-like syntax extension to ECMAScript without any defined semantics. OK, I'd agree that JSX is not strictly a template language. But in the end, all of these compile down to HTML. Not by string interpolation, but as a language that is parsed into a syntax tree, then rendered into HTML properly with an internal understanding of valid structure. YAML with templating is fancy string interpolation, it's not a template language (or at least a poorly implemented one). reply neallindsay 16 hours agorootparentI am aware that Haml and Pug call themselves template languages, but they are not. In a template language, the source is a \"template\" that has some special syntax to fill in some bits. I don't think that's a very idiosyncratic definition. Pretty much any programming language can output a bunch of text, but most of them are not template languages. Java has XMLBuilder, but that doesn't make it a template language for outputting XML. But PHP is a template language, even though it's not recommended to use it that way anymore. reply lioeters 15 hours agorootparentWell, it's true that Haml calls itself a \"templating system\", and Pug uses the term \"template engine\". That's 3 out of 3, you win. ;) PHP is a scripting language that is also a template processor, but I wouldn't call it a template language. So we disagree on several points, but no big deal. A big disadvantage of PHP, in relation to your original point about \"fancy string interpolation\", is that it does not natively understand the target output HTML syntactically and structurally. reply int_19h 13 hours agoparentprevNot all template languages are string template languages, though. If you consider PHP a templating language for text, for example, then by the same logic XQuery is a templating language for XML. reply kevincox 21 hours agoparentprevThis is the essence of the problem! Yaml and templates are just distractions. It just boils down to the fact that \"string\" is a very general type and we use it lazily. My personal rule: Every time a value is inserted into a string it must be properly encoded. I wrote a full blog post around this a while back https://kevincox.ca/2022/02/08/escape-everything/. But the TL;DR is that every string has a format which needs to be respected wether that be HTML, SQL or human-readable terminal output. Every time you put some value into a string you should be properly encoding it into that format. But we rarely do. reply Izkata 19 hours agorootparent> My personal rule: Every time a value is inserted into a string it must be properly encoded. This is how Django templates have done it for over a decade. You have to go out of your way to tell it not to escape the values if for some reason you need that. reply roenxi 22 hours agoprevI see a problem here. I'm not certain if the sort of person who would choose YAML as their configuration language sees a problem here. There is a direct conflict between human-centred data representations and computer-centred. Computers love things that look like a bit like a Lisp. Humans like things that look a bit like Python. If you're the sort of person who wants to use a computer to manipulate their Kubernetes config then you'd be secretly annoyed that Kubernetes uses YAML. However, it appears the Kubernetes community are mainly YAML people, so why would they mind that their config files will be horrible to work with once programming logic gets involved? The downside of YAML is exactly this scenario, and I believe the people involved in K8s are generally cluey enough to see that coming. > YAML is a superset of JSON The spec writers can put whatever they want in their document, but I don't think this is true. If you go in and convert all the YAML config to JSON, the DevOps team is going to get upset. The two data formats have the same semantic representation, but so do all languages compiled to the same CPU arch. JSON and YAML are disjoint in practice. Mixing the two isn't a good idea. reply Joker_vD 22 hours agoparentThe ironic thing is that, IIRC, k8s manifests were supposed to be machine-generated from the k8s's inception, you weren't supposed to write them by hand... of course, people wrote them by hand anyway, until it became unbearable ― at which point they've started templating them because that's how the things always seem to progress: manually-written text is almost never replaced by machine-generated config-serialized-to-text, it's replaced by templated-but-originally-still-manually-written text. reply baq 22 hours agorootparent> k8s manifests were supposed to be machine-generated from the k8s's inception, failed spectacularly at not being inconvenient enough for their intended purpose. one of those cases where unreadable by design would be a most welcome feature. reply mortehu 22 hours agoparentprev\"YAML is a superset of JSON\" only means that any JSON document is a valid YAML document. It does not mean YAML is equal to JSON. reply gregwebs 22 hours agoprevWe are switching to cuelang [1]. IMHO it is better designed than Jsonette. Since Kubeenetes already has state reconciliation, the only thing missing in this setup is deletion. But that can now be accomplished with the prune feature. [2] [1] https://cuelang.org/docs/integrations/k8s/ [2] https://kubernetes.io/blog/2023/05/09/introducing-kubectl-ap... reply lantry 18 hours agoparentI can second cuelang. We started using it at work and it's so nice. Some of the error messages are a little hard to decipher, but that's acceptable because it catches so many errors up front. The few times I have to write yaml directly, it now feels so tedious in comparison. reply gtirloni 21 hours agoprevI love YAML and I curse it every single day that I'm working with Helm charts. People ask me what I'd use to deploy apps on Kubernetes and I say I hate Helm and would still use it for a single reason: everybody is using it, I don't want to create a snowflake infrastructure that only I understand. Still, back in the day I thought jsonnet would win this battle but here we are, cursing Helm and templates. That's the power of upstream decisions. reply BiteCode_dev 22 hours agoprevThis is where I usually pitch in with \"Have your heard of CUELang, our lord and savior?\": https://cuelang.org/ - Not turing complete yet sufficiently expressive to DRY - Define schema and data with the same language, in a separate or same file. With union types. - Generate YAML or JSON. Can validate itself, or a YAML or JSON file. The biggest drawback being the only implementation is currently in go, meaning you may have to subprocess of ffi. reply 1oooqooq 22 hours agoparentwe have a pipeline that ingest very concise cuelang files. then it generates json files for each application for a tool that will create xml definitions which then are applied to a xls which the architects own, to spit out a yaml that we use to apply our helm charts. the charts deploy a k8s client which then interact with the main cluster via json using the api. took a while, but we are using the best tool for each job. reply ljm 21 hours agorootparentjust throw in a kafka cluster so you can pipe each step through an event bus and you'll have an enterprise-grade deployment setup reply BiteCode_dev 19 hours agorootparentprevYou used JSON twice, how casual. Your API should clearly be using protobuf. reply planede 22 hours agoparentprevHow does it compare to dhall? reply arianvanp 22 hours agorootparentDhall's lack of any form of type inference makes it very verbose and difficult to refactor in my opinion. (I'm the author of dhall-kubernetes and never ended up using it in production; funnily enough). Dhall is also extremely slow. We had kubernetes manifests that took _minutes_ to type-check. Cue is basically instant. This matters a lot to me. I find cue very ergonomic. Also it treating both types and values as values is very neat. You write your types and your values in the same syntax and everything unifies neatly. but I sometimes miss its lack of functions. Cue also being to ingest protobuf definitions and openapi schemas makes it very quick and easy to integrate with your project. Have a new Kubernetes CRD you want to have type-checked in cue? No problem just run `cue get go k8s.io/api/myapi/v1alpha1` and off you go you have all your type definitions imported from Go to Cue! Especially for k8s this makes for very fast development and iteration cycle. I've wanted to take a look at https://nickel-lang.org/ which is a \"what if cue had functions\" language. but to be honest Cue kind of serves my needs. reply ParetoOptimal 6 hours agorootparent> Dhall is also extremely slow. We had kubernetes manifests that took _minutes_ to type-check. Cue is basically instant. Everyone wants type-safety, but no one wants to wait for the type-checker :) Maybe in this case dhall with type checks equivalent to dhall would be slower, but I notice in many places people say \"strong type-checking is valuable\" while still expecting similar compile times as languages with weaker type systems. reply letmeinhere 19 hours agorootparentprevSpeaking of Nickel, they've got a great document detailing the reasons for their design (for example why they chose not embed in a general-purpose language like Pulumi) and how Nickel compares to other config languages like Dhall and CUE: https://github.com/tweag/nickel/blob/master/RATIONALE.md reply gregwebs 20 hours agorootparentprevCue was designed very much with k8s in mind and developed tutorials and integrations for it early on. Dhall was designed pre-k8s. Dhall had to introduce a defaults feature: before that it was completely unusable for k8s. Dhall has functions, which are natural to programmers- particularly from an FP background, Dhall would be trivial to start using. Whereas it takes some getting used to cue's unifications- but there is enough documentation and integration for getting going with k8s to make up for it. Dhall has unique features for stably importing configurations from remote locations. reply resters 21 hours agoprevIn my view, the presence of YAML templating is a red flag in any codebase or system. YAML got its popularity with the advent of Ruby on Rails, largely due to the simplicity of the database.yml file as an aid in database connection string abstraction that felt extremely clean to Java programmers who were used to complicated XML files full of DSN names and connection string peculiarities. The evolution of the database.yml file into something arguably as complex as the thing it was intended to replace is described in the article below: https://dev.to/andreimaxim/the-rails-databaseyml-file-4dm9 reply nhumrich 21 hours agoprevI will tell you exactly why we template yaml. Its the exact same reason every code base has ugly parts. And that's the evolution of complexity. At first, you have a yaml file. No templates, no variables. Just a good old standard yaml. Then, suddenly you need to introduce a single variable. Templating out the one variable is pretty easy, so you do it, and it's still mostly for humans to edit. Well, now you have a yaml file and template engine already in place. So when one more thing pops up, you template it out. 8 features later, you wonder what you've done. Only, if we go back in time, each step was actually the most efficient. Introducing anything else at step 1 would be over-engineering. Introducing it anywhere else would lead to a large refactor and possible regressions. To top it off, this is not business logic. Your devs are not touching this yaml all that much. So is it worth \"fixing\", probably not. reply Joker_vD 22 hours agoprevThe title of TFA was actually my reaction when I learned what Helm was actually doing. Initially I thought Helm would take an input file of YAML-with-template-bits, parse that YAML as an object, then use the provided template bits to fill in the parts of that object, then serialize the object back to YAML and write it out. Sounds reasonable, right? Nope, it's literal text substitution, so if you want to have a valid YAML as the output you better count your indentation on your fingers, and track where the newlines go or don't go. reply globallyunique 17 hours agoprevWe wrote a backend service at Lyft in Python and at some point needed to do some string interpolation for experimentation. In a rush someone implemented this in YAML (no new deps needed). This ended up being the bane of the teams existence. Almost impossible to test if something was going to break in runtime, could only verify it was valid yaml but many other things were infeasible, super hard to debug - it soured me on YAML for years. reply difosfor 22 hours agoprevWouldn't it be a better idea to use an existing programming language instead of cooking up numerous half baked templating languages? reply berkes 22 hours agoparentYes. Except you then have to sensor that programming language severely. Maybe you can accept some endless loop, but you probably don't want the CI orchestrator to start mining Monero, instead of bootstrapping and configging servers and services. A solution to that sensorship might be a very limited WASM runtime: one that offers a very few API's, has severely limited resources and timeouts and such. So people can write their orchestration in Python, Javascript or Rust or even Brainfuck if they want, but what that orchestration can do, and for how long it can do that, and how much memory, space and so on it gets, all is very limiting. While that may work, it's far harder to think of than \"lets make another {{templating|language}}\" inside this YAML that we already have and everyone else uses. reply sigwinch28 21 hours agorootparentI don't see any practical difference w.r.t. cybersecurity between \"I blindly applied this pile of YAML to my production kubernetes clusters without looking at it\" and \"I blindly downloaded and ran this computer program on my CI runner without looking at it\". A supply chain attack on the former means that your environment is compromised. So does the latter. reply berkes 20 hours agorootparentI do see a difference. GitHub actions isn't going to run your Python code on its orchestration infra. Nor is DigitalOcean or Fly.io or CircleCI. They all convened around \"YAML\" because it's a very limited set of instructions. I'm quite sure you cannot write a bitcoin miner (or something that opens a backdoor) in Liquid inside YAML in the DSL that Github Actions has. I am 100% sure you can write a bitcoin miner in Python, Javascript, Lua, or any programming language that Github would use to replace their YAML config. reply _visgean 18 hours agorootparentyou can still have a json output of the python code and compare it with in similar way to how atlantis work. reply linsomniac 18 hours agoprevAnsible convinced me that doing programming tasks in YAML is insanity, so I started an experiment: What would Ansible be like if it's syntax were more like Python than YAML. https://github.com/linsomniac/uplaybook I spent around 3 months over the holidays exploring that by implementing a \"micro Ansible\", I have a pretty solid tool that implements it, but haven't had much \"seat time\" with it: working on it rather than in it. But what I've done has convinced me that there are some benefits. reply embik 22 hours agoprevI am really sad that jsonnet / ksonnet never really took off. It’s a great way to template, but has a bit of a learning curve in my experience. I suspect that is why it’s niche. If you like what is presented in this article, take a look at Grafana Tanka (https://tanka.dev). reply twic 18 hours agoparentI was reading the description of Jsonnet and wondering why we don't just use JavaScript. Read a file, evaluate it, take the value of the last expression as the output, and blat it out as JSON. The environment could be enriched with some handy functions for working with structures. They could just be normal JavaScript functions. For example, a version of Object.assign which understands that \"key+\" syntax in objects. Or a function which removes entries from arrays and objects if they have undefined values, making it easy to make entries conditional. Those things are simple enough to write on demand that this might not even have to be a packaged tool. Just a thing you do with npm. reply madjam002 22 hours agoparentprevYeah similarly I'm using Nix to template K8s templates and I've never looked back. Helm is great for deploying 3rd party applications easily but I've never seen the appeal for using it for in house services, templating YAML is gross indeed. reply Leo_Verto 22 hours agoparentprevThe fact that it's a purely functional programming language with lazy evaluation is really powerful but steepens the learning curve for devs who haven't worked with functional languages. The stdlib is also pretty sparse, missing some commonly required functions. reply int_19h 13 hours agorootparent> The stdlib is also pretty sparse, missing some commonly required functions. This seems to be the general curse of template languages. For some reason, their authors have this near-religious belief in removing every \"unneeded\" feature, which in practice results in having to write 10 incomprehensible lines of code to do something that could be easily done in one line of readable code in a proper PL. reply anentropic 22 hours agorootparentprev> The fact that it's a purely functional programming language with lazy evaluation is really powerful but steepens the learning curve for devs who haven't worked with functional languages. does it really though? what part do they struggle with? reply cannonpalms 21 hours agorootparentIME engineers struggle with folds most. reply Thev00d00 22 hours agoprevIn my experience there is a near zero uptake of jsonnet or similar amongst \"regular\" i.e less ops inclined developers. gotmpl is a lot easier to grok if you are coming in cold. Yes it sucks for anything mildly complex, but the barrier to entry is significantly lower. Generation via real programming languages is the future I am hoping for. reply Anarch157a 22 hours agoparentJsonnet looks like a case of XKCD-927[0]. I fully agree with you that real programing languages are the way to go for generating anything more complex. [0] https://xkcd.com/927/ reply datadeft 22 hours agoprevIndeed why? However the conclusion I have is not to use JSON but to use a type safe configuration language that can express my intent much better making illegal states impossible. One example of such lang is Dhall. https://dhall-lang.org/ reply Aeolun 22 hours agoparentIf I’m going to use a whole language to generate my config already, why would I use anything but the language my application is written in? Everything can export JSON after all. reply datadeft 19 hours agorootparentBecause your language might not have a nice type system. For example Python -> JSON is going to produce worse guarantees than DHALL. reply vlfig 22 hours agorootparentprevDifferent requirements, different guarantees. Principle of least power. Have a look at https://docs.dhall-lang.org/discussions/Safety-guarantees.ht.... reply Draiken 21 hours agorootparentThis makes no sense to me. You have complex enough logic to warrant a language, you should use a real language. You'll have more support, less obscure issues, a solid standard library and whatever else you want, because it's a REAL language. If the argument is \"someone in my team uses recursion to write the YAML files, so I'll disallow it\", then the issue is not with the language, it's with the team. What I have found on my career is that many Ops people sell themselves short and hesitate to dive into learning and fully using an actual language. I've yet to understand why, but I've seen it multiple times. They then end up using pseudo-languages in configuration files to avoid this small step towards using an actual language, and then complain about how awful those pseudo-languages are. reply datadeft 19 hours agorootparent> You have complex enough logic to warrant a language, you should use a real language. Not sure what you mean. Dhall is a real language: Dhall is not a Turing-complete programming language, which is why Dhall’s type system can provide safety guarantees on par with non-programmable configuration file formats. Specifically, Dhall is a “total” functional programming language, which means that: You can always type-check an expression in a finite amount of time If an expression type-checks then evaluating that expression always succeeds in a finite amount of time reply happymellon 22 hours agoparentprevPulumi over Terraform. CDK over Cloudformation. Don't hand craft configuration files, these aren't new lessons. I remember being first introduced to Troposphere, which was pretty awesome. reply carlosrdrz 21 hours agoprevCan someone help me understand what is the advantage of using jsonnet, cue, or something else vs a simple python script (or dialect, like starlark), when you have the need of dynamically creating some sort of config? I've used jsonnet in the past to create k8s files, but I don't work in that space anymore. I don't remember it being better or easier than writing a python script that outputs JSON. Not even taking into account maintainability and such. Maybe I'm missing something? reply ekimekim 19 hours agoparentTo add to the sibling comments, after going from a jsonnet-based setup to a Typescript-based one (via pulumi), the biggest thing I missed from jsonnet was the native object merge operations which are very useful for this kind of work as it lets you say \"I want one of these, but with these changes\" even when the objects are highly nested, and you can specify whether to merge or override for each individual key. But ultimately this was a minor issue and I think it's far more important that you use something like this (whether a DSL or a mainstream PL) and that you're not trying to do string templating of YAML. reply ants_everywhere 21 hours agoparentprevThey're various points along the Turing complete config generator vs declarative config spectrum. Declarative config is ideal in lots of ways for mission critical things, but hard to create lots of because of boiler plate. A turing-complete general purpose language is entirely unconstrained in its ability to generate config, so it's difficult to understand all the possible configs it can generate. And it's difficult to write policy that forbids certain kinds of config to be generated by something like Python. And when you need to do an emergency-rollback, it can be hard to debug a Python script that generates your config. Starlark is a little better because it's deliberately constrained not to be as powerful as Python. Jsonnet is, IIUC, basically an open source version of the borgcfg tool they've had at Google forever. My recollection is that Borgcfg had the reputation of being an unreadable nightmare that nobody understood. In practice, of course, people did understand it but I don't think anyone loved working with it. Brian Grant, creator of Kubernetes, wrote up his thoughts on various config approaches in this Google doc: https://docs.google.com/document/d/1cLPGweVEYrVqQvBLJg6sxV-T.... reply IshKebab 21 hours agoparentprevI definitely wouldn't use Python because it isn't sandboxed, and users will end up doing crazy things like network calls in your config. Starlark is a good option though. People will talk about Jsonnet not being Turing complete, but IMO that is completely irrelvant. Turing completeness has zero practical significance for configs. reply kungfufrog 21 hours agoprevThere's 2 things on the horizon here for Kubernetes that give me hope. KCL, its own configuration language, and Timoni, which builds off CUE and corrects some of the shortcomings of Helm. Though these days, OLM and the Quarkus operator SDK give you a completely viable alternative approach to Helm that enables you to express much more complex functionality and dependency relationships over the lifecycle of resources. An example would be doing a DB backup before upgrading to a new release etc. Obviously this power comes at a cost. reply m000 16 hours agoprevYes, templating YAML is crazy. But is the answer jsonnet? That's even more batshit. Why hasn't anyone opted for a \"patch-based\" approach? I.e. start with a base YAML/JSON file, apply a second file over it, apply this third one, and use the result as the config. How you generate these files is entirely up to you. reply vimax",
    "originSummary": [
      "The author criticizes the practice of templating YAML and proposes using JSON instead for better configuration management.",
      "They argue that YAML templating becomes challenging as configuration complexity increases, particularly with optional fields and nested structures.",
      "The author introduces Jsonnet as a solution, emphasizing its capability to generate JSON config effortlessly and handle optional fields and nested structures more effectively.",
      "They also mention the use of Jsonnet in their tool kr8 for simplified configuration management in Kubernetes clusters."
    ],
    "commentSummary": [
      "The text discusses various configuration languages and tools, such as YAML, JSON, TOML, AWS CDK, Terraform, Ansible, Chef, HCL, LOGO, Nix, Helm, Dhall, CDK8s, and more.",
      "Participants engage in conversations and debates regarding the pros and cons of different languages, expressing frustrations and proposing alternatives.",
      "Programming languages with a strong standard library are preferred, emphasizing the importance of simplicity, ease of use, and maintainability in configuration files. Some users suggest the use of DSLs or embedded languages for enhanced functionality and flexibility."
    ],
    "points": 417,
    "commentCount": 594,
    "retryCount": 0,
    "time": 1706007521
  },
  {
    "id": 39104363,
    "title": "The Framework Laptop 16: Great Linux Support and Customizability Receive Positive Reviews",
    "originLink": "https://www.phoronix.com/review/framework-laptop-16",
    "originBody": "Framework Laptop 16 Delivers Great Linux Support & Performance, Excellent Customizability Written by Michael Larabel in Computers on 23 January 2024 at 10:00 AM EST. The review embargo has now expired on the Framework Laptop 16, the latest innovative and upgradeable laptop from this company that has made quite a name for itself with modular and user-upgradeable laptop designs for both AMD and Intel. The new Framework Laptop 16 offers even more customizability around the keyboard/touchpad and other options including over using a Radeon RX 7700S graphics module and more. Besides the immense customizability options and upgrades available with the Framework Laptop 16, the new model employs the AMD Ryzen 7040HS processor for even greater performance over the AMD Ryzen 7040U found with the latest Framework 13 model. Since first testing out the Framework Laptop in 2021 and then last November reviewing the Framework 13 with AMD Ryzen 7040U, I've been a big fan of the Framework Laptops. When first announced years ago the Framework Laptop sounded great in theory but ultimately questionable about how well it would work out in practice and whether they'd be able to firmly deliver and stand the test of time. Well, they've been nailing it with execution and continuing to prove themselves with each new iteration and making their upgradeable products all the more exciting and driving greater interest with each succeeding launch. In not reading too much on the Framework Laptop 16 prior to its arrival, I was blown away by the additional customizability available with this 16-inch laptop thanks to the larger form factor while the motherboard, I/O ports, and other upgradeable options with prior Framework Laptops remain available. Framework also continues to make the design files open-source and leverage other open aspects of their upgradeable laptop designs to really offer a compelling solution. Plus, as with the Framework 13, there is nice Linux support. With the prior model there was initially a Linux hiccup causing the BIOS needing an update for the AMD graphics support, but the Framework 16 was in good shape. If you are running a modern Linux distribution with an up-to-date Mesa, Linux kernel, and AMDGPU firmware, you should be good to go. The Framework Laptop 16 is a bit heavier -- fully kitted out it's a bit heavy, one of my few complains with the new unit. Plus in maintaining the slimness of the overall laptop if wanting to use the Gigabit Ethernet port it sticks out from the side of the laptop. The weight (with the discrete graphics module) and breaking the sleak design when using certain I/O ports / upgrades are really the only critiques I've had with the unit in my testing thus far. Pricing on the Framework Laptop 16 starts at $1399 USD for the DIY model or $1699 USD for the pre-built version with Microsoft Windows pre-installed. It's more expensive than the Framework 13 as going for the higher-end Ryzen 7040HS processor plus the additional customizability options thanks to the larger design. The base pricing on the Framework Laptop 16 is with the Ryzen 7 7840HS while the Ryzen 9 7940HS is available (5.1GHz vs. 5.2GHz maximum boost frequency) for an additional $200, there are DDR5-5600 memory options from 8GB up to 64GB (all replaceable DIMMs), up to two NVMe SSDs, the optional graphics module for enjoying Radeon RX 7700S discrete graphics rather than the integrated RX 780M graphics, various input module options from an RGB macropad to numpad, and the various expansion cards. Another great feature worth pointing out with the Framework Laptop 16 is the display's 165Hz refresh rate that also supports VRR and FreeSync. The display is very nice with the Framework Laptop 16 with a 1500:1 contrast ratio and 500nit brightness. 31 Comments - Next Page Tweet Page 1 - Introduction Page 2 - Framework Laptop 16 Linux Support Page 3 - Browser Benchmarks, Graphics, Kernel Compilation Page 4 - Code Compilation Tests, OpenVINO AI Page 5 - AI Llamafile, PyTorch AI Page 6 - Blender, Other Creator Workloads Page 7 - GIMP, RawTherapee, OpenSCAD, Video Encoding Benchmarks Page 8 - Framework Laptop 16 Is Great Page: 1 2 3 4 5 6 7 8 Next Page",
    "commentLink": "https://news.ycombinator.com/item?id=39104363",
    "commentBody": "Framework Laptop 16 Review (phoronix.com)393 points by mikece 18 hours agohidepastfavorite355 comments jakamau 14 hours agoI have the DIY FW13 Intel 11th Gen with Fedora. I've had an overall positive experience and have enjoyed tinkering with it. My only negative with it has been the issue specific to 11th gens where the CMOS drains and eventually dies if the laptop is left unplugged for days-to-weeks at a time. On the whole Framework handled the issue well, there was no permanent fix without soldering or replacing the board. The company was upfront, provided support, a replacement battery, and even published a how-to on modifying the mainboard after the fact. With a start-up I was expecting some bugs. This one was irritating but not a deal breaker. I think how they handled the problem and how they've proven their commitment to upgradeability through the 12th, 13th, and Ryzen boards speaks well of the company. While I am extremely keen on the AMD versions that have rolled out recently, I can't justify the purchase when my current FW13 still works well enough. The FW16 probably isn't for me but I hope it's successful. I really want to see the docking station that can double as an eGPU using the FW16 discrete GPU module. There was a prototype mentioned in passing about a year ago but it's been radio silence since then. I hope the success of Framework laptops and the growing market for gaming handhelds like the Steam Deck makes the modular eGPU concept a little more reasonable. It's still incredibly niche but one can dream. reply Rebelgecko 9 hours agoparentI have mixed feelings. I bought mine hoping Framework would improve the SW (especially firmware and drivers) over time but that hasn't been the case. My 12th gen has issues with abysmal battery life while sleeping (not just the regular Intel 12th gen sleep complaints, but batt life varies greatly depending on which expansion cards you have on the laptop while sleeping eg USB-A vs -C). Framework has been beta testing a FW update to partially improve this since 2022 and last I checked the beta still had side effects like bricking the left USB-C ports under certain conditions. Even though Fedora is (afaict?) the best supposed Linux distro, there's still known issues that have persisted for years like the brightness keys not working (there's technically a workaround but it breaks a different feature that I would like to use) If the laptop as it is today meets your needs, go for it... but one shouldn't buy it assuming that known issues will be fixed later on. reply starkparker 9 hours agorootparent> Even though Fedora is (afaict?) the best supposed Linux distro As someone who had a 12th-gen mainboard and upgraded to an AMD board, and assuming you meant \"supported\" here, the rest of the points here are fine but this one rather explicitly is off. Ubuntu is the primary supported distro across the board: https://frame.work/linux Fedora was recommended for AMD mainboards when Framework started shipping them, because Fedora ships newer kernels sooner, which got upstream AMD compatibility fixes out faster, which meant Fedora users could install Framework's firmware, driver, and BIOS updates sooner with fewer workarounds. When Ubuntu 22.04.3 LTS shipped a 6.2 kernel in August, it went back in front across mainboards. reply zamadatix 7 hours agorootparentThere isn't really anything on that page says Ubuntu is the primary supported distro and Fedora isn't. They seem to both be equally supported with no preference according to both the listings and the wording on the page. reply mespe 7 hours agorootparentprevTyping this on 12th gen running Fedora, and I have none of the issues you describe. I've been daily driving this laptop for 2 years, and my only complaint is the mediocre battery life (I get 5-6 hours with mixed use and around 50% display brightness). I did switch from the glossy to matte display, which was a massive improvement for use on the go. reply Rebelgecko 4 hours agorootparentIf you don't mind sharing, what modules do you have plugged in and how much battery do you lose if you don't touch the laptop for 2-3 days? reply ochoseis 10 hours agoparentprev> On the whole Framework handled the issue well, there was no permanent fix without soldering or replacing the board. The company was upfront, provided support, a replacement battery, and even published a how-to on modifying the mainboard after the fact. IMHO, handling it well would mean recalling and replacing the defective mainboards (so long as they’re in warranty). They must have a ton of brand equity / good will if customers are that willing to roll up their sleeves. reply spartanatreyu 10 hours agorootparent> IMHO, handling it well would mean recalling and replacing the defective mainboards (so long as they’re in warranty). Just remember, this flaw was on the one model right at the start of the company. Recalling the boards at that point (where the company probably wasn't making a profit yet) could have killed the company. Definitely not worth doing. But short of a total recall, they did the next best thing. They released detailed instructions on how to repair a laptop while having the schematics for the laptop be open, and going out of their way to design the laptop in such a way to make repair as easy as possible, and said that self repairs would not affect any warrantees. No other laptop manufacturer would have done that. That's why they are seen as good in the community's eyes. reply baby_souffle 5 hours agorootparent> > IMHO, handling it well would mean recalling and replacing the defective mainboards (so long as they’re in warranty). > No other laptop manufacturer would have done that. > That's why they are seen as good in the community's eyes. Yep. We're not aiming for _perfect_ here. Framework just has to be better than the abysmally low bar set by (almost) every other laptop maker. reply Aeolun 10 hours agorootparentprevIf someone gave me the option between soldering it myself and having it repaired, I’d for for soldering too. I can do that right now, while getting it repaired means sending it back and forth. reply tomrod 6 hours agorootparentprevTheir help process included me taking several pictures and descriptions of wonky behavior for when graphics were glitching. Which was fine at the time, but would have preferred to just RMA the unit so I would have a working unit (we purchased to test for business use). reply tomrod 6 hours agoparentprevYour setup is identical to mine. We tested bringing them in for business use. On one, the graphics would randomly glitch. We replaced every part of the system except the screen and it would still glitch, so I replaced with Thinkpad. Just need a solid system. After two years of sporadic use, our engineer with Fedora+Framework gave it up as it started burning him (2nd degree)! Overall, I liked the experience and the mission, but was sad we've had the experience of glitch+burning. reply 0x49d1 4 hours agoparentprevThe owner of DIY FW13 12 Gen here. The only problem I've faced was fingerprint scanner: it just fell off (I don't know why, I've not done anything crazy to the machine, worked almost stationary). I've just replaced the button with sticky plastic rectangle and that's it: button works, fingerprint - no. But this is a minor issue really: overall this is the best laptop I've had considering that I also can upgrade it more cheaply in the future, I suggest almost everyone to consider this brand when searching for new machine. reply ncallaway 10 hours agoparentprev> I really want to see the docking station that can double as an eGPU using the FW16 discrete GPU module Yes! As another owner of the FW13, if they released some kind of external adapter for the FW16 GPU I'd definitely purchase the GPU and the adapter the day its open for sale. reply nrp 9 hours agorootparentWe showed a proof of concept of exactly this actually at our initial launch event for Framework Laptop 16 last year. It's still something we see as an interesting use case to support, but we don't have a timeline around productization of it. reply ncallaway 3 hours agorootparentWell, I’ll be there when you get to it! I understand it’s a small company, and there must be a million priorities. reply PasteBinSpecial 8 hours agorootparentprevHave you ever considered designing a steam deck / handheld form factor that could take old motherboards from FW laptops? Unsure how viable that would be, but it would help justify that proof of concept (imho). Would be really cool if the GPU could serve multiple devices across generations like that somehow, eventually. Buy one FW laptop, upgrade it, use the old parts for a new handheld, use the old GPU as a dock for that handheld when you get a new one for the laptop. reply locusm 7 hours agorootparentYou mean like this? https://www.hackster.io/news/pitstoptech-s-framework-mainboa... reply schmorptron 12 hours agoparentprevIt looks like the next-gen Zen 5 AMD cpus will be another large leap anyways, so not needing to upgrade now should make you even happier down the line! :) reply 0x38B 11 hours agoprevMy 11th gen Intel Framework is my only PC and has been a great machine. I use it for dev, gaming, and general computing. Integrated Intel graphics goes surprisingly far if you run games at 720p - Skyrim, Dishonered, the Mass Effects, and more recently Battlebit all run well. I'd imagine the more recent Ryzens would be much better. I want the 16 for more screen real-estate and better gaming; I've had family notice my laptop and make positive comments, so I may pass it on when I eventually upgrade. On a related note, I saw someone else with a Framework laptop at a coffee shop the other day here in Alaska - super cool! First time I've seen another Framework user :) reply callalex 4 hours agoparentWhile I agree with you that pc gaming is such a mature space that you can have an entire lifetime of fun with a historical catalog, the games you are describing are brushing up against the category of “retro gaming” at this point and the performance you are describing would be considered poor in the year those titles were released. reply jandrese 5 hours agoparentprev> run games at 720p - Skyrim, Dishonered, the Mass Effects 720p and games that are 12+ years old or are deliberately low poly. That's not the most ringing of endorsements. reply j5155 8 hours agoparentprevFor what it’s worth, that makes at least 3 of us in Alaska with one :) reply tulsidas 8 hours agorootparentunless you were the one he saw, and there are only 2 :) reply nrp 18 hours agoprevI'm happy to answer any questions folks have on this product. reply esskay 17 hours agoparentAre there any plans to offer larger batteries and/or improve battery efficiency? From what I've read battery life is still an issue many seem to be having. With the greatest respect it's 2024, a laptop should be capable of 8 hours at the very minimum for a mid range model but I'm seeing a lot of people getting sub 2 hours. The 16 ticks all the boxes for me but I've held off for now as having the possibility of worse battery life than my old 2015 mac isn't as you can imagine making for a compelling upgrade. reply nrp 17 hours agorootparentFor reference, here is Notebookcheck's review stating 9 hours of real world battery life in web browsing (though on Windows): https://www.notebookcheck.net/Radeon-RX-7700S-performance-de... For Linux, when following our installation guides for power optimization, power consumption with integrated graphics should be similar. Edit: Tom's Hardware similarly reports 9 hours on Windows (with Graphics Module installed): https://www.tomshardware.com/laptops/framework-laptop-16-rev.... reply ParetoOptimal 8 hours agorootparentOne huge caveat that makes me most unhappy with my framework 13 purchase is power draw on HDMI BIOS bug gives me 1-2 hours battery life. As a result, its beginning to resemble a paperweight. reply oDot 9 hours agorootparentprevCan you elaborate more about the tradeoffs here? I'm in the same boat, looking for as light as possible with a much battery as possible, was banking on the 16 yet we get 9 hours instead of (an assumed) ~10-11, and 2.1/2.4kg. Somehow the Thinkpad Extreme X1 and the new Zephyrus G16 have similar CPUs, dGPUs, a >90Whr battery and they weigh less than 2kg. Where's most the weight going? Will I be able to buy a FW 16 and eliminate weight to achieve similar specs? Literally asking about using a \"stripped down version\". reply theossuary 8 hours agorootparentFrankly that weight is from the modularity of the system. To build something repairable/modular requires scaffolding this isn't necessary if you solder and glue everything together; that's why companies like Apple do it. You are giving up some things in order to get the promised repairability of a framework. If you want the lightest laptop with the longest battery life, you'd get a Mac. If you want the best bang for your buck go with a basic Ultrabook. If your willing to have a bit more weight, size and a bit less battery in exchange for repairability/customization, that's the niche framework fits well. reply hecanjog 17 hours agorootparentprevI'm surprised to be reading that, honestly. I have a first batch framework 13 without any upgrades and I spent the summer working with it outside. Nothing super crazy, 4-6 hour stretches in the park, but I never remember cutting it close with the battery. As long as we're sharing anecdotes, I'm happy with the battery life FWIW... Edit: I'm running linux. I don't recall doing any battery optimization but maybe I installed or configured something a few years ago? I don't change things often. Another edit: I just checked and I do have TLP installed! reply Maskawanian 17 hours agorootparentI love framework's mission. I would not change the past on buying a framework. To provide a counter example, quite often I open my bag to find that my Framework 13 has cooked itself in the bag. The battery life, and the bad intel sleep management have been a thorn in my side since I got it. But the power management does leave much to be desired. Given LTT's review of the laptop, it may be best to wait for the 2nd revision of this which hopefully will deal with the deck flex and screen consistency issues. reply leeman2016 17 hours agorootparentYou're not alone with the cooking laptop in a bag running Linux issue. I had those in the past with Dell/Toshiba laptops in the past. reply seanp2k2 10 hours agorootparentIt's funny but sad how this has literally been an issue for 20+ years and yet still no one can fix it. I've tried with many laptops, both Windows and Linux, and the only one that can reliably go into a backpack with >90% charge and come out after a flight with >80% is a Macbook, and that's also been true for more than a decade. reply callalex 3 hours agorootparentIt does feel like Linux sleep is nearly impossible to do well. My go-to example of jaw-dropping, industry-leading Linux hardware support is the Steam Deck. They deserve lots of praise for the monumental effort and achievement to make that product work as well as it does. The sleep functionality is trash though, even after they put a ton of work in. It’s excellent in that I can suspend any software and have it come back to life exactly the way I expect (no small feat!) and it’s excellent that it never cooks itself when unattended in a case or bag, but it is total trash that it eats 20-30% battery a day while “off”. That’s not a bug, but expected behavior. It’s just not what I have come to expect from a modern computer, and portable Macs haven’t behaved that way since about 2005. reply buildbot 15 hours agorootparentprevDell Windows laptops will also cook themselves. (For example: https://www.dell.com/community/en/conversations/xps/dell-957...) Macbooks are the only device I would trust to not light a bag on fire… reply bombcar 13 hours agorootparentAnecdata but I've had a MacBook do the cooking once, but it thermal throttled itself so it just got warm, not super hot. reply kennydude 12 hours agorootparentYeah I once had my old Macbook Air get insanely hot to the point it felt dangerous. I left it in the sink (dry and in case it blew up it would be somewhere non-flammable) to calm down and it was fine a few hours later. reply pomian 10 hours agorootparentprevIs this going to sleep? Or hibernate? I found that most notebooks(under Windows) are horrible at \"sleeping\", but if you enable hibernate, then the notebook can last days, without losing charge. (Dell, HP, Asus, etc.) reply xarope 6 hours agorootparentprevAnother counterpoint, I have that same not-suspending-thus-cooking-in-my-bag issue often with a thinkpad P14s running windows 10 (intel i7-1270p) . However, my (5+ yrs old?) T480s running linuxmint never does it (i5-8250u). YMMV reply jwells89 17 hours agorootparentprevI think the expectation for battery life for most these days is closer to what can be found on e.g. M-series MacBooks or in the x86 world, HP Dragonfly and some ASUS Zenbooks, which range between 14 and 22 hours. reply AnthonyMouse 12 hours agorootparentBattery life is inherently a trade off against performance and weight. It's also a trade off against competence, because you can kill the battery right quick if drivers don't idle things properly, but even after you address that you still have the other thing. Which makes 16+ hour battery life an odd choice for most people. Who uses their laptop for 16 contiguous hours with no opportunity to charge? It could have been lighter or faster. Naturally Framework has the potential to make it flexible: Have a dual-use bay where one of the options is a second battery, and then if you want it you take the weight/battery life trade off in favor of battery life, and if not the machine can be lighter. reply jwells89 11 hours agorootparentLong battery life isn’t so much about contiguous use as it is about not needing to think about charging as often and being able to not carry a laptop charger brick on excursions. It also lets you still have “normal” laptop life left over after periods of high-intensity usage. Also generally speaking more efficient laptops are cooler which is generally a quality welcome in a device sometimes used in a person’s lap. reply noirbot 10 hours agorootparentYea, the Framework's battery issue for me is mostly that there is almost literally no point I could ever trust not having a power outlet nearby with the cable. I've had times where the computer, while fully shut down, loses 30% of the battery life in a few hours. Sure, it's the bulk of a Macbook Air, but I often leave that open, with the screen on, running VMs for work all day without it being plugged in. I expect that any time I open it, I'll have at least 30-60 minutes of use before I need to plug it in. My Framework, more often than not, tells me it's about to emergency shut down as soon as I log in. reply throwaway2037 8 hours agorootparent> I've had times where the computer, while fully shut down, loses 30% of the battery life in a few hours. Did you report this issue to Framework? Do other users have similar issues? reply AnthonyMouse 8 hours agorootparentprev> being able to not carry a laptop charger brick on excursions. It also lets you still have “normal” laptop life left over after periods of high-intensity usage. This is just the same trade off. For a given level of efficiency you could have put in a smaller battery and offset the weight of a second battery / charging brick that you would then only need when you need many hours of battery life. > Also generally speaking more efficient laptops are cooler which is generally a quality welcome in a device sometimes used in a person’s lap. That isn't really part of the trade off either. The faster ones generally don't have worse performance per watt -- they're often better because they have more cores with lower clock speeds. You get 100% more cores with 75% more performance for 50% more power consumption. For the same load they generate less heat. See also \"race to idle\". They only generate more heat in absolute terms if you put more load on them than the slower alternative would have been able to handle. reply sandworm101 8 hours agorootparentprevMe. Road trips. Bad weather. Airports without charging options. 16h would be a bad day but it isnt outside the possible. Also, power outages and when i just forget to charge overnight. reply emptysongglass 11 hours agorootparentprevI have an 11th gen and have terrible battery life. Always have. I would not describe the Linux support from Framework as sterling but bad battery life remains the same between my Linux and Windows sides if a little better on Windows. reply TkTech 14 hours agorootparentprevBattery is what prevents me from taking my 13 framework with me when traveling. I spend most of my time in pycharm, rebuild docker containers constantly, and have celery + postgres + redis running full tilt. The fan is running constantly, it's always burning hot, and the battery lasts under 2 hours (if I'm really lucky). Previous generation Macbook running the same workload last for 6+ hours. That said, for someone who isn't going to be running every core at 100% all the time, I'd still recommend it. Edit: Oh, and under arch with the latest kernel, I find many monitors that cannot display video output over usb-c, like the Dell U4919DW. reply nathancahill 11 hours agorootparentMildly curious what kind of task requires that usage profile on a laptop (in general but especially while traveling?) reply adastra22 9 hours agorootparentYou don’t dev on a laptop? reply callalex 3 hours agorootparentprevI think you need to clarify your question because the comment you are replying to clearly states the workload they have? reply TkTech 8 hours agorootparentprevWhen I occasionally go into the office it's a ~4 hour round trip by train, and I work during that time. It's simply running a dev environment. reply olddustytrail 11 hours agorootparentprevAt the end of the day, all the work a CPU does turns into heat. If you want less heat you need the CPU to do fewer calculations. If you have processes pegging CPUs at 100% and you don't want them to, then use a cgroup to limit their CPU time. reply callalex 3 hours agorootparent>If you want less heat you need the CPU to do fewer calculations. You do understand that different machines can do the same calculations with different amounts of energy, right? reply mixmastamyk 1 hour agorootparentWe all understand battery life isn't framework's strong point. Still, if your dev environment is using CPU constantly you're likely doing something inefficiently. Redis and postgres don't do much unless you ask them to, for example. reply pella 16 hours agorootparentprev> Are there any plans to offer larger batteries and/or improve battery efficiency? discussion: https://community.frame.work/t/simple-extra-battery-for-the-... reply cassepipe 15 hours agorootparentprevVery likely. They already announced recently a better battery for the 13 models so I don't see why they wouldn't if they have the opportunity reply unethical_ban 12 hours agorootparentprevI don't know about my 100-0 battery time. What I know is that suspend on Fedora/Framework is *not* valid for overnight or longer sessions. My battery will drain 50% in two days on suspend, vs weeks for a MacBook. I now make sure to save my work and shut the laptop off completely at the end of a session. reply 8f2ab37a-ed6c 10 hours agoparentprevHow's out-of-the-box support for multi-monitor setups with Ubuntu? How about supporting higher refresh rate external monitors? How does the trackpad feel compared to what people are used to with Apple devices on MacOS, is it at all comparable? I remember that being my least favorite part of using Linux laptops like what System 76 would put out. How's the device build? Plasticky? Flexing? Once again, I dreaded that about the System76 machines, which IIRC are just rebranded Clevos/Compals. reply judge2020 10 hours agorootparent> How's the device build? Plasticky? Flexing? From the LTT review (note that Linus has personally invested in the company, but says he did not review the team's script) at timestamp[0]: > The keyboard on the Framework 16 is unacceptable at this price [...] wasn't able to get up to my full typing speed in particular while doing really quick double presses due to the chassis flex, a huge disappointment compared to the keyboard on the Framework 13 > The problem is the midplate [..] with the keyboard off, you can just see how not supportive this super thin piece of metal is. Fortunately, this being a framework, disassembling only takes about a minute and we were able to chuck some thermal pads in the areas where it was squishiest, and.. holy crap, that is so much better. > The amount that the chassis flex influences the feel of the keyboard is huge and the difference that this mod has made is night and day. With the thermal pads in there, this keyboard isn't quite on the same level of Asus or Alienware but comfortably beats MSI and Razor and bring it much more in-line with the price. 0: https://youtu.be/eUCm4wKarpQ?t=204 reply depressedpanda 16 hours agoparentprevWhen will it be available in Sweden? I followed the prompt on the website and signed up for the newsletter, but I got so much marketing spam that was totally irrelevant to my one and only question that I had to unsubscribe. reply KomoD 13 hours agorootparentCurious as well > but I got so much marketing spam that was totally irrelevant to my one and only question that I had to unsubscribe Make sure you don't tick the \"Stay up to date with Framework newsletters\" checkbox, I didn't and haven't received any marketing reply theshrike79 15 hours agorootparentprevSame question but for Finland. I’ve got the money here, had it for at least two years. I just don’t have a way to give it to Framework reply finnjohnsen2 11 hours agorootparentSame question, same frustration, from Norway reply throwaway2037 8 hours agorootparentprevMy guess: The real problem is warranty returns by mail. reply born2discover 10 hours agorootparentprevSame question but for Switzerland. reply lousken 14 hours agorootparentprevrest of the EU is supported via reshipping https://knowledgebase.frame.work/en_us/eu-unsupported-SJByUb... reply theshrike79 13 hours agorootparent\"Supported\" is a stretch. > All warranty service will require a shipping address within one of the EU countries we ship to. So if I need to warranty something, I need to figure out how to have a shipping address in, say, Germany? Easy! Oh and if the 2.5k€ laptop gets damaged by a 3rd party shipping company, I'm on the hook for it. Yea, I'll wait. I can't figure out why it's so hard to ship stuff within the EU. The whole purpose of the European Union is to make moving products easy inside its borders. reply KomoD 13 hours agorootparent> I can't figure out why it's so hard to ship stuff within the EU it's not, at least not in my experience. reply folmar 11 hours agorootparentShipping is quite easy, especially for small and expensive things like laptops, where adding cost of DHL/DPD shipping is not a major deal. Tangentially, as they are doing it already, selling _to consumers_ is quite hard. Each country has different regulation placing a lot of obligations on the seller. reply ffgjgf1 2 hours agorootparent> Each country has different regulation IIRC they are fairly consistent across the EU (simplifying cross border commerce is one of main reasons it exists) reply KomoD 10 hours agorootparentprev> Each country has different regulation placing a lot of obligations on the seller. Such as? Just curious reply selectodude 9 hours agorootparentWarranty terms, language support, needing a legal representative in every country, to start. reply theshrike79 2 hours agorootparentI'm a 1000% sure the place I order my stuff from in Germany doesn't have a representative in Finland. That's the whole damn point of the EU. I think there might be some regulation on having the instructions written in the native language, but that's like 100€ to a translator for an one-time job. No laptop comes with an extensive manual. Warranty terms are EU-wide too mostly, there are some country-specific exceptions, but nothing that would require a huge legal team to handle. reply emptysongglass 11 hours agorootparentprevWhat's the best reshipping service for the rest of the EU? reply natrys 17 hours agoparentprevAny chance we can get Japanese keyboard? You asked for feedback on it a while ago: https://community.frame.work/t/request-review-of-korean-belg... (am not even Japanese, just have bad habit of using thumb clusters a lot, and this is the only layout with small space bar) reply nrp 17 hours agorootparentThe keyboards we requested artwork feedback on are all ones that we're working with our keyboard supplier on, but we don't have specific availability timelines that we can share. We certainly make sure to have the main keyboards for a country available before we launch there, but we've also in many instances launched a keyboard language far ahead of time. reply csdvrx 10 hours agorootparentprev> (am not even Japanese, just have bad habit of using thumb clusters a lot, and this is the only layout with small space bar) Same, the 106/109 keys JP layout is wonderful! I use the Ro key as a quick desktop toggle (right next to PageUp on my Thinkpad keyboard) and the Yen key as a full-word erase (like Ctrl-W on bash) The extra left and right thumbs are also extrely helpful to have Home and End in an easy-to reach position. reply m463 9 hours agorootparentthese are interesting hacks, glad you let people know about this possibility. reply csdvrx 9 hours agorootparentIt is a very nice keyboard layout. I'm very happy I discovered it (even if it was by mistake!) Another example I can think about: remap the Ro key to toggle between international layout while it's pressed, so you can switch between layouts with a key right next to the shift key. For example, on a US/RU settting, press Ro while pressing the 6 keys to the right of Tab in a sequence and you would get ЙЦУКЕН, don't press Ro and do the same: you would get QWERTY. Mixing and matching in both upper and lowercase would be very easy. reply mixmastamyk 54 minutes agorootparentCould you make the key under the C act as a Cmd/Ctrl key like a Mac? reply seusscat 18 hours agoparentprevNo questions. But I just want to say congrats on getting another SKU out. I'm a very happy customer of an AMD FW13. The product has been pretty excellent and I absolutely love owning a notebook that I feel like I really \"own\". reply lawn 16 hours agoparentprevBeen patiently waiting here in Sweden to be able to order one. You got an AMD option now, this is the last hurdle. (An ergonomic and programmable keyboard ia a dream. Maybe I could mod it myself.) reply nekoeth0 16 hours agoparentprevAny plans of doing OLED displays? It's the only reason why I got my XPS13; but if given the choice, I'd have jumped immediately to the F13 OLED. reply m463 9 hours agorootparentIs OLED really that interesting? I've watched OLED tv's and in rooms with any sunlight the dark parts become unusable. I've also seen reviews for OLED computer monitors, where people are annoyed that the screen will auto-dim at inopportune times. reply jseliger 8 hours agorootparentIs OLED really that interesting I'd say \"good\" rather than \"interesting\" and yes, it is, particularly for anyone who likes dark mode. reply alwayslikethis 17 hours agoparentprevHi - Linux still has spotty fractional scaling support. It would be nice to have a display option that is either usable without fractional scaling (1920x1200) or one that works with 2x scaling (3840x2400). reply ctsdownloads 15 hours agorootparentIt's on the road to improve, as GNOME continues work here. KDE users have reported a cleaner look overall for applications that are not displaying in a way folks would like to see. Fedora Workstation for example, keeps folks to a 100 or 200% scale due to this, but it can be enabled easily enough if on e prefers. This is an area of active development that will continue to see improvement. reply alwayslikethis 13 hours agorootparentNote GTK3/4 doesn't have true fractional scaling. What it does is scaling up to 2x and downsample to the required resolution. This is not that visible but it does produce some blur, along with the performance overhead. Qt5 doesn't support it either, though Qt6 does. Of course we'll see improvement, but I doubt it would get to the same level as Windows in the next 5 years or so, which is a long time. reply vetinari 11 hours agorootparent> This is not that visible but it does produce some blur At these pixel sizes (except 125%), it is irrelevant. MacOS and iPhone do exactly the same for years, and nobody was ever bothered. > along with the performance overhead. By the increased resolution of the framebuffer; the scaling itself is done by hardware (and here I do not mean GPU; I mean output encoder). reply alwayslikethis 10 hours agorootparent125% and 150% are really common for available laptops though. The former for the 1920x1200 13-14 inch laptops, the latter for 2560x1600 15-16 inch laptops. iOS doesn't do fractionally scaling, only 2x, and Macs are all designed to run at 200%, though you can set it to 175% or 150%. MacOS looks equally terrible at 125% or 100% (this is due to the lack of subpixel rendering, but I digress) reply vetinari 13 minutes agorootparent> iOS doesn't do fractionally scaling, only 2x, Oh, it does. The framebuffer is integer scaled, but the physical display has different (lower) resolution. As I wrote above, this mismatch is handled by the output encoder. > Macs are all designed to run at 200%, though you can set it to 175% or 150%. Macs since around 2016 ship with ~175% default. Again, the framebuffer is integer scaled (make a screenshot and see for yourself), and then fit to the display with lower resolution. They do not support 150% or 175% though; these numbers are never shown in UI, just some description like \"more space\". This is not just Apple-esque hiding of everything that might sound technical; they really do not support 150% or 175%. In reality, it is more like 177,78% or 152,38%; they get something out of it, but that is a different topic. It is exactly the same approach that Gnome / Mutter uses for fractional scaling. Except that Mutter did the mistake with exact scales. reply sroussey 7 hours agorootparentprevOh, iPhones do fractional scaling now, and have for years. The display has gotten much more pixel dense. It is not reported to the developer or the user what the real resolution is. reply tristan957 16 hours agorootparentprevLinux support will definitely get better within the next year, but yeah this is my biggest pain point. Electron and other XWayland apps look terrible. Is there some environment variable that I can set to launch Electron apps with the correct command line arguments for Wayland? Edit: Looks like I found what I am looking for: https://www.electronjs.org/docs/latest/api/environment-varia.... Electron apps are still blurry though. Hmm... Also, seems like I need to expose something to Electron apps that are packaged in Flatpak. Needs more research. reply alwayslikethis 16 hours agorootparentUse a better DE. KDE allows not scaling xwayland apps, and Hyprland does too. Just set Xft.dpi to let it scale the old fashioned way. Electron can be started with wayland but it's often buggy. reply tristan957 15 hours agorootparentGNOME meets all my needs except this one, so I think I will stay here. Definitely looking for a less drastic solution. reply vetinari 11 hours agorootparentprevMany electron apps either ignore the ozone flags, or straight up ship with ancient electron, that doesn't support ozone/wayland at all. The only way forward is bug the maintainers of the apps to do the right thing and support wayland properly. Electron itself does. reply tristan957 4 hours agorootparentSlack seems to work great with Wayland, so props to them. VSCode and Discord were giving me trouble however. Thanks for your insight. reply jckahn 17 hours agoparentprevHello! I’m in FW16 batch 4 and excited to support your product and company. Thanks for driving innovation in this space! I’m holding out for an NVIDIA GPU, or at least some hardware to accelerate running local AI models. Do you have any expansion bay plans for that? reply kinow 16 hours agorootparentYeah, me too, waiting for an NVIDIA GPU for Blender (AMD and Intel GPUs perform really badly with Blender) reply chris-orgmenta 17 hours agoparentprevHi there, Do you think that you will move from batches to continuous production the next few years (I understand this is difficult logistics)? If Frameworks were available with 3 day shipping, then I would be able to recommend them to people more successfully. Also out of interest, do you mind sharing your stance (and estimates for current and future demand) for touchscreens? Presumably you're estimating low demand for this at moment since it's not a priority. Would be great to see stats on this (since at the moment it's mostly just informal discussion / passing comments in the community/forums) Thanks in advance. Appreciate everything you've all done. reply nrp 17 hours agorootparentWe're in stock on both Intel and AMD Framework Laptop 13 configurations, and they ship from inventory within a week. We start out each new product under a pre-order system, but then move into normal production and ordering once we fulfill all pre-orders. reply chris-orgmenta 16 hours agorootparentOK great, thanks And appreciate that any expectations forFrom my personal use, it's a wash at best—some keys are easier to reach, and others are harder I agree. > the layout promotes ulnar deviation even more than does a standard keyboard Hard disagree. You may choose to hold your wrists in such a manner that this is true, but that's on you. I'm a touch typist, and I switched to ortho partly because it makes a lot more sense for touch typing. Touch typing is taught in columns, and when those columns are slanted like this \\ there really is no justification. So ortho lets me scratch that itch to fix the keyboard. reply BadHumans 15 hours agorootparentprevThere will probably never make one. I'd be surprised if even % of laptop users even know what an ortholinear layout is. You have a better chance of an aftermarket one. reply juujian 18 hours agoparentprevI hope with the new modular keyboard tray we will get some decent option. I would only buy it if there was one with a track point tbh, third party or otherwise. reply jwells89 17 hours agorootparentI’ve been hopeful for more keyboard options on Frameworks too. Ideally I’d like HHKB layout with trackpoint and accompanying trackpad module with three buttons at the top, but would be happy with only HHKB layout or only trackpoint. reply gary_0 16 hours agorootparentprevI dream of having a laptop with a standard tenkeyless layout. If I end up buying a Framework at some point I might build my own TKL keyboard, or pay handsomely to have one custom-built, such is my burning desire to keep my existing muscle-memory. reply paulmd 16 hours agorootparentYeah I’m not a fan of the numpad on laptops. If im doing serious data entry I’ll get out a numpad (Keychron C1) or a full keyboard. And it actively reduces the space for the rest of the KB layout, speakers, etc. But as you say, the cool part about framework is that everyone can customize it to their own tastes. reply genman 17 hours agorootparentprevCertainly the colorful led backlight must have been the highest priority. reply ijhuygft776 16 hours agoparentprevI never used a laptop keyboard that I liked (I currently have a thinkpad)... probably because I'm used to these old style IBM keyboards with numeric keypad https://m.media-amazon.com/images/I/71UJ8OXnZjL._AC_SL1500_.... reply m463 9 hours agorootparentthey used to be so much better. I typed on a friend's ancient powerbook and the keyboard made a lasting impression on me. I found the sculpted keys to be smooth and comfortable. This was when flat keys were first coming out. I think this one: https://upload.wikimedia.org/wikipedia/commons/3/31/Wallstre... reply ijhuygft776 12 hours agorootparentprevto me, it would make a lot more sense to replace the lines of numbers on top of the keyboard with the numeric keypad (instead of getting rid of it)... it is so much easier to type numbers with it.... reply pachico 18 hours agoparentprevI personally never experienced a keyboard as comfortable as my fw13. I am coming from XPS ones and never tried ThinkPads, thought. reply Hamuko 17 hours agorootparentThe LTT video says that the 16-inch Framework has a much worse keyboard than the 13-inch one. reply nrp 17 hours agorootparentThe physical key mechanism is the same as on the Framework Laptop 13 (the same tooling too). The LTT video noted that they found flex in the mid plate that the keyboard rests on, which we are investigating. reply happymellon 17 hours agorootparentprevDo you know what is the travel on the 13 and 16 Framework keyboards? reply nrp 16 hours agorootparent1.5mm key travel. reply m463 9 hours agoparentprevI would love something like that. Something with curved keys that fit the contour of your finger instead of the \"everybody-copy-apple\" flat key nonsense. Curved keys are more comfortable, putting even pressure on your finger instead of high pressure in one place. They also give tactile feedback to help center your finger on each key, making typing more accurate and faster. reply dartharva 17 hours agoparentprevDoes a current-gen laptop featuring an X201-like keyboard exist? Has such a thing even existed in the last five years? reply Liskni_si 22 minutes agorootparentApart from the 51nb stuff from China (https://www.xyte.ch/mods/x210-x2100/ and the like), the ThinkPad 25 is the closest official thing, but it's over 6 years old now (released in October 2017, but mine's still going strong and I haven't even replaced the batteries yet). reply RomanPushkin 16 hours agorootparentprevYes, through enthusiasts from China. They make their own motherboards and install modern components. There is no warranty though, and you need to wire them funds, there is also no returns. It worked for me though. I have 64GB ram, 2 SSDs 1TB each. It's mostly silent after I configured things in BIOS. The battery life still sucks. reply binkHN 17 hours agorootparentprevLenovo continues to reduce key travel in favor of the more popular thin and light crowd. reply wazoox 17 hours agorootparentcase in point : my 2020 Lenovo Ideapad keyboard looks like this: https://demo.intellique.org/nextcloud/index.php/s/6jJ3r4brrj... reply binkHN 17 hours agoparentprev> ...modded X201 with updated ... motherboard... How far are you getting with these updates? I agree the keyboards of old are legendary, but at some point your productivity is negatively affected by legacy CPUs and related. reply LeifCarrotson 17 hours agorootparentThe X201 mods are pretty comprehensive, the 51NB team and others like XY Tech have commissioned entire replacement motherboards that carry i7-8550u or i7-10710u processors. See [1]. And the old 35W processors in some of these laptops will still deliver solid performance. The new ones will turbo higher, but throttle quicker. You get higher default clock speed and better thermals with the older processors. Of course memory bandwidth and peripheral connectivity are better with newer processors, but they're perfectly usable. If what you want to do to be productive is edit text and maybe render HTML and browse files, and your OS/IDE/browser does not place unnecessary demands on your computer, then an X201, especially modded, cannot be the blame for your lack of productivity. https://www.xyte.ch/ reply RomanPushkin 15 hours agorootparentprev> How far are you getting with these updates? I can run Slack native app and the fan doesn't kick in! Kidding, but I have Slack running, and containers, and IDEs. Everything. My CPU is pretty decent, 64GB RAM, 2 SSDs, 1TB each. This is modded X201 from enthusiasts from China. These laptops are somewhat niche, but you can buy them. You will run all the software you need, and even more. The only downside is battery life, I hope we'll have new types of batteries some time soon, so it won't bother me too much. reply tempest_ 17 hours agorootparentprevIt really depends on how you develop. For some people laptops never have enough power and are essentially a thin client to something else. reply vehemenz 12 hours agoparentprevIt would never happen, but I'd like to have a no-keyboard option. I never use my laptop keyboard because I have an HHKB, rendering it unnecessary. Instead of a keyboard, I'd like a flat surface where I can rest my HHKB and maybe an L-shaped connector to plug in the USB-C. reply m463 9 hours agorootparentI wish that portable form factor like the dolch pc would come back. monitor part of the case with a fold out detachable keyboard. this is a terrible example of what I mean: http://www.computinghistory.org.uk/userdata/images/large/PRO... but imagine something like that with a more modern take. EDIT: ok here it is: http://portexa.com/wp-content/uploads/2018/04/FlexPAC-III-23... look up \"flexpac iii\" reply tadbit 11 hours agorootparentprevSounds like you want Framework to release a mini pc reply silon42 17 hours agoparentprevYeah... I'd want a full height mechanical keyboard myself. The current offering is missing International English ISO - Linux variant for me. reply leetharris 17 hours agoparentprevI have not used a Windows laptop outside of Razer and maybe Surface in many years. How do those Thinkpad keyboards compare to new Macbook Pro keyboards? I've always found the Macbook Pro keyboards to be \"fine,\" but I'm curious if I am actually missing out on a better experience. reply rollcat 17 hours agorootparent> I've always found the Macbook Pro keyboards to be \"fine,\" [...] Have you tried the previous, \"butterfly\" generation? It should've been criminal to ship that. reply Cu3PO42 16 hours agorootparentI have and I think it's fine. The newer ones are better, but I never understood the enormous dislike for it. IMO the pre-butterfly keyboard is still better than even the current iteration. reply patrec 16 hours agorootparentprevPersonally, I think the butterfly keyboard is great. Assuming, of course, that you got one that does not suffer mechanical reliability issues. This opinion is hardly universal but seems to be shared by quite a few good typists. I'm typing this on a fancy mechanical keyboard, because I care quite a bit about keyboard quality (and consequently hate most laptop keyboards). So it's not that I just don't know better. reply RomanPushkin 15 hours agorootparentprev> Macbook Pro keyboards to be \"fine,\" but I'm curious if I am actually missing out on a better experience Yes, you are missing a better experience. You might try out Bluetooth X201-like keyboards built by enthusiasts from China, if you can buy them. I tried to contact with no luck though. But it can be a good DYI project, if you have one for sale, let me know. Here is the link https://www.taobao.com/list/item/615448425938.htm They 3D-print the body, and use keyboards from Thinkpad laptops. reply voytec 14 hours agorootparentprev> I've always found the Macbook Pro keyboards to be \"fine,\" I haven't bought a (non-phone) computer made by Apple since, and due to, the MBP 2017 keyboard fiasco. It was the single most shitty and unusable keyboard I have had a displeasure working on, and I've owned some super cheap entry laptops in the past. To say that it was crap, trash, garbage, joke, spit in consumer's face feels like a laughably low effort on expressing how useless it was. I bought this laptop as a pre-order before general availability and reported issues to Apple before all the media shitstorm began. Support reps have expressed their concerns and assured me over the phone that it has to be an isolated problem with an individual unit. I've sent the laptop for servicing within a 2 weeks since purchase. They replaced the keyboard and, for some reason, also the mainboard and screen and sent it back. Exact same issues but before I got the mostly-replaced device, issues with the butterfly keyboard already caught media attention and it was a hot topic. I was traveling with a laptop and a mechanical Keychron keyboard, ffs. I used external keyboard during flights, at Starbucks and in other public places. Any attempt to use built-in butterfly joke of a keyboard was so frustrating that carrying an external keyboard was a reasonable trade-off. Apple later parted ways with $50 million to settle the class-action. US-based buyers got up to $395 (I got nothing in EU) but still ended up with non-resellable devices. Jony Ive probably got a great bonus for this stunt. I'm happy for you that you have experienced only \"fine\" keyboards on MBPs. Would I have enemies, I wouldn't wish the misfortune of having to use MBP 2017 on any of them. reply whalesalad 17 hours agoparentprevHow long does your battery last? reply christkv 15 hours agoparentprevLooks like the keyboard was decent once you put some thermal pads under the keyboard to stiffen it up lol. I like the fact that you can easily mod things yourself though. reply redeeman 18 hours agoparentprevnext [2 more] [flagged] yjftsjthsd-h 18 hours agorootparentYou can just enable the firmware option to swap fn/ctrl. Or use software to make caps a ctrl, which is better anyways. reply cdata 13 hours agoprevJust gonna add my voice to the chorus of folks singing the praises of Framework's laptops. I have used the 13 for the last few years, and I'm planning to upgrade to a 16 soon. As a long-time Linux user, it's been my favorite laptop by far. A great machine, and a zero-compromise experience from a hardware support PoV. In fact, when you consider Framework's standard-setting level of user serviceability, it makes other laptops seem like a pretty major compromise. reply thenobsta 17 hours agoprevI'll chime in and say that I love my FW13. It's a great machine. I got the DIY kit and had my 7yo put it together. It was straightforward with a little help from dad (even installing Ubuntu!). Now we fight over who gets to use it. reply nrp 16 hours agoparentIt's awesome to hear that. A big part of creating Framework was the urge to prevent a world where kids could no longer tinker with their parents' computers. reply ornornor 15 hours agorootparentAny chance you’ll ship to Switzerland soon? Our only options are to ship it to Germany or France, go pick it up, pay import fees and taxes on top of the country’s 20% VAT, and then be denied warranty because delivering to a pick up address voids the warranty. It’s quite frustrating :/ reply grudg3 12 hours agorootparentTagging onto this, I still am unable to buy it in NZ, after expressing interest 2 years ago. reply thenobsta 15 hours agorootparentprevLove it. I'm excited to upgrade the mainboard and to put the old board in a new shell or the cooler master case. We'll see what the kiddo wants to do. Maybe she'll become a gamer and we'll get a 16. Anyway, you've got my total support and $. I get that we're a niche market though. It would be great to see my non-techie friends sporting the Framework Gear -- I think they'd love the pre-built machine, but the messaging steers them to thinking it's only for DIY-ers. E.g. -- My sister would love the FW13 because she doesn't have special performance needs and cares about sustainability, but she got very intimidated buying an MBA, so guiding her through buying a Framework will be some work on my end (...messaging that would land for her would be that Framework solves all your basic problems (browsing, video-chatting, and netflix) and when you need more power you can upgrade for only the cost of the components). reply 2OEH8eoCRo0 16 hours agorootparentprevThat's wonderful to hear. I attribute my innate love of computers to my father making a mess of the kitchen table while working on the family PC. reply nextos 15 hours agoparentprevHow are the thermals and fan noise? reply Asmod4n 12 hours agorootparentAccording to the reviews i've seen its about 20 degrees cooler than other laptops with the same spec, making it way more quiter. The battery time is also several hours longer than compareable laptops of the same size. reply nextos 11 hours agorootparentAnd fan noise at idle / web browsing? reply Asmod4n 1 hour agorootparentthe fans turn on later than laptops of other brands do, so probably silent. but when they turn on they are a bit louder at first, but goes down rather quickly. reply dheera 15 hours agoparentprevI love my FW13 too. I just wish it had more options for components. I can of course pick my own SSD and RAM configuration but that's true of every Lenovo laptop. I wish I could get a 4K screen, an eInk screen, a touchscreen, a Dvorak keyboard, a OLED keyboard, an IMU/GPS/barometer expansion module, a pico projector expansion module, a software-defined-radio expansion module, a larger aperture webcam module, an IR webcam module, a depth camera webcam module, an Arduino expansion module, an IR emitter expansion module that controls TVs, etc etc etc. But nobody seems interested in making this stuff. I guess the community that uses Framework laptops isn't really capable of that level of hardware engineering (me included) and the companies in China doing hardware haven't caught onto making stuff for Framework. reply nrp 15 hours agorootparentWe've seen Microcontroller, SDR, and IR Expansion Cards from the community! We'd definitely love to see some of the even more complex ones though, and expect that as the community continues to mature and grow, some will make it through: https://community.frame.work/c/developer-program/85 reply dheera 14 hours agorootparentYes, I've seen those though unfortunately I'm not a hardware engineer and would have a hard time reproducing them. Even if I could figure out how to get a PCB made from a KiCAD file I wouldn't be able to do the surface-mount soldering without botching everything. I'd love to be able to just pay for them and buy them and then get rolling with software :) reply iguessthislldo 13 hours agorootparentI just got my AMD Framework 13, which I'm mostly happy with, but I'm in the same boat. It's great that's it's possible to attach these devices to the laptop, but the true nature of this seems to be that you have to make these things because there isn't a real market for them beyond what Framework is selling. I did come up with something I would want enough that I'm willing to try to make it though. Taking inspiration from some other modules like it, I'm currently looking into adding this microcontroller with a transparent case: https://github.com/01Space/ESP32-C3FH4-RGB If I'm happy with it, I was going to try to have it show battery status like some other laptops have. This might be possible by wiring up another microcontroller to the SMBus on the battery. I'm still researching that part though. reply ammar2 12 hours agorootparentprev> Even if I could figure out how to get a PCB made from a KiCAD file I wouldn't be able to do the surface-mount soldering without botching everything. For what it's worth, some places like JLCPCB can source and solder SMT components to your designed boards as long as you pick from their available parts library. reply user_7832 14 hours agorootparentprev> I wish I could get a 4K screen, an eInk screen, a touchscreen, a Dvorak keyboard, a OLED keyboard, an IMU/GPS/barometer expansion module, a pico projector expansion module, a software-defined-radio expansion module, a larger aperture webcam module, an IR webcam module, a depth camera webcam module, an Arduino expansion module, an IR emitter expansion module that controls TVs, etc etc etc. > But nobody seems interested in making this stuff. I guess the community that uses Framework laptops isn't really capable of that level of hardware engineering (me included) and the companies in China doing hardware haven't caught onto making stuff for Framework. I relate to your sentiment of wanting tons of modules, but I'm going to disagree on the skills of the community. For some of these options (4k/eink screens for eg), you need such a panel to be available (panelook to search -> aliexpress/ebay to buy). SDRs, IMUs & sensors could be doable relatively \"easily\" if they interface via USB/PCIE. There are lots of hardware tinkerers in youtube, hackaday and of course even here on HN. I'm a mechanical engineer with some electrical/electronics knowledge, and I could likely make a half-decent module if I had the time/money/reason/ethusiasm for it. There's a guy who's turned an iPad screen into the FW13 display (on the framework community). There are probably dozens of people who may read this comment who are far more skilled than I am. If you are very keen to do this yourself, you might want to start with learning 3d modelling and building up from that (from a mechanical side). 3d printing metals with something like SLS is commercially (relatively) easy and (imo) accessible compared to say 10-20 years back. reply vaylian 13 hours agorootparent> I'm a mechanical engineer with some electrical/electronics knowledge, and I could likely make a half-decent module if I had the time/money/reason/ethusiasm for it. I consider myself to be a decent software engineer. But I am inexperienced in the area of creating my own hardware. I guess a lot of things are just knowing how to stick together pre-made components like \"panels, SDRs, IMUs & sensors\"? But how important is it to know circuit design? There are a lot of unknown unknowns. And it is not clear what a good learning path would be to get started with hardware hacking as a hobbyist. reply user_7832 12 hours agorootparent> I consider myself to be a decent software engineer. But I am inexperienced in the area of creating my own hardware. I guess a lot of things are just knowing how to stick together pre-made components like \"panels, SDRs, IMUs & sensors\"? But how important is it to know circuit design? For most of the sensors mentioned in the initial comment, I would assume there are already commercial USB/eDP/PCIE sensors/components, and at least for framework these connectors on the motherboard are clearly exposed. I'd imagine it's possible to have some RJ-xxx sensor needing a RJ-USB converter too, but with the sensor+converter I think it is as easy as \"design a box, throw them in\". HOWEVER - ESD is something I've heard can be pretty nasty and show up where you least expect it to. Proper grounding design would help. Beyond that, I am not aware of any \"major\" issues (there's also general environmental/power noise but ground could help with some of those issues like the \"wonky touchscreen when charging\". Also stuff like debouncing and pullup/down resistors if using buttons). My personal knowledge of circuit design isn't very good, I studied it till high school and had an intro to EE class in uni (which actually wasn't bad, learnt the basics of opamp circuits etc) but I don't remember much of those now. > There are a lot of unknown unknowns. And it is not clear what a good learning path would be to get started with hardware hacking as a hobbyist. I think if you want to get started, the most \"comprehensive\" way would be by looking at syllabus of undergraduate EE programs (or MechE for the physical side), and perhaps going through the relevant/interesting chapters. However the \"easiest\" will be by just building basic circuits - using an arduino, or making a hand-wired mechanical keyboard. Googling issues like \"arduino button changes on its own\" will give results like https://arduino.stackexchange.com/questions/186/button-state... which talk about floating pins. This is as much as I know on this topic (I have \"tried learning more esp. in high speed designs but that's vast), if anyone else has helpful suggestions I'll be happy to learn :) reply dheera 14 hours agorootparentprev> you need such a panel to be available Aren't they available? Just take apart a used MacBook for a retina display, or take apart a Boox 13.3\" tablet for the eInk display. Or just buy this https://www.waveshare.com/13.3inch-e-paper-hat.htm But good luck reverse engineering it. I wouldn't know where to start. reply user_7832 13 hours agorootparent> Aren't they available? Just take apart a used MacBook for a retina display, or take apart a Boox 13.3\" tablet for the eInk display. Yeah for sure, what I meant is that DIYing a fully custom screen with a custom resolution etc isn't very easy :) If you're buying a panel or scavenging an existing panel it's much \"easier\". Of course that hasn't stopped youtubers from designing their own 7-segment LCD panels... (example https://www.youtube.com/watch?v=_zoeeR3geTA or https://youtu.be/ZA5vlDdpbkw) reply thenobsta 15 hours agorootparentprevI get that. There's tons of amazing possibilities and it's fun to imagine them and to hope they come out. I'm going to try to make something with my kiddo for the FW, but of course the focus will be on bonding and helping my kiddo build skills, understanding, and connection. So we might not quite get to putting the e-ink reader on the backside of the lid. reply corethree 13 hours agorootparentprev>the companies in China doing hardware haven't caught onto making stuff for Framework. Don't worry. They will catch on... making and copying not just components but the entire framework. It really depends on how popular it gets. reply nfriedly 15 hours agoprevOne more person chiming in to say that I'm really happy with my 13\" AMD Framework laptop. It wakes from sleep and reconnects wifi before I'm done opening the lid, I can charge it from either side, the screen is beautiful, it's nice and lightweight, the performance and battery life are good enough that I don't think about it. On the down side, the touchpad is a notable downgrade from the macbook I use for work, and the speakers are down-ish-firing and fairly weak. Also, twice in the ~1.5 months I've had it, it has completely frozen for about 30 seconds (even the mouse cursor didn't respond) before hitting a windows bluescreen and then rebooting. No idea what's causing that, but there is some suggestion on the forum that it's the AMD graphics drivers. However, I also have a GPD Win Mini with a similar APU and it's never once frozen like that on me. I'm probably going to switch it to Ubuntu in the near future, so that might sidestep the freezing issue, although I expect it to eventually be sorted out either way. But, again, overall I'm very happy with it. reply mey 13 hours agoparentI am less bullish on the screen of the FW13. The color accuracy at various brightness levels leaves a bit to be desired. Gamma calibration seems to be problematic. My Gen 1 Surface Book, Dell Ultrasharp monitors (that are a decade old and only 99% sRGB), and M1 Macbook Air all have much better displays for accuracy out of the box. The upside is that, if/when FW offers a better screen in the future (with touch?) I feel comfortable doing the swap myself. reply dheera 15 hours agoparentprev> It wakes from sleep and reconnects wifi before I'm done opening the lid My experience was this is only true for \"s2idle\" suspend but that mode causes battery drain of 30% every 8 hours which was way more than I was willing to tolerate. It's okay I guess if you're just commuting and unplugging-plugging or something. But it didn't work for me for leisure travel when I would often go 24 hours without opening my laptop. In \"deep\" sleep mode (which only works with some SSDs) power drain is minimal but it takes upwards of 15 seconds to resume and reconnect to wifi. This is what I use now. I just tolerate the 15 seconds, but I wish it were as fast as a Macbook. reply nfriedly 15 hours agorootparentYou're probably right about that. Mine hibernates after a few hours, and then it does take a bit longer to be ready to use, but I've never noticed significant battery drain in between usages. My SSD is a SK Hynix P41, which I believe does support the lower power states. reply dheera 15 hours agorootparentYes, I use a SK Hynix P31. I previously used a Samsung 980 Pro and that did NOT work with deep sleep. reply radus 15 hours agorootparentprevThis is my experience as well :/ reply davewood 18 hours agoprevhow long will it run on battery? my FW13 12th gen (debian) lasts maybe 2 hours and even in suspend mode it drains way to fast (/sys/power/mem_sleep = deep) reply nrp 18 hours agoparentPhoronix has some power consumption information in their review: https://www.phoronix.com/review/framework-laptop-16/8 For Framework Laptop 13 12th Gen, we have an article on optimizing power consumption (this one is written for Ubuntu, but should largely apply to Debian): https://knowledgebase.frame.work/en_us/optimizing-ubuntu-bat... reply jeffbee 17 hours agorootparentHaha and they say Linux is hard to use! What could be simpler? reply xtracto 13 hours agorootparentYou are not kidding haha, I saw the article and immediately thought the same. I use a Dell Latitude as my work computer and run it with Linux (Mint). It is a constant struggle between battery life, sleep states (lack of), camera not working, bluetooth not working and whatnot. I still prefer working on Linux than suffering the UX trash that is Windows, but god if there was some paid Linux version that prevented all the necessary tinkering :( reply nolist_policy 16 hours agorootparentprevOr just get the Framework Chromebook which works great out of the box. reply ativzzz 17 hours agoparentprevYea I got a FW a few years ago and regret it. For just a few hundred $$$ more I could've gotten an M1. The battery situation makes me never use it. Compare to a macbook when I can just close it and open it up weeks later and it just turns on with plenty of charge still left. Every time I need to use the FW i need to plug it in first or charge it if I want to take it somewhere. Defeats the point of a portable computing device When I travel with it, i need to make sure I shut it down and not just close the lid, or it discharges and cooks my backpack Everything else is fine though reply depressedpanda 17 hours agorootparentProtip: Make sure hibernation works then enable suspend-then-hibernate in logind.conf reply ativzzz 15 hours agorootparentI got the FW when I was between jobs so I had a bunch of time to tinker with it and mess around with Linux configs. Now I have a baby and a job so unfortunately my desire to tinker with config is pretty much 0 at this point reply rmbyrro 13 hours agorootparentprevMaking hibernation work on Linux is the pain. Is it even possible on the Framework? reply esskay 17 hours agoparentprevOOf thats a tough sell, two hours is pretty pathetic. I can't imagine how any laptop maker can be selling a laptop in 2024 with anything shorter than 8 hours and keep a straight face. This surely has to be a software issue, I can't imagine they'd have been silly enough to fit such a tiny battery! reply askonomm 17 hours agorootparentYup, comparing to a similarly priced MacBook that goes for ~20 hours it's ridiculously bad. reply NotSammyHagar 9 hours agorootparentPretty much every other index and amd laptop suffers with poor battery life. I get about 4-5 hours on the chromeos variant of this laptop. reply motiejus 18 hours agoparentprevYou are not alone with both issues. Framework12, i5, NixOS. My milleage is 2.5~3h, but usage is light (vim mainly, often not even a browser). I spent quite some time trying different things to optimize it, but never got more than realistic 3 hours. Happy with other aspects though. reply soulnothing 17 hours agorootparentFrom an 11th gen I get about 6 to 7 with light usage, two to three with any development. It's largely a thin client at this point. Battery health is at 92%. I tried upgrading to the ryzen and when it was good it was really good. I was able to keep a user mode libvirt vm running for dev work and mid brightness under 5W power draw. That used slirp networking, adding a bridge or default nat nic takes up about 2w to 3w of it's own power. But like most windows laptops the suspend mucked things up. Not even power draw while asleep, but when awaking from sleep the power minimum was 10w with it more often at 20w with similar usage. I tried several wifi cards, nvme drives, port configurations etc. Also tried Fedora, Ubuntu and Nixos. On Linux this carries over to the discussion of tlp vs power profile daemon, and soon tund. I saw much better performance and regularity with tlp, but that seems like it's not the path forward. The steam deck shows that suspend can be fixed and done well with decent battery life under linux. reply steinuil 17 hours agorootparentprevI have a Framework 13 12th gen i5 as well, running NixOS, but I definitely get a lot more than 3 hours! I'm usually running some terminals and Firefox. I definitely had to play with powertop a bit and remove some programs that consumed a lot of battery (for example, the blueman tray applet had to go). I'd recommend setting powerManagement.enable = true and powerManagement.powertop.enable = true, and letting powertop run in the background while on battery for a few hours to identify the worst offenders. This is my configuration: https://kirarin.hootr.club/git/steinuil/flakes/src/branch/ma... reply motiejus 1 hour agorootparentYour flake is very helpful, thanks! I cargo-culted tlp configuration, will see how it goes. > and letting powertop run in the background while on battery for a few hours to identify the worst offenders. How do you do the analysis after running powertop in the background? reply whalesalad 17 hours agorootparentprevMy M2 air will outlast the workday. Obscene battery life. I can’t imagine using a Linux notebook after this level of performance. reply smoldesu 17 hours agorootparentAs long as you don't use Docker. My last job gave me an M1 Air for container debugging and devops, and it was a comically bad fit for the task. I ended up going back to my cheap x86 Linux host for most of the dirty work, just because it ran cooler. Now, if someone could find me a native Docker host that lasts all day... then we're in business. reply whalesalad 17 hours agorootparentI run all the heavy stuff on a remote dev node. My laptop is just vscode+ssh, tmux in iTerm, a browser, Spotify and slack. reply smoldesu 17 hours agorootparentThe wave of the future! Half the people in my team did that, I didn't feel right about the EC2 costs. To each their own, I guess. reply whalesalad 17 hours agorootparentI mainly dev on my home machine, a 13900k linux desktop. When not at home I utilize tailscale to get remote access. Was just in Vegas for a week and it worked great. I plugged my laptop in once to charge. reply nolist_policy 16 hours agorootparentprevOr just get the Framework Chromebook and get the best of both worlds. reply NotSammyHagar 9 hours agorootparentI have the chromebook, it's great. So easy to do linux stuff. I do wish the cpu scheduler or something related to chromeos could be improved for heavy cpu use. If I run firefox in the linux env, and I open a bunch of tabs, the fan spins up a good amount. It would be soooo much better if there was a way to tell chromeos to use a max amount of resources temporarily, like opening a web browser with lots of tabs. That's my only real complaint, when I overload the system with work. reply ctsdownloads 18 hours agoparentprevThis AMD config uses s2idle and the battery life very much depends on the usage itself. Ideally, running UMA is going to yield a longer life than say, running from dGPU. For gaming, we have folks using dGPU only as needed. Provides choice. reply CarVac 17 hours agoparentprevTwo hours?? I get 6 with Ubuntu on my 1240P Framework, and that's with the BIOS limiting the battery to 80%. reply nrp 17 hours agorootparentWith 12th Gen, 6-7 hours looks like around what we'd expect for normal, real life usage on Linux. With 13th Gen or Ryzen 7040 Series, we've seen even better, e.g. (though on Windows for this reviewer): https://arstechnica.com/gadgets/2023/05/review-framework-lap... reply stebalien 17 hours agoparentprevMy FW13 AMD laptop (61Wh battery) can last 11hr+, technically. If I'm doing anything other than light web browsing, that quickly drops to 8hr. If I'm watching videos, it's more like 5hr. Unfortunately, at least on Linux, it requires quite a bit of tuning for the moment. But there are some pretty good guides. Suspend battery life still isn't great, but it's _much_ better (with s2idle supported) on the latest-gen AMD platform. I previously had the 11th gen Intel and... I got much better battery life than you, but it was still pretty bad. reply aquova 17 hours agorootparentThis is really interesting to me. I too have an 11th gen Intel machine running Arch, and while I get better battery life than 2 hours, it's still the weakest part of the system, and I very rarely put it to sleep, I just turn the whole machine off. Someday I was planning on upgrading to the AMD motherboard, but didn't really see a reason to do so yet, but this might accelerate my plans. reply stebalien 16 hours agorootparentYeah, sleep on the 11th gen is basically worthless. But the battery upgrade (especially after a few years of wear and tear) and the new AMD board are worth it. ... unless you watch a lot of video. Hardware video decoding uses more power than software video decoding in many cases: https://gitlab.freedesktop.org/mesa/mesa/-/issues/10223 reply hakcermani 17 hours agoparentprev.. same here FW13 12gen .. fast battery drain even in sleep mode. It is reported due to the expansion cards that I guess cannot be turned off ? reply seabrookmx 18 hours agoparentprevI get 3-3.5 under Fedora and the 'Power Saver' gnome power profile, but yeah it's not great. reply ctsdownloads 17 hours agorootparentMake sure you are on the latest PPD - this matters and provides a noticeable improvement. https://copr.fedorainfracloud.org/coprs/mariolimonciello/pow... and you are on at least kernel 6.6.12. reply jcastro 17 hours agorootparentI'm running with this setup and getting at least 5h, I haven't measured it recently but it's definitely makes a difference! reply Filligree 16 hours agorootparentprevThe defaults should work. What’s the point in buying a Linux-branded laptop if they can’t get the basics right? reply seabrookmx 14 hours agorootparentIt's not Linux-branded to be fair (they label it as a \"DIY\" device that ships with no OS), but yeah.. I'm not really one to deviate too far from the defaults on a device I use for work. reply progval 12 hours agoparentprevAnother data point: my FW13 12th gen, also on Debian, reaches 6 hours. I didn't tune anything other than cap the CPU to 2GHz in order to avoid fan noise. reply Analemma_ 17 hours agoparentprevI would really like to switch from my M1 MacBook to a Framework Laptop, but the battery life difference being almost an order of magnitude makes it a complete non-starter. I like Framework, but this needs to be at the absolute top of their priority list to the exclusion of almost everything else. reply binkHN 17 hours agorootparentFramework's hands are tied; the battery life with Apple's CPUs simply can't be touched by the biggest players like Intel and AMD. reply askonomm 17 hours agorootparentTouched maybe not, but it's almost 20 hours worse. Surely it doesn't have to be THAT bad? reply p_l 17 hours agorootparentIt's also a Linux issue that was ignored for years by users and done developers who instead pushed where possible reenabling clunkier older operating modes. My understanding is that one of the reasons Linux on M-series macs doesn't have the problem is that Asahi team doesn't take similarly crappy attitude. Also, the issue appears to show up on other 7040 Ryzen laptops, so I hope this finally gets us proper \"modern sleep\" support instead of instructions to disable it in firmware setup. reply binkHN 15 hours agorootparent> the issue appears to show up on other 7040 Ryzen laptops, so I hope this finally gets us proper \"modern sleep\" support instead of instructions to disable it in firmware setup. My ThinkPad has modern sleep support for the 7840U; sleep and wake are nearly instant with very little battery use while sleeping. reply prewett 16 hours agorootparentprevApple's 22 hr rating for the 16\" MBP is a maximum for a niche task, it's only (up to) 15 hrs of \"wireless web\", which is a more typical usage and would only be about 12 hrs worse. All that configurability of the Framework takes up space, so its battery is 15% less (85 W/h compare to 100 W/h for the MBP). The MBP has a CPU and instruction set that was optimized for low power from the beginning, compared to x86 which has 40 years of ad-hoc cruft and assumed wall power in the beginning, so it may not even be possible to implement the whole instruction set in low-power. (Intel tried, and did not succeed. Could be BigCO ineffeciencies, but could also be that it just isn't realistic.) But Intel/AMD can't switch architectures like Apple can, because they don't control the software. There's no guarantee that the buyer of a hypothetical improved instruction set Intel CPU has access to a Rosetta program (even if Intel had the imagination to do that). On top of that, Apple has been optimizing that CPU for 15 years, and is has access to the leading node. Additionally, (presumably because of the lack of legacy cruft) Apple has space on their die for huge caches and the GPU. On-die GPU eliminates power consumption due to an additional discrete component. Large caches also help things go faster, which means the CPU can drop down to low-power mode quicker. Since Apple owns the CPU, it can customize the CPU for its needs, and it has relentless optimized for low power consumption, even to the extent of putting in a few new instructions for the OS. Apple owns the OS, so it can have all kinds of power-saving features that a mass-market OS like Windows cannot feasibly implement. It is not in Microsoft's interest to take advantage of every little power savings a motherboard manufacturer might add: extra complexity (= bugs and maintenance costs) with no extra revenue potential. Linux has a similar problem, and additionally there are enough problems needing attention that I expect power optimizations beyond the big ones just do not have the interest / resources. For instance, if a 5% improvement would require a large kernel / driver refactor, I suspect it's a hard sell. Plus, macOS doesn't need to support anywhere near the number of configurations that Linux does, so it probably is less effort to do. So all those 5% increases that Apple can do add up. Then there is the aspect that Apple can tune its OS for power saving. Update Cocoa to save energy and everyone's app uses less power. I expect GTK and Qt have other more pressing problems. On top of that, I expect Wayland and especially Xorg are not designed with minimal power consumption in mind. Etc, all the way down. That said, 2.5 hrs does seem like it could definitely be improved. reply celrod 15 hours agorootparentI would love to see a Snapdragon Elite X in a Framework. reply zilti 17 hours agorootparentprevI'm getting at least 8 hours out of my AMD framework. reply 63 more comments... GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The Framework Laptop 16 is receiving positive reviews for its strong support of Linux, high performance, and customizable options.",
      "It features the AMD Ryzen 7040HS processor for improved performance and offers various upgrade options.",
      "The laptop's design files are open-source, and it is compatible with modern Linux distributions. However, it is slightly heavier and some I/O ports may impact its sleek design. The pricing starts at $1399 for the DIY model and $1699 for the pre-built version. Overall, the laptop is praised for its features and display quality."
    ],
    "commentSummary": [
      "The Framework Laptop is discussed, covering topics such as CMOS draining, supported Linux distros, battery life, availability, keyboard preferences, hardware upgrades, and comparisons with other laptops.",
      "Users appreciate the company's commitment to upgradeability and customization.",
      "Some concerns and suggestions are raised, specifically regarding battery life and power optimization."
    ],
    "points": 393,
    "commentCount": 355,
    "retryCount": 0,
    "time": 1706022733
  },
  {
    "id": 39102002,
    "title": "Godot Engine Offers Free Nintendo Switch Port",
    "originLink": "https://www.rawrlab.com/godot_nintendo_switch_free_port.html",
    "originBody": "Announcement of a free Godot™ engine port for Nintendo Switch™ We are pleased to announce the availability of a free Godot Engine port for authorised Nintendo Switch developers, supporting projects made with versions 3.5.x and 4.1.x. This port represents a collaborative effort, co-authored by multiple members of the forums on the Nintendo Developer Portal. It underscores our commitment to make Godot Engine more accessible to a wider range of creators. Key features of this Godot Engine port: · Complimentary access: Available at no cost to all authorised Nintendo Switch developers. · License: The source code is distributed under the MIT License, offering broad usage and modification rights. · Software provisioning: The software is provided \"as is,\" without any implied warranty or support of any kind. · Basic functionality included: This port includes only basic functionality. · No C# or GDNative/GDExtension: Only GDScript is supported. Native extensions are not supported, but you can try to convert them into internal modules. · Optimization Level: It is important to note that the port is not highly optimized but is adequately efficient for most small to mid-sized game projects. · Expandability: Having access to the source code, developers with C++ knowledge have the opportunity to add and integrate additional functionalities as needed. · No support provided: This port is provided without support. Developers are encouraged to engage with the community on the Nintendo Developer Portal for collaboration and assistance. We recommend partnering with porting companies for medium or large-sized projects. · Access Procedure: Access can be requested through the Nintendo Developer Portal. Additional details are available for authorised Nintendo Switch developers in the forum section of the Nintendo Developer Portal. Now there’s no excuse not to use Godot Engine for making indie games for Nintendo Switch! We look forward to the innovative and engaging games that will come from the Godot Engine development community. A few of the games using this port Get in touch to add yours! heyrawrlab.com Nintendo Switch is a trademark of Nintendo. Godot is a trademark of Godot Foundation. Hosted by RAWRLAB Games.",
    "commentLink": "https://news.ycombinator.com/item?id=39102002",
    "commentBody": "Free Godot engine port for Nintendo Switch (rawrlab.com)379 points by ekianjo 22 hours agohidepastfavorite209 comments weinzierl 20 hours agoMy 12 year old and I made a little game in Godot and it turned out pretty playable. I think in her peer group the second most popular device after the iPhone is the Switch. It would be incredibly cool if we could get our game to run there. I already found two GitHub repos with Godot ports, and this RAWRLAB announcement confirms that it will be doable on a technical level. However, I reckon that we do not qualify for authorized Nintendo Switch developer. What would be the easiest way to get the game running on just our Switch? I already ordered a Mig Switch and a dumper[1], but I'm not sure if this is the best way to go. [1] https://migswitch.store/ reply lastgeniusua 20 hours agoparentI'm not sure the Mig Switch will allow you to play homebrew, it is designed to replicate the (apparently reverse-engineered) security protocol of game cards signed by Nintendo. The Github repos you've seen previously are probably your best bet, they're designed for homebrew development, but to launch these you'd need to mod your Switch: with an unpatched original model (could easily be verified on https://ismyswitchpatched.com/) it's as easy as connecting two pins with a paperclip and injecting a USB payload with your PC, with patched later models this requires a modchip soldered onto the board. reply gjsman-1000 18 hours agorootparentEven with that though, home-brew games are almost nonexistent on the Switch, or extremely minimal. Having the ability to launch a game with a modded Switch, doesn't mean you have access to the SDK, or the documentation. Without those things, you really can't write any useful programs. It's like gaining access to the cockpit of a plane, and having no manual or labels on what all the buttons do. If you truly want to develop a Switch game, you need the SDK, and dev hardware is basically necessary as well. That's only coming from a deal with Nintendo. reply derefr 17 hours agorootparent> Having the ability to launch a game with a modded Switch, doesn't mean you have access to the SDK, or the documentation. The very same websites and forums and Discord servers where the people who come up with jailbreaks for the Switch hang out, can also point you to leaked copies of SDKs, and ways to use these with a jailbroken switch and/or an emulator like Yuzu. (After all, while most published console homebrew — including jailbreaks — use ground-up rewrites of support libraries, it's a lot easier to tinker on Proof-of-Concepts for exploit chains using the official SDK.) > and dev hardware is basically necessary as well. That's only coming from a deal with Nintendo. Funny enough, you can find all sorts of dev hardware for old consoles on eBay... and yes, that's despite all such kits being closed-lifecycle systems that are supposed to be returned to the OEM when no longer used. (I think the main way these make it out into the market, is through the company that owns them going bankrupt and getting its assets liquidated.) reply kmeisthax 16 hours agorootparentModern console devkits don't boot unless they have an Internet connection through the manufacturer's VPN, so buying devkits on eBay is useless now. They started doing this with the Xbox One. The reason why they did this is because of leakers. The Xbox 360's devkits connected to a separate Xbox Live network called PartnerNet, and anyone who wanted to test Xbox Live functionality - e.g. buying games or DLC - needed to actually upload their game to PartnerNet. This meant that everyone with a devkit got full access to a lot of prerelease games. There were rings of people with devkits leaking games for gamer clout. The way they got access to the hardware was interesting. Microsoft actually didn't let liquidators touch the consoles[0], but they still needed to dispose of them. The electronics recyclers Microsoft hired didn't do a good job of this, so there was a cottage industry of people taking debug fused CPUs off destroyed motherboards and swapping them onto retail boards. This would give you something identical to a low-spec devkit that lets you run unsigned code and connect to PartnerNet, but doesn't have any of the crazier debug capabilities useful for development. [0] Notably, the state of Rhode Island tried liquidating the devkits of the Kingdoms of Amalur developers and got stopped by Microsoft reply derefr 15 hours agorootparentInteresting! Has anyone gone to the effort to jailbreak the devkits + reimplement open versions of any network services the devkit depends on? reply kmeisthax 14 hours agorootparentNo, you're better off jailbreaking retail units. That being said, most retail jailbreaks also let you jailbreak development consoles anyway. Development and retail hardware is very close to one another, they differ purely in what debug interfaces are available and what DRM gets enforced. reply seba_dos1 15 hours agorootparentprevdevkitPro (libnx) has a complete OpenGL implementation with solid SDL2 port. I have ported my game engine to it with ease. There are even homebrew Godot ports for Switch that have been going on for years now. There's absolutely no trouble with writing proper homebrew for the Switch, no deals with Nintendo necessary. reply JoshTriplett 17 hours agorootparentprev> Even with that though, home-brew games are almost nonexistent on the Switch, or extremely minimal. One major reason for that is that Nintendo aggressively permabans Switch devices that appear to be modded, preventing them from ever being used online. reply graphe 15 hours agorootparentHomebrew exists on other consoles with this like the PS4 or PS3. There are zero online games worth playing on switch. reply circuit10 17 hours agorootparentprevThis usually won’t happen if you don’t pirate anything, cheat or do anything that looks like either of those, but there’s always a risk reply jimmaswell 15 hours agorootparentIt's just a worse Steam Deck at that point. reply traverseda 17 hours agorootparentprevYou can run full ubuntu on it, so I guess people don't see much need for \"homebrew\". You can just run any game that compiles for arm linux on it. reply jchw 19 hours agoparentprevIf you're dealing with a Nintendo Switch that has the RCM exploit, that is probably the easiest path to getting unsigned code. I think Mig Switch is probably only useful for commercial game backups and piracy. I have only used the RCM exploit to boot Linux, but I believe you can also use it to boot custom firmware like Atmosphere, and presumably that is the path to which you could see your own games running in Horizon. reply realslimjd 19 hours agoparentprevNintendo really wanted to encourage indie gaming on the Switch, so any individual can register to be an authorized developer: https://developer.nintendo.com/. reply revjx 17 hours agorootparentIt's not that easy, unfortunately - I'm a registered developer (for the Wii U) and to be 'enabled' for Switch development, you need to jump through a bunch of hoops including describing your previous experience with games development and so on. The barrier for Wii U was significantly lower thanks to the 'Nintendo Web Framework', a web tech SDK that let you build games without needing to use tooling like Unity or mess around with the more complicated SDKs Nintendo provide. You didn't need prior games dev experience to get registered for that. That said - maybe they've relaxed the criteria a bit now to give first time game devs a chance to get published on Switch. reply araes 15 hours agorootparentThey're still that strict. I attempted to get authorized several months ago and Nintendo did not even bother to respond to a developer with a few phone game and app releases. reply Aeolun 9 hours agorootparentTheir store is full of very nearly junk releases. How do they distinguish between a lot of experience, and a lot of experience with junk? reply gs17 16 hours agorootparentprev>and to be 'enabled' for Switch development, you need to jump through a bunch of hoops including describing your previous experience with games development and so on. I don't think they're that strict, there's a lot of garbage on the Switch store. reply kmeisthax 16 hours agorootparentThe strictness isn't there to keep garbage off the console store. That was just Nintendo revisionism. It's there because they need to know who to sue into a crater if you use developer mode to play pirated games. reply djmips 15 hours agorootparentI'm not sure you can use developer mode to play pirate games but it's not out of reach I suppose. Seems like a lot of work. reply kmeisthax 14 hours agorootparentSo, on most systems nowadays, retail software is encrypted and signed with entirely different PKI from developer applications. You would have to obtain cracked retail software first, then resign it with your own keys. This is how you run pirated iOS apps now, which is only possible because Apple hands out developer keys like candy. It'd at least be theoretically possible to do the same thing with a console devkit and decrypted games. This is also narrowly considering only the example of someone trying to pirate new releases for that given system. More broadly, the console manufacturers have class solidarity[0], and don't want you doing anything that might be a copyright violation - even if you aren't hurting them or their developers specifically. You could port over an emulator and steal older games, you could modify new games (even ones you own), or you could make fan games. They want absolutely none of that on their hardware, and the only way to guarantee that is to make sure everyone who touches a devkit is wearing contractual handcuffs. [0] Vaguely Marxist term for \"these people are in the same social situation, therefore their interests are aligned\" reply GlickWick 14 hours agorootparentprevIt’s extremely hard. I’ve tried to get my released games into the Switch and am rejected because I don’t have a team with console launch experience. Nintendo asks for team member resumes etc. Of course at the same time they have tons of junk in the store. It turns out the trick is to just use a publisher. reply weinzierl 19 hours agorootparentprevI saw that initially and was encouraged, but my further research suggests that you need to buy a development kit about which I could find virtually no info. I can only assume that it is expensive. reply nogridbag 19 hours agorootparentIt looks like a Switch dev kit is only ~$450 from Googling. I believe dev kits in the past were far much expensive. reply BiteCode_dev 18 hours agorootparent\"only\" If it's to dev a game with your kid, that pricey. Pricier than the console itself. Pricier than to dev for android or iphone, or any laptop. Or the steam deck for that matter. Expensive dev kits made sense for devs 20 years ago, but today they are just a way to lock the ecosystem. reply nonethewiser 16 hours agorootparentMostly agreed, but it could also easily be $10k. reply gjsman-1000 18 hours agorootparentprevYou’re kidding right? $450 for a dev kit is a steal compared to older console generations, which were thousands of dollars per device. Also, I think it is completely fair for Nintendo to think: If you won’t spare even $450 for a dev kit, there’s no way we want your game. By the way, consider what a Switch regularly costs: $299. You are paying Nintendo only $150 for the privilege of sending you a custom, low-run, modified device that is different in both software and hardware, combined with (likely) an account representative for business and technical questions. That’s kind of cheap. reply bjtitus 18 hours agorootparentWon't anyone think of the giant multinational corporations instead of the kids learning to program for once. reply gjsman-1000 18 hours agorootparentNo kid is going to learn how to program in Assembly and C++, when interfacing with a custom microkernel, custom graphics API, and all of the technical requirements Nintendo demands (i.e. you must have a launch screen, loading times cannot be longer than X, read speeds cannot peak higher than Y), while cross-compiling your game and all dependencies to ARM, and precompiling all shaders ahead of time, for the Tegra X1, into your game. That's what developing for the Switch literally entails. Develop your game first and then you might get there. This is a game console, not a PC. Developing games, for better or worse, whether it should be this way or not, is an extreme privilege. Sony basically never allows individual developers to register in any capacity. Xbox does, but if you want to actually publish a game, you're going to be in for a hard time. reply bee_rider 16 hours agorootparentIt has around a 1Ghz cpu, you have no idea if their little homebrew games will stress that or need all the assembly tuning. Kids have programmed games for their calculators. This is like “throw it together to show off your friends,” it doesn’t need to meet Nintendo’s UI standards. reply gjsman-1000 15 hours agorootparent> This is like “throw it together to show off your friends,” it doesn’t need to meet Nintendo’s UI standards. In that case, unless it goes on the eShop (thus meeting Nintendo's standards), every single one of her friends will also need a devkit to be able to run it; as retail units can't decrypt devkit-encrypted games, nor vice versa. Might as well get a Steam Deck before convincing Nintendo to send over 10 devkits for children. reply Nullabillity 5 hours agorootparentprev> This is a game console, not a PC. Game consoles are arbitrarily restricted PCs. reply robertlagrant 18 hours agorootparentprev> Won't anyone think of the giant multinational corporations instead of the kids learning to program for once. It's not a binary choice. Kids can learn to program in 1000 different ways. Getting social Brownie points for pointing out someone else isn't being incredibly generous is pretty poor behaviour, I think. reply weinzierl 17 hours agorootparentKids can learn to program in 1000 different ways, but will they? The biggest motivation is to show-off your creation and if all your friends play on the Switch and your game isn't there, it might as well not exist at all. I learned programming on a Commodore 64, because I wanted to make my own games. Back at that time the playing field was fairly level as evidenced by the fact that a couple of the prominent game developers were almost kids themselves. reply Nullabillity 5 hours agorootparentprev> You are paying Nintendo only $150 for the privilege of sending you a custom, low-run, modified device that is different in both software and hardware, combined with (likely) an account representative for business and technical questions. A distinction that _they_ created to begin with. reply ekianjo 17 hours agorootparentprevyou dont need any of that shite to run your own game on the Steam deck reply bee_rider 16 hours agorootparent> My 12 year old and I made a little game in Godot and it turned out pretty playable. > I think in her peer group the second most popular device after the iPhone is the Switch. It would be incredibly cool if we could get our game to run there. I don’t think buying her friends Steam Decks is a very scalable solution for this sort of thing. reply derefr 17 hours agorootparentprevYou're comparing apples to oranges. The Steam Deck is, fundamentally, a PC. PC software is written to run using \"some\" of a PC's resources, because PCs have a bunch of extra resource \"headroom\" for running userland software plus a bunch of other junk on them at the same time. You can run a game in debug mode, within an IDE, within Windows, on a Steam Deck. And maybe also OBS to stream yourself doing that! The Switch — and all other game consoles ever made thusfar — are embedded systems: systems designed to run software in such a way that the software can make use of 99% or more of the system's hardware resources (because, per the unit economics of such systems, that's the only way to make the system's low per-unit BOM, translate to good perf for the software.) Also, unlike with PCs, embedded systems can give a developer performance guarantees — a number of CPU cores, amount of RAM, etc. that will always be 100% dedicated to running your software, with even hardware interrupts being offloaded somewhere else. And the platform will be extremely uniform, with all retail units having (at least) those same numbers. (Though there may eventually be a revision of the hardware that bumps the guaranteed resources up for software that opts into that — see e.g. how the Gameboy Color runs Gameboy games, or the \"New\" 3DS, etc.) So, unlike with a PC, when writing embedded software, you can and should write it to use \"all the resources\" — to \"wastefully\" spend memory caching whatever you can, run background logic to pre-compute things, etc., so that your software can get the best realtime runtime perf. It's not like anything else is running. Take the Unix idea of a process being nice(1), and invert it. This means two things: 1. when a game is running on a retail device — if you've developed it following best practices — then there's no extra room for anything else to be running. Your software, by design, hogs everything. The OS shrinks down to just running on one e-core of one accessory CPU (and on older consoles, gets terminated altogether, with the user software instead being expected to keep the hardware going using an exokernel library provided as part of the SDK.) There's no room to run an IDE or debugger. So retail devices fundamentally can't be used to develop games in such a way that they \"mirror production\" — to have the room to develop your game on such a device, you'd have to somehow build your game to use half the RAM and CPU it usually uses — but only sometimes. Which is likely impossible, for most software architectures; but even if it were possible, you'd no longer be mirroring production. 2. even with a devkit — essentially these days just the same hardware but with double the RAM to fit a debug build of the game (both for the debug symbols, and so that it doesn't need to take two hours to LTO the executable each time you rebuild like it does for building retail) — you still need a host PC, because you want development to mirror production at least somewhat closely, and that means that you can't be spending half the CPU budget on a userland debugger + telemetry. Instead, devkit hardware has a hardware debugger + telemetry, that feed one side of a USB-serial connection to a host PC, where the other side is a special IDE that knows how to talk to such a setup. None of this is unique to game consoles, mind you. You do the same thing to develop for an Arduino, or a wireless router, or a smart watch, or your car's infotainment system. \"Tethered hardware devkits that externalize the development environment so the embedded software can expand to fill the device\" are just how embedded systems engineering works. --- And yes, embedded-systems hardware ecosystems also often involve \"QA\" or \"tester\" units, that are basically retail units that either allow you to run [the ecosystem's specific, often proprietary] media containing unsigned code; or at least, allow you to install a specific signing key and then run media containing code signed by that key. So you could, in theory, do your development by building for QA units. (That is, you could, if you thought that printf debugging with two-hour iterations is the be-all and end-all of development cadence.) Homebrew embedded-systems tinkering communities tend to ignore such systems, though, mostly due to the requirement of burning the media. Even when the media isn't proprietary, it's still often something like a surface-mount eMMC flash chip, where the embedded-systems vendor expects you to be able to program that and put it on your QA board (and will, of course, point you to their hardware support partner who will sell you a programmer for it.) While there are many hardware hackers among Homebrew people doing fun reverse-engineering attacks, there are few true Electronics Engineers in the homebrew scene who have these sorts of Shenzhen-parts-store-looking industrial test-equipment labs in their basements. The average homebrewer doesn't even own an oscilloscope. :) It's just easier to either work with true dev kits (where all you need is to \"convince\" the thing that you're authorized to feed it software over the serial tether); and/or to reverse-engineer and exploit your way into a retail unit, such that you can then feed it software through some channel that is attainable for the average dev (usually, these days, that means getting the system to launch software from an SD card that was supposed to just be there to store state data.) reply BiteCode_dev 17 hours agorootparentprev\"Expensive dev kits made sense for devs 20 years ago, but today they are just a way to lock the ecosystem. \" reply 999900000999 17 hours agorootparentprevNintendo doesn't owe you a cheap way to deploy your custom binaries to devices. Buy a Steam Deck if you want that. Build it for Web GL or Android. Plenty of options exist. I'm not sure what OP actually wants here. Even if you can get your game running on your own Switch, it's not easy to get it up on the store. reply bee_rider 16 hours agorootparentWhat they actually want is a cheap way to develop games with their kid and possibly share them over sneakernet with her friends. Nintendo obviously doesn’t owe them that, and it doesn’t seem to be supported. But, this is a good place to cast around for ideas. reply 999900000999 15 hours agorootparentA Switch is simply not the right device. You can punish a Web GL build of the game to Itch.io and everyone could try it straight from their phones. reply Nullabillity 5 hours agorootparentprevWe don't owe Nintendo to let them sell their junk. reply bakugo 12 hours agorootparentprev> Nintendo doesn't owe you a cheap way to deploy your custom binaries to devices. Why not? Why should I not be allowed to run whatever I want on a device I supposedly own? reply diggan 18 hours agorootparentprevMake the game available on the Switch store for like $5. 450/5 ~= 100, so after 100 purchases you've made back the money. Sure, you'd have to spend extra time to polish stuff, and that would probably not be super easy, but afterwards you have a fully published game the daughter and all her friends could install. reply hitpointdrew 17 hours agorootparentThis might not be feasible. If you are making a game for yourself and your kid, you might not be using assets (art, sound, etc.) that you could distribute/sell in your game. I made a game for my daughters 3rd birthday, recorded some Blipi and Coco Melon songs off youtube that I put in the game. I would not have the right to sell the game without securing rights to the songs (unless I want a lawsuit on my hands). reply diggan 17 hours agorootparentHow would you distribute that over the Switch in the first place to the daughter and/or friends if you don't have the licenses to the assets you're using? reply troupo 18 hours agorootparentprevThis is the insanity I don't understand. Developing for desktop systems is essentially free. Developing for iOS devices is $99 a year (plus store commission) for literally anyone. Developing for consoles? Oh, you have to be a business entity, you have to pass approval process, you have to buy a dev kit, you have to... And then also pay a commission. reply ammar2 18 hours agorootparent> Developing for iOS devices is $99 a year (plus store commission) for literally anyone. Don't forget having to buy a MacOS device in this equation. reply helboi4 18 hours agorootparentYeah that's pretty prohibitive for most people who couldn't spare $450 for a switch dev kit reply troupo 18 hours agorootparentprevI bought an XBox, too. reply mschuster91 17 hours agorootparentprevDevelop on a Hackintosh (either bare metal or VM, there's more than enough guides for both), build using Github Actions, and that's it. If you want to stay fully legit, rent a Mac instance from AWS or the myriad \"Mac-as-a-Service\" companies and be done with it. reply pjmlp 18 hours agorootparentprevThank US 1983 crash for that. Gatekeeping is a way to avoid a minium level of quality, instead of a dumpster copy cat trash that inudates all the stores where there is no control. Want to learn how to do consoles? Get a toy handeld using ESP32 or Arduino. reply starkparker 9 hours agorootparentThere's quite notoriously no minimum level of _quality_ required to publish on the Switch store. Here are three identical low-quality Switch games from three different publishers: https://www.nintendo.com/us/store/products/word-chef-switch/ https://www.nintendo.com/us/store/products/chef-word-ardee-s... https://www.nintendo.com/us/store/products/eat-your-letters-... And here's the Unity asset that these games use almost verbatim: https://assetstore.unity.com/packages/templates/packs/word-s... If Nintendo wanted to gatekeep on quality, publishers wouldn't be able to publish Unity assets directly to the shop, much less multiple publishers shipping the same asset. If OP's kid buys a Unity asset and publishes it directly to the eShop, she'll have done the same amount of work as these publishers and produced the same quality of app. She can't because of Nintendo's developer program, not because Nintendo would disqualify it as poor quality. reply lesostep 1 hour agorootparent> If OP's kid buys a Unity asset and publishes it directly to the eShop I think the root case of the \"kids can't write games for their peers\" isn't Nintendo or anyone else. Both Apple and Switch are praised by the parents for their restricted platforms. You can give a kid both of them and be sure that that kid wouldn't install age-inappropriate apps or games. Self-made stuff needs to be moderated heavily or kids will share pornographic games (i mean they are kids. Who haven't tried to play at least one \"age-inappropriate\" game by the age of sixteen?). In my humble opinion, Nintendo just don't want to do too much moderation, so they added hoops after hoops to jump through, until the amount of low-effort apps decreased enough for moderators to do their job thoroughly for every app. > I think in her peer group the second most popular device after the iPhone is the Switch. I would say that the inability of sharing anything self-made is why apple and switch are popular choice to buy for your kid. No need to talk about safety measures, if you lock them in a system. So, IMHO, the conflict isn't between kid devs and nintendo, its between kid devs and their peers parents. reply pjmlp 3 hours agorootparentprevJust because some garbage ends up on the store, doesn't mean all the garbage gets through like on the PC and Android. Even club bouncers occasionally let the wrong folks get in. OP's kid has plenty of other options to play with game development. reply anthk 17 hours agorootparentprevPC gaming had tons of shovelware since the beginning and even got both good propietary and libre software games with great quality. So did the Game Boy, which the NIntendo seal of quality was almost given for free because lots of games were junk even under the GBC era. reply pjmlp 17 hours agorootparentExactly because anyone can do it. reply anthk 16 hours agorootparentThen explain some \"games\" for the Game Boy Color where the quality was very subpar. reply pjmlp 15 hours agorootparentYou said it yourself, Nintendo platform bouncers let too many spoil the party. Here is the thing, we aren't entitled to anything in life. Don't like the way consoles work? Don't buy them. reply dvdkon 15 hours agorootparent> Here is the thing, we aren't entitled to anything in life. Are you sure about that? I'm entitled to a lot of things if my country's laws are anything to go by, for some reasonable definition of entitlement. I could be entitled to running my own software on a Switch if lawmakers say so. The EU already forced Apple, so what's another platform? reply pjmlp 13 hours agorootparentStart a petition to see how far you will go, good luck. reply cableshaft 13 hours agorootparentprevAlso you have to pay to get your game rated by the ESRB (at least if it's physical. Maybe not required to pay money for digital games from what I'm seeing with a Google search? Although we had digital games we still got ESRB rated). It's been more than a decade since I worked on a console game, but I think at the time that cost us $750. reply kmeisthax 16 hours agorootparentprevXbox One/Series has Dev Mode, but that's only for UWP apps which have severe resource restrictions. If you want to use the Dedicated partition you need a devkit. The reasons why consoles lock down developer access so hard is because those interfaces are also very useful for piracy. On iOS, because you can get access to developer tools, you can use AltStore to sign arbitrary binaries with your own dev key[0]. Apple may have said \"sideloading is a cybercriminal's best friend\", but what they meant is \"you're all going to steal iOS apps if we let the general public install software without DRM\"[1]. The old Microsoft \"Darknet\" paper[3] built up a sort of theory of piracy, which I still find useful to invoke here. In the paper, they break the piracy ecosystem down into five pieces: 1. Rippers, who pirate new works and make them available 2/5. Seeders, who transmit new works across the network[4] 3. Players that are capable of rendering creative works 4. Indexes that provide information as to where pirated works can be found To stop piracy, you need to block at least one of these steps. For a variety of reasons, legal action against any of these actors is difficult. Players and indexes are identifiable, meaning we can sue them, but they have legitimate, non-pirate-y uses. Rippers and seeders are hard to conclusively identify, making them impossible to sue. Of course, when people using the power of the state cannot identify criminals, they revert to collective punishment - or in this case, technical restrictions. You can't DRM the seeders or the indexes, but you can encrypt the media, which gives legal leverage over player vendors: either make your player enforce our licensing terms or do not play our media. But there's still a problem: players can still play unprotected media. Rippers will just strip the DRM and release unprotected data that will play anywhere. This is why the RIAA fought tooth-and-nail to ban DRM-free DAT and MP3 players, and only settled for legal restrictions on DRM removal. Console manufacturers have the advantage that each new console is it's own medium - they can just make players that only play DRM-encumbered work, and then nobody can pirate anything, even if rippers strip the DRM. Hell, even if people jailbreak the players - you control the supply of players, so you can ensure whatever updates you use to 'prevent tampering' are installed before the user even gets to the console. And this is far cheaper than working on new obfuscations that some socially maladjusted loner will break in a few days. [0] At least, if you trust Riley Testut with your iCloud account password. [1] In general, people who want to enforce copyright rarely, if ever, come out and say it. The public is generally unsympathetic to copyright owners. It's easier to conflate security of their work from copying with security of your data. [3] https://www.cs.ucdavis.edu/~rogaway/classes/188/materials/da... [4] In the original Darknet paper, this was broken down into 'transmission' and 'caching'. This division makes sense for the FTP topsite scene, but not BitTorrent trackers. reply amalcon 18 hours agorootparentprevA chunk of it is copy protection. Dev devices can necessarily run software not signed by the manufacturer, which means copy protection can be bypassed using PC-like methods and loaded to a dev device. Thus, they want to make sure the dev device is not the device that most consumers purchase. I think it's a poor tradeoff for such a locked down environment, and anti-educational at that. Obviously console manufacturers have a different opinion on the matter. reply gjsman-1000 18 hours agorootparentIt's a little more complicated than that. Dev devices are typically physically different - containing debug lines and connections that are not present in retail. The Switch itself has 2 different types of dev console: One that looks very much like retail, and another that is about 4x as thick and has every port under the sun. Secondly, these consoles are physically fused differently. Instead of having retail encryption keys burned into the SoC, they have custom keys issued to the developer installed. This means dev consoles cannot run retail software for lack of a key to decrypt it - but they will decrypt, and run, anything the developer signs. Both of these things are physical modifications, ultimately. Dev consoles, thus, do not come off the same production line as retail; but are customized and modified devices with their own manufacturing process. That's not cheap. Sure, the Xbox does get away with the retail console having a \"Dev Mode.\" That's a testament to the Xbox's security having gone 12 years without a crack. Making physically different retail and developer consoles is a much safer solution for anyone who isn't Xbox. reply Nullabillity 5 hours agorootparentYou're starting at the wrong end. There is no need for there to be a separate \"dev console\". reply 3836293648 18 hours agorootparentprevIt's because they want(ed) to keep a higher standard of quality on their store and keep homebrewers and cheaters away. reply mh- 17 hours agorootparentI believe you're right, but the first part didn't work. The Switch eShop has an incredible amount of shovelware. reply 3836293648 10 hours agorootparentHence the want(ed). reply coffeebeqn 19 hours agorootparentprevIt’s not going to be crazy but you should expect to spend like a $1000 total for everything and have a real company set up reply fxtentacle 19 hours agoparentprevJust email Nintendo. They sometimes make exceptions for interesting indie developers and/or hobbyists. And the dev kits are affordable. reply falker 20 hours agoparentprevThe migswitch will not allow you to run unsigned code, only clones of official cartridges. Get an old “unpatched” v1 switch or newer with a rp2040 modchip. reply DeltaWhy 6 hours agoparentprevOthers have commented on homebrew, but there are a couple \"game engines\" available for the Switch that don't require modding. Nintendo released their own, Game Maker Garage, which uses a block-based coding environment. There are also two I know of that use a more traditional programming language: SmileBASIC and Fuze. Both are similar to PICO-8 or other \"fantasy consoles\" where you get a full environment that runs on-device, with a code editor, tools for editing sprites, tile maps, music, and sounds, and a library of stock assets. Fuze also has a cheap \"player\" app so your kid's friends wouldn't need to buy the full app to play anything she makes. I've only poked around them a bit, but they seem like pretty capable environments that might be a good option for a kid that's interested in programming. The obvious limitation is that you're locked in to that platform - as far as I can tell there's no way to run your games on PC. reply stavros 19 hours agoparentprevHow does the Mig Switch site not say at all what the product is? I clicked around for two minutes and still have no clue what it is or does. reply weberer 19 hours agorootparentIt is a flashcard. It lets you load copies of games onto a SD card and the Switch will read them as official cartridges. The site mentions that its only for backups of your own purchased games, but most people buy them for piracy. reply stavros 19 hours agorootparentThat's very interesting, thanks, I didn't realize you could load downloaded games onto a Switch like that. I imagined they have some sort of hardware verification. reply traverseda 19 hours agorootparentThey do, this is a new and exciting product, likely with an FPGA built in to bypass those hardware locks. reply sureglymop 18 hours agorootparentprevOverall this is very interesting. The game switching mechanism seems a bit fiddly though and I hope there will be a version with a button. I wouldn't be surprised though if the whole operation will be shut down completely by Nintendo so it might not be a bad idea to buy one. The same thing happened with the DragonInjector (small payload runner for jailbroken switches that fits in the cartridge slot). reply traverseda 19 hours agorootparentprevWell one of it's main uses is playing pirated nintendo switch games, which they're not going to advertise. reply stavros 19 hours agorootparentSure, but I guess we all know what \"backups\" is code for. I just didn't realize they managed to bypass the protections for the Switch. reply Retr0id 16 hours agorootparentLaw enforcement knows what backups is code for, too reply stavros 15 hours agorootparentThe thing they call for when they see a black person? reply roryokane 13 hours agorootparentprevA product description is available if you click the “View full details” link to the bottom right of the Buy button, which leads to https://migswitch.store/products/mig-switch-pre-order. I agree that that information should be more prominent. reply cptaj 19 hours agorootparentprevIts a flashcart reply Retr0id 16 hours agoparentprevI'd recommend canceling your Mig Switch order. First and foremost, it financially supports some very sketchy players in the commercial piracy (under)world. Secondly, it won't let you play homebrew games - only officially signed nintendo software. Thirdly, you'll have to keep your console offline forever, if you plan to use it - you can bet Nintendo will start revoking cart certificates and banning consoles at the first opportunity (each genuine cartridge is uniquely identifiable). We have completely free solutions to both \"\"\"backups\"\"\" and homebrew. Your best bet is to find a cheap model with an unpatched bootrom, and your second best bet is to install an rp2040-based glitching modchip (or find someone to install it for you, if you're not comfortable with microsoldering). reply graphe 15 hours agorootparentWhat online switch game is worth playing? I'm sure the mig will be decrypted, and they will just fake the signature for the homebrew games. I think that's how some of the older homebrew worked. The scene is pretty dead since smartphones exist though. I agree he should just get an old one but aside from maybe not liking the mig guys I don't see a problem with the product. The amount of blood from mining in Africa for my minerals probably isn't too moral either. reply Retr0id 15 hours agorootparent> just fake the signature That's not how cryptography works. reply graphe 15 hours agorootparentCan you explain how it works? I read a bit here. https://wololo.net/2024/01/10/it-appears-team-xecuter-are-ba... it seems you can use different game certs to make it launch. reply Retr0id 15 hours agorootparentEach physical cart has a unique certificate, signed by a private key held by Nintendo. That certificate is verified locally by the console, against a public key in the firmware. Without CFW, you have no way to bypass that signature verification. In short, the flashcart works by making a full clone of the cart, cert and all. The console doesn't know it's not talking to an original game. That is, until Nintendo's servers notice multiple consoles playing the \"same\" cartridge at the same time. If you really are only making and playing backups of your own games then you'd probably be fine, but I don't think anyone really buys these products with that use case in mind. reply graphe 14 hours agorootparentThank you for your explanation. I thought homebrew would be possible if you have a legit game, have homebrew then just use the cert to play it and make the game console think it's a game. I think if it like a GameShark, you load it with a cert then you switch the game. reply Retr0id 14 hours agorootparentEverything is signed, there's a chain of trust. reply bakugo 12 hours agorootparentprevThe unique certificate is mainly used to verify that a cart is authorized to be played online, the game files themselves are signed in their entirety and a console not running CFW will not accept any modified files. reply Nullabillity 4 hours agorootparentprevBuying a Switch in the first place supports some very sketchy players. reply Retr0id 2 hours agorootparentThe console itself is practically a loss leader, it's buying the games that's sketchy :P To clarify my overall stance, I am: Broadly anti-nintendo (good games, sketchy company), extremely pro- modding and homebrew, mostly pro-piracy, but firmly against the commercialization of piracy. Making things available for free is good, taking things just so you can profit from them yourself is bad - and that's how I see MigSwitch et al. reply joenot443 18 hours agoparentprevYour best bet would be to buy an unpatched Switch from eBay (https://www.ebay.com/itm/144952360896, etc), make or purchase an RCM jig, and dive into the homebrew scene. GbaTemp is a good place to start. Outside of buying an official dev kit, I think that may be your only option. reply gundamdoubleO 18 hours agoparentprevI wouldn't recommend using the mig unless you're willing to risk a console ban from all online services (if you care about that). reply torginus 16 hours agoparentprevI honestly don't get why console makers get a boner at the thought of disallowing third party software. What do they gain out of this? With proper security protocols you can sandbox your application well enough that hackers shouldn't be able to get to the OS. And if they do find an exploit, the vendor can always force an OS upgrade, and prevent downgrades with hardware fuses, with new games requiring the latest OS. It's not like console vendors aren't doing all this right now. reply ook 16 hours agorootparentRoyalties. Quality Control. Brand Alignment (historically a big deal for Nintendo). Scheduling input. Security by obscurity. reply araes 15 hours agorootparentprevTragedy of the commons. By making the barrier to entry high, it means that only significantly capitalized businesses with significant assets to wager and lose can actually publish anything. It means that there's (less) app spam, because not every couch publisher can type \"ChatGPT, make me a game with the default Unity assets.\" reply ranger207 16 hours agorootparentprevIt's largely the same reasons Apple doesn't want alternative app stores on their phones reply graphe 15 hours agorootparentprevIf you want a device that does everything buy it. I don't care if my Roku doesn't do something all arm devices can do even though it can. I didn't buy a PS4 to install Ubuntu I want to pop a game in. reply djur 16 hours agorootparentprevMaking piracy more difficult, primarily. reply georgeecollins 16 hours agorootparentI think that is right, but I also think that quality control is very important to console manufacturers, particularly Nintendo. You don't think about it so much because the consoles haven't changed that much in market position in years. There is lore that bad quality software destroyed the 2600. Battles SEGA and Nintendo, SEGA and Sony, Sony and MSFT really depended a lot on the quality of titles and the users experience. So they tend to want to control the experience more then say Google or Apple do on the phone. You can have shitty apps on a phone, but its a much bigger problem for a console. reply punkybr3wster 12 hours agoparentprevUnrelated but I’m kind of terrified what the mig is going to do to the market. Crazy piece of tech but the signed carts getting cloned are going to get real interesting real fast. reply tombert 17 hours agoprevI know \"asymptotically close to zero\" about game development and the like, so if someone who works in this space could answer this I'd be appreciative: what, if anything, would make you use Unity or Unreal instead of Godot for making a game today? This isn't meant as a passive aggressive question to push some kind of open source narrative, but a genuine curiosity. Since I know basically nothing about the industry, I'm curious to what something like Unity or Unreal still offers over an engine that has no licensing fees. reply SXX 17 hours agoparentIndie gamedev company co-founder here: 14 people team, 2 games released + next in progress and signed with publisher. There are few big reasons why you can't just go and use Godot. Publishers. Basically 3 years ago when we made our first game together using Godot it was main reason why we were rejected by some publishers. Now situation can be slightly different, but many publishers still have in-house pipeline that only work for Unity and Unreal: QA, localization and console porting. Asset Stores. Majority of modern games are not built from scratch and selection of ready-to-use components and assets available commercially would be like 1000x for two major commercial engines. Hiring. It's take years and several commercial projects of experience to actually become effective at using specific game engine. There are just far less Godot experts arounds especially those with experience of making commercial game while working in team. These reasons can sound like inertia, but it took decades for Unreal and Unity to conquer their market share and build their moat. I myself big open source and Linux nerd, but sadly it's will take another decade for Godot to make sensible dent in this market. PS: We released first game on Godot and our following projects are built on Unity. reply cevn 15 hours agorootparentSounds really unfortunate. I also know little about this process, what do the publishers do that we cannot do ourselves? I guess they have pre existing deals with the major console makers that allows them to send things to market with little friction? Maybe that's why some indie devs just target PC, they lock themselves out of the console market but also don't have to deal with publishers? reply SXX 3 hours agorootparentFirst and most important publishers provide funding. To give an example here is a work you need to build a small game and you can imagine how much it costs: - 1 project manager / year. - 1 game designer / year. - 1-2 programmer / year. - 2-3 months month of sound design / music. - 3-5 artists / year. Even if you have a great multi-skill team you need at least 5-6 people for a 6-9-12 months and it's would be very much skeleton crew. There are cases where team of 1-3 people working part-time without funding can build a great game, but it's require both sticking to specific genres and greater use of ready-to-use assets which means it must be 3D. Game that small indie company will build in 6 month will take 3-6 years with a team working part-time. Indie devs who self publishing only target PC because commercial porting sertvices cost starting from $10,000-15,000 per platform and it's gonna be way more expensive if game wasnt optimized for gamepad and big screens from beginning. And it's just impractical to port it yourself because it will take many months to pass certifications since know-how for proprietary platforms is not available on stack overflow. reply mardifoufs 15 hours agorootparentprevUnity was already massive by its 5-7th year. It had tons and tons of released games at that stage. Not sure exactly why that hasn't happened with Godot but i guess it might still happen, just a bit more slowly. reply wk_end 14 hours agorootparentBiggest reason is that there weren't any decent competitors around as Unity built momentum. At the time it was Unity, a (difficult-to-work-with, difficult-to-gain-access-to, expensive) proprietary engine, or something vastly less feature-filled like Game Maker. reply ghostbrainalpha 12 hours agorootparentprevNice answer. I have to say both the games on your Profile look great, and I am definitely going to try one tonight. Do you recommend I start with Dwarven Skykeep or Vengence of Mr. Peppermint if I'm playing on a Steam Deck? reply SXX 4 hours agorootparentI tested both on Steam Deck and VOMP certainly works better on the platform since Beat 'Em Up just have easier controls and we supported Xbox gamepad anyway. For Dwarven Skykeep controls include both keyboard and mouse and while playable from touchpads it's far from perfect. Though it's the one made in Godot and it took us 3+ years to build. :-) reply LarsDu88 17 hours agoparentprevI've done game jams with both Godot and Unity. Mainly the plug-ins, particularly third party plug-ins for unity are better. One of them being cinemachine. Another example would be a tool call technique collider creator that let's you paint collider primitives. That tool alone saved about 300 hours of work. For many developers the Cinemachine tool is very useful as well. The other feature is the ability to edit game objects while the game is running in editor mode for debugging. This is the sole major feature Godot currently lacks compared to unreal and Unity imo Beyond that Unity has a better overall design IMO encouraging users towards composition over inheritance and NOT encouraging use of some one shot interpreted scripting language (GDscript) Does this justify the 100x greater size of Unity as an organization over Godot? Probably not reply tombert 17 hours agorootparentThis is exactly the feedback I was interested in...thanks! I think these kinds of criticisms are necessary for open source stuff to become competitive with proprietary offerings; it feels like Blender, for example, has taken these criticisms pretty seriously and as a result it's started to become competitive with proprietary tools (at least in the \"prosumer\" and lighter professional spaces). I'm sure they're already doing this, but Godot should probably keep a list of all these bits of feedback and start fundraising to build these features. reply prox 15 hours agorootparentThe Godot leads basically discussed this during the last conference that they are definitely not in the right zone to take over significant developer share for Unreal or Unity, they hope in new versions they can inch closer and closer. reply mattlondon 13 hours agorootparentprev> The other feature is the ability to edit game objects while the game is running in editor mode for debugging. +1 to this. Having switched to trying Godot Vs unity, this was the biggest \"missing feature\" that made godot harder to work with Vs unity. (I am no pro, not even indie, just a hobbyist) In unity you can pause that game and view and/or modify game object's variables/properties/etc in real time. In Godot it feels like it just compiles and runs a binary that is disconnected from the editor. Yes you can debug the code, but the \"live\" integration with the visual editor is very useful when prototyping and trying out ideas as it lets you move things around and change variables, which you can't really do with debugging reply sli 13 hours agorootparentWhen's the last time you tried it? This feature has been in Godot for as long as I care to remember, I use it daily. reply gs17 15 hours agorootparentprev>cinemachine Check out Phantom Camera: https://github.com/ramokz/phantom-camera reply johnnyjeans 17 hours agoparentprevMy opinion as a gamedev who doesn't work with engines like this: -Unity and Unreal both offer better performance guarantees and more featureful runtimes than Godot -Both represent much more mature ecosystems, with tooling that's again, more powerful than Godot -Both have more robust communities and larger talent pools All of this said, I'd be more likely to use Godot for a quick personal project than either Unity or Unreal. But then I'd be more likely to choose Raylib over any of these options to begin with. Other people's choices might be influenced by the above, or they might choose Unity/Unreal out of familiarity, or possibly even something as simple as marketing hype/market perception. I'm not really sure the current state of Godot, but last I toyed around with it, I got the impression of something that was in-between Gamemaker and Unity and that dominates my perception of it today. reply beardedmoose 17 hours agoparentprevA few years ago I started messing with Unity and for the most part it was a decent engine with many platforms you can target which is a huge plus. Their whole licensing issue of late made me think though, do I really want to put years of my life into building a game when the owner of the engine can make changes which negatively impact my project? Now, this could happen with any engine that you don't own but Epic uses their engine to create games themselves so they have an interest in further development which also benefits the users of the engine. Godot is a newer kid on the block but has gained a lot of interest, especially after the Unity stunt. Now that it can target a Nintendo Switch there is even more reason to choose this engine as coding for game consoles can be pretty tricky as they usually have a lot of restrictions and tests they have to pass before you can launch on the platform. Unreal and Unity offer maturity and vast knowledge available on the web for both if you want to learn the engine. Unreal has a visual scripting language which some love, both engines have storefronts where you can buy plugins, code, objects, all sorts of things to help make your game. reply Capricorn2481 7 hours agorootparentBut does it target switch? Or does a company offer porting to switch with potential issues? reply kevindamm 17 hours agoparentprevUnity and Unreal still have more brand recognition, I'm sure there are project managers and creative managers that hadn't heard of Godot yet (though that is diminishing, I'm sure). More importantly, there is institutionalized knowledge and established tooling in place at a lot of dev studios. The expertise in one engine takes at least a year or more to acquire before you can really make the engine purr. But it also affects the whole toolchain, not just the part inside the IDE -- assets created elsewhere (models, animations, audio) typically go through some massaging to be compatible in Unity or Unreal, or the particulars of the scene/project and this often done through some automation, that automation would likely have to be rewritten for a Godot workflow, or even if it didn't, that's certainly what a PM or other decision maker would anticipate. There's also the asset store full of plugins, or in-house developed plugins, which extend the engine and would also likely need at least some part of it rewritten or replaced. reply pfist 17 hours agoparentprevI have over ten years experience with Unreal, so the cost of switching engines would be very high, and must offer something irresistible in return. Regarding your second question: Unreal effectively has no licensing fee for most developers. The engine is free, you get access to the full source code, and you pay a 5% royalty only after you have made $1 million gross revenue. reply sbergot 17 hours agoparentprevI am also not from the industry but here are some godot downsides from what I understand: - script performances are worst than unreal or unity - no asset market place - 3d features are not up to par with other engines (level of details, shadows, etc) For a classic 2d game it seems to have no significant downside. For a resource intensive 3d game it will depend. reply tezza 16 hours agoparentprevUnity so much more polished Unity has Timeline (and Cinemachine) Unity has Entity Component which is superior for many coders to Godot’s node system Unity has better 3D Unity has large bank of answered questions Unity exports to WebGL Unity Asset Store Assets are high quality, abundant and cost effective reply djur 16 hours agorootparent> Entity Component which is superior for many coders In what way? reply tezza 16 hours agorootparentMany ways. When you have a reference to something it is easy to add,query, change elements to that something Even unrelated things can be added this way (RigidBody, Raycasters) whereas Godot needs separate parallel node trees to accomplish the same thing reply gs17 15 hours agorootparentprev>and Cinemachine Phantom Camera does most of that in Godot. >Unity exports to WebGL Godot also does that, albeit without C# support until the next major version. reply tezza 15 hours agorootparentGodot WebGL is not production ready. It-kinda-works is as good as it gets. Godot webgl will not run on MacOS, defeating the whole point of WebGL/HTML5 “Godot 4's HTML5 exports currently cannot run on macOS and iOS due to upstream bugs with SharedArrayBuffer and WebGL 2.0” https://docs.godotengine.org/en/stable/tutorials/export/expo... reply vunderba 9 hours agorootparent\"Godot 4's HTML5 exports currently cannot run on macOS and iOS due to upstream bugs with SharedArrayBuffer and WebGL 2.0\" I have read this line of the documentation a dozen times and I still don't understand what they mean by it. An HTML5 game is supposed to be run in a browser, are they saying that web GL 2.0 games won't run on a Mac even in a chromium browser? Or are they saying that the export process to create the HTML5 game won't run on a Mac? reply mattlondon 13 hours agorootparentprevNot sure that macos 8s the whole point of webgl/html5. The market share for Mac gamers must be very, very low. Like under 1%-low? reply hombre_fatal 12 hours agorootparentThose numbers are for desktop game sales, not people who might want to play your game in their browser. Being unable to load up a browser game on macOS is a pretty big deal when the main upside of a browser compile target is portability and accessibility. reply mardifoufs 15 hours agorootparentprevThe web export is very very bad currently. I know there is a PR where that is getting worked on, but currently it's basically recommended to use Godot 3.6 if you want to web export. It outright does not work for safari (and Im not sure if safari is the issue in this case, or at least not anymore. Webgl2 has been fully supported since v15 ?) reply vunderba 9 hours agorootparentThis is the reason that I still haven't switched over to Godot and I continue to develop games using phaser JS. Forcing users to physically install a game onto their computer is simply too much friction for 99.9% of your prospective audience. Lack of robust HTML5 support is a dealbreaker. reply Thaxll 14 hours agoparentprevYou're taking the problem in the wrong order, it's more why would I use Godot which is vastly inferior to unreal and unity. Now what is inferior, well performance is bad, they have their own scripting language which no serious dev would want to use, they have c# integration but it's not on part. Assets, tooling is also inferior, you can't make console game with Godot etc... Networking wise Godot has not much either. That's the difference between an open source engine with not much investment and 100's $ put into unreal and unity. reply xgkickt 17 hours agoparentprevWhatever toolset most of the team are experienced and productive in, and support basically. reply jimmaswell 14 hours agoparentprevCompared to Godot, as a hobby/volunteer game dev on some projects spanning a decade or more: I know Unity already, I enjoy it and have no compelling reason to leave, and it's much more mature in features, optimization, etc. reply djmips 15 hours agoparentprevEfficiency - if you really need it. Unreal, gives you source, so you technically have everything you need to customize and optimize for the console you're targeting. reply tubs 13 hours agoparentprevThe navigation and path planning in godot are pretty primitive. reply Aissen 19 hours agoprevIt's interesting that they did not keep this a paid product, and use that as a lever: 1. to either get more customers 2. or to get Nintendo to remove the terms forbidding the open source release of software built on their platform I am guessing that 1. will happen anyway, and 2. isn't needed since they have found a middle ground with Nintendo hosting details on their developer portal. reply teamonkey 19 hours agoparentThe fact that Godot requires/required a third party studio to port your game to consoles is a major turnoff and one of the things preventing developers jumping on the Godot train. From a solo dev point of view, too scary and expensive; from an indie company's point of view it's an unwanted extra expense and a potential point of failure that you can't control. This opens things up considerably, despite the Nintendo walled garden. reply TillE 16 hours agorootparent\"Official\" paid console ports are coming very soon (Q1 2024) from W4 Games, the company run by Godot's core developers. https://w4games.com/products/ reply LelouBil 19 hours agorootparentprevI'd assume for solo devs porting to multiple platforms is harder without external help. reply runevault 16 hours agorootparentprevNo open source project can do it without third party support, the NDAs/etc required for getting access to the SDKs to build against those platforms prohibits it. reply weinzierl 18 hours agoparentprevWhat does 2. mean exactly? That I am not allowed to publish the source code of my Switch game as open source? What if my Switch port shared code (for example all the GDScript) with other ports. Does Nintendo also forbid me to publish this as open source? Can you use open source assets in a Switch game? reply crysin 17 hours agorootparentI'm not a licensed Nintendo developer but off of my knowledge from the Wii U era... The Godot game engine cannot include any of the Nintendo Switch SDK nor can your game include any of the Nintendo Switch SDK or APIs unless you are a registered Nintendo developer and have signed their NDA. You cannot share any of Nintendos proprietary code or information. So you could share your source code for a Switch game, minus any code or SDKs specific to the Switch. reply weinzierl 15 hours agorootparentThanks! reply magic_hamster 19 hours agoprevThis is great news! Too bad there's no C# support as of yet. But it's definitely a step in the right direction. Plus, not all projects need the performance boost of C#. Seems like Godot is about to grab a bit more market share from Unity. reply lionkor 16 hours agoparentC# is fully supported, you just install godot mono reply vunderba 9 hours agorootparentEven outside of the Nintendo switch, C# is only partially supported. If you want to develop web games in Godot 4 you can't use C#. reply sandyarmstrong 13 hours agorootparentprevFrom the page: > No C# or GDNative/GDExtension: Only GDScript is supported. Native extensions are not supported, but you can try to convert them into internal modules. reply FrustratedMonky 18 hours agoparentprevIn what way? The website says there is C# support. What facet are you describing needs it? Curious, because would also like to use C#, and website says it does, you say it doesn't. I'm wondering what is up. reply tslater2006 18 hours agorootparentThe linked page says the following: No C# or GDNative/GDExtension: Only GDScript is supported. Native extensions are not supported, but you can try to convert them into internal modules. reply FrustratedMonky 16 hours agorootparentRight from site https://godotengine.org/features/#script \"\"Leverage your C# experience to feel right at home If you're an experienced C# user, Godot offers you first-class support for the .NET platform. Power your game with familiar libraries and give them performance boost, while still benefiting from close engine integration. Note: .NET support is provided as a dedicated engine executable. C# support is available for desktop and mobile platforms as of Godot 4.2. Web support should be added in the future, but until then, Godot 3 remains a supported option.\"\" reply strich 16 hours agorootparentHe's talking about the Switch port library. It didn't support C# yet. But also on the wider topic Godot still struggles with making C# a first class citizen compared to its home baked script language. They're improving things but it's not great yet. reply FrustratedMonky 16 hours agorootparentGot it. Thank You. reply johnnyjeans 16 hours agorootparentprevThis likely doesn't apply to the Switch version due to platform restrictions/technical issues yet to be resolved. reply mattsan 18 hours agorootparentprevThe website says this: https://arc.net/l/quote/btugrszz reply FrustratedMonky 16 hours agorootparenthttps://godotengine.org/features/#script \"\"Leverage your C# experience to feel right at home If you're an experienced C# user, Godot offers you first-class support for the .NET platform. Power your game with familiar libraries and give them performance boost, while still benefiting from close engine integration.\"\" reply TheRoque 20 hours agoprevWhat these people do is super cool. How can one learn to be in a team that does things like this ? reply delecti 18 hours agoparentBest bet is probably to start by making a game. One option is to start by building a simple game using an existing engine, and eventually get deeper and deeper into the libraries you're using until you decide to contribute back to that engine, or create your own. Alternatively you could just start by making something incredibly simple, a Pong or NES Mario clone, and then work your way up through the generations to something more modern. The first approach is probably easier, as you'd have lots of tutorials from the community around the modern engine. reply Grimblewald 20 hours agoparentprevCouple of ways I can think of. 1. Build it and they will come 2. Talk to coworkers about a cool idea you had, gather the interested and start leading that project reply echelon 18 hours agorootparent> 1. Build it and they will come This is absolutely true. Make sure you have a Discord or some social means of connecting with folks. reply fwsgonzo 22 hours agoprevUnsurprisingly, no plugins. Just plain-old GDScript, but embedding a faster scripting solution into the C++ codebase is not too hard for serious work. reply andybak 20 hours agoparentWhy unsurprisingly? I'm not familiar with console ecosystems. reply a1o 20 hours agorootparentThe engine has an interpreter for Godot script so if you port it you can already run the games. Native code is it's own porting problem, it should be easy for a developer but game developers in particular that knows an engine may not be familiar with writing native code - they may have been using a plugin they found on the internet and now they will have to port that native code too by themselves. reply runevault 16 hours agoparentprevI'll be curious if once the AOT c# hooks are added (hopefully with 4.3) if that will be able to work or not. reply Capricorn2481 17 hours agoprevI am interested in Godot, but seeing these projects makes me more nervous than not. I always wonder - How seamless is it - How screwed am I if it doesn't work - Would the headache/risk of dealing with Unity be less than the potential catastrophe of not getting a working switch version reply a1o 20 hours agoprevThere are a few companies to choose from here: https://github.com/godotengine/godot-docs/blob/master/tutori... reply kevincox 16 hours agoparent> Currently, the only console Godot officially supports is Steam Deck (through the official Linux export templates). This line makes me so happy that the Steam Deck is doing well. I hope they can show that by making it easier for developers and open source projects they will get more and better games. I'm sure it isn't a huge difference, but if over time an ecosystem can be built it would be very powerful. reply danShumway 15 hours agoprev> License: The source code is distributed under the MIT License, offering broad usage and modification rights. I might be missing something, but how did they manage to swing this? I thought proprietary NDAs around the Switch's API were the biggest holdups preventing Switch support from being added to Godot itself. I would have assumed this would be free but would require signing some kind of source-available license that forced you to still respect the NDAs. reply wmf 14 hours agoparentIt's not vanilla MIT but some kind of MIT + NDA. reply Vespasian 18 hours agoprevHow does that work with the MIT license? I presume they signed a rather restrictive proprieatary license and/or an NDA to get access to nintendos sdks and tool (as is common on other consoles). Then one person breaking the NDA would make the Switch Godotport (and therefore some of Nintendos property (method names /usage )) legally available to anybody else (they didn't sign an NDA after all and it's probably not a copyright violation unless it includes actual code/binary). I am not a lawyer so I'm probably very wrong. reply anthk 17 hours agoparentMIT allows that. LGPL would in theory but you would need to release the potential changes you did to the game engine. GPL woudn't in any way. reply kevincox 16 hours agorootparentBut if they give me the source under the MIT license then I am free to redistribute it. So while they would be in their right to make a proprietary port it seems like they wouldn't be able to legally release it under MIT. (Or at least not the whole thing, as code that touches Nintendo's API would need more restrictions) reply vladms 16 hours agorootparentMy understanding: it is possible to license under a different license any additions or modifications they made. So the original code can remain MIT, but their changes (API integrations, etc) will be under a proprietary license (note: this would not be possible with GPL for example, but MIT is not like that). reply s17n 17 hours agoprevHow can they restrict access to authorized developers but also use the MIT license? Can't any authorized developer simply post the code on the internet? reply SXX 17 hours agoparentBasically to use code they provide under MIT you also need to couple it with a proprietary SDK from Nintendo. If you post any proprietary bits on internet Nintendo will eat you alive. reply kevincox 16 hours agorootparentBut presumably the port touches the proprietary bits. So either A) that interface is not allowed to be distributed under the MIT license and they have a problem with Nintendo or B) the interface is fine to publish under MIT so why not make it more available? reply TillE 16 hours agorootparentIt's comparable to how open source code can also be patent-encumbered. It's MIT licensed but Nintendo's NDA still applies. The license can't supersede that. reply s17n 15 hours agorootparentI feel like the license definitely can supersede that - you obviously wouldn't be GPL compliant if you required everyone who got access to your code to sign an NDA, for example. reply ronsor 10 hours agorootparentNintendo's NDA would only apply to people that signed the NDA. If some third party acquired the code without signing the NDA, the NDA shouldn't apply to them. reply fl0id 17 hours agoparentprevYou answered your own question. reply fifilura 14 hours agoprevIf you want to make games together with your kids there is also Fuze4 for Switch. https://www.fuze.co.uk/nintendo-switch.html reply weberer 19 hours agoprevMy Switch is hacked. Can I still use this to make unsigned Godot games and run them on my Switch? reply thejsa 19 hours agoparentNot without a Nintendo SDK license agreement and signing their NDA. On the other hand, there’s a nice homebrew port you could certainly use that’s been around for years: https://github.com/Stary2001/godot reply Ghufran007 16 hours agoprevI'm intrigued by Godot popping up in Switch games - Sonic Colors being a case in point. What's newsworthy about this particular use? Did Sonic Team do a custom port with Godot? reply drzaiusx11 19 hours agoprevI thought Godot was already being used in Switch games for the past couple years; for example, Sonic Colors used Godot for the engine. What differentiates this release? Did the sonic team make their own port or something? reply koromak 19 hours agoparentIn the PC-to-console world, there is no open solution. Everyone either builds their own ports, or (more likely) pays a porting company to do the work for them, who in turn have built there own solution. Console developer kits are proprietary, no one is allowed to share the API or source code. This seems to be unique in that, if you get a License from Nintendo, you're allowed to access this companies kit for free. reply ukd1 18 hours agorootparentWhich then is MIT licensed - so why can't that then be further shared? reply gjsman-1000 18 hours agorootparentYou aren’t prohibited under the license, but you are prohibited under the contract you agreed to, to get that code. For example, imagine a company hired you to write a library they want to later release as GPL. They still might have rules that you can’t release that library except under certain conditions. The real purpose of the MIT license is not free sharing with the world in this case - it’s so that developers don’t feel legal risk by using this port and building it into their games. reply ukd1 15 hours agorootparentSo, it's not really MIT then - as it's MIT + something else. reply danShumway 15 hours agorootparentprev> You aren’t prohibited under the license, but you are prohibited under the contract you agreed to, to get that code. This doesn't make sense to me. MIT is a license to share and modify code. If another license can supersede that and remove rights granted by the actual license, then what prevents any company from releasing their code under MIT and then adding a shrinkwrap EULA that prevents looking at the code or modifying it? The license wouldn't mean anything. > imagine a company hired you to write a library they want to later release as GPL. They still might have rules that you can’t release that library except under certain conditions. Before it's released as GPL, sure. But once it's actually released as GPL, that company can't prevent you from looking at the code or modifying it or redistributing it. At best, they can sever their relationship with you over doing so, but they can't legally challenge you doing so. If that wasn't the case, nothing would prevent a company from taking 3rd-party GPL code, building on top of it, releasing the product as GPL, but then adding additional contractual requirements to get access to their product that prevented anyone from looking at or sharing that code. The GPL wouldn't have any power or enforceability. Companies do circumvent the GPL a lot and they get called out on it when they don't allow customers to get copies of the source code or to share it online. I've never heard anyone seriously suggest that a valid defense would be for them to say, \"okay, yes we're using GPL code but to buy the product you have to sign a contract and that contract legally prevents you from sharing the code.\" Those restrictions would themselves be a GPL violation. reply wmf 14 hours agorootparentwhat prevents any company from releasing their code under MIT and then adding a shrinkwrap EULA that prevents looking at the code or modifying it? The license wouldn't mean anything. This has been an intentional feature of MIT/BSD licenses all along. Tons of proprietary products are built on MIT/BSD code. reply gjsman-1000 14 hours agorootparentprevDo you have free speech? Yes, it's your constitutional right. But then you become a lawyer and you get some clients. Do you have the freedom to say whatever you want about your client's case, in public? No, you don't. You have given up your right to exercise free speech on those specific issues. Another example. You have free speech to criticize McDonalds. You can mock them all you like. But let's say, you sign a contract to become a PR Spokesman for them. You then mock them publicly, on their official Twitter account. Can you be sued? Yes you can, and your constitutional right will not defend you. Contracts are not licenses. Contracts can give licenses, licenses cannot give contracts. In this case, while you are receiving an MIT-licensed work, you are under contract that you will not share this information with anyone. Contracts always supersede licenses. If you do give that information to anyone in violation of your contract, while the license itself means that the creator of the Godot port will not sue you, Nintendo may sue you for the breach because you are under contract with them. > If that wasn't the case, nothing would prevent a company from taking 3rd-party GPL code, building on top of it, releasing the product as GPL, but then adding additional contractual requirements to get access to their product that prevented anyone from looking at or sharing that code. Congratulations, that's actually the law right now at a certain level, and what the furor over Red Hat Enterprise Linux was about. Red Hat decided to release open-source code under a contract that prohibits distribution. The initial response was to have companies break that contract, but try to remain anonymous, as the receivers of that GPL code would be theoretically safe. This was later deemed too risky, unworkable, and antagonizing; which is why Alma Linux and others have given up on that approach, and just aim for general bug-for-bug compatibility from scratch. reply wly_cdgr 18 hours agoprevVery cool project, but if they want this to matter for commercial indie devs, it needs to have C# support. It's not so much about performance, it's more about working with a marketable-skill language, ease of coming over from Unity / Monogame, and expertise level of developer community. reply ChadNauseam 15 hours agoprevIgnorant question: I don't understand why this type of thing is necessary. IIUC there is nothing illegal about looking at leaked SDK docs and using emulators to contribute a Switch build target for Godot. Then game developers who want to develop Switch games could buy a devkit and build their games for switch, just like how I can build an APK for Android in Unity and load it onto my phone. Is the only thing stopping open source game engines from doing this a lack of time/funding/expertise/interest? reply b33j0r 17 hours agoprevSo to clarify, a free game engine is being made available for free, if I have paid and/or been approved to be a qualified developer? At least words still mean something reply SXX 17 hours agoparentConsole version is using proprietary SDK and they can only provide you source code if you signed NDA with Nintendo. That's it. reply ufo 19 hours agoprevWhat was the status quo before this? reply koromak 19 hours agoparentYou find a company who does Godot ports and pay them, with zero visibility into the underlying tech reply tussa 20 hours agoprev [–] Does Nintendo take a 50% cut if you publish on their eShop? reply utf_8x 20 hours agoparent [–] It's most likely around 30% which seems to be the norm for stores like this reply teamonkey 15 hours agorootparentEpic’s store is 12%. Tim Sweeney tweeted that Epic take 5% as profit and the rest are costs[1]. Puts the other stores’ 30% in context, though Xbox/Playstation/Nintendo do significantly more QA than Epic. [1] https://twitter.com/TimSweeneyEpic/status/112044179501033881... reply miohtama 19 hours agorootparentprev [–] Also, physical retail distribution is likely around the same 30% ballpark. reply wmf 14 hours agorootparentFor a console game you'd have to pay a 10-20% title fee and then the retailer also takes 30-40%. reply prophesi 18 hours agorootparentprev [–] And is why Apple chose 30% when they launched the app store; it was already an acceptable charge, and could be justified with storage/bandwidth costs and the suite of tools/services built for it. That doesn't seem to line up in my books, but we'll see how the next few years go while the 30% commission standard still has a target on its back. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Godot Engine has released a free port of their engine specifically for Nintendo Switch developers.",
      "The port is compatible with certain versions of Godot Engine and offers basic functionality, although it does not support C# or GDNative/GDExtension.",
      "While the port may not be highly optimized, it is suitable for small to mid-sized game projects. Interested developers can request access through the Nintendo Developer Portal and are encouraged to seek support from the community."
    ],
    "commentSummary": [
      "The conversation covers multiple aspects of game development, such as homebrew games, piracy, and challenges faced in developing for consoles like the Nintendo Switch.",
      "The limitations of developing and distributing games on different platforms are discussed, as well as the use of popular game development engines like Godot, Unity, and Unreal.",
      "The importance of licensing and contracts in the gaming industry is emphasized, shedding light on the complexities and restrictions faced by developers in this field."
    ],
    "points": 379,
    "commentCount": 209,
    "retryCount": 0,
    "time": 1706009218
  },
  {
    "id": 39103419,
    "title": "Is Life an Expected Phase Transition in the Universe?",
    "originLink": "https://arxiv.org/abs/2401.09514",
    "originBody": "Quantitative Biology > Populations and Evolution arXiv:2401.09514 (q-bio) [Submitted on 17 Jan 2024] Title:Is the Emergence of Life an Expected Phase Transition in the Evolving Universe? Authors:Stuart Kauffman, Andrea Roli Download PDF Abstract:We propose a novel definition of life in terms of which its emergence in the universe is expected, and its ever-creative open-ended evolution is entailed by no law. Living organisms are Kantian Wholes that achieve Catalytic Closure, Constraint Closure, and Spatial Closure. We here unite for the first time two established mathematical theories, namely Collectively Autocatalytic Sets and the Theory of the Adjacent Possible. The former establishes that a first-order phase transition to molecular reproduction is expected in the chemical evolution of the universe where the diversity and complexity of molecules increases; the latter posits that, under loose hypotheses, if the system starts with a small number of beginning molecules, each of which can combine with copies of itself or other molecules to make new molecules, over time the number of kinds of molecules increases slowly but then explodes upward hyperbolically. Together these theories imply that life is expected as a phase transition in the evolving universe. The familiar distinction between software and hardware loses its meaning in living cells. We propose new ways to study the phylogeny of metabolisms, new astronomical ways to search for life on exoplanets, new experiments to seek the emergence of the most rudimentary life, and the hint of a coherent testable pathway to prokaryotes with template replication and coding. Subjects: Populations and Evolution (q-bio.PE); Biological Physics (physics.bio-ph) Cite as: arXiv:2401.09514 [q-bio.PE](or arXiv:2401.09514v1 [q-bio.PE] for this version)https://doi.org/10.48550/arXiv.2401.09514 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Stuart Kauffman [view email] [v1] Wed, 17 Jan 2024 15:22:32 UTC (1,654 KB) Full-text links: Access Paper: Download PDF view license Current browse context: q-bio.PEnewrecent2401 Change to browse by: physics physics.bio-ph q-bio References & Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer (What is the Explorer?) Litmaps Toggle Litmaps (What is Litmaps?) scite.ai Toggle scite Smart Citations (What are Smart Citations?) Code, Data, Media Code, Data and Media Associated with this Article Links to Code Toggle CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub Toggle DagsHub (What is DagsHub?) Links to Code Toggle Papers with Code (What is Papers with Code?) ScienceCast Toggle ScienceCast (What is ScienceCast?) Demos Demos Replicate Toggle Replicate (What is Replicate?) Spaces Toggle Hugging Face Spaces (What is Spaces?) Spaces Toggle TXYZ.AI (What is TXYZ.AI?) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower (What are Influence Flowers?) Connected Papers Toggle Connected Papers (What is Connected Papers?) Core recommender toggle CORE Recommender (What is CORE?) About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs. Which authors of this paper are endorsers?Disable MathJax (What is MathJax?)",
    "commentLink": "https://news.ycombinator.com/item?id=39103419",
    "commentBody": "Is the emergence of life an expected phase transition in the evolving universe? (arxiv.org)373 points by harscoat 19 hours agohidepastfavorite327 comments johngossman 15 hours agoThere is a whole genre of these, starting with (as mentioned in the paper's introduction) \"What is Life?\" by Schrodinger. I've been idly working my way through a bunch of them, including Monod's \"Chance and Necessity\" (dated but excellent), Nick Lane's whole series of books (notably \"The Vital Question\"), Nurse's \"What is Life?\" (good if you want to learn about yeast), Zimmer's \"Life's Edge\" (haven't finished it yet, seems good). Honestly, the details change, and the emphasis of each author, but they are all speculative and hand-wavey. Pre-paradigmatic. My favorite quote is from \"Life on the Edge\" by McFadden and Al-Khalili: \"Biologists cannot even agree on a unique definition of life itself; but that hasn’t stopped them from unraveling aspects of the cell, the double helix, photosynthesis, enzymes and a host of other living phenomena\" reply wanderingstan 14 hours agoparentIt’s been a while, but in this vein I enjoyed Chaitin’s Toward a Mathematical definition of “Life”: http://home.thep.lu.se/~henrik/mnxa09/Chaitin1979.pdf Blew my mind as I started to wrap my brain around information theory. reply jiggawatts 10 hours agorootparentThe Lex Fridman interview with Lee Cronin \"Controversial Nature Paper on Evolution of Life and Universe\" really opened my eyes. I find myself agreeing with the essential points Lee lays out, that there is a mathematical / physics-based definition for life or life-like systems such as advanced AI and robotics. Link: https://www.youtube.com/watch?v=CGiDqhSdLHk reply jmcqk6 15 hours agoparentprevIt's a complex topic. I think it really got kicked off with \"What is Life?\" and we've been able to build more details on it since then. There are many parts of the story that we know in incredible details. Chaos theory, information theory, non-equillibrium thermodynamics, complexity and emergence, auto-catalytic chemistry are all just parts of it, and each one are massive fields of study on their own. I'm not sure there will ever be a synthesis of all these things that creates a paradigm of some sort. There's simply too much. reply ixaxaar 15 hours agorootparentI think the complexity researchers over the years have created a wide range of tooling for analysing complex systems across a large number of domains. Consider complexity economics, computational social sciences, network sciences and cascading networks, evolutionary theories, parts of systems biology, connectomics and computational neuroscience etc. The Santa Fe institute has been at the forefront and has an amazing collections of publications in their own press. David krakaeur's book \"Worlds Hidden in Plain Sight: The Evolving Idea of Complexity at the Santa Fe Institute\" is like a 20 year survey paper and an amazing read. reply photonthug 10 hours agorootparentI've been on this same journey through the Santa Fe institute pubs on this topic, which all started in a weird place because I wondered what kind of bizarre scientific research outfit had cormac McCarthy on staff as an artist in residence apart from the rest of the conspicuously multidisciplinary luminaries. What a unique, strange, and special intellectual atmosphere they must have had going in their prime and hopefully the best days are ahead still. It seems hard to compare to anything else.. one thinks of bell labs but probably only the peak of the Vienna circle comes close. We need more places with artists, philosophers, mathematicians, physicists, and computer scientists rubbing shoulders in the halls. reply fuzzfactor 2 hours agorootparentprev>Is the emergence of life an expected phase transition in the evolving universe? Depends on whose expectations you are trying to meet. To some it would be a major transition, to others it's just another everyday minor non-event in the very macro scheme of things. If you're going to look at it mathematically it probably makes a difference when the number of non-extreme planetary environments begins to become significant compared to the number of possible chemical reactions among the limited number of elements in nature. Only one of these numbers can reach truly astronomical proportions. reply johngossman 11 hours agorootparentprevAgree on “What is Life?” but recently learned that Leo Szilard is the first one to connect entropy and life, in a 1929 paper. https://fab.cba.mit.edu/classes/863.18/notes/computation/Szi... Szilard seems to have been involved in everything. reply LASR 7 hours agoparentprevMy hypothesis is that there is some objective definition of these phase changes such that the precise definitions of life etc doesn't matter. My personal stab at an objective defintion: There is a sudden emergence of many more pathways for decay of the universe through entropy. The mechanisms that accelerate the creation of these new pathways is the set that includes \"life\". Now with that base definition, we can talk about various forms of life - ones that involve DNA for example. But also others that do not. reply p-e-w 7 hours agorootparentThat sounds like metaphysics. The Universe doesn't \"want\" to increase entropy, entropy increase is simply a consequence of applying basic statistics to matter and energy. There is no mechanism that actively explores various pathways for making entropy increase faster. reply scirpaceus 6 hours agorootparentThis is also an argument that didn't sit well with me when I heard Guillaume Verdon / Beff Jezos on his recent Lex podcast state that the universe wants to produce more entropy, and that life evolved the way it has because it's more efficient at producing heat and entropy than, say, a rock. Perhaps he said that loosely as a figure of speech, because it should be obvious (religious beliefs aside) that the universe does not \"want\" anything. The remaining question is whether the emergence of life as an efficient entropy generator is coincidental to the laws of thermodynamics, or incentivized (in an evolutionary pressure sense) by them. Of relevance, the tendency of matter to organize optimally into energy-absorbing and heat-dissipating structures is a whole theory of its own - see MIT Jeremy England's theory https://www.quantamagazine.org/a-new-thermodynamics-theory-o... reply creer 3 hours agorootparentYes, this \"want\" of pop science is annoying. I don't see anything lacking with simply using something like \"enables\", \"allows\". An environment with sticks allows evolutionary paths that can use sticks. A world with pools of high energy makes it easier for energy-using systems to develop there as opposed to outside of them. A world with light - go figure - makes it possible for light-using systems to subsist. reply shiroiuma 13 minutes agorootparentprevIt can be said to \"want\" these things. When referring to something without consciousness or free will, such as a magnet, we can colloquially say something like \"the magnet wants to move towards a (larger) ferrous object\". Of course, this attempt at movement through magnetic force is just a consequence of the magnetic attraction between the two objects and not some type of conscious thought, but criticizing the use of the word \"want\" here seems like needless pedantry to me. This is HN, not a peer-reviewed scientific paper. reply mycall 5 hours agorootparentprev> There is no mechanism that actively explores various pathways for making entropy increase faster. Dark matter? reply zukzuk 15 hours agoparentprevI'd add \"Into the Cool\" by Eric D. Schneider and Dorion Sagan to that reading list. I'm pretty sure that one was originally recommended here on HN, so I guess I'm feeding it back into the echo chamber. But well worth a read! reply blueprint 14 hours agoparentprev\"life is the process from birth until death\" \"life is the way to make anything happen to you\" without a body (via life), a consciousness cant do anything reply Tepix 6 hours agorootparentIf you're merely running in a computer you could still communicate with someone else and do things that way. reply blueprint 2 hours agorootparentthat's called interfacing with a \"body\" thanks for the downvotes, philosophy noobs reply jyounker 17 hours agoprevI went I read the first few paragraphs I thought, \"Is someone ripping off Stuart Kaufmann? He was writing about this idea thirty years ago.\" Then I read the first author: Stuart Kaufmann. For those of you following along at home, Kaufmann has been developing the ideas here for decades. The paper is less a \"here is a new idea\" and much more \"here is a concise summary of 50 years of work\". The words and thoughts seem opaque, but this is case where they actually have concrete and specific meanings. It's worth noting too, that towards the end of the article he outlines experiments that could be used to falsify the theory. If you want a really hard-core dive into the ideas, then check out his 1993 book, \"On The Origins of Order\" (ISBN 978-0-19-507951-7). https://global.oup.com/academic/product/the-origins-of-order... reply jamesblonde 14 hours agoparentRecommend reading along with order out of chaos and anything from Prigogine in the 80s (not his last book). reply lachlan_gray 18 hours agoprevIt's fascinating to me that the complexity of life always goes up. Outside extinction events, complex life generally seems to become more favourable over time. It's interesting that (to my knowledge) we don't see an ecosystem lose complexity in its entirety unless it's dying. Simple organisms make a bedrock for complex organisms, and while the complex organisms have more specific needs, they are better at exploring, gaining, branching out. So they also kind of make a nest for the simple organisms by sprawling into the void and finding habitable niches that simple organisms wouldn't reach on their own. On the time scale of technology, we began reaching out to other intelligences the second that we could, began trying to make them the second we thought we could know how. It's very reasonable to say that percolation is a defining property of life and intelligence. I also think a lot of scifi's like hyperion, neuromancer, foundation. In human writing of the future, it seems like the endgame of higher intelligence is to find or create other intelligences, and get closer to them. Then interesting things happen in the wake of that. reply jmcqk6 17 hours agoparentCheck out the work of Ilya Prigogine - he won a Nobel prize in the 70's for his work on self-organizing complexity. There is a selection force in the universe for increasing complexity, being driven by the dissipation of energy. When you have systems in non-equilibrium, there is a strong pressure to explore the possibility space to find ever more efficient ways to dissipate energy. A bacteria the size of a grain of sand dissipates much, much more energy than the grain of sand, and so there is a very strong \"preference\" for bacteria in this regard. reply forinti 17 hours agorootparentThe Sun outputs 3.9 x 10^26W, but it weighs ~10^30kg, so that would mean 10^-4W/kg. Human beings are closer to 1W/kg at rest. reply evanb 12 hours agorootparentThe sun's power density is approximately the same as compost. reply worldsayshi 8 hours agorootparentSo does that mean that making a fusion reactor makes as much sense as building a containment vessel for a compost reactor? reply lajy 7 hours agorootparentStars have to have enough mass to have enough gravity to start and sustain a nuclear reaction that constantly blasts energy out in all directions. This limits just how energy-dense they can get. What they lack there, they make up for in size and lifespan. We can make sustained nuclear reactions in much less space using engineered pressure instead of gravity, so that skews the energy density ratio. reply hermitcrab 2 hours agorootparentprevFusion only happens in the Sun due to incredibly rare quantum tunnelling events. That is why the sun can burn for billions of years. It takes a long time for the energy generated to reach the surface and escape, that is why the sun has a high temperature. IIRC fusion reactors on earth use magnetic confinement to raise the temperature much higher than the sun's interior for a much higher rate of fusion reactions. I'm sure someone will be along to correct me if I got that wrong. ;0) reply bmitc 6 hours agorootparentprevThe Sun uses gravity, due to its mass, for fusion containment. Fusion reactors on Earth use electromagnetic containment. reply drewrv 12 hours agorootparentprevCan you elaborate on that or drop a link that explains more? reply evanb 4 hours agorootparentHere's a decent stackexchange discussion: https://physics.stackexchange.com/questions/370899/suns-powe... reply mr_toad 10 hours agorootparentprevThe rate of fusion in the Suns core is actually very slow and gram for gram it is not producing much energy. But the core of the Sun is well insulated by the vast bulk of itself, so even a slow release of energy builds up to extreme temperatures. https://en.wikipedia.org/wiki/Sun#Structure_and_fusion reply s1artibartfast 12 hours agorootparentprevLive/fresh compost creates heat as a byproduct of cellular metabolism during decomposition. I assume this is is using thermal output of this compost as a measure of energy. What more do you want to know. reply jmcqk6 16 hours agorootparentprevWow, that's a great way of describing this! Definitely noted for future use. reply epistasis 14 hours agorootparentReally brings up the difference between the energy density of the fusion reaction and the energy density of fusion plus the confinement system to enable the reaction! reply anticensor 12 hours agorootparentHmm, then the challenge is to find a more efficient confinement for a fusion reaction. reply s1artibartfast 12 hours agorootparentprevThe average American also outputs about 125W/kg if you include our technology, and not metabolism. US Primary energy consumption = 30 Trillion Kwh/ year [1] 30 Trillion Kwh/ year / 8760 hrs = 3.3×10^12 W (watts) 3.3×10^12 W (watts)/ 330million = 10 kW/ person 10 kW/ person / 80kg = 125 W/kg https://www.eia.gov/energyexplained/us-energy-facts/ reply notfed 14 hours agorootparentprevThis is enlightening. Has anyone posted some kind of comparison table somewhere? reply adrianN 16 hours agorootparentprevNow do a lump of plutonium. reply conesus 16 hours agorootparentHow did that lump of plutonium become a lump? That part wasn’t natural. reply mapreduce 16 hours agorootparent\"How did that lump of plutonium become a lump? That part wasn’t natural.\" Allow me to become a little philosophical but since human beings which are product of nature made plutonium, isn't the making of plutonium natural too? I mean everything that is happening in this universe is natural! I know the general usage of the words \"artificial\" for human-made and \"natural\" for everything else. But when we are talking at the grand scale of life and universe I think a human-made plutonium is as natural as bee-made honey. reply namaria 15 hours agorootparentI love debating this with people but ultimately it's just playing games with semantics. The notion of artificial x natural is very recent and very localized. Some cultures would differentiate raw from cooked in a similar sense. But it's like talking about what is really green vs what is really blue. Completely circular since it depends on the definitions of the terms. reply jonhohle 15 hours agorootparentIt's easily extendable to the animal world as well. Is a nest created by a bird or a den created by one of various mammals natural or artificial? Is a nest made by mice in my garage from synthetic fabrics, flexible plastics, and whatever plant matter it can find natural or synthetic? My wife was recently asked to make a meal for someone who didn't eat \"processed\" foods. What level of manipulation needs to happen before a food is \"processed\"? Can beans or rice be dried and put in a bag? Can chicken broth be used if it's homemade, but the chicken came from a commercial farm? Or is extracting broth from a chicken processing it? I've increasingly noticed many sub-cultures adopting odd definitions and interpretations of commonly used language with the expectation that everyone who interacts with their group understand their dialects. It's not really jargon or vernacular since the words are common to the language, just used to mean something different than the general population would understand. Similarly, artificial is now assumed to mean bad and natural good, when neither ascribe value by definition or in practice. reply namaria 14 hours agorootparentYou could approach the language problem like that as well. Before imperial efforts in recent centuries to normalize languages in certain territories, there was no Standard French or German or Italian. Vocabulary and accents changed slowly across the landscape, following geography - places isolated diverged and places integrated converged. Migration, trade and conquest added layers of complexity to this variation. But your idea that people are failing to use Standard English and creating language subcultures around peculiar meanings of artificial/processed/chemical vs natural/homemade/organic is itself based on a very artificial distribution of language. reply achierius 15 hours agorootparentprevPerhaps a better phrase to have used would have been \"not prior to a complex system\" -- as the lump of plutonium exists only because of a complex system doing its thing, it should be considered a consequence of the complex system rather than an alternative. reply InitialLastName 12 hours agorootparentTo attempt to pedantically clarify, is \"exists only because of a complex system doing its thing\" not true of basically any pure lump of material that exists? To me, the actions of stars fusing heavy atoms and then those atoms ending up in lumps of material somewhere sounds like a pretty complex system doing its thing. reply forinti 16 hours agorootparentprevLife is a great entropy accelerator. reply mensetmanusman 15 hours agorootparentcompared to a black hole though? reply forinti 14 hours agorootparentIt might be too soon to tell. We might start creating them ourselves. reply bell-cot 15 hours agorootparentprevWhich isotope? It makes quite a difference, and there's no \"naturally occurring\" with Pu. reply echelon 12 hours agorootparentprevSo we really are batteries for the Matrix. Joking aside, this comparison is really beautiful. reply bloopernova 13 hours agorootparentprevI absolutely adore the simplicity behind this: Energy must balance, so it flows from high concentrations to lower concentrations. Things that help that flow are selected for. From that springs everything we are. Utterly amazing to me that from a gradient plus some random chemicals plus time equals humans, sex, violence, loss, birth, love, games, music, and so much more. All just to help the earth cool down a little bit faster. reply kaashif 2 hours agorootparent> Things that help that flow are selected for. I don't get this part, why does this follow? Why would life that transfers heat faster get selected for? This seems backwards to me: in reality it seems like life that is more capable or more adapted reproduces more, and that results in more heat moving around most of the time. But progress can go backwards, if there were a nuclear war this would reduce heat transfer. reply friend_and_foe 3 hours agorootparentprevIt doesn't help the earth cool down faster, it actually traps energy for longer in metabolic processes. But it does red shift it. The film of muck on the surface that we call life lowers the planet's albedo and the radiation that shines out of it is of a longer wavelength than if it were just reflected. reply tsunamifury 12 hours agorootparentprevAbsolutely none of this follows reply WaxProlix 16 hours agorootparentprevSo we're all agents of Entropy, spinning our circles until we die all in the service of heat death? I don't like it! reply namaria 15 hours agorootparentI don't understand this view at all. We are what we are. Entropy is a concept in our mind and heat death is a very particular phenomenon that depends on the systems experiencing it being very specifically defined in ways that the Universe cannot be. We are. Descartes had the right idea. The experience of consciousness is irreducible. To say \"we are atoms thinking\" and variations thereof is utterly meaningless. All words and concepts used for such word play is dependent on the experience of human consciousness. They don't exist independently. Consciousness is the only thing we can posit that has independent existence. Everything else is just a concept created by a conscious being. It's like the tree falling in the forest with no one to hear it. A universe without consciousness experiencing it really exists? Existence is a property of consciousness. We can't conceive of things that are not experienced, by definition. Even if we imagine a dead universe with just energy and no consciousness, that is an image that exists inside of a conscious mind. reply dvt 15 hours agorootparent> Existence is a property of consciousness. Kant would vehemently disagree. Not that I'm a Kantian, but he makes some pretty good points. So I think there's a lot of work here that needs to be done to make your argument stronger. reply wolvesechoes 14 hours agorootparent> Not that I'm a Kantian, but he makes some pretty good points It is even debatable what points he really made. As always in philosophy. reply dvt 10 hours agorootparentI'd say that existence not being a predicate of any object is kind of non-debatable :) reply wolvesechoes 55 minutes agorootparentIt is, actually. Kant differentiates between \"real predicates\" and \"logical predicates\", and only states that existence doesn't belong to the first category, because it \"doesn't enhance the concept\". The whole distinction is quite unclear, and Kant's claims can be interpreted in multiple ways, as people do in some publications. Actually this famous slogan that \"existence is not a predicate\" is closer to proposals of Russell and Quine. Disclaimer: quoted phrases from Kant may not match exactly what you can find in English translations. I know Kant from Polish translations, so I improvised a bit. reply namaria 15 hours agorootparentprevI'm not into winning debates. I just find interesting that conscious beings think they can imagine the absence of consciousness. For us consciousness is the fundamental reality. Furthermore I find the verbosity of German philosophy nearly unbearable, to be honest. reply zoogeny 12 hours agorootparentYou say: \"We can't conceive of things that are not experienced, by definition\" I mean, you probably don't even realize that this view is influenced by Kant. You are giving a very poor retread of 200+ year old German idealism. For example, from the Wikipedia article on Critique of Pure Reason [1], Kant's major work: In the preface to the first edition, Kant explains that by a \"critique of pure reason\" he means a critique \"of the faculty of reason in general, in respect of all knowledge after which it may strive independently of all experience\" I think it is fair to criticize his prose but it isn't like you don't have access to well over 200 years of commentary and follow-ups to one of the most famous and important philosophers within the Western tradition. For a gentle introduction I suggest this video [2] (42 minutes) where Geoffrey Warnock (at the time the Vice Chancellor at Oxford) provides an overview of Kant's ideas. It is also fair to disagree with Kant, but it is pretty obvious when you are talking about the subject he dominates while having no experience with his work. The reason he is so famous is that he had very compelling things to say on this very subject. 1. https://en.wikipedia.org/wiki/Critique_of_Pure_Reason 2. https://www.youtube.com/watch?v=wlEMkAkGS1I reply namaria 2 hours agorootparentIt's a fairly narrow view if you think I shouldn't philosophize unless I conform in full to all YOU have read. reply mordechai9000 15 hours agorootparentprevAs someone said about thermodynamics: You can't win, you can't break even, and you must play the game. (Source unknown) reply mrkstu 17 hours agorootparentprevIs there already a theory for life->greater complexity similar to that of entropy in physics? It seems just as inexorable. reply pizza 17 hours agorootparentYou might really like the short book What is Life? by Schrödinger, it delves into exactly that. reply bmitc 17 hours agorootparentprevThe theory is that they are one in the same, or better to say that entropy is the process that drives life. In that, life grows in complexity in order to dissipate heat (i.e., increase entropy) more efficiently. Just look at Earth. Life is incredibly complex but is ultimately driving everything towards dust. reply lioeters 16 hours agorootparent> entropy is the process that drives life Depending on how you look at it, life is driven by the opposite of entropy. Schrödinger in his book, What is Life?, calls it \"negative entropy\" or even \"free energy\". > In the 1944 book What is Life?, Austrian physicist Erwin Schrödinger, who in 1933 had won the Nobel Prize in Physics, theorized that life – contrary to the general tendency dictated by the second law of thermodynamics, which states that the entropy of an isolated system tends to increase – decreases or keeps constant its entropy by feeding on negative entropy. > The problem of organization in living systems increasing despite the second law is known as the Schrödinger paradox. This, Schrödinger argues, is what differentiates life from other forms of the organization of matter. > Schrödinger asked the question: \"How does the living organism avoid decay?\" The obvious answer is: \"By eating, drinking, breathing and (in the case of plants) assimilating.\" While energy from nutrients is necessary to sustain an organism's order, Schrödinger also presciently postulated the existence of other molecules equally necessary for creating the order observed in living organisms: > \"An organism's astonishing gift of concentrating a stream of order on itself and thus escaping the decay into atomic chaos – of drinking orderliness from a suitable environment – seems to be connected with the presence of the aperiodic solids...\" We now know that this \"aperiodic\" crystal is DNA, and that its irregular arrangement is a form of information. https://en.wikipedia.org/wiki/Entropy_and_life#Negative_entr... reply bmitc 13 hours agorootparentI don't think you understand my point or what Schrodinger wrote. The Wikipedia synopsis of Schrodinger is unlikely to be accurate. https://www.quantamagazine.org/a-new-thermodynamics-theory-o... https://www.bbvaopenmind.com/en/science/leading-figures/ilya... reply lioeters 12 hours agorootparentThank you, I'm familiar with the work of Ilya Prigogine, and the first linked article is one of my favorites that I've read several times. The relationship between life and entropy is a fascinating topic for sure. reply bmitc 9 hours agorootparentIt seems we've talked at each other then, haha. reply knome 17 hours agorootparentprev>life grows in complexity in order to dissipate heat This seems silly on the level of the anthropic principle. It's like claiming a calculator exists to use up electricity. We're eddies in the flow of energy from high to low entropy because it's free energy. We create more entropy in capturing energy than we capture because it's impossible not to. There's no purpose for life there. It's just where life lives. reply keithwhor 16 hours agorootparent> It's like claiming a calculator exists to use up electricity. sighs in bitcoin reply uoaei 16 hours agorootparentprevThere is a minor pedantic point to make that akshually heat dissipation carves channels of energy flow that look to us like life rather than the causality going in the other direction. reply manmal 16 hours agorootparentprevVery interesting. Do you have an intuition for why there is selection pressure towards energy dissipation? reply jmcqk6 15 hours agorootparentMy current intuition goes something like this: Energy MUST flow. No matter how energy is captured and stored, there is a pressure for it to continue moving. The movement of energy means that matter is always moving, and new configurations are always being \"discovered.\" Some configurations allow for energy to flow more easily, and when one of those configurations is \"discovered\" the movement of energy keeps that configuration in place. I think it's literally a strange attractor from chaos theory. Areas of stable energy flow create correlations across space time that allow for more complex correlations to emerge. reply rileyphone 14 hours agorootparentHere’s an interesting network-focused view as to why that might be: https://royalsocietypublishing.org/doi/10.1098/rspb.2012.286... reply sydbarrett74 12 hours agorootparentprevSorry if this question is answered elsewhere in this post's replies, but would you say that matter can be viewed as 'captured' energy, or energy at rest? reply nyssos 12 hours agorootparent> matter can be viewed as 'captured' energy, or energy at rest? No, matter isn't 'captured' energy any more than it's 'captured' mass or momentum. Energy is a quantity, not a substance. reply PaulDavisThe1st 11 hours agorootparentEnergy is a behavior, or alternatively a probability. reply PaulDavisThe1st 11 hours agorootparentprevTweak your intuition: energy IS motion. When things are not moving, there is no energy. When there is energy, things are moving. reply jmcqk6 10 hours agorootparentHow would you describe a rock sitting on the top of a mountain? It is not moving relative to the matter around it, but it certainly contains quite a bit of energy, in a variety of different forms. I am trying to recognize that even in that rock, energy is dissipating in multiple ways, and any number of different events can lead to it dissipating in different ways. I suppose you can say that the individual atoms composing the rock are moving, but are those movements connected to the potential energy it has by virtue of being far from the relative minimum in a gravity well? That rock could fall down the mountain mostly intact - a highly energetic event, compared to being eroded away chemically by rain over millennia. reply pizza 8 hours agorootparentA static point mass in a gravitational field is already at its lowest energy state (a literal ground state if you will). It can only really do one thing, which is to stay. The logarithm of the number of states then is zero. So in some sense it cannot encode information at all. In fact I think more generally speaking, things that are at a global minimum of energy level cannot really be used to encode information (unless you have a mechanism to add entropy w/o increasing energy), because they have one state, and the log of the number of states then gives you zero bits. Probably another way to look at the idea that gravitational potential path-independence means memorylessness means devoid of bits of information. reply csomar 6 hours agorootparentprevThe rock is continuously applying pressure to the mountain. That's why we have round planets and not huge random looking rocks. On an atomic level, the atoms themselves are full of action, though that will take a really long time to see the consequences of that. ps: I subscribe to the heat death theory. reply PaulDavisThe1st 10 hours agorootparentprevpotential energy is. IMO, largely a pedagogical tool. it's an explanatory position, rather than a thing. the rock, does, obviously \"contain\" energy thanks to e=mc2. but the notion that the rock is energetically in a different state as it sits on top of the mountain that when it sits at the bottom never sat well with me in high school, and it still doesn't, 45 years later. reply nerdbert 9 hours agorootparentIt takes energy to move a rock to the top of a mountain, and you can get some of that energy back (minus friction etc) if you let it roll down. So it's got something. reply JoeAltmaier 10 hours agorootparentprevThen energy is relative? An object is 'stationary' in space, a comet comes along and hits it. The comet has energy. OR the comet is just sitting there, 'stationary', the object hits it. Not the object has energy. There's no total energy to a system, except calculated from some point of view? reply PaulDavisThe1st 10 hours agorootparentenergy is a term we use that describes the probability for some unit of matter to be in a different state at time T' than it was at time T. if you consider the state to include position - relative to a frame of view - then energy describes (at least in part) the likelihood that the position at time T' is different than at time T reply calf 16 hours agorootparentprevThat is mind blowing! It reminds me of Conway and Wolfram's automata theories although perhaps those models did not emphasize the energy aspect of it. reply jamesblonde 14 hours agorootparentprevYou forgot to mention that Prigogine's model includes a system boundary. Within the system, the 2nd law of thermodynamics no longer holds - the system does not tend towards entropy, as the system ingests energy and exports entropy. reply PaulDavisThe1st 11 hours agorootparentPut in Prigogine's terms: the system is not at, and does not reach, equilibrium (the state that most other chemical science tends to assume). reply ajuc 12 hours agorootparentprevSo where's the bacteria outcompeting moon dust? Theories which make life inevitable are inherently shaky because we have 1 sample. reply bmitc 5 hours agorootparentWhere are the conditions for life on the Moon? You can't have something if the conditions to form it are missing. Your question is equivalent to asking why stars don't form outside dust clouds. reply ajuc 1 hour agorootparentThere are ways for the matter on the moon to dissipate energy faster than moon dust. We could build a moon base for example. It doesn't do it. In fact as far as we know - most of the space in the universe is doing the dissipation in a very inefficient way. Which means the theory that it will do it simply because of the \"selection pressure\" is empirically wrong. reply nerdbert 9 hours agorootparentprevYou can't have theories without life to conceive of them, so in fact life is inevitable under any theory. reply ajuc 1 hour agorootparentThat's just antrophic principle, no need for any selection pressure or any particular theory at all. That's why all of them are shaky. reply hughesjj 15 hours agorootparentprevI love it. Was susskind inspired by this at all with his complexity=action (duality) [1] hypothesis? Theres a bunch of lectures of him on YouTube going off about how complexity increases asymptotically greater than entropy in a black hole, but I need to refresh myself on the lecture. Disclosure: I'm an idiot, and may be spewing nonsense [1] https://en.m.wikipedia.org/wiki/CA-duality reply UniverseHacker 14 hours agoparentprev> It's fascinating to me that the complexity of life always goes up. This isn't true- many things evolved to be simpler over time. For example, some viruses are beleived to have evolved from parasitic bacteria, which themselves evolved from free living bacteria. Many other parasites have simplified and lost the ability to survive without a host. You also have examples like many cave and underground animals losing eye sight and pigmentation. Also consider things like marine mammals losing limbs land mammals had, and many sedentary/fixed marine invertebrates evolving from free swimming ones. There are costs to complexity, and so organisms evolve it when needed, and lose it quickly when it isn't giving an advantage... there is not an \"arrow of complexity\" that only moves one direction. reply empath-nirvana 17 hours agoparentprev> It's fascinating to me that the complexity of life always goes up. A lot of lifeforms evolve to be more simple, not more complex -- I think what you have is sort of a distribution of complexity, and as life continues to evolve, the upper bound keeps getting pushed up as some organisms push the boundary of complexity, but I don't think it's at all true that in general life involves to be more complex. reply calamari4065 16 hours agorootparentSingle-celled organisms are almost infinitely more complex than, say a self-replicating RNA molecule. That again is vastly more complex than a protein or an amino acid. Similarly, a human is nearly infinitely more complex than a single-celled organism. Evolution causes organisms to fill an ecological niche. Simple niches for simple organisms will always exist, and simple life will always exist, even as the upper bounds of organic complexity trend unerringly upward. Life tends toward complexity, but that doesn't obviate the need for simple organisms. reply 0cf8612b2e1e 14 hours agorootparentprevArguably, viruses exist by shedding as much complexity as possible. Trim their genome to the absolute minimum which can still propagate. reply 3rd3 12 hours agorootparentAs others have noted, it is more about the maximum complexity increasing than mean or median. Simple structures keep existing as long as they have their niche, and a human's niche is not (yet) that of viruses. This also reminds of Gall's law that complex systems evolve from simpler ones. You can also see it in neural nets, where larger ones have a higher spatiotemporal resolution and can do more complex things. More model capacity allows to model the environment and self more accurately which allows to outperform other structures in negentropy consumption often at the cost of the other structures (zero sum). This exerts selection pressure toward increasing complexity. That also largely explains group and country disparities. I am not sure that non-evolving things really fit into the same pattern. A burning fire does not necessarily displace inert matter, nor did it arise from competition. Physics and chemistry are more fractal-like possibly the result of enumeration of all computational structures (see Tegmark's mathematical universe hypothesis or Wolfram's ideas on the computational universe). Not fractal-like in terms of self-similarity (although there is some at different scales), but fractal-like in terms of chaotic complexity like a pseudorandom number generator but with more rule-like structures in between. Wolfram also classified such computational patterns. reply nextaccountic 14 hours agorootparentprevWhich has some analogy to computer viruses (specially in simulated environments where they are generated through optimization algorithms, rather than being engineered by an human software developer) reply tivert 17 hours agoparentprev> It's fascinating to me that the complexity of life always goes up. Outside extinction events, complex life generally seems to become more favourable over time. It's interesting that (to my knowledge) we don't see an ecosystem lose complexity in its entirety unless it's dying. I think that's a biased take. Complex life may be better at exploiting a more table environment, but too much disruption can kill it. \"Less complex\" life seems able to adapt more quickly to more extreme changes (e.g. the much greater diversity of bacterial metabolism). Extinction events are inevitable, and environmental disruption will inevitably become more and more challenging until everything dies (though it may take a billion years), and during that time I think the trend will be for complexity to decrease. So ultimately, I think you're overgeneralizing one phase. reply Balgair 17 hours agoparentprevNit-pick: > It's fascinating to me that the complexity of life always goes up Depending (heavily) on how we define complexity, this is not always true. If we define complexity as the number of genes an organism has (a big if there), then we see that evolutionary pressure will often get rid of genes to improve fitness. This is somewhat common in bacteria and other 'small' organisms that are in 'stable' environments, but can happen even in 'higher' lifeforms (Sorry, I can't seem to remember the paper on this, but I vaguely recall it had something to do with jellyfish. Again, sorry!) reply johngossman 15 hours agoparentprevStephen Gould wrote a whole book disputing this idea that the complexity of life increases: \"Full House: The Spread of Excellence from Plato to Darwin.\" The basic argument is that almost all life on Earth is still prokaryotic...the rest of us are just a rounding error. He wrote several books about how evolution was not directional, notably \"Wonderful Life\" and \"Time's Arrow\". I'm not completely convinced by any of these, but worth reading. reply RosanaAnaDana 10 hours agorootparentEffectively, we've been out-competed in all the low complexity niche; life adds complexity to take advantage of novel niche where there is less competition. reply vlovich123 17 hours agoparentprevAnother cool perspective is that simple organisms evolved to coordinate with each other to build complex organisms that could protect the simple organisms in hostile environments they might not / would take longer to explore. Think about all the gut bacteria that survives in humanity during reproduction and viruses and bacteria that invade and hitch rides. You can view humans as the life form or you can view the bacteria within us as the life form and humans are the organic machine they’ve constructed and control (eg look at how the gut/brain connection can effect your mood and decision making without you even being conscious to it) reply pocketarc 17 hours agorootparentI think that is a far more fun way of looking at it! But the bacteria aren't the whole story - we also have our own cells, all doing their thing and all participating (even if bacteria in us are -also- participating). We're just groups of trillions of cells all working together to keep themselves alive and reproducing. And then we go work together with other blobs of trillions of cells, just to further that goal: survival of the cells. These groups of cells that started working together many, many billions of generations ago, are now looking at space exploration, colonising other planets, and wondering if there are other big groups of cells on other planets. There's no way they'd have gotten there if they'd kept living alone as single-celled organisms. That's fun to think about. reply lurker616 11 hours agorootparentSo each of us is basically an AGI for the tiny cells reply sfink 13 hours agoparentprevMy first reaction was: wat? Isn't that only from the perspective of a relatively complex organism? Life constantly explores in all the directions it can, so it's no wonder that one frontier of that exploration is towards increasing complexity. And there are natural limits to decreasing complexity. (Though those limits are beyond what we would call \"life\". You don't need to be capable of reproduction if you can borrow a host's capability. We're all just host mechanisms for the parasitic reproduction of Pollan's corn, Adams's digital watches, and bad analogies.) Maybe that's what the abstract referred to as \"the Theory of the Adjacent Possible\"? I've only read the abstract. But your argument of ecosystem complexity is totally valid. Though I guess if an ecosystem decreases in complexity, then it has to end up in a different type of simplicity than it was the last time it was there, because otherwise you already know that it evolves out of that spot (assuming some amount of determinism). Temporarily, though, this can and does happen. Invasive species often obliterate a lot of complexity, presumably until either their weaknesses are discovered through the very changing conditions that allowed the natives to flourish in the first place, or until they evolve complexity of their own. There's another way to derive increasing complexity from a small number of laws, though. There are multiple resources and multiple ways to access them. Optimizing for any one of those results in overspecializing and becoming less fit for accessing most of the others. There's no one best answer that works for everything. You always have a delicate balance between overgeneralizing and overspecializing, and the area between those provides a lot of different ecological niches, and even more if you look at the battle stretched out over time. (The configurations are unstable; you could have a thousand species optimized for particular resources that get clobbered by a generalist that poisons the specialists, then the energy required by the poisoners makes them lose out to generalist nonpoisoners, which enables specialization again, not to mention evolved immunity... the wheel goes round and round, picking up crud as it rolls.) reply hyperthesis 17 hours agoparentprevThe maximum complexity increases, but the average complexity? Bacteria outnumber us. It's more that it diffuses evenly rather than having a specific direction. reply RyEgswuCsn 17 hours agoparentprev> It's fascinating to me that the complexity of life always goes up. I feel this is only the case because the ecosystem keeps receiving useful/low-entropy energy inputs from the Sun. reply blacksmith_tb 12 hours agoparentprevI would say we have a complex organism bias though, really the most successful life is simple, more than 90% of earth's biomass is plants, ants, fungi, bacteria (obviously some of those are more complex than others, but none of them are posting on HN quite). reply SkyMarshal 12 hours agorootparentSounds like the complexity of life follows a power law distribution, where most of it is simple to moderately complex, but a few species are orders of magnitude more complex than the vast majority. Eg, the vast majority of complexity among living organisms derives from just a few species. reply flanked-evergl 16 hours agoparentprevAs entropy goes up in the universe complexity first increases and then decreases. And life is probably a consequence of this. Sean Carroll explains it quite well in his book The Big Picture and also in this video series from minute physics https://m.youtube.com/playlist?list=PLoaVOjvkzQtyZF-2VpJrxPz... reply cmrdporcupine 17 hours agoparentprev> It's fascinating to me that the complexity of life always goes up. Sounds like observer bias. In terms of number of individuals, the vast vast majority of life on this planet is single cell prokaryotes, and always has been. And in terms of total bio-mass only plants exceed them but that's just because of how plants work (cover the surface with bio-solar-panels) Both bacteria and archaea haven't substantially changed in 3.5-4 billion years. They swap genes as needed, and drop them when they're too costly and unneeded. And they're dominant, and everywhere They were here since just a few hundred million years (or less) after the earth formed. And when conditions on the planet become more hostile again, in the long run it could be the case that eukaryotes are just a historical blip (and a fluke, to boot). And if there's something we recognize as life out there beyond earth... it's likely to look like prokaryotes. The galaxy might be swimming in that kind of thing. There is a strong philosophical/ideological bias in our culture to see the world in terms of \"progress\"; a teleological bias, seeing the universe as proceeding in stages towards some order. It just so happens we almost always seem to define this progress as \"inevitably\" leading to ... us, or \"beyond\" us into whatever fantasy for the future is laying dormant in the present. It feels remarkably pre-Copernican. reply bmitc 5 hours agorootparentI don't understand what you're balking at. I didn't see anyone imply that the complexity of life going up leads to \"us\". The complexity of life does trend up. Nothing you mentioned refutes that. reply alanbernstein 16 hours agoparentprevI think you're describing the maximum complexity of life over time, which is an interesting thing to think about: the life forms that stand out among the rest for \"how far\" they evolve. In terms of other measures (total biomass, long-term survival, short-term adaptability), the life forms that stand out, historically, are very different. Ants, roaches, sharks, bacteria. reply HarHarVeryFunny 17 hours agoparentprevI think the trend towards complexity is more just due to more complex things being built out of less complex ones - complexity creates/supports greater complexity, so it's a natural progression. For higher level animals complexity may also be more inherently favorable since it supports a more customized environmental \"fit\", and helps in the predator-prey arms race. reply alphazard 17 hours agorootparentYeah this is it. It's very unlikely to see something which cannot be decomposed into similar parts come into existence. Something highly complex and irreducible. But simple things are likely to come into existence. So given that we see complicated things, we should assume that they are reducible, and that they came from simpler things. This creates the appearance of a \"trend\" as you say. But it's really that the complicated things couldn't exist before and now they can. Another effect is that it's possible to be more fit (in the Darwinian sense) when you are more complicated. The fittest system with complexity n is n. The rate at which things are destroyed is inversely proportional to their fitness (definitionally). So more complicated things can be better at staying around. See also: https://en.wikipedia.org/wiki/Assembly_theory reply poulsbohemian 16 hours agoparentprev> complex life generally seems to become more favourable over time. Talk more about this, as I'm not sure how you are arriving at this conclusion... it feels a bit like when people talk about evolution being in some way directed as opposed to just being. reply MacsHeadroom 16 hours agorootparentEvolution is \"directed\" towards the exploitation of free energy, inevitably producing increasingly complex niche methods of obtaining and dissipating energy. reply svieira 15 hours agoparentprevIn Count to the Eschaton the \"grand project\" is \"the sophistication of all matter\" and it's been going on since the beginning. reply lioeters 15 hours agorootparent> The Count to the Eschaton Sequence is a six-novel series written by John C. Wright. reply NoMoreNicksLeft 17 hours agoparentprevI think it was a Greg Bear novel, but a line from it struck me as insightful... paraphrased badly, \"even evolution is evolving\". reply tsunamifury 12 hours agoparentprevThere are some excellent if controversial theoretical explanations for this in Assembly Theory reply colordrops 16 hours agoparentprevhttps://en.wikipedia.org/wiki/Anthropic_principle reply EGreg 11 hours agoparentprevHey this dude comes to mind. Anyone remember him? Whatever came of his theories? I found them to be implausible due to the implications they’d have on the Drake equation https://www.quantamagazine.org/a-new-thermodynamics-theory-o... https://xkcd.com/384/ reply Nevermark 16 hours agoparentprevTwo reliable effects predict runaway complexity for any initially simple life form in a non-trivial environment. 1) BOOT STRAPPING COMPLEXITY: Non-trivial static environment: Something simple is rarely the global efficiency optimum in a non-trivial environment. There is nothing trivial about chemistry and the myriad of terrains created by physics in the non-living world. So simple living things, in competition, quickly get more complex. 2) ACCELERATING COMPLEXITY: Dynamic environment: In a competitive ecosystem of continually diversifying life forms, the ecosystem gets more complex, so competing in the ecosystem both enables and requires more complexity. The exponential increase in complexity produces qualitatively new modes of complexity leveraging beyond initial resources: such as specialization, food chains, parasitical strategies, mutual or cyclical symbioses, discarded products that become new resources, colonization of new environments and energy sources, flexible behaviors based on conditions, greater utilization of existing environments and resources, cooperation within multi-cell colonies, specialization and reproductive coordination within cell colonies (creatures), communication and coordination between similar and different life forms, tool use, tool creation, environment shaping, anticipation and planning, curiosity driven learning, aggregation and recombination of knowledge, resource trading systems, systems to promote positive sum interactions, and suppress negative sum interactions, engineering, invention, science, automation, etc. -- TLDR: Non-trivial environments provide initial opportunities for complexity to improve efficiency. Complexity feedback in ecosystems exponentially accelerates further complexity. Exponential growth of life's complexity on Earth shows no signs of relenting. Qualitatively new forms of complexity keep appearing. Conscious intelligence, culture, technology and automation are more continuations than breaks from this trend. reply HarHarVeryFunny 18 hours agoprevReading the introductory paragraph to the paper, it sounds like a rehash of Kaufmann's (very good) book \"At Home in the Universe\", which at this point is almost 30 years old. Not sure what this paper adds, but will read it to find out. The thesis of Kaufmann's book is that the emergence of life, given supporting conditions (variety of source chemicals in environment, sources of energy, maybe water/mixing) is all but inevitable (hence life being \"at home\" in the universe) rather than being some rare event. The reasoning is that when these preconditions are met there will be a variety of chemical chain reactions occurring where the product of one reaction is used as the input to the next, and eventually reaction chains that include products that act as catalysts for parts of the reaction chain. These types of reaction can be considered as a primitive metabolism - consuming certain environmental chemicals and producing others useful to the metabolism. From here to proto-cells and the beginning of evolution all it takes is some sort of cell-like container which (e.g.) need be nothing more than than something like froth on the seashore, based out of whatever may be floating on the water surface. Initial \"reproduction\" would be based on physical agitation (e.g wave action) breaking cells and creating new ones. Different locations would have different micro-environments with different locally occurring reaction chains and \"proliferation/survival of the fittest\" would be the very beginning of evolution, as those reactions better able to utilize chemical sources and support their own structure/metabolism would become more widespread. Anyway, a good book and plausible thesis in general (one could easily adapt the specifics from seashore to deep sea thermal vents etc). reply libeclipse 16 hours agoparentDid you see the authors of the paper? reply HarHarVeryFunny 15 hours agorootparentYes - that's what I meant. Kaufmann rehashing his old work (but presumably adding something to it too). reply ta8645 19 hours agoprevCan't help but wonder, is AI an expected phase transition in the evolution of life in the universe? Is life really just the larval stage of a higher order intelligence? reply HarHarVeryFunny 17 hours agoparentI'd say so - it seems that life has to be created via evolution/competition, and left to run long enough evolution (survival/proliferation of the fittest) is likely produce organisms/entities that are not only better fit to the environment, but also better fit to the game. Evolution will tend to producing things that are better at evolving (faster to adapt). This includes things like multi-cellular life and sexual reproduction (creating variety via DNA mixing). One type of evolutionary niche that seems almost inevitable to arise in any complex environment is intelligence - the generalist able to survive and thrive in a variety of circumstances, and in the competitive game of evolution greater intelligence should outcompete lesser intelligence. Eventually you'll get critters sufficiently intelligent to build AI of their own level or higher, which may be regarded as another way to win the game of evolution - an intelligence that can evolve much faster than the type that bootstrapped it. It's interesting to consider does AI/AL (artificial life) really need to become autonomous and stand-alone, or can it be more like a virus that needs a host to survive. Stage one AI obviously needs a host, but maybe it never really needs to become stand-alone? It reminds me of (git author) Linus Torvalds' quote \"real mean don't need backups\" - you just release your software and have confidence it'll get replicated in git repositories worldwide. Maybe AI can be robust to extinction (not need a backup/body) just by becoming ubiquitous ? reply creer 12 hours agorootparentRight: hosts - or symbiotic life forms are a perfectly legit way to go. Plenty of them. And some form of \"augmentation\" might more socially / politically acceptable (ugh) than \"AI on the loose\". reply svachalek 5 hours agoparentprevIn the 1980s Gregory Benford explored this with his Galactic Center Saga books. I really enjoyed the series especially the middle one, Great Sky River. reply educaysean 9 hours agoparentprevImagine if this was indeed the case, what a time to be alive! We're witnessing the moment as the noise in the sonogram morphs into a recognizable shape of a baby. It's our heritage, our future generation, human 2.0, Machina Sapiens. Literally created after our own image too. I'm so proud I could cry. reply HeatrayEnjoyer 9 hours agorootparentI would be more enthralled if this didn't also mean literal extinction of us and everything that matters to us. reply namrog84 8 hours agorootparentIs it an extinction or just another type of evolution of humans? Evolution isn't the right word but AI will presumably be from us and carry some or the things that matter to us most likely. Sure you aren't passing on dna like to natural born children but not all children have same ideals or cares of same things. reply haolez 19 hours agoparentprevIn the context of the universe, I wouldn't call it \"intelligence\" versus \"artificial intelligence\". I would call it \"organic intelligence\" vs \"inorganic intelligence\". reply irrational 19 hours agorootparenthttp://www.eastoftheweb.com/short-stories/UBooks/TheyMade.sh... reply neom 19 hours agorootparentprevhttps://en.wikipedia.org/wiki/Animism :) reply junon 19 hours agorootparentNot quite sure this is what GP meant. reply Koshkin 18 hours agorootparentprevThat's reductionist. (Also, can't A.I. be \"organic,\" say, like the OLED? :) reply SketchySeaBeast 18 hours agoparentprev\"expected phase transition\" seems a loaded phrasing, and implies a deterministic evolution, which I really don't think we should assume. reply jyounker 18 hours agorootparentAn expected phase transition in this context is stochastic. The transition to order is expected, and there are bulk properties that are the same on each run, but the exact details differ each time. reply TriangleEdge 6 hours agoparentprevCould be that what we experience as the universe is only a minor fraction of what exists. I define life as \"processing information\", so AI by definition is a life form given this definition. reply creer 12 hours agoparentprevThat seems pretty likely - with some chance of hybrid still possible. That is, does AI take off and leave the goop in the dust. Or does AI become an augmentation of the current life forms - in an integrated form which perhaps can be admitted as continuation. The current AI products require quite a bit of compute power - but then augmentation doesn't need to be \"on-board\" the organic life form. reply creer 12 hours agoparentprevExcept for AI not being life yet. I'll go with intelligence already, but not yet growing, reproducing, producing, interacting or much of anything you might choose for \"life\". Which is cool: a (to be) life form which starts with intelligence before life! reply throwaway143829 19 hours agoparentprevMakes you wonder what comes after AI. What's the \"higher order\" after AI that exists today or that will exist in 10 years? I'd guess we will never understand that level of intelligence, unless AI augments our brains somehow. reply digging 17 hours agorootparent> unless AI augments our brains somehow Frankly, the more I think about AI, the less sense it makes to me that biological, single-body humans have any place in the future. As soon as we can digitize our minds, why wouldn't people begin to do so? Bodies could be inhabited at will, and death will be a thing of the past as we're able to store backups. I'm sure some will refuse and be left behind, just as we have Amish communities today, with a similar level of influence on civilization. And in the case of digital people, I think it's likely they'll share in the intellectual advancements of AI, if such a distinction even exists. reply creer 12 hours agorootparentDigitization is one direction but I think augmentation is perhaps a more likely one. Or a first one. Digitization can follow in a \"Ship of Theseus\" fashion. And augmentation branches then in AI as symbiont versus AI as desktop assistant. reply digging 11 hours agorootparent> Digitization is one direction but I think augmentation is perhaps a more likely one. Or a first one. First seems likely, but as a permanent alternative I don't know why a species would eschew lightspeed transportation and effortless immortality for the fragility and slowness of an organic body. It's possible there are good reasons, but I don't know of any. reply creer 10 hours agorootparentDigitization, upload, has always seemed to me an iffy goal. The brains' packaging doesn't seem very amenable to any of our current technologies, as far as being able to \"read\" it. And then once read emulating it seems just as difficult. Once uploaded comes the issue of getting computing time to run it (the economics and politics of prefering run time of X over run time of Y). And maintaining the computing platform. Certainly there are immense advantages to the digitized form - of course. By contrast, augmentation (which is what we already do) seems straightforward. And seems to fit current society \"easily\" (haha - or let's say it will be tough enough as a first stage.) So that from the point of view of a next epoch in life forms, AI fits more immediately in the struggle of AI as symbiont, AI as independent, or AI as desktop assistant. reply saalweachter 15 hours agorootparentprevOnce flying cars are as cheap and easy to own and operate as regular cars, why would anyone buy a non-flying car? reply digging 13 hours agorootparentI think you're going to need to explain your position a little better. reply saalweachter 5 hours agorootparentUpload, like flying cars, is a particular vision of a scifi future technology which may not come to pass. You're prognosticating a future \"when X...\", when that \"when\" is a very big \"if\". reply eagerpace 18 hours agorootparentprevPerhaps they’ll keep creating smaller transistors and more powerful processors eventually landing on a soft tissue-based version powered by glucose. reply idiotsecant 17 hours agorootparentprevYou mean the second order toposophic level? https://www.orionsarm.com/eg-topic/492d6fafbef2a Careful you might be going down a 14 hour rabbit hole. reply paxys 19 hours agoparentprevNo, because AI is (for now at least) shaped and constrained by us humans rather than developing free based on the laws of the universe. Is it really \"evolution\" if a judge can rule that it violates copyright and stop all progress overnight? Or a random developer can add a bit of code to make sure the answers appease the right set of people? What we have today is a crude software approximation mimicking what we think AI should be, but that AI itself is nowhere in sight. reply digging 17 hours agorootparent> shaped and constrained by us humans rather than developing free based on the laws of the universe This logic doesn't hold. Humans are part of the universe and obey all its laws. It's arbitrary to say bacteria and bonobos and stone tools are naturally occurring but AI aren't. We distinguish them because we're conscious and we have the experience of choice, but to say our creations aren't natural to the universe implies that our consciousness is not a natural phenomenon. reply moate 11 hours agorootparentIt feels like you’re simply stating the predators and outside influences that are affecting AI’s evolution. Humans killed the dodo, maybe we kill the AI next reply louthy 18 hours agorootparentprev> No, because AI is (for now at least) shaped and constrained by us humans rather than developing free based on the laws of the universe Aren’t humans part of the universe’s rules? What makes AI development any less ‘free’ than any other emergent property? reply basil-rash 18 hours agorootparentprevWhat makes you confident our evolution didn’t occur the same way? The “fossil records” of the two are similar in many ways: lots of baby steps, giant leap with no known intermediary states, lots of baby steps, … reply SketchySeaBeast 18 hours agorootparent> giant leap with no known intermediary states That's just evidence of absent records, not that there are no records. reply basil-rash 18 hours agorootparentYes and? reply SketchySeaBeast 18 hours agorootparentAnd if we're building off a bad initial premise it weakens the whole argument. \"AI could be evolving just like us!\" doesn't make sense when we don't know how we evolved. reply basil-rash 18 hours agorootparentThe initial argument was just as bad: “AI can’t be evolving like us”, when we have no clue how we evolved. All I’m doing is calling out the intellectual dishonesty is making any claims about something we know nothing of. reply ta8645 18 hours agorootparentprev> (for now at least) Yeah, a lot of people get hung up on the term AI as it exists today, and protesting that it doesn't deserve such consideration. I should have been more explicit that I was speaking in the much more general sense, and on an evolutionary timescale, not about technology we'd recognize today. reply darepublic 18 hours agoparentprevBy AI I guess you mean complex self replicating machines that were originally created by other forms of life reply nonameiguess 14 hours agoparentprevI'd reframe this question. What constitutes a phase transition at all in the sense being talked about here isn't super clear. There's a clear definition in chemistry and it has analogies in cosmology as the entire universe overall went through some early phase transitions in the vacuum state when it was of much greater average density. These are all related to qualititative changes in the properties of matter as temperature and density change. I would grant that life is a qualitatively different state of matter but it isn't as obvious as the more familiar phase transitions. We don't have a clear demarcation for what is and isn't life. This paper attempts to give a definition, but the fact that that is being done at all shows there isn't one already that is universally agreed-upon, unlike the definition of what is solid versus what is liquid. I guess all life we're aware of consists minimally of a semi-permeable barrier, ingests and stores energy inside of this barrier, and locally reduces entropy inside the barrier while dissipating heat and/or other byproducts into its environment. Life is, of course, not the only thing that does this. My house fits the same description. The only real line in the sand we have between things we consider alive and things we consider tools is that things we consider alive are all born and descended from other living creatures, not assembled from found or fabricated parts. Ultimately, though, this is a difference in origin, not a difference in quality or capability. Any tool, including electronic computing devices, can potentially have all of the same qualities as life if we could figure out how to make them self-assembling, self-healing, and self-reproducing. I guess we can do this with software, but it isn't obvious to me how to even demarcate a unit of \"software\" as an individual entity. How to demarcate intelligent from unintelligent software is even less clear, but nothing about the underlying state of matter the computations run on is any different, so I don't see how it involves anything we can call a phase transition without severely straining the term. reply Cacti 19 hours agoparentprevAI is not intelligent, even in the abstract sense reply willy_k 17 hours agorootparentCould you elaborate? Do you mean the current state of AI? I would argue that current models have some behaviors that one could liken to intelligence, even if it’s all just operations on 1s and 0s. Of course, this depends on your definition of intelligence. Mine is along the lines of “can develop a representation of a problem space and use that to predict optimal actions given current input”. Which current AI, most animals, fungi, and humans can do. Sentience is a different question, I’d argue that only humans and a few species of animal (Dolphins, Elephants, apes) are sentient as of now, though it seems highly possible that machines will join that group by the end of the century, if not sooner. reply coldtea 17 hours agorootparentprevAnd I say it is. What now? How can these two statements be reconciliated? (Pointing out that this is like, your opinion, man) reply MetallicDragon 16 hours agorootparentprevUsing what definition of intelligence? reply Avicebron 19 hours agoparentprevI imagine anything backing that up lends some credibility to the true believers, likely why this poorly written paper has made it to hn. A rule of thumb around here is I often measure my salt grain size by how self-important the article makes hners feel. reply brabel 16 hours agorootparentAre you aware that this \"poorly written paper\" is the work of a leading researcher of the origins of life on Earth? reply ta8645 19 hours agorootparentprevWell it's true, or not, regardless of how any of us feel about it. It's just fun to wonder. reply coldtea 17 hours agorootparent>Well it's true, or not, regardless of how any of us feel about it Yes, but not with the same probabilities of being true in both cases (the cases being whether we feel good or bad about it). Something makes it to HN because HNers like it. And not true things (feel good articles and popular sentiments) are more likely to be liked while not being true, compared to true but not likable stories. reply ruffrey 14 hours agoprevRelated, lately I have been enthralled with the work of Michael Levin at Tufts. He studies things like goal directed behavior of cells and systems of cells. Here is an intro to his work: https://www.youtube.com/watch?v=p3lsYlod5OU reply asow92 19 hours agoprev> We are, truly, Of Nature, not Above Nature. This sentiment has always made me question when people say things are \"unnatural\", \"artificial\", or \"synthetic\": If we ourselves are of nature, and these things are a byproduct of us, then aren't they naturally occurring? edit: added \"synthetic\" to reduce ambiguity. reply basil-rash 19 hours agoparentThat’s one of those revelations that only sounds profound when everyone involved is really stoned. Outside of that, everyone knows the meaning of “unnatural” and we all get that the colloquial meaning and a strict etymological analysis don’t quite align. reply wharvle 18 hours agorootparentThe deeper insight is that every definition of a word breaks down when you try to pin it precisely and in some absolute, universal-context sense to an exact meaning. That doesn’t mean they can’t be very, very useful. reply simiones 18 hours agoparentprev\"unnatural\" or \"artificial\" have a very clear meaning: made by the intervention of humans in ways outside our base anatomical functions (so excluding babys or spilled blood). This intervention can be mechanical (Stonehenge), or biological (breeding animals to become more useful to humans), or chemical (synthesizing oil from plastic), or a complex combination of all of them (whisky); it is also transitive: anything created by an artificial object is also artificial. The fact that a naturally occurring thing (human beings) can create artificial things is not surprising under this definition. The definition can also be theoretically extended to other human-like agents, like hypothetical aliens. It hasn't been practically very necessary so far. Edit: I should note that this is the sense of \"artificial\" or \"unnatural\" that is used in the context of the article. There is a secondary meaning, used in phrases such as \"artificial sweetner\" vs \"natural pesticide\" that I don't think stands up too well to serious scrutiny. reply TedDoesntTalk 18 hours agorootparentNothing can be made or manufactured that is unnatural or artificial. By definition, anything that can exist in this universe is naturally part of the universe. reply simiones 17 hours agorootparent> By definition, anything that can exist in this universe is naturally part of the universe. Agreed. > Nothing can be made or manufactured that is unnatural or artificial. This doesn't follow in any way, not with the definition I gave, or with any definition I have ever heard. How do you define \"artificial\" such that an iMac is not artificial? I've told you my definition (anything created by a human that is not an anatomical/physiological process of that human), and by my definition it is very clearly artificial (iMacs are created by humans, and they are not a bodily secretion of humans). reply TedDoesntTalk 9 hours agorootparent> How do you define \"artificial\" such that an iMac is not artificial? I don’t. iMacs are made of naturally-occurring materials. Those materials are mined or refined or formed in laboratories and factories, then assembled into something we call an iMac. All of those materials and processes are part of this universe and occur within the laws of then universe. They are all natural. There really is nothing artificial in this regard. But that doesn’t mean some things aren’t detrimental to our health or well-being or the well-being of other life. reply asow92 18 hours agorootparentprevWhen birds make nests outside of their base anatomical functions, are they \"unnatural\"? reply simiones 18 hours agorootparentNo, because birds are not humans. To be fair, \"unnatural\" is sometimes extended to refer to \"life\" instead of \"humans\", typically only when talking about the evolution of life or the search for extraterrestrial life. In that sense, then, yes - bird nests or termite mounds or coral reefs are unnatural. A more common wording for this same idea is something like \"not created by geological processes\". reply asow92 18 hours agorootparent> \"not created by geological processes\" Did life begin with the geological process of protein chains forming in geothermal vents? I don't know, but it begs whether it is natural or not. reply simiones 17 hours agorootparentIt's perfectly consistent that the origin of life itself can be a geological process but that the products of life itself are a separate category. reply asow92 18 hours agorootparentprev> We are, truly, Of Nature, not Above Nature. What gives humans the special designation of their byproducts being unnatural? reply wharvle 16 hours agorootparentWhat's special is we decided that having a different category for things humans do is useful. It can both be true that everything humans do or create is natural because all of existence and everything in it is natural, and also that some things humans do or create are artificial or un-natural, without contradiction, because context may be taken into account, and the same words can mean different things depending on how and why they're employed. reply simiones 17 hours agorootparentprevYou're asking the question backwards. We have a concept, \"byproduct of human action\". We needed a word for this concept. We more or less arbitrarily chose \"unnatural\" or \"artificial\" as the words for this concept. You can argue that \"unnatural\" was a bad choice for this concept. But that's irrelevant to what words mean. There are other words like this - for example, \"antisemitic\" means \"something that is against Jewish people\", even though \"semitic\" means \"of Jewish or Arabic or Phoenician etc. descent\". So something discriminatory against semitic peoples is not necessarily anti-semitic. Natural language doesn't follow strict logical rules. And of course, natural language is in fact itself an artificial, unnatural, construct. reply asow92 16 hours agorootparentNaturally, I would prefer to agree to disagree if you’re willing. reply r34 18 hours agoparentprevThat's why I use to claim that for me everything is a priori natural. Additionally I disagree every time I hear that \"culture is the opposite of nature\" (not sure where it comes from, but seems to be a well-grounded philosophical concept). For me it can't be so by the rules of logic alone. On the other side: we have a lot of taboos in the language/culture and not all of them are bad in terms of social well-being or happiness of individuals (the very simple example is that we sometimes lie to our kids). And I think that what we hide behind those taboos tends to emerge as \"unnatural\" or rather usually \"supernatural\". I also usually don't agree that we don't need a revolution in physics, but I understand it is so successful in creating all those working machines and we have to maintain them... ;) reply carapace 11 hours agoparentprevI feel that people who are clever enough to raise this question are also clearly clever enough to answer it for themselves. Anyway, operationally speaking the difference is in how easily the matter in question can be digested by living things. If it's easy to digest it's food, if it can't be digested at all then it's worse than artificial: it's anti-life. reply mc32 19 hours agoparentprevOk, maybe synthetic is a better word for many instances of their use. As in synthesized with the aid human intervention. reply GoldenRacer 19 hours agorootparentWhere exactly is the line drawn for how much and what type of human intervention is required? When I cook food, human intervention is causing chemical reactions that change the composition of the food. I doubt many people consider grill marks to be unnatural or synthetic. reply simiones 18 hours agorootparentI think the line is typically drawn at any human intervention. I doubt many humans consider steaks to be a naturally occurring phenomenon. Now, there is a secondary fuzzy notion of \"artificial\" typically used in relation to \"chemicals\". I don't think that definition stands up to most serious scrutiny, and is at any rate unrelated to this article. reply asow92 18 hours agorootparentThat's exactly what's being—albeit atypically—advocated for here: That even steaks are a naturally occurring byproduct of humans and cows because humans and cows naturally exist. reply simiones 17 hours agorootparentSure, but then of course absolutely everything is \"naturally occurring\". Plastic is a naturally occurring substance, computers are naturally occurring objects, C++ is a natural language. Perhaps then only miracles from God (for those who believe in such things) are unnatural? reply asow92 16 hours agorootparentIn your example, I’d opt for supernatural over unnatural, and I get your meaning. reply asow92 17 hours agorootparentprevNow you're getting it. reply simiones 17 hours agorootparentI am, but this is just not what those words mean, to anyone. reply MacsHeadroom 15 hours agorootparentPlenty of people do not believe the conceptualization of a natural/synthetic divide does any good. There are entire subsets of philosophy, feminism, cyborg theory, etc. which talk about this. reply asow92 16 hours agorootparentprevIt may be heterodox, but it’s what it means to me, and I’m sure I’m not completely alone in feeling so. reply xixixao 19 hours agorootparentprevIn which case coral reefs are also synthetic :) reply simiones 18 hours agorootparentMost coral reefs predate human intervention by a few hundred million years. reply Drakim 19 hours agoparentprevIt makes more sense when you realize that unnatural and artificial are societal words akin to immoral or bad. Within that context they are perfectly crumulent words, it's only when we wish to have them be objective outside humanity that they don't make sense anymore. reply Koshkin 18 hours agorootparent> societal words Indeed, all words are \"societal.\" The meaning(s) of a word is/are always a matter of convention (or tradition). reply lutusp 15 hours agoprev\"Is the emergence of life an expected phase transition in the evolving universe? (arxiv.org)\" Easily answered: * We don't know how consciousness comes into being, indeed we can't rigorously define it or unambiguously identify its presence or absence. * We believe we have it, but we aren't sure whether other animals and/or objects possess it. * Therefore, based on Occam's razor, we may provisionally assume that all matter possesses some degree of consciousness -- this is the simplest assumption. * The alternative would be to argue for a consciousness exceptionalism in \"life\" forms for which there is no evidence and many counterarguments. * Therefore it follows that ... wait for it ... life is not a special state of matter or energy. * Therefore the emergence of life doesn't represent a phase transition that confronts physical laws or requires an explanation. reply gfodor 15 hours agoparentThat's not an answer, that's an argument. Occam's Razor isn't like the second law of thermodynamics. It's a heuristic. reply onetimeuse92304 15 hours agorootparentYes. You can't prove anything by proving it is the simplest explanation. The only way to prove things is to prove that it is the only possible explanation. Occam's Razor is just a useful mind tool that helps us navigate the world based on the observation that oftentimes the simplest explanation tends to be what is actually happening. BTW lots of other logical mistakes. reply IggleSniggle 10 hours agorootparentIt's also a useful tool in science. For example, we do not know and cannot prove the one-way speed of light, only a round trip speed of light. That light travels at the same speed in all directions is just a useful application of Occam's Razor; it is effectively unknowable and no experiments have yet been devised that can falsify the alternatives. Having a set one-way speed of light is a convention reply lutusp 7 hours agorootparentprev> That's not an answer, that's an argument. Fair enough, and I agree. But there may be no answer in a scientific sense -- testable, falsifiable and so forth. > Occam's Razor isn't like the second law of thermodynamics. It's a heuristic. Yes, true, but it's not meant to force a conclusion, only offer guidance, which might be entirely wrong at times. reply willmadden 15 hours agorootparentprevThe paper defines life as a chemical reaction, which is also a heuristic. We do not know if aspects of \"life\" exist outside of our present understanding of physics. It's great to claw away at the edges, but I don't think anyone can answer the question \"what is life?\" at this point. reply mekoka 5 hours agoparentprevHardline materialism is indeed dying and the presumption that all matter possesses a degree of consciousness currently has two major undercurrents: panpsychism and idealism. Most scientists that operate under the assumption that consciousness, as we experience it, is an emergent property align toward panpsychism, matter and consciousness somehow being co-primitives in the ontology to kickstart reality. But there are many problems with panpsychism, the first being that it's a dualism, that is it requires two separate magic tricks at bootup. Since physics only knows about matter, panpsychism is not considered a scientific theory, just an interesting philosophy. The later problems with it are leftovers from the formerly popular materialist framework. How does the proto consciousness of matter grow to give us the taste of sugar, or love? Where is that conscious property? How to identify it? Etc. Now, from an Occam's razor's perspective, I think the less popular idealism (consciousness is the primitive, then it boots up matter) is actually the most parsimonious. It's the least intuitive at first, but when you take the time to listen to arguments from a scientific and analytical philosophy standpoint (e.g Don Hoffman, Bernardo Kastrup), it's the only one that actually makes any kind of sense, while in the process solving most of the numerous problems raised in physics and philosophy (e.g. local realism is false, so where is matter when consciousness doesn't experience it? If the probability that we're in the one real reality 1/N, why would we assume that we are? And many more). It also gives us as a bonus that it manages to reconcile those fields with the nondual intuition from traditions that document accounts of people who have lived with that realization (e.g. Buddhism, Advaita, Taoism, Sufism, Christian mysticism). It does require a profound conceptual paradigm shift. Way deeper than at first glance we realize is necessary. So I generally just recommend that people dive into it by reading or listening to discussions by the people that I've mentioned, rather than knee-jerk responding. reply bsza 14 hours agoparentprev> We don't know how consciousness comes into being Consciousness and life are completely different things. The article doesn't even mention consciousness. > Therefore it follows that ... wait for it ... life is not a special state of matter or energy It is special in the sense that it's remarkably complex e.g. compared to a mineral. It is not special in the sense that it obeys the same laws as a mineral. > Therefore the emergence of life doesn't represent a phase transition that confronts physical laws or requires an explanation No one claims it confronts physical laws. In my understanding, the article states: life may emerge naturally as molecules bump into each other, combine and eventually become complex enough that self-sustaining/self-replicating systems can come into being by chance. This process might be deterministic enough that it happens everywhere roughly at the same time (on a cosmological time scale). reply lutusp 7 hours agorootparent>> We don't know how consciousness comes into being > Consciousness and life are completely different things. Yes, but they have a dependent relationship. We argue that life doesn't require consciousness, but we also argue that consciousness requires life. IMHO both arguments have issues. > The article doesn't even mention consciousness. Yes, that's true, but it's an identifier of life -- that and the ability to reproduce. If the wind waves a flag, we don't attribute life to the wind. If a person waves a flag, we do. If a robot waves a flag, well, that's an open question at the moment. > ... This process might be deterministic enough that it happens everywhere roughly at the same time (on a cosmological time scale). Ah, I see an issue with that. We now know there are galaxies much older than ours (and younger), and if life is a common property of matter (still not determined) and if it relies on a certain set of preconditions, that would mean life would appear at widely different times, depending on location. reply nox101 14 hours agoparentprev> Therefore, based on Occam's razor, we may provisionally assume that all matter possesses some degree of consciousness -- this is the simplest assumption. Plenty of things don't follow that. Heat something up it gets hotter but at some point it tips the scale and burns. Cool down water and it gets colder but at some point it hits a threshold and freezes. 1000s of similar examples. Put a bunch of hydrogen together and not much happens. Put enough and it becomes a star. It seems like consciousness could follow the same pattern and still fit occam's razor reply cameldrv 15 hours agoparentprevI don't think Kauffman is making any claims about consciousness here. I agree that consciousness is a big mystery. Kauffman's theory (haven't yet read this paper, but have read a lot of his other work), is that self-replicating entities are essentially inevitable in environments that have above some level of chemical complexity. That chemical complexity also weakly implies that there is useable free energy, which is also a requirement for self replication. reply rileyphone 14 hours agorootparentFwiw Kauffman has a very interesting stance on consciousness that he is hinting at here. Consider this recent paper [1] or interview [0] where he explains his thinking. Cool to see a respected scientist exploring areas that were previously looked down as quackery by academia. 0. https://youtu.be/XWbxdREQ6xM?si=t2-AywFf2cnQfAni 1. https://www.sciencedirect.com/science/article/abs/pii/S03032... reply xmonkee 15 hours agoparentprevI agree with the overall thrust of your argument, but I disagree with this one somewhat: > Therefore, based on Occam's razor, we may provisionally assume that all matter possesses some degree of consciousness -- this is the simplest assumption. An even simpler first few steps: * We experience matter, but we aren't sure if matter exists outside of our experience of it * Therefore, based on Occam's razor, we may provisionally assume that all matter exists only within our consciousness -- this is the simplest assumption. reply lutusp 7 hours agorootparent> * We experience matter, but we aren't sure if matter exists outside of our experience of it A nice point. One might call it the the solipsistic argument. Not easy to argue against, and not unlike the debate about consciousness -- we aren't close to saying what it is, or which assemblies of matter can be reliably said to possess it. reply kurthr 15 hours agoparentprevWhat's interesting here is that life on earth has commonly defined has been around gathering negative entropy for several billion years (photosynthesis and the rise of oxygen for over 3 billion) and growing exponentially on earth. That is a sizeable fraction of the age of the universe. Human like ancestors have been using fire for over a million years (based on the oxygen generated above), agriculture for thousands, and industry for hundreds. All of these are exponential increases in energy density, entropy generation, and negative entropy consumption. To not describe them as phase changes is odd. It's telling that economic growth over the last 4000 years (first farming and now industry in the last ~400) also appear to be exponential over several orders of magnitude. Whether there's a quick end to this or not is an open question, but they still look like phase changes. That weather is starting to be affected is equally telling. A conflation of consciousness with life seems weird, though. If ordered psuedo-crystals are life (or clay-RNA / DNA crystals) or even viruses are life, I don't know anyone claiming they're conscious. It would be like assuming life caused capitalism or industry, when certainly they fit very different exponential curves. Making arguments that industrialization (use of power other than human/animal work) isn't a \"phase change\" in economy would also be similarly strange. Now, whether it \"confronts physical laws\" or requires an explanation, I have no idea. They do seem to be transitions at least as interesting as weather on Jupiter. https://communities.springernature.com/posts/were-humans-the... https://en.wikipedia.org/wiki/Earliest_known_life_forms https://mason.gmu.edu/~rhanson/longgrow.pdf reply mensetmanusman 15 hours agoparentprevIs there evidence of consciousness in a rock? reply bongodongobob 15 hours agorootparentIt's a reducto ad absurdum argument. Either consciousness only emerges in living things, or it exists in everything. The brain is made of tissue and tissue is made of matter, therefore, it exists in all mater or doesn't exist at all. I believe this is the thrust of panpsychism/panentheism. reply lukifer 15 hours agorootparentprevThere isn’t the same kind of ontological boundary in a rock, as there is an organism. My hunch is: there’s a low-grade subjectivity/qualia to all of the universe, which would include the particles of the rock, but the rock would not experience an independent “rock-ness”. The consciousness of organisms are perhaps more like a dissociative state, analogous to a gravity well of subjectivity, such that the qualia are concentrated inside a boundary (the entity that is intelligently chasing energy differentials), and excluding most information outside this boundary. The primary reason for this hunch is that “ouch” is an experience, rather than merely a mathematical/algorithmic update to a neural net (compare with artificial NN’s, chemical feedback loops, etc). An “ouch” is not needed to strengthen the signal; just tilt the training feedback stronger as necessary (-10,000 points to “eat the berry that made us sick”). And it seems prohibitively expensive expensive to bootstrap “strange loop” subjectivity merely to strengthen those numbers via an “ouch”. But instead, if some form of subjectivity were to pre-exist, Darwinian pressures would co-opt it as an efficient feedback loop mechanism, and then iterate until arriving at the “consciousness” of animals and humans (including an incentive to pay more attention to qualia in the organism boundary, and only minimally outside it) reply daxfohl 8 hours agorootparentI disagree. Chemicals are perfectly sufficient as a feedback mechanism. I'd argue consciousness is a boring byproduct of evolution. You survive by predicting the actions of your competition. You predict your competition by introspecting yourself. Definitionally, consciousness. reply lukifer 7 hours agorootparentI certainly don't rule that out. But even the Hofstadter \"strange loop\" model explains awareness, moreso than the qualia of experience. (Show me an \"ouch\", and I'll show a way to accomplish identical behavior via -X weighting of utils.) And, it invokes the question of whether a sufficiently advanced predictive function in silicon/code would also have consciousness and/or qualia. If yes, one essentially lands in IIT [0] (according to some, implying that even a light switch has a 1-bit micro-consciousness). If no, then one wonders what utility qualia brings to the table, compared to simply optimizing the algorithm to work without subjectivity. (Would ML training be more effective if we could somehow add \"yuck\" and \"yum\" experiences? How would we tell, assuming we aren't accidentally doing it already?) For another perspective, Peter Watts' novel \"Blindsight\" explores the idea that consciousness is merely a temporary local optimum, and eventually selected against, in favor of life forms which are intelligent without being conscious. [0] https://en.wikipedia.org/wiki/Integrated_information_theory [1] https://www.goodreads.com/book/show/48484.Blindsight reply daxfohl 6 hours agorootparentOne thing that hadn't occurred to me before, in light of some other threads arguing that it all comes down to maximizing entropy, which sounds reasonable, is that consciousness implies the ability to make mistakes, and mistakes imply missing the maximum. How to reconcile with consciousness maximizing entropy? Maybe Blindsight is right; maybe there's a future life that has no consciousness and thus is always maximally entropic. Or maybe it's a zero-sum game of simultaneous moves between entropy and some other force, or spatially-separated moves such that relativity cannot order them, making nondeterminism the optimal strategy. But now I'm making stuff up. reply mensetmanusman 14 hours agorootparentprevIf we scale the rock up to earth, the ontological boundary would be similar, so maybe the earth is conscious? reply lukifer 7 hours agorootparentI'd certainly entertain it as possibility. From a purely physical perspective, I think the Gaia Hypothesis [0] has some validity to it, even if the analogy to DNA organisms is deeply imperfect. Where I stumble, though, is identifying a way in which the Earth is actively chasing any sort of free energy differential, meaning no iterated selection pressures, and (probably) nothing we could call intelligence. [0] https://en.wikipedia.org/wiki/Gaia_hypothesis reply genman 14 hours agorootparentprevIt is also possible that your defined level of consciousness will reach zero at certain level of entropy. Or it is possible that the signal processing speed will decrease to the level to be effectively zero in any meaningful time scale. reply schnitzelstoat 19 hours agoprevI think life is going to be quite common as it seems it just requires liquid water and an extremely common type of rock in order to form alkaline hydro-thermic vents. It appears that life developed quite quickly in this manner after the formation of the Earth. The leap from bacteria and archaea to eukaryotes, however, took billions of years. So complex life may be rare. reply pfdietz 17 hours agoparentThe smallest independent system we know of that is capable of Darwinian evolution has billions of atoms. How do you bridge the gap between \"waste and rocks\" and this system? Water and rocks may be necessary, but you have no evidence they are sufficient. reply mavili 19 hours agoparentprevShould be so common, right? Yet we're struggling to find any form of life. Not complex, just anything that lives. reply simiones 18 hours agorootparentI don't think we have any tools that could possibly detect bacterial life anywhere where we don't have a physical presence at the moment. That is, even if Venus were teeming with simple life on every square cm, I don't think any of our instruments could pick it up unless we send a probe to collect and analyze samples. So, at the moment we have no hope of detecting this type of life outside the solar system even if it were universal. reply jodrellblank 16 hours agorootparentWe've got photos from the surface of Venus: https://www.planetary.org/articles/every-picture-from-venus-... It's visibly not covered in moss, fungus, grasses, algae, slime. On Earth we talk about hot hydrothermal vents underwater: https://www.nhm.ac.uk/discover/survival-at-hydrothermal-vent... - this says \"the Pompeii worm [...] is one of the most heat-resistant multicellular animals on the planet, able to withstand temperature spikes of over 80°C. 'Most animals can't cope with anything over 40°C. Very close to the hot fluid, there are typically only microorganisms. These can survive in temperatures up to around 120°C,' explains Maggie.\" The surface of Venus averages 462°C. Okay we won't know for sure unless we collect samples, but we do know any life would have to be heat resistant beyond anything we know of - beyond what our metal space probes could tolerate - and invisible to the naked camera even in 'teeming' quantities, and leaving no trace of waste gasses in the atmosphere which we can detect remotely. [ https://www.youtube.com/watch?v=nvkMwv9EYQQ - Venera: The Incredible Probe that the Soviets Sent to Venus ] reply nojs 6 hours agorootparentVenus is perhaps a bad example but the point is true for all the other more likely places (Europa and others with subsurface oceans). We just haven’t explored them enough yet. reply saalweachter 15 hours agorootparentprevHumans, living on one planet: \"Life is everywhere!\" Humans, after soft landing probes on two other planets: \"Life is nowhere out there! It must be only here!\" reply mseepgood 2 hours agoprevWhat's the universe's long game? Does it compete with other universes, and why does it need life to do so? reply tobbe2064 18 hours agoprevThis reminds me. There was a paper published a couple of years ago and posted here on HN that actually calculated the probability of life aminoac",
    "originSummary": [
      "The paper introduces a new definition of life based on the concepts of Catalytic Closure, Constraint Closure, and Spatial Closure.",
      "The authors combine mathematical theories to argue that life is expected as a phase transition in the evolving universe.",
      "The paper suggests new approaches to studying the phylogeny of metabolisms, searching for life on exoplanets, and experimentally exploring the emergence of rudimentary life."
    ],
    "commentSummary": [
      "The concept of life as a phase transition in the evolving universe is explored in this article and discussion.",
      "The interdisciplinary nature of studying life and the contributions of the Santa Fe Institute are emphasized.",
      "The connection between entropy and life, mathematical/physics-based definition of life, and the relationship between the universe's tendency towards generating entropy and the efficiency of life are examined."
    ],
    "points": 373,
    "commentCount": 327,
    "retryCount": 0,
    "time": 1706018763
  },
  {
    "id": 39105114,
    "title": "Mozilla's New Firefox Linux Package for Ubuntu and Debian-based Distributions",
    "originLink": "https://blog.mozilla.org/en/products/4-reasons-to-try-mozillas-new-firefox-linux-package-for-ubuntu-and-debian-derivatives/",
    "originBody": "Products 4 reasons to try Mozilla’s new Firefox Linux package for Ubuntu and Debian derivatives January 23, 2024 Gabriel Bustamente & Johan Lorenzo Great news for Linux users, after months of testing, Mozilla released today a new package for Firefox on Linux (specifically on Ubuntu, Debian, and any Debian-based distribution). If you’ve heard about Linux, which is known for its open-source software and an alternative to traditional operating systems (OS), and are curious to learn more, here are four reasons why you should give our new Firefox on Linux package a try. 1. Adaptable to fit your needs Browsers are complex applications that support many scenarios in people’s daily lives and we’ve been working on improving sandbox implementations. This is why, while Firefox gets fully compatible with Snap and Flatpak, we want to offer a native package too. Firefox is available in several official formats on Linux including the Mozilla .tar.bz2 builds and sandboxed packages like Snap and Flatpak. 2. 100% built by Mozilla We are grateful for those who choose Firefox on Linux, making it a popular option and for many, their default browser. Previously, Firefox .deb packages needed the help of people and organizations (depending on the linux distribution) outside of Mozilla. With this new package, we offer Firefox assembled from its source code, without any modifications, built and supported by Mozilla. 💪 3. Better performance For more than 25 years, Mozilla has built a reputation for building free and open-source web browsers. Because the Firefox browser is open-source, we know Firefox inside and out, including how to get the best from it. For example, we built Firefox with advanced compiler-based optimizations for better performance. Note: If you are using another .deb package, you may or may not get all the optimizations we intended – it depends on the package’s maintainers. 4. Faster updates Getting the latest version with features and security fixes is key to having a good experience whenever you use Firefox. Now, our new APT repository is directly connected to the Firefox release process, so you will receive the latest updates whenever we make them available. Tip: you will still need to restart Firefox for the latest version. 😁 Good news: many Linux distributions come with Firefox pre-installed through their package manager and it’s already set as the default browser. 🙌 Can’t find it, here’s a direct link to try our .deb new Firefox on Linux package, plus, our how to install Firefox on Linux guide. Try our Firefox on Linux package today! The Firefox on Linux package now available for Ubuntu and Debian derivatives Try the latest Firefox on Linux package",
    "commentLink": "https://news.ycombinator.com/item?id=39105114",
    "commentBody": "Mozilla's new Firefox Linux package for Ubuntu and Debian derivatives (blog.mozilla.org)316 points by mfsch 18 hours agohidepastfavorite149 comments lolinder 17 hours ago> Getting the latest version with features and security fixes is key to having a good experience whenever you use Firefox. Now, our new APT repository is directly connected to the Firefox release process, so you will receive the latest updates whenever we make them available. Tip: you will still need to restart Firefox for the latest version. In the past when I've installed Firefox through a .deb, it has had this annoying habit of requiring me to restart my browser whenever it updates in the background. I'll be going about my day and all of the sudden every URL will redirect me to about:restartrequired [0] and I'll have to shut everything down to keep going. It's not clear from this announcement if they've fixed that or not. If they haven't, I'll probably just continue to install Firefox from the .tar.gz files they provide [1]. If you drop them in a directory you have write permissions to, Firefox can auto-update itself the same way it does on Windows, without any forced interruptions. [0] https://otechworld.com/wp-content/uploads/2022/04/restart-fi... [1] https://support.mozilla.org/en-US/kb/install-firefox-linux#w... reply mfsch 17 hours agoparentI’m not sure about these stable packages, but for the nightly packages they introduced recently they explicitly mention on [1] that you can keep browsing: > Following community discussions, we have updated the post to highlight that Firefox can continue browsing after an APT upgrade, allowing people to restart at their convenience. [1]: https://blog.nightly.mozilla.org/2023/10/30/introducing-mozi... reply lolinder 16 hours agorootparentOh, that's great! Between that very clear statement and the fact that in this post they call out that \"you will still need to restart Firefox for the latest version\", it sounds like they've finally fixed it! reply kbrosnan 16 hours agoparentprevI expect this would still be a problem for this .deb. The package manger is doing the updating which is what causes the problem for Firefox. The restart notice provides a way for Firefox to signal to the user that the binary on disk doesn't match the binary running. Without the warning Firefox used to randomly crash when creating new processes. The warning allows the user to perform an orderly restart (not great but neither are crashes). As the parent states the tar.gz will avoid the problem as it uses Mozilla's update process that is used across platform. A minimum set of steps to use the tar.gz are * Extract tar.gz to somewhere like /opt/firefox/ * Set the permissions so that the user or a group can read, write and execute /opt/firefox/ * Create or copy a Firefox.desktop file [1] and place it in the correct folder [2] so it shows up in your launcher [1] https://specifications.freedesktop.org/desktop-entry-spec/de... [2] https://specifications.freedesktop.org/menu-spec/latest/ar01... reply dTP90pN 13 hours agorootparentMozilla implemented a fork server to fix this issue. It is enabled in the new deb packages, but not yet in \"normal\" Firefox: https://bugzilla.mozilla.org/show_bug.cgi?id=1609882 https://bugzilla.mozilla.org/show_bug.cgi?id=1850026 reply agateau 11 hours agorootparentprevShameless plug: I wrote a tiny script to install Firefox from their tar.gz, without requiring root access: https://github.com/agateau/tmfi. reply pmontra 16 hours agoparentprevI remember that it required me to restart whenever I opened a new tab, or I wouldn't be able to use that tab. Anything in already opened tabs still worked. Then I switched from Ubuntu to Debian and I also installed the tar.gz from Mozilla. As you wrote, it updates itself. As a welcome coincidence I checked the version right now and the Help, About Firefox dialog showed me an updating status. There is a updates/0/update.status file in Firefox directory that had the \"downloading\" content. It's \"applied\" now. The update.version file contains \"122.0\" and there is a 20 MB file names \"update.mar\". The file \"last-update.log\" contains many \"PREPARE PATCH\", \"EXECUTE PATCH\", \"FINISHED PATCH\" lines on shared libraries file and other files. Apparently the new version is waiting in the updated/ directory. The About dialog still reports 121.0.1 and has a \"Restart to update Firefox\" button. I'll wait to see what happens and how long I can keep using the current version before having to switch to the new one. reply lolinder 14 hours agorootparent> I remember that it required me to restart whenever I opened a new tab, or I wouldn't be able to use that tab. Anything in already opened tabs still worked. That might have been the case. I almost invariably open every link in a new tab, so for my use case it would have felt the same as nothing working. reply dotancohen 13 hours agorootparentTree Style Tab user? reply vetinari 17 hours agoparentprevThe tar.gz build has exactly the same problem (it just doesn't complain about it): it needs to close the files from the old package and open the ones from the new package. Hence the restart. You can minimize the downtime by just restarting the browser and then using HistoryRestore previous session. The flatpak build avoids this problem by keeping both versions on the disk, up until you stop the app. Only at this time, the old version is removed, and the next start will be the new version. reply lolinder 16 hours agorootparent> it just doesn't complain about it This is the key point. I'm assuming that it downloads the update and then waits for me to agree to install it, but however it works it doesn't interrupt my browsing while I'm in the middle of something. reply gcp 13 hours agorootparentThis is how Mozilla's own updater works (if you use the .tar.gz version), but the distro package updater just overwrites everything without waiting and applications like Firefox have no control over that. As a user, you'd really want to disable unattended updates for such software. It's not an issue for these .deb packages because they enable the (experimental) forkserver. reply Dylan16807 15 hours agorootparentprev> The tar.gz build has exactly the same problem (it just doesn't complain about it) You're mixing up two problems here. There's \"please restart to update\" that you can ignore/postpone, and there's \"I have already updated and now going to sites is mostly broken\" that you can't. reply bachmeier 16 hours agorootparentprev> You can minimize the downtime by just restarting the browser and then using HistoryRestore previous session. I hated the required restart with a passion when they first started it because restoring the previous session often failed. Once that worked, it was merely miserable (you still have to log in to all your sites). reply RamRodification 16 hours agorootparent> you still have to log in to all your sites Are you running very tight cookie/session settings or something? A restart definitely does not result in having to log in again for the stuff I'm logged in to. reply kzrdude 16 hours agorootparentprevThe deb version does more than complain, it can also crash and have tabs crash due to needing restart. reply gcp 13 hours agorootparentThe whole point of the about:restartrequired warning is to avoid this problem. If you're crashing, I hope you've been filing bugs. reply krferriter 14 hours agoparentprevThat's a terrible user experience. I don't remember that happening when I used Firefox through .deb files or the Arch AUR packages. I don't want it actually updating in the background. I'm fine with it checking for updates and even pre-staging the update files for the next time I restart the browser. Swapping out the app files underneath a running browser process in the background, so no new content can be opened, sucks. Maybe I didn't use the .deb files and just used the .tar.gz. I could see that. I know I used .deb files for Chrome but I can't recall for Firefox, now that I'm thinking about it maybe I did just use the .tar.gz. I remember having to create and edit .desktop files for it. Seems counterintuitive to have a better update experience through a manually managed .tar.gz unzipped directory than through a package file actually meant to be managed through a more formal package manager. reply amarshall 14 hours agoparentprev> it has had this annoying habit of requiring me to restart my browser whenever it updates in the background This happens because Firefox detects that its files have changed underneath it while its running—this can cause problems due to, I guess, the multi-process architecture and sandboxing not liking mismatches between what is running and what it might start. The built-in updater performs updates in a way that doesn’t cause this. Package managers that do not update Firefox in-place (e.g. Nix) do not have this problem either. As noted in a sibling comment, this new package does not seem to have the issue either. I wonder what they changed. reply thorvaldsson 14 hours agoparentprevThis behaviour can be disabled by blacklisting Firefox from APT's unnattended upgrades. I wrote[0] something up on this last year if anyone is interested. [0]: https://hth.is/2023/01/10/blacklist-firefox/ reply TrianguloY 16 hours agoparentprevOne of the benefits of kde offline updates is that this never happens, things are never updated while you are using them. You need to restart from time to time though, I do it weekly. reply curt15 16 hours agorootparentRestarting your whole system is not necessary for the firefox flatpak though -- updates are installed in a parallel directory and applied the next time the user re-launches Firefox. reply wkat4242 16 hours agoparentprevIt does that on FreeBSD as well yeah. Not a huge deal for me as I only install updates when I'm not in the middle of things. I love an OS that doesn't force me to do anythingthe “Ubuntu Mozilla Team” does not consist of people from Mozilla Kudos to open source maintainers, the hidden heroes who make your life easier while expecting nothing in return. reply ghostpepper 17 hours agorootparentprevThis is confusing because it’s posted on Mozilla’s official site reply cassianoleal 17 hours agorootparentThe thing posted on Mozilla's website (the OP) is official. The PPA posted at the root of this comment thread isn't. reply EasyMark 8 hours agoparentprevGet rid of your PPA package and use the official repo, it's 99.9% better in almost all cases. More details on google/youtube on how to do that, it's too much to type on HN and should be on the first page of google/duckduckgo. I would guess the people supporting the PPA will stop updating it after a while due to there being an official repo to use. reply eduction 14 hours agoprevDebian/Ubuntu is also the sole chosen distro for Spotify and Signal first-party native packages. I try to use Fedora whenever possible so have noticed that Debian tends to be the first choice, even for \"client\" type software, which makes sense considering the popularity and cross compatibility of Ubuntu+Debian. (You can generally still get stuff on Fedora - they do their own Firefox package of course, Signal is a flatpak, etc) reply declan_roberts 15 hours agoprevThis is a great update from the Mozilla team! Please continue the hard work. The only thing missing is a continued commitment to privacy and liberty. What has changed since the infamous “We Need More Deplatforming (2021)”[1] article by CEO Mitchell Baker? I absolutely can not move past this and I think Mozilla needs to make a strong commitment to our civil rights. 1. https://blog.mozilla.org/en/mozilla/we-need-more-than-deplat... reply thejsa 14 hours agoparent> the infamous “We Need More Deplatforming (2021)”[1] article That seems a misquote; the title of the article you link is \"We need more than deplatforming\", and it isn't apparent to me that it advocates for deplatforming at all. reply wolverine876 14 hours agoparentprevWhy do you think it has changed? reply nairboon 16 hours agoprevSo, we have come full circle? There used to be a .deb repo, then there wasn't and now we have one again. reply WD-42 16 hours agoparent.deb packages, just like server side rendering, are the new hotness. reply mminer237 12 hours agorootparent.deb packages have been the primary method of distributing programs on Linux for well over a decade at this point. reply imp0cat 13 hours agorootparentprevNah, they never went out of style. reply EasyMark 8 hours agorootparentprevdon't forget jquery and anti frameworks becoming cool again. reply glandium 11 hours agoparentprevI don't remember Mozilla ever having a repo for Debian packages. Did I miss something? reply p4bl0 15 hours agoprevIs there really a point these days in having the very last release rather than the ESR version packaged by Debian? If so what are the benefits for end users? reply jillesvangurp 15 hours agoparentIn case of security fixes, you will stay unpatched for a bit less time. ESR is intended for places that really don't like any form of change like rigid enterprises and banks and such. End users should probably steer clear of that unless they have a good reason not to; which they generally don't. Except for a false sense of security. I personally don't see a good reason to opt out of security changes for any longer than strictly necessary and that's exactly what you do when using ESR. First they get backported by Mozilla. That's after normal users receive them. This takes time. Then testing takes place because they don't want to push out a hasty fix for ESR. This takes more time. And then third party packagers need to pick up the changes and repackage (which is mostly pointless and adds more time). And then eventually it rolls out days/weeks/months after normal users have long received the patches. I don't seen any good reason for such lengthy delays for normal users. In some big companies where they manually review updates for workstations it's a compromise between the extra work and stability. But the tradeoff is timely access to security fixes; which ESR simply doesn't provide. reply ndriscoll 14 hours agorootparentEnd users also have plenty of reason to dislike change. Change tends to mean things like exciting new spying and UI regressions. Meanwhile, security concerns are often overblown. If you're just using your browser for e.g. email, news, facebook, youtube, netflix, amazon, and your bank, and not venturing out into the seedier parts of the web, you're probably at ~0% risk of some RCE exploit. In any case, an adblocker is probably better protection than auto-updates. reply IWeldMelons 14 hours agorootparentYep. vaapi was crashing on several builds in a row on a popos machine, till it magically stopped doing that 2 weeks ago. ESR though, worked just fine. reply gcp 13 hours agorootparentprevChange tends to mean things like exciting new spying and UI regressions Or missing out on new anti-fingerprinting and anti-tracking improvements. Note that adblockers don't generally do the former. reply Dalewyn 14 hours agorootparentprevI don't see a problem waiting for alpha- and beta-testing to finish first before the new code is distributed to actual normal users. Note, the \"normal users\" you describe are involuntary testers who got forced into the role because keeping testers on the payroll is so last century. reply Tijdreiziger 13 hours agorootparentThe voluntary testers are the Firefox Nightly and Firefox Beta users, not the Firefox stable users. reply Dalewyn 6 hours agorootparentI said involuntary testers. The guys who sign up for the nightlies and beta know full well what they signed up for, but not the involuntary testers aka \"normal users\". reply EasyMark 8 hours agoparentprevThey are always adding new stuff, but I use floorp which is based on ESR and well I'm still surviving and internetting even in January 2024. I think you're in good company. I really just need a browser, and firefox is my choice, but floorp is just a bit more customizable in appearence and I like that (no need to mess around with user.css). Let them test and debug the new fangled stuff and I'll get a super stable version by the time it hits ESR reply maxloh 15 hours agoparentprevIf you are a web developer, you would better develop your app using the latest dev tools and targeting the latest web specifications only. Most companies don't prioritize legacy support, so there is no point in developing for Firefox ESR. reply Karellen 14 hours agorootparent> Most companies don't prioritize legacy support, On the other hand, the more legacy you support, the larger your potential customer base is. Also, if everyone else in the sector only supports the most recent release of only two browsers, you might have that extended customer base all to yourself. > you would better develop your app using the latest dev tools and targeting the latest web specifications only. That is one school of thought. Another is that you should regularly use the oldest platform (hardware, OS, browser) that you want to support. That way you won't get to two weeks before release, decide you ought to test on a dual-core 4Gb machine like you were planning on supporting, and realise that although it technically runs, it's so frustratingly slow that no-one would want to, but there's not enough time to make the changes you'd need to get it working \"acceptably\". OTOH, if you were using it on hardware where it ran like a dog for you from the 3rd sprint, you'd probably have got round to making the changes for it to be acceptable on that era platform. reply maxloh 12 hours agorootparent> On the other hand, the more legacy you support, the larger your potential customer base is. It is more a decision about the trade-off between costs and benefits. Supporting really old browsers like IE is a pain in the ass. > Another is that you should regularly use the oldest platform (hardware, OS, browser) that you want to support. It is possible to manually slowing down your browser with dev tools. https://imgur.com/a/99XbwN9 reply EasyMark 8 hours agorootparentprevESR isn't -that- old. jeesh. If you develop on ESR and it works well it's definitely going to work in something newer. However every web dev I know tries in multiple versions and multiple browsers. reply cf100clunk 12 hours agoparentprevMy system, my choice. I'll be testing Mozilla's packages, for sure. As for Debian's standard response to using non-Debian packages, they have the following wiki entry: https://wiki.debian.org/DontBreakDebian reply hanniabu 15 hours agoparentprevexploit patches reply maxloh 15 hours agorootparentFirefox ESR will receive security patches too, so this argument doesn't apply IMO. reply wkat4242 16 hours agoprevOh good!! Snap freethere's a reason why some want their packages to come from the people who actually made the software Not necessarily either. Think of the Audacity fiasco. We were very comfortable using the Debain package but not their Flatpack and obviously not the snap. I realise I'm replying to a subtext but the context is the same. reply sylvestre 15 hours agorootparentprevFeels like an excellent track record to me: https://tracker.debian.org/pkg/firefox-esr ;) And Mike is also a prolific Firefox developer. reply mardifoufs 15 hours agorootparentAhhh I wasn't familiar with this package in particular. I just have had a bunch of ... weird issues dues to some quirks in some packaged browsers. Basically it was never a standard install, and it sucked for ironing out issues. Glad to hear the Debian packages are doing great, not surprised either! reply esaym 16 hours agoprevPerhaps I'm missing something, but like 6 years ago, I downloaded firefox on linux from mozilla which came as a tar file that I extracted into my home directory and made shortcuts to. It has been working fine and auto updating ever since :shrug: I don't see what the point of this is. reply mrweasel 14 hours agoparentIt's handy if multiple users use the same machine, or if you're on a managed system. reply hysan 12 hours agoprevDoes FF still not use KDE’s native file picker without installing a (not readily available) package to modify it? I stopped trying to use FF on Linux a long time ago, but seeing a headline with Linux in it made me perk up. reply bee_rider 15 hours agoprevI wonder if they’ll keep this up permanently. Tempted to switch (maybe I can remove my system’s Snap infection), but redoing bookmarks and extensions (and getting my ublock config and OneTab bookmarks) is a pain. And I’ll have to figure out how to not lose my passwords this time. What a pain. reply MaKey 15 hours agoparentYou can just use your old Firefox profile. Copy it from ~/snap/firefox/common/.mozilla/firefox to ~/.mozilla/firefox/ reply bee_rider 15 hours agorootparentOh dang, nice. I ended up exporting everything and then re-importing. But it was not as annoying as I expected. I think I was mostly dreading it because I’d lost passwords in the past to this sort of thing, because I wasn’t expecting it, so I didn’t export beforehand. Just bad associations in my brain. reply Gabrys1 14 hours agoparentprevWhy would you do that? Your firefox config should be kept in .mozilla/firefox (or is it .config/mozilla now?) and simply picked up by the new firefox binary. The only time this doesn't work is when you install an older version whose configuration \"format\" is not compatible with the newer version's one. Edit: a sibling commenter just made me aware of the config for the \"snappy\" version actually resides in another directory. That said you can manually copy the files to the standard place reply bee_rider 11 hours agorootparentI did that because I didn’t know where the snap files were stored, or that it was different from the normal spot. reply ur-whale 17 hours agoprevThe missed the key 5th benefit: Allows you to sudo apt-get purge snapd without annoying unwanted consequences. reply Osiris 16 hours agoparentI recently had a problem where snapd kept sigfaulting and the fix was to purge it completely. It turns out that everything I had in snap was available in flatpak, which I prefer anyway. reply EasyMark 8 hours agorootparentthere is also appimage sometimes if you ever need something that isn't flatpak or system packages. It launches fast just like flatpak and unlike snap. reply mortallywounded 15 hours agoparentprevDon't forget `sudo apt-mark hold snapd` to prevent reinstalling it as a side dependency. reply dizhn 14 hours agoparentprevI believe lxd is still only available as snap packages on Ubuntu. I am not using it anymore but it's actually quite nice. Fortunately it's now been forked as incus and both Debian and openSUSE will have it. Probably a red-haty distro too. reply ColonelBlimp 16 hours agoparentprevYou didn’t have to wait for this new Firefox package to stop using snaps, did you? reply arzig 16 hours agorootparentThe Firefox deb in the repos redirected and did a snap install or some weirdness at least at some point. reply jlarocco 16 hours agorootparentMust have been an Ubuntu problem. I install Firefox via Debian's package, and have never had 'snapd' installed. reply arzig 15 hours agorootparentYes. Ubuntu maintained their own deb package that redirected to a snap install. reply ColonelBlimp 15 hours agorootparentprevFlatpaks are a perfectly functional option https://flathub.org/apps/org.mozilla.firefox reply chrsw 16 hours agoprevThis seems like great news. Managing software through one distro supported interface is less mental burden on me, the user. For developers, it's more work, I get that. reply mobilemidget 15 hours agoprevThey write \"advanced compiler-based optimizations\"; I guess that doesn't fall under open source? reply sfink 15 hours agoparentThink of it more as \"we compile with -O3\". If you installed with the .tar.bz2, you'd get that already. If you use a 3rd party produced .deb, they might build with -O2. That's just an example; the blog is probably referring to PGO and/or LTO: profile-guided optimization and link time optimization, which require some fiddly setup that I believe third parties have traditionally not bothered with. And yes, it's all open source. You can see all of the bits that go into producing that .tar.bz2. You can even see the full build log if you like, eg by going to https://treeherder.mozilla.org/jobs?repo=mozilla-release&sea... . Pick your platform, click on Bpgo, click on B, select the \"log\" link down in the lower left. There are plenty of Mozilla-related things to complain about, openness of the browser development process is not one of them. reply Jach 12 hours agorootparentGentoo users like me fiddle with such things, they're just flags you can turn on before compiling. It's been a while since I experimented though, I didn't notice any advantage, with a disadvantage being an increase in compile time. I haven't tried on my newer machine yet though... but I suspect I wouldn't notice any difference. reply asadotzler 15 hours agoparentprevWrong https://firefox-source-docs.mozilla.org/build/buildsystem/pg... reply Timber-6539 15 hours agoprevTakes 7 steps to install Firefox this way as opposed to apt-get install. reply hnarn 15 hours agoparentThis comment doesn’t make any sense. Installing a deb file is done with apt, what are you actually referring to when you say “apt-get install”? reply dsr_ 14 hours agorootparentapt, apt-get, aptitude and synaptic are all front ends to the same package management system. reply hnarn 13 hours agorootparentI’m well aware of that. How is that in any way relevant to the question I was asking? reply Timber-6539 10 hours agorootparentapt-get install will automatically download and install the deb for you. reply hnarn 1 hour agorootparentAre you insinuating that this would not happen with the instructions from Firefox? reply Timber-6539 28 minutes agorootparentNot unless you create an automated script for the instructions. Am strictly picking on the guide. reply Gabrys1 14 hours agoparentprev1. Add the apt repo 2. Add the repo's public key 3. apt-get update 4. apt-get install firefox What 3 steps am I missing? reply hnarn 13 hours agorootparentCounting “steps” is inane to begin with. Those could all be done with a one-liner, does that make it one step? reply Timber-6539 10 hours agorootparentYou could do this with a one-liner I suppose, but the Firefox guide still gives you 7 steps to accomplish something apt will do with a single command. reply hnarn 1 hour agorootparentYou are either trolling or you have a very fundamental lack of understanding for what those seven steps actually do and how package management works. reply Timber-6539 26 minutes agorootparentAm simply saying the alternative to doing this is a \"one-click\" apt-get install. Not sure why you have your panties in a twist. reply whalesalad 15 hours agoparentprevbut you are on an older version of ffx reply Timber-6539 9 hours agorootparent*temporarily delayed version. This is a complicated nuance depending on the release cadence of your distro. reply aquova 16 hours agoprevIs there more technical information than this article provides? Did Firefox not have a .deb package until now? It's been a few years since I used Ubuntu, but I find that really surprising. I'm not sure what this new package is or how it differs. reply michaelt 16 hours agoparent* In the ancient past, when dinosaurs roamed the earth, Ubuntu distributed Firefox as a .deb * Recently (well, in the last 2-4 years) to boost the uptake of Canonical's \"snap\" packaging system, Ubuntu switched to only distributing Firefox as a snap. * There are a lot of snap haters around. Not least because canonical fucked up the update mechanism in the first year or two, had a bunch of performance problems (now mostly solved?), and made a bunch of weird broken snaps for things like docker. * An unofficial firefox package for Ubuntu was then created, so people who didn't want to use the snap could avoid doing so. * By some accounts, Mozilla is keen on controlling the entire release channel, so security updates don't have to wait on volunteer maintainers. (IDK if this is true or not, but I've heard second-hand claims) * This new package lets Mozilla release to users directly, and lets snap haters avoid snap without using unofficial packages. reply olyjohn 15 hours agorootparentYeah the Snap package is still annoying. Like every day it pops up and tells me to close Firefox to update it. So I close it, and then nothing happens. Then I wait, and nothing happens. Occasionally, it notices that I closed Firefox and applies the update. But it's a frustrating annoyance. reply mardifoufs 15 hours agorootparentprevI thought the decision to switch to snaps was made by Mozilla. Did that turn out to be false? reply exitzer0 16 hours agoparentprevDepending on your distro, the Firefox package you see in the official repo is ... well packaged by the repo maintianers and some distros will include customizations like adding in bookmarks to the distro's welcome/docs/pages/etc. Sometimes even more things like feature settings are changed. It is usually not anything major and usually things that you might prefer like disabling pocket, etc. What Mozilla is calling out here is a seperate, Mozilla-direct repo. This means you skip the official repos. Everyone should think carefully about what this means. reply bee_rider 15 hours agorootparentOn Ubuntu, Firefox is packaged as a snap, not as a normal .deb. It is pretty bad. reply tenebrisalietum 16 hours agoparentprevMozilla is officially releasing a .deb for Firefox now. The one in the repos is not maintained by Mozilla. reply account42 17 hours agoprev [6 more] [flagged] perihelions 17 hours agoparentWhat anti-features do Debian's maintainers remove from Firefox, in practice? (Meaning the one they've already been maintaining for a while, a firefox-esr package. There's also an upcoming one for the normal release cadence). edit: Answering my own question, https://wiki.debian.org/Firefox#Disabling_automatic_connecti... (\"Here is a table of the above parameters and how Debian diverges from the upstream default:\") Looks to me like a fairly small, unimportant delta. edit #2: I didn't mean to criticize your comment overly strongly! I hope I wasn't the reason you got flagged—I tried to vouch you but I regret that wasn't enough. reply yjftsjthsd-h 16 hours agorootparentHuh, that's interesting - I've definitely used distro packages that disabled some(?) of the telemetry stuff. If Debian doesn't, I wonder what distro I'm remembering. Or maybe they used to and just don't now. reply perihelions 16 hours agorootparentIt does disable telemetry! This is the list I linked—it's all different types of automatic network connections, from anonymized telemetry to update checks to speculative connections. reply yoavm 17 hours agoparentprevThis seems harsh considering they're still making the only true user-agent in the world. Which maintainer are you trusting so much, and what anti-features are they removing? reply ok123456 17 hours agoparentprev [–] The anti-features that I'm aware of are runtime configuration flags and profile settings. Having an official build from Mozilla makes a supply chain attack harder. Additionally, they claim to compile with PGO, which random builds may not have. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Mozilla has launched a new Firefox package designed specifically for Linux, including Ubuntu, Debian, and other Debian-based distributions.",
      "The package is built and supported by Mozilla, ensuring better performance and faster updates.",
      "Users can download the package directly from the Mozilla website, offering adaptability and seamless integration with the Firefox release process."
    ],
    "commentSummary": [
      "Mozilla has released a Firefox Linux package for Ubuntu and Debian derivatives, providing users with direct updates from the Firefox release process.",
      "The package's effectiveness in resolving the need for browser restarts after updates is uncertain.",
      "Users have varying preferences for installing and updating Firefox on Linux systems, with discussions around uninterrupted browsing, inconveniences of restarts, update interruptions, login and crash issues."
    ],
    "points": 316,
    "commentCount": 149,
    "retryCount": 0,
    "time": 1706025619
  },
  {
    "id": 39106464,
    "title": "Gene therapy enables 11-year-old boy to hear, opening doors for treatment of congenital deafness",
    "originLink": "https://www.nytimes.com/2024/01/23/health/deaf-gene-therapy.html",
    "originBody": "ADVERTISEMENT SKIP ADVERTISEMENT Gene Therapy Allows an 11-Year-Old Boy to Hear for the First Time The genetic treatment targeted a particular kind of congenital deafness and will soon be tried in children who are younger. Share full article 353 Aissam Dam, 11, the first person to receive gene therapy in the U.S. for congenital deafness, at the Children’s Hospital of Philadelphia. Credit... Hannah Beier for The New York Times By Gina Kolata Gina Kolata visited the Children’s Hospital of Philadelphia and met with Aissam Dam, his father and the researchers they worked with. Jan. 23, 2024 Aissam Dam, an 11-year-old boy, grew up in a world of profound silence. He was born deaf and had never heard anything. While living in a poor community in Morocco, he expressed himself with a sign language he invented and had no schooling. Last year, after moving to Spain, his family took him to a hearing specialist, who made a surprising suggestion: Aissam might be eligible for a clinical trial using gene therapy. On Oct. 4, Aissam was treated at the Children’s Hospital of Philadelphia, becoming the first person to get gene therapy in the United States for congenital deafness. The goal was to provide him with hearing, but the researchers had no idea if the treatment would work or, if it did, how much he would hear. The treatment was a success, introducing a child who had known nothing of sound to a new world. “There’s no sound I don’t like,” Aissam said, with the help of interpreters during an interview last week. “They’re all good.” While hundreds of millions of people in the world live with hearing loss that is defined as disabling, Aissam is among those whose deafness is congenital. His is an extremely rare form, caused by a mutation in a single gene, otoferlin. Otoferlin deafness affects about 200,000 people worldwide. The goal of the gene therapy is to replace the mutated otoferlin gene in patients’ ears with a functional gene. Although it will take years for doctors to sign up many more patients — and younger ones — to further test the therapy, researchers said that success for patients like Aissam could lead to gene therapies that target other forms of congenital deafness. It is a “groundbreaking” study, said Dr. Dylan K. Chan, a pediatric otolaryngologist at the University of California, San Francisco, and director of its Children’s Communication Center; he was not involved in the trial. The one in which Aissam participated is supported by Eli Lilly and a small biotechnology firm it owns, Akouos. Investigators hope to eventually expand the study to six centers across the United States. Image Special earphones being used for Aissam’s hearing test. His form of deafness is rare, caused by a mutation in a single gene, otoferlin. Credit... Hannah Beier for The New York Times Aissam’s trial is one of five that are either underway (the others are in China and Europe) or about to start. Investigators from all five of the studies will be presenting their data on Feb. 3 at a meeting of the Association for Research in Otolaryngology. The studies, researchers said, mark a new frontier for gene therapy which, until now, had steered clear of hearing loss. “There has never been a biological or medical or surgical way to correct the underlying biological changes that cause the inner ear to not function,” Dr. Chan said. Although otoferlin mutations are not the most common cause of congenital deafness, there is a reason so many researchers started with it. That form of congenital deafness, said Dr. John A. Germiller, an otolaryngologist who is leading the CHOP study, is “low hanging fruit.” The mutated otoferlin gene destroys a protein in the inner ear’s hair cells necessary to transmit sound to the brain. With many of the other mutations that cause deafness, hair cells die during infancy or even at the fetal stage. But with otoferlin deafness, hair cells can survive for years, allowing time for the defective gene to be replaced with gene therapy. Image Aissam’s trial at the Children’s Hospital of Philadelphia is among five that are either underway (the others are in China and Europe) or about to start. Credit... Hannah Beier for The New York Times There’s an advantage in using gene therapy to allow children to hear. Most of the mutations that affect hearing — there are approximately 150 — do not affect any other part of the body. Some genes are actually unique to the ear. The inner ear is a small closed compartment, so gene therapy delivered there would not affect cells in other parts of the body, said Manny Simons, chief executive and co-founder of Akouos and senior vice president of gene therapy at Lilly. But getting the genes to the cochlea, a spiral-shaped cavity close to the center of the skull, is challenging. The cochlea is filled with fluid, is lined with 3,500 hair cells and is encased in a dense dome of bone with a tiny, round membrane. Sound sets off a wave of fluid in the cochlea and stimulates the hair cells to transmit signals to the brain. Each hair responds to a different frequency, enabling a person to hear the richness of sound. The gene therapy consists of a harmless virus carrying new otoferlin genes in two drops of liquid that are delicately injected down the length of the cochlea, delivering the genes to each hair cell. Yet despite the promise of otoferlin gene therapy, finding the right patients for the trial was difficult. One issue is the very idea of treating deafness. “There is an internal Deaf community that doesn’t see itself as needing to be cured,” said Dr. Robert C. Nutt, a developmental and behavioral pediatrician in Wilmington, N.C., who is deaf. Some Deaf parents, he added, celebrate when their newborn baby’s hearing test indicates that the baby is deaf too and so can be part of their community. Making the issue of gene therapy even more complicated is the standard intervention for otoferlin hearing loss: a cochlear implant. The device, which uses electrodes to stimulate auditory nerves in the inner ear, allows patients to hear sounds, especially those needed to understand speech. But the implant does not provide the full richness of sound — and is said to assist in hearing but without restoring it completely. Image Dr. John Germiller, an otolaryngologist who is leading the CHOP study. Credit... Hannah Beier for The New York Times Most babies born with otoferlin deafness get cochlear implants in infancy and are therefore ineligible for the trial. The implants somewhat alter the cochlea, which could hamper the interpretation of gene therapy results. The Food and Drug Administration, which allowed the CHOP study to go forward, asked that, for safety reasons, the researchers start with older children, not infants, and treat only one ear. The challenge for the U.S. study was to find older children whose parents would agree to the study, who had otoferlin deafness and who did not have cochlear implants. Aissam never had cochlear implants. He never had schooling in Morocco to help him develop communication skills. But three years ago, when he was 8, his father, Youssef Dam, a construction worker, got a job in Barcelona, Spain. For the first time, Aissam went to school, enrolling in a school for the deaf, where he learned Spanish Sign Language. Soon after, his family learned of the gene therapy trial. When Aissam was deemed eligible to be patient No. 1, Lilly and Akouos paid for him and his father to live in Philadelphia for four months, while Aissam received gene therapy and follow-up hearing tests. No one knew whether the nerve cells that communicate with the hair cells of the cochlea would still be intact and functional in someone who had been deaf for 11 years, Dr. Simons of Lilly said. It was not even clear what dose of the new genes to give. All that the researchers had to go on were studies with mice. “We were flying blind,” Dr. Germiller said. Aissam’s results, his doctors said, were remarkable. In an interview at CHOP, his father said through an interpreter — he speaks a North African language from the Amazigh family, commonly known as Berber — that Aissam was hearing traffic noises just days after the treatment. When Aissam had a hearing test two months later, his hearing in the treated ear was close to normal. But no matter how well the gene therapy works, the researchers recognize that Aissam may never be able to understand or speak a language, Dr. Germiller said. The brain has a narrow window for learning to speak beginning around ages 2 to 3, he explained. After age 5, the window for learning spoken language is permanently shut. Hearing can still help patients even if they never learn to speak, he noted. They can hear traffic or know when someone is trying to communicate. The ability to hear also can help with lip reading. Image Aissam signing to an interpreter during an interview at the children’s hospital. Credit... Hannah Beier for The New York Times Now that gene therapy has proved safe for Aissam and for another child in Taiwan treated two months after him, researchers at the hospital in Philadelphia are able to move on to younger children. They have two lined up, a 3-year-old boy from Miami and a 3-year-old girl from San Francisco, both of whom got cochlear implants in only one ear, so that the other could be treated with gene therapy. If the Lilly trial of otoferlin gene therapy is proved to be effective and safe, “there will be a lot of interest in other genes” that cause deafness, said Dr. Margaret A. Kenna, an otolaryngologist at Boston Children’s Hospital and professor of otolaryngology at Harvard Medical School. Dr. Kenna, an investigator in the Lilly trial, added, “It’s been a long time coming.” “For decades people have been saying, ‘When is this going to work?’” Dr. Kenna said. “I didn’t think gene therapy would begin in my practice lifetime. But here it is.” Of the other studies, two are in China where investigators are treating younger children and in both ears. Results from one, supported by the National Natural Science Foundation of China and Shanghai Refreshgene Therapeutics, will be reported Wednesday in the journal The Lancet. The other is supported by Otovia Therapeutics and various programs in China. A third study is sponsored by Regeneron and Decibel Therapeutics. Researchers in Europe so far have treated one child, who is younger than 2, and in one ear. Another study by Sensorion is expected to start this month. On a recent frigid morning, Aissam sat in a conference room at CHOP and, with the help of three translators, patiently answered questions about his remarkable experience. He’s a solemn child with a round face and big brown eyes. There was an interpreter for his father, and the sign language team had a Certified Deaf Interpreter — a person who is deaf translated his signs into American Sign Language — and an interpreter who knew American Sign Language and spoke his words. Their system worked to a certain extent but robbed the conversation of spontaneity and forced Aissam to answer in short sentences or phrases, minimizing the expression of his personality. But Aissam managed to convey the wonder of hearing. Noises and voices frightened him initially, he said. But then, as the world of sound opened up, he began to enjoy every sound he heard — elevators, voices, the sound of scissors snipping his hair at a barbershop. And there was music, which he heard for the first time one day while getting his hair cut. Asked if there was a sound he particularly liked, Aissam did not hesitate. “People,” he signed. Gina Kolata reports on diseases and treatments, how treatments are discovered and tested, and how they affect people. More about Gina Kolata A version of this article appears in print on , Section A, Page 1 of the New York edition with the headline: Innovative Use of Gene Therapy Lets Boy Hear for the First Time. Order ReprintsToday’s PaperSubscribe 353 Share full article 353 ADVERTISEMENT SKIP ADVERTISEMENT",
    "commentLink": "https://news.ycombinator.com/item?id=39106464",
    "commentBody": "Gene therapy allows an 11-year-old boy to hear (nytimes.com)294 points by mikhael 16 hours agohidepastfavorite153 comments neonate 12 hours agohttps://archive.ph/3FmSZ twostorytower 11 hours agoprevBut no matter how well the gene therapy works, the researchers recognize that Aissam may never be able to understand or speak a language, Dr. Germiller said. The brain has a narrow window for learning to speak beginning around ages 2 to 3, he explained. After age 5, the window for learning spoken language is permanently shut. Wow that's incredibly sad, but I am glad that this will eventually get into the ears of thousands of deaf newborns. Incredible medical advancement. Gives me hope that one day my tinnitus may have a cure. reply ebiester 10 hours agoparentI am confused at this: the window that people speak about is largely one of having any language. Aissam seems to have a language, albeit an idiomatic one that was developed to communicate with parents. If so, he has developed the speech pathways, even if any given language will be one of second language acquisition. Now there may be another reason, but the article is either missing context or the question was not expressed in a way where the doctor answered in a way that follows the science around the critical language period, as I understand it (at least) reply dbcurtis 9 hours agorootparentLanguage development starts earlier than most people think. Babies babble with all the sounds the human vocal tract can make, but by 6 months they are only babbling with the sounds of the language in their household. Up to age 12 or so you can learn a second language accent-free, but at that age an important developmental milestone cements phonemes in place. About 45 years ago I heard Chomsky speak on the idea that the human brain is wired to learn a grammar as much as a bird is wired for birdsong. So learning some grammar is innate, but the particulars are up to environment. Source: 1/2 a century ago I was a bit of a developmental linguistics nerd. Disclaimer: But many memories have faded. reply dataflow 4 hours agorootparent> Up to age 12 or so you can learn a second language accent-free, but at that age an important developmental milestone cements phonemes in place. Seems... dubious? What about people who immigrate later (like in high school) and actually pick up the sounds and accents flawlessly? I've seen folks like that and I'm pretty sure they weren't in high school at age 12. reply resonious 2 hours agorootparentTo take your point farther, I've met plenty of people who've learned their second language as an adult and can speak nearly accent free. As far as I can tell, the only difference between those with and without an accent is conscious effort. The whole idea that there's a window that closes when you're a kid has seemed a little weird to me. Adults learn new vocab and grammar all the time. Learning another language is the same, just a little more extreme. I wonder if there's some actual scientific evidence for the language learning window, and not just some developmental psychology observations. reply mariuolo 45 minutes agorootparentPerhaps their native languages were phoneme-rich and could map those to their L2? reply Tor3 2 hours agorootparentprevIt's very very rare. Though it's not a cut-off at age 12, it just gets more and more rare (and takes longer) from around ten and older. Not sure if it's so much about the vocal tract, it's more about the brain's ability to hear sounds (my wife can't hear the difference between a number of sounds despite having lived in my country for many years and speaking the language well). In all my life I've only met two 100% accent-free speakers who learned my language as adults, and a third one who was almost there. Everyone else has something I can detect. But children.. a five-year old Japanese girl could repeat everything I said with perfect pronunciation and intonation, first try. Slightly exceptional girl perhaps, she learned the language in a very short time. reply dataflow 2 hours agorootparent> Though it's not a cut-off at age 12, it just gets more and more rare (and takes longer) from around ten and older. I mean, nobody is disputing \"it gets harder as you become older\". I totally believe that. Lots of things gradually become harder as you grow older, and language doesn't seem particularly different in that regard. The question is whether that's because your body \"cements phonemes in place\" around age 12, or whether there's something else at play that's likely gradual and not such a sharp boundary. The fact that it's rare might be just due to the (a) effort required to learn something new being higher in general, or the (b) perceived RoI being lower, or a ton of other factors that don't boil down to \"your phonemes are cemented in place\"... right? Anecdotally I know in at least one particular case that I observed and inquired about, that person (who's also very smart and hard-working in general) told me they made a very deliberate effort over a handful of years to improve their accent after immigrating, and that's how they sounded like a native now. I totally believe that many people are just unwilling to invest the effort required (which certainly increases with age). I'm just finding it hard to believe there's some biological force preventing you from doing it past age 12, given I've seen otherwise. > it's more about the brain's ability to hear sounds (my wife can't hear the difference between a number of sounds despite having lived in my country for many years and speaking the language well). That might be true for some sounds for some people, but I also have a hard time believing it's such a general thing to the extent you're painting it here. It seems more likely to me the explanation is something else, like maybe nobody has managed to give her a good enough explanation as to how they're different sounds. (Maybe not the best example, but I had a hard time distinguishing ch and s in German until someone explained to me how they're each pronounced. Now I can hear them much better, and pronounce them not-too-awfully too.) reply Tor3 1 hour agorootparentThe sounds my wife can't separate are sounds which aren't separate in her native language - the mapping is trivial. It's the same with others from her country, it just varies between individuals (but strongly correlated with age, and to what exposure they've had to other languages when younger). And believe me, it's not about putting in the effort. But to distinguish certain sounds my wife has to watch my lips - this is particularly noticeable if I dictate and she writes. As for training - she received two years of training (30 hours a week) with expert teachers who knew a lot of tricks for how to hear and (not the least) pronounce sounds. Tricks that I didn't know about. But I've also seen this with American and some English adults trying to learn Norwegian - a great many of them can't hear the difference between vowels which, to me, are totally different. Can't hear the difference between the words \"har\" and \"her\", for example (NB: Norwegian sounds. Not English vowels). It seems to take a couple of years of daily ear training (or rather, brain training). As always, there are exceptions. But those exceptions are truly standing out. (Added: As soon as there's context or visibility the problem is much reduced - but it's still there, as soon as there's only audio and their language level isn't good enough to \"select\" the right words from context). reply lucioperca 1 hour agorootparenthttps://www.youtube.com/watch?v=qXxV2C1ri2k reply radicalbyte 1 hour agorootparentprevIt's bullshit. I can speak Dutch without an English accent and I learned the language in my 20s, having 0 exposure to it in my youth. There are to main sounds which are in Dutch but not English ([ui] and the hard [G]) and one which isn't in some English dialects (the rolling [R]). However you can absolutely learn them as an adult. It just requires serious training (years of hard practice, same as with sports). You literally have to build up the facial muscles. One thing I will admit: I choose not to pronounce the [ui] sound properly due to a combination of being lazy, identifying as an Dutch-as-second-language speaker and because for some absolutely irrational it sounds really childish to my years. That latter point played a surprising role in my lack of ability with the French language. It feels theatrical in the way that certain queer people choose to project their speak and I do not want to project as being something I am not (or be confused as someone making crude n-phobic caricatures which would be 1000x worse because it could make someone else feel insulted). Honestly it's probably a tick I have from being raised to be a polite British gentleman :) reply darkerside 35 minutes agorootparentI've noticed that some people speak foreign languages with what I consider to be quite poor accents, but they enunciate strongly. If they are smiling, it almost comes off to me like they are making fun of the language or just pretending they can speak it well. To the point I've laughed along with them before realizing my mistake and cringing. And then I wonder which of us is more fluent? The one who with the better accent or the one who can more confidently project their thoughts in that language regardless of \"skill\"? (Probably the latter) reply iopq 3 hours agorootparentprevAnecdotally, I know a Serbian who went to study to the US for a year in high school and has a perfect Californian accent. reply dataflow 3 hours agorootparentThere you go! reply bobmaxup 2 hours agorootparentprevTo be fair, they did qualify the statement with \"or so\". reply dataflow 2 hours agorootparentThis isn't 12 vs. 12.5 I'm disputing, it's like 12 vs. >= 16. reply hiAndrewQuinn 2 hours agorootparentprevOh, rad, I didn't know accents worked like that! So if I speak to my kids in English, and my wife speaks to them in Finnish, they'll get to grow up with a disconcerting mix of newscaster English and northern Finnish drawl. I wonder if getting exposed to a bunch of languages as a kid is why I have a (relatively) mild accent in Finnish now, despite only starting to learn at 26 or so. reply omeid2 5 hours agorootparentprev> Up to age 12 or so you can learn a second language accent-free, but at that age an important developmental milestone cements phonemes in place. This is only true if the second language has sounds that you don't have in your first language. reply Tor3 2 hours agorootparentOh, right. I forgot all those Dutch speed skaters who used to train in Norway a couple of generations back. Ard Schenk.. Kees Verkerk and many others. The majority of them learned Norwegian completely accent-free (you really had to listen to figure out they weren't native), despite not learning the language as kids. I figured that must have been because Dutch has such a variety of native sounds that it basically covers Norwegian language sounds. reply dbcurtis 4 hours agorootparentprevCan you name a language pair where that works? I can’t think of one. reply iopq 3 hours agorootparentIf your native language is Ukrainian, when you learn Russian you don't get more phonemes, unless you count slightly different mouth positions of vowels. reply dataflow 3 hours agorootparentprevhttps://linguistics.stackexchange.com/a/30633 reply SamBam 9 hours agorootparentprevYeah, this seems confusing. Obviously his brain has a solid understanding of language, and he even recently learned a new language (Spanish Sign Language) so learning another language should be possible. It sounds like the researchers are saying there's something special about learning spoken language. But it seems to me that there can't have been many cases similar to his. reply mlyle 8 hours agorootparent> It sounds like the researchers are saying there's something special about learning spoken language. But it seems to me that there can't have been many cases similar to his. We've learned a lot from people who have received cochlear implants at different ages. Earlier implantation is strongly associated with functional spoken language use and fluent speech. There's a big benefit before age 5; a large proportion of those implanted before 24 months basically have normal language skills, while few after age 5 ever fully \"catch up.\" edit: Here's a study of prelingually deafened adult outcomes with CIs https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5720870/ All of those studied had acquired spoken language before implantation and had some degree of effective hearing earlier in life, so were not fully deafened before the language acquisition window. The implants provide an improvement of quality of life but do not allow most of even this population to e.g. understand spoken language on TV without subtitles. reply Retric 7 hours agorootparentListening to TV conversations is a high bar. In conversations people can ask for clarification or to slow down etc. Which explains: Before implantation, 7% of the patients were able to have telephone conversations. vs After implantation, 60% of patients are able to have telephone conversations. Also, the technology dramatically improved over time so we don’t have long term data on high quality implants. reply mlyle 6 hours agorootparentStill, pay attention to context. This was people who were successfully using hearing aids and oral language before implantation. Even in this subpopulation, they did not do nearly as well as children do, even though this subpopulation was less deafened than most deaf children. > Also, the technology dramatically improved over time so we don’t have long term data on high quality implants. We do have enough series to know that 5 year olds receiving treatment have (on average) significantly worse outcomes than 18-24mos. reply Retric 5 hours agorootparentFor the timescales we have tested there’s significant differences, and in terms of quality of life it’s clear early intervention is a significant benefit. However, slower adaptation isn’t zero adaptation. The limits for people implanted at 5 when they are 50 is still an open and IMO interesting question. reply mlyle 4 hours agorootparent> The limits for people implanted at 5 when they are 50 is still an open and IMO interesting question. Slower development usually means a lower plateau, and I think we pretty much have to assume as such (and can be prepared to be pleasantly surprised). Else, we get to wishful thinking: older people on older devices developed more slowly and plateaued at a lesser value of hearing. Now, we have implanted older people on newer devices, and they're developing more slowly, but hey, maybe they'll eventually develop fully normal hearing. reply Retric 3 hours agorootparentDo you have a source for that plateau? I’ve read that early success predicts future success on age adjusted tests. But the children were still improving in absolute terms over a decade post surgery. reply mlyle 2 hours agorootparentI was arguing mostly from the standpoint of delays in development are almost always correlated with lower ultimate attainment, no matter what measure you're looking at. But if you want cochlear implant specific data, here-- https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10760633/ You're right that time narrows the gap between early implantation and later implantation, but the slope of that narrowing is pretty small by the 20 year mark (and barely statistically significant in this moderately-sized study) and the gap is relatively big. The difference of time of implantation between the two groups was relatively small (mean implantation at 45 months vs. 34 months) and produces a gap that's durable for decades. >130 months is way, way, out from 45 months. reply nsxwolf 5 hours agorootparentprevNever fully catching up sounds a lot different than \"window for learning spoken language is permanently shut\". Am I missing something? reply mlyle 4 hours agorootparent\"Permanently shut\" for everyone is probably an exaggeration. A better description \"very, very few of those [with hearing restored] after age 5, who had never had any hearing before, develop anything close to normal spoken language skills.\" reply graphe 8 hours agorootparentprevI recall that they trained a part of an insect's head like the nasal passage and it was able to be used for language better than the model at that time chatGPT2. So there's something innate in nature that can learn human languages. reply SamBam 7 hours agorootparentCan you find that study? reply foobiekr 4 hours agorootparentprevAudio processing is a bit light sight processing - once you miss the critical development period, which is not directly related to the \"critical period\" for language learning, you will never actually develop them. People can develop something, for example people who had oxygen-destroyed corneas causing blindness who later got corneal transplants, but it will never be vision as you know it. reply Tor3 2 hours agorootparentSearch \"kitten vertical lines experiment\" on the Famous Search Engine. Kittens not allowed to see horizontal lines for the first few months would never be able to see horizontal lines, ever. It's at least somewhat like that with humans too. Ever wondered why some kids are wearing a patch over one eye? If the child needs glasses but (in particular) when they didn't get proper correction early on then they may have double vision, and what the brain does is to block one eye. That eye, despite \"seeing\", will lose the paths in the brain necessary for seeing well. The patch forces the brain to start using the eye again. This happened to me - the doctor told my father \"no need to check this regularly\", and after some years one of my eyes had indeed lost resolution. It's still like that. One eye can see very well, the other at much lower resolution. Though I found that even at middle age it was possible to improve that to some extent - not the actual resolution, but the brain's ability to actively use the eye could be improved a little. I would read books with only one eye. Could only read half a page at the beginning. But it's impossible to recover the vision I lost as a child, which was caused by the brain ignoring the eye. reply wildylion 1 hour agorootparentI have the same however my other eye is fine with regard to resolution - I just need glasses or a contact lens. But yes, the binocular vision is permanently shot (though I get some improvement at times). That's what you get for being an insufferably stubborn kid. reply branko_d 1 hour agorootparentprevI have the same situation. The annoying part is that my \"untrained\" eye is not near-sighted, but my \"trained\" eye is. I suspect it was different in the childhood (untrained: far-sighted, trained: not far-sighted) and then shifted in the direction of near-sightedness over time. reply hoseja 2 hours agorootparentprevThe ad-hoc language (generically called \"kitchensign\" apparently?) might be too primitive. reply blitz_skull 9 hours agoparentprevI find this incredibly hard to believe with all of the research on neuroplasticity. Not to mention there’s a VERY famous case that proves this is not a hard and fast rule: Helen Keller. reply phire 7 hours agorootparentWorth keeping in mind that Helen Keller didn't loose her hearing (and sight) until an illness at 19 months old. At this age, a child's brain has already locked in the sounds for their native language and lost the ability to learn non-native sounds (hell, research suggests that unborn infants can recognise the difference between their mothers native language and foreign languages before they even leave the womb). The typical child will have been using single word sentences for months and just starting to move onto two word sentences. Keller might have regressed to zero language abilities after her illness, but she didn't need to start completely from scratch when she learned how to speak. reply SnazzyJeff 5 hours agorootparent> Worth keeping in mind that Helen Keller didn't loose her hearing (and sight) until an illness at 19 months old. While this is in fact an important sample, this doesn't imply much about how humans develop after 19 months, much less how they develop before 19 months. > At this age, a child's brain has already locked in the sounds for their native language and lost the ability to learn non-native sounds (hell, research suggests that unborn infants can recognise the difference between their mothers native language and foreign languages before they even leave the womb). We have nearly zero clue how the child's brain recognizes their \"native language\". We know they react differently at different stages of their development to the same stimulus, which is occasionally linguistic. We have nearly zero clue what the mechanism is that corresponds input to measurable output. This is a very disingenuous characterization of the data. It's also worth mentioning that the root of this question is trivially false—people obviously learn language after the age of five. Such haphazard presentation (at best) should not be taken seriously. reply SnazzyJeff 30 minutes agorootparentprevif only this site could manage something more complicated than the dialectic of \"not retarded enough for y-combinator\" and \"too retarded for y-ycombinator\" one day, y-combinator will give a shit about disability. There is not enough money in the game for the powers to be to care yet. reply hmcq6 8 hours agorootparentprevI just can't imagine we have that much hard data on the topic. Unless there have been massive breakthroughs in hearing aid tech that I'm unaware of. Cochlear implants are amazing but my understanding is they're not 100% restorative. To make a bad metaphorical comparison with blindness, they're like glasses that restore your vision but if the only shape produced were shutter shades. (pic for reference: http://lh6.ggpht.com/nML2bdK30Z0OS3cHBINnLcXCv6XVI8dWpLvMu8m...) reply bradrn 9 hours agorootparentprevHelen Keller had already learnt a ‘home sign’ system, which was presumably language-like enough to allow her to learn English later. reply avarun 9 hours agorootparentAnd this kid knows sign language too. reply SnazzyJeff 5 hours agorootparentprevHellen Keller never developed the skill to listen to spoken language. I agree with you fwiw, but your argument needs to acknowledge the above statement. reply smeej 11 hours agoparentprevIs it the capacity to learn to speak a language, or the capacity to learn to understand spoken language that shuts? Or both? It's not quite clear from the way it's phrased here if maybe the inability to speak is only a consequence of the inability to understand, or if it's theoretically separable. Aren't there people who learn to speak, albeit with an accent, even though they have never been able to hear? So they might learn to read lips and speak, even though they wouldn't be able to understand a spoken language if they gained the ability to hear? I ask because I'm interested to know which parts of brain research might eventually try to prop that door open. Granted, most people born with this genetic condition would probably just be treated shortly after birth and learn spoken language during the normal time frame, not go through some special other treatment just to prop that mental door open, but I'd still be interested to understand what's actually going on in the brain better. reply makeitdouble 10 hours agorootparent> Aren't there people who learn to speak, albeit with an accent, even though they have never been able to hear? Yes, some people go through \"speech therapy\" and train to emit the right sounds while not hearing the output (but I think they rely on the inner vibrations ?). Understandably that requires a ton of training on top of existing skills and not everyone ends up with something workable. Part of the existing skills is the the ability to vocalize the sounds in the first place, and if a kid never intentionally vocalized for 11 years, I wonder if their vocal cords could ever develop to a point they can make the range of sounds needed. reply smeej 7 hours agorootparentIt does seem like the difficulty, then, must be with the comprehension of spoken language, then, not strictly the speaking of it. I had wondered about this for awhile, how when you see adults have their cochlear implants turned on for the first time, sometimes they respond as though they do understand what people are saying to them. I had wondered how they could possibly know how to interpret the sounds as specific words, even if they knew the words, but this makes it seem like that's not what's happening. They're probably still reading lips to understand the words themselves. reply bb88 10 hours agorootparentprevThere's a youtube video of Helen Keller who was both blind and deaf. She learned to speak in her adult life. I don't have the link handy but it's entitled: \"Helen Keller Speaks.\" If you look at the video it seems she even appeared to pick up the accent of her teacher. reply jimbob45 9 hours agorootparentWas she not profoundly deaf? reply bb88 9 hours agorootparentWell she was deaf deaf and blind blind. Not partially deaf or blind. The vocalizations and mouth movements could have been learned by touch. Tongue movements would have been harder to learn which explains why her vocalizations are hard to understand. reply SnazzyJeff 5 hours agoparentprev> But no matter how well the gene therapy works, the researchers recognize that Aissam may never be able to understand or speak a language, Dr. Germiller said. The brain has a narrow window for learning to speak beginning around ages 2 to 3, he explained. After age 5, the window for learning spoken language is permanently shut. This is trivially false. How are you acting like this person can be taken seriously? At best, they're wildly hyperbolic in their statements. At worst, they're funded to push a polemic. reply Tor3 1 hour agorootparentI too have to ask why this is \"trivially false\". We hardly have even anecdotal references to people who never heard language until after five (except for stories about children raised by wolves and couldn't learn to speak - not exactly stories we can trust). Of course that's goes the other way too - which studies are Dr. Germiller referencing? But again - if it was \"trivially false\" this would mean that it's something generally known because it's observable. And it isn't, as far as I'm aware. reply MathMonkeyMan 4 hours agorootparentprevHow is it trivially false? I know nothing about it. reply SnazzyJeff 26 minutes agorootparentWell, the part that was claimed. \"The brain has a narrow window for learning to speak beginning around ages 2 to 3, he explained. After age 5, the window for learning spoken language is permanently shut.\" The person seems to mistake the term \"speech\" for the phrase \"language comprehension\"—the field moved past that decades ago. reply judge2020 4 hours agorootparentprevMaybe the quote is about the critical period hypothesis[0], which is not universally accepted. 0: https://en.wikipedia.org/wiki/Critical_period_hypothesis reply fouc 9 hours agoparentprevI suspect this is a reference to the neural mapping around auditory signals, but given that the brain is still capable of changing at any point in life I disagree it's \"permanently shut\" reply TomK32 2 hours agoparentprevThe brain is a surprising organ, it's only been four month since the kid can hear anything and maybe the brain is powerful enough to use this first-time influx of new impulses as the same start when learning language as a baby. After all, the brain needs 25 years to fully form. reply lawlessone 11 hours agoparentprevEven still there's plenty of situations where this could save his life. reply ricardobeat 11 hours agoparentprev> the window for learning spoken language is permanently shut People still learn languages with completely different sounds when they are much older? Japanese, the african click-sound languages... is it some lower-level abstraction that goes missing? reply dghughes 8 hours agorootparentBut those are not the only language they know though it's just a different language. The parts of a person's brain where language and speech exist are already developed. It's a mix of several areas for comprehension, speech, flow of speech - it's quite complex and not a single spot in a person's brain. reply GauntletWizard 11 hours agorootparentprevI think the true answer is not impossibility, but significant, near insurmountable difficulty. The sound processing is not hooked up to cognition in the way it would be in a brain that had always had sensory input from the ears. Aissam would first need to learn to differentiate tones, voices, mouth-sounds, consonants vs vowels, etc. That's a lot to ask of a brain that had no understanding of that form of input at all. But all of this may turn out to be untrue! Our understanding of language acquisition comes from Feral Children[1], who had no language understanding at all, but could hear. Aissam has language skills, though developed late - The article mentions he started learning Spanish Sign Language at 8 years old. That's already a remarkable feat. This might overturn our views of language acquisition, which were mostly formed in the 1800's; Pedagogy has come a long way since then. [1] https://en.wikipedia.org/wiki/Language_acquisition#As_a_typi... reply lucubratory 11 hours agorootparentprevFor reference, the language you're thinking of is Xhosa reply mkl 5 hours agorootparentThey said \"languages\" for a reason. There are quite a few: https://en.wikipedia.org/wiki/Click_consonant#Languages_with... reply teaearlgraycold 11 hours agorootparentprevI think it’s the ability to understand any language through sound. Presumably other languages learned lean heavily on what you already know from other languages. reply spywaregorilla 11 hours agoparentprevHow are born deaf people able to learn to speak if this is the case? reply janeerie 11 hours agorootparentDeaf children can be taught to speak by very explicitly demonstrating tongue/throat position. It's a pretty arduous process and has fallen out of favor, so most deaf people who don't get a cochlear implant will use sign language only. However, with early implantation language acquisition is relatively easy (thought it varies per child). reply stank345 11 hours agorootparentprevThe sensory medium is separate from one's capacity to learn and use language. Sign languages have grammar, vocabulary, \"accents\" etc just like spoken languages. reply zamadatix 11 hours agorootparentI think they mean how can a person born deaf learn to make speech if the above quote says this individual will not be able to speak a language. I think the answer to that is it's more \"they won't be able to make speech like a person born with hearing would do by listening and naturally learning\" rather than \"they won't be able to try to make sounds with their voice they are not able to process auditorily\". reply spywaregorilla 10 hours agorootparentMoreso comprehend it. It seems impossible to suggest that a person could learn to vocalize language but not to understand those vocalizations. It may be impossible in spite of what it seems. reply heyoni 8 hours agoparentprevIs this one in deaf children or others with receptive language but no ability to speak? reply tedd4u 10 hours agoparentprevCame here to say I hope this can someday lead to a tinnitus cure :'| reply iopq 3 hours agorootparentFunnily enough, just listening to high pitched noises reduces my tinnitus: https://www.youtube.com/watch?v=xUZOSg3a1rk it turns out it's a \"phantom limb\" problem of hearing - when your high pitch hearing ability decreases, you start to have phantom sounds \"fill in the blanks\" at the frequency it got worse at you can test it yourself by generating sine waves and seeing when your hearing becomes worse https://www.szynalski.com/tone-generator/ mine drops off at 12.5KHz and goes almost completely silent above 16KHz the tinnitus frequency is about 13Khz! Trying to listen to quiet noises between 12Khz and 16Khz trained me to be more sensitive to those sounds and to generate less tinnitus reply magicalhippo 1 hour agorootparentAh interesting. I had noticed my tinnitus would get worse when I had been sitting in silence for a while, especially with over-ear headphones to further dampen ambient sounds (ie album ran out and I was so preoccupied with coding I didn't put another on). Fortunately mine is just at a mild annoyance level so far, but will try your trick. reply jv22222 10 hours agoparentprevYou probably know about all about meniere's. If not, have you tried going off salt? reply Etheryte 9 hours agorootparentHave you tried going off pseudoscience? reply holdsleeper 8 hours agorootparentWhat part of their comment was \"pseudoscience\"? reply JumpCrisscross 5 hours agorootparent> What part of their comment was \"pseudoscience\"? Is hypernatremia a cause of Meniere’s? reply MathMonkeyMan 4 hours agorootparentReducing salt might help with symptoms[1]. [1]: https://www.mountsinai.org/health-library/special-topic/m-ni... reply doublerabbit 11 hours agoparentprev> After age 5, the window for learning spoken language is permanently shut. It's fascinating how our brains are wired in such a way to enable read-only mode at an certain age in development. reply SnazzyJeff 5 hours agorootparent> It's fascinating how our brains are wired in such a way to enable read-only mode at an certain age in development. You're responding to a quote that is trivially false with a quick google. Ok. reply s1artibartfast 11 hours agorootparentprevI think it is more like telling the marketing team that you can't add an HDMI port to the computuer because it has already finished the production run. That is to say, it is as much of a hardware issue as a software issue. reply shermantanktop 11 hours agorootparentGet ready to repeat yourself, because the marketing team really wants that port. The CMO just said \"We should really add an HDMI port in a code patch because it would help OEM sales a lot.\" A sales engineer has agreed and is scheduling a brainstorm session. reply SnazzyJeff 5 hours agoparentprev> Gives me hope that one day my tinnitus may have a cure. We are all chained to reality. We must all accept reality or kill ourselves trying to. reply mgl 9 hours agoprevNot particularly connected but for anyone interested in analyzing their DNA: You may get your somehow accurate (not: medical grade accurate) raw DNA sequence from Ancestry DNA kit. Why from Ancestry and not other similar services like 23andme? Because they probably have the best accuracy for the money. You may then submit your DNA code to https://promethease.com/ that builds a personal DNA report based on connecting a file of DNA genotypes to the scientific findings cited in SNPedia. You may learn a few things about yourself and your kids, which may also include severe conditions which could unfold in the future. Sample report: https://files.snpedia.com/reports/promethease_data/promethea... Disclaimer: sharing your DNA is always risky reply jkingsman 8 hours agoparentWithout commenting on the risks vs. rewards of sharing your genetic material, services like Nebula Genomics have reasonably priced (USD$249 30x, USD$899 100x) sequencing that's extremely high quality and suitable for getting into learning bioinformatics, if you're willing to wait for a few months and that's your jam (i.e. the data is sufficiently deep and full coverage that you can get meaningful results from it as opposed to the limited view of SNP analysis like 23andme or ulta-low-depth sequencing). The last frontier for consumer/prosumer genomics is hifi sequencing for correctly getting at your hard-to-read areas that are full of long repeated runs. Dante Labs offers sequencing that targets this for about USD$1900, but it's an evolving area in terms of bang for your buck. reply derefr 8 hours agorootparentMaybe an odd question, but — given modern technology, if you were a bio lab tech and bioinformaticist yourself, would it be practical to just order some used equipment off eBay and build yourself a home DNA sequencing setup? And if so, would it then just be a matter of time and effort (rather than equipment and materials cost) to do a more thorough sequencing of your own DNA than any lab would ever be willing to do for you? reply new299 8 hours agorootparentI’ve done this: https://aseq.substack.com/p/bringing-up-an-old-ebay-miseq Your issues are that you will still need to purchase reagents from the sequencing instrument vendor. They will try and push you toward a service contract. Each kit will cost ~$600 (cheapest kit) an old Illumina sequencer which you can still buy reagents for will cost at least $5000. Doing a whole genome this way would be expensive… I’d guess $10K to $20K perhaps? You’d need a lot of kits… or one of the high spec sequencers (NextSeq 550 etc). Alternatively you could look at getting a nanopore sequencer. This will be cheap but the data quality is different (and may not be comparable/require high coverage for certain applications). I’d guess you could do a (30x) whole genome forSome Deaf parents, he added, celebrate when their newborn baby’s hearing test indicates that the baby is deaf too and so can be part of their community. Tough one. You want to respect the wishes of the parents, but you also want the kid to have the option to hear (and understand spoken language) when they are an adult and can make their own decisions. You may not be able to have both, given that this kind of deafness is progressive, and even with gene therapy you evidently need to treat it when young to give the child any hope of hearing. What if it turns out the kid wants to be able to hear, but by the time they are of age, it's too late and their inner ear’s hair cells are all dead? reply Modified3019 10 hours agoparentDenying their child an entire sense because the idea of the child having a full human experience makes the parent feel lonely, is straight up child abuse. Good parents want what’s best for their child, not themselves. reply IAmNotACellist 5 hours agorootparent>makes the parent feel lonely You misunderstood. It says some of them celebrate that their child will have the opportunity to be a part of their community and culture. From what they've experienced, they can see it'd be a profound shame if their child isn't able to participate in something they've had so much positive experience from. Though that's not universally true. Also a child growing up hearing with deaf parents will have a whole set of problems that they would find challenging to meet, and could fear not being able to help. On top of that, cochlear implants are not miracle devices, and as I understand it, deaf children who get it will still have significant hearing and speech issues and may end up isolated from both sides. reply Tor3 1 hour agorootparentBut many children grow up with deaf parents - one or sometimes both of them. And they learn sign language to perfection, as any child can learn languages to perfection. It's hard to see that the ability to also hear will make them not able to participate in anything. reply didntcheck 1 hour agorootparentYep. If the \"community\" is excluding them for being able to hear, that sounds like their problem reply kelnos 3 hours agorootparentprevRegardless of the deaf community potentially being a place of rich, positive culture, deafness is still a disadvantage and a burden in everyday life. I get that people need to accept things like deafness or blindness, and adopting a community and sharing the support that provides is a big part of that. But denying your child treatment that would allow them to have all their senses, because you want them to be a part of your community and culture, is a selfish act, full stop. If parents are expected to try to give their child the best life possible, a treatment to restore a deaf infant's hearing is a no-brainer. It's table stakes. I agree that denying a child that is abuse. reply richbell 6 hours agorootparentprevA popular belief in the Deaf community is that having hearing children is undesirable. Some people argue that things like cochlear implants are \"genocide.\" reply KingMob 3 hours agorootparentReminds me of the rabbis accusing Jewish people of marrying non-Jews as participating in a \"Silent Holocaust\". Way to drive people away, my dude. In some ways, it's a moot point, since iiuc, most deaf children are born to hearing adults, and not within the Deaf community. Genetic treatments will almost certainly be preferred by those parents, and the Deaf community will slowly age up and die out in a few generations for lack of replenishment. reply kelnos 3 hours agorootparentWell, a few generations after all types of congenital deafness are treatable. The gene therapy in the article so far only treats one, fairly uncommon, genetic cause. reply kelnos 3 hours agoparentprevMy first reaction to that line in the article is that it's incredibly selfish of a parent to feel this way, and then to potentially deny the kid treatment that could restore their kid's hearing. But I agree with a sibling poster that this is actually straight-up child abuse. I'm generally skeptical of people who invoke child protective services for all sorts of imagined things, but I think this qualifies. If a child is born deaf, that deafness is treatable, and the parents refuse to treat their child, that child should be removed from the care of those parents, treated, and placed with a family that doesn't put their own selfish needs over the health of their child. reply PlunderBunny 10 hours agoparentprevWouldn't a child that can hear (and therefore speak) born to deaf parents also learn sign language as a matter of course? I.e. they would be able to communicate with their parents in the parent's preferred medium either way. reply akoboldfrying 9 hours agorootparentYes, they could also learn to sign, just as children growing up in bilingual families learn both languages without issues. This puts to bed any question over whether the child can participate in the parents' community. I firmly agree with another commenter that any deliberate effort to restrict a child's sense experience is child abuse. I'll add that I think it's about the most selfish thing I can imagine, and that I put it in the same category as female genital mutilation. reply lacrimacida 7 hours agorootparentI dont think this is happening too often and I also have a hard time understanding how this could be put into practice unless the community is isolated and cultish. Hearing abled children will develop more or less their hearing naturally. If they’re not completely isolated they will learn to communicate with others even without their parents help. reply Loughla 11 hours agoparentprevDeaf culture (with a capital D) is a fascinating study in what it means to have a disability. The definition of disability is impairing one or more major life function. Capital D says that's not them. They just communicate differently. So. If they have that culture, is it bad for them to celebrate that they can share in it with their children? For reference, I think it's bad. But I can see the logic. reply themaninthedark 10 hours agorootparentI can't understand it. Someone who is deaf has a large number of obstacles to overcome and it is amazing that they are able to do so. Neuro-Atypical people could make the same argument, they just think and process things differently. But why wish that your children or anyone else has to overcome the same obstacles? reply rocqua 13 minutes agorootparentI believe people with down-syndrome have a similar community with similar worries. The worries are more about genetic screening shrinking their community than about children, but I feel like the sentiment of wanting to protect and share their community is the same. reply IAmNotACellist 6 hours agorootparentprevThey have obstacles to overcome, but those obstacles are mostly imposed on them by the wider culture, as I've come to understand it. Having spent 4 years learning ASL and being close friends with a lot of deaf people now, I'm starting to conclude that ASL and especially technology makes it clear it doesn't have to be a disability at all, at least for people who were born deaf. Losing a sense is still quite a disability. reply cortesoft 6 hours agorootparentprev> Neuro-Atypical people could make the same argument, they just think and process things differently. Many have. There is a major movement in the community to treat neurodivergence as something other than a disability. reply overstay8930 2 hours agorootparentI'm surprised it's taken so long for academia to catch up, militaries have figured out decades ago that these \"disabilities\" are just people who are good at different things and are assigned tasks that they can do much better than the average person. It is no coincidence that people with ADHD are drawn to certain fields, for example. reply rocqua 11 minutes agorootparentDo you have any such examples? Only thing I know of in a military context is colorblind people being better at defeating camouflage, and I believe that isn't actually used in practice. reply RoyalHenOil 2 hours agorootparentprevI guess the question for me is, if they have a hearing child, what's stopping that child from being a part of their parents' culture? If a Deaf couple had two children, one with hearing and one without, would the hearing child be excluded from the community and only the non-hearing child welcomed? Or is the worry that the hearing child will leave the Deaf community and move on to greener pastures once they grow up, while the non-hearing child will have no choice but to stay? Either way, it paints a pretty grim picture. reply kelnos 3 hours agorootparentprevI see the logic, but it feels rooted in a sort of state of extreme denial, based on a false starting premise. Deafness, in our world, is a disability. Not being able to hear does actually exclude you from a lot of things that us hearing folks take for granted for the most part. I think it's amazing that so many deaf people are able to function in the world as well as they can. They should be proud of what they've accomplished. But... man, no no no no. And it's not just communication, either. Like... deliberately denying a child the opportunity to hear birdsong, raindrops landing on a roof, the crashing of ocean waves, their cat purring and meowing at them. Hell, being able to listen to human-made music, more than just feeling the vibrations if it's loud enough and the speakers are on the floor. That's criminally abusive. If parents had a child with normal hearing, and deliberately damaged it to make the child deaf, we'd call that abuse. Why is refusing a treatment to restore hearing not at least in the same ballpark? reply oefnak 10 hours agorootparentprevYou don't need to be deaf to communicate with deaf people, right? You could learn sign language either way. reply actionfromafar 10 hours agorootparentWhat does it mean to be a bat? (Not just be able to talk to one.) reply throwuxiytayq 10 hours agorootparentAre deaf people a different species now? reply coffeemug 7 hours agorootparentprevThis argument is a classic case of \"Yeah, sure, I mean, if you spend all day shuffling words around, you can make anything sound bad, Morty.\" reply mbil 9 hours agorootparentprevA little off topic, but the 2019 movie Sound of Metal explores some of deaf culture and is a pretty excellent film for anybody interested. reply at_a_remove 8 hours agorootparentprevI'm curious to know if their stance and logic extends to taking government money, registering as disabled for any kind of benefit, and so forth. If it isn't a disability, don't take the money. reply graphe 7 hours agorootparentIf you can't get aid for it, I don't think it's classified a disability. What if they get rid of it later? I registered for adhd in my old school and I don't even mention it anymore since I didn't need the extra time for exams. reply throwuxiytayq 11 hours agoparentprevMoronic tribalism at its worst. This is why we can't have nice things. reply BurningFrog 6 hours agorootparentWe have nice things! reply collegeburner 9 hours agoparentprevno actually, we don't. not any more than we want to allow parents to circumsise their children, or feed them garbage until they have a BMI of 35 reply chewmieser 11 hours agoprevGift article link: https://www.nytimes.com/2024/01/23/health/deaf-gene-therapy.... reply DoingIsLearning 2 hours agoprevQuestion for the Bio people, from what I've seen gene therapies so far target very narrow scope sometimes rare diseases. Is there any 'broad spectrum' type gene therapy currently in the pipeline of any company out there? (not specific to audiology) And is the narrow scope because it is easy to control in these early days or is that we simple don't know enough to make more complex gene therapeutics without understanding collateral damage, side-effects, for example? reply kozinc 1 hour agoparentBeing a layman too, as I understand it, it's because the deafness is caused by a single (mutated) gene which makes it (comparatively) easy to change. In this case, because the altered gene causes a necessary protein to be faulty or missing, gene transfer therapy can introduce a normal copy of the gene to recover the function of the protein. For other diseases caused like this you're mostly right on the money - sometimes because while a single gene might be the culprit, the mechanism is unknown, sometimes mutations in some ‘single-gene disorders’ may not be in a single-gene at all, not to even mention other possible interactions or the risks inherent to gene therapy. reply CommanderData 13 hours agoprevIf true this is ground breaking news especially in the Audiology community. There is zero treatment besides hearing aids / cochlear implants for sensory hearing loss in Human history until now. reply starttoaster 10 hours agoparentThis is incredible news to me, as someone with horrible tinnitus, I'm choosing to take this to mean that some day we may even have a treatment. If I ever could hear the sound of absolute silence again, I think it would reduce me to tears for a while. Hearing a ringing sound everywhere you go isn't literal torture, but it's maybe just below it, and I've been living with it for the past decade thanks to poor choices in the military. I recognize that they solved a completely different issue, by the way. But the fact that it's possible to do this means to me that a different treatment for tinnitus may some day also be. reply lIIllIIllIIllII 5 hours agorootparentnote - maybe for others suffering with it given your background but idk - I've had low-level tinnitus for ages, at some point it suddenly got really bad (hearing it over street traffic), turns out my ears were totally filled with wax which muffled sound and destroyed my SNR, therefore tinnitus back to normal slight tinnitus once removed. very easy process. reply adventured 7 hours agorootparentprevWhat are the modern approaches used to try to lessen tinnitus? I've seen stories about therapy targeting white or brown style noise at it, adjusting frequency until it has an affect. And that over time it can, for some people, reduce the tinnitus. reply ijhuygft776 11 hours agoparentprevwhile this is great, couldn't something like Neuralink help too? reply ceejayoz 11 hours agorootparentNeuralink has a long way to go to demonstrate that capability. reply GauntletWizard 11 hours agorootparentprevRepairing our own bodies/preventing damage initially has significant advantages over something like Neuralink; Not fighting the bodies' own self repair mechanisms being a huge one. reply coderintherye 11 hours agoprevLooking up Otoferlin (the gene in question) led to this accessible and understandable paper: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5283607/ (2016) reply somethoughts 11 hours agoparentAlso a slide deck/presentation that seems related by Akouos - the original biotech company that Eli Lilly acquired. https://akouos.com/wp-content/uploads/2021/05/2021_0503_ASGC... reply aquafox 3 hours agoprev> His is an extremely rare form, caused by a mutation in a single gene, otoferlin No. Otoferlin is the protein encoded by the OTOF gene. I wish science journalists would be more science literal. reply CreRecombinase 2 hours agoparentNo. OTOF is the gene symbol. Referring to the gene as otoferlin is perfectly legitimate reply smallhands 1 hour agoprevlanguage he invented and had no schooling. Last year, after moving to Spain, his family took him to a hearing specialist, who made a surprising suggestion: Aissam might be eligible for a clinical trial using gene therapy. Exploitation. I wonder how much this doctor get paid for this ! reply bratwurst3000 9 hours agoprevIs this relevant to tinnitus? Sounds like it could help but I am no expert reply CommanderData 56 minutes agoparentNot directly, but breaking it down - they've successfully delivered viral code to inner hair cells. I have been following Tinnitus for quite a while now and it's assumed it's caused by some form of trauma to the Cochlear, either auditory nerve, IHC/OHC or SGN cells. Even with patient's with no record to prior trauma (potentially immune system dysfunction or compromised blood labyrinth barrier / BLB). Now if this is continues to show success we could start seeing more therapies targeting the Cochlear by way of gene therapy and it potentially helping people with Tinnitus by treating it's underlying cause. I.e. Regenerating lost cells in the Cochlear. reply ikesau 7 hours agoprev [–] Does this go both ways? can we use gene therapy to make 11-year old boys deaf? reply copperx 6 hours agoparent [–] That's the dream of many deaf parents. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "An 11-year-old boy named Aissam Dam has become the first person in the United States to receive gene therapy for congenital deafness.",
      "The gene therapy specifically targets a mutation in the otoferlin gene, which affects approximately 200,000 people globally.",
      "Aissam's treatment was successful, granting him the ability to hear for the first time.",
      "This study has the potential to pave the way for gene therapies for other forms of congenital deafness.",
      "Researchers presented their data on this groundbreaking trial on February 3rd.",
      "The therapy involves injecting new otoferlin genes into the delicate cochlea, which has proven to be a challenging process.",
      "While cochlear implants have been the standard intervention for otoferlin deafness, they do not offer the same quality of sound as gene therapy.",
      "The success of Aissam's treatment has allowed the study to expand to younger children.",
      "If proven effective and safe, gene therapies for other genetic causes of deafness may gain increased interest.",
      "However, finding suitable candidates for the therapy has been difficult due to skepticism within the Deaf community."
    ],
    "commentSummary": [
      "The discussions revolve around language learning, cochlear implants, vision, deafness, gene therapy, and tinnitus treatments.",
      "Participants debate topics such as the critical period for language acquisition, the effects of cochlear implants on language skills, and the ability to learn multiple languages.",
      "The discussions also cover issues such as the importance of accents in language learning, advancements in technology for cochlear implants and vision correction, and ethical concerns regarding gene therapy for deafness."
    ],
    "points": 294,
    "commentCount": 153,
    "retryCount": 0,
    "time": 1706030794
  },
  {
    "id": 39111539,
    "title": "FBI's Safe-Deposit Box Seizures Ruled Unconstitutional by Appeals Court",
    "originLink": "https://reason.com/2024/01/23/appeals-court-fbis-safe-deposit-box-seizures-violated-fourth-amendment/",
    "originBody": "Fourth Amendment Appeals Court: FBI's Safe-Deposit Box Seizures Violated Fourth Amendment Cases like this are exactly why the Fourth Amendment was adopted in the first place, wrote federal Judge Milan D. Smith Jr. Eric Boehm1.23.2024 5:15 PM Share on FacebookShare on TwitterShare on RedditShare by emailPrint friendly versionCopy page URL Media Contact & Reprint Requests ((Source: Search and Seizure of Box No. 8309 at U.S. Private Vaults v. USA (U.S. District Court, Central District of California)) The FBI violated the Fourth Amendment when its agents rifled through the contents of more than 700 safe-deposit boxes in the aftermath of a March 2021 raid, a panel of federal appeals court judges ruled unanimously on Tuesday. In doing so, the judges at the 9th Circuit Court of Appeals confirmed what innocent victims of the raid and their attorneys have been arguing for years: that the FBI overstepped the bounds of its warrant issued in the case and failed to follow proper protocol when federal agents cracked open safe-deposit boxes, ran the contents past drug-sniffing dogs, and tried to seize some of the money and other valuables found in the boxes. The 9th Circuit's ruling pivots on a detail of the case that Reason first highlighted more than a year ago: the existence of so-called \"supplemental instructions\" for the handling of the safe-deposit boxes seized at U.S. Private Vaults in Beverly Hills. The warrant authorizing the raid expressly forbade federal agents from engaging in a \"criminal search or seizure of the contents of the safety [sic] deposit boxes.\" Under typical FBI procedure, the boxes should have been taken into custody until they could be returned to their rightful owners. But those \"supplemental instructions\" drawn up by the special agent in charge of the operation told agents to be on the lookout for cash stored inside the safe-deposit boxes and to note \"anything which suggests the cash may be criminal proceeds.\" It is \"particularly troubling,\" wrote Judge Milan D. Smith Jr. in Tuesday's ruling, that the government was unable to provide any \"limiting principle to how far a hypothetical 'inventory search' conducted pursuant to customized instructions can go.\" Elsewhere in the ruling, Smith theorized that if a government agency were \"given the discretion to create customized inventory policies\" for \"each car it impounds and each person detained, the ensuing search stops looking like an 'inventory' meant to simply protect property and looks more like a criminal investigation of that particular car or person, i.e, more like a 'ruse.'\" \"If there remained any doubt whether the government conducted a 'criminal search or seizure,' that doubt is put to rest by the fact that the government has already used some of the information from inside the boxes to obtain additional warrants to further its investigations and begin new ones,\" Smith wrote. \"The Ninth Circuit today held that the FBI violated the Fourth Amendment rights of hundreds of people by breaking into their safe deposit boxes to try to forfeit everything worth taking,\" Robert Frommer, an attorney with the Institute for Justice, a libertarian legal nonprofit that represented some of the plaintiffs in the case, tells Reason. He said the case should bring renewed attention to a congressional proposal to reform federal forfeiture laws in order to \"stop federal cops from continuing to act like robbers.\" A spokesperson for the FBI declined to comment on the ruling and referred the matter to the U.S. Attorney's Office, which did not respond to Reason's request for comment. The FBI's ruse in the U.S. Private Vault's case seemed to unravel after the district court in August 2021 made public certain details of the raid's planning stages that the FBI had tried to keep hidden. Those documents, including depositions with agents involved in plotting the raid, revealed that the FBI had planned to use civil forfeiture proceedings against the contents of the safe-deposit boxes but did not provide that information to the magistrate judge who issued the warrant for the raid. (Full disclosure: Reason submitted an amicus brief in the case arguing that the depositions and other documents should be made public.) District Court Judge R. Gary Klausner later ruled that \"there can be no question that the government expected, or even hoped, to find criminal evidence during its inventory.\" However, Klausner upheld the FBI's conduct as being within the bounds of the Fourth Amendment because the improper conduct was not the sole reason why the FBI opened the safe-deposit boxes and searched through their contents. The Ninth Circuit said Tuesday that Klausner was wrong to reach that conclusion and remanded the case back to the district court. In the ruling, Smith wrote that the arrangement called to mind the various \"writs of assistance\" used by the British authorities to conduct nearly limitless searches of private property prior to the Revolutionary War. \"It was those very abuses of power, after all, that led to the adoption of the Fourth Amendment in the first place,\" Smith concluded. And it is cases like this one that should remind all Americans why the Fourth Amendment still matters today.",
    "commentLink": "https://news.ycombinator.com/item?id=39111539",
    "commentBody": "Appeals Court: FBI's Safe-Deposit Box Seizures Violated Fourth Amendment (reason.com)290 points by walterbell 10 hours agohidepastfavorite125 comments p1mrx 5 hours agoAs a thought experiment, I was wondering how to store something in a box/envelope, and know if someone else has looked at it. My conclusion was to search for \"holographic tamper\" on AliExpress, and carefully examine the photos to find models where each sticker has a number and a differently-aligned background pattern. Prices are around $5 for 250. Then you could use several stickers to seal whatever container, and take photos to record each sticker and its pattern. There are ways to exploit this system, but it probably helps against a non-sophisticated attacker. reply jamesrom 4 hours agoparentState of the art is a bag of beans: https://dys2p.com/en/2021-12-tamper-evident-protection.html reply cfield 5 hours agoparentprevHow about glitter nail polish? https://news.ycombinator.com/item?id=6986327 reply jacquesm 4 hours agorootparentThat's so funny, I once long ago worked on a project that suspended retro reflective beads in a solution to get unique patterns, this is a very simple variation on that theme and probably a more effective one. reply p1mrx 5 hours agorootparentprevYeah, that may be a better idea. I see that some brands of nail polish use coarser flecks that would be easier to compare visually. reply dpifke 3 hours agoparentprevThe past few years at DEF CON, there's been a \"village\" where you can practice (or watch others) defeating such schemes: https://info.defcon.org/village/?id=67 reply tastyfreeze 4 hours agoparentprevGo old school with a wax seal and a signet ring. reply theogravity 9 hours agoprevWhat are the chances the seized items are returned in its entirety to the owner? reply khazhoux 8 hours agoparentI can't find source now, but I'd read before that they didn't properly inventory the items, and so now some large amount of it can't be traced back to owners. reply cogman10 8 hours agorootparentIt's more complicated than that. The reason for the raid in the first place was because the FBI suspected (correctly) that the owners of this lockbox company knowingly facilitated money laundering. The company was setup to maintain privacy. By it's nature, who owned what was ambiguous. There was none or no identifying information. What's at issue is if the FBI could use the contents of these boxes to further prosecute previously unknown crimes. The lower court said yes, the upper court no. The initial taking was legal, it's how the property was handled after the fact that ran afowl the law. reply pclmulqdq 7 hours agorootparentThere's the whole civil asset forfeiture angle to this: they may have been ostensibly planning to have people defend their deposit box in court, only to get charged with a litany of crimes related to its contents. They didn't keep any of the articles separate, though, which makes me think this was just a legal robbery. reply kevin_thibedeau 6 hours agorootparentDefinitely a case of malicious non-compliance. The police love to ignore their oath when they face no repercussions. reply data_monkey 5 hours agorootparentI hate everything about our anarcho-tyranical state. reply anonym29 5 hours agorootparentA 'state' in the sense of a government is the antithesis of anarcho-anything. reply logicchains 5 hours agorootparentAnarcho-tyranny means a government that doesn't enforce any of the laws that protect people, only enforces laws that protect itself. So it's like the absence of a government in the sense that a government is supposed to be tasked with protecting people's life and property. reply anonym29 4 hours agorootparentWell that's objectively poor phrasing, then. 'Anarcho as a prefix refers to Anarchy, which refers to a society being in a state of not having authorities. You could argue that having authorities that abuse their authority to self-serving ends renders that authority illegitimate, but by the admission of Anarcho-tyranny's own proponents (as a theory), they still recognize the existence of these \"authorities\", and just contest the governance decisions made by those authorities. That's certainly a bad government, but to call it anarcho-anything is just downright confusing. reply GauntletWizard 4 hours agorootparent> Anarchy > a state of disorder due to absence or nonrecognition of authority or other controlling systems Emphasis mine. The Government failing to recognize it's own laws and founding principles absolutely qualifies as anarchy. Anarchy as a state(1) of disorder predates Anarchy as a form of government for a state(2). reply trinsic2 3 hours agorootparentIf you want a real understanding of a word, look to its etymology. Anarchy 1530s, \"absence of government,\" from French anarchie or directly from Medieval Latin anarchia, from Greek anarkhia \"lack of a leader, the state of people without a government\" (in Athens, used of the Year of Thirty Tyrants, 404 B.C., when there was no archon), abstract noun from anarkhos \"rulerless,\" from an- \"without\" (see an- (1)) + arkhos \"leader\" (see archon). From 1660s as \"confusion or absence of authority in general;\" by 1849 in reference to the social theory advocating \"order without power,\" with associations and co-operatives taking the place of direct government, as formulated in the 1830s by French political philosopher Pierre-Joseph Proudhon (1809-1865). reply ETH_start 5 hours agorootparentprevThe anarchic aspect comes from the void in accountability that comes with large bureaucracies like states. It is this dysfunctionionality that makes free market oriented societies that place greater limits on the power of the state generally more prosperous than societies with significant government intervention, despite the latter theoretically being better able to address a litany of collective action problems. reply roenxi 2 hours agorootparentI get what you're saying and agree. But even theoretically a corporation is much better at dealing with collective action problems. Setting up a new entity that looks a lot like a corporation is the standard approach to moving a needle. The government doesn't have the bandwidth to move on collective action problems that people don't already have figured out by some other means, voting isn't a great tool for doing more than the really basic stuff like keeping a military and police force functional. And even for things like the military, private sector institutions are better at building all the components. We have corporations (using a very wide definition that includes non-profit entities that are legally similar) that tackle every problem under the sun. You can't expect beat an entity that exists for the sole purpose of addressing a problem with a general purpose schizophrenic institution like a government. reply justinclift 4 hours agorootparentprevYeah, that sounds dodgy as all heck. So if someone had (say) $1000 in cash in there, they'd be told to pick out the bank notes or something. At which point it's just gambling as to whether any particular bank note is going to get you arrested or not due to it having a bad history. reply exhilaration 4 hours agorootparentRelated story: 4 or 5 years ago my brother got held up on the street in Baltimore in a daytime robbery. The cops drove by 30 seconds later, my brother pointed out the guy, and they caught him right away. My brother was told he could only get his money back if he could provide the serial numbers for the cash he had stolen. Of course he couldn't do that. reply chaps 44 minutes agorootparentHah. Yeah. Maybe 10y ago I was robbed. The investigator who took my case, a month later, asked me if I had anything to drink that night. I said, yeah, one beer about 7hr earlier. He said that because I drank that night that would close the investigation and that was that. reply boeingUH60 1 hour agorootparentprevI smell BS in this story.. reply treyfitty 47 minutes agorootparentBaltimore police is known to be corrupt, what’s BS about the police officers citing unreasonable demands for proof in an attempt to pocket the money? reply CogitoCogito 2 hours agorootparentprevWere there witnesses or video or some other proof independent of your brother’s claims? reply matheusmoreira 6 hours agorootparentprev> The reason for the raid in the first place was because the FBI suspected (correctly) that the owners of this lockbox company knowingly facilitated money laundering. Not a good enough reason to violate people's rights. reply beaeglebeached 6 hours agorootparentEvery single gas station near the US major border knowingly 100% facilitate money laundering, and knowingly engage in a business where there is no doubt they are both taking and generating the proceeds by selling their gas. Their gas pumps should be seized for investigation. reply adolph 7 hours agorootparentprev> By it's nature, who owned what was ambiguous. There was none or no identifying information. Why are you lying or making up stuff? Even the raiding party's memo doesn't claim that: The memo states that “[e]ach inventory [would] likely include the following”: the USPV box door with its lock, a form with emergency contact information, the physical deposit box, and the box’s contents. Opinion by Judge Milan D. Smith, Jr.: https://cdn.ca9.uscourts.gov/datastore/opinions/2024/01/23/2... reply k1t 5 hours agorootparentSaying \"By it's nature, who owned what was ambiguous. There was none or no identifying information.\" seems reasonable given what was reported. In an indictment against U.S. Private Vaults, Inc., the U.S. attorney for Los Angeles accused the company of marketing itself deliberately to attract criminals, saying it brazenly promoted itself as a place customers could store valuables with confidence that tax authorities would be hard-pressed to learn their identities or what was stored in their locked boxes. To access the facility, customers needed no identification; it took just an eye and hand scan to unlock the door. “We don’t even want to know your name,” it advertised, according to prosecutors. https://www.latimes.com/california/story/2021-04-02/fbi-beve... reply beaeglebeached 5 hours agorootparentIt was ambiguous to the owner. Not necessarily the FBI. My understanding was the identifying info may placed by the customer inside the box, with the agreement being the box would not be opened by the hosts except under defined exceptions. reply hanniabu 8 hours agorootparentprevOf course. Best way to rob a bank is to be an officer of the law. reply EasyMark 4 hours agoparentprevI highly doubt the fba didn't tag and bag the contents by box #. they didn't just throw them into a big pile in the middle of the floor reply bagels 4 hours agorootparentIs this a joke? Did you read about this case? Because that is basically what happened. Some people got parts of their things back. reply EasyMark 4 hours agorootparentI read the article. The article ruled they broke the terms of the warrant. How in the world do you think that the FBI wouldn't bag and tag stuff if they planned on using it in court at a later date? Any fresh out of school lawyer could get that tossed in a heart beat. The people will get back their stuff after the appeals. How could they do that without the fbi tracking what was in the SDBs? reply bagels 20 minutes agorootparentApologies for being flippant, I thought you were trolling. https://ij.org/press-release/innocent-security-deposit-box-r... reply MertsA 1 hour agorootparentprevNot this article, there's been plenty of reporting on this case as the seizure happened back in March 2021. He's not exaggerating, the FBI has \"lost\" the contents of some of the boxes when some owners fought back on the civil asset theft. https://youtu.be/O436x2zbRwI?si=dB7GFWxpguEOObr8&t=235 reply Geisterde 28 minutes agorootparentprevThis is assuming the fbi doesnt drop the case and keep the goods. reply sonotathrowaway 7 hours agoparentprevThere’s a better chance they destroy the items and destroy the records of ownership of cash out of spite. reply bobthepanda 9 hours agoprevSide note: are there still places that have safety deposit boxes? Nearly all the banks in my area no longer offer them. reply DaiPlusPlus 8 hours agoparentBy \"bank\" do you mean one of those \"customer experience centers\" which offers the usual teller services and on-site card replacements, and has maybe 1 or 2 people working as tellers but none of them are loan-officers, and instead of a vault they have a convenience-store-style safe? \"Real bank\" branches do still exist - but they aren't as commonplace as they used to be. My nearest one for my credit-union is a good 45 minute drive away, ugh. reply bagels 51 minutes agoparentprevMy local BofA offers it, and then somehow needed to close the branch for a few years during the pandemic. Was really inconvenient to access the box in a locked, unstaffed branch. reply pclmulqdq 7 hours agoparentprevYour lawyer's bank vault is still okay. Beyond that, you're pretty much screwed. reply sampli 9 hours agoparentprevYes, tons of banks still have them reply evancox100 5 hours agorootparentNot for long, likely. reply ambarp2 2 hours agorootparentChase in San Francisco told me that one factor is a handful of deposit box users who come into their branch multiple times/day to access their box. That’s a huge burden on their staff’s time. reply j-bos 2 hours agorootparentIt's funny how cost cutting turns once normal services and responsibilities into a \"burden\". reply dannyw 38 minutes agorootparentSafety deposit boxes are not meant as active drop boxes. It's like someone using YouTube as a general purpose data storage service. reply EasyMark 4 hours agorootparentprevwhy is that? reply grubbs 7 hours agoparentprevWells Fargo still offers them. FWIW. reply pardoned_turkey 7 hours agorootparentThey had looong waiting lists in most places in the Bay Area, though. reply edm0nd 6 hours agoparentprevyour local credit union does and they are awesome and cheap. reply macintux 5 hours agorootparentMy local credit union is getting rid of them. I was kicked out last year, and they told me the branches that still have them probably won't for long. reply AlbertCory 7 hours agoparentprevMy local Wells Fargo does. reply ajross 9 hours agoparentprevnext [11 more] [flagged] avarun 9 hours agorootparentIt doesn’t take a lot of deliberation to realize the obvious reason, which is that this is a very clear 4th amendment violation which can then serve as suitable precedent for (or even have a chilling effect on) future instances where law enforcement may decide to abuse their powers. It’s quite obvious that not all cases on the same topic are equally easy to argue. reply AnthonyMouse 7 hours agorootparentThere's another obvious reason, which is that rich people have resources. When cops steal from ordinary people, they rarely have the resources to fight it even if they hadn't just been robbed. So then the case never makes it to an appeals court for the media to cover it. Or it does make it to court, because they're not actually poor, they're actually drug dealers with money to hire lawyers, but the courts are vaguely aware of this and then side with the police even if what the police did was entirely scandalous and unjust, because the victims are drug dealers. Which is how we get many terrible precedents. So it's a rare opportunity when the police target some Beverly Hills high society chaps because it makes the people who would rather burn the constitution than let a drug dealer keep their money turn around and ask what stops this from being used against someone like themselves. And when they realize the answer is nothing, that's when you can often get a good precedent set. reply nickff 9 hours agorootparentprevWhat do you mean by: >\"they decided to go to the mattresses\" Reason is covering the case because it's significant for Fourth Amendment precedent, they are not actually involved in the case as far as I can tell. reply colechristensen 9 hours agorootparent\"go to the mattresses\" is a reference to The Godfather book and film, means \"go to war\" or something like that. The fighters would sleep together in poorly appointed safehouses that would just have a bunch of mattresses on the floor. reply schlauerfox 9 hours agorootparentto go to the mat is way older than the godfather. reply AnimalMuppet 8 hours agorootparentBut \"go to the mat\" I believe comes from wrestling, not from safe houses. reply CPLX 9 hours agorootparentprevThat’s not at all a good description of what happened here. The safety deposit box company was pretty clearly a criminal enterprise, it marketed itself to drug dealers and so on. A few random rich people might have wandered in but it was a fairly unique company. The point here is that this shouldn’t fucking matter. The government should have to prove its case anyways by having individualized probable cause and a warrant for each and every safe deposit box, and going on a fishing expedition is patently unconstitutional. reply Eji1700 8 hours agorootparentI think being missed is more than that: > The warrant authorizing the raid expressly forbade federal agents from engaging in a \"criminal search or seizure of the contents of the safety [sic] deposit boxes.\" and then > Under typical FBI procedure, the boxes should have been taken into custody until they could be returned to their rightful owners. But those \"supplemental instructions\" drawn up by the special agent in charge of the operation told agents to be on the lookout for cash stored inside the safe-deposit boxes and to note \"anything which suggests the cash may be criminal proceeds.\" So no judge was involved in the decision to seize cash. It was an agent on the site who came up with supplemental instructions. There are a million reasons this should be illegal. reply marcus_holmes 9 hours agorootparentprevEspecially since they can apparently keep anything they find if they suspect it's the proceeds of crime, right? (they don't need a successful conviction to forfeit the goods, as I remember, just the suspicion). Which seems crazy to me as a foreigner. As TFA points out, it allows cops to behave as robbers legally. reply sandworm101 8 hours agorootparentYup. Which is why people look for places, like deposit boxes, where it is harder for them to grab stuff. reply mrinterweb 4 hours agoprevI feel this should set precedent for search of other protected property and information, including encrypted data. reply NoZebra120vClip 7 hours agoprevI tried to maintain a safe deposit box recently, and it was a comedy of errors. Well, it would've been comedy if it hadn't been my valuables at stake in there. They couldn't get the paperwork right. From the first time I signed up and was issued a box, the paperwork was screwed up in some royal fashion, I would thoroughly read it and point out that they messed it up (always unfavorable to me) and they'd fix it up apologetically. This is a bank. Bankers do paperwork for a living and their livelihood depends on dotting \"i\"s and crossing \"t\"s, so to speak! The worst experience was the second visit when I discovered that my keys no longer worked. Well, I'd brought only one key of course, and when we discovered that it had stopped working, they demanded for me to go back and bring in the twin key to test that one too. Of course it was jammed. I was absolutely appalled. The \"senior relationship manager\" said the boxes are old and tend to seize up like that. I demanded a letter in writing with their proposal to drill the lock and rekey it, for free, under my supervision. So I came in on the Drill Day and, you know how they have that little private Viewing Room where you put the box, open it, and inspect its contents? Well, the drilling contractor had put all his tools and toolbox inside that room, so it was full of valuable property already. I was sort of flabbergasted at the clear security breaches they were committing, constantly. This is a bank!!! So eventually I caught wind that safe deposit boxes are completely deprecated. The last straw is when I asked the banker what would happen in a power outage - could I retrieve my belongings? He said nah, the last power outage they had, the cameras stopped working so they closed and wouldn't allow any customers in. Well, shit, one purpose of having offsite storage was to stash some cash and emergency supplies in case the shit hits the fan, and they're telling me it'll be off-limits in case of the slightest outage or civil unrest. So I decided that an SDB is useless for my (or anyone's) purposes and I closed it out and said bye-bye. They never screw up the paperwork on my checking or savings accounts. reply neilv 5 hours agoparentI liked the start of the Jason Bourne series, Unfortunately, for a lot of emergency scenarios in which you'd want cash and supplies, you wouldn't be able to access your SDB. One use of SDB that I like is to get estate documents to heirs. My SDB includes a GnuCash one-pager printout of my financial accounts, usually current to within a few months. It also contains a one-pager that documents the remaining info that I think might affect my estate, like what health and renter's insurance policies I had. Even if a neighbor in my apartment building leaves their unlit gas stove burner on before leaving town for the weekend (yes, this happened), this time obliterating me and any documents at home, heirs will eventually get the SDB, hopefully with exactly the documents they need. reply crazygringo 5 hours agoparentprev> Well, the drilling contractor had put all his tools and toolbox inside that room, so it was full of valuable property already. I'm confused, what's the problem here? You were getting your lock drilled. What were you worried about happening? > Well, shit, one purpose of having offsite storage was to stash some cash and emergency supplies What made you think a security deposit box is the solution for that, in case of civil unrest? Banks aren't even open in evenings or weekends. The boxes are intended for long-term storage of things you don't need immediately or in an emergency. Emergency cash and supplies, why wouldn't you keep those at home? Surely you can find a good hiding place if you're concerned about people finding them? reply Spooky23 5 hours agoparentprevBanks aren’t particularly good at what they do with property. Thee was a story a few years ago about how Bank of America lost millions in cash to shrink in poorly run counting rooms and armored car operations. They hid it for years by moving cash around from branch to branch ahead of the auditors. reply roywiggins 3 hours agorootparentSafe Deposit Boxes Aren’t Safe (NYT, 2019) > There are an estimated 25 million safe deposit boxes in America, and they operate in a legal gray zone within the highly regulated banking industry. There are no federal laws governing the boxes; no rules require banks to compensate customers if their property is stolen or destroyed. > Every year, a few hundred customers report to the authorities that valuable items — art, memorabilia, diamonds, jewelry, rare coins, stacks of cash — have disappeared from their safe deposit boxes. Sometimes the fault lies with the customer. People remove items and then forget having done so. Others allow children or spouses access to their boxes, and don’t realize that they have been removing things. But even when a bank is clearly at fault, customers rarely recover more than a small fraction of what they’ve lost — if they recover anything at all. The combination of lax regulations and customers not paying attention to the fine print of their box-leasing agreements allows many banks to deflect responsibility when valuables are damaged or go missing... > Wells Fargo’s safe-deposit-box contract caps the bank’s liability at $500. Citigroup limits it to 500 times the box’s annual rent, while JPMorgan Chase has a $25,000 ceiling on its liability. Banks typically argue — and courts have in many cases agreed — that customers are bound by the bank’s most-current terms, even if they leased their box years or even decades earlier. https://www.nytimes.com/2019/07/19/business/safe-deposit-box... reply hammyhavoc 7 hours agoparentprevIf the shit hits the fan to the point that you need emergency supplies, I doubt cash money is going to do much for you. reply rightbyte 41 minutes agorootparentI mean, \"shit hits the fan\" can be a personal tragedy or local one. Like you have to leave your home in a rush to not be covered in lava, like on Iceland. reply graphe 6 hours agorootparentprevThe government doesn't have to fall for him to need both. A natural disaster or a bad breakup are two scenarios I can think of where cash money is useful. reply hammyhavoc 6 hours agorootparentAnd there's going to be a breakup during a storm? What makes you think their location would be natural disaster proof if it can't even handle a blackout? For what you spend on these hypothetical scenarios on an ongoing basis, you may as well just figure out a much better method—like keeping cash on you, in the house, in your vehicle. Like cameras, the best one is the one you have on you. \"Bugout bags\" probably way more sensible over this nonsense. reply jbverschoor 8 hours agoprevThe Bill of rights was created with a good reason and it shows. Many countries continue to abuse their power and are trying everything to change regulation in order to keep and increase their control and power over on their slaves, oh I mean citizens. Just a bunch of power hungry and usually corrupt people reply JumpCrisscross 8 hours agoparent> increase their control and power over on their slaves, oh I mean citizens Muddying these lines does a disservice to your argument. We aren’t slaves nor serfs. We are mostly disinterested. reply whatshisface 8 hours agorootparentHaving the government literally enforce slavery isn't in living memory but Americans learn about it in school. reply ssalka 7 hours agorootparentIt actually is in living memory: the 13th amendment expressly grants the government the right to use slavery as punishment for a crime. https://constitution.congress.gov/constitution/amendment-13/ reply EMIRELADERO 7 hours agorootparentWhile the point still stands, the 13th Amendment's exception for punishment doesn't ban prison labor as we know it today. When it was ratified, \"involuntary servitude\" was only applicable to private parties, not labor performed for the State. If the exception didn't exist, the 13th would have still allowed for prison labor. What the exception does is ban the now-extint practice of \"convict leasing\"[1] [1] https://en.wikipedia.org/wiki/Convict_leasing reply reactordev 7 hours agorootparentIt still happens… reply reactordev 7 hours agorootparentprevLook into private prisons and work camps, where they mostly work - it’s on farms. Just like the good old days. And master still manages to pay 27¢ an hour ($2.16 a day!). reply potsandpans 7 hours agorootparentprevwho's this we you're speaking about? reply diob 7 hours agorootparentprevI mean, we do have for profit prisons and use our immense prison population as slaves. reply janalsncm 7 hours agorootparentThat doesn’t make all of us slaves reply jbverschoor 5 hours agorootparentTaxation: income tax is a relatively new re-invention. Even more for other types of taxation like VAT or capital gains. I’m all for paying for services though. But this is basically stealing or claiming from a person’s fruits of labor. Passports/citizenship: Although most people will see a passport a sa travel document, you could also see it as a certificate of ownership. In many countries it is illegal to have multiple passports which supports this argument. Try resigning from the United States. It’s very difficult. Also, passports and border patrol have been used to prevent people from leaving a country in order to exploit their labor, and slavery: > During World War I, European governments introduced border passport requirements for security reasons, and to control the emigration of people with useful skills. These controls remained in place after the war, becoming a standard, though controversial, procedure Lastly, if you do not comply or pay taxes to your owner, you will be taken by force and sometimes with the result of injury of death. reply janalsncm 3 hours agorootparentI have heard people claiming that taxation is theft. I have never heard anyone claim that taxation makes us slaves. Income tax is even less coercive than previous forms taxation, where people were taken from whether or not they could afford it. With income tax, people who do not work do not pay it. In fact many people who do work do not pay income tax. reply jbverschoor 1 hour agorootparentThat’s exactly the difference between upper and lower class. You need money. If you work, you give half to the gov. If you earn “passive”, it still happens, but rates are a lot lower reply mjan22640 1 hour agorootparentprev> But this is basically stealing or claiming from a person’s fruits of labor. It is actually the opposite. It all belongs to the rulers, and they generously gift you a portion of it. reply gamblor956 5 hours agorootparentprevIncome taxes, transaction taxes, and customs duties are thousands of years old. It's always been how kings and governments raised the money to pay their armies. reply jbverschoor 5 hours agorootparentTaxes on certain goods, yes. Incometax, nope. It’s little over 100 years old. VAT I think about 75. And the list goes on. In the uk it started around 1800, a few years before ending slavery. Could it just be a substitute? reply nooron 2 hours agorootparentIn American chattel slavery, women were kidnapped, stuffed into boats where malnourished strangers died around them, beaten, forced to change their names and religion, and raped. The children of those rapes were routinely sold at auction, and their parents would never see them again. If the children were caught secretly learning to read, they would be beaten. Income taxes are annoying, but they are not slavery. reply jbverschoor 1 hour agorootparentOf course. That’s a whole different aspect and I wouldn’t want to compare to that part of it. reply JumpCrisscross 2 hours agorootparentprev> Incometax, nope Sharecropping taxes are income taxes. There weren’t formal income taxes because peasants didn’t have incomes. They paid a tax to their lord that was fixed as a function of the productivity of the land they farmed. reply Detrytus 1 hour agorootparentIncome tax isn't there to get government an income, it's a surveillance tool. You have to write down how much money you earned, and HOW did you earn it, and sometimes even what things did you spend it on. People today are brainwashed to believe that it's OK for the government to know that, but when income tax was first introduced I believe that was one of the most common arguments against it: invading one's privacy. reply diob 7 hours agorootparentprevI think given the existence of debtor prisons that becomes arguable. And some do utilize the laws and systems in our country to increase the amount of slaves though. This is likely just the tip of the iceberg: https://en.wikipedia.org/wiki/Kids_for_cash_scandal I'm not going to argue for the hyperbole of us all being slaves though. reply EasyMark 4 hours agoparentprevI figured they mostly just sat in a room and figured out how the British monarchy had screwed them over time and again and figured that was a pretty good list to start with reply blindriver 8 hours agoprevThis is so obvious it’s sickening. Everyone associated should be fired. What an affront against the constitution. reply JumpCrisscross 8 hours agoparent> so obvious it’s sickening It’s really not. The Third Party Doctrine has muddied the Fourth Amendment’s legal boundaries. The fact that a district judge originally ruled in the FBI’s favour should show that. > Everyone associated should be fired If they continue to think they did the right thing, sure. That’s incapacitation. But prioritising retribution isn’t helpful when restitution and deterrence are unfulfilled. reply indymike 8 hours agorootparent> The Third Party Doctrine has muddied the Fourth Amendment’s legal boundaries. No, the third party doctrine was a mistake. > But prioritising retribution isn’t helpful when restitution I'm not sure in this case, that is what is going on. The sequence of decisions at the FBI and DOJ were so terrifyingly bad that the carriers of the ideas leading to those decisions need to be removed from the institution. They are a cancer. reply bmulcahy 8 hours agorootparentprevOne way of understanding the previous comment in light of what you've said is: the third-party doctrine is so obviously wrong it's sickening. reply JumpCrisscross 8 hours agorootparent> the third-party doctrine is so obviously wrong it's sickening Perhaps I’m overindexing on the term “sickening,” but it seems unhelpful to bring an emotion like disgust into a technical legal discussion. Where one has (and doesn’t have) a reasonable expectation of privacy isn’t trivial. I think it’s obvious that a cop who recognizes a fugitive in public is acting reasonably while the same cop doing a facial-recognition search at a public school parking lot may not. Where does searching public Twitter photos lie? What if they’re only accessible with a login? If you can’t tell, I think the third-party doctrine as presently interpreted is wrong. But the history leading up to it is incredibly reasonable, recent and well documented. reply AnthonyMouse 6 hours agorootparent> I think it’s obvious that a cop who recognizes a fugitive in public is acting reasonably while the same cop doing a facial-recognition search at a public school parking lot may not. Where does searching public Twitter photos lie? What if they’re only accessible with a login? There are some pretty obvious and reasonable lines we could draw here. For example, is the thing available to the general population, or only to specific parties who haven't chosen to make it public? This doesn't necessarily answer your question, because you might e.g. have a reasonable expectation that data which is ephemerally available to the general population is not being recorded en masse and indexed into a central database, but it provides a boundary that would have eliminated a large swath of the trouble. reply JumpCrisscross 5 hours agorootparent> some pretty obvious and reasonable lines we could draw here I agree. The problem is we have, on one hand, the police absolutists, and on the other hand, people who want to express their outrage more than do anything real. Neither bothers educating themselves on the legal merits of the other side’s. Both turn the Third Party Doctrine into a totem. The Third Party Doctrine was created by Congress. Barring SCOTUS overturning half a century of law (again), that means the Congress must remake it. The lack of a popular alternative directly leads to the Doctrine’s persistence. We could draw these lines. But we don’t because we’re too busy expressing conniptions. reply AnthonyMouse 4 hours agorootparentThe main purpose of constitutional protections is to bind Congress. Regardless of whether Congress is inclined to fix it, they couldn't really fix it anyway (short of a constitutional amendment), because otherwise the next time there is a crisis the Fighting Evil with Evil Act gets passed and undoes it. reply zerocrates 4 hours agorootparentprevThe third-party doctrine was created by the courts, not Congress. US v. Miller and Smith v. Maryland are the typically-cited sources of it. reply beaeglebeached 4 hours agorootparentIs this the same Miller where the courts ruled in a case against an undefended dead guy that NFA, which now is amended to outlaw registry of new automatic weapons, was constitutional because it was just a revenue collection law and preserved right to military-type weapons? reply zerocrates 1 hour agorootparentNo, different Miller. This one is from 1976. reply indymike 8 hours agorootparentprev> Perhaps I’m overindexing on the term “sickening,” but it seems unhelpful to bring an emotion like disgust into a technical legal discussion. When true is redefined to false, it tends to raise emotions to a high level. Injustice often results in people's lives being disrupted and ruined where the ruin never should have happened. We should be disgusted by injustice. reply JumpCrisscross 8 hours agorootparent> We should be disgusted by injustice Sure, do that. This isn’t that. Being “disgusted” by the Third Party Doctrine, broadly, isn’t the same as being offended by injustice. reply indymike 6 hours agorootparentThe third party doctrine is disgustingly unjust. reply EasyMark 4 hours agorootparentprevWhy tho? Judges have used emotional words in decisions for centuries. It is disgusting and abusive of what should be both logical and common sense upon reading the most basic rights we have in the Bill of Rights. Emotion has a place in life as well as much as cold hard logic. reply lmm 6 hours agorootparentprev> If they continue to think they did the right thing, sure. That’s incapacitation. But prioritising retribution isn’t helpful when restitution and deterrence are unfulfilled. Firing would be a deterrent. Given qualified immunity, the consequences for this kind of abuse of power are small, and that's a factor in why so much of it happens. reply hamhock666 8 hours agoparentprevWe need someone in the executive branch willing to enforce the constitution for anything to happen reply mindslight 8 hours agoparentprev\"Fired\" ?! I think you mean thrown in prison for armed robbery and conspiracy to commit armed robbery. Being unconstitutional thus illegal, these actions could not have been carried out under governmental authority. Thus these actions should be prosecuted as anyone else's would be. I'm not necessarily a fan of creating such high stakes setups. But until these institutions are reformed around the concept of equity such that their victims are routinely and predictably reimbursed for the harms perpetrated by the institutions, we should fall back to insisting on criminal punishment for the individuals who enable them. reply SllX 8 hours agorootparentYeah, but wait till you hear about qualified immunity. reply mindslight 8 hours agorootparentI've certainly heard of it and it's one part of the cancer creating this state of societal breakdown. The concept needs to be soundly rejected. Any immunity shield should be narrowly scoped based on following written procedures. Acting within those procedures should mean that liability falls only on the institution. Acting outside those procedures should mean the institution and the agent are jointly/severally liable. One of the foundations of our society is that nobody is above the law, and government agencies need to stop being given this cheat code where they harm innocent victims and then just walk away from the situation unless it is especially egregious. reply SllX 8 hours agorootparentYou’re preaching to the choir but the issue is that a lot of people “reject” it, and it is still an active part of American jurisprudence. reply mindslight 7 hours agorootparentSure, the comment I was responding to (\"fired\") wasn't grounded in how things actually are either. The point is if we're talking about what ought to be, then let's aim for actual reform and actual justice rather than some token firings. reply mastry 9 hours agoprevAlmost three years to come to this conclusion? reply adrr 8 hours agoparentFor a business that had a public store front(1) and a yelp page. How many legitimate people had their property seized without due process. (1) https://beverlypress.com/wp-content/uploads/2022/03/US.Priva... reply AnimalMuppet 8 hours agoparentprevAlmost three years for an appeals court to come to this conclusion. That's (at least) two trials. (I mean, yes, it should be a no-brainer, but even people who are no-brainer wrong get their full chance in court.) reply pardoned_turkey 8 hours agorootparentIt's a bit more complicated than that (but no less reprehensible). The government obtained a warrant to seize the business and, as a part of that, to do a routine inventory of what's inside the boxes. After securing the warrant, the FBI said \"sike, we're gonna rifle through customers' stuff to see if we can uncover new crimes... and we're gonna keep the contents via civil forfeiture too.\" Several folks sued, and then the government dragged their feet for a while, but eventually said \"ok, we'll return your stuff, you have no legal standing anymore.\" The courts agreed. The only outstanding issue was whether the government should be ordered to destroy any records / copies of what they found in the deposit boxes they were not supposed to investigate in the first place. That's where the courts differed. The appeals court decision is a pretty scathing criticism of the whole fiasco, but strictly speaking, it only delivers a verdict on that last question (yes, the government has to destroy it). reply autoexec 8 hours agorootparent> The only outstanding issue was whether the government should be ordered to destroy any records / copies of what they found We really need to reign in theft under the guise of civil forfeiture. If clear abuses like this don't get that issue raised in courts I wonder what it's going to take. reply dragonwriter 2 hours agorootparentprev> Almost three years for an appeals court to come to this conclusion. That's (at least) two trials. No, it is one trial. Appeals courts do not (in general [0], and did not in this case) conduct trials. [0] There are cases where what is usually an appellate court, like the SCOTUS, has original jurisdiction over certain cases and conducts trials, but in those cases it is not an appellate court and there is no prior trial, and there are cases where specialized lower courts (small claims, traffic courts where they are separate and not a function of a court of general jurisdiction, etc.), can be appealed to a trial court of general jurisdiction, which will conduct a trial, either in the normal sense or on the record of the prior trial. reply fallingknife 7 hours agoprev [–] And nobody at the FBI who authorized this illegal seizure will be charged with a crime or even be fired. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "A federal appeals court has found that the FBI violated the Fourth Amendment by searching and seizing the contents of more than 700 safe-deposit boxes without following proper protocol.",
      "The court determined that the FBI exceeded the scope of its warrant by conducting a \"criminal search or seizure\" and using the obtained information to obtain additional warrants.",
      "The ruling underscores the necessity for reforming federal forfeiture laws and emphasizes the significance of the Fourth Amendment in safeguarding individuals' rights."
    ],
    "commentSummary": [
      "A recent court ruling concluded that the FBI's seizure of safe-deposit boxes violated the Fourth Amendment, which protects against unreasonable searches and seizures.",
      "The mishandling of the seized items by the FBI resulted in the loss of traceability and potential return to the owners.",
      "The raid on the lockbox company was initiated under suspicions of money laundering, but the court ruled that the contents of the boxes could not be used to prosecute unknown crimes. Concerns exist that the seizure was part of civil asset forfeiture with intentions to charge individuals with crimes related to their deposit boxes."
    ],
    "points": 290,
    "commentCount": 125,
    "retryCount": 0,
    "time": 1706052984
  },
  {
    "id": 39110434,
    "title": "Explore Waterway Connections with WaterwayMap.org",
    "originLink": "https://waterwaymap.org",
    "originBody": "WaterwayMap.org - OSM River Basins🏞 WaterwayMap How are waterway connected in OpenStreetMap.⏳ Loading map please wait...Data fromColours on the map are randomly assigned. 2 colours 3 colours 4 colours 5 colours 6 colours 7 colours 11 colours filterParamsChanged(len_filter) ); \"> Exclude based on length: Min. lengthkmMax. lengthkm This area: osm.org r.ok).catch(e => false); this.disabled = false; if (!res_ok) {show_josm_not_running = true;setTimeout(() => show_josm_not_running=false, 5000); }\" > edit in josm JOSM isn't running...r.ok).catch(e => false); this.disabled = false; if (!res_ok) {show_josm_not_running = true;setTimeout(() => show_josm_not_running=false, 5000); }\" > edit in josm (new layer) JOSM isn't running... edit in iDgeo: url (open on mobile) Map Data© OpenStreetMap contributors (open data licence). Loops in Waterways (mistakes) 📜 source amandasaurus/waterwaymap.org on github ?subject=waterwaymap.org%20contact\">📧 report problem Updates on Social Media: #WaterwayMapOrg from @amapanda@en.osm.town",
    "commentLink": "https://news.ycombinator.com/item?id=39110434",
    "commentBody": "Waterway Map (waterwaymap.org)285 points by wcedmisten 12 hours agohidepastfavorite78 comments s3krit 6 hours agoI've been waiting for the day that HN would have something tangentially related to the English and Welsh canal system so I could gush about it. For those that don't know, we have around 2500 miles of (mostly) navigable canals around the UK. They were built during the industrial revolution prior to the invention of trains such that we could transport coal and other goods relatively quickly and without damage (consider trying to transport pottery, for instance, on the perilous roads of the late 1700s. If the bumpy roads didn't destroy your wares, the highwaymen might). These days, obviously, all that has passed and instead we have a community of almost entirely leasure boaters - some choosing to own narrowboats [1] purely for pleasure, but a significant amount of us choose to live on them. Some based in marinas but quite a few of us (myself included) preferring a much more nomadic lifestyle - being obliged by law to move every 14 days at a maximum to new areas and consequentially experiencing the richness and beauty of what is, in my mind, a living museum - a testament to our past. Life aboard a narrowboat is cozy. You never have much space but I don't find myself lacking. It gets cold in winter, but most of us have multifuel stoves which we stuff with coal and (often foraged) wood. Right now I'm sat listening to the wind and rain lashing against the boat (it's almost 4am so I should really get to bed), the boat gently rocking from the weather. And I can honestly say that there is nothing half so much worth doing as simply messing about in boats. [1] - https://en.wikipedia.org/wiki/Narrowboat reply Doctor_Fegg 2 hours agoparentAnother narrowboater here - though leisure not liveaboard. We moor our 40-footer in Worcester, just off the Severn. I worked with the canals for many years, including a spell at British Waterways and then editing Waterways World magazine, and I still draw the maps for WW. I have a 75%-finished canal mapping app I really need to get round to releasing… reply lostlogin 1 hour agorootparentSome of us here would love to see it, or even just hear the concept. reply heads 5 hours agoparentprevIt’s always baffled me how the regard that the average Englishman holds for itinerant folk is wrapped in a big if statement: if (boat) … # All good else: raise TravellerAlert() reply Symbiote 40 minutes agorootparentNone of us have ever seen a marina or mooring place covered in litter and human waste, or been subjected to petty crime by the boaters. reply justneedaname 1 hour agorootparentprevNarrowboaters don't have the same reputation for a reason though. I've only had a handful of experiences with those who choose caravans as their method of itinerancy but they've all been negative. Everyone I know has had the same experience. It's disingenuous to suggest people hold differing views about them for no reason reply actionfromafar 1 hour agorootparentprevbool boat = social_standing || affluence; reply mock-possum 5 hours agorootparentprevDouble standards are practically a necessity to maintain bigotry. I’d be more surprised to find prejudice that lacks that kind of hypocrisy. reply JetSetWilly 21 minutes agorootparentThe narrowboaters thast stay near me don't seem to indulge in open air defecation everywhere around, they don't appear to litter and leave plastic and gas canisters in incredible quanties everywhere before buggering off, and they don't go door to door with dodgy schemes. The travellers that occasionally stay in the field next to my house do all of this and more. Pretending that this difference doesn't exist and that there's some \"double standard\" just means you have zero direct experience of the matter and just want to have some self-righteous \"UK bad\" moaning. Like it or lump it reputations are often deserved and caused by experience rather than by random prejudice. reply jampekka 1 hour agoparentprevLoved the canals when I lived in England (Leeds). I didn't narrowboat, but rode bike (including commute) on the canal sidewalks. Canals tend to have wildish nature around them (quite rare in England) and they are digged totally flat (with a very subtle incline/decline, apart from waterlocks). They have decent sidewalks that make them ideal to bike along (quite rare in England). They are very long so you can have a daytrip to visit multiple towns. And they don't have cars (quite rare in England). Canals are great! reply bdsa 7 minutes agorootparentThe \"sidewalks\" are towpaths - horses and/or people would walk along those to pull boats along. reply AlexMuir 4 hours agoparentprevFellow boater here - I live on a Dutch Barge. Also awake at 0530 with creaking lines in this storm. Lovely lifestyle. We registered our new baby’s address as the boat on a birth certificate last week and had no problems. Good luck to any future researcher geocoding that! I expected a postcode to be required but it wasn’t :) reply mattpallissard 2 hours agoparentprevI'm not a liveaboard but I live in an archipelago and have done my fair share of commercial fishing. IMO: When the weather is nice, there is no better way to travel than by boat. If the weather is poor, there is no worse way to travel than by boat. That said, while being stuck at anchor due to bad weather can be frustrating, there is something pleasant about laying next to the stove while you're being tossed about... Unless you're trying to sleep while you're listening to your anchor drag. reply fernly 5 hours agoparentprevThere are several narrowboaters who record their travels on YT, perhaps the best, certainly one of the first, is \"Travels by Narrowboat\", with 6 seasons on Prime[1] and the more recent years on YT[2]. [1] https://www.amazon.com/s?k=travels+by+narrowboat [2] https://www.youtube.com/@CountryHouseGent reply PetitPrince 1 hour agoparentprevSo, how does that work when you have to receive letters ? Do you have a PO box somewhere or a home port of some sort ? reply gavinhoward 5 hours agoparentprevI only found out about narrowboats recently from [1] (found at [2]), and they sound so nice! I've wanted a tiny house or a skoolie, and I think if I was in Britain, I'd go for a narrowboat just as easily. [1]: https://qmacro.org/blog/posts/2024/01/09/battlestation-2024/ [2]: https://lobste.rs/s/jrh1od/lobsters_battlestations_screensho... reply lostlogin 1 hour agorootparent> I only found out about narrowboats recently They make excellent walking and biking routes. They are flat, no cars, often have a pub, wildlife is frequently visible and they are usually quiet. Absolutely loves my time biking them in England. reply mock-possum 5 hours agoparentprevIf you were to give take a broad guess at the cost of moving around like that, in terms of maintaining supplies and securing a place to stay when you’re not in the mood to move with the current, what would that come out to, say monthly? reply londons_explore 3 hours agorootparentThere is a massive time Vs money tradeoff with boats. You can do it for almost zero money (perhaps $100/month) if you put many hours per day into maintaining the boat yourself and making everything you need yourself. Or you can pay for good gear and get all maintenance done by a professional and your canal boat will be costing more like $3000/month - and when you do that, it becomes a rich persons hobby. reply femto 8 hours agoprevAnd its inverse: the watershed calculator: https://mghydro.com/watersheds/ Edit: and river runner: https://river-runner-global.samlearner.com/ reply macintux 10 hours agoprevWhen I was young, my grandfather talked about the possibility of taking his boat from his home in central Florida all the way to the coast. 40+ years later, this fills me with delight. He could have done it. reply croemer 8 hours agoparentDefinitely not all of these waterways are navigable. There are even underground ones, essentially pipes. reply spot13 8 hours agorootparentRight. Like trails in the woods, they shift and evolve. reply jschrf 6 hours agorootparentBingo. Trying to map the flow of water is like trying to map the flow of time. reply pavon 8 hours agoprevWho mapped these? In New Mexico we have a ton of dry arroyos that maybe have a couple inches of flowing water a few times a year. Zooming in at places I'm familiar with, a surprising number of them are mapped. I know of rez roads[1] nearby that aren't on OSM or any map I know of, but the ditch we used for cross country practice is. Wild! [1] Two-track dirt roads on the Navajo Reservation that are not regularly maintained, or officially numbered/named but which are actively used. reply Mr-Frog 7 hours agoparentIn the USA, this is likely sourced from the National Hydrography Dataset: https://www.usgs.gov/national-hydrography/national-hydrograp... reply RicoElectrico 8 hours agoparentprevYou can go to https://openstreetmap.org/ , zoom in and enable the map data layer. History is accessible from the object pages. reply NelsonMinar 7 hours agoparentprevSometimes waterway maps include calculated flowlines. These are algorithmically derived from digital elevation data and more accurately represent where water would flow were there water flowing. That's really important not just for New Mexico arroyos but for most of the surface of the earth; there are a lot more flowlines than perennial streams. I don't know the provenance of this data though. It's pretty spotty, I don't think someone just imported the NHD flowlines dataset or something. reply lmum 6 hours agoparentprevI added a set of arroyos in New Mexico using USGS quadrangle sheets, which originate from digital elevation models and ground surveys in the 1980s. They're tagged as \"intermittent streams\" in OSM. There's a lot more work to do outside the national forests and major cities. One thing I noticed is that the dry beds shape road and settlement patterns, beyond just being useful for navigation. NM is undermapped relative to other states. I've wondered if this has to do with the complex governance and land claims (which can make it difficult to do bulk data imports). reply yosito 5 hours agoparentprevI'm in Thailand, and zooming in to places I'm familiar with, a surprising number of rivers and waterfalls are completely missing. I suppose there are quite a few factors that affect data quality in different regions around the world. reply aqfamnzc 5 hours agoparentprevIt is from OpenStreetMap. (see header and footer.) You can fix any errors you see! reply nightbrawler 6 hours agoparentprevI'm in New Mexico too, there's some creeks I frequent that are considered \"navigable waterways\" and unless it's monsoon season, you barely get your ankles wet lol. reply devilbunny 5 hours agorootparentBy the legal standards that I understand to apply, “navigable waterway” is any waterway that is, or could be made, navigable. So it doesn’t take much. reply inasio 9 hours agoprevThe Yucatan peninsula in Mexico is very interesting here, completely empty of waterways which would make one think it's a dessert area per other similar areas of the map, yet it's a very lush tropical jungle. A ton of water yet all of it runs underground, cenotes [0] are (mostly) undergound sinkholes, amazing to swim in [0] https://en.wikipedia.org/wiki/Cenote reply ijustlovemath 7 hours agoparentFun fact about cenotes: most of them were formed from the geologic upset that followed the asteroid that wiped out the dinosaurs 65Mya reply southernplaces7 6 hours agorootparentExactly, and if you look at a hydrological map of cenote locations in the Yucatan Penninsula, a very obvious, concentrated majority of them align very closely with the ancient buried crater's curvature. It's fascinating, and especially when you consider that of the millions of tourists and locals who visit the cenotes annualy, very few realize what the scope and ferociously violent origins are of the lovely little \"scattered\" blue pools they play in.. reply jkubicek 10 hours agoprevThe \"Navigable by canoe\" filter seems to filter out everything, which is a bummer, because I've always wondered how far up into the Sierras I could drop a canoe and still be able to paddle back to the bay area. reply reaperducer 10 hours agoparentThe \"Navigable by canoe\" filter seems to filter out everything Selecting \"Rivers\" shows an awful lot of dry gulches and alluvial fans that haven't had water in them in half a century or more. reply Affric 9 hours agorootparentYep. Navigable by canoe leaves Australia with about 50 km of two rivers in Gippsland to be navigable. It shows the Hunter and the Nepean as a single system. Connectivity between ephemeral waterways which are joined by ephemeral lakes is poor. All in all I think it speaks to the ownership of the market by private interests and the lack of utility of water navigation for most people. reply dylan604 7 hours agorootparentprevIs there a \"drag canoe\" option? reply r0m4n0 5 hours agoparentprevAs a native Sacramentan and boater, you can definitely get from the Sierra to the bay. Would you take your boat out to get around dams? (Lake Natoma dam, Folsom’s dam, etc). If you wanted and could do that, you positively could do the ride. The American is fed by snow melt from the top of the sierra and plenty of water in the spring to ride it down. Once you reach the delta the current would help you less so you better have some good oars. Also be ready to navigate some serious rapids in some spots on the American. Nothing that a little kayaking class wouldn’t prepare you for. They used to have a few large riverboats that used to bring people from Sacramento to the bay. One called the Delta Queen and the other called the Delta King. Three presidents even rode on them including Herbert Hoover, Harry Truman, and Jimmy Carter. They permanently affixed the Delta King on the docks in Old Sacramento where you can still eat at a restaurant or stay in an onboard hotel. https://en.m.wikipedia.org/wiki/Delta_King All that to say, plenty of boats in and around the Central Valley. reply ghaff 9 hours agoparentprevThere are a ton of rivers and sections thereof that are easily navigable flatwater in MA which don't show up. reply bravefoot 7 hours agoparentprevIn the valley, we talk about the Cosumnes and Clavey rivers as \"undammed\" but they may still have flood control and irrigation equipment in them. Growing up near the Mokulumne, one day I'd like to go from Camanche to the sea. It's not that far but would still have at least one portage at Woodbridge reply jjav 7 hours agoparentprev> how far up into the Sierras I could drop a canoe and still be able to paddle back to the bay area I've thought of doing this, I wonder if there are any good resources on best paths to take. reply ghaff 10 hours agoparentprevYou have both hills and urban areas blocking about the whole east side of San Francisco Bay. Hard for me to imagine being able to get through that. And further north you've got the Central Valley. reply cowsandmilk 10 hours agorootparentI'm not sure how hills or urban areas block the ability to canoe... Waterways always are locally low and canoe trips on rivers will typically have hills on either side of you. And I've been on plenty of canoe trips that went through the downtown of a city. Most older cities are built in some place navigable by boat. reply callalex 6 hours agorootparentThere are many dams and gigantic underground pipes that carry the water through the area. It works fine for the water but not for kayakers. For example the Briones Reservoir is upstream of the San Pablo Reservoir connected by huge gravity-powered pipes under the mountains between them. They are in fact redoing them right now. reply ghaff 9 hours agorootparentprevThe parent was asking about getting down to the SF Bay from somewhere in Sierras. I'd have to study a map in detail but I doubt there are much in the way of east/west routes in that area. Also, while there are navigable waterways in cities they tend to be very limited. There's often one river in older cities that flows through the center somewhere. It really depends on the watersheds. You can get to Boston from pretty far north but you would have to cheat by going down the coast from the mouth of the Merrimack River which I think captures all the rivers in southern NH/northern MA. reply chad_oliver 9 hours agorootparentSan Francisco owes its growth as a city to the fact that the Bay provides a connection between the Sierras (and their goldfields) and the Pacific Ocean. Regarding Boston, the interesting thing is that it used to be connected to the Merrimack via the Middlesex Canal. My understanding is that this is silted up now (which you presumably already know) but it shows how many more connections we used to have. reply jkubicek 9 hours agorootparentprevThere are a significant number of canal systems to the east of the bay. I have no where boating is allowed, but the ones we frequently drive past are ~100 feet wide and extend at least out to Stockton and Sacramento. I have no clue if you can get past those cities, though. reply jjav 7 hours agorootparentprevEven cargo ships go all the way to Stockton and Sacramento. reply ghaff 6 hours agorootparentLooking at a map more closely I don't know how much is practical in a canoe, but the answer to up thread is probably however far you can up towards Lake Tahoe then down through Folsom Lake to (most of the way?) to Sacramento then Stockton (or just directly to the bay) and probably to the Bay from there. Not sure you can do the equivalent further south but it looks like you could do a lot of it that way--at least in theory. reply ioseph 10 hours agoparentprevIt showed one path in my area which is a portage trail for canoes connecting two waterways reply AlbertCory 9 hours agoprevThere's a group of people who regularly sail a great circle up the Atlantic, down the St. Lawrence & the Great Lakes, through the Chicago River & canals to the Illinois River to the Mississippi, and east along the Gulf Coast & around Florida. I can't remember anything more about it than that. Anyone? reply dnlbyl 9 hours agoparentI believe that is the Great Loop: https://oceanservice.noaa.gov/facts/great-loop.html reply pomian 6 hours agorootparentWhat is maximum mast height? There are loops in Europe like this, but you need a mast that you can take down, which means you are limited in boat size - for sailboats anyway. reply topkai22 5 hours agorootparent19.6’,limited by a bridge outside Chicago. Several routes require a maximum 15’ height and others are 17’. https://www.greatloop.org/great-loop-information/great-loop-... I’ve been utterly fascinated by this since I learned about it a few months ago. It seems like a very approachable but still serious adventure and achievement once done. reply AlbertCory 8 hours agorootparentprevSounds like a great trip (IF you like being on a boat). reply londons_explore 3 hours agoprevI was surprised at the rivers in the middle of the Sahara. But upon zooming in on Google satellite view, I do see dry riverbeds in those places, and even the occasional tree along the route of the 'river', so I guess occasionally there must be a storm and those rivers become wet and flow. reply swarnie 2 hours agoparentSimilarly i was surprised to find its marked a river in my village i didn't know existed. Turns out its the drainage ditch they dug to stop the new Barrett boxes from flooding. I think the data may need some work.... reply anonu 5 hours agoprevThis is cool - makes me realize how incredible the Intracoastal Waterway is [1]: 3000 miles of mostly protected waterways along the US Eastern Seaboard. Definitely on my bucket list to sail it down one day. https://en.wikipedia.org/wiki/Intracoastal_Waterway reply hasoleju 4 hours agoprevIf you zoom in you see a length in km next to each river. For small ones it looks like it is the total length of that waterway. For bigger rivers that I'm familiar with I was not able to make sense of that number. It is not the length, it's too big for that. Maybe it's the total length of that waterway and all connected waterways? It also does not change when I travel along that waterway. My example is the river Neckar close to Heidelberg in Germany. The number there is:2595963 km In reality the length of each river is measured from it's estuary. For bigger waterways in Germany you see a sign every kilometer with a number. The number is the distance in kilometers to the estuary of that waterway. reply Doctor_Fegg 2 hours agoparent> Maybe it's the total length of that waterway and all connected waterways? Exactly this. reply gry 5 hours agoprevMinnesota's has expansive rivers feeding the Mississippi[1] and it has one million acres designated to a canoe wilderness -- the Boundary Waters Canoe Area Wilderness -- bounded by the Canadian Quetico to the north[2]. They are navigable by canoe by design, yet they are flitting dots. It gives me pause; what is this site trying to convey? It's a fantastic effort, yet for the water we have, it doesn't match. [1] https://www.dnr.state.mn.us/watertrails/interactive_map/inde... [2] https://www.paddleplanner.com/tools/maps/bwcaqueticomap.aspx reply mcdonje 9 hours agoprevWeird how so many streams end at the Massachusetts state line. Not the dev's fault of course. I've noticed a similar difference of how rock formations are recorded at state line boundaries on USGS maps. Dealing with different datasets from different bureaucracies is an intractable problem. reply tppiotrowski 9 hours agoparentSee: USGS 3DEP Program reply mxfh 9 hours agoprevSeems great for debugging waterway topologies. Some things I noticed after a few minutes: - A lot of smaller rivers system and tributaries seem not to be connected all the way through as seeing smaller disconnected systems with shorter total lengths. - small rivers that and at the shore geometry of larger rivers but are not connected to the main centerline - some streams are disconnected by other waterbodies where they should not be, as there seems to be little consensus on how to connect waterways through lakes and other waterbodies, having an unnamed waterway along the centerline connect through the named lake seems to be a good compromise to not mess up rendering but helps with linking up topologies. https://help.openstreetmap.org/questions/52843/does-the-rive... reply throw0101d 8 hours agoprevBunch of rivers in/around Toronto (Canada), but we've also lost a few to history as well: * https://www.lostrivers.ca/disappearing.html * https://www.hiddenhydrology.org/toronto-lost-rivers/ * https://www.blogto.com/city/2014/02/5_lost_rivers_that_run_u... reply vvpan 8 hours agoprevTo those who are thinking about waterway travel I recommend the book \"The Unlikely Voyage of Jack De Crow: A Mirror Odyssey from North Wales to the Black Sea\". reply goatbrain 8 hours agoparentWould also recommend \"Travels by Narrowboat\" available on Amazon Prime. The chug-chug of the diesel engine as John moves from one canal to another (with some sort of horrific/terrific can-based recipe thrown in every now and then) is great background TV to have on for that sort of occasion. reply eudoraexplora 7 hours agoprevLooks like I know what I'll be doing this weekend. Bravo to whomever put this together! reply maxglute 4 hours agoprevIs there a legend somewhere? Or a way to delineate something like a small culvert versus large canal. reply fanf2 9 hours agoprevWeirdly, in Cambridge, the River Cam is not marked as a natural waterway (fair enough, it is canalized) but the entirely artificial drainage ditches around the college back gardens and Hobson’s Conduit are. reply gerdesj 8 hours agoparentThe data on Open Street Map (OSM) doesn't materialise mysteriously out of the ether! It is the result of people like you and I doing recces. For just one effort to grab data there is a great app called \"Street Complete\" (SC) - give it a bash. You know how the Cam is so why not tell us all? I live in Yeovil, Somerset and after a few sessions on SC, OSM has way more detail on my immediate surroundings than Google's cars will ever gather. They (Google int al) will probably \"steal\" my work eventually but then it is public knowledge so not stealing at all. SC gathers a lot of information. For example it wants to know about accessibility, which has to be a laudable goal. OSM doesn't flog adverts so it is rather more inclusive than anything that the FAAANAAAANGS might contemplate. It isn't driven by financial profit, so you get back what you put in. reply yawpitch 10 hours agoprevKinda neat to turn on the Navigable By Boat filter and zoom in on England to find where I live, and all the 2500 miles of actually connected and navigable English (and Welsh) canals and rivers I’ve been on over the last seven years. Or course there’s also a whole lot of “waterways” that sure as hell aren’t navigable by any boat that can support a human, but might have been at some point. reply s3krit 6 hours agoparentI went straight into these comments hoping to find another HN (potentially narrow) boater! What's interesting and somewhat amusing is that this map has determined that the section of the Soar that joins with the Trent just north of Loughborough is apparently unnavigable. This time of year I'd generally agree. reply bemusedthrow75 10 hours agoparentprevYep. The first one I went looking for was the Fleet, which is something of a ghost river at the end of its journey. reply tamimio 9 hours agoprev [–] Cool, brb selling my car and getting a boat! reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "WaterwayMap.org is a website that shows the interconnectedness of waterways in OpenStreetMap.",
      "Users have the ability to filter the map by length and make edits using JOSM or iD.",
      "The map data is obtained from OpenStreetMap contributors and users can report any issues they come across."
    ],
    "commentSummary": [
      "The discussion explores various subjects such as canals, waterways, and boating across different regions.",
      "Topics include the English and Welsh canal system, the lifestyle of narrowboat owners, and the accuracy of waterway mapping on OpenStreetMap.",
      "It also delves into the presence of cenotes in the Yucatan peninsula, the navigability of waterways in California and the Bay Area, and the Intracoastal Waterway along the US Eastern Seaboard."
    ],
    "points": 285,
    "commentCount": 78,
    "retryCount": 0,
    "time": 1706046666
  },
  {
    "id": 39106916,
    "title": "Study: Higher vehicle hoods in trucks and SUVs increase pedestrian deaths",
    "originLink": "https://arstechnica.com/cars/2024/01/higher-vehicle-hoods-significantly-increase-pedestrian-deaths-study-finds/",
    "originBody": "go low, go slow — Higher vehicle hoods significantly increase pedestrian deaths, study finds Single-vehicle, single-pedestrian crash data for 2016-2021 finds hoods a problem. Jonathan M. Gitlin - 1/23/2024, 3:55 PM Enlarge / It actually feels intimidating standing next to a vehicle like this GMC Yukon Denali when the hood is level with your shoulder. Jonathan Gitlin reader comments 401 It's hard to escape the fact that American trucks and SUVs have been on a steroid-infused diet for the last few years. The trend was all too apparent at the last auto show we went to—at Chicago in 2020, I felt physically threatened just standing next to some of the products on display by GMC and its competitors. Intuitively, the supersized hood heights on these pickups seem more dangerous to vulnerable road users, but now there's hard data to support that. It hasn't been a great few years to be a pedestrian in the United States. These most vulnerable road users started being killed by drivers more frequently in 2020, and while some states were able to reverse that trend, others went the other way, making 2022—the last year for which there is full data—the most deadly year on record for US pedestrians. The problem has multiple causes. For decades, urban planners have prioritized car traffic above everything else, and our built environment favors speeding vehicles at the cost of people trying to cross roads or cycle. But it's not all just the fault of those planners, as the vehicles we drive play a large role, too. Some of that is the switch from sedans to crossovers, SUVs, and pickup trucks. Data from the 1990s found that a pedestrian hit by a light truck was two to three times more likely to be killed, with another study finding that light trucks were twice as likely to injure a pedestrian than a car, especially at low speed. Now, a new study published in the journal Economics of Transportation has analyzed the National Highway Traffic Safety Administration's crash data from 2016–2021, looking at crashes involving one vehicle and one pedestrian. The author, Justin Tyndall at the University of Hawai'i, matched NHTSA's crash reporting sampling system data for those years to vehicle specifications where the vehicle's VIN was included in the CRSS data. Advertisement Tyndall's data set started with 13,783 single-vehicle, single-pedestrian crashes, then filtered out those instances where there was no VIN recorded, except if the report included make and model. He also removed entries that did not record other important variables, such as vehicle speed, leaving a sample size of 3,375 crashes. To make sure the smaller data set was still representative, Tyndall looked at the full data set as well as the final sample. He found \"that average crash characteristics are similar across the two samples, suggesting that the reduced sample is broadly representative of the original data set,\" although he notes that 6.7 percent of crashes in the large set resulted in a pedestrian death, while 9.1 percent of crashes in the smaller, final sample were fatal for the pedestrian. Pickups and SUVs are more dangerous to pedestrians There were 1,779 unique vehicles (as determined by make, model, and model year) in the data set. Pickups and full-size SUVs had significantly taller hoods than the average car, at 28 percent and 27 percent, respectively. Minivans weren't much better, at 24 percent taller than the hood on an average sedan. Even compact SUVs—also known as crossovers—were 19 percent taller. Pickups and full-size SUVs were also much heavier than the average vehicle: 55 percent for SUVs and 51 percent for pickup trucks. Tyndall also notes that while the data set only spans six years, over that time, \"the median front-end height increased by 5 percent,\" while weight increased slightly less (3 percent), and the chance that the vehicle was a light truck rather than a car went up by 11 percent. Of the 3,375 crashes, 308 saw the vehicle kill the pedestrian. When examined by vehicle type, vans proved to be the least dangerous to pedestrians, with a 6.6 percent chance of death. Cars were a bit worse—8.5 percent of pedestrians hit by a sedan or hatchback were killed. Compact SUVs were roughly the same as cars at 8.8 percent. Page: 1 2 Next → reader comments 401 Jonathan M. Gitlin Jonathan is the Automotive Editor at Ars Technica. He has a BSc and PhD in Pharmacology. In 2014 he decided to indulge his lifelong passion for the car by leaving the National Human Genome Research Institute and launching Ars Technica's automotive coverage. He lives in Washington, DC. Advertisement Promoted Comments Defenestrar Thank you for linking to an open paper. I wondered about the mechanism of impact with respect to vehicle height and the paper indeed got into this. Pedestrians may be particularly vulnerable to changing vehicle heights as the high front-ends of large vehicles mean the point of first contact in a collision with a pedestrian is more likely to be in the torso or head, rather than the legs. Vehicles with higher front-ends are also more likely to push the pedestrian under the vehicle rather than deflecting them onto the hood. Which followed by a summary of simulations done by other researchers with respect to injury. January 23, 2024 at 4:16 pm DovePig Since some people still don't seem to get that a tall, blunt front obviously results in very unfavourable crash kinematics for the struck pedestrian and worse and more numerous torso injuries, a little infographic from a similar IIHS study, fig. \"Comparative risk of pedestrian fatality by hood leading edge height and shape\": (colours edited to better show the point) January 23, 2024 at 7:44 pm Channel Ars Technica ← Previous story Next story → Related Stories Today on Ars",
    "commentLink": "https://news.ycombinator.com/item?id=39106916",
    "commentBody": "Higher vehicle hoods significantly increase pedestrian deaths, study finds (arstechnica.com)263 points by pseudolus 16 hours agohidepastfavorite273 comments tasty_freeze 14 hours agoThere is another angle here besides the immediate impact (pun intended) of getting hit by a big truck. I'm nearly 60, but I can remember what it was like learning to drive back in 1980. Before SUVs and huge pickup trucks became common, and before tinting windows to near opacity became a thing, it was possible to see road conditions three cars ahead -- you simply looked through the windows of the car or cars in front of you. I found it unnerving to drive behind a delivery truck because all I could see of the road was the back of the truck, so I'd change lanes so I could have more advanced notice of what was ahead of me. These days that is mostly over: windows are too opaque, and very often that is moot because the vehicle immediately before me is well above my Corolla's vantage point. reply Symbiote 13 hours agoparentIt's even more noticeable cycling. An adult on a bicycle has their head just above roof level of an ordinary car, which is excellent as it gives a view of cars coming the other way. I go through the area where all the really rich people live, and it's easy to see over their sports cars, luxury sedans etc. Continuing through the wannabe-rich area, my view is then blocked by SUVs and similar. (Whether this is important depends mostly on how well separated car and bicycle traffic is.) reply wharvle 14 hours agoparentprevWindows and front/back windshields are way smaller now, too. It's like we're all driving around looking out of tank-viewport-slits. (I'm sure there are exceptions in some models—I drove a buddy's Jeep-truck the other day and the unusually-good-by-modern-standards visibility nearly turned me into a Jeep guy on the spot—but that's the trend) reply saltcured 13 hours agorootparentCars in the 80s and 90s definitely had more of a \"greenhouse\" feel to them, with mostly transparent glass and small framing. Due to a mixture of standards, this has changed. The roof cap and pillars have thickened and curved for roll-over protection, and front pillars are also thicker to hold air bags that didn't exist in those older cars. The windows sills are higher up relative to the driver, to improve side-impact protection. There are other changes too, such as dark coatings around the edges of windshields and fat mounts behind the rear view mirror to hold sensors, cameras, etc. As a tall driver, I find these changes very frustrating. The rear view mirror is capable of hiding a crossing car at a 4-way stop, and I sometimes cannot even see overhead traffic lights when stopped in the first position at an intersection. reply crote 7 hours agorootparent> Due to a mixture of standards, this has changed. The roof cap and pillars have thickened and curved for roll-over protection, and front pillars are also thicker to hold air bags that didn't exist in those older cars. The windows sills are higher up relative to the driver, to improve side-impact protection. I've heard this argument before, but I'm simply not buying it. Cars like the Honda Civic or Volkswagen Golf have an excellent safety record, outclassing many giant SUVs while still having relatively small A-pillars and large windows on all sides. It is much more likely that it is simply a design choice, making them look \"tough\", \"manly\", and \"aggressive\" - just like that giant snowplow front which has become popular over the past few years. If anything, giant SUVs are less safe, because they have to follow the less-strict safety standards for small trucks rather than passenger cars. They are especially prone to rollovers due to their high center of gravity. People feel safer in an SUV, because they are surrounded by similarly-sized vehicles rather than being surrounded by cars twice your size, but that doesn't actually make them safer. reply Tanoc 4 hours agorootparentWell... There are a few regulations I know exactly that contributed to this. One was the roof crush rollover standard. I believe it was passed in 2007 and stated that any vehicle must be able to hold up it's entire weight upon it's roof in the even of rolling over and being upside down. The other was the front small overlap test, which hit the vehicle directly in line with one of the front wheels. This shunted the wheel and suspension directly back against the firewall, often completely collapsing the A-pillar. In response A-pillars were thickened and given a much more convex shape at the bottom. Aerodynamics also meant that the windshields are also raked further back. Beyond a specific angle an acute triangle begins to weaken, and the acute angles of many A-pillars and C/D-pillars are well beyond that weakening point, necessitating complex inner structures that one again make them thicker. Since 1996 the IIHS and EuroNCAP have also become more and more stringent with the rear passenger side impact intrusion test, designed to protect children in the event of a side-on collision. One of the unintended consequences of this test was the head restriction height for seats, where cars with higher shoulders (the bottom edge of the greenhouse) in the rear did better in preventing a child's head from escaping the vehicle than cars with even shoulders. This is why only very tall cars seem to have flat shoulders and why small vehicles such as the Mazda3 look like someone canted the entire car forward fifteen degrees. reply mturmon 13 hours agorootparentprevYes. As you may know, recent safety regulations prompted by rollover incidents have meant that the \"A\" and \"C\" pillars connecting the body to the roof have been made wider. [See the first 2 paragraphs of the \"Design\" section of: https://en.wikipedia.org/wiki/Pillar_(car) ] It seems like there's a second-order effect here on visibility that may not have been really appreciated. reply epage 11 hours agorootparentWith my previous car, I never remember having visibility issues. I then got a 2013. The pillars aie ierfect pedestian blockerr, especially when making turns. I don't see how me doing better in a roll over is better than me running someone over. reply gibbitz 10 hours agorootparentI've noticed this in the cars I've had over the past 15 years or so too. We hit a deer two years ago because it was moving perfectly in time with the side curtain airbag laden support column in my field of view. I barely saw it when I hit it even! I understand that rillover accidents have lead to these supports being thicker, but why was the solution not to lower the center of gravity on cars so they are less prone to rolling over? reply Tanoc 4 hours agorootparentBecause changing the center of gravity is an expensive engineering solution. Crossovers came about as a loophole to CAFE (Corporate Average Fuel Economy) regulations, as CUVs are considered \"light trucks\" and are thus tested differently than pedestrian passenger vehicles. Since CUVs had to adhere to that \"light truck\" designation before the EPA just gave up and let them do whatever, often times that meant they had to have SUV like features such as taller bodies, higher ride heights, and thicker unibodies to enable towing. Lifting an existing sedan platform and creating a subframe for the rear was cheaper than engineering an entirely new platform with a lower center of gravity, and thus that's what they've been doing since 1999. Even as sedans drop like flies leaving only CUVs, SUVs, and trucks, they still engineer them as if they were just adding ride height to a sedan. reply manwe150 5 hours agorootparentprevFWIW that phenomenon is a geometry thing, not just a fluke. If you are on a collision course with an obstacle at constant speed, than the angle remains constant (as the configuration preserves congruent triangles until impact) and the obstacle can easily remain invisible for a long time, until impact, hiding behind the pillars or even just hidden by the blind spot in your eye if you haven’t moved your head reply hwillis 13 hours agorootparentprevHmm. I'm not sure how true that is, but that might be a perception thing because hoods have gotten longer and windshields have gotten more slanted. Being farther from the front of the car/windshield makes your view angle smaller even if the height of the window hasn't changed. Surprisingly, sloped windows are more of a safety thing than a fuel efficiency or aerodynamics thing. If the windshield is too sloped, it acts like a wedge that throws air up above the car and causes a ton of turbulence. It's more important that the airflow stays stuck to the top surface of the car, and a steeper windshield helps with that. reply wharvle 13 hours agorootparentOK, the glass may or may not be smaller on the windshield and rear (I haven't measured, and yeah, a greater slant could be the mechanism, not less glass) but the viewable area is smaller. Side windows are 100% for-sure smaller, because all the structures around them have gotten much thicker. reply aidenn0 14 hours agoparentprevNon-trucks attempting a right-turn-on-red next to trucks are also more likely to hit a pedestrian, though if they are driving in a sane manner, it's unlikely to be fatal. I cannot see a pedestrian over the hood of a truck stopped just short of the crosswalk until my bumper is already well into the crosswalk. reply anonymouskimmer 13 hours agorootparentNot nearly as bad as hitting someone, but it's also annoying when a person is honking behind me and I'm thinking \"Dude, I can't turn right until the light turns green because I can't see oncoming traffic since the truck/SUV in the left lane pulled forward.\". reply Zancarius 12 hours agorootparentOh, I've been honked a few times in the past few months right after pulling up to a light just after it turned red and the cross traffic has a green light. It's like... \"you really expect me to pull out INTO traffic because you're so impatient?!\" The first time it happened the person kept hitting their horn until they saw the cross traffic. Amazing. reply mbostleman 11 hours agoparentprevI think that's more of a function of the relative difference between the sizes of your vehicle and the other vehicle, not the absolute size of the other vehicle. The relative size is a big factor in why mixing bikes with cars is so deadly. The other one, of course, is relative speed. Years ago I got my CDL and there was a multiple choice question to the effect of what is the safest speed to drive. Three of the answers were all relative to the speed limit and the correct answer was relative to other traffic: the safest speed is the speed of the cars around you, or zero relative speed. reply cyckl 9 hours agoparentprevThis is one of the reasons I daily drive a 1990 BMW 325i—the visibility is just unparalleled. reply okdood64 4 hours agorootparentEven with the increase visibility, I'm sure you'd be much safer in a 2020 BMW 3-series. Heck even a 2010. reply porkbeer 8 hours agorootparentprevSame with most 90s cars. My old honda is practically a greenhouse, and was ctitisized for its c pillar at the time. reply stefan_ 14 hours agoparentprevStreets haven't gotten wider since then either. Maybe less of a problem in the US where everything is planned around humongous fire trucks, but in the EU if you had a street that could easily accommodate two moving lanes with parking on either side, that has basically narrowed down to one moving lane. Also reminds me of all the regulation around car lights - you literally specified the beam pattern light intensity color and what not, but you couldn't be bothered to narrowly restrict the height off the ground? Great, now everyone in a sedan is being blinded by all sorts of SUVs and trucks. Utter failure. reply Jayakumark 13 hours agorootparentTo add to it, they should put a limit on Lumens on Headlight something like not to exceed 2000 lumens, Also its temperature should be 3000k range like old cars but new ones are like 8000k. yesterday night - saw a light that must be atleast 20000 lumens its like watching sun, cant see anything for a few seconds. reply _puk 13 hours agorootparentThey (The RAC[0]) are actually pushing for regulation here in the UK along those lines. There seems to be quite a push from the public to do something, so maybe it'll change one day.. \"According to the Royal Society for the Prevention of Accidents, between the ages of 15 and 65, the time it takes to recover from glare increases from one to nine seconds. At 60 miles an hour, that’s 250 yards in nine seconds. Baroness Hayter said: \"The Group’s first interaction with Ministers led them to say: No problem here, no evidence of deaths or serious injuries. Since then, the public have reached out to tell us they disagree, and that many are stopping driving at night, with eight out of 10 drivers surveyed wanting action to reduce glare. \"We know other countries share our concern, with drivers demanding action. The Government needs to heed our call for action and be on the side of road safety.\" 0: https://www.rac.co.uk/drive/news/motoring-news/drivers-deman... reply Zancarius 12 hours agorootparentprev> Great, now everyone in a sedan is being blinded by all sorts of SUVs and trucks. Utter failure. I drive a truck and live in a mountainous area. Trust me, it's not just sedans. Nearly everyone drives around with their brights on and it seems they no longer bother to turn them down when approaching traffic (automatic dimming features don't work well up here). Consequently, night time driving is dangerous. I don't know what their advertised beam pattern is, but it's like driving passed spotlights aimed at your eyes while on the road when you're on the outside lane. FWIW mine are halogens (2014). Plenty bright (and aim-able—for towing), but they're useless when you're temporarily blinded by super bright LEDs. reply ryandrake 8 hours agorootparentI rode with a work acquaintance at night once, and noticed he kept his high beams on the whole time, despite oncoming traffic. I eventually said something, like shouldn't you dim your lights when there's someone oncoming? He looked at me like I had antlers growing out of my forehead. \"Duh... The high beams are brighter and let me see better.\" That's the bottom line: Not even a thought about other drivers. And these people are everywhere. reply soco 5 minutes agorootparentMy 10 years old VW turns off the high beams when it detects incoming traffic, or in general stronger ambient lights (like, street lighting in a locality). But in the US there are no such requirements, I assume. reply devilbunny 10 hours agorootparentprev> automatic dimming features don't work well up here That's disappointing to hear, but a counterpoint: the first car I ever drove that had auto-dimming was a Ford Escape rental in Utah. I drove from SLC to Moab and back, plenty of night driving in mountains, and it performed far beyond my expectations. I really never had to adjust the lights manually. Which is to say, probably more to do with the specific implementation than the terrain. reply gibbitz 10 hours agorootparentprevYeah you shouldn't be able to see your car's shadow (cast by the car behinds headlights) on the road in front of you when your headlights are on... reply crote 6 hours agorootparentprev> Streets haven't gotten wider since then either. In proper urban design that's actually a feature. People don't care about the posted speed limit - they probably won't even notice the sign. Instead they drive at the speed which they feel is appropriate for the road. A lot of the suburban infrastructure in the US was developed around collision safety. This means wide roads and large setbacks, all in order to get rid of anything to collide with. However, this means that a regular residential street now feels extremely safe to drive: it's essentially a highway, so people will drive at highway speeds. On the other hand, plenty of suburban road infrastructure in Europe has been explicitly designed to feel unsafe. The roads are intentionally made narrow, visibility is poor, and you'll run into plenty of obstacles like chicanes. Although plenty of that is a leftover from the pre-car era, it's nowadays an intentional design decision for new roads even when there is plenty of space. It's physically impossible to drive on those streets at highway speeds, so people are forced to slow down. Anyone with even the vaguest sense of self-preservation will drive a lot slower with a 6-inch clearance to oncoming traffic than with a 6-feet clearance. The lower speeds in turn actually make the roads safer, and make it possible to share them with cyclists and other low-speed traffic. Fun fact: the narrowest residential street allowed in the US is wider than a highway lane in The Netherlands. reply dysfunction 12 hours agorootparentprevStreets may not have gotten wider in a given city/town since then, but there's been a lot of population growth and development in that time in the sunbelt where urban planning has favored very wide roads. So even if roads themselves aren't getting wider, people have been moving to places where roads are wide. reply twoWhlsGud 11 hours agorootparentThis. I was recently in Phoenix for a meeting and was struck by the state of the roads there. Lots of paint on the roads bike lines on 4 or 6 lane 45 mph streets where lots of folks were doing 55+. I class myself as someone who will bike a lot of places in Seattle but I didn't see a single road (i'm sure there are some somewhere there) that I'd be willing to ride in Phoenix. reply burnerburnson 13 hours agoparentprevI regularly pass people on the right and then cut them off getting back into the left lane just so that I don't have to be stuck behind someone I can't see around. The tinted front/rear windows really piss me off because they make my drive more dangerous for very little reason. I'd make them illegal if it were up to me. reply dcotter 12 hours agorootparentKnow what's already illegal? Passing on the right: > Laws that cover passing when crossing the centerline of the roadway is not required (where there are multiple lanes in the same direction), often say something like this: > The driver of a vehicle overtaking another vehicle proceeding in the same direction shall pass to the left thereof at a safe distance and shall not again drive to the right side of the roadway until safely clear of the overtaken vehicle. source: https://www.nolo.com/legal-encyclopedia/free-books/beat-tick... reply Zancarius 11 hours agorootparentDepends on the state. https://law.justia.com/codes/new-mexico/2011/chapter66/artic... Which is probably a good thing since drivers from out of state (ahem, Texas) tend to prefer to left lane, will go 10-15 under the posted speed limit, and promptly go 10-15 OVER the posted speed limit when you attempt to overtake them. reply stronglikedan 12 hours agorootparentprevIt's not illegal everywhere, and it's silly that it's illegal anywhere. reply dcotter 11 hours agorootparentNot silly. Here's why: 1. Passing on the right, i.e. passenger side, means being in the driver's blind spot longer. 2. On- and off-ramps are almost always on the right, as are police making traffic stops, ambulances, cars pulled over on the shoulder, sidewalks, pedestrians, cross streets, etc. 3. Slower traffic is supposed to be in the right lane (because of #2), so a car accelerating to pass the car in front of it, suddenly in a lane of slow traffic, is a safety hazard to the slower drivers, regardless of #2. I'm writing from the US, so for me left lane = driver's side. reply anonymouskimmer 12 hours agorootparentprevThe right lanes are generally explicitly intended to be slower lanes. Making them optionally faster lanes can be dangerously disruptive. As well as, when the rightmost lane is used to pass, difficult for new vehicles to merge from side roads. These laws are made so people know what to expect for safe driving. When we're all being chauffeured in autonomous vehicles it'll be safe to rethink them. reply porkbeer 8 hours agorootparentprevPassing on the right is legal in Most states, including mine. Goung slow in the left lane however, is a crime. reply uoaei 11 hours agorootparentprevThose jurisdictions typically have more strongly enforced \"slower traffic keep right\" laws also. Why the double standard? reply el_benhameen 13 hours agorootparentprevTinted front windshields are illegal in California, among other states, fwiw. The law just isn’t enforced, just like modded exhaust, and residential speed limits … and passing on the right. reply Stratoscope 12 hours agorootparentIt's perfectly legal in California to safely pass on the right on a multilane highway and in some other situations. What is illegal is using anything other than a normal traffic lane to do this, such as the shoulder. Of course this does not excuse GP's practice where \"I regularly pass people on the right and then cut them off getting back into the left lane...\" Unless I misunderstood GP's comment, that sounds like reckless driving. Source: CVC 21754. The driver of a vehicle may overtake and pass to the right of another vehicle only under the following conditions: (c) Upon any highway outside of a business or residence district with unobstructed pavement of sufficient width and clearly marked for two or more lines of moving traffic in the direction of travel. CVC 21755. (a) The driver of a vehicle may overtake and pass another vehicle upon the right only under conditions permitting that movement in safety. In no event shall that movement be made by driving off the paved or main-traveled portion of the roadway. [other subsections omitted here, see below for full text] https://law.justia.com/codes/california/2022/code-veh/divisi... https://law.justia.com/codes/california/2022/code-veh/divisi... reply uoaei 11 hours agorootparentYou were being pedantic up until here, so let's continue with the presumption of pedantry: what GP described is very far from reckless driving because they knew exactly what they were doing. You can cut people off safely, if dickishly, because of how the laws of physics work: cars can't suddenly teleport into where you will be, acceleration (especially at those speeds) is relatively slow for the vast majority of passenger vehicles. reply manwe150 5 hours agorootparentprevI hate this so much around me in Massachusetts. Makes it impossible to know what the driver is going to do at an intersection when I can’t seem them. Often they resort to rolling down their window to be able to politely gesture at me, which would not be necessary if I could see them in the first place as should be legally requiredreply BobaFloutist 13 hours agorootparentprev>I'd make them illegal if it were up to me. They mostly are (at least the front windshield is), it's just not really enforced. reply gardenhedge 12 hours agoparentprevJust stay well enough back from the car in front of you and you'll be fine.. reply jetrink 14 hours agoprevMy proposed regulation is to introduce a hazardous vehicle license for large trucks and SUVs. Everyone is automatically granted one alongside their normal drivers license, but if you're caught driving recklessly (weaving through traffic on the highway, aggressively tailgating, speeding through residential or urban areas), you lose it and you have to drive a normal-sized car. Perhaps there's a way to earn it back through paying a fine and taking a safe driving course. The classification of vehicles as hazardous should be based on factors known to increase pedestrian and vehicle collision fatalities to encourage safe designs. reply sschueller 13 hours agoparentIn Europe vehicles like the Humer electric can't even be driven with a regular license because of its weight (3500kg is the limit). It's basically a large truck and requires a much more involved license. reply Hamuko 13 hours agorootparentThe EV Hummer is probably impossible to get registered as anything other than a truck, but there's some trickery you can do with regular pick-up trucks. I found at least one F-150 on sale that is registered as a van and you can drive it with a regular passenger car (B) license instead of a light truck (C1) license. Shockingly, the van F-150 isn't even prohibitely expensive in tax: just 531€ per year. Although with 1.70€ to 1.90€ per litre of 95E10, you'll probably just deposit all of your savings at the pump instead. reply consp 11 hours agorootparentThe F150 lightnings registered as a VAN can carry two passengers and about 250kg before going over it's registered and allowable weight limit. As a civilian those fines are hefty where I live, doing it as a company is extremely expensive. It's a useless vehicle except maybe for towing. If you fill all seats with some hefty adults wou'll already cross the weight limit. reply BobaFloutist 13 hours agoparentprevJust require a class A/B license to drive anything above...I don't know, 4000lbs? Exclude batteries from the weight if that becomes a problem for electric cars. reply gnicholas 12 hours agorootparentAlthough I see why you'd want to exclude batteries from weight, I'm not sure it's totally justified. After all, F = MA regardless. A friend of mine was hit by an EV and suffered broken bones, despite being belted, and on the opposite side of the car that was hit. If it had been a lighter vehicle, her injuries wouldn't have been as bad. reply BobaFloutist 12 hours agorootparentMy main thinking was there are a lot of things weight can (somewhat) be a shorthand for: Visibility past/around the car, visibility from within the car, carbon emissions, difficulty to drive, amount of space taken to park, deadliness of collision due directly to weight, impact on roads, and deadliness of collision due to form factor. Of those, the deadliness of collision due to weight and impact on roads are the only ones that still hold up if it's an EV. I just want to avoid a situation in which someone gets a gas engine instead of an EV of the same form factor because they would need a special license to drive the EV, which just barely gets over the weight limit. But I would also be open to this law in any form. reply punkybr3wster 12 hours agorootparentprevIn the F150 Lightning the batteries even on a standard range weigh as much as a Volkswagen. The standard range batteries weigh something like 1800lbs. The ER model has about 25% more battery and the Platinum weight is like 7150lbs! So maybe around 2100lbs of that is battery? So you’re still over 5000lbs on that monster without the batteries. reply sokoloff 12 hours agorootparentI feel confident in wagering that GP didn't intend to allow driving an F150 with a regular license. reply anonymouskimmer 11 hours agorootparentprev> In the F150 Lightning the batteries even on a standard range weigh as much as a Volkswagen. Even a Smart Fortwo is minimally 1800 lbs curb weight. How old is this Volkswagen? :) reply jjav 10 hours agorootparentprev> Exclude batteries from the weight if that becomes a problem for electric cars. Mass is mass, whether in the form of battery or frame metal. While encouraging lighter cars would be a huge win, carving a massive exception to heavy EVs completely defeats the point. reply BobaFloutist 9 hours agorootparentMass is mass, but electric cars of the same size tend to be heavier than their ICE counterparts. If the only goal is to keep cars light, then it completely defeats the point. If the goal is to keep cars smaller, and is at all informed by emissions than it makes sense to have at least part of a carve out for batteries. reply porkbeer 8 hours agorootparentprevWhy exclude ecars? They are just as dangerous. reply TheCoelacanth 10 hours agoparentprevThey should just be outright banned. The high hood doesn't serve any practical purpose. reply mikrotikker 10 hours agorootparentHow do you even work on the engine in that thing? reply TrevorJ 12 hours agoparentprevThe reason some of these vehicles ARE so big IS the regulations. We could look at rolling back THOSE changes first. reply punkybr3wster 12 hours agorootparentThis is the one time I always pull out my “Thanks Obama!” without being ironic. reply actionfromafar 11 hours agorootparentYes, clearly the US needed less sedans and more F150s. reply punkybr3wster 8 hours agorootparentQuite the opposite. Getting rid of small truck classifications ballooned the size of suvs and pickup trucks which is partially why we are in the mess we are in. There just aren’t small trucks anymore because of the changes. Few people need the monster suvs and pickup trucks that are on the roads these days. reply mrguyorama 11 hours agorootparentprevConsumers just prefer larger vehicles here in the US, and those same preferences are also becoming the norm in other countries. You can't blame the modern Camry being gigantic on the Chicken Tax. You can't blame larger vehicles being associated with luxury on efficiency regulations. reply bitfilped 11 hours agorootparentI don't think that's accurate, vehicles are being made larger to get around emission limitations. I'd love a new small truck like what Toyota used to make and ford f150/ranger's used to be ~10-20 years ago, but such vehicles just don't really exist anymore. reply twoWhlsGud 11 hours agorootparentprevConsumers prefer a vehicle that looks like it'll keep them and their family alive on the streets as given - if some other persons kid ends up dead as a result of their choice - well \"whateves\". Make the streets less dangerous and they might prefer something cheaper and as a side bonus less deadly to their neighbors. reply adamredwoods 7 hours agoparentprevYes. I've always wondered if a larger vehicle hit a smaller vehicle, can they sue the larger vehicle for knowing that it could possibly inflict more damage to the smaller car? In other words, more care to avoid collisions? reply FooBarBizBazz 13 hours agoparentprevI like the spirit, but don't like the unintended consequence that it would create a stigma around sedans -- \"oh, those are the cars for the bad drivers\". We need to do something that makes SUV drivers feel ridiculous and humiliated. Or, failing that, just make it expensive: Weight and size dependent tolls, say (using the fourth power of weight, possibly, to match road wear equations?). Or just fewer lanes in which those vehicles are allowed. \"You don't get the elite lane.\" reply jjav 9 hours agorootparent> I like the spirit, but don't like the unintended consequence that it would create a stigma around sedans It doesn't need to be anything punitive, just make it that more training is required. You start off with a basic license that allows for something like a 1.0 liter engine car (I know, also need to change regulations to encourage the manufacturing of small cars). Then with more experience and more training you can level up to higher power and weight categories, if you want. The training needs to become more and more difficult (not paper DMV tests), that cuts out the people who want the 9000lb truck just for status since most people won't bother. If you need that 9000lb truck enough to go through the training, have at it. reply crote 6 hours agorootparentprevEasy: mandate a maximum size for parking spaces, and use a physical barrier to prevent double-parking. Have fun driving your oversized SUV when have to do parkour every time you get into your vehicle. reply HeyLaughingBoy 13 hours agorootparentprev> We need to do something that makes SUV drivers feel ridiculous and humiliated Oh, that couldn't possibly have any side effects. reply HeatrayEnjoyer 13 hours agorootparentHopefully it would have an effect. 95% of the people in these giant walls of steel have no reason to be driving them. reply tomjakubowski 11 hours agorootparentthey need protection from the other people also driving giant walls of steel for no other reason than... reply HeyLaughingBoy 12 hours agorootparentprevI see. And how have you determined that? reply dysfunction 12 hours agorootparentMaybe not 95%, but quite a lot of truck owners when surveyed essentially never use them for towing or hauling https://www.axios.com/2023/01/23/pickup-trucks-f150-size-wei... reply HeatrayEnjoyer 3 hours agorootparentprevMy eyeballs, for one. reply danaris 13 hours agorootparentprevIf we're going to try to \"punish\" SUV drivers for driving those vehicles, then I think we need to both recognize and do some things: 1) We need to recognize that there's a wide variety of vehicles in the SUV category. Many of them have replaced (not supplemented, replaced, because they sell better and car companies are hyperoptimizing their profits like everyone else) old standards like station wagons and minivans in manufacturer lineups. Most of them do not have the stupidly-high hoods that this article is actually about: those are primarily on pickup trucks. 2) We need to recognize that, at least for some SUVs, there are genuine, non-overcompensating use cases. Like driving on snowy, icy winter roads in the northern US and all of Canada. 3) Having recognized these things, we need to make sure there is provision in place for the people who have actual needs these vehicles are fulfilling—whether because they fall into the smaller category of people who would always need these things, or because they fall into the much larger category of people who would have bought a minivan or station wagon in the '80s and '90s, but most of those have gone away—before we start treating them all like the worst members of the category. Full disclosure: I drive a Subaru Outback. I drive it for three main reasons: it's extremely reliable, it has amazing cargo capacity (which I do use regularly), and its AWD is a godsend on the roads in upstate NY in the winter. (Is it possible to drive on these roads without it? Absolutely; I drove a Toyota Corolla for over a decade. But I am much less stressed with the AWD.) I just bought my second one, after shopping around extensively to find something that would fulfill my requirements, but get better gas mileage (which, to be fair, the Outback's is actually shockingly good for an SUV). The Outback is also basically the shape of a station wagon. It does not have an unhealthily high hood. I honestly don't know how its weight compares to other non-SUVs, but my understanding is that right now, the heaviest cars are electric cars, so using weight alone is also not a great metric. Ultimately, I think what people like you need to do is consider this question: Are you actually trying to solve a real problem with what cars are on the road? Or are you just looking for a socially-acceptable group of people to bully and be mean to? Because your proposals sound a lot like they're aimed at the latter, and very little like they'd be effective at the former. reply kjkjadksj 12 hours agorootparentSuv style vehicles are actually a lot more dangerous in the snow. Its so much more mass you are dealing with and damage when you lose grip entirely on ice under gravity power alone. On video clips of this sedans and such might kind of bump against a parked car and come to a stop while the big Suburban goes on to total a parked car with all the kinetic energy. If you want a snow tank, get a car that weighs like ~2500lbs, put on actual snow tires, and keep the transmission in high gear to engine brake. It also helps to learn to brake traction and skid with control in a snowy empty parking lot. reply jjav 9 hours agorootparentprev> We need to recognize that, at least for some SUVs, there are genuine, non-overcompensating use cases. For the things I think of as SUVs, I don't think there is any legitimate use case to be honest. Big SUVs often have less interior room than a sedan or station wagon so interior room is not a use case. They have way less cargo capacity than a pickup so that's not a use case either. They fit less people than a van, so fail again. They are top-heavy and not nimble, another fail. I don't understand the point of SUVs. I definitely would not support any kind of SUV ban simply because I don't believe in bans, to each their own, but just make a SUV-license require a lot more training which will discourage most soccer-moms/dads from wanting one. You say Subaru Outback though. I would have never called that a SUV. It's a wagon. I do see the Subaru website calls it a SUV, which I find weird. I guess the definition of SUV has broadened to mean just about anything. reply jghn 5 hours agorootparentA friend has a recent model outback. Compared the the outbacks of 20 years ago it seems huge. I didn’t believe him at first that it was an outback. reply bejk22 12 hours agorootparentprevI won't recognize (2) because it's false. Any current - and past - fwd will work just fine in Canadian snowy and icy conditions. Sure if you drive a shitty propulsion car you will get stuck everywhere but those cars are the exception. Edit: you acknowledged yourself that argument is mostly bs later in your comment... reply jamwil 11 hours agorootparentCalgarian here. AWD does help in the snow. reply tomjakubowski 11 hours agorootparentFWD comes standard on the most popular SUVs sold in the US and Canada. I don't know for sure, but it wouldn't surprise me if only a minority of SUVs here are AWD. reply twoWhlsGud 10 hours agorootparentprevDecent points, but it's good to remember that all cars have AWS (all wheel stop) - if you're not careful have AWD can make it easier to miss the transition to where AWS is no longer sufficient. In the NW the main attraction of AWD is not getting stuck in ski area parking lots and not having to put on chains when the roads prematurely force chaining up \"except for AWD\" because the state patrol knows almost no one has snow tires. (And I've never gotten stuck with snows on my FWD Jetta, but have had to put on chains all too many times before I really needed to - which is enough of a pain that I'll probably get AWD on my next car.) And trailheads around here also often are a lot easier to get to with a bit of extra ground clearance. So I may well give in and get an SUV for my next car. But the real driver (so to speak) will be that given the predominance of trucks on the road, not driving one yourself becomes a risk. (You can see the signal in the IIHS results.) So I'll be looking at how small I can go to meet my needs and minimize externalities, I suppose. All of that said, the more threatening the roads look to potential bikers and walkers the fewer we'll get of them. And whether it's happiness, health or efficiency you're trying optimize, more of those are pretty clearly a win. I've biked and walked American cities and country roads for over half a century, and while there have been many improvements over that period, the last five years have seen a series of what feel like steady reversals in the NW US. More and bigger trucks are part of a complex set of setbacks but feel like a key one to me. It doesn't take too many aggressive moves by monster trucks to make you wonder what you're doing out there. If we want further progress, we'll have to figure out how to mitigate this problem. reply crote 6 hours agorootparentprev> Like driving on snowy, icy winter roads in the northern US and all of Canada. No. Plenty of people get by with normal-sized cars under similar conditions in countries like Sweden, Norway, and Finland. Not to mention that SUVs are actually worse in such environments due to their almost-universal terrible performance in the Moose Test. reply FooBarBizBazz 11 hours agorootparentprev> Full disclosure: I drive a Subaru Outback. [...T]he Outback's [gas mileage] is actually shockingly good for an SUV). Funny, I call the Outback an AWD station wagon, not an SUV at all. I've got no problem with that; I'd be much happier if people bought those. Indeed, the very existence of Subarus seems to make most SUVs unnecessary. The only SUV Subaru makes (that I am aware of) is the Forester (and while that's a little larger than my ideal, it's not gigantic). > We need to recognize that there's a wide variety of vehicles in the SUV category. If I were Supreme Ruler, I would permit the Honda CRV (SUV), Toyota RAV4 (SUV), and Ford Maverick (truck) to exist, but no larger (ignoring commercial vehicles). Also station wagons, and minivans up to the size of the Honda Odyssey. (As I am not Supreme Ruler, I recognize that this has all the weight of a random opinion on the Internet.) > Are you actually trying to solve a real problem with what cars are on the road? Or are you just looking for a socially-acceptable group of people to bully and be mean to? Full disclosure: I walk everywhere, or else I take the bus -- and on the rare occasions that I rent or borrow a car, it's typically a small sedan. I react negatively to oversized vehicles (a) because I view them as a threat to my person, and (b) because I'm acutely aware of the arms-race dynamics here. People buy big SUVs because they \"feel safer\", i.e., in a crash, they are more likely to survive and kill the other driver, rather than the other way around. Recognizing the primal violence underlying this, I respond that the solution is more primal violence, to disincentivize this selfishness and arrest the arms race before it goes any further. None of which, I will add, applies to the Subaru Outback, which I'm totally cool with. reply twoWhlsGud 10 hours agorootparentNote that Subaru raised the Outback up higher some years ago specifically so it would qualify as a light truck some years ago. So while I agree that it's more station wagonny than most light trucks it actually legally qualifies as one. I remember because I actually wrote a cranky letter to Subaru US about it and they sent back a polite response saying \"hey, incentives!\" more or less. https://www.nytimes.com/2004/01/13/business/to-avoid-fuel-li... reply baggachipz 14 hours agoprevYeah but the drivers get to cosplay as Rugged Men™. The only thing that'll reverse the trend and stop this from getting worse will be government regulation. Then come the cries of \"Now they're taking our trucks!\" reply sickofparadox 14 hours agoparentIt's government regulations that made trucks get this big, most people simply buy what is available in the form factor they like. Trucks are the most popular form factor in the United States (with pickups a close second)[1], and the manufacturers keep making the cars bigger because of the poorly written CAFE laws[2]. I, and I suspect most Americans with pickups, would prefer to purchase one the size of a '90s F-150 as compared to the monsters of today but manufacturers can't or won't sell me one. Making up some macho strawman to attack actually obfuscates the problem and makes discussing solutions more difficult. [1] https://www.motortrend.com/features/car-types-models-body-st.... [2]https://www.thedrive.com/news/small-cars-are-getting-huge-ar... reply cameldrv 13 hours agorootparentYou can still get something like a 90s F-150, but it's now called a midsize truck. The main difference is that the bed will be smaller and the interior will be larger. There are even some smaller unibody trucks like the Maverick coming on the market. What you can't seem to get is something like a first generation Tacoma or 90s S-10 without 4WD that's low to the ground. All of the newer trucks have very high bed sides that make loading them from the side a huge pain even if you're tall. reply sickofparadox 13 hours agorootparentThey're a step in the right direction for certain, I have what would now be a mid-size pickup myself, but even the Ford Mavericks are pretty big compared to these 90s cars. reply arh68 9 hours agorootparentprevTrucks sure are popular, but gov't regulations didn't \"make\" them big. Given the popularity of F-250s / 2500s, which hardly meet any CAFE standard, the truckers don't yearn for the Ranger. They like 12mpg turbo diesel. We could repeal CAFE, get Ford to make them, but they still wouldn't sell. Tacoma loyalists keep the mid-size truck alive, but they're just a small fraction of 1500/2500 buyers. People like big trucks, and that's fine: but why blame it on the gov't ? reply njarboe 6 hours agorootparentWhy do people like big trucks? Marketing works. Since there has been a 25% import tax on trucks (and SUVs) since the 1960's and not on cars, domestic manufacturers have been trying to get more and more people to buy them instead of cars. Seems to have worked. reply closewith 14 hours agorootparentprevDoes anyone call an SUV a truck? Surely pickups are trucks. reply el_benhameen 13 hours agorootparentMy wife is from the Midwest, and on visits there I’ve regularly heard people refer to suburbans/explorers/etc. as “trucks”. Essentially the same platform, but with an enclosed rear with seats. reply skyyler 14 hours agorootparentprevAccording to chrysler, the PT Cruiser was a \"light truck\". I do think it's common for people to call large SUVs like the Ford Excursion \"trucks\". reply porkbeer 8 hours agorootparentThey were only classed as trucks because they could not pass emmissions otherwise. A great example of the truck laws perverse incentives to automakers. reply oh_sigh 12 hours agorootparentprevIf you see enough pickup trucks with bed caps you'll start to just view SUVs as those reply AlgorithmicTime 14 hours agorootparentprevSome SUVs qualify as trucks under the definitions written into the law. reply stcredzero 13 hours agorootparentprevIt's government regulations that made trucks get this big, most people simply buy what is available in the form factor they like. Lots of Americans want smaller trucks. I just saw a Netflix show where a main character prized his old Toyota pickup. This is also evidenced in the importation of Japanese \"K-car\" trucks. reply sickofparadox 13 hours agorootparent>Lots of Americans want smaller trucks I agree! I believe I said similar in my above comment. I'm looking into getting one myself, though I'm apprehensive about purchasing a car built in 99 without being able to test drive or inspect it. reply jewayne 14 hours agorootparentprev>Making up some macho strawman to attack actually obfuscates the problem and makes discussing solutions more difficult. Blaming the government for getting captured by industry also obfuscates the problem and makes discussing solutions more difficult. The current (grotesque) CAFE standards went in during the last months of the Bush administration, as a gift to the automakers. reply sickofparadox 14 hours agorootparentAt the end of the day, the government makes the laws - regardless of any influences upon it. I think we both want the laws to change, we just disagree about what changes are needed. I encourage you to call your representatives, as I have done. reply autoexec 12 hours agorootparentprevIt also looks like the solution to the problem isn't to get rid of the regulations, but to change them so that light trucks and giant cars no longer get a break on emissions/fuel economy standards. Revised regulations that incentive smaller cars would solve a lot of problems. reply ToucanLoucan 13 hours agorootparentprev> It's government regulations that made trucks get this big This is presented incredibly dishonestly in this context. It's well understood that automakers had their grubby little hands all over emissions regulations in the late Bush admin. They spent oodles of cash, tons of lobbyist time, and BEGGED for the exclusions for trucks that made complete sense at the time for TRUCKS, as in, pickup trucks used by laborers that needed the power and relaxed emissions standards that they asked for. And then, once that was done, set about changing 2/3 of their sales into trucks, so they could continue selling ever larger vehicles at ever higher prices with ever worse fuel economy. > most people simply buy what is available in the form factor they like. Trucks are the most popular form factor in the United States (with pickups a close second)[1] Which is directly traceable to substantial and aggressive marketing pushes by the American auto industry to shove trucks and SUV's down American's throats, because they could sell them for higher prices than the vans and station wagons that were already popular at the time. And even now, the solution proposed for all the issues these oversized stupid machines cause, is more sensors, more cameras, more safety features that, OH WOW, they get to charge more money for! No way! > and the manufacturers keep making the cars bigger because of the poorly written CAFE laws[2]. Again, the way this is framed posits that the CAFE standards were flawed output by the legislators themselves, and not the result of back and forth negotiations with the auto industry for decades prior. > I, and I suspect most Americans with pickups, would prefer to purchase one the size of a '90s F-150 as compared to the monsters of today but manufacturers can't or won't sell me one. Except now after decades of this shit, even if you can find a smaller vehicle, many consumers have (correctly) identified that their neighbors are driving suburban panzers, and not having one yourself puts you and yours at an elevated risk in a collision. Tons of people have reasonably sized vehicles in this country, and if you get t-boned by some jack-off in a lifted F-350 driving one, there is a not-insubstantial chance you're going to die, because those vehicles are not designed with safety in mind: they are designed to appeal to a marketing demographic that has been created: the modern man seeking to reclaim his masculinity because his accounting job doesn't let him imagine himself a hunter seeking the mammoth well enough, or whatever the fuck. > Making up some macho strawman to attack actually obfuscates the problem and makes discussing solutions more difficult. It isn't making up strawmen, it's pointing to a strawman manufactured by the auto industry that needs to be burned. The ONLY reason all these stupid machines are out driving today is because we as a society permit it. That can be changed. We are allowed to simply say that if you cannot demonstrate competence to handle a vehicle of this size, you do not get to drive one, end of discussion. It's not like we haven't had multiple classes of drivers licenses since basically the inception of drivers' licenses for this exact reason: because handling a 55 foot LTL truck is harder than handling a Honda Civic. This is a cultural issue as much as it is a political one. You can't just not take into account the long-term and well documented history of the auto industry and it's involvement here, any more than you can not take into account the documented history of the NRA/gun manufacturers with regard to our gun problem. reply B56b 12 hours agorootparentWith the ubiquity of SUVs at this point I can't imagine that many of them are being sold to men wanting to feel more \"Rugged\". I would think the vast majority are bought by those who want the additional space and safety of these cars, like families. It's going to be very difficult to convince consumers to drive cars smaller cars they see as less safe without some significant costs imposed on larger cars. reply twoWhlsGud 10 hours agorootparentprevKeith Bradsher's High and Mighty is a somewhat old but masterful book on the subject of how the SUV was put on the road to dominance: https://en.wikipedia.org/wiki/High_and_Mighty_(book) well worth reading if you're trying to understand how we got in this mess reply njarboe 14 hours agoparentprevTrucks have a 25% import tax in the US and cars don't since the 1960. US domestic vehicle manufacturers have spent many decades to convince people to buy trucks and SUVs because of this fact. Google \"chicken tax\". Large station wagons used to be quite popular in the 1960's reply kspacewalk2 13 hours agorootparentIt's not just because of this fact. It's also because trucks and SUVs are exempted from fuel efficiency standards that apply to sedans. So out sedans go, Ford doesn't even bother with them at all anymore. https://www.washingtonpost.com/business/2023/04/07/trucks-ou... reply njarboe 6 hours agorootparentThat is another reason. But the smallest trucks were put in the car efficiency standards. There used to be a Ford F100. Full size truck but lighter load capacity. RIP. Tax gas if you want people to burn less of it. reply olyjohn 14 hours agoparentprevPlease don't ignore that every other person also has moved from driving cars to crossovers that offer no actual advantages over a regular car. Most people want to sit high, and that's what sells crossovers. The hoods are higher in them too and we can't discount this and blame it all on big trucks. reply somerandomqaguy 13 hours agorootparent...what? If you've got back, hip, knee, or ankle problems, then trying to bend down into a low car is an undignified exercise at best, a struggle at worst. A CUV doesn't have that problem, you just sit into it like a chair. For a family with young children, securing child seats into a CUV involves a lot less bending over then in lower car. Bear in mind that many child seats are 40 lbs. If you live in a snowy area, the greater ground clearance makes it less likely to get stuck in residential roads that are lower priority for snow removal. You also have better approach angles as well, which can become a concern if you live in an area that's very hilly with steep driveways. You probably don't see advantages, and that's fine. But that doesn't mean they don't exist. reply olyjohn 13 hours agorootparentMinivan. You can get a minivan that has more room than a crossover, lower load height so that you hurt even less getting in, and you have more headroom to get car seats in and out. They're far superior in every way. But everybody quit buying them because of some \"uncool\" factor. It's stupid. The ground clearance on most crossovers is a joke. I have to point out how my Lotus Elise has 6 inches of ground clearance, and most CUVs have maybe an inch more than that. They have low-hanging diffs that hang up, and no recovery points for when you do get stuck. They're still the same car, barely jacked up. They look higher, but they aren't that much higher. Your one inch of extra clearance isn't saving you. The black plastic fenders don't make it rugged or capable. Most are just FWD, and the ones that are AWD just overheat and burn out the transfer case. I live in a rural area, in the mountains, with lots of steep hills. My long ass driveway doesn't get plowed, my street is extremely low priority for plowing, never had problems getting around with 2wd cars. You sound like my neighbors who keep saying that I need a big 4wd truck to live out here. People just keep making these excuses to buy bigger cars that sit higher, without actually evaluating if these vehicles actually have the things they're marketed to be able to do. reply twoWhlsGud 10 hours agorootparentI rented a Chrysler Pacifica Hybrid on my last business trip. No snow (SW) but a mix of suburban and rural roads and I was surprised at how much more fun it was to drive than the RAV4 I had on my last business trip (not to mention the giagantor 4Runner I rented over the winter break - to stop that thing you pass a motion, second it, send it to the executive branch and then hope it gets signed by the time you get to the stop sign at the bottom of the hill, but I digress). Modern minivans have apparently reached some odd pinnacle of car evolution right as they are going extinct. I imagine there's an alternate timeline where we have jetpacks and a general cure for cancer but only farm hands drive pickups... (And you definitely don't step down into anything to get into it-quite the opposite. The driver's chair even auto pulls back to let you in which is definitely geezer friendly : ) reply silisili 13 hours agorootparentprevFirst thing I thought of. I had to move from a sedans to crossover/small SUV when my knees couldn't take getting in and out anymore. I'm sure that's not the case for everyone, or even most people, but who am I to gatekeep. reply FooBarBizBazz 13 hours agorootparentprevMaybe this is the way. We should run ad campaigns emphasizing that large, high vehicles are for the physically infirm. Make them as sexy as walkers with toilet seats. Show images of doddering old people getting into them. reply Johnny555 14 hours agorootparentprevthat offer no actual advantages over a regular car. Most people want to sit high Isn't that the advantage? In a world where SUV's and trucks are more popular than sedans, sitting higher gives a sight advantage (and probably safety advantage in a side collision with one of those high cars). I drive a midsized sedan most of the time, but when I drive the RV (which has a higher seating position than many SUV's), I love the extra forward visibility, instead being at tailgate level, I can see through the back window of the truck/SUV in front of me. reply autoexec 11 hours agorootparentThat's just led to an arms race where nobody wins. Everybody wants to get higher and higher to get a less obstructed view. Restricting the height of cars would at least put a cap on the escalation while a push for smaller cars would make it less of a problem for everyone over time. reply olyjohn 13 hours agorootparentprevSeating height doesn't help you see over cars when everybody else is also in a crossover at the same height as you. reply sokoloff 12 hours agorootparentAs compared to a lower car, it absolutely helps. (I used to drive a lowered Alfa Romeo Spider. Sometimes I could see traffic better underneath the lifted trucks in front of me...) I can't control what cars other people buy and drive. I can control what I buy and drive. reply watwut 2 hours agorootparentWhich is why suggested regulation that would control what cars other people buy and drive. reply Johnny555 13 hours agorootparentprevBut it helps you see car brake lights ahead through the back window of the SUV in front of you reply carlosjobim 12 hours agorootparentprevYou can also look at things that are not cars, for example the road. reply closewith 14 hours agorootparentprevEase of entry alone, especially for the elderly or with young children, is an incredible advantage. What it sounds like is that crossovers don't offer advantages that appeal to you. reply olyjohn 13 hours agorootparentMy grandma has a minivan. It's way better than a crossover for ease of getting into. The floor height is nice and low so she doesn't have to step way up into it. The roof height is high so she doesn't have to scrunch in. It's easier to strap in car seats, it's easier to load your kids into. It's not a marketing joke like crossovers are. What it sounds like, is that you didn't actually shop very hard for the right vehicle that actually has those advantages. reply closewith 1 hour agorootparentMinivans, at least in my market, are more or less double the price of crossovers, so they're not like-for-like. You're very dismissive in your comments here. I wonder if some self reflection is in order here. reply infecto 14 hours agoparentprevWhile I do believe people often buy trucks for the looks and emotional feeling, I equally feel that your projection itself is just as dangerous as the idea behind it. reply stcredzero 13 hours agorootparentYeah, contemptuously painting a diverse group of people with a broad brush as pernicious -- History has something to say about this mental move. reply baggachipz 12 hours agorootparenthttps://en.wikipedia.org/wiki/Godwin's_law reply stcredzero 10 hours agorootparentYour comment is the thing which invokes the thing that triggers Godwin's law in this thread. It's entirely possible, and quite morally important to call out the particular mental move, nonetheless. In 2024, I find it's often now people who really, really want to make that mental move, who invoke Godwin's, so they can end the current conversation and find another conversation where they can. reply Hamuko 13 hours agorootparentprevI have a hard time believing that truck drivers are a diverse group of people when they're really only popular in a single region of the world. reply rhuru 12 hours agorootparentIn the top 10 most sold vehicles in USA 8-9 are trucks if I remember correctly. Ford F150 and Chevy Silvardo are typically number 1 and 2 consistently for decades. At that scale everything is likely to be diverse. For the soyboys drinking their vegan milk and driving their lime bike to pride parade, it might come as a surprise, but it in indeed is. ( Large vehicles are important part of American economy and mostly driven by blue collar workers doing their work. For example things like plumbing, construction site work etc. The problem is that this argument is demonstrably not true. Trucks vastly outnumber blue collar workers, and most blue collar workers would be better served by a Volkswagen Transporter or a Mercedes-Benz Sprinter. You're not seeing those in the US because they have an insane import tax. The traditional target for pickup trucks, farmers, almost universally hate modern trucks because they are simply way too tall to be practical - they'd rather have a 1990 model than a 2024 one. The vast majority of pickup trucks see zero offroad use, and practically never carry anything in their bed. They are essentially pavement princesses used to commute to an office job. reply steve_gh 2 hours agorootparentSo in my professional life I work optimising operations for large fleets of maintenance workers. The UK Zero Emission Vehicle Mandate (ZEVM) is setting annual targets for the sale of new EVs in the UK, including vans. The govt stick is large penalties against motor manufacturers for non compliance. Fleet operators are beginning to come under pressure because they can't buy new ICE vans as they are not being made, forcing a transition to EVs. This is an issue, because the traditional work van (Ford Transit or similar) has about a 150KwHr battery. So you can't do a full recharge on a 1 Phase 7Kw charger in less than 20hrs. And most of these vans have sat outside the drivers house at night so they can go straight yo the first job in the morning. At the moment the answer looks like data science (but hey I'm a data scientist). With better data on root caused of problems, and on what equipment is actually where, we are able to get much better at knowing what the right tools and parts are for each job. And that enables the vans to carry less stock, which in turn enables us to reduce the van size down to something more like a Berlingo, which has a smaller battery and can be charged overnight on a household EV charger. reply the_gastropod 11 hours agorootparentprev> Large vehicles are important part of American economy and mostly driven by blue collar workers doing their work Aaaaaaaabsolutely not! Trucks outnumber cars in every U.S. state. \"Blue Collar\" workers represent something like 16% of the U.S. workforce. The vast majority of truck drivers are not blue collar workers. Other countries have blue collar workers too. And trucks! Have you seen what they look like? Check this bad-boy out. This is peak performance when it comes to manly-man workin' trucks. [1] [1] https://c8.alamy.com/comp/2FA3BTR/a-small-blue-pickup-utilit... reply twoWhlsGud 10 hours agorootparentNot to forget the ubiquitous small delivery vans all over Europe that seem to work fine for craft folk. Not everything that's been turned into a culture war by those who are up for profiting needs be. reply KennyBlanken 14 hours agoparentprevEvery time I see someone behind the wheel of the absolute largest SUV a company offers, it's a small woman. Even as far back when SUVs first became a thing, I noticed that the smallest moms at school at the biggest SUVs. They love the \"might\" that a huge vehicle gives them. reply filoleg 14 hours agorootparentGod, this is pretty much my mother. She couldn’t care any less about “ruggedness” or that “tough guy” image. Her only argument ever is “safety.” I am not in it to change her mind. But whenever she brings this topic up and I show her the actual safety ratings for different vehicles, it’s almost as if her brain shuts down. She would say something along the lines “uh oh idk, maybe, who knows, it doesn’t feel as safe,” and the whole thing gets forgotten. Right until she decides to bring it up again from scratch at some point later, as if our previous conversations about it never happened. Lowkey, I think it would be an interesting idea to mandate displaying brightly colored safety ratings for every car on display at a dealership. No need to overcomplicate it by showing the entire stat sheet with a bajillion different numbers. Just one giant number for the overall rating, and about 4-5 subcategory numbers (e.g., driver safety score, passenger safety score, etc.). I think 1-2 of those metrics should be “the safety sub-category on which our car scored the worst.” My only worry about this is that the metrics themselves become the target goal, leading to either car manufacturers influencing safety rating boards or them maliciously complying by gaming the metrics just for those measured categories at the expense of everything else. Though the latter isn’t as much of a concern, given that it is pretty difficult to accomplish, and it would still have a large negative effect on the overall score. reply matsemann 13 hours agorootparentEvery year, loads of kids die because a family member runs over them by accident in the driveway, a parking lot etc. With these huge cars, you don't properly see around them, and 360 cameras and sensors will not make up for all of that. So yeah, \"safety\". reply filoleg 12 hours agorootparentA lot of those car manufacturers just don’t care, and it isn’t exclusive to large cars either (though the damage they can do is obviously much higher, making them more dangerous to pedestrians). I like sports cars, so I tried test driving a Camaro about 5 years ago. You would think that visibility on a rather small and fast 2-seater would be at least better than on an average SUV. It was singularly the worst car I’ve ever driven in terms of visibility, compared to even most SUVs. Not even joking, it feels like driving a military tank, but just faster and smaller, with the field of view being extremely reminiscent of seeing the road through a thin horizontal slit. It’s not like it got worse over time either, because I remember the 2013 version was at least just as awful. Way to ruin a fine car with that tank-slit visibility. This specific case didn’t even have anything inherently to do with the type or size of the car (unlike with some giant trucks), so it made me extremely mad. It was quite literally for nothing. reply axpy 51 minutes agorootparentA Camaro is not a small 2 seater by any means, it’s a rather big muscle car. If you want to experience what a small 2 seater feels like, try a Miata or a Cayman. Those 2 have outstanding visibility. Many sports car have bad visibility due to the mid engine blocking the rearview. reply shiroiuma 7 hours agorootparentprevI got to ride as passenger in someone's new Camaro at about that time, and I fully agree: the visibility was absolutely horrendous. I couldn't believe it. How is such a vehicle legal to drive? Does the US not have any kind of auto regulations governing visibility at all? reply sokoloff 12 hours agorootparentprev> whenever she brings this topic up and I show her the actual safety ratings for different vehicles, it’s almost as if her brain shuts down The NHTSA safety ratings come with the following notation: \"Note: A vehicle’s rating, or Overall Vehicle Score, can be compared with other vehicles of similar size and weight\" She may be, whether knowingly or not, interpreting the data offered more in-line with the guidance provided with the data. reply trgn 14 hours agorootparentprevAll true. _everybody_ drives huge SUVs. A few rugged men(TM) driving a large truck isn't the problem, they're couleur locale. It'd be adorable, like people wearing cowboy hats in Texas. The web of interwoven incentive structures - reptile brains, cafe standards, cheap gas, safety arms race, ... - pushing _everybody_ towards larger vehicles is the problem. System and id are completely misaligned, killing thousands in the process. reply aidenn0 14 hours agorootparent> _everybody_ drives huge SUVs. A few rugged men(TM) driving a large truck isn't the problem, they're couleur locale. It'd be adorable, like people wearing cowboy hats in Texas. Roughly 18% of the light vehicles sold in the US in 2023 were pickup trucks. The top 3 selling models in the US were pickup trucks. Yes there were about 3x as many SUVs sold[1] as pickups, but pickups aren't nothing. 1: I partitioned the \"light truck\" class into just pickups and SUVs; 78% of cars sold in 2023 were classified as \"light trucks\" per [2]; I got the 18% figure for pickups by totaling the sales of the 8 most popular pickup models by hand and dividing by 14.9M 2: https://carsurance.net/insights/car-sales-statistics/ reply drewcoo 14 hours agorootparentprev> Every time I see someone behind the wheel of the absolute largest SUV a company offers, it's a small woman With tiny dogs. Maybe they're the preferred vehicle of tiny dogs. reply steve_gh 2 hours agorootparentIn the UK we call these \"Chelsea tractors\". 9lIe a large vehicle more suited to offload conditions being driven short distances round highly affluent areas by people who wouldn't know the countryside if it bit them on the arsenal. reply beretguy 14 hours agorootparentprevSo… Rugged Women™, then. reply 7e 14 hours agorootparentprevIf you’re short, you prefer a vehicle which elevates your view. reply crote 6 hours agorootparentIf you're short, you can't see a damn thing over the hood of those vehicles. An entire class of schoolchildren could be standing right in front of your vehicle and you wouldn't even notice. reply astura 6 hours agorootparent>An entire class of schoolchildren could be standing right in front of your vehicle and you wouldn't even notice. This sounds like hyperbole but is actually not hyperbole - https://youtu.be/NDH3FDfVQl0?t=180 I'm a short woman and drive a sedan. reply dingnuts 14 hours agorootparentprevit's because they are mothers with >2 children. You cannot fit three car seats in the backseat of modern sedans, and station wagons don't meet the emissions requirements. Large SUVs meet CAFE regulations by being large enough to be regulated as a different class of vehicle. Want smaller trucks? Complain to your Congresscritter to reign in the EPA and force them to write better regulation that does not have these unintended effects. reply alistairSH 14 hours agorootparentThere are WAY more trucks/SUVs out there than 3-child households. And I'm not sure what you mean by \"wagons don't meet emissions requirements\" - wagons would meet the same regulations as sedans. reply aidenn0 13 hours agorootparentWagons are heavier and less aerodynamic than sedans. When coupled with the same engine they are less fuel efficient (compare the E450 sedan to wagon). The fact that they are heavier and sell poorly means they will only usually be available with the large(r,st) engine (no E350 wagon in the US). Note that I used the Mercedes because it's the only non-compact wagon I'm aware of for sale in the US. reply markdoubleyou 13 hours agorootparentThere's also the Audi RS6 Avant, a good option for anyone interested in spending $160K on a station wagon with 621 horsepower. reply aidenn0 11 hours agorootparentOh, that beats out the AMG E-series wagon by 18 HP! reply alistairSH 12 hours agorootparentprevThat's fair, but the wagon should still outperform an equivalent cross-over in just about any measure. What I don't know is where the line is between car-based cross-over and actual truck (as it related to emissions and safety standards). reply aidenn0 11 hours agorootparentThere's an envelope, rather than a single line, but one line that makes up that envelope is AWD, such that some models without optional AWD are cars, but trucks with optional AWD reply jewayne 14 hours agorootparentprevI love how everybody here who mentions the CAFE regulations neglects to mention that they were a gift to the automakers. reply nytesky 13 hours agorootparentprevI fit 3 diono car seats across my Honda Fit. Most aren’t even using the 3rd row for seating. Now a wagon or SUV is good for carpooling, but a minivan obv would be best. reply trgn 14 hours agorootparentprevAll true, also sad. That aside, how's the EPA responsible, not sure if I understand the connection? reply alistairSH 14 hours agorootparentTrucks (and SUVs) don't follow the same emissions regulations as sedans/wagons. I don't know if that's just EPA policy or part of the legislation that allows them to regulate auto emissions. But, it's also Congress's fault for leaving the chicken tax in place (25% tariff on imported trucks/vans). This essentially allowed domestic brands to price gouge on trucks/suvs (as VW at the time, and then the Japanese brands as well) couldn't compete. reply aidenn0 13 hours agorootparentToyota has plants in the US; I assume they make the Tacoma in the US, which should avoid the chicken tax? reply alistairSH 12 hours agorootparentCorrect. The Tacoma and Tundra are assembled in the US so should avoid the tax. The Land Cruiser is still produced in Japan, so subject to the tax. Same for the Lexus GX (no Toyota equiv in US) and LX (rebadged Land Cruiser). The old mini-truck and T100 were made in Japan (except maybe a year or two at the end of T100 production). The tax was implemented in the 60s or 70s, as VW was trying to get into the van and truck market in the US. reply practicemaths 14 hours agorootparentprevTBF Trucks and SUVs are relatively easier to drive in certain regards. Such as being able to see over parked traffic when making a turn. And I've found driving taller vehicles more comfortable for my back coincidentally. reply mecsred 14 hours agorootparentWhen you become the parked traffic, how does anyone see over you? Guess they need an even taller vehicle. reply practicemaths 14 hours agorootparentYup. There's certainly a feedback loop. Just stating things that make these larger vehicles more easier to drive, especially for women whom on average are generally shorter so, even with pumping the seat up it's still considerably harder to see around traffic. reply mecsred 13 hours agorootparentI definitely get it. My girlfriend drives an SUV and I'd be lying if I said I never felt glad about that the way drivers and roads are out here. She's also been in an accident (with minimal injuries miraculously) where an SUV wrecked her E-bike in a pedestrian crossing because it \"couldn't see\" her. I find it important to continually bring up the fact that it can be different. More public transit and bike paths. If I we could take bikes around the city without worrying about being killed or having them stolen it would be a dream. reply arwhatever 14 hours agorootparentprevUntil the parked vehicles also become taller reply pokerface_86 13 hours agorootparentprevin no world does having a higher center of gravity, and much higher mass make a car easier to drive. i find them painful to drive because i feel so unstable and slow. reply seabird 13 hours agorootparentYour average American driver has no understanding of this and never will. There's a reason the goofball in the Tahoe XL that takes every corner at 5mph is fine with going 90mph on the highway; they truly have zero idea on how to assess the handling of a vehicle. reply havefunbesafe 13 hours agoparentprevThe only way to reverse the trend is to create a motocross hitch that doesn't pulverize the suspension of a car/crossover. reply HeatrayEnjoyer 13 hours agoparentprevBetter their trucks than my family members. reply FooBarBizBazz 13 hours agoparentprevYou think it's all dudes driving these things though? Surely yeah, that's the case for big stupid pickup trucks. But SUVs are driven by anxious Starbucks moms. \"I need to feel safe.\" Hausfrauenpanzeren. reply hipadev23 14 hours agoparentprevGovernment regulation is what forced manufacturers to make bigger vehicles. reply jewayne 14 hours agorootparentBut who asked for the government regulations? Perhaps the manufacturers themselves? Edit: Yeah, it was the manufacturers. reply porphyra 14 hours agoprevVans are a lot better for both transporting people (more spacious compared to SUVs) and goods (easier loading and protected from the weather compared to pickups), plus of course having vastly better visibility. It always seems odd how trucks and SUVs are seen as status symbols in the US whereas in, say, Hong Kong, it is the MPV that is the status symbol. I suppose certain tax and emissions regulations in favor of trucks also contribute to their popularity. reply toast0 13 hours agoparentVans tend to have pretty good visibility, but not compared to a single cab truck. Loading into a van can be easier or harder depending on the goods. Larger items are easier into a truck, IMHO, since you don't have to rearrange or remove the seating, and don't have to negotiate door openings, and sometimes it's useful to get help over the sides of the bed. Lower typical height of van floors can be helpful though. Pickup beds tend to have more and better tiedown points, too. I have a pickup and a van, and a c-max, and I would most likely give up the c-max first; especially if either or both the van and the pickup had reasonable fuel efficiency. reply crote 6 hours agorootparent> Larger items are easier into a truck, IMHO, since you don't have to rearrange or remove the seating Think less minivan and more Mercedes-Benz Sprinter. You load them more like a moving truck: giant doors in the rear, big empty space to put stuff in. reply RangerScience 14 hours agoparentprevAFAIK, exactly this - something about how trucks and SUVs are classified differently and so have different emission standards, which results in other effects that are then appealing to consumers. Plus, of course, marketing. reply toast0 10 hours agorootparentMinivans are light trucks for fleet purposes, so they can also get bigger engines and what not, if that's what people are looking for. reply aidenn0 13 hours agoparentprevCan't even buy the MPV in the US anymore... reply riversflow 14 hours agoparentprevVans are not better for goods as a universal rule, they are better for certain classes of goods. I do not want a yard of manure/dirt/gravel dumped in my van, but thats a common use of a pickup in my neck of the woods. Like extremely common. > Hong Kong Hot take: The Americas just aren't developed to the level of Eurasia, and we should stop pretending like they are. reply mlinhares 14 hours agorootparentI think this is an incentives game, Americans have much more land and money to pay for expensive development (like suburbs) that just do not make financial sense in the EU or Asia in general, as they either lack land or it would be too expensive to pay for the infrastructure needed to make it all work. I also haven't met an American that has been to a high quality of life and walkable city in the EU and didn't come back with a changed perspective on what life could be like. Once you visit places like London, Paris, Barcelona, Berlin, Amsterdam and the like and stay there for a while you rethink the suburban life. I'm in the burbs because having small kids in big US cities is super shitty and expensive but as soon as they get older the goal is to find a nice city to move to. Either here or in Europe. reply toast0 12 hours agorootparentSuburbia is expensive in some ways, but suburbs are often built as a less expensive place to live or at least less expensive for a given size. Of course, it's often hard to find a studio apartment in the suburbs and it's hard to find a single story detached home in the city, so it's comparing different kinds of apples; they're comparable but not fungible. I had a very nice visit to Paris and London for business many years ago. And I tried to get my boss to transfer me to the Paris office. But I moved from the suburbs to a ruralish community where I can live on a 9 acre wooded lot where I know my neighbors but rarely hear them. reply mlinhares 7 hours agorootparentThey’re only less expensive on paper because the rest of the county, state and federal government are footing the bill for infrastructure, gas for all the driving and everything else. If we were paying the actual price of suburbia people would likely be moving closer to the cities. reply RcouF1uZ4gsC 13 hours agorootparentprev> I also haven't met an American that has been to a high quality of life and walkable city in the EU and didn't come back with a changed perspective on what life could be like. Once you visit places like London, Paris, Barcelona, Berlin, Amsterdam and the like and stay there for a while you rethink the suburban life. Then you haven’t met enough people. Lots of Americans, like me, visit those cities, enjoy our visit, and come back happy to live in suburbia with our big homes and yards. Those cities are great to visit, but I wouldn’t want to try to raise a family there. reply leotravis10 14 hours agoprevRelated: Vehicles with higher, more vertical front ends pose greater risk to pedestrians (iihs.org)322 points by yours truly463 comments https://news.ycombinator.com/item?id=38267588 reply astrolx 14 hours agoprevNo sh*t sherlock! As a pedestrian who found himself on the receiving end of a hood and survived via a last-second jump to end up on top of the said hood, I certainly concur. reply 082349872349872 15 hours agoprevInterestingly, agricultural tractors have evolved to have better sightlines. Could that be because for these machines, the pedestrian deaths are usually members of the tractor drivers' family? https://www.profi.co.uk/wp-content/uploads/sites/8/2020/06/2... vs https://bigiron.blob.core.windows.net/public/items/72b53c753... reply matsemann 13 hours agoparentLondon has enforced a Direct Vision Standard [1 example image] for big vehicles. I hope my city will soon as well. Having large trucks, trailers etc. in the city with no visibility to pedestrians is a death trap. My city's new trash truck [2]. Instead of the standard \"high\" trailer seating above the engine, it's now down on a pedestrian level and can see clearly in what's normally a \"blind spot\" when turning right. [1]: https://fncdn.blob.core.windows.net/web-clean/1/root/direct-... [2]: https://storage.googleapis.com/smallstep/sites/33/2022/02/Re... reply kube-system 14 hours agoparentprevAgricultural tractors aren't made with any consideration for pedestrian safety. The reason they have good visibility is because that makes them easier to use, and there is no other competing reason not to surround the driver with a lot of glass. Passenger vehicles have other requirements that complicate this: 1. They must be attractive enough to sell in volume to the end buyer 2. They require large A-pillars to absorb crash impacts You could make an SUV that looks big fish bowl, but it would be difficult to simultaneously achieve safety targets and be a commercial success. reply alan-hn 7 hours agorootparentI'm sure you could achieve safety targets with a vehicle that was safe for those outside of it as well, perhaps safety for your children crossing the road should be attractive to consumers? reply porphyra 14 hours agoparentprevChildren keep getting killed by their own parents' SUVs though... reply mtoner23 14 hours agorootparentSome SUVs even have front cameras now to help see children reply kayodelycaon 14 hours agorootparentMine also has cameras and radars on the sides. Makes parking easier and backing out safer. The radars can see speeding cars in the parking lot before I can. :) reply trgn 14 hours agoparentprevOne of the reason I'm holding on to an old car, apart from out of spite, is the big windows and fantastic visibility. It's like stepping into a sunroom compared to new cars. reply Swizec 14 hours agoparentprev> Could that be because for these machines, the pedestrian deaths are usually members of the tractor drivers' family I have a different guess after seeing tractors in use as a kid: Driving tractors is a high precision affair. You're moving around expensive equipment that you don't want to bump. You're plowing next to all sorts of hazards you don't want to fall into. You have to park within an inch of attachments before hooking them on. Good sight lines help with all this. reply 48864w6ui 44 minutes agorootparentIt looks like sloped noses are about a decade old and are sold as hi vis for work. Now that ag tractors have them, might Chelsea tractors eventually copy the look? reply alan-hn 14 hours agorootparentprevOne could say that navigating a large vehicle through streets populated by humans should be considered a high precision affair, or is the difference expensive equipment vs human lives? reply Swizec 14 hours agorootparent> is the difference expensive equipment vs human lives? The difference is in safety margins. You really shouldn't drive a vehicle to within 2cm of any object on public streets. Not even while parking. 0.5m (2ft) is about the closest you should ever drive your car to anything really. Whereas for a tractor getting within 2cm is normal operating procedure. You won't be able to grab a 500kg (1100lb) attachment and drag it over to your tractor because you stopped too far away. reply u32480932048 13 hours agorootparent> Not even while parking. 0.5m (2ft) is about the closest you should ever drive your car to anything really. I've lived in several US states and the law has always been to park less than a foot from the curb. reply axus 13 hours agorootparentprevI try not to get within 6 feet of a human in my car, let alone 6 inches. reply alan-hn 13 hours agorootparentIt can be difficult to manage when you can't see them over your hood reply riversflow 14 hours agorootparentprevAlso, tractors are built to do work, not keep their occupants safe in a crash. reply NikkiA 14 hours agorootparentNot totally true, tractors gained roll bars and roll cages 40 or so years ago for a reason, and it was safety. reply trgn 14 hours agorootparentprev> not keep their occupants safe in a crash. perfect expression of the tragedy of the commons. Until there's top down government intervention, people choose to be either part of the problem or part of the solution. reply tomaskafka 11 hours agoprevIt seems that no one has yet posted the famous article linking SUVs to immaturity and low self esteem, so here we go. > Car companies managed this remarkable feat because they ran—and continue to run—quite possibly the most sophisticated marketing operations on the planet. They knew what people really wanted: to project an image of selfish superiority. And then they sold it to them at a markup > Who has been buying SUVs since automakers turned them into family vehicles? They tend to be people who are insecure and vain. They are frequently nervous about their marriages and uncomfortable about parenthood. They often lack confidence in their driving skills. Above all, they are apt to be self-centered and self-absorbed, with little interest in their neighbors or communities. https://www.vice.com/en/article/m7q7eb/electric-or-not-big-s... reply Nifty3929 10 hours agoparentJudgement and condescension are unlikely to change minds or behavior, or add anything else of value. It will only make people wonder who really has an issue with immaturity and low self esteem. reply tomaskafka 1 hour agorootparentThose words are the characteristics that the actual SUV marketers see in their data and user behavior :). reply Havoc 14 hours agoprevAfter the cybertruck my working assumption had been that US regulators had just given up on pedestrian safety. reply iknowstuff 14 hours agoparentThe cybertruck has not been shown to have lower pedestrian safety than other trucks. Ironically, you’re commenting on an article about higher hoods, and the CT has one of the lowest hoods among trucks on the market. reply kspacewalk2 13 hours agorootparentThe Cybetruck's obnoxious hood height[0] and curb weight[1] all but guarantees that it's dangerous for pedestrians. Not that singling out Cybetruck is completely fair, all large EVs are strictly worse for pedestrian safety due to their weight. [0] https://www.forbes.com/sites/petercohan/2023/12/05/with-litt... [1] https://www.autoweek.com/news/a46013576/tesla-cybertruck-spe... reply grecy 13 hours agorootparentThe Cybertruck is lighter than the Rivian and the F-150 Lightning. It's around 3,000lbs lighter than the electric Hummer. It's also lower than all of them. https://www.caranddriver.com/news/a46031051/2024-tesla-cyber... reply kspacewalk2 9 hours agorootparentSlightly lighter than Rivian R1 and roughly same weight as F-150 Lightning. But I agree, quite possibly not the absolute most dangerous vehicle on the road. reply 1970-01-01 12 hours agorootparentprevThe height is adjustable via air springs. It can ride lower, higher, or equal to those. reply grecy 9 hours agorootparentIt can't ride at either of it's two highest settings when going above 15 and 25 mph respectively. reply Havoc 14 hours agorootparentprevI'm commenting on pedestrian safety & regulators absence. Sharp edges and rolled steel instead of crumple zones is absolutely a bad time if you're a pedestrian. High hoods isn't the only possible risk. reply iknowstuff 13 hours agorootparent1) It seems you are confusing crumple zones (which the ct definitely has lol) with cushioning the impact for a pedestrian. 2) sure, but the only research we have says higher hood = much more deadly. Cybertruck has a lower hood with a better angle. So why single it out when the jury is out on what’s more important for safety reply u32480932048 13 hours agorootparentCybertruck Man Bad. reply kjkjadksj 12 hours agorootparentprevThe thing looks like a maul made for splitting wood and you know its owners will drive like they have a lawyer on retainer. reply maxglute 4 hours agoparentprevI thought there were regulations against sharp angles on hoods. I assume cybertruck geometry does not violate, but those chamfers still look dangerous as hell. reply autoexec 11 hours agoparentprevHonestly all the \"self-driving\" car companies allowed to beta test their products on public streets show a strong disregard for pedestrian/public safety. reply ChrisLTD 6 hours agoprevThe US government is completely MIA on this issue. There’s no reason people need these monster trucks to drive to and from their job every day or to and from the grocery store. reply sf_rob 13 hours agoprevI got in an argument with the Ford CMO on Twitter about this. He said that the F150 has maintained the overall footprint for years, which is true. However, if you super-impose an image of older F150s and newer ones (which I did) you find that the hood is much more \"cubic\" leading to a higher edge and much worse viewing angle. I believe this is entirely stylistic. reply kjkjadksj 12 hours agoparentAnother big difference between a new one and a 50 year old one, is the cabin on the old one is basically like driving in an aquarium tank in terms of visibility with the lack of a or b pillar obstruction and low belt lines. Suspension is also generally lower on smaller tires from the ones I still see around town from the 70s. Sometimes their hoods aren’t much taller than a sedan but that could be from the suspension setup. reply TacticalCoder 8 hours agoprevThey're also do not seem aerodynamic. Note that many cars now have features meant to protect pedestrians a bit if there's an impact: the bonnet shall rise with springs, instantly (not unlike an airbag) and then the springs shall absorb the impact a bit. Some bonnet even have airbag outside the car that are supposed to protect pedestrians. But, yup, these huge, tall, flat hoods don't seem that great. Drive a Porsche guys, not a RAM ; ) reply tfourb 14 hours agoprevI honestly do not understand, why regulators have not mandated limits on front dead angles (space occluded from view by the hood) and max vehicle hood hight by now. Well, I understand why it hasn't happened in the U.S. (how could any regulator curtail the manhood of the truck aficionados?), but in the E.U. regulators usually are more on top of these things. reply HeyLaughingBoy 13 hours agoparentAre you saying that trucks with high hoods are prohibited in the EU? reply CalRobert 13 hours agorootparentThey're not. The Netherlands is full of Dodge Rams. reply crote 6 hours agorootparentIf by \"full\" you mean \"might encounter a single one on your commute\", then yes. reply cbondurant 14 hours agoprevthe current pavement princess trends have left me doubting that any more regulation of any kind will happen in the US. reply alistairSH 14 hours agoparentNot just the pavement princess stuff - Congress is simply doing less period.[1] Through most of the 20th century, 1000+ laws/year was common. Since 2000, that's dropped to <500 laws/year. 1 - https://en.wikipedia.org/wiki/List_of_United_States_federal_... reply mensetmanusman 13 hours agorootparentPages of laws passed is a better metric as the laws have become more bundled due to partisanship. reply kjkjadksj 12 hours agorootparentEven then, one era might write more tersely than another. reply kjkjadksj 12 hours agorootparentprevHard to measure between eras with the evolution of pork barrel politicking and other recent cultural practices. reply havblue 14 hours agoprevThere are definitely polls that show women find men in trucks more attractive in the US than, say, vans. If you want to signal wealth and refinement get a luxury vehicle. If you want to signal utility and masculinity get a truck. reply 0xdde 13 hours agoparentOf course studies showing anything are out there. Much less likely that they are properly set up and therefore believable. reply actionfromafar 11 hours agoparentprevGreat. Tax the trucks more and driving one will be an even better signal! reply HeyLaughingBoy 13 hours agoparentprevSeriously? There are polls that show anything you want. And if you have trouble finding one, you just create your own. Polls are essentially meaningless. reply francisofascii 12 hours agoparentprevhonestly, I believe it. I would blame the truck ads that have brainwashed Americans into thinking that driving a truck makes you more masculine or tough. reply grecy 13 hours agoparentprevI would be very shocked to learn those polls were funded by the makers of large SUVs and trucks. /s reply quadyeast 12 hours agoprevThe opaque tinted side windows make it hard to tell if the driver sees you at the pedestrian cross. I'm often forced to wait till they leave and they sometimes wait for me so ... reply sporkland 9 hours agoparentIllegal but not enforced in California. I say we let these people run a few more years and then basically revoke the license of anyone that has done this for a year. The front dark tinted window is nearly 100% correlated with reckless, aggressive driving for me. reply beretguy 14 hours agoprevWhoever injures - or knock on wood kills - somebody with a truck with such a high hood should serve a mandatory prison sentence. reply dingnuts 14 hours agoparentvehicular manslaughter is already a crime, and these trucks have been regulated INTO existence, not out of consumer desire and a lack of regulation. Consumers preferred small trucks until they disappeared due to CAFE standards. If you want the government to intervene, their first step should be to remove the regulations that encourage this growth in vehicle size to begin with, rather than going after consumers who have very little choice in trucks reply acdha 14 hours agorootparentThe average American truck buyer picked it as a lifestyle accessory - it’s not like there was a huge shift in the percentage of trades jobs over the last few decades! Liability would be one way to encourage expressing that fashion aesthetic in other ways. reply s1artibartfast 12 hours agorootparentLiability already exists. what next? reply acdha 9 hours agorootparentNo, not effectively. Drivers are given extreme deference by law enforcement and in lawsuits where it’s usually assumed that the victim was at fault unless video evidence exists and is immediately available, and concepts like contributory negligence are often used to reduce legal responsibility without regard for the relative damage levels. Insurance is required, but the coverage levels are low so it’s common for the people who do successfully sue often get less than they need for ongoing treatment and support. We can’t change everything all at once but requiring more comprehensive coverage in general and especially for the most dangerous vehicles would be a good start. reply s1artibartfast 4 hours agorootparentBut contributory negligence should never depend on the relative damage level. If an accident had 10 or 1 million in damages, and was 80-20% why would the total ever matter? I think your points about insurance are worth thinking about, minimum liability is lower than most expected earnings. reply s1artibartfast 14 hours agoparentprevI think it should depend on the circumstances. If the pedestrian is at fault, I think the individual, or their estate, should have to pay for the truck repairs and therapy for the driver. reply matsemann 13 hours agorootparentIn the Norwegian traffic law, it specifically says that you are to slow down in areas with children. So \"the child ran into the street chasing a ball right in front of my truck\" isn't a valid excuse, as that's something \"to be expected\" when driving in a residential area. At least in theory. Unfortunately the cops here almost never wants to prosecute cars hitting kids or pedestrians. There's always an excuse. \"The sun was in their eye\", \"kid didn't wear high vis (it was daytime)\". reply s1artibartfast 13 hours agorootparentI agree that there is an ammount of reasonable care drivers should have. However, I think the contribution of pedestrians is generally understated. AT least in the US, 33% of pedestrians involved in fatal accidents were drunk. 16% were on freeways. 59% were on non-freeway arterials, while 22% were on local streets. [1] I found it surprising that fatalities are far more likely to involve Pedestrians being drunk or jaywalking on high-speed throughfare than drunk drivers. https://www.ghsa.org/sites/default/files/2020-02/GHSA-Pedest... reply TheCoelacanth 10 hours agorootparentIn those cases, the person who designed the road should be charged with manslaughter. People don't just walk along the side of a freeway because they felt like it. They do it because negligent planners left them with no better route. reply burnerburnson 13 hours agorootparentprevI'd guess the majority of pedestrians who are at fault for accidents are vagrants who can't or won't pay for any damage they cause. I don't see well-to-do businessmen jumping into the road all that often, but I do see deadbeat beggars do it every day. reply 14 more comments... GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "A recent study reveals that the height of vehicle hoods in American trucks and SUVs is linked to a rise in pedestrian fatalities.",
      "Analysis of crash data from 2016-2021 demonstrates that pickups and SUVs have substantially taller hoods than standard cars, making them more hazardous to pedestrians.",
      "The study also indicates that pedestrians are more likely to be killed in accidents involving pickups and SUVs compared to vans and cars, highlighting the impact of larger vehicles on the increasing pedestrian death rates in the United States."
    ],
    "commentSummary": [
      "Taller vehicle hoods, larger SUVs, and trucks contribute to increased pedestrian fatalities by limiting visibility for drivers and cyclists.",
      "Safety regulations aimed at protecting occupants have inadvertently compromised driver visibility, leading to debates about prioritizing safety over appearance in car design.",
      "Concerns include reduced visibility caused by wider pillars, headlight brightness regulations, passing on the right, vehicle size, electric vehicle weight, and the popularity of SUVs and trucks. Discussions focus on improving road safety and addressing concerns related to larger vehicles."
    ],
    "points": 263,
    "commentCount": 273,
    "retryCount": 0,
    "time": 1706032472
  },
  {
    "id": 39107620,
    "title": "Hourglass Diffusion Transformer: High-resolution Image Generation at Scale",
    "originLink": "https://crowsonkb.github.io/hourglass-diffusion-transformers/",
    "originBody": "Scalable High-Resolution Pixel-Space Image Synthesis with Hourglass Diffusion Transformers Katherine Crowson*1, Stefan Andreas Baumann*2, Alex Birch*3, Tanishq Mathew Abraham1, Daniel Z Kaplan4 Enrico Shippole4 Stability AI1, LMU Munich2, Birchlabs3, Independent Researchers4 Preprint, 2024 *Indicates Equal Contribution Paper Code arXiv Samples generated directly in RGB pixel space using our HDiT models trained on FFHQ-10242 and ImageNet-2562. Abstract We present the Hourglass Diffusion Transformer (HDiT), an image generative model that exhibits linear scaling with pixel count, supporting training at high-resolution (e.g. 10242) directly in pixel-space. Building on the Transformer architecture, which is known to scale to billions of parameters, it bridges the gap between the efficiency of convolutional U-Nets and the scalability of Transformers. HDiT trains successfully without typical high-resolution training techniques such as multiscale architectures, latent autoencoders or self-conditioning. We demonstrate that HDiT performs competitively with existing models on ImageNet-2562, and sets a new state-of-the-art for diffusion models on FFHQ-10242. Efficiency Scaling of computational cost w.r.t. target resolution of our HDiT-B/4 model vs. DiT-B/4 (Peebles & Xie, 2023), both in pixel space. At megapixel resolutions, our model incurs less than 1% of the computational cost compared to the standard diffusion transformer DiT at a comparable size. High-level Architecture Overview High-level overview of our HDiT architecture, specifically the version for ImageNet at input resolutions of 2562 at patch size p = 4, which has three levels. For any doubling in target resolution, another neighborhood attention block is added. \"lerp\" denotes a linear interpolation with learnable interpolation weight. All HDiT blocks have the noise level and the conditioning (embedded jointly using a mapping network) as additional inputs. Files We provide the 50k generated samples used for FID computation for our 557M ImageNet model without CFG (part 1, 2, 3, 4, 5, 6, 7, 8), with CFG = 1.3 (part 1, 2, 3, 4, 5, 6, 7, 8), and for our FFHQ-10242 model without CFG (part 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21). BibTeX @misc{crowson2024hourglass, title = {{S}calable {H}igh-{R}esolution {P}ixel-{S}pace {I}mage {S}ynthesis with {H}ourglass {D}iffusion {T}ransformers}, author = {Katherine Crowson and Stefan Andreas Baumann and Alex Birch and Tanishq Mathew Abraham and Daniel Z Kaplan and Enrico Shippole}, year = {2024} } This page was built using the Academic Project Page Template which was adopted from the Nerfies project page. You are free to borrow the of this website, we just ask that you link back to this page in the footer. This website is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License.",
    "commentLink": "https://news.ycombinator.com/item?id=39107620",
    "commentBody": "Direct pixel-space megapixel image generation with diffusion models (crowsonkb.github.io)231 points by stefanbaumann 15 hours agohidepastfavorite43 comments Birch-san 12 hours agoI'm one of the authors; happy to answer questions. this arch is of course nice for high-resolution synthesis, but there's some other cool stuff worth mentioning.. activations are small! so you can enjoy bigger batch sizes. this is due to the 4x patching we do on the ingress to the model, and the effectiveness of neighbourhood attention in joining patches at the seams. the model's inductive biases are pretty different than (for example) a convolutional UNet's. the innermost levels seem to train easily, so images can have good global coherence early in training. there's no convolutions! so you don't need to worry about artifacts stemming from convolution padding, or having canvas edge padding artifacts leak an implicit position bias. we can finally see what high-resolution diffusion outputs look like _without_ latents! personally I think current latent VAEs don't _really_ achieve the high resolutions they claim (otherwise fine details like text would survive a VAE roundtrip faithfully); it's common to see latent diffusion outputs with smudgy skin or blurry fur. what I'd like to see in the future of latent diffusion is to listen to the Emu paper and use more channels, or a less ambitious upsample. it's a transformer! so we can try applying to it everything we know about transformers, like sigma reparameterisation or multimodality. some tricks like masked training will require extra support in [NATTEN](https://github.com/SHI-Labs/NATTEN), but we're very happy with its featureset and performance so far. but honestly I'm most excited about the efficiency. there's too little work on making pretraining possible at GPU-poor scale. so I was very happy to see HDiT could succeed at small-scale tasks within the resources I had at home (you can get nice oxford flowers samples at 256x256px with half an hour on a 4090). I think with models that are better fits for the problem, perhaps we can get good results with smaller models. and I'd like to see big tech go that direction too! -Alex Birch reply michaelt 24 minutes agoparentI appreciate the restraint of showing the speedup on a log-scale chart rather than trying to show a 99% speed up any other way. I see your headline speed comparison is to \"Pixel-space DiT-B/4\" - but how does your model compare to the likes of SDXL? I gather they spent $$$$$$ on training etc, so I'd understand if direct comparisons don't make sense. And do you have any results on things that are traditionally challenging for generative AI, like clocks and mirrors? reply ttul 10 hours agoparentprevHi Alex Amazing work. I scanned the paper and dusted off my aging memories of Jeremy Howard’s course. Will your model live happily alongside the existing SD infrastructure such as ControlNet, IPAdapter, and the like? Obviously we will have to retrain these to fit onto your model, but conceptually, does your model have natural places where adapters of various kinds can be attached? reply Birch-san 10 hours agorootparentregarding ControlNet: we have a UNet backbone, so the idea of \"make trainable copies of the encoder blocks\" sounds possible. the other part, \"use a zero-inited dense layer to project the peer-encoder output and add it to the frozen-decoder output\" also sounds fine. not quite sure what they do with the mid-block but I doubt there'd be any problem there. regarding IPAdapter: I'm not familiar with it, but from the code it looks like they just run cross-attention again and sum the two attention outputs. feels a bit weird to me, because the attention probabilities add up to 2 instead of 1. and they scale the bonus attention output only instead of lerping. it'd make more sense to me to formulate it as a cross-cross attention (Q against cat([key0, key1]) and cat([val0, val1])), but maybe they wanted it to begin as a no-op at the start of training or something. anyway.. yes, all of that should work fine with HDiT. the paper doesn't implement cross-attention, but it can be added in the standard way (e.g. like stable-diffusion) or as self-cross attention (e.g. DeepFloyd IF or Imagen). I'd recommend though to make use of HDiT's mapping network. in our attention blocks, the input gets AdaNormed against the condition from the mapping network. this is currently used to convey stuff like class conditions, Karras augmentation conditions and timestep embeddings. but it supports conditioning on custom (single-token) conditions of your choosing. so you could use this to condition on an image embed (this would give you the same image-conditioning control as IPAdapter but via a simpler mechanism). reply bravura 9 hours agorootparentprevIPAdapter, I am curious if there are useful GUIs for this? Creating image masks through uploading to colab is not so cute. reply orbital-decay 8 hours agorootparentHere's one example: https://github.com/Acly/krita-ai-diffusion/ But generally, most other UIs support it. It has serious limitations though, for example it center-crops the input to 224x224px. (which is enough for a surprisingly large amount of uses, but not enough for many others) reply ttul 8 hours agorootparentYes. I discussed this issue with the author of the ComfyUI IP-Adapter nodes. It would doubtless be handy if someone could end-to-end train a higher resolution IP-Adapter model that integrated its own variant of CLIPVision that is not subject to the 224px constraint. I have no idea what kind of horsepower would be required for that. A latent space CLIPVision model would be cool too. Presumably you could leverage the semantic richness of the latent space to efficiently train a more powerful CLIPVision. I don’t know whether anyone has tried this. Maybe there is a good reason for that. reply bertdb 1 hour agoparentprevDid you do any inpainting experiments? I can imagine a pixel-space diffusion model to be better at it than one with a latent auto-encoder. reply stefanbaumann 52 minutes agorootparentNot yet, we focused on the architecture for this paper. I totally agree with you though - pixel space is generally less limiting than a latent space for diffusion, so we would expect good performance inpainting behavior and other editing tasks. reply sophrocyne 10 hours agoparentprevAlex - I run Invoke (one of the popular OSS SD UIs for pros) Thanks for your work - it’s been impactful since the early days of the project. Excited to see where we get to this year. reply Birch-san 10 hours agorootparentah, originally lstein/stable-diffusion? yeah that was an important fork for us Mac users in the early days. I have to confess I've still never used a UI. :) this year I'm hoping for efficiency and small models! even if it's proprietary. if our work can reduce some energy usage behind closed doors that'd still be a good outcome. reply sophrocyne 6 hours agorootparentYes, indeed. Lincoln's still an active maintainer. Energy efficiency is key - Especially with some of these extremely inefficient (wasteful, even) features like real-time canvas. Good luck - Let us know if/how we can help. reply fpgaminer 9 hours agoprevSeems like a solid paper from a skim through it. My rough summary: The popular large scale diffusion models like StableDiffusion are CNN based at their heart, with attention layers sprinkled throughout. This paper builds on recent research exploring whether competitive image diffusion models can be built out of purely transformers, no CNN layers. In this paper they build a similar U-Net like structure, but out of transformer layers, to improve efficiency compared to a straight Transformer. They also use local attention when the resolution is high to save on computational cost, but regular global attention in the middle to maintain global coherence. Based on ablation studies this allows them to maintain or slightly improve FID score compared to Transformer-only diffusion models that don't do U-net like structures, but at 1/10th the computation cost. An incredible feat for sure. There is a variety of details: RoPE positional encoding, GEGELU activations, RMSNorm, learnable skip connections, learnable cosine-sim attention, neighborhood attention for the local attention, etc. The biggest gains in FID occur when the authors use \"soft-min-snr\" as the loss function; FID drops from 41 to 28! Lots of ablation study was done across all their changes (see Table 1). Training is otherwise completely standard AdamW, 5e-4, 0.01, 256 batch, constant LR, 400k steps for most experiments at 128x128 resolution. So yeah, overall seems like solid work that combines a great mixture of techniques and pushes Transformer based diffusion forward. If scaled up I'm not sure it would be \"revolutionary\" in terms of FID compared to SDXL or DALLE3, mostly because SD and DALLE already use attention obviating the scaling issue, and lots of tricks like diffusion based VAEs. But it's likely to provide a nice incremental improvement in FID, since in general Transformers perform better than CNNs unless the CNNs are _heavily_ tuned. And being pixel based rather than latent based has many advantages. reply Birch-san 9 hours agoparentFID doesn't reward high-resolution detail. the inception feature size is 299x299! so we are forced to downsample our FFHQ-1024 samples to compute FID. it also doesn't punish poor detail either! this advantages latent diffusion, which can claim to achieve a high resolution but without actually needing to have correct textures to get good metrics. reply tbalsam 13 hours agoprevI enjoyed this paper (I share a discord with the author so I read it a bit earlier). It's not entirely clear from the comparison numbers at the end, but I think the big argument here is efficiency for the amount of performance achieved. One can get lower FID numbers, but also with a ton of compute. I can't really speak technically to it as I've not given it a super in depth look, but this seems like a nice set of motifs for going halfway between a standard attention network and a convnet in terms of compute cost (and maybe performance)? The large-resolution scaling seems to be a strong suit as a result. :) reply stefanbaumann 13 hours agoparentThanks a lot! Yeah, the main motivation was trying to find a way to enable transformers to do high-resolution image synthesis: transformers are known to scale well to extreme, multi-billion parameter scales and typically offer superior coherency & composition in image generation, but current architectures are too expensive to train at scale for high-resolution inputs. By using a hierarchical architecture and local attention at high-resolution scales (but retaining global attention at low-resolution scales), it becomes viable to apply transformers at these scales. Additionally, this architecture can now directly be trained on megapixel-scale inputs and generate high-quality results without having to progressively grow the resolution over the training or applying other \"tricks\" typically needed to make models at these resolutions work well. reply nwoli 12 hours agoparentprevWhich discord if its open to the public? I was on one woth kath in 2021 and loved her insights, would love to again reply SEGyges 3 hours agorootparentYou and the guy below you in this thread should probably tag me on twitter, same tag as here, I can point you. I do not especially want to leave the discord link in a frontpage hn thread. reply fpgaminer 9 hours agorootparentprevSame; a good ML focused discord would be great. Training ViTs all day is lonely work. I'm mostly locked into skimming the \"Research\" channels of image generation discords. LAION used to be decent with a good amount of interesting discussion, but it seems to have devolved into toxicity in the last year. reply SEGyges 3 hours agorootparentSee my other comment replying to that. reply l33tman 8 hours agorootparentprevLAION is good reply imjonse 13 minutes agoprevAre there any public pretrained checkpoints available or planned? reply gutianpei 10 hours agoprevI think NATTEN does not support cross attention, wonder if the authors have tried any text-conditioned cases? Does the cross-attention can only add to regular attention? Or added through adanorm? reply Birch-san 9 hours agoparentcross-attention doesn't need to involve NATTEN. there's no neighbourhood involved because it's not self-attention. so you can do it the stable-diffusion way: after self-attention, run torch sdp with Q=image and K=V=text. I tried adding \"stable-diffusion-style\" cross-attn to HDiT, text-conditioning on small class-conditional datasets (oxford flowers), embedding the class labels as text prompts with Phi-1.5. trained it for a few minutes, and the images were relevant to the prompts, so it seemed to be working fine. but if instead of a text condition you have a single-token condition (class label) then yeah the adanorm would be a simpler way. reply artninja1988 11 hours agoprevLooking at the output image examples, very nice, although they seem a little blurry. But I guess that's a dataset issue? Have you tried training anything above 1024x1024? Hope someone releases a model based on this since open source pixel space models are a rarity afaik reply Birch-san 9 hours agoparentthe FFHQ-1024 examples shouldn't be blurry. you can download the originals from the project page[0] — click any image in the teaser, or download our 50k samples. the ImageNet-256 examples also aren't typically blurry (but they are 256x256 so your viewer may be bicubic scaling them or something). the ImageNet dataset _can_ have blurry, compressed or low resolution training samples, which can afflict some classes more than others, and we learn to produce samples like the training set. [0] https://crowsonkb.github.io/hourglass-diffusion-transformers... reply GaggiX 12 hours agoprevI hope that all these insights about diffusion model training that have been explored in last few years will be used by Stability AI to train their large text-to-image models, because when it comes to that they just use to most basic pipeline you can imagine with plenty of problems that get \"solved\" by some workarounds, for example to train SDXL they used the scheduler used by the DDPM paper(2020), epsilon-objective and noise-offset, an ugly workaround that was created when people realized that SD v1.5 wasn't able to generate images that were too dark or bright, a problem related to the epsilon-objective that cause the model to always generate images with a mean close to 0 (the same as the gaussian noise). A few people have finetuned Stable Diffusion models on v-objective and solved the problem from the root. reply SEGyges 10 hours agoparentI have good news about who wrote this paper reply GaggiX 9 hours agorootparentTwo authors are from Stability AI, that's the reason why I wrote the comment. reply sorenjan 12 hours agoprevThis is probably a stupid question, but what kind of image generation does this do? The architecture overview shows \"input image\", and I don't see anything about text to image. Is it super resolution? Does class-conditional mean that it takes a class like \"car\" or \"face\" and generate a new random image of that class? reply Birch-san 9 hours agoparent> Is it super resolution? nope, we don't do Imagen-style super-resolution. we go direct to high resolution with a single-stage model. reply sorenjan 9 hours agorootparentI was referring to the input image in the diagram, what is that and how is the output image generated from it? Is it 256x256 noise that gets denoised into an image? I guess what I'm really asking is what guides the process into the final image if it's not text to image? reply stefanbaumann 9 hours agorootparentThe \"input image\" is just the noisy sample from the previous timestep, yes. The overall architecture diagram does not explicitly show the conditioning mechanism, which is a small separate network. For this paper, we only trained on class-conditional ImageNet and completely unconditional megapixel-scale FFHQ. Training large-scale text-to-image models with this architecture is something we have not yet attempted, although there's no indication that this shouldn't work with a few tweaks. reply sorenjan 8 hours agorootparentThank you, I'm not used to reading this kind of research papers but I think I got the gist of it now. Can this architecture be used to distill models that need fewer timesteps like LCMs or SDXL turbo? reply stefanbaumann 8 hours agorootparentBoth Latent Consistency Models and Adversarial Diffusion Distillation (the method behind SDXL Turbo) are methods that do not depend on any specific properties of the backbone. So, as Hourglass Diffusion Transformers are just a new kind of backbone that can be used just like the Diffusion U-Nets in Stable Diffusion (XL), these methods should also be applicable to it. reply GaggiX 12 hours agoparentprevIf it's Imagenet class-conditioned, FFHQ unconditioned. >Does class-conditional mean that it takes a class like \"car\" or \"face\" and generate a new random image of that class? Yup reply boievi 13 hours agoprev [8 more] [flagged] tbalsam 13 hours agoparent [–] Making a bank account work for you is a hard discipline and requires budgeting and the like. True, not all of us can \"hack\" it, but that doesn't mean that with some community classes and help you'll be able to use your bank account well! Why do I feel like a chatbot with this message. reply observationist 12 hours agorootparentYou're actually talking to a bot, in this particular case. 12 minutes old with -2 karma. :berk: reply tbalsam 12 hours agorootparentDid you seriously just berk me on HN. What next, is a Walmart cashier going to do that RobloxNite \"dab dance\" or whatever on me? Discusting. Dusgraseful. reply selimthegrim 12 hours agorootparentYou can take the Katt Williams route in response, or the way of Zen. The choice is yours. reply LoganDark 12 hours agorootparentprev [–] HN to SEO spam pipeline reply tbalsam 12 hours agorootparent [–] Ah yes, the checks notes ever popular bank account hacking SEO spam line. (Legitimately I am confused but maybe it is a one-two scam or the like, lolz. <3 :')))) ;'PPPP ;'PPPP) reply LoganDark 10 hours agorootparent [–] Sure. \"pay me xx amount of money to hack your enemy's bank account\", or possibly \"pay me xx amount of money to get your money back after you've been hacked\" reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The Hourglass Diffusion Transformer (HDiT) is a new image generative model that can train at high resolutions directly in pixel-space.",
      "HDiT combines the efficiency of convolutional U-Nets with the scalability of Transformers, resulting in impressive performance on ImageNet and setting a new state-of-the-art for diffusion models on FFHQ at high resolution.",
      "The model showcases efficient scaling in terms of computational cost when compared to other models, and the authors provide generated samples for evaluation."
    ],
    "commentSummary": [
      "The article introduces a high-resolution image generation model that avoids using convolutions to reduce potential artifacts.",
      "The model incorporates patching and neighborhood attention, showing potential efficiency and scalability.",
      "The paper explores the integration of the model into existing SD infrastructure and addresses concerns about compatibility. It also discusses the possibility of using transformer techniques.",
      "The article mentions the use of ML-focused discord servers, cross-attention, text-to-image generation, and Stability AI's large text-to-image models.",
      "Diffusion U-Nets, class-conditioned image generation, and budgeting for bank accounts are other topics briefly mentioned in the article."
    ],
    "points": 231,
    "commentCount": 43,
    "retryCount": 0,
    "time": 1706035138
  },
  {
    "id": 39109481,
    "title": "Why is Machine Learning Challenging?",
    "originLink": "https://ai.stanford.edu/~zayd/why-is-machine-learning-hard.html",
    "originBody": "Why is machine learning 'hard'? 7 minute read tweet hacker news discussion There have been tremendous advances made in making machine learning more accessible over the past few years. Online courses have emerged, well-written textbooks have gathered cutting edge research into an easier to digest format and countless frameworks have emerged to abstract the low level messiness associated with building machine learning systems. In some cases these advancements have made it possible to drop an existing model into your application with a basic understanding of how the algorithm works and a few lines of code. However, machine learning remains a relatively ‘hard’ problem. There is no doubt the science of advancing machine learning algorithms through research is difficult. It requires creativity, experimentation and tenacity. Machine learning remains a hard problem when implementing existing algorithms and models to work well for your new application. Engineers specializing in machine learning continue to command a salary premium in the job market over standard software engineers. This difficulty is often not due to math - because of the aforementioned frameworks machine learning implementations do not require intense mathematics. An aspect of this difficulty involves building an intuition for what tool should be leveraged to solve a problem. This requires being aware of available algorithms and models and the trade-offs and constraints of each one. By itself this skill is learned through exposure to these models (classes, textbooks and papers) but even more so by attempting to implement and test out these models yourself. However, this type of knowledge building exists in all areas of computer science and is not unique to machine learning. Regular software engineering requires awareness of the trade offs of competing frameworks, tools and techniques and judicious design decisions. The difficulty is that machine learning is a fundamentally hard debugging problem. Debugging for machine learning happens in two cases: 1) your algorithm doesn't work or 2) your algorithm doesn't work well enough. What is unique about machine learning is that it is ‘exponentially’ harder to figure out what is wrong when things don’t work as expected. Compounding this debugging difficulty, there is often a delay in debugging cycles between implementing a fix or upgrade and seeing the result. Very rarely does an algorithm work the first time and so this ends up being where the majority of time is spent in building algorithms. Exponentially Difficult Debugging In standard software engineering when you craft a solution to a problem and something doesn’t work as expected in most cases you have two dimensions along which things could have gone wrong: algorithmic or implementation issues. For example, take the case of a simple recursive algorithm: def recursion(input): if input is endCase: return transform(input) else: return recursion(transform(input)) We can enumerate the failure cases when the code does not work as expected. In this example the grid might look like this: Along the horizontal axis we have a few examples of where we might have made a mistake in the algorithm design. And along the vertical axis we have a few examples of where things might have gone wrong in the implementation of the algorithm. Along any one dimension we might have a combination of issues (i.e. multiple implementation bugs) but the only time we will have a working solution is if both the algorithm and the implementation are correct. The debugging process then becomes a matter of combining the signals you have about the bug (compiler error messages, program outputs etc.) with your intuition on where the problem might be. These signals and heuristics help you prune the search space of possible bugs into something manageable. In the case of machine learning pipelines there are two additional dimensions along which bugs are common: the actual model and the data. To illustrate these dimensions, the simplest example is of training logistic regression using stochastic gradient descent. Here algorithm correctness includes correctness of the gradient descent update equations. Implementation correctness includes correct computations of the features and parameter updates. Bugs in the data often involve noisy labels, mistakes made in preprocessing, not having the right supervisory signal or even not enough data. Bugs in the model may involve actual limitations in the modeling capabilities. For example, this may be using a linear classifier when your true decision boundaries are non-linear. Our debugging process goes from a 2D grid to a 4D hypercube (three out of four dimensions drawn above for clarity). The fourth data dimension can be visualized as a sequence of these cubes (note that there is only one cube with a correct solution). The reason this is 'exponentially' harder is because if there are n possible ways things could go wrong in one dimension there are n x n ways things could go wrong in 2D and n x n x n x n ways things can go wrong in 4D. It becomes essential to build an intuition for where something went wrong based on the signals available. Luckily for machine learning algorithms you also have more signals to figure out where things went wrong. For example, signals that are particularly useful are plots of your loss function on your training and test sets, actual output from your algorithm on your development data set and summary statistics of the intermediate computations in your algorithm. Delayed Debugging Cycles The second compounding factor that complicates machine learning debugging is long debugging cycles. It is often tens of hours or days between implementing a potential fix and getting a resulting signal on whether the change was successful. We know from web development that development modes that enable auto-refresh significantly improve developer productivity. This is because you are able to minimize disruptions to the development flow. This is often not possible in machine learning - training an algorithm on your data set might take on the order of hours or days. Models in deep learning are particularly prone to these delays in debugging cycles. Long debugging cycles force a 'parallel' experimentation paradigm. Machine learning developers will run multiple experiments because the bottleneck is often the training of the algorithm. By doing things in parallel you hope to exploit instruction pipelining (for the developer not the processor). The major disadvantage of being forced to work in this way is that you are unable to leverage the cumulative knowledge you build as you sequentially debug or experiment. Machine learning often boils down to the art of developing an intuition for where something went wrong (or could work better) when there are many dimensions of things that could go wrong (or work better). This is a key skill that you develop as you continue to build out machine learning projects: you begin to associate certain behavior signals with where the problem likely is in your debugging space. In my own work there are many examples of this. For example, one of the earliest issues I ran into while training neural networks was periodicity in my training loss function. The loss function would decay as it went over the data but every so often it would jump back up to a higher value. After much trial and error I eventually learned that this is often the case of a training set that has not been correctly randomized (it was an implementation issue that looked like a data issue) and is a problem when you are using stochastic gradient algorithms that process the data in small batches. In conclusion, fast and effective debugging is the skill that is most required for implementing modern day machine learning pipelines. tweet hacker news discussion Posted on: Thu 10 November 2016 Category: machine-learning",
    "commentLink": "https://news.ycombinator.com/item?id=39109481",
    "commentBody": "Why is machine learning 'hard'? (2016) (stanford.edu)218 points by jxmorris12 13 hours agohidepastfavorite104 comments DeathArrow 13 minutes agoAs with everything, if you don't know the fundamentals, the basics, you are very limited on what you can achieve and find stuff difficult. To use a parallel from software if you just know a programming language, how to call libraries and APIs you can't compare to a guy with a solid CS background who also understands algorithms, data structures, complexity, math and knows how computers work down to NAND gates. Because I have studied some ML fundamentals during my batchelor and master degrees I know I am not competent in ML and AI. But I know how to become competent if I need or want to. Most people do believe finishing a bootcamp and calling some frameworks in Python makes them competent. reply jurgenaut23 8 minutes agoparentYes, bootcamps are really a great way to score high on the Dunning-Kruger score, but not so much on any other metric of value. You need lots of sweat and tears to become a capable and competent ML engineer, especially to tell apart intrinsic limitations (i.e., no signal in your data) vs extrinsic issues (e.g., a bug anywhere between ETL and prediction, or a methodological mistake). reply 4death4 7 hours agoprevI used to work on an ML research team. In addition to what the author mentions, there is an entirely separate issue: whether or not what you're attempting to do is possible with the approach you've chosen. Consider making an iOS app. For the most part, an experienced software engineer can tell you if making a given app is possible, and they'll have a relatively clear idea about the steps required to realize the idea. Compare this to ML problems: you don't often know if your data or model selection can produce the results you want. On top of that, you don't know if you're not getting results because of a bug (i.e. the debugging issues mentioned by the author) or if because you have fundamental blocks elsewhere in the pipeline. reply owlninja 7 hours agoparentYou nailed where I currently stand. At my company I've been a jack of all trades but mostly software/dba work. My boss and I were very excited about ML when the hype cycle was taking off several years ago and completed a successful project. Fast forward to today, I got loaned out to another team that lost their data scientist, and for the first time in my career I'm having to say - \"I don't think we can do what you want.\" To me the \"science\" part really stands out. I have a decent grasp of methodologies and tools, but after weeks of dissecting the issue my conclusion is that they just don't have enough useful data... reply jxramos 5 hours agorootparentIt can be a very empirical art. If you can't generate more data at the time you can sometimes invest in reviewing the hand labeling ground truth to verify no false classifications slipped by. reply throwaway8877 6 hours agorootparentprevThe situation is not bad then. Can they collect more data? Can they generate more data? reply dchichkov 4 hours agoparentprevOn top of that, vast majority of engineers and researchers who had joined the field, only did it in the last few years. While, like with many other fields, it takes decades to get to a level of a well-rounded expert. One paper a day, one or two projects a year. It just takes time. No matter how brilliant or talented you are. And then the research moves on. And more is different. A GFLOPS shift to TFLOPS and then PFLOPS over a single decade is a seismic shift. reply DeathArrow 26 minutes agoparentprev>whether or not what you're attempting to do is possible with the approach you've chosen Knowing that depends on your level of understanding the field and the math behind and also experience. If you just know how to make API calls, then it's hard. What would be problematic if you want to do sentiment analysis for some product reviews? Result is the public perception within a margin of error, you have your data, you know what you want, you know how to get there. reply jurgenaut23 11 minutes agorootparentWell, even with a high level of understanding, any sufficiently advanced use case will still have some uncertainty regarding its \"feasibility\". Of course, you might think that some problems are \"solved\", e.g., OCR, translation, (common) object recognition, but MANY other problems exist where, no matter how experienced and knowledgeable you are, you can only have an educated guess as to whether a given model can achieve a given performance without actually trying it out. Where experience and knowledge really pays off is in telling apart model performance from bugs. There is a real know-how in troubleshooting ML pipelines and models in general. reply tbrownaw 6 hours agoparentprev> you don't often know if your data or model selection can produce the results you want. Like, not knowing if your data set actually contains anything predictive of what you're trying to predict? reply janalsncm 6 hours agorootparentHere’s an example of something similar. Say you have a baseline model with an AUC of 0.8. There’s a cool feature you’d like to add. After a week or two of software engineering to add it, you get it into your pipeline. AUC doesn’t budge. Is it because you added it in the wrong place? Is the feature too noisy? Is it because the feature is just a function of your existing features? Is it because your model isn’t big enough to learn the new feature? Is there a logical bug in your implementation? All of these hypotheses will take on the order of days to check. reply DeathArrow 10 minutes agorootparent>AUC doesn’t budge. Is it because you added it in the wrong place? Is the feature too noisy? Is it because the feature is just a function of your existing features? Is it because your model isn’t big enough to learn the new feature? Is there a logical bug in your implementation? Or is it because lack of expertise and experience and because someone tries stuff blindly without understanding a bit in the hope they will nail it with enough fiddling? reply p1esk 4 hours agorootparentprevAll of these hypotheses will take on the order of days to check. OK, but you can check them, right? How is that different from a regular software bug? reply janalsncm 3 hours agorootparentIn software engineering you can test things in something on the order of seconds to minutes. Functions have fixed contracts which can be unit tested. In ML your turnaround time is days. That alone makes things harder. Further, some of the problems I listed are open-ended which makes it very difficult to debug them. reply rezonant 1 hour agorootparent> In software engineering you can test things in something on the order of seconds to minutes. Functions have fixed contracts which can be unit tested. I think this only applies to a certain subset of software engineering, the one that rhymes with \"tine of christmas\". Implementing bitstream formats is an area I'm very familiar with, and I dance when an issue takes seconds to resolve. Sometimes you need to physically haul a vendor's equipment to the lab. In broadcast we have this thing called \"Interops\" where tons of software and hardware vendors do just this, but in a more convention-esque style (actually is often done at actual conventions). reply hcks 3 hours agorootparentprevWhy do you spend weeks adding something instead of directly testing all the later hypotheses? reply Chirono 1 hour agorootparentIn some cases you can directly test hypotheses like that, but more often than not, there isn’t a way to test without just trying. reply galaxyLogic 5 hours agorootparentprevThe Farmer is Turkey's best friend. Turkey believes so because every day Farmer gives Turkey food, lots of food. Farmer also keeps the Turkey warm and safe from predators. Turkey predicts that the Farmer will keep on being his best friend also tomorrow. Until one day, around Thanksgiving, the prediction goes wrong, awfully wrong. reply sanroot99 5 hours agorootparentThree body problem, reference,yeh reply dylan604 5 hours agoparentprevI have the same sentiments about DIY electronic designs. If I take someone else's designs and build it at home, I know it's all on my build skills lacking if it doesn't work as there is already working examples. If I design a device from the electronics to the software, I don't know if the thing isn't working because of bugs in the code, problems with the build of the electronics, or fundamental flaw in the design itself. At least not without a ton of time debugging it all. reply Animats 4 hours agorootparentHowever, we now have techniques for debugging electronics. Electronics tends to be designed to be decomposable into subunits, with some way to do unit testing. At least in the prototype, before it's shrunk for production. Test gear can be expensive, but it exists, all the way down to the wafer if needed. That wasn't always the case. Electronics problems used to be more mysterious. The conquest of electronic design and debugging is what allows making really complex electronics that works. It really is amazing that smartphones work at all, with all those radios in that little case. That RF engineers can get a GPS receiver and a GSM transmitter to work a few centimeters apart is just amazing. Machine learning isn't that far along yet. When it doesn't work, the tools for figuring out why are inadequate. It's not even clear yet if this is a technique problem which can be fixed with tooling, or an inherent problem with having a huge matrix of weights. reply mikrotikker 4 hours agorootparentI never understood the black magic behind things like 4g until I saw a teardown of some pole equipment and saw the the solid copper beam forming cavities inside. Blew my mind. reply 3abiton 1 hour agoparentprevI think one issue also, is that ML is so large as a field, it ecompasses huge subfields, or related fields (statistics, optimizatiob, etc...) reply raincole 7 hours agoparentprevBut aren't all science basically like this? If you know your hypothsis works before you do the experiments, it's not science anymore. reply eru 3 hours agorootparentYou are right. That's why you want to avoid doing science, when you can. Ideally, you want to be solving engineering problems instead. reply nerdponx 6 hours agorootparentprevYes, and science is \"hard\" compared to software development in a lot of ways. Less certainty of success and poorly defined success criteria. reply dylan604 5 hours agorootparentprevSometimes something you've done works, but you really don't know why/how. You then have to walk it back to figure out by experimenting what is causing it to work. I feel like this happen(ed|s) in chemistry a lot. Was it the fact that I stirred it counter clockwise this time, or that I got distracted and the temp went 5° hotter than intended, or that I didn't quite clean my beaker properly and some residue contaminated this batch, or any number of other steps. reply bugglebeetle 6 hours agorootparentprevNot really. It’s easy to tell relative to existing methods whether the size of data will solve the problem. For example, if you’re trying to solve a classification problem with a large number of labels, but only have a small amount of training data for some (or all) of them, it will probably never work. reply DeathArrow 6 minutes agorootparent>but only have a small amount of training data for some (or all) of them, it will probably never work. Transfer learning might help in some cases. reply krisoft 46 minutes agorootparentprev> classification problem with a large number of labels, but only have a small amount of training data for some (or all) of them, it will probably never work That is true. On the other hand i have seen someone once perform a trick which looked miraculous to me. We had a classification problem with a small number of labels (~3). And one of the labels had unfortunately way less samples in our training set. Then someone trained a GAN to turn the images of the abundant labels into images of the rare labels. We added those syntetically generated images to the training set and it improved our classification performance as best as we could tell. That one still feels a bit like black magic to me to be honest. Almost as if we got more out of less with a trick. reply senthil_rajasek 6 hours agoparentprevI have always thought of ML (not DL) as phenomena that can be modelled mathematically. It turns out that not all problems have a great mathematical model like self driving cars for instance and so the search continues... reply croutons 2 hours agorootparentAll of ML, including DL, are literally implemented using mathematical models. Alas, a model is just a model and doesn’t imply it works well or imply that it’s simple or easily discoverable. reply eru 3 hours agorootparentprevWhy would self-driving cars not have a great mathematical model? Or do you mean that the models are either black boxes (like deep learning) that we don't understand, and the white box models are not good enough? reply MichaelZuo 7 hours agoparentprevi.e. there a lot more unknown unknowns, which takes a lot more effort and intelligence to not stumble into haphazardly then most other fields. reply sackfield 10 hours agoprevI think a big difference between ML and regular programming is how the components at scale make the systems viable. When I was learning computer science, it seemed quite intuitive to me that you would start out with assembly, then go to a C like compiler, then abstract that to a JIT/dynamic type language, and go from that to the UI. I could see how each step in the layer added value and presented its tradeoffs. Contrast that to ML and even though I have done a large amount of work in it (in both university and in industry), I still can't fully appreciate how the building blocks interact to form an entire system. I find that I use intuition from other systems I have read about or implemented (e.g. decision trees and tabular data, ReLUs and images) to reason about the results in new systems and guess at better configurations and architectures. Might say more about me, but I always found ML was a \"start big and go backwards\" deal whereas computer science was a \"start small and go forwards\" deal. reply DeathArrow 0 minutes agoparentI would start with a solid understanding of probabilities, statistics, calculus. reply barelyauser 10 hours agoparentprevWhen building models it is useful to spend some time finding out what you already know about the problem. Things you yet don't know you know. This kind of knowledge will greatly simplify the model. I see newcomers making this mistake very often. In industrial vision, for example, the newcomers like to create very complicated models. I then show them that the \"box\" you trained a entire model to recognize will actually always be there in this position for the camera, because it sits in a conveyor belt which restricts its lateral movement. The problem can simply be solved with simple image processing. Stuff like that happens all the time. reply shostack 5 hours agorootparentWhen I was working on a pet project to teach myself how to build a scoring model based on analysis of images on mobile I went down a whole rabbit hole on how to detect where the image is in a photo to draw a box around it and then compress to 500x500. In reality, if I'm using a phone,I can just create a square frame for the user to center the image in and then compress. Sometimes the simplest solution is the one you don't get to till after you slog through the harder approach. I'm glad I learned a bit about image processing with Pytorch along the way. reply Cacti 9 hours agorootparentprevYou let newcomers dick around for months with a net on an industrial vision problem? This stuff was solved two decades ago. Why didn’t you just tell them? reply barelyauser 9 hours agorootparentThey usually don't take months. Would be optimal if I could catch them at the get go. Not what happens most of the time. When that happen most of the times you will se a manager who is not technical leading a group selected by \"professional\" recruiters. There is a lot of waste out there. reply Cacti 8 hours agorootparentOh. I understand. Carry on then lol Been there. And you’re right. reply wegfawefgawefg 6 hours agorootparentprevSometimes its really hard to convince an organization that their strategy wont work. Especially if ego or higher ups are convinced. reply chefandy 5 hours agorootparentMany developers also mistakenly think their mighty dev super brain genius powers make them capable of accurately evaluating and critiquing any profession. I’ve had many developers try to explain design to me, even knowing I’m an experienced, educated professional designer. I can see why some might roll their eyes. reply roenxi 8 hours agoprevMachine learning isn't comparable to software development. It is a statistical modelling exercise. This is like asking why advertising is hard - if a non-expert wades in to a different domain then they may find it has different challenges than what they are used to! This is just a specific case of the normal things that analysts routinely deal with. The major challenges in this youthful field of machine learning are building appropriate hardware and making it work. That, so far, has kept it the domain of the software engineer. As the situation continues to stabilise this is going to become the playground of statisticians and analysts. Or to put it another way - if you compare any field to software engineering, the problem is that other disciplines have a much harder time debugging things. Software is almost unique in that debugging is cheaper and quicker than building things right the first time. reply wegfawefgawefg 6 hours agoparentI dont feel like im doing statistical modeling when i do ml. Usually feels more like pipe alignment, followed by tremendous amounts of debugging. reply laichzeit0 51 minutes agorootparentThe heuristic I use for distinguishing between statistical modelling, machine learning and AI is is through feature engineering and model specification: - Statistical modelling: Manual feature engineering, manual model specification (y = ax + b) - Machine learning: Manual feature engineering, automated model specification (y = ax + b or y = ax^2 + b, I don't care, the algorithm should figure it out). - AI: Automated feature engineering (e.g. CNN), automated model specification reply salty_biscuits 6 hours agorootparentprevThat is also what statistical modelling feels like. EDA and data cleaning. reply p1esk 3 hours agorootparentprevIt depends on what is it you do when you “do ml”. reply p-e-w 7 hours agoparentprev> Machine learning isn't comparable to software development. It is a statistical modelling exercise. It's neither of the two. Machine learning isn't comparable to any other human endeavor because in many cases, much more value comes out of the models than (seemingly) goes in. LLMs for example are punching way above their weight. The ideas underlying their software implementations are extremely simple compared to the incredibly complex behavior they produce. Take some neural networks that can be explained to a bright high schooler, add a few more relatively basic ML concepts, then push an unfiltered dump of half the Internet into it, and suddenly you get a machine that talks like a human. Obviously I'm simplifying here, but consider that state-of-the-art LLM architectures are still simple enough that they can be completely understood through a 10-hour online course, and can be implemented in a few hundred lines of PyTorch code. That's absolutely bananas considering that the end result is something that can write a poem about airplanes in the style of Beowulf. reply eru 3 hours agorootparentI'm not sure it's as simple as you make it sound. Lots of problems have very simple solutions. And progress often means finding simpler solutions over time. But coming up with those solutions, and debugging them, is what's hard. For a comparison, have a look at how pistols got simpler over the last two hundred years. Have a look at the intricate mechanism of the P08 Luger https://www.youtube.com/watch?v=9adOzT_qMq0 and compare it to a modern pistol of your choice. (And the P08 Luger is already pretty late invention.) Or have a look at modern Ikea furniture, which can be assembled and understood by untrained members of the general public. But designing new Ikea furniture is much harder. reply doctorM 12 hours agoprevI'm a bit sceptical of the exponentially harder debugging claim. First it looks polynomially harder for the given example :p. Second other engineering domains arguably have additional dimensions which correspond to the machine learning ones mentioned in the article. The choice of which high level algorithm to implement is another dimension to traditional software engineering that seemingly exists and corresponds to the model dimension. This is often codified as 'design'. The data dimension often exists as well in standard learning software engineering. [Think of a system that is 'downstream' of other]. It's probably a lot simpler to deal with these dimensions in standard software engineering - but then this is what makes machine learning harder, not that there are simply 'more dimensions'. The delayed debugging cycles point seems a lot more valid. reply jolt42 10 hours agoparentThe article also pretends that there is only one correct answer, which seems atypical of the domain. The 1 green spot should extend somewhat fuzzily in each dimension in the ML case. reply janalsncm 6 hours agorootparentThere isn’t only one correct answer. Quite the opposite actually, many configurations give you local maximas. The difficulty is that it can be hard to explain from first principles why one local maxima is good. reply pizzaknife 11 hours agoparentprevi would subscribe to your newsletter if you offered one. reply hooande 4 hours agoprevDebugging is a problem. But the real problem I'm seeing is our expectations as software developers. We're used to being able to fix any problem that we see. If a div is misaligned or a column of numbers is wrong we can open the file, find the offending lines of code and FIX it. Machine learning is different because every implementation has a known error rate. If your application has a measured 80% accuracy then 20% of cases WILL have an error. You don't know which 20% and you don't get to choose. There's no way to notice a problem and immediately fix it, like you can with almost every other kind of engineering. At best you can expand your dataset, incorporate new models, fix actual bugs in the code. Doing those things could increase the accuracy up to, say, 85%. This means there will be fewer errors overall, but the one that you happened to notice may or may not still be there. There's no way to directly intervene. I see a lot of people who are new to the field struggle with this. There are many ways to improve models and handle edge cases. But not being able to fix a problem that's in front of you takes some getting used to. reply PaulHoule 11 hours agoprevThe #1 thing that makes it ‘hard’ in real life is that nobody wants to make training and test sets. So we have 50,000 papers on the NIST digits but no insight into ‘would this work for a different problem?’ (Ironically the latter might have been exactly what academics would have needed to understand why these algorithms work!) reply whiplash451 11 hours agoparentYou’re not paying tribute to MNIST-1D and many other datasets (including the massive segmentation dataset released by Meta with SAM). Read the literature before lecturing the community. reply Mltrw 9 hours agorootparentWe still don't have enough data and people are still wasting their time with trying to extend algorithms instead of making better training data. I've worked on a dozen ml projects two of them before Alex net came out and I've never gone wrong by spending 80% of my time creating a dataset specific to the problem and then using whatever algorithm is top dog right now. Labelled data is king. reply beckhamc 10 hours agoparentprevThe issue is the obsession with benchmark datasets and their flaky evaluation reply graphe 10 hours agorootparentWhat else could you do to test it besides it works for me and this test said it's good at talking? reply mistrial9 9 hours agoparentprevno, this is routinely cited in introductory remarks these days, but ignores some practical aspects of the competitive context, among other things. reply ramesh31 11 hours agoparentprevWould there be enough of a financial incentive to do so? Seems like a prime startup opportunity. reply MichaelRo 10 hours agorootparent>> Seems like a prime startup opportunity. Sometimes it's just ... hard. Apply some thought maybe before blindly parroting \"profit!\" Reporter: \"Why is it hard to cure cancer?\". Crowd: \"Would there be enough of a financial incentive to do so? Seems like a prime startup opportunity!\" Reporter: \"Why is it hard to end World poverty?\". Crowd: \"Would there be enough of a financial incentive to do so? Seems like a prime startup opportunity!\" Reporter: \"Why is it hard to build a warp engine?\". Crowd: \"Would there be enough of a financial incentive to do so? Seems like a prime startup opportunity!\" Reporter: \"Why is it hard to wipe your ass using the left hand?\". Crowd: \"Would there be enough of a financial incentive to do so? Seems like a prime startup opportunity!\" You get the idea... reply aleph_minus_one 9 hours agorootparent> Reporter: \"Why is it hard to cure cancer?\". Crowd: \"Would there be enough of a financial incentive to do so? Seems like a prime startup opportunity!\" What you want to optimize for is the money amount that you make at some quantile of the probablity distribution of the profits; say, the profits that are guaranteed in the best, say, 3 %, 5 %, 10 % or even 20 % of all possible outcomes. With a probablity of 97 % (if you choose the best 3 % of the outcomes), you won't make sufficient money if you attempt to cure cancer to be worth the risk, so the financial incentive is not there. TLDR: Financial incentives do matter, but work differently from how many people think that they are structured. reply wegfawefgawefg 6 hours agorootparentprevA cure for cancer would be terribly profitable. For a while. reply Buttons840 10 hours agorootparentprevI was just thinking the same, but I'm skeptical. When researchers want to publish a paper, are they going to pay extra money for extra difficulty in publishing their paper? No, they'll just use whatever toy environment is free or already established and get that paper published! reply Mltrw 9 hours agorootparentprevThere is plenty of money in it but you need to sell b2b and tp enterprise. That is not fun and as such no one is doing it. Put another way,if I were trying to do a start up in this space I'd spend 50% of my budget on marketing 25% on a third world data labelling sweatshop, 20% on data pipeline engineering and 5% on sexy ml stuff. reply rzzzz 11 hours agorootparentprevI believe that Scale.ai was founded to do exactly this. reply yshvrdhn 8 hours agoprevI think one of the issues is that fixing a problem is a lot harder in ML than in software engineering. You know that the model fails on this particular data point. If you have identified a bug in the code and wrote a fix did a pull request as long you are able to test the code for conditions you failed on you would have solved the problem. With modern ml especially with nueral nets as long as you don't have a way to spin up a data engine to track the problems you are facing and collect similar points you problem is not fixed. reply nonethewiser 7 hours agoparentCertainly resonates with me. When the problem is something like a list being the wrong shape or two lists not maintaining a parallel order its basically invisible unless you load the mental model of the code and think deeply about it. Not like you’re going to notice your list of length 2,340,383 should start with [0.12, 1.67, 0.66 instead of [0.412, 0.567, 0.23 reply pbh101 3 hours agoparentprevI remember running into a paper from Google circa 2017 IIRC discussing the maintainability issues with machine learning models but haven’t been able to track down since. Does anyone know which one this is and have a link? reply hollerith 3 hours agorootparenthttps://research.google/pubs/machine-learning-the-high-inter... reply pbh101 3 hours agorootparentLooks like I found a similar one just now too from the group. Thanks! https://proceedings.neurips.cc/paper_files/paper/2015/file/8... reply DeathArrow 34 minutes agoprevWhat do we even mean by \"Machine Learning\"? Understanding ML algorithms and the math behind, being able to change the algorithms and devise new ones? Using some ML libraries and frameworks? Taking an implemented solution and training it and fine tuning parameters? reply itissid 8 hours agoprevGreat teachers(if you can find them): Andrej Karpathy, Andrew Gelman & Ben Goodrich(Columbia), Subbarao Khambampati(ASU) to name a few I know of. Go Where the hard problems are (or find someone who is doing it): If you don't have a good intuition where to get good problems to practice on for a pay, choose a place to work where a Data Scientist is not just building dashboards/analytics but the company/team relies on them for answer to questions like: \"What goals should we set for the next half based on what you see\"? ML practitioner (read: use ML tech to do/debug X) different from ML Engineer (Read: Implement ML algorithm X e2e on data ) is different from Applied Statistician (think marketing sciences or powering experiments like A/B tests): All three areas of work in different areas of ML in one form or another. But make sure what you want to work in is clear in your head and your expectations from it. A lot of ML/Stats can be not with big data and yet really intuitive: I would say look for a problem domain in social/life/pharma/eco/political/survey/edtech sciences. They are full of intuitive models that need to be explainable and are often debuggable. An example here is usage of Stan software for Multilevel/Heirarchical Regression problems. Training here also makes you a great DS. reply jruohonen 13 hours agoprev\"An aspect of this difficulty involves building an intuition for what tool should be leveraged to solve a problem.\" While I agree with the good point about debugging, like many others, I am rather worried that we're increasingly deploying AI/ML where we shouldn't be deploying it. Hence, the above quote. reply sjwhevvvvvsj 11 hours agoparentI’m old enough to have learned that the secret to success is much less knowing the tool of the moment than picking the right tool for a job. The right tool may in fact be the new one, and LLM do open a lot of doors with zero shot capabilities, but oftentimes they can underperform a well tuned heuristic. It’s the ability to pick the right tool that is key. reply wwarner 9 hours agoparentprevWant to agree with you, as so many ML apps seem to be solutions looking for problems. But I actually feel that we are rapidly deploying ML in a development context for vastly improved results. The way that good models are built relies on many ML steps, and when the results finally come together the result is superior to what could have been custom designed. Broad adoption of something like probabilistic programming is coming soon. reply nazka 1 hour agoprevFor me it’s 2 things: math is already hard even with a teacher and after you found a job it’s just you and your screen. The other part is research moves at a crazy speed it’s very hard to keep up I think while having a job. reply Aunche 10 hours agoprevI don't think machine learning is particularly hard. It just involves a lot of brute force work and most of the time, you get a middling result that isn't particularly exciting. One of my friends had no CS background whatsoever but managed to make a basic Runescape mining bot by manually labeling hundreds of the correct colored rocks. His account got banned after a couple days though. reply Xcelerate 11 hours agoprevWith regard to model selection, one thing I learned a long time ago that provides powerful intuitive guidance on which model to use is the question: \"How could this be compressed further?\" There are some deep connections between data compression and generalized learning, both at the statistical level and even lower at the algorithmic level (see Solomonoff induction). For a specific example at the statistical level, suppose you fit a linear trendline to some data points using OLS. Now compute the standard deviation of the residual terms for each data point, and using the CDF of the normal distribution, for each residual, map its value into the interval [0, 1]. Sum together the logarithms of the trendline coefficients, the standard deviation, and the normalized residuals. This value is approximately proportional to \"sizeof(datamodel) + sizeof(model)\". It represents how well you compressed the data using the OLS model. But now suppose you plot the distribution of the residuals and find out that they do not in fact resemble a Gaussian distribution. This is a problem because our model assumed the error terms were distributed normally, and since this is not the case, our compression is suboptimal. So you back out some function f that closely maps between the uniform distribution on [0, 1] and the distribution that the residuals form and use this f to define a new model: yᵢ = m*xᵢ + b + εᵢ, with εᵢ distributed according to f(x;Θ), Θ being a parameter vector. When you sum the logarithms again, you will find that the new total is smaller than the original total obtained using OLS. The new trend line coefficients will slightly mess up the residual distribution again, so iterate on this process until you've converged on stable values for m, b, and Θ. At the algorithmic level, the recommendation to use compression as a model selection guide applies even to LLMs, but it's a bit harder to use in practice because \"sizeof(datamodel) + sizeof(model)\" isn't the entire story here. Suppose you had a \"perfect\" language model. In this case, you would achieve minimization of K(datatraining data), where K is Kolmogorov complexity. In practice, what is being minimized with each new LLM version is \"sizeof(dataLLM model) + sizeof(training dataLLM model) + sizeof(LLM model)\". You can assume \"sizeof(LLM model)\" is the smallest Turing machine equivalent to the LLM program. reply epgui 7 hours agoprevI find it difficult to buy the argument that “it’s not difficult because of the math”, at least in the way the author meant. I do however literally agree with the author: ML is not difficult because of the math. It’s difficult because for some reason people think the math is not important. But ML is math. reply nnevatie 2 hours agoprevI found the explanation quite convoluted (pun intended). Any system with billions of parameters is bound to be difficult to reason about. reply dang 12 hours agoprevDiscussed at the time: Why is machine learning ‘hard’? - https://news.ycombinator.com/item?id=12936891 - Nov 2016 (88 comments) reply epistasis 12 hours agoparentLove that thread. The top comment is excellent: > Like picking hyperparamters - time and time again I've asked experts/trainers/colleagues: \"How do I know what type of model to use? How many layers? How many nodes per layer? Dropout or not?\" etc etc And the answer is always along the lines of \"just try a load of stuff and pick the one that works best\". > To me, that feels weird and worrying. It's like we don't yet understand ML properly yet to definitively say, for a given data set, what sort of model we'll need. This embodies the very fundamental difference between science and engineering. With science, you make a discovery, but rarely do we ask \"what was the magical combination that let me find the needle in the haystack today?\" We instead just pass on the needle and show everyone we found it. Should we work on finding out the magic behind hyperparameters? In bioinformatics, the brilliant mathematician Lior Pachter once attacked the problem of sequence alignment using the tools of tropical algebra: what parameters to the alignment algorithms resulted in which regimes of solutions? It was beautiful. It was great to understand. But I'm not sure if it even ever got published (though it likely did). Having reasonable parameters is more important than understanding how to pick them from first principles, because even if you know all the possible output regimes for different segments of the hyper parameter space, really the only thing we care about is getting a functionally trained model at the end. Sometimes deeper understanding provides deeper insights to the problems at hand. But often, they don't, even when the deeper understanding is beautiful. If the hammer works when you hold it a certain way, that's great, but understanding all possible ways to hold a hammer doesn't always help get the nail in better. reply sjwhevvvvvsj 11 hours agorootparentI do a lot of model tuning and I’m almost ashamed to say I tell GPT what performance I’m aiming for and have it generate the hyper parameters (as in just literally give me a code block). Then I see what works, tell GPT, and try again. I’m deeply uncomfortable with such a method…but my models perform quite well. Note I spend a TON of time generating the right training data, so it’s not random. reply danielmarkbruce 10 hours agorootparent1/8th (soon to be 1/2) of the working world: \"I do a lot of X and I'm almost ashamed to say I tell GPT Y then I see if it works and try again\". reply sjwhevvvvvsj 3 hours agorootparentWell, I do want to know more about how it works. Anything important I will teach myself, it’s just hard to justify the time investment during work hours when the robot does it. Which I think is also important: these tools save time, but with downsides. reply logtempo 10 hours agorootparentprev> Sometimes deeper understanding provides deeper insights to the problems at hand. But often, they don't, even when the deeper understanding is beautiful. If the hammer works when you hold it a certain way, that's great, but understanding all possible ways to hold a hammer doesn't always help get the nail in better. Is it true? I mean, in mathematics having a proof of something is way stronger than having a conjecture. And in engineering, proving that your solution is optimal is way stronger than saying \"hey look, I tried many things and finally it works!\". Worse, in statistics if you throw a bunch of tests and pick the one that \"works\" you might have false conclusions all the time. And AI is statistics. Sure it works to test out 10 datatset and whatever number of different machine learning, but it takes time and money and might be suboptimal from an engineering POV. reply ummonk 11 hours agorootparentprev> This embodies the very fundamental difference between science and engineering. Not really though. In engineering, you have heuristics, even if you don't know why they work. In the case of deep learning / AI, there seems to be very little in the way of built up heuristic knowledge - it's just \"try stuff and see what works for every problem\". reply robotresearcher 8 hours agorootparentAll the model topologies that have names are heuristics. The idea of a 'layer' is a heuristic. And so on. You don't really just try stuff. You choose very few things to try from the space of models. And you choose craftily. We have quite a lot of domain craftiness now, if you think about it that way. reply liuxiansheng 10 hours agorootparentprevI think if this was truly the case then wouldn't ML algorithm development be a solved problem with AutoML? I don't think AutoML is close to ubiquitous which means there must still be value in heuristics and a deeper understanding of our tools. reply epistasis 10 hours agorootparentprevI think that's also the difference between science and engineering: has the tool/technology been around enough to learn heuristics, or is everything still in the \"fuck around and find out\" phase? reply amelius 11 hours agorootparentprevThe hammer analogy doesn't make much sense because for a hammer we can actually use our scientific knowledge to compute the best possible way to hold the tool, and we can make instruments that are better than hammers, like pneumatic hammers, pile drivers, etc. With your argument, we would be stuck with the good old, but basic hammer for the rest of time. reply epistasis 11 hours agorootparentThat seems like a different analogy; making better hammers is a different thing than understanding why holding a hammer a certain way works well. We did eventually invent enough physics to understand why we hold hammers where we do, but we got really far just experimenting without first principles. And even if we use first principles, we are going to discover a lot more by actually using the modified hand-held hammer and testing it, than necessarily hitting it out of the park with great physical modeling of the hammer and the biomechanics of the human body. And in any case, I'm not saying we shouldn't search for deep understanding of what hyperparameters work on a first try, I'm just saying there's a good chance that even if the principles are fully discovered, it may be that calculating using those principles is more expensive than a bunch of experimentation and won't matter in the end. That's the trick about science, it's more about finding the right question to answer than how to find answers, and often times the best questions only become apparent afterwards. reply logiduck 10 hours agorootparentprevYes, this makes it very difficult to apply ML and RL in non-simulated scenarios. With simulated scenarios you can just replay and \"sweep\" across hyperparameters to find the best one. In a realworld scenario with limited information, fine tuning hyperparameters is much harder as you quickly find yourself in local maxima. reply naveen99 10 hours agoparentprevSimilar: Machine learning is still too hard for software engineers - https://news.ycombinator.com/item?id=30432987 - 151 comments Machine learning is easier than it looks - https://news.ycombinator.com/item?id=6770785 167 comments reply phkahler 11 hours agoprev>> It becomes essential to build an intuition for where something went wrong based on the signals available. This has always been my approach. I learned programming way before I had access to debuggers and other methods to dig in, set breakpoints and step through code to see where it was going wrong. As a result, when I got in the real world I kind of looked down on people using those tools (mostly because I hate tools actually). But then I saw people get to the root of problems that I don't think I ever could have solved, and I started to appreciate those tools and the detail you could get to. My preference is still to have a great understanding of how algorithms work, how the code is written, and what the problem is, and noodle out what and where things may be going wrong. I only switch to detailed monitoring of the insides when \"thinking about it\" fails. Maybe I should have gone into this ML stuff ;-) reply crotchfire 3 hours agoprevTo be fair, chip design has both of these problems to a much greater degree than machine learning: - Exponentially Difficult Debugging - Delayed Debugging Cycles reply jvans 8 hours agoprevMachine learning systems are hard because your system can be badly under performing but still doing objectively great. I've seen systems that produce amazing improvements in core business metrics and then years later some subtle bug in the labeling process is accidentally uncovered and fixing it boosts performance by an additional 20%. A ton of time in ML is spent on minimizing the surface area for bugs because it can be difficult to even know they exist. reply nerdponx 7 hours agoparentIt's hard because there's rarely one single correct solution, design, answer, etc. It's hard in the same way as any other open-ended research work is hard. It will never not be hard in that sense. reply adamnemecek 9 hours agoprevIt's harder because there are no unifying theories. The ones that exist are seriously lacking. reply Cacti 11 hours agoprevthere are uncountable sets all over the place, and in practical terms, the repl loop may have a week long training lag after you hit enter. also, the data is almost always complete shit. lol. there’s no mystery why it’s hard. reply sfink 10 hours agoprev [–] This article annoyed me. Not because I think it overestimates any of the difficulties, but it condescendingly compares against \"standard software engineers\" as if their problems are so much easier and lack additional dimensions beyond algorithm and implementation. It doesn't help that they call N^4 \"exponential\". Long debugging cycles are not new. In fact, the field started out that way, when computers were slow enough that even a single edit-compile-run cycle could take hours (or days and some political capital, if you go back far enough). Even today, long debugging cycles are far from being restricted to ML, especially for failures that are only seen intermittently and in production. Performance mattering is not unique to ML. Obviously. Data issues are not unique to ML. Anytime you need to run against a \"representative workload\", you'll bump into them. Heck, anytime you run against a large test suite you'll run into issues with the quality of the comparison data, especially if the tests are long in the tooth. Furthermore, anything you're doing statistics on in general is going to bump into data issues—unless doing statistics is automatically ML? ML lacks various other dimensions of difficulty. Distributed systems. Web browsers that need to execute arbitrary code that hasn't been written yet. Backwards compatibility. Forwards compatibility. In-production databases. Cross platform development. Power usage optimization. Concurrency (ML can have this, but it doesn't always need to, and it can be in the \"embarrassingly parallel\" bucket). Deep dependency graphs. The author thinks ML development is hard because that's what they work on. They are but a grasshopper. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Machine learning is a challenging field because it involves implementing complex algorithms and models and debugging them.",
      "Debugging machine learning algorithms is more difficult than traditional software engineering because it involves dealing with bugs in the model and data.",
      "Debugging cycles in machine learning can take longer, leading to delays in getting feedback on fixes. Developing an intuition for identifying and fixing issues is crucial in this field."
    ],
    "commentSummary": [
      "The Hacker News discussion focuses on the challenges and complexities of machine learning (ML), emphasizing the need for a strong background in computer science, algorithms, and mathematics.",
      "The discussion highlights the uncertainties in ML projects, the difficulties of debugging ML models compared to traditional software engineering, and the challenges of training models with limited data.",
      "The importance of selecting the appropriate tool for the task and the financial incentive of addressing these challenges are also mentioned in the discussion."
    ],
    "points": 218,
    "commentCount": 104,
    "retryCount": 0,
    "time": 1706042605
  },
  {
    "id": 39106355,
    "title": "NSA's Secret Ban on Furby Revealed: Concerns Over Spy Device Capabilities",
    "originLink": "https://www.404media.co/these-are-the-notorious-nsa-furby-documents-showing-spy-agency-freaking-out-about-childrens-toy/",
    "originBody": "Subscribe Join the newsletter to get the latest updates. Success Great! Check your inbox and click the link. Error Please enter a valid email address. 🖥 404 Media is reported and written by humans for humans. Sign up above for free access to this article. The NSA has finally released a treasure trove of documents about the brief Furby panic of 1998 and 1999 at America’s top spy agency, in which it banned the toy from its offices as a potential spy device, discussed the toy’s ability to “learn” using an “artificial intelligent chip onboard” on an internal listserv, and ultimately was embarrassed by attention from the press after an employee leaked news of the ban to The Washington Post. The NSA’s interest in and concern with the spying capabilities of the Furby—the iconic furry robot toy—has been documented over the years by various news outlets, YouTube channels, and the Federal Aviation Administration (which banned Furby operation during takeoff and landing). But previous write-ups rely on a brief news story in the Washington Post from January 13, 1999 called “A TOY STORY OF HAIRY ESPIONAGE,” which noted that Furby had been banned from the NSA’s offices in Maryland in part because they were worried that NSA employees would discuss classified information to the Furby, which could learn from it and would possibly repeat what it’d heard at a later date. kotaKat, a \"snarky internet bobcat\" whose interests include furry fandom and infosec, filed a Freedom of Information Act request with the NSA a year ago because he was “bored in a group chat one night” and was discussing the episode, he told me. “I myself kept hearing about the stories about the ‘Furby Alert’ (turns out it's actually the ‘Furbie Alert’) but nobody ever actually provided hardline evidence of it. I just wanted to know what the policy memo on the NSA Intranet was.” I have acquired the fabled NSA \"FURBIE ALERT\" memo. I have a significant amount of documentation that came back on an FOIA and I'll be scanning it in the coming days. Stay tuned. pic.twitter.com/Fyo04dm4Oo — (da)kota/the/Kæt (@dakotathekat) January 22, 2024 On Monday, kotaKat got a manilla envelope of documents in the mail from the NSA mailed to their house with more than 60 pages of documents, including the original listserv thread, employee discussions about whether the Furby was actually a security threat, an internal memo trying to do damage control over the Washington Post story, and an exhaustive list of all of the media outlets that originally wrote about the ban. As a public service, they quickly scanned the documents and published them on the Internet Archive: “you can't type fast enough to explain the level of shitpost request you made a year ago to the Department of Defense and suddenly get 60+ pages in a manilla envelope sent straight to your doorstep.” This post is for paid members only Become a paid member for unlimited ad-free access to articles, bonus podcast content, and more. Sign up for free access to this post Free members get access to posts like this one along with an email round-up of our week's stories. Subscribe Already have an account? Sign in",
    "commentLink": "https://news.ycombinator.com/item?id=39106355",
    "commentBody": "The NSA Furby Documents (404media.co)213 points by gumby 16 hours agohidepastfavorite100 comments Pwntastic 15 hours agoThe FOIA documents are up on archive.org now: https://archive.org/details/nsa-furby-memo/ I'm amused at page 8 of the listserve doc, in which someone points out that the ongoing discussion may at some point be released to the public under FOIA and to consider how it might look after showing up on the front page of a news site reply j-wags 15 hours agoparentIt's interesting to see how quickly the norms around cybersecurity changed. In 1999 the NSA was worried about avoiding ridicule for banning simple electronics in secure areas. In 2010 Stuxnet was introduced via simple electronics into a secure area and set back the Iranian nuclear program by several years. Some of the people receiving these furby emails were probably already conceiving of (or actively working on) Stuxnet-like capabilities. Maybe a future FOIA request will reveal several teams quietly emailing up the org chart to absolutely not relax the rule for furbies. reply gnfargbl 14 hours agorootparentNSA dealt with cases of espionage via the introduction of simple electronics into secure areas decades before [1] [2], so awareness of the risk was likely widespread. The issue here seems to have been that in 1999, it was a relative novelty for random consumer devices to have a recording functionality. Hard to imagine now, but there we are. [1] https://en.wikipedia.org/wiki/The_Thing_(listening_device) [2] https://www.cryptomuseum.com/covert/bugs/selectric/index.htm reply FirmwareBurner 12 hours agorootparent>in 1999, it was a relative novelty for random consumer devices to have a recording functionality. Hard to imagine now, but there we are. For added context, the plot of the movie Charlie's Angels from the year 2000, was about stopping an evil guy from using some evil software he developed to ... track people using their cellphones. (ㆆ _ ㆆ) reply kevin_thibedeau 10 hours agorootparentprevThe IR transmit capability of the Furby was a legitimate threat. PalmPilots of the era were permitted inside a SCIF providing they had their IR ports covered by an opaque sticker and weren't a model with a radio. reply halJordan 12 hours agorootparentprevWikipedia claims the nsa's active cyber mission (anachronistic terminology ) was up and running from as early as 1997, so there were definitely people having those thoughts and working those capabilities. And we're totally ignoring people like Markus Hess in the 80s. Thank you for taking the time to add perspective to the knee jerk reactions. reply wolverine876 14 hours agorootparentprevNSA is a military agency; their norm has always been to protect US assets and attack others. reply halJordan 12 hours agorootparentThe NSA is not a military agency. It is within the dod, it provides combat support. But it is emphatically not a military agency. reply callalex 5 hours agorootparentIt’s important to remember that “DOD” is a pathetic rebranding attempt of what it was originally founded as and continues to operate as: the Department of War. reply crmd 11 hours agorootparentprevEmphatically? The director of the NSA is required to be a four star general and concurrently serves as commander of US Cyber Command. Ostensibly non-military, perhaps. reply jacoblambda 5 hours agorootparentThe director is required to be a commissioned officer (and upon taking the role gets the O-10 grade) because they also head Cyber Command and the CSS (Central Security Service) but they don't really \"run\" the NSA like you'd expect say the director of the FBI or CIA to. Their role instead is mostly to coordinate the interaction between the NSA, CSS, and CYBERCOM on top of running CYBERCOM itself. When it comes to actual day to day operations however the deputy director (who is required to be a technically experienced civilian) actually runs things and reports directly to the president. The director isn't actually in the deputy director's chain of command however one of their job roles is to provide advisory support to the director when needed. reply dmix 4 hours agorootparentprevThe assistant Secretary of Health is also a commissioned 4 star admiral but they have never seen been the inside of a Navy ship. The last one was a pediatrician before joining. There's a lot of symbolic and tradition based commissions like that through the US gov. reply jandrewrogers 5 hours agorootparentprevYou are incorrect. There are two categories of intelligence agencies in the US as a matter of law, with significantly different legal authority and structure. Intelligence agencies like the CIA, FBI, et al are non-military. NSA is explicitly a military agency and operates under military authority. The easiest way to tell the difference is that the Director of military agencies is always an active military flag officer. It isn't just a superficial distinction, they operate quite differently. reply wolverine876 5 hours agorootparentprevHow are you defining \"military\"? reply DANmode 3 hours agorootparentSubscribed. reply pharrington 10 hours agorootparentprevThe NSA literally performs cyber attacks on foreign countries. reply sandworm101 12 hours agorootparentprevIt is no more a military agency than NASA or the USGS. Having military customers doesn't make an agency or company part of that military. reply wolverine876 6 hours agorootparentNSA is in the Department of Defense, commanded by a uniformed officer, employing many (how many?) members of the military. reply kotaKat 13 hours agoparentprevYep. Who would have guessed 25 years later I'd be bored and then a year later this packet showed up at my doorstep? It's oddly perfect timing, around all the AI discourse. :) reply qingcharles 11 hours agorootparentI need to redo my FOIA request [0]. I was investigated by the Secret Service in 1996 as they thought I intended to assassinate President Clinton. This was down to me selling a selling a shell account on a Linux server to someone, who in retrospect, might have had fundamentalist ideals and that person sending a detailed email to the White House outlining their plot, from my domain. I always wanted to see the chain of events that led to the Special Branch turning up on my door step in England. [0] I FOIA'd this a couple of years back, but I changed address and never got the documents, only a letter to say it was being worked on. reply 0xEF 13 hours agorootparentprevI'm surprised it only took them a year. Would you care to share more about your experience on filing FOIA? The circles I run in seem to view it as a clunky, bloated process, but I feel like it has gotten better than when it was introduced. I have zero first-hand experience, though. reply kotaKat 13 hours agorootparentNo problem to help, but bad news: Every government agency has different processes. You'll have to go through their own FOIA office. The NSA FOIA form is actually really easy: https://www.nsa.gov/about/contact-us/Submit-a-FOIA-Request/ I simply asked for what I wanted (information about policy memos about 'Furby Alerts' and recording devices at the NSA from late 1998 to early 1999) and submitted the form. About a month later I got a response back from the NSA acknowledging they got my request, and located records that were part of another FOIA request being processed as well, so I'd get those documents as well once released. And then... yesterday afternoon I got the message \"hey what did you get from the DoD?\" - bewildered, sending me a photo of the cover (in the full article). They finally delivered, and I hastily scanned my spoils for everyone. :) reply qingcharles 11 hours agorootparentAnd to piggyback on your comment. State FOIA is a different beast to federal FOIA. Lots of states have much tighter timelines. Illinois requires the government body to respond with the records within 5 business days. reply trevyn 11 hours agorootparentprevhttps://www.muckrock.com/ seems to have a lot of good information on this too, and fun-to-look-through archives of requests and responses. reply jdewerd 15 hours agoparentprevThey wanted to avoid FURBYGATE. They avoided FURBYGATE. Sounds reasonable to me! reply nerdponx 14 hours agorootparentRight. The whole email thread seems very reasonable to me. TFA characterizing this as \"freaking out\" is nonsense. reply neilv 11 hours agoprevNote that this was several years after performance artists (not even state-level actors) had demonstrated compromising toys retail supply chain with hacked firmware. https://www.mentalfloss.com/article/547659/barbie-liberation... So, look of concern at whomever thought it was a good idea to bring an effectively blackbox electronic device with a microphone into a secure area where those were prohibited. Kudos to whomever raised the issue. Someone should've done a proof of concept mod (firmware or hardware) of a Trojan Furby to appear (to visual and X-ray inspection) to have the stock hardware, but do something nefarious. Or shown how, say, the stock Furby hardware and firmware turned sound into RF leakage. reply refulgentis 15 hours agoprevContext: Furbys were the toy for a year or two, and were actively marketed as learning from speech, had an active mic, and did adjust their speech based on what they heard, \"learning\" to speak English from Furbish. [^1] It's not so different from the fundamental fear of Alexa/Assistant/microphones that's fairly well diffused now. Except the Furby actively claimed to learn how to speak based on your speech, and had a built-in feedback loop to make it appear as such. In retrospect it looks like it more was \"shift mix towards English based on how much you've heard\" than \"add words you heard to your speech patterns\" [^1]: https://www.listenandlearn.org/blog/no-you-cant-teach-your-f... reply yorwba 15 hours agoparentOf course some people really wanted to teach it to say new things, and figured out how to swap out the audio files (among other modifications): https://github.com/Jeija/bluefluff Fun fact: If you mess up and need to reset the furby, the procedure is to turn it upside down and hold down the tongue while pulling the tail for ten seconds. reply patrickmay 14 hours agorootparentInstructions unclear. Toddler still not speaking clearly, but appears upset. reply gumby 4 hours agorootparentRepeat until it works, or discard and produce another. reply zenolove 11 hours agorootparentprev> What I have achieved so far > • Understand large parts of Furby's BLE communication protocol > • Open a secret debug menu in Furby's LCD eyes Then I looked at the project logo again and it spooked me out reply folmar 11 hours agorootparentprevNote that this works for Furby Connect, original Furby had IrDA only. reply kube-system 15 hours agoparentprevMany voice assistants do record your voice and send those recordings elsewhere: e.g. https://www.amazon.com/gp/help/customer/display.html%3FnodeI... reply duskwuff 14 hours agorootparentThe Furby came out in 1998. Less than 50% of US homes even owned a computer at the time, let alone had Internet access (and that was usually dialup if they did). Cellular networks were largely voice-only and quite expensive. In short: even if Furbies had some way to record data (which they didn't), there would have been no practical way for them to exfiltrate it. reply pnw 13 hours agorootparentPracticality has never been an issue for spies. Look at the lengths the Soviets went to for surveillance. https://en.wikipedia.org/wiki/The_Thing_(listening_device) reply jabyess 13 hours agorootparentthe craziest thing about this is: > The Thing was designed by Soviet Russian inventor Leon Theremin,[7] best known for his invention of the theremin, an electronic musical instrument. reply hoseja 1 hour agorootparentHuh. Makes sense actually, both are induction-at-distance audio devices. reply kube-system 14 hours agorootparentprev> there would have been no practical way for them to exfiltrate it. Pick it up and carry it? It's not like analog tape recorders are permitted in these places either. All outside recording devices are banned. See the link in the now top-comment: https://news.ycombinator.com/item?id=39107224 reply rezonant 8 hours agorootparentprevOn the other hand, Ooh, looks like this Furby has learned the English words \"new\", \"nuclear\", \"facility\" and \"Ohio\"! /s reply refulgentis 15 hours agorootparentprevAbsolutely. Is there a portion of my comment that indicated otherwise? I can still edit it for clarity (I thought that wasn't allowed after a reply occurred) reply kube-system 15 hours agorootparentI'm not arguing with you, just adding to the conversation. While the Furby was feared to be recording, but actually wasn't, voice assistants can be a real concern in that they actually do. reply pvg 15 hours agorootparentprevOne of these fears is rational and based on things people know are in fact taking place. The other one is isn't, so drawing the parallel seems iffy. Maybe it's a little closer to the fear your phone is listening to you and that's how you get eerily targeted ads when browsing the web. reply odyssey7 15 hours agorootparentWhat makes the two fears fundamentally different? reply pvg 15 hours agorootparentOne is the fear of the possible consequences of something you know - with a voice assistant, you know you are being recorded and the recordings are sent somewhere. 'Is furby spying on me' is a vague suspicion but it's not (for most people with the fear) based on any known facts about the furby. reply refulgentis 15 hours ago [flagged]rootparentnext [3 more] I don't think you read my comment fully, the Furby thing was real, based on known facts, that were trumpeted by the manufacturer. The idea the Furby was \"[not] real\" persisting after reading the comment, is probably why it seemed like I was saying the voice assistants don't record voice. reply pvg 14 hours agorootparentI read the comment and explained why I don't think it's the fear of the same thing. Maybe you didn't read my comment fully! A Furby didn't have the capacity to meaningfully spy on you. You could be afraid that it actually does but it didn't. A voice assistant is already, in a sense, actually spying on you and you know that - the manufacturer tells you upfront. These aren't the same kind of fear. reply refulgentis 12 hours agorootparentThe Furby manufacturer told you upfront: - it listened all the time - it learned to speak, word by word, via your speech The first comment, 10 comments up, was specifically written to provide that context: the Furby manufacturer was up front about spying. Working with you, and steel-manning your contributions: - You're trying to explain a distinction you see between local data processing and remote data processing. i.e. a microphone in a room recording you isn't spying, but a microphone with a data connection is \"in a sense, actually spying\" on you \"meaningfully\". - example: \"the Furby didn't relay audio data anywhere other than the Furby, and I'd like to point out the voice assistant does - your comment intends to highlight the Furby listened, but it only listened locally. Mentioning voice assistants and using them in an analogy may give a reader the understanding voice assistants process data locally, like Furbys\" reply ClassyJacket 9 hours agoparentprevThat confirms what I remember from that time - kids were convinced you could teach it to say things, but I never saw any of them succeed. I think a big toy company would be somewhat averse to having their flagship product spew racial slurs. reply IAmLiterallyAB 9 hours agoprevFun fact: the 2016 model Furby uses an obscure ISA you've never heard of (and if you have, let me know!) called µnSP. I managed to get Pong running on a Furby a few years ago and by far the hardest part was figuring out that completely undocumented instruction set. Wrote added support for it to Ghidra which was a cool learning experience reply huppeldepup 4 hours agoparentis this documented somewhere? reply swozey 15 hours agoprevWhats gov policy around Alexas and like half the IOT market? My botvac even has a microphone. I'm sure it's \"don't ever speak about outside of this room\" sort of thing. I guess phone calls would be over a secure line. Are there secure cell phone towers/whatever? I'm curious how gov phones are hardened. reply alistairSH 15 hours agoparentIn any SCIF or SCIF-like office space, they're all prohibited. You leave your cell phone at the front door of the secured area. Internet access is via SIPRNet (for classified) or NIPRNet (non-classified, but secured). Phones are through dedicated secure switchboards. The above is common in the DC area (lots of DoD contractors). reply tylerflick 15 hours agorootparentA relative of mine used to work in this space 20 years ago. Seems policies haven’t changed at all. Tangental story about how serious the Gov takes OpSec. When I was in Iraq, a Marine in my unit found a roll of red Classified tape. He thought it would be cool to put a strip on his personal laptop, which was confiscated almost immediately. It was very clearly a personal machine, but policy is policy, and he never got that laptop back. reply alistairSH 15 hours agorootparentOh yeah, they take it seriously most of the time. But you do get seemingly odd outputs from those procedures. Case in point... Many years ago, I worked part-time for a small construction cost management contractor. They did some TS work for DoD/State (usually combo projects, where NSA/CIA/Army had a wing of a consulate that State managed). I did not have a TS (or any other clearance) at the time. One day, I'm tasked with counting the windows and doors in an old hospital in Munich. All the room numbers are Sharpied out in one half of the building. So, it's pretty obvious \"men in black pajamas\" are using that wing. I just don't know the room numbers. Seemed super weird to me that only the numbers were considered secured info. I'm sure there was an explanation. Years later, a friend-of-a-friend was moving to Munich to do \"State Department\" work (he was an HVAC contractor with a TS). Off hand, I said \"oh, I bet you'll be in wing X, floor Y or Z in the old hospital\". He about fell over that somebody in no way associated with his agency would know that. Got a chuckle from me. reply coolspot 14 hours agorootparentThank you for publishing this info, comrade! Ve arr going to chek all old Munich hospitals. reply alistairSH 11 hours agorootparentIt may or may not be in Munich. Regardless, WikiLeaks already spilled the beans. reply hwillis 13 hours agorootparentprev> Seems policies haven’t changed at all. Yes and no. CUI was created: https://en.wikipedia.org/wiki/Controlled_Unclassified_Inform... The number of SCIFs increased a ton, especially in contractors being allowed to have their own SCSI rooms. The number of clearances also went up a lot, and the cycle time on granting a clearance got much faster. Overall some things got relaxed, other things got stricter, scale increased everywhere. IMO the biggest factor in the increase is just the ever-increasing DoD budget reply px43 12 hours agorootparentprevI like this idea of magical red tape that makes things disappear. Did he test it on any other items? reply akira2501 11 hours agorootparentprev> Tangental story about how serious the Gov takes OpSec. ...and yet, Chelsea Manning walked in with nothing more than a CD player and a self labeled CD-RW and exfiltrated tons of data from a secured facility. > and he never got that laptop back. There are several morals to this story. reply pastword 13 hours agorootparentprevFrom a friend who worked in IT work at DIA c. 2000: there were an absurd, non-zero number of researchers with clearances who surfed for porn while on [SN]IPRNet, networks they knew were monitored, and unsurprisingly were caught and lost their careers. Nonzero. I'd posit the reason it continued for so long was the real reasons for termination were kept secret to avoid organizational and political embarrassment but at the expense of not setting an example. If individuals in this particular demographic are hired but lack self-control and are sexually frustrated, then they're potentially huge liabilities to being recruited by adversaries (MICE). It would seem that before issuing clearances, these factors should be assessed rather than going through a standard clipboard audit by the FBI. And, while holding clearances, positive socialization opportunities should be encouraged if not artfully arranged. Who's ever going to leave a job or be disloyal when your boss or some coworkers expedite the love lives of those who aren't already full in that regard? This implies fostering a layer of socially astute managers. It would be a radical departure for government culture perhaps, but a necessary one to ensure the integrity and stability of a clandestine community. Happiness isn't just recognition or sufficient autonomy, but total happiness beyond work. (Throw away the \"work-life balance\" cliche that is tired and paid lip-service to.) reply RajT88 15 hours agorootparentprevMy company infosec training actually advises you don't have voice assistants or cellphones in your work area. They even make light of it in the video: \"I know it sounds crazy, but it's not\". Google and Amazon as the biggest voice assistant makers are, of course, our competitors. But they are competitors to I would say most software companies in some fashion. reply ljf 15 hours agorootparentWe have been told that so many times at work, but I know most snr people seem to leave them and their smart watches in listen mode as they occasionally go off in video calls. reply gumby 4 hours agorootparentOnce in a zoom call my watch said “sorry, I didn’t understand that”… and simultaneously the watch the other person on the call was wearing said the same thing! reply SnazzyJeff 15 hours agorootparentprevKnowing what exploits are like in the private/state sector that seems like a no-brainer if your threat model includes a well-funded attacker. reply miki123211 14 hours agorootparentprevI wouldn't be surprised if something like the Apple Vision Pro becomes common in such spaces (and for classified / company-confidential work in general) over the next few years. I think the combination of biometric authentication with a display that is immune to cameras and shoulder-surfing is really powerful. If the device has anti-screenshot protection and automatically logs the user out when removed from their head, there's virtually no way to quickly transfer sensitive documents out of it. reply l33t7332273 12 hours agorootparentI would be floored if that happened. SCIFs and cameras are like oil and water. reply chatmasta 14 hours agorootparentprevHow strictly are SCIF policies enforced? I'm just a civilian who's never had exposure to that world, but based on my experience with other parts of the government, I'd expect SCIF compliance to fall on a broad spectrum from \"sloppy or non-existent\" to \"overly strict and paranoid.\" Is my intuition accurate? Who's accountable for the compliance of a given SCIF - can anyone with clearance \"setup a SCIF\" or does it need to be registered, audited, etc? reply dwheeler 14 hours agorootparentIn my experience, they are seriously enforced, though any time you have a large number of people you'll definitely find exceptions. The threat of massive fines and long jail times tends to encourage compliance. Also, many of the people who work in SCIFs know they are dealing with information that, if released, could lead to a number of people getting killed (think intelligence sources) or a country being unable to defend itself because a US weapon system was compromised (think Ukraine). Nation-states are working to extract information from SCIFs, it's not a theoretical problem, and SCIF users know this. reply qingcharles 11 hours agorootparentprevI always remember the posters inside RAF secure spaces that say \"IN EVENT OF EMERGENCY, SECURE ALL HARD DRIVES, THEN EXIT THE BUILDING.\" reply alistairSH 12 hours agorootparentprevI don't work in this space, but many of my friends do, as did my father. SCIF policies are usually strictly enforced. But, that's the most secure workplace available to civilians and they aren't all that common. They also tend to be located in facilities that are higher-than-normal security. Out here in Reston, all my friends who work in SCIFs are also in fenced/gated complexes with paramilitary guards. There are secure (but not SCIF) facilities that probably vary more. My father's little 6 person contracting office had a secure room, with a Dod approved design and a safe inside, for contracts that required that level of security (State/DoD facilities in China and Russia required TS clearance, other projects varied). The people that work in SCIFs also generally take it seriously. TS+poly is worth a big chunk of salary here in DC and not something to risk (and that's ignoring that flaunting those laws is a felony for anybody not named Trump). And most believe in the mission (whatever that happens to be). The work spans everything from military hardware to CIA or NSA operations. And a lot of stuff that probably doesn't really need to be TS, but that's a whole other discussion. reply nox101 14 hours agorootparentprevI wonder how that's going to work in our augmented future. Especially if people replace non-functional eyes and ears with digital ones. reply nonameiguess 15 hours agoparentprevIt's actually more restrictive than the sibling makes it sound. A SCIF can't have any radio-transmitting device, recording device, or storage media without special approval. Computers hooked up to classified networks can't have USB ports. Even medical devices are case by case. My wife requires hearing aids and needed them to be analyzed and approved by a security team before she could bring them in. Pacemakers require approval. The phones and networks are hardened by being their own separate network from public networks. The lines are all buried and protected and utilize hardware-encrypted point to point tunnels to merge with public backbone fiber. I've told an anecdote here many times of working at a facility where AT&T contractors dug too close to a JWICS fiber cable and had an unmarked black SUV show up in minutes to confiscate all of their gear and question them. Keep in mind the military has been encrypting radio traffic over hostile territory for a century, so they don't even necessarily require the lines themselves to be physically secure as long as the endpoint devices are. Encryption keys are loaded from hardware random number generators that are synced manually on some rotating basis determined by local command or national policy, depending on the intended reach of the comms device. The NSA has something called a key management infrastructure for the wide-area computer net that replaced the legacy system a few years ago that is similar to PKI, but keys are only issued in-person and stored on unnetworked hardware key loaders that are kept in locked arms rooms on military installations (or with deployed units). There is, of course, also a DoD and IC PKI so they can still use develop and use regular web applications and browsers, but it is also more restrictive than regular PKI. Everything requires client certs and mutual TLS and you need to be personally sponsored to get your personal certificates. It's actually really cool the way the JWICS websites work because your client cert provides an identity that is linked to your sponsoring agency's clearance database and web apps automatically redact content on the server side that you are not cleared to see. It's possible I'm making up memories but I think I've seen at least a few cases where some applications can do this inside of a single page, but typically you get a denial for an entire application if you're not cleared for the highest level data it provides. I almost hate to say it because it's antithetical to the Internet and Hacker News ethos, but it's a testament to how well networked applications could work with a central authority and no anonymity. You don't need passwords. Accounts are provisioned automatically. SSO is global to the entire network. You only need one identity. But no, your office can't have Alexa. reply mhink 14 hours agorootparent> I almost hate to say it because it's antithetical to the Internet and Hacker News ethos, but it's a testament to how well networked applications could work with a central authority and no anonymity. You don't need passwords. Accounts are provisioned automatically. SSO is global to the entire network. You only need one identity. But no, your office can't have Alexa. I don't think it's necessarily a dealbreaker if you consider this: from a purely technical standpoint, there's nothing really stopping anyone from setting up a certificate authority- the only issue is getting service providers to trust it enough to accept those client certs as sufficient identification. I could easily imagine a world where I receive an \"official\" client cert from a government (which I can use to thoroughly prove my identity if needed) as well as several \"pseudonymous\" certs from various other CAs that I may use from time to time. The main difference between CAs would be the kind of attestations they provide for a given certificate holder. For example, I could imagine a CA which (for example) is set up to attest that any holder of a certificate signed by them is a medical doctor, but will not (by policy) divulge any additional information. Or perhaps a CA which acts as a judge of good character- they may issue pseudonymous or anonymous certs, but provide a way for application owners to complain about the behavior of a user presenting that cert. I'm sure there are plenty of holes that can be poked in this model but I don't think it'd be completely out of the question? reply sandworm101 14 hours agoparentprevThere is an entire industry for secure phones. Many have to be \"unlocked\" before dialing other secure phones. It isnt simple. Getting a normal phone line to passively carry an encrypted call is a bit of a hack. reply hiatus 14 hours agorootparent> It isnt simple. Getting a normal phone line to passively carry an encrypted call is a bit of a hack. How so? It would seem fairly trivial considering we have ways of sending data over phone lines as sound for decades. reply sandworm101 14 hours agorootparentBecause the signal transmitted over normal phones has to be encrypted. That encrypted signal will then be digitized/compressed by the standard phone line. Any artifacts in the phone line digitization might turn the encrypted signal into gibberish. Its like compressing a jpeg too many times. So you need an encryption method that isnt simple digitization. You need something that is encrypted but essentially sounds like human speech so that the digitization/compression process does not damage it. https://gdmissionsystems.com/products/encryption/secure-voic... https://www.cryptomuseum.com/crypto/gd/viper/ reply WatchDog 10 hours agorootparentWe all used this kind of advanced technology to connect to the internet back in the 90s. reply sandworm101 8 hours agorootparentNot really. The phone lines were not compressed then. reply dTal 14 hours agorootparentprevA hack? The entire point of encryption is to permit messages to be sent over insecure channels, no? reply sandworm101 14 hours agorootparentThe hack is getting the unsecure system not to damage your encrypted signal, to carry even though it is expecting plain voice talking rather than a stream of binary digits. reply CrazyStat 13 hours agorootparentWe’ve been doing that for dialup internet for decades. reply richardwhiuk 13 hours agorootparentDialup actively co-operates with the telephone system - e.g. the screeching at the start is designed to disable echo cancellers and other such mechanisms. reply sandworm101 12 hours agorootparentprevDialup doesn't work over every phone line, especially over sat voice lines. reply arpa 12 hours agorootparentprevPOTS didn't have an opus audio codec. reply aerostable_slug 15 hours agoprevYears ago, I used to see low quality sun-faded warnings printed from color inkjets about Furby on entries to NNSA secure spaces. I hadn't thought about that little fellow in the longest time... I'm guessing there are still a few taped up in various Labs at less-used portals. reply moi2388 3 hours agoprevSeems sensible. Even if the furbie by design weren’t spying, they definitely could have been used as such. No non-agency bought assets or assets which are not screened completely should never be in their offices. Toy or not. For the same reason you’d expect them to ban USB sticks within the premises reply itishappy 16 hours agoprevNow I'm curious if there's any evidence of Furbies actually being used for espionage. reply bragr 15 hours agoparentFurbies just have a simple microcontroller and the code has been released [1]. It's a clever bit of code to give the impression of intelligence, but it doesn't have anything like the abilities in urban legends. You could put other hardware in them of course, they'd be prime targets for that kind of thing. [1] https://archive.org/details/furby-source/mode/2up reply masswerk 13 hours agorootparentOMG, it's 6502 code! (Or for some variant of the 6502.) Marginally interesting, the source uses standard MOS assembler syntax, but Intel-like xxH notation for hex values, rather than $xx. --- [Edit] According to Wikipedia, actually a Sunplus SPC81A microcontroller using the 6502 instruction set, but lacking the Y register: > The first Furby model was based around a 6502-style Sunplus SPC81A microcontroller, which had 80 KiB of ROM and 128 bytes of RAM. Its core differed from the original 6502 in the lack of the Y index register. The TSP50C04 chip from Texas Instruments, implementing the linear predictive coding codec, was used for voice synthesis. https://en.wikipedia.org/wiki/Furby reply wolverine876 14 hours agorootparentprev> It's a clever bit of code to give the impression of intelligence AI hasn't changed. reply rarely 15 hours agoparentprevit's certainly possible with a modified furby. there could have been a voice recorder placed inside, but that threat seems possible with other toys as well, maybe there was some opportunity due to the popularity of furbys. in terms of the furby's unmodified hardware capabilities, the microphone was simply used for volume level reaction. reading through the furby's firmware, the mic was used as a peak volume input. reply xsmasher 12 hours agoparentprevProbably better to hide your microphone in something that is more commonplace and doesn't already have a security hysteria around it. reply n4jm4 14 hours agoprevAnalysis is fun, but any device with a microphone or camera represents a security risk for sensitive environments... Fropies. reply nickaflip 12 hours agoprevI used to like reading 404, but they need to chill out on posting articles about porn. reply 1oooqooq 12 hours agoprevthese are the caliber of the American praetorian guard who owns our politicians. reply CamperBob2 11 hours agoparentEh, I don't see anything inappropriate in these documents. If they seem overly paranoid, it's because some major security breaches have historically involved silly things like this. Where do you draw the line between a Furby and a Casio SK-1 and a Teddy Ruxpin and a Minidisc recorder and any number of other stateful gadgets of the day, especially when the Furby is brand new and nobody really knows what's inside it? The NSA is an intelligence agency. The NSA doesn't want people bringing things in that might have the ability to exfiltrate voices or other signals, and in any event the NSA doesn't want random employees talking about it to the press. Where's the element of surprise here? I don't understand why it was even newsworthy in the first place. As for the intelligence agencies \"owning our politicians,\" LOL. If there were the slightest truth to that, Trump's headstone would read \"1946-2016.\" reply itomato 15 hours agoprevTell us more about these IRC channels responsive to FOIA request reply cush 12 hours agoprev [–] Pretty sure this was a Simpsons episode reply px43 12 hours agoparent [–] The Simpsons episode was referencing the media frenzy when this happened in 1999. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The NSA has recently disclosed documents regarding its ban on the Furby toy in the late 1990s, suspecting it could be exploited as a surveillance tool.",
      "The released documents, obtained through a Freedom of Information Act request, comprise internal conversations, a memo outlining the ban, and a list of media sources that reported on the incident.",
      "The documents are now publicly accessible and can be found online, though paid members have unrestricted access, while free members receive a weekly summary of news stories."
    ],
    "commentSummary": [
      "The discussion delves into security concerns surrounding devices like Furbys and voice assistants with recording capabilities.",
      "The NSA's classification as a military agency and experiences with filing FOIA requests are also discussed.",
      "The conversation explores the potential risks of compromised toys and electronic devices, as well as the importance of government security protocols and procedures."
    ],
    "points": 213,
    "commentCount": 100,
    "retryCount": 0,
    "time": 1706030464
  }
]
